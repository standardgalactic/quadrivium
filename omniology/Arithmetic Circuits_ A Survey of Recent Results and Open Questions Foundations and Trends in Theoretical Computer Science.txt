Foundations and Trends R
⃝in
Theoretical Computer Science
Vol. 5, Nos. 3–4 (2009) 207–388
c⃝2010 A. Shpilka and A. Yehudayoﬀ
DOI: 10.1561/0400000039
Arithmetic Circuits: A Survey of Recent
Results and Open Questions
By Amir Shpilka and Amir Yehudayoﬀ
Contents
1
Introduction
209
1.1
Basic Deﬁnitions
211
1.2
Arithmetic Complexity
212
1.3
Arithmetic Circuit Classes
217
1.4
Road Map
219
1.5
Additional Reading
223
2
Structural Results
224
2.1
Universal Circuits
225
2.2
Homogenization
227
2.3
Partial Derivatives
230
2.4
Depth Reduction
233
2.5
Coping with Division Gates
240
2.6
Discussion
243
3
Lower Bounds
245
3.1
Existence of Hard Zero-One Polynomials
245
3.2
General Circuits and Formulas
247
3.3
Monotone Circuits
251
3.4
Noncommutative Computation
253
3.5
Constant Depth Circuits
256

3.6
Multilinear Circuits and Formulas
265
3.7
Circuits with Bounded Coeﬃcients
277
3.8
Approaches for Proving Lower Bounds
279
3.9
Natural Proofs for Arithmetic Circuits?
288
3.10 Meta Lower Bounds
289
4
Polynomial Identity Testing
291
4.1
Generators and Hitting Sets
294
4.2
Randomized Algorithms
296
4.3
PIT and Lower Bounds: Hardness-Randomness Tradeoﬀs
302
4.4
Sparse Polynomials
311
4.5
Noncommutative Formulas
314
4.6
Depth-3 Circuits
321
4.7
Depth-4 Circuits
330
4.8
Read-Once Formulas
344
4.9
Relation to Other Problems
353
4.10 Concluding Remarks
359
5
Reconstruction of Arithmetic Circuits
361
5.1
Hardness of Reconstruction
363
5.2
Interpolation of Sparse Polynomials
366
5.3
Learning via Partial Derivative
367
5.4
Reconstruction of Depth-3 Circuits
372
5.5
Concluding Remarks
378
Acknowledgments
379
References
380

Foundations and Trends R
⃝in
Theoretical Computer Science
Vol. 5, Nos. 3–4 (2009) 207–388
c⃝2010 A. Shpilka and A. Yehudayoﬀ
DOI: 10.1561/0400000039
Arithmetic Circuits: A Survey of Recent
Results and Open Questions
Amir Shpilka1 and Amir Yehudayoﬀ2
1 Faculty of Computer Science, Technion, Haifa 32000, Israel,
shpilka@cs.technion.ac.il
2 Faculty of Mathematics, Technion, Haifa 32000, Israel,
amir.yehudayoﬀ@gmail.com
Abstract
A large class of problems in symbolic computation can be expressed as
the task of computing some polynomials; and arithmetic circuits form
the most standard model for studying the complexity of such computa-
tions. This algebraic model of computation attracted a large amount of
research in the last ﬁve decades, partially due to its simplicity and ele-
gance. Being a more structured model than Boolean circuits, one could
hope that the fundamental problems of theoretical computer science,
such as separating P from NP, will be easier to solve for arithmetic
circuits. However, in spite of the appearing simplicity and the vast
amount of mathematical tools available, no major breakthrough has
been seen. In fact, all the fundamental questions are still open for this
model as well. Nevertheless, there has been a lot of progress in the area
and beautiful results have been found, some in the last few years. As
examples we mention the connection between polynomial identity test-
ing and lower bounds of Kabanets and Impagliazzo, the lower bounds

of Raz for multilinear formulas, and two new approaches for proving
lower bounds: Geometric Complexity Theory and Elusive Functions.
The goal of this monograph is to survey the ﬁeld of arithmetic circuit
complexity, focusing mainly on what we ﬁnd to be the most interesting
and accessible research directions. We aim to cover the main results and
techniques, with an emphasis on works from the last two decades. In
particular, we discuss the recent lower bounds for multilinear circuits
and formulas, the advances in the question of deterministically checking
polynomial identities, and the results regarding reconstruction of arith-
metic circuits. We do, however, also cover part of the classical works
on arithmetic circuits. In order to keep this monograph at a reasonable
length, we do not give full proofs of most theorems, but rather try to
convey the main ideas behind each proof and demonstrate it, where
possible, by proving some special cases.

1
Introduction
Arithmetic circuits are the most natural and standard model for com-
puting polynomials. In this model the inputs are variables x1,...,xn,
and the computation is performed using the arithmetic operations +,×
and may involve constants from a ﬁeld F. The output of an arithmetic
circuit is thus a polynomial (or a set of polynomials) in the input vari-
ables. The complexity measures associated with such circuits are size
and depth which capture the number of operations and the maximal
distance between an input and an output, respectively.
The most fundamental problems in algebraic complexity are related
to the complexity of arithmetic circuits: providing eﬃcient algorithms
for algebraic problems (e.g., matrix multiplication), proving lower
bounds on the size and depth of arithmetic circuits, giving eﬃcient
deterministic algorithms for polynomial identity testing, and ﬁnding
eﬃcient reconstruction algorithms for polynomials computed by arith-
metic circuits (the latter problem is sometimes referred to as learning
arithmetic circuits or interpolating arithmetic circuits).
In the past 50 years, we have seen a ﬂurry of beautiful and eﬃcient
algorithms for algebraic problems. For example, Cooley and Tukey’s
algorithm for the Discrete Fourier Transform [38], Strassen’s algorithm
209

210
Introduction
and those following it for Matrix Multiplication [39, 131] (see [30] for
a detailed survey of algorithms for matrix multiplication), algorithms
for factoring polynomials (see [72, 146, 147] for surveys of results in
this area), and Csanky’s algorithm for parallel computation of deter-
minant as well as all other linear algebra problems [40]. In this survey
we shall not give details of these algorithms, but rather focus on com-
plexity questions related to arithmetic circuits, mainly on the problem
of proving lower bounds for arithmetic circuits and the question of
deterministically deciding polynomial identities.
Arithmetic circuits are a highly structured model of computation
compared to Boolean circuits. For example, when studying arithmetic
circuits we are interested in syntactic computation of polynomials,
whereas in the study of Boolean circuits we are interested in the seman-
tics of the computation. In other words, in the Boolean case we are not
interested in any speciﬁc polynomial representation of the function but
rather we just want to compute some representation of it, while in the
arithmetic world we focus on a speciﬁc representation of the function.
As such, one may hope that the P vs. NP question will be easier to solve
in this model. However, in spite of many eﬀorts, we are still far from
understanding this fundamental problem. In fact, our understanding
of most problems is far from being complete. In particular, we do not
have strong lower bounds for arithmetic circuits; We do not know how
to deterministically and eﬃciently determine whether a given arith-
metic circuit computes the zero polynomial; and we do not know how
to eﬃciently reconstruct a circuit using only queries to the polyno-
mial it computes. Although seemingly diﬀerent, these three problems
are strongly related to each other, and it is usually the case that a
new understanding of one problem sheds light on the other problems
as well.
In recent years there has been some progress on these important
problems for several interesting classes of arithmetic circuits. In this
monograph we aim to describe this recent progress. In particular, we
shall cover the new lower bounds on the size of multilinear circuits, the
new identity testing algorithms for several restricted classes of circuits
and their connection to circuit lower bounds, and the recent reconstruc-
tion algorithms for depth-3 arithmetic circuits. We also present many

1.1 Basic Deﬁnitions
211
open questions that we view as natural “next step” questions, given
our current state of knowledge.
1.1
Basic Deﬁnitions
Before any further discussion, we give the basic deﬁnitions related to
arithmetic circuits.
Deﬁnition 1.1(Arithmetic circuits). An arithmetic circuit Φ over
the ﬁeld F and the set of variables X (usually, X = {x1,...,xn}) is a
directed acyclic graph as follows. The vertices of Φ are called gates.
Every gate in Φ of in-degree 0 is labeled by either a variable from X
or a ﬁeld element from F. Every other gate in Φ is labeled by either ×
or + and has in-degree 2. An arithmetic circuit is called a formula if it
is a directed tree whose edges are directed from the leaves to the root.
Every gate of in-degree 0 is called an input gate (even when the gate
is labeled by a ﬁeld element). Every gate of out-degree 0 is called an
output gate. Every gate labeled by × is called a product gate and every
gate labeled by + is called a sum gate. The size of Φ, denoted |Φ|, is the
number of edges in Φ. The depth of a gate v in Φ, denoted depth (v),
is the length of the longest directed path reaching v. The depth of Φ
is the maximal depth of a gate in Φ. When speaking of bounded depth
circuits — circuits whose depth is bounded by a constant independent
of |X| — we do not have a restriction on the fan-in. For two gates u
and v in Φ, if (u,v) is an edge in Φ, then u is called a child of v, and v
is called a parent of u.
An arithmetic circuit computes a polynomial in a natural way: An
input gate labeled by α ∈F ∪X computes the polynomial α. A product
gate computes the product of the polynomials computed by its children.
A sum gate computes the sum of the polynomials computed by its
children.
For a gate v in Φ, deﬁne Φv to be the sub-circuit of Φ rooted at v.
Denote by Xv the set of variables that occur in the circuit Φv. We
usually denote by fv the polynomial in F[Xv] computed by the gate v
in Φ. We sometimes abuse notation and denote by Φv the polynomial

212
Introduction
computed by v as well. Deﬁne the degree of a gate v, denoted deg(v),
to be the total degree of the polynomial fv (e.g., the total degree of
x2
1x2 + x1 + 1 is three, whereas the individual degrees are at most two).
The degree of Φ is the maximal degree of a gate in Φ.
It is clear that every polynomial f ∈F[X] can be computed by an
arithmetic circuit and by an arithmetic formula. The main question is
how many gates are needed for the computation.
The deﬁnition above shows an evident diﬀerence between arithmetic
circuits and Boolean circuits. While Boolean circuits can perform oper-
ations on the “bit representation” of the input ﬁeld elements, that are
not necessarily the arithmetic operations, arithmetic circuits cannot.
Nevertheless, most algorithms for algebraic problems ﬁt naturally into
the framework of arithmetic circuits.
One last thing to note is that we always regard an arithmetic circuit
as computing a polynomial in F[X] and not a function from F|X| to F.
In general, every polynomial deﬁnes a unique function, but a function
can usually be expressed as a polynomial in many ways. For example,
the polynomial x2 −x is not the zero polynomial as it has nonzero
coeﬃcients. However, over the ﬁeld with two elements, F2, it computes
the zero function. This distinction is especially important when study-
ing the identity testing problem. This is another diﬀerence between the
Boolean world and the arithmetic world.
Remark 1.1. For the rest of the survey, unless otherwise stated, the
results hold for arbitrary ﬁelds. In most cases, for simplicity of discus-
sion and notation, we do not explicitly state the dependence on the ﬁeld.
In general, the question of which ﬁeld we are working over is important
and can make a diﬀerence, both from a theoretical point of view and
from a practical point of view. The main examples of ﬁelds that the
reader should bear in mind are prime ﬁelds and the real numbers.
1.2
Arithmetic Complexity
Arithmetic complexity classes were ﬁrst deﬁned in the seminal works
of Valiant [138, 141]. Valiant gave analogous deﬁnitions for the classes
P and NP in the algebraic world, and showed complete problems for

1.2 Arithmetic Complexity
213
these classes. We now give a very brief overview of these classes and
state the main known results. As this material was covered in many
places, we do not give any proofs here. For a more detailed treatment
and proofs, we refer the interested reader to Refs. [29, 30, 61].
We begin by deﬁning the class VP, the algebraic analog of the class
P. Originally, Valiant called this class the class of p-bounded polyno-
mials (computed by “polynomially bounded” circuits), but nowadays
the notation VP is used (where V is an acronym for Valiant).
Deﬁnition 1.2. A family of polynomials {fn} over F is p-bounded if
there exists some polynomial t : N →N such that for every n, both the
number of variables in fn and the degree of fn are at most t(n), and
there is an arithmetic circuit of size at most t(n) computing fn. The
class VPF consists of all p-bounded families over F.
The polynomial fn(x) = x2n, for example, can be computed by size
O(n) circuits, but it is not in VP as its degree is not polynomial. One
motivation for this degree restriction comes from computation over,
say, the rational numbers: if the degree is too high then we cannot
eﬃciently represent the value of the polynomial on a given input by a
“standard” Boolean circuit. Also note that in the deﬁnition we do not
require the circuit computing fn to have a polynomial degree, but, as
we shall later see, this property holds without loss of generality (see
Theorem 2.2 below).
An interesting family in VP is the family of determinants,
DETn(X) =

σ∈Sn
sgn(σ)
n

i=1
xi,σ(i),
where X = (xi,j) is an n × n matrix, Sn is the set of permutations of
n elements and sgn(σ) is the signature of the permutation σ. It is a
nice exercise to ﬁnd a polynomial size arithmetic circuit for DETn that
does not use divisions.
Remark 1.2. For the rest of the survey we sometimes say polyno-
mial and mean a family of polynomials, e.g., when we talk of the

214
Introduction
determinant polynomial we actually talk about the family of deter-
minant polynomials.
We now deﬁne VNP, the algebraic analog of the class NP.
Deﬁnition 1.3. A family of polynomials {fn} over F is p-deﬁnable
if there exist two polynomially bounded functions t,k : N →N and a
family {gn} in VPF such that for every n,
fn(x1,...,xk(n)) =

w∈{0,1}t(n)
gt(n)(x1,...,xk(n),w1,...,wt(n)).
The class VNPF consists of all p-deﬁnable families over F.
Roughly speaking, VNP is the class of polynomials f so that given a
monomial, one can eﬃciently compute the coeﬃcient of this monomial
in f (this does not follow immediately from the deﬁnition, for more
details see, e.g., Refs. [61, 141]). To better understand the connection to
NP, one can think of the variables w = (w1,...,wt(n)) as the “witness,”
and so summing over all witnesses is the arithmetic analog of searching
for a witness in NP. The existential quantiﬁer in the deﬁnition of NP
is translated to the algebraic operation of addition. In some sense, this
makes VNP a version of #P as well. The canonical example for a family
in VNP is the family of permanents of n × n matrices
PERMn(X) =

σ∈Sn
n

i=1
xi,σ(i).
(1.1)
One way to see that permanent is in VNP is by Ryser’s formula that
also gives the smallest known circuit computing permanent (which is
also a depth-3 circuit).
Fact 1.1 ([114]). For every n ∈N, PERMn(X) = 
T⊆[n](−1)n−|T|
n
i=1

j∈T xi,j.
It follows by deﬁnition that VP ⊆VNP. Valiant’s hypothesis says
that VP is a strict subclass of VNP.

1.2 Arithmetic Complexity
215
Valiant’s hypothesis I: VP ̸= VNP.
As arithmetic circuits are more structured than Boolean circuits,
one could hope that proving Valiant’s hypothesis should be easier than
its Boolean counterpart. A very weak version of this statement was
proved in Ref. [61], where it was shown that in a non-associative world,
where variables are not assumed to satisfy the identity (xy)z = x(yz),
Valiant’s hypothesis I holds. This non-associative statement is an
evidence for the obvious: the more structured the world is, the easier
it is to prove lower bounds. Speciﬁcally, in a non-associative world
algorithms cannot exploit “symmetries” that follow from associativity.
Indeed, in such a world it is more diﬃcult to design algorithms and
lower bounds are easier to prove.
Besides deﬁning the classes VP and VNP, Valiant also gave complete
problems for these classes. He described a reduction between families
of polynomials and gave complete families with respect to it.
Deﬁnition 1.4. A polynomial f(x1,...,xn) over F is called a projec-
tion of a polynomial g(y1,...,ym) over F if there exists an assignment
ρ ∈({x1,...,xn} ∪F)m such that f(x1,...,xn) ≡g(ρ1,...,ρm). In other
words, f can be derived from g by a simple substitution. This deﬁnition
can be extended to projections between families of polynomials. The
family {fn} is a p-projection of the family {gn} if there exists a poly-
nomially bounded t : N →N such that for every n, fn is a projection of
gt(n).
Both VP and VNP are closed under projections, e.g., if f is in VP
then any projection of f is also in VP. Valiant showed that permanent
is complete for the class VNP.
Theorem 1.1 ([138]). For any ﬁeld F such that char(F) ̸= 2, the
family {PERMn} is VNP-complete. Namely, any family in VNP is a
p-projection of it.
Valiant’s hypothesis I is thus equivalent to proving a super-
polynomial lower bound on the size of circuits computing the perma-
nent. We note that a stronger version of Theorem 1.1 was proved in

216
Introduction
Ref. [61], where it was shown that permanent is VNP-complete even in
a very weak computational world where the variables are not assumed
to be commutative nor associative.
Valiant also showed that determinant is VP-complete with respect
to quasi-polynomial projections.
Theorem 1.2 ([138]). The family {DETn} is VP-complete with
respect to quasi-polynomial projections. That is, for any family {fn}
in VP there exists a function t : N →N satisfying1 t(n) = nO(logn) such
that fn is a projection of DETt(n). In fact, if we change the deﬁni-
tion of VP to VQP by replacing polynomial by quasi-polynomial (i.e.,
2polylog(n)), then determinant is VQP-complete.
This theorem follows immediately from the next two theorems that
show that arithmetic circuits are “shallow,” and that determinant can
“simulate” small formulas.
Theorem 1.3 ([143]). Let f be a degree r polynomial computed by
a size s circuit. Then f can be computed by a circuit of size poly(r,s)
and depth O(logr(logr + logs)).
Theorem 1.3 was proved in a seminal work of Valiant et al. [143].
It is commonly rephrased as VP = VNC2, where VNCk denotes polyno-
mial size and polynomial degree arithmetic circuits of depth O(logk n).
Clearly, VNC1 ⊆VNC2 ⊆... ⊆VP, and Theorem 1.3 shows that in fact
the chain halts after two steps. Since determinant is in VP, Theorem 1.3
implies that determinant has a formula of quasi-polynomial size (more
generally, every polynomial in VNC2 has a formula of quasi-polynomial
size).
Theorem 1.4([138]). For any polynomial f in F[X] that can be com-
puted by a formula of size s over F, there is a matrix A of dimensions
(s + 1) × (s + 1) whose entries are in X ∪F such that DET(A) = f.
1 Unless stated otherwise, logarithms are in base two.

1.3 Arithmetic Circuit Classes
217
As determinant is complete for VQP, an algebraic analog of the P vs.
NP question is the question of “embedding” permanent in determinant.
Valiant’s hypothesis II: VNP ̸⊂VQP.
This hypothesis is also known as Valiant’s extended hypothesis.
Stated diﬀerently, the hypothesis is that the permanent does not belong
to VQP. Thus, in order to prove Valiant’s extended hypothesis it suf-
ﬁces to prove that one cannot represent PERMn as the determinant of
a matrix of dimension quasi-polynomial in n. Currently, the best lower
bounds on the dimension of such a matrix are given by the following
theorem of [31, 92].
Theorem 1.5 ([31, 92]). Let F be a ﬁeld of characteristic diﬀerent
than two and let X = (xi,j)i,j∈[n] be a matrix of variables. Then, any
matrix A whose entries are linear functions in {xi,j}i,j∈[n] over F such
that DET(A) = PERMn(X) must be of dimension at least n2/2.
Here is a rough sketch of the idea behind Mignon and Ressayre’s
proof of Theorem 1.5. Compute the rank of the Hessian matrix, i.e., the
matrix of second partial derivatives, of both PERMn(X) and DET(A).
This rank for PERMn(X) is at least (roughly) n2, whereas for DET(A)
this rank is of order D, where D is the dimension of A.
Valiant’s extended hypothesis gives a way for reformulating a ques-
tion about circuits as a purely algebraic question: the VQP vs. VNP
problem is equivalent to the problem of embedding the permanent
inside the determinant. One advantage of this formulation is that the
combinatorial structure of circuits does not appear in it.
Open Problem 1. Improve the lower bound on the dimension of a
matrix A with entries that are linear functions in {xi,j}i,j∈[n] such that
DET(A) = PERMn(X).
1.3
Arithmetic Circuit Classes
In addition to the general model of arithmetic circuits, introduced
in Section 1.1, we will be considering several other, more restricted,

218
Introduction
classes of arithmetic circuits. In particular, we will be interested
in bounded depth arithmetic circuits, and even more speciﬁcally in
depth-3 and depth-4 circuits, in multilinear circuits, noncommutative
circuits and more. We shall now deﬁne some of these classes and
discuss their importance.
The model of bounded depth circuits was already deﬁned in
Section 1.1. Two important subclasses of bounded depth circuits that
we shall focus on in this monograph are depth-3 circuits, also known
as ΣΠΣ circuits and depth-4 circuits known as ΣΠΣΠ circuits. A ΣΠΣ
circuit is a depth-3 circuit with an addition gate at the top, a middle
layer of multiplication gates, and then a level of addition gates at the
bottom. A ΣΠΣ circuit with s multiplication gates compute polyno-
mials of the form s
i=1
di
j=1 ℓi,j(x1,...,xn), where the ℓi,j’s are linear
functions. Although a very restricted model this is the ﬁrst class for
which we do not have any strong lower bounds, over ﬁelds of charac-
teristic zero (see Section 3.5). Moreover, in Section 3.8.2 we discuss a
result of Raz [105] showing that strong lower bounds for (a restricted
subclass of) ΣΠΣ circuits imply super-polynomial lower bound on the
formula complexity of permanent.
Similar to depth-3 circuits, a ΣΠΣΠ circuit is composed of
four alternating layers of addition and multiplication gates. Thus,
a
size
s
ΣΠΣΠ
circuit
computes
a
polynomial
of
the
form
s
i=1
di
j=1 fi,j(x1,...,xn), where the fi,j’s are polynomials of degree
at most s having at most s monomials (i.e., they are s-sparse polyno-
mials). The importance of ΣΠΣΠ circuits stems for two main reasons.
Depth-4 is the ﬁrst depth for which we do not have strong lower bounds
for any ﬁeld of characteristic diﬀerent than 2 (over F2 lower bounds
follow from the results of Razborov and Smolensky [112, 130]). The
best known lower bounds, due to Raz [103], are smaller than n2 (see
Section 3.5). Another important reason is that, with respect to proving
exponential lower bounds, ΣΠΣΠ circuits are as interesting as general
arithmetic circuits. Namely, an n-variate degree n polynomial can be
computed by a sub-exponential arithmetic circuit if and only if it can
be computed by a sub-exponential ΣΠΣΠ circuit. This result, due to
Agrawal and Vinay [5], is discussed in Section 2.4. Furthermore, deran-
domizing the polynomial identity testing problem for such circuits is
almost equivalent to derandomizing it for general arithmetic circuits.

1.4 Road Map
219
Thus, in order to understand the main open problems in arithmetic
circuit complexity, one can focus on depth-4 circuits, instead of general
arithmetic circuits, without loss of generality.
Another important model that we discuss in this monograph is mul-
tilinear circuits. A polynomial f ∈F[X] is called multilinear if the indi-
vidual degree of each variable in f is at most one. An arithmetic cir-
cuit Φ is called multilinear if every gate in Φ computes a multilinear
polynomial. An arithmetic circuit Φ is called syntactically multilinear
if for every product gate v = v1 × v2 in Φ, the two sets Xv1 and Xv2 are
disjoint (recall that Xu is the set of variables that occur in the circuit
Φu). Syntactically multilinear circuits are clearly multilinear but the
other direction is not true in general.
While being a very restricted model of computation, multilinear
circuits and formulas form a very interesting class as for many multi-
linear polynomials, e.g., permanent and iterated matrix multiplication,
the currently best arithmetic circuits computing them are multilinear.
Indeed, computing a multilinear polynomial with a circuit that is not
multilinear requires some “non-intuitive” cancellations of monomials.
We do not however, that such “clever” cancellations occur, e.g., in small
arithmetic circuits computing the determinant. In particular, we do not
know today of polynomial size multilinear circuits computing the deter-
minant. Being a natural model for computing multilinear polynomials,
multilinear circuits are an interesting and an important class of circuits
and we discuss the best results known for them.
In addition to bounded depth circuits and multilinear circuits we
shall also study monotone circuits, noncommutative circuits, circuits
with bounded coeﬃcients and read-once formulas. We shall give the
relevant deﬁnitions when we ﬁrst discuss each of these classes.
1.4
Road Map
Here is a short overview of the content of this survey.
1.4.1
Structural Results
Due to its algebraic nature, the model of arithmetic circuits is more
structured than the model of Boolean circuits. As such, we are able
to prove results in the arithmetic world that in the Boolean case are

220
Introduction
still open. In Section 2 we discuss some of the works on the structure
of arithmetic circuits. These structural properties of arithmetic circuits
are also used as starting points to proving lower bounds. We now discuss
three examples of such structural results and their connection to lower
bounds.
A striking result due to [143] is that in the arithmetic world VP =
VNC2 (see Theorem 1.3). This is in contrast to the Boolean world, where
it is conjectured that P ̸= NC. Subsequently, Agrawal and Vinay [5]
proved a depth-4 version of this statement, showing that in order to
prove exponential lower bounds on the size of general arithmetic circuits
one just needs to prove exponential lower bounds on the size of depth-4
circuits.
A surprising result due to Baur and Strassen [16], that strongly relies
on the underlying algebraic structure, states that computing a polyno-
mial f(x1,...,xn) is essentially equivalent to simultaneously comput-
ing f and all of its n partial derivatives
∂f
∂x1 ,..., ∂f
∂xn . Thus, proving a
lower bound on the size of a circuit computing a set of polynomials is
as diﬃcult as proving a lower bound for a single polynomial. Alterna-
tively, perhaps more optimistically, proving lower bounds should not
be so diﬃcult, as instead of proving a lower bound on the computation
of a single polynomial we can try and prove a lower bound for circuits
computing many polynomials. This principle actually turned out to be
useful in at least two cases: showing that divisions are not necessary in
computing polynomials by general arithmetic circuits [133] and proving
lower bounds for multilinear circuits [107].
An interesting fact is that arithmetic circuits computing homoge-
neous polynomials can be transformed to be homogeneous, with only a
small overhead. Recently, Raz [105] proved that for formulas, this trans-
formation can be done at a smaller cost than what was known before.
In particular, Raz showed that if one can prove (very) strong lower
bounds on tensor rank then one obtains super-polynomial lower bounds
on formula-size. Since tensor rank is no other than the size of the small-
est set-multilinear depth-3 circuit computing the “tensor,” Raz’s result
says that a very strong lower bound for the (very restricted) model
of set-multilinear depth-3 circuits implies a lower bound for general
formulas.

1.4 Road Map
221
1.4.2
Lower Bounds for Arithmetic Circuits
One of the biggest challenges of algebraic complexity is proving lower
bounds on circuit-size. Unlike the case of Boolean circuits, super-
linear lower bounds on the size of general arithmetic circuits are
known [16, 133]. Contrarily, no strong lower bounds are known for
bounded depth arithmetic circuits. In particular, no super-quadratic
lower bound is known even for circuits of depth 4, when char(F) ̸= 2.
In Section 3 we survey the known lower bounds and discuss some
proofs in more detail. In particular, we explain Strassen’s degree bound
that gives a super-linear lower bound for general circuits [133] and
Kalorkoti’s quadratic lower bound on the size of general formulas [70].
We discuss the lower bounds for the size of bounded depth circuits [122,
103]. We then consider in detail depth-3 circuits, which is the “ﬁrst”
model for which proving lower bounds seems to be a diﬃcult task.
In this section we also explain the following two-step “technique”
for proving lower bound for arithmetic circuits. The ﬁrst step is based
on the fact that polynomials computed by small arithmetic circuits
can be presented as a sum of a small number of products of “simpler”
polynomials (this is one of the structural theorems that we prove). The
second step is using the so-called partial derivative method to bound the
complexity of such polynomials. By applying these two steps, we derive
lower bounds for various classes of arithmetic circuits, such as monotone
arithmetic circuits [68, 109, 121, 135] and multilinear formulas [102,
104, 108].
Finally, we present several approaches for proving lower bounds
on circuit-size, and discuss the possibility of generalizing the Natural
Proofs approach of Razborov and Rudich [111] to the algebraic setting.
1.4.3
Polynomial Identity Testing
Polynomial identity testing (PIT) is the problem of deciding whether
a given arithmetic circuit computes the identically zero polynomial.
Many randomized algorithms are known for this problem yet its deter-
ministic complexity is still far from understood. Recently, it was dis-
covered that this problem is strongly related to the question of proving
lower bounds [69].

222
Introduction
In Section 4 we ﬁrst survey and sketch the proofs of randomized
algorithms for PIT. We then discuss the relation between lower bounds
and derandomization of PIT algorithms. One of the surprising results
in this context is that a deterministic (black-box) polynomial-time
algorithms for PIT of depth-4 arithmetic circuits implies a (quasi-
polynomial time) derandomization of the problem for general arith-
metic circuits.
We then present several deterministic algorithms for restricted
classes of arithmetic circuits. We do not cover all known algorithms
but rather present what we view as the most notable techniques in the
area. Speciﬁcally, we give one of the many algorithms for sparse poly-
nomials [86]. We show a polynomial-time algorithm for PIT of noncom-
mutative formulas that is based on the partial derivative method [106].
We then describe two algorithms for depth-3 circuits with a bounded
top fan-in. The ﬁrst is the local ring algorithm of [81] that works in the
non-black-box model (which we refer to as the white-box model) and
the second is the algorithm of [43, 76] that is based on the rank method
(with the strengthening of [80, 117, 118]). After that, we present two
results for depth-4 circuits. The ﬁrst is by [116] that gave a polynomial
time PIT for the so-called diagonal circuits, based on the ideas of [106].
The second result is by [75] that gave a PIT algorithm for depth-4 mul-
tilinear circuits with bounded top fan-in, based on ideas from [76] and
[127]. Finally, we present the algorithm of [126, 127] for identity testing
of sums of read-once formulas that strengthen some of the results for
depth-3 circuits and that inﬂuenced [75].
1.4.4
Reconstruction of Arithmetic Circuits
In Section 5 we consider the problem of reconstructing arithmetic cir-
cuits, which is the algebraic analog of the learning problem of Boolean
circuits. This problem is clearly related to PIT, as an identity testing
algorithm for a circuit class gives a way of distinguishing between dif-
ferent circuits from that class and can thus be helpful in designing a
learning algorithm.
We discuss the similarities and diﬀerences between the reconstruc-
tion problem and analogous problems in the Boolean world. We then

1.5 Additional Reading
223
give some hardness results on the reconstruction problem. After that
we discuss several known reconstruction algorithms. First, we explain
how to reconstruct sparse polynomials. Then we discuss the multiplicity
automata technique of [17] and its extension for arithmetic circuits [85].
Basically, this technique can be thought of as learning via partial deriva-
tives. At the end, we move to depth-3 circuits with a bounded top fan-in
and sketch the algorithms of [77, 125] that are based on ideas from the
identity testing algorithm of [43, 77].
1.5
Additional Reading
We decided to focus this survey on recent results in arithmetic circuit
complexity, mainly on lower bounds and identity testing algorithms,
and so many beautiful results in algebraic complexity, both new and
old, were left out. We now mention some of the topics that are not
discussed in this monograph and give references to relevant papers.
Most of these topics are discussed in the comprehensive book [30] and
the (unfortunately, still relevant) survey of Strassen [134].
One important area that we do not cover is algorithms for algebraic
problems, an area that has been yielding many beautiful works. A par-
tial list of algorithms include Cooley and Tukey’s FFT algorithm [38],
fast matrix multiplication [39] (and the new algorithmic approach of
[36, 37]), eﬃcient polynomial factorization (see the surveys [72, 146]
and the recent [84]) and the deterministic primality testing algorithm
of [4].
Another topic that we do not really discuss is that of linear and
bilinear complexity. Here, one is interested in the complexity of com-
puting linear transformations and bilinear forms using linear or bilinear
circuits, respectively. The complexity of computing univariate polyno-
mials is another topic that we decided not to include. The interested
reader is referred to the aforementioned book [30] and survey [134].
Several other models of algebraic computations also received a lot
of attention. Among them we mention the Blum–Shub–Smale model of
computing over the reals and algebraic decision trees, more information
can be found in [9, 21, 30].

2
Structural Results
Arithmetic circuits are very structured compared to Boolean circuits.
For example, in the algebraic world we have useful notions such as
degree, homogeneity, and partial derivatives that do not have “nice”
counterparts in the Boolean world. In this section, we discuss some
results that describe deep understanding regarding the structure of
arithmetic circuits that were obtained in the last four decades. Some of
these results are fairly basic, like transforming a circuit into a homoge-
neous one. But others, such as the seminal work [143] showing that in
the arithmetic world P = NC2, are more sophisticated. We will try and
give the main ideas underlying each of the results without necessarily
giving full proofs.
We begin this chapter by proving a basic result: the existence of uni-
versal arithmetic circuits (Theorem 2.1). This result plays an important
role in the Elusive functions approach of Raz [103] that we present
in Section 3.8.3 (although, for simplicity, we present there a speciﬁc
instance that does not rely on Theorem 2.1). We then discuss Homog-
enization (Theorems 2.2 and 2.3) and multilinearization of arithmetic
circuits. After that we move to discussing the important result of Baur
and Strassen on computing partial derivatives (Theorem 2.5). Following
224

2.1 Universal Circuits
225
that we describe results on depth reduction (Theorems 2.7, 2.8, and
2.11), and prove Theorem 1.3. We end this chapter by explaining how
to cope with division gates (Theorems 2.13 and 2.14).
2.1
Universal Circuits
We start with a basic result of Raz [103] showing the existence of a
universal arithmetic circuit.
Recall that a polynomial f is homogeneous if all of its monomials
have the same degree. Given a polynomial f, we denote by Hi(f) its
homogeneous part of degree i. Namely, all monomials of degree exactly
i appearing in f. Clearly, f = r
i=0 Hi[f], where r is the degree of f.
We say that a circuit is homogeneous if each of its gates computes a
homogeneous polynomial.
Deﬁnition 2.1 (Universal circuits). A circuit Φ is called universal
for n inputs and n outputs circuits of size s, that compute homogenous
polynomials of degree r, if the following holds: For every n homogeneous
polynomials f1(x1,...,xn),...,fn(x1,...,xn) of degree r, that can be
simultaneously computed by a circuit of size s, there exists a circuit Ψ
computing f1,...,fn as well, such that the computation graph of Ψ is
the same as the graph of Φ.
In other words, a circuit is universal if for any circuit Ψ of size s,
there is an appropriate labeling of the inputs of Φ such that the result-
ing circuit computes the same polynomials as Ψ. Alternatively, every
circuit Ψ is a “projection” of the universal circuit.
The universal circuit that we shall describe has the following addi-
tional structure, which gives some information on “how do arithmetic
circuits compute polynomials.” Let Φ be a homogenous circuit. We say
that Φ is in a normal-homogenous-form if it satisﬁes the following prop-
erties: 1. All input gates are labeled by a variable, speciﬁcally, no input
gate is labeled by a ﬁeld element. 2. All edges leaving input gates are
connected to sum gates. 3. All output gates are sum gates. 4. Gates
are alternating, namely, if v is a product gate and u is a child of v
then u is a sum gate, and vice versa. 5. The fan-in of every product

226
Structural Results
gate is exactly two (we do not restrict the fan-in of sum gates). 6. The
out-degree of every addition gate is at most one.
The following theorem tells us that we can eﬃciently construct a
universal circuit.
Theorem 2.1 ([103]). For any nonzero integers s ≥n and r, we can
construct in time poly(s,r) a circuit Φ in a normal-homogenous-form
with at most O(r4s) nodes that is universal for n inputs and n outputs
circuits of size s that compute homogenous polynomials of degree r.
Proof. [Sketch] The proof is in two steps. First we show that for every
circuit Ψ there exists a circuit in normal-homogeneous-form computing
the same polynomial. More accurately, Ψ will be a “projection” of
the circuit that we will ﬁnd. Then we show that how to construct a
universal circuit by basically demonstrating a circuit that contains as
“sub-circuits” all possible normal-homogeneous-form circuits (with the
appropriate set of parameters).
Assume that we are given a circuit Ψ, computing a polynomial f,
that we wish to “embed” in the universal circuit. Theorem 2.2 below
shows that there exists a homogeneous circuit, of size O(r2s), comput-
ing H0(f),...,Hr(f) simultaneously. While this circuit is not necessar-
ily in normal-homogeneous-form, we can make it such by a few simple
changes: Condition 1 is easily satisﬁed by labeling input gates by new
variables when needed (thus, f is a obtained by substituting values to
some variables of the new circuit). Conditions 2, 3, and 6 are satisﬁed
by adding a few sum gates when needed. Condition 4 is satisﬁed by
adding product gates when needed. Condition 5 is satisﬁed as we do
not change the fan-in of product gates.
It remains to show that the graph we obtained can be embedded
in a universal graph. The proof of Theorem 2.2 gives a bound on the
number of sum and product gates in the circuit that we obtained at
the end of the ﬁrst step. Assume that there were s+ sum gates and s×
product gates. Consider a circuit consisting of 2r alternating levels of
sum and product gates containing s+ and s× gates each, respectively.
Now, for every possible edge between a sum gate and a product gate we

2.2 Homogenization
227
put a little gadget, involving a new “help-variable,” such that by setting
the help-variable to 0 we essentially “erase” the edge and by setting it
to 1 we keep the edge. It is not diﬃcult to see that such a gadget is
easy to construct and therefore we basically cover all possible circuits
of normal-homogeneous-form (of a certain size and degree) in this way.
It is clear that the circuit that was obtained in the ﬁrst step can be
computed by making the appropriate assignment to the variables of
this circuit.
2.2
Homogenization
We now discuss a simple and useful property of algebraic computation.
Namely, that we can decompose any computation to its homogeneous
parts without increasing the size by too much. This useful observation
is implicit in Ref. [133].
Theorem 2.2([133]). If f has an arithmetic circuit Φ of size s, then
for every r ∈N, there is a homogeneous circuit Ψ of size at most O(r2s)
computing H0[f],H1[f],...,Hr[f].
This transformation allows us to assume without loss of generality
that circuits for families in VP have polynomial degrees as well, i.e., that
all their intermediate computations are also low degree polynomials.
Proof. [Sketch] We describe how to construct Ψ. For every gate v
in Φ, we deﬁne r + 1 gates in Ψ, which we denote (v,0),...,(v,r),
in such a way that (v,i) computes Hi(Φv). We construct Ψ induc-
tively as follows. If v is an input gate, we can clearly deﬁne (v,i)
as an input gate with the appropriate properties. If Φv = Φu + Φw,
deﬁne Ψ(v,i) = Ψ(u,i) + Ψ(w,i) for all i. If Φv = Φu × Φw, deﬁne Ψ(v,i) =
i
j=0 Ψ(u,j) × Ψ(w,i−j). Induction implies that Ψ has the claimed func-
tionality. Every gate in Φ corresponds to at most O((r + 1)2) gates in
Ψ (each product gate requires O((r + 1)2) additional sum gates), and
so |Ψ| = O(r2s).
The process described above takes a general circuit and trans-
forms it into a homogeneous one with the same functionality while

228
Structural Results
not increasing the size by much. However, as described this process
may increase the depth by a factor that is logarithmic in the degree
(due to the loss in each product gate). In the case of formulas, an
increase in depth is translated to the exponent. In particular, it may
transform a general formula of size s to a homogeneous formula of size
sO(logr). Thus, this procedure is not eﬃcient when working with formu-
las. Recently, Raz [105] showed that, for a certain range of parameters,
one can obtain an eﬃcient construction for formulas as well. For exam-
ple, it follows from Raz’s work that a polynomial size formula comput-
ing a homogeneous polynomial of degree O(logn) can be assumed to
be homogeneous, without loss of generality.
Theorem 2.3([105]). Let f be a homogeneous polynomial of degree r
that can be computed by an arithmetic formula of size s. Then, f can
be computed by a homogeneous formula of size poly(s)
r+O(logs)
r

.
Proof. [Sketch] Let Φ be a formula of size s computing f. Theorem 2.7
below, which is a simple observation, tells us that we can assume, with-
out loss of generality, that the depth of Φ is at most O(logs). This may
cause a polynomial increase in size.
The homogeneous formulas that we shall construct will have depth
O(logs · logr), but nevertheless their sizes will be much smaller than
2O(logs·logr). The basic construction is similar to the one in Theorem 2.2
and the main diﬀerence is that we count the number of times each gate
is “used” in a more accurate way.
For every integer d, denote by Gd the family of monotone non-
increasing functions χ from {0,1,...,d} to {0,1,...,r}. I.e., all func-
tions χ satisfying χ(i + 1) ≤χ(i) for every i ∈{0,...,d −1}. Clearly,
the size of Gd is
r+d+1
r

.
For every gate v in Φ, deﬁne the product-height of v, denoted ph(v),
to be the number of product gates on the path from v to the out-
put gate. Here is how we transform Φ to a homogeneous formula Ψ:
Every gate v in Φ with ph(v) = d will be duplicated to |Gd| gates in Ψ
labeled (v,χ), for every χ ∈Gd, so that Ψ(v,χ) computes Hχ(d)(Φv), the
χ(d)-th homogeneous part of the polynomial v computes in Φ. We con-
struct Ψ by induction. When v is an input gate it is clear how to label

2.2 Homogenization
229
(v,χ) for all χ. When v = v1 + v2, then ph(v) = ph(v1) = ph(v2), and
we deﬁne (v,χ) = (v1,χ) + (v2,χ) for all χ. Finally, when v = v1 × v2,
we do the following. Consider some χ ∈Gd where d = ph(v). For every
ℓ∈{0,...,χ(d)}, deﬁne χℓ∈Gd+1 as
χℓ(i) =
χ(i)
if i ∈[d]
ℓ
if i = d + 1 .
Set
(v,χ) =
χ(d)

ℓ=0
(v1,χℓ) × (v2,χχ(d)−ℓ).
Induction implies that Ψ is homogeneous with the claimed func-
tionality. The way we construct Ψ also implies that it is a formula.
The crucial point is that when v = v1 × v2, the function χℓtells us for
which value of χ we “used” the nodes (v1,χℓ),(v2,χℓ). The size of Ψ
is as claimed, since the number of times we duplicate each vertex is at
most
r+depth(Φ)+1
r

≤
r+O(logs)
r

.
We do not know whether computing a polynomial by a homogeneous
formula, when the degree is not too small, can cause a severe blow-up
in size.
Open Problem 2. Are homogeneous formulas super-polynomially
weaker than general formulas?
In Ref. [63] it was shown that for multilinear formulas (i.e., for-
mulas that each of their gates computes a multilinear polynomial, see
Section 2.2.1) homogeneity is indeed a weakness. Namely, in some cases,
homogeneous multilinear formulas are super-polynomially weaker than
multilinear formulas.
2.2.1
Multilinearization
As with homogenization we can study the task of transforming a cir-
cuit computing a multilinear polynomial into a multilinear circuit. In
contrast to homogenization, the most eﬃcient way known today for

230
Structural Results
transforming a general circuit to a multilinear one causes an exponen-
tial blow-up in size [100]. In spite of this, we do not have strong lowers
bound for multilinear computation (see Section 3.6). Thus, we do not
know whether this loss in size is necessary or not. Speciﬁcally, the fol-
lowing question is open.
Open Problem 3. Are multilinear circuits super-polynomially weaker
than general circuits?
Furthermore, given a multilinear circuit it is very natural to try
and transform it into a syntactically multilinear circuit. However, the
following question is still open.
Open Problem 4. Are multilinear circuits super-polynomially more
powerful than syntactically multilinear circuits?
If instead of circuits we were to consider formulas then the situation
becomes much simpler. In particular, syntactically multilinear formulas
are equivalent to multilinear formulas [104].
Theorem 2.4([104]). If f can be computed by a multilinear formula
of size s, then f can also be computed by a syntactically multilinear
formula of size s.
Proof. [Sketch] The idea is to go over the formula gate by gate and
guarantee that it satisﬁes the necessary condition. Let v = v1 × v2 be a
gate in a multilinear formula Φ and assume that x ∈Xv1 ∩Xv2. As Φ
is multilinear, the polynomial Φv = Φv1Φv2 is multilinear, and so the
degree of x in either Φv1 or Φv2 is zero. Without loss of generality,
assume that the degree of x in Φv1 is zero. In this case, substitute
x = 0 in Φv1. This does not aﬀect the overall functionality of Φ.
2.3
Partial Derivatives
We now describe a fairly surprising result by Baur and Strassen [16]. We
start by deﬁning the notion of partial derivatives. The partial derivative

2.3 Partial Derivatives
231
with respect to a variable x is deﬁned as follows: ∂x(x) = 1, and for a
polynomial f that does not contain x, ∂x(f) = 0. To deﬁne ∂x(f) for
a general polynomial, we use the two identities ∂x(f + g) = ∂x(f) +
∂x(g) and ∂x(fg) = ∂x(f)g + f∂x(g). It is not diﬃcult to prove that this
is a well-deﬁned notion that makes sense over any ﬁeld. Note however
that while the deﬁnition is the same for all ﬁelds, a partial derivative
can behave diﬀerently over diﬀerent ﬁelds. For example, over the real
numbers ∂x(x2) = 2x, while over the ﬁeld with two elements ∂x(x2) = 0.
A useful property of this deﬁnition is that it satisﬁes the chain rule for
partial derivatives.
Baur and Strassen showed that computing f(x1,...,xn) is as hard as
simultaneously computing the n + 1 polynomials f,∂x1(f),...,∂xn(f).
Theorem 2.5 ([16]). Let f(x1,...,xn) be a polynomial that has a
circuit Φ of size s and depth d. Then, there exists a circuit Ψ of
size O(s) and depth O(d) computing (simultaneously) the polynomials
∂x1(f),...,∂xn(f).
This result is very interesting as a priori one might expect that the
circuit complexity of ∂x1(f),...,∂xn(f) will be of order n · s rather than
O(s), as these are n polynomials that are, intuitively, “as complex” as
f is.
To prove the theorem we need to deﬁne how to take a partial deriva-
tive with respect to a gate of the circuit. We give a slightly more general
deﬁnition than what the proof requires as it will be useful in Section 2.4.
Let v, w be two gates in Φ and denote by fv and fw the polynomials
computed at v and w, respectively. Let Φw=y be the circuit in which we
delete the two edges entering w and label it with a new variable y (if w
is an input gate then we simply label it with y). Denote by fv,w(X,y)
the polynomial computed by v in Φw=y. Finally, deﬁne
∂wfv = (∂yfv,w)|y=fw,
namely, the polynomial obtained by substituting fw to y in the
polynomial ∂yfv,w. Clearly, the polynomial ∂wfv is deﬁned over the
variables X.

232
Structural Results
Proof. We prove the existence of Ψ by induction on s, computing
derivatives from the root down (in fact, we describe how to construct Ψ
given Φ). If Φ is an input gate, constructing Ψ is straightforward. Oth-
erwise, let v be the deepest gate in Φ (i.e., the gate that is the farthest
from the output gate) and denote its children, which are input gates,
by u,w. Consider the circuit Φv=y, and denote its output polynomial by
fv=y. As we deleted two edges, Φv=y is smaller than Φ. By induction,
there exists a circuit Ψ′ computing ∂x1(fv=y),...,∂xn(fv=y),∂y(fv=y) of
size O(s −1). Denote by X′ the set of variables that label either u or
w in Φ (X′ could be empty). Note that f = fv=y

y=fv, where fv is the
polynomial that v computes in Φ. The chain rule for partial derivatives
implies that for every xi,
∂xi(f) = ∂xi(fv=y)

y=fv + ∂y(fv=y)

y=fv · ∂xi(fv).
Therefore, for every xi ̸∈X′, ∂xi(f) = ∂xi(fv=y)

y=fv. Since ∂xi(fv) is
either a variable or a ﬁeld element, and since the size of X′ is at most
two, we can compute {∂xi(f)}xi∈X′ by adding at most a constant num-
ber of gates and using the gates in Ψ′. The size of Ψ is thus at most
O(s −1) + O(1) = O(s). The statement about the depth follows by
induction as well.
Theorem 2.5 shows that computing the ﬁrst partial derivatives of
an arithmetic circuit is an easy task. What about second order partial
derivatives? We can deﬁne ∂xi,xj(f) = ∂xi(∂xj(f)) (the order does not
matter). It is not known whether one can compute all the order n2
second partial derivatives with only a constant increase in size. Having
such a result will automatically imply an optimal circuit for matrix
multiplication: Assume we wish to multiply two n × n matrices A,B.
Consider the polynomial xABy with x,y two n-dimensional vectors.
This polynomial has an O(n2) size circuit as we can compute xA and
By and then take their inner product. On the other hand, the second
partial derivative of xABy with respect to xi,yj is (AB)i,j. Therefore,
if the answer to the following open problem is positive then we can
multiply two matrices in time O(n2).
Open Problem 5. Does an analog of Theorem 2.5 hold for second-
order partial derivatives?

2.4 Depth Reduction
233
We mention that Raz [101] showed that any bounded-coeﬃcients
circuit (i.e., a circuit that uses only, say, real numbers of absolute value
at most one) computing the product of two n × n real matrices must be
of size at least Ω(n2 logn) (see Section 3.7). This lower bound rules out
the possibility that a transformation like the one in Ref. [16] (namely,
a transformation that does not introduce large constants) can yield a
constant blow-up in size for computing second-order partial derivatives.
2.4
Depth Reduction
In this section we discuss depth reduction for arithmetic computation.
The main idea will be to express the circuit using partial derivatives of
intermediate computations. We start by discussing some properties of
partial derivatives. In what follows we consider a homogeneous circuit
Φ and use the notations introduces in Section 2.3.
Claim 2.6. Let v,w be two gates of a homogeneous circuit Φ. Denote
with fv and fw the polynomial computes by v and w, respectively.
(1) Either ∂wfv is zero or ∂wfv is a homogeneous polynomial of
total degree deg(v) −deg(w).
(2) Assume that v is a product gate with children v1 and
v2 such that deg(v1) ≥deg(v2). If deg(w) > deg(v)/2 then
∂wfv = fv2 · ∂wfv1.
(3) Assume that v is a sum gate with children v1 and v2. Then
∂wfv = ∂wfv1 + ∂wfv2.
Proof. To see Item 1 recall that both fv,fw are homogeneous. As fv =
fv,w|y=fw, and by deﬁnition of ∂wfv, it follows that if ∂wfv is nonzero
then it is of degree deg(v) −deg(w). For Item 2 note that since v is
a product gate, deg(v) = deg(v1) + deg(v2). By assumption we have
deg(v2) < deg(w). The gate w is thus not in the sub-circuit rooted at
v2 (as the circuit is homogeneous). Hence ∂wfv2 = 0. Item 3 follows
from the deﬁnition of ∂wfv.
We start with the simplest type of depth reduction, that of formulas
(such a reduction holds for Boolean formulas as well).

234
Structural Results
Theorem 2.7. Let f be computed by an arithmetic formula of size
s. Then f can also be computed by an arithmetic formula of depth
O(logs) (and hence of size poly(s)).
Proof. [Sketch] The proof is by a simple induction on the formula-size.
For simplicity, in this proof we deﬁne the size of a formula to be the
number of input gates in it (this is the same as the number of edges
up to a constant factor). Let Φ be a formula of size s computing f. Let
v be the root of Φ. Choose a vertex w in Φ so that s/3 ≤|Φw| ≤2s/3.
Such a vertex always exists as if w is a gate with children w1,w2 then
|Φw| = |Φw1| + |Φw2|. Since Φ is a formula, fv,w is linear in y (recall
the deﬁnition of fv,w in Section 2.3). I.e., fv,w = ∂yfv,w · y + fv,w|y=0.
This implies that:
f = fv = fv,w|y=fw = ∂wfv · fw + fv,w|y=0.
(2.1)
Each of the polynomials fw,fv,w|y=0 is computed by a formula of size
at most 2s/3. It can also be shown that ∂wfv can be computed by
a formula of size |Φv| −|Φw| ≤2s/3 (this follows by induction on v).
Therefore, induction on s implies that each of these three polynomials
can be computed by a formula of depth O(log(s) −log(2/3)). Equation
(2.1) completes the proof.
We continue with the more elaborate depth reduction, that of cir-
cuits. It exhibits a remarkable property of arithmetic circuits: without
loss of generality, we can assume that polynomial size circuits of poly-
nomial degree are of poly-logarithmic depth. This was proved in the
seminal work [143]. Such a property is not believed to hold in the
Boolean setting. One example of its power is that the determinant of
an n × n matrix can be computed by a polynomial size circuit of depth
O(log2 n) (although the ﬁrst prove of this claim [40] did not use a depth
reduction).
Theorem 2.8([143]). For every homogeneous degree r polynomial f
computed by a circuit Φ of size s, there is a homogeneous circuit Ψ

2.4 Depth Reduction
235
of size poly(r,s) computing f with the following additional structure.
(1) The circuit Ψ has alternating levels of sum and product gates.
(2) Each product gate v in Ψ computes the product of ﬁve polynomials,
each of degree at most 2deg(v)/3. (3) Sum gates have arbitrary fan-in.
In particular, the number of levels in Ψ is O(logr).
It is clear that Theorem 2.8 implies Theorem 1.3, which states that
if f is a polynomial of degree r computed by a size s circuit, then f
can be computed by a size poly(s,r) circuit with fan-in at most two of
depth O(logr(logr + logs)).
Proof. To prove the theorem we shall use properties of partial deriva-
tives in order to express fv,∂wfv as functions in lower degree polyno-
mial, as we now describe. For an integer m ∈N, denote by Gm the set of
multiplication gates t in Φ with children t1 and t2 such that m < deg(t)
and deg(t1),deg(t2) ≤m.
Claim 2.9. Let m > 0 be an integer, and let v,w be two gates so that
deg(w) ≤m < deg(v) < 2deg(w). Then,
fv =

t∈Gm
ft · ∂tfv
and
∂wfv =

t∈Gm
∂wft · ∂tfv.
Proof. We ﬁrst show that fv = 
t∈Gm ft · ∂tfv.
Since m < deg(v), there is a directed path from (at least one gate
in) Gm to v in Φ. We prove the claim by induction on the length of the
longest directed path from Gm to v. Let v1 and v2 be the children of v
in Φ.
Induction Base: Assume that v ∈Gm. Thus, deg(v1) ≤m and
deg(v2) ≤m. So, every gate t in Gm, other than v, is not in Φv, which
implies that ∂tfv = 0. Hence, since ∂vfv = 1,
fv = fv · 1 +

t∈Gm:t̸=v
ft · 0 =

t∈Gm
ft · ∂tfv.
Induction Step: Consider the following two cases:
Case one: Assume that v is an addition gate. Since the circuit is
homogeneous, deg(v1) = deg(v2) = deg(v) (otherwise we can ignore one

236
Structural Results
of the gates). So, by induction,
fv1 =

t∈Gm
ft · ∂tfv1 and fv2 =

t∈Gm
ft · ∂tfv2.
Claim 2.6 hence implies that
fv = fv1 + fv2 =

t∈Gm
ft · (∂tfv1 + ∂tfv2) =

t∈Gm
ft · ∂tfv.
Case two: Assume that v is a product gate, and assume without
loss of generality that deg(v1) ≥deg(v2). Since v ̸∈Gm, we have m <
deg(v1) ≤2m. So, by induction,
fv1 =

t∈Gm
ft · ∂tfv1.
For all gates t ∈Gm, we have deg(v) ≤2m < 2deg(t). By Item 2 of
Claim 2.6 we get
fv = fv1 · fv2 =

t∈Gm
ft · (fv2 · ∂tfv1) =

t∈Gm
ft · ∂tfv.
The proof of the second claim, i.e., that ∂wfv = 
t∈Gm ∂wft · ∂tfv,
follows the same lines and is left to the reader.
We now proceed with the proof of Theorem 2.8. First, we assume
without loss of generality that s ≥n. Second, using Theorem 2.2, we
can assume without loss of generality that the circuit computing f, Φ,
is a homogeneous arithmetic circuit of size s′ = O(r2s). To prove the
theorem we construct Ψ. The construction is done in steps where at
the i-th step we do the following:
(1) Compute all polynomials fv, for gates v in Φ such that 2i−1 <
deg(v) ≤2i.
(2) After we ﬁnish the ﬁrst part, we compute all polynomials
∂wfv, for all (appropriate) gates v,w so that
2i−1 < deg(v) −deg(w) ≤2i and deg(v) < 2deg(w).
We will show that we can preform the ﬁrst part of the i-th step by
adding a layer consisting of O(s′2) product gates, of fan-in at most 3,

2.4 Depth Reduction
237
and a layer of O(s′) sum gates, each of fan-in O(s′). Similarly, for the
second part we need to add a layer consisting of O(s′3) product gates,
of fan-in at most 3, and a layer of O(s′2) sum gates, each of fan-in
O(s′), to the circuit that was computed in the ﬁrst part. Overall, this
increases the size by O(s′3) and the depth by O(1).
The Construction of Ψ
We use the following conventions: Gates in Φ are denoted by v, t,
and w. For a gate v, we denote by v′ the gate in Ψ computing fv.
For two (appropriate) gates v,w, we denote by (w,v) the gate in Ψ
computing ∂wfv.
Step 0: For every gate v in Φ such that deg(v) ≤1, the polynomial
fv is linear. So, since s′ ≥n, we can compute fv with a linear arithmetic
circuit of size O(s′) and depth O(1). For every two gates v and w in Φ
such that deg(v) −deg(w) ≤1, Claim 2.6 implies that ∂wfv is linear as
well. So, again, we can compute ∂wfv with a linear circuit of size O(s′)
and depth O(1).
Step i + 1: Assume that we already completed step i.
We ﬁrst compute the fv’s. Let v be a gate of degree 2i < deg(v) ≤
2i+1, and denote
m = 2i.
Recall that if a gate t is not in Φv, then ∂tfv = 0. Thus, by Claim 2.9,
fv =

t∈Tv
ft · ∂tfv =

t∈Tv
ft1 · ft2 · ∂tfv,
(2.2)
where Tv is the set of gates t ∈Gm, with children t1 and t2, such that t
is in Φv. Clearly, m < deg(t) ≤2m and deg(t1),deg(t2) ≤m. Hence,
deg(v) −deg(t) ≤2i+1 −2i = 2i and deg(v) ≤2i+1 < 2deg(t). There-
fore, ft1, ft2, and ∂tfv were already computed. Using Equation (2.2),
we can compute fv, using previously computed gates, by adding one
layer of O(s′) product gates with fan-in 3 and a sum gate with fan-in
O(s′).
It remains to compute the ∂wfv’s. Let v,w be two gates so that
2i < deg(v) −deg(w) ≤2i+1 and deg(v) < 2deg(w).
Now, denote m = 2i + deg(w). Thus, deg(w) ≤m < deg(v) < 2deg(w).
Recall that if a gate t is not in Φv, then ∂tfv = 0. Also, by Claim 2.6,

238
Structural Results
if a gate t admits deg(t) > deg(v), then ∂tfv = 0. Hence, by Claim 2.9,
∂wfv =

t∈Tv
∂wft · ∂tfv.
Clearly, for t ∈Tv, we have deg(t) ≤deg(v) < 2deg(w). Denote the chil-
dren of t ∈T by t1 and t2, and assume (without loss of generality) that
w is in Φt1. In particular, deg(w) ≤deg(t1) and deg(t1) ≥deg(t2). Using
Item 2 of Claim 2.6, we obtain
∂wfv =

t∈Tv
ft2 · ∂wft1 · ∂tfv.
(2.3)
We now show that all the polynomials ft2, ∂wft1, and ∂tfv were
already computed, including the ﬁrst part of the i + 1 step described
above.
Since
deg(v) ≤2i+1 + deg(w) ≤2i+1 + deg(t1) = 2i+1 + deg(t) −
deg(t2), it holds that deg(t2) ≤2i+1 + deg(t) −deg(v) ≤2i+1. There-
fore, ft2 was already computed (perhaps in the ﬁrst part of the i + 1
step).
As
deg(t1) ≤m = 2i + deg(w),
we
have
deg(t1) −deg(w) ≤2i.
Since deg(t1) ≤deg(t) ≤deg(v) < 2deg(w), the polynomial ∂wft1 was
already computed.
Finally, from deg(t) > m = 2i + deg(w), we get deg(v) −deg(t) <
deg(v) −2i −deg(w) ≤2i+1 −2i = 2i. Since deg(v) ≤2i+1 + deg(w)≤
2(2i + deg(w)) < 2deg(t), the polynomial ∂tfv was already computed.
Concluding, using Equation (2.3) we can thus compute ∂wfv by
adding O(s′) product gates of fan-in at most 3 and a sum gate of fan-in
O(s′).
We thus almost completed the proof of Theorem 2.8. The only
remaining problem may be that not in every product gate the degrees
of the children are small enough. By slightly modifying the proof it
is not hard to get the claim at the cost of increasing the fan-in of
each multiplication gate to 5 (e.g., by replacing ft1, in Equation (2.2),
with its expression as a sum of products of lower degree polynomials,
if needed).
A natural thing to ask is whether Theorem 2.8 can be strength-
ened if we have a bound on the homogeneous formula complexity of f.

2.4 Depth Reduction
239
The following theorem of Hrubeˇs and Yehudayoﬀ[63] provides such a
strengthening. We leave the proof as an exercise to the reader.
Theorem 2.10 ([63]). For every homogeneous polynomial f
of
degree r computed by a homogeneous formula Φ of fan-in at most two
and of size s, there exist degree r polynomials f1,...,fs′, where s′ ≤
s + 1, such that the following holds. (1) f = f1 + ··· + fs′. (2). Every fi
is of the form fi = fi,1 · fi,2 ···fi,ki where (1/3)jr ≤deg(fi,j) ≤(2/3)jr
and deg(fi,ki) = 1. (3) The total number of variables in f1,1,...,fs′,ks′,
counted with multiplicities, is at most s. (4) Each fi,j can be computed
by a homogeneous formula of size at most s. (5) If Φ is multilinear then
so are the formulas for the fi,j’s.
Theorem 2.8 can also be used to imply the following “depth-4 ver-
sion of Theorem 1.3” by [5]. Recall that ΣΠΣΠ is the class of depth-4
circuits composed of alternating levels of sum and product gates with
a sum gate at the top.
Theorem 2.11 ([5]). Let f(x1,...,xn) be a polynomial of degree r =
O(n) over F. If there exists a circuit of size s = 2o(r+rlog(n/r)) for f,
then there exists a ΣΠΣΠ circuit of size 2o(r+rlog(n/r)) for f as well.
Furthermore, the fan-in of the top layer of product gates is bounded
by ℓ(n), where ℓis any suﬃciently slowly growing function tending to
inﬁnity with n, and the fan-in of the bottom layer of product gates is
bounded by o(r).
Proof. [Sketch] Consider the circuit guaranteed by Theorem 2.8. To
prove the theorem, we break that circuit to two parts: the ﬁrst part is
composed of the topmost t levels of multiplication gates together with
the addition gates above them, and the second part is the rest of the
circuit. Consider the polynomial computed by the ﬁrst part (imagine
that we connect new variables to the bottom layer of this circuit). It is a
polynomial of degree at most 5t in poly(r,s) variables, and hence can be
computed by a depth-2 circuit of size O(5tpoly(r,s)+5t
5t

); this is the sum
of monomials representation. On the other hand, each gate in the sec-
ond part computes a polynomial of degree at most D = deg(f)/(3/2)t,

240
Structural Results
and hence it has a depth-2 circuit of size order
n+D
D

. By composing
these circuits we obtain a depth-4 circuit of size O(5tpoly(r,s)+5t
5t
n+D
D

)
computing f. Optimization over t completes the proof.
An immediate corollary of the above theorem is that if we can prove
an exp(n) lower bound for the size of depth-4 circuits, computing poly-
nomials of degree Ω(n), then we automatically get an exp(n) lower
bound for the size of general arithmetic circuits! Very recently, Koiran
[88] proved the following extension of Theorem 2.11, using the connec-
tion between arithmetic circuits and arithmetic branching programs
(see Section 3.4).
Theorem 2.12 ([88]). Let f be an n-variate polynomial of degree r
that can be computed by a polynomial size arithmetic circuit. Then f
can be computed by a ΣΠΣΠ circuit with nO(logr) addition gates and
nO(√rlogr) multiplication gates. Furthermore, multiplication gates have
fan-in at most
√
3r + 1.
Thus, a 2n1/2+ε lower bound on the depth-4 complexity of the per-
manent (of n × n matrices) is suﬃcient for separating VP from VNP.
Currently, the best lower bound for depth-4 circuits (when char(F) ̸= 2)
is of the form n1+c for some constant c > 0, see Refs. [103, 122].
Open Problem 6. Prove super-polynomial lower bounds for the size
of depth-4 circuits over ﬁelds of characteristic ̸= 2.
2.5
Coping with Division Gates
A natural question to ask is why not add the division operator ÷ to the
set of allowed arithmetic operations. Indeed this model was considered
in the past. Note that when we allow divisions, each gate computes a
rational function instead of a polynomial. Thus, we require that the
circuit never divides by the zero polynomial. An answer to the above
question was ﬁrst given by Strassen [133]. Strassen showed that over
inﬁnite ﬁelds divisions do not add power to the model (more generally,

2.5 Coping with Division Gates
241
Strassen’s approach works when the ﬁeld contains nonzeros for the
polynomial that we divide by [24]). This result was recently extended
to ﬁnite ﬁelds as well [62]. In the proof of this theorem we again use
the homogenization procedure discussed above.
Theorem 2.13 ([62,
133]). If a polynomial f ∈F[x1,...,xn] of
degree r can be computed by an arithmetic circuit Φ of size s using
the operations +,×,÷, then there is a circuit Ψ of size poly(s,r,n) that
uses only the arithmetic operations +,× and computes f.
Proof. [Sketch] We start by sketching the proof for large enough ﬁelds.
We ﬁrst show that by slightly increasing the size of the circuit we can
assume that the only division gate appears at the top of the circuit.
In other words, we can assume that f = h ÷ g where h and g are com-
puted by the two children of the output gate, and the output gate is the
only gate labeled by ÷. To see this, duplicate each gate v to two gates
(v,numerator) and (v,denominator) in a way such that (v,numerator)
computes the numerator of the rational function that v computes
and (v,denominator) computes the denominator of the function com-
puted by v. This can be easily done using the following identities:
h1/g1 + h2/g2 = (h1g2 + h2g1)/(g1g2), h1/g1 × h2/g2 = (h1h2)/(g1g2),
and h1/g1 ÷ h2/g2 = (h1g2)/(h2g1).
Finally, we show how to eliminate the only division gate. Write
f = h/g. Since the ﬁeld is large enough, g(α1,...,αn) ̸= 0 for some
α1,...,αn ∈F. By translating the input and multiplying by a ﬁeld
element, with a slight increase in size, we can thus assume that
g(0,...,0) = 1. In this case,
f = h
g =
h
1 −(1 −g) =
∞

j=0
h(1 −g)j,
where the last equality means that every homogeneous part of f is the
same as the homogeneous part of the right-hand side. As 1 −g does
not contain constants, the minimal degree of a monomial in (1 −g)j
is j. Therefore, for every i ∈{0,...,r},
Hi(f) =
∞

j=0
Hi(h(1 −g)j) =
i

j=0
Hi(h(1 −g)j).

242
Structural Results
We can therefore eﬃciently compute all the homogeneous parts of f,
using the homogeneous parts of the polynomials {h(1 −g)j : j ∈[r]},
each of which has a circuit of size poly(s,r). Theorem 2.2 now completes
the proof.
To prove the theorem for small ﬁelds, we need to “simulate” a large
ﬁeld. We now explain how this can be done without increasing the size
by much. To make a ﬁeld F “larger” we consider a ﬁeld extension E
of F that is large enough for our purposes (to eliminate divisions we
need E to be of size poly(r,n)). The point is that we can think of the
elements of E as vectors with entries in F, and can thus simulate the
arithmetic operations over E by operations over F: If ¯a = (a1,...,ak)
and ¯b = (b1,...,bk) are two elements of E ∼= Fk, where k is the degree
of E over F (k = O(log(rs)) suﬃces), then the sum of ¯a and ¯b over E
is just the entry-wise sum of ¯a and ¯b as vectors in Fk. The product
of ¯a and ¯b over E can be deﬁned coordinate-wise as (¯a × ¯b)i = λi(¯a,¯b)
for i ∈[k], where λi is a (ﬁxed) bilinear form, namely, a k × k matrix
with entries in F. To simulate a circuit over E by a circuit over F, we
“duplicate” each gate k times and simulate the arithmetic operations
over E by the arithmetic operations over F, as described above.
We have just seen that we can eliminate division gates from circuits
without paying too much. Can we do the same for formulas? It turns
out that we can, as the following theorem shows.
Theorem 2.14. If a polynomial f ∈F[x1,...,xn] of degree r can be
computed by a formula Φ of size s using the operations +,×,÷ when
the size of F is at least sr + 1, then there is a formula Ψ computing f
that uses only +,× of size poly(s,r). For arbitrary ﬁnite ﬁelds, the size
of Ψ is at most (sr)O(loglog(sr)).
Proof. [Sketch] The proof is similar to the proof of Theorem 2.13. We,
therefore, explain mostly the parts of the argument that are diﬀerent
from the proof for circuits. Theorem 2.7 tells us that without loss of
generality the depth of Φ is O(logs). We transform Φ to a formula that
has only one ÷ gate at the top, as in the proof of Theorem 2.13. This
step increases the depth of Φ by a constant factor, so its depth remains

2.6 Discussion
243
O(logs). Write f = h/g, where both h and g have formulas of depth
O(logs). Again, for every i ∈{0,...,r},
Hi(f) =
∞

j=0
Hi(h(1 −g)j) =
i

j=0
Hi(h(1 −g)j).
Each h(1 −g)j has a formula Ψj of depth D = O(logs + logr) and
degree R ≤rs. Unlike the case of arithmetic circuits, we do not know an
eﬃcient way of transforming a formula to a homogeneous form (unless
the degree is small). However, this is not a problem as we do not need
a homogeneous formula, but rather we just need to compute Hi(Ψj).
When the ﬁeld is large we achieve this by multiplying each variable
xk in Ψj by a new variable y to obtain a new formula Ψ′
j(y) of depth
O(D). We have that
Ψ′
j(y) =

i∈{0,...,R}
yiHi(Ψj).
We can thus evaluate Ψ′
j(y) at R + 1 diﬀerent y’s, and use interpola-
tion to recover Hi(Ψj). Note that the division operations required for
interpolation are of ﬁeld elements and not of polynomials. This can be
done using a formula of depth O(D + logR), in particular, a formula
of size poly(r,s).
When the ﬁeld is small, we need to work over a ﬁeld extension of
degree k ≤O(log(rs)) over F. To do this, as in Theorem 2.13 above,
we can work with k × k matrices. This may increase the depth by a
factor of O(logk), which in terms of size means a factor of O(logk) in
the exponent.
2.6
Discussion
In this section we proved several fundamental results on the structure
of arithmetic circuits. We have seen that there is a universal circuit,
discussed eﬃcient ways of transforming a circuit to a homogeneous
one, proved that ﬁrst partial derivatives can be computed eﬃciently
(simultaneously), learned how to eliminate division gates, and more.
While the importance of results like Theorem 2.8 implying VP ̸=
VNC2 is evident, the signiﬁcant of the other results will become even

244
Structural Results
clearer in Section 3, where they will play an instrumental role in prov-
ing lower bounds for diﬀerent models of arithmetic circuits. As we shall
see in Section 3, almost all known lower bound proofs can be presented
as following a very general scheme: ﬁrst prove that polynomials that are
computed by the underlying circuit class poses some “simple” struc-
ture. This “simple” structure will help us to understand and isolate the
“weaknesses” of that circuit class. The lower bound will then follow by
constructing an explicit polynomial that does not admit this “simple”
structure.

3
Lower Bounds
In this section we survey most of the known lower bounds on the size
of arithmetic circuits and formulas. We start by discussing the exis-
tence of hard polynomials, i.e., polynomials that cannot be computed
by small circuits. We then consider lower bounds for explicit polyno-
mials, namely, polynomials in VNP. We ﬁrst deal with lower bounds
for general circuits and formulas, and then move to lower bounds for
restricted models, such as monotone, bounded depth, and multilinear
circuits. To conclude, we describe several possible approaches for prov-
ing lower bounds.
3.1
Existence of Hard Zero-One Polynomials
Finding explicit hard polynomials is the goal we are after in this section.
In order to even start our journey, we may ask ourselves “do there exist
hard polynomials?” The answer to this question is, of course, yes. Over
ﬁnite ﬁelds, standard counting arguments tell us that there exist hard
polynomials with zero-one coeﬃcients. The idea is to show that the
number of polynomials that have small circuits is much smaller than
the total number of polynomials, and so most polynomials are in fact
245

246
Lower Bounds
hard. Over inﬁnite ﬁelds, the existence of hard polynomials follows by
counting dimensions instead of counting polynomials, as there is an
inﬁnite number of circuits and polynomials. In particular, it is less
obvious how to prove the existence of hard polynomials with zero-one
coeﬃcients, over inﬁnite ﬁelds.
In Ref. [62] it was shown1 that over any ﬁeld there exist hard poly-
nomials with zero-one coeﬃcients. The proof uses notions from alge-
braic geometry, the degree and the dimension of a variety, to show that
there are few polynomials with zero-one coeﬃcients that are computed
by small circuits.
Theorem 3.1. For every n,r ∈N and for every ﬁeld F, there exists a
polynomial f in F[x1,...,xn] of degree r with zero-one coeﬃcients so
that every circuit computing f has size at least Ω
	
n+r
r

.
A key observation behind this proof is thinking of a circuit as deﬁn-
ing a polynomial map. We shall discuss this idea in more detail in
Section 3.8.
Proof. [Sketch] Our goal is to show that the number of polynomials
with zero-one coeﬃcients that are computed by small circuits is small.
The tricky part is to count this number when the ﬁeld is inﬁnite as,
for example, there are inﬁnitely many circuits of size one. To do so,
we use a dimension argument, which is encapsulated in the following
lemma. We will not prove the lemma (its proof relies on an appropriate
dimension argument).
Lemma 3.2 ([62]). Let F be a ﬁeld, and let F : Fn →Fm be a poly-
nomial map of degree r > 0 (i.e., each coordinate of F is an n-variate
polynomial of degree at most r). Then |F(Fn) ∩{0,1}m| ≤(2r)n.
To use this lemma we show how to construct a polynomial map
from each circuit. For a circuit Φ of size at most s and degree at most
1 In Ref. [145] it is claimed, without a proof, that the methods of Heintz and Sieveking [57]
imply a similar statement.

3.2 General Circuits and Formulas
247
r in the variables X = {x1,...,xn}, let Ψ be the circuit obtained by
replacing every ﬁeld element that occurs in Φ by a new variable zi,
i ∈[s]. The circuit Ψ thus computes a polynomial in the variables X
and Z = {zi}i∈[s]. We call such a circuit Ψ a skeleton, as it contains no
ﬁeld elements. The circuit Φ is thus a projection of Ψ: the polynomial
computed by Φ can be obtained from the polynomial computed by Ψ
by an appropriate substitution of ﬁeld elements to the variables Z.
We now explain how the polynomial computed by Ψ can be viewed
as a polynomial map F = FΨ from Fs to FN, with N =
n+r
r

. The
coordinates of F are labeled by monomials M of degree at most r in the
variables X, and we denote F = (FM)M. There are N such monomials,
and so there are N entries in F. The entry FM ∈F[Z] is the coeﬃcient
of the monomial M in the polynomial computed by Ψ. In particular,
each entry FM is a polynomial of degree at most 2s in the Z variables.
Thus, F is a degree 2s polynomial map.
We conclude that when Φ computes a polynomial with zero-one
coeﬃcients, then the vector of coeﬃcients it is contained in the image of
FΨ. However, Lemma 3.2 tells us that the size of the image of FΨ inside
{0,1}N is at most (2s+1)s = 2O(s2). Thus every skeleton can “cover”
at most 2O(s2) such polynomials. We are almost done. It remains to
observe that there are relatively few skeletons: by counting the number
of directed acyclic graphs (there are no ﬁeld elements in a skeleton) we
see that the number of skeletons is at most sO(s) ≤2O(s2).
This implies that the number of polynomials with zero-one coeﬃ-
cients that can be computed by circuits of size at most s is at most
2O(s2). To complete the proof, notice that the number of polynomials
with zero-one coeﬃcients in n variables and degree r is 2N. This implies
that most polynomials in n variables and degree r require circuits of
size at least Ω(
√
N).
3.2
General Circuits and Formulas
We now describe Strassen’s super-linear lower bound on the size of
general circuits [132]. As a comparison, we mention that no super-linear
lower bound on the size of Boolean circuits is known.

248
Lower Bounds
Theorem 3.3 ([132]). Every circuit computing the n polynomials
xr
1,...,xr
n has size Ω(nlogr).
Proof. [Sketch] The key ingredient in the proof is Bezout’s theorem
from algebraic geometry, which states the following. For every k poly-
nomials f1,...,fk of degrees r1,...,rk over some set of variables, the
number of solutions (with multiplicities) to the k equations f1 = f2 =
··· = fk = 0 is either at most r1 · r2 ···rk or inﬁnite. For example, when
fi = xr
i −1, i ∈{1,...,n}, the number of solutions to the equations is
rn when the ﬁeld is algebraically closed.
Now, assume that there is a circuit Φ of size s computing xr
1,...,xr
n.
The goal is to show that the circuit Φ deﬁnes a list of roughly s degree
two equations whose solution set is “equivalent” to the set of solutions
of xr
1 −1 = ··· = xr
n −1 = 0. By Bezout’s theorem this will imply that
s must be large. Here is a sketch of the argument. For every gate v
in Φ, introduce a new variable yv. For every input gate v in Φ labeled
by α, which is either a variable or a ﬁeld element, write the equation
yv −α = 0. For every v = u ⋆w with ⋆∈{+,×}, write the equation
yv −(yu ⋆yw) = 0. For every output gate v (i.e., a gate computing xr
i )
also write the equation yv −1 = 0. We thus have a list of at most 2s
equations in the variables x1,...,xn and {yv}v∈Φ, each of degree at most
two. Denote this list of equation by E. It is not hard to see that since Φ
computes xr
1,...,xr
n, every solution to xr
1 −1 = ··· = xr
n −1 = 0 gives a
solution to E and vice versa. Bezout’s theorem tells us that the number
of solution to E is at most 22s, as the number of solutions is ﬁnite.
Therefore, rn ≤22s or s ≥Ω(nlogr).
Remark 3.1. The proof actually shows that the number of product
gates in Φ is at least Ω(nlogr). This lower bound is tight, as there is a
circuit of size O(nlogr) computing these polynomials.
The lower bound above is for a circuit computing a family of n
polynomials. However, Baur and Strassen managed to get such a lower
bound for a circuit computing a single polynomial [16]. The idea is to

3.2 General Circuits and Formulas
249
use Theorem 2.5 and deﬁne a polynomial whose partial derivatives are
the xr
i -s.
Theorem 3.4([16, 132]). The size of a circuit computing xr
1 + ··· +
xr
n is at least Ω(nlogr), as long as r does not divide the characteristic
of the underlying ﬁeld.
Proof. Theorem 2.5 implies that if there is a circuit of size s for
xr
1 + ··· + xr
n, then there is a circuit of size at most O(s) computing
rxr−1
1
,...,rxr−1
n
. Theorem 3.3 thus implies that s ≥Ω(nlogr).
Improving this lower bound is a long-standing open problem.
Open Problem 7. Prove a better lower bound for general circuits.
For example, prove a super-linear lower bound for the size of a circuit
computing an explicit polynomial of constant degree.
For arithmetic formulas better lower bounds are known. Kalorkoti
proved an Ω(n3) lower bound on the formula-size of determinant [70].
His argument can be thought of as the algebraic analog of Nechiporuk’s
argument (for the Boolean setting), that gives a lower bound on the
Boolean formula-size of a Boolean function f by counting the number
of diﬀerent sub-functions of f. Kalorkoti also proved a quadratic lower
bound on the size of formulas computing 
i∈[n]

j∈[n] xj
iyj (the lower
bound for determinant is not quadratic in the number of variables).
This quadratic lower bound is tight, and so this method cannot be
used to prove stronger lower bounds.
Theorem 3.5([70]). Every formula computing the determinant of an
n × n matrix is of size at least Ω(n3).
In contrast, the smallest formula computing determinant is of size
nO(logn).
Proof. [Sketch]
We
use
the
notion
of
transcendental
degree
(abbreviated td). A set of polynomials g1,...,gt ∈F[X] is said to be

250
Lower Bounds
algebraically independent
if the only polynomial F ∈F[y1,...,yt]
satisfying F(g1,...,gt) ≡0 is the zero polynomial. Deﬁne td(g1,...,gt)
as the maximal size of a subset S of [t] so that {gi}i∈S are algebraically
independent. Using this notion we deﬁne the following complexity
measure for polynomials: Let f ∈F[X] be a polynomial and X′ ⊂X a
set of variables. The measure tdX′(f) is deﬁned as follows. Expand f
according to X′, namely, write f = 
q∈Q gq · q, where Q is the set
of all monomials (including the monomial 1) in X′ of degree at most
deg(f), and each gq is a polynomial in X \ X′. Deﬁne tdX′(f) as the
transcendental degree of {gq}q∈Q.
The main step in Kalorkoti’s proof is the following lemma (he con-
siders the case when division gates are allowed, which makes his proof
somewhat more elaborate).
Lemma 3.6. Let f ∈F[X] and X1,...,Xt be a partition of X. Then
every arithmetic formula for f must be of size at least Ω(
i∈[t] tdXi(f)).
Proof. [Sketch] The basic property we shall use is that for every X′ ⊆X
and two polynomials f1 and f2, both tdX′(f1 + f2) and tdX′(f1 · f2)
are at most tdX′(f1) + tdX′(f2). With this in mind, the lemma follows
by induction on the size of the smallest formula Φ computing f. If Φ
is a single variable, the lemma trivially holds as tdXi(x) is one if x is
in Xi and zero otherwise. If Φ = Φ1 ⋆Φ2 with ⋆∈{+,×}, then for every
i ∈[t], we have tdXi(Φ) ≤tdXi(Φ1) + tdXi(Φ2). Induction, therefore,
implies
|Φ| ≥|Φ1| + |Φ2| = Ω


i∈[t]
tdXi(Φ1) +

i∈[t]
tdXi(Φ2)


≥Ω


i∈[t]
tdXi(Φ)

.
To conclude the lower bound for determinant, partition X =
(xi,j) to X1,...,Xn with Xi = {xi+j−1,j}j∈[n], where the indices are
taken modulo n. Using the lemma above, it suﬃces to show that

3.3 Monotone Circuits
251

i∈[n] tdXi(DET) = Ω(n3). By symmetry it is suﬃcient to prove that
tdX1(DET) ≥Ω(n2). To see this, observe that determinant contains all
monomials of the form xi,jxj,i · 
ℓ̸∈{i,j} xℓ,ℓwith i ̸= j in [n]. Think-
ing of xi,jxj,i as the coeﬃcient of the monomial 
ℓ̸∈{i,j} xℓ,ℓin the
variables X1, we have that tdX1(DET) is at least the transcendental
degree of {xi,jxj,i}i̸=j, which is Ω(n2). As a concluding remark, we
note that for any partition of the matrix X to sets X1,...,Xt we have

i∈[t] tdXi(DET) ≤n4.
Proving better lower bounds for arithmetic formulas is a great
challenge.
Open Problem 8. Prove a super-polynomial lower bound on the size
of arithmetic formulas.
3.3
Monotone Circuits
Monotone arithmetic circuits are circuits over order ﬁelds, such as Q
and R, that use only positive numbers as coeﬃcients. Intuitively, such
circuits compute polynomials in a “straightforward” way, as there are
no cancellations during the computation. This simple model of compu-
tation has been studied in many works, and exponential lower bounds
for the size of monotone circuits are well known (this is true for the
arithmetic case as well as for the Boolean case). Jerrum and Snir [68]
proved the following lower bound (see also Refs. [121, 135]). We present
a simpler but less accurate analysis that is based on the structural
understanding given in Section 2.
Theorem 3.7 ([68]). Every monotone circuit computing the perma-
nent of an n × n matrix has size 2Ω(n).
Proof. [Sketch] Let Φ be a monotone circuit computing PERM(X),
where X is an n × n matrix. Induction on the size of Φ, as in The-
orem 2.7 above, implies that the permanent can be decomposed as
PERM(X) = s
i=1 gihi, where s = O(|Φ|), all the coeﬃcients in gi and

252
Lower Bounds
hi are non-negative, the degree of each gi is between n/3 and 2n/3 and
deg(gi) + deg(hi) = n (we do not assume anything on the complexity
of gi and hi). Since Φ is monotone, every monomial that occurs in gihi
occurs in the permanent as well.
Fix some g = gi and h = hi as above. Let Rg be the rows i in X
so that a variable from row i occurs in g, and similarly deﬁne Rh.
Let Cg be the columns j in X so that a variable from column j occurs
in g, and similarly deﬁne Ch. Monotonicity implies that Rg ∩Rh = ∅
and Cg ∩Ch = ∅. In addition, it implies |Rg| = |Cg| and |Rh| = |Ch|
(as otherwise, gh will contain a monomial that does not belong to
the permanent). As deg(gh) = n it follows that the size of Rg is the
degree of g and the size of Rh is the degree of h. The properties above
imply that the number of monomials in gh is at most (|Rg|)!(|Rh|)! ≤
(2n/3)!(n/3)!. Since the number of monomials that occur in permanent
is n!, the size of Φ is at least Ω
 n
n/3

= 2Ω(n).
A diﬀerent method for proving lower bounds that are tight for mono-
tone circuits was recently presented in Ref. [109] (the above lower bound
for permanent is not tight; the smallest known monotone circuit for
permanent is of size 2O(nlogn)). The idea behind the proof is to ﬁrst
decompose the polynomial in question in a similar way to the proof
above, i.e., to represent it as a sum of a product of polynomials, and
then to use results from additive combinatorics to conclude the lower
bound.
Valiant showed that even one “negation” gate is exponentially pow-
erful [140], that is, there exists a polynomial f that has a polynomial
size circuit with only one “minus” gate, but every monotone circuit for f
has exponential size. We note that given an arithmetic circuit, we can
eﬃciently transform it to a circuit with just one “minus” gate (similar
to the ﬁrst step in the elimination of division gates, see Theorem 2.13).
Recently, it was shown that even for constant degree polynomials one
“minus” gate gives additional power [64], although their arguments
only apply to formulas.
Open Problem 9. Prove a separation between general and monotone
circuits for polynomials of constant degree.

3.4 Noncommutative Computation
253
3.4
Noncommutative Computation
In
the
previous
section
we
considered
a
restricted
model
of
computation. The restriction was a “physical” one, a restriction on
the coeﬃcients used in the circuit. Here we consider a diﬀerent type of
restriction, a restriction on the way of “interpreting” the circuit. Given
an arithmetic circuit Φ, we can interpret the polynomial computed by Φ
in the “usual” way, or we can interpret the polynomial computed by
Φ in a noncommutative way. Namely, we can think of the variables as
living in a noncommutative world, so that the circuit may not “assume”
that xy = yx for variables x,y. For example, from a commutative point
of view (x + y) × (x −y) = x2 −y2, whereas the noncommutative way
gives (x + y) × (x −y) = x2 + yx −xy + y2. Hence, since we have less
possible ways to create cancellations, it is more diﬃcult to compute a
given polynomial in the noncommutative model than in the commuta-
tive model. This makes the task of proving lower bounds for noncommu-
tative circuits easier than for commutative circuits. Indeed, in terms of
proving lower bounds, more is known about the noncommutative world
than the commutative one.
We start by discussing Nisan’s noncommutative formula-size lower
bound [98].
Theorem 3.8 ([98]). There exists an explicit noncommutative poly-
nomial f of polynomial degree that can be computed by a noncommu-
tative circuit of linear size, but every noncommutative formula for f
has exponential size. In addition, viewed as a commutative polynomial,
f can be computed by a polynomial size commutative formula.
This exponential lower bound yields an exponential separation
between noncommutative circuit-size and formula-size. This is in con-
trast to the commutative world, where every circuit having a poly-
nomial size and degree can be simulated by a quasi-polynomial size
formula (an immediate corollary of Theorem 2.8). Another interest-
ing point is that the polynomial f given in Theorem 3.8, viewed
as a commutative polynomial, has a polynomial size commutative
formula, and hence noncommutative formulas are exponentially weaker

254
Lower Bounds
than commutative ones. Nisan also proved similar exponential lower
bounds on the formula-size of the noncommutative permanent and
determinant.
Chien and Sinclair strengthened Nisan’s result in the following way
[35]. Nisan proved that, say, determinant cannot be computed by small
noncommutative formulas as a formal polynomial. This does not rule
out the possibility that over some other noncommutative domains, that
are more structured than the free noncommutative algebra, it is possible
to compute the determinant by a small formula. Chien and Sinclair
proved that even when the computation is over 2 × 2 real matrices,
which form the “most structured non-trivial noncommutative domain,”
there are no small formulas for determinant.
We now sketch the proof of Theorem 3.8.
Proof. [Sketch]
To
understand
noncommutative
formulas,
Nisan
deﬁned the notion of an arithmetic branching program (ABP).
Deﬁnition 3.1(ABP). An ABP is a layered graph with d + 1 layers
as follows. The layers are labeled by 0,1,...,d. The edges of the graph
go from layer i to layer i + 1. Every edge e is labeled by a homoge-
neous linear form ℓe = 
j∈[n] ce,jxj. Layer 0 has only one vertex called
the source, and layer d has only one vertex called the sink. For every
directed path from the source to the sink γ = (e1,e2,...,ed), deﬁne the
polynomial associated to γ, denoted f[γ], as follows: f[γ] = ℓe1ℓe2 ···ℓed.
The polynomial computed by an ABP is 
γ f[γ].
The ﬁrst step in the proof is to simulate noncommutative formu-
las by ABPs. This is done in two stages. In the ﬁrst stage we con-
struct by induction a “messy” ABP as follows. We deﬁne ABP(Φ) to
be the ABP corresponding to the formula Φ by induction. When Φ
is an input gate, it is clear how to deﬁne ABP(Φ). To construct
ABP(Φ1 × Φ2) we connect ABP(Φ1) and ABP(Φ2) sequentially, and
to construct ABP(Φ1 + Φ2) we connect ABP(Φ1) and ABP(Φ2) in a
parallel way. In the second stage we transform the “messy” ABP to
an ABP as follows. Split every vertex v to r vertices (v,1),...,(v,r),
where r is the degree of the polynomial computed by Φ. Put all vertices

3.4 Noncommutative Computation
255
of the form (v,i) in layer i. Wire the edges so that (v,i) computes the
homogeneous part of degree i of v (here the wiring is slightly more elab-
orate than in Theorem 2.2). The second stage may cause an increase
in size by a factor of O(r).
The second step in the proof is to ﬁnd a “weakness” of noncommuta-
tive ABPs. To do so, Nisan used the so-called partial derivative matrix.
Given a homogeneous noncommutative polynomial f of degree r in
x1,...,xn and k ∈{0,1,...,r}, deﬁne Mk(f) as the nk by nr−k matrix
whose ((i1,...,ik),(j1,...,jr−k)) entry is the coeﬃcient of the monomial
xi1 ···xikxj1 ···xjr−k in f. Note that due to non-commutativity, every
noncommutative monomial of degree r can be uniquely decomposed to
a product of two monomials of degrees k and r −k. The “weakness”
of noncommutative ABPs is that if the k-th layer in a noncommuta-
tive ABP has t vertices, then the polynomial f computed by the ABP
admits rank(Mk(f)) ≤t. Indeed, denote by h1,...,ht the polynomials
computed by the vertices in layer k, thinking of these vertices as sinks.
Denote by g1,...,gt the polynomials computed by the sink when the
sources are the t vertices in layer k, respectively. Thus, f = t
i=1 higi
and so Mk(f) = t
i=1 Mk(higi). It is not hard to see that each of the
matrices Mk(higi) has rank one, and hence rank(Mk(f)) ≤t.
Therefore, if a polynomial f of degree 2r admits rank(Mr(f)) ≥R,
then every noncommutative ABP for f has size at least R, and so every
noncommutative formula for f is large as well. We now give an exam-
ple of a degree 2n polynomial with R = 2n. Let x0
1,x1
1,x0
2,x1
2,...,x0
n,x1
n
be 2n variables. For every a = (a1,...,an) ∈{0,1}n, deﬁne the degree
2n monomial ma = xan
n xan−1
n−1 ···xa1
1 xa1
1 xa2
2 ···xan
n . Set f = 
a ma. Thus,
Mn(f) is a permutation matrix of size 2n × 2n and so its rank is 2n.
By the discussion above, f requires exponentially large noncommuta-
tive formulas. Note, however, that f can be computed by a linear size
noncommutative circuit (of linear depth, of course); deﬁne f0 = 1 and
fi = x0
i fi−1x0
i + x1
i fi−1x1
i for i ∈[n]. It is clear that f = fn has a small
circuit.
Albeit this understanding of noncommutative formulas, we do not
know any lower bound for noncommutative circuits, that is better
than the lower bound for commutative circuits that we already know.

256
Lower Bounds
In Section 3.8.5 we present an approach of Hrubeˇs et al. [60] for proving
lower bounds for noncommutative circuits.
Open Problem 10. Prove a super-polynomial lower bound on the
size of noncommutative circuits.
We remark that the fact that rank(Mk(f)) ≤t (in the notations of
the proof) will play an important role in Section 4.5 where we discuss
deterministic algorithms for polynomial identity testing of noncommu-
tative formulas.
3.5
Constant Depth Circuits
We now address a second family of restricted circuits, constant depth
circuits (recall that for constant depth circuits the fan-in is unbounded).
One of the main motivations for studying restricted families of circuits
is to gain a better understanding of the general model. In Boolean
circuit complexity, the model of bounded depth circuits has received
a lot of attention and exponential lower bounds (that deteriorate as
the depth increases) were proved for it [53], but no lower bounds for
general Boolean circuits were derived from those results. In particu-
lar, currently we do not know how to translate a constant depth lower
bound of the form exp(n), for any Boolean function f, to anything more
than a proof that f cannot be computed by linear size and logarithmic
depth Boolean circuits (see e.g., the recent Survey [144]). However, in
sharp contrast to the Boolean case, if we could prove an exp(n) lower
bound for any n-variate multilinear polynomial even for depth-4 arith-
metic circuits then this immediately implies an exp(n) lower bound for
general arithmetic circuits [5, 103, 88]. Thus, understanding shallow
arithmetic circuits is almost as diﬃcult and important task as under-
standing general circuits. This fact gives a very strong motivation for
studying small depth circuits.
We start this section with a brief survey of known lower bounds for
the size of constant depth circuits. We then consider depth-3 circuit,
the “ﬁrst depth that we do not understand.” Finally, we discuss circuits
of arbitrary constant depth.

3.5 Constant Depth Circuits
257
The simplest type of constant depth circuits are depth-2 circuits.
Such circuits compute either a sum of monomials or a product of linear
functions. This means that the size of a depth-2 circuit computing an
irreducible f is just the number of monomials in f.
Our understanding of (even) depth-3 circuits is far from complete.
Recall that depth-3 ΣΠΣ circuits compute a sum of products of linear
forms (there is also the less interesting case of ΠΣΠ circuits, that are
“closer” to depth-2 circuits, which we do not discuss here). Over ﬁnite
ﬁelds, Grigoriev and Karpinski [50] and Grigoriev and Razborov [52]
proved exponential lower bound on the size of ΣΠΣ circuits. In fact,
their lower bound holds for any circuit computing the function MODq
(where q is a prime number diﬀerent than the characteristic of the
ﬁeld), which is stronger than a lower bound for computing some spe-
ciﬁc polynomial-representation of the function (recall that the MODq
function is deﬁned as MODq(x1,...,xn) = 1 iﬀ
i xi = 0 mod q, think-
ing of xi as an integer).
Theorem 3.9([50, 52]). Let p ̸= q be two primes. Then, every ΣΠΣ
circuit computing the n-variate MODq function over Fp must be of size
at least 2Ω(n).
Proof. [Sketch] Let Φ be a ΣΠΣ circuit over the ﬁeld with p elements Fp.
The idea is to partition the product gates of Φ into two sets, and to ﬁnd
the weakness of each of these sets. We partition the gates of Φ according
to their rank: Every product gate v in Φ of degree dv multiplies dv
linear functions. Deﬁne the rank of v as the dimension of the span of
these dv linear functions. Partition the product gates of Φ into VHIGH
and VLOW , where VHIGH is the set of product gates of rank at least R
(we shall choose R = εn for a constant ε > 0 that depends on p and q),
and VLOW contains the rest of the gates.
We now describe the weakness of ΣΠΣ circuits (over ﬁnite ﬁelds).
If we substitute random ﬁeld elements as inputs, each gate in VHIGH
is nonzero with probability at most (1 −1/p)R = 2−cpR. On the other
hand, each product gate in VLOW can be represented by a low degree
polynomial, namely, a polynomial of degree at most pR. The union

258
Lower Bounds
bound hence implies that a ΣΠΣ circuit Φ of size s can be well approx-
imated by a low degree polynomial. More accurately, there exists a
polynomial g ∈Fp[x1,...,xn] of degree at most pR so that
Pr
a [Φ(a) ̸= g(a)] ≤2−cpRs,
where a is uniform in Fn
p.
Smolensky [130] proved that the MODq function cannot be well
approximated by low degree polynomials over the Boolean cube. Intu-
itively, this property of the MODq function should complete our proof.
However, Smolensky’s result holds over the Boolean cube, and does not
imply the needed property over Fp. To overcome this, Grigoriev and
Razborov use the observation that the uniform distribution over Fn
p is
a convex combination of distributions that are uniform over translates
of Fn
2 in Fn
p. The proof now immediately follows.
Over large ﬁelds, however, the best lower bound known on the size
of ΣΠΣ circuits is quadratic [129, 123]. In fact, over inﬁnite ﬁelds no
super-linear lower bound on the number of product gates in depth-3
circuits is known. We shall now give a proof of a nearly quadratic lower
bound for depth-3 circuits computing the determinant.
Theorem 3.10 ([129]). Every ΣΠΣ circuit computing the determi-
nant of an n × n matrix must be of size at least Ω(n4/logn).
Proof. [Sketch] The proof combines two common methods in lower
bounds proofs. We eliminate gates of high degree by restricting our
inputs to some aﬃne space (thus making certain linear functions
vanish). Then, we claim that the space of partial derivatives of the
remaining circuit is of somewhat low rank whereas that of the deter-
minant has high rank. For the latter property we shall rely on the fact
that determinant is downward-self-reducible in a strong sense.
Let us start with some notation. Recall that for a polynomial f ∈
F[x1,...,xn], we denote with ∂xi(f) the partial derivative of f with
respect to xi. We also deﬁne ∂{x1,x2}(f) as ∂x1(∂x2(f)) = ∂x2(∂x1(f)).
Notice that the order does not matter. For any S ⊆[n], we can similarly

3.5 Constant Depth Circuits
259
deﬁne ∂S(f). Finally, for an integer k, we denote by ∂(f;k) the vector
space over F spanned by all polynomials of the form ∂S(f), where S is
a sized k set.
We shall use the following two simple claims.
Claim 3.11. For every product gate v of degree r in a ΣΠΣ circuit,
the dimension of ∂(fv;k) is at most
r
k

, where fv is the polynomial
that v computes.
Proof. Assume fv = r
i=1 ℓi, where each ℓi is a linear form. The space
∂(fv;k) is spanned by the set {
i∈T ℓi : T ⊆[r], |T| = r −k}, whose
size is at most
r
k

.
Claim 3.12. For every k, the dimension of ∂(DETn;k) is at least
n
k
2,
where DETn is the determinant of an n × n matrix X = (xi,j).
Proof. For every two sets R = {r1 < r2 < ··· < rk} and C = {c1 <
c2 < ··· < ck} of [n], let S(R,C) = {xr1,c1,...,xrk,ck}. The polyno-
mial ∂S(R,C)(DETn) is the determinant of the n −k × n −k matrix
that is obtained by deleting the rows indexed by R and columns
indexed by C (these are all monomials that “contain” the monomial
xr1,c1xr2,c2 ···xrk,ck). The set of all such ∂S(R,C)(DETn) is linearly inde-
pendent over F. The number of such pairs (R,C) is
n
k
2.
The two claims above tell us that if all product gates in a ΣΠΣ
circuit Φ computing DETn are of degree at most D, then the size of Φ
is at least
n
k
2/
D
k

for every k. Choosing k = n2/(D · e), Stirling’s
approximation tells us that the size of such a Φ is at least Ω(en2/(D·e)).
Thus, if D is linear in n then we get an exponential lower bound. How-
ever, in general this does not have to be the case and so our argument
is based on the following case analysis. If there are “many” gates of
“high” degree then clearly the circuit has many wires. I.e., a lower
bound immediately follows. On the other hand if the circuit has a few
gates of high degree then by restricting the inputs to a subspace we can

260
Lower Bounds
eliminate all gates and still get a circuit computing the determinant
of a slightly smaller matrix. We now explain the details behind this
argument: So far we established that when all product gates are of
small degree, the circuit must be large. In particular, if all the prod-
uct gates containing the variable x1,n are of degree at most D, then
the size of Φ is at least Ω(e(n−1)2/(D·e)). Indeed, consider the circuit
Ψ = Φ|x1,n=1 −Φ|x1,n=0 where we eliminate all product gates not con-
taining x1,n in Φ and only maintain the gates of Φ containing x1,n (each
such gate will “appear” twice in Ψ). Clearly Ψ computes DETn−1 and
all its gates are of degree at most D. Hence a lower bound on the size
of Ψ and hence on the size of Φ follows. Clearly, the same reasoning
works if we consider x1,n−1,x1,n−2,...,x1,1 and not just x1,n.
If this is not the case, i.e., there is a gate v1,n of degree at least
D that contains x1,n, then there is a linear function ℓ1 so that the
restriction of the polynomial Φv1,n to the subspace x1,n = ℓ1 is zero,
i.e., Φv1,n|x1,n=ℓ1 = 0. Applying this restriction, we can eliminate at
least D wires from Φ. We repeat this line of argument sequentially for
x1,n−1,x1,n−2,...,x1,2. If at some point we ﬁnd that all gates contain-
ing x1,i (after the substitutions made to x1,n,...,x1,i+1) are of degree D
then we proceed as in our former argument. Otherwise we ﬁnd an appro-
priate substitution to x1,i. At the end of the process we get that either
there are at least Ω(en2/(De)) gates in Φ or that we eliminated (n −1)D
wires. In the latter case we set x1,1 = 1 and xi,1 = 0 for all i ∈{2,...,n}.
This gives a circuit computing DETn−1. To conclude, if we denote by
ΣΠΣ(DETn) the size of the smallest ΣΠΣ circuit computing the deter-
minant of an n × n matrix, then for every D,
ΣΠΣ(detn) ≥max(Ω(e(n−1)2/(D·e)),ΣΠΣ(detn−1) + (n −1)D).
Choosing D = n2/(4elogn), the theorem follows by induction.
Open Problem 11. Prove super-polynomial lower bounds on the size
of ΣΠΣ circuits over inﬁnite ﬁelds.
For depths higher than three weaker lower bounds are known.
Raz [103] and Shoup and Smolensky [122] proved a lower bound of

3.5 Constant Depth Circuits
261
(roughly) n1+1/d on the size of depth-d circuits over arbitrary ﬁelds.
The main diﬀerence between [122] and [103] is that [122] proved2 the
lower bound for a polynomial of degree n, whereas Raz [103] proved
the lower bound for a polynomial of constant degree, O(d). This dif-
ference is particularly interesting when we are dealing with constant
depth circuits, since without loss of generality we can assume that
the best arithmetic circuit for a constant degree polynomial is of con-
stant depth. Hence, if the lower bound of Raz [103] was a suﬃciently
large polynomial rather than just super-linear, then it would hold for
general, arbitrary depth, circuits as well. We now discuss Raz’s lower
bound [103].
Theorem 3.13 ([103]). For every d ∈N, there exists a polynomial f
with zero-one coeﬃcients of degree at most O(d) in O(nd) variables so
that every depth-d circuit computing f is of size at least n1+Ω(1/d).
Raz’s proof uses the language of Elusive Functions. To simplify the
presentation, we prove the theorem without using elusive functions (for
more details about Elusive Functions see Section 3.8.3). The argument
that we give is close in spirit to the proof of Shoup and Smolensky [122].
Proof. [Sketch] We shall use the following complexity measure. Let G
be a ﬁeld extension of F (e.g., G = F(Y ), the ﬁeld of rational functions
over a new set of variables Y ). Let F = {fa : a ∈[n]} be a family of
homogeneous degree one polynomials in the variables X = {xb : b ∈[n]}
over G. Think of each fa as a vector of coeﬃcients: for a variable xb,
denote by f[a,b] the coeﬃcient of xb in fa. Deﬁne Dm(F) as the span
over F of all expressions of the form 
(a,b)∈S f[a,b], where S is a subset
of [n] × [n] of size m. As G is an extension ﬁeld of F, this vector space
is not trivial. We are mainly interested in the dimension of Dm(F) as
a vector space over F.
The following claim is the basic observation behind the lower bound.
A variant of this lemma appeared in Ref. [122].
2 In fact, this can be deduced using the techniques of Shoup and Smolensky [122], but it
was not stated nor proved there.

262
Lower Bounds
Lemma 3.14. Let F be a family of homogeneous degree one polyno-
mials that can be computed by a depth-d circuit Φ of size s over G.
Then the dimension of Dm(F) is at most
s+m
m
d.
Proof. As every polynomial in F is linear and homogeneous, we can
assume without loss of generality that every gate in Φ computes a
homogeneous degree one polynomial. We can transform Φ to such a
form, without increasing the size and depth, by keeping only the “lin-
ear homogeneous part” of every gate (here we allow gates to compute
arbitrary linear combinations of their inputs).
Denote by Ti the set of elements of G that label edges that enter
gates of depth i −1 in Φ. By the assumed structure of Φ, every
coeﬃcient in f[a,b] is a sum of products of the form 
i∈[d] ti with
ti ∈Ti ∪{1}. Thus Dm(F) is contained in the span over F of all expres-
sions of the form
(t1,1t1,2 ···t1,m)(t2,1t2,2 ···t2,m)···(td,1td,2 ···td,m)
with every ti,j ∈Ti ∪{1}. By a rough estimate, the number of such
expressions is at most
s+m
m
d.
To prove a lower bound, it thus suﬃces to ﬁnd a family F so that
the dimension of Dm(F) is large. Raz deﬁned such a family as follows.
Assume that n is prime, and consider the ﬁeld of rational functions
G = F(Y ) with Y = {yi,j : i ∈[5d],j ∈[n]} and the variables X = {xi :
i ∈[n]}. For every a,b ∈[n], deﬁne f[a,b] ∈G as
f[a,b] =

i∈[5d]
yi,ai+b,
where ai + b is taken modulo n. This deﬁnes a family F of n homoge-
neous degree one polynomials in the variables X over G. An important
point in the argument is that we can also think of F as a family of
polynomials over the ﬁeld F in the variables X,Y (so the “ﬁnal” family
of polynomials is in F[X,Y ]). Note that since the output polynomials
are homogeneous linear forms in the X variables, we can still assume
that each gate in the circuit computes a homogeneous linear form in X
and therefore the argument remains the same.

3.5 Constant Depth Circuits
263
Claim 3.15. The dimension of Dm(F), with F deﬁned above, for m =
⌊n1−1/(2d)⌋, is at least 1
2
n2
m

.
Proof. For
every
set
S ⊂[n] × [n]
of
size
m,
deﬁne
f[S] =

(a,b)∈S f[a,b]. Notice that f[S] is a monomial in the Y variables. It
thus suﬃces to show that the number of diﬀerent Y monomials in the
set {f[S] : S ⊂[n] × [n],∥S| = m} is at least one half of the total num-
ber of such sets S.
We say that a set S is unique if for every (a,b) ̸∈S, one of the vari-
ables in {yi,ai+b : i ∈[5d]} does not appear in f[S]. The term “unique”
originates from the following property. If S is unique and f[S′] = f[S],
then every (a,b) not in S is also not contained in S′, and therefore
S = S′.
We now claim that at least half of the sets S ⊂[n] × [n] of size m are
unique. This will complete the proof of the claim. To see that this holds,
let us choose S uniformly at random and prove that S is unique with
probability at least one half. For every (a,b) ∈[n] × [n] and i ∈[5d],
denote by La,b,i the “line” {(a′,b′) ∈[n] × [n] : (a −a′) + i(b −b′) = 0
mod n}. For every (a,b), the following events are equal
{one of the variables {yi,ai+b : i ∈[5d]} does not appear in f[S]}
≡{there exists i ∈[5d] so that S ∩La,b,i = ∅}.
Denote by Ea,b,i the event {S ∩La,b,i ̸= ∅}. The size of La,b,i \ {(a,b)} is
n −1 and the size of [n] × [n] \ {(a,b)} is n2 −1. As for i ̸= i′ in [5d],
the two lines La,b,i,La,b,i′ intersect only at the point (a,b), for every
a,b,i, we have
Pr[Ea,b,i|Ea,b,1,...,Ea,b,i−1,(a,b) ̸∈S]
≤Pr[Ea,b,i|(a,b) ̸∈S] ≤|S|(n −1)/(n2 −1),
which implies
Pr


i∈[5d]
Ea,b,i
(a,b) ̸∈S


≤(|S|(n −1)/(n2 −1))5d ≤n−5d/(2d) ≤n−2/2.

264
Lower Bounds
Applying the union bound again, the probability that S in not unique
is at most n2n−2/2 = 1/2.
Lemma 3.14 with m = ⌊n1−1/(2d)⌋and Claim 3.15 imply that if a
depth-d circuit of size s computes F then
n2
m

/2 ≤
s+m
m
d, which
implies s ≥n1+Ω(1/d). It just remains to conclude the same lower bound
for a single polynomial rather than a family of n polynomials. Theo-
rem 2.5 tells us that the constant depth circuit-size of F is equivalent
to that of
f =

a∈[n]
zafa,
where F = {fa : a ∈[n]}, and Z = {za : a ∈[n]} is a new set of vari-
ables. This completes the proof (to conclude the theorem for all inte-
gers n, we can use the density of prime numbers). We remark that the
“ﬁnal” polynomial for which the lower bounds is proved, is a polyno-
mial in the n + 5nd + n variables X,Y,Z over F. As explained above,
this does not change the conclusion.
To conclude our discussion of constant depth circuits, we consider a
special type of ΣΠΣ circuits which we suggest as a step toward solving
Problem 11.
Denote σk,m = 
S⊂[m]:|S|=k

i∈S xi, the k-th symmetric polyno-
mial in m variables. Ben-Or gave a ΣΠΣ circuit of size O(n2) that,
simultaneously, computes σ0,n,...,σn,n (see Ref. [129]). Here is a rough
description of this circuit: evaluate the polynomial f(z) = (z + x1)(z +
x2)···(z + xn) = n
i=0 zn−iσi at n + 1 diﬀerent points, and recover the
coeﬃcient of zn−i, which is σi, using interpolation. This construction
works as long as the ﬁeld is large enough, namely, contains at least n + 1
points so that interpolation is possible. In contrast, in the Boolean
world we know that computing the symmetric polynomials requires
exponentially large constant depth circuits. This is another evidence of
the diﬀerence between the ﬁnite ﬁeld case and the inﬁnite ﬁeld case.
In [123] it was shown that for every polynomial f(x1,...,xn)
of degree r over the ﬁeld of complex numbers C, there exists
m = m(f) ∈N so that f = σr,m(y1,...,ym), where y1,...,ym are linear

3.6 Multilinear Circuits and Formulas
265
functions in x1,...,xn. Using Ben-Or’s construction, we conclude that,
over C, every polynomial f has a ΣΠΣ circuit of size roughly (m(f))2.
Therefore, proving lower bounds on m(f) over C is easier than proving
lower bound on the size of a ΣΠΣ circuit for f.
Open Problem 12. Prove super-polynomial lower bounds on m(f)
over C.
Currently the best lower bound known on m(f) is linear in n [123].
Thus, even the problem of proving super-linear lower bounds on m(f)
is open.
3.6
Multilinear Circuits and Formulas
In the multilinear world our understanding of formulas, for which strong
lower bounds are known, is quite good (but still far from being com-
plete), whereas our understanding of circuits, for which only weak
bounds are known, could be greatly improved. We start by stating
the lower bounds and separations known for multilinear formulas, and
explaining the ideas behind the proofs of these results. We then sketch
the proof of the best known lower bound for multilinear circuits, and
ﬁnally we discuss lower bounds and separations for constant depth mul-
tilinear circuits. The reader is advised to recall the relevant deﬁnitions
from Section 2.2.1.
The ﬁrst super-polynomial lower bounds on the size of multilinear
formulas were proved by Raz [104]. We shall give a sketch of the proof
idea below.
Theorem 3.16 ([104]). Every multilinear formula computing either
the permanent or the determinant of an n × n matrix must be of size
at least nΩ(logn).
Subsequently, Raz showed a super-polynomial separation between
multilinear circuit-size and formula-size [102]. Later, Raz and Yehuday-
oﬀ[108] gave a simpliﬁed proof for this separation.

266
Lower Bounds
Theorem 3.17([102]). There exists a polynomial f that can be com-
puted by a polynomial size syntactically multilinear circuit, but every
multilinear formula for f must be of super-polynomial size.
As for general circuits (Theorem 2.8 above), syntactically multi-
linear circuits can be simulated eﬃciently by syntactically multilinear
circuits of depth log2 n. Theorem 3.17 therefore shows that in the multi-
linear world NC1 ̸= NC2. In addition, by examining the proof of Theo-
rem 3.17 we see that the “ideas” of Theorem 3.16 are not suﬃcient
for proving super-polynomial lower bounds on the size of syntacti-
cally multilinear circuits (as the lower bound proof is the same as in
Theorem 3.16).
The best lower bound known on the size of syntactically multilinear
circuits is slightly better than the one known for general circuits [107].
Theorem 3.18([107]). There is an explicit polynomial f ∈VNP with
0/1 coeﬃcients so that every syntactically multilinear circuit comput-
ing f must be of size Ω(n4/3/log2 n).
As a side remark, we note that one can show that a lower bound of
s on the size of syntactically multilinear circuits automatically implies
an Ω(s/n3) lower bound on the size of noncommutative circuits [60].
Open Problem 13. Prove a super-polynomial lower bound on the
size of multilinear circuits.
We now move on to discussing the ideas required to prove
Theorem 3.16. The two main tools required for proving lower bounds
for multilinear formulas are the partial derivative matrix and random
partitions of the variables. These tools were ﬁrst used in this context
by Raz, and they could be seen as an adaptation/evolvement of similar
ideas from the earlier works [53, 98, 100]. We ﬁrst give a brief descrip-
tion of these concepts, and then discuss how to combine them to a
proof.

3.6 Multilinear Circuits and Formulas
267
The partial derivative matrix was previously discussed in the con-
text of noncommutative computation (see Section 3.4). Here we use
it in a slightly diﬀerent manner. Given a polynomial f in two sets of
variables Y = {y1,...,ym} and Z = {z1,...,zm}, deﬁne M(f) as the 2m
by 2m matrix whose (S,T) entry, S,T ⊆[m], is the coeﬃcient of the
monomial 
i∈S yi

j∈T zj in f. The partial derivative matrix has a few
useful and simple properties (for more details see, e.g., Ref. [104]).
Lemma 3.19. Given two polynomials f and g,
(1) rank(M(f + g)) ≤rank(M(f)) + rank(M(g)),
(2) rank(M(f · g)) ≤rank(M(f)) · rank(M(g)) (here we take
the multilinear part of f · g), and
(3) rank(M(f)) ≤2min(Y (f),Z(f)) ≤2(Y (f)+Z(f))/2, where Y (f)
is the number of Y variables that occur in f and Z(f) is
the number of Z variables that occur in f.
As in the noncommutative world, we think of the rank of this
matrix as an “evidence of hardness” of a given polynomial. How-
ever, the situation in the multilinear world is not as simple as in the
noncommutative world. For example, the partial derivative matrix of
f = (y1 + z1)(y2 + z2)···(ym + zm) has full rank, but f has a linear
size formula. To overcome this diﬃcultly, Raz suggested to consider
the rank with respect to a random partition of the variables to two
sets. Speciﬁcally, let X = {x1,...,x2m} be a set of variables. A parti-
tion A is a one-to-one map from X to Y ∪Z. In other words, it is a
renaming of the variables X by Y and Z. For a polynomial f in X,
deﬁne f[A] as the polynomial in Y,Z that is obtained by substituting
every xi by A(xi) in f. For our needs we will only consider partitions
in which |Y | = |Z|. The following theorem is the crux of Raz’s idea and
demonstrates a “weakness” of multilinear formulas.
Theorem 3.20. ([102]) There is a constant ε > 0 so that if a multi-
linear formula of size nεlogn computes an n-variate polynomial f, then
there exists a partition A such that the rank of M(f[A]) is not full.

268
Lower Bounds
We say that a polynomial f has full rank if M(f[A]) has full rank
for every A. Theorem 3.20 thus tells us that every full rank polynomial
requires multilinear formulas of super-polynomial size. Now, to prove
a lower bound for multilinear formulas, we just need to ﬁnd a full
rank polynomial. We address this issue after sketching the proof of the
theorem (we present a slightly diﬀerent proof than Raz’s).
Proof. [Sketch] The ﬁrst step is to decompose f into “building blocks,”
which we call log-product polynomials. A polynomial g is log-product
if it is a product of t = (logn)/100 polynomials g = g1 · g2 ···gt so that
the set of variables X can be partitioned into t sets X1,...,Xt, each of
size at least n1/2, where every gi is deﬁned over the variable-set Xi. Let
Φ be a multilinear formula of size s −1 computing f.
Lemma 3.21. Every polynomial f computed by a multilinear formula
of size s can be written as a sum of s + 1 polynomials f = f1 + ··· +
fs+1, where each fi is a log-product polynomial.
Proof. [Sketch] Find a gate v in Φ so that n/3 ≤|Xv| ≤2n/3. By induc-
tion, Φv is a sum of |Φv| log-product polynomials in the variables Xv.
As Φ is multilinear it holds that f = g · Φv + h, where g is a polynomial
in X \ Xv and h is the polynomial computed by Φ after labeling v by
zero. The structure of f now follows by using the induction hypothesis
for h.
The second part of the proof is the following probability esti-
mate. Choose A uniformly at random from the set of all partitions.
For every log-product polynomial g = g1 · g2 ···gt, the probability of
the event E(g) = {rank(M(g[A])) ≥2n/2−n1/16} is at most n−Ω(t) =
n−Ω(logn). This probability estimate, together with the union bound
and Item (1) from Lemma 3.19 complete the proof of the theorem,
since
Pr
A [rank(M(f[A])) = 2n/2] ≤Pr
A [E(f1) ∪E(f2) ∪··· ∪E(fs)]
≤sn−Ω(logn) ≤1/2,
for ε > 0 a small enough constant. To prove this probability estimate,
we argue that E = E(g) is roughly the intersection of Ω(t) independent

3.6 Multilinear Circuits and Formulas
269
events, each of which happens with probability at most order n−3/16.
We ﬁrst write this statement (which completes the proof) formally, and
then explain it in more detail. The formal statement is
Pr[E] ≤Pr[∀j ∈[t]
rank(M(gj[A])) ≥2|Xj|/2−n1/16]
≤Pr[∀j ∈[t]
|A(Xj) ∩Y | −|Xj|/2
 ≤n1/16]
≤(cn1/16|Xj|−1/2)t ≤n−Ω(t),
where c > 0 is a constant and X1,...,Xt is the partition of X corre-
sponding to the representation of g as a log-product polynomial. The
ﬁrst inequality holds (from a contra-positive point of view) by proper-
ties (3.19) and (3.19) from Lemma 3.19: if there exists j′ ∈[t] so that
rank(M(gj′[A])) < 2|Xj′|/2−n1/16 then
rank(M(g[A])) ≤2

j̸=j′ |Xj|/2 · rank(M(gj′[A])) < 2n/2−n1/16.
The second inequality holds by property (3.19) from Lemma 3.19, as for
any two real numbers α,β we have min(α,β) = (α + β)/2 −|α −β|/2.
The third inequality holds by the union bound and standard properties
of the hyper-geometric distribution. Finally, the last inequality holds
as |Xj| ≥n1/2.
Raz also showed that both determinant and permanent are full rank
polynomials (in some, more general, sense), thus proving Theorem 3.16.
To be able to view permanent and determinant as full rank polynomi-
als, we deﬁne a slightly more general family of partitions. A partition
A of an n × n matrix X is a map from X to Y ∪Z ∪{0,1}, where
Y = {y1,...,ym}, Z = {z1,...,zm} and m = [n1/3], deﬁned as follows.
Let R1 = {r1(1),...,r1(m)}, R2 = {r2(1),...,r2(m)} be two disjoint
subsets of [n], and let C1 = {c1(1),...,c1(m)}, C2 = {c2(1),...,c2(m)}
be another pair of disjoint subsets of [n]. These two pairs correspond
to subsets of rows and columns of X, respectively. For every i ∈[m], A
can be deﬁned as
xr1(i),c1(i)
xr1(i),c2(i)
xr2(i),c1(i)
xr2(i),c2(i)

→A
yi
zi
1
1

or as
xr1(i),c1(i)
xr1(i),c2(i)
xr2(i),c1(i)
xr2(i),c2(i)

→A
yi
1
zi
1

.

270
Lower Bounds
Denote {j1 < ··· < jn−2m} = [n] \ (R1 ∪R2) and {ℓ1 < ··· < ℓn−2m} =
[n] \ (C1 ∪C2). For every i ∈[n −2m], set A(xji,ℓi) = 1. For every vari-
able x ∈X that A is not already deﬁned on, set A(x) = 0. So, up to a
permutation, the matrix X after the substitution A looks like









B1
...
Bm
1
...
1









,
where each Bi is a 2 × 2 matrix such that DET(Bi) = yi −zi. For
every A, we have DET[A] = 
i∈[m](yi −zi), and so the rank of
M(DET[A]) is full. It turns out that similar probability estimates to
the ones in the proof of Theorem 3.20 hold when we consider the uni-
form distribution on such partitions instead of the uniform distribution
on all partitions.
Although we know how to prove lower bounds for multilinear formu-
las, we still do not understand well enough their computational power
compared to that of general formulas.
Open Problem 14. Are multilinear formulas weaker than general
formulas?
We now give a sketch of the proof of Theorem 3.18. In fact we will
sketch the proof of the following theorem. Theorem 3.18 follows by
exhibiting a polynomial with 0/1 coeﬃcients that have the required
properties. We shall not give the details of this construction here.
Theorem 3.22 ([107]). Let Ψ be a syntactically multilinear arith-
metic circuit over the ﬁeld G and the set of variables X = {x1,...,xn}
computing f. Let Y = {y1,...,ym} and Z = {z1,...,zm} be two sets of
variables (where n = 2m and m is even). If for all partitions A of X to
Y and Z Rank(M(f[A])) = 2m, then |Ψ| = Ω( n4/3
log2 n).

3.6 Multilinear Circuits and Formulas
271
Proof. [Sketch] The proof combines the ideas of Theorems 3.20 and 2.5
together with the general theme of looking for a “special” structure of
polynomials computed by small circuits.
The ﬁrst step of the proof is the following analog of Theorem 2.5.
The proof resembles the proof of Theorem 2.5 and so we leave it to
the reader. Note that the lemma actually guarantees the existence of
a syntactic multilinear circuit Ψ′ that has more structure than in the
case of general circuits.
Lemma 3.23. Let Ψ be a syntactically multilinear arithmetic circuit
over the ﬁeld F and the set of variables X computing f. Then, there
exists an arithmetic circuit Ψ′ over the ﬁeld F and the set of variables X
such that the following hold:
1. Ψ′ computes all n partial derivatives ∂x1(f),...,∂xn(f).
2. |Ψ′| = O(|Ψ|).
3. Ψ′ is a syntactically multilinear arithmetic circuit.
4. For every i ∈[n], if vi is the gate computing ∂xi(f) in Ψ′,
then xi ̸∈var(vi), where var(vi) is the set of variables in the
sub-circuit rooted at vi.
Another important ingredient in the proof is the following. Let Φ
be a syntactically multilinear arithmetic circuit over the ﬁeld F and
the variables X = {x1,...,xn}. Fix τ = 3logn. Deﬁne L(Φ,τ), the set
of lower leveled gates in Φ, as the set of all gates u in Φ, such that 2τ <
|var(u)| < n −2τ, and u has a father u′ such that |var(u′)| ≥n −2τ.
The next theorem shows that if a circuit does not have enough lower
leveled gates then it computes a (relatively) low rank polynomial.
Theorem 3.24. Let f be a polynomial that is computed by a syntac-
tically multilinear arithmetic circuit Φ, over the ﬁeld F and the set of
variables X = {x1,...,xn}. Let Y = {y1,...,ym} and Z = {z1,...,zm}
be two sets of variables (where n = 2m and m is even). Let τ = 3logn,
and let L = L(Φ,τ) be the set of lower leveled gates in Φ. Let c > 0
be a small enough constant (c = 1/1000 suﬃces). Assume |L| < c
τ n1/3.

272
Lower Bounds
Then, there exists a partition A of X to Y and Z such that
rank(M(f[A])) < 2m−1.
At ﬁrst sight this theorem seems very weak as it only guarantees
the existence of (roughly) n1/3 gates in the lower leveled set. The pic-
ture becomes clearer when one considers Lemma 3.23 and the following
simple observation.
Observation 3.1. Let A be a partition of X. If Rank(M(f[A])) = 2m.
Then, for every xi ∈X
rank(M(∂xi(f)[A])) = 2m−1.
Our goal will be to show that for every variable xi the set of lower
leveled gates in the sub-circuit computing ∂xi(f) is large and that these
sets are “almost” disjoint.
Indeed, let Ψ′ be the arithmetic circuit computing all n par-
tial derivatives of f given by Lemma 3.23. Let τ = 3logn, and let
L = L(Ψ′,τ) be the set of lower leveled gates in Ψ′. Deﬁne U = U(Ψ′,τ),
the set of upper leveled gates in Ψ′, to be the set of all gates u ∈Ψ′
that have a child in L and that satisfy |var(u)| ≥n −2τ. To prove the
theorem, we will bound from below the size of U.
Let i ∈[n]. Set gi = ∂xif. Let vi be the gate computing gi in Ψ′.
Denote by Ψ′
i the arithmetic circuit Ψ′
vi. Deﬁne Li = L(Ψ′
i,τ) to be the
set of lower leveled gates in Ψ′
i. It is not diﬃcult to see that Li ⊆L. In
addition, Theorem 3.24 and Observation 3.1 imply that |Li| ≥c
τ n1/3.
For a gate v in Ψ′, deﬁne Cv = |{i ∈[n] : v is a gate in Ψ′
i}|. For i ∈[n],
deﬁne Ui = {u′ ∈U : u′ is a gate in Ψ′
i}. Thus, for all i ∈[n], we have
Ui ⊆U. Hence,

i∈[n]
|Ui| = |{(u′,i) : u′ ∈U
and
i ∈[n]
are such that u′ is a gate in Ψ′
i}| =

u′∈U
Cu′.

3.6 Multilinear Circuits and Formulas
273
Let i ∈[n]. As Li ⊆L it follows that for every gate u ∈Li there is a cor-
responding gate u′ ∈Ui, which is a father of u. Thus, since the in-degree
of the gates in Ui is 2, we have |Li| ≤2|Ui|. Recall that, for all u′ ∈U,
it holds that |var(u′)| ≥n −2τ. Property 4 of Lemma 3.23 now implies
that every u′ ∈U
admits Cu′ ≤n −|var(u′)| ≤n −(n −2τ) = 2τ.
Combining all of the above we get
c
τ n4/3 ≤

i∈[n]
|Li| ≤2

i∈[n]
|Ui| = 2

u′∈U
Cu′ ≤2|U| · 2τ.
In particular, |U| = Ω( n4/3
log2 n) and the result follows.
Thus, all that is left to do is to prove Theorem 3.24. This is the
main technical diﬃculty of Raz et al. [107].
Proof. [Sketch of Proof of Theorem 3.24] The proof is based on a struc-
ture theorem and a probability estimate. Denote L = {u1,...,uℓ}. Set
Yi = Yui and Zi = Zui to be the set of Y variables and Z variables that
appear in the sub-circuit of Φ[A] rooted at ui, respectively. Denote with
vf the output gate of Φ.
Theorem 3.25.
For every partition A, f[A] can be expressed
as f[A] = 
i∈[ℓ] giΦui + g, where g,g1,...,gℓ∈F[Y,Z] are multilinear
polynomials such that for all i ∈[ℓ], the set of variables that occur
in gi and the set Yi ∪Zi are disjoint, g is the polynomial computed
by vf after substituting (in Φ[A]) every u ∈L by 0 and, furthermore,
deg(g) ≤4τ.
The proof of the theorem is by induction on the structure of Φ and
we leave it to the readers. Another ingredient required for the proof
of Theorem 3.24 is the following probabilistic argument whose proof
is also left to the readers. We say that a gate v is k-unbalanced if
||Yv| −|Zv|| ≥2k.
Lemma 3.26. Let Ψ be an arithmetic circuit over the ﬁeld F in
the set of variables X = {x1,...,xn}. Let Y = {y1,...,ym} and Z =
{z1,...,zm} be two sets of variables (where n = 2m and m is even). Let

274
Lower Bounds
X1 ⊂X be a subset of X of size n/4. Let A be a random partition of
X to Y and Z, conditioned on the event A(X1) ⊂Y . Let β be such
that 0 < β < 1, and let v be a gate inΨ such that nβ < |Xv| < n −nβ.
Then, for any integer k ∈N,
PrA[v is not k-unbalanced in Ψ[A] | A(X1) ⊂Y ] = O(kn−β/2).
We continue with the proof of Theorem 3.24. The ﬁrst step is to
use Lemma 3.26 together with a “clever” union bound to show that
there exists a partition A such that every u ∈L is τ-unbalanced in
Φ[A]. In the second step, we use Theorem 3.25 to express f[A] as
f[A] = 
i∈[ℓ] giΦui + g. Since each u ∈L is τ-unbalanced, Lemma 3.19
implies that rank(M(giΦui[A])) ≤2m−τ. As deg(g) ≤4τ it holds that
rank(M(g[A])) ≤m4τ (this is an upper bound on the number of nonzero
rows and columns in the matrix M(g[A])). Hence, rank(M(f[A])) ≤
|L| · 2m−τ + m4τ < 2m−1, where the last inequality holds as τ = 3logn
and m = n/2.
This completes the overview of the proof of Theorem 3.22.
For bounded depth multilinear circuits we can prove much stronger
lower bounds, similar to those known for bounded depth Boolean
circuits [53].
Theorem 3.27([110]). Every depth-d multilinear circuits computing
either the permanent or the determinant of an n × n matrix must be
of size 2nΩ(1/d).
In fact, for bounded depth multilinear circuits we have a strong
separation result.
Theorem 3.28 ([110]). For every constant d, there is an n-variate
polynomial that is computed by a polynomial-size polynomial-degree
syntactically multilinear circuit of product-depth3
d, but every
3 The product-depth of a gate v is the maximal number of product gates in a directed path
reaching v. Depth and product-depth are equivalent up to a factor of two, for circuits of
unbounded fan-in.

3.6 Multilinear Circuits and Formulas
275
multilinear circuit of product-depth d −1 computing it must be of size
at least nΩ(log1/2d(n)).
We now explain the idea behind the proof of Theorem 3.27. The
proof of Theorem 3.28 is more involved and we shall not discuss it here
(the lower bound part is similar to the proof of Theorem 3.27, but
the construction requires a detailed discussion). To demonstrate the
main idea we give a high-level description of the proof of the following
theorem.
Theorem 3.29 ([110]). There is a constant ε > 0 so that if a multi-
linear product-depth d circuit of size at most 2nε/d computes an
n-variate polynomial f, then there exists a partition A such that the
rank of M(f[A]) is not full. (Here n is large enough compared to d.)
Proof. [High-level] The main idea behind the proof of the nΩ(logn) lower
bound for the size of multilinear formulas was to express a multilinear
formula of size s as a sum of s polynomials, each of which is a product
of t = O(logn) polynomials with at least n1/2 variables each, and then
applying a random partition to the variables. Then, using probabil-
ity estimates, we obtained a lower bound of the form nΩ(t). A similar
though more elaborate analysis can be performed for constant depth
multilinear circuits as well. In what follows we assume (using Theo-
rem 2.4) that Φ is a syntactic multilinear formula (making Φ a formula
may increase the size by a polynomial factor).
Deﬁnition 3.2. A polynomial f over the variables X = {x1,...,xn} is
called d-weak if for t = [n1/(2d)] one of the following holds:
(1) f = g1 · g2 ···gt, where for every i ̸= j in [k] the two poly-
nomials fi,fj are deﬁned over disjoint sets of variables, and
for every i in [k] the variable-set of fi has size at least t.
(2) f = ℓ· g, where ℓis a linear form over at least t variables
and the variable-set of g is disjoint from that of ℓ.

276
Lower Bounds
The following lemma is the basic decomposition required for the
lower bound proof.
Lemma 3.30. Let d ∈N and let Φ be a multilinear circuit of product-
depth d and size s over n variables computing h. Then there exist
d-weak polynomials h1,...,hs+1 so that
h = h1 + h2 + ··· + hs+1.
Proof. [High-level] The lemma follows by similar arguments to the
proof of Theorem 2.7. The basic observation is that (as long as Φ con-
tains at least n/2 variables) either
(1) Φ has a product gate v that has at least t children, each
computing a polynomial over at least t variables; or
(2) Φ contains a sum gate v that computes a linear function
over at least t variables.
This property holds as Φ has product-depth d and is syntactically mul-
tilinear. With this property (as in Theorem 2.7), we can write h as
h = gvhv + h′
v,
where hv is the polynomial computed at v, gv is a polynomial whose
variable-set is disjoint from that of hv, and h′
v is another polynomial
that can be computed by a smaller formula. The lemma now follows
by induction on the size of Φ, as the polynomial gv · hv is d-weak.
As in the proof of Theorem 3.20, we will use random partitions. A
one-to-one map from X to Y,Z with |Y | = |X|/2 is called a partition.
In the following A is chosen uniformly at random from the family of
all partitions.
Lemma 3.31. Let h be a d-weak polynomial. Then
Pr
A [rank(M(h[A])) ≥2n/2−t1/4] ≤2−Ω(t).

3.7 Circuits with Bounded Coeﬃcients
277
Proof. Assume that h can be written as h = g1 · g2 ···gt as in the def-
inition of d-weak. Similar to the proof of Theorem 3.20, the event
{rank(M(h[A])) ≥2n/2−t1/4} is roughly the intersection of t indepen-
dent events, the probability of which is at most 1/2. Otherwise, h =
ℓ· g, and then for all A,
rank(M(h[A])) ≤rank(M(ℓ[A])) · rank(M(g[A])) ≤2 · 2(n−t)/2.
The two lemmas imply that
Pr
A [rank(M(f[A])) = 2n/2] ≤

i∈[s+1]
Pr
A [rank(M(fi[A]))
≥2n/2/(s + 1)] ≤(s + 1)2−Ω(t) < 1,
where s ≤2nε/d is the size of the circuit for f and f1,...,fs+1 are d-weak
polynomials (assuming ε > 0 is small enough).
In conclusion, we see that strong lower bounds are known for mul-
tilinear formulas and for bounded depth multilinear circuits. However,
many open problems still remain, most signiﬁcantly proving super poly-
nomial lower bounds for general multilinear circuits (or even just for
syntactically multilinear circuits).
3.7
Circuits with Bounded Coeﬃcients
When computing a polynomial (over, say, the complex numbers) whose
coeﬃcients have bounded absolute value, say, at most 1, it is not clear
whether the use of large constants can help the computation. In partic-
ular it seems unnatural to use large coeﬃcients in order to compute a
polynomial that has, say, 0/1 coeﬃcients. This motivates the study of
circuits with bounded coeﬃcients. A circuit with bounded coeﬃcients
is a circuit in which the input gates are labeled by variables and not by
ﬁeld elements, and the edges are labeled by ﬁeld elements with absolute
value at most 1 (any other bound c ≥1 on the absolute value can be
translated to 1 by a simple reduction). The computation is performed
as before with the notable diﬀerence that every edge is multiplied by the
constant labeling it. Such circuits were suggested by Morgenstern [94],

278
Lower Bounds
and were later studied by Chazelle [33] and Raz [101] as well. Morgen-
stern proved the following lower bound [94].
Theorem 3.32. ([94]) Every circuit with bounded coeﬃcients com-
puting the discrete Fourier transform of ¯x = (x1,...,xn) must be of size
Ω(nlogn).
This lower bound is tight, as there is a circuit with bounded coeﬃ-
cients of size O(nlogn) computing the Fourier transform [38] (at least
when n is a power of two).
Proof. [Sketch] For simplicity, we shall consider computation over R.
Consider a circuit Φ computing n linear forms ⟨¯α1, ¯x⟩,...,⟨¯αn, ¯x⟩in the
variables ¯x, where ⟨·,·⟩denotes the usual inner product in Rn (note
that DFT has exactly this form). Since these are homogeneous linear
forms, we can assume without loss of generality that Φ is a linear
circuit, that is, all computations done in Φ are of the form Φv = a1Φv1 +
a2Φv2, with a1,a2 ∈R. As we consider the bounded coeﬃcients model
we have that |a1|,|a2| ≤1. For every gate v in Φ, we can thus associate
a vector ¯βv in Rn so that v computes ⟨¯βv, ¯x⟩. The main idea behind
the proof is to consider the volume of the polytope deﬁned by the
linear functions computed at the gates of the circuit. Let ¯α1,..., ¯αn be
n vectors in Rn. The volume of these n vectors is the absolute value of
the determinant of the n × n matrix whose rows are ¯α1,..., ¯αn. We now
deﬁne a progress measure that does not increase fast for circuits with
bounded coeﬃcients. For a linear circuit Ψ, denote by volume(Ψ) the
maximum volume of ¯βv1,..., ¯βvn, over all choices of n gates v1,...,vn
in Ψ. Since the coeﬃcients in Φ are bounded in absolute value by 1,
a sum gate can increase the volume, of the circuits computed by its
children, by at most a factor of two. Indeed, it follows by a simple
induction that the volume of a size s circuit, with coeﬃcients bounded
by 1, is at most 2s. Since the volume of the n vectors deﬁning the Fourier
transform is nn/2, any circuit with bounded coeﬃcients computing it
has size at least Ω(nlogn).
Generalizing Morgenstern’s ideas, Raz proved the following lower
bound [101].

3.8 Approaches for Proving Lower Bounds
279
Theorem 3.33 ([101]). Every circuit with bounded coeﬃcients com-
puting the product of two n × n matrices must be of size Ω(n2 logn).
Note that these two lower bounds hold for polynomials of constant
degree. In contrast, no super-linear lower bound is known on the size
of general circuits computing constant degree polynomials (recall Open
Problem 7).
3.8
Approaches for Proving Lower Bounds
In this section we shortly discuss several general approaches for proving
lower bounds. We shall not give the most general treatment of each of
these approaches but rather give the main ideas, mostly by considering
some speciﬁc examples.
3.8.1
Rigidity
The notion of rigidity was introduced by Valiant [137] for the purpose
of proving size-depth tradeoﬀs, namely, showing that a circuit of depth
O(logn) and linear size cannot compute some polynomial f. We will
demonstrate this method for the task of computing a linear transfor-
mation, although it is more general and can take diﬀerent forms in
diﬀerent contexts.
We say that a matrix M is (S,R)-rigid if by changing at most S
entries in every row of M one cannot decrease its rank below R. The
integer S is called the sparsity parameter and R the rank parameter.
Theorem 3.34 ([137]). If an n × n matrix M is (S,R)-rigid with
S = n1/10 and R ≥n/100 then any circuit computing Mx, for x =
(x1,...,xn), cannot be of (simultaneously) depth O(logn) and size
O(n).
Proof. [Sketch] Assume toward a contradiction that there exists a cir-
cuit Φ of depth O(logn) and size s = O(n) computing Mx. We can
assume without loss of generality that every gate in Φ computes a
homogeneous linear form (at the cost of a constant blow-up in size).

280
Lower Bounds
Valiant’s idea is to show that there exists a set V of gates in Φ of
size |V | = o(s), so that after deleting all the edges connected to V , the
circuit becomes of depth at most (logn)/10. This is a combinatorial
claim on graphs of logarithmic depth and has nothing to do with the
function being computed by the circuit. We leave the proof of this claim
to the reader.
With this observation in mind, one can argue that M has a “special”
structure. Denote by Fi the linear form deﬁned by the i-th row of M.
Now, if we delete V from Φ, then we are left with a circuit of depth at
most (logn)/10, and therefore each output gate originally computing
some Fi is now connected to at most n1/10 input gates. We can thus
express each Fi as a linear combination of the linear forms {Φv}v∈V and
of at most n1/10 input variables. In other words, the matrix M can be
written as a sum of a matrix of rank at most o(s) = o(n) (whose rows
are spanned by {Φv}v∈V ) and a sparse matrix having at most n1/10
nonzero entries in every row. This is a contradiction, as M is rigid.
To prove a size-depth tradeoﬀ, it thus suﬃces to ﬁnd an explicit
rigid matrix, a matrix that cannot be expressed as a sum of a low
rank matrix and a sparse matrix. Counting arguments imply that most
matrices are in fact rigid. However, and as usual, we do not know of
any explicit rigid matrix. It seems that the main diﬃculty in ﬁnding
a rigid matrix comes from the fact that we need to argue about two
diﬀerent structures at the same time: low rank matrices and sparse
matrices. We have a good understanding of each of these families, but
we do not understand how to jointly analyze them. For some recent
result on rigidity see Ref. [42].
3.8.2
Tensor Rank
The tensor rank approach is quite general as well. Here we give two
examples of its possible applicability: a circuit-size lower bound and a
formula-size lower bound. We start by deﬁning some notions related
to tensors. A three-dimensional tensor T is a map T : [n]3 →F. A ten-
sor is a higher dimensional version of a matrix: an n × n matrix M is
simply a map M : [n]2 →F. A rank one tensor is a tensor T so that
T(i1,i2,i3) = t1(i1)t2(i2)t3(i3), where t1,t2, and t3 are maps from [n]

3.8 Approaches for Proving Lower Bounds
281
to F. This coincides with the deﬁnition of rank one matrices: a matrix
M has rank one if M(i1,i2) = m1(i1)m2(i2). The rank of a tensor T is
the minimal R ∈N so that T = R
j=1 Tj with every Tj being a rank one
tensor. Tensor rank is a generalization of matrix rank, and it is not dif-
ﬁcult to see that, e.g., the rank of a three-dimensional tensor is at most
n2. As the next theorem shows, tensor rank is an important measure
that is closely related to the circuit complexity of the underlying tensor.
Theorem 3.35 ([133]). Let T
be a three-dimensional tensor of
tensor rank at least R. Then the polynomial F(x,y,z) = 
i,j,k∈[n]
T(i,j,k)xiyjzk requires circuits of size at least Ω(R), over R.
Proof. [Sketch] Let Ti be the matrix deﬁned by Ti(j,k) = T(i,j,k).
Consider a circuit Φ computing the n bilinear forms F1,...,Fn deﬁned
by T1,...,Tn, respectively. Namely, Fi = 
j,k Ti(j,k)yjzk. By Theo-
rem 2.5, the circuit complexity of F1,...,Fn is the same as that of F,
up to constant factors. By possibly increasing the size by a constant
factor we can assume without loss of generality that every gate v in Φ
computes a linear form or a bilinear form fv = 
j,k Tv(j,k)yjzk. Con-
sider the set V of product gates v = u × w in Φ so that fu and fw
are linear forms. By a homogenization argument, fu is a linear form in
y1,...,yn and fw is a linear form in z1,...,zn. A simple argument shows
that each Fi is a linear combination of {fv}v∈V . By simple algebraic
manipulations one gets that the tensor rank of T is at most |V |. The
size of Φ is hence at least Ω(|V |) ≥Ω(R).
We remark that circuits of the kind that was discussed in the proof
are called bilinear circuit. The best known lower bounds for bilinear
circuits for problems such as polynomial multiplication or matrix mul-
tiplication are given in Refs. [19, 25, 74, 124].
Tensor rank is a generalization of matrix rank (which we understand
pretty well) to higher dimensions. However, we do not know of any
explicit tensor of high rank. This shows the striking diﬀerence between
tensors and matrices. Another evidence of that diﬀerence is reﬂected in
the computational complexity of the two notions. Computing the rank
of a matrix is fairly easy, e.g., using Gaussian elimination. On the other

282
Lower Bounds
hand, H˚astad proved that computing the tensor rank of a given tensor
is NP-hard [54].
Counting arguments show that most three-dimensional tensors are
of rank Ω(n2) (similar to the existence of hard polynomials). However,
the following question is open.
Open Problem 15. Find an explicit three-dimensional tensor whose
rank is ω(n).
We now discuss a recent connection between tensor rank and
formula-size lower bounds that was found by Raz [105]. For this we
need to consider k-dimensional tensors, that are a straightforward gen-
eralization of three-dimensional tensors.
Theorem 3.36. ([105]) Let T : [n]k →F be an explicit tensor (i.e.,
given i1,...,ik we can compute T(i1,...,ik) in polynomial time) with
k = O(logn/loglogn). If the tensor rank of T is at least nk(1−o(1)) (in
particular, k must tend to inﬁnity as n tends to inﬁnity, since the tensor
rank of T is always at most nk−1) then permanent requires formulas of
super-polynomial size.
Proof. [Sketch] The proof consists of two diﬀerent parts. First we ana-
lyzes set-multilinear polynomials. A set-multilinear polynomial with
respect to a set S of size k is a polynomial in k disjoint sets of variables
{Xi}i∈S so that every monomial that appears in it is of the form 
i∈S xi
with xi ∈Xi. Raz showed that as long as k ≤O(logn/loglogn) we
can eﬃciently transform a general formula computing a set-multilinear
polynomial to a set-multilinear formula computing the same polyno-
mial. Namely, to a formula so that for every v = v1 × v2 in it, the
polynomial fv1 is set-multilinear with respect to a subset S1 ⊆S, the
polynomial fv2 is set-multilinear with respect to a subset S2 ⊆S, and
S1 ∩S2 = ∅. In particular, fv is set-multilinear with respect to S1 ∪S2.
Theorem 3.37. ([105]) Let Φ be a formula of size s that computes
a set-multilinear polynomial with respect to [k]. Then there exists a

3.8 Approaches for Proving Lower Bounds
283
set-multilinear formula Ψ of size at most (O(logs))ks that computes
the same polynomial.
Proof. The proof is similar to the proof of Theorem 2.3.
Secondly, we prove that the maximal tensor rank that a set-
multilinear formula can compute is relatively small. Note that a tensor
T : [n]k →F deﬁnes a set-multilinear polynomial fT in the variables
{xi,j : i ∈[n],j ∈[k]} in a natural way: the coeﬃcient of k
j=1 xij,j in
fT is T(i1,i2,...,ik).
Lemma 3.38. Let T : [n]k →F be a tensor with k ≤O(logn/loglogn).
If there exists a set-multilinear formula of size nc for the polynomial
fT then the tensor rank of T is at most nk(1−2−O(c)).
Proof. [High-level] Raz proved the following intuitive statement. A set-
multilinear formula of size nc that computes the “polynomial with the
maximal tensor rank” must have a very speciﬁc structure: in particular,
the topmost 2−O(c)k gates must be product gates. To prove this he used
the following three simple properties of tensor rank: (1) the tensor rank
of an ℓ-dimensional tensor is at most nℓ−1, (2) the tensor rank of 
i Ti
is at most the sum of the tensor ranks of the Ti’s, and (3) the tensor
rank of T1 ⊗T2 is at most the product of the tensor ranks of T1 and
T2 (if T1 : [n]k1 →F and T2 : [n]k2 →F, then T1 ⊗T2 : [n]k1+k2 →F is
deﬁned by T1 ⊗T2(¯i1,¯i2) = T1(¯i1)T2(¯i2)). The proof of this structure
is too long to include here, but the idea is that properties (2) and (3)
above tell us that tensor rank cannot increase too quickly in a formula,
and property (1) tells us that we should do the sum operations “as
low as possible.” This argument could be thought of as a convexity
argument.
With this structure, we can write fT as a product of at least k′ =
2−O(c)k polynomials of the form fTi. Property (1) tells us that the
tensor rank of each Ti is at most nki−1, where 
i ki = k. Now, property
(3) implies that the tensor rank of T is at most nk−k′, since T = T1 ⊗
T2 ⊗··· ⊗Tk′, as claimed.

284
Lower Bounds
The theorem and lemma above tell us that if a k-dimensional ten-
sor T with k ≤O(logn/loglogn) has tensor rank at least nk(1−o(1))
then fT cannot be computed by a polynomial size formula, as oth-
erwise it can also be computed by a polynomial size set-multilinear
formula and this is impossible. The conclusion about permanent
holds by its completeness and the assumption that T is explicit (see
Theorem 1.1).
Currently, the best lower bound on tensor rank is Ω(n⌊k/2⌋) by Nisan
and Wigderson [100] for k-dimensional tensors, e.g., the symmetric ten-
sor T(i1,...,ik) = 1 iﬀ|{i1,...,ik}| = k. Here is a sketch of the simple
proof of such a lower bound: take a tensor T and write it as an n⌊k/2⌋
by n⌈k/2⌉matrix M by partitioning its k “dimensions” to two disjoint
sets. Observe that if T has rank one then M has rank one as well. This
implies that the tensor rank of T is at least the rank of the matrix M.
Hence, if M has full rank then T has rank at least n⌊k/2⌋. We note, how-
ever, that much tighter lower bounds are required in order to obtain
lower bounds for the formula-size of the permanent.
3.8.3
Elusive Functions
We now discuss the Elusive Functions approach for proving circuit-size
lower bounds suggested by Raz [103]. Here too we focus on a particular
example in order to explain the more general approach.
There are several structure results that we can use as the starting
point of the argument. Here we (roughly) use the one given by Theo-
rem 2.8. Let f be a polynomial of degree d in the variables x1,...,xn.
Given a circuit Φ computing f, there exist s ∼|Φ| polynomials f1,...,fs
such that
f =

i
fi,
(3.1)
where every fi can be written as fi = fi,1 · fi,2, where fi,1,fi,2 are of
degrees at most 2d/3.
We now wish to understand how the coeﬃcients of the monomials
in fi look like. For every T = (t1,...,tn) ∈Nn we deﬁne the mono-
mial xT = xt1
1 ···xtn
n . We denote deg(T) = 
k tk. Given T ∈Nn and

3.8 Approaches for Proving Lower Bounds
285
j ∈{1,2}, let yi,j(T) be the coeﬃcient of the monomial xT in fi,j.
Now, think of the yi,j(T)’s as new variables and deﬁne Yi = Yi,1 ∪Yi,2,
where Yi,j = {yi,j(T) : deg(T) ≤2d/3}. It follows that the coeﬃcient
of xT in fi is 
T1+T2=T yi,1(T1) · yi,2(T2). In particular, this coef-
ﬁcient is a degree two polynomial in the variables Yi. This argu-
ment shows that each coeﬃcient in f is a degree two polynomial in
the variables Y = 
i Yi. Denote the size of Y by S. We have that
|Y | = S ≤O(n2d/3s).
Let N =
n+d
d

. We think of [N] as being the set of all T ∈Nn of
degree at most d. Consider the polynomial map ϕ from FS to FN that
is deﬁned above. Namely, the T-th output of ϕ is s
i=1

T1+T2=T yi,1 ·
yi,2, i.e., the degree two polynomial corresponding to the coeﬃcient of
the monomial xT in f. Now, every substitution of ﬁeld elements to the
variables Y deﬁnes a degree d polynomial in the variables x1,...,xn. We
say that this polynomial is in the image of ϕ. More importantly, every
degree d polynomial that has a circuit of size roughly s is contained in
the image of ϕ. In other words, in order to ﬁnd an explicit polynomial
that cannot be computed by circuits of size s, we just need to ﬁnd an
explicit point in FN that is not in the image of the degree two map ϕ.
Why should such a point exist? Well, if the circuits are not too large,
i.e., S ≪N, then the input dimension of ϕ, S, is much smaller than the
output dimension, N, so such points must exist (see also Lemma 3.2).
Raz called such points Elusive Functions as they elude the maps deﬁned
by small circuits. To conclude, separating VNP from VP boils down to
solving an algorithmic problem: ﬁnding a point outside the image of a
given degree two polynomial map.
The example above considers the problem of eluding degree two
polynomial maps. More generally, Raz proved that if one can elude
degree r maps ϕ : FS →FN, for many settings of r,S,N, then a lower
bound on the circuit complexity of the permanent follows.
3.8.4
Geometric Complexity Theory
We now address Mulmuley and Sohoni’s approach for showing that
VP ̸= VNP [95, 96], which is called geometric complexity theory (GCT).
For simplicity, we focus on the case where the underlying ﬁeld is C.

286
Lower Bounds
We also focus on a speciﬁc goal: showing that permanent cannot be
“embedded” in a polynomial size determinant, solving Open Problem 1.
To argue this, GCT suggests to look at the symmetries of both deter-
minant and permanent.
Consider the determinant of an n × n matrix X = (xi,j). As a vec-
tor of coeﬃcients, it is an N-dimensional vector with N being roughly
n2
n

, as it is a multilinear homogeneous polynomial of degree n in n2
variables. Every n × n matrix U can act on N-dimensional vectors
by linearly transforming the variables X: Given a polynomial f(X),
which is also an N-dimensional vector, we deﬁne U ◦f(X) = f(U−1X)
(indeed, (U1U2) ◦f = U1 ◦(U2 ◦f)). An important property of deter-
minant (that in fact uniquely deﬁnes it) is that if U is in SL(n,C),
that is, DET(U) = 1, then U ◦DET = DET. Moreover, any U such
that U ◦DET = DET is in SL(n,C). In other words, SL(n,C) is in the
stabilizer of determinant.
The stabilizer of permanent is diﬀerent. If we act with SL(n,C) on
permanent, we may change it to a diﬀerent polynomial. However, every
n × n permutation matrix Π is in the stabilizer of the permanent,
Π ◦PERM(X) =

σ∈Sn
n

i=1
xΠ(i),σ(Π(i)) =

π∈Sn
n

i=1
xi,π(i) = PERM(X).
This diﬀerence between determinant and permanent is the reason,
according to GCT, that the permanent cannot be embedded in a small
determinant. In rough terms, permanent is much less symmetric than
determinant, its stabilizer is not as rich, and so in order to embed
permanent in determinant we must use high dimensions, to be able
to ﬁt the disorder. To prove this intuitive claim, GCT suggests to use
notions from many other areas of mathematics, such as representation
theory, invariant theory, and algebraic geometry. It seems, however,
that the current mathematical knowledge is very far from being able
to prove even simpler claims.
The appealing part of GCT is that it seems to overcome the barrier
of natural proofs (see Section 3.9 below). Speciﬁcally, it is known that
the stabilizers of both determinant and permanent characterize them,
e.g., the only polynomial f so that A ◦f = f for all A in the stabilizer
of determinant is f = DET, and similarly for the permanent. This is

3.8 Approaches for Proving Lower Bounds
287
highly unusual, in the sense that a random polynomial has a trivial
stabilizer, and in particular it is not characterized by it. Intuitively, this
means that if indeed we prove a lower bound based on these arguments,
it may work only for a small family of polynomials. Thus, (perhaps)
overcoming the natural proofs barrier.
3.8.5
Sum-of-Squares Problem
In [60] a new approach for proving lower bounds on the size of non-
commutative circuits was given. Speciﬁcally, Hrubeˇs et al. [60] showed
a reduction between the noncommutative complexity of permanent to
a classical mathematical problem called the sum-of-squares problem,
that arises in the areas of topology and algebra. The reader is referred
to the introduction of [60] for more background. We give a brief and
partial survey of this result.
For an integer n, deﬁne the sum-of-square complexity of n as the
minimal integer k so that there exist n complex matrices M1,...,Mn,
each of dimension n × k, so that
(1) for every i ∈[n], we have MiMt
i = I, where I is the n × n
identity and Mt is M transposed,
(2) for every i ̸= j in [n], we have MiMt
j = −MjMt
i .
Equivalently, the sum-of-squares complexity of n is the minimal integer
k so that there exist complex bilinear forms f1,...,fk, in {x1,...,xn}
and {y1,...,yn}, so that the following equality holds
(x2
1 + ··· + x2
n)(y2
1 + ··· + y2
n) = f2
1 + ··· + f2
k.
The following theorem relates sum-of-squares complexity to non-
commutative circuit complexity. The sum-of-squares complexity can
be deﬁned over any ﬁeld, but is related to circuit complexity only over
ﬁelds of characteristic diﬀerent than two, in which √−1 exists.
Theorem 3.39([60]). If the sum-of-square complexity of n is at least
Ω(n1+ε) with ε > 0 a constant, then permanent requires noncommuta-
tive circuits of exponential size over the complex numbers.

288
Lower Bounds
We do not include the proof of this statement here, as it is too
technical. We note, however, that Hrubeˇs et al. [60] also proved lower
bounds on the sum-of-squares complexity in some restricted cases. For
example, if the matrices Mi are restricted to be integer matrices, then k
must be at least Ω(n6/5). We also note that the sum-of-squares problem
has a few other equivalent formulations, e.g., as a question about degree
four polynomials and as a question about “embedding of unit spheres.”
3.9
Natural Proofs for Arithmetic Circuits?
As mentioned before, almost no lower bounds are known for Boolean
circuits. A remarkable result of Razborov and Rudich [111] gives a
partial explanation for this lack of success. In their work, Razborov
and Rudich introduced the concept of natural proofs and showed that a
natural proof cannot yield a super-polynomial lower bound and that all
existing techniques give rise to natural proofs. Speciﬁcally, a property
P of Boolean functions from {0,1}n to {0,1} is natural if it satisﬁes the
following requirements:
(1) “many” functions have P,
(2) it is “easy” to verify whether a given function has P by look-
ing at its truth table, and
(3) any Boolean circuit computing a function with the property
P has to be of “large” size.
Razborov and Rudich showed that all known lower bounds were proved
by analyzing natural properties (where the meaning of “many,” “easy”
and “large,” depend on the speciﬁc lower bound). The key point in their
argument is that a natural proof of a super-polynomial lower bound
implies that there are no families of pseudorandom functions. Namely,
families of functions that “behave like” the family of all functions, from
the point of view of a computationally eﬃcient observer: properties (1)
and (2) imply that any family of pseudorandom functions must con-
tain a function with the property P. However, under widely believed
complexity assumptions, families of pseudorandom functions that can
be computed by small circuits exist, and so condition (3) is violated.
Another way of stating this results is: if hard Boolean functions exist

3.10 Meta Lower Bounds
289
(i.e., our hardness assumptions are justiﬁed and so pseudorandom func-
tions exist) then it is hard to prove hardness results.
For arithmetic circuits no such argument exists. By considering
known lower bounds one sees that they are all “natural” in a sense.
For example, the known lower bounds for multilinear complexity and
noncommutative complexity are obtained by studying the rank of the
partial derivative matrix. Notice that (1) simple counting arguments
show that the rank of the partial derivative matrix of almost all poly-
nomials is high; and (2) given a polynomial as a list of coeﬃcients, we
can eﬃciently compute the rank of its partial derivatives matrix.
In order to apply Razborov and Rudich’s reasoning all that is
required is a construction of a family of pseudorandom polynomials
(based, of course, on reasonable hardness assumptions). Unfortunately
no such construction is known today. A natural approach would be
to try to apply the construction given in Ref. [111], that is based on
the construction of Goldreich et al. [48], in the arithmetic setting. The
problem with this idea is that the construction of Goldreich et al. is
highly sequential and by arithmetizing it one gets a polynomial of an
exponentially high degree. For a further discussion see Aaronson’s blog
post on the topic [1].
Open Problem 16.
Give an arithmetic analog of Razborov and
Rudich‘s result. Speciﬁcally, give an algebraic analog for pseudorandom
function generators. I.e., a family of pseudorandom functions that can
be computed by polynomial size (and polynomial degree) arithmetic
circuits.
3.10
Meta Lower Bounds
This chapter dealt with the fundamental problem of proving that a
certain computational task is hard. Speciﬁcally, with the problem of
ﬁnding explicit polynomials that require large circuits or formulas. We
conclude the chapter with a meta discussion of the basic ideas behind
this line of research.
When trying to understand what a given computational class (like
monotone circuits, multilinear circuits, etc.) can or cannot do, it is

290
Lower Bounds
natural to try and bring this class into some normal form. The advan-
tage of such a normal form is that it may unveil a certain weakness
of the underlying computational class. Finding such a weakness is, in
many cases, the key step in proving hardness results. Indeed, this line
of thought appeared many times in this chapter. Below is a table that
summarizes this approach for diﬀerent computational classes.
Circuit class
Normal form
Weakness
Monotone circuits
f = 
i higi, each hi,gi
monotone of degree
∼r/2
f has a few monomials
Noncommutative
formulas
Arithmetic branching
programs
Partial derivative matrix of
low rank
ΣΠΣ over Fp
In normal form
Well approximated by a low
degree polynomial
Constant depth circuits
In normal form
Vector space of coeﬃcients of
low dimension
Multilinear formulas
Sum of log-product
polynomials
Partial derivative matrix of
low rank, for a random
partition
Multilinear circuits
Sum of “structured”
polynomials
Partial derivative matrix of
low rank for some partition
Bounded coeﬃcient
Linear/bilinear circuits
Compute matrices with small
determinants
(Approaches)
Linear circuits
In normal form
Cannot compute rigid
matrices
Bilinear circuits
In normal form
Result has a low tensor rank

4
Polynomial Identity Testing
Polynomial Identity Testing (PIT) is a fundamental problem in
algebraic complexity: We are given an arithmetic circuit computing
a multivariate polynomial over some ﬁeld, and we have to determine
whether that polynomial is identically zero or not. In other words, we
want the polynomial to be formally zero (i.e., that all its coeﬃcients are
zero) and not just zero as a function over the ﬁeld. As a simple example
for the diﬀerence between these two notions of “zero,” consider x2 −x,
which is the zero function over F2 but not the zero polynomial. Note,
however, that if the size of the ﬁeld is higher than the degree of the
polynomial then a polynomial is formally zero if and only if it is zero as
a function. This is a well-known fact that can be found, e.g., in Ref. [6].
Solving
polynomial
identities
is
a
central
question
in
both
complexity theory and algorithm-design. For example, the best par-
allel algorithms for ﬁnding perfect matchings are based on testing
whether a given determinant is formally zero or not [32, 78, 90, 97].
Other algorithms based on identity testing are the primality testing
algorithm of [3, 4], algorithms for testing equivalence of read-once
branching programs [22], and more. In complexity theory, identity
testing played a major role in results such as IP = PSPACE [91, 120],
291

292
Polynomial Identity Testing
MIP = NEXPTIME [15], and the proof of the PCP theorem [10, 11].
Identity testing algorithms also lead to interpolation/learning algo-
rithms for sparse polynomials, see [18, 51, 86] and references within,
for depth-3 circuits [125, 76], and for read-once arithmetic formulas
[126, 127].
There are two well-studied models in which the PIT problem is
considered. The ﬁrst is the so-called black-box model in which the only
access to the circuit is by asking for its value on inputs of our choice. It
is clear that every deterministic algorithm in the black-box model must
produce a test set for the circuit, namely, a set of points such that if
the circuit vanishes on all points in the set then the circuit computes
the zero polynomial. Note that such an algorithm is non-adaptive as it
stops at the ﬁrst nonzero input. The second setting is the non-black-
box model in which the circuit is given as input. In particular, we have
access to the polynomials that are computed at the gates of the circuit.
We call this model the white-box model. Clearly, the white-box model
is the easier amongst the two, although the PIT problem is notoriously
hard also for this model.
The diﬃculty of the PIT problem stems from that the polynomial
is not given explicitly as a list of coeﬃcients, but rather implicitly by a
circuit or a formula. Converting an implicit representation to an explicit
one requires, in general, exponential time and so cannot be performed
eﬃciently. On the other hand, the advantage of such an implicit repre-
sentation is that the underlying polynomial can be evaluated eﬃciently
on any given input, as long as ﬁeld operations can be done eﬃciently.
Using this property, randomized (block-box) algorithms were designed
for the problem [41, 119, 148]. These randomized algorithms require
assignments from a relatively large ﬁeld. To better understand this,
consider a black-box algorithm testing only zero-one assignments. As
the “black-box” may contain a polynomial “accepting” only a single
input one can easily prove that 2n queries are required to determine
whether it computes the zero polynomial or not. Therefore, when con-
sidering PIT over ﬁnite ﬁelds we allow the algorithm to test assignments
from a polynomially large extension ﬁeld.
The challenge that remains is to design deterministic algorithms for
PIT, or, less ambitiously, to reduce the amount of randomness required

293
for solving the problem. The importance of derandomizing PIT follows
from its many applications. For example, the famous algorithm for
primality testing of Agrawal et al. [4] is based on giving a determin-
istic algorithm for a speciﬁc polynomial identity, using the derandom-
ization ideas of Agrawal and Biswass [3]. Speciﬁcally, their algorithm
veriﬁes that (x + a)n −xn −a is the zero polynomial modulo n, for
special values of a (indeed, the identity is zero for every a ∈Zn if
and only if n is a prime). Another example is that derandomization
of PIT will imply a deterministic parallel algorithm for ﬁnding perfect
matchings [97] which is an important open problem. Besides its algo-
rithmic implications, derandomization of PIT may lead to strong lower
bounds for arithmetic circuits. In particular, as we shall soon see, if
PIT can be solved deterministically in the black-box model, then there
exists an “explicit” polynomial that requires exponential size arithmetic
circuits.
Determining the complexity of PIT is one of the greatest chal-
lenges of theoretical computer science. It is one of a few problems for
which we have coRP algorithms [41, 119, 148], but no deterministic
sub-exponential time algorithms. A partial explanation for the hard-
ness of obtaining deterministic algorithms was given by Kabanets and
Impagliazzo [69], who showed that an eﬃcient deterministic algorithm
for PIT, even in the white-box model, implies that NEXP does not
have polynomial size arithmetic circuits. Namely, if PIT has polyno-
mial time deterministic algorithms, then either permanent cannot be
computed by polynomial size arithmetic circuits or NEXP ̸⊆P/poly.
In Refs. [2, 56] it was observed that a deterministic polynomial time
black-box PIT algorithm implies an exponential lower bound for a
polynomial whose coeﬃcients can be computed in PSPACE. While this
gives a stronger conclusion, we note that the result of [69] holds both
in the black-box model and in the white-box model, whereas that of
[2, 56] only holds in the black-box model. Kabanets and Impagliazzo
also showed an implication in the other direction: a super-polynomial
lower bound for the size of arithmetic circuits yields a deterministic
sub-exponential time algorithms for PIT. In Ref. [44] an (almost) anal-
ogous result to [69] was obtained for bounded depth circuits. These
results highlight the tight connection between PIT and lower bounds,

294
Polynomial Identity Testing
and raise the question of obtaining eﬃcient deterministic PIT algo-
rithms for models in which strong lower bounds are known.
We organize this chapter according to what we view as the four
main themes in the study of the PIT problem. The ﬁrst is devising
randomness-eﬃcient randomized algorithms for the problem. These
algorithms work for general arithmetic circuits. The second research
direction is trying to better understand the hardness of derandomizing
PIT, mainly by connecting deterministic PIT algorithms to circuit
lower bounds. The third theme is the study of PIT for restricted
models of arithmetic circuits. Here the main research goal is not the
restricted model itself, but rather the understanding of the more
general case, via some special instantiations of it. Finally, we discuss
results connecting PIT to other problems in algebraic complexity such
as polynomial factorization and the isolation lemma (ﬁrst deﬁned and
proved in Ref. [97]).
4.1
Generators and Hitting Sets
We start by deﬁning the basic notions of generators and hitting sets
and shortly discuss the relation between them. A polynomial mapping
G = (G1,...,Gn) : Ft →Fn is a generator for the circuit class M if for
every nonzero n-variate polynomial1 f ∈M, it holds that f(G) ̸≡0.
In other words, the polynomial f ◦G is not formally zero. The image of
the map G is Im(G) = G(Ft). Ideally, t should be very small compared to
n. A set H ⊆Fn is a hitting set for a circuit class M if for every nonzero
polynomial f ∈M, there exists a ∈H such that f(a) ̸= 0. A generator
for M can also be viewed as a polynomial map whose image contains
a hitting set for M (when the ﬁeld is large enough). That is, for every
nonzero f ∈M, there exists a ∈Im(G) such that f(a) ̸= 0. The follow-
ing lemma shows that we can eﬃciently construct a generator given a
hitting set and vice versa. From this point on generators are always
polynomial maps.
Lemma 4.1. Let |F| > n. Given a hitting set H ⊆Fn for a circuit
class M, there is an algorithm that, in time poly(|H|,n), constructs
1 We write f ∈M when f can be computed by a circuit from M.

4.1 Generators and Hitting Sets
295
a map G : Ft →Fn with t = ⌈log n |H|⌉that is a generator for M. Fur-
thermore, the individual degrees of each Gi are bounded by n −1.
In the other direction, let G : Ft →Fn be a generator for a circuit
class M such that the individual degrees of each Gi are bounded by r.
If M contains polynomials with individual degrees at most D then for
every set S ⊆F of size |S| > rD it holds that H = G(St) is a hitting set
for M.
Proof. [Sketch] Denote H = {¯a1,...,¯ah}, where h = |H|. Let ¯y1,..., ¯yh
be diﬀerent elements in Ft. We shall deﬁne G so that G(¯yi) = ¯ai. To do
so, we need to solve a set of linear equations, in the coeﬃcients of G,
that always has a solution due to choice of parameters.
The proof of the other direction follows by the fact that if f : Ft →F
has individual degrees at most rD then f(St) ̸≡0. While this is a well-
known fact we state it here as we shall use it many times implicitly.
The statement that we give is from Ref. [6]. The proof is by a simple
induction and is omitted.
Fact 4.1 ([6]). Let f(x1,...,xn) be a polynomial over an arbitrary
ﬁeld F. Suppose that the degree of f as a polynomial in xi is at most
ri, for 1 ≤i ≤n. Let Si ⊆F be a set of size at least ri + 1.2 Then, if f
is not formally zero then there exists (α1,...,αn) ∈S1 × S2 × ... × Sn
such that f(α1,...,αn) ̸= 0.
The
following
is
an
immediate
and
important
property
of
generators.
Observation 4.1. Let f = f1 · f2 ···fk be a product of nonzero poly-
nomials fi ∈M and let G be a generator for M. Then f(G) ̸≡0.
At times, we would like to use only partial substitutions. Given
a subset I ⊆[n], we deﬁne the mapping GI as (GI)i = Gi when i ∈I
and (GI)i = xi when i ̸∈I. In words, GI is the same as G in entries
2 This is where we need F to be “large enough.”

296
Polynomial Identity Testing
in I and is the “original” variables in entries not in I. In addition,
in a somewhat abuse of notations, deﬁne f ◦GI def
= f|GI to be the
polynomial resulting from substituting the polynomial Gi to the
variable xi in f, for every i ∈I.
Observation 4.2. Let M be a class of circuits and let G be a generator
for n-variate polynomials in M. Let I ⊆[n] and f ∈M be a nonzero
polynomial. Then f|GI ̸≡0. Moreover, if |F| is large enough then there
exists a ∈Im

GI
such that f(a) ̸= 0.
Finally, we observe that adding generators for two diﬀerent circuit
classes gives one generator for both classes.
Observation 4.3. Let G′ : Ft′ →Fn be a generator for M′ and G′′ :
Ft′′ →Fn be a generator for M′′. Assume (w.l.o.g.) that the zero vector
is in the image of both generators.3 Then G : Ft′+t′′ →Fn deﬁned as
G(y′,y′′) = G′(y′) + G′′(y′′) is a generator for both M′ and M′′.
4.2
Randomized Algorithms
There are several diﬀerent randomized algorithms for the PIT problem.
The so-called Schwartz–Zippel algorithm [41, 119, 148] is based on the
observation that by substituting random values to the variables, from a
large enough domain, one gets, with high probability, a zero value only if
the polynomial is zero. This is perhaps the simplest possible randomized
algorithm known and in spite (or perhaps because) of its simplicity it
has found many applications in theoretical computer science [91, 97,
120]. The Schwartz–Zippel algorithm has a randomness-error tradeoﬀ;
in order to reduce the error the algorithm uses more random bits. Chen
and Kao [34] and Lewin and Vadhan [89] designed algorithms that
have time-error tradeoﬀ; to achieve smaller error the algorithms need
to run for a longer time. These algorithms require less random bits,
3 This is a small technical requirement that is needed for handling models in which shifting
an input by a constant is not permitted (e.g., ΣΠ circuits). Clearly, adding the zero vector
to the image can be done at a very small cost.

4.2 Randomized Algorithms
297
compared to the Schwartz–Zippel algorithm, but have worse running
time.4 The Chen–Kao algorithm [34] works over the rational numbers,
while the Lewin–Vadhan algorithm [89] works for any ﬁeld. A diﬀerent
algorithmic approach was suggested by Agrawal and Biswass [3]. They
map the polynomial computed by the circuit to a univariate polynomial
of a very high degree, and then compute this polynomial modulo a
random low degree polynomial chosen from a carefully speciﬁed family
of low degree polynomials. This is a white-box algorithm, as in order to
compute the remainder of the polynomial division, one needs to access
the intermediate computations performed at the gates of the circuit.
The advantage of this approach was demonstrated in Ref. [4], where a
deterministic algorithm for primality testing, that follows this scheme
was given.
4.2.1
The Schwartz–Zippel Algorithm
As mentioned above, the Schwartz–Zippel algorithm is based on the
observation that a nonzero low degree polynomial does not have too
many zeros. The proof is by an easy induction on the number of vari-
ables and is left to the reader.
Lemma 4.2 ([119, 148]). Let f(x1,...,xn) be a nonzero polynomial
of degree at most r, and let T ⊆F. If we choose a = (a1,...,an) ∈T n
uniformly at random, then Pr[f(a) = 0] ≤r/|T|.
The lemma suggests a randomized algorithm for PIT: given a
degree r polynomial f(x1,...,xn), pick at random a ∈T n and check
whether f(a) = 0. If f ̸≡0, the probability of error is at most r/|T|,
and if f ≡0, we are always correct. To achieve error of at most ϵ, we
should pick a set T of size |T| ≥r/ϵ. This requires n · ⌈log(r/ϵ)⌉random
bits. Another corollary of Lemma 4.2 is that there is a small hitting
set for all polynomial size arithmetic circuits. The proof is by a simple
application of the union bound.
4 We note that using expanders graphs one can always obtain time-error tradeoﬀs (see,
e.g., [58]). The advantage of Refs. [34, 89] is that they provide additional insight into the
structure of the PIT problem.

298
Polynomial Identity Testing
Theorem 4.3. For every n,r,s and a ﬁeld F of size |F| ≥max(r2,s),
there exists a set H ⊆Fn of size |H| = poly(r,s) that is a hitting set
for all circuits of size at most s and degree at most r.
Proof. [Sketch] We ﬁrst describe the proof over ﬁnite ﬁelds. By counting
the number of directed acyclic graphs (DAGs for short) we get that
there are at most |F|2s · s2s arithmetic circuits over F of size at most s
(this bound holds for circuits of fan-in two). Let H be a set of size
4rs chosen uniformly at random from Fn. Lemma 4.2 tells us that the
probability that a nonzero circuit Φ of degree at most r vanishes on
all points in H is at most (r/|F|)|H| ≤|F|−4rs. By the union bound, it
follows that there is such a hitting set H for all nonzero circuits of size
at most s and degree at most r.
When |F| is inﬁnite, say F = Q, a slightly diﬀerent argument is used.
Here, the set of all polynomials computed by some arithmetic circuit of
size s is contained in the union of s2s manifolds, one for each possible
DAG, of dimension s each, that are obtained by varying the ﬁeld con-
stants. One can now prove that when we pick a point at random (say,
according to the Gaussian measure) from Fn, the set of polynomials
that vanish at that point and that are computed by size s arithmetic
circuits is a manifold of dimension at most s −1. Repeating this argu-
ment O(s) times we get the required result.
The hitting set guaranteed by the theorem is not explicit, of course,
and the main challenge is to come up with explicit constructions of
hitting sets.
4.2.2
Time-Error Tradeoﬀ
We now present the Chen–Kao and Lewin–Vadhan algorithms that
give time-error tradeoﬀs. Namely, to reduce the error the algorithm
does not invest more random bits, but rather runs longer. As men-
tioned earlier, using expander graphs, one can obtain such algorithms
in a “generic” way (see Section 1.3.3 in [58]). However, the results of
Refs. [34, 89] provide more information on the structure of the PIT

4.2 Randomized Algorithms
299
problem compared to the generic algorithm and highlight how alge-
braic independence could be used in this context.
Theorem 4.4 ([34]). Let f ∈Q[x1,...,xn] have individual degree at
most ri in xi, for every i ∈[n], and denote r = maxi ri. Assume that
each coeﬃcient of f has bit length at most L. Then there is a random-
ized algorithm that for every ϵ > 0, decides correctly, with probability
at least 1 −ϵ, whether f ≡0, using n
i=1⌈log(ri + 1)⌉random bits and
running time T = poly((L + rlog(nlogr))/ε).
For comparison, consider the case when f is multilinear. In this
case, in order to get error ϵ = 1/poly(n), the Schwartz–Zippel algorithm
needs O(nlogn) random bits, whereas the Chen–Kao algorithm uses
only O(n) random bits, but requires a longer running time.
Proof. [Sketch] The idea behind the algorithm is that if (a1,...,an)
are algebraically independent over Q then f ≡0 iﬀf(a1,...,an) = 0.
Clearly, algebraically independent numbers are transcendental over Q,
so instead of using them, we should pick numbers whose annihilating
polynomial is of high degree. However, such numbers must be irra-
tionals and so we cannot use them as inputs either (they have an inﬁnite
length bit representation). Nevertheless, one can hope that by truncat-
ing the ai’s we still get numbers that low degree polynomials do not
vanish on. This is of course not true as for any given (rational) n-
tuple we can ﬁnd a low degree polynomial f that vanishes on it. Chen
and Kao solve this by considering the truncation of all conjugates to
(a1,...,an) and then picking a conjugate at random. The point is that
no low degree polynomial can vanish on too many of those conjugates.
We now present the formal argument.
Denote ki = ⌈log(ri + 1)⌉and k = maxi ki. Let {pi,j}i∈[n],j∈[k] be the
ﬁrst nk primes. For every i ∈[n] and j ∈[ki], let bi,j ∈{0,1} be a bit,
and denote b = (bi,j)i,j. The basic observation behind the algorithm is
that if we set π(b)
i
= ki
j=1(−1)bi,j√pi,j, then Galois theory (see, e.g.,
Morandi [93]) tells us that
f(x1,...,xn) ≡0
⇔
f(π(b)
1 ,...,π(b)
n ) = 0,
(4.1)

300
Polynomial Identity Testing
as the ﬁeld Q(π(b)
1 ,...,π(b)
j+1) has degree 2kj+1 over Q(π(b)
1 ,...,π(b)
j ), and
so by induction if we know that f(π(b)
1 ,...,π(b)
j ,xj+1,...,xn) ̸≡0, then
also f(π(b)
1 ,...,π(b)
j+1,xj+2,...,xn) ̸≡0. Note that π(b1)
i
and π(b2)
i
are con-
jugates, for any b1 and b2.
Since the π(b)
i ’s are irrational numbers, Chen and Kao proposed the
following randomized algorithm. Choose b uniformly at random (i.e.,
choose random conjugates) and let qi,j be the rounding of √pi,j at
the ℓ-th position after the radix point in binary representation (ℓis
the parameter that determines the error). Set a(b)
i
= ki
j=1(−1)bi,jqi,j.
Accept if and only if f(a(b)
1 ,...,a(b)
n ) ̸= 0. Note that the algorithm uses

i ki = 
i⌈log(ri + 1)⌉random bits.
We now give the idea behind the analysis. We will bound from below
the value of f(a(b)
1 ,...,a(b)
n ) as a function of r,ℓ, and n. Galois theory
also tells us that the product, over all choices of b, of f(π(b)
1 ,...,π(b)
n ) is
a nonzero integer. In other words, as long as f is nonzero,


b
f(b)
 ≥1,
(4.2)
where f(b) = f(π(b)
1 ,...,π(b)
n ). The idea now is to show that each |f(b)| is
not too large, which implies that most of the multiplicands cannot be
too small in absolute value. Consequently, as a(b)
i
is not “too far” from
π(b)
i , the same conclusion holds for f(a(b)
1 ,...,a(b)
n ), implying that it is
nonzero with high probability. We now give a more formal explanation.
By the prime number theorem, √pi,j ≤O(

nklog(nk)). Hence,
regardless of b, |π(b)
i | ≤O(k

nklog(nk)). As f has degree at most r in
each variable, it has at most nr+1 monomials. Since the bit length of
each coeﬃcient is at most L, we get that for every b,
|f(b)| = |f(π(b)
1 ,...,π(b)
n )| ≤nr+1 · (O(k

nk log(nk)))r · 2L def
= 2m.
Equation (4.2) thus implies that, for every t > 0, the fraction of b’s
for which |f(b)| ≤2−t is at most m/(m + t). In other words, the
fraction of b’s for which |f(b)| > 2−t is at least t/(m + t). We now
show that for every such b, and an appropriate choice of ℓ, it holds
that f(a(b)
1 ,...,a(b)
n ) ̸= 0. Indeed, as |qi,j −pi,j| ≤2−ℓ, we have that for

4.2 Randomized Algorithms
301
each b, |a(b)
i
−π(b)
i | ≤k2−ℓ. Using the estimate |xe −ye| ≤|x −y| · e ·
max(x,y)e−1, we get that
f(a(b)
1 ,...,a(b)
n ) −f(π(b)
1 ,...,π(b)
n )
 ≤2m · 2−ℓ+logr ≤(∗) 2−t−1,
where inequality (∗) holds when ℓ≥m + logr + t + 1. For such an ℓ,
we thus have
f(a(b)
1 ,...,a(b)
n )
 >
f(π(b)
1 ,...,π(b)
n )
 −2−t−1 ≥2−t−1 > 0.
To better understand the parameters, for ℓ= m/ε + logr + 1 and
t = ℓ−m −logr −1, we have that the error probability is at most
m/(m + t) = ϵ. The number of random bits used is n
i=1⌈log(ri + 1)⌉
and the running time it takes to “prepare” the inputs {a(b)
i } is polyno-
mial in n and in ℓ≤O((L + rlog(nlogr))/ε).
The algorithm seems to heavily rely on Q being the underlying ﬁeld.
Lewin and Vadhan [89] showed how to extend it to the case of ﬁnite
ﬁelds as well (in fact their algorithm works over any ﬁeld). The main
idea is to ﬁnd the “correct” ﬁnite ﬁeld analogs of the notions used by
Chen and Kao [34]: irreducible polynomials instead of prime numbers,
inﬁnite power series instead of square roots, and truncation modulo
xℓinstead of truncating after the ℓ-th bit. Since the algorithm and its
analysis are not very diﬀerent than that of Chen and Kao, we refer the
reader to Ref. [89] for a proof.
Theorem 4.5 ([89]). Let f ∈F[x1,...,xn] be of degree at most ri
in xi over some ﬁnite ﬁeld F. Then there is a randomized algorithm
that, for every ϵ > 0, decides correctly, with probability at least 1 −ϵ,
whether f ≡0, using n
i=1⌈log(ri + 1)⌉random bits and running time
T = poly(rn/ε). This algorithm can work in the black-box model, given
access to a polynomial size extension ﬁeld of F (unless F is already large
enough).
4.2.3
The Agrawal–Biswass Algorithm
Assume that Φ is an arithmetic circuit computing a polynomial f of
individual degrees smaller than r. Consider the substitution xi = yri.

302
Polynomial Identity Testing
It is not hard to see that f(x1,...,xn) ≡0 iﬀfy
def
= f(yr,...,yrn) ≡0.
The idea of Agrawal and Biswass [3] is to use the Chinese Remainder
Theorem in order to reduce the degree of fy. Speciﬁcally, they ﬁnd a
relatively small set of low degree co-prime polynomials, {gi(y)}i∈I, and
check whether fy ≡0 mod gi, for each i ∈I. The Chinese Remainder
Theorem implies that when 
i deg(gi) > deg(fy) it holds that f ≡0 ⇔
∀i ∈I fy ≡0 mod gi. As the gi’s are of low degree, it is relatively
easy to verify whether fy ≡0 mod gi when Φ is given to us. Agrawal
and Biswass in fact do something more sophisticated. They give a set
of polynomials that are not co-prime, but rather the least common
multiple of any large enough subset of them (say, of size at least ϵ|I|)
has high degree. Their randomized algorithm simply picks one gi at
random, and checks whether fy ≡0 mod gi. This gives an algorithm
whose error is at most ϵ, as the following theorem shows.
Theorem 4.6([3]). Let Φ be an arithmetic circuit of size s computing
a polynomial f of degree at most ri in xi, for every i. For every ϵ > 0,
there is a randomized algorithm that, with probability at least 1 −ϵ,
decides correctly whether f ≡0. The algorithm uses ⌈n
i=1 logri⌉ran-
dom bits and works in time poly(n,s,1/ϵ,log(q)), when the ﬁeld is of
size q. For ﬁelds of characteristic zero, a similar result holds, with a
dependence on the bit length of the coeﬃcients instead of the ﬁeld size.
4.3
PIT and Lower Bounds: Hardness-Randomness
Tradeoﬀs
The hardness-randomness paradigm in computer science (roughly) says
that hard functions exist iﬀone can derandomize any (polynomial time)
randomized algorithm. The intuition being that if a function is hard to
compute then its output looks random (to an eﬃcient algorithm) and
vice versa. A prominent example is Impagliazzo and Wigderson’s proof
that if E requires exponential size Boolean circuits then BPP = P [66].
In other words, they showed that if there are hard (explicit) func-
tions then randomness is not essential for computation. As for the
other direction, Impagliazzo et al. [65] showed that derandomizing

4.3 PIT and Lower Bounds: Hardness-Randomness Tradeoﬀs
303
promise-BPP will imply that NEXP ̸⊂P/poly. In recent years, similar
results were obtained for arithmetic circuits. We shall now review these
results and sketch their proofs.
In [69] Kabanets and Impagliazzo showed that derandomizing PIT
implies arithmetic lower bounds for NEXP. More precisely, they proved
the following theorem.
Theorem 4.7 ([69]). The following three assumptions cannot be
simultaneously true.
(1) NEXP ⊆P/poly.
(2) Permanent is computable by polynomial size arithmetic
circuits over Z.
(3) There is a (non-deterministic5) sub-exponential time algo-
rithm for PIT.
Stated diﬀerently, derandomizing PIT implies either a Boolean
lower bound for NEXP or an arithmetic lower bound for permanent.
We shall later see a result in the reverse direction as well, namely, that
lower bounds for arithmetic circuits imply derandomization of PIT. It
is an interesting example where a lower bound gives an upper bound
(PIT algorithm) and vice versa. Before sketching the proof, we describe
one high-level part of it. Assuming that permanent has small circuits
and that we can solve PIT eﬃciently and deterministically, we can
obtain an eﬃcient non-deterministic uniform algorithm for permanent:
simply guess a small circuit for permanent and then verify its correct-
ness, using the self-reducibility of permanent and the PIT algorithm.
As permanent is hard for the polynomial hierarchy, we do not expect
it to have an eﬃcient non-deterministic algorithm.
Proof. [Sketch] Consider the Boolean language 0-1-Perm comprising
all pairs (M,v), where M is an n × n matrix with 0/1 entries, v is a
string of O(nlogn) bits representing an integer, and PERM(M) = v.
Assume for a contradiction that all three assumptions hold. We will
reach a contradiction by showing that these assumptions imply that
5 Recall that PIT is in co-RP, but is not known to be in NSUBEXP.

304
Polynomial Identity Testing
co-NEXP ⊂NSUBEXP = ∩ϵ>0NTIME(2nϵ), which is false (this can be
shown by a diagonalization argument).
Valiant’s theorem that 0-1-Perm is #P-complete [139] implies that
if 0-1-Perm is in NSUBEXP then P#P ⊂NSUBEXP. In Ref. [65] it was
shown that NEXP ⊆P/poly implies that NEXP = co-NEXP = MA. By
applying Toda’s theorem that PH ⊂P#P [136], it thus follows, under
our assumptions, that if 0-1-Perm is in NSUBEXP then co-NEXP ⊂
P#P ⊂NSUBEXP.
It therefore suﬃces to show that if there is a (non-deterministic)
sub-exponential time algorithm for PIT and permanent is computable
by polynomial size arithmetic circuits over Z, then 0-1-Perm is in
NSUBEXP. The idea is to use the self-reducibility of permanent. For
an n × n matrix X = (xi,j), denote by Xj the sub-matrix obtained by
deleting the ﬁrst row and j-th column from X. Thus, PERM(X) =
n
j=1 x1,j · PERM(Xj). Now, assuming that permanent has polyno-
mial size arithmetic circuits, we can guess such a circuit Φn using
non-determinism. Given such a guess Φn, construct, for every 1 ≤m ≤
n −1, a circuit Φm computing the permanent of an m × m matrix by
substituting zero-one values to the appropriate variables in Φn. Now,
for every 1 < m ≤n, write the identity
Φm(X(m)) =
m

j=1
x(m)
1,j · Φm−1(X(m)
j
),
where X(m) is an m × m matrix of variables. In addition, consider the
identity Φ1(x) = x. It is not diﬃcult to show that if all these n identities
hold, then Φn indeed computes permanent. Using the sub-exponential
time PIT algorithm, we can verify that all these identities hold, and
thus guarantee that Φn computes permanent. This shows that 0-1-Perm
is in NSUBEXP, which is what we wanted to prove.
This proof holds in the black-box model as well as in the white-box
one. For the case of black-box algorithms, Heintz and Schnorr, and
later Agrawal, proved a more straightforward result [2, 56]. The idea
behind the proof is that given a small hitting set, we can ﬁnd a nonzero
polynomial that vanishes on all the points of the set. This immediately
implies that this polynomial has high circuit complexity.

4.3 PIT and Lower Bounds: Hardness-Randomness Tradeoﬀs
305
Theorem 4.8 ([2, 56]). Let T : N →N be a monotone increasing
function. If there is a black-box deterministic PIT algorithm that runs
in time T(s) for arithmetic circuits of size s, then there is an n-variate
polynomial whose coeﬃcients are computed in PSPACE that requires
arithmetic circuits of size at least T −1(exp(n)), where exp(n) means cn
with c > 1 and T −1 is the inverse function of T.
For example, if T(s) = poly(s), we get an exponential lower bound
for the size of arithmetic circuits, and if T(s) = exp(poly(logs)), the
lower bound is of the form exp(nε).
Proof. [Sketch] Let H be the hitting set generated by the algorithm.
Clearly, |H| ≤T(s). Using simple interpolation, we can compute in
PSPACE the coeﬃcients of a nonzero polynomial f(y1,...,yn), with
n = O(logT(s)), satisfying f(a) = 0 for every a ∈H. It is clear that f
cannot be computed by a size s circuit, as any circuit of size at most s
that vanishes on all points of H is the zero polynomial. The circuit-size
of f is thus at least s = T −1(exp(n)), as required.
This proof highlights, in a more direct way, the relation between
PIT and proving circuit lower bounds. This result, however, does not
imply lower bounds for permanent as we are only guaranteed a lower
bound for a polynomial whose coeﬃcients are computed in PSPACE. It
is an interesting question whether from any black-box PIT algorithm
one can get a lower bound for a polynomial in VNP.
Open Problem 17. Assume that an arithmetic circuit class C has a
PIT algorithm that runs in time poly(s) for circuits of size s. Does the
permanent require super-polynomial circuits from C?
So far we have seen that PIT algorithms imply lower bounds. Next
we shall see the other direction, that lower bounds for arithmetic cir-
cuits imply eﬃcient deterministic PIT algorithms. The ﬁrst such result
was proved by Kabanets and Impagliazzo [69].

306
Polynomial Identity Testing
Theorem 4.9 ([69]). Let F be a large enough ﬁeld (of size at least
some polynomial in n) and s : N →N be a monotone increasing func-
tion. Assume that a multilinear m-variate polynomial f(x1,...,xm)
cannot be computed by size s(m) arithmetic circuits over F. Then,
there is a deterministic black-box PIT algorithm for arithmetic circuits
in n variables, of polynomial degree over F, that runs in time
exp((s−1(poly(t)))2) for size t = t(n) ≥n circuits.
To make sense of the parameters, consider the case when we have an
exp(n) lower bound. Then, we get a PIT algorithm for that runs in time
exp((logn)2) for poly(n)-size circuits. If the lower bound is exp(nϵ),
then we get an exp(poly(logn))-time PIT algorithm for poly(n)-size
circuits. In general, if the lower bound is super-polynomial in n, then
the PIT algorithm will be sub-exponential.
Proof. The proof is similar in nature to the proof of Nisan and Wigder-
son [99] and in particular uses an arithmetic analog of the so-called
NW-generator. The next lemma both deﬁnes and guarantees the exis-
tence of NW-designs. The proof of the lemma can be found in Ref. [99].
Lemma 4.10. Let n,m be integers such that n < 2m. There exists a
family of sets S1,...,Sn ⊂[k] with k = O(m2/log(n)) so that (1) for
each i ∈[n] it holds that |Si| = m, and (2) for every i ̸= j in [n] we
have that |Si ∩Sj| ≤log(n). This family of sets can be constructed
deterministically in time poly(n,2k). Such a family is called an NW-
design.
Given such an NW-design and the “hard” polynomial f, we
construct the hitting set as follows. Let m = m(t) be such that
s(m) > tO(1), the exact value of the O(1) in the exponent will be
determined later. Consider an NW-design S1,...,Sn with parame-
ters m and n (note that n < 2m, as f is multilinear and hence s
is at most exponential). Deﬁne the map G : Fk →Fn as G(y) =
(f(y|S1),...,f(y|Sn)), where y|Si is the restriction of y to the entries
in Si, namely, y|Si = (yj1,yj2,...,yjm) with Si = {j1 < j2 < ··· < jm}.

4.3 PIT and Lower Bounds: Hardness-Randomness Tradeoﬀs
307
We now show that for any size t circuit Φ, it holds that Φ ≡0 iﬀ
Φ(G) ≡0. One direction is trivial. The other direction is proved using
the “hybrid” method. Assume for a contradiction that Φ(G) ≡0 but
Φ ̸≡0. Then, there exists i ∈[n] such that
Φ(f(y|S1),...,f(y|Si),xi+1,...,xn) ̸≡0
but
Φ(f(y|S1),...,f(y|Si+1),xi+2,...,xn) ≡0.
As F is large enough, by ﬁxing the values of xi+2,...,xn to appropriate
ﬁeld elements ai+2,...,an, we get that there is a circuit Ψ in k + 1
variables such that
Ψ(y,z) def
= Φ(f(y|S1),...,f(y|Si),z,ai+2,...,an) ̸≡0
but
Ψ(y,f(y|Si+1)) ≡0.
Without loss of generality, assume that Si+1 = [m]. We can further ﬁx
values bm+1,...,bk to ym+1,...,yk such that
Ψ′(y1,...,ym,z) def
= Ψ(y1,...,ym,bm+1,...,bk,z) ̸≡0.
Here
is
the
point
where
we
use
the
design
property:
as
for
every j < i + 1 we have |Sj ∩Si+1| ≤logn, each of the polynomials
f(y|Sj)

ym+1=bm+1,...,yk=bk has a circuit of size poly(n) (it is a multilin-
ear polynomial with ≤logn variables). Therefore, Ψ′ has a circuit of
size t + poly(n). Since
Ψ′(y1,...,ym,z) ̸≡0
but
Ψ′(y1,...,ym,f(y1,...,ym)) ≡0,
we get that z −f(y1,...,ym) is a factor of Ψ′. The following factoring
result by Kaltofen [71] now tells us that f(y1,...,ym) has a circuit of
size poly(t(n) + poly(n)) = tO(1).
Theorem 4.11. There is a probabilistic polynomial-time algorithm
that gets as input an arithmetic circuit of size s computing a polynomial
g ∈F[x1,...,xn] of total degree at most r and outputs, with probability
at least 3/4, integers e1,e2,...,er′ > 0 and circuits Φ1,Φ2,...,Φr′, each
of size at most poly(s,r,log|F|), computing irreducible polynomials
h1,h2,...,hr‘ such that g = r′
i=1 hei
i . In case the characteristic q of F

308
Polynomial Identity Testing
divides any ei, i.e., ei = qkie′
i with e′
i not divisible by q, the algorithm
returns e′
i instead of ei, and the corresponding arithmetic circuit com-
putes hqki
i
instead of hi. For F = Q, the size of the produced arithmetic
circuits is at most poly(s,r,L), where L is the size of the maximal
coeﬃcient in g.
The polynomial f(y1,...,ym) thus has an arithmetic circuit of size
tO(1). The lower bound on f implies that tO(1) > s(m), but this contra-
dicts our choice of m. We conclude that Φ ≡0 iﬀΦ(G) ≡0. As f and
Φ have degrees poly(n), we get that Φ(G) has degree at most poly(n).
Thus, if we evaluate Φ(G) on all points in Ak, where A ⊆F is a set of
size poly(n), we get that Φ ≡0 iﬀΦ(G(Ak)) = {0}. This gives a hitting
set of size poly(n)k = poly(n)m2/logn = exp((s−1(tO(1)))2).
Using the ideas of the theorem above and of Theorems 2.11 and 4.8,
Agrawal and Vinay proved that a polynomial size hitting set for depth-
4 circuits gives rise to a quasi-polynomial size hitting set for general
arithmetic circuits [5].
Theorem 4.12 ([5]). If for every s there is an eﬃcient way to con-
struct a hitting set of size poly(s) for ΣΠΣΠ arithmetic circuits of
size s, then in time exp(log2(s)) one can construct a hitting set for
general arithmetic circuits of size s.
Proof. [Sketch] The idea is that if we have a polynomial size hitting
set for depth-4 circuits, then Theorem 4.8 guarantees a polynomial
that requires exponential size depth-4 circuits. Theorem 2.11 and the
discussion following it imply that this polynomial actually requires
exponential size general arithmetic circuits. By Theorem 4.9, this
implies the existence of a quasi-polynomial size hitting set for general
circuits.
The following question is thus as interesting as the general PIT
question.

4.3 PIT and Lower Bounds: Hardness-Randomness Tradeoﬀs
309
Open Problem 18. Derandomize the black-box PIT problem for
ΣΠΣΠ arithmetic circuits.
The result of Agrawal and Vinay requires an exp(n) lower bound
for depth-4 circuits in order to obtain exponential lower bounds for
general circuits and thus derandomize PIT. But, what can we con-
clude if we only have lower bounds of the form exp(nε) for depth-
4 circuits, or even just super-polynomial lower bounds? What if we
have lower bounds for a larger depth? A close inspection of the proof
of Theorem 4.9 shows that the use of Kaltofen’s factoring algorithm
requires the lower bound to hold for general arithmetic circuits and not
just for bounded depth circuits (this is the case since Kaltofen’s factor-
ing algorithm does not preserve depth). A partial answer was given in
Ref. [44] where it was shown that exponential lower bounds for depth-d
circuits imply quasi-polynomial time deterministic algorithms for PIT
of depth-(d −5) circuits, assuming that the polynomial computed by
the circuit has individual degrees at most poly(logn). The idea behind
this result is to replace Kaltofen’s factoring algorithm with the follow-
ing root ﬁnding result that basically says that if z −f(x1,...,xn) is
a factor of Ψ(x1,...,xn,z), where Ψ is a small depth-d circuit with a
small degree in z, then f can be computed by a small arithmetic circuit
of depth at most d + 3.
Theorem 4.13. Let n,s,r,m,t,d be integers such that s ≥n. Let F
be a ﬁeld which has at least max{mt + 1,r + 1} elements. Let
g(x1,...,xn,z) ∈F[x1,...,xn,z] be a nonzero polynomial with deg(g) ≤
t and degz(g) ≤r
(i.e., the degree of z
in g
is at most
r)
that
has
a
depth-d
arithmetic
circuit
of
size
s
over
F.
Let
f(x1,...,xn) ∈F[x1,...,xn] be a polynomial with deg(f) ≤m such that
g(x1,...,xn,f(x1,...,xn)) ≡0. Then f(x1,...,xn) can be computed by
a circuit of size poly(s,mr) and depth d + 3 over F.
We do not prove this theorem here but only give the idea behind
the proof. Newton’s method (also known as Hensel’s lifting lemma)
provides a general way for ﬁnding roots of a given function. In our case,

310
Polynomial Identity Testing
we apply this method to the polynomial g with the intention of ﬁnding
the root f of g. The proof follows by showing that, with some careful
manipulation, Newton’s method actually implies that the complexity
of f is not much larger than that of g.
Given the theorem above one can follow the proof of Theorem 4.9
to conclude the following analog for the case of bounded depth circuits.
Theorem 4.14([44]). Let d be an integer, ϵ > 0 a real number and F
a ﬁnite ﬁeld. Let f ∈F[x1,...,xn] be an explicit polynomial of degree
poly(n) such that any depth-d circuit computing it has size at least 2nϵ.
Then, the black-box PIT problem for polynomial-size depth-(d −5)
circuits, having individual degrees at most poly(logn), can be solved
deterministically in time exp(poly(logn)).
It is an intriguing question whether one can factor polynomials, or
even just compute factors that are linear in some variable, without
increasing the depth by much.
Open Problem 19. Let f(x1,...,xn,z) be computed by a depth-d
size s circuit. Is it true that any factor of f can be computed by a
depth-O(d) size poly(s) circuit? What about factors that are linear
in z?
The results that were described in this section give tight relations
between PIT and lower bounds. In Section 3.6 we saw that quasi-
polynomial lower bounds are known for multilinear formulas, and that
for constant-depth multilinear formulas exponential lower bounds are
known. Unfortunately, the results of Refs. [44, 69] require lower bounds
for general circuits and not just for multilinear circuits. We therefore
would like to understand whether one can use lower bounds for weak
models to obtain PIT algorithms (for the corresponding weak models).
As we shall see in Section 4.8, Shpilka and Volkovich [126, 127] use a
lower bound for sums of read-once formulas, to obtain a PIT algorithm
for that model. It is a very interesting question to understand whether
this can be achieved for other models as well.

4.4 Sparse Polynomials
311
Open Problem 20. Is there a way to transform lower bounds for mul-
tilinear circuits to PIT algorithms for some model of multilinear circuits
or formulas? What if we have a (slightly) super-polynomial lower bound
for arithmetic formulas, can we use it to get a sub-exponential time PIT
algorithm for arithmetic formulas?
As we shall see in Section 4.5, Raz and Shpilka [106] obtained a
polynomial time (white-box) PIT algorithm for noncommutative for-
mulas. Their proof does not transform the lower bound of Nisan [98],
discussed in Section 3.4, to a PIT algorithm but rather relies on the
intuition gained in the proof of the lower bound to design the algorithm.
Therefore, we may hope that even if there is no clear way of transform-
ing lower bounds for multilinear formulas into PIT algorithms, that we
can still use the intuition gained in the lower bounds proofs to design
such algorithms.
Open Problem 21. Give a deterministic sub-exponential time iden-
tity testing algorithm for multilinear formulas.
4.4
Sparse Polynomials
Sparse polynomials are polynomials having a small number of mono-
mials. We will be mostly interested in the case when the number of
monomials m is polynomial in the number of variables. In other words,
we will be interested in polynomials that have polynomial size ΣΠ cir-
cuits. This model is of course the simplest and “most explicit” way for
computing and representing a polynomial: the polynomial is given by
its list of coeﬃcients. As such, the polynomial identity testing problem
for this model attracted a lot of research and many algorithms were
devised for it, see Refs. [86, 20] and references therein.
In this section we give the proof of Klivans and Spielman [86] for
identity testing of sparse polynomials. The main idea is to reduce the
multivariate problem to a univariate problem. Namely, to generate from
each sparse polynomial f a univariate polynomial ˆf and then check
whether ˆf ≡0.

312
Polynomial Identity Testing
Assume that the unknown polynomial is f(x1,...,xn) and that its
(total) degree is smaller than r. Consider the substitution xi = yri. Any
two diﬀerent monomials of f are mapped to diﬀerent powers of y, and so
ˆf(y) def
= f(yr,...,yrn) ≡0 iﬀf ≡0. Therefore, all that we have to do now
is to check whether a univariate polynomial is identically zero or not.
This can be easily achieved by querying ˆf on deg( ˆf) many points. The
problem with this approach is that deg( ˆf) can be exponentially large.
Recall that this is the same problem that faced Agrawal and Biswass
in Section 4.2.3. Here, Klivans and Spielman handle this by picking
some prime p = O(max(r,mn + 1)) and making the substitution xi =
yri mod p. The problem now is that diﬀerent monomials in f can be
mapped to the same monomial in ˆf and can therefore cancel each
other. To overcome this diﬃculty, they consider all substitutions of the
form xi = yki mod p for all integers k ∈[mn + 1]. They show that for
at least one of these k’s, at least one monomial of f will be uniquely
mapped to a monomial of ˆf, and this will give that f ≡0 iﬀˆf ≡0,
with the additional property deg( ˆf) < rp. By evaluating ˆf in rp many
points we can check whether f ≡0. We now give the formal details of
the proof.
Theorem 4.15 ([86]). Let f be a nonzero n-variate polynomial of
individual degrees smaller than r, that has (at most) m monomials
over F. Let p be a prime larger than max(r,mn + 1). Then, there
is some k ∈[mn + 1] such that the univariate polynomial
ˆf(y) def
=
f(y,yk1 mod p,...,ykn−1 mod p) is not identically zero, and of degree at
most rp −1. Hence, if S ⊂F is of size at least rp then evaluating f on
all points of the form (y,yk1 mod p,...,ykn−1 mod p) for y ∈S is a hitting
set for f. In particular, when m and r are polynomial in n we get a
deterministic polynomial time PIT algorithm by testing this for every
k ∈[mn + 1].
Proof. Let f = m
j=1 cjxr(j), where r(j) = (r(j)1,...,r(j)n) and xr(j) =
xr(j)1
1
xr(j)2
2
···xr(j)n
n
. Consider the substitution xi = yki mod p. The
monomial xr(j) is mapped to y
n
i=1 r(j)i(ki−1 mod p). We will show that
for some choice of k ∈[mn + 1], there exists j ∈[m] such that the

4.4 Sparse Polynomials
313
monomial xr(j) is mapped uniquely to y
n
i=1 r(j)i(ki−1 mod p), which
implies that ˆf(y) ̸≡0. To see this, observe that if xr(j) and xr(j′)
are mapped to the same y monomial, then n
i=1 r(j)i(ki−1 mod p) =
n
i=1 r(j′)i(ki−1 mod p). The last equality implies that for any such
pair j ̸= j′, it holds that n
i=1 r(j)iki−1
mod p
=
n
i=1 r(j′)iki−1. As
r < p, it holds that r(j) ̸= r(j′) modulo p. Thus, a bad “event” can
happen only if k is a root of the nonzero polynomial gj,j′(z) def
=
n
i=1(r(j)i −r(j′)i)zi−1 over Fp. Fix some j0 such that cj0 ̸= 0. There
are m diﬀerent polynomials gj0,j(z). As deg(gj0,j(z)) ≤n −1, it follows
that there are at most mn diﬀerent values of k that can be roots of any
of these polynomials. Since mn + 1 < p, there exists some k ∈[mn + 1]
that is not a root of any of the gj0,j(z)’s. For this k, we get that xr(j0)
was mapped uniquely to some y monomial.
The bit length of the points on which we evaluate f can be as large
as mlogm, however, using the Chinese Remainder Theorem this can
be reduced to O(lognr). We omit the details of this reduction.
As mentioned above, there were other deterministic PIT algorithms
for sparse polynomials prior to Ref. [86]. The main diﬀerence between
the various algorithms is the bit length of the points on which one
evaluates the polynomial and the exact size of the hitting set con-
structed. We refer the interested reader to Ref. [86] where these issues
are discussed.
In [20] a slight twist of the above result was considered in which we
do not have a polynomial bound on the degree of f but instead we are
assured that f (which is sparse) is computed by a size s circuit (that
does not necessarily have small depth). This implies that deg(f) ≤2s.
Note that the size of the hitting set constructed in Theorem 4.15
is polynomial in the degree, which is not eﬃcient when s = poly(n).
Bl¨aser et al. gave a deterministic PIT algorithm, with running time
poly(m,n,s), for such circuits [20]. The algorithm is black-box over Z
and white-box over ﬁnite ﬁelds. Their black-box algorithm basically
evaluates the polynomial on points of the form (kr0,...,krn−1) mod p
for several diﬀerent primes p and all points k ∈Fp.6
6 In fact, the size of the hitting set over Z is polynomial in L, which is an upper bound on
the bit length of the coeﬃcients of the underlying polynomial.

314
Polynomial Identity Testing
4.5
Noncommutative Formulas
In Ref. [106] a deterministic, polynomial time, white-box algorithm
for PIT of noncommutative formulas was given. Besides the model of
sparse polynomials, this is the only “complete” model of computation
for which polynomial time deterministic PIT algorithms are known.
This algorithm also plays a role in Saxena’s [116] PIT algorithm for
the so-called diagonal circuits, see Section 4.7.1.
The algorithm of Raz and Shpilka [106] relies on the fact, shown in
Section 3.4, that a noncommutative formula can be simulated eﬃciently
by an ABP. The algorithm, in fact, works also for the more general
model of noncommutative ABP’s. We ﬁrst give a sketch of the argu-
ment. Given an ABP A, computing a polynomial f(x1,...,xn), we can
construct, for every monomial of the form xixj (i may be equal to j),
a new ABP that computes the part of f containing all monomials that
“start” with xixj. Since we are working in the noncommutative poly-
nomial ring, A computes the zero polynomial iﬀall the new ABPs com-
pute the zero polynomial as well. We now explain how to merge the new
ABPs to a single ABP ˆ
A, which has one less level, such that ˆ
A is identi-
cally zero iﬀA is identically zero. ˆ
A is constructed by removing from A
the vertices of level 1 and the edges adjacent to them, and then connect-
ing the source directly to level 2. The technical part is the labeling of the
new edges, going out of the source, with the “correct” linear functions
(recall that every edge is labeled with a linear form). The naive way of
replacing each term of the form xixj with a new variable yi,j does not
work, as in this way the number of variables introduced after k steps
can be as large as n2k. The main point in the argument is that the num-
ber of new variables is never larger than O(s + n), and this is achieved
by the “correct” labeling. We now give a more formal explanation.
Let A be an ABP with levels 0,...,d. Denote by m1 and m2 the
number of vertices in levels 1 and 2 of A, respectively. Denote by vin
the source of A and by vout the sink of A. Let v1,...,vm2 be the vertices
in level 2 of A. Let A(v,vout) be the polynomial that is computed by
the ABP with the vertex v as source and vout as sink. By deﬁnition,
A(vin,vout) =

i∈[m2]
A(vin,vi) · A(vi,vout).

4.5 Noncommutative Formulas
315
For every i ∈[m2], we can express A(vin,vi) as
A(vin,vi) =

1≤k≤n
αi,kxk2 +

1≤j<k≤n
βi,j,kxjxk +

1≤j<k≤n
γi,j,kxkxj.
We thus have
A(vin,vout) =

1≤k≤n
xk2 
i∈[m2]
αi,k · A(vi,vout)
+

1≤j<k≤n
xjxk

i∈[m2]
βi,j,k · A(vi,vout)
+

1≤j<k≤n
xkxj

i∈[m2]
γi,j,k · A(vi,vout).
The next claim now follows, as we are working in the noncommutative
polynomial ring.
Claim 4.16. A computes the zero polynomial iﬀfor every 1 ≤k ≤n,

i∈[m2]
αi,k · A(vi,vout) ≡0,
and for every 1 ≤j < k ≤n,

i∈[m2]
βi,j,k · A(vi,vout) ≡

i∈[m2]
γi,j,k · A(vi,vout) ≡0.
For simplicity of notation, deﬁne the following vectors
αk = (α1,k,α2,k,...,αm2,k),
1 ≤k ≤n,
βj,k = (β1,j,k,β2,j,k,...,βm2,j,k),
1 ≤j < k ≤n,
γj,k = (γ1,j,k,γ2,j,k,...,γm2,j,k),
1 ≤j < k ≤n,
and
a = (A(v1,vout),A(v2,vout),...,A(vm2,vout)).
The claim tells us that A(vin,vout) ≡0 iﬀfor every 1 ≤j < k ≤n,
⟨αk,a⟩≡⟨βj,k,a⟩≡⟨γj,k,a⟩≡0,
where
⟨·,·⟩
is
the
inner
product
form.
Consider
V =
span{αk,βj,k,γj,k}j,k. Stated diﬀerently, A(vin,vout) ≡0 iﬀa ⊥V . Let

316
Polynomial Identity Testing
d = dim(V ) ≤m2 and u1,...,ud be a basis for V . Note that as we work
in the white-box model, we can easily compute αk,βj,k,γj,k and hence
we can eﬃciently compute a basis of V . Denote uj = (uj,1,...,uj,m2).
For i ∈[m2], deﬁne the linear form
ℓi(y1,...,yd) =

j∈[d]
uj,iyj.
We thus have
ˆ
A def
=

i∈[m2]
ℓi · A(vi,vout) =

j∈[d]
yj

i∈[m2]
uj,i · A(vi,vout) =

j∈[d]
yj⟨uj,a⟩,
which implies
ˆ
A ≡0
⇔
∀j ∈[d]
⟨uj,a⟩≡0
⇔
A ≡0.
Notice that
ˆ
A can be computed by an ABP with d −1 levels in
the following manner. Remove all the vertices in level 1 from A.
For every i ∈[m2], connect vi to the source with an edge labeled
by ℓi. We now give a bound on the number of operations needed
to construct
ˆ
A, given A. With O(m1 · m2 · n2) operations we can
compute the vectors {αk,βj,k,γj,k}j,k. We can compute the ℓi’s with
O((n2 + m2) · n2 · m2) operations, using Gaussian elimination. Thus,
using O(m2 · n4 + m2
2 · n2 + m1 · m2 · n2) operations we can transform
A to ˆ
A.
Theorem 4.17([106]). Let A be an ABP of size s, then we can deter-
ministically verify whether A ≡0 in time O(s5 + s · n4).
Proof. The size of A is s = m1 + ··· + md, where mi is the size of level
i. By the above procedure, we can reduce A to an ABP with two levels
in d steps. After the i-th step, we have an ABP in at most n + mi+1
variables (where md = 1). The reduction, therefore, runs in time
O
d−1

i=1
mi+1 · (mi + n)4 + (mi+1)2 · (mi + n)2
+ mi · mi+1 · (mi + n)2

= O(s5 + s · n4).

4.5 Noncommutative Formulas
317
As we can eﬃciently verify whether an ABP with two levels is identi-
cally zero or not, the result follows.
This algorithm runs in polynomial time but it works only in the
white-box setting. It is thus an interesting question to ﬁnd a black-box
algorithm for the problem. Any black-box algorithm will need to make
assignments from a noncommutative domain, as otherwise it will make
a mistake, e.g., on the polynomial xy −yx which is nonzero when x
and y do not commute. In the next section we present randomized
noncommutative PIT algorithms.
4.5.1
Randomized Noncommutative PIT Algorithms
The ﬁrst such algorithm was given by Bogdanov and Wee [23], that
presented a noncommutative analog of the Schwartz–Zippel algorithm.
A nontrivial extension is required as, e.g., the polynomial xy −yx is
nonzero in the noncommutative world but vanishes whenever the inputs
come from a commutative domain.
Theorem 4.18 ([23]). There exists a black-box, randomized identity
testing algorithm for the class of noncommutative degree d n-variate
polynomials that uses O(dlognlog(dlogn/ϵ)) random bits and succeeds
with probability 1 −ϵ.
The algorithm is based on a classical theorem of Amitsur and
Levitzki saying that over any ﬁeld F the matrix algebra Fk×k does
not satisfy any polynomial identity of degree less than 2k [7]. In other
words, for every nonzero polynomial g(x1,...,xn) of degree smaller than
2k, there exist k × k matrices A1,...,An such that g(A1,...,An) is
nonzero. For example, for xy −yx, we have
0
1
1
0
0
−1
1
0

−
0
−1
1
0
0
1
1
0

=
1
0
0
−1

−
−1
0
0
1

=
2
0
0
2

.
The high-level idea behind the Bogdanov–Wee algorithm is the
following. Let f(x1,...,xn) be a noncommutative polynomial of

318
Polynomial Identity Testing
degree r. Evaluate f on random r × r matrices over F (we need to
assume that F is large enough, otherwise we pick matrices over an
extension ﬁeld). After this substitution, f computes an r × r matrix
in which each entry is a degree r polynomial in the entries of the n
matrices. If f is not a polynomial identity for Fr×r, then some entry
of f computes a nonzero polynomial of degree at most r in the entries
of the matrices that we picked. In particular, if we pick the matrices
at random then by the usual commutative version of the Schwartz–
Zippel lemma f will compute a nonzero matrix with high probability.
Furthermore, we may obtain a lower bound for this probability via
the commutative Schwartz–Zippel lemma. While this is the main idea
behind the algorithm, Bogdanov and Wee use another trick in order to
optimize parameters. Roughly, they pick the random matrices from a
linear subspace where some entries of the matrices are ﬁxed to zero, and
the remaining entries are chosen randomly from some subset T ⊆F.
In Ref. [13] a randomized algorithm based on the so-called isolation
lemma and that uses ideas from automata theory was given. To explain
their approach, we ﬁrst recall some standard automata theory (see,
e.g., Ref. [59]). Fix a deterministic ﬁnite automaton A = (Q,Σ,δ,q0,qf)
that takes inputs from Σ∗, where Q is the set of states, Σ is the
alphabet, δ : Q × Σ →Q is the transition function, and q0 and qf are
the initial and ﬁnal states, respectively. For each letter w ∈Σ, let
δw : Q →Q be deﬁned by δw(q) = δ(q,w). Deﬁne the transition matrix
Mw to be the |Q| × |Q| matrix given by Mw(q,q′) = 1 if δw(q) = q′ and
Mw(q,q′) = 0 otherwise. For a word ¯w = w1w2 ...wk ∈Σ∗, we deﬁne
M ¯w = Mw1 ···Mwk, and if ¯w is the empty string then M ¯w is the identity
matrix. Let δ ¯w denote the natural extension of the transition function to
¯w (if ¯w is the empty string then δ ¯w is simply the identity function). As
before, we have M ¯w(q,q′) = 1 iﬀδ ¯w(q) = q′. Speciﬁcally, M ¯w(q0,qf) = 1
iﬀ¯w is accepted by the automaton A.
The authors of Ref. [13] use automata to get a randomized PIT
algorithm in the following way. Let Φ be the noncommutative circuit
that we wish to test. Let A = (Q,Σ,δ,q0,qf) be a ﬁnite automaton over
the alphabet Σ = {x1,...,xn} and deﬁne the matrices Mxi ∈F|Q|×|Q| as
before. Consider the matrix Mout obtained by evaluating Φ on the input
(Mx1,...,Mxn). The crucial observation is that Mout is determined

4.5 Noncommutative Formulas
319
completely by the polynomial f computed by Φ, and the structure
of Φ itself is otherwise irrelevant. In particular, Mout is always zero
when f ≡0. Consider what happens when Φ computes a single mono-
mial, i.e., f(x1,...,xn) = c · xj1 ···xjr, where c is a ﬁeld element. In this
case, the output matrix Mout is the matrix c · Mxj1 ···Mxjr = c · M ¯w,
for ¯w = xj1 ···xjr. Speciﬁcally, Mout(q0,qf) is zero when A rejects ¯w,
and equals c when A accepts ¯w. In general, suppose that Φ computes
the polynomial f = 
i∈[t] ci · mi, where mi = ri
k=1 xik. Let ¯wi denote
the string representing the monomial mi. Finally, let S(f,A) = {i ∈[t] :
A accepts ¯wi}. It is not hard to see that Mout(q0,qf) = 
i∈S(f,A) ci.
Thus, when evaluating Φ on the input (Mx1,...,Mxn) we obtain a
matrix that reﬂects how the automaton acts on f, when viewed as
the set of strings corresponding to its monomials. In particular, if A
is an automaton that accepts exactly one string that corresponds to
a monomial of f, then Mout(q0,qf) is the coeﬃcient of that monomial
in f. In order to use this observation we recall the isolation lemma of
Mulmuley et al. [97].
Lemma 4.19. Let U be a universe of size u and F be any family of
subsets of U. Let W : U →[2u] denote a weight assignment function
to elements of U. Then, when W is picked uniformly at random, the
probability that there exists a unique minimum-weight set in F is at
least 1/2 (the weight of a set is the sum of the weights of its elements).
To obtain a “noncommutative Schwartz–Zippel algorithm” we do
the following. Let f be a noncommutative polynomial of degree r. Con-
sider the universe Ut = [t] × [n] with t ∈[r]. We identify each monomial
m = xi1 ···xit with the set Sm = {(1,i1),(2,i2),...,(t,it)} ⊆Ut. Con-
sider the family F of all sets Sm such that the monomial m has a
nonzero coeﬃcient in f. The isolation lemma tells us that if we assign
random weights from [2tn] to the elements of Ut then with probability
at least 1/2 there is a unique minimum-weight set in F. In Ref. [13]
Arvind and Mukhopadhyay construct a family of small automata
{Aw,t : w ∈[2nr],t ∈[r]} such that the automaton Aw,t accepts pre-
cisely all the strings m of length t such that the weight of Sm is w.
Thus, the automaton corresponding to the minimum weight accepts

320
Polynomial Identity Testing
only one string (monomial). Now, for each automaton Aw,t, plug the n
matrices that it deﬁnes to f as inputs in the way described above. If
the output matrix of some automaton is nonzero, then the algorithm
declares f ̸≡0, otherwise it declares f ≡0. By the isolation lemma,
with probability at least 1/2 the algorithm is correct. Repeating this
procedure several times, with “fresh” weight function at each run, we
can reduce the error as we like.
We end this section by stating two open questions.
Open Problem 22. Derandomize black-box PIT for noncommutative
arithmetic formulas.
Another interesting problem is to derandomize PIT for non-
commutative circuits, even in the white-box setting. Currently no
sub-exponential time deterministic algorithm is known. We note that
similar to the general PIT question, derandomizing PIT for noncom-
mutative circuits is equivalent to proving lower bounds in the sense
of [69]. Speciﬁcally, Arvind et al. [14] proved the following.
Theorem 4.20. ([14]) If PIT for noncommutative circuits can be
done in deterministic sub-exponential time, then either NEXP ̸⊆P/poly
or the noncommutative permanent does not have polynomial-size non-
commutative circuits.
The proof is essentially the same as the proof of Theorem 4.7 and
is left to the readers. Thus, derandomizing PIT of noncommutative
circuits can be viewed as a ﬁrst step toward proving super-polynomial
lower bound for noncommutative circuits (recall Open Problem 10).
Open Problem 23. Derandomize PIT for noncommutative arith-
metic circuits.
Finally,
we
note
that
while
for
general
circuits
Kabanets
and Impagliazzo showed that lower bounds imply PIT algorithm
(Theorem 4.9), no such result is known for noncommutative circuits.

4.6 Depth-3 Circuits
321
Open Problem 24. Obtain an analog of Theorem 4.9 for noncom-
mutative circuits. I.e., show that an exponential lower bound for non-
commutative circuits computing a multilinear polynomial, implies a
sub-exponential time deterministic black-box PIT algorithm for non-
commutative circuits.
4.6
Depth-3 Circuits
Theorem 4.12 shows that black-box derandomization of ΣΠΣΠ circuits
is almost equivalent to black-box derandomization of PIT for general
circuits. Currently only the case of depth-2 is solved, as we saw in
Section 4.4. The next step is, therefore, depth-3 circuits. In the last
few years this model attracted a lot of research but still the question
of derandomizing PIT for depth-3 circuits remains open. We note that
for depth-3 circuits the interesting case is ΣΠΣ circuits and not ΠΣΠ
circuits, as the latter follows by the depth-2 case.
Open Problem 25. Give a deterministic sub-exponential time PIT
algorithm for ΣΠΣ circuits.
As the general question is still open, research has focused on
restricted depth-3 circuits, in hope of gaining a better understanding of
the general case. Klivans and Spielman [86] raised the problem of giving
a deterministic white-box PIT algorithm for ΣΠΣ circuits, even when
the top fan-in is three. Namely, when there are only three multiplica-
tion gates. This question was ﬁrst solved in Ref. [43] who gave a white-
box quasi-polynomial time deterministic PIT algorithm for ΣΠΣ(k)
circuits (depth-3 circuits with top fan-in k). This result was signiﬁ-
cantly improved by Kayal and Saxena [81] who gave an nO(k) time
white-box PIT algorithm for ΣΠΣ(k) circuits. Consequently, Arvind
and Mukhopadhyay [12] gave a somewhat simpler algorithm of the
same running time for the problem. In Ref. [76] it was shown how to
generalize the approach of [43] to the black-box setting, thus giving
the ﬁrst quasi-polynomial time black-box algorithm for the problem.
This result was later improved by Saxena and Seshadhri [117] who

322
Polynomial Identity Testing
gave a better quasi-polynomial-time-algorithm over ﬁnite ﬁelds, and by
Kayal and Saraf [80] who gave a polynomial-time algorithm, of running
time nkO(k), over Q and R. Very recently, Saxena and Seshadhri [118]
obtained improved algorithms for both ﬁnite ﬁelds, where the algo-
rithm runs in quasi-polynomial time, and for Q and R, where it runs in
time nO(k2). The algorithms of [80, 117, 118] are the same as the orig-
inal algorithm of Karnin and Shpilka [76], the diﬀerence is that they
improve the parameters in the main lemma of Dvir and Shpilka [43],
on which the analysis of Karnin and Shpilka [76] relies.
4.6.1
White-Box Algorithms
The following theorem of Kayal and Saxena [81] gives the best white-
box algorithm for ΣΠΣ(k) circuits.
Theorem 4.21([81]). There is a deterministic poly(n,rk)-time white-
box PIT algorithm for ΣΠΣ(k) circuits of degree r.
We shall present the ideas behind the algorithm of Kayal and Saxena
without giving the full details. We start by a simpliﬁed version of
the algorithm. Let Φ be a given ΣΠΣ(k) circuit of degree r. Assume
that we can ﬁnd (r + 1) co-prime linear functions ℓ1,...,ℓr+1 that
appear in Φ. In this case Φ ≡0 mod 
i∈[r+1] ℓi iﬀΦ ≡0, as deg(Φ) <
deg(
i∈[r+1] ℓi). In other words, Φ ≡0 iﬀfor every ℓi it holds that
Φ|ℓi=0 ≡0. (imagine that ℓi = xi and in this case Φ|ℓi=0 is the circuit
obtained by substituting xi = 0. The general case is handled similarly.)
Since each ℓi appears in Φ, the circuit Φ|ℓi=0 is a ΣΠΣ(k −1) circuit.
This gives rise to an inductive argument: ﬁnd “enough” co-prime lin-
ear functions that appear in the circuit and check all the restriction
of the circuit to the linear functions. The base case for the induction
is k = 2 which is trivial to check, as a ΣΠΣ(2) circuit is zero iﬀits
two multiplication gates have the same factorization (with a diﬀerent
sign). Thus, if we could ﬁnd such a set of linear functions, we would
be done.
In the general case, however, we may not be able to ﬁnd such a
set. For example, this will occur if all linear functions appear with high

4.6 Depth-3 Circuits
323
multiplicity. Kayal and Saxena circumvent this diﬃculty by working
modulo powers of linear functions and performing computations in the
ring F[x1,...,xn]/⟨xe1
1 ,...,xer
r ⟩. To better understand their approach,
let us ﬁrst consider the case k = 3. It is not diﬃcult to see (e.g., using a
greedy argument) that if the multiplication gates are not identical, and
no linear function appears in all three gates, then we can ﬁnd co-prime
powers of linear functions ℓe1
1 ,...,ℓem
m such that each ℓei
i divides exactly
one of the multiplication gates and e1 + ··· + em > r. The algorithm
now checks whether Φ ≡0 mod ℓei
i
for every i. Let us consider what
happens when we set ℓ1 = 0. For simplicity, assume without loss of
generality that ℓ1 = x1. The circuit Φ mod xe1
1 is a ΣΠΣ(2) circuit in
the ring F[x1,...,xn]/⟨xe1
1 ⟩. We now repeat the same procedure for this
circuit, that is, ﬁnd a set of co-prime powers of linear functions (that
are also co-prime to x1), each dividing one of the two remaining gates,
such that their product is of high degree. By doing so we reduced PIT
for ΣΠΣ(3) circuits to O(r2) instances of the problem of verifying that
a given multiplication gate is zero in the ring F[x1,...,xn]/⟨xe1
1 ,xe2
2 ⟩.
This can be done by observing that a product of linear functions is
zero modulo ⟨xe1
1 ,xe2
2 ⟩iﬀthe product of all the factors that only involve
x1,x2 is zero in this ring, which can be veriﬁed in time poly(n,e1 · e2).
To extend the proof to higher values of k, one has to make sure
that the Chinese Remainder Theorem also holds for rings of the
form F[x1,...,xn]/⟨xe1
1 ,...,xer
r ⟩, which is what Kayal and Saxena [81]
did. This enables an inductive argument since the same argument
as above can be used to reduce PIT of ΣΠΣ(k) circuits to at most
r + 1 instances of PIT of ΣΠΣ(k −1) circuits over rings “similar” to
F[x1,...,xn]/⟨xe1
1 ⟩. This description also shows that the running time
is poly(n,rk).
4.6.2
Black-Box Algorithms
We now explain the main ideas behind [43, 76, 80, 117, 118]. All these
works are based on the following structural theorem from [43]. The
improvements of [80, 117, 118] are based on improving the parameters
in the theorem. Before stating the theorem we deﬁne the notions of
simple and minimal circuits.

324
Polynomial Identity Testing
Deﬁnition 4.1. A ΣΠΣ(k) circuit Φ = 
i∈[k] Ψi, where each Ψi is a
ΠΣ circuit, is minimal if there is no nonempty set I ⊊[k] such that

i∈I Ψi ≡0. The circuit is simple if no linear function is a factor of all
the multiplication gates, that is, if gcd(Ψ1,...,Ψk) = 1.
Clearly, one can create an identically zero ΣΠΣ(k) circuit by adding
together several smaller depth-3 circuits that are identically zero, or
by multiplying a zero ΣΠΣ(k) circuit by some linear function. The
following theorem of Dvir and Shpilka [43] shows that if an identically
zero circuit was not constructed in this way, i.e., the circuit is simple
and minimal, then it depends on a few linear functions. To state the
theorem we need the following deﬁnitions.
Deﬁnition 4.2. Let Φ = 
i∈[k] Ψi be a ΣΠΣ(k) circuit. Denote Ψi =

j∈[ri] ℓi,j, where each ℓi,j is a linear function. The degree of Φ is
deg(Φ) = maxi∈[k] ri. The rank of Φ is rank(Φ) = dim(span{ℓi,j : i ∈
[k],j ∈[ri]}).
The theorem below is the structural theorem of Dvir and
Shpilka [43] with the improved bounds of Saxena and Seshadhri [118].
Theorem 4.22 ([43, 80, 117, 118]). For every ﬁeld F, there exists
a function R(k,r) = RF(k,r) such that if Φ is a simple and minimal
ΣΠΣ(k) circuit of degree r over F that computes the zero polynomial,
then rank(Φ) ≤R(k,r). If F is a ﬁnite ﬁeld then R(k,r) = O(k2 logr).
If F = R or Q then R(k,r) = k2.
The interesting point about the theorem is that the upper bound on
the rank does not depend on n. Roughly, when a ΣΠΣ circuit is simple,
minimal, and computes the zero polynomial, it cannot involve too many
linearly independent linear functions. This property will play a major
role in all the black-box PIT algorithms as well as in the reconstruction
algorithms given in Section 5.4. We now sketch the proof of the theorem.
The actual proof is quite long and is diﬀerent for ﬁnite ﬁelds and for
R,Q. We only explain the case k = 3 over ﬁnite ﬁelds and over the reals.

4.6 Depth-3 Circuits
325
Proof. [Sketch for k = 3] Let Φ = Ψ1 + Ψ2 + Ψ3 be a circuit com-
puting
the
zero
polynomial
with
multiplication
gates
Ψ1,Ψ2,
and Ψ3. Assume without loss of generality that deg(Ψ1) = deg(Ψ2) =
deg(Ψ3) = r and that all the linear forms in Φ are homogeneous.
Indeed, we can replace Ψi = 
j∈[ri](ai,j,0 + 
k∈[n] ai,j,kxk) by Ψ′
i =
yr′
i 
j∈[ri](ai,j,0y + 
k∈[n] ai,j,kxk), where y is a new variables and r′
i
is such that deg(Ψ′
i) = r. Denote R = rank(Φ). Then, there exist R
linearly independent linear functions ℓ1,...,ℓR in Φ. By applying an
invertible linear transformation, we can assume without loss of gener-
ality that ℓi = xi for every i ∈[R]. Consider the circuit Φ|xi=0 for some
i ∈[R]. Clearly, Φ|xi=0 ≡0. As ℓi = xi appears in Φ, some product gate
becomes zero in Φ|xi=0. Observe that neither of the other two product
gates will become zero after setting xi = 0. Indeed, if xi divides both
Ψ1 and Ψ2 then, as Ψ3 = −Ψ1 −Ψ2, we get that xi divides Ψ3 as well.
But, since Φ is simple this is a contradiction. We thus have that Φ|xi=0
is a zero ΣΠΣ(2) circuit. This is possible only if the two product gates
multiply the same linear functions, up to multiplication by constants.
The variable xi thus induces a partial matching of the linear functions
in the circuit. This matching contains r pairs of linear functions such
that for every pair (ℓ,ℓ′) in the matching, the two linear functions ℓand
ℓ′ belong to two diﬀerent multiplication gates and ℓ|xi=0 = α · ℓ′|xi=0
for some nonzero constant α ∈F. Denote with Mi the matching induced
by xi. The next claim gives more information about these pairs.
Claim 4.23. For every (ℓ,ℓ′) in Mi it holds that xi ∈span{ℓ,ℓ′}.
Proof. Denote ℓ= 
j∈[n] ajxj and ℓ′ = 
j∈[n] bjxj. Since ℓ|xi=0 = α ·
ℓ′|xi=0, it holds that aj = α · bj for every j ̸= i in [n]. Since the two
linear forms ℓ,ℓ′ are linearly independent, as Φ is simple, it must hold
that ai ̸= α · bi. Hence, xi = (ℓ−α · ℓ′)/(ai −α · bi).
Claim 4.23 tells us that every pair (ℓ,ℓ′) in Mi spans xi. We also have
that all the matchings {Mi}i∈[R] are contained in a set of at most 3r
linear functions, and that each matching contains r pairs. Standard
arguments from information theory (e.g., Theorem 1.2 of Dvir and

326
Polynomial Identity Testing
Shpilka [43] concerning locally decodable codes) now show that, regard-
less of the ﬁeld, 3r ≥exp(R). In other words, R = O(logr). This proved
the claimed result for ﬁnite ﬁelds.
The argument above works for any ﬁeld, but when F = R,Q one can
get a better bound. To obtain the improved bound we use a slightly
diﬀerent point of view, as conjectured in Ref. [43] and ﬁrst proved in
Ref. [80]. Given a linear function ℓ(x) = 
i∈[n] ai · xi we map it to the
line ϕ(ℓ) in Rn passing through a = (a1,...,an) and the origin. Given
a simple ΣΠΣ(3) circuit computing the zero polynomial, consider the
three multi-sets of lines corresponding to each of the three multipli-
cation gates. Let H ⊂Rn be a hyperplane such that each of the lines
deﬁned above intersects H in exactly one point (a random H has this
property). For a linear function ℓfrom the circuit, denote this intersec-
tion point by h(ℓ) = ϕ(ℓ) ∩H. This gives a multi-set of points in H.
Color all these points by three colors, according to the diﬀerent multipli-
cation gate that they came from (every point receives exactly one color,
as Φ is simple). As the circuit is zero, similar to the proof of Claim 4.23,
for every ℓand ℓ′ that belong to two diﬀerent multiplication gates, there
is some ℓ′′ in the third gate such that ℓ′′ ∈span(ℓ,ℓ′). This implies that
the point h(ℓ′′) belongs to the line passing through h(ℓ) and h(ℓ′). The
three colored sets thus have the property that every line that contains
points from two sets must also contain a point from the third set. By
[45] this implies that the points belong to a three-dimensional subspace
of Rn. It follows that, originally, the linear function in the circuit span
a vector space of dimension at most 4.
We note that the result of Edelstein and Kelly [45] is a colored ver-
sion of the famous Sylvester–Gallai theorem, that shows that if we have
a collection of points in the plane such that any line passing through
two points contains a third point then the points must be collinear.
The proof above only holds for the case k = 3. The extension to
larger values of k is by induction [43, 80, 117, 118]. We do not give
further details of the proofs, which are based on the arguments sketched
above.
Theorem 4.22 enables us to explain the black-box algorithm of
Karnin and Shpilka [76].

4.6 Depth-3 Circuits
327
Theorem 4.24 ([76]). There is a deterministic nO(R(k,r))-time black-
box PIT algorithm for ΣΠΣ(k) circuits of degree r.
Before giving the proof we need the following deﬁnition. Let Φ =

i∈[k] Ψi be a ΣΠΣ(k) circuit. Let g = gcd(Ψ1,...,Ψk) be the greatest
common divisor of all the multiplication gates. Clearly, g is a product
of linear functions. The simpliﬁcation of Φ is
sim(Φ) = Φ/g.
In words, it is the circuit obtained by deleting g from each Ψi. Speciﬁ-
cally, sim(Φ) is a simple ΣΠΣ(k) circuit.
Proof. [Sketch] Denote R = R(k,r). The ﬁrst step is to construct a set
of m = poly(n) linear transformations T = {Ti : F2R →Fn}i∈[m] such
that for every vector space V , of linear functions from Fn to F, it holds
that all but a polynomially small fraction of the transformations T in T
satisfy that
dim(V ◦T) = min(dim(V ),2R),
where V ◦T = span{ℓ◦T : ℓ∈V } and ◦is composition. Intuitively, for
any such V , for most of the transformations T in T , the linear functions
in V remain as linearly independent as possible when composed with T.
Indeed, dim(V ◦T) is never larger than min(dim(V ),2R). Gabizon and
Raz [47] constructed a set T with the required properties (see Ref. [76]
for the exact translation of the construction of Gabizon and Raz [47]
to our setting). We omit the details of this construction. For a ΣΠΣ(k)
circuit Φ and T ∈T , denote by Φ ◦T the ΣΠΣ(k) circuit in 2R
variables y = (y1,...,y2R) obtained by substituting xℓby (Ty)ℓin Φ.
Assume the existence of such a set T . Let Φ = 
i∈[k] Ψi be a
ΣΠΣ(k) circuit where Ψi = 
j∈[ri] ℓi,j. For every nonempty I ⊆[k],
let ΦI = 
i∈I ΨI. Deﬁne VI as the span of all linear functions that
occur in sim(ΦI). In addition, for every pair of linear functions
ℓ,ℓ′ in sim(Φ), consider the space Wℓ,ℓ′ = span{ℓ,ℓ′}. By the union
bound, T contains a transformation T such that for every VI and
every such Wℓ,ℓ′, it holds that dim(VI ◦T) = min(dim(VI),2R) and

328
Polynomial Identity Testing
dim(Wℓ,ℓ′ ◦T) = min(dim(Wℓ,ℓ′),2R). We now show that Φ ≡0 iﬀ
Φ ◦T ≡0. As a ﬁrst step we prove a claim showing that if the rank of
sim(ΦI ◦T) is small then so is the rank of sim(ΦI).
Claim 4.25. For every nonempty I, we have rank(sim(ΦI ◦T)) =
min(rank(sim(ΦI)),2R).
Proof. We ﬁrst show that gcd(ΦI ◦T) = (gcd(ΦI)) ◦T. Namely, that
no new linear functions were added to the greatest common divisor.
Assume toward a contradiction that gcd(ΦI ◦T) ̸= (gcd(ΦI)) ◦T. It
must be the case that for some ℓthat occurs in ΦI but does not occur
in gcd(ΦI), the linear form ℓ◦T belongs to all the product gates
in ΦI ◦T. Therefore, there is some ℓ′ in ΦI such that ℓand ℓ′ are
linearly independent but ℓ◦T and ℓ′ ◦T are linearly dependent. This
contradicts the choice of T, as dim(Wℓ,ℓ′ ◦T) < 2. A similar argument
shows that for every ℓin gcd(ΦI) the linear form ℓ◦T is nonzero. We
thus get that sim(ΦI ◦T) = sim(ΦI) ◦T. The result now follows by
the choice of T, when considering VI ◦T.
The following lemma is the ﬁnal step before describing the algorithm.
Lemma 4.26. Φ ≡0 iﬀΦ ◦T ≡0.
Proof. Assume that Φ ◦T ≡0. Our goal is to show that in this case
Φ ≡0 (the other direction is trivial). Let [k] = I1 ∪I2 ∪... ∪Ic be the
unique partition satisfying that each of the circuits ΦIj ◦T is minimal
and zero. Consider sim(ΦIj ◦T). It is a simple, minimal and zero depth-
3 circuit, and so by Theorem 4.22 it holds that rank(sim(ΦIj ◦T)) ≤R.
Claim 4.25 implies that rank(sim(ΦIj)) = rank(sim(ΦIj) ◦T). In other
words, the rank of sim(ΦIj) does not change after composition with T.
Therefore, we also have that sim(ΦIj) ≡0, because T is “an invertible
transformation” between sim(ΦIj) and sim(ΦIj ◦T). Speciﬁcally,
Φ = c
j=1 ΦIj ≡0.
To obtain a PIT algorithm, using such a “good” T, all that we
have to do is to check whether Φ ◦T is identically zero. As Φ ◦T is a

4.6 Depth-3 Circuits
329
degree r polynomial in 2R variables, by evaluating it on all points in
S2R, where S ⊆F is a set of size r + 1, we can test whether it is zero
or not. The PIT algorithm, therefore, simply checks whether for all
T ∈T it holds that (Φ ◦T)(S2R) = {0}, and decides accordingly.
When the underlying ΣΠΣ(k) circuit is multilinear, namely, every
product gate in the circuit computes a multilinear polynomial, a
stronger result can be proved. Indeed, notice that the rank of a simple
multilinear ΣΠΣ(k) circuit of degree r is at least r. This follows since
all the linear functions in any one of its multiplication gates have dis-
joint support and are therefore linearly independent. Combining this
with the bound R(k,r) ≤O(k3 logr), we get that r ≤O(k3 logr) and
hence r ≤O(k3 logk) and R(k,r) = O(k3 logk). There is no clear way,
however, to use this improved rank bound for multilinear circuits, as in
the proof of Theorem 4.24, since after composing a multilinear circuit
with a linear transformation we do not necessarily get a multilinear
circuit. In Ref. [76] a set of transformations T that has the required
properties and that also keeps the circuit multilinear, was constructed.
Using this set T , Karnin and Shpilka [76] obtained an nexp(k2)-time
black-box PIT algorithm for multilinear ΣΠΣ(k) circuits. This result
was later improved in Ref. [127] that gave an nO(k)-time algorithm.
Interestingly, the result of Ref. [127] does not rely on the rank bound,
but rather uses their PIT algorithm for sums of read-once formulas.
We discuss this result in Section 4.8 (Theorem 4.43). In spite of all the
work the following question is still open.
Open Problem 26. Give a deterministic sub-exponential time PIT
algorithm for multilinear ΣΠΣ circuits.
In Section 4.5 we saw a white-box PIT algorithm for noncommu-
tative formulas. A special type of formulas that can be thought of as
a model of noncommutative formulas is the model of set-multilinear
depth-3 circuits [100]. Recall that a set-multilinear depth-3 circuit has
the following structure. The variables X are given as the union of r
disjoint sets X = X1 ∪X2 ∪··· ∪Xr. The circuit is a depth-3 circuit
in the variables X with the additional property that every product gate

330
Polynomial Identity Testing
has the form Ψ = r
i=1 ℓi(Xi), each ℓi is a linear form in Xi. In words,
each product gate multiplies a linear function in X1 with a linear func-
tion in X2 with a linear function in X3 and so forth. Clearly, every
monomial that appears in the polynomial computed by such a circuit
contains exactly one variable from each of the sets X1,...,Xr. It is not
diﬃcult to see that such circuits compute the same polynomial also in a
noncommutative world. Namely, they do not “use” commutativity dur-
ing the computation (as we always multiply a linear function in X1 with
a linear function in X2, etc.). Stated diﬀerently, depth-3 set-multilinear
circuits form a restricted sub-model both of multilinear depth-3 circuits
and of noncommutative formulas. As a ﬁrst step toward understanding
Open problem 26 one can try to solve the following problem, for which
we already saw a white-box algorithm.
Open Problem 27. Give a deterministic sub-exponential time black-
box PIT algorithm for set-multilinear ΣΠΣ circuits.
Another sub-model of depth-3 circuits that was considered is the
so-called model of diagonal circuits, deﬁned by Saxena [116]. A diagonal
depth-3 circuit is a depth-3 circuit in which the number of diﬀerent
linear functions in each multiplication gate is bounded by some number
k, and the degree is polynomial. As a comparison, in ΣΠΣ(k) circuits we
bound the top fan-in by k and here we do not restrict the top fan-in but
rather restrict the number of “diﬀerent” children of each product gate.
Saxena showed how to adapt the noncommutative PIT algorithm of Raz
and Shpilka [106] to this scenario as well, thus obtaining a polynomial
time white-box algorithm when k = O(1). We explain his approach in
Section 4.7.1 where we discuss a generalization of this model, namely,
diagonal depth-4 circuits.
4.7
Depth-4 Circuits
Theorem 4.12 tells us that derandomization of PIT of depth-4 circuits
implies a quasi-polynomial time derandomization of PIT of general
arithmetic circuits. It is, therefore, not surprising that very little is
known even for restricted sub-models of depth-4 circuits. We brieﬂy
describe the sub-models for which such algorithms are known and then

4.7 Depth-4 Circuits
331
present the algorithms. In what follows we say that a polynomial is
s-sparse if it has at most s monomials. Recall that a ΣΠΣΠ circuit of
size s has the form
Φ =
k

i=1
Ψi,
where
Ψi =
ri

j=1
Pi,j(x1,...,xn),
and each Pi,j is an s-sparse polynomial.
(4.3)
We consider two diﬀerent restrictions of this model. The ﬁrst, deﬁned
by Saxena, is that of diagonal depth-4 circuits [116]. In a nutshell,
these are circuits in which the number of diﬀerent Pi,j’s in Ψi is small
(constant, mostly), and each Pi,j is a sum of univariate polynomials,
Pi,j(x) = n
m=1 gi,j,m(xm). Saxena showed how to reduce the PIT prob-
lem of diagonal circuits to that of (a generalized version of) noncom-
mutative formulas, thus obtaining a polynomial time white-box PIT
algorithm for a certain range of parameters. We discuss this result in
Section 4.7.1. Another model that was recently considered is multilinear
ΣΠΣΠ(k) circuits. In this model each of the k product gates Ψi com-
putes a multilinear polynomial. In other words, the Pi,j’s are multilin-
ear polynomials, and all the factors of Ψi are variable disjoint. For this
model, Karnin et al. [75] gave a quasi-polynomial time black-box PIT
algorithm (for k = polylog(n)). We describe this result in Section 4.7.2.
In Ref. [12] a white-box PIT algorithm for another restricted version of
ΣΠΣΠ circuits was given. The circuits have to be restricted in the fol-
lowing way. The top fan-in is constant, i.e., k = O(1), each Pi,j depends
only on c = O(1) variables, and each xm appears in at most ck diﬀer-
ent Pi,j’s. Under these assumptions (in fact, some more assumptions
are needed), Arvind and Mukhopadhyay [12] gave a polynomial-time
algorithm. However, we shall later see that the black-box algorithm
of Karnin et al. [75] can be adapted to work in this case as well.
Very recently, Saraf and Volkovich [115] were able to obtain a poly-
nomial time black-box algorithm for multilinear ΣΠΣΠ(k) circuits, for
a constant k, thus improving the result of [75]. We shortly discuss this
result in Section 4.7.2.

332
Polynomial Identity Testing
4.7.1
Diagonal Circuits
A diagonal depth-4 circuit has the following form: Φ = k
i=1 Ψi, each
product gate Ψi is of the form Ψi = ri
j=1 P ei,j
i,j , and each Pi,j is a sum
of univariate polynomials, Pi,j = n
m=1 gi,j,m(xm). Saxena [116] gave a
white-box algorithm for such circuits using the approach described in
Section 4.5.
Theorem 4.27 ([116]). There is a deterministic algorithm that,
given as input a diagonal depth-4 circuit Φ as above, runs in time
poly(nk,maxi∈[k] (1 + ei,1) · (1 + ei,2)···(1 + ei,ri)) and decides cor-
rectly whether Φ ≡0.
Proof. [Sketch] While the theorem holds over any ﬁeld, we shall sketch
the argument only for ﬁelds of characteristic zero. The idea behind
Saxena’s algorithm is to consider a “dual form” for each product gate.
This dual form will enable us to view the computation as done in a
noncommutative world. For a power series f(x1,...,xn,z,z1,...,zm),
denote by [zeze1
1 ···zem
m ]f the coeﬃcient of zeze1
1 ···zem
m
in f, which
is a polynomial in x1,...,xn. Consider a product gate Ψ = r
j=1 P ej
j
with Pj = n
m=1 gj,m(xm). Let e = e1 + ··· + er. By considering the
Taylor expansion of the exponential function, exp(ξ) = ∞
ℓ=0 ξℓ/ℓ!, and
its truncated Taylor expansion, Ee(ξ) = 1 + ξ + ξ2/2! + ··· + ξe/e!, we
observe that
(e1!e2!···er!)−1Ψ = [zeze1
1 ···zer
r ]exp(P1z1z)···exp(Przrz)
= [zeze1
1 ···zer
r ]
n

m=1
exp


r

j=1
gj,m(xm)zjz


= [zeze1
1 ···zer
r ]
n

m=1
Ee


r

j=1
gj,m(xm)zjz

.
(4.4)
Equation (4.4) is of polynomials of degree ne in z. We can therefore
interpolate and obtain the coeﬃcient of ze. In other words, there exist

4.7 Depth-4 Circuits
333
α0,...,αne and β0,...,βne in F such that
[zeze1
1 ···zer
r ]
n

m=1
Ee


r

j=1
gj,m(xm)zjz


=
ne

ℓ=0
βℓ· [ze1
1 ···zer
r ]
n

m=1
Ee


r

j=1
gj,m(xm)zjαℓ

.
Let I = ⟨ze1+1
1
,...,zer+1
r
⟩be the ideal generated by ze1+1
1
,...,zer+1
r
in
the ring of polynomials. Set, R = F[z1,...,zr]/I. It follows that for some
ﬁeld elements {β′
ℓ} we have that
Ψ · ze1
1 ···zer
r ≡
ne

ℓ=0
β′
ℓ
n

m=1
Ee


r

j=1
gj,m(xm)zjαℓ


overR.
(4.5)
Indeed, the coeﬃcient of any monomial other than ze1
1 ···zer
r
in the
RHS of Equation (4.5) must involve some zei+1
i
and is thus zero in R.
Equation (4.5) is called the dual form of a product gate in diagonal
circuits. Observe that it expresses the multiplication gate as a sum of
ne multiplication gates, where each is a product of univariate polyno-
mials. This structure explains why the algorithm for noncommutative
formulas can be applied to this setting.
Now, given our circuit Φ = k
i=1 Ψi with Ψi = ri
j=1 P ei,j
i,j , denote
Ii = ⟨zei,1+1
i,1
,...,z
ei,ri+1
i,ri
⟩and Ri = F[zi,1,...,zi,ri]/Ii. The dual form of
the product gates Ψi is
Ψi · zei,1
i,1 ···z
ei,ri
i,ri ≡
nei

ℓ=0
βi,ℓ
n

m=1
Eei


ri

j=1
gi,j,m(xm)zi,jαℓ


overRi.
We would like to work over a single ring R, and so we deﬁne the ideal
I to be generated by the following relations:
(1) For every i ∈[k] and j ∈[ri], the relation zei,j+1
i,j
= 0.
(2) For every i ̸= i′ in [k], j ∈[ri] and j′ ∈[ri′], the relation
zi,j · zi′,j′ = 0.
(3) For
every
i,i′ ∈[k],
the
relation
zei,1
i,1 ···z
ei,ri
i,ri =
z
ei′,1
i′,1 ···z
ei′,ri′
i′,ri′ .

334
Polynomial Identity Testing
Let R = F[z1,1,...,zk,rk]/I. A standard calculation reveals that R is
an algebra of dimension k
i=1(1 + ei,1)···(1 + ei,ri) −2(k −1). By the
discussion above, we get that over R
Φ · ze1,1
1,1 ···z
e1,r1
1,r1
=
k

i=1
Ψi · zei,1
i,1 ···z
ei,ri
i,ri
=
k

i=1
nei

ℓ=0
βi,ℓ
n

m=1
Eei


ri

j=1
gi,j,m(xm)zi,jαℓ


overR.
(4.6)
As Equation (4.6) can be represented as k
i=1
nei
ℓ=0 fi,ℓ,1(x1)···
fi,ℓ,n(xn), we can view it as a noncommutative polynomial in x1,...,xn.
In Section 4.5 we described the PIT algorithm of Raz and Shpilka [106]
for noncommutative formulas, that basically runs in time polynomial
in the size of the formula, which in our case can be thought of as
nk · maxi ei. It turns out that this algorithm can be extended to work
over the algebra R at the cost multiplying the running time by a polyno-
mial in dim(R). This gives a PIT algorithm for diagonal depth-4 circuits
with running time poly(nk,maxi(1 + ei,1)···(1 + ei,ri)). In particular,
if ri = O(1) then the algorithm runs in time polynomial in the size of
the circuit.
Theorem 4.27 tells us that there is a model of ΣΠΣΠ circuits with
unbounded top fan-in for which a polynomial time white-box PIT algo-
rithm is known. This comes at the cost of bounding the number of
diﬀerent factors that each multiplication gate has and the requirement
that each Pi,j is the sum of univariate polynomials. Besides diagonal
depth-4 circuits, only noncommutative depth-4 formulas give a sub-
model of depth-4 circuits with unbounded top fan-in, for which eﬃcient
(white-box) PIT algorithms are known.
4.7.2
Multilinear ΣΠΣΠ(k) Circuits
We now describe the idea and proof of the PIT algorithm of Karnin
et al. [75] for multilinear ΣΠΣΠ(k) circuits.

4.7 Depth-4 Circuits
335
Theorem 4.28
([75]). There
is
an
algorithm
that
in
time
nO(k6 log(k)log2 s) constructs a hitting set for multilinear ΣΠΣΠ(k)
circuits of size s in n variables.
To explain the proof we ﬁrst recall that a multilinear ΣΠΣΠ cir-
cuit Φ has the following structure. Each product gate is of the form
Ψi = ri
j=1 Pi,j and for every j ̸= j′ in [ri], the polynomials Pi,j,Pi,j′
are deﬁned on disjoint sets of variables. The PIT algorithm of Karnin
et al. [75] builds on a reduction from identity testing of multilinear
ΣΠΣΠ(k) circuits to identity testing of a special type of such circuits,
circuits for which for every i,j it holds that |var(Pi,j)| ≤|var(Φ)|/c,
where var(P) is the set of variables that appear in P and c is some
parameter. We call such circuits c-compressed circuits. The main ingre-
dient in the proof is the following lemma on the structure of multilin-
ear ΣΠΣΠ(k) circuits. For a set of (indices of) variables I ⊆[n], let
m(I) = 
i∈I xi be the product of the variables in I.
Theorem 4.29. Let P be a nonzero n-variate polynomial computable
by a multilinear ΣΠΣΠ(k) circuit of size s. Let c > 0 be a parameter.
Then there exists I ⊆[n] of size |I| ≤2logn · logs · kc for which the
following holds: Let P = 
T⊆I m(T)PT be the expansion of P with
respect to the variables in I. In particular, each PT is a polynomial
in the variables (whose indices are) in [n] \ I. There exists a set T ⊆I
such that: (1) PT is a nonzero polynomial, and (2) PT = Q · H, where Q
is a product of s-sparse polynomials and H is computable by a simple,7
c-compressed ΣΠΣΠ(k) circuit of size s.
We ﬁrst give the proof of the theorem and then explain how it can
be used to derive a PIT algorithm.
Proof. Given a product gate Ψi, write Ψi = Ai · Bi where Ai is the
product of all the Pi,j’s such that |var(Pi,j)| > var(P)/2c and Bi is the
7 The deﬁnition of simple here is the same: a ΣΠΣΠ(k) circuit Φ = k
i=1 Ψi is simple if
gcd(Ψ1,Ψ2,...,Ψk) = 1.

336
Polynomial Identity Testing
product of the other factors. As Ψi is multilinear, the number of factors
in Ai is at most 2c. Therefore, since Pi,j is s-sparse, we get that Ai is
s2c-sparse. We would like to somehow “remove” the Ai part, as the
next lemma hints at.
Lemma 4.30. Let f ∈F[x1,...,xn] be a multilinear s-sparse polyno-
mial. Then, there exists a set of variables I ⊆var(f) of size |I| ≤logs
such that when writing f = 
T⊆I m(T)fT , it holds that for some
T ⊆I, the polynomial fT is a monomial.
Proof. We prove the lemma by induction on the size of var(f). Con-
sider a variable x ∈var(f) that does not divide f (if all variables divide
f then f is a monomial and we are done). Write f = xg + h. There is a
polynomial f′ ∈{g,h} that is s/2-sparse. Induction, therefore, implies
that a set of size at most log(s/2) = logs −1 with the required prop-
erty, with respect to f′, exists. Add x to this set and observe that it
satisﬁes the claim.
The lemma tells us that there is some set of variables I and T ⊂I
such that after taking a partial derivative with respect to the variables
in T and setting the variables in I \ T to zero, f becomes a (nonzero)
monomial. With this in mind, let us continue the proof. Pick a set Ii
for each Ai, according to Lemma 4.30, and let I′ = k
i=1 Ii. Speciﬁcally,
|I′| ≤k · logs2c = 2cklogs. In addition, there is some T ′ ⊆I′ such that
after taking a partial derivative with respect to the variables in T ′ and
setting the variables in I′ \ T ′ to zero, all the Ψi’s become products
of s-sparse polynomials, each of which depends on at most |var(P)|/2c
variables, and the resulting polynomial is still nonzero (this can be
seen, e.g., by considering the expansion of P w.r.t. I′). It seems that
we are done, but this is not quite the case. Denote by ˜P1 the polynomial
∂T ′(P)|xI′\T ′=0, where ∂T ′(P) is the partial derivative of P with respect
to all variables in T ′ and |xI′\T ′=0 means substituting xi = 0 for all
i ∈I′ \ T ′. The circuit computing ˜P1 is not necessarily simple and so
we let Q1 be the greatest common divisor of all the product gates
of ∂T ′(P)|xI′\T ′=0 and set P1 = ˜P1/Q1. Clearly, the circuit for P1 is
simple. Now, it may be the case that |var(P1)| < |var(P)|/2 and so P1

4.7 Depth-4 Circuits
337
is not c-compressed. If this is indeed the case then we simply repeat
the process for P1. This can happen at most logn times as the number
of variables shrinks by a factor of at least two after each “unsuccessful”
round. Now, we are basically done. We obtained a set I of size at most
2logn · logs · kc (which is the union of all sets I′ constructed in the
diﬀerent stages) and a set T ⊂I so that the following holds. Let Q be
the greatest common divisor of all the product gates of ∂T (P)|xI\T =0.
Denote H = ∂T (P)|xI\T =0/Q. Then H is both c-compressed and simple
as claimed.
Theorem 4.29 suggests the following algorithmic approach. Start
by constructing a hitting set for c-compressed circuits. Then, “guess”
the set I guaranteed by the lemma (going over all possibilities requires
quasi-polynomial time). Assume that we have the “correct” I. Theo-
rem 4.29 guarantees that for some T ⊆I, the coeﬃcient8 of m(T) is a
(nonzero) simple, c-compressed circuit. Therefore, by trying each of the
possible substitutions to the variables in [n] \ I, from the hitting set for
c-compressed circuits, we are guaranteed that for some substitution the
resulting polynomial (in the I variables) will be nonzero. Now, all that
we have to do is to verify in a black-box manner whether a multilinear
polynomial in the |I| variables is nonzero and this can be done in time
exp(|I|), which is quasi-polynomial in n.
To conclude, our goal is to come up with a hitting set for
c-compressed circuits. As we shall soon see the hitting set for
c-compressed ΣΠΣΠ(k) circuits is based on a generator for multilinear
ΣΠΣΠ(k −1) circuits. Thus, our construction is recursive in nature.
The idea behind the PIT algorithm for c-compressed circuits is that
such circuits “contain” a nonzero multilinear depth-3 circuit. In fact,
we prove that there is a small, but not too small, set J of variables
and an assignment, coming from a hitting set for sparse polynomials,
to the variables not in J so that after we make this assignment, we are
left with a nonzero multilinear depth-3 circuit. To identify this “good”
set of variables we use the following simple lemma (the proof is by a
straightforward greedy argument).
8 The coeﬃcient is in fact a product of sparse polynomials and a c-compressed circuit, but
this does not change the picture much.

338
Polynomial Identity Testing
Lemma 4.31. Let P ∈F[x1,...,xn] be computed by a ΣΠΣΠ(k) mul-
tilinear c-compressed circuit Φ = k
i=1
ri
j=1 Pi,j. Then there exists
a set J ⊆var(P) of size |J| ≥c/k such that for every i,j we have
|J ∩var(Pi,j)| ≤1.
Let J be the set guaranteed by the lemma. Think of c as small so
we can actually “guess” J (we can always make J smaller if it is too
large). We shall try to ﬁx the variables in [n] \ J so that the resulting
polynomial is computed by a nonzero multilinear ΣΠΣ(k) circuit. For
this end we shall use generators (see Section 4.1 for the deﬁnition) for
sparse polynomials. We construct a generator for c-compressed circuits
by induction on the top fan-in, k. As mentioned above, the inductive
argument will in fact assume that we already have a generator for
multilinear ΣΠΣΠ(k −1) circuits.
The base case of the induction is k = 1. It is easy to see that a gen-
erator for s-sparse polynomials is also a generator for this case. Indeed,
such a circuit is a product of s-sparse polynomials, and Observation 4.1
guarantees that a generator for s-sparse polynomials is also a generator
for a product of s-sparse polynomials. By Theorem 4.15 and Lemma 4.1
we get that there exists a generator S2s2 : Ft →Fn for the class of (2s2)-
sparse polynomials with t = O(logs + logn). We now assume that, for
some parameter tk−1, we have a mapping Gk−1 : Ftk−1 →Fn that is a
generator for sparse polynomials as well as for ΣΠΣΠ(k −1) circuits of
size s (recall Observation 4.3), and show how to construct a generator
for c-compressed ΣΠΣΠ(k) circuits of size s. Denote by R(k) a number
larger than the rank of every minimal and simple multilinear ΣΠΣ(k)
circuit that computes the zero polynomial. The discussion following the
proof of Theorem 4.24 tells us that R(k) = O(k3 logk). In the following
lemma we show that if c = kR(k) then when we restrict the variables
in [n] \ J to Gk−1, we obtain a nonzero polynomial.
Lemma 4.32. Let k ≥2 and 0 ̸≡P ∈F[x1,...,xn] be computed by a
simple multilinear (k · R(k))-compressed ΣΠΣΠ(k) circuit of size s. In
addition, let Gk−1 be a generator for multilinear ΣΠΣΠ(k −1) circuits
of size s as well as for (2s2)-sparse polynomials. Then, there is a set

4.7 Depth-4 Circuits
339
J ⊆var(P) of size R(k) so that (recall the notations in Section 4.1)
P ◦Gvar(P)\J
k−1
̸≡0.
In words, after substituting the generator for the variables in [n] \ J,
the resulting circuit remains nonzero.
Proof. Let Φ = k
i=1
ri
j=1 Pi,j be a simple multilinear (k · R(k))-
compressed ΣΠΣΠ(k) circuit of size s computing P. If Φ is not minimal,
then P can be computed by a ΣΠΣΠ(k −1) circuit of size s and we are
done. So we can assume without loss of generality that Φ is minimal.
Let J be a set promised by Lemma 4.31 and let ¯J = [n] \ J. We can fur-
ther assume without loss of generality that |J| = k · R(k)/k = R(k), by
removing arbitrary indices from J if required. Via a reduction to depth-
3 circuits, we describe how to ﬁnd an assignment for x ¯J = (xj)j∈¯J such
that the restriction of P to that assignment in nonzero. For every Pi,j
in Φ, there is at most one variable xℓso that xℓ∈J ∩Pi,j. Let fi,j,gi,j
be such that Pi,j = fi,jxℓ+ gi,j (if no such xℓexists then Pi,j is a con-
stant). Note that two such polynomials Pi1,j1,Pi2,j2 have a common fac-
tor iﬀQi1,j1,i2,j2
def
= fi1,j1 · gi2,j2 −fi2,j2 · gi1,j1 ≡0. Let Q be the set of
all such nonzero Qi1,j1,i2,j2’s. The following lemma gives a suﬃcient con-
dition that a given assignment to x ¯J results in a simple, minimal, and
nonzero depth-3 circuit. Let Φ1,...,Φ2k−2 be the proper sub-circuits
of Φ, excluding the empty circuit. They are all ΣΠΣΠ(k −1) circuits
of size at most s.
Lemma 4.33. Let
ˆΦ =
2k−2

i=1
Φi ·

Q∈Q
Q.
Let a ∈Fn be such that ˆΦ|x ¯
J=a ¯
J ̸≡0. Then Φ|x ¯
J=a ¯
J is a simple and
minimal multilinear ΣΠΣ(k) circuit.
We will show that Φ|x ¯
J=a ¯
J is nonzero after the proof of the lemma.

340
Polynomial Identity Testing
Proof. The circuit Φ|x ¯
J=a ¯
J is minimal, since all of the sub-circuits of
Φ are factors of ˆΦ and so if one of them is zero then so is ˆΦ|x ¯
J=a ¯
J.
Due to the same reason, no Pi,j is reduced to zero. By the choice
of J, the size of var(Pi,j) is at most one, which implies that Φ|x ¯
J=a ¯
J
is a ΣΠΣ(k) circuit and it is clearly multilinear. We now show that
Φ|x ¯
J=a ¯
J is simple. For two polynomials f,g, write f ≁g if f/g is
not a ﬁeld element. Let Pi1,j1 ≁Pi2,j2 in Φ. If var(Pi1,j1|x ¯
J=a ¯
J) ̸=
var(Pi2,j2|x ¯
J=a ¯
J) then Pi1,j1|x ¯
J=a ¯
J ≁Pi2,j2|x ¯
J=a ¯
J. If, on the other hand,
var(Pi1,j1|x ¯
J=a ¯
J) = var(Pi2,j2|x ¯
J=a ¯
J) then as Qi1,j1,i2,j2|x ¯
J=a ¯
J ̸≡0 it fol-
lows that Pi1,j1|x ¯
J=a ¯
J ≁Pi2,j2|x ¯
J=a ¯
J. As Φ is simple, we can thus
conclude that Φ|x ¯
J=a ¯
J is simple.
We return to the proof of Lemma 4.32. The polynomial ˆΦ is a prod-
uct of (2s2)-sparse polynomials and ΣΠΣΠ(k −1) circuits of size s
each. Therefore, ˆΦ|x ¯
J=G ¯
J
k−1 ̸≡0. Lemma 4.33 thus implies that there
exists a ∈Im(Gk−1) for which Φ|x ¯
J=a ¯
J is a simple, minimal, and mul-
tilinear ΣΠΣ(k) circuit. The circuit Φ|x ¯
J=a ¯
J contains R(k) variables
(the previous proof shows that all the variables in J “survived”) and
any linear function appearing in it contains only one variable. The rank
of Φ|x ¯
J=a ¯
J is hence R(k). By the deﬁnition of R(k), this implies that
P ◦G ¯J
k−1 ̸≡0.
We now show how to construct Gk : Ftk →Fn using Gk−1. The con-
struction uses the following two statements that were proved above.
First, Theorem 4.29 implies that it suﬃces for Gk to satisfy the following
property: for every subset I of the variables of size |I| ≤2logn · logs ·
k · (kR(k)), we can leave the variables in I “alive” while applying a
generator for both (kR(k))-compressed circuits and for sparse polyno-
mials to the rest of the variables. Indeed, the polynomial PT given by
Theorem 4.29 is a product of a (kR(k))-compressed circuit with sparse
polynomials. Second, Lemma 4.32 applied to PT tells us that there is
a set J ⊆var(P) of size R(k) so that PT ◦Gvar(P)\J
k−1
̸≡0.
Concluding, the only property that we need Gk to have is that it can
keep “alive” any subset of the variables of size at most “|I| + |J|” ≤
2logn · logs · k2R(k) + R(k) ≤3logn · logs · k2R(k) while applying
Gk−1 to the rest of the variables. A generator that keeps variables

4.7 Depth-4 Circuits
341
“alive” was constructed in Ref. [127] in the context of PIT of sums
of read-once formulas. Speciﬁcally, Shpilka and Volkovich [127] con-
struct a map Gt from F2t to Fn so that its image contains all points
a ∈Fn with at most t nonzero entries. In the next section we describe
this construction. Joining all the diﬀerent ingredients of this section
together we get Theorem 4.28.
Theorem 4.34. Let P ∈F[x1,...,xn] be a nonzero polynomial com-
puted by a multilinear ΣΠΣΠ(k) circuit of size s. Let Gt is the gener-
ator that keeps variables “alive,” constructed in Section 4.7.2.1 below.
Then for every t ≥3k3R(k)log(s)log(n) it holds that P(Gt(y,z) +
S2s2(w)) ̸≡0, where y,z,w are vectors of new variables.
Proof. The proof is by induction on k. For k = 1, the polynomial
P is a product of (2s2)-sparse polynomials. By deﬁnition of S2s2
and Observations 4.1 and 4.4, we get that P(Gt + S2s2) ̸≡0, as
claimed. Assume that k ≥2. Let I,J ⊆[n] be the subsets guaran-
teed by Theorem 4.29 and Lemma 4.32, respectively, and set U =
I ∪J. By the induction hypothesis, the mapping Gm(y,z) + S2s2(w)
with m =

3(k −1)3R(k −1)log(s)log(n)
 
is a generator for both
ΣΠΣΠ(k −1) circuits and for (2s2)-sparse polynomials. From Theo-
rem 4.29 and Lemma 4.32 (see the discussion above) it follows that
there is a point a ∈

G[n]\U
m
+ S[n]\U
2s2

so that P(a) ̸= 0. Since |U| ≤
3k2R(k)log(s)log(n) ≤t −m, Observation 4.4 below implies
Im
	
G[n]\U
m
+ S[n]\U
2s2

⊆Im
	
G[n]\U
m
+ S2s2

⊆Im(Gt + S2s2)
and thus a ∈Im(Gt + S2s2), as claimed.
Theorem 4.34 shows the existence of a generator for ΣΠΣΠ(k)
circuits. To conclude the proof of Theorem 4.28 we observe the
following facts: First, the degree of every coordinate of Gt and
of S2s2
is n −1. Furthermore, the generator Gt(y,z) + S2s2(w)
has “seed-length” O(k3R(k)log(s)log(n) + log(s) + log(n)). There-
fore,
Lemma
4.1
implies
that
there
is
a
hitting
set
of
size
nO(k3R(k)log(s)log(n)) = nO(k6 logklog2(s)) as claimed.

342
Polynomial Identity Testing
Very recently, Saraf and Volkovich improved the result of Theo-
rem 4.28 [115]. In fact, they gave a black-box algorithm of running
time n ˜O(k2). The main idea behind their proof is an extension of The-
orem 4.22 to the case of multilinear ΣΠΣΠ(k) circuits. Speciﬁcally,
note that if a depth-3 multilinear multiplication gate has rank R, then,
because of multilinearity, it computes an nR-sparse polynomial. By
replacing “rank” with “sparsity” Saraf and Volkovich [115] were able
to show that if a simple and minimal multilinear ΣΠΣΠ(k) circuit com-
putes the zero polynomial then any of its multiplication gates computes
a sparse polynomial. Using ideas similar to those of Klivans and Spiel-
man [86] (see the proof of Theorem 4.15) they managed to gradually
reduce the sparsity of the output polynomial. Applying their structural
theorem they were able to prove that the output polynomial does not
become zero in this process, thus obtaining an eﬃcient way of trans-
forming a nonzero polynomial computed by a ΣΠΣΠ(k) circuit to a
sparse polynomial. When the polynomial is sparse enough they simply
run the [86] PIT test for sparse polynomial.
Theorem 4.35 ([115]). There is a deterministic nO(k2 logk) time
black-box PIT algorithm for multilinear ΣΠΣΠ(k) circuits.
We mention an obvious next step question.
Open Problem 28. Give a sub-exponential time PIT algorithm for
ΣΠΣΠ(k) circuits.
4.7.2.1
The Generator of [127]
In this section we construct a map from F2t to Fn with the
following property: Its image contains all points a ∈Fn with at
most t nonzero entries. Throughout the entire section we ﬁx a set
A = {α1,α2,...,αn} ⊆F of n distinct elements.9
9 We assume w.l.o.g. that |F| > n as we are allowed to use elements from an appropriate
extension ﬁeld.

4.7 Depth-4 Circuits
343
Deﬁnition 4.3.
For every i ∈[n] let ui(w) : F →F be the i-th
Lagrange Interpolation polynomial for the set A. That is, each ui(w)
is polynomial of degree n −1 satisfying ui(αj) = 1 if i = j and zero
otherwise. For every i ∈[n] and t ≥1 deﬁne Gi
t(y1,...,yt,z1,...,zt) :
F2t →F as
Gi
t(y1,...,yt,z1,...,zt) def
=
t

j=1
ui(yj) · zj .
Finally, let Gt(y1,...,yt,z1,...,zt) : F2t →Fn be deﬁned as
Gt(y1,...,yt,z1,...,zt) def
= (G1
t ,G2
t ,...,Gn
t )
=


t

j=1
u1(yj) · zj,
t

j=1
u2(yj) · zj,...,
t

j=1
un(yj) · zj

.
The following is an immediate observation that was used in the
proof of Theorem 4.34.
Observation 4.4. The following properties hold:
(1) For every t ≥1, it holds Gt(¯y,¯0) ≡0.
(2) Denote with ¯ei ∈{0,1}n the vector that has 1 in the i-th
coordinate and 0 elsewhere. Then
Gt+1 = Gt +
n

i=1
ui(yt+1) · zt+1 · ¯ei.
Hence,
for
every
t ≥1
and
αm ∈A
we
have
that
Gt+1|yt+1=αm = Gt + zt+1 · ¯em.
(3) Let t,m ∈N, I ⊆[n] and |I| ≤t. Then, it holds that
Im
	
G[n]\I
t

⊆Im(Gt+m).
Property (4.4) is the one showing that Gt can keep any t variables
“alive”.

344
Polynomial Identity Testing
4.8
Read-Once Formulas
As mentioned above, in spite of the known lower bounds for multilinear
formulas, there is no PIT algorithm even for multilinear ΣΠΣ formulas
(Open Problem 26). Due to this lack of progress, the model of read-
once formulas (ROF) — a restricted model of multilinear formulas —
was considered. Roughly, an ROF is an arithmetic formula in which
every variable labels at most one leaf. It is clear that every ROF is also
a multilinear formula. We now give the formal deﬁnition.
Deﬁnition 4.4. A read-once arithmetic formula (ROF) over a ﬁeld F
in the variables {x1,...,xn} is a binary tree as follows. Like in a formula,
the leaves are labeled by variables and the internal nodes by {+,×}. In
addition, every node is labeled by a pair of ﬁeld elements (α,β) ∈F2.
Each input variable labels at most one leaf. The computation is per-
formed in the following way. A leaf labeled by xi and (α,β) computes
αxi + β. If a node v is labeled by ⋆∈{+,×} and (α,β) and its children
compute the polynomials f1 and f2, then v computes α(f1 ⋆f2) + β.
We say that f is a read-once polynomial (ROP) if it can be computed
by an ROF. An ROF that does not contain addition gates is called a
multiplicative ROF. In a similar way, deﬁne multiplicative ROP.
Clearly, not every polynomial is an ROP. For example, in Ref. [126]
it is shown that x1x2 + x2x3 + x1x3 is not an ROP.
It is not hard to see that there is a linear time white-box PIT
algorithm for ROFs. The best deterministic black-box PIT algorithm
for the problem, however, runs in time nO(logn) [127]. The following
theorem of Shpilka and Volkovich [127] gives such a black-box PIT
algorithm for sums of a small number of ROFs.
Theorem 4.36 ([127]). There is a deterministic algorithm for decid-
ing whether a polynomial f = f1 + ··· + fk, where f1,...,fk are ROPs
in {x1,...,xn} and k ≤n/3, is zero or not. The algorithm runs in time
nO(k) in the white-box model and in time nO(k+logn) in the black-box
model.

4.8 Read-Once Formulas
345
The proof given by Shpilka and Volkovich [127] has three parts.
First, they prove the theorem for k = 1. Second, they prove a hardness
of representation result, which is basically a lower bound for sum of
read-once formulas that have a certain property. Third, they show how
to combine the PIT for a single ROP with the hardness of representa-
tion result to obtain the PIT for sums of ROPs.
We
start
with
the
proof
for
the
case
k = 1.
Shpilka
and
Volkovich [127] proved that the mapping Glogn
constructed in
Section 4.7.2 is a generator for ROFs.
Theorem 4.37([127]). Let f be an ROP. If f ̸≡0 then f ◦Gt is not
constant for t = ⌈logn⌉+ 1. Hence, if S ⊆F is a set of size n2 + 1, then
f ≡0 iﬀf ◦Gt is zero on S2t. This gives a black-box PIT algorithm of
running time n4logn+O(1) for ROFs.
Proof. The proof is by induction on the number of variables n. The
simple idea behind the induction is that the two children of the output
node compute two variable-disjoint ROPs. Hence, at least one of them
contains at most n/2 variables. We therefore need to ﬁnd a generator
that does not cause too many cancellations between variable-disjoint
ROPs. Since Gt allows us to keep Ω(t) variables “alive,” we can show
that it has the required property.
The case n ≤2 is easily veriﬁed. For n > 2, consider the structure
of the ROF computing f. If f = α(f1 × f2) + β then the induction
hypothesis immediately implies the claim for f. So we can assume f =
f1 + f2 and without loss of generality |var(f1)| ≤n/2. By the induction
hypothesis f1 ◦Gt−1 is not constant. Since f is an ROF one can prove
(by induction on the size of the ROF) that there exists some variable
xm such that f1(G1
t−1,...,Gm−1
t−1 ,xm,Gm+1
t−1 ,...,Gn
t−1) depends on xm.10
Thus, by Property 4.4 of Observation 4.4, f1 ◦Gt|yt=am depends on zm,
where yt,zm are variables of Gt+1. As xm does not belong to var(f2)
10 Surprisingly, this is not true for general polynomials, e.g., consider g(x1,x2,x3) =
x1 · x2 + x1 · x3 + x2 · x3 over F2. Substituting x1 = x2 = x3 = y gives the polynomial
g(y,y,y) = y. However, g(x1,y,y) = y and so it does not depend on x1 and similarly for
any other xi.

346
Polynomial Identity Testing
(since f is an ROF) we get that f ◦Gt|yt=am = f1 ◦Gt|yt=am + f2 ◦
Gr|yt=am depends on zm and therefore f ◦Gt is not constant. In par-
ticular, f ◦Gt is a nonzero polynomial of degree at most n2 in each
variable. Evaluating it on S2t gives zero if and only if f is zero.
We now proceed to the second step. We ﬁrst deﬁne the notion of
a justifying assignment that in short can be described as an assign-
ment that “preserves dependencies.” This notion was ﬁrst deﬁned in
Ref. [28] for the aim of reconstructing read-once formulas. Justifying
assignments turn out to be a very important tool for dealing with ROFs.
Deﬁnition 4.5. We say that an assignment a ∈Fn is a justifying
assignment of a polynomial f if for every subset I ⊆var(f) we have
that var(f|xI=aI) = var(f) \ I. We also say that f is a-justiﬁed in this
case. We say that an assignment a is a weakly-justifying assignment
of f if the condition above holds when |I| = 1.
The following is a simple observation.
Observation 4.5. A multilinear polynomial f is a-justiﬁed iﬀfor every
xi ∈var(f) it holds that11 ∂xi(f)(a) ̸= 0.
The following useful claim was proved in Ref. [127]. It basically
shows how to use PIT for a given circuit class in order to construct a
common justifying assignment for a set of polynomials constructed by
circuits in this class. Here we prove the claim for the special case of
ROFs, taking into account Theorem 4.37.
Lemma 4.38. Let f1,...,fk be n-variate ROPs. Let T ⊆F be of size
|T| = kn3 + 1. Then Hk
n
def
= Gt(T 2t), for t = ⌈logn⌉+ 1, contains a com-
mon justifying assignment for f1,...,fk. Given the ROFs computing the
fi’s, we can ﬁnd this common justifying assignment in time poly(n,k).
11 In Ref. [126] the notion of discrete partial derivative was used. The discrete partial deriva-
tive of f with respect to xi is deﬁned as f|xi=1 −f|xi=0. Notice that for multilinear
polynomials this is the same as ∂xi(f) and so we will use our notations instead.

4.8 Read-Once Formulas
347
Proof. For i ∈[n],j ∈[k], set gj
i
def
= ∂xi(fj) ◦Gt. As a partial derivative
of an ROP is also an ROP, Theorem 4.37 implies that if ∂xi(fj) ̸≡0
then gj
i ̸≡0. Consider the polynomial
g def
=

i,j|gj
i ̸≡0
gj
i .
It is a nonzero (2t)-variate polynomial of degree at most kn3 in each
variable. As T is large enough, g|T 2t ̸≡0. Equivalently, there exists
a ∈T 2t so that if gj
i ̸≡0 then gj
i (a) ̸= 0. Let i ∈[n],j ∈[k] be such
that xi ∈var(fj). Then ∂xi(fj) ̸≡0 and hence gj
i ̸≡0, which implies
∂xi(fj)(Gt(a)) = gj
i (a) ̸= 0. By Observation 4.5 it follows that Gt(a) is
a justifying assignment for every fj.
In the white-box case we can actually use the O(n) time white-box
PIT algorithm for ROFs in order to obtain a common justifying
assignment. The idea is to ﬁnd the entries of the point a one by
one, using the PIT algorithm for the gj
i ’s. For more details, see
Ref. [126].
Lemma 4.38 tells us that there is a subset Hk
n ⊂Fn of size |Hk
n| =
poly(klogn,nlogn) that contains a common justifying assignment for
every set of k ROPs. Assume that a = (a1,...,an) ∈Fn is such an
assignment for f1,...,fk. By shifting the variables xi ←xi + ai we can
assume that the zero vector, ¯0 ∈Fn, is justifying for all the fi’s. The
following theorem of Shpilka and Volkovich [127] shows a lower bound
for a sum of ¯0-justiﬁed read-once formulas.
Theorem 4.39. The monomial Pn = x1 · x2 ··· · xn cannot be com-
puted as a sum of k ≤n/3 read-once formulas that are weakly-¯0-
justiﬁed.
The polynomial x1 · x2 ··· · xn is an ROP that is not weakly-¯0-
justiﬁed. It is not diﬃcult to see that using a simple interpolation (for
the polynomial f(y) = n
i=1(y + xi)), one can represent Pn as a sum
of n read-once formulas that are weakly-¯0-justiﬁed. Thus, the theorem
above is almost tight.

348
Polynomial Identity Testing
Proof. The proof is by induction on k. The cases k ∈{0,1} follow by
deﬁnition. We now assume that k ≥2 and that n ≥3k. Let f1,...,fk be
weakly-¯0-justiﬁed ROPs over F[x1,...,xn]. Assume toward a contradic-
tion that 
i∈[k] fi = Pn. The idea of the proof is to eliminate a “large”
number of ROPs at the cost of a “small” number of variables. Specif-
ically, we ﬁnd a small set of (indices of) input variables J ⊆[n −1]
and a nonzero constant α ∈F so that if we take a partial derivative
with respect to all of the variables in J and substitute xn = α then we
eliminate many fi’s in a way that the rest of the ROPs remain weakly-
¯0-justiﬁed. We thus get a representation of ∂J(Pn)|xn=α = α · Pn′, with
a relatively large n′, as a sum of a small number of weakly-¯0-justiﬁed
ROPs. We then use the induction hypothesis to reach a contradiction.
We now proceed with the proof. There are two cases to consider.
Case 1: There exist i ̸= j in [n] and m in [k] such that ∂xi∂xj(fm) ≡0,
namely, fm does not contain xixj in any of its monomials. Assume
without loss of generality that i = n −1,j = n, and m = k. It thus
holds that 
i∈[k−1] ∂xn∂xn−1(fi) = Pn−2. It may be the case that more
than one fm vanishes when we take a partial derivative with respect
to xn,xn−1, however they cannot all vanish as Pn contains xnxn−1.
Lemma 4.40 below (whose simple proof is omitted) guarantees that the
polynomials ∂xn∂xn−1(f1),...,∂xn∂xn−1(fk) are also weakly-¯0-justiﬁed
ROPs.
Lemma 4.40. A partial derivative of a weakly-¯0-justiﬁed ROP is a
weakly-¯0-justiﬁed ROP.
We thus obtain a representation of Pn−2 as a sum of 0 < k′ ≤k −1
weakly-¯0-justiﬁed ROPs such that 0 < 3k′ ≤3k −3 < n −2, a contra-
diction.
Case 2: For every i ̸= j in [n] and m in [k] we have that ∂xi∂xj(fm) ̸≡0.
In this case the ROFs for f1,...,fk do not contain any + gate, that
is, they are multiplicative ROPs. The following lemma of Shpilka and
Volkovich [127] is intuitive when thinking about the tree structure of a
multiplicative ROF, yet the proof is rather technical and so we omit it.

4.8 Read-Once Formulas
349
Lemma 4.41. Let g be a multiplicative ROP with |var(g)| ≥2. Then,
for every xi ∈var(g) there exists xj ∈var(g) such that ∂xj(g) = (xi −
α)h for some α ∈F and an ROP h such that var(h) = var(g) \ {xi,xj}
If, in addition, g is weakly-¯0-justiﬁed then so is h. Moreover, in this
case α ̸= 0 and there is at most one element β ̸= α ∈F such that g|xi=β
is not weakly-¯0-justiﬁed.
Lemma 4.41 implies that for every m ∈[k] there exist jm ∈[n], a
nonzero αm ∈F, and an ROP hm such that ∂xjm(fm) = (xn −αm)hm.
Let A = {αm | m ∈[k]}. Clearly, 0 /∈A. For every α ∈A, let Eα =
{m ∈[k] | αm = α} and Bα = {m ∈[k]|αm ̸= α and fm|xn=α
is not
weakly-¯0-justiﬁed}. Intuitively, Eα is set of the ROPs that can be elim-
inated by substituting xn = α and Bα is set of (“bad”) ROPs that will
become non-weakly-¯0-justiﬁed upon the substitution and thus require
a special treatment. From the deﬁnition of A we have that |Eα| ≥1 and

α∈A |Eα| = k. In fact, the Eα’s form a partition of [k]. Lemma 4.41
implies that for each α ̸= α′ in A the sets Bα and Bα′ are disjoint (since
for every ROP there exists at most one “bad” value of xn) and there-
fore 
α∈A |Bα| ≤k. Hence, there exists α0 ∈A such that |Bα0| ≤|Eα0|
and 0 < |Eα0|. Let I = Eα0 ∪Bα0 ⊆[k] and J = {jm | m ∈I } ⊆[n].
By choice, 1 ≤|J| ≤|I| ≤|Eα0| + |Bα0| ≤2|Eα0| and n /∈J. For every
m ∈[k], deﬁne f′
m = ∂J(fm), where ∂J(·) is the partial derivative
with respect to {xj : j ∈J}. We note the following properties: By
Lemma 4.40, every f′
m is a weakly-¯0-justiﬁed ROP. Secondly, for every
m ∈I we have that f′
m = (xn −αm)h′
m for some ROP h′
m(¯x). Indeed,
as jm ∈J we have that
f′
m = ∂J(fm) = ∂J\{jm}(∂{jm}(fm)) = ∂J\{jm}((xn −αm)hm)
= (xn −αm) · ∂J\{jm}hm.
In addition, for every m ∈I we have that h′
m is a weakly-¯0-justiﬁed
ROP. This follows from the fact that a partial derivative of a weakly-
¯0-justiﬁed ROP is also a weakly-¯0-justiﬁed ROP and the previous two
properties.
For m ∈[k] consider the ROP f′′
m = f′
m|xn=α0. Based on the above
we can conclude the following: (1) For every m ∈Eα0, since αm = α0, it

350
Polynomial Identity Testing
holds that f′′
m = (α0 −αm)h′
m ≡0. (2) For every m ∈Bα0 we have that
f′′
m = (α0 −αm)h′
m is a nonzero weakly-¯0-justiﬁed ROP. In contrast to
fm, the structure of f′
m guarantees that it remains weakly-¯0-justiﬁed
when substituting xn = α0. (3) For m ∈[k] \ I the deﬁnitions of
Eα0 and Bα0 guarantee that fm|xn=α0 is a weakly-¯0-justiﬁed ROP.
Lemma 4.40 implies that the same holds for f′′
m. In this case it is also
possible that f′′
m ≡0.
Concluding, we get that f′′
m ≡0 for m ∈Eα0
and f′′
m
is a
weakly-¯0-justiﬁed ROP for m ∈[k] \ Eα0. Without loss of generality,
assume that J = {n′ + 1,n′ + 2,...,n −2,n −1} for some n′. Thus

m∈[k] f′′
m = ∂J(Pn|xn=α0) = α0 · Pn′. That is, we found a represen-
tation of α0 · Pn′ as a sum of weakly-¯0-justiﬁed ROPs, where at
least |Eα0| of the ROPs are zeros. As 2|Eα0| ≥|J| = (n −1) −n′ and
|Eα| ≥1, we found a representation of α0 · Pn′ as a sum of 0 ≤k′ < k
weakly-¯0-justiﬁed ROPs such that
0 ≤3k′ ≤3(k −|Eα|) = 3k −3|Eα| ≤n −3|Eα| ≤n′ + 1 −|Eα| ≤n′.
By induction we get that α0 = 0, which is a contradiction as α0 ∈A
and 0 /∈A.
Using this hardness of representation result we get the claimed PIT
for sum of ROFs.
Proof. [Proof of Theorem 4.36] Let Ak
n = {w ∈{0,1}n : # of ones in w
is at most k}. Let Hk
n be the set constructed in Lemma 4.38. Deﬁne
J k
n = Ak
n + Hk
n = {v + u : v ∈Ak
n and u ∈Hk
n}. We now show that if
f = 
i∈[k] fi ̸≡0 then f|J k
n ̸≡0. Indeed, let ρ ∈Hk
n be a common jus-
tifying assignment to the fi’s. We will show that ρ + Ak
n contains a
nonzero assignment for f. By shifting the variables (xi ←xi + ρi) it is
enough to prove the following claim.
Claim 4.42. Let f = 
i∈[k] fi ̸≡0 be a sum of ¯0-justiﬁed ROPs. Then
f|Akn ̸≡0.
Proof. The proof is by induction on the number of variables in f. If
|var(f)| ≤k then, since f is a multilinear polynomial, evaluating it

4.8 Read-Once Formulas
351
on all zero-one assignments tells us whether it is identically zero or
not. If |var(f)| > k, on the other hand, consider gi = f|xi=0 for each
variable xi. Each gi is a ¯0-justiﬁed ROP. By the induction hypothesis,
evaluating gi on Ak
n−1 ⊂Ak
n tells us whether gi is identically zero or
not. If one of the gi’s is not zero then so is f and we are done. If all the
gi’s are zero then either f ≡0 or the polynomial Pn divides f. Indeed,
if f ̸≡0 but gi ≡0 then xi is a factor of f. But now Theorem 4.39 tells
us that if f = 
i∈[k] fi = c · Pn for some constant c ∈F, then k > n/3,
which is a contradiction.
The black-box identity testing algorithm now follows immediately:
simply evaluate f on all points in J k
n . The running time is |J k
n | =
nO(k+logn). The algorithm in the white-box case is based on the same
approach but is faster, due to the white-box part of Lemma 4.38 saying
that we can ﬁnd a common justifying assignment in time poly(n,k);
having a common justifying assignment a at hand, we simply evaluate
f on Ak
n + a.
An interesting aspect of the proof of Theorem 4.36 is that it uses a
lower bound for a very weak model (sum of weakly-¯0-justiﬁed ROFs)
in order to obtain a PIT algorithm. In addition, the lower bound is
proved for a very easy polynomial, Pn, which is diﬃcult only when we
consider weakly-¯0-justiﬁed ROFs. Furthermore, the way that the lower
bound is used is very diﬀerent from the way suggested in Theorem 4.9.
Speciﬁcally, the hitting set is constructed by considering the zero set
of the “hard” polynomial Pn. This zero set is very simple and very
structured (it is the union of the halfspaces xi = 0), and it can be shown
that if f vanishes on all points in the hitting set then Pn is a factor of f.
The lower bound is then invoked to imply that f = 0. It is an intriguing
question whether this approach can be adapted to other scenarios to
yield a new way of transforming a lower bound to a PIT algorithm.
Theorem 4.36 gives a quasi-polynomial time black-box algorithm
for sum of k ROFs. No polynomial time black-box algorithm is known
when k = O(1). In particular the following question is still open.
Open Problem 29. Give a polynomial time black-box PIT algorithm
for read-once formulas.

352
Polynomial Identity Testing
In Ref. [127] it was shown that (a slightly modiﬁed version of) The-
orem 4.36 actually holds for the more general case of preprocessed read-
once formulas. In this model we can replace xi with a univariate poly-
nomial pi(xi). When we consider a sum of k ROFs, we can have a
diﬀerent polynomial pj
i(xi) for each fj. If deg(pj
i) < r for all i,j then
two modiﬁcations are required in order to get the PIT algorithm. The
ﬁrst is to slightly change the size of T in Lemma 4.38 to O(kn3r).
The second is to replace Ak
n by the set of all vectors having at most k
nonzero coordinates and taking values in S, where S ⊂F can be taken
to be any set of size r + 1. These changes give a PIT algorithm for
preprocessed ROFs of running time (nr)O(k+logn) (some more work is
required to prove that these changes indeed give the required result).
Another model that was considered in Ref. [127] is that of small
depth read-once formulas. An interesting point in the deﬁnition is that
such formulas have alternating levels of + gates and MUL gates, where
an MUL gate can compute any multiplicative ROF in its inputs. This
generalizes the usual notion of bounded depth formulas in which we
have alternating levels of + and ×. The authors of Ref. [127] give an
nO(d+k)-time black-box PIT algorithm for the sum of k depth-d ROFs.
Since multilinear ΣΠΣ(k) circuits are actually a sum of k ROFs, this
gives the currently best PIT algorithm for them.12
Theorem 4.43 ([127]). There is an nO(k)-time black-box PIT algo-
rithm for multilinear ΣΠΣ(k) circuits.
Jansen et al. [67] showed how to generalize Theorem 4.36 to get a
PIT algorithm for a sum of k read-once arithmetic branching programs
of roughly the same running time. The proof is similar to the proof of
Theorem 4.36. In particular it is based on ﬁrst solving the case k = 1
and then proving a hardness of representation theorem using essentially
the same ideas. The interested reader is referred to [67] for details.
Very recently, Anderson et al. [8] obtained a deterministic PIT
algorithm for read-k formulas. Observe that this model contains as
12 The theorem also holds if we consider preprocessed multilinear ΣΠΣ(k) circuits, which
form a sub-model of depth-4 circuits.

4.9 Relation to Other Problems
353
sub-models multilinear depth-3 and depth-4 formulas as well as sum
of ROFs. The PIT algorithm of Anderson et al. [8] runs in time nkO(k)
in the white-box model and in time nkO(k)·logn in the black-box model.
While for the sub-models mentioned above the running time is worse
than those given in Theorems 4.43, 4.28 and 4.36, the algorithm of
Anderson et al. [8] works for a more general model. Interestingly, the
approach of Anderson et al. combines the technique developed in the
proofs of Theorems 4.28 and 4.36. The basic idea is to apply partial
derivatives (using the technique of Theorem 4.36) to the formula and
show that if we take the “correct” set of partial derivatives then the for-
mula becomes a sum of c-compressed formulas. I.e., formulas that each
of their factors depends on a relatively small number of variables. At
this point, the algorithm reduces each factor to a linear form by ﬁxing
the values of most variables, using ideas similar to the proof of Theo-
rem 4.28. Finally, using the rank bound for multilinear ΣΠΣ(k) circuits
(Theorem 4.22) the authors prove that the resulting depth-3 formula
is nonzero. We stress that while this is the basic scheme, the proof is
of course more complicated and does not use previous techniques in a
straightforward manner.
Theorem 4.44 ([8]). There is a deterministic PIT algorithm for
read-k multilinear formulas. The algorithm runs in time nkO(k) in the
white-box model and in time nkO(k)·logn in the black-box model.
4.9
Relation to Other Problems
In this section we discuss the relation between PIT and other prob-
lems. We start by showing the relation to the problem of polynomial
factorization. We then discuss the problem of read-once testing which
is a generalization of the PIT problem.
4.9.1
Polynomial Factorization
Consider the following approach for PIT of multilinear formulas. Start
by making the formula into a read-once formula, that is, a formula
in which each variable labels at most one leaf. This can be done by

354
Polynomial Identity Testing
replacing the j-th occurrence of xi with a new variable xi,j. then, check
whether this formula is zero or not. If it is zero then the original formu-
las was also zero and we are done. Otherwise start replacing each xi,j
with xi. After each replacement we would like to verify that the result-
ing formula is still not zero. When replacing xi,j with xi, we get zero
iﬀthe linear function xi −xi,j is a factor of the formula that we have
at hand. Thus, we somehow have to ﬁnd a way of verifying whether a
linear function is a factor of a multilinear formula. Furthermore, as we
start with a read-once formula for which PIT is known (Section 4.8),
we can assume that we know many inputs on which the formula is
not zero. One may hope that before replacing xi,j with xi we some-
how managed to obtain inputs that will enable us to verify whether
xi −xi,j is a factor of the formula or not. This, of course, is not for-
mal, but it does show the importance of understanding how to factor
a multilinear formula given a PIT algorithm, or even just some (struc-
tured) set of nonzero inputs. As the diﬀerent factors of a multilinear
formula are variable disjoint this motivates the study of factorization of
polynomials to variable-disjoint factors. Henceforth, following Shpilka
and Volkovich [128], we refer to this problem as the decomposition
problem.
Let X = {x1,...,xn} be the set of variables. For a set I ⊆[n], denote
by XI the set of variables whose indices belong to I. A polynomial
f(X) is said to be decomposable if it can be written as f(X) = g(XI) ·
h(X[n]\I) for some I ⊆[n] of size 0 < |I| < n. The indecomposable
factors of a polynomial f(X) are polynomials g1(XI1),...,gk(XIk)
such that the Ij’s are disjoint sets of indices, f(X) = g1(XI1) ·
g2(XI2)···gk(XIk) and the gi’s are indecomposable. It is not diﬃcult to
see that every polynomial has a unique factorization to indecomposable
factors, up to multiplication by ﬁeld elements. The problem of polyno-
mial decomposition is deﬁned in the following way: Given an arithmetic
circuit from some circuit class M computing a polynomial f, we have
to output circuits for each of the indecomposable factors of f. If we only
have a black-box access to f then we have to output a black-box for
each of the indecomposable factors of f. We refer to this as black-box
polynomial decomposition. Clearly, ﬁnding the indecomposable factors
of a polynomial f is an easier task than ﬁnding all of the irreducible

4.9 Relation to Other Problems
355
factors of f. However, for the natural class of multilinear polynomials
these two problems are in fact the same.
The problem can be considered in its decision version as well. That
is, given an arithmetic circuit computing a multivariate polynomial
decide whether the polynomial is decomposable or not. In the deci-
sion version the algorithm just has to answer “yes” or “no” and is not
required to ﬁnd the decomposition.
Many randomized algorithms are known for factoring multivariate
polynomials in the black-box and white-box settings (see, e.g.,
Refs. [72, 146, 147]). These algorithms clearly imply randomized algo-
rithms for the decomposition problem. Similar to the case of PIT, it is a
long-standing open question whether there is an eﬃcient deterministic
algorithm for factoring multivariate polynomials (see Refs. [79, 147]).
Moreover, there is no known deterministic algorithm for the decision
version of the problem and not even for the simpler case of multilinear
polynomials. The authors of Ref. [128] showed that this is not a coinci-
dence. Namely, they show that PIT and decomposing are closely related
tasks. One direction is quite easy — showing that deterministic decom-
posing (even in its decision version) implies deterministic PIT. In what
follows M is some ﬁxed model of arithmetic circuits, for example, M
can be the class of multilinear formulas, depth-4 circuits and so forth.
Observation 4.6. Assume that there is a deterministic algorithm for
the decision version of the polynomial decomposition problem, that is,
an algorithm that when given access (explicit or via a black-box) to a
size s degree r circuit C ∈M runs in time T(s,r) and outputs “yes”
iﬀthe polynomial computed by C is decomposable. Then there is a
deterministic algorithm that runs in time O(T(s + 2,r)) and solves the
PIT problem for size s degree r circuits from M.
Proof. Let C be an arithmetic circuit. Consider C′ def
= C + y · z where
y,z are new variables. Note that C′ is decomposable iﬀC ̸≡0 (in addi-
tion, C′ is multilinear iﬀC is).
The other direction is more involved and in fact we may need a PIT
algorithm for a slightly larger class of circuits than the circuit that

356
Polynomial Identity Testing
we wish to decompose. Given a circuit class M, the circuit class MV
is deﬁned as follows. The class MV contains all circuits of the form
C1 + C2 × C3 where C1,C2,C3 ∈M and C2,C3 are variable disjoint.
Theorem 4.45 ([128]). Assume that there is a deterministic algo-
rithm that when given access (explicit or via a black-box) to a circuit
C ∈MV , of size 3s and degree r, runs in time T(s,r) and decides
whether C ≡0. Then, there is a deterministic algorithm that when
given access (explicit or via a black-box) to a size s degree r circuit C′
from M, runs in time O(n3 · r · T(s,r)) and decomposes C′. Moreover,
each decomposable factor belongs to M and is of size at most s.
The theorem says that in order to ﬁnd the decomposable factors of
circuits from M it suﬃces to have a PIT algorithm for a slightly larger
family of circuits MV . In fact, for most natural circuit classes, MV is
the same class as M, e.g., when M is the class of multilinear formulas.
On the other hand, when M is the class of ROFs then the theorem
requires a PIT algorithm for the sum of two ROFs. It is an interesting
question to prove a similar result using a PIT for M itself and not for
a larger class.
Proof. Let f be a polynomial that is computed by a size s degree r
circuit C ∈M. The proof actually gives an algorithm that returns a
partition of [n] to I1,...,Ik such that the decomposition of f is f =
h1(XI1) · ··· · hk(XIk) for some indecomposable polynomials h1,...,hk.
We call the partition I = {I1,...,Ik} the variable-partition of f. The
algorithm has the following four steps.
(1) Find a justifying assignment a to f.
(2) Recursively, ﬁnd the variable-partition I′ of f|xn=an.
(3) For every set I ∈I′, use the PIT to check whether C(a) ·
C ≡C|xI=aI · C|x[n]\I=a[n]\I. If this is the case then add I
to I. If it is not the case, then move to the next I. At
the end, add the remaining elements to I. Namely, I ←
I ∪{[n] \ ∪I∈II}.
(4) For every I ∈I, let hI = f|x[n]\I=a[n]\I and α = f(a)1−|I|.
Output f = α
I∈I hI.

4.9 Relation to Other Problems
357
We now explain why the algorithm works. Assume that the ﬁrst
step works, that is, that we found a justifying assignment a for f (this
is given to us by assumption). The proof is by induction on n. The
case n = 1 is trivial as a univariate polynomial is indecomposable. Now
assume that n > 1. Let f = h1(XI1) · ··· · hk−1(XIk−1) · hk(XIk) be the
decomposition of f, where I = {I1,...,Ik} is its variable-partition.
Assume without loss of generality that n ∈Ik. Consider f′ = f|xn=an.
It holds that f′ = h1 · ··· · hk−1 · g1 · g2 · ··· · gℓwhere the gi’s are the
indecomposable factors of hk|xn=an. Denote by Ik = {Ik,1,...,Ik,ℓ} the
variable-partition of hk|xn=an. As a is a justifying assignment of f,
we obtain that var(f′) = {xi : i ∈[n −1]}. From the uniqueness of the
decomposition and by the induction hypothesis, the recursive applica-
tion of the algorithm on f′ returns the variable-partition of f′, that is,
I′ = {I1,...,Ik−1,Ik,1,...,Ik,ℓ}.
Lemma 4.46. Let F ∈F[x1,...,xn] be a polynomial and let a ∈Fn be
a justifying assignment for it. Then for every I ⊆[n] of size 0 < |I| < n,
we have F(a) · F ≡F|xI=aI · F|x[n]\I=a[n]\I iﬀI is a disjoint union of
sets from the variable-partition of F.
Proof. Assume that equality holds. Since F is a-justiﬁed, both F|xI=aI
and F|x[n]\I=a[n]\I are nonzero, and hence F(a) ̸= 0. Consequently, if
we deﬁne g(X[n]\I) = F|xI=aI and h(XI) = (F|x[n]\I=a[n]\I)/F(a), we
obtain that F = g(X[n]\I) · h(XI). The uniqueness of the decomposition
of F thus implies that I is a disjoint union of sets from the variable-
partition of F.
To prove the other direction notice that we can write F ≡
g(X[n]\I) · h(XI) for two polynomials g and h. This implies F|xI=aI ≡
g(X) · h(a) and similarly F|x[n]\I=a[n]\I ≡g(a) · h(X). Hence, F(a) ·
F ≡g(a) · h(a) · g(X) · h(x) ≡F|xI=aI · F|x[n]\I=a[n]\I.
The lemma tells us that the third and fourth steps of the algorithm
indeed ﬁnd the decomposition of f. To ﬁnish the proof, we now analyze
the running time of the algorithm. An analog of Lemma 4.38 shows
that, given a PIT algorithm for circuits of the form Cxi=1 −Cxi=0,
that runs in time T(s,r), when C ∈M is of size s and degree r, we

358
Polynomial Identity Testing
can ﬁnd a justifying assignment for C in time O(n3 · r · T(s,r)). We
omit the proof of this extension and the interested reader can ﬁnd it
in Ref. [126]. Once we have a justifying assignment it is not diﬃcult
to see that the recursion runs in time O(n2 · T(s,r)). Thus, the total
running time can be bounded from above by O(n3 · r · (T(s,r))).
We note two interesting aspects of Theorem 4.45. The ﬁrst is that
the complexity of a decomposable factor of Φ is the same as the com-
plexity of Φ. I.e., even when Φ comes from a restricted family of circuits
we are guaranteed that its decomposable factors belong to the same
restricted family. This is not known for the case of general factors, see,
e.g., Open Problem 19. The second interesting aspect is that the the-
orem transforms a PIT algorithm for a weak class to decomposition
of a slightly weaker class. In general, similar to Theorem 4.9, one can
expect that a strong lower bound for arithmetic circuits will lead to
strong derandomization results. However, it is not clear that a lower
bound for a weak circuit class (like the one that follows from assum-
ing the existence of a polynomial time black-box PIT algorithm, see
Theorem 4.8) can help in derandomization, even for weaker classes.
4.9.2
Read-Once Testing
Consider the following generalization of the PIT problem. Given a poly-
nomial, either explicitly by a circuit or as a black-box, decide whether it
computes an ROP. Namely, decide whether there is a read-once formula
computing it. Following [126] we refer to this problem as the read-
once testing problem (ROT for short). Surprisingly enough, Shpilka
and Volkovich [126] proved that this problem is (roughly) equivalent to
PIT. That is, there is a deterministic algorithm for PIT iﬀthere is a
deterministic algorithm for ROT.
As in Theorem 4.45 we need a PIT algorithm for a slightly larger
circuit class than the one we wish to do ROT for. In what follows we
assume that M is some circuit class such that there is a deterministic
PIT algorithm for the circuit class MV (deﬁned as before).
Theorem 4.47.
There is a deterministic algorithm that, given
an n-variate size s degree r circuit C ∈M, runs in time poly

4.10 Concluding Remarks
359
(n,r,s,T(s,r)), where T(s,r) is the cost of a single PIT algorithm for
size s degree r circuits from MV , and outputs “yes” iﬀC is an ROP.
Proof. [Sketch] The algorithm consists of the following steps.
(1) Find a justifying assignment a for the polynomial computed
by C. This can be done in time O(n3 · r · T(s,r)) by a gen-
eralized version of Lemma 4.38.
(2) Reconstruct an ROF from the justifying assignment a. In
Ref. [55] it was shown that given a black-box access and a
justifying assignment for a polynomial f, one can construct,
in time poly(n), a read-once formula Φ such that if f is an
ROP then Φ computes f. At this stage we have Φ and all
we have to do is verify whether C and Φ compute the same
polynomial.
(3) Verify that C ≡Φ. The idea is to recursively ensure that
every gate v of Φ computes the same polynomial as a
“corresponding” restriction of C. To give a rough sketch,
consider v, the root of Φ. Denote Φ = α · (Φv1 ⋆Φv2) + β,
where v1,v2 are the two children of v and ⋆∈{+,×}.
Assume that the variables of vi are Ii. The two sets I1 and
I2 are disjoint. Consider the circuit C1 = C|xI2=aI2. Simi-
larly deﬁne C2, and the ROFs Φ1 and Φ2. Recursively ver-
ify that Ci ≡Φi. The only thing left is to verify that indeed
C ≡α · (C1 ⋆C2) + β. This can be done as we have a PIT
algorithm for circuits in MV which is exactly what we need.
This step requires O(n · T(s,r)) time.
4.10
Concluding Remarks
In this chapter we surveyed most of the known deterministic identity
testing algorithms. We saw that the main question is to ﬁnd black-
box algorithms for depth-4 circuits. We also saw that no algorithm
is known even for ΣΠΣ circuits and that current techniques can deal
with circuits with a bound on either (1) the top fan-in or (2) the num-
ber of times a variable is read or (3) the number of diﬀerent functions

360
Polynomial Identity Testing
appearing in a multiplication gate. The only exception to this is the
white-box algorithm of Raz and Shpilka [106] that handles the case
of set-multilinear ΣΠΣ circuits. We think that the ﬁrst step toward a
better understanding of ΣΠΣ circuits (and hopefully ΣΠΣΠ circuits) is
answering Open Problem 27. Namely, giving an eﬃcient deterministic
black-box identity testing algorithm for set-multilinear ΣΠΣ circuits.
Another possible approach to ΣΠΣ circuits is obtaining PIT algorithms
for the symmetric model discussed at the end of Section 3.5: decide
whether a given polynomial f of the form f = σr,m(ℓ1,...,ℓm) is equiv-
alently zero, where σr,m is the degree r elementary symmetric polyno-
mial in m variables and the ℓi’s are linear functions in x1,...,xn. This
question is open even in the white-box model.
Open Problem 30. Give a deterministic PIT algorithm for polyno-
mials of the form f = σr,m(ℓ1,...,ℓm).
At the end of the chapter we saw a connection between PIT and
polynomial decomposition. It is interesting to understand the relation
between problems in RP. In particular it may be the case that deran-
domizing PIT implies derandomization of the general polynomial fac-
torization problem. Note, however, that a decomposable factor of a
circuit Φ has essentially the same complexity as Φ, even when Φ is,
say, a sum of read-once formulas or a bounded depth circuit, etc. In
the case of general factorization no such result is known, see Open
Problem 19.

5
Reconstruction of Arithmetic Circuits
The reconstruction problem for arithmetic circuits is deﬁned as follows.
We are given a black-box “holding” an arithmetic circuit, that belongs
to a predetermined family of circuits, e.g., bounded depth circuits,
multilinear circuits, read-once formulas, etc., and that computes some
multivariate polynomial f. We are allowed to ask for the value of f
on inputs of our choice. We then have to come up with an arithmetic
circuit computing the same polynomial of roughly the same complexity
as the black-box circuit. The goal is to minimize the number of queries
and running time required for constructing the circuit.
A reconstruction algorithm is eﬃcient if its running time is
polynomial in the circuit complexity of the black-box polynomial.
This problem is the algebraic analog of the learning with mem-
bership and equivalence queries problem from computational learn-
ing theory of Boolean functions.1 In the model of membership and
equivalence queries, the learner can query the function on inputs of
her choice (membership queries). In addition whenever the learner
constructs a hypothesis she can ask for a counterexample, if one exists
(an equivalence query). Namely, an input x such that the hypothesis
1 For a good background on computational learning theory, see Ref. [83].
361

362
Reconstruction of Arithmetic Circuits
and the underlying function disagree on. Eventually the learner has to
come up with a hypothesis that computes the function exactly. In the
arithmetic setting equivalence queries can be replaced by membership
queries, as by picking an input at random we can tell any two diﬀerence
polynomials apart.
A closely related notion is that of interpolation of a polynomial.
In the interpolation problem the goal of the learner is to ﬁnd the
coeﬃcients of the underlying polynomial using membership queries.
While this problem is closely related to the reconstruction problem it
is not exactly the same. Note, however, that for sparse polynomials
(ΣΠ circuits) interpolation and reconstruction are the same problem.
In the Boolean world, the PAC learning model is more often con-
sidered than the model of learning with membership and equivalence
queries. In the PAC model the learner gets examples of the form
(x,f(x)) where the input x is drawn from some unknown distribu-
tion D on inputs. She then has to come up with a hypothesis that
agrees with f on most inputs, according to D. In the arithmetic world,
however, we believe that the reconstruction problem is more natural to
study than the PAC learning one. One reason is that it seems natural
to think of the input for a polynomial as coming from the uniform dis-
tribution and not from some “strange” distribution that PAC learning
algorithms have to deal with. Another reason is that any two polyno-
mials are far from each other, so any “reasonable” learning algorithm
should compute the polynomial exactly.
It is clear that PIT algorithms are strongly related to reconstruc-
tion algorithms. E.g., a black-box algorithm provides a test set for the
underlying class of circuits, that is, a set of points that distinguishes
any two diﬀerent circuits from the class. In other words, black-box PIT
algorithms provide a set of points with the property that evaluating a
circuit from the underlying class on all the points in the set, completely
determines the circuit. The problem remains: “how to reconstruct the
circuit from the values that it obtains on a test set?”
Currently, reconstruction algorithms are known for depth-2 circuits,
see Refs. [18, 49, 51, 86] and references within, for depth-3 circuits with
bounded top fan-in [77, 125] and for set-multilinear noncommutative
formulas [17, 85]. It is an interesting question whether there is a generic
way of transforming a black-box PIT algorithm to a reconstruction

5.1 Hardness of Reconstruction
363
algorithm. On the face of it the answer should be negative as a ran-
dom set should be a good test set and there is no clear way of using
a random set of points to generate circuits. However, by considering
PIT algorithms of a certain type, like the ones suggested by Agar-
wal [2] and Shpilka and Volkovich [127], it seems possible that one
would be able to transform a successful PIT algorithm to a reconstruc-
tion algorithm. Consider, for example, the case of read-once formulas
(ROFs). Although a weak model, it received a lot of attention in the
learning community and several randomized learning algorithms were
devised for it Refs. [26, 27, 28, 55]. In Shpilka and Volkovich [126, 127] a
PIT algorithm for sums of read-once formulas was given. Consequently,
Shpilka and Volkovich [127] obtained a deterministic quasi-polynomial
time learning algorithms for read-once formulas. However, the problem
of learning a sum of read-once formulas, even the sum of two read-once
formulas, is still open.
Open Problem 31. Give sub-exponential reconstruction algorithm
for the sum of a small number of read-once arithmetic formulas.
Although there is a clear relation between PIT and reconstruc-
tion, it may be the case that we will have randomized reconstruction
algorithms without a corresponding deterministic PIT algorithm. For
example, in Refs. [26, 27, 28, 55] randomized polynomial time recon-
struction algorithms for ROFs were given, much before the designed
of PIT algorithms for it. In addition, the PIT algorithm of Shpilka
and Volkovich [127], that was discovered later, only implies a quasi-
polynomial time reconstruction algorithm due to the size of the hitting
set. We note however that the algorithm of [127] has the advantage of
being deterministic whereas the previous algorithms [26, 27, 28, 55] are
all randomized.
We now explain the main ideas behind some of the known recon-
struction algorithms and discuss some known hardness results.
5.1
Hardness of Reconstruction
By experience, reconstruction algorithms exist only for classes for which
strong lower bounds are known. It is thus natural to ask whether

364
Reconstruction of Arithmetic Circuits
eﬃcient reconstruction algorithms imply strong lower bounds for the
underlying circuit class. If we could show such a result then we will
immediately get that for classes for which lower bounds are hard to
prove (like depth-4 circuits), one cannot expect to obtain an eﬃcient
reconstruction algorithm. In the Boolean world such results were ﬁrst
proved by Valiant [142] (for more on hardness of learning see Ref. [87]
and references within). In the arithmetic world much less is known.
This is mostly due to the fact that most cryptographic assumptions
that were used to prove hardness of learning do not have counterparts
in the arithmetic world (recall the discussion in Section 3.9). Neverthe-
less, recently some hardness of reconstruction results were proved for
arithmetic circuits as well.
In Ref. [87], Klivans and Sherstov proved that assuming the hard-
ness of the shortest vector problem (SVP) for quantum algorithms, no
PAC learning algorithm exists for ΣΠΣ circuits. The f(n)-SVP ques-
tion is the problem of computing an f(n) multiplicative approximation
to the length of the shortest vector in a given lattice in Rn. Regev
constructed a new public-key cryptosystem based on the assumption
that ˜O(n1.5)-SVP is hard for quantum computers [113]. Klivans and
Sherstov observed that the decrypting function for Regev’s public-key
encryption scheme can be computed by small ΣΠΣ circuits. Using a
lemma of Kearns and Valiant [82] that basically says that if a decryp-
tion function of a secure public-key cryptosystem can be computed by
a circuit class C then C does not have eﬃcient learning algorithms,
Klivans and Sherstov [87] concluded that no eﬃcient PAC learning
algorithms exist for ΣΠΣ circuits (assuming the security of Regev’s
public-key cryptosystem).
Theorem 5.1 ([87]). Assume that polynomial-size ΣΠΣ arithmetic
circuits are PAC-learnable in polynomial time. Then there is a
polynomial-time quantum solution to ˜O(n1.5)-SVP.
While this theorem provides a hardness of PAC learning result for
ΣΠΣ circuits, it does not say much about the general reconstruction
problem. The main diﬀerence between these problems is that in the
case of reconstruction we are allowed to ask for the value of the circuits

5.1 Hardness of Reconstruction
365
on inputs of our choice. Furthermore, in the reconstruction problem we
usually consider inputs coming from the uniform distribution. On the
other hand, in the PAC model the goal is to learn the circuit using ran-
dom examples that come from some unknown distribution. Speciﬁcally,
the adversary is allowed to choose a distribution that is supported on
a “strange” set of inputs. Thus, giving a PAC learning algorithm that
works for any distribution may be a more diﬃcult task than recon-
structing the circuit.
Fortnow and Klivans [46] show hardness of learning results for arith-
metic circuits and formulas using ideas similar to those of Kabanets and
Impagliazzo [69].
Theorem 5.2 ([46]). Let C be a family of polynomial size arithmetic
formulas. Assume that C is exactly learnable from membership and
equivalence queries in polynomial time and that the hypothesis of
the learner is an arithmetic formula. Then there exists a polynomial
f ∈EXPRP such that f ̸∈C. If we allow both C and the hypothesis
to be arithmetic circuits then there exists an f ∈ZPEXPRP such that
f ̸∈C.
When saying f ∈EXPRP, etc. we mean that there is an EXPRP
machine that computes f (when the input is given in bits).
Proof. [Sketch] The idea of the proof is similar to the idea behind
Theorem 4.7. Assume that there is a learning algorithm for C, using
membership and equivalence queries. Assume further that permanent
is computable by a polynomial size circuit from C. We shall show, using
the self-reducibility of permanent, that this implies that permanent is in
ZPPRP. This is done by an iterative construction of circuits for PERMm,
permanent of m × m matrices, as follows. When m = 1 this is trivial.
Assume now that we constructed a circuit for PERMm. To construct a
circuit for PERMm+1, run the learning algorithm for C with PERMm+1
as the objective polynomial. The problem is, of course, to answer mem-
bership and equivalence queries. Using the circuit for PERMm, we can
easily answer membership queries for PERMm+1. Equivalence queries
can be simulated using randomness: since the constructed circuit is

366
Reconstruction of Arithmetic Circuits
small, its degree is not too high, which implies that a random input
will be a counterexample. We can thus learn a small circuit for PERMn.
To conclude the proof we note the following. If permanent has
polynomial size circuits from C and if EXP ⊆C (otherwise there is
nothing to prove), then, as in the proof of Theorem 4.7, permanent is
complete for EXP. Hence EXP ⊆ZPPRP. By padding this implies that
EE ⊆ZPEXPRP (where EE = DTIME(22O(n))). As EE contains functions
with super-polynomial circuit complexity, so does ZPEXPRP.
We end this section by asking whether a similar result can be proved
for VNP.
Open Problem 32. Prove that if an arithmetic circuit class C is
learnable then the permanent does not have polynomial size circuits
from C.
5.2
Interpolation of Sparse Polynomials
Similar to the PIT algorithms from Section 4.4, many diﬀerent interpo-
lation algorithms were designed for the class of sparse polynomials (see,
e.g., Refs. [18, 51, 86] and references within). As before, we describe
the approach of Klivans and Spielman [86]. The PIT algorithm given
in Section 4.4 can be easily extended to an interpolation algorithm.
Assume that the unknown polynomial is f(x1,...,xn) with deg(f) < r.
Recall the substitution xi = yki mod p, for a large enough prime p. The
proof of Theorem 4.15 shows that for some k, each monomial of f is
mapped uniquely to a monomial of ˆf def
= f(y,yk1 mod p,...,ykn−1 mod p).
By evaluating this polynomial on deg( ˆf) + 1 points we can recover
all the coeﬃcients of f. It remains to ﬁnd the monomials corre-
sponding to each of the coeﬃcients. To do so let γ ̸∈{0,1} be
some ﬁeld element. Consider the polynomials fi
def
= f(x1,...,xi−1,γ ·
xi,xi+1,...,xn). As before, we can interpolate the polynomials !fi =
fi(y,yk1 mod p,...,ykn−1 mod p). The important observation is that the
monomials of !fi are the same as the monomials of ˆf and the only
diﬀerence is that the coeﬃcients of the monomials that resulted from
monomials containing xi changed. By comparing the coeﬃcients of !fi

5.3 Learning via Partial Derivative
367
and those of ˆf, we can easily recover the monomials of f, thus obtaining
the following theorem.
Theorem 5.3 ([86]). In time polynomial in n,m,r, and log(|F|), we
can output a test set of point that given the values of a degree r
n-variate polynomial over F with at most m monomials at every point
in the set, we can ﬁnd the coeﬃcients and monomials of the polyno-
mial in polynomial time. If F = R, then every entry of each point in the
set has bit length at most O(log(nr)). When F is ﬁnite of size smaller
than (nr)6 the entries of the points come from the smallest algebraic
extension ﬁeld of F of size at least (nr)6.
5.3
Learning via Partial Derivative
Beimel et al. [17] gave learning algorithms that use membership and
equivalence queries for several classes of Boolean functions. The output
of the algorithm is a multiplicity automaton (see deﬁnition below). They
mainly considered Boolean function classes but they also applied their
algorithms to the class of low degree polynomials over ﬁnite ﬁelds,
and even to a model that can be thought of as set-multilinear depth-
3 circuits. In Ref. [85] it was noticed that this learning algorithm can
actually learn any class of arithmetic circuits that compute polynomials
whose space of partial derivatives has low dimension. In this section we
deﬁne the model of multiplicity automata, give the algorithm of Beimel
et al. [17] and explain the connection to partial derivatives.
Deﬁnition 5.1. A multiplicity automaton A of size s over F consists
of a vector γ = (γ1,...,γs) ∈Fs and a set of matrices {Aσ}σ∈F, where
each Aσ is an s × s matrix over F. The output of A on input x =
(x1,...,xn) ∈Fn is deﬁned to be the ﬁrst coordinate of the vector2
(1
i=n Axi)γ.
Intuitively, each matrix Aσ corresponds to the transition matrix
of the automaton for the symbol σ ∈F. Iterative matrix multiplication
2 We denote Axn · Axn−1 · ··· · Ax1 by 1
i=n Axi.

368
Reconstruction of Arithmetic Circuits
keeps track of the weighted sum of paths from state i to state j. The ﬁrst
row of the iterated product corresponds to transition values starting
from the initial state and γ determines the acceptance criteria. This
deﬁnition can be extended to any input length and not just to n-tuples,
but we restrict our discussions to inputs from Fn.
Assume that a polynomial f(x1,...,xn) with individual degrees
bounded by r can be computed by a multiplicity automaton of size s.
Let α0,...,αr be distinct ﬁeld elements. Let A(z) be the unique
s × s matrix whose entries are degree r polynomials in the variable z
such that for every αi it holds that A(αi) = Aαi. One can construct
A(z) by interpolation. Observe that this implies that f(x1,...,xn) =
(A(xn)···A(x1) · γ)1. This representation immediately connects multi-
plicity automata to read-once oblivious algebraic branching programs.
Speciﬁcally, f can be computed by an ABP of width s, such that the
only variable labeling edges between the i-th level and the i + 1’st level
is xi. An important notion related to multiplicity automata is that of
Hankel matrices.
Deﬁnition 5.2.
Let f : Fn →F be a polynomial. We construct a
matrix H whose rows and columns are indexed by strings3 in F≤n
in the following way. For a string x, deﬁne |x| to be the length of x,
for example, |01| = 2. For two strings x,y, deﬁne the (x,y) entry of H
to be f(x ◦y) if |x| + |y| = n and 0 otherwise. The resulting matrix H
is called the Hankel matrix of f. Deﬁne Hk to be the k-th block of H,
i.e., Hk is the sub-matrix of H deﬁne by all rows x so that |x| = k and
all columns y so that |y| = n −k.
The following key fact relates the rank of the Hankel matrix of a
polynomial with the size of a multiplicity automaton computing it.
Theorem 5.4. The rank of the Hankel matrix of f (over F) is equal
to the size of the smallest multiplicity automaton computing f.
3 The set F≤n is the set of strings of length at most n over the alphabet F. For practical
purposes, if F is too large then we can consider the Hankel matrix with respect to strings
in S≤n for some S ⊆F. As long as |S| > deg(f) the rest of the discussion remains the
same.

5.3 Learning via Partial Derivative
369
Proof. [Sketch] We only prove one direction here as it will be the one
used in our algorithms. Let H be the Hankel matrix of an n-variate
polynomial f and assume that rank(H) = s. Let a1,a2,...,as ∈F≤n be
such that the rows indexed by them form a basis to the row space of H
(we can assume that f ̸= 0). Without loss of generality, assume that a1
is the empty string, denoted a1 = ε. Set γ = (f(a1),...,f(as)) where if
|ai| ̸= n then we set f(ai) = 0. For every σ ∈F, deﬁne the matrix Aσ as
follows: the i-th row of Aσ is the unique vector (α1,...,αs) satisfying
Hai◦σ = 
i∈[s] αi · Hai, where Hw is the row of H indexed by w. One
can now prove by induction on |w| that for every w ∈F≤n it holds that
(Aw · γ)i = f(ai ◦w).
The learning algorithm works in stages. In each stage it learns
another row of the Hankel matrix that is independent of the rows
learned so far. When a basis to the row space of the matrix is obtained
the algorithm constructs a multiplicity automata that has the same
Hankel matrix as the underlying polynomial, which is what we are after.
More speciﬁcally, at the beginning of the k-th iteration, the algorithm
holds a set of rows X = {x1,...,xk} ⊆F≤n and a set of columns Y =
{y1,...,yk} ⊆F≤n. For a string z, let ˆHz be the restriction of the row
Hz to the k coordinates in Y . Given z and Y , the vector ˆHz can be com-
puted using k = |Y | membership queries. It will hold that ˆHx1,..., ˆHxk
are linearly independent. Using these vectors the algorithm constructs
a hypothesis h, in a manner similar to the proof of Theorem 5.4, and
asks an equivalence query. A counterexample to h leads to adding a
new element to both X and Y , in a way that preserves the above
properties. This immediately implies that the number of iterations is
bounded by rank(H). We shall now give a more detailed description of
the algorithm. To simplify the algorithm we redeﬁne f so that f(ε) = 1
(f remains zero on all other strings of length diﬀerent than n).
(1) Set x1 = y1 = ε, X = Y = {ε} and k = 1. At this point, ˆHε
has one coordinate which equals 1.
(2) Construct a hypothesis h, as in the proof of Theo-
rem 5.4. Namely, set γ = (f(x1),...,f(xk)). For every4
4 Actually, if f has degree at most r in each variable then we only need to do so for all
σ ∈S where S ⊆F is of size r + 1.

370
Reconstruction of Arithmetic Circuits
σ ∈F, construct the matrix ˆAσ as follows: its i-th row is
the coeﬃcients of the vector ˆHxi◦σ when expressed as a lin-
ear combination of the vectors ˆHx1,..., ˆHxk. This is pos-
sible as ˆHx1,..., ˆHxk are linearly independent vectors in
Fk. These matrices, together with γ, deﬁne a multiplicity
automaton and our hypothesis h is the function computed
by this automaton.
(3) Ask whether h = f. If the answer is YES then halt and
output h. Otherwise the answer is NO and we obtain
a counterexample z (this is the equivalence query). We
now ﬁnd a preﬁx w ◦σ of z such that there exist con-
stants α1,...,αk ∈F satisfying 
i∈[k] αi ˆHxi = ˆHw but for
some y ∈Y it holds that 
i∈[k] αi ˆHxi(σ ◦y) ̸= ˆHw(σ ◦y).
Namely, the way in which the row w is spanned by
ˆHx1,..., ˆHxk, cannot be extended to the column σ ◦y. Now,
set X ←X ∪{w} and Y ←Y ∪{σ ◦y}. Repeat Step 2.
The correctness of the algorithm is implied by the following two
claims that we leave to the reader. The proof of the ﬁrst claim is by
induction on the structure of z and it uses essentially the same kind of
arguments as the proof of Theorem 5.4.
Claim 5.5. Let z be a counterexample to h found in Step 5.3 (i.e.,
f(z) ̸= h(z)). Then, there exists a preﬁx w ◦σ having the required
properties.
The second claim follows by construction.
Claim 5.6. At each step the rows ˆHx1,..., ˆHxk are linearly indepen-
dent.
Concluding, we obtain the following theorem.
Theorem 5.7 ([17]). Let f : Fn →F be a polynomial of individual
degrees bounded by r such that rank(H(f)) = R over F, where H(f) is

5.3 Learning via Partial Derivative
371
the Hankel matrix of f. Then, f is exactly learnable by the above algo-
rithm in time poly(n,r,R) from equivalence and membership queries.
To explain the relation between Hankel matrix and partial deriva-
tives we introduce some notations. For a function f(x1,...,xn)
and k ≤n, deﬁne ∂k(f) = { ∂f
∂M | M is a monomial in x1,...,xk} and
rankk(f) = dim(span(∂k(f))).
Theorem 5.8([85]). Let f(x1,...,xn) be a degree r polynomial. Then
for every k ≤n, the k-th block of the Hankel matrix H(f) of f admits
rank(Hk(f)) ≤rankk(f). If f is multilinear then equality holds.
The proof of the theorem is quite easy and relies on Taylor expansion
of a polynomial and therefore we omit it. Combining Theorems 5.7 and
5.8 we get learning algorithms for several circuit classes.
Theorem 5.9 (Learnability of depth-3 circuits). Let f be com-
puted by a set-multilinear ΣΠΣ circuit with s product gates over
n variables. Then f is learnable, from membership and equivalence
queries, in time poly(n,s). If f is computed by a ΣΠΣ circuit with s
multiplication gates each of degree at most r then f is learnable in time
poly(n,2r,s).
The prove of the theorem is immediate once we notice the following.
In the case of set-multilinear circuits, 
k∈[n] rankk(f) ≤ns. In the case
of general ΣΠΣ circuits, 
k∈[n] rankk(f) ≤2rns.
Finally, we obtain a learning algorithm for set-multilinear noncom-
mutative formulas (this generalizes the ﬁrst case of Theorem 5.9).
Theorem 5.10. Let f(X1,...,Xr) be a set-multilinear polynomial in
the variables X1,...,Xr, the size of each |Xi| is n. Assume that f is
computed by a set-multilinear noncommutative formula of size s. Then
f can be learned in time poly(n,r,s).
The proof follows since 
k∈[n] rankk(f) ≤ns (see the proof of
Theorem 3.8 in Section 3.4).

372
Reconstruction of Arithmetic Circuits
5.4
Reconstruction of Depth-3 Circuits
So far we have seen learning algorithms for classes of circuits com-
puting polynomials whose partial derivatives span a low dimensional
space (sparse polynomials have this property as well). In this section
we will give an example of a reconstruction algorithm for the class
of ΣΠΣ(k) circuits. This class can compute polynomials whose par-
tial derivatives space has an exponentially large dimension. For exam-
ple, the polynomial f = (x1 + x2) · (x3 + x4)···(xn−1 + xn) satisﬁes
rankk(f) ≥
n/2
k

.
The reconstruction algorithm presented returns a ΣΠΣ circuit.
When the “original” circuit is multilinear, the algorithm outputs a mul-
tilinear ΣΠΣ(k) circuit and runs in polynomial time. When the circuit
is not multilinear, the algorithm returns a ΣΠΣ circuit with quasi-
polynomially many multiplication5 gates and runs in quasi-polynomial
time. This algorithm is very diﬀerent from the previous ones and it
heavily relies on Theorem 4.22.
Theorem 5.11 ([77]). Let f be an n-variate polynomial computed
by a ΣΠΣ(k) circuit of degree d over F. Then there is a reconstruction
algorithm for f, that runs in time poly(n) · exp(log(|F|) · log(d)O(k3)),
and outputs a ΣΠΣ(K,d) circuit for f, where K = exp(log(d)k). When
|F| = O(d5) the algorithm may ask queries from a polynomial size alge-
braic extension ﬁeld of F.
In the case that f can be computed by a multilinear ΣΠΣ(k) circuit,
there is a reconstruction algorithm that runs in time (n + |F|)2O(k log k)
and outputs a multilinear ΣΠΣ(k) circuits for it. When |F| < n5 The
algorithm may ask queries from a polynomial size algebraic extension
ﬁeld of F.
The rest of the section is organized as follows. First we will explain
the idea behind the algorithm of Shpilka [125] that works for circuits
with two multiplication gates (k = 2) and then the extension of Ref. [77]
that works for the case of k = O(1) multiplication gates.
5 By generalizing the notion of a multiplication gate one can view the output of the algorithm
as a sum of k generalized multiplication gates (see Ref. [77]).

5.4 Reconstruction of Depth-3 Circuits
373
5.4.1
The Case k = 2
Let us start with the simplest case k = 1. Here the circuit is simply a
product of linear functions so all we need to do is to ﬁnd its irreducible
factors. This is easily done by using the black-box factoring algorithm
of Kaltofen and Trager [73].
We would like to extend this approach for the case of two product
gates. Obviously, we run into diﬃculties as the circuit itself can be
irreducible. Furthermore, even if it is reducible then there is no reason
that we could recover the linear functions from its irreducible factors.
To overcome this diﬃculty [125] had the following idea. Let Φ = M1 +
M2 be the unknown circuit, each Mi is a product gate. Look for a linear
function ℓthat is a factor of M1 and not of M2 (if such a function exists).
Consider the circuit Φ|ℓ=0. This circuit has exactly one multiplication
gate M2|ℓ=0 and so we can recover all its irreducible factors. Now, this
still does not give us M2 but rather only its restriction to the space
{x : ℓ(x) = 0}. To overcome this, Shpilka [125] actually looks for many
diﬀerent functions ℓ1,...,ℓt that are linearly independent and divide
M1 and not M2. The algorithm then computes M2|ℓ1=0,...,M2|ℓt=0
and from these t diﬀerent circuits reconstructs M2. Once M2 is known
we can look at Φ −M2 and recover M1 using factoring.
There are, of course, many problems with this approach. The ﬁrst is
ensuring that such linear functions even exist. If the rank of the circuit
is small, for example, then they do not exist. The second problem is
that even if such functions exist, we need to ﬁnd them, and, after all,
the space of linear functions is exponentially large. The third problem
is that even if we overcome the ﬁrst two obstacles, we still need to
reconstruct M2 from the diﬀerent circuits M2|ℓi=0.
We now explain how to overcome each of these diﬃculties. As the
algorithm is very complicated and contains many details, we only sketch
the ideas and avoid small (yet delicate) points that are dealt with in
Ref. [125]. In what follows, to ease the reading, we assume that Φ is
simple, namely, that its product gates have no common factors. The
algorithm, of course, must deal with the non-simple case.
We ﬁrst explain how the algorithm works when the rank of Φ is
larger than O(log2(n)). We would like to ﬁnd linear functions ℓ1,...,ℓt,

374
Reconstruction of Arithmetic Circuits
t = O(logn), such that all the ℓi’s divide M1 but none of them divides
M2. To do this, instead of considering the problem over Fn, we restrict
the inputs to the circuits to some random linear space V ⊂Fn of dimen-
sion dim(V ) = D. It is not very diﬃcult to show that with high prob-
ability, Φ|V still has high rank and that the functions ℓi|V still do not
divide M2|V . Now, since the dimension of the space is rather small
we can actually “guess”6 the functions ℓ1|V ,...,ℓt|V . Hence, we can
assume that we have ℓ1|V ,...,ℓt|V . We can now factor each of the cir-
cuits M2|V,ℓ1|V =0,...,M2|V,ℓt|V =0. At this point a new problem arises:
how do we “glue” those gates together to get M2|V . Let us look more
closely at this question. By applying an invertible linear transforma-
tion, we can assume without loss of generality that ℓi = xi for every i.
What we actually have is M2|V , except that for each i ∈[t] someone
reveals all its linear factors but erases xi from it. The consequence of
setting xi = 0 is that diﬀerent linear functions can become identical
after this setting. It is not clear how to combine the diﬀerent factors of
the circuits M2|V,xi=0 together. Shpilka [125] overcomes this by show-
ing that (for this choice of t) there must exist xi and xj and a linear
factor L of M2|V such that the multiplicity of L in M2|V is equal to
its multiplicity in M2|V,xi=0 and to its multiplicity in M2|V,xj=0. This
means that by inspecting the factors of M2|V,xi=0 and M2|V,xj=0 we can
ﬁnd this function L (or some other function with the same property) as
follows. By considering the restriction L|xi=0 we can recover the coef-
ﬁcients of xj in L, and similarly by considering L|xj=0 we can recover
the coeﬃcients of xi (this does not work when L depends only on xi
and xj; this “bad” event happens with very small probability over the
choice of V ). Now we can remove L from M2|V and recursively learn
M2|V /L. The proof of the existence of such a function L is very similar
to the proof of Theorem 4.22 so we skip it. Now, given M2|V we can
ﬁnd M1|V by factoring the polynomial Φ|V −M2|V .
At the end of the above argument we have the circuit Φ|V . The
problem is of course ﬁnding the circuit Φ itself. Here we use the
6 By “guess” we mean that we go over all possibilities, for each one we run the reconstruction
algorithm, and at the end check which of the solutions that we found is the correct one
using a PIT algorithm.

5.4 Reconstruction of Depth-3 Circuits
375
structural theorem (Theorem 4.22) to claim that the circuit that we
found is unique. Namely, if Ψ = N1 + N2 is another ΣΠΣ(2) circuit
computing Φ|V then either N1 = M1|V or N2 = M1|V . This follows from
the observation that M1|V + M2|V −N1 −N2 ≡0 and the fact that
rank(Φ|V ) ≥R(4,r). Knowing that the circuit is unique, we can “lift”
it to Fn as follows. Repeat the learning algorithm and learn each of the
circuits Φ|Vi, where Vi = span(V ∪{ei}) and ei is the i-th basis vector.
Denote Φ|Vi = Mi
1 + Mi
2. The uniqueness of Φ|V guarantees that we can
simply “glue” together the linear functions in the diﬀerent Mi
1’s whose
restriction gives the same linear function in M1|V (as we picked V at
random, diﬀerent linear functions in Φ will be mapped to “far away”
functions in Φ|V ). This concludes the algorithm for the case that the
rank of Φ is high. In this case the algorithm actually returns a ΣΠΣ(2)
circuit.
We now turn to the low rank case that has a diﬀerent algorithm,
but of roughly the same structure. As before we restrict the circuit to
a random low dimensional space. Then we ﬁnd a representation of Φ|V
as a polynomial in a small set of linear functions. This can be done,
as the linear functions in the circuit have low rank. Assume that we
found a representation Φ|V = f(ℓ1,...,ℓρ). What we do next is ﬁnd a
representation of the form Φ|Vi = f(ℓi
1,...,ℓi
ρ), where ℓi
j|V = ℓj, and the
Vi’s are deﬁned as before. The requirement ℓi
j|V = ℓj guarantees that
the ℓi
j’s are “consistent” with each other. It is not hard to prove that
such a representation exists (given that ρ = rank(Φ)). Once we have the
ℓi
j’s, we can again combine them together and obtain a representation
Φ = f(L1,...,Lρ) for some linear functions. This is a very rough sketch
and the interested reader is referred to [125] for more details. In the
low rank case the algorithm does not return a ΣΠΣ(2) circuit.
In the case of multilinear ΣΠΣ(2) circuits, [125] gives a polynomial-
time algorithm. The main diﬀerence is that the “critical” rank is now a
constant and not poly-logarithmic. A delicate point is that the space V
cannot be random as the restriction of a multilinear polynomial to a
random subspace is usually not multilinear. Nevertheless, one can show
that instead of choosing a random V it is enough to set variables to
constants at random (leaving a constant number of variables “alive”).

376
Reconstruction of Arithmetic Circuits
As the dimension of the resulting subspace is constant, the algorithm
runs in polynomial time. It is also interesting that in this case the
algorithm actually returns a multilinear ΣΠΣ(k) circuit.
5.4.2
The Case of General k
We now explain the idea behind the algorithm of [77] for reconstruct-
ing ΣΠΣ(k) circuits. Before we describe their algorithm let us explain
the diﬃculties in extending the algorithm above for the case k > 2. The
algorithm for ΣΠΣ(2) circuits has the following three steps. (a) Restrict
the circuit to a low dimensional space. (b) Either learn one multiplica-
tion gate at a time in the case of high rank, or ﬁnd a representation of
the circuit as a function in a small number of linear forms in the case of
low rank. (c) “Lift” the circuit from the subspace to the whole space.
Several problems occur when trying to make this approach work when
we have many product gates. First of all, it may be the case that the
circuit is of high rank, yet some of its multiplication gates share many
common linear functions. For example, this can happen if the circuit is
a sum of a high rank circuit with a low rank circuit. In this case it is
not clear how to use the previous algorithm. Let us ignore this problem
for now and consider the next diﬃculty. Previously, in the high rank
case we learnt each product gate separately, by restricting the circuit
to a subspace on which one of the gates vanishes. It is not clear that
we can do the same when we have more than two gates. Finally, even if
we do learn Φ|V , it is not obvious that we can lift it as it involves both
“high rank” parts and “low rank” parts. Speciﬁcally, the representation
of Φ|V is not unique and therefore it is not clear that the circuits Φ|Vi
can be “glued” together (this is closely related to the ﬁrst problem that
we discussed).
We now explain how Karnin and Shpilka [77] handle these prob-
lems. The main idea is to deﬁne a more robust notion of a product
gate. They deﬁne a distance function between gates ∆(M1,M2) def
=
rank((M1 + M2)/gcd(M1,M2)). In words, the distance between two
gates is the rank of their sum after we remove their common linear
factors. As it turns out, ∆obeys the triangle inequality and so can be
viewed as giving a metric for product gates. Furthermore, it also make

5.4 Reconstruction of Depth-3 Circuits
377
sense to deﬁne (in the same manner) the function ∆(M1,...,Mt). They
use this metric to cluster product gates in a way that any two clus-
ters are far. That is, they ﬁrst partition [k] to t sets I1,...,It and then
deﬁne Φj = 
i∈Ij Mi. Clearly, Φ = 
j∈[t] Φj. The important property
that the Φj’s satisfy is that rank(Φj) is small but for any j1 ̸= j2, i1 ∈Ij1
and i2 ∈Ij2 it holds that ∆(Mi1,Mi2) is large. In words, the rank of
the sum of the gates in each of the partitions is small (after removing
their greatest common divisor) and any two clusters are far from each
other. An important conclusion of this deﬁnition is that it guarantees
uniqueness. Indeed, if Φ = 
j∈J Ψj is another representation of Φ as
a sum of at most k far clusters, then it must be the case that |J| = t
and after permuting the indices, Φj = Ψj. We call each Φj a gener-
alized multiplication gate (with this deﬁnition each low rank circuit
is a single generalized product gate). Another implication is that for
a random subspace V (of polylog dimension), with high probability
Φ|V = 
j∈[t] Φj|V is the unique representation of Φ|V as a sum of gen-
eralized multiplication gates. This property is very important because if
we can learn Φ|V then using the uniqueness we can hope to lift each Φj
separately to Fn as was done in the case k = 2.
The second problem that we need to deal with is how to learn Φ|V .
For this, Karnin and Shpilka [77] show that if we pick a subspace V
of a high enough dimension (polylog(n) suﬃces) at random then there
exists a reconstruction tree of depth at most k for Φ|V . A reconstruction
tree is a tree whose nodes are labeled with subspaces of V having the
following four properties. (1) The root is labeled with V . (2) If U is a
child of W then U is a subspace of W of co-dimension one. (3) The
subspaces in the children of each node span the subspace in the node.
(4) There exists a generalized product gate Φj of Φ such that for each
leaf U of the tree Φj|U ̸≡0, but all the other generalized product gates
vanish on U. Now, if {Ui} are the children of W then from {Φj|Ui}
we can reconstruct Φj|W , by going bottom up. Indeed, if U labels a
leaf then we can reconstruct Φj|U by simple factoring, as all the other
gates vanish on U. Then, we reconstruct Φj|U for all subspaces U that
label nodes just above the leaves (using similar ideas to the case k = 2),
and so forth. Eventually, we can reconstruct Φj|V . Then, we consider
Φ|V −Φj|V and repeat the same argument until we reconstruct the

378
Reconstruction of Arithmetic Circuits
other generalized multiplication gates. As in the case k = 2, we ﬁnd
the adequate reconstruction trees by simply trying out all possibilities.
As the dimension of V is poly-logarithmic and k is a constant going
over all trees takes quasi-polynomial time.
Once we ﬁnd Φ|V , using the fact that the representation is unique,
we can apply the same ideas as in the case k = 2 to lift the repre-
sentation to Fn. It is not hard to show that this algorithm outputs a
representation of Φ as a sum of at most k generalized multiplication
gates.
We shall not elaborate more on this algorithm but rather explain
why this can be viewed as a generalization of the algorithm for the case
k = 2. When a ΣΠΣ(2) circuit has a high rank then we view it as a sum
of two clusters (each gate is clustered only with itself). On the other
hand, in the low rank case we view the circuit as a single generalized
multiplication gate and then learn this gate. Thus, the algorithm for
reconstructing ΣΠΣ(2) circuits either returns two clusters in the case
of high rank or one cluster in the case of low rank.
In the case of multilinear ΣΠΣ(k) circuits, similar arguments are
used and again Karnin and Shpilka [77] manage to reconstruct such cir-
cuits in polynomial time (with k appearing in the exponent of course).
5.5
Concluding Remarks
In this chapter we discussed the reconstruction problem of arithmetic
circuits. We saw some hardness results and described a few algo-
rithms. We ﬁnd the state of the art, however, to be not satisfactory
in both directions. Speciﬁcally, we expect stronger hardness results to
hold. E.g., that reconstruction implies lower bounds for more natu-
ral polynomials than those guaranteed by Theorem 5.2 (see, e.g., Open
Problem 32). The situation with reconstruction algorithms is even more
depressing. We believe that one should be able to reconstruct circuits
in classes for which PIT algorithms are known. The problem of recon-
structing sums of read-once formulas, Open Problem 31, is an inter-
esting question in this direction. The special case of reconstructing
read-once formulas is known but was not covered in this survey (the
interested reader is referred to [26, 27, 28, 55, 126, 127]).

Acknowledgments
We thank Michael Forbes for his helpful comments that improved
the presentation and Madhu Sudan for encouraging us to write this
monograph.
379

References
[1] S.
Aaronson,
“Arithmetic
natural
proofs
theory
is
sought,”
http://
scottaaronson.com/blog/?p=336, 2008.
[2] M. Agrawal, “Proving lower bounds via pseudo-random generators,” in Pro-
ceedings of the 25th FSTTCS, vol. 3821 of LNCS, pp. 92–105, 2005.
[3] M. Agrawal and S. Biswas, “Primality and identity testing via Chinese remain-
dering,” Journal of the ACM, vol. 50, pp. 429–443, 2003.
[4] M. Agrawal, N. Kayal, and N. Saxena, “Primes is in P,” Annals of Mathemat-
ics, vol. 160, pp. 781–793, 2004.
[5] M. Agrawal and V. Vinay, “Arithmetic circuits: A chasm at depth four,” in
Proceedings of the 49th Annual FOCS, pp. 67–75, 2008.
[6] N. Alon, “Combinatorial nullstellensatz,” Combinatorics, Probability and
Computing, vol. 8, pp. 7–29, 1999.
[7] S. A. Amitsur and J. Levitzki, “Minimal identities for algebras,” Proceedings
of the American Mathematical Society, vol. 1, pp. 449–463, 1950.
[8] M. Anderson, D. van Melkebeek, and I. Volkovich, “Derandomizing poly-
nomial identity testing for multilinear constant-read formulae,” Electronic
Colloquium on Computational Complexity (ECCC), vol. 135, 2010. http://
www.eccc.uni-trier.de/report/2010/189/.
[9] S. Arora and B. Barak, Computational Complexity: A Modern Approach. Cam-
bridge University Press, 2009.
[10] S. Arora, C. Lund, R. Motwani, M. Sudan, and M. Szegedy, “Proof veriﬁcation
and the hardness of approximation problems,” Journal of the ACM, vol. 45,
pp. 501–555, 1998.
[11] S. Arora and S. Safra, “Probabilistic checking of proofs: A new characteriza-
tion of NP,” Journal of the ACM, vol. 45, pp. 70–122, 1998.
380

References
381
[12] V. Arvind and P. Mukhopadhyay, “The monomial ideal membership prob-
lem and polynomial identity testing,” in Proceedings of the 18th ISAAC,
pp. 800–811, 2007.
[13] V. Arvind and P. Mukhopadhyay, “Derandomizing the isolation lemma and
lower bounds for circuit size,” in APPROX-RANDOM, pp. 276–289, 2008.
[14] V. Arvind, P. Mukhopadhyay, and S. Srinivasan, “New results on noncom-
mutative and commutative polynomial identity testing,” in Proceedings of the
23rd Annual CCC, pp. 268–279, 2008.
[15] L. Babai, L. Fortnow, and C. Lund, “Non-deterministic exponential time has
two-prover interactive protocols,” Computational Complexity, vol. 1, pp. 3–40,
1991.
[16] W. Baur and V. Strassen, “The complexity of partial derivatives,” Theoretical
Compter Science, vol. 22, pp. 317–330, 1983.
[17] A. Beimel, F. Bergadano, N. H. Bshouty, E. Kushilevitz, and S. Varricchio,
“Learning functions represented as multiplicity automata,” Journal of the
ACM, vol. 47, pp. 506–530, 2000.
[18] M. Ben-Or and P. Tiwari, “A deterministic algorithm for sparse multivari-
ate polynominal interpolation,” in Proceedings of the 20th Annual STOC,
pp. 301–309, 1988.
[19] M. Bl¨aser, “A 5/2n2-lower bound for the rank of n × n-matrix multiplication
over arbitrary ﬁelds,” in Proceedings of the 40th Annual FOCS, pp. 45–50,
1999.
[20] M. Bl¨aser, M. Hardt, R. J. Lipton, and N. K. Vishnoi, “Deterministically test-
ing sparse polynomial identities of unbounded degree,” Information Processing
Letters, vol. 109, pp. 187–192, 2009.
[21] L. Blum, F. Cucker, M. Shub, and S. Smale, Complexity and Real Computa-
tion. Springer, 1997.
[22] M. Blum, A. K. Chandra, and M. N. Wegman, “Equivalence of free boolean
graphs can be tested in polynomial time,” Information Processing Letters,
vol. 10, pp. 80–82, 1980.
[23] A. Bogdanov and H. Wee, “More on noncommutative polynomial identity test-
ing,” in Proceedings of the 20th Annual IEEE Conference on Computational
Complexity, pp. 92–99, 2005.
[24] A. Borodin, J. von zur Gathen, and J. E. Hopcroft, “Fast parallel matrix and
GCD computations,” Information and Control, vol. 52, pp. 241–256, 1982.
[25] M. R. Brown and D. P. Dobkin, “An improved lower bound on polynomial
multiplication,” IEEE Transactions on Computers, vol. 29, pp. 337–340, 1980.
[26] D. Bshouty and N. H. Bshouty, “On interpolating arithmetic read-once formu-
las with exponentiation,” Journal of Computer and System Sciences, vol. 56,
pp. 112–124, 1998.
[27] N. H. Bshouty and R. Cleve, “Interpolating arithmetic read-once formulas in
parallel,” SIAM Journal on Computing, vol. 27, pp. 401–413, 1998.
[28] N. H. Bshouty, T. R. Hancock, and L. Hellerstein, “Learning arithmetic read-
once formulas,” SIAM Journal on Computing, vol. 24, pp. 706–735, 1995.
[29] P. B¨urgisser, “On the structure of valiant’s complexity classes,” Discrete Math-
ematics & Theoretical Computer Science, vol. 3, pp. 73–94, 1999.

382
References
[30] P. B¨urgisser, M. Clausen, and M. A. Shokrollahi, Algebraic Complexity Theory.
Springer, 1997.
[31] J. Cai, X. Chen, and D. Li, “A quadratic lower bound for the permanent and
determinant problem over any characteristic != 2,” in Proceedings of the 40th
Annual STOC, pp. 491–498, 2008.
[32] S. Chari, P. Rohatgi, and A. Srinivasan, “Randomness-optimal unique element
isolation with applications to perfect matching and related problems,” SIAM
Journal on Computing, vol. 24, pp. 1036–1050, 1995.
[33] B. Chazelle, “A spectral approach to lower bounds with applications to geo-
metric searching,” SIAM Journal on Computing, vol. 27, pp. 545–556, 1998.
[34] Z. Chen and M. Kao, “Reducing randomness via irrational numbers,” SIAM
Journal on Computing, vol. 29, pp. 1247–1256, 2000.
[35] S. Chien and A. Sinclair, “Algebras with polynomial identities and computing
the determinant,” SIAM Journal on Computing, vol. 37, pp. 252–266, 2007.
[36] H. Cohn, R. D. Kleinberg, B. Szegedy, and C. Umans, “Group-theoretic algo-
rithms for matrix multiplication,” in Proceedings of the 46th Annual FOCS,
pp. 379–388, 2005.
[37] H. Cohn and C. Umans, “A group-theoretic approach to fast matrix multipli-
cation,” in Proceedings of the 44th Annual FOCS, pp. 438–449, 2003.
[38] J. W. Cooley and J. W. Tukey, “An algorithm for the machine calculation of
complex fourier series,” Mathematics of Computation, vol. 19, pp. 297–301,
1965.
[39] D. Coppersmith and S. Winograd, “Matrix multiplication via arithmetic pro-
gression,” Journal of Symbolic Computation, vol. 9, pp. 251–280, 1990.
[40] L. Csanky, “Fast parallel matrix inversion algorithms,” SIAM Journal on
Computing, vol. 5, pp. 618–623, 1976.
[41] R. A. DeMillo and R. J. Lipton, “A probabilistic remark on algebraic program
testing,” Information Processing Letters, vol. 7, pp. 193–195, 1978.
[42] Z. Dvir, “On matrix rigidity and locally self-correctable codes,” in Proceedings
of the 25th Annual CCC, pp. 291–298, 2010.
[43] Z. Dvir and A. Shpilka, “Locally decodable codes with 2 queries and poly-
nomial identity testing for depth 3 circuits,” SIAM Journal on Computing,
vol. 36, pp. 1404–1434, 2006.
[44] Z. Dvir, A. Shpilka, and A. Yehudayoﬀ, “Hardness-randomness tradeoﬀs for
bounded depth arithmetic circuits,” SIAM Journal on Computing, vol. 39,
pp. 1279–1293, 2009.
[45] M. Edelstein and L. M. Kelly, “Bisecants of ﬁnite collections of sets in linear
spaces,” Canadanian Journal of Mathematics, vol. 18, pp. 375–280, 1966.
[46] L. Fortnow and A. R. Klivans, “Eﬃcient learning algorithms yield circuit lower
bounds,” Journal of Computer System Science, vol. 75, pp. 27–36, 2009.
[47] A. Gabizon and R. Raz, “Deterministic extractors for aﬃne sources over large
ﬁelds,” Combinatorica, vol. 28, pp. 415–440, 2008.
[48] O. Goldreich, S. Goldwasser, and S. Micali, “How to construct random func-
tions,” Journal of the ACM, vol. 33, pp. 792–807, 1986.
[49] D. Grigoriev and M. Karpinski, “The matching problem for bipartite graphs
with polynomially bounded permanents is in NC (extended abstract),” in
Proceedings of the 28th Annual FOCS, pp. 166–172, 1987.

References
383
[50] D. Grigoriev and M. Karpinski, “An exponential lower bound for depth 3
arithmetic circuits,” in Proceedings of the 30th Annual STOC, pp. 577–582,
1998.
[51] D. Grigoriev, M. Karpinski, and M. F. Singer, “Fast parallel algorithms for
sparse multivariate polynomial interpolation over ﬁnite ﬁelds,” SIAM Journal
on Computing, vol. 19, pp. 1059–1063, 1990.
[52] D. Grigoriev and A. A. Razborov, “Exponential complexity lower bounds for
depth 3 arithmetic circuits in algebras of functions over ﬁnite ﬁelds,” Applica-
ble Algebra in Engineering, Communication and Computing, vol. 10, pp. 465–
487, 2000.
[53] J. H˚astad, “Almost optimal lower bounds for small depth circuits,” in Pro-
ceedings of the 18th Annual STOC, pp. 6–20, 1986.
[54] J. H˚astad, “Tensor rank is np-complete,” Journal of Algorithms, vol. 11,
pp. 644–654, 1990.
[55] T. R. Hancock and L. Hellerstein, “Learning read-once formulas over ﬁelds
and extended bases,” in Proceedings of the 4th Annual COLT, pp. 326–336,
1991.
[56] J. Heintz and C. P. Schnorr, “Testing polynomials which are easy to compute
(extended abstract),” in Proceedings of the 12th annual STOC, pp. 262–272,
1980.
[57] J. Heintz and M. Sieveking, “Lower bounds for polynomials with algebraic
coeﬃcients,” Theoretical Computer Science, vol. 11, pp. 321–330, 1980.
[58] S. Hoory, N. Linial, and A. Wigderson, “Expander graphs and their applica-
tions,” Bulletin of the American Mathematical Society, vol. 43, pp. 439–561,
2006.
[59] J. E. Hopcroft, R. Motwani, and J. D. Ullman, Introduction to Automata
Theory,
Languages,
and
Computation.
Pearson
Education,
2nd
ed.,
2000.
[60] P. Hrubeˇs, A. Wigderson, and A. Yehudayoﬀ, “Non-commutative circuits and
the sum-of-squares problem,” in Proceedings of the 42nd Annual ACM Sym-
posium on Theory of Computing (STOC), pp. 667–676, 2010.
[61] P. Hrubeˇs, A. Wigderson, and A. Yehudayoﬀ, “Relationless completeness
and separations,” in Proceedings of the 25th Conference on Computational
Complexity, pp. 280–290, 2010.
[62] P. Hrubeˇs and A. Yehudayoﬀ, “Arithmetic complexity in algebraic exten-
sions,” Manuscript, 2009.
[63] P. Hrubeˇs and A. Yehudayoﬀ, “Homogeneous formulas and symmetric poly-
nomials,” CoRR, abs/0907.2621, 2009.
[64] P. Hrubeˇs and A. Yehudayoﬀ, “Monotone separations for constant degree poly-
nomials,” Information Processing Letters, vol. 110, pp. 1–3, 2009.
[65] R. Impagliazzo, V. Kabanets, and A. Wigderson, “In search of an easy witness:
Exponential time vs. probabilistic polynomial time,” Journal of Computer and
System Sciences, vol. 65, pp. 672–694, 2002.
[66] R. Impagliazzo and A. Wigderson, “P=BPP unless E has subexponential cir-
cuits: derandomizing the XOR lemma,” in Proceedings of the 29th STOC,
pp. 220–229, 1997.

384
References
[67] M. Jansen, Y. Qiao, and J. Sarma, “Deterministic identity testing of read-once
algebraic branching programs,” CoRR, abs/0912.2565, 2009.
[68] M. Jerrum and M. Snir, “Some exact complexity results for straight-line
computations over semi-rings,” Technical Report CRS-58–80, University of
Edinburgh, 1980.
[69] V. Kabanets and R. Impagliazzo, “Derandomizing polynomial identity tests
means proving circuit lower bounds,” Computational Complexity, vol. 13,
pp. 1–46, 2004.
[70] K. Kalorkoti, “A lower bound for the formula size of rational functions,” SIAM
Journal of Computing, vol. 14, pp. 678–687, 1985.
[71] E. Kaltofen, “Factorization of polynomials given by straight-line programs,”
in Randomness in Computation, vol. 5 of Advances in Computing Research,
(S. Micali, ed.), pp. 375–412, 1989.
[72] E. Kaltofen, “Polynomial factorization: A success story,” in ISSAC, pp. 3–4,
2003.
[73] E. Kaltofen and B. M. Trager, “Computing with polynomials given by black
boxes for their evaluations: Greatest common divisors, factorization, sepa-
ration of numerators and denominators,” Journal of Symbolic Computation,
vol. 9, pp. 301–320, 1990.
[74] M. Kaminski, “A lower bound on the complexity of polynomial multiplication
over ﬁnite ﬁelds,” SIAM Journal on Computing, vol. 34, pp. 960–992, 2005.
[75] Z. S. Karnin, P. Mukhopadhyay, A. Shpilka, and I. Volkovich, “Deterministic
identity testing of depth 4 multilinear circuits with bounded top fan-in,” in
Proceedings of the 42nd Annual STOC, pp. 649–658, 2010.
[76] Z. S. Karnin and A. Shpilka, “Deterministic black box polynomial identity
testing of depth-3 arithmetic circuits with bounded top fan-in,” in Proceedings
of the 23rd Annual CCC, pp. 280–291, 2008.
[77] Z. S. Karnin and A. Shpilka, “Reconstruction of generalized depth-3 arithmetic
circuits with bounded top fan-in,” in Proceedings of the 24th Annual CCC,
pp. 274–285, 2009.
[78] R. Karp, E. Upfal, and A. Wigderson, “Constructing a perfect matching is in
random NC,” Combinatorica, vol. 6, pp. 35–48, 1, 1986.
[79] N. Kayal, “Derandomizing some number-theoretic and algebraic algorithms,”
PhD thesis, Indian Institute of Technology, Kanpur, India, 2007.
[80] N. Kayal and S. Saraf, “Blackbox polynomial identity testing for depth 3
circuits,” in Proceedings of the 50th Annual FOCS, pp. 198–207, 2009.
[81] N. Kayal and N. Saxena, “Polynomial identity testing for depth 3 circuits,”
Computational Complexity, vol. 16, pp. 115–138, 2007.
[82] M. J. Kearns and L. G. Valiant, “Cryptographic limitations on learning
boolean formulae and ﬁnite automata,” Journal of the ACM, vol. 41, pp. 67–
95, 1994.
[83] M. J. Kearns and U. V. Vazirani, An Introduction to Computational Learning
Theory. Cambridge, MA, USA: MIT Press, 1994.
[84] K.
S.
Kedlaya
and
C.
Umans,
“Fast
modular
composition
in
any
characteristic,” in Proceedings of the 49th Annual Symposium on Foundations
of Computer Science (FOCS), pp. 146–155, 2008.

References
385
[85] A. Klivans and A. Shpilka, “Learning restricted models of arithmetic circuits,”
Theory of Computing, vol. 2, pp. 185–206, 2006.
[86] A. Klivans and D. Spielman, “Randomness eﬃcient identity testing of multi-
variate polynomials,” in Proceedings of the 33rd Annual STOC, pp. 216–223,
2001.
[87] A. R. Klivans and A. A. Sherstov, “Cryptographic hardness for learning inter-
sections of halfspaces,” Journal of Computer and System Sciences, vol. 75,
pp. 2–12, 2009.
[88] P. Koiran, “Arithmetic circuits: The chasm at depth four gets wider,” CoRR,
abs/1006.4700, 2010.
[89] D. Lewin and S. Vadhan, “Checking polynomial identities over any ﬁeld:
Towards a derandomization?,” in Proceedings of the 30th Annual STOC,
pp. 428–437, 1998.
[90] L. Lovasz, “On determinants, matchings, and random algorithms,” in Funda-
mentals of Computing Theory, (L. Budach, ed.), Akademia-Verlag, 1979.
[91] C. Lund, L. Fortnow, H. Karloﬀ, and N. Nisan, “Algebraic methods for
interactive proof systems,” Journal of the ACM, vol. 39, pp. 859–868,
1992.
[92] T. Mignon and N. Ressayre, “A quadratic bound for the determinant and
permanent problem,” International Mathematics Research Notices, vol. 79,
pp. 4241–4253, 2004.
[93] P. Morandi, Graduate Texts in Mathematics 167: Field and Galois Theory.
Springer-Verlag, New York, 1996.
[94] J. Morgenstern, “Note on a lower bound on the linear complexity of the fast
Fourier transform,” Journal of the ACM, vol. 20, pp. 305–306, 1973.
[95] K. Mulmuley and M. A. Sohoni, “Geometric complexity theory i: An approach
to the P vs. NP and related problems,” SIAM Journal on Computing, vol. 31,
pp. 496–526, 2001.
[96] K. Mulmuley and M. A. Sohoni, “Geometric complexity theory ii: Towards
explicit obstructions for embeddings among class varieties,” SIAM Journal on
Computing, vol. 38, pp. 1175–1206, 2008.
[97] K. Mulmuley, U. Vazirani, and V. Vazirani, “Matching is as easy as matrix
inversion,” Combinatorica, vol. 7, pp. 105–113, 1987.
[98] N. Nisan, “Lower bounds for non-commutative computation,” in Proceedings
of the 23rd Annual STOC, pp. 410–418, 1991.
[99] N. Nisan and A. Wigderson, “Hardness vs. randomness,” Journal of Computer
System Sciences, vol. 49, pp. 149–167, 1994.
[100] N. Nisan and A. Wigderson, “Lower bound on arithmetic circuits via partial
derivatives,” Computational Complexity, vol. 6, pp. 217–234, 1996.
[101] R. Raz, “On the complexity of matrix product,” SIAM Journal on Computing,
vol. 32, pp. 1356–1369, 2003.
[102] R. Raz, “Separation of multilinear circuit and formula size,” Theory of Com-
puting, vol. 2, pp. 121–135, 2006.
[103] R. Raz, “Elusive functions and lower bounds for arithmetic circuits,” in
Proceedings of the 40th Annual ACM Symposium on Theory of Computing
(STOC), pp. 711–720, 2008.

386
References
[104] R. Raz, “Multi-linear formulas for permanent and determinant are of super-
polynomial size,” Journal of the ACM, vol. 56, 2009.
[105] R. Raz, “Tensor-rank and lower bounds for arithmetic formulas,” in Proceed-
ings of the 42nd Annual STOC, pp. 659–666, 2010.
[106] R. Raz and A. Shpilka, “Deterministic polynomial identity testing in non
commutative models,” Computational Complexity, vol. 14, pp. 1–19, 2005.
[107] R. Raz, A. Shpilka, and A. Yehudayoﬀ, “A lower bound for the size of syntac-
tically multilinear arithmetic circuits,” SIAM Journal on Computing, vol. 38,
pp. 1624–1647, 2008.
[108] R. Raz and A. Yehudayoﬀ, “Balancing syntactically multilinear arithmetic
circuits,” Computational Complexity, vol. 17, pp. 515–535, 2008.
[109] R. Raz and A. Yehudayoﬀ, “Multilinear formulas, maximal-partition discrep-
ancy and mixed-sources extractors,” in Proceedings of the 49th Annual FOCS,
pp. 273–282, 2008.
[110] R. Raz and A. Yehudayoﬀ, “Lower bounds and separations for constant depth
multilinear circuits,” Computational Complexity, vol. 18, pp. 171–207, 2009.
[111] A. A. Razboeov and S. Rudich, “Natural proofs,” Journal of Computer and
System Sciences, vol. 55, pp. 24–35, 1997.
[112] A. A. Razborov, “Lower bounds for the size of circuits with bounded
depth with basis {∧,⊕},” Matematicheskie Zametki, pp. 598–607, 1987.
in Russian.
[113] O. Regev, “On lattices, learning with errors, random linear codes, and cryp-
tography,” Journal of the ACM, vol. 56, 2009.
[114] H. J. Ryser, Combinatorial Mathematics, vol. 14. Carus Mathematical Mono-
graphs, 1963.
[115] S. Saraf and I. Volkovich, “Black-box identity testing of depth-4 multilinear
circuits,” Manuscript, 2010.
[116] N. Saxena, “Diagonal circuit identity testing and lower bounds,” in ICALP
(1), pp. 60–71, 2008.
[117] N. Saxena and C. Seshadhri, “An almost optimal rank bound for depth-3
identities,” in Proceedings of the 24th Annual CCC, pp. 137–148, 2009.
[118] N. Saxena and C. Seshadhri, “From Sylvester-Gallai conﬁgurations to rank
bounds: Improved black-box identity test for deph-3 circuits,” in Proceedings
of the 51st Annual FOCS, pp. 21–30, 2010.
[119] J. T. Schwartz, “Fast probabilistic algorithms for veriﬁcation of polynomial
identities,” Journal of the ACM, vol. 27, pp. 701–717, 1980.
[120] A. Shamir, “IP = PSPACE,” Journal of the ACM, vol. 39, pp. 869–877, 1992.
[121] E. Shamir and M. Snir, “Lower bounds on the number of multiplications and
the number of additions in monotone computations,” Technical Report RC-
6757, IBM, 1977.
[122] V. Shoup and R. Smolensky, “Lower bounds for polynomial evaluation and
interpolation problems,” in SFCS ’91: Proceedings of the 32nd Annual Sym-
posium on Foundations of Computer Science, pp. 378–383, Washington, DC,
USA: IEEE Computer Society, 1991.
[123] A. Shpilka, “Aﬃne projections of symmetric polynomials,” Journal of Com-
puter and System Sciences, vol. 65, pp. 639–659, 2002.

References
387
[124] A. Shpilka, “Lower bounds for matrix product,” SIAM Journal on Computing,
vol. 32, pp. 1185–1200, 2003.
[125] A. Shpilka, “Interpolation of depth-3 arithmetic circuits with two multiplica-
tion gates,” SIAM Journal on Computing, vol. 38, pp. 2130–2161, 2009.
[126] A. Shpilka and I. Volkovich, “Read-once polynomial identity testing,” in Pro-
ceedings of the 40th Annual STOC, pp. 507–516, 2008.
[127] A. Shpilka and I. Volkovich, “Improved polynomial identity testing for read-
once formulas,” in APPROX-RANDOM, pp. 700–713, 2009.
[128] A. Shpilka and I. Volkovich, “On the relation between polynomial identity
testing and ﬁnding variable disjoint factors,” in ICALP (1), pp. 408–419,
2010.
[129] A. Shpilka and A. Wigderson, “Depth-3 arithmetic circuits over ﬁelds of char-
acteristic zero,” Computational Complexity, vol. 10, pp. 1–27, 2001.
[130] R. Smolensky, “Algebraic methods in the theory of lower bounds for Boolean
circuit complexity,” in Proceedings of the 19th Annual STOC, pp. 77–82, 1987.
[131] V. Strassen, “Gaussian elimination is not optimal,” Numerische Mathematik,
vol. 13, pp. 354–356, 1969.
[132] V. Strassen, “Die berechnungskomplexi¨at von elementarsymmetrischen funk-
tionen und von interpolationskoeﬃzienten,” Numerische Mathematik, vol. 20,
pp. 238–251, 1973.
[133] V. Strassen, “Vermeidung von divisionen,” The Journal f¨ur die Reine und
Angewandte Mathematik, vol. 264, pp. 182–202, 1973.
[134] V. Strassen, “Algebraic complexity theory,” in Handbook of Theoretical Com-
puter Science, vol. A: Algorithms and Complexity (A), pp. 633–672, Elsevier
and MIT Press, 1990.
[135] P. Tiwari and M. Tompa, “A direct version of Shamir and Snir’s lower bounds
on monotone circuit depth,” Information Processing Letters, vol. 49, pp. 243–
248, 1994.
[136] S. Toda, “PP is as hard as the polynomial time hierarchy,” SIAM Journal on
Computing, vol. 20, pp. 865–877, 1991.
[137] L. G. Valiant, “Graph-theoretic arguments in low-level complexity,” in Lecture
notes in Computer Science, vol. 53, pp. 162–176, Springer, 1977.
[138] L. G. Valiant, “Completeness classes in algebra,” in Proceedings of the 11th
Annual STOC, pp. 249–261, 1979.
[139] L. G. Valiant, “The complexity of computing the permanent,” Theoretical
Computer Science, vol. 8, pp. 189–201, 1979.
[140] L. G. Valiant, “Negation can be exponentially powerful,” Theoretical Com-
puter Science, vol. 12, pp. 303–314, November 1980.
[141] L. G. Valiant, “Reducibility by algebraic projections,” L’Enseignement Math-
ematique, vol. 28, pp. 253–268, 1982.
[142] L. G. Valiant, “A theory of the learnable,” Communications of the ACM,
vol. 27, pp. 1134–1142, 1984.
[143] L. G. Valiant, S. Skyum, S. Berkowitz, and C. Rackoﬀ, “Fast parallel com-
putation of polynomials using few processors,” SIAM Journal on Computing,
vol. 12, pp. 641–644, November 1983.

388
References
[144] E. Viola, “The sum of small-bias generators fools polynomials of degree,”
Computational Complexity, vol. 18, pp. 209–217, 2009.
[145] J. von zur Gathen, “Feasible arithmetic computations: Valiant’s hypothesis,”
Journal of Symbolic Computation, vol. 4, pp. 137–172, 1987.
[146] J. von zur Gathen, “Who was who in polynomial factorization,” in ISSAC,
p. 2, 2006.
[147] J. von zur Gathen and J. Gerhard, Modern Computer Algebra. Cambridge
University Press, 1999.
[148] R. Zippel, “Probabilistic algorithms for sparse polynomials,” in Symbolic and
Algebraic Computation, pp. 216–226, 1979.


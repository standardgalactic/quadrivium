SYNTHESIS LECTURES ON
HUMAN LANGUAGE  TECHNOLOGIES
C
M
&
Morgan   Claypool Publishers
&
Graeme Hirst, Series Editor
Web Corpus
Construction
Roland Schäfer
Felix Bildhauer
www.allitebooks.com

www.allitebooks.com

Web Corpus Construction
www.allitebooks.com

www.allitebooks.com

Synthesis Lectures on Human
Language Technologies
Editor
Graeme Hirst, University of Toronto
Synthesis Lectures on Human Language Technologies is edited by Graeme Hirst of the University
of Toronto. e series consists of 50- to 150-page monographs on topics relating to natural language
processing, computational linguistics, information retrieval, and spoken language understanding.
Emphasis is on important new techniques, on new applications, and on topics that combine two or
more HLT subﬁelds.
Web Corpus Construction
Roland Schäfer and Felix Bildhauer
2013
Recognizing Textual Entailment: Models and Applications
Ido Dagan, Dan Roth, Mark Sammons, and Fabio Massimo Zanzotto
2013
Semi-Supervised Learning and Domain Adaptation in Natural Language Processing
Anders Søgaard
2013
Linguistic Fundamentals for Natural Language Processing: 100 Essentials from
Morphology and Syntax
Emily M. Bender
2013
Semantic Relations Between Nominals
Vivi Nastase, Preslav Nakov, Diarmuid Ó Séaghdha, and Stan Szpakowicz
2013
Computational Modeling of Narrative
Inderjeet Mani
2012
Natural Language Processing for Historical Texts
Michael Piotrowski
2012
www.allitebooks.com

iv
Sentiment Analysis and Opinion Mining
Bing Liu
2012
Discourse Processing
Manfred Stede
2011
Bitext Alignment
Jörg Tiedemann
2011
Linguistic Structure Prediction
Noah A. Smith
2011
Learning to Rank for Information Retrieval and Natural Language Processing
Hang Li
2011
Computational Modeling of Human Language Acquisition
Afra Alishahi
2010
Introduction to Arabic Natural Language Processing
Nizar Y. Habash
2010
Cross-Language Information Retrieval
Jian-Yun Nie
2010
Automated Grammatical Error Detection for Language Learners
Claudia Leacock, Martin Chodorow, Michael Gamon, and Joel Tetreault
2010
Data-Intensive Text Processing with MapReduce
Jimmy Lin and Chris Dyer
2010
Semantic Role Labeling
Martha Palmer, Daniel Gildea, and Nianwen Xue
2010
Spoken Dialogue Systems
Kristiina Jokinen and Michael McTear
2009
www.allitebooks.com

v
Introduction to Chinese Natural Language Processing
Kam-Fai Wong, Wenjie Li, Ruifeng Xu, and Zheng-sheng Zhang
2009
Introduction to Linguistic Annotation and Text Analytics
Graham Wilcock
2009
Dependency Parsing
Sandra Kübler, Ryan McDonald, and Joakim Nivre
2009
Statistical Language Models for Information Retrieval
ChengXiang Zhai
2008
www.allitebooks.com

Copyright © 2013 by Morgan & Claypool
All rights reserved. No part of this publication may be reproduced, stored in a retrieval system, or transmitted in
any form or by any means—electronic, mechanical, photocopy, recording, or any other except for brief quotations
in printed reviews, without the prior permission of the publisher.
Web Corpus Construction
Roland Schäfer and Felix Bildhauer
www.morganclaypool.com
ISBN: 9781608459834
paperback
ISBN: 9781608459841
ebook
DOI 10.2200/S00508ED1V01Y201305HLT022
A Publication in the Morgan & Claypool Publishers series
SYNTHESIS LECTURES ON HUMAN LANGUAGE TECHNOLOGIES
Lecture #22
Series Editor: Graeme Hirst, University of Toronto
Series ISSN
Synthesis Lectures on Human Language Technologies
Print 1947-4040
Electronic 1947-4059
www.allitebooks.com

Web Corpus Construction
Roland Schäfer and Felix Bildhauer
Freie Universität Berlin
SYNTHESIS LECTURES ON HUMAN LANGUAGE TECHNOLOGIES #22
C
M
&
cLaypool
Morgan
publishers
&
www.allitebooks.com

ABSTRACT
e World Wide Web constitutes the largest existing source of texts written in a great variety of
languages. A feasible and sound way of exploiting this data for linguistic research is to compile
a static corpus for a given language. ere are several adavantages of this approach: (i) Work-
ing with such corpora obviates the problems encountered when using Internet search engines
in quantitative linguistic research (such as non-transparent ranking algorithms). (ii) Creating a
corpus from web data is virtually free. (iii) e size of corpora compiled from the WWW may
exceed by several orders of magnitudes the size of language resources oﬀered elsewhere. (iv) e
data is locally available to the user, and it can be linguistically post-processed and queried with
the tools preferred by her/him.
is book addresses the main practical tasks in the creation of web corpora up to giga-token
size. Among these tasks are the sampling process (i. e., web crawling) and the usual cleanups in-
cluding boilerplate removal and removal of duplicated content. Linguistic processing and prob-
lems with linguistic processing coming from the diﬀerent kinds of noise in web corpora are also
covered. Finally, the authors show how web corpora can be evaluated and compared to other
corpora (such as traditionally compiled corpora).
For additional material please visit the companion website
http://sites.morganclaypool.com/wcc
KEYWORDS
corpus creation, web corpora, web crawling, web characterization, boilerplate re-
moval, language identiﬁcation, duplicate detection, near-duplicate detection, tok-
enization, POS tagging, noisy data, corpus evaluation, corpus comparison, keyword
extraction
www.allitebooks.com

ix
Contents
Preface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xiii
Acknowledgments. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xv
1
Web Corpora . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
2
Data Collection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
2.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
2.2
e Structure of the Web . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
2.2.1 General Properties. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
2.2.2 Accessibility and Stability of Web pages . . . . . . . . . . . . . . . . . . . . . . . . . . 9
2.2.3 What’s in a (National) Top Level Domain? . . . . . . . . . . . . . . . . . . . . . . . 11
2.2.4 Problematic Segments of the Web . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
2.3
Crawling Basics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
2.3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
2.3.2 Corpus Construction From Search Engine Results . . . . . . . . . . . . . . . . . 16
2.3.3 Crawlers and Crawler Performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
2.3.4 Conﬁguration Details and Politeness . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
2.3.5 Seed URL Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
2.4
More on Crawling Strategies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
2.4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
2.4.2 Biases and the PageRank . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
2.4.3 Focused Crawling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
3
Post-Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
3.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
3.2
Basic Cleanups . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
3.2.1 HTML stripping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
3.2.2 Character References and Entities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
3.2.3 Character Sets and Conversion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
3.2.4 Further Normalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
3.3
Boilerplate Removal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48

x
3.3.1 Introduction to Boilerplate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
3.3.2 Feature Extraction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
3.3.3 Choice of the Machine Learning Method . . . . . . . . . . . . . . . . . . . . . . . . 55
3.4
Language Identiﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
3.5
Duplicate Detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
3.5.1 Types of Duplication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
3.5.2 Perfect Duplicates and Hashing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60
3.5.3 Near Duplicates, Jaccard Coeﬃcients, and Shingling . . . . . . . . . . . . . . . 61
4
Linguistic Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65
4.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65
4.2
Basics of Tokenization, Part-Of-Speech Tagging, and Lemmatization . . . . . . . 66
4.2.1 Tokenization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66
4.2.2 Part-Of-Speech Tagging. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68
4.2.3 Lemmatization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
4.3
Linguistic Post-Processing of Noisy Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
4.3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
4.3.2 Treatment of Noisy Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71
4.4
Tokenizing Web Texts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72
4.4.1 Example: Missing Whitespace . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72
4.4.2 Example: Emoticons . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74
4.5
POS Tagging and Lemmatization of Web Texts . . . . . . . . . . . . . . . . . . . . . . . . 75
4.5.1 Tracing Back Errors in POS Tagging . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75
4.6
Orthographic Normalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79
4.7
Software for Linguistic Post-Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82
5
Corpus Evaluation and Comparison . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85
5.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85
5.2
Rough Quality Check . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85
5.2.1 Word and Sentence Lengths. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86
5.2.2 Duplication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90
5.3
Measuring Corpus Similarity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92
5.3.1 Inspecting Frequency Lists . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93
5.3.2 Hypothesis Testing with 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94
5.3.3 Hypothesis Testing with Spearman’s Rank Correlation . . . . . . . . . . . . . . 95
5.3.4 Using Test Statistics without Hypothesis Testing . . . . . . . . . . . . . . . . . . 97
5.4
Comparing Keywords. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98

xi
5.4.1 Keyword Extraction with 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99
5.4.2 Keyword Extraction Using the Ratio of Relative Frequencies . . . . . . . . . 99
5.4.3 Variants and Reﬁnements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102
5.5
Extrinsic Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104
5.6
Corpus Composition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106
5.6.1 Estimating Corpus Composition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106
5.6.2 Measuring Corpus Composition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107
5.6.3 Interpreting Corpus Composition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107
5.7
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109
Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
Authors’ Biographies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129


xiii
Preface
Our approach to the subject of web corpus construction is guided by our own practical experi-
ence in the area. Coming from an empirically oriented linguistics background, we required large
amounts of data for empirical research in various languages, including more or less non-standard
language. However, we noticed that, depending on the research question and the language of
interest, appropriate text resources are not always available and/or freely accessible and in the ap-
propriate form (cf. Section 1 for examples). erefore, we took the work by the WaCky initiative
[Baroni et al., 2009] and the Leipzig Corpora Collection (LCC, Biemann et al., 2007; Goldhahn
et al., 2012) as a starting point to build our own large corpora from web data, leading to the
development of the texrex software suite and the COW (“COrpora from the web”) corpora.¹;²
We dealt with the usual technical problems in web corpus construction, like boilerplate
removal and deduplication, noticing that there was no concise and reasonably complete introduc-
tory textbook on these technicalities available, although there are overview articles like Fletcher
[2011]; Kilgarriﬀand Grefenstette [2003]; Lüdeling et al. [2007]. Additionally, it became clear
to us that even the mere use of web corpora for linguistic research requires extra precautions and
more in-depth knowledge about the corpus construction process compared to the use of estab-
lished and “clean” corpus resources. is knowledge—mostly speciﬁc to web corpora—includes
important matters like:
• How was the corpus material sampled, which in this case means “crawled”?
• Which parts of the documents are removed in the usual “cleaning” steps, and with which
accuracy?
• Which documents are removed completely by which criteria, for example, near-duplicate
documents?
• What kinds of noise are present in the data itself (e. g., misspellings), and what was nor-
malized, removed, etc., by the corpus designers?
• Which kinds of noise might be introduced by the post-processing, such as tokenization
errors, inaccurate POS tagging, etc.?
e literature on these subjects comes to some extent (or rather to a large extent) from
the search engine and data mining sphere, as well as from Computational Linguistics. It is also
quite diverse, and no canonical set of papers has been established yet, making it diﬃcult to get a
complete picture in a short time. We hope to have compiled an overview of the papers which can
be considered recommended readings for anyone who wishes to compile a web corpus using their
¹http://sourceforge.net/projects/texrex/
²http://www.corporafromtheweb.org/

xiv
PREFACE
own tools (own crawlers, boilerplate detectors, deduplication software, etc.) or using available
tools.³ Equally important is our second goal, namely that this tutorial puts any web corpus user
in a position to make educated use of the available resources.
Although the book is primarily intended as a tutorial, this double goal and the extremely
diverse background which might be required leads to a mix of more practical and more theoretical
sections. Especially, Chapter 2 on data collection contains the least amount of practical recom-
mendation, mainly because data collection (primarily: web crawling) has—in our perception—
received the least attention (in terms of fundamental research) within the web corpus construction
community. Chapters 3 on non-linguistic post-processing and 4 on linguistic post-processing are
probably the most practical chapters. Chapter 5 brieﬂy touches upon the question of how we can
assess the quality of a web corpus (mainly by comparing it to other corpora). us, it is of high
theoretical relevance while containing concrete recommendations regarding some methods which
can be used.
Roland Schäfer and Felix Bildhauer
July 2013
³We make some software recommendations, but strictly from the open source world. We do this not so much out of dogmatism,
but rather because there are open source variants of all important tools and libraries available, and nobody has to pay for the
relevant software.

xv
Acknowledgments
Much of the material in this book was presented as a foundational course at the European Summer
School in Logic, Language and Information (ESSLLI) 2012 in Opole, Poland, by the authors.
We would like to thank the ESSLLI organizers for giving us the chance to teach the course.
We also thank the participants of the ESSLLI course for their valuable feedback and discussion,
especially Ekaterina Chernyak (NRU-HSE, Moscow, Russia). Also, we are grateful for many
comments by participants of diverse talks, presentations, and workshops held between 2011 and
2013. Furthermore, we would like to thank Adam Kilgarriﬀand two anonymous reviewers for
detailed and helpful comments on a draft version of this book. Any errors, omissions, and inad-
equacies which remain are probably due to us not listening to all these people.
We could not have written this tutorial without our prior work on our own corpora and
tools. erefore, we thank Stefan Müller (Freie Universität Berlin) for allowing us to stress the
computing infrastructure of the German Grammar work group to its limits. We also thank the
GNU/Linux support team at the Zedat data centre of Freie Universität Berlin for their technical
support (Robert Schüttler, Holger Weiß, and many others). Finally, we thank our student research
assistant, Sarah Dietzfelbinger, for doing much of the dirty work (like generating training data
for classiﬁers).
e second author’s work on this book was funded by the Deutsche Forschungsgemeinschaft,
SFB 632 “Information Structure,” Project A6.
Roland Schäfer would like to thank his parents for substantial support in a critical phase of
the writing of this book.
Felix Bildhauer is very much indebted to Chiaoi and Oskar for their patience and support
while he was working on this book.
Roland Schäfer and Felix Bildhauer
July 2013


1
C H A P T E R
1
Web Corpora
Although corpus-based Linguistics has seen a rise in popularity over the past few decades, for
many research questions the available resources are sometimes too small, sometimes too unbal-
anced, or they are balanced according to inappropriate criteria for the task, sometimes too close to
the respective standard language (again, for certain types of research questions), and sometimes
they are simply too expensive. Sometimes, it is also the case that the interfaces provided to access
available corpora are too restricted in search or export options to do serious quantitative research
or use the corpus for Computational Linguistics tasks. In addition, many freely available corpora
cannot be downloaded as a whole, which is required for many applications in Computational
Linguistics. Examples of the above include:
• e German Deutsches Referenzkorpus (DeReKo; Kupietz et al., 2010) by the Institut für
Deutsche Sprache (IDS) is large (currently over 5 billion words), but it contains predomi-
nantly newspaper text and is therefore unsuitable for research which requires a corpus con-
taining a variety of registers, genres, etc.
• e corpus by the Digitales Wörterbuch der Deutschen Sprache (DWDS; Geyken, 2006) is
a carefully balanced corpus of the German language of the 20th century, optimized for
lexicographic research. However, it contains only 123 million tokens. On top of the small
size, export options are highly limited, and many texts in the corpus are not licensed for
export by non-project members.¹
• Most of this is true for the British National Corpus (BNC; Leech, 1993).
• e corpora distributed by the Linguistic Data Consortium are small and expensive. ey
often add huge value through rich annotation, but while this is extremely useful for some
types of research, in some areas researchers need corpora several orders of magnitude larger.
E. g., the Penn Treebank [Marcus et al., 1993, 1999] contains roughly 4.5 million words
and costs $3,150 at the time of this writing according to the web page.²
• e French Frantext corpus, provided by Analyse et Traitement Informatique de la Langue
Française (ATILF), is a collection consisting predominantly of ﬁctional texts and philo-
sophical literature.³;⁴ As of early 2013, it comprises slightly over four thousand documents
¹Since any criticism regarding the speciﬁc composition of balanced corpora is guided by individual scientiﬁc needs and therefore
futile, we will not engage in it.
²http://ldc.upenn.edu/
³http://www.frantext.fr/
⁴http://www.atilf.fr/

2
1. WEB CORPORA
(the providers do not publish any token counts on the website), ranging from the 12th to
21st century (and including 850 texts from 1950 or later).
• e Spanish Corpus de Referencia del Español Actual (CREA) by Academia Real Española
contains 155 million tokens and is a balanced corpus of predominantly written language
from many diﬀerent Spanish-speaking countries. Access through a WWW interface is free,
but the query language is rather limited, there are bugs in the query engine, the display
options are limited, and query results cannot be exported.
• e Spanish Corpus del Español [Davies, 2002] is oﬀered with an advanced query interface
and contains texts from the 13th to the 20th centuries which sum up to 100 million word
tokens. However, contemporary Spanish (20th century) is represented by a mere 20 million
tokens.
• e Swedish Språkbanken project at Göteborgs Universitet oﬀers free access to roughly 1
billion tokens through a web interface.⁵ It is thus quite large. However, it cannot be down-
loaded as a whole.
In eoretical Linguistics, researchers sometimes try to obviate limitations of available re-
sources through Googleology. Especially when data on low-frequency or non-standard phenom-
ena is needed, search engine queries (mostly using Google’s service) are used to look for single
occurrences of some grammatical construction, or—even worse—result counts returned for such
queries are used for more or less formal statistical inference. is must be considered bad science
by any account. Everything that needs to be said about Googleology was already said in Kilgarriﬀ
[2006]. Problems with Googleology include, but are not restricted to:
1. Search engines are designed to favor precision over recall [Manning et al., 2009, 432] ac-
cording to non-linguistic relevance criteria. is means that we can never be sure that we
have found all or even some of the linguistically relevant results.
2. e ranking criteria are not known exactly, although usually variants of PageRank-like mea-
sures [Brin and Page, 1998] are used (cf. Section 2.4.2). Ranking can even be inﬂuenced by
economical factors (sponsored ranking). For good research practice, some form or random
sampling from the corpus would be required.
3. Search engines adapt the ranking of the results according to information like the language
of the browser or the headers sent by the client which inform the server about the user’s
preferred language, geo-location of the client’s IP address, etc. In practice, this means that
two identical queries almost never result in the same results being displayed, which makes
any claim based on these results non-reproducible—a clear indication of bad science.
4. Search engines expand and reduce search terms without providing feedback about the exact
nature of the expansions and reductions. is includes methods like spelling correction and
morphological analysis [Cucerzan and Brill, 2004]. Queries are also optimized, e. g., by
bracketing sub-expressions of the original query in order to improve the overall precision.
⁵http://spraakbanken.gu.se/
www.allitebooks.com

3
Such methods are often based on analyses of query logs [Guo et al., 2008; Hagen et al.,
2011; Risvik et al., 2003]. Especially if we take query result counts to make quantitative
statements, we do not know even remotely what kind of (expanded or reduced) queries
they actually represent.
5. Despite performing a lot of such covert linguistic processing, search engines oﬀer no lin-
guistic annotation. Controlled wildcarding (including wildcarding over parts-of-speech or
lemmata) is not available. is means that we cannot apply the usual search heuristics re-
quired to ﬁnd all relevant examples in a corpus.
6. Search engines return estimated total counts, which ﬂuctuate on a daily basis. A few pa-
pers, such as Eu [2008] and recently Rayson et al. [2012] discuss this ﬂuctuation and its
impact on linguistic research. In Rayson et al. [2012], a method is suggested to derive sta-
ble frequencies from ﬂuctuating search engine counts through long-term observation. It is
stated that more advanced time series statistics could provide even better results. e counts
would still depend on which documents the search engine considers worth indexing, and
the practical feasibility compared to the use of a static web corpus needs to be proven.
7. In any case, counts returned by search engines are usually page counts, not token counts.
Whether the number of web pages on which a term or expression occurs is relevant at all is
a very open question (but see Keller and Lapata, 2003, who ﬁnd a high correlation between
the counts returned by search engines and frequencies in traditional corpora like the BNC).
8. e “corpus” which is indexed by a search engine (some part of the indexable web deemed
relevant by the search engine provider) changes frequently, including the massive creation
of new pages and the removal of old ones. To deal with this, the index of a search engine
is constantly updated, but diﬀerent pages are re-indexed at quite diﬀerent intervals. is
means that pseudo-scientiﬁc results obtained from search engines cannot be reproduced by
other researchers. Even if all other points of criticism put forward here were invalid, this
single fact makes reporting search engine results bad science.
To avoid such problems with search engine results and to ﬁll the gaps left by available tra-
ditionally compiled corpora, web corpora as popularized by the WaCky initiative [Baroni et al.,
2009] or the Leipzig Corpora Collection (LCC, Biemann et al., 2007) are an ideal solution. If
available traditional or web corpora do not suﬃce, then a web corpus can in principle be compiled
by any researcher ad hoc and according to speciﬁc design decisions. e Leipzig Corpora are avail-
able in many languages, but for legal reasons, only sentence-wise shuﬄed versions are published,
which makes them inappropriate for any research at the document-level. e WaCky corpora
are considerably larger and contain whole documents, but they are only available for a handful
of European languages. In some cases, even larger corpora than the WaCky corpora might be
required.
To summarize, the main advantages of web corpora are:
1. ey can be constructed at no cost beyond expenses for hard drive space, CPU power, and
bandwidth.

4
1. WEB CORPORA
2. ey can reach almost arbitrary sizes, far in the giga-token region.
3. Entirely new registers and genres are available exclusively in web corpora (blogs, forums,
etc.). Some of these genres cover texts which are much closer to spontaneous (although not
necessarily spoken) language than texts contained in traditionally compiled corpora.
4. ey are available locally for all kinds of processing using standard tools, database systems,
etc., such that the user is not bound by the limitations of some query interface.
5. web corpus construction allows for a form of random sampling from the population of
web documents (even uniform sampling, where each document has the same chance of
being sampled). Furthermore, the population of web documents is very large and extremely
diverse in terms of genres, styles, etc. Constructing truly random samples from such a huge
and diverse population is not possible by means of traditional corpus construction. Corpora
constructed in such a way could therefore be a valuable addition to available resources based
on stratiﬁed sampling (i. e., balanced corpora).
e advantages have to be balanced against the disadvantages:
1. e copyright situation is unclear in some countries where there is no fair use policy (like
Germany). Especially the redistribution of the corpora is problematic under such condi-
tions.
2. Although there are advanced methods to clean web corpora, they still contain signiﬁcant
amounts of noisy data, such as spam web pages, redundant auto-generated content from
content management systems, misspellings, etc. Compared to users of traditional corpora,
users of web corpora must therefore be more aware of the steps which were taken in the
construction of the corpus, such that they are aware of potential distortions of their results.
As an example which will be explained in detail in Chapters 4 and 5 the word type count
for web corpora is usually much too high to be plausible due to a large amount of noisy
material, such that naïvely drawn statistical conclusions might be invalid.
Since web corpora can serve many diﬀerent purposes, it is probably impossible to satisfy
everyone with one single introductory text book. We look at the subject mainly from the point
of view of empirical linguistics, which is why we kept some sections shorter than computational
linguists probably would have. For example, this is certainly true for Section 2.4.3 about focused
crawling (the task of mining the web for documents about certain topics, in certain languages,
etc.). In making such decisions, we mainly adopted the following guideline in order to stay com-
patible with non-computational linguists: If there are available tools which solve a task without
requiring that the user have advanced programming skills, we deal with it in detail, otherwise
we discuss it brieﬂy and include suggestions for further reading. Creating and running a focused
crawler is deﬁnitely a task that requires advanced programming skills, and we therefore kept the
section about it comparatively short.
roughout the book, we will be suggesting that certain technical decisions are actually
design decisions, because they inﬂuence the features of the ﬁnal corpus. is is especially crucial

5
because web corpus construction (as understood here) is a process which must usually be fully
automated due to the size of the corpora. All automatic processing alters the original data, it
comes with a certain error rate, and it might decrease the quality of the data instead of increasing
it. is concerns the tasks of removing duplicate documents, removing design elements (menus,
copyright strings, etc.) from web pages, and other cleanup and processing procedures. A good
example is removal of duplication. If it is performed at the paragraph level instead of the document
level, single documents will end up being incomplete in the ﬁnal corpus. For some researchers,
this might simply not be acceptable. Given the diverse applications of web corpora, users have to
make the design decisions themselves, based on our description of how the procedures work.
We have divided the book into four main chapters. Each chapter covers a major topic in
web corpus construction, and they are ordered in the usual order of practical corpus construction.
Chapter 2 describes the theory and technology of data collection (crawling). Chapter 3 provides
the non-linguistic cleansing which is usually applied to the collected data. Chapter 4 discusses the
problems which arise in the linguistic processing (tokenizing and annotations like POS tagging) of
web data. Finally, Chapter 5 introduces some methods of determining the quality of the compiled
corpora.

r

7
C H A P T E R
2
Data Collection
2.1
INTRODUCTION
In this book, we consider a web corpus to be a static collection of a number of documents down-
loaded from the World Wide Web (or WWW or just web). Some aspects which we discuss are
only relevant if the web corpus to be constructed is intended for work in empirical Linguistics or
Computational Linguistics. Clearly, data collection and post-processing for search engine appli-
cations partially require diﬀerent strategies. While in search engine applications the primary goal
is to ﬁnd and index documents relevant to user search queries, documents in a web corpus should
meet the linguistic criteria set by the corpus designers. Also, while search engines’ databases are
kept up to date by constantly refreshing (re-crawling) the data, a web corpus should (at least
for linguistic research) not change in order to allow for reproducible corpus experiments. Since
there are probably as many sets of design criteria for corpora in linguistic research as there are
researchers in the ﬁeld, we try to lay out the relevant knowledge and the necessary procedures
such that any linguist or computational linguist is put in a position to decide upon a method of
designing their corpus.
In this chapter, we discuss the acquisition of the raw data, which is the ﬁrst crucial step in
web corpus construction, under these premises. e properties of the ﬁnal corpus is the sum of
the properties of these web documents minus the documents/properties lost/changed in the post-
processing steps to be outlined in later chapters. erefore, it is crucial to give some attention to
the structure of the web and the methods which we use to discover the documents (i. e., crawling).
e structure of the web is brieﬂy described in Section 2.2. Crawling is covered in greater detail
in Sections 2.3.2 through 2.4. Due to the practical goals of this book, the amount of theoretical
considerations is kept at a minimum. However, it cannot be avoided that this chapter is the least
practical of all, mainly because data collection for web corpus construction has received the least
amount of attention from the linguistic community. A more thorough but still very accessible
introduction to most topics of this chapter can be found in Chapters 19–21 of Manning et al.
[2009]. A more detailed introduction to crawling is Olston and Najork [2010].¹
Having read through this chapter, readers should:
¹We ignore incremental crawling (strategies of re-crawling pages at certain intervals to check whether they still exist and to
refresh a search engine index) in this book, simply because it is very speciﬁc to search engine-related crawling. Since web
corpora are static resources according to our deﬁnition, incremental crawling does not make sense. It is covered in Chapter 5
of Olston and Najork [2010].

8
2. DATA COLLECTION
1. know about the structure of the web and which web pages they can expect to ﬁnd through
web crawling,
2. be able to ﬁnd sources from which they can obtain the URLs needed to start a crawl,
3. know how to conﬁgure available crawler software such that they download and store mostly
relevant documents without putting too much strain on the servers from which they request
the documents.
Secondarily, readers are made aware of the following advanced topics, which we do not
cover exhaustively, though:
1. how crawling strategies aﬀect the sample taken from the web graph,
2. where they can look for further information regarding more advanced and intelligent crawl-
ing strategies which allow predictions about the language, the contents, etc., of documents
not yet downloaded.
2.2
THE STRUCTURE OF THE WEB
2.2.1
GENERAL PROPERTIES
e most remarkable feature of the web is its connectedness, the fact that it contains so-called
hypertext—a concept described as early as 1989 in Berners-Lee [1989]. Essentially, this means
that it contains documents, and that these documents link to each other. Each link takes the
form of a Uniform Resource Locator (URL) which speciﬁes the transfer protocol in most cases
as either http:// or https://. Let I.p/ be the set of pages linking to p and O.p/ the set of
pages linked to by p, then the in-degree ID.p/ of p is simply jI.p/j, the out-degree OD.p/ is
jO.p/j.² I. e., each page has a number (possibly 0) of pages to which it links—its out-degree OD—
and a number (possibly 0) of documents by which it is linked—its in-degree ID. Links cannot be
followed backward, and the web graph is thus a directed cyclic graph. e in-degrees of pages are
empirically distributed according to a Pareto distribution or power law with 2:1 being reported
for the ˛ parameter [Manning et al., 2009, 426]. (But see Section 2.2.3 and especially Figure 2.3
for more details on the ˛ parameter.) For the proportion P.i/ of pages having in-degree i, we
have:
P.i/ D
1
i2:1 D i 2:1
(2.1)
In terms of network theory, this distribution of links makes the World Wide Web a scale-
free network, as opposed to a random network.³ So, there is a relatively small number of pages
with a very high in-degree and a large number of pages with a low in-degree. Pages with a high
in-degree are comparatively easy to ﬁnd by following links from other pages. However, they are
not necessarily the best pages to download for web corpus construction. Part of Section 2.4 is
²e terms (web) page and (web) document are used interchangeably here.
³An accessible introduction to scale-free networks is Barabási and Bonabeau [2003].

2.2. THE STRUCTURE OF THE WEB
9
devoted to the task of optimizing the document discovery process toward exploring pages with
low in-degrees.
In an early paper, Broder et al. [2000] divided the web into various components, based on
how they are connected. Broder et al. introduced what is called the bowtie structure of the web
(Fig. 2.1).
Figure 2.1: Simpliﬁed bowtie structure of the web [Broder et al., 2000].
A page p in IN has ID.p/ D 0 and OD.p/ > 0, such that it cannot be found by following
links. Pages in OUT have OD.p/ D 0 (except for links to pages in a TENDRIL, cf. below) and an
ID.p/ > 0, such that they can be found by following links, but, in fact, constitute a dead end. e
strongly connected component (SCC) contains only pages with both ID.p/ > 0 and OD.p/ > 0,
and it is the case that for any pages p1 and p2 within SCC, there is a link path from p1 to p2.
A TENDRIL is either a page or a chain of linked pages which is linked to by a page from IN or
OUT which does not lead to the SCC. A TUBE, which also can be a single page or a chain of
pages, connects IN and OUT without oﬀering a path to any of the SCC pages.
In Broder et al. [2000], it was found that the components have roughly equal sizes. e
considerable size of IN, for example, is due to the fact that any freshly created page is usually not
linked to by any other page at the moment of its creation. Typical pages in OUT are company
websites, which often do not link to external pages. A more detailed and recent paper about the
size of the components is Serrano et al. [2007]. e next section deals with general accessibility
properties of web pages which lie beyond the question of whether they can be reached by following
links.
2.2.2
ACCESSIBILITY AND STABILITY OF WEB PAGES
Crawling relies on the links between web pages to discover huge numbers of them, simply by
recursively following some or all links from the downloaded pages. For this to work, the crawling

10
2. DATA COLLECTION
process obviously needs to be started with a set of start URLs (the seeds) from which the discovery
process begins. is section discusses which types of pages a crawler can retrieve, given this basic
strategy.
A page p that has ID.p/ D 0 cannot be retrieved by crawling unless its URL is known
at the start. However, there are pages which cannot be retrieved for other reasons, even if their
URLs are known. For example, a company web server might only serve documents to IP addresses
which belong to the company. If the crawler is run from outside the company, it will not be able
to download such content. Also, many web servers require login information to access all or some
of the content provided (paid services, social networks, forums, etc.).
Even if no login is required, certain content is only served after a form has been ﬁlled out.
e scripts which process the form input usually make sure that the input makes sense, i. e., that
it was entered by a human and not guessed by a machine. Of course, even without such checks,
the output generated by a form might be empty if the input is nonsensical. If a machine enters
random words into the query ﬁeld of a database front end which looks up information about
medical drugs, for example, the output will most likely be empty in most cases. Such documents
have to be considered part of the so-called Deep Web, which cannot be accessed without signif-
icant additional overhead compared to simple crawling. Search engine companies use advanced
guessing strategies on web pages containing forms in order to surface otherwise hidden database
content. Such techniques are beyond the scope of this book, but for further reading, Madhavan
et al. [2009] is recommended.
Also, server administrators can ask crawling software to stay away from certain documents
using the Robots Exclusion standard by creating a ﬁle robots.txt directly in the root directory of
the server. e ﬁle contains information for (all or speciﬁc) crawlers as to which directories and
documents they may retrieve, and which ones they must not retrieve. Pages blocked by Robots
Exclusion are not technically part of the Deep Web, but are nevertheless inaccessible for web
corpus construction. As will be explained in Section 2.3.3, not obeying a server’s Robots Exclusion
requests is unwise and can lead to a crawling institution acquiring a bad reputation. In the worst
case, server administrators might block access by the crawler at the IP level.
Furthermore, a distinction must be made between a web URL and the document which
can be retrieved by requesting the URL. Even traditional static pages are updated sporadically or
periodically, i. e., old content is deleted or altered, and new content is added. us, a URL is never
an appropriate identiﬁer to address the contents of a single document. With the introduction
of content-management systems, which generate pages from content databases each time the
page is accessed (using HTTP header information or IP-based client geo-location to generate an
appropriate page), this is now more true than ever. Not even two exactly synchronous accesses to
a web page necessarily lead to the same document.
Finally, links are ephemeral, a property known as link rot. A URL which points to a docu-
ment is never guaranteed to point to anything in the future. Link rot was studied from the early

2.2. THE STRUCTURE OF THE WEB
11
days of the development of the web. Seminal papers include Fetterly et al. [2003] and Bar-Yossef
et al. [2004].
2.2.3
WHAT’S IN A (NATIONAL) TOP LEVEL DOMAIN?
In this section, we brieﬂy discuss what levels of structure (strata) are usually assumed to be dis-
cernible within the whole web. We focus especially on (national) top level domains (TLD), be-
cause they play a major role in web corpus construction. us, with this section, we begin to
consider the web as the population of documents from which we draw a sample when we build a
static web corpus. e stratiﬁcation of the ﬁnal corpus depends on the stratiﬁcation of the popu-
lation (this section) and on our method of sampling (Sections 2.3 and 2.4).
Firstly, we can examine the more or less technical stratiﬁcation of the web, as it is done in
so-called web characterization. Technical features examined in web characterization include, e. g.,
URL lengths, quantities of diﬀerent image formats, server software, link structure. A large-scale
comparison of features, methods, and results (for 24 diﬀerent countries) can be found in Baeza-
Yates et al. [2007]. Put in a larger context, such features can be correlated with external features,
as it is done in so-called Webometrics. A seminal paper on Webometrics, the quantitative study
of the structure of the web in relation to social, economical, political, and other external variables,
was Björneborn and Ingwersen [2004]. For an introductory text, cf. elwall [2009].
Björneborn and Ingwersen [2004] and the subsequent literature suggest that there are
around ten levels of granularity in web characterization, and we reproduce here the diagram from
Baeza-Yates et al. [2007, 3] in Figure 2.2.
Byte
Character
Word
Block
Page
Sub-Site
Site
Domain
TLD
National TLD
Global Web
Figure 2.2: Web characterization: levels of granularity.
While the byte level is only relevant technically and in relation to the character level (e. g.,
in cases where one byte does not necessarily correspond to one character, cf. Section 3.2.3), the
character level is of some importance for web corpus construction. is is because language iden-

12
2. DATA COLLECTION
tiﬁcation (Section 3.4) is often based on the distribution of characters or character n-grams in
documents. e word level can also be used in language identiﬁcation, but additionally, words
and blocks are also important in register and genre classiﬁcation (not covered in this book).
e sub-site, site (roughly equivalent to host) and domain (roughly equivalent to server)
levels provide important clues as to the nature of the sample of documents. If, for example, our
web corpus is supposed to be a sample from a TLD, then for determining the success of the
sampling process, it is crucial to check whether the distribution of hosts within the TLD ﬁts the
distribution of hosts in the corpus.
Finally, TLDs and national TLDs are important because many monolingual web corpora
are based on crawls of national TLDs which belong to a country with a single dominant national
language as a crude method of building monolingual corpora. is is true, e. g., for the WaCky
corpora [Baroni et al., 2009] and the COW corpora [Schäfer and Bildhauer, 2012]. Also, many
comparatively easy-to-calculate metrics have been studied for national TLDs. In Baeza-Yates
et al. [2007], a greater number of studies of national TLDs were aggregated, and the authors
found that most metrics have characteristic and stable distributions across all national TLDs
(Figure 2.3). Almost all metrics are distributed according to a power law, and the ˛ parameter
can be estimated (cf. also Section 2.2.1). Such known metrics could and often should be used in
adjusting the sampling procedure and in the evaluation of ﬁnal corpora.⁴;⁵;⁶
In Figure 2.3, nothing is said about the distribution of languages. is is because the situa-
tion is diﬀerent for every TLD. In cases where there is only one or one clearly dominant national
language, the majority of pages not in this language is usually in English. In African countries,
English pages are reported to account for over 75% of all pages, in Spain for 30%, and for 8%
in Chile. For multilingual countries, statistics vary according to the individual socio-linguistic
situation.
Since it was said above that crawling national TLDs is a crude method of constructing
monolingual corpora, some consideration should be given to the question of how crude this
method actually is. First of all, for countries with more than one dominant language or dialect,
additional language identiﬁcation and ﬁltering is required. Secondly, the method does not work
equally well for all national TLDs, especially when this TLD does not have high prestige. e
.us TLD is a well-known example of a TLD with low prestige. If we crawled the .us domain,
the yield of the crawl might be comparatively poor (considering the size of the U. S. English
web), and certain types of documents might be underrepresented in the sample, simply because
major organizations never use the .us TLD. e .es TLD is a good example of a mix of such
problems. Among the oﬃcial languages are three closely related ones (Castilian, Catalan, and
Galician), which are all in use, even in online newspapers and on oﬃcial websites. On the other
⁴In the Double Pareto cases, two ˛ values are ﬁtted. For example, ﬁle sizes are reported to follow a power law for smaller ﬁles,
and another power law for larger ﬁles.
⁵Values for within-site links are normalized relative to the number of documents within the respective site.
⁶Basic data, including metrics for (national) TLDs, can be obtained from the survey regularly conducted by the Internet Systems
Consortium: http://www.isc.org/solutions/survey/
www.allitebooks.com

2.2. THE STRUCTURE OF THE WEB
13
Metric
Distribution
Parameter and mean examples for TLDs
ﬁle sizes in KB
Double Pareto
Korea: Nx D 10, ˛1 D 0:4, ˛2 D 3:7
Brazil: Nx D 21, ˛1 D 0:3, ˛2 D 3:4
page age in months
Exponential
Brazil: Nx D 11:6,  D 2:1 (rate)
Greece: Nx D 17:7,  D 1:6 (rate)
pages per site
Pareto
Spain: Nx D 52, ˛ D 1:1
Italy: Nx D 410, ˛ D 1:3
sites per domain
Pareto
U. K.: Nx D 1:2, ˛ D 1:6
Spain: Nx D 2:5, ˛ D 2:3
in-degree
Pareto
Chile: Nx D 8:3, ˛ D 2:0
U. K.: Nx D 16:2, ˛ D 1:8
out-degree
Double Pareto
Spain: Nx D 3:6, ˛1 D 0:9, ˛2 D 4:2
Italy: Nx D 31:9, ˛1 D 0:7, ˛2 D 2:5
average within-site links
Double Pareto
Spain: Nx D 0:1, ˛1 D 1:5, ˛2 D 2:5
Brazil: Nx D 3:5, ˛1 D 0:8, ˛2 D 2:9
URL length (characters)
Log-Normal
Portugal: Nx D 67
U. K.: Nx D 76
Figure 2.3: Examples of web characterization metrics [Baeza-Yates et al., 2007] with illustrative ex-
amples. Countries were chosen here to illustrate the typical range of values as found in the aforemen-
tioned paper.
hand, the .es domain does have a mixed prestige in Spain. While the largest Spanish newspaper
El País prefers the non-national domain elpais.com, the second largest newspaper El Mundo
uses the national domain elmundo.es.
Another question goes beyond the problems just outlined, namely whether national TLDs
of diﬀerent countries actually represent national variants. Cook and Hirst [2012] recently found
that for .ca and .uk, features of Canadian and U. K. English can actually be detected in corpora
compiled from national TLD crawls. e method they use is based on comparisons to tradition-
ally compiled corpora of the respective varieties, applying both keyword comparison (Kilgarriﬀ,
2009; see also Section 5.4) and comparison of spelling variants. Such tendencies, however, are
only observable for entire corpora, and they do not warrant the conclusion that any speciﬁc docu-
ment downloaded from the .uk domain can be safely assumed to represent U. K. English. In fact,
the known similarity of a U. K. web corpus to a traditional corpus of U. K. English like the BNC
[Leech, 1993] with respect to keyword distributions does not necessarily allow us to draw any
further conclusions, for example that the syntax of the web corpus bears a signiﬁcant resemblance

14
2. DATA COLLECTION
to the traditional corpus. It is an open question whether linguists could and should do research
on regional variants of languages using crawls of national TLDs.⁷
2.2.4
PROBLEMATIC SEGMENTS OF THE WEB
In this section, we brieﬂy discuss two kinds of segments of the web that are most likely unattractive
for corpus construction. We also discuss how they can be avoided, leading over to Section 2.3.
First, with the advent of content management systems and other kinds of content generated
ad hoc by scripts executed on web servers, in most cases using underlying data bases (generally
called dynamic web pages), many websites are of virtually inﬁnite size. e standard example cited
in many papers is a script-based calendar system which generates links from any day, month, or
year view to the subsequent view of the same kind. e virtually inﬁnite segments of the web,
which are generated by such systems, can be detected relatively easily because:
1. all links are host-internal,
2. the link URLs contain HTTP GET parameters (after the ?).
is means that such link sequences can be avoided by:
1. setting a quota on the maximum number of pages per host or per server allowed in our
sample,
2. stripping all GET parameters from link URLs, or
3. keeping track of and limiting the depth of a link chain a crawler follows within a single
host.
Techniques 1 and 2 are problematic because they will inevitably lead to many pages being
excluded that are not part of the kind of web graph segment which we are trying to ignore. Espe-
cially host and server quotas might lead to many pages being excluded from the sample. As it was
shown in Figure 2.3, the pages per site and sites per domain metrics follow a Pareto distribution.
Strongly represented hosts (forum and blog providers like Blogspot or Wordpress or newspapers)
would most likely not be represented adequately in the sample. Technique 3 is therefore prefer-
able.
e second kind of problematic segment is more diﬃcult to detect, mostly because it is
created with the intention of being hard to detect. Search engines use not only the contents
of web documents to determine whether the document is relevant to a speciﬁc user query, but
they also weight the results by the link context a page stands in. Simply speaking, the higher
the in-degree of a page, the higher its relevance (cf. Section 2.4.2 for a brief introduction to the
more adequate PageRank metric). is strategy is used and/or abused by content providers to
improve their position in search engine rankings. A bundle of techniques called search engine
⁷In 2013, Mark Davies released a corpus of English web documents classiﬁed according to Google geo-location information.
No scientiﬁc papers have been released describing or evaluating the quality of this resource, but it might be an interesting
project in the context of the questions discussed in this section: http://corpus2.byu.edu/glowbe/

2.3. CRAWLING BASICS
15
optimization (SEO) is used, one of the most popular being link farm creation. A link farm is a
number of servers or hosts which serve documents with very high link degrees among them. is
heightens the reputation of the pages in search engines, such that links from the link farm to,
e. g., product listings or commercial services also acquire a high reputation.
Link farms are not as easy to detect as are virtually inﬁnite (but benevolent) websites, be-
cause:
1. they are distributed between diﬀerent hosts,
2. they often serve fake content, either auto-generated or obtained by scraping content from
other websites,
3. there are other benevolent communities with similar link structures (e. g., the so-called
blogosphere).
Techniques to avoid the sampling of link farms are beyond the scope of this book. e
number of papers dedicated to SEO and counter-measures by search engines is huge, but a rec-
ommended introduction to this whole ﬁeld of adversarial web search is Castillo and Davison
[2011]. Some available crawlers (to be discussed immediately in Section 2.3) implement at least
some techniques to avoid link farms, or they can be extended to implement such techniques.
2.3
CRAWLING BASICS
2.3.1
INTRODUCTION
Crawling (sometimes also called spidering, with minimal semantic diﬀerences) is the recursive
process of discovering and downloading web pages by following links extracted (or harvested)
from pages already known. In its simplest form, crawling is a breadth-ﬁrst traversal of the web
graph by extracting all URLs from all downloaded pages and adding them to one huge queue
of URLs of pages yet to be downloaded. At least under the assumption that the web is static,
and given the macroscopic structure of the web as described in Section 2.2.1, one start URL
from the IN or SCC component is enough to start a discovery process which will eventually
lead to the discovery of the whole SCC. is is because we required that for each pair of pages
p1 and p2 in SCC, there must be a link path from p1 to p2. In theory, it is also possible to
do an exhaustive Random Walk through the web graph, if we have enough crawling time and
a backtrack mechanism to return from accidentally crawled pages in OUT or a TENDRIL. In
a Random Walk, the software follows a randomly selected single link from each page without
queueing the other links. More on Random Walks can be found in Section 2.4.
Of course, the web is not static (although, pragmatically, for many experiments such an
assumption is made), because it contains potential inﬁnities, and it is extremely large. erefore,
exhaustive crawls are practically impossible.⁸ e immense size of the web means that a web
⁸Notice that inﬁnite paths such as described in Section 2.2.4 are not necessarily a TENDRIL. Taking up the calendar example,
each auto-generated page might also contain links back to normal SCC pages, such that the inﬁnite path is fully contained in
SCC.

16
2. DATA COLLECTION
corpus will always be a rather small sample from it. Even in 2005, the indexable web (roughly
the web without the Deep Web and virtual inﬁnities) was estimated at 11.5 billion pages by Gulli
and Signorini [2005], and in July 2008, Jesse Alpert and Nissan Hajaj of Google announced in
an oﬃcial blog post that they had indexed the signiﬁcantly larger number of one trillion web
pages.⁹ e art of eﬃcient crawling therefore consists in ﬁnding the most relevant pages as early
as possible during the crawl. is is usually formulated as the task of maximizing the weighted
coverage WC of a crawl at each crawl time t, where C.t/ is the set of crawled pages at t and w
a weight function, which determines the relevance of each page relative to the goal of the crawl
[Olston and Najork, 2010, 29]:
WC.t/ D
X
p2C.t/
w.p/
(2.2)
e deﬁnition of w is crucial in determining strategies for the optimization of the weighted
coverage. As will be discussed in Section 2.4, available crawling strategies are often well under-
stood under a search engine (SE) perspective, where the weight function wSE is such that, for
example, pages with a high in-degree (or rather a high PageRank, cf. Section 2.4.2 ) are assigned
a high weight. However, if we are looking for pages which might not necessarily be popular or
authoritative in a search engine sense, strategies have to be adapted.
Before going into such details, however, we ﬁrst need to clarify the practical steps which
are identical for virtually all crawling strategies. ese are:
1. Collect a set of URLs to start with (usually called seed URLs or just seeds),
2. conﬁgure the crawler software to accept only desired content (TLD restrictions, ﬁle name
patterns, MIME types, ﬁle sizes, languages and encodings, etc.),
3. decide on crawler politeness settings, i. e., the settings which inﬂuence the strain the crawler
puts on any of the hosts from which it tries to retrieve documents (the number of requests
within a certain period of time, the bandwidth, etc.),
4. run the crawler, usually observing its progress to make sure the crawl does not “go wrong.”
Since a popular implementation of step 1 is identical to a simpler and crawling-free method
of web corpus construction—the BootCaT method—we ﬁrst describe this method now in Sec-
tion 2.3.2.
2.3.2
CORPUS CONSTRUCTION FROM SEARCH ENGINE RESULTS
In this section, we describe the BootCaT method of web corpus construction, mainly because its
ﬁrst step can also be used as a method to gather seed URLs, and because it played a historic role
in practical ad-hoc web corpus construction. It was described in Baroni and Bernardini [2004]
and is implemented as a chain of shell scripts plus a Java-based GUI called BootCaT (“Boot-
⁹http://googleblog.blogspot.de/2008/07/we-knew-web-was-big.html

2.3. CRAWLING BASICS
17
strapping Corpora And Terms from the Web”).¹⁰ e goal is to obtain random samples from a
search engine’s index (possibly restricted to URLs from one national TLD), download the pages
corresponding to the returned URLs, and create a corpus from them. e corpora are relatively
small compared to crawled corpora, but the method itself and its implementation are quite user-
friendly, minimizing the eﬀort that needs to be invested in the construction of the corpus.
As it was described in Chapter 1, search engine query results are not uniform random sam-
ples from the search engine’s index, but the exact opposite, namely samples which are extremely
biased toward documents which the search engine provider has classiﬁed as relevant according to
some metric unknown to the user. For entirely diﬀerent purposes, namely the comparative bench-
marking of search engine performance and coverage, researchers have tried to ﬁgure out ways of
sampling randomly from search engines’ indexes without having direct access to the database.
is kind of research starts with Bharat and Broder [1998]; an important later paper was Bar-
Yossef and Gurevich [2006], which also summarizes the criticism of the method proposed by
Bharat and Broder.
In Bharat and Broder [1998], it is suggested to
1. construct a lexicon of terms with their frequencies as found on the web,
2. eliminate extremely low frequency terms (which might be misspellings or other types of
noisy tokens as discussed in Chapter 4),
3. formulate random multi-word queries (both conjunctions and disjunctions) by sampling
words from the lexicon according to their frequency,
4. pick URLs from the set of URLs returned by the search engine uniformly at random.
ere is no restriction to using query words in a speciﬁc language in this proposal. As Bharat
and Broder notice, there are a number of biases which this method introduces. ere is a ranking
bias, because for most queries, the search engine could return many more results than it actually
does. Only the results which are considered to be the most relevant are actually returned by the
search engine, which makes true uniform random sampling from the results impossible.
e second most important bias is the query bias. By sending very speciﬁc combinations of
terms, we ask for longer, content-rich pages, because they have a higher chance of matching such
queries. In the worst case (also mentioned in Bar-Yossef and Gurevich, 2006), many results are
dictionaries or word lists. To illustrate these theoretical ﬁndings, Figure 2.4 shows the result of one
of our own experiments from May 2012. We queried a meta-search engine with 10,000 unique
4-tuples constructed from 5,000 mid-frequency Celex [Baayen et al., 1995] word forms (rank
1,001–6,000). 127,910 URLs were harvested; 70,897 of them were unique (55.43%). e ﬁgure
shows the top 15 returned pages and how often they were returned. It is immediately obvious
that querying random 4-tuples very often leads to documents which are word lists and similar
resources.
¹⁰http://bootcat.sslmit.unibo.it/

18
2. DATA COLLECTION
Count
URL
3947
http://www.let.rug.nl/˜vannoord/ftp/DutchBrillTagging/Entire_Corpus/BIGWORDLIST
2768
http://www.ai.rug.nl/vakinformatie/PTenS/pract/docs/totrank.txt
2682
http://hmi.ewi.utwente.nl/spraakgroep/capita[…]bn-baseline.until310806.51K.v2.vocab
2022
http://www.ntg.nl/spelling/latin1/woorden.min
1293
http://www.ntg.nl/spelling/latin1/woorden.med
1088
http://anw.inl.nl/doc/lemmalijst%2015%20plus.txt
1021
http://anw.inl.nl/doc/fulllemmalist.txt
959
http://www.ekudos.nl/artikel/nieuw?url=$href&amp;title=$title&amp;desc=$lead
948
http://homepages.cwi.nl/˜jve/lexsem/Lemma_Pos_Frq_CGN.lst
762
http://www.ntg.nl/spelling/ibmpc437/woorden.min
650
http://www.win.tue.nl/˜keesh/dokuwiki/lib/exe/fetch.php?[…]dutchwords.txt
603
https://www.os3.nl/_media/2008-2009/students/willem_toorop/woorden.txt
591
http://www.ai.rug.nl/nl/vakinformatie/PTenS/pract/docs/totrank.txt
534
http://ens.ewi.tudelft.nl/donau/groene_boekje
364
http://www.let.rug.nl/˜vannoord/ftp/DutchBrillTagging/Entire_Corpus/BIGBIGRAMLIST
343
http://frii.nl/outfile.txt
331
https://www.os3.nl/_media/2008-2009/[…]/ssn/practicum/opdracht_2/dutch.txt
313
http://www.ai.rug.nl/vakinformatie/PTenS/docs/totrank.txt
309
http://www.ntg.nl/spelling/ibmpc437/woorden.med
275
http://www.let.rug.nl/˜vannoord/ftp/DutchBrillTagging/Entire_Corpus/TRAINING.LEXICON
Figure 2.4: Top pages returned for random 4-tuples (Dutch Celex word list) sent to a meta-search
engine with the number of times they were returned. Very long lines are abbreviated by […].
To alleviate biases, Bar-Yossef and Gurevich suggest and evaluate Monte Carlo methods
of random sampling under diﬃcult sampling conditions, such as the Metropolis-Hastings algo-
rithm. ey also modify the sampling procedure, eﬀectively performing a Random Walk through
the search engines’ indexes. For the BootCaT method, however, the assumption is that sending
3- or 4-tuples of mid-frequency terms as conjunct queries to search engines and harvesting the
ﬁrst n results is a random enough process or at least that it delivers documents which are relevant
and diverse enough for corpus construction [Ciaramita and Baroni, 2006]. It has to be kept in
mind, however, that the returned documents will at least be biased toward word lists and long
documents. Whether a corpus designer is satisﬁed with this is an individual decision.
We also have to keep in mind the facts mentioned in Chapter 1 about implicit search term
modiﬁcation. Whether or not they are a result of sponsored ranking, search results for random
tuples as in Figure 2.5 are common. Notice the morphological modiﬁcation of search terms (glory
for glorious) and even the expansion of lexical words to brand names (circles to PureCircle). is,
above all, means that controlling the frequencies of the words used for the construction of the
queries is quite ineﬀective, because the search engine heavily modiﬁes the queries. See Chapter 1
(esp. page 2) for further search term modiﬁcations applied by search engines.
One ﬁnal practical problem makes the BootCaT method unattractive. To construct corpora
of considerable size, a lot of queries need to be sent to the search engine in an automated way.
Search engines usually do not allow this both legally and technically. API-based access for bulk
requests used to be available during beta phases of all major search engine providers, but the last

2.3. CRAWLING BASICS
19
Query: glorious discretion virtually unhappy
Dancing in the Glory of Monsters Jason Stearns[1]
Dancing in the Glory of Monsters Jason Stearns[1] - Free ebook download as …and could
legislate by decree and change the constitution at his discretion. …a fanciful spiritual
order that sold banking licenses in the name of a virtual state. …was hospitalized in South
Africa—and was obviously unhappy with the question.
www.scribd.com/…/Dancing-in-the-Glory-of-… – Cachad
Query: circles texts ingredients procurement
PureCircle: high purity stevia volumes up 20% plus
9 Jul 2012 …Carbohydrates and ﬁbres (sugar, starches)  Cereals and bakery preparations 
Chocolate and confectionery ingredients  Cultures, enzymes, yeast …Text size Print Email
…High purity stevia volume sales at PureCircle have increased by …kindly send me details
of procuring good quality stevia extract in …
www.foodnavigator.com/…/PureCircle-High-p… – Cachad
Figure 2.5: Top-ranked Google results for some random English tuples on July 31, 2012, queried
using a Swedish Firefox from a German IP. e output is a verbatim reproduction of the entries in the
Google results page, including the document summary.
free API by a major provider (Microsoft Bing) was shut down in late 2012. Major providers like
Bing, Google, and Yahoo also do not serve clients like curl or GNU wget, which could be abused
to send huge amounts of requests without an API, extracting the results from the normal HTML
pages returned.¹¹;¹² Finally, even if someone manages to send bulk queries, her/his IP addresses
usually get banned very quickly. us, the BootCaT method can no longer be used productively,
unless the user is willing to pay and capable of paying signiﬁcant amounts of money for search
engine API access. It can be test-run, however, using the BootCaT tools and very limited demo
access. ese problems with the BootCaT method have consequences for crawling approaches,
as will become clear in Section 2.3.5.
2.3.3
CRAWLERS AND CRAWLER PERFORMANCE
We now turn to the basic architecture of crawler software with a focus on those conﬁgurable parts
of the software which inﬂuence the performance. A crawler (also: bot, robot, spider) is a piece
of software that performs the act of crawling as deﬁned in Section 2.3.1. Starting from a set of
URLs (seeds), it proceeds to discover virtually unlimited amounts of pages by following links.
Using a crawler, as opposed to BootCaT-like tools, therefore allows the construction of corpora
¹¹http://curl.haxx.se/
¹²http://www.gnu.org/software/wget

20
2. DATA COLLECTION
virtually unlimited in size. For example, Pomikálek et al. [2012] describe a 70 Billion word corpus
of English based on the 1 Billion page ClueWeb09 data set,¹³ which was obtained via crawling.
Crawler architectures are conceptually rather simple, but require careful implementations
if the crawler is intended to sustain a high download rate over long crawl times. To give an ap-
proximation of the rate at which the crawler must download documents, we ﬁrst need to get an
impression of how many of the downloaded documents will make it into the corpus. Of course,
this rate of loss due to cleanups depends on many design decisions, i. e., on the deﬁnition of
which documents should be removed. Table 2.6 gives the ﬁgures for the DECOW2012 corpus
[Schäfer and Bildhauer, 2012]. e cleanup steps correspond roughly to what will be described
in Chapter 3.
Algorithm removes…
No. of documents
Percentage
very short pages
93,604,922
71.67%
non-text documents
16,882,377
12.93%
perfect duplicates
3,179,884
2.43%
near-duplicates
9,175,335
7.03%
total
122,842,518
94.06%
Figure2.6: Amount of documents lost in the cleanup steps of DECOW2012, a 9 billion token corpus
of German from a crawl of the .de domain.
Over 90% of the documents (in this case 94.06%) do not make it into the ﬁnal corpus. Let
us assume for the sake of simplicity that we could keep 10% of the documents. is means that
we have to crawl at least ten times as many documents as we actually require. For a one million
document corpus, we would have to crawl 27.8 hours at a rate of 100 documents per second. If
we only achieve half that speed, crawling time will be 55.5 hours, etc.
For each document, the crawler has to perform DNS lookups, possibly wait for a few sec-
onds due to politeness reasons (Section 2.3.4), send the HTTP request, wait for the response,
maybe retry, and ﬁnally extract the links from the retrieved documents. erefore, a reasonably
high download rate can only be achieved by a multi-threaded program or a fully parallelized piece
of software which is capable of running on a cluster of machines. is, however, increases the
complexity of the whole system, since the huge data structures which we will describe in the next
paragraphs need to be synchronized between the diﬀerent machines.
An introduction to crawler design is Chapter 20 of Manning et al. [2009]. Here, we only
list the most important components that a crawler user minimally has to be aware of, ignoring
implementation and parallelization issues completely. A crawler consists of the following com-
ponents (not mentioning those which are less critical in conﬁguration, like the ﬁle writer which
stores the downloaded content):
¹³http://lemurproject.org/clueweb09/

2.3. CRAWLING BASICS
21
• fetcher—a massively multi-threaded component which downloads documents correspond-
ing to URLs in the frontier,
• parser or harvester—a component which extracts new URLs from web pages (at least from
<a href></a> tags),
• URL ﬁlters—code that discards URLs which are duplicate or which do not conform to
certain criteria (length, blacklisted hosts, Robots Exclusion, etc.),
• frontier—data structures which store, queue, and prioritize URLs and ﬁnally pass them to
the fetcher.
Crawlers can slow down because of huge data structures in the frontier which grow over
time, and because the fetcher usually spends a lot of time just waiting. To illustrate frontier growth,
assume a frontier contains 100 million URLs and a URL is a string of a mean length of 64 bytes
(which is roughly adequate, cf. Figure 2.3 on page 13). is produces approximately 5.96 GB
of data just for the URLs in the frontier. Such amounts of data can be processed in memory
today, but if we want to calculate with even simple weight functions (cf. Section 2.3.1) in order to
re-prioritize such a large queue, we will lose a lot of time. To make a crawler scale arbitrarily, we
cannot even rely on there being enough system memory, however, and disk-based re-prioritization
of such huge queues takes considerably longer.
Even the URL ﬁlters create large data structures, for example if a URL Seen Test is used.
A URL Seen Test determines for any harvested URL whether it was already downloaded or is
already queued to avoid multiple queuing of the same URL. Performing a naïve database lookup
would be far too slow, so usually Bloom ﬁlters or similar algorithms are used.¹⁴ Classic Bloom
ﬁlters require approximately m bits for a set S with n D jSj and  the desired probability of a
false positive [Broder and Mitzenmacher, 2004, 491]:
m  nlog2. 1
 /
ln 2
(2.3)
For example, assuming jSj D 108 and a false positive rate of one every 15 keys on average
( D 10 5), we require at least 91:17 MB of RAM. is is not a huge amount of RAM by today’s
standards, but if we go up to jSj D 109, we are already at 911:69 MB, etc. To make the software
scale arbitrarily, such (and much worse) scenarios have to be considered.¹⁵
As mentioned before, the second bottleneck during the runtime of a crawler is the long
idle (i. e., waiting) time during or in-between requests. First of all, DNS lookups (the process of
¹⁴A Bloom ﬁlter as proposed in Bloom [1970] is an eﬃcient set membership detection algorithm with no chance of false
negatives and a known chance of false positives. Bloom ﬁlters do not store the keys themselves, but store a single long bit array
A of a length l (proportional to the desired error rate), with all bits initially unset. To add a key, the key is hashed with n hash
functions which produce values ai 2 Œ1::l. For each added key and each of its corresponding hash values ai, i 2 Œ1::n, the
bit at AŒai is set. To check for set membership, one checks whether the bits at all AŒai are set. To avoid false negatives, it
is not allowed to remove values from the set, i. e., bits cannot be unset. A good introduction to and survey of Bloom ﬁlters is
Broder and Mitzenmacher [2004].
¹⁵Space-optimized variants of the Bloom ﬁlter as discussed for example in Pagh et al. [2005] usually come with a time penalty
and vice-versa.

22
2. DATA COLLECTION
requesting an IP address for a host name) take up time, such that DNS information for recently
encountered hosts should be cached. Additional activity and waiting time is incurred by politeness
conventions (cf. Section 2.3.4). When the fetcher ﬁnally requests the document, it goes to sleep,
possibly for seconds, just waiting for the data to arrive. Extracting the links, however, can then
be done eﬀectively by regular expression matching (i. e., not necessarily using a DOM parse, cf.
Section 3.2.1), and does not bring about many performance problems. Given these delays, the
crawler can only achieve reasonable performance when it is massively parallel. Parallelization in
this case does not necessarily mean that a lot of processing power is needed: e threads/processes
will be in a sleeping/waiting state most of the time.
Finally, it should be mentioned that the calculation of the crawl rate on page 20 was naïve
in one important way. e crawler can guess (by simple regular expression matching) that certain
types of URLs are deﬁnitely not of the desired type. Usually, URLs ending in .jpg or .flv etc.
can be safely assumed to lead to binary non-text multimedia content. However, many URLs do
not clearly signal the type of data which is served when they are requested, for example if the
default page from a directory is called (URL ends in /), or when a server script is called (ﬁle
part of the URL ends in .php, .asp, etc., possibly followed by GET parameters after the ?).
e crawler has to download such content but has to dispose of it immediately, just by looking at
the MIME type of the received data (which is only known after successful download). It could
be conﬁgured to keep only text/html and text/xml for example. Since the calculation of the
download rate was based only on the original amount of HTML data that was actually stored
by the crawler, we need to discover and download even more documents. It is also wise to set a
maximum ﬁle size, instructing the crawler to stop downloading a ﬁle larger than a few hundred
KB. is is because such large ﬁles are most likely binary/multimedia ﬁles. Without a size limit,
we also regularly crawled documents over 1 GB large which were in fact HTML documents
generated by faulty server scripts stuck in a loop, emitting the same error message until some
server or crawler timeout was reached. Table 2.8 (at the end of Section 2.3.4) summarizes all
important performance-critical features of a crawler mentioned so far.
Available industry-strength and free crawler products include Heritrix [Mohr et al.,
2004].¹⁶ It is relatively easy to conﬁgure, oﬀers robust single-machine parallelization, but is not
well suited to be run on a cluster of machines. A crawler which is capable of running on clusters of
machines (but can of course also run on a single machine) is Nutch.¹⁷ e overhead of setting up
a cluster both in terms of work time and in terms of parallelization overhead should be carefully
balanced against the expected gain in scalability, however. In our experience, crawling in the re-
gion of a few 108 documents does not require setting up a cluster. If the Java VM is given at least
5 GB of RAM, and the crawler stores its data ﬁles on fast machine-local hard drives (not NFS
storage or other slow ﬁle systems), then such a crawl can be completed within days or maximally
a few weeks, using a pool of 100 to 200 threads.
¹⁶http://webarchive.jira.com/wiki/display/Heritrix
¹⁷http://nutch.apache.org/
www.allitebooks.com

2.3. CRAWLING BASICS
23
2.3.4
CONFIGURATION DETAILS AND POLITENESS
It was already mentioned in Section 2.3.3 that there should be some criteria by which the crawler
discards documents during and after download. However, some simple ﬁlters can also be applied
to the harvested URLs before they are even queued, avoiding some of the overhead of documents
which are crawled but later discarded. is section brieﬂy mentions these ﬁlters as well as the
necessary politeness settings any crawler should implement.
Among the ﬁlter criteria applied to URLs are restrictions to a certain domain, either at
the TLD level or at the server level. If we want to compile a corpus of online newspaper text,
we might want to restrict the servers to be crawled to those of the ten or twenty most important
newspaper servers, for example. URL-based scope restrictions are like a whitelist, which explicitly
instructs the crawler what to crawl.
On the other hand, the crawler should be capable of handling IP and URL blacklists. For
a variety of purposes (e. g., ad ﬁltering in browsers), there are block lists available, listing IPs or
server names of spam or ad servers, for example. If the crawler just discards URLs which match
an entry from such a list, it can save a signiﬁcant amount of time. e same might be true if the
user has identiﬁed link farms or simply keywords which occur in certain types of URLs leading
to websites which do not serve documents containing much usable text (like porn sites, gambling
sites, etc.).
URLs are also usually canonicalized. is minimally involves lowercasing them, since
URLs are not case-sensitive, and we can thus eliminate spurious duplicates. It is also possi-
ble to remove https URLs or alter them to http, because a secure protocol is not needed
since no actual personal data is transmitted. Along the same lines, login information as in
http://user:pass@server should be removed, and if desired, even GET parameters after
the ? can be purged. If not purged, they should at least be sorted alphabetically to avoid URLs
which point to the same content, but which contain the same parameters in a diﬀerent order.
For example, the following GET parameter speciﬁcations are equivalent: ?p1=v1&p2=v2 and
?p2=v2&p1=v1.
Session IDs in GET parameters can always be safely removed. ey are IDs in the form of
hash strings by which a server can identify a user who interacts with the server over a span of time
(the session). e server uses the ID to identify stored information about a user over successive
requests made by this user, e. g., a shopping basket, viewing preferences, form data entered in
previous steps, and even login credentials, provided the user passes the correct session ID with
each request. Among the many ways of transmitting a session ID is adding them to the GET
parameters, where diﬀerent server scripting languages sometimes preﬁx their own name to the
name of the parameter (JSESSIONID for Java, PHPSESSID for PHP, etc.). e most important
reason to remove them is that they might create a lot of unique URLs pointing to the same
content, and that the URL Seen Test will classify all of them as unseen.
As for the politeness, there are common recommendations and the Robots Exclusion stan-
dard. e common recommendations concern the frequency with which the crawler sends re-

24
2. DATA COLLECTION
quests to a single host. First of all, most crawlers make sure that there are not two threads which
request documents from the same server (IP) at the same time. Additionally, the crawler should
wait several seconds (at least three, better ﬁve) before re-requesting the same page after a time-
out or before requesting another page from the same server. It is therefore necessary to cache the
information about which servers have been sent a request recently.
Robots Exclusion allows the web server administrator to explicitly ask all crawlers or a
speciﬁc crawler to obey certain restrictions via the robots.txt ﬁle. It should be noticed that the
Robots Exclusion standard is not an oﬃcial standard, but rather a de facto standard with slightly
varying interpretations by diﬀerent crawlers.
To make Robots Exclusion work, a crawler must identify itself by a stable identiﬁer via
the User-Agent header sent in HTTP requests. e identiﬁer contains a name (Googlebot for
Google’s crawler) and optionally information about the crawler, for example a URL which leads
administrators to a website containing information about the purpose of the crawl. It is almost
inevitable that crawler operators receive occasional complaints by server administrators, and in
such cases it is helpful if the operator can rightfully claim that all standard politeness conventions
were obeyed. e header might look like this, for example:
berlin-fu-cow (+http://hpsg.fu-berlin.de/cow)
e identiﬁer can be used by server administrators in the robots.txt to identify a section
of instructions for this speciﬁc crawler, cf. Figure 2.7. e section contains a sequence of Allow
and Disallow directives, which allow/disallow access to directories or ﬁles speciﬁed relative to
the host where the robots ﬁle itself resides. e snippet in Figure 2.7 ﬁrst disallows access to
any ﬁle under the root directory, i. e., it denies access to everything. en, there is an Allow
directive which opens up the /archives/ path (and everything below it). e directives have to
be interpreted sequentially, and for each concrete ﬁle, the last directive which applies determines
the actual access rights. Also, the crawler is asked to wait 10 seconds in between requests to this
host through the Crawl-delay directive.
berlin-fu-cow:
Disallow: /
Allow: /archives/
Crawl-delay: 10
Figure 2.7: Sample robots.txt entry.
us, before the crawler requests a page, or better before a URL is added to the queue in
the ﬁrst place, the robots.txt for the host must be parsed to check whether the crawler is allowed
to download the URL in question. Consequently, Robots Exclusion information should also be
cached, because it is a waste of bandwidth and time to fetch and parse the robots.txt each time in
case multiple pages from the same host are downloaded.

2.3. CRAWLING BASICS
25
Figure 2.8 summarizes important user-conﬁgurable and performance-critical features of a
crawler mentioned in Section 2.3.3 and this section as well as their purpose. We now turn to the
task of gathering seed URLs in Section 2.3.5.
Feature
Purpose
URL scope restriction
Focus the crawl on TLDs like .uk or (lists of) servers.
URL ﬁle pattern ﬁlters
Do not queue URLs which clearly lead to undesirable content.
URL canonicalization
Strip login information, session IDs, make lowercase to avoid spu-
rious duplicates.
IP and URL blacklists
Avoid spam, link farms, ads, etc.
ﬁle size restrictions
Discard too small ﬁles after download and too large ﬁles while down-
loading.
MIME type ﬁlter
Discard undesirable ﬁle types after download.
fetcher thread pool size
Control the overall performance, especially the download rate.
general politeness delays
Be nice to servers if no robots.txt is speciﬁed.
conﬁgurable User-Agent header
Make crawler recognizable.
Figure 2.8: Summary of critical user-conﬁgurable features of a crawler and their purposes.
2.3.5
SEED URL GENERATION
In this section we look at the task of collecting seed URLs for the crawler to start the page discov-
ery process described in Section 2.3.1. We assume that seeds in one speciﬁc language for mono-
lingual corpus construction are required. Essentially, web corpus designers need to ask themselves
the following questions:
1. How many seed URLs are required?
2. Does any notion of “quality” play a role in seed selection?
3. Do they have to be diverse, i. e., cover a lot of diﬀerent hosts, documents from diﬀerent
genres, etc.?
As for question 1, the answer depends on what the corpus designer expects. As argued in
Section 2.2.1, one start URL from SCC is enough to eventually locate all pages within SCC. If we
start with one such page and harvest and queue each URL we discover, then we discover the whole
SCC and OUT. However, it might be the case that certain seeds help to maximize the weighted
coverage (WC) as deﬁned in Section 2.3.1, so question 1 leads directly to question 2. Using a lot
of seeds to maximize the WC only makes sense if they help to discover more relevant documents
more quickly, making both questions unanswerable without a deﬁnition of a weight function w to
determine the relevance of pages.¹⁸ In an experiment in 2011, we deﬁned a makeshift weight func-
¹⁸In connection with the crawling strategy/sampling method, a more fundamental question than WC maximization arises:
Does the selection of seeds inﬂuence the chance of discovering relevant segments of the web graph? is question can only
be answered against a signiﬁcant background in graph sampling methods (cf. Section 2.4 for a minimal background). Maiya
and Berger-Wolf, 2011, 110, present some recent results from comparable networks, showing that many common crawling
strategies suﬀer from relatively high seed sensitivity.

26
2. DATA COLLECTION
tion (similar to Suchomel and Pomikálek, 2012): If the corpus post-processing chain (HTML
removal, boilerplate removal, language/text detection, perfect duplicate removal) does not discard
the document, its weight is 1, else it is 0. Usually [Baroni et al., 2009; Schäfer and Bildhauer,
2012], seed URLs collected using a procedure identical to the BootCaT method (Section 2.3.2)
are considered high quality seeds, so we performed two relatively short crawls (amassing 400 GB
of raw data each) of the .es domain, looking for Castilian documents. One was seeded with
231,484 seed URLs from search engine queries, one with 10 such seed URLs. Figure 2.9 plots
the ratio of output data and input data for snapshots of 400 MB over the crawler runtime.
Figure 2.9 shows that the ﬁnal output/input ratio is roughly the same for a crawl with many
seeds and for a crawl with few seeds, both in terms of the number of ﬁles and the amount of data
left over after post-processing. In the initial phase, however, the crawl started with many seeds
clearly achieves a better WC. is is true for both the ﬁle count and the amount of data, and it
goes on for some time after the seeds themselves are exhausted. With many seeds, the crawler
obviously downloads longer documents, fewer non-text documents, and fewer duplicates in the
initial phase, producing more output for the total amount of data downloaded. All this does not
happen much faster (cf. t1::500), but the drop in time eﬃciency, which is inevitable with every
crawl, does not occur while crawling the ﬁrst 400 GB: While t501::1000 goes up to 35:73h with
few seeds, it is still at 18:75h with many seeds.¹⁹
We see that seed URLs from search engine queries are biased toward long and content-
rich documents with a high chance of being in the correct language (because of the conjunct
tuple query method used to obtain them). We also see that in the breadth-ﬁrst neighborhood of
such documents, there are other similar documents. All in all, it appears as if the drop in crawler
eﬃciency comes roughly 200 GB later according to this experiment. Clearly, the importance of
the improved WC in the initial phase is debatable if the ﬁnal amount of data crawled is in the area
of 5 TB or higher. Keeping in mind that even the deﬁnition of the weight function used in the
experiment is very debatable (because it says that, in essence, we want a corpus which is biased
toward long documents), we can plausibly assume that a moderate amount of seeds is suﬃcient.
In Section 2.4.3, we brieﬂy describe focused crawling strategies which could sustain a high WC
throughout the crawl. is is not (or only weakly) possible by selecting more or better seeds, but
by adapting the algorithm by which the crawler selects and prioritizes the URLs to follow.
If we do not ﬁnd a way of collecting any reasonable amount of seeds from search engines
using the BootCaT method, there are still other options. If language-classiﬁed seeds are required,
the Open Directory Project oﬀers links to websites in diverse languages in the World category.²⁰
e full data can be downloaded as an RDF dump, and links are easily extracted. If the corpus de-
signer is satisﬁed with newspaper seeds, directories like ABYZ New Links can be used.²¹ Finally,
Wikipedia dumps are available in many languages, and they usually contain many links to sites
¹⁹e crawlers were run on the same machine, with the same bandwidth and using the same NFS storage for crawler state, logs,
and data. Both were run with 75 fetcher threads. e NFS storage explains for the overall suboptimal time performance.
²⁰http://www.dmoz.org/
²¹http://abyznewslinks.com/

2.3. CRAWLING BASICS
27
0
200
400
600
800
1000
0.00
0.02
0.04
0.06
0.08
0.10
0
200
400
600
800
1000
0.00
0.02
0.04
0.06
0.08
0.10
Many seeds (MB O/I ratio)
Few seeds (MB O/I ratio)
Nx1::500 D 0:027, Nx501::1000 D 0:011
Nx1::500 D 0:018, Nx501::1000 D 0:012
Total: 9:18 GB
Total: 7:02 GB
0
200
400
600
800
1000
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0
200
400
600
800
1000
0.00
0.05
0.10
0.15
0.20
0.25
0.30
Many seeds (document O/I ratio)
Few seeds (document O/I ratio)
Nx1::500 D 0:141, Nx501::1000 D 0:071
Nx1::500 D 0:093, Nx501::1000 D 0:074
Total: 1; 149; 294 documents
Total: 1; 334; 873 documents
t1::500 D 19:62h, t501::1000 D 18:75h
t1::500 D 20:33h, t501::1000 D 35:73h
Figure 2.9: Comparison of the yield from breadth-ﬁrst crawls in the .es TLD, started with (i)
231,484 and (ii) 10 Yahoo seed URLs. Castilian 4-tuples were queried. Crawled using Heritrix
1.14 in May and June 2011. e x-axis is the n-th archive ﬁle produced by the crawler (400 MB of
uncompressed input data each). e y-axis is the output/input ratio for these archive ﬁles. e input
underwent HTML-stripping, boilerplate removal, text detection based on high-frequency function
words, and perfect duplicate detection. Means were calculated for the ﬁrst and second half of the
crawls. e vertical line marks the 120th archive, where the seed URLs were exhausted.
in these languages. Since a larger number of links in Wikipedia dumps leads to English websites,
applying additional language identiﬁcation for the seeds is advised (cf. Section 3.4).

28
2. DATA COLLECTION
One problem with crawling monolingual corpora which we have not discussed in detail
so far is crawling for languages which cannot be associated with a (set of) national TLDs. Even
if we ﬁnd seeds in such languages, any random crawl will sooner or later deliver a majority of
documents in other languages, signiﬁcantly spoiling the WC. We will discuss a possible solution
in Section 2.4.3.
2.4
MORE ON CRAWLING STRATEGIES
2.4.1
INTRODUCTION
So far, we have implicitly pretended that implementing the crawling process in software just
requires a series of engineering decisions, for example dealing with implementation tasks such as
the URL Seen Test, huge queues, multi-threading, etc. (all in Section 2.3.3). In this section, we
examine some quite fundamental questions regarding crawling strategies. e discussion is most
likely relevant for researchers (for example theoretical/empirical linguists) who see web corpus
construction as an act of sampling corpus documents from a population of web documents. In
this case, it is vital to ask about the nature of the sampling process (for example its randomness),
because it decides to a great deal how well insights gained from the sample generalize to the
population. For example, as Schäfer and Bildhauer [2012] notice, in both the WaCky corpora and
the COW corpora, a huge percentage of the crawled documents come from the web hosts which
were already represented in the seed URLs, which were collected from search engine results (84%
for deWaC, 56% for DECOW2012, and 95% for SECOW2011). is is suspicious, although
not yet a proof that there is necessarily something wrong with the sampling procedure.
A further and maybe more serious illustrative example can be found by a simple query in
the deWaC corpus. In this corpus, the second most frequent sequence of two tokens identiﬁed
as proper names is Falun Gong with 51,641 occurrences. At the same time, the sequence Gerhard
Schröder is ranked third with 24,481 (not even half as many) occurrences.²² Since the corpus
was probably crawled in 2005 (maybe 2006) according to Baroni et al. [2009], the documents
were collected during a time when Gerhard Schröder was still chancellor of Germany or had just
resigned. Table 2.10 compares the deWaC counts of the two sequences with the 2005 and 2006
stratum of the DeReKo newspaper reference corpus by the Institut für Deutsche Sprache [Kupietz
et al., 2010], which is widely used by corpus linguists working on German.
is shows that the frequencies of certain proper names (and therefore most likely the dis-
tribution of topics) in the deWaC do not generalize to their frequencies in any kind of relevant
population. e crawling procedure is clearly the most likely cause of such strange results. Re-
searchers for whom such questions are irrelevant (maybe some computational linguists) can skip
this section entirely. However, the selection of sampling procedures is also relevant in web char-
acterization and related ﬁelds, cf. for example Becchetti et al. [2006]. For all other readers, this
²²deWaC was processed with the TreeTagger (cf. Section 4.7). Since the tagger did not recognize all occurrences of Falun Gong
as proper names, we give the corrected ﬁgures here, i. e., the number of all occurrences, whether they were recognized as proper
names or not.

2.4. MORE ON CRAWLING STRATEGIES
29
Sequence
deWaC
DeReKo 2005
DeReKo 2006
Gerhard Schröder
15.05
(24,481)
61.77
(6,325)
10.04
(1,329)
Falun Gong
31.74
(51,641)
0.15
(15)
0.12
(16)
Figure 2.10: Comparison of the counts per million tokens (total count in parentheses) in deWaC
(1.62 billion tokens) and the DeReKo strata for 2005 (132 million tokens) and 2006 (164 million
tokens) for Falun Gong and Gerhard Schröder. e DeReKo archive used was W-öﬀentlich, release
DeReKo-2012-II.
section sheds some light on the possible inﬂuence of the selection of the crawling strategy on the
composition of the corpus.²³ By the crawling strategy, we mean the algorithm by which we pene-
trate the web graph, i. e., how we select links to queue and harvest, and in which order we follow
them. A fundamental distinction [Gjoka et al., 2011] is between graph traversal techniques (each
node is only visited once) and Random Walks (nodes might be revisited). We coarsely refer to all
these strategies as crawling.
e simplest and most widely used crawling strategy (tacitly assumed in the previous sec-
tions) is breadth-ﬁrst, a traversal technique. In a pure breadth-ﬁrst crawl, all links are harvested
and queued in the order they are found, using the URL Seen Test to avoid multiple queueing.
Parallelization and politeness restrictions often cause local deviation from pure breadth-ﬁrst. But
despite these limitations, Mohr et al. [2004], for example, call the default queuing strategy in
Heritrix breadth-ﬁrst. e questions we need to ask are:
1. Does breadth-ﬁrst introduce problematic sampling biases?
2. Is breadth-ﬁrst a good maximizer of the WC?
3. Are there alternative strategies, and are they eﬃcient enough to be used for the construction
of large corpora?
It should be noticed that these questions will lead to superﬁcially contradictory answers.
For example, breadth-ﬁrst might introduce sampling biases, but it might at the same time be
the only option for the construction of very large corpora. In general, there is no single perfect
strategy independent of the design goals of the corpus. We discuss these questions in Sections
2.4.2 and 2.4.3.
2.4.2
BIASES AND THE PAGERANK
If a sampling process is conducted in a way that does not produce the intended distribution, it is
biased. For sampling from the population of web documents (cf. Section 2.2), the most neutral
deﬁnition of an unbiased sample would be a sample according to a discrete uniform random
distribution, i. e., one where each page has an equal chance of being sampled. Even if this is not
²³Obviously, another factor is the selection of seed URLs, which was covered in Section 2.3.5.

30
2. DATA COLLECTION
the goal for all web corpus designers (cf. also Section 2.4.3), in this section, we ﬁrst explain that
more or less naïve crawling leads to potentially undesirable biases and deﬁnitely does not give each
page an equal chance of being sampled. is poses a problem because our sample is usually such
a small portion of the whole web that, if it is not a uniform random sample, we cannot be sure to
have obtained a representative sample in any sense. Put diﬀerently, if we have taken only a very
small non-uniform sample, we do not know whether the relevant documents are included. is is
entirely independent of our deﬁnition of what a relevant document is. Even if we are ultimately
not interested in a uniform random sample from the documents on the web (but, for example, a
sample biased toward longer documents containing coherent text), the sampling method should
not introduce other arbitrary and uncontrollable biases.
Many papers study the eﬀectiveness of crawling strategies as the task of ﬁnding pages with
a high PageRank early and of ﬁnding a large enough segment of the important pages from the
whole web at all.²⁴ Such papers include Abiteboul et al. [2003]; Baeza-Yates et al. [2005]; Cho and
Schonfeld [2007]; Fetterly et al. [2009]; Najork and Wiener [2001]. It was found that breadth-
ﬁrst manages to ﬁnd relevant pages (pages with a high in-degree) early in a crawl [Najork and
Wiener, 2001]. In Abiteboul et al. [2003], Online Page Importance Computation (OPIC) is
suggested, basically a method of guessing the relevance of the pages while the crawl is going on
and prioritizing URLs accordingly. In Baeza-Yates et al. [2005], several strategies are compared
speciﬁcally for large intranet or TLD crawls, and breadth-ﬁrst is evaluated as inferior to OPIC
and other strategies.²⁵
Now, since our goal is not search engine design but web corpus construction, we might
want to look for sampling methods that allow us to correct for biases toward pages with a high
in-degree or PageRank. In order to correct for a bias, its mathematical properties must be known.
In a series of theoretical and experimental papers [Achlioptas et al., 2005; Kurant et al., 2010,
2011; Maiya and Berger-Wolf, 2011], it was found that breadth-ﬁrst is biased toward pages with
high in-degrees, but that this bias has not (yet) been mathematically speciﬁed. In non-exhaustive
breadth-ﬁrst samples from a graph, the distribution of in-degrees is distributed by a power law,
even if the distribution in the graph from which the sample was taken is entirely diﬀerent, for
example a Poisson distribution [Achlioptas et al., 2005]. is is bad news, because it means that
if we rely on breadth-ﬁrst crawling for web corpus construction, we do not know exactly what we
are doing and what population of documents the corpus represents. All we know for sure is that
in the crawled set of documents (ultimately, the corpus), the in-degrees of the documents follow
a power law.
So far, the linguistically oriented web corpus construction community does not seem to
have paid much attention to this, but breadth-ﬁrst sampling does have potentially problematic
properties. As an example, Kornai and Hálacsy [2008] spend a few pages comparing the data rate
at which diﬀerent crawlers operate on their machines, and describe their own crawler as follows:
²⁴e PageRank is a metric based on the in-degree designed to measure the in-degree in a more balanced way, and it will be
introduced further below in this section.
²⁵e Nutch crawler comes with an OPIC implementation.

2.4. MORE ON CRAWLING STRATEGIES
31
We do not manage at all, let alone concurrently, link data, recency of crawl per host,
or URL ordering. is simpliﬁes the code enormously, and eliminates nearly all the
performance problems that plague heritrix, nutch, larbin and other highly developed
crawlers where clever management of such data is the central eﬀort. [Kornai and
Hálacsy, 2008, 9]
A year earlier, Issac [2007] presented a crawler for web corpus construction, which is clearly
described as a naïve breadth-ﬁrst system, without even mentioning the term breadth-ﬁrst.²⁶ Given
the aforementioned problems with small breadth-ﬁrst sampling from largely unknown popula-
tions, such approaches seem quite cavalier. However, some corpus designers explicitly state that
they are not interested in unbiased sampling, e. g.:
[…] obtaining a sample of unbiased documents is not the same as obtaining an unbi-
ased sample of documents. us, we will not motivate our method in terms of whether
it favors unbiased samples from the web, but in terms of whether the documents that
are sampled appear to be balanced with respect to a set of deliberately biased samples.
[Ciaramita and Baroni, 2006, 127f]
In this tradition, the goal of web corpus construction is to obtain a corpus which shows
a degree of variation (e. g., in terms of genres) comparable to the BNC, for example. However,
Ciaramita and Baroni [2006, 131] mention unbiased sampling as possible future work.
Luckily, for a number of crawling strategies, the bias of the sampling method is known,
and there are counter-measures to take. If we know that a crawling technique brings about a bias
toward pages with a high Page/Rank, then we can try to compensate for the bias, i. e., calculate
a metric and use it to adjust the probability of being sampled for each page. is is the case
with speciﬁc types of Random Walks, which are known to be biased toward pages with a high
PageRank. Already in Henzinger et al. [2000], a method for performing Random Walks through
the web and correcting for the PageRank bias using a simple form of rejection sampling was
suggested. Later papers describe other strategies and characterize them mathematically, often
testing it on networks which are similar but not identical to the web, like online social networks
(OSN) or peer-to-peer networks (P2P), e. g., Gjoka et al. [2011]; Rusmevichientong et al. [2001].
A major diﬀerence between the web and, for example, many OSN graphs is that the web is
directed (i. e., there is no way of determining I.p/ for a page p), while OSNs are often undirected.
is means that some of the strategies explored in such papers cannot be applied to web crawling
without signiﬁcant overhead.
We now describe the reasonably simple approach by Henzinger et al., which is based on a
crawling strategy that has a known PageRank bias, which is then corrected for. e main problem
with PageRank bias correction is that it is impossible to calculate the PageRank online at crawler
runtime, and that oﬄine PageRank calculations require signiﬁcant resources. For eﬃciency rea-
sons, the PageRank can therefore be estimated, be it oﬄine or online. First, we need to brieﬂy
²⁶What is more, both crawlers are likely to violate politeness conventions and ignore Robots Exclusion. Kornai and Hálacsy
actually mention that they were—at least at the time—not using robots.txt information.

32
2. DATA COLLECTION
explain the PageRank itself. It was introduced in Brin and Page [1998] and is one of the key ideas
behind the success of Google’s search engine. e key ideas of the PageRank are:
1. e PageRank of a page is high if it is linked to by many pages with a high PageRank.
2. Pages with few outlinks contribute more to the PageRanks of the pages they link to.
3. e PageRank PR.p/ of a page p is the equilibrium probability that a Markovian random
walker through the web graph is on page p.
Point 3 is based on the idea that the PageRank models the behavior of a human web surfer
who starts with an arbitrary page. e surfer would then click on a randomly selected out link
on the current page repeatedly, eﬀectively performing a Random Walk through the web graph.
ere remains, however, a certain probability that the surfer loses interest in the current route and,
instead of clicking on a random link, starts all over with a new arbitrary page (like a bookmark or
a search engine query result). e long-standing assumption in the search engine sphere is that
PageRank or variants of it are useful in maximizing the WC of a crawl (pages with high PageRank
should be crawled early), because these are the pages a search engine user would ﬁnd most relevant
to a query matched by these pages.
Mathematically, from each page p, the walker follows a random member of the set O.p/
of pages linked to by p with a ﬁxed probability .1   d/  jO.p/j, or jumps to a random page
with probability d. Values for d are usually assumed to be between 0:10 and 0:15. e PageRank
PR.p/ of a page p is deﬁned as follows, where I.p/ is the set of pages linking to p and N the
number of pages in the whole graph:
PR.p/ D d
N C .1   d/
X
q2I.p/
PR.q/
jO.q/j
(2.4)
Actual calculations of PageRank involve intermediate-level linear algebra and are beyond
the scope of this introduction, but there is a short introduction in Manning et al. [2009] and
an accessible introduction in Bryan and Leise [2006]. ere are numerous software packages
available to calculate the PageRank for a simulated or empirically observed graph, such as igraph
(which includes Python and R interfaces).²⁷
A PageRank-based Random Walk crawler needs to have a start URL and keep a certain
amount of jump URLs (roughly equivalent to the seed URLs) to which it can jump with proba-
bility d (or when it reaches a page in OUT). Otherwise, it follows a randomly selected link from
each page to another page, where the probability is .1   d/  jO.p/j for each link on p. For a toy
graph, Figures 2.11 and 2.12 illustrate how a random jump changes the transition probabilities
for a Random Walk through the graph.
Turning back to bias correction according to Henzinger et al. [2000], they estimate the
PageRank PR.p/ via the visit ratio VR.p/, where L is the length of the crawl (number of transi-
tions made) and V.p/ the number of visits to p during the crawl:
²⁷http://igraph.sourceforge.net/
www.allitebooks.com

2.4. MORE ON CRAWLING STRATEGIES
33
Figure 2.11: A simple web-like directed graph.
A
B
C
D
E
A
0
0
0
0
1
B
0
0
0.5
0
0.5
C
0.5
0
0
0.5
0
D
0
0.5
0
0
0.5
E
0
0
1
0
0
A
B
C
D
E
A
0.03
0.03
0.03
0.03
0.88
B
0.03
0.03
0.455
0.03
0.455
C
0.455
0.03
0.03
0.455
0.03
D
0.03
0.455
0.03
0.03
0.455
E
0.03
0.03
0.88
0.03
0.03
Figure 2.12: Transition probabilities for Figure 2.11 without and with random jumps (transition from
row to column), d D 0:15.
PR.p/  VR.p/ D V.p/
L
(2.5)
is means that in practice, we have to allow for revisits, lowering the eﬃciency of the
crawl. After the crawl is ﬁnished, the ﬁnal sample is taken from the crawl, where the probability
of the page being sampled from the crawl is inversely proportional to its VR, i. e., its estimated
PR. After the procedure, the probability that a page is sampled should be the same for all pages:
Pr.p is sampled/ D Pr.p is crawled/  Pr.p is sampledjp is crawled/
(2.6)
ey estimate the following for the ﬁrst term of the right-hand side:
Pr.p is crawled/  L  PR.p/  L  VR.p/
(2.7)
ey state the following for the second term on the right-hand side:
Pr.p is sampledjp is crawled/ / PR.p/ 1
(2.8)
is just means that they sample from the results of the Random Walk using a skewed
probability distribution over all crawled p such that the probability of drawing p is inverse to
its (estimated) PageRank. Obviously, the ﬁnal output in terms of the number of documents is

34
2. DATA COLLECTION
lowered further by this post-crawl sampling procedure. Figure 2.13 shows a simple calculation
for the pages p1::6 encountered in a ﬁctional crawl with L D 10.
Page
Visits
VR
VR 1
Pr (sampled from crawl)
p1
4
0:4
2:5
0:053
p2
2
0:2
5
0:106
p3::p6
1
0:1
10
0:213
Figure 2.13: Sample calculation of PageRank bias correction for a toy PageRank Random Walk with
L D 10.
As the authors themselves notice, a certain PageRank bias remains, most likely due to
the inaccuracy of the PageRank estimation. e method has, however, the advantage of being
relatively simple to calculate. In Rusmevichientong et al. [2001], a related method which the
authors call Directed Sample is reported, which delivers unbiased samples, and which also does
not require that the web be directed.
One disclaimer is in order: Any such advanced sampling is most likely not suitable if cor-
pus size is of the essence, since Random Walks lead to a high number of page revisits (self-loops)
and we cannot keep all pages we visited (e. g., if we sample all nodes from the simple Random
Walk in Figure 2.13, then nothing would be gained). e Directed Sample algorithm by Rus-
mevichientong et al. [2001] introduces a signiﬁcant crawling overhead to better estimate the
in-degree of each page. For another method (Regular Sample), the authors report that in a sam-
ple of 2; 980; 668 nodes, 14; 064 were unique. If large corpora are required, some form of (maybe
focused, cf. Section 2.4.3) breadth-ﬁrst like technique is still the best option to choose.
To the best of our knowledge, the questions of how more advanced sampling methods
aﬀect the ﬁnal web corpus, and how the size of the ﬁnal corpus given a certain crawling eﬀort
can and should be balanced against the degree of validity of the sampling procedure, have not
been examined for linguistically oriented web corpus construction. e next section introduces
methods of steering the crawl toward desirable content, which is superﬁcially the opposite of
biasing.
2.4.3
FOCUSED CRAWLING
Finally, we mention ways of “biasing” a crawl toward desired pages, an advanced method to im-
prove the WC for a well-deﬁned weight function. Technically, this should not be called biasing
(because it is a desired eﬀect), although biasing and focusing bear certain similarities. e motiva-
tion for focusing a crawl in the context of web corpus construction is that we are usually looking
for documents (i) in a certain language, (ii) which are not spam, (iii) do not contain predomi-
nantly binary media ﬁles (like audio or video portals), etc. is problem is totally distinct from
the problem of graph-theoretical biases. A purely uniform random sample from the web contains

2.4. MORE ON CRAWLING STRATEGIES
35
a lot of material which we would not use in a corpus, just as much as a biased breadth-ﬁrst crawl
does. e idea of focusing is that we better make the selection as early as possible, i. e., ideally
before a web page is requested. Originally, crawlers optimized to ﬁnd documents according to
genres, languages, etc., are called scoped crawlers, and focused/topical crawlers are a sub-type of
scoped crawler looking for documents about certain subjects [Olston and Najork, 2010, 35]. We
use the term “focusing” in a broader sense.
Focusing almost becomes a necessity when we build a corpus of a language that is not
associated with a TLD (and which is not English). is is because, even if we use seed URLs
pointing to pages exclusively in a certain language, a crawler following links by any unfocused
strategy will very soon crawl documents in all sorts of languages (predominantly English). We
have seen in Section 2.3.5 a strong inﬂuence of the seeds on the results in the very initial phase
of the crawl, before it quickly moves too far away from the neighborhood of the seed URLs (esp.
Figure 2.9). Relying on seed URL quality is thus not enough for speciﬁc crawling tasks, and we
need a way of sustaining a high WC throughout the crawl.
Focused crawlers go back to Chakrabarti et al. [1999]. Usually, the links themselves within
a document and the text in a window around the links are analyzed to come up with a predic-
tion about the usefulness of the link given the crawl’s scope or focus. Often, the relevance of the
linking page itself or a larger link context are also included in the calculation. Various heuristics
and machine learning methods are used for the calculation. An adequate coverage of the ﬁeld
is impossible to give here, but a selection of papers on the subject is: Almpanidis et al. [2007];
Chakrabarti et al. [1998]; Cho et al. [1998]; Gomes and Silva [2005]; Menczer et al. [2004];
Safran et al. [2012]; Srinivasan et al. [2005]. For language detection based only on URLs, prob-
ably the most important kind of detection required for web corpus construction, see Baykan et al.
[2008]. For topic and genre detection from URLs, see Abramson and Aha [2009]; Baykan et al.
[2009].
A simple method which cannot be called focusing in a strict technical sense is described
in Suchomel and Pomikálek [2012].²⁸ e authors integrate their post-processing (roughly the
steps described in Chapter 3) into their own crawler and collect statistics about the ﬁnal yield from
each encountered host (like the out/in ratio from Section 2.3.5, esp. Figure 2.9, but calculated per
host). Hosts receive a penalty for low yield, up to a point where they get eﬀectively blacklisted.
From the information in the paper, it appears that the crawler implements a breadth-ﬁrst strategy
otherwise. Although the eﬀect in WC maximization is moderate (as reported by the authors),
the method is suitable for collecting more corpus data in a shorter time. Since it is a breadth-
ﬁrst system with online host blacklisting, the biases of this sampling method are quite diﬃcult to
characterize, however. It is not focused crawling in that it does not actively favor the discovery of
relevant pages/hosts, but merely blacklists irrelevant pages/hosts.
²⁸e crawler is available as SpiderLing: http://nlp.fi.muni.cz/trac/spiderling/

36
2. DATA COLLECTION
SUMMARY
Data collection is the simplest step in web corpus construction, in the sense that it can be decom-
posed into the two simple steps: (i) get some seed URLs and (ii) run the crawler. However, the
pages which are crawled are a sample from a population (the web documents), and the sampling
procedure pre-determines to a large extent the nature of the ﬁnal corpus. We have introduced
the steps which have to be taken, like seed URL collection and crawler conﬁguration, but we
have also given hints about the major decisions which corpus designers make when they select a
crawling strategy, even when selecting a speciﬁc piece of crawler software. To this end, we have
discussed the structure of the web itself as well as crawling strategies. Some strategies lead to larger
corpora—like breadth-ﬁrst crawling, maybe even with focusing. Some will deliver signiﬁcantly
smaller corpora for the same eﬀort—like Random Walks with sampling bias correction—but will
be free of sampling biases. In any case, we have only covered the collection of raw data so far,
which needs to undergo diverse post-processing steps as described in Chapters 3 and 4.

37
C H A P T E R
3
Post-Processing
3.1
INTRODUCTION
By post-processing, we refer to the non-linguistic cleanups which are required to turn the col-
lection of downloaded HTML documents into a collection of documents ready to be included
in a corpus. is involves cleanups within the documents and the removal of documents which
do not meet certain criteria. e end product of the post-processing chain is often simply a set
of plain text documents. Of course, putting all raw HTML ﬁles without the post-processing as
described in this chapter into the corpus is hypothetically also an option. In this case, the corpus
query engine should be capable of rendering HTML in order to make the corpus readable for
humans, which is of course an achievable task. e real problem with keeping the HTML docu-
ments is, however, not the bad human-readability of HTML code, but the problems of linguistic
post-processing and the statistics which will be derived from the corpus:
1. Tokenizers (Section 4.2) and all subsequent tools which add linguistic annotation expect text
only, no HTML markup, scripts, etc. If we do not strip the HTML code (Section 3.2.1),
we have to make sure these tools only see the natural language text from the web document.
2. e documents are encoded in diﬀerent character encodings (both Unicode and non-
Unicode), which is diﬃcult to index, search, and display if we do not convert all documents
to one encoding (Section 3.2.3).
3. A lot of text on web pages is redundant, non-content text, for example navigational ele-
ments, copyright notices, ads. If we do not remove this so-called boilerplate material (Sec-
tion 3.3), statistics like word counts, sentence lengths, etc., will be distorted. See Chapter 5
about the problems which arise even because of the error rate of automatic boilerplate re-
moval.
4. Not all pages are (predominantly) in the desired language. It is a good idea to remove docu-
ments which have no or only a small portion of text in the target language (Section 3.4). is
includes documents which contain words in the target language, but not sentences. Such
cases include tag clouds, lists of references or lists of names (e. g., signers of a petition).
5. Finally, many of the documents which the crawler downloads are duplicates of other docu-
ments, or they are very similar to other documents. Again, if we do not detect and remove
them (Section 3.5), statistics derived from the corpus might be completely oﬀ.
In this chapter, we describe ways of implementing such cleanups. It should be kept in mind
that each step involves design decisions, much like the selection of a crawling strategy (Chapter 2)

38
3. POST-PROCESSING
does. e nature of the ﬁnal corpus changes according to what is removed from and what is left
in the corpus.
Having read through this chapter, readers should:
1. know about the structure and encodings used in (X)HTML and XML documents, and how
to remove the markup using their own implementation or available tools and libraries,
2. have a good idea about what deﬁnes boilerplate on a web page, and how it is possible to
detect and remove it,
3. know about the sources of perfect and near duplication and inclusion in web pages, and
how duplication can be removed from a corpus (one method is described in enough detail
for self-implementation).
3.2
BASIC CLEANUPS
3.2.1
HTML STRIPPING
In this section, we discuss the most basic and necessary cleanup step, namely markup removal. We
speak of HTML removal consistently, but XML removal is a virtually identical task. In fact, there
is a version of HTML (XHTML) which is based on XML. Formal speciﬁcations of HTML can
be found in the archives of the World Wide Web Consortium or W3C (the organization which
is responsible for all web-related standardizations), where primarily HTML 4.0, HTML 4.01,
and HTML 5 are of interest, secondarily also XHTML and XML.¹
An HTML document is a text document which consists entirely of HTML elements,
which for the purpose of HTML removal can be classiﬁed as follows:
1. elements with enclosing tags and literal text in between of the form:
<tag> literal text </tag>,
2. empty elements of the form:
<tag> (not in XHTML) or <tag></tag> or <tag /> (XHTML abbreviation),
3. elements which contain special, non-literal data, mostly <script></script> tags for
script to be executed in the browser,
Elements can have attributes (which usually have values) which are added to the opening
tag in the form <tag attribute="value">, where the value is always included in single or
double quotes. Elements can be nested.
To remove well-formed HTML without preserving any of the document structure, it suf-
ﬁces to remove all tags in <…> and also remove the text in <script></script> and similar
elements. Almost no parsing in the proper sense is required for this. A tentative list of elements
which can be removed entirely is:
• <script></script> (usually JavaScript)
¹http://www.w3.org/

3.2. BASIC CLEANUPS
39
• <style></style> (inline CSS)
• <head></head> (header, contains a lot of other invisible elements)
• <form></form> (form data is rarely desirable corpus content)
• <applet></applet>, <object></object>, <embed></embed> (binary content)
• <code></code> (programming code or similar)
• <pre></pre> (pre-formatted literal text, often—but not always—code examples)
HTML 5 brings a range of new elements which can be removed entirely, such as:
• <audio></audio> and <video></video> (media inclusion)
• <track></track> (multimedia annotation)
• <figure></figure> (ﬂoating ﬁgures)
HTML 5 also introduces more diﬃcult-to-handle elements, like the <ruby></ruby> el-
ement. It is used to layout pronunciation information for East Asian characters, material which
creators of Japanese corpora, for example, might not want to discard, but for which the alignment
with the original characters must be preserved without interrupting the sequence of these original
characters. Naïve HTML stripping is complicated by such features, and probably the best way of
dealing with it is the implementation of separate HTML 4 and HTML 5 strippers.
A more principled way of removing HTML is by using a DOM (Document Object Model)
parse of the document. e DOM is a standardized API speciﬁcation for representing full parses
of (X)HTML and XML documents and for dynamic updates of the contents of the parse by
scripts.² It was designed as a standard to be implemented by rendering clients (primarily web
browsers), but most modern programming languages oﬀer a DOM library which can be used to
get a parse of markup documents. DOM implementations which conform to the standard should
be identical in their behavior (except for notational variants of the underlying programming lan-
guage).
e document is parsed into a tree structure composed of objects (data structures) repre-
senting tree nodes, which are aware of their local context in the tree (mother and daughter nodes).
For our purposes, a DOM parse can be regarded as a tree with a document node as its root, which
has several element daughter nodes, which have as daughters other element nodes, attribute
nodes and text nodes, corresponding to the kind of markup contained in HTML documents
as described above. For a markup fragment like the following, Figure 3.1 shows a DOM tree
structure.
<document>
<element1 attribute1 attribute2>
text
<element1>
<element2 ...>
²http://www.w3.org/DOM/

40
3. POST-PROCESSING
...
</element2>
...
</document>
Figure 3.1: Structure of the DOM parse of a document.
To remove markup, it is feasible to obtain a full DOM parse from some library, then tra-
verse the tree and extract all text nodes. Whether the overhead of the DOM parse and the
tree traversal (compared to ﬂat stripping) is acceptable, depends on the speed requirements and
the eﬃciency of the DOM parser which is used. It should be kept in mind that the robust-
ness of any ﬂat or structured method of parsing HTML depends on the correctness of the
markup. Unfortunately, HTML as found on the web often does not conform to the expected
standards. is is mostly because web browsers are quite fault-tolerant when rendering HTML,
such that errors in the markup are not visible to designers and visitors of the web page. Typi-
cal errors include non-closing tags, spaces between < and the tag identiﬁer (< tag>), attribute
values which are not in quotes (admissible but not recommended in plain HTML), overlap-
ping tags (<tag1><tag2></tag1></tag2>), all sorts of attribute quoting errors (e. g., <tag
attribute="value>), etc.
Both in ﬂat and DOM approaches to markup removal, such error conditions can have
serious consequences. In the worst case, markup can end up in the ﬁnal corpus, which might be
considered worse than non-markup text which is lost. One way to deal with such erroneous input
is to use an HTML validator as oﬀered by the W3C and discard faulty documents.³ Since this
reduces the number of documents in the corpus, another option is to use a program/library like
HTML Tidy to ﬁx incorrect HTML, as mentioned by Spousta et al. [2008].⁴ Of course, if a DOM
library is used, it might include some error correction, which should be checked before resorting
to additional libraries like HTML Tidy. e time overhead of the extra cleanup step might be
rewarded by higher accuracy in the subsequent steps, however.
Finally, for very simple approaches, oﬀ-the-shelf HTML strippers can be used. Since strip-
ping other formats (like PDF and word processor formats) is signiﬁcantly more complicated, such
³http://validator.w3.org/
⁴http://www.w3.org/People/Raggett/tidy/

3.2. BASIC CLEANUPS
41
tools are also a valid option if not just HTML documents are included in the corpus. Open source
tools are available with many distributions of GNU/Linux, such as html2text, odt2text (Open
Document Text format), pdftotext (from xpdf), and there are numerous commercial products
available. Especially with monolithic HTML strippers, it should be checked which versions of
HTML they convert correctly to avoid incorrect results. A disadvantage of monolithic tools is
that we cannot extract features from the markup, which are needed, for example, in boilerplate
removal (Section 3.3).
3.2.2
CHARACTER REFERENCES AND ENTITIES
An additional part of the diverse HTML speciﬁcations are character references and entities, be-
ginning with & and ending with ; ey are used to encode characters which cannot be represented
in the normal encoding of the document (cf. Section 3.2.3). For example, the ISO-8859-1 char-
acter set, which is widely used for major European languages like English, German, Swedish,
etc., cannot represent the Euro currency sign €. In ISO-8859-1 documents, content creators can
insert it by the textual entity &euro; instead of switching to another encoding (e. g., Unicode)
for the document. Also, they are used to render literal versions of the protected characters of the
markup (for example, < as &lt; and > as &gt;). For textual entities like &euro; or &epsilon;
for , translation tables have to be used to convert them to appropriate Unicode characters.
Alternatively, characters can be encoded by using numeric references to their Unicode code-
point (cf. Section 3.2.3). ere is a decimal notation, introduced by &#, and a hexadecimal nota-
tion, introduced by &#x. For the € sign, we get &#8364; in decimal and &#x20AC; in hexadecimal
notation. ese can be converted directly by converting the string number representation to the
corresponding integer, interpreting this as a Unicode codepoint, and inserting the corresponding
Unicode character. is procedure assumes that the document is encoded as Unicode. If it is not,
most of these references can only be deleted, which means that information from the original
document is lost.
In real life, entities and references also come in faulty variants, where blanks after the & (&
euro;) or before the ; (&euro ;) are quite frequent. Writing a robust entity converter is still
a quite simple programming exercise, at least under the assumption that the document’s target
encoding is a Unicode encoding and all entities can be represented in the output. e next section
gives a very brief overview of character sets, and argues that Unicode encodings should be used
under any circumstances.
3.2.3
CHARACTER SETS AND CONVERSION
Letters and other characters need to be encoded numerically for electronic processing, i. e., each
letter to be rendered or printed is encoded by a speciﬁc number, the codepoint. e document is a
sequence of such numeric encodings. In this section, we sketch the important types of encodings,
and how to convert between them. For many languages, web documents come in more than one
encoding. For example, signiﬁcant amounts of German documents under the .de domain come

42
3. POST-PROCESSING
in UTF-8, ISO-8859-1, and Windows-1252. As the target encoding for all corpus documents,
a Unicode encoding like UTF-8 is strongly recommended. UTF-8 is most widely used and of-
fers the best trade-oﬀbetween universal coverage of Unicode codepoints and size requirements
for most languages. For a more in-depth coverage of character sets and encodings, any practical
introduction to Unicode will do, such as Korpela [2006]. We only cover the basics here.
e encoding to which almost all standardized encodings today go back is ASCII. It is a
7-bit encoding, thus capable of representing 128 characters, of which some are control characters
like tab (9) or newline (10). In terms of alphabets, it is just comprehensive enough for English
texts. e layout of these 128 codepoints is still the same in all subsequent encodings.
To represent alphabets with additional characters (like French ç, German ä, Swedish å,
etc.) 8-bit/1-byte encodings were introduced, which use the additional upper 128 codepoints to
encode the other characters. Examples of such 1-byte encodings include ISO-8859 (Western),
KOI-8 (Cyrillic), Windows-1252 (Western). To encode the huge number of East Asian charac-
ters, more complex multi-byte encodings were invented. Japanese documents can be encoded in
at least three traditional encodings: JIS, Shift-JIS, and EUC. e basic idea of these encodings
is to use sequences of bytes to encode codepoints which do not ﬁt into a single byte. us, nu-
merous alternative encodings emerged over time (some even vendor-speciﬁc), such that converter
software needs to be aware of a lot of diﬀerent translations.
e ISO-8859 family has 15 variants variants (1–16, 12 is undeﬁned), where the upper
block of 128 codes varies. However, ISO-8859-1 has the widest coverage of European languages
among all ISO-8859 variants and was taken as the basis for the Unicode standards Universal
Character Set 2 and 4 (UCS-2, UCS-4), where the numbers 2 and 4 stand for the number of bytes
required to encode a codepoint. e Unicode initiative is the attempt to provide a single standard
by which codepoints can be mapped to characters. e Unicode character sets were created such
that the ﬁrst 128 codepoints correspond exactly to ASCII, and the ﬁrst 256 codepoints correspond
exactly to ISO-8859-1 (which includes ASCII). UCS-2 adds one more byte and thus reaches
a capacity of 65,536 codepoints in total; UCS-4 is a 4-byte encoding, allowing for hypothetical
4,294,967,296 characters (less in the actual standardization), of which the ﬁrst 65,536 are identical
to those in UCS-2. Figure 3.2 illustrates the inclusion relations between ASCII, ISO-8859-1,
UCS-2, and UCS-4. is means that an ASCII document is a valid ISO-8859-1 document, an
ISO-8859-1 document is a valid UCS-2 document, and a UCS-2 document is a valid UCS-4
document (but of course not the other way round). Problems are caused by all the traditional
encodings, which often need to be mapped in more complex ways to UCS codepoints.
Additional complexity is added because UCS is usually encoded in the Unicode Trans-
formation Formats UTF-8 and UTF-16. To write plain (untransformed) UCS-2 (or UCS-4),
for example, each codepoint requires two (or four) bytes, which is a waste for the most frequent
codepoints in European languages (namely the ASCII codepoints). To make the encoding more
compact, a UTF-8 encoded document is a series of single bytes, and variable-length sequences
of 1 to 4 bytes encode a single UCS codepoint. ere are ﬁve diﬀerent types of bytes, which can
www.allitebooks.com

3.2. BASIC CLEANUPS
43
7 bit
+1 bit
+8 bit
+16 bit
128 chars
+128 chars
+65,280 chars
C232   216 chars
UCS-4
UCS-2
ISO-8859-1
ASCII
Figure 3.2: Inclusion relations from ASCII to UCS-4.
be recognized by the bit sequence with which they begin. ese ﬁve types are listed below, where
each x can be 0 or 1. ese x bits are used for the actual numerical encoding of the codepoint:
• a single byte: 0xxxxxxx
• a byte that is ﬁrst in 2-byte sequence: 110xxxxx
• a byte that is ﬁrst in 3-byte sequence: 1110xxxx
• a byte that is ﬁrst in 4-byte sequence: 11110xxx
• a non-ﬁrst byte in a sequence: 10xxxxxx
Apart from the single byte starting with 0, all other types of UTF-8 bytes are part of a
sequence with one of the sequence starting bytes and one to three following bytes. All bytes
which do not conform to one of these types are invalid bytes in any UTF-8-encoded document.
Now, for example, the range from untranslated binary 10000000 (128–the 129th codepoint
in 8-bit encodings) to 10111111 (191) cannot be represented by a single byte anymore, because
the initial sequence 10 is reserved for following bytes in a sequence. From this it follows that
UTF-8 is downward-compatible with ASCII, but not with ISO-8859 encodings, because ISO-
8859 uses the range from 128 to 191 to encode characters. Instead, codepoint 128 has to be en-
coded in UTF-8 as 11000010-10000000 (194-128) and codepoint 191 as 11000010-10111111
(194-191), and similar for the rest of the codepoints up to 256. e maximum number of bits
available for encoding codepoints is 21 in UTF-8, such that codepoints from 0 (as 00000000) to
2,097,151 (11110111-10111111-10111111-10111111) can theoretically be encoded. e types
of admissible sequences are summarized in Figure 3.3.
ﬁrst byte
full bitmask
byte length
bits available
0xxxxxxx
0xxxxxxx
1
7
110xxxxx
110xxxxx
10xxxxxx
2
11
1110xxxx
1110xxxx
10xxxxxx
10xxxxxx
3
16
11110xxx
11110xxx
10xxxxxx
10xxxxxx
10xxxxxx
4
21
Figure 3.3: UTF-8 sequences, where the x bits are used to encode the actual UCS codepoint.

44
3. POST-PROCESSING
No matter what the target encoding is, however, each downloaded document should be
checked for its encoding and, if necessary, converted. ere are two ways of detecting the encod-
ing: either by relying on the encoding speciﬁed in the document header, or using a tool/library to
detect it. Such software uses statistics of byte and byte sequence frequencies to guess the encoding
of a document with a certain conﬁdence. If the document declares its encoding, relying on the
declaration is safer than using software to detect it. Usually, an HTML document should declare
in its <head></head> element, a meta element such as this:⁵
<meta http-equiv="Content-Type" content="text/html;charset=iso-8859-1"></meta>
e HTML stripper (both ﬂat and DOM based) can be easily modiﬁed to extract any
such information from <meta> elements. e encoding might also have been transmitted in the
Content-Type HTTP header, in which case the crawler must have written the headers to make
the information accessible at post-processing time.
For character set detection, available libraries can be used. e most comprehensive and best
supported is IBM’s International Components for Unicode (ICU).⁶ It has detection, conversion,
regular expression matching for Unicode encodings, and even layout capabilities. Its detection
function outputs the conﬁdence at which a character set was detected, which is especially useful
because the diﬀerent ASCII-based 1-byte encodings are often diﬃcult to tell apart. Of course,
if an ISO-8859-15 document is recognized as ISO-8859-1, not too much harm will be done
when it is converted to UTF-8, but incorrectly detected encodings are a source of noise. If the
conﬁdence is low, we might discard the document completely. Further free libraries and tools
standardly available on GNU/Linux systems include the iconv tool and the recode library. Reg-
ular expression matching for UTF-8 is also oﬀered by standard regular expression libraries like
PCRE.
3.2.4
FURTHER NORMALIZATION
We now turn to some simple cleanups which could help to lower the level of noise and duplication
in the corpus without too much eﬀort. ese are second-pass HTML and code removal, dehy-
phenation, reduction of repetitions, duplicate line removal, blanking of URLs, and other address
information. ere is probably more that could be done. It is always a good idea to look at the
data (e. g., a random selection of a few thousand documents) and look for noise which can be
ﬁxed by such means.
Second-pass HTML (and script) removal might be necessary for the following reasons:
1. e HTML removal might have left traces of HTML in the corpus, usually as the result
of faulty input documents.
2. ere are a lot of pages on the web which are about HTML, scripting or programming
languages, etc. Sometimes, code listings and examples are inserted into the literal text by
⁵For HTML 4, cf. http://www.w3.org/TR/html4/charset.html.
⁶http://site.icu-project.org/

3.2. BASIC CLEANUPS
45
means of entities, such that <div> is inserted as &lt; div&gt;, etc. Also, if <pre></pre>
blocks have not been removed by default, these introduce a large number of code listings in
the corpus.
e reason to remove such material is that it is not linguistically relevant material. If it is
left in the corpus, tokens like var (C-style syntax) or br (HTML tag) will turn up as noise in
the token lists, and some types might have increased token counts, such as void (C-style variable
type, but also a word of English). We know of no oﬀ-the-shelf tool to perform such cleanups.
We were successful with a simple strategy looking for JavaScript and CSS keywords as well as
tag-like patterns and remove blocks/lines which contain such material in a high proportion. e
threshold is, of course, a design decision and has to be determined heuristically for each corpus
construction project.
Dehyphenation is the process of reconstructing pre-hyphenated words. For example, when
people paste content from word processors into web content management systems, or when
scanned documents processed with Optical Character Recognition (OCR) software are in the
corpus, there might be hyphenated words at ends of lines. It is diﬃcult to reliably infer ends of
lines from HTML, so we should work under the assumption that information about them is not
available. is means that naïve methods as described in Grefenstette and Tapanainen [1994]
cannot work on the kind of data we encounter in web corpora.⁷ e method basically consists in
concatenating two strings if one occurs at the end of a line and ends in - and the next one occurs at
the beginning of the following line. Actually, such simple approaches are dangerous even if ends
of line are present, because such sequences can come from many sources, such as (with examples
from our own UKCOW2011 corpus, a small test corpus of 890 million tokens):
1. true hyphenations (to be concatenated):
• It has a graph- ing facility for scatterplots
2. words spelled with hyphens (also new creations) but with accidental space:
• any child whose self- esteem needs a boost
• I called upon my Uranus- Neptune entity
3. abbreviated coordinated compounds:
• mountain biking, horseriding, and hang- and paragliding
4. dashes written as hyphens without spaces:
• some cases- and interpretation - of classic 1960s D-class movies
ere might be diﬀerent or additional cases to consider depending on the language, and
the frequencies of the diﬀerent cases might vary in diﬀerent languages. German has a very high
occurrence rate of abbreviated coordinated compounds, for example.
⁷A data-driven method is superﬁcially described in Zamorano et al. [2011], which is probably more promising.

46
3. POST-PROCESSING
Any dehyphenation software should be able to decide for any candidate string between one
of the following edit actions:
1. Merge: Remove hyphen and concatenate both strings.
graph- ing ! graphing
2. Concatenate: Keep hyphen and concatenate both strings.
self- esteem ! self-esteem
3. Dashify: Insert a blank before the hyphen, optionally replace it with a dash.
some cases- and interpretation ! some cases—and interpretation
4. Null: Leave everything as it is.
hang- and paragliding ! hang- and paragliding
Using available dictionaries (maybe even hyphenation dictionaries) to perform a lookup in
order to ﬁnd out whether the candidate is a hyphenated word, is most likely not a promising
approach in web corpus construction. Given the huge amount of noisy types in web corpora (cf.
Chapters 4 and 5), and given the fact that in any natural language corpus, there is also a huge
number of non-noisy types which are hapax legomena (words found only once), such approaches
are not robust enough.
Possibly the best idea is to look at frequencies of unigrams and bigrams bootstrapped from
the corpus itself and derive some kind of language model from it. In the case of graph- ing, for
example, the frequencies of the following unigrams are of interest (with token frequencies (not
case-sensitive) from UKCOW2011):
1. graph- (f D 2)
2. graph (f D 10; 026)
3. ing (f D 525)
4. graph-ing (f D 0)
5. graphing (f D 200)
6. graph- ing (f D 1)
Notice that the token-frequencies of 1, 3, and 6 must always be greater or equal to 1 if
graph- ing is in the corpus. Informally, any successful dehyphenation method should be able to
detect that because of the low frequency of graph- and possibly also graph- ing compared to the
relatively high frequency of graphing, the candidate should be merged. One problem is the high
frequency of ing, which occurs in many hyphenated gerunds, such that ing as a token is actually
far more frequent than the gerund graphing. To the best of our knowledge, no principled method
has been described to work this out.
Reduction of repetitions concerns issues which will be dealt with again in Chapters 4 and
5. Especially in blogs, forums, etc., we ﬁnd extremely long sequences of repeated characters. Peo-
ple make abundant use of certain punctuation characters like ??????????????, !!!!!!!!!!!! or mixed
!?!???!?!?!?!?!?!!!?. Depending on how the tokenizer deals with such cases (cf. Section 4.2), they

3.2. BASIC CLEANUPS
47
can be a considerable source of noise. For example, the tokenizer might split them into one token
per character, which quite spoils the token count. Tokenizers probably react diﬀerently to rep-
etitions of alphabetical characters such as in ageeeeeeeeeeeeeeees (from UKCOW2011), but under
certain design goals, corpus designers might also want to remove or standardize such cases. Of
course, some researchers might be interested speciﬁcally in such creative orthography, so remov-
ing or reducing such sequences is, again, a design decision. Reducing punctuation is quite simple.
e texrex software suite, for example, has a mode in which any repetitions of punctuation char-
acters are reduced to a sequence where each of the characters from the original sequence occurs
only once, e. g., !?!???!?!?!?!?!?!!!? becomes !? For letter sequences, the graphemic rules of the
target language can be used to reduce them to a reasonable length. Fortunately, spell-chekers like
GNU Aspell are also good at ﬁxing such cases (cf. Section 4.6), such that they might be used on
these cases as part of the linguistic post-processing.
Potentially less of a diﬃcult design decision is the removal of duplicate lines or blocks.
Mainly (but not exclusively) in boilerplate parts of the web page, lines may appear multiple times
in a row. ey can be easily removed. However, it might be a good idea to do this after the
deboilerplating algorithm has been applied, because it might change some of the metrics which
are used for feature extraction in the deboilerplater.
e same is true for the last minor cleanup to be mentioned here: blanking of URLs and
email addresses. Individual URLs and email addresses are usually not linguistically relevant and
should be removed in order to protect privacy. More or less complete regular expressions can
be construed, which ﬁnd and replace such material. Since sometimes these addresses are used
in sentences with an assignable part of speech, they should be replaced by some ﬁxed string
and not just removed. ere are examples like (from UKCOW2011): You may also e-mail him
at editor@.overton-on-dee.co.uk. It could be turned into something like: You may also e-mail him at
emailblank. Replacing instead of removing such addresses allows the POS tagger (cf. Section 4.5)
to guess a part-of-speech, which can under certain conditions improve the overall accuracy of the
tagger.
A more diﬃcult task is the removal of other address information, phone numbers, etc. Some
of them (like phone numbers) can also be detected with reasonable accuracy by regular expressions.
Street names and similar information are more diﬃcult to recognize. However, addresses and
email addresses often are located in boilerplate regions (together with copyright notices, etc.).
Among the features extracted for boilerplate removal can be the number of email addresses, phone
numbers, etc. Blanking out such information before the boilerplate remover does its job might
reduce its accuracy. We now turn to boilerplate removal in Section 3.3.

48
3. POST-PROCESSING
3.3
BOILERPLATE REMOVAL
3.3.1
INTRODUCTION TO BOILERPLATE
We have described in Section 3.2.1 that HTML removal is essentially a trivial task. After a suc-
cessful removal of the markup, only the text (without the graphics and other multimedia content)
that would be visible in a version of the page as rendered in a web browser remains. is text, how-
ever, might contain parts that we do not want to include in a corpus. Figure 3.4 shows regions on
a rendered page that are of such a nature.
Figure 3.4: A web page with diﬀerent regions of boilerplate marked.

3.3. BOILERPLATE REMOVAL
49
e A region contains layers of navigational menus, which are auto-generated for each
page of the institution. Especially with crawling strategies that tend to exhaustively download
all documents served by a host, the bigramm university news could be overrepresented in the
corpus. Even with better crawling strategies, typical menu words such as homepage, sitemap, contact
could be overrepresented. Since it is content management systems which insert these words in
such high numbers (and not humans producing them this often), removing menus is common
practice in corpus construction.⁸ Regions B and C are navigational elements which inform the
user about her/his position within the navigation logic of the site. Region C contains what is
usually called breadcrumb navigation. Region D holds linked content from the same content
category (in this case, press releases). Such regions are especially problematic, because they often
contain the beginning of the text of the linked document, broken oﬀafter a few words, usually
in the middle of a sentence, maybe followed by [Read more], (more), …, etc. If not removed,
such elements introduce redundant content (because these links occur many times on pages from
the same server). Also, machine-truncated sentence fragments are linguistically of limited use.
Region E consists of addresses (electronic and street addresses) and update information. As was
mentioned in Section 3.2.4, such blocks can possibly be recognized by (among other things) the
high number of email addresses which occur in them. Finally, block F contains a mirror of parts
of the navigational menus.
e task of boilerplate removal is to automatically identify and remove the text in areas
A–F, and leave only the actual text. e success of the respective method is measured using the
customary metrics of Precision, Recall, and F.⁹ Here, Precision is deﬁned as the proportion of
the correctly returned blocks, areas, sentences, etc. (whatever the chosen unit is) from the set of
correctly and falsely returned such elements:
Precision D
jcorrectly returnedj
jcorrectly returned [ falsely returnedj
(3.1)
e Recall is deﬁned as the proportion of the correctly returned blocks from the set of
correctly returned and falsely deleted blocks (in other words the proportion of actually returned
elements from the set of those elements which should have been returned):
Recall D
jcorrectly returnedj
jcorrectly returned [ falsely deletedj
(3.2)
e F measure is the harmonic mean of the two and a measure to evaluate the overall
quality of the algorithm:
F D 2  Precision  Recall
Precision C Recall
(3.3)
⁸But notice that, of course, search engine providers are most likely not interested in indexing such content, either, and also
perform boilerplate removal.
⁹Here and throughout, we use the term F as a shorthand for F1.

50
3. POST-PROCESSING
We implicitly describe approaches to boilerplate removal for HTML 4 or older, as
well as XHTML. HTML 5 introduces some features which might actually make boilerplate
removal simpler and more reliable. It speciﬁes elements such as <header></header> and
<footer></footer> for document headers and footers, <nav></nav> for navigational areas,
and even <article></article> for self-contained blocks which can be treated as single docu-
ments.
In the absence of such content- and structure-declaring markup, we have to ﬁnd features of
the markup and the text within a block which allow us to automatically classify it as either boil-
erplate or good text. We assume here that some kind of block boundaries were extracted in the
HTML stripping process (cf. Section 3.2.1). With ﬂat HTML stripping, splitting up the doc-
uments into blocks (which often correspond to natural language paragraphs) can be achieved by
inserting block boundaries where certain tags occur. For example, we can insert block boundaries
wherever <p>, <br>, <div>, and maybe some other tags (those which cause paragraph breaks
in the rendered web page) occur. For those blocks, relevant features can be calculated (cf. Sec-
tion 3.3.2), and those blocks which are found to be boilerplate based on those features can be
removed. Alternatively, from a DOM tree, sub-trees which are classiﬁed as boilerplate can be re-
moved. In this scenario, the document is stripped by traversing the tree and extracting text nodes,
or by saving the stripped DOM tree as HTML again and sending it to some generic HTML
stripper. Some contestants in the CLEANEVAL competition [Baroni et al., 2008] used such a
method, ﬁnally sending the cleansed HTML document to the Lynx text-only browser, which
can be used to strip HTML.¹⁰ In DOM-based extraction, blocks correspond to sub-trees in the
DOM tree (for example, all div and p elements).
ere is no most natural or intrinsically best way of ﬁnding such blocks, and some ap-
proaches use fundamentally diﬀerent techniques. For example, Gao and Abou-Assaleh [2007] use
a computationally more expensive visually oriented DOM-based segmentation method, which
tries to locate rectangular visible areas, roughly like the ones framed in Figure 3.4 (VIPS, Cai et al.,
2003). Since many of the informative features for boilerplate removal come from the markup, us-
ing some markup-based block/area splitting is by far the most popular. We now describe the kind
of features which are extracted for boilerplate detection in Section 3.3.2.¹¹
3.3.2
FEATURE EXTRACTION
Automatic boilerplate removal (or deboilerplating) can be accomplished using heuristic weight
calculation formulae [Baroni et al., 2009; Gao and Abou-Assaleh, 2007; Pomikálek, 2011]. More
commonly, however, some form of (usually) supervised machine learning is used for this task. I. e.,
¹⁰http://lynx.isc.org/
¹¹We do not discuss approaches which try to ﬁgure out templates used for whole sites, especially when content management
systems (CMS) are used. Clearly, high accuracy in content extraction can be achieved if the template according to which a
site’s CMS formats its articles is known exactly. However, this approach is usually diﬃcult and/or expensive to implement for
huge web crawls not restricted to a limited number of hosts. Cf. Bar-Yossef and Rajagopalan [2002]; Chakrabarti et al. [2007]
as entry points.

3.3. BOILERPLATE REMOVAL
51
an algorithm has to be chosen which is capable of learning a binary decision (a block of text from
the document is/is not boilerplate) from a set of training data annotated by humans, and the
decision (as learned) should be generalizable from the training set to any new unknown data of
the same type. e algorithm learns the decision based on a set of features which have to be
extracted for each of the blocks in the training set and for the unknown data in a production run.
ere have been a number of papers about boilerplate detection and removal. Most promi-
nently, the CLEANEVAL competition [Baroni et al., 2008] produced a number of eﬀorts,
namely Bauer et al. [2007], Evert [2007], Gao and Abou-Assaleh [2007], Girardi [2007], Hoﬀ-
mann and Weerkamp [2007], Issac [2007], Marek et al. [2007], Saralegi and Leturia [2007]. e
WaCky corpora have undergone a very simple boilerplate removal method, which is nevertheless
reported to be very eﬀective [Baroni et al., 2009]. Based on Marek et al. [2007], Spousta et al.
[2008] claim to have achieved a very high accuracy. Other approaches include Kohlschütter et al.
[2010]; Pasternack and Roth [2009]. Recently, Pomikálek [2011] has developed yet another so-
lution, including an online interface for quick evaluations.¹² Schäfer and Bildhauer [2012] also
proposed a deboilerplater system as part of the texrex software package, which they report as
being very fast while being also reasonably accurate.
We now present a selection of features, primarily ones used in the method by Spousta et al.,
because they use a very comprehensive feature set. Features are grouped into classes, and we will
illustrate some of them immediately, using an analysis of the page depicted in Figure 3.4.¹³
• Markup-related features
– tag class that started the block (e. g., <div>, <p>, etc.),
– length of the markup in characters or bytes,
– length of the non-markup text in characters or bytes,
– ratio of markup and non-markup,
– count of opening and closing tags,
– count of anchor tags/links (<a>),
• Stop words etc.
– count of email addresses and URLs in the literal text,
– count of stop words or phrases,
– number of copyright (©) or similar characters typical of boilerplate regions,
• Graphemic features
– count/ratio of numbers in the text,
– count/ratio of uppercase/lowercase letters in the text,
– count/ratio of punctuation/non-punctuation characters in the text,
¹²http://nlp.fi.muni.cz/projekty/justext/
¹³It should be mentioned that there are also attempts at deboilerplating that work without any of the markup related features
as mentioned here, but rely entirely on features of the natural language text, e. g., Evert [2007].

52
3. POST-PROCESSING
• Linguistically motivated features
– block ends in a sentence-ﬁnal punctuation character (at least [.?!]),
– count of sentences,
– average sentence length,
– average word length,
– conﬁdence of a language identiﬁer that the text is in the target language,
• Whole-document features
– length of the whole document with/without markup,
– overall ratio of markup/non-markup,
Obviously, all these features require little or no linguistic processing. Other features, which
do require such processing, are conceivable. Each block could be run through a parser, for example,
to see for how many sentences in the block the parser ﬁnds a parse. At least intuitively, if the parser
ﬁnds a lot of sentences for which there is a parse, the chances of the block containing a list of
menu items is quite low. e performance penalty would be severe, however: Since deduplication
(Section 3.5) works better on documents which have already been cleansed from boilerplate, it is
better to apply deboilerplating before deduplication. is means that the number of documents
is still very high at this stage, and the documents often contain many more boilerplate blocks
than text blocks. e amount of text which would have to be parsed is thus very high, which
can quickly lead to days and weeks of extra processing time. is is true for all more expensive
feature extraction and machine learning methods, even if they are not as expensive as parsing. For
example, Spousta et al. [2008] achieves quite good results with over 40 features and Conditional
Random Fields: Precision D 0:81, Recall D 0:8, F D 0:8. An eﬃciency of 300,000 documents
per CPU day is reported, which might already be too slow in some situations. e cost of extra
features or more expensive algorithms must therefore be balanced very carefully against the added
accuracy, which can be measured using the standard metrics of Precision, Recall, and F .
Figures 3.5 and 3.6 exemplify and visualize the usefulness of certain features. e sentence
count (a) goes up in the text region, so does the average sentence length (b). e proportion of
the markup (c) approaches zero in the text region; clearly, there are a lot of anchor tags and more
design-speciﬁc markup in the non-text regions, causing the proportion of markup to be higher.
e percentile in the text body (d) is an interesting feature. It is percentile in/proportion of the
whole non-markup text reached at the end of the block. As it is, it helps the deboilerplater to
identify the beginning and the end regions of the document, where there is usually only boiler-
plate. But clearly, the text mass grows only very slowly in the boilerplate regions, but rapidly in
the text regions. is might be informative for approaches which seek to extract coherent larger
areas of text, possibly accepting small amounts of boilerplate in the middle. Visually, the propor-
tion of letters within the text (e) is not very informative in this example (which does not mean it
is uninformative in general). e proportion of lowercase (vs. uppercase) letters (f) is consistently
www.allitebooks.com

3.3. BOILERPLATE REMOVAL
53
0
50
100
150
0
1
2
3
4
5
6
A
B C T EF
0
0
50
100
150
0
50
100
150
A
B C T EF
0
(a) Sentence count (estimated as
(b) Average sentence length (characters)
number of wordsnumber of [.!?])
0
50
100
150
0.0
0.2
0.4
0.6
0.8
1.0
A
B C T EF
0
0
50
100
150
0.0
0.2
0.4
0.6
0.8
1.0
A
B C T EF
0
(c) Markup proportion
(d) Percentile in text body
Figure 3.5: Distribution of some deboilerplater features extracted for the page in Figure 3.4, labels
are at the top. e x-axis is the n-th block, block boundaries were inserted at certain tags like <div>
and <p>. e vertical lines mark the beginnings of the regions from Figure 3.4; T marks the beginning
of the text region; 0 is material hidden by scripts and thus invisible in the rendered version; region D
is inserted by scripts and does not appear in the HTML source.
higher in the text region, and the proportion of punctuation characters (g) might be signiﬁcantly
raised. Finally, the number of anchor tags per character is consistently low.
Even in this illustrative example, there are certain jumps of some values between the blocks
in the non-boilerplate region. ere are also boilerplate blocks with at least some values being
more indicative of non-boilerplate areas. As we mentioned, these and other or similar features
are fed into some kind of machine learning software or used in heuristics for some non-machine

54
3. POST-PROCESSING
0
50
100
150
0.4
0.6
0.8
1.0
A
B C T EF
0
0
50
100
150
0.70
0.80
0.90
1.00
A
B C T EF
0
(e) Letter proportion in text
(f) Lowercase proportion in letters
0
50
100
150
0.00
0.05
0.10
0.15
A
B C T EF
0
0
50
100
150
0.00
0.10
0.20
0.30
A
B C T EF
0
(g) Punctuation proportion text
(h) Ratio anchors/character
Figure 3.6: Continuation of Figure 3.5. * Some low-end outliers are invisible. ** Some high-end out-
liers are invisible.
learning calculation of the goodness of the text in the block. Such accidental jumps in the values
might lead to erratic deletions of blocks in the middle of good regions as well as scattered blocks of
boilerplate remaining in the corpus when the machine learning algorithm is applied. To alleviate
this, a method can be chosen which does not consider the blocks in isolation, but also uses the
context of each block for the decision. For all other methods, the neighboring values for HTML
tag density and similar values can be added to a block’s feature set as additional features to achieve
something similar. is will in the best case lead to a smoother distribution of the delete and keep
decisions. In Section 3.3.3, we brieﬂy mention some machine learning options which have been
used.

3.3. BOILERPLATE REMOVAL
55
3.3.3
CHOICE OF THE MACHINE LEARNING METHOD
All sorts of methods have been used in boilerplate removal:
• Decision Trees, Language Models, Genetic Algorithms (evolving a regular expression) in
Hoﬀmann and Weerkamp [2007],
• naïve (in the words of the author) Language Models in Evert [2007],
• Support Vector Machines (SVM) in Bauer et al. [2007],
• Conditional Random Fields (CRF) in Marek et al. [2007]; Spousta et al. [2008],
• Naïve Bayes Classiﬁer in Pasternack and Roth [2009],
• Multi-Layer Perceptron (MLP) in Schäfer and Bildhauer [2012].
ere is no general recommendation, because the choice of algorithm (or decision to use
just some heuristics/hand-crafted formula) depends on the requirements for a given task, includ-
ing such considerations as the number of documents which have to be processed in which span
of time, the available machine power, the kind of web pages to be processed, maybe even the
programming language used for the project. Some methods are more accurate than others, given
the extracted features. Some algorithms require features with certain levels of measurement or
scalings. Sometimes, performance is of the essence, and some accuracy can be traded in.
For software systems which are redistributed to other users, additional considerations might
play a role. For example, since accuracy must be evaluated in terms of precision (roughly: the better
the precision the cleaner the corpus) and recall (roughly: the better the recall the more complete
are the documents), implementations which allow end-users of the software product to adjust
precision and recall for pre-trained models oﬀer an additional value. e Multi-Layer Perceptron
used Schäfer and Bildhauer [2012] and the texrex software is of such a nature. e Perceptron
itself calculates for each paragraph a value between 0 (low probability that it is boilplate) and 1
(high probability that it is boilerplate). To arrive at a binary classiﬁcation, a threshold somewhere
between 0 and 1 has to be applied. If a clean corpus is more important than complete documents,
the threshold can be set to a higher value, and vice versa. Figure 3.7 plots the precision and the
recall according to threshold settings between 0 and 1.
In any given project, it might be a good idea to benchmark various learning algorithms, fea-
ture sets, and training data sets to ﬁnd the best conﬁguration in terms of accuracy and eﬃciency.
An excellent tool to test many machine learning approaches eﬀectively and systematically is Weka
[Hall et al., 2009], which is used also in the comprehensive introductory book by Hall and Witten
[2011].¹⁴ e RapidMiner tool also oﬀers a huge number of algorithms, including data transfor-
mations.¹⁵ Other available comprehensive machine learning applications and libraries have other
advantages. For example, the Apache Mahout library is integrated with Apache Hadoop to allow
for easy parallelization on clusters of machines.¹⁶
¹⁴http://www.cs.waikato.ac.nz/ml/weka/
¹⁵http://sourceforge.net/projects/rapidminer/
¹⁶http://mahout.apache.org/

56
3. POST-PROCESSING
0.2
0.4
0.6
0.8
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Recall
Precision
F
Correct
Figure 3.7: Precision, recall and F for the generic MLP from Schäfer and Bildhauer [2012], ac-
cording to the cutoﬀ/threshold above which blocks count as boilerplate. e x-axis is the threshold.
Fmax D 0:75 (marked with a dot) at Threshold D 0:48 with Precision D 0:83 and Recall D 0:68
Finally, one aspect of block-wise boilerplate removal should be considered. If the recall of
the applied algorithm is lower than 1 (which it always is), then there are blocks missing which
would have contained good corpus material. In the worst case (combined with block detection
failure), a sentence spanning two blocks is truncated. is means that for certain types of linguistic
research such as Information Structure (when givenness of discourse referents is important), the
corpus might be unusable or at least of inferior quality. Also, in Information Extraction (e. g.,
distributional semantic approaches like LSA, Landauer et al., 1998) missing and broken contexts
of words or sequences of words can lower the quality of the results. Corpus designers can consider
the following options to deal with such problems:
1. Do not remove blocks arbitrarily, but keep blocks from a single window (possibly containing
several blocks) for which the overall boilerplate level is minimized [Baroni et al., 2009].
2. Keep everything and annotate potential boilerplate as such.
Option 1 does not solve all aforementioned problems. From the linguistic side, it might
matter that a discourse referent could still be introduced outside the selected window. is solution

3.4. LANGUAGE IDENTIFICATION
57
is also inadequate considering certain kinds of web document layouts such as forum layouts, which
typically contain a constant alternation of boilerplate blocks (date and time of posting, user aliases,
etc.) and normal text blocks (the actual forum posts), often in very long sequences. Option 2
massively increases the amount of data in the ﬁnal corpus, but is in our view the only option for
any kind of document-related linguistic research. It allows users of the ﬁnal product to restrict
queries and statistical analyses to the likely good blocks of the documents, but to see the boilerplate
regions when required. is approach has not yet been implemented, as far as we know.
ere are a number of readily available pieces of software which implement boilerplate
removal, such as boilerpipe, BootCaT, jusText, NCleaner, texrex.¹⁷;¹⁸;¹⁹;²⁰;²¹
is concludes the short introduction to boilerplate removal, and it leaves two major open
tasks in the non-linguistic post-processing department. e ﬁrst one is the task of identifying the
language of the document, which is required both for monolingual corpora (ﬁlter out documents
not in the target language) and multilingual corpora (assign a feature to each document which
identiﬁes its language). As it was said above, language identiﬁcation could also be used as a feature
in boilerplate removal (p. 52), and it could be used in focused crawlers (Section 2.4.3), but we now
move on and describe the general method separately in Section 3.4.
3.4
LANGUAGE IDENTIFICATION
In this section, we will describe approaches to language identiﬁcation very brieﬂy. In the context
of web corpus construction, language identiﬁcation is required, because even if national TLDs are
crawled or language guessing is already performed at crawl time based on URLs (cf. Section 2.4.3),
there will always be documents in the crawl data which are not in the target language. Language
identiﬁcation is therefore yet another necessary cleansing procedure. e brevity of this section is
mainly due to the fact that language identiﬁcation has been around for quite a while and can be
considered a stable discipline.
An early and very accessible paper summarizing the two main approaches is described in
Grefenstette [1995]. e two approaches are based on (i) character n-gram statistics plus machine
learning/language modelling or (ii) function word statistics. e majority of software developers
as well as authors (compare early papers such as Cavnar and Trenkle, 1994; Dunning, 1994) favors
the n-gram approach because it needs relatively little training data, is domain-independent (works
on all kinds of texts from the target languages), and is quite accurate also on shorter strings.
Recently, identifying languages for very short texts like messages in microblogging systems
or query strings has become a new focus of research. For example, cf. Carter et al. [2012]; Got-
tron and Lipka [2010]. However, some of the approaches to language identiﬁcation in microblog
posts use information external to the text (such as user proﬁles and linked user proﬁles), which
¹⁷http://code.google.com/p/boilerpipe/
¹⁸http://bootcat.sslmit.unibo.it/
¹⁹http://sourceforge.net/projects/webascorpus/files/NCleaner/
²⁰http://code.google.com/p/justext/
²¹http://sourceforge.net/projects/texrex/

58
3. POST-PROCESSING
are not available in web corpus construction. Another problem very speciﬁc but not exclusive to
web data is multilingual documents, i. e., documents which contain a mix of languages or, quite
frequently, one dominant language with shorter blocks from another language (mostly English).
A character n-gram approach is described in Teahan [2000], and in a more recent paper (which
tries to resuscitate the word-based approach), Řehůřek and Kolkus [2009] also describe a method
to achieve segmentation plus identiﬁcation.
One task very much related to language identiﬁcation, and which to our knowledge has
received very little attention, is the detection of documents containing non-text. For example,
many pages on the web contain tag clouds (isolated nouns, adjectives, and other lexical words),
which end up as incoherent word lists in the corpus. Other examples are dictionary lists, lists
of products or advantages of a product (maybe in the form of truncated noun phrases), or lists
of signers of a petition. Such documents are often identiﬁed as being in the target language by
standard tools based on character n-gram statistics, but many corpus designers would probably
rather not have them in the corpus. e function word method is better suited to detect such
documents, and for the WaCky corpora and the COW corpora, more or less heuristic methods
were used to ﬁlter documents with few or no function words. e accuracy of both approaches
has not been reported properly, so we leave out the details here. Since the common classiﬁcation
methods in language identiﬁcation are supervised, however, they require a manually classiﬁed
training input. is means that corpus designers need to operationalize what a non-text document
is, which is not an easy task in our experience. To aggravate the situation, there are of course
documents which contain “good” text along with tag clouds, lists, and foreign material in all
kinds of proportions.
Finally, we would like to mention that there are many libraries and tools implementing
the standard approaches to language identiﬁcation available. Among the freely available ones
which come packed with a lot of language models (more than 50) are mguesser, written in C,
language-detection, written in Java, and langid.py, written in Python.²²;²³;²⁴
3.5
DUPLICATE DETECTION
3.5.1
TYPES OF DUPLICATION
A huge number of the documents found on the web are not unique. Content creators copy from
each other or buy content from the same sources (such as news agencies); some providers also
serve the same content from diﬀerent hosts, etc. Blogs and other content management systems
create duplication very systematically by oﬀering diﬀerent views of the same content.
Systematically, we can say that duplication comes in the following types:
1. full duplication of documents (e. g., two servers generating content from one database),
²²http://www.mnogosearch.org/guesser/
²³http://code.google.com/p/language-detection/
²⁴http://github.com/saffsd/langid.py/

3.5. DUPLICATE DETECTION
59
Figure 3.8: Blog content management system introducing redundancy through multiple views of the
same content. On the left is the homepage view containing the n most recent posts, including the
post Erotische Schriften. On the right is the page with the individual view for the single post Erotische
Schriften.²⁵
2. partial duplication of documents (e. g., by quotation, plagiarism, etc.),
3. document inclusion (e. g., in blog systems as illustrated above in Figure 3.8),
4. in-document duplication (mostly forum threads with “Quote” function being used exces-
sively).
We do not treat in-site duplication as a case of its own, because it is usually covered by one
of the above types. In-site duplication occurs when bits of text occur on many pages of one site.
If this happens, it is mostly caused by blog or CMS systems and a case of either partial duplica-
tion or inclusion. When detected in the ﬁnal corpus, in-site duplication can also be indicative of
insuﬃcient boilerplate removal.
In Section 3.5.2, we describe the simple methods by which perfect duplicates can be de-
tected, and in Section 3.5.3, we deal with the slightly more diﬃcult matter of how to detect
documents which share a lot of content and are quite similar. Both methods can be applied at
any level. ey can be applied to the sentences or blocks within a document, to documents of
one host, or to a whole corpus of documents. e only thing that changes is the feasibility of
a one-by-one comparison. While it is usually no problem (although ineﬃcient) to compare the
sentences from a document pairwise (because most documents are small enough), doing the same
with all documents of a large corpus quickly becomes too time consuming.
As an example, assume the corpus is 107 documents large before deduplication. Pairwise
comparison of documents means that we have to make
²⁵From: http://www.lawblog.de/

60
3. POST-PROCESSING
 
107
2
!
D 5  1013
(3.4)
comparisons. At a speed of 1s per comparison, this will take almost 580 days. us, to do
document comparison for large corpora, we need to break down the task such that it scales much
better than pairwise comparison. Both types of duplicate detection can be performed in such a
way.
However, what we do with the identiﬁed duplicates is, once more, a design decision. Within
a document, duplicate paragraphs might be removed, maybe inserting a backreference to indicate
to corpus users that there was a repeated paragraph (and which one it was). In document dedu-
plication, we might decide to erase all except one randomly chosen document from each set of
perfectly identical documents and to remove the shorter one of two near duplicates (to maximize
the amount of text in the deduplicated corpus). In the original approach described in Section 3.5.3
[Broder et al., 1997], the goal of the authors was to cluster documents by similarity.
3.5.2
PERFECT DUPLICATES AND HASHING
Detecting perfect duplicates can be achieved by keeping hashes of documents already seen, and
discarding documents with the same hash. A hash function calculates a numerical representation
with a ﬁxed bit length (a hash value) of another object (the key, in our case exclusively character
strings/documents). Ideally, two diﬀerent keys are rarely represented by the same hash value, and
similar keys are not represented by similar hash values. If two non-identical keys accidentally have
the same hash value, it is called a collision. As a general rule, the longer the hash value (in bits),
the more rarely there is a collision.
Since for two identical documents any hash function calculates the same value, it suﬃces
to keep a list of hashes of each document seen, simply discarding documents which have a hash
value which is already known. e eﬃciency of the procedure depends on the total number of
documents, the eﬃciency of the hash function, and the sorting/lookup algorithm used to store
the hash values. e number of false positives (documents falsely discarded as documents) depends
on the quality of the hash function and the hash length. Likely, using a Bloom Filter as described
in Section 2.3.3 for the URL Seen Test will also provide the best compromise between accuracy
and eﬃciency here.
However, even a single character missing, added, or changed in one of two otherwise iden-
tical documents, will lead to a false negative (two near duplicate documents being kept). is is
because of the otherwise desirable property of hash functions that a similarity of the keys does
not lead to similar or even identical hash values. To illustrate, assume the very short documents
d1 and d2, only diﬀering by a single comma:
d1 DYesterday we calculated a hash value eﬃciently and accurately.
d2 DYesterday, we calculated a hash value eﬃciently and accurately.

3.5. DUPLICATE DETECTION
61
For example, the 32-bit Fowler–Noll–Vo hash function (FNV-1) produces the follow-
ing entirely diﬀerent hexadecimal values: FNV1.d1/ D 5c517c3d and FNV1.d2/ D e1235bef.²⁶
Since a Bloom Filter is based on hashing, this problem will also aﬀect Bloom Filter implementa-
tions. In Section 3.5.3, we introduce a standard method to detect such cases of near duplication.
3.5.3
NEAR DUPLICATES, JACCARD COEFFICIENTS, AND SHINGLING
To test whether two documents are quite similar, although not identical, we need a measure of
document similarity. A straightforward such measure is the Jaccard Coeﬃcient [Manning et al.,
2009, 61]. It measures the similarity of two sets, such that we can derive the set of token n-grams
from a document and compare the result. For example, the token bigram sets (call them bigram
ﬁngerprints F2) of the documents d1 and d2 from Section 3.5.2 are as follows (non-identical
members in bold print):
F2.d1/ D f(Yesterday;we),(we;calculated),(calculated;a),(a;hash),(hash;value),
(value;eﬃciently),(eﬃciently;and),(and;accurately),(accurately;.)g
F2.d1/ D f(Yesterday;,),(,;we),(we;calculated),(calculated;a),(a;hash),(hash;value),
(value;eﬃciently),(eﬃciently;and),(and;accurately),(accurately;.)g
e Jaccard Coeﬃcient J of these sets is calculated as:
J.F2.d1/; F2.d2// D jF2.d1/ \ F2.d2/j
jF2.d1/ [ F2.d2/j D 8
11  0:73
(3.5)
We could now go on and determine a threshold for the Jaccard Coeﬃcient, above which
the shorter of the documents is erased from the collection. Of course, it is again impractical to
calculate Jaccard Coeﬃcients for all pairs of documents in a large collection (Section 3.5.1). e
shingling approach suggested by Broder et al. [1997] therefore reduces the n-gram (n customarily
between 4 and 6) ﬁngerprint to a smaller but representative set of hash values, which can be sorted
and processed in a linear fashion.²⁷ Here, we only describe the practical procedure for shingling
without clustering. For the proofs and the extension to clustering, cf. Broder et al. [1997] and
Section 19.6 from Manning et al. [2009]. For an overview and evaluation of other techniques, cf.
Henzinger [2006].
e simpliﬁed shingling procedure without clustering can be decomposed into the follow-
ing steps. We leave the question of how to hash the n-grams and how to select the representative
hashes to be explained later in this section:
1. Construct a shingle ﬁngerprint FSj for each document dj, where a shingle ﬁngerprint is a
ﬁxed-length set of hashes called shingles (see below).
²⁶e algorithm is under standardization by the Internet Engineering Task Force: https://www.ietf.org/.
²⁷Originally the method was called w-shingling. We omit the w for convenience reasons.

62
3. POST-PROCESSING
2. Store (in memory or on disk) for each shingle si and each document ﬁngerprint FSj , such
that si 2 FSj , a pair hsi; Uj i, where Uj is a unique identiﬁer of document dj, for example
the document’s URL or a numeric index into a table of such URLs.
3. Sort the shingle—document identiﬁer pairs by the shingle values.
4. Process the sorted pairs, and write new pairs of two document identiﬁers such that for
every two shingle—document identiﬁer pairs where hsk; Uli and hsk; Umi, a pair hUl; Umi is
created. is can be done by linearly processing the sorted pairs, because pairs with identical
shingles are next to each other.
5. Sort the document identiﬁer pairs hUl; Umi and count the frequency of each type of pair.
e frequency of hUl; Umi is the number of shared shingles between dl and dm.
6. If the frequency of some hUl; Umi exceeds a certain threshold, erase one of the documents
corresponding to Ul and Um (preferably the smaller one).
e trick, informally speaking, is that we represent the document n-grams by a set which is
smaller (on average) and that we massively reduce the comparison task by turning into the linear
processing of a sorted list. e computationally most demanding task is the shingle creation (cf.
below). e sorting processes in steps 3 and 5 can be performed eﬃciently using a divide–sort–
merge approach, where smaller ﬁles are sorted separately and then merged in a zipper-like fashion.
Even oﬀ-the-shelf tools like GNU sort apply this strategy and can be used in these cases (if text
ﬁles are used to store the tuples) for hands-on implementations, and the counting in step (5) can
be accomplished using GNU uniq.
For very frequent shingles, step 4 obviously creates quite a lot of pairs of document identi-
ﬁers. Although it is not a sound procedure, shingles which occur in more than a certain number
of documents (a threshold to be set heuristically) can be discarded completely to avoid this—at
the cost of some accuracy.
Finally, how are the shingle ﬁngerprints created? e procedure (for each document) is in
fact quite simple:²⁸
1. Create the set N of the document’s n-grams.
2. Hash the n-grams using an i-bit hash function, forming the set H of hashes. Notice that
jNj D jHj.
3. Permute the hashes using j (usually several hundred) random permutations of the i-bit
integers, creating the sets P1::Pj of permuted hashes. Notice that jNj D jHj D jPnj for
each Pn from P1::Pj.
4. Take the minimum value min.Pn/ from each Pn in P1::Pj and put them in the shingle
ﬁngerprint FS. Notice that jFSj D j.
e technically demanding task here is step 3, because coming up with random permu-
tations of 64-bit integers (the recommended hash size) is not a simple task. A good solution to
²⁸We describe the minimum hash method; cf. the referenced papers and introductions for the similar modulo hash method.

3.5. DUPLICATE DETECTION
63
avoid this is to use Rabin hashes [Rabin, 1981]. ey can be computed very eﬃciently, and there
is a huge supply of diﬀerent Rabin hash functions which can be used instead of permutations of
a single hash: e Rabin hash function is seeded with a binary representation of an irreducible
polynomial of degree j (for j-bit hashes), and we can thus easily create hundreds of diﬀerent
Rabin hash functions ad hoc. We are not aware of a Rabin hash implementation in standard
libraries, but there are eﬃcient sample implementations, for example an ObjectPascal imple-
mentation in our own texrex suite, which is based on a Java implementation also available with
the source code under an open license.²⁹ Among the available all-in-one software solutions for
near-duplicate removal are texrex and Onion.³⁰
SUMMARY
In this chapter, we have described commonly applied techniques to cleanse raw web data from
whatever might be considered undesirable corpus material: markup, boilerplate, foreign language
material, duplicates. It must be kept in mind that all these techniques involve design decisions and
alter the data in some way. is is especially true because we usually use computational methods
which do not perfectly implement our design decisions, but introduce a certain error. is (as well
as the nature of the data itself) causes problems for the subsequent processing steps (Chapter 4),
but it also has to be kept in mind by corpus users. For example, if boilerplate removal was applied
at the sentence, paragraph, or block level, then certain types of research require extra caution.
Chapter 5 will introduce some simple and eﬀective methods to assess the amount of some types
of noise introduced by non-linguistic post-processing.
²⁹http://sourceforge.net/projects/rabinhash/
³⁰http://code.google.com/p/onion/


65
C H A P T E R
4
Linguistic Processing
4.1
INTRODUCTION
After having applied the processing steps discussed in the previous chapters, we ﬁnally have a (pos-
sibly huge) body of clean text. In this chapter, we discuss the basics of linguistic post-processing:
In order to make the web corpus a usable resource for linguistic research, the minimal processing
is to split it up into word and sentence tokens. Moreover, the more levels of additional reliable
linguistic annotation there are, the more useful a corpus will be. ere are many diﬀerent kinds of
annotations, both on the word level and on the level of larger spans of text. ese include part-of-
speech labels and lemmas, syntactic phrase structure and dependency structure, semantic labels
such as word senses or named entities, semantic/pragmatic annotation levels like co-reference,
information structural labels such as topic and focus, and many more. Some corpora also include
meta-data for the whole document (e. g., authorship, date of authorship, etc., genre, domain),
which can also be exploited in linguistic studies. Given the size of most web corpora, tokeniza-
tion as well as all other annotations have to be done automatically.
is chapter deals with the basic steps of linguistic post-processing, namely tokenization,
part-of-speech tagging, and lemmatization. ere is a huge number of tools freely available for
these tasks, and we mention only some of them. e main purpose of this chapter is to draw
attention to the typical problems that arise with the kind of noisy data speciﬁc to web corpora.
Where applicable, we point out possible solutions and workarounds. Since we have optimal access
to the data from diverse processing stages of our own corpora, we use them to derive examples.
Having read through this chapter, readers should:
1. have an understanding of what tokenizing, lemmatizing, and part-of-speech tagging a text
usually involves,
2. have an idea of the main sources of noise in web corpora and which problems this noise
causes for standard NLP tools,
3. know how the amount of some types of this noise can be reduced in relatively simple ways
while being aware that other kinds of noise are much harder to ﬁx automatically in a reliable
manner.

66
4. LINGUISTIC PROCESSING
4.2
BASICS OF TOKENIZATION, PART-OF-SPEECH
TAGGING, AND LEMMATIZATION
In this section, we give an overview of the techniques used in automatic tokenization, part-of-
speech tagging, and lemmatization, before we turn to the speciﬁc challenges that web corpora
present in this area. Readers with a background in NLP can skip the following paragraphs and
continue with Section 4.3.
4.2.1
TOKENIZATION
e raw data used to build a corpus is usually contained in one or more text ﬁles. e text con-
sist of strings of characters, including formatting characters like newline or carriage return. In
order to make it usable for most applications, it has to be broken up into smaller, linguistically
meaningful units. From a linguistic perspective, the resulting tokens should be of a size such that
linguistic features can be attributed to them. Minimally, these units are words and sentences.
Splitting up words into smaller morphological units is also possible, although it is rarely done.
Phrasal constituents (usually consisting of several words but being smaller than sentences) are of
course also bearers of linguistic features, but these are much harder to detect automatically than
words or sentences. eir automatic identiﬁcation typically requires that the data have undergone
tokenization already, along with a part-of-speech annotation for each word token. Tokenization
usually refers to the process of dividing the input into word tokens, whereas detecting sentence
boundaries is also referred to as sentence splitting.
In the writing systems of many languages, a word token is a string of non-whitespace char-
acters, bounded on either side by whitespace or some punctuation mark. For instance, among the
major writing systems of the world, this is true for the Cyrillic and Latin alphabets. On the other
hand, a signiﬁcant number of (predominantly Asian) writing systems do not explicitly mark word
boundaries. As a consequence, words have to be discovered in an unlabeled stream of symbols (see
Goldsmith, 2010, for a summary of commonly used techniques). Well-known examples include
the Chinese ideographic script and Chinese-based syllabic scripts (Japanese Kana). Automatic
tokenization of such texts is a much more challenging task that has drawn considerable attention
over the last two-and-a-half decades (see overviews in Wu and Tseng, 1993; Teahan et al., 2000;
Xue, 2003; Huang et al., 2007). For example, approaches to tokenizing Chinese (e. g., Chen and
Liu, 1992) often involve a dictionary against which the input string is matched, and a heuristics
that resolves potential ambiguities (when a string of characters can be parsed into more than one
sequence of lexical items). Other approaches are purely statistical (e. g., Sproat and Shih, 1990 for
Chinese; Ando and Lee, 2003 for Japanese) or combine lexico-grammatical knowledge with sta-
tistical information (e. g., Sproat et al., 1996). More recently, machine learning approaches have
been proposed for this kind of task as well (see Huang et al., 2007, and references therein). In sum,
then, tokenization of non-segmented languages has developed into its own sub-ﬁeld of compu-
tational linguistics, which cannot be covered adequately in this introduction. In what follows, we
will therefore concentrate on languages which mark word boundaries explicitly in written text.

4.2. TOKENIZATION, PART-OF-SPEECH TAGGING, AND LEMMATIZATION
67
As a ﬁrst approximation, a tokenizer could split strings on whitespace and on a small num-
ber of dedicated punctuation marks, such as [.!?]. is simplistic approach will produce correct
results in many, but not all cases. e two main reasons for this are:
• Some punctuation marks, most notably the period, are ambiguous between several func-
tions. For example, a period can mark the end of a sentence (in which case it is a separate
token), it can be part of an abbreviation (in which case it is not a separate token), it can oc-
cur as part of ordinal numbers (in which case it is not a separate token), or its function can
be a combination of these (e. g., an abbreviation at the end of a sentence). e ambiguity of
periods thus concerns both word tokenization and sentence splitting.
• Not every instance of a whitespace character marks a word boundary. Multi-word expres-
sions (MWEs, words with space(s)) consist of a sequence of two or more units separated by
whitespace, and because of its high degree of lexicalization, the entire sequence is consid-
ered as a single token. What exactly should count as a multi-word expression is a matter of
debate and depends on linguistic considerations as well as on the particular language under
investigation. English MWEs (presented here as examples to give a rough idea) arguably
include constructions like compound nouns (vice president), ﬁxed phrases (by and large),
complex prepositions/adverbials (in order to), idioms (bite the bullet), phrasal verbs (to pick
up) and named entities (European Union).
Many tokenizers use hand-coded rules in order to split the input into tokens. Examples
of tokenizers using this technique are the Penn Treebank tokenizer [MacIntyre, 1995], Ucto
[van Gompel et al., 2012], and the tokenization script that ships with the TreeTagger [Schmid,
1994b]. Tokenizers of this kind match a number of regular expressions against the input in a par-
ticular order. Some of them also perform sentence splitting. In addition, it is very common to use
one or more lists of strings that should not be split any further, most notably lists of multi-word
expressions and known abbreviations in a particular language (see Grefenstette and Tapanainen,
1994, for an early evaluation of diﬀerent strategies for distinguishing between abbreviations and
sentence boundaries). Furthermore, tokenizers of this kind usually come equipped with a heuris-
tics in order to handle ambiguous punctuation marks. is is necessary because no list of abbre-
viations will ever be complete, as abbreviating words is a productive process.
On the other hand, there are also machine learning approaches to tokenization. In such
approaches, regularities of punctuation are extracted automatically from text. is usually requires
manually annotated training data (as in Palmer and Hearst, 1997), but unsupervised methods have
been proposed as well (e. g., Kiss and Strunk, 2006; Schmid, 2000).
A primary cue for ambiguity resolution, exploited by many tokenizers (both the hand-
crafted ones and the ones based on machine learning), is provided by the token which follows an
ambiguous case, most notably capitalization and character class. Some tokenizers also make use
of a larger context. For example, Palmer and Hearst [1997] use 3-grams on each side of a period;
Mikheev [2002] extracts information from the current document; Grefenstette and Tapanainen

68
4. LINGUISTIC PROCESSING
[1994], Kiss and Strunk [2006], and Schmid [2000] use information computed from the entire
corpus.
e kind of information that tokenizers rely on also varies: word-based tokenizers may
look up information about (potential) tokens in a lexicon; some sentence detectors use part-of-
speech information, thus requiring labeled input data (an example is the system of Palmer and
Hearst, 1997). Other systems extract various kinds of frequency information (e. g., capitalization
vs. non-capitalization of the same word, Schmid, 2000; collocation measures, Kiss and Strunk,
2006) which can then be used to decide ambiguous cases.
High-quality tokenization of the data is essential for subsequent steps of linguistic post-
processing, since virtually all of these steps involve reasoning about word tokens (or their annota-
tions) in one way or another, and shortcomings in tokenization can lead to substantially increased
error rates in later, higher level linguistic processing. State-of-the-art tokenizers and sentence
splitters achieve a very high accuracy (some of them exceeding 99%) when tested on a manually
annotated gold standard corpus. For English, it is usually a corpus like the Brown Corpus or
the Wall Street Journal Corpus. For an overview of the tasks and challenges of tokenization and
sentence boundary detection, see e. g., Mikheev [2003].
4.2.2
PART-OF-SPEECH TAGGING
Part-of-speech tagging (POS tagging) is the process by which each token in a corpus is assigned
its part of speech. Voutilainen [2003] notes that part-of-speech tagging has been a topic of re-
search since the late 1950s, but it was only in the late 1970s that the accuracy of such systems
reached a relatively high level (around 96% of correctly classiﬁed words). Essentially, given a to-
kenized input text, a tagger determines the possible part-of-speech tags of a token, for example
by looking them up in a lexicon. If a token is ambiguous between two or more parts of speech,
the tagger must determine which POS is the correct one in the given context (disambiguation).
If a token is unknown, i. e., not in the lexicon, the tagger must guess its part-of-speech tag. e
information necessary for disambiguation may come from looking at the context of an ambiguous
token (typically the POS tags of one or two preceding tokens), and from features of the token
itself, such as the frequency with which it occurs as a particular part of speech [Manning and
Schütze, 1999, Ch. 10]. Similarly, the context can be exploited to guess the part of speech of
an unknown word, in combination with information of the unknown word itself, for example
morphological cues such as suﬃxes and orthographic cues like capitalization.
A host of diﬀerent techniques have been explored to tackle these tasks, including hand-
crafted rules (see Voutilainen, 2003 for an introduction), rules induced by transformation learn-
ing [Brill, 1992, 1995], combinations of rules with a statistical component (e. g., Tapanainen and
Voutilainen, 1994), hidden Markov models (HMMs; see Manning and Schütze, 1999 for an
overview), HMMs in combination with decision trees (e. g., Schmid, 1994b), maximum entropy
models (e. g., Berger et al., 1996; Ratnaparkhi, 1998), as well as machine learning approaches
such as artiﬁcial neural networks (e. g., Schmid, 1994a), Conditional Random Fields (e. g., Laf-

4.2. TOKENIZATION, PART-OF-SPEECH TAGGING, AND LEMMATIZATION
69
ferty et al., 2001), Support Vector Machines (e. g., Giménez and Màrquez, 2003), and k-nearest
neighbor (e. g., van den Bosch et al., 2007).
Reported per-word accuracy of state-of-the-art taggers is usually in the range of 96–98%
on standard written language, usually newspaper texts. Reported ﬁgures for some taggers even
exceed 98% (e. g., Tapanainen and Voutilainen, 1994). However, these ﬁgures must be interpreted
considering that up to 90% accuracy can be achieved just by always choosing the most frequent
part of speech of a given token in ambiguous cases (see Church and Mercer, 1993). us, the
baseline that taggers have to exceed is quite high to begin with. e accuracy is considerably
lower for unknown words. Tagging accuracy in web data will be addressed in Section 4.5.
From a practical point of view, there are considerable diﬀerences between implemented
POS taggers both in terms of processing speed and the amount of memory required. Another
important diﬀerence is that while many taggers have to be trained with hand-annotated data,
other taggers can also learn from raw text. us, when we are planning to train a tagger (instead
of using one of the language models that ship with most taggers), then the choice of a particular
tagger also depends on whether or not annotated training data is available. At the end of this
chapter, we oﬀer a list of suggestions for freely available NLP software.
4.2.3
LEMMATIZATION
Lemmatization is the process of reducing a set of inﬂected or derivationally related word forms
to a smaller number of more general representations (see Fitschen and Gupta, 2008, on which
much of the following paragraph is based).
A relatively simple variant of lemmatization is stemming, the process of truncating words,
usually by applying some kind of heuristics. e remaining part of the word form does not in all
cases correspond to a lexical item in the particular language, and the removed material may or may
not correspond to derivational or inﬂectional morphemes (see Porter, 1980, for a classic stemming
algorithm for English). More elaborate variants of lemmatization involve a lexicon and perform an
analysis of a word form’s morphological structure which can then be output as a morphological tag,
along with a lemma. Computationally, morphological analyses are very commonly implemented
using ﬁnite-state techniques (see Goldsmith, 2010, for a summary and references).
Depending on the language, a substantial proportion of word forms may be ambiguous
in that they allow for more than one morphological parse. For instance, Yuret and Türe [2006]
report that nearly 50% of the words in running text are morphologically ambiguous in Turkish.
It is thus necessary to include a component that disambiguates between several morphological
analyses of a given word form. Such disambiguation can be rule-governed or probabilistic, or a
combination of these, and it may rely on additional information, such as part-of-speech tags. In
practice, part-of-speech tagging and lemmatization are often performed jointly by the same piece
of software.
As Fitschen and Gupta [2008] stress, the accuracy of lemmatization depends to a large
extent on the quality and size of the lexicon. In case a word form cannot be mapped to one of

70
4. LINGUISTIC PROCESSING
the lemmas in the lexicon, lemmatizers follow diﬀerent strategies. For example, a lemma may
be marked as unknown, a lemma may be a simple copy of the word form found in the input, or
the lemmatizer may try to guess a lemma on the basis of a partial morphological analysis (for
instance, if a suﬃx could be recognized). Issues concerning the lemmatizer’s lexicon are especially
relevant, as web corpora tend to have a particularly large vocabulary and many items are unlikely
to be listed in any lexicon for reasons to be discussed in the next section.
4.3
LINGUISTIC POST-PROCESSING OF NOISY DATA
4.3.1
INTRODUCTION
As stated above, state-of-the-art tokenizers and sentence splitters achieve a very high accuracy
when tested on carefully edited, standard written language. Similarly, reported accuracy in part-
of-speech tagging usually refers to performance tests on standard written language. As a matter of
fact, it is often the same corpora that are used in benchmarking tokenizers and sentence splitters.
However, the kind of data we are dealing with when constructing web corpora presents its own
challenges for tokenizers and part-of-speech taggers, most of all because web corpora tend to
contain a relatively high level of noise even after all the cleanup steps discussed in the previous
chapters. Noise in web corpora includes:¹
• faulty punctuation, in particular omitted punctuation marks or omitted whitespace after
punctuation marks,
• non-standard orthography, either erroneously or as part of particular writing conventions in
certain web genres, for example, ignoring standard capitalization rules or using contracted
forms (like English dunno < don’t know, German haste < hast du ‘have you’),
• incomplete cleansing of code and HTML-markup during (non-linguistic) post-processing,
• all kinds of strings that resist straightforward tokenization and POS tagging even from a
conceptual point of view, for example:
%SystemRoot%\System32
$RapperDenIchNichtKenne
$ENV{PERL5DB}
Here’s-a-belt-for-the-Rat
AbCdEfGhIjKlMnOpQrStUvWxYz
But why is noise such a problem at all, given the huge size of web corpora and the fact that
the vast majority of units are still tokenized and POS tagged correctly? Web corpus designers and
users have to care about this because noise in the form of tokenization errors, misspellings, foreign
language material, non-words, etc., leads to a very high count of lexical types, in particular hapax
¹Notice that much of what must be called noise in this context might be called the relevant part of the signal in certain ﬁelds
of research, such as research on non-standard spellings. e reason why we classify it as noise is that it deviates from the kind
of language that most NLP tools are designed to cope with, and it therefore causes diﬃculties.

4.3. LINGUISTIC POST-PROCESSING OF NOISY DATA
71
legomena (words that occur only once in the corpus). We will refer to types created by this kind
of error as pseudo-types.
An immediate consequence of the presence of pseudo-types is that any ﬁgure concerning
the lexicon size (number of diﬀerent tokens in the corpus) will be distorted, perhaps massively
so. is can reduce usability of the corpus for research questions that need to refer to the lexicon
size in one way or another. Figure 4.1 illustrates this with data from the German DECOW2012
corpus (see also Liu and Curran, 2006, for similar counts in other web corpora):
N tokens:
9,108,097,177
N types:
63,569,767
N hapax legomena:
39,988,127
Figure 4.1: Proliferation of types: type and token counts for German web corpus DECOW2012.
On closer inspection, it turns out that in this speciﬁc corpus, about one half of all hapax
legomena is due to the four sources of noise mentioned above (see Figure 4.2). In this particular
corpus then, we have roughly 20 million pseudo-types that do not occur more than once. However,
a fair number of such pseudo-types are instantiated by more than one token in a large web corpus
if no measures are taken to prevent it. For example, as will be illustrated in Section 4.5, some
spelling errors are quite common, thus adding to the count of pseudo-types. us, noise can have
a huge inﬂuence on important properties of a web corpus.
Some of this noise can be reduced in the linguistic post-processing, but a certain level of
noise will most likely always remain in web corpora of giga-token size, and this has to be borne
in mind because it has consequences for the kind of linguistic research questions that can be
answered using the ﬁnal corpus.
Source
%
95% CI (˙%)
misspelling
20:0
5:0
tokenization error
17:6
4:7
non-word
7:6
3:3
foreign-language material
6:8
3:1
rare word
46:8
6:2
number
1:2
1:3
Figure 4.2: Noise in web corpora: classiﬁcation of hapax legomena (DECOW2012). Estimated pro-
portions of diﬀerent categories (n D 250), with 95% conﬁdence interval (CI).
4.3.2
TREATMENT OF NOISY DATA
In the context of natural language processing, the handling of noisy data has attracted an increas-
ing number of researchers in recent years due to the ever-growing amount of text available from

72
4. LINGUISTIC PROCESSING
noisy web sources (as in informal text types like emails, chat protocols, blogs, forums, tweets, etc.),
and this is reﬂected in a growing body of literature on the subject. Some of the noise problems
we ﬁnd in web corpora, e. g., the omission of whitespace, are comparable to those encountered
with texts obtained from OCR’d (optical character recognition) sources (cf. Esakov et al., 1994;
Lopresti, 2009). Other problems, such as the omission of punctuation marks, are reminiscent
of tokenization/sentence splitting of data obtained from automatic speech recognition, where
orthographic end-of-sentence markers are missing altogether and sentence boundaries must be
inferred using other cues [Ostendorf et al., 2007]. Still other problems, like misspellings, have
also attracted attention in other areas of Computational Linguistics, such as text mining, term
extraction, ontology building, information retrieval, and query optimization. For an overview of
classical techniques for spelling correction, see Kukich [1992]. For a more recent survey on diﬀer-
ent aspects of the noise problem in natural language processing, and techniques for dealing with
it, see e. g., Subramaniam et al. [2009] and the references in Section 4.6 below.
However, to the best of our knowledge, the applicability of such techniques in the creation
of web corpora has not been evaluated yet. Hence, it is an open question whether they can be
exploited for normalizing web corpora.² e principal challenge, as we see it, is that constructing
a resource suitable for linguistic research requires a very high degree of precision. If in doubt, it is
better to leave the data unchanged than to apply inadequate normalizations. As a consequence,
rather than proposing a general solution to the noise problem in the linguistic post-processing
of web corpora, we will illustrate the kind of problems that typically arise when tokenizing, lem-
matizing, and part-of-speech tagging noisy web texts, and show how some of them can be over-
come (or at least, reduced) by applying a number of simple measures. ese are intended to give
the reader a rough idea of the kind of pre-processing that may be required. In any case, anyone
building a web corpus needs to inspect their data, detect potential problems, and ﬁnd a suit-
able solution. In Section 4.4, we illustrate the usefulness of applying some simple but eﬃcient
pre-processing steps before tokenizing noisy web data by two examples: omitted whitespace after
sentence-ending punctuation and emoticons.
4.4
TOKENIZING WEB TEXTS
4.4.1
EXAMPLE: MISSING WHITESPACE
Consider the omission of whitespace after end-of-sentence punctuation. Token-internal periods,
as exempliﬁed with data from the British National Corpus in Figure 4.3, are not much of an
issue when tokenizing standard written language because erroneous omissions of whitespace after
an end-of-sentence period are relatively rare (cf. Schmid, 2000, 14 for a similar assumption).³
However, such omissions are more frequent in the informal text types that are likely to populate
²Some sparse remarks are found, e. g., in Fletcher [2011], but without mentioning concrete web corpus projects which have
applied spell-checking.
³But this is not to say that the problem is nonexistent with standard written language: there are also instances of erroneously
omitted whitespace after punctuation in various sections of the BNC, similar to those in Fig. 4.4.

4.4. TOKENIZING WEB TEXTS
73
web corpora. Figure 4.4 illustrates this with data from the English UKCOW2012 corpus (ca. 6
billion tokens).
Token(s)
Original Context
12.6.1986
e Daily Telegraph, 12.6.1986
340p.p.m.
these small vein zircons are U-rich (average 340p.p.m.)
$2.1m 41.8m
mid-term net losses stood at $2.1m up from losses of 41.8m last time
$2.1m
Performances start at 7.30pm Wednesday-Saturday
Figure 4.3: Examples of token-internal periods in “standard” text (BNC): can be ignored by tok-
enizer/sentence splitter.
Token
Original Context
part.ere
I have played a very modest part.ere are a whole lot of people who
cloth.ere
eg with a dry linen or cotton cloth.ere are all sorts of peg pastes w
reason.If
lding you back for a speciﬁc reason.If we are honest, our equipment
quarry.You
uising slowly looking for our quarry.You are in the most superb wilder
otters.You
mentioned so far is the giant otters.You may have seen these on TV as
weeks.We
sed in this action-packed two weeks.We have been fortunate enough to
Figure4.4: Omitted whitespace after punctuation. e ﬁrst column shows the token that would result
if no splitting on the token-internal period were performed. e second column shows some of the
context in which the token appears in the corpus.
One possibility to handle this problem is to separate these so-called run-ons before passing
the text to the tokenizer. A simple regular expression that singles out (a subset of) the problematic
cases is given here:
[:alpha:]\{2,\}[.?!;:,][:upper:][:lower:]\+
A tool like GNU sed could be used to insert a whitespace character after the punctuation mark.
It may take a few trials until the best strategy for a particular task is found, and it is a good idea
to evaluate the eﬀect of such substitutions on a sample before processing the entire corpus. For
example, substitutions based on the above regular expression had a precision of 94:3% (˙2:9%
at a 95% conﬁdence level). at is, 94:3% of the substitutions aﬀected the sort of orthographic
irregularity they were supposed to, but there were also a few false positives (mainly URLs con-
taining capital letters). Other regular expressions could have matched more run-ons, for example
instances where the second word includes punctuation marks (such as the apostrophe in ok.You’ll)
or does not start with a capital letter (new sentences starting with a lower-case letter are occasion-
ally found in informal writing), or where whitespace is missing not after [.?!;:,], but rather
after a quotation mark or a closing parenthesis (as in ok.”You). Extending the above regular ex-
pression to match such cases would likely raise the recall (i. e., match more run-ons), but almost

74
4. LINGUISTIC PROCESSING
certainly at the cost of lower precision (i. e., more matched strings which are not run-ons) if no
further measures are taken.
One way of achieving a higher precision is to perform a lexicon look-up for the two putative
words at either side of the putatively missing whitespace, and only insert a whitespace if both are
found in the lexicon. For example, using GNU Aspell’s British English lexicon, along with a
slightly more complicated regular expression covering all the additional cases just mentioned, we
obtained not a single false positive in experiments on the UKCOW2012 corpus.⁴ Alternatively, a
domain-speciﬁc lexicon can be bootstrapped from the corpus during a ﬁrst pass and look-ups can
then be performed with that lexicon. Another option is to check during a ﬁrst pass whether the
second putative word is a frequent sentence starter in the corpus, and use this information during
a second pass for deciding whether or not a whitespace should be inserted [Schmid, 2008].
us, using simple substitutions before tokenization, instances of non-standard punctua-
tion (and other irregularities) can be ﬁxed that would otherwise result in pseudo tokens (increasing
the number of hapax legomena), and get in the way of proper sentence boundary detection. In
addition, a few lines of code written in a script language around a regular expression are suﬃ-
cient to considerably enhance the quality of such substitutions, e. g., by checking strings against
a lexicon or by computing frequencies of occurrence.
4.4.2
EXAMPLE: EMOTICONS
Another characteristic of many texts obtained from the web is the widespread use of emoticons
of all kinds, like those illustrated in Figures 4.5 and 4.6. Emoticons do not normally appear in
newspaper texts or other texts written in a more formal register. us, any tokenizer designed for
such kinds of text will likely split emoticons into their component parts if no measures are taken
to prevent this. For example, the Penn Treebank tokenizer would split them up, as well as the
TreeTagger tokenizer. However, at least in our view, emoticons should be preserved in the ﬁnal
corpus, not only because splitting them up means distorting the frequency of their component
parts, but also because they are a unique feature of certain genres of electronic communication
and constitute a topic of investigation in their own right. So, if emoticons should be preserved
for the ﬁnal corpus, the tokenizer must be modiﬁed in a way that it does not split them into
their component parts. With rule-based tokenizers, this can easily be achieved by adding one or
more rules, e. g., in the form of regular expressions that match emoticons, or as a list of strings
that must not be split. In our experience, a combination of these works well. For example, in the
Dutch web corpus NLCOW2012 (total: 2.36 billion tokens), we recognized a total of almost
400,000 emoticons using this simple method.
:)
;)
;-)
:D
;D
:)))
:P
Figure 4.5: Normal ASCII smileys (conveniently also referred to here as emoticons).
⁴http://aspell.net/

4.5. POS TAGGING AND LEMMATIZATION OF WEB TEXTS
75
:wink:
:rolleyes:
:lol:
:confused:
:mrgreen:
:angel:
:cool:
Figure 4.6: Emoticons in the markdown syntax of the popular forum software phpBB (usually ren-
dered as images on the actual web page).
4.5
POS TAGGING AND LEMMATIZATION OF WEB TEXTS
POS taggers that were trained on standard written language and which are reported to deliver
high accuracy on this kind of data, are normally not as good at handling web texts. Giesbrecht and
Evert [2009] study the performance of several popular and freely available state-of-the-art POS
taggers: TnT [Brants, 2000], TreeTagger [Schmid, 1995], UIMA Tagger, SVMTool [Giménez and
Màrquez, 2004] and the Stanford Tagger [Toutanova et al., 2003].⁵;⁶ All taggers were trained
on the same corpus of German newspaper texts (TIGER; Brants et al., 2002). Giesbrecht and
Evert ﬁnd that all taggers fall short of reaching the high level of accuracy reported in the respective
publications, even in tagging standard newspaper texts. e performance on web data is still lower,
but varies with the genre of the text. For all taggers, the lowest accuracy (between 79:97% and
88:01%) is achieved in tagging texts from online forums. is is to be expected, since the language
used in many web forums is indeed informal and diﬀers quite dramatically from carefully edited
newspaper text. In terms of overall accuracy on web data, TreeTagger (using the pre-packaged
parameter ﬁle) outperforms the other taggers (at 94:77% accuracy). e authors also report that
the ratio of unknown words is substantially higher in web corpora than in newspaper corpora.
Since for all tested taggers, per-word accuracy is better with known words than with unknown
words, they identify the relatively large vocabulary of web corpora as one the sources of low tagger
performance. As we will show below, and in line with the ﬁndings of Giesbrecht and Evert [2009],
the principal challenges in part-of-speech tagging of web texts are:
• noise, mostly in the form of non-standard orthography, both intentional (genre-speciﬁc
writing conventions) and unintentional (spelling errors), as well as foreign language mate-
rial,
• lexical characteristics of web texts (domain terms).
In order to enhance tagging accuracy of web texts, we need an idea of how much impact
each one of these factors has. In the following section, we illustrate this.
4.5.1
TRACING BACK ERRORS IN POS TAGGING
As a ﬁrst example, consider the data in Figure 4.7, taken from the English UKCOW2011 corpus
(0.9 billion tokens). e corpus was POS tagged with TreeTagger using the standard English
parameter ﬁle. In Figure 4.7, we list the top 60 word forms that were unknown to the tagger.
Among the unknown word forms, there are newly coined terms, colloquial and genre-speciﬁc
⁵http://uima.apache.org/downloads/sandbox/hmmTaggerUsersGuide/hmmTaggerUsersGuide.html
⁶http://www.lsi.upc.edu/~nlp/SVMTool/

76
4. LINGUISTIC PROCESSING
expressions (like website, blog, url), cases of non-standard or faulty orthography (dont, thats), as
well as word forms that could not be found in the lexicon because the input was not tokenized
correctly (and/, days., ’e). Many of the unknown forms are assigned a correct part-of-speech
tag because the tagger guessed it successfully from the context, such as website, blog, healthcare.
Others are probably classiﬁed incorrectly, e. g., Regulations is tagged as a proper noun, and F1
as an adjective. Notice also how the tagger interprets capitalization of a word as evidence for a
classiﬁcation as proper noun, even if it knows the word in its non-capitalized form.
Rank
Word form
POS tag
1
website
NN
2
email
NN
3
blog
NN
4
url
NN
5
Boro
NP
6
Devil
NP
7
Google
NP
8
and/
NN
9
Blog
NP
10
Blogger
NP
11
Library
NP
12
websites
NNS
13
Programme
NP
14
Support
NP
15
broadband
NN
16
Windows
NP
17
Scheme
NP
18
healthcare
NN
19
carers
NNS
20
download
NN
Rank
Word form
POS tag
21
Ofcom
NP
22
HMRC
NP
23
download
VB
24
dont
VBP
25
blogs
NNS
26
Tribunal
NP
27
e-mail
NP
28
Practice
NP
29
days.
NN
30
Online
NP
31
MPA
NP
32
Regulations
NP
33
dont
NN
34
Award
NP
35
Twitter
NP
36
eg
NN
37
iPhone
NN
38
apps
NNS
39
3D
JJ
40
Linux
NP
Rank
Word form
POS tag
41
Advice
NP
42
Equality
NP
43
Panel
NP
44
’I
NNS
45
Schedule
NP
46
Facebook
NP
47
Awards
NP
48
Higher
NP
49
emails
NNS
50
Lottery
NP
51
Primary
NP
52
F1
JJ
53
Syndrome
NP
54
NOT
NP
55
Teaching
NP
56
’e
JJ
57
Ombudsman
NP
58
email
VB
59
thats
NNS
60
Directory
NP
Figure 4.7: Top 60 word forms of UKCOW2011 that are not in TreeTagger’s lexicon (standard
English parameter ﬁle, using the Penn Treebank tagset).
So, while the TreeTagger lexicon lacked quite a number of words, the tagger was able to
make correct guesses for a considerable number of unknown word forms. It is instructive to look at
the sources of unknown words to get a better overview of which should count as noise and which
should count as data, especially in documents which contain a high amount of informal language.
We identiﬁed those documents in the DECOW2012 corpus (most of them forum discussions
and blog entries), using a simple heuristic as suggested in Schäfer and Sayatz (submitted).⁷ For the
resulting sub-corpus (N  1:9 billion tokens), it is expected that a standard POS tagger delivers
particularly low accuracy. e texts were processed with TreeTagger, using the German standard
parameter ﬁle and no additional lexicon. Words unknown to the tagger were extracted along
with the POS tag which the tagger had assigned to them. For the 1,000 most frequent word-
POS combinations, the tagger reached only 26:3% overall accuracy. e cases in which the tagger
⁷I. e., we looked for occurrences of the short forms of the German indeﬁnite article, such as nem, ne, ner, which are highly
indicative of informal writing.

4.5. POS TAGGING AND LEMMATIZATION OF WEB TEXTS
77
did not assign the correct POS tag were manually classiﬁed, the results of which are shown in
Figure 4.8.
Class
%
non-standard orthography
32:3
lexicon gaps
19:8
foreign language material
18:9
emoticons
13:7
named entities
5:4
tokenization errors
3:1
other
6:8
Figure 4.8: Classiﬁcation of errors in POS tagging for a sub-corpus of informal language from the
German DECOW2012.
e main source of unknown word forms (in this portion of the corpus) is non-standard
orthography, and we do not expect such forms to be part of the lexicon of a standard POS tagger.
e second most important category is words that are not in the tagger’s lexicon although they
are spelled correctly. About 50% of them are domain terms, and another 40% are abbreviations.
Foreign-language material also has its fair share in causing tagging errors.⁸ Emoticons constitute
another and relatively important class. Named entities, tokenization errors, and all other sources
taken together make up for the rest of unknown words. Taking a closer look at the cases of non-
standard orthography, we see that it comprises a number of quite diﬀerent cases, as shown in
Figure 4.9, examples of which are given in Figure 4.10.
Class
%
genre-speciﬁc spellings
59:2
omitted whitespace
13:4
variants
19:7
ordinary typos
7:56
Figure 4.9: Breakdown of non-standard orthography in informal language of DECOW2012.
Genre-speciﬁc spellings are intentional deviations from standard orthography. In typical
cases, informal spoken language is imitated, including contractions of ﬁnite verbs, prepositions or
conjunctions with a following pronoun or article, dropped characters, and orthographical render-
ings of dialectal pronunciations. Most cases of omitted whitespace are probably also intentional
genre-speciﬁc spellings, but some could also be plain typos. Variants are spellings where the non-
ASCII characters ä, ö, ü, ß are replaced with the ASCII-compatible alternative strings ae, oe, ue,
⁸Note that the distinction between domain terms and foreign-language material is not always straightforward, as German and
many other languages borrow much of their specialized vocabulary from English.

78
4. LINGUISTIC PROCESSING
ss, respectively, as well as words that diﬀer from their standard spelling only in case, e. g., nouns
starting with a lower case letter, or adverbs starting with an upper case letter. Strictly speaking,
not all of these are even cases of non-standard orthography. e tagger sometimes just failed to
recognize sentence-initial occurences of words which are usually written in lower case sentence-
internally. Finally, there are also unintentional orthographic errors, including purely typographic
errors and cognitive errors (due to ignorance of the standard orthography; see Kukich, 1992).
non-standard
standard
translation
G-
ﬁnds
ﬁnde es
‘ﬁnd it’
weils
weil es
‘because it’
aufn
auf den
‘on the’
gesehn
gesehen
‘seen’
geb
gebe
‘(I) give’
ned
nicht
‘not’
bissl
bisschen
‘some’
M
schonmal
schon mal
‘ever’
Vorallem
Vor allem
‘predominantly’
non-standard
standard
translation
V
fuer
für
‘for’
koennte
könnte
‘could’
leute
Leute
‘people’
Hatte
(sent. init.)
‘had’
Vielen
(sent. init.)
‘much/many’
O
nciht
nicht
‘not’
cih
ich
‘I’
ziehmlich
ziemlich
‘quite’
vieleicht
vielleicht
‘maybe’
Figure 4.10: Non-standard spellings (examples from DECOW2012).
Now, the point of going this far into the details of incorrectly tagged unknown words is
that we need to ﬁnd ways of reducing these errors. It turns out that a number of simple measures
are suitable to reduce the impact of some sources of error:
1. Extending the tagger’s lexicon with domain terms. A list of candidate terms can be gener-
ated from the web corpus by looking for words the tagger did not know. is means that a
previous step of POS tagging is necessary. Sort by descending frequency and annotate a few
thousand by hand. is will not solve the problem completely, but it is likely to substan-
tially enhance the tagger’s lexical coverage on web data. Methods have been proposed for
semi-automatically extending a lexicon with domain terms. For example, in the context of
automatic spelling correction, Bloem et al. [2012] report that their approach of automati-
cally adding unknown words that occur a certain number of times works well for individual,
closely delineated domains. It is unclear, however, if this method is applicable to very large
corpora covering a multitude of diﬀerent domains. Also, the gain in lexical coverage must
be balanced against the risk that a misspelled word accidentally coincides with another valid
word form if the lexicon is very large [Peterson, 1986], which in turn could lead to an overall
decrease in tagging accuracy.
2. Extending the lexicon with genre-speciﬁc spellings. Adding non-standard orthography to
the tagger’s lexicon is perfectly justiﬁable in building web corpora. e procedure would
be the same as in 1, but this step might involve modifying the tagset as well. For example,
neither the Penn tagset for English [Marcus et al., 1993] nor the STTS-tagset for German

4.6. ORTHOGRAPHIC NORMALIZATION
79
[Schiller et al., 1999] provide compound tags for contracted forms such as I’m, wanna,
gonna, or German kannste (standard: kannst du ‘can you’) etc. Note that some taggers will
not allow you to use POS tags that were not included in their training data.
3. Extending the lexicon with emoticons. is is an obvious step, too, but care must be taken
not to split emoticons when tokenizing the text (see Section 4.4.2). However, people might
also use emoticons to stand in for words of diverse parts of speech, such that a single special
tag for emoticons might also be inadequate for some cases.
4. Simplifying the tagset. Giesbrecht and Evert [2009] show how a simpler tagset reduces the
error rate when tagging web data.
5. Retraining the tagger on web data. is is probably the most time consuming solution, as
many POS taggers require annotated training data.
Unknown words have a major impact not only on the performance of part-of-speech tag-
gers, but also on the accuracy of lexicon-based lemmatizers. Since the vocabulary of a web corpus
is usually very large and tends to include all kinds of rare domain words, non-words, and words
written with non-standard spelling, lemmatization of web corpora is a challenge similar to part-
of-speech tagging. While the measures listed above are meant to increase the accuracy of POS
tagging, at least some of them seem also suitable to improve the results of lemmatization. In
addition to these steps, it is worthwhile to consider orthographic normalization as an additional
pre-processing measure that will likely have a positive eﬀect on the outcome of both lemmatiza-
tion and part-of-speech tagging.
4.6
ORTHOGRAPHIC NORMALIZATION
Dealing with true orthographic errors is a delicate issue, both in terms of technical implemen-
tation and in terms of corpus design. It is certainly not an option to include all (or a substantial
number of) typos in the tagger’s lexicon, along with the POS tag and lemma of the intended
word. As illustrated in Figure 4.11 with the German verb übernimmt (‘takes over’), a large web
corpus easily contains dozens of diﬀerent misspellings of the same word, and many of them are
not hapax legomena.
A more promising approach is orthographical normalization of non-standard spellings be-
fore POS tagging. Automatic spelling error detection and correction has been studied since the
1960s. Kukich [1992] oﬀers a survey on the development of the discipline from its beginnings
to the 1990s. More recent approaches include Ahmad and Kondrak, 2005; Cucerzan and Brill,
2004; Li et al., 2006. While simple detection of orthographic errors resulting in non-words is a
relatively straightforward task (which can be performed, e. g., on the basis of character n-grams
or by lexicon look-up) and can be done by just considering the misspelled word in isolation, other
types of errors (primarily those which result in another correct word of the same language) are
harder to detect and usually require looking at some context and linguistic features.

80
4. LINGUISTIC PROCESSING
ubernimmt
übernimmt (297440)
81
1
überninmmt
6
1
übernimmnt
1
1
überniemt
6
1
ӧbernimmt
6
1
übernimnmt
5
1
übernimmet
11
1
überniehmt
2
2
überniemt
6
1
übernihmt
17
1
überniiiiiiiimmmmmmmt
1
12
überniommt
4
1
übernimrtnt
4
2
überniummt
2
1
überninntt
übernimmtt
2
2
...
2
1
Misspelled form
Correct form (frequency)
N
Edit distance
Figure4.11: Some spelling variants of German übernimmt (‘takes over’) from DECOW2012. Almost
all misspellings are within one deletion/insertion/substitution of distance from the correct spelling.
Going beyond mere detection of misspelled words and trying to replace them by correct
forms makes the task harder as well. Usually, spellcheckers suggest a list of possible substitutions
from a lexicon and rank them based on similarity with the misspelled word. One popular simi-
larity measure is the edit distance between the misspelled form and the candidate form, which is
deﬁned as the minimal number of character insertions, deletions, substitutions, and transpositions
necessary to transform one string into the other [Damerau, 1964; Levenshtein, 1966]. Empirical
studies on spelling errors suggest that the majority of misspelled words is no further than one edit
distance from the intended word. Reported ratios are in the range of approx. 70–95%, but these
ﬁgures arguably depend to a large extent on the type of application (see Kukich, 1992). Another
popular technique is to map each word to a similarity key such that phonetically similar words
receive identical or similar keys (Odell and Russell, 1918; Philips, 1990; Postel, 1969, and others)
and then measure the edit distance on the keys.
Still more demanding is the task of non-interactive, automatic spelling correction, where,
in addition to generating a list of substitution candidates, exactly one of them must be selected
to replace the misspelled word in the text. Edit distance alone is of little help here because often
more than one substitution candidate has the minimal edit distance to the misspelled word. As an
example, consider the misspelled word tru in an English text. Correction candidates (as returned
by GNU Aspell) include true and try, but both are at the same edit distance (D 1) from tru. At-
tempts have been made at making a decision on the basis of other features that can be computed by

4.6. ORTHOGRAPHIC NORMALIZATION
81
considering the misspelled word in isolation, such as weighting of the diﬀerent edit operations, or
weighting according to the individual characters involved in an edit operation, giving less weight
to more typical/more frequent spelling errors. A number of approaches take into account some
of the context surrounding the misspelled word, e. g., in the form of word n-grams or POS tag
n-grams, in order to select a substitution word from among several candidates. Moreover, context
information on the level of the document, the entire corpus and other corpora (both general and
domain speciﬁc) can be exploited for assessing the appropriateness of a substitution candidate, for
example, by ranking up candidates that are likely to occur in texts of a speciﬁc domain (see e. g.,
Wong et al., 2007, for a recent proposal to combine information from various sources in selecting
a candidate). Finally, normalizing noisy texts may also involve such things as expanding abbrevi-
ations, restoring improper casing and correcting tokenization errors, and a number of approaches
and tools have been proposed to tackle this task (see e. g., Clark, 2003; Clark and Araki, 2011;
Sproat et al., 2001; Wong et al., 2007).
e upshot of this is that automatic spelling correction uses heuristics and does not nec-
essarily achieve 100% accuracy, and this is an important aspect when we want to treat texts that
are going to be used in linguistic research. Perhaps most importantly, care must be taken not
to normalize rare words that are unlikely to be listed in a spellchecker’s dictionary (cf. the high
count of rare but regular words among the hapax legomena in Figure 4.2). Discussing normaliza-
tion methods in detail is beyond the scope of this introduction, but applying them in web corpus
construction and evaluating the results is clearly a topic of future research.
From a linguistic perspective, it is desirable that text normalization is done in a non-
destructive way, wherever this is possible, that is, leaving the original text intact. e advantage is
that users of the ﬁnal corpus do not have to rely on the imperfect results of, for example, automatic
spelling correction. Instead, users are free to access (and base their research on) the original, non-
normalized data, which can be crucial in some circumstances. Non-destructive normalization can
be done by regarding the normalized forms of tokens as an additional annotation layer. Other au-
tomatic annotation tools (such as a POS tagger and a lemmatizer) can then be run on this layer to
produce yet other annotation layers (e. g., the part of speech information of the corrected tokens).
Figure 4.12 illustrates this with data from UKCOW2011. As a major disadvantage, this increases
(potentially doubles) the size of the ﬁnal corpus.
at said, the amount of time and eﬀort necessary for cleaning and normalizing a web
corpus must be balanced against the usefulness of such measures when it comes to working with
the ﬁnal corpus. Applying some cleaning and normalization is probably a good idea when building
a corpus that will be used by more than a handful of people and is intended for research on a greater
number of research questions, but the situation may be diﬀerent if the goal is constructing an ad
hoc corpus for a closely delimited task where exact type/token counts or accurate POS tagging are
not essential. us, the decision about what degree of normalization is appropriate is an individual
one.

82
4. LINGUISTIC PROCESSING
Word
POS
Lemma
Corr.Word
Corr.POS
Corr.Lemma
…
…
the
DT
the
the
DT
the
players
NNS
player
players
NNS
player
play
VBP
play
play
VBP
play
.
SENT
.
.
SENT
.
e
DT
the
e
DT
the
FA
NP
FA
FA
NP
FA
does
VBZ
do
does
VBZ
do
abosolutley
JJ
<unknown>
absolutely
RB
absolutely
nothing
NN
nothing
nothing
NN
nothing
to
TO
to
to
TO
to
help
VB
help
help
VB
help
Clubs
NNS
club
Clubs
NNS
club
,
,
,
,
,
,
…
…
Figure 4.12: Non-destructive text normalization using annotation layers (vertical text, as suitable for
processing with corpus query engines).
4.7
SOFTWARE FOR LINGUISTIC POST-PROCESSING
ere are a lot of tools for linguistic processing around. Given the nature of the data and the size
of giga-token web corpora (in the range of tens of gigabytes of data), a few considerations should
be kept in mind when choosing a linguistic post-processing tool. e evaluation of these tools
is based mostly on our own experience with them. is is not intended as an exhaustive list of
appropriate software packages, which would be beyond the scope of this book.
1. Availability: Whether a package is free and/or open source will probably play a role in many
academic contexts. Likewise, individuals might not be in a position to pay a substantial
amount of money for NLP software.
2. Resources: A number of tools come suitably conﬁgured and will work (more or less) out-of-
the-box for the kind of data we are concerned with. For example, some tools include larger
numbers of language models, while others may require users to train such models on their
own data.
3. Performance: Processing time and accuracy have to be balanced against each other. In the
context of web corpus construction, the usually huge amounts of data make it necessary to
consider this issue carefully as there are huge diﬀerences in time performance between, for
example, diﬀerent taggers. is may be less of an issue when a large and high quality corpus
is built that will be archived and re-used for a relatively long period of time by many people.
In such a case, waiting several weeks for a tagger to ﬁnish is an acceptable investment. On
the other hand, if the goal is to quickly create an ad hoc web corpus for a special, restricted
purpose, any post-processing task that takes more than a few days may be unacceptable.

4.7. SOFTWARE FOR LINGUISTIC POST-PROCESSING
83
One way to deal with this problem is parallelization of individual tasks, which usually re-
duces processing time by a substantial factor. A very simple solution that does not require
any complicated setups and conﬁgurations is GNU Parallel, which takes care of splitting
the input ﬁle into chunks, runs independent processes (e. g., taggers) on them, and joins
the output from these processes in the correct order.⁹ More advanced options for parallel
processing include frameworks like Apache Hadoop allowing Map-Reduce operations on
many machines.¹⁰ However, the eﬀort and time of setting up such a solution (let alone the
cost of buying a cluster of machines) do not always stand in an optimal relationship with
the gain in processing time, at least not when the goal is creating only a single web corpus.
at said, some tools are:
• TreeTagger [Schmid, 1995] is a very fast and very popular HMM tagger that also performs
lemmatization.¹¹ Models are available for quite a number of languages, but (as of mid-2013)
none of them are trained on web texts. It was used to process the WaCky corpora and some
of the COW corpora.
• RFTagger is a POS tagger and morphological analyzer designed for ﬁne-grained part-of-
speech tagsets.¹² It ships with models for German, Czech, Slovene, and Hungarian.
• FreeLing [Padró and Stanilovsky, 2012] is a suite of NLP tools with dictionaries and mod-
els for a number of Romance languages plus English, Russian, and Welsh.¹³ It is not as fast
as TreeTagger, but processing speed is still acceptable even for very large corpora.
• e Stanford Tagger [Toutanova et al., 2003] is a maximum entropy tagger for which
reported accuracy on standard written language is quite high.¹⁴ In an experiment on tagging
web data [Giesbrecht and Evert, 2009], it was outperformed by TreeTagger when trained
on standard written texts, both in accuracy and in computing time. In addition, depending
on the language model, RAM requirements can easily exceed 8G. Interestingly, it ships
with a model trained on the deWaC web corpus, but to our knowledge, performance on
web data has not yet been re-evaluated using this model.
• Frog [van den Bosch et al., 2007] is a memory based (k-nearest neighbor) tagger/
morphological analyzer with high reported accuracy.¹⁵ It also includes a chunker, a de-
pendency parser, and a named entity recognizer. In our experiments, it does not compare
favorably with, e. g., TreeTagger in terms of eﬃciency, even when used only for POS tag-
ging.
⁹http://www.gnu.org/software/parallel/
¹⁰http://hadoop.apache.org/
¹¹http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/
¹²http://www.cis.uni-muenchen.de/~schmid/tools/RFTagger/
¹³http://nlp.lsi.upc.edu/freeling/
¹⁴http://www-nlp.stanford.edu/software/tagger.shtml
¹⁵http://ilk.uvt.nl/frog/

84
4. LINGUISTIC PROCESSING
• Ucto [van Gompel et al., 2012] tokenizes Unicode text using hand-written rules (ordered
regular expressions, supporting unicode character classes).¹⁶ It is included as part of the
Frog processing chain, but can also be used as a stand-alone application. Ucto ships with a
set of conﬁguration ﬁles for various languages and is easily adaptable to suit speciﬁc needs.
• splitta [Gillick, 2009] oﬀers statistical sentence boundary detection using a Naive Bayes
classiﬁer or, alternatively, a Support Vector Machine.¹⁷ As of 2013, the distribution provides
models for English trained on standard written language (Wall Street Journal corpus and
Brown corpus). Training routines are included to build models for languages other than
English.
• NLTK [Bird et al., 2009] is a collection of Python modules, covering many diﬀerent aspects
of NLP such as tokenization, POS tagging, parsing, named entity recognition, etc.¹⁸
• Apache OpenNLP is a software suite covering several tasks in natural language process-
ing, including tokenization, sentence boundary detection, POS tagging, chunking, parsing,
named entity recognition, and document classiﬁcation.¹⁹
SUMMARY
In this chapter, we have discussed and illustrated the primary steps of the linguistic post-
processing and normalization of web data (tokenization, sentence boundary detection, POS tag-
ging, lemmatization). It should have become clear that web documents contain various types of
noise which make these steps diﬃcult and lead to an accuracy which is measurably lower than for
traditionally compiled corpora. e result is, for example, that certain counts derived from the
corpus have to be treated with care (like type and token counts). Since there is no general recipe
for cleaning web corpora, we illustrated some of the steps which can be taken to improve the
overall quality of the linguistic post-processing. However, the main conceptual problem concerns
the distinction between noise and data. Each normalization alters the data and might introduce
new or even more noise, depending on the deﬁnition of noise. All this makes some form of quality
control necessary. In the next chapter, we sketch some ways of how the quality and composition
of web corpora can be assessed.
¹⁶http://ilk.uvt.nl/ucto/
¹⁷http://code.google.com/p/splitta/
¹⁸http://nltk.org/
¹⁹http://opennlp.apache.org/

85
C H A P T E R
5
Corpus Evaluation and
Comparison
5.1
INTRODUCTION
is chapter deals with ways to evaluate the quality of a web corpus. In Section 5.2, we show how
to assess the overall technical quality of the corpus, suggesting a number of procedures appro-
priate to reveal potential shortcomings in crawling and post-processing (most notably boilerplate
removal and duplicate detection), but also ﬂaws in the general cleanup and linguistic post pro-
cessing. However, the fact that a web corpus is acceptable from a technical point of view does not
tell us much about the characteristics of the language and documents contained in it, and how it
compares to other corpora in this respect. For example, one might be interested in the similarity
of the web corpus to corpora that have known characteristics and which are established, widely
used resources in the Corpus Linguistics community (sometimes called reference corpora). To this
end, Section 5.3 takes a quantitative perspective on corpus comparison and introduces various
measures of corpus similarity. Section 5.4 is dedicated to the comparison of keywords extracted
from corpora in a variety of ways, thus concentrating on the topics/domains covered by the cor-
pora. Another way of evaluating a web corpus, sometimes called extrinsic evaluation, is to look
at it it from the perspective of speciﬁc linguistic uses of the corpus, possibly in comparison to
traditional corpora. A short overview of such approaches will be given in Section 5.5. Finally,
Section 5.6 brieﬂy addresses corpus composition (in terms of text types, genres, authorship, etc.)
and the questions of representativeness and balance.
Having read through this chapter, readers should:
1. know a few measures that can be used to assess the technical quality of their corpus (success
of cleansing, deduplication etc.),
2. be familiar with a variety of diﬀerent ways in which corpora can be evaluated, including
measuring similarity between corpora, content-oriented comparison with extracted key-
words, task-speciﬁc evaluation, and assessment of corpus composition,
3. be aware of the limitations of their corpus in terms of balance and representativeness.
5.2
ROUGH QUALITY CHECK
As a ﬁrst step in evaluating a new web corpus, it is advisable to inspect it for obvious ﬂaws in the
processing chain. Such shortcomings include (massive) failure in removing pieces of boilerplate,

86
5. CORPUS EVALUATION AND COMPARISON
duplicate documents (or parts of them) that have gone undetected, and systematic ﬂaws in tok-
enization. A technique that allows for quick assessment of these aspects is computing a number of
statistics from the corpus data and checking whether the distribution of the values is plausible. If
it is not, a detailed inspection is necessary. In case the detected anomalies point to serious short-
comings in post-processing, it might even be necessary to redo the processing step which caused
the problem. Depending on the software used in processing the raw data, a few changes in the
parameter settings might be suﬃcient to ﬁx the problem. More often than not, there are issues
particular to a given corpus, calling for an iterative approach in constructing the ﬁnal corpus. In
this section, we will illustrate a rough quality check by looking at the distribution of word and
sentence lengths as well as duplicated sentences.
5.2.1
WORD AND SENTENCE LENGTHS
A number of statistics can be computed in order to assess the technical quality of a web corpus.
For example, there are over 200 statistics available for some of the corpora in the Leipzig Corpora
Collection [Biemann et al., 2007; Goldhahn et al., 2012], many of which could be exploited for
the purpose of quality control.¹ We will concentrate here on the distribution of word length and
sentence length, because they are easy to compute and they have turned out to be particularly
helpful in detecting anomalies in the corpus [Eckart et al., 2012].
Research on the distribution of word length has a tradition dating back over a hundred
years. A good entry point to the subject is Grzybek [2007]. Mathematical modeling of empiri-
cal word length distributions depends on factors such as text type, unit of measurement (letters
vs. phonemes, syllables vs. morphemes), and even on individual authors. A variety of theoret-
ical distributions have been proposed for modeling word length, including the geometric, the
log-normal, the negative binomial, the Hyper-Pascal, and, very prominently, the Poisson dis-
tribution with several generalizations (including Conway-Maxwell-Poisson and Hyper-Poisson)
and variants (e. g., positive Poisson, Cohen Poisson). Several of these, such as the log-normal, the
negative binomial, the Hyper-Pascal and some of the Poisson family of distributions have also
been explored in modeling sentence length. As in the case of word length, the unit of measure-
ment (e. g., number of words vs. number of clauses) plays a crucial role in choosing an appropriate
theoretical distribution (see Best, 2001, 2005, for a compact overview).
Successful modeling of word and sentence length is usually based on texts written by a sin-
gle author (or even only parts of such texts), and the ﬁndings do not necessarily generalize to a
collection of wildly diﬀerent genres written by a multitude of authors, i. e., web corpora.² For this
reason, our rough quality check does not involve ﬁtting a model to the data and drawing conclu-
sions from the goodness-of-ﬁt. Rather, massive anomalies in the distribution can be detected by
visual inspection of the plotted data, and it is suﬃcient to assume as a rule of thumb that both
¹http://cls.informatik.uni-leipzig.de/
²Some authors even exclude this possibility altogether, cf. Grzybek [2007, 18]: “any combination of diﬀerent texts turns out
to be a ‘quasi-text’, destroying the internal rules of text self-regulation. e very same, of course, has to be said about corpus
analyses, since a corpus, from this point of view, is nothing but a quasi text.”

5.2. ROUGH QUALITY CHECK
87
word length and sentence length follow a positively skewed distribution, i. e., a distribution with
an early mode and an extended tail of low frequencies as word/sentence length increases.
Figure 5.1 shows the distribution of word lengths, measured in number of characters, in
a sample of the German DECOW2012 corpus (n D 14:8 million tokens). By what we can tell
from the plot, the overall shape is ﬁne. e most frequently occurring word length is 3. It is a
good idea to take a quick look at the frequency list of such words to make sure this is not an
artifact of post-processing. Figure 5.2 shows the frequency list of words of length 3 in the sample
from DECOW2012. Of the 20 most frequent items, 19 are regular German function words and
one is the ellipsis (…). ere is thus no reason to suspect that the high frequency of tokens of
length 3 is due to an artifact in the post-processing chain.
Turning to the tail of the distribution, however, there are a few words longer than 40, and
even longer than 1,000 characters. Taking a closer look at these (Figure 5.3 gives some examples),
it becomes evident that some of them are regular German words or are due to colloquial writing
style (e. g., phrasal pre-modiﬁcation of nouns), but others clearly point to shortcomings in both
non-linguistic and linguistic post processing. Yet, the percentage of words over 40 characters in
length is less than 0.005% in this particular sample, including long regular German words along-
side a number of non-words, so the problem seems to be rather negligible for most applications
of the corpus.
0
10
20
30
40
0.00
0.05
0.10
0.15
0.20
0.25
0
200
400
600
800
1000
1400
0.00
0.05
0.10
0.15
0.20
0.25
Word lengths up to 40 characters
All word lengths
Figure 5.1: Distribution of word lengths (measured in number of characters) in a sample (n D 14:8
million tokens) of the German DECOW2012 corpus.
Turning now to the distribution of sentence length in the sample, given in Figure 5.4, the
general shape is of the expected positively skewed type. However, sentences of length 1 are heav-

88
5. CORPUS EVALUATION AND COMPARISON
Rank
Freq
Type
Rank
Freq
Type
1
348450
die
11
93237
ein
2
344946
und
12
87815
für
3
323051
der
13
73666
dem
4
159149
das
14
64336
des
5
147918
ich
15
59375
als
6
146950
ist
16
57841
…
7
140616
den
17
57808
man
8
121540
mit
18
56971
Die
9
112725
von
19
55475
sie
10
95628
auf
20
54600
bei
Figure 5.2: Words of length 3: ﬁrst part of frequency list, generated from a sample of the German
DECOW2012 corpus (n D 14:8 million tokens).
Length
“Word”
Class
60
Mein-Kind-hat-Nix-Gemacht-Lehrer-Sind-An-Allem-Schuld-
Mütter
long noun modiﬁer
60
e3.jpg&width=800m&height=600m&bodyTag=%3CBODY-
%20style%3D%22m
partial URL
71
-HKEY_CURRENT_USER\Software\Microsoft\MediaPlayer-
\Player\RecentFileList
part of a system
path
79
SachverständigengutachtenSachverständigerSäuglingserstaus-
stattungSchadensersatz
post-processing
error (deleted
whitespace)
100
er-Dachtr%C3%A4gersystem%2FZ%C3%B6lzer-Dachtr%C3-
%A4ger+Zubeh%C3%B6r%2FSeitlicher+Hublift&seite=shop/
partial URL
266
< Ich habe es das erste mal gemalt und bin froh, dass es überhaupt
was geworden ist. Sowohl bei Renamon als auch bei Wargreymon
hatte ich die ganze Zeit Vorlagen daneben liegen. Bei WGmon hatte
ich auch Glück, eine Vorlage zu ﬁnden, wo es eine Ähnliche Pose
hatte >
tokenization error
(sequence between
angle brackets
treated as XML-
tag)
Figure 5.3: Tokens over 40 characters in length (from a sample of DECOW2012).
ily over-represented, which calls for further assessment. Figure 5.5 shows a number of diﬀerent
sentences of length 1 from the sample, sorted by frequency. Several cases can be distinguished
here:
1. Artifacts of post-processing: For this particular corpus, URLs and emails addresses in the
documents were replaced with the strings urlblank and emailblank, respectively. In a consid-
erable number of cases, these ended up as (pseudo-)sentences, partly because the annotation
scheme for this corpus does not allow material to be outside of a sentence region. Any ma-

5.2. ROUGH QUALITY CHECK
89
0
10
20
30
40
50
60
0.00
0.01
0.02
0.03
0.04
0.05
0
100
200
300
400
0.00
0.01
0.02
0.03
0.04
0.05
Sentence lengths up to 40 tokens
All sentence lengths
Figure 5.4: Distribution of sentence lengths (measured in number of tokens) in a sample (n D 14:8
million tokens, 1 million sentences) of the German DECOW2012 corpus.
terial occurring in between two recognized sentences was thus labeled as a sentence as well.
A similar explanation holds for NEUMARKT, the name of a town which precedes every
individual text on the page of a local newspaper (shown in Figure 5.6). Ideally, such items
would be recognized and labeled as headings or sub-headings, not sentences.
2. Incorrect tokenization: Broken phpBB emoticons (e. g., :mrgreen, :rolleyes) and the broken
abbreviation Evtl (for Eventuell, ‘perhaps’), which the tokenizer did not recognize as such,
probably as an eﬀect of capitalization.
3. Failure in boilerplate removal: mehr/Mehr (‘more’) certainly derives from the ‘read more…’
links in the original HTML documents, which ideally should have been recognized and
removed during post-processing. Figure 5.6 illustrates this. Similarly, top is a navigational
element that should not show up in the ﬁnal corpus, and object probably comes from pro-
gramming code snippets.
4. Regular one-word utterances: Danke (‘thank you’), Ja (‘yes’), Nein (‘no’), Warum (‘why’)
5. Emoticons: Tokens such as :D :( :p may or may not be viewed as equivalents to sentences,
depending on linguistic decisions. It is a question of corpus design that has to be answered
individually.
As for the tail of the distribution, there are strikingly long sentences (consisting of 200 or
more tokens). For reasons of space, we will not illustrate these with examples, but rather name a
few cases that are likely to turn up:

90
5. CORPUS EVALUATION AND COMPARISON
1. Extremely long normal sentences: is is more likely to happen if the characters relevant
for sentence boundary detection are restricted to [.?!] and do not include e. g., [;:].
2. Tokenization errors: If the tokenizer is conﬁgured to leave XML tags intact, then precau-
tions must be taken for occurrences of angled brackets that are not part of an XML tag.
Otherwise, several words (in the worst case, entire sentences) may end up as a single token
in the corpus.
3. Non-normalized punctuation (see Section 4.4.1): If omitted whitespace after an end-of-
sentence punctuation mark is not corrected for, and the tokenizer is not conﬁgured to handle
such cases, the sentence boundary will not be detected, resulting in two or more sentences
glued together.
4. Genre-speciﬁc style of writing: Some text types tolerate a relaxed use of punctuation marks.
E. g., postings to discussion forums often contain hardly any periods at all, even if they
comprise several sentences.
5. Lists and enumerations in the original HTML document: As text from such elements often
lacks end of sentence markers, all items of the list are likely to be treated as forming a single
sentence.
In sum, then, the distribution of word and sentence length in this example reveals several
weak points in the post-processing chain. At this point, if these shortcomings are judged as severe,
adjustments can be made to the respective post-processing tools, and one or more of them can
be re-applied to the data. It should be kept in mind, though, that some of the ﬂaws illustrated
above arise from non-trivial problems, for example classifying strings as a heading (for example,
NEUMARKT) when they lack proper markup in the original HTML document, or detecting
sentences with missing end-of-sentence punctuation. It is almost certain that large web corpora
can never reach a degree of cleanliness comparable to traditional corpora. More often than not,
re-doing post-processing steps at this stage will involve balancing time and eﬀort against the
potential gain of having ﬁxed certain anomalies, and it may often be the case that knowing the
characteristics and limitations of large web corpora is suﬃcient to avoid unwarranted conclusions
in doing linguistic work with these corpora.
5.2.2
DUPLICATION
Imperfect (near-)duplicate removal on the document level might still leave duplicate sentences or
paragraphs in the ﬁnal corpus. Massive duplication of larger n-grams can be indicative of short-
comings in (near-)duplicate detection and failure in boilerplate removal. A ﬁrst step in detecting
such anomalies is searching for massively duplicated sentences. Figure 5.7 illustrates this with
the counts for some repeated sentences in a sample (1 million sentences) of the German DE-
COW2012 corpus. Among the repeated sentences, there are a number of short utterances like
Danke (‘ank you’) that could plausibly have been generated by diﬀerent writers on diﬀerent oc-
casions, and this type of duplication is expected for very short sentences. ey are marked ‘R’ in
the table. On the other hand, the sentences marked ‘B’ are clearly cases of boilerplate that should

5.2. ROUGH QUALITY CHECK
91
Freq.
Type
Freq.
Type
4554
urlblank
175
Nein
1367
:D
168
NEUMARKT
561
mehr
155
Ja
316
:rolleyes
146
Warum
272
:confused
146
top
242
ˆ
139
:p
237
:(
137
:eek
220
emailblank
130
:cool
203
:-
113
Evtl
200
:lol
110
:mad
195
Danke
108
:mrgreen
189
Mehr
99
object
188
:wink
85
S.
Figure 5.5: Sentences of length 1: upper part of the frequency list for a sample of 1 million sentences
from DECOW2012.
Figure 5.6: Where duplicated 1-token sentences (NEUMARKT; Mehr) originate: screenshot from
a local German news website, http://www.neumarktonline.de.
have been removed in post processing, most of them originating from forum and blog software.
One case in this sample (marked ‘T’) is the title of a show for which several online shops were
selling tickets at the time of the crawl. It is thus not a prototypical instance of boilerplate, but
it is not a prototypical instance of human-generated text either, which illustrates again the fact
that distinguishing between ‘good text’ and ‘bad text’ is a delicate matter even conceptually. In
any event, the clear cases of boilerplate that are detectable by looking at duplicated sentences may
provide useful information when it comes to ﬁne-tuning the post-processing tool chain. Since du-
plicated material does not necessarily surface in the form of sentences, it may also be worthwhile
to inspect duplication of (reasonably large) n-grams.

92
5. CORPUS EVALUATION AND COMPARISON
Freq.
Sentence
Category
195
Danke
R
189
Mehr
B
175
Nein
R
160
Powered by vBulletin® Version 4.1.7 Copyright © 2011 Adduco Digital e.K. und vBul-
letin Solutions Inc. Alle Rechte vorbehalten
B
155
Ja
R
146
Warum
R
127
Um die detaillierte Vollansicht mit Formatierung und Bildern zu betrachten bitte hier
klicken
B
94
Dieses ist eine vereinfachte Darstellung unseres Foreninhaltes
B
94
Der Preis ist einmalig und muss bei Verlängerung des Marktes nicht nochmals beglichen
werden
B
83
Hallo
R
75
Quelle urlblank
B
64
Vielen Dank
R
62
Zünd an es kommt der Weihnachtsmann
T
59
Alle Rechte vorbehalten
B
58
Mit freundlichen Grüßen
R
58
Du kannst auf Beiträge in diesem Forum nicht antworten
B
54
Du kannst deine Beiträge in diesem Forum nicht löschen
B
53
Richtig
R
52
Schade
R
52
Es ist Ihnen nicht erlaubt Anhänge hochzuladen
B
51
Stimmt
R
50
Du darfst deine Beiträge in diesem Forum nicht ändern
B
Figure 5.7: Duplicated sentences in a sample of 1 million sentences of the German DECOW2012
web corpus. e ‘B’ cases are instances of boilerplate that were not properly removed; ‘R’ cases are
regular, short German utterances; the status of the ‘T’ case is unclear (it is the title of a show announced
on many ticket-selling websites).
5.3
MEASURING CORPUS SIMILARITY
One aspect of corpus evaluation is certainly the question of how the web corpus compares to other
corpora. For example, a web corpus can be compared to other web corpora that went through
a similar (or diﬀerent) post-processing chain. Another interesting question to ask is how the
web corpus compares with traditional, widely used corpora that have known characteristics. For
example, is the web corpus larger, but otherwise similar to traditional corpora, or does it have
its own properties that distinguish it from such corpora? Of course, at this point, the question
arises of what it means for two corpora to be similar to each other, and there is not a simple single
answer to this question.
For instance, similarity can be assessed in terms of corpus composition (text types, topics,
authorship, etc.), an issue that will be brieﬂy addressed in Section 5.6. Another way of examining
the similarity of two corpora consists in computing some overall similarity measure by comparing

5.3. MEASURING CORPUS SIMILARITY
93
the distribution of linguistic entities contained in the corpora, for example, the distribution of
lexical items. e comparison may also be based on higher-level linguistic constructs (such as
part-of-speech categories). In this case, however, the result will depend not only on properties of
the corpora, but also on the accuracy of the linguistic processing tool(s), which may perform with
diﬀerent accuracy on diﬀerent corpora and may thus introduce biases. A similar point is also made
in Kilgarriﬀ[2001], which we recommend as an entry point to corpus comparison in terms of
similarity measures, and much of what follows in this section draws on it. Yet another method of
comparing corpora is extracting keywords, either from individual documents or from the entire
collection, and then use these in describing the corpora from a content-related point of view.
5.3.1
INSPECTING FREQUENCY LISTS
A simplistic approach to comparing corpora is to inspect frequency lists derived from them. For
example, the table in Figure 5.8 lists the 20 most frequent common nouns in the French FR-
COW2011 corpus and in the WaCKy initiative’s frWaC corpus. e overlap seems to be sub-
stantial, even though the data for the two corpora were crawled several years apart and using
diﬀerent seed URL sets (thousands of seed URLs in the case of frWaC, and only ten in the case
of FRCOW2011). Still, this kind of comparison is of course impressionistic and does not provide
any real measure of the diﬀerence, nor does it answer the question of whether the two corpora
are signiﬁcantly similar to (or diﬀerent from) each other. Next, we will therefore address corpus
comparison from a hypothesis-testing perspective.
Rank
FRCOW 2011 (few seed URLs)
FRWAC (many seed URLs)
1
année ‘year’
site
2
travail ‘work’
an
3
temps ‘time’
travail
4
an ‘year’
jour
5
jour ‘day’
année
6
pays ‘country’
service
7
monde ‘world’
temps
8
vie ‘life’
article
9
personne ‘person’
personne
10
homme ‘man’
projet
11
service
information
12
cas ‘case’
entreprise ‘company’
13
droit ‘right’
recherche ‘(re-)search’
14
eﬀet ‘eﬀect’
vie
15
projet ‘project’
droit
16
question
page
17
enfant ‘child’
formation (‘education’)
18
fois ‘time (occasion)’
commentaire ‘comment’
19
place
cas
20
site
fois
Figure 5.8: e 20 most frequent common nouns in FRCOW2011 and frWaC.

94
5. CORPUS EVALUATION AND COMPARISON
5.3.2
HYPOTHESIS TESTING WITH 2
At ﬁrst sight, it seems plausible that the 2 test for homogeneity could be used to test whether
the two corpora are samples from the same population (or at least, from populations with the
same distribution). In our context, the test compares the frequencies of a word type in the two
corpora in the following way:
Corpus 1
Corpus 2
word X
freq(X)
freq(X)
: word X
freq(:X)
freq(:X)
For the purpose of comparing two corpora with respect to the frequency of a word, the table
has 2 columns and 2 rows, and the 2 statistic can be calculated using the formula in 5.1, where
i and j are column and row numbers, respectively. Oij is the observed value of a cell, and Eij
is the expected value of that cell given the marginal sums. e test statistic approximates a 2
distribution with one degree of freedom. If the expected value in a cell is less than 5, then Yate’s
continuity correction should be applied. However, given the high token counts in the upper part
of a sorted frequency list, this will not be an issue in the following example, where the expected
value of a cell Eij is the corresponding row total multiplied by the corresponding column total,
divided by the sum of all observations.
2 D
2
X
i;j D1
.Oij   Eij /2
Eij
(5.1)
e null hypothesis is that a word has the same distribution in both corpora. If it cannot
be rejected, then any diﬀerence in the frequency of a word would be assumed to be due to chance
(sampling error), and there would be no reason to assume that the two corpora are samples from
diﬀerent populations. e alternative hypothesis states that there is a diﬀerence which chance
alone is unlikely to have caused. If we assume the alternative hypothesis, we conclude that the
word is distributed diﬀerently in the two corpora and that the two corpora are not random samples
from the same population. Figure 5.9 illustrates this with data from two Spanish web corpora.
A 2 statistic is calculated for each word (the example shows the 14 most frequent items, which
happen to be the same in the two corpora). e p-values of the individual tests indicate that in
ﬁve cases (de, que, y, los, se), the probability that the diﬀerence in frequency has arisen by chance
alone is below 0.01.
In addition, a 2 test could be computed for the 14  2-table in 5.9 as a whole. In this
example, the result would be 2 D 76:87, p < .001 on 13 degrees of freedom, and this could be
interpreted as showing that Corpus 1 and Corpus 2 diﬀer substantially from one another.
Now, contrary to what these statistically signiﬁcant diﬀerences suggest, Corpus 1 and
Corpus 2 are in fact random samples from the same population (the same larger corpus ES-
COW2012), thus the 2 test would seem to be too sensitive here. One reason is that the ran-
domness assumption is generally violated when it comes to the distribution of words in texts

5.3. MEASURING CORPUS SIMILARITY
95
Frequency
2
p
Word
C
C
de
6781719
6802262
32.99
<.001***
,
5627749
5633555
3.12
.077
la
3613946
3614049
0.001
.975
.
3574395
3579032
3.08
.079
que
2963992
2956662
9.36
<.010**
y
2642241
2653365
23.88
<.001***
en
2562028
2564809
1.53
.217
el
2450353
2446328
3.40
.065
a
1885112
1882813
1.44
.230
los
1597103
1603537
13.09
<.001***
del
1173860
1172623
0.67
.415
se
1139311
1143202
6.68
<.010**
las
1054729
1054924
0.02
.896
un
1001556
1000106
1.07
.302
Figure 5.9: 2 statistics calculated for the 14 most frequent word types in two Spanish web corpora
(110 million tokens each).
(see e. g., Baayen, 2001). eir probability of occurrence has been shown to depend on variables
such as genre, author, topic (cf. Church and Gale, 1995, for variation across texts) as well as on
discourse-level cohesion (cf. Baayen, 2001, for variation within texts), in addition to structural
constraints imposed by the grammar. As Kilgarriﬀ[2001] points out, such diﬀerences in word
frequency distributions do not generally average out, even when thousands of texts are joined to
form a corpus.
e second reason hypothesis testing with 2 tests is not ideally suited for corpus compar-
ison is that, other things being equal, the 2 statistic grows as a function of sample size. With
huge sample sizes, even minor diﬀerences in the distribution of a word will provide enough evi-
dence to reject the null hypothesis (see discussion in Kilgarriﬀ, 2001). is second point can be
illustrated by doubling the size of the two corpora, as shown in Figure 5.10. Again, Corpus 3
and Corpus 4 are random samples from the same population, but this time, the diﬀerence in
frequency is signiﬁcant for 9 out of 14 words.
5.3.3
HYPOTHESIS TESTING WITH SPEARMAN’S RANK CORRELATION
Another way to assess the similarity of two corpora which perhaps suggests itself is computing
a Spearman correlation for (a number of lexical items from) ranked frequency lists of the two
corpora. e null hypothesis in this case is that there is no (or only chance) correlation between
the ranked frequency lists of Corpus 1 and Corpus 2. e alternative hypothesis states that the
frequency ranks of Corpus 1 and Corpus 2 are correlated, and this can be interpreted as evidence
that Corpus 1 and Corpus 2 bear a statistically signiﬁcant similarity to each other. If there are
no (or only a few) ties in the ranks, Spearman’s correlation coeﬃcient can be computed using the

96
5. CORPUS EVALUATION AND COMPARISON
Form
Corpus 3
Corpus 4
2
p
de
13579307
13563617
9.65
<.010**
,
11264561
11274225
4.38
.036*
la
7227386
7236002
5.31
.021*
.
7157262
7150075
3.72
.054
que
5924861
5932089
4.53
.033*
y
5303604
5292011
12.98
<.001***
en
5117964
5123526
3.10
.078
el
4885947
4900224
21.31
<.001***
a
3766747
3773854
6.82
<.010**
los
3203514
3193313
16.49
<.001***
del
2340110
2338707
0.42
.515
se
2277149
2284887
13.27
<.001***
las
2105694
2109117
2.81
.094
un
1998736
2002337
3.27
.070
Figure 5.10: Doubling corpus size: 2 statistics calculated for the 14 most frequent word types in two
Spanish web corpora (220 million tokens each).
simpliﬁed formula given in 5.2, where D is the diﬀerence in rank for a given item in Corpus 1
and Corpus 2, and n is the number of ranked pairs considered.
rs D 1   6 P D2
n.n2   1/
(5.2)
Figure 5.11 shows the result of calculating a Spearman correlation for the same two corpora
that were used to illustrate the 2 test above. Since the frequency ranks for all 14 words considered
here are the same in both corpora, the result is a perfect correlation (rs D 1, p < :001), defeating
the null hypothesis. e conclusion would be that Corpus 1 and Corpus 2 bear a statistically
signiﬁcant similarity to each other. Note that this seems rather incompatible with the conclusion
that we would have to draw had we used the 2 test on the same data. It is interesting to see how
the Spearman correlation behaves in case two rather dissimilar corpora are compared. Figure 5.12
shows the calculation for a comparison of Corpus 1 with a hypothetical Corpus X that has quite
diﬀerent frequencies and ranks for many of the words. But even in this case, the rank correlation is
statistically signiﬁcant (p < 0:05), indicating that the two corpora are similar. us, when used for
hypothesis testing as in this example, the Spearman correlation is not a suitable measure because
it tends to reject the null hypothesis even if there are linguistically interesting diﬀerences between
two corpora. Moreover, as Kilgarriﬀ[2001] notes, diﬀerences among the top-ranked items have
the same impact on the correlation coeﬃcient as diﬀerences between lower-ranked items, but
from a linguistic perspective, diﬀerences among the ﬁrst few ranks are arguably quite substantial,
while diﬀerences among lower ranks are probably not.

5.3. MEASURING CORPUS SIMILARITY
97
Form
Corpus 1
Corpus 2
Rank Corpus 1
Rank Corpus 2
D
D2
de
6781719
6802262
1
1
0
0
,
5627749
5633555
2
2
0
0
la
3613946
3614049
3
3
0
0
.
3574395
3579032
4
4
0
0
que
2963992
2956662
5
5
0
0
y
2642241
2653365
6
6
0
0
en
2562028
2564809
7
7
0
0
el
2450353
2446328
8
8
0
0
a
1885112
1882813
9
9
0
0
los
1597103
1603537
10
10
0
0
del
1173860
1172623
11
11
0
0
se
1139311
1143202
12
12
0
0
las
1054729
1054924
13
13
0
0
un
1001556
1000106
14
14
0
0
rs D 1   6 P D2
n.n2   1/ D 1  6  0
14.142   1/ D 1   0 D 1
Figure 5.11: Spearman’s rank correlation calculated for the 14 most frequent lexical items in Corpus 1
and Corpus 2: a perfect correlation.
5.3.4
USING TEST STATISTICS WITHOUT HYPOTHESIS TESTING
As the above examples illustrate, if the question of whether two corpora are similar or diﬀer-
ent is answered by hypothesis testing, both 2 and the rank correlation fail to capture linguistic
intuitions about what it means for two corpora to be similar or diﬀerent. However, as opposed
to testing a null hypothesis, such test statistics can also be used for ranking a set of candidates.
For example, in detecting collocations, the t-statistic and other measures have been interpreted as
indicators of collocation strength, without doing any hypothesis testing [Manning and Schütze,
1999, Ch. 5]. Similarly, Kilgarriﬀ[2001] suggests to do away with hypothesis testing in com-
paring corpora, and instead use test statistics as a measure of relative similarity between corpora,
without looking at p-values. While this approach does not provide a measure of similarity in ab-
solute terms (“Corpus 1 and Corpus 2 have a statistically signiﬁcant similarity”), it can be used
to assess the relative similarity between corpora (“Corpus 1 is more similar to Corpus 2 than to
Corpus 3”). e question of course remains as to which test statistic best captures a relevant no-
tion of “similarity.” Kilgarriﬀ[2001] approaches this issue experimentally by preparing a set of
diﬀerent corpora, each of which has a diﬀerent composition in terms of text types. A number of
gold standard judgments about their similarity can thus be derived, and diﬀerent test statistics can
be evaluated as to how accurately they reﬂect these gold standard judgments. e main ﬁnding
is that the 2 statistic outperforms both the Spearman correlation coeﬃcient and several variants
of cross-entropy measures. e procedure is to compute a 2 statistic for a 2  m contingency
table for m most frequent words of Corpus 1 and Corpus 2 taken together (Kilgarriﬀ, 2001, re-

98
5. CORPUS EVALUATION AND COMPARISON
Form
Corpus 1
Corpus X
Rank 1
Rank X
D
D2
de
6781719
2446328
1
8
-7
49
,
5627749
2564809
2
7
-5
25
la
3613946
2653365
3
6
-3
9
.
3574395
2956662
4
5
-1
1
que
2963992
3579032
5
4
1
1
y
2642241
3614049
6
3
3
9
en
2562028
5633555
7
2
5
25
el
2450353
6802262
8
1
7
49
a
1885112
1882813
9
9
0
0
los
1597103
1000106
10
14
4
16
del
1173860
1054924
11
13
2
4
se
1139311
1143202
12
12
0
0
las
1054729
1172623
13
11
2
4
un
1001556
1603537
14
10
4
16
P = 208
rs D 1   6 P D2
n.n2   1/ D 1  6  208
14.142   1/ D 1   1248
2730 D 0:543; p < :05
Figure 5.12: Spearman rank correlation between two rather dissimilar corpora: still a signiﬁcant cor-
relation coeﬃcient.
ports that m D 500 works reasonably well). e 2 value is normalized by the degrees of freedom
(m   1) and can then be used for pairwise comparisons of corpora (which must be of the same
size, though, because the 2 value increases with absolute frequency).
5.4
COMPARING KEYWORDS
Another option for assessing the similarity of corpora is to compare lists of keywords extracted
from them. As opposed to the inspection of frequency lists as shown in Figure 5.8 above, keywords
are not just words that are frequent in a corpus (or document), but words that are in some sense
characteristic of a corpus (or document). A word’s degree of key-ness in a corpus (or document) can
be established in a variety of ways, but it often involves frequency. Crucially, key-ness in this sense
is a relative notion: a word is characteristic of a text/document because there are other relevant
documents/corpora of which the word is not (or less) characteristic. e reference point can be
some other corpus of about the same size, or it can be a much larger (and usually more varied)
corpus. Common approaches to keyword extraction compute a statistic from the frequency of each
word occurring in a document (or corpus) and the frequency of the same word in the reference
corpus. e statistic is then used to determine to what extent a word is characteristic of a document
(or corpus). In addition, some approaches calculate a p-value for each statistic, and sometimes a
measure of each word’s dispersion (whether it is common to many documents or occurs in just a
few) is taken into account as well. Approaches diﬀer as to the statistic they use, for example:

5.4. COMPARING KEYWORDS
99
• Ratio of relative frequencies (e. g., Damerau, 1993; Edmundson and Wyllys, 1961; Kilgar-
riﬀ, 2012),
• Yule’s [1944] diﬀerence coeﬃcient (e. g., Hoﬂand and Johansson, 1982),
• 2 (e. g., Scott, 1997),
• -2 Log-Likelihood (e. g., Scott, 2001),
• Mann-Whitney U (Kilgarriﬀ, 2001),
• term frequency by inverse document frequency (tf-idf, going back to Spärck Jones, 1972).
e diﬀerent statistics used for keyword extraction may result in quite diﬀerent keyword
lists for the same pair of corpora. We will illustrate this by comparing keywords extracted on the
basis of 2 with keywords obtained using the ratio of relative frequencies.
5.4.1
KEYWORD EXTRACTION WITH 2
A 2 value is computed from a 22 table for each word type in the joined corpus (Corpus 1 +
Corpus 2). at value is multiplied by the sign of the ﬁrst table cell’s Observed-Expected value,
which is positive if a word is over-represented in Corpus 1 and negative in case it is under-
represented. Without this step, it is impossible to tell whether a given word is characteristic of
Corpus 1 or Corpus 2. e word list is then sorted on the resulting score. e words on top of the
sorted list are over-represented in Corpus 1 (with respect to Corpus 2) and are thus considered to
be keywords of Corpus 1. Figures 5.13 and 5.14 illustrate this with two samples from the German
DECOW2012 corpus.
e number of function words on these two lists is particularly striking. If the aim of key-
word extraction is to draw conclusions about prominently represented domains in the corpora,
then function words are rather poor keywords. However, function words ending up as keywords
may warrant conclusions about text types/genres that are characteristic of a corpus. In the present
example, the fact that ﬁrst and second person pronouns, emoticons, and non-standard spellings
(hab < habe ‘(I) have’) are among the most characteristic words of Corpus 2 suggests that this
corpus contains rather informal text types (like postings to discussion forums). We will get back
to the issue of content words below when we contrast the results to the list of keywords obtained
using the ratio of relative frequencies.
5.4.2
KEYWORD EXTRACTION USING THE RATIO OF RELATIVE
FREQUENCIES
e ratio of relative frequencies was suggested as a measure early in research on automatic docu-
ment summarization and indexing [Edmundson and Wyllys, 1961], and it has also been used for
extracting multi-word keywords [Damerau, 1993]. e technique consists in dividing a word’s
relative frequency in a document by its relative frequency in a reference document collection,
which can be general (mixed domains) or domain speciﬁc. Kilgarriﬀ[2012] shows how keywords
from diﬀerent frequency bands can be obtained by varying the value of a smoothing constant

100
5. CORPUS EVALUATION AND COMPARISON
Score
Word
Translation
f Corpus 1
f Corpus 2
213857
Selbstbewusstsein
‘self-consciousness’
230086
7103
180761
der
‘the’
25823958
23332624
112942
Stärken
‘strengths’
147041
13782
110867
des
‘of.the’
5467541
4511218
109470
stärken
‘strengthen’
150609
16807
81735
in
‘in’
14531108
13291861
80314
Niederösterreich
‘Lower Austria
81732
997
69230
und
‘and’
27615823
26209999
67723
Steiermark
‘Styria’
69845
1165
61955
Schwächen
‘weaknesses’
101680
17034
58046
Sie
‘you’
2389178
1929354
48998
Selbstbewusstseinstraining
‘self-consciousness training’
48100
3
45075
Die
‘the’
4725965
4176917
41216
Oberösterreich
‘Upper Austria’
43930
1217
38921
werden
‘become’
3889092
3424772
33635
,
,
61275629
60440998
33447
die
‘the’
26896810
26075513
33069
wir
‘we’
2751969
2388203
32967
Gott
‘God’
277819
161587
27182
»
»
499957
355500
26135
durch
‘through’
2028104
1749480
25822
uns
‘us’
1432263
1196461
25367
von
‘from’
8958078
8458655
24745
Beispiele
‘examples’
95118
38807
24657
«
«
437106
308515
23838
Menschen
‘people’
804843
633205
23655
zur
‘to.the’
1887285
1631892
23466
In
‘in
1267897
1056218
23359
sich
‘oneself’
7073007
6636924
23056
Coaching
‘coaching’
32678
3958
Figure 5.13: Corpus-wise keyword extraction with 2: top 30 words over-represented in Corpus 1.
Score D 2  sgn.O1;1   E1;1/
(smoothing is necessary anyway to avoid division by zero in case the frequency of a word in the
reference corpus is zero). For every word occurring in at least one of the two corpora, the relative
frequency in each corpus is calculated after adding a smoothing constant to both numerator and
denominator. Next, the ratio of the relative frequencies in Corpus 1 and Corpus 2 is taken, and
the list of all ratios is sorted. e words at the top and bottom of the list are the keywords of
Corpus 1 and Corpus 2, respectively. Figures 5.15 and 5.16 show the list of keywords generated
by this method for the same two samples which were used in the 2 example above.
As opposed to the keywords extracted with 2, there are no function words at all here.
e keywords of Corpus 1 are dominated by content words from the domain of psychological
coaching, most of which do not occur at all in Corpus 2. On further inspection, this turns out
to be a combined failure of the crawling strategy that was used to build the corpus (breadth-ﬁrst
with additional Heritrix host bias, see Section 2.3), and the technique used for near duplicate
detection. Corpus 1 contains several thousand documents from the server of a life coaching com-
pany. e documents are virtually identical, except that a diﬀerent name of a town is inserted in

5.4. COMPARING KEYWORDS
101
Score
Word
Translation
f Corpus 1
f Corpus 2
-49228
es
‘it’
7191558
8203365
-52339
!
!
3539669
4250013
-55500
jetzt
‘now’
1093904
1497137
-58057
so
‘so’
4078118
4881829
-58090
meine
‘my’
743321
1086311
-61785
was
‘what’
2368986
2994141
-62641
wenn
‘if’
2525537
3175758
-62847
:zustimm:
‘agree
4
64046
-65120
bin
‘am’
898077
1296452
-68224
das
‘the’
11114277
12601969
-68699
:
:
7361865
8552945
-69360
Du
‘you’
856126
1258723
-70448
dann
‘then’
2705021
3418243
-73501
:-D
:-D
4080
86533
-74531
schon
‘already’
2299966
2975671
-79816
mich
‘me’
1776035
2391232
-80099
auch
‘too’
7618001
8919734
-99309
da
‘there’
2125894
2876793
-102364
aber
‘but’
3900764
4932552
-115949
du
‘you’
1501636
2189889
-117379
hab
‘have’
606783
1065271
-138531
nicht
‘not’
9679787
11587724
-144946
habe
‘have’
1725867
2552673
-166247
ja
‘yes’
1806276
2714682
-176159
mir
‘me’
2190317
3215638
-176735
Ich
‘I’
2664716
3791898
-186177
?
?
3889053
5278796
-186838
mal
‘once’
2000220
3014011
-333374
...
...
3418151
5189644
-709423
ich
‘I’
8606043
12673625
Figure 5.14: Corpus-wise keyword extraction with 2: top 30 words over-represented in Corpus 2.
ScoreD 2  sgn.O1;1   E1;1/
almost each sentence. is makes the sets of n-grams derived from the documents quite diﬀerent,
leading to the failure of the shingling approach as described in Section 3.5.3. e keywords as
they appear here are those occurring frequently in all those documents. us, as a side eﬀect, the
keyword list sheds some light on the limitations of this approach to near duplicate removal.
As for the lower part of the keyword list (Figure 5.16), our assumption that Corpus 2
contains more text written in an informal style than Corpus 1 seems to be conﬁrmed here. But,
interestingly, the majority among the top 30 keywords are emoticons (in the phpBB style) here,
while 2 returned predominantly function words. e reason for this diﬀerence is that the ratio
of relative frequencies does not depend on the words’ absolute frequencies, whereas the 2 grows
with increasing absolute frequency of the words even if the ratio remains constant, as illustrated
in Figure 5.17. Since function words normally have very high frequencies, they can be expected
to turn up as keywords if 2 is used as a measure.

102
5. CORPUS EVALUATION AND COMPARISON
Ratio
Word
Translation
f Corpus 1
f Corpus 2
14193
IntSel®-Selbstbewusstseinstraining
‘IntSel® self-consciousness training’
13929
0
12252
Selbstbewusstseinstraining
‘self-consciousness training’
48100
3
11414
IntSel®-Selbstbewusstseinstrainings
‘IntSel® self-consciousness trainings’
11201
0
7253
moviac
moviac (NE)
7118
0
5022
Kinder-Selbstbewusstseins-Coach
‘children’s self-consciousness coach’
4928
0
3876
www.theaterstuebchen.de
www.theaterstuebchen.de
3803
0
2876
Selbstbewusstseinstrainer
‘self-consciousness coach’
2822
0
2854
’schmökern
‘to browse’
2800
0
2853
IntSel®-Wertekonzept
‘IntSel® scheme of values’
2799
0
2853
IntSel®-Stärkenleiter
‘IntSel® scale of strengths’
2799
0
2853
Angst-Vermeidungsstrategien
‘fear avoidance strategies’
2799
0
2853
’Austherapierte
‘healed persons’
2799
0
2693
HR-Lieblingsschiﬀ
‘HR favorite ship’
2642
0
2613
Schwehm
Schwehm (NE)
17949
6
2158
www.bauemotion.de
www.bauemotion.de
2117
0
2068
:futsch:
emoticon
2029
0
1961
Litaraturmarkt
‘literature market’
1924
0
1721
or’al
or’al (NE)
1688
0
1631
Grujicic
Grujicic (NE)
1600
0
1591
:ﬂetch:
emoticon
1561
0
1554
Terror-Die
terror-the
1524
0
1550
Architektur-Meldungen
‘architecture news’
1520
0
1496
Beamt-er/
‘state employee’
1467
0
1469
91785
91785
2883
1
1464
Systemcoach
‘system coach’
2872
1
1438
Party-Highlight
‘party highlight’
1410
0
1435
Tiergefahren
‘danger from animals’
1407
0
1434
Tybrang
Tybrang (NE)
1406
0
1433
Titelvertei-digung
‘title defense’
1405
0
1433
Riesen-Herausforderer
‘big contender’
1405
0
Figure 5.15: Corpus-wise keyword extraction using the ratio of relative frequencies: top 30 words
over-represented in Corpus 1.
5.4.3
VARIANTS AND REFINEMENTS
While the above examples illustrate the basic mechanism, there are several reﬁnements of the
methods which address various shortcomings of the basic approach. To begin with, using word
forms has the disadvantage that homonyms (such as bank, house, handle, each of which has sev-
eral meanings) are counted together. On the other hand, inﬂected forms of the same lemma are
counted separately (such as give, gives, gave, given). is is undesirable in most contexts, since
keywords are usually meant to represent central concepts of a text, not formal aspects. is prob-
lem does not arise if words are disambiguated (morphologically, syntactically, semantically) before
measuring their frequencies. Such an approach would thus operate with lemmas, or pairs of the
form hword form, POSi or hword form, sensei.
Alternatively, keyword extraction can be based on the dispersion of a word in the two cor-
pora, for instance, measured in terms of a word’s document frequency (the number of documents
it occurs in; see Scott, 2001, for an example). It is also possible to combine a word’s frequency

5.4. COMPARING KEYWORDS
103
Ratio
Word
Translation
f Corpus 1
f Corpus 2
4.268E-04
:ﬁve:
emoticon
1
4774
4.265E-04
@vandeStonehill
@vandeStonehill
0
2388
4.265E-04
#WeLove
#WeLove
0
2388
4.265E-04
#HighSchoolMusical3
#HighSchoolMusical3
0
2388
4.208E-04
:aargh:
emoticon
0
2420
4.195E-04
:meinemeinung
emoticon
0
2428
4.189E-04
|supergri
emoticon
0
2431
4.125E-04
:zickig:
emoticon
0
2469
4.107E-04
*seh
*seh
0
2480
4.098E-04
|kopfkrat
emoticon
0
2485
4.096E-04
kostenlose-urteile.de
kostenlose-urteile.de
0
2486
4.072E-04
Migrantenrat
‘immigrants’ board’
1
5003
4.072E-04
MV-Politiker
‘MV-politician’
0
2501
3.971E-04
berlin.business-on.de
berlin.business-on.de
0
2565
3.731E-04
:fürcht:
emoticon
0
2730
3.655E-04
:leiderja:
emoticon
0
2787
3.473E-04
Bollywoodsbest
Bollywoodsbest
0
2933
3.329E-04
Fotoserver
‘photo server’
0
3060
3.192E-04
:urgs:
emoticon
0
3191
3.174E-04
:habenmuss:
emoticon
0
3209
3.168E-04
Stachelhausen
Stachelhausen (NE)
1
6431
2.906E-04
:rothlol:
emoticon
0
3505
2.637E-04
Juusuf
Juusuf (NE)
0
3863
2.489E-04
:zumgluecknein:
emoticon
0
4092
2.136E-04
:zufrieden:
emoticon
0
4770
1.953E-04
:menno:
emoticon
0
5217
1.818E-04
@ProSieben
@ProSieben
0
5602
1.682E-04
:dollschaem:
emoticon
0
6056
1.331E-04
:traeum:
emoticon
0
7656
1.281E-04
:dollfreu:
emoticon
0
7955
Figure 5.16: Corpus-wise keyword extraction using the ratio of relative frequencies: top 30 words
over-represented in Corpus 2.
Corpus 1
Corpus 2
f absolute
f relative
f absolute
f relative
Ratio of rel. freqs.
2
50
.05
100
.1
0.5
17.3
100
.1
200
.2
0.5
38.4
200
.2
400
.4
0.5
94.3
Figure 5.17: 2 vs. ratio of relative frequencies in keyword extraction, illustrated by two corpora of
1,000 tokens each.
and a measure of its dispersion for computing its key-ness (e. g., Rayson, 2003; for an overview
of diﬀerent measures of dispersion, see Gries, 2008).
Moreover, keywords can be extracted not only for a corpus as a whole, but also for individual
documents. An example is Scott’s [1997] approach, which compares the frequency list of each
document in Corpus 1 with the frequency list of Corpus 2. Corpus 1 as a whole can then be
characterized by computing key-keywords (words that are keywords in many documents).

104
5. CORPUS EVALUATION AND COMPARISON
Finally, a very prominent approach to document-wise keyword extraction that combines
frequency and dispersion of a word is based on term frequency by inverse document frequency.
TF-IDF was originally suggested in Spärck Jones [1972] for term weighting in the context of
document retrieval, and it is widely used today in many variants (see Manning et al., 2009, Ch. 6
for an overview). e underlying assumption is that a term is more characteristic of a document
if it occurs frequently in that document, but does not occur in many other documents. In its most
basic form, tf-idf is calculated as shown in 5.3, where TFt;d is the frequency of term t in document
d, N is the total number of documents in the corpus that serves as reference point, and DFt is
the number of documents in that corpus which contain term t at least once.
TF:IDFt;d D TFt;d  log N
DFt
(5.3)
Summing up the main points of this section, we have seen that assessing corpus similarity in
terms of hypothesis testing leads to uninformative results from a linguistic point of view. On the
other hand, the test statistics themselves can be used as a measure of relative similarity between
corpora. From among the many variants of keyword extraction, we illustrated two methods whose
results converged only partially, indicating that it is probably useful to apply more than one such
method and compare the results.
5.5
EXTRINSIC EVALUATION
Extrinsic evaluation of a corpus means judging it in the context of a particular task and often
involves a comparison with other corpora. As Kilgarriﬀet al. [in prep.] point out, a broad range
of applications require similar properties of a corpus (e. g., low rate of duplication, high degree
of cleanliness) and target “language in general” (e. g., dictionaries and many NLP tools). us,
evaluating a corpus with respect to one not too specialized task arguably also sheds light on its
quality with respect to more or less related tasks, and perhaps also indicates its general usefulness
for typical linguistic applications. Extrinsic evaluation of web corpora has been performed in a
variety of areas, ranging from classical linguistic applications (e. g., compiling lists of synonyms
and collocations) to the training of NLP tools (e. g., spell checkers, POS taggers).
For example, in the domain of NLP applications, Liu and Curran [2006] ﬁnd that using
their 10 billion token web corpus of English as a training set for (a particular sub-task of) auto-
matic spelling correction produces results similar to when the system is trained on the English
Gigaword corpus (2 billion tokens of newspaper text; Graﬀand Cieri, 2003). In addition, training
the system on their web corpus yields better results than training on search engine counts, which
again underlines the advantages of using static, full-text corpora. Giesbrecht and Evert’s [2009]
comparison of diﬀerent part-of-speech taggers, though not originally conceived as an evaluation
of the corpora involved, nevertheless aﬀords some conclusions as to the suitability of using a web
corpus as training data for a (particular) POS tagger.

5.5. EXTRINSIC EVALUATION
105
Distributional semantics, and in particular the automatic construction of thesauri (extended
synonym lists for a given target word), is another area that has ﬁgured prominently in the extrinsic
evaluation of corpora. In general terms, the procedure is to establish a gold standard of (quasi-)
synonyms for a set of target words and then compare it with a set of synonyms automatically
extracted from each of the candidate corpora. Individual studies diﬀer in the details of how the
gold standard is constructed and in the technical details concerning the identiﬁcation of synonyms
(type of context used in constructing the vectors, calculation of similarity between vectors). Fre-
quently, the results of such studies suggest that traditional corpora yield better results than web
corpora of about the same size, but are outperformed by larger web corpora. For example, building
on work by Curran and Moens [2002], Liu and Curran [2006] ﬁnd that their 10 billion tokens
web corpus of English performs equally well at the thesaurus task as the 2 billion tokens English
Gigaword corpus, but that a sample of 2 billion tokens from their web corpus does not. A sim-
ilar result is reported in Versley and Panchenko [2012], who compare a number of web corpora
(two purpose-made German web corpora and the German deWaC corpus) with a traditional cor-
pus of newspaper text (TüPP-D/Z; Müller, 2004). Faruqui and Padó [2010] study the eﬀect of
augmenting a NER system (Named Entity Recognition) with distributional information about
terms, thus incorporating semantic generalizations. eir main ﬁnding is that it makes very little
or no diﬀerence in practice whether the similarity clustering is done using a 175M tokens Ger-
man newswire corpus (Huge German Corpus) or a sample of the German deWaC of the same
size.³
Another application suitable to the evaluation of corpora is the extraction of collocations.
Similarly to the thesaurus task, this usually involves deﬁning a gold standard set of collocations for
a given set of target words. Collocations are automatically extracted from the candidate corpora
and compared to the gold standard, and measures such as precision and recall can be computed to
characterize the suitability of a given corpus for this particular task. For example, Ferraresi et al.
[2008] ﬁnd that their English and French web corpora (ukWaC and frWaC, respectively) are
comparable to a balanced, traditional corpus (such as the BNC) in terms of suitability for a speciﬁc
lexicographic purpose, namely a simulated revision of an English-French bilingual dictionary.
Kilgarriﬀet al. [in prep.] evaluate a number of English and Czech corpora, both web-derived
and traditional, on the collocation extraction task. Apart from discussing the role of gold standard
construction, their main ﬁnding is that large corpus sizes tend to yield considerably better results,
which means that web corpora (the ones used in the study are in the range of 10 billion tokens)
are in an advantageous position with respect to smaller, traditional corpora. On the other hand,
corpus annotation also plays a signiﬁcant role, which is evidenced by the fact that collocation
extraction from a (smaller) parsed corpus outperforms the much larger web corpus in the case of
Czech. See also Biemann et al. [in prep.] for an evaluation of diﬀerent web corpora and the BNC
with respect to collocation extraction.
³http://www.ims.uni-stuttgart.de/forschung/ressourcen/korpora/hgc.html

106
5. CORPUS EVALUATION AND COMPARISON
To sum up, corpora can be evaluated with respect to a variety of tasks, some of which we
brieﬂy illustrated above. Many of these studies converge on the ﬁnding that web corpora do not
generally perform noticeably worse than traditional corpora of the same size. Large web corpora
frequently outperform smaller, traditional corpora, for instance when a speciﬁc task values the
amount of available data more highly than the cleanliness of a corpus.
5.6
CORPUS COMPOSITION
When building a corpus based on a web crawl, documents are usually not sampled following a
pre-established stratiﬁcation scheme. In the best case, one might end up with a random sample of
the (relevant part of the) WWW, but even in this case, the exact composition of the ﬁnal corpus
is not known and needs to be established after corpus construction. Documents in a collection can
be classiﬁed along a large number of axes. A basic distinction in corpus composition can be drawn
between external criteria (situational determinants such as topic, genre, mode of publication, etc.)
and internal criteria (linguistic characteristics of a document; cf. Atkins et al., 1992; Biber, 1993).
Among the external criteria, another distinction can be made between content-related dimen-
sions (such as topic or domain) and other dimensions. Web documents are a challenge in this
respect because some of them instantiate text types/genres that were not usually present in tra-
ditional (i. e., non-web) document collections. Recent years have seen a number of extensions
of more traditional classiﬁcation schemes to accommodate web genres (e. g., Biber and Kurjian,
2007; Rehm et al., 2008; Sinclair, 1996; Open Directory Project and many more), but there is no
such thing as an established inventory of web genres. A variety of aspects of web genres is dis-
cussed in Mehler et al. [2010], which we recommend for approaching this topic. An example of a
classiﬁcation scheme is Sharoﬀ[2006] (based on the EAGLES scheme), which posits ﬁve major
dimensions: “authorship, mode (aka channel), knowledge expected from the audience, the aim of
text production and the generalized domain.”
5.6.1
ESTIMATING CORPUS COMPOSITION
Once a classiﬁcation scheme is chosen, the classiﬁcation itself can be carried out manually or au-
tomatically. By manually annotating a sample from the corpus, the distribution of a given variable
in the population (which in this case is the corpus) can be estimated within a desired conﬁdence
interval. In order to train an automatic classiﬁer, hand-annotated training data is usually needed as
well, so manual annotation is probably the ﬁrst step both for estimating corpus composition, and
for automatic classiﬁcation of the documents. We will not deal with the latter here (but see, for
example, Sebastiani, 2002, for a survey of automated text categorization in the context of machine
learning; Sharoﬀ, 2010 for an application to web corpora). When hand-annotation is carried out
in order to estimate the relative frequency of a category in the population, the size of the conﬁ-
dence interval must be balanced against the sample size. e point here is that hand-coding of a
few hundred documents (as a minimum) is necessary in order to stay within acceptable conﬁdence
intervals (e. g., not greater than ˙5%) even in the worst case where a category makes up for 50%

5.6. CORPUS COMPOSITION
107
of the data. When the annotation task is shared across several annotators, it is advisable to code
a number of documents in parallel, check the inter-annotator agreement and, if necessary, clarify
the application of the annotation guidelines before proceeding with the annotation.
5.6.2
MEASURING CORPUS COMPOSITION
Ciaramita and Baroni [2006] suggest a procedure for determining whether a corpus is “reasonably
varied in terms of the topics (and, perhaps, textual types) it represents.” e aim of these authors
is not to collect a random sample of documents from the WWW, but rather to gather a collection
of documents that is balanced in the sense that no topic or text type is heavily over-represented in
it (i. e., a corpus which is deliberately biased with respect to a random sample, in a way considered
favorable). eir procedure consists in calculating the distance of the target corpus to each one of a
number of corpora that are deliberately biased toward some topic or text type. Biased corpora can
be built by applying the BootCaT method (see 2.3.2, also for restrictions of the BootCaT method
which apply as of today) on appropriately biased collections of word tuples. Distance is based on
word frequencies and measured as relative entropy (or Kullback-Leibler distance, Kullback and
Leibler, 1951). For each corpus, the distances to all other corpora are averaged to yield a mean
distance. e expectation is that, if the target corpus is unbiased (“varied”), it arguably has a
smaller mean distance than all biased corpora (because it is somewhere “in between” all these
biased corpora). However, this method only makes sense if the corpus builders share Ciaramita
and Baroni’s [2006] ideal of a varied corpus. If, on the other hand, the goal in constructing a web
corpus is something close to a uniform random sample of the accessible web, then the method
does not apply.
5.6.3
INTERPRETING CORPUS COMPOSITION
Once the composition of a web corpus is known (or rather: has been estimated), it can be com-
pared to the composition of other corpora. One question that has played a role in recent years
is whether the composition of web corpora bears enough similarity to established, traditionally
compiled corpora to be considered as alternatives in situations where more data is needed than
traditional corpora can provide. Many of the corpora that are widely used in Corpus Linguis-
tics aim at representativeness and are balanced, meaning that their composition reﬂects a ratio
that the corpus designers establish in advance, according to what they think a balanced corpus
should contain. e Brown corpus, the LOB corpus, the British National Corpus, and the Ger-
man DWDS corpus are all balanced in this sense. Very large web corpora, on the other hand, are
typically not balanced, because they are not obtained on the basis of a stratiﬁed sampling scheme,
but by collecting documents more or less randomly from the web. If no measures are taken to
correct for this, the composition of a web corpus is likely to be a result of the speciﬁc crawling
strategy, plus potential biases introduced by post processing steps that discard certain documents
for some reason or other.

108
5. CORPUS EVALUATION AND COMPARISON
Balance and the question of how representativeness can be achieved in corpus design can be
considered from quite diﬀerent perspectives (see, among many others, Atkins et al., 1992; Biber,
1993):
• Production: Corpus composition should reﬂect the distribution of texts produced in a speech
community.
• Reception: Corpus composition should reﬂect the impact (number of recipients) of diﬀerent
text types (e. g., Leech, 2007).
• Relevance: Corpus composition should reﬂect the importance/inﬂuence of a text type, etc.,
in a speech community (discussed in Biber, 1993).
Determining any of these variables with any certainty is practically unfeasible, and for some
of them, such as cultural relevance, it is not even clear how they could possibly be measured
[Hunston, 2008, 162; McEnery et al., 2006, 16].
Another problem concerning balance is that
[…] arguments that a particular corpus is representative, or balanced, are inevitably
circular, in that the categories we are invited to observe are artifacts of the design
procedure [Hunston, 2008, 164]
From this perspective, the fact that typical web corpora are not balanced does not appear
to be too serious a drawback, especially because their usually very large size has the potential to
make up for it. As Leech [2007, 138] puts it:
[…] in general, the larger a corpus is, and the more diverse it is in terms of genres and
other language varieties, the more balanced and representative it will be.
Of course, all that can be said about a corpus which is based on a random sample from a
segment of the web is that it represents the distribution of diﬀerent text types, topics, etc., within
this segment of the web. Such a corpus might be considered highly valuable by some, and totally
useless by others, which underlines the fact that there are no good or bad corpora in absolute
terms. Rather, we totally agree with Hunston [2008] and others that a corpus can only be judged
with respect to a particular purpose. It may also be useful to think of corpus construction as a
cyclic process, in which
th[e] corpus is used and analyzed and its strengths and weaknesses identiﬁed and
reported. In the light of this experience and feedback the corpus is enhanced by the
addition or deletion of material and the cycle is repeated continually. [Atkins et al.,
1992, 4]
We close this chapter with a quote from [McEnery et al., 2006, 73]:
Corpus-building is of necessity a marriage of perfection and pragmatism. […] It is
advisable to base your claims on your corpus and avoid unreasonable generalizations.

5.7. SUMMARY
109
5.7
SUMMARY
In this chapter, we have exempliﬁed some ways of assessing a corpus’s properties and quality,
mostly under the premise that corpus quality is at best a relative notion. Some simple but eﬀec-
tive techniques which work corpus-internally (e. g., examining word and sentence length distribu-
tions) were demonstrated. Other methods which we have introduced work by comparing aspects
of the corpus, like word frequencies or keywords, to other corpora. ese can be used, for example,
to compare a web corpus to some reference corpus, or specialized (biased) corpora to more gen-
eral corpora. All these techniques have the added advantage of potentially revealing ﬂaws in the
crawling and post-processing procedures as introduced in Chapters 2–4. In addition, we intro-
duced the notion of extrinsic, task-oriented corpus evaluation and presented examples from the
areas of automatic thesaurus building and collocation extraction. Finally, we brieﬂy discussed how
information about the composition of a corpus, which is often available for traditionally compiled
corpora as an artifact of the stratiﬁed sampling procedure, has to be collected after corpus con-
struction when it comes to web corpora. e results of such an assessment of corpus composition
can also be used to compare the web corpus to other corpora.


111
Bibliography
Abiteboul, S., Preda, M., and Cobena, G. (2003). Adaptive on-line page importance computa-
tion. In Proceedings of the 12th International Conference on World Wide Web, WWW ’03, pages
280–290, New York, NY, USA. ACM. DOI: 10.1145/775152.775192. 30
Abramson, M. and Aha, D. (2009). What’s in a URL? Genre classiﬁcation from URLs. Technical
report, AAAI Technical Report WS-12-09. 35
Achlioptas, D., Clauset, A., Kempe, D., and Moore, C. (2005). On the bias of traceroute sam-
pling: or, power-law degree distributions in regular graphs. In Proceedings of the thirty-seventh
annual ACM symposium on eory of computing, STOC ’05, pages 694–703, New York, NY,
USA. ACM. DOI: 10.1145/1060590.1060693. 30
Ahmad, F. and Kondrak, G. (2005).
Learning a spelling error model from search
query logs.
In HLT/EMNLP. e Association for Computational Linguistics. DOI:
10.3115/1220575.1220695. 79
Almpanidis, G., Kotropoulos, C., and Pitas, I. (2007). Combining text and link analysis for
focused crawling-an application for vertical search engines. Inf. Syst., 32(6):886–908. DOI:
10.1016/j.is.2006.09.004. 35
Ando, R. K. and Lee, L. (2003). Mostly-unsupervised statistical segmentation of Japanese Kanji
sequences. Natural Language Engineering, 9(2):127–149.
DOI: 10.1017/S1351324902002954. 66
Atkins, S., Clear, J., and Ostler, N. (1992). Corpus design criteria. Literary and Linguistic Com-
puting, 7(1):1–16. DOI: 10.1093/llc/7.1.1. 106, 108
Baayen, H. R., Piepenbrock, R., and Gulikers, L. (1995). e CELEX lexical database (CD-
ROM). Technical report, Linguistic Data Consortium, University of Pennsylvania, Philadel-
phia. 17
Baayen, R. H. (2001). Word frequency distributions. Kluwer, Dordrecht. DOI: 10.1007/978-94-
010-0844-0. 95
Baeza-Yates, R., Castillo, C., and Efthimiadis, E. N. (2007). Characterization of national Web
domains. ACM Trans. Internet Technol., 7(2). DOI: 10.1145/1239971.1239973. 11, 12, 13

112
BIBLIOGRAPHY
Baeza-Yates, R., Castillo, C., Marin, M., and Rodriguez, A. (2005). Crawling a country: better
strategies than breadth-ﬁrst for web page ordering. In Special interest tracks and posters of the
14th International Conference on World Wide Web, WWW ’05, pages 864–872, New York, NY,
USA. ACM. DOI: 10.1145/1062745.1062768. 30
Bar-Yossef, Z., Broder, A. Z., Kumar, R., and Tomkins, A. (2004).
Sic transit gloria telae:
towards an understanding of the web’s decay. In Proceedings of the 13th International Con-
ference on World Wide Web, WWW ’04, pages 328–337, New York, NY, USA. ACM. DOI:
10.1145/988672.988716. 11
Bar-Yossef, Z. and Gurevich, M. (2006). Random sampling from a search engine’s index. In
Proceedings of WWW 2006, pages 367–376, Edinburgh. DOI: 10.1145/1411509.1411514. 17,
18
Bar-Yossef, Z. and Rajagopalan, S. (2002). Template detection via data mining and its applica-
tions. In In Proceedings of the 11th International Conference on World Wide Web, pages 580–591.
DOI: 10.1145/511446.511522. 50
Barabási, A.-L. and Bonabeau, E. (2003). Scale-free networks. Scientiﬁc American, 288(5):60–69.
DOI: 10.1038/scientiﬁcamerican0503-60. 8
Baroni, M. and Bernardini, S. (2004). BootCaT: Bootstrapping corpora and terms from the web.
In Proceedings of LREC 04, pages 1313–1316. 16
Baroni, M. and Bernardini, S., editors (2006).
WaCky! Working papers on the Web as Corpus.
GEDIT, Bologna. 115, 126
Baroni, M., Bernardini, S., Ferraresi, A., and Zanchetta, E. (2009). e WaCky Wide Web: A
collection of very large linguistically processed web-crawled corpora. Language Resources and
Evaluation, 43(3):209–226. DOI: 10.1007/s10579-009-9081-4. xiii, 3, 12, 26, 28, 50, 51, 56
Baroni, M., Chantree, F., Kilgarriﬀ, A., and Sharoﬀ, S. (2008). CleanEval: A competition for
cleaning webpages. In Proceedings of LREC 06, pages 638–643, Marrakech. ELRA. 50, 51
Bauer, D., Degen, J., Deng, X., Herger, P., Gasthaus, J., Giesbrecht, E., Jansen, L., Kalina, C.,
Krüger, T., Märtin, R., Schmidt, M., Scholler, S., Steger, J., Stemle, E., and Evert, S. (2007).
Filtering the internet by automatic subtree classiﬁcation. In Fairon et al. [2007], pages 111–
122. 51, 55
Baykan, E., Henzinger, M., Marian, L., and Weber, I. (2009). Purely URL-based topic classiﬁ-
cation. In Proceedings of the 18th International Conference on World Wide Web, pages 1109–1110.
DOI: 10.1145/1526709.1526880. 35
Baykan, E., Henzinger, M., and Weber, I. (2008). Web page language identiﬁcation based on
URLs. In Proceedings of the VLDB Endowment, pages 176–187. 35

BIBLIOGRAPHY
113
Becchetti, L., Castillo, C., Donato, D., and Fazzone, A. (2006). A comparison of sampling
techniques for Web characterization. In Proceedings of the Workshop on Link Analysis 2006
(LinkKDD). ACM Press. 28
Berger, A. L., Della Pietra, S. A., and Della Pietra, V. J. (1996). A maximum entropy approach
to natural language processing. Computational Linguistics, 22(1):39–71. 68
Berners-Lee, T. (1989). Information management: A proposal. Technical report, W3C. 8
Best, K.-H. (2001). Probability distributions of language entities. Journal of Quantitative Lin-
guistics, 8(1):1–11. DOI: 10.1076/jqul.8.1.1.4091. 86
Best, K.-H. (2005). Satzlänge. In Köhler, R., Altmann, G., and Piotrowski, R. G., editors,
Quantitative Linguistik. Ein Internationales Handbuch, volume 27 of Handbücher zur Sprach-
und Kommunikationswissenschaft, chapter 22, pages 298–304. de Gruyter, Berlin/New York. 86
Bharat, K. and Broder, A. (1998). A technique for measuring the relative size and overlap of
public web search engines. In Proceedings of the 7th International World Wide Web Conference,
pages 379–388. Elsevier Science. DOI: 10.1016/S0169-7552(98)00127-5. 17
Biber, D. (1993).
Representativeness in corpus design.
Literary and Linguistic Computing,
8(4):243–257. DOI: 10.1093/llc/8.4.243. 106, 108
Biber, D. and Kurjian, J. (2007). Towards a taxonomy of web registers an text types: a multidi-
mensional analysis. In Hundt et al. [2007], pages 109–131. 106
Biemann, C., Bildhauer, F., Evert, S., Goldhahn, D., Quasthoﬀ, U., Schäfer, R., and Zesch, T. (in
prep.). Scalable construction of high-quality web corpora. Special issue of Journal for Language
Technology and Computational Linguistics. 105
Biemann, C., Heyer, G., Quasthoﬀ, U., and Richter, M. (2007). e Leipzig Corpora Collection
- Monolingual corpora of standard size. In Proceedings of Corpus Linguistics 2007, Birmingham,
UK. xiii, 3, 86
Bird, S., Loper, E., and Klein, E. (2009). Natural Language Processing with Python. O’Reilly,
Sebastopol, Calif. 84
Björneborn, L. and Ingwersen, P. (2004). Toward a basic framework for Webometrics. J. Am.
Soc. Inf. Sci., 55(14):1216–1227. DOI: 10.1002/asi.20077. 11
Bloem, J., Regneri, M., and ater, S. (2012). Robust processing of noisy web-collected data. In
Jancsary, J., editor, Proceedings of KONVENS 2012, pages 189–193. ÖGAI. 78
Bloom, B. (1970). Space/time trade-oﬀs in hash coding with allowable errors. Communications
of ACM, 13(7):422–426. DOI: 10.1145/362686.362692. 21

114
BIBLIOGRAPHY
Brants, S., Dipper, S., Hansen, S., Lezius, W., and Smith, G. (2002). e TIGER treebank. In
Proceedings of the Workshop on Treebanks and Linguistic eories, Sozopol. 75
Brants, T. (2000). TnT – a statistical part-of-speech tagger. In ANLP, pages 224–231. DOI:
10.3115/974147.974178. 75
Brill, E. (1992). A simple rule-based part of speech tagger. In Proceedings of the workshop on
Speech and Natural Language, HLT ’91, pages 112–116, Stroudsburg, PA, USA. Association
for Computational Linguistics. DOI: 10.3115/1075527.1075553. 68
Brill, E. (1995). Transformation-based error-driven learning and natural language processing: a
case study in part-of-speech tagging. Computational Linguistics, 21(4):543–565. 68
Brin, S. and Page, L. (1998). e anatomy of a large-scale hypertextual web search engine. In
Proceedings of the 7th International World Wide Web Conference, pages 107–117. Elsevier Science.
2, 32
Broder, A., Kumar, R., Maghoul, F., Raghavan, P., Stata, R., Tomkins, A., and Wiener, J. L.
(2000).
Graph structure in the web.
In Proceedings of the 9th International World Wide
Web conference on Computer Networks: e International Journal of Computer and Telecommu-
nications Networking, pages 309–320. North-Holland Publishing Co. DOI: 10.1016/S1389-
1286(00)00083-9. 9
Broder, A. and Mitzenmacher, M. (2004). Network applications of Bloom ﬁlters: A survey.
Internet Mathematics, 1(4):485–509. DOI: 10.1080/15427951.2004.10129096. 21
Broder, A. Z., Glassman, S. C., Manasse, M. S., and Zweig, G. (1997). Syntactic clustering of
the Web. Technical Note 1997-115, SRC, Palo Alto. 60, 61
Bryan, K. and Leise, T. (2006). e $25,000,000,000 eigenvector: e linear algebra behind
Google. SIAM Review, 48(3):569–581. DOI: 10.1137/050623280. 32
Cai, D., Yu, S., Wen, J., and Ma, W. (2003). Extracting content structure for web pages based on
visual representation. In Proceedings of the 5th Asia-Paciﬁc Web Conference on Web Technologies
and Applications, pages 406–417. DOI: 10.1007/3-540-36901-5_42. 50
Calzolari, N., Choukri, K., Declerck, T., Doğan, M. U., Maegaard, B., Mariani, J., Odijk, J.,
and Piperidis, S., editors (2012). Proceedings of the Eight International Conference on Language
Resources and Evaluation (LREC’12), Istanbul. ELRA. 118, 123, 125
Carter, S., Weerkamp, W., and Tsagkias, M. (2012). Microblog language identiﬁcation: over-
coming the limitations of short, unedited and idiomatic text. Language Resources and Evalua-
tion, pages 1–21. DOI: 10.1007/s10579-012-9195-y. 57

BIBLIOGRAPHY
115
Castillo, C. and Davison, B. D. (2011). Adversarial Web Search, volume 4(5) of Foundations and
Trends in Information Retrieval. now Publishers, Hanover, MA. 15
Cavnar, W. B. and Trenkle, J. M. (1994). N-gram-based text categorization. In Proceedings
of SDAIR-94, 3rd Annual Symposium on Document Analysis and Information Retrieval, pages
161–175. 57
Chakrabarti, D., Kumar, R., and Punera, K. (2007). Page-level template detection via isotonic
smoothing. In Proceedings of the 16th International Conference on World Wide Web, pages 61–70.
DOI: 10.1145/1242572.1242582. 50
Chakrabarti, S., Dom, B., and Indyk, P. (1998). Enhanced hypertext categorization using hy-
perlinks. In Proceedings of the 1998 ACM SIGMOD International Conference on Management
of data, SIGMOD ’98, pages 307–318. ACM. DOI: 10.1145/276304.276332. 35
Chakrabarti, S., van den Berg, M., and Dom, B. (1999).
Focused crawling: a new ap-
proach to topic-speciﬁc web resource discovery. Computer Networks, 31:1623–1640. DOI:
10.1016/S1389-1286(99)00052-3. 35
Chen, K.-J. and Liu, S.-H. (1992). Word identiﬁcation for Mandarin Chinese sentences. In
COLING, pages 101–107. DOI: 10.3115/992066.992085. 66
Cho, J., García-Molina, H., and Page, L. (1998).
Eﬃcient crawling through URL order-
ing. In Proceedings of the 7th International World Wide Web Conference. DOI: 10.1016/S0169-
7552(98)00108-1. 35
Cho, J. and Schonfeld, U. (2007). Rankmass crawler: A crawler with high personalized PageRank
coverage guarantee. In Proceedings of the 33rd International Conference on Very Large Data Bases.
30
Church, K. W. and Gale, W. A. (1995).
Poisson mixtures.
Natural Language Engineering,
1(2):163–190. DOI: 10.1017/S1351324900000139. 95
Church, K. W. and Mercer, R. L. (1993). Introduction to the special issue on computational
linguistics using large corpora. Computational Linguistics, 19(1):1–24. 69
Ciaramita, M. and Baroni, M. (2006). Measuring web-corpus randomness: A progress report.
In Baroni and Bernardini [2006], pages 127–158. 18, 31, 107
Clark, A. (2003). Pre-processing very noisy text. In Proceedings of the Workshop on Shallow Pro-
cessing of Large Corpora (SProLaC 2003), 27 March, 2003, pages 12–22. 81
Clark, E. and Araki, K. (2011). Text normalization in social media: progress, problems and
applications for a pre-processing system of casual English. Procedia - Social and Behavioral
Sciences, 27:2–11. DOI: 10.1016/j.sbspro.2011.10.577. 81

116
BIBLIOGRAPHY
Cook, P. and Hirst, G. (2012). Do web-corpora from top-level domains represent national va-
rieties of English? In Proceedings of the 11th International Conference on the Statistical Analysis
of Textual Data, pages 281–293, Liège. 13
Cucerzan, S. and Brill, E. (2004). Spelling correction as an iterative process that exploits the
collective knowledge of web users. In EMNLP, pages 293–300. ACL. 2, 79
Curran, J. R. and Moens, M. (2002). Scaling context space. In Proceedings of the 40th An-
nual Meeting on Association for Computational Linguistics, ACL ’02, pages 231–238. DOI:
10.3115/1073083.1073123. 105
Damerau, F. J. (1964). A technique for computer detection and correction of spelling errors.
Communications of the ACM, 7(3):171–176. DOI: 10.1145/363958.363994. 80
Damerau, F. J. (1993). Generating and evaluating domain-oriented multi-word terms from texts.
Information Processing & Management, 29(4):433–447. DOI: 10.1016/0306-4573(93)90039-
G. 99
Davies, M. (2002). Corpus del español: 100 million words, 1200s-1900s. 2
Dunning, T. (1994). Statistical identiﬁcation of language. Technical Report MCCS-94-273,
Computing Research Laboratory, New Mexico State University. 57
Eckart, T., Quasthoﬀ, U., and Goldhahn, D. (2012). e inﬂuence of corpus quality on statistical
measurements on language resources. In Proceedings of LREC 08, pages 2318–2321, Istanbul.
86
Edmundson, H. P. and Wyllys, R. E. (1961). Automatic abstracting and indexing. Survey and
recommendations. Communications of the ACM, 4(5):226–234. DOI: 10.1145/366532.366545.
99
Esakov, J., Lopresti, D. P., and Sandberg, J. S. (1994). Classiﬁcation and distribution of opti-
cal character recognition errors. In Proceedings of the IS&T/SPIE International Symposium on
Electronic Imaging, volume 2181, pages 204–216. DOI: 10.1117/12.171108. 72
Eu, J. (2008). Testing search engine frequencies: Patterns of inconsistency. Corpus Linguistics
and Linguistic eory, 4(2):177–207. DOI: 10.1515/CLLT.2008.008. 3
Evert, S. (2007). StupidOS: A high-precision approach to boilerplate removal. In Fairon et al.
[2007], pages 123–134. 51, 55
Fairon, C., Naets, H., Kilgarriﬀ, A., and de Schryver, G.-M., editors (2007). Building and Explor-
ing Web Corpora: Proceedings of the 3rd Web as Corpus Workshop (Incorporating CLEANEVAL),
Louvain. Presses universitaires de Louvain. 112, 116, 117, 118, 119, 122, 125

BIBLIOGRAPHY
117
Faruqui, M. and Padó, S. (2010). Training and evaluating a German named entity recognizer
with semantic generalization. In Proceedings of KONVENS 2010, Saarbrücken, Germany. 105
Ferraresi, A., Bernardini, S., Picci, G., and Baroni, M. (2008). Web corpora for bilingual lexicog-
raphy: A pilot study of English/French collocation extraction and translation. In Proceedings of
UCCTS: International Symposium on Using Corpora in Contrastive and Translation Studies. 105
Fetterly, D., Craswell, N., and Vinay, V. (2009). e impact of crawl policy on web search eﬀec-
tiveness. In Proceedings of the 32nd International ACM SIGIR Conference on Research and De-
velopment in Information Retrieval, SIGIR ’09, pages 580–587, New York, NY, USA. ACM.
DOI: 10.1145/1571941.1572041. 30
Fetterly, D., Manasse, M., Najork, M., and Wiener, J. (2003). A large-scale study of the evolution
of web pages. In Proceedings of the 12th International Conference on World Wide Web. DOI:
10.1145/775152.775246. 11
Fitschen, A. and Gupta, P. (2008). Lemmatising and morphological tagging. In Lüdeling and
Kytö [2008], pages 552–564. 69
Fletcher, W. H. (2011). Corpus analysis of the World Wide Web. In Chapelle, C. A., editor,
Encyclopedia of Applied Linguistics. Wiley-Blackwell, Hoboken. xiii, 72
Gao, W. and Abou-Assaleh, T. (2007). GenieKnows web page cleaning system. In Fairon et al.
[2007], pages 135–140. 50, 51
Geyken, A. (2006). e DWDS corpus: A reference corpus for the German language of the
20th Century. In Fellbaum, C. D., editor, Collocations and Idioms: Linguistic, Lexicographic,
and Computational Aspects, pages 23–40. Continuum Press, London. 1
Giesbrecht, E. and Evert, S. (2009). Part-of-speech (POS) tagging – a solved task? An evaluation
of POS taggers for the German Web as Corpus. In Alegria, I., Leturia, I., and Sharoﬀ, S.,
editors, Proceedings of the Fifth Web as Corpus Workshop (WAC5), pages 27–35, San Sebastián.
75, 79, 83, 104
Gillick, D. (2009). Sentence boundary detection and the problem with the U.S. In Proceedings
of Human Language Technologies: e 2009 Annual Conference of the North American Chapter of
the Association for Computational Linguistics, Companion Volume: Short Papers, pages 241–244,
Stroudsburg, PA, USA. Association for Computational Linguistics. 84
Giménez, J. and Màrquez, L. (2003).
Fast and accurate part-of-speech tagging: e
SVM approach revisited.
In Nicolov, N., Bontcheva, K., Angelova, G., and Mitkov,
R., editors, RANLP, pages 153–163. John Benjamins, Amsterdam/Philadelphia. DOI:
10.1162/coli.2000.27.4.603b. 69

118
BIBLIOGRAPHY
Giménez, J. and Màrquez, L. (2004). SVMTool. a general POS tagger generator based on Sup-
port Vector Machines. In Lino, M. T., Xavier, M. F., Ferreira, F., Costa, R., and Silva, R.,
editors, Proceedings of the 4th International Conference on Language Ressources and Evaluation
(LREC 2004), pages 43–46, Lisbon, Portugal. 75
Girardi, C. (2007). Htmcleaner: Extracting the relevant text from the web pages. In Fairon et al.
[2007], pages 141–144. 51
Gjoka, M., Kurant, M., Butts, C. T., and Markopoulou, A. (2011). A walk in Facebook: a case
study of unbiased sampling of Facebook. In Proceedings of IEEE INFOCOM 2010, San Diego.
IEEE. DOI: 10.1109/INFCOM.2010.5462078. 29, 31
Goldhahn, D., Eckart, T., and Quasthoﬀ, U. (2012). Building large monolingual dictionaries at
the Leipzig Corpora Collection: From 100 to 200 languages. In Calzolari et al. [2012]. xiii,
86
Goldsmith, J. A. (2010). Segmentation and morphology. In Clark, A., Fox, C., and Lappin, S.,
editors, e handbook of computational linguistics and natural language processing, pages 364–393.
Wiley-Backwell, Chichester. DOI: 10.1002/9781444324044. 66, 69
Gomes, D. and Silva, M. J. (2005). Characterizing a national community web. ACM Trans.
Internet Technol., 5(3):508–531. DOI: 10.1145/1084772.1084775. 35
Gottron, T. and Lipka, N. (2010). A comparison of language identiﬁcation approaches on short,
query-style texts. In Gurrin, C., He, Y., Kazai, G., Kruschwitz, U., Little, S., Roelleke, T.,
Rüger, S., and Rijsbergen, K., editors, Advances in Information Retrieval, volume 5993 of Lec-
ture Notes in Computer Science, pages 611–614. Springer Berlin Heidelberg. 57
Graﬀ, D. and Cieri, C. (2003). English Gigaword. Linguistic Data Consortium, Philadelphia.
104
Grefenstette, G. (1995). Comparing two language identiﬁcation schemes. In Proceedings of the
3rd Internation conference on Statistical Analysis of Textual Data (JADT 1995), pages 263–268,
Rome. 57
Grefenstette, G. and Tapanainen, P. (1994). What is a word? What is a sentence? In Proceedings
of 3rd Conference on Computational Lexicography and Text Research. 45, 67
Gries, S. (2008). Dispersions and adjusted frequencies in corpora. International Journal of Corpus
Linguistics, 13(4):403–437. DOI: 10.1075/ijcl.13.4.02gri. 103
Grzybek, P. (2007). History and methodology of word length studies. In Grzybek, P., editor,
Contributions to the science of text and language. Word length studies and related issues, pages 15–
90. Springer, Dordrecht. 86

BIBLIOGRAPHY
119
Gulli, A. and Signorini, A. (2005). e indexable web is more than 11.5 billion pages. In Special
interest tracks and posters of the 14th International Conference on World Wide Web, WWW ’05,
pages 902–903, New York, NY, USA. ACM. DOI: 10.1145/1062745.1062789. 16
Guo, J., Xu, G., Li, H., and Cheng, X. (2008). A uniﬁed and discriminative model for query
reﬁnement. In Proceedings of the 31st Annual International ACM SIGIR Conference on Research
and Development in Information Retrieval, SIGIR ’08, pages 379–386, New York, NY, USA.
ACM. DOI: 10.1145/1390334.1390400. 3
Hagen, M., Potthast, M., Stein, B., and Bräutigam, C. (2011). Query segmentation revisited. In
Proceedings of the 20th International Conference on World Wide Web, WWW ’11, pages 97–106,
New York, NY, USA. ACM. DOI: 10.1145/1963405.1963423. 3
Hall, M., Frank, E., Holmes, G., Pfahringer, B., Reutemann, P., and Witten, I. H. (2009).
e WEKA data mining software: An update. SIGKDD Explorations, 11(1):10–18. DOI:
10.1145/1656274.1656278. 55
Hall, M. and Witten, I. H. (2011). Data mining: practical machine learning tools and techniques.
Kaufmann, Burlington. 55
Henzinger, M. (2006). Finding nearduplicate web pages: A largescale evaluation of algorithms.
In Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Devel-
opment in Information Retrieval, pages 284–291. DOI: 10.1145/1148170.1148222. 61
Henzinger, M. R., Heydon, A., Mitzenmacher, M., and Najork, M. (2000). On near-uniform
URL sampling. In Proceedings of the 9th International World Wide Web Conference on Computer
Networks: e International Journal of Computer and Telecommunications Networking, pages 295–
308. North-Holland Publishing Co. DOI: 10.1016/S1389-1286(00)00055-4. 31, 32
Hoﬀmann, K. and Weerkamp, W. (2007). Web corpus cleaning using content and structure. In
Fairon et al. [2007], pages 145–154. 51, 55
Hoﬂand, K. and Johansson, S. (1982). Word frequencies in British and American English. e
Norwegian Computing Centre for the Humanities, Bergen. 99
Huang, C.-R., Šimon, P., Hsieh, S.-K., and Prévot, L. (2007). Rethinking Chinese word seg-
mentation: tokenization, character classiﬁcation, or wordbreak identiﬁcation. In Proceedings
of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL
’07, pages 69–72, Stroudsburg, PA, USA. Association for Computational Linguistics. DOI:
10.3115/1557769.1557791. 66
Hundt, M., Nesselhauf, N., and Biewer, C., editors (2007). Corpus linguistics and the web. Rodopi,
Amsterdam and New York. 113, 121, 122

120
BIBLIOGRAPHY
Hunston, S. (2008). Collection strategies and design decisions. In Lüdeling and Kytö [2008],
pages 154–168. 108
Issac, F. (2007). Yet another web crawler. In Fairon, C., Naets, H., Kilgarriﬀ, A., and de Schryver,
G.-M., editors, Proceedings of the 3rd Web as Corpus Workshop (WAC 3) – Building and Exploring
Web Corpora, pages 57–68, Louvain. Presses universitaires de Louvain. 31, 51
Keller, F. and Lapata, M. (2003). Using the web to obtain frequencies for unseen bigrams. Com-
putational Linguistics, 29(3):459–484. 3
Kilgarriﬀ, A. (2001). Comparing corpora. International Journal of Corpus Linguistics, 6(1):97–
133. DOI: 10.1075/ijcl.6.1.05kil. 93, 95, 96, 97, 99
Kilgarriﬀ, A. (2006). Googleology is bad science. Computational Linguistics, 33(1):147–151.
DOI: 10.1162/coli.2007.33.1.147. 2
Kilgarriﬀ, A. (2009). Simple maths for keywords. In Proceedings of the Corpus Linguistics Confer-
ence, Liverpool. 13
Kilgarriﬀ, A. (2012). Getting to know your corpus. In Sojka, P., Horák, A., Kope￿ek, I., and
Pala, K., editors, Text, Speech and Dialogue - 15th International Conference, TSD 2012, Brno,
Czech Republic, September 3-7, 2012. Proceedings, pages 3–15, Heidelberg. Springer. DOI:
10.1007/978-3-642-23538-2. 99
Kilgarriﬀ, A., Baisa, V., Jakubicek, M., Kovara, V., and Rychly, P. (in prep.). How to evaluate a
corpus. ms. 104, 105
Kilgarriﬀ, A. and Grefenstette, G. (2003). Introduction to the special issue on the Web as corpus.
Computational Linguistics, 29:333–347. DOI: 10.1162/089120103322711569. xiii
Kiss, T. and Strunk, J. (2006). Unsupervised multilingual sentence boundary detection. Compu-
tational Linguistics, 34(4):485–525. DOI: 10.1162/coli.2006.32.4.485. 67, 68
Kohlschütter, C., Fankhauser, P., and Nejdl, W. (2010).
Boilerplate detection using shallow
text features. In Proceedings of the third ACM International Conference on Web Search and Data
Mining, pages 441–450. DOI: 10.1145/1718487.1718542. 51
Kornai, A. and Hálacsy, P. (2008). Google for the linguist on a budget. In Evert, S., Kilgarriﬀ,
A., and Sharoﬀ, S., editors, Proceedings of the 4th Web as CorpusWorkshop (WAC-4) – Can we
beat Google?, pages 8–11, Marrakech. 30, 31
Korpela, J. K. (2006). Unicode Explained. O’Reilly Media, Sebastopol. 42
Kukich, K. (1992). Techniques for automatically correcting words in text. ACM Computing
Surveys, 24(4):377–439. DOI: 10.1145/146370.146380. 72, 78, 79, 80

BIBLIOGRAPHY
121
Kullback, S. and Leibler, R. A. (1951). On information and suﬃciency. e Annals of Mathe-
matical Statistics, 22(1):79–86. DOI: 10.1214/aoms/1177729694. 107
Kupietz, M., Belica, C., Keibel, H., and Witt, A. (2010). e German reference corpus DeReKo:
A primordial sample for linguistic research. In Calzolari, N., Choukri, K., Maegaard, B., Mar-
iani, J., Odijk, J., Piperidis, S., Rosner, M., and Tapias, D., editors, Proceedings of the Seventh
International Conference on Language Resources and Evaluation (LREC’10), pages 1848–1854,
Valletta, Malta. European Language Resources Association (ELRA). 1, 28
Kurant, M., Markopoulou, A., and iran, P. (2010). On the bias of BFS (Breadth First Search).
In International Teletraﬃc Congress (ITC 22). DOI: 10.1109/ITC.2010.5608727. 30
Kurant, M., Gjoka, M., Butts, C. T., and Markopoulou, A. (2011).
Walking on a graph
with a magnifying glass: stratiﬁed sampling via weighted random walks.
In Proceedings
of the ACM SIGMETRICS Joint International Conference on Measurement and Modeling of
Computer Systems, SIGMETRICS ’11, pages 281–292, New York, NY, USA. ACM. DOI:
10.1145/1993744.1993773. 30
Laﬀerty, J. D., McCallum, A., and Pereira, F. C. N. (2001). Conditional Random Fields: Prob-
abilistic models for segmenting and labeling sequence data. In Proceedings of the Eighteenth
International Conference on Machine Learning, ICML ’01, pages 282–289, San Francisco, CA,
USA. Morgan Kaufmann Publishers Inc. 68
Landauer, T. K., Foltz, P. W., and Laham, D. (1998). Introduction to Latent Semantic Analysis.
Discourse Processes, 25:259–84. DOI: 10.1080/01638539809545028. 56
Leech, G. (1993).
100 Million words of English.
English Today, 9(1):9–15. DOI:
10.1017/S0266078400006854. 1, 13
Leech, G. (2007). New resources or just better old ones? e Holy Grail of representativeness.
In Hundt et al. [2007], pages 133–149. 108
Levenshtein, V. I. (1966). Binary codes capable of correcting deletions, insertions, and reversals.
Soviet Physics Doklady, 10(8):707–710. 80
Li, M., Zhu, M., Zhang, Y., and Zhou, M. (2006). Exploring distributional similarity based
models for query spelling correction. In Calzolari, N., Cardie, C., and Isabelle, P., editors,
ACL 2006, 21st International Conference on Computational Linguistics and 44th Annual Meeting
of the Association for Computational Linguistics, Proceedings of the Conference, Sydney, Australia,
17–21 July 2006. e Association for Computer Linguistics. 79
Liu, V. and Curran, J. R. (2006). Web text corpus for natural language processing. In 11th
Conference of the European Chapter of the Association for Computational Linguistics: EACL 2006,
pages 233–240. 71, 104, 105

122
BIBLIOGRAPHY
Lopresti, D. (2009). Optical character recognition errors and their eﬀects on natural language
processing. International Journal on Document Analysis and Recognition, 12(3):141–151. DOI:
10.1145/1390749.1390753. 72
Lüdeling, A., Evert, S., and Baroni, M. (2007). Using the web for linguistic purposes. In Hundt
et al. [2007], pages 7–24. xiii
Lüdeling, A. and Kytö, M., editors (2008). Corpus linguistics: an International Handbook. Walter
de Gruyter, Berlin. DOI: 10.1515/9783110211429. 117, 120, 125
MacIntyre, R. (1995). Sed script to produce Penn Treebank tokenization on arbitrary raw text.
http://www.cis.upenn.edu/~treebank/tokenization.html. 67
Madhavan, J., Afanasiev, L., Antova, L., and Halevy, A. (2009). Harnessing the Deep Web:
Present and Future. In 4th Biennial Conference on Innovative Data Systems Research (CIDR).
10
Maiya, A. S. and Berger-Wolf, T. Y. (2011). Beneﬁts of bias: towards better characterization
of network sampling. In Proceedings of the 17th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, KDD ’11, pages 105–113, New York, NY, USA. ACM.
DOI: 10.1145/2020408.2020431. 25, 30
Manning, C., Raghavan, P., and Schütze, H. (2009). An Introduction to Information Retrieval.
CUP, Cambridge. 2, 7, 8, 20, 32, 61, 104
Manning, C. D. and Schütze, H. (1999). Foundations of statistical natural language processing.
MIT Press, Cambridge, MA. 68, 97
Marcus, M. P., Santorini, B., and Marcinkiewicz, M. A. (1993). Building a large annotated
corpus of English: e Penn Treebank. Computational Linguistics, 19(2):313–330. 1, 78
Marcus, M. P., Santorini, B., Marcinkiewicz, M. A., and Taylor, A. (1999). Treebank-3. Tech-
nical report, Linguistic Data Consortium, Philadelphia. 1
Marek, M., Pecina, P., and Spousta, M. (2007). Web page cleaning with Conditional Random
Fields. In Fairon et al. [2007], pages 155–162. 51, 55
McEnery, T., Xiao, R., and Tono, Y. (2006). Corpus-based language studies. An advanced resource
book. Routledge, London and New York. 108
Mehler, A., Sharoﬀ, S., and Santini, M., editors (2010). Genres on the web. Computational models
and empirical studies, volume 42 of Text, Speech and Language Technology. Springer, Dordrecht.
106, 126

BIBLIOGRAPHY
123
Menczer, F., Pant, G., and Srinivasan, P. (2004). Topical web crawlers: Evaluating adaptive
algorithms. ACM Trans. Internet Technol., 4(4):378–419. DOI: 10.1145/1031114.1031117.
35
Mikheev, A. (2002). Periods, Capitalized Words, etc. Computational Linguistics, 28(3):289–318.
DOI: 10.1162/089120102760275992. 67
Mikheev, A. (2003). Text segmentation. In Mitkov [2003], chapter 10, pages 201–218. 68
Mitkov, R., editor (2003). e Oxford handbook of computational linguistics. Oxford University
Press, Oxford. 123, 127
Mohr, G., Stack, M., Ranitovic, I., Avery, D., and Kimpton, M. (2004). Introduction to Heritrix,
an archival quality web crawler. In Proceedings of the 4th International Web Archiving Workshop
(IWAW’04). 22, 29
Müller, F. H. (2004). Stylebook for the Tübingen partially parsed corpus of written German
(TüPP-D/Z). Technical report, Seminar für Sprachwissenschaft, Universität Tübingen. 105
Najork, M. and Wiener, J. L. (2001).
Breadth-ﬁrst crawling yields high-quality pages.
In
Proceedings of the 10th Conference on World Wide Web, pages 114–118. Elsevier Science. DOI:
10.1145/371920.371965. 30
Odell, M. K. and Russell, R. C. (1918). U.S. Patents 1261167 (1918), 1435663 (1922). 80
Olston, C. and Najork, M. (2010). Web Crawling, volume 4(3) of Foundations and Trends in
Information Retrieval. now Publishers, Hanover, MA. 7, 16, 35
Ostendorf, M., Favre, B., Grishman, R., Hakkani-Tur, D., Harper, M., Hillard, D., Hirschberg,
J., Ji, H., Kahn, J. G., Liu, Y., Maskey, S., Matusov, E., Ney, H., Rosenberg, A., Shriberg, E.,
Wang, W., and Wooters, C. (2007). Speech segmentation and its impact on spoken document
processing. Manuscript, Signal Processing Magazine. 72
Padró, L. and Stanilovsky, E. (2012). Freeling 3.0: Towards wider multilinguality. In Calzolari
et al. [2012], pages 2473–2479. 83
Pagh, A., Pagh, R., and Rao, S. S. (2005). An optimal Bloom ﬁlter replacement. In Proceedings
of the Sixteenth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 823–829. DOI:
10.1145/1070432.1070548. 21
Palmer, D. D. and Hearst, M. A. (1997). Adaptive multilingual sentence boundary disambigua-
tion. Computational Linguistics, 23(2):241–267. 67, 68
Pasternack, J. and Roth, D. (2009). Extracting article text from the web with maximum subse-
quence segmentation. In Proceedings of the 18th International Conference on World Wide Web,
pages 971–980. DOI: 10.1145/1526709.1526840. 51, 55

124
BIBLIOGRAPHY
Peterson, J. L. (1986).
A note on undetected typing errors.
Communications of the ACM,
29(7):633–637. DOI: 10.1145/6138.6146. 78
Philips, L. (1990). Hanging on the metaphone. Computer Language Magazine, 7(12):38–44. 80
Pomikálek, J. (2011). Removing boilerplate and duplicate content from web corpora. PhD thesis,
Masaryk University Faculty of Informatics, Brno. 50, 51
Pomikálek, J., Jakubíček, M., and Rychlý, P. (2012). Building a 70 billion word corpus of English
from ClueWeb. In Proceedings of LREC 08, pages 502–506. 20
Porter, M. F. (1980).
An algorithm for suﬃx stripping.
Program, 14(3):130–137. DOI:
10.1108/eb046814. 69
Postel, J. (1969). Die Kölner Phonetik. Ein Verfahren zur Identiﬁzierung von Personennamen
auf der Grundlage der Gestaltanalyse. IBM-Nachrichten, 19:925–931. 80
Rabin, M. O. (1981). Fingerprinting by random polynomials. Technical Report TR-CSE-03-01,
Center for Research in Computing Technology, Harvard University, Harvard. 63
Ratnaparkhi, A. (1998). Maximum entropy models for natural language ambiguity resolution.
IRCS Tech Report IRCS-98-15, University of Pennsylvania, Institute for Research in Cogni-
tive Science. 68
Rayson, P. (2003). Matrix: A statistical method and software tool for linguistic analysis through corpus
comparison. PhD thesis, Lancaster University. 103
Rayson, P., Charles, O., and Auty, I. (2012). Can Google count? Estimating search engine result
consistency. In Kilgarriﬀ, A. and Sharoﬀ, S., editors, Proceedings of the seventh Web as Corpus
Workshop, pages 23–30. 3
Rehm, G., Santini, M., Mehler, A., Braslavski, P., Gleim, R., Stubbe, A., Symonenko, S.,
Tavosanis, M., and Vidulin, V. (2008). Towards a reference corpus of web genres for the
evaluation of genre identiﬁcation systems. In Calzolari, N., Choukri, K., Maegaard, B., Mar-
iani, J., Odjik, J., Piperidis, S., and Tapias, D., editors, Proceedings of the Sixth International
Conference on Language Resources and Evaluation (LREC’08), Marrakech, Morocco. European
Language Resources Association (ELRA) 106
Řehůřek, R. and Kolkus, M. (2009). Language identiﬁcation on the web: Extending the dictio-
nary method. In Gelbukh, A., editor, Computational Linguistics and Intelligent Text Processing,
volume 5449 of Lecture Notes in Computer Science, pages 357–368. Springer Berlin Heidelberg.
58
Risvik, K. M., Mikołjewski, T., and Boros, P. (2003). Query segmentation for web search. In
Proceedings of the 12th International Conference on World Wide Web, WWW ’03, New York, NY,
USA. ACM. 3

BIBLIOGRAPHY
125
Rusmevichientong, P., Pennock, D. M., Lawrence, S., and Giles, C. L. (2001). Methods for
sampling pages uniformly from the World Wide Web. In AAAI Fall Symposium on Using
Uncertainty Within Computation, pages 121–128. 31, 34
Safran, M., Althagaﬁ, A., and Che, D. (2012). Improving relevance prediction for focused Web
crawlers. In IEEE/ACIS 11th International Conference on Computer and Information Science
(ICIS), 2012, pages 161–166. DOI: 10.1109/ICIS.2012.61. 35
Saralegi, X. and Leturia, I. (2007). Kimatu, a tool for cleaning non-content text parts from
HTML docs. In Fairon et al. [2007], pages 163–167. 51
Schiller, A., Teufel, S., Stöckert, C., and ielen, C. (1999). Guidelines für das Tagging deutscher
Textkorpora mit STTS (kleines und großes Tagset). Institut für maschinelle Sprachverarbeitung,
Universität Stuttgart and Institut für Sprachwissenschaft, Universität Tübingen, Stuttgart and
Tübingen. 79
Schmid, H. (1994a). Part-of-speech tagging with neural networks. In COLING, pages 172–176.
DOI: 10.3115/991886.991915. 68
Schmid, H. (1994b). Probabilistic part-of-speech tagging using decision trees. In Proceedings of
International Conference on New Methods in Language Processing. DOI: 10.1007/BFb0026668.
67, 68
Schmid, H. (1995). Improvements in part-of-speech tagging with an application to German.
In Proceedings of the EACL SIGDAT-Workshop, Dublin, Ireland. DOI: 10.1007/978-94-017-
2390-9_2. 75, 83
Schmid, H. (2000). Unsupervised learning of period disambiguation for tokenisation. Internal
report, IMS, Universität Stuttgart. 67, 68, 72
Schmid, H. (2008). Tokenizing and part-of-speech tagging. In Lüdeling and Kytö [2008], pages
527–551. 74
Schäfer, R. and Bildhauer, F. (2012). Building large corpora from the web using a new eﬃcient
tool chain. In Calzolari et al. [2012], pages 486–493. 12, 20, 26, 28, 51, 55, 56
Schäfer, R. and Sayatz, U. (submitted). Die Kurzformen des Indeﬁnitartikels im Deutschen. 76
Scott, M. (1997). PC analysis of key words - and key key words. System, 25(2):233–245. DOI:
10.1016/S0346-251X(97)00011-0. 99, 103
Scott, M. (2001). Comparing corpora and identifying keywords, collocations, frequency distri-
butions through the WordSmith Tools suite of computer programs. In Ghadessy, M., Henry,
A., and Roseberry, R., editors, Small Corpus Studies and ELT, pages 47–67. Benjamins, Ams-
terdam and Philadelphia. 99, 102

126
BIBLIOGRAPHY
Sebastiani, F. (2002). Machine learning in automated text categorization. ACM Computing
Surveys, 34(1):1–47. DOI: 10.1145/505282.505283. 106
Serrano, M. A., Maguitman, A., Boguñá, M., Fortunato, S., and Vespignani, A. (2007). Decod-
ing the structure of the WWW: a comparative analysis of web crawls. ACM Trans. Web, 1(2).
DOI: 10.1145/1255438.1255442. 9
Sharoﬀ, S. (2006). Creating general-purpose corpora using automated search engine queries. In
Baroni and Bernardini [2006], pages 63–98. 106
Sharoﬀ, S. (2010). In the garden and in the jungle: comparing genres in the BNC and internet.
In Mehler et al. [2010], pages 149–166. 106
Sinclair, J. (1996). Preliminary recommendations on corpus typology. Technical Report EAG–
TCWG–CTYP/P, Expert Advisory Group on Language Engineering Standards document.
106
Spärck Jones, K. (1972). A statistical interpretation of term speciﬁcity and its application in
retrieval. Journal of Documentation, 28(1):11–21. 99, 104
Spousta, M., Marek, M., and Pecina, P. (2008). Victor: e web-page cleaning tool. In Evert,
S., Kilgarriﬀ, A., and Sharoﬀ, S., editors, Proceedings of the 4th Web as Corpus Workshop, pages
12–17, Marrakech. 40, 51, 52, 55
Sproat, R., Black, A. W., Chen, S., Kumar, S., Ostendorf, M., and Richards, C. (2001).
Normalization of non-standard words. Computer Speech and Language, 15:287–333. DOI:
10.1006/csla.2001.0169. 81
Sproat, R. and Shih, C. (1990). A statistical method for ﬁnding word boundaries in Chinese
text. Computer Processing of Chinese and Oriental Languages, 4:336–351. 66
Sproat, R., Shih, C., Gale, W., and Chang, N. (1996).
A stochastic ﬁnite-state word-
segmentation algorithm for Chinese. Computational Linguistics, 22(3):377–404. 66
Srinivasan, P., Menczer, F., and Pant, G. (2005). A general evaluation framework for topical
crawlers. Inf. Retr., 8(3):417–447. DOI: 10.1007/s10791-005-6993-5. 35
Subramaniam, L. V., Roy, S., Faruquie, T. A., and Negi, S. (2009). A survey of types of text
noise and techniques to handle noisy text. In Proceedings of e ird Workshop on Analytics for
Noisy Unstructured Text Data, AND ’09, pages 115–122, New York, NY, USA. ACM. DOI:
10.1145/1568296.1568315. 72
Suchomel, V. and Pomikálek, J. (2012). Eﬀcient Web crawling for large text corpora. In Kilgarriﬀ,
A. and Sharoﬀ, S., editors, Proceedings of the seventh Web as Corpus Workshop, pages 40–44. 26,
35

BIBLIOGRAPHY
127
Tapanainen, P. and Voutilainen, A. (1994). Tagging accurately: Don’t guess if you know. In
Proceedings of the Fourth Conference on Applied Natural Language Processing, ANLC ’94, pages
47–52. DOI: 10.3115/974358.974370. 68, 69
Teahan, W. J. (2000). Text classiﬁcation and segmentation using Minimum Cross Entropy. In
Proceeding of RIAO, pages 943–961. 58
Teahan, W. J., McNab, R., Wen, Y., and Witten, I. H. (2000).
A compression-based al-
gorithm for Chinese word segmentation. Computational Linguistics, 26(3):375–393. DOI:
10.1162/089120100561746. 66
elwall, M. (2009). Introduction to Webometrics: quantitative web research for the social sci-
ences. Synthesis Lectures on Information Concepts, Retrieval, and Services, 1(1):1–116. DOI:
10.2200/S00176ED1V01Y200903ICR004. 11
Toutanova, K., Klein, D., Manning, C. D., and Singer, Y. (2003).
Feature-rich
part-of-speech tagging with a cyclic dependency network.
In HLT-NAACL. DOI:
10.3115/1073445.1073478. 75, 83
van den Bosch, A., Busser, B., Canisius, S., and Daelemans, W. (2007). An eﬃcient memory-
based morphosyntactic tagger and parser for Dutch. In Dirix, P., Schuurman, I., Vandeghin-
ste, V., and Van Eynde, F., editors, Selected Papers of the 17th Computational Linguistics in the
Netherlands Meeting (Leuven, Belgium), pages 99–114, Utrecht. LOT. 69, 83
van Gompel, M., van der Sloot, K., and van den Bosch, A. (2012). Ucto: Unicode tokeniser. Ver-
sion 0.5.3. reference guide. ILK Technical Report ILK 12-05, Induction of Linguistic Knowl-
edge Research Group, Tilburg Centre for Cognition and Communication, Tilburg University,
Tilburg. 67, 84
Versley, Y. and Panchenko, Y. (2012). Not just bigger: Towards better-quality web corpora. In
Kilgarriﬀ, A. and Sharoﬀ, S., editors, Proceedings of the seventh Web as Corpus Workshop, pages
44–52. 105
Voutilainen, A. (2003). Part-of-speech tagging. In Mitkov [2003], chapter 11, pages 219–232.
68
Wong, W., Liu, W., and Bennamoun, M. (2007). Enhanced integrated scoring for cleaning dirty
texts. In IJCAI Workshop on Analytics for Noisy Unstructured Text Data (AND);, Hyderabad,
India. 81
Wu, Z. and Tseng, G. (1993).
Chinese text segmentation for text retrieval: Achievements
and problems. Journal of the American Society for Information Science, 44(9):532–542. DOI:
10.1002/(SICI)1097-4571(199310)44:9%3C532::AID-ASI3%3E3.0.CO;2-M. 66

128
BIBLIOGRAPHY
Xue, N. (2003). Chinese word segmentation as character tagging. Computational Linguistics and
Chinese Language Processing, 8(1):29–48. 66
Yule, G. (1944). e statistical study of literary vocabulary. Cambridge University Press, Cam-
bridge. 99
Yuret, D. and Türe, F. (2006). Learning morphological disambiguation rules for Turkish. In
Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages
328–334. Association for Computational Linguistics. DOI: 10.3115/1220835.1220877. 69
Zamorano, J. P., del Rosal García, E., and Lara, I. A. (2011). Design and development of Iberia:
a corpus of scientiﬁc Spanish. Corpora, 6:145–158. DOI: 10.3366/cor.2011.0010. 45

129
Authors’ Biographies
ROLAND SCHÄFER
Roland Schäfer studied eoretical and Indo-European Linguistics as well as Japanese Linguis-
tics at Marburg and Bochum Universities. He completed his doctorate Arguments and Adjuncts
at the Syntax-Semantics Interface in 2008 at Göttingen University, supervised by Gert Webelhuth
and Regine Eckardt. Since then, he has been working as a research assistant at Freie Universität
Berlin, mainly doing corpus-based research on semantic and morpho-syntactic phenomena. In
2011, he started working on the COW (“Corpora from the Web”) project with Felix Bildhauer.
His teaching experience covers a wide range of topics including eoretical and Corpus Linguis-
tics, English and German Linguistics, as well as Computational Linguistics.
FELIX BILDHAUER
Felix Bildhauer studied Romance Linguistics at the Universities of Göttingen and Barcelona. He
received his doctorate from the University of Bremen in 2008 with a dissertation on representing
information structure in a Head-Driven Phrase Structure Grammar of Spanish. Since 2007, he
has been working as a research assistant at Freie Universität Berlin, focusing on corpus-based
approaches to information structure in German and various Romance languages. With Roland
Schäfer, he started compiling corpora from the Web to overcome the lack of large, available cor-
pora in some of the languages they are working on. He has taught courses on a variety of subjects,
including syntax, phonology, semantics, Corpus Linguistics, and Computational Linguistics.



Principles of Program Analysis 

Springer-Verlag Berlin Heidelberg GmbH 

Flemming Nielson Hanne Riis Nielson 
Chris Hankin 
Principles 
of Program Analysis 
With 56 Figures and 51 Tables 
Springer 

Flemming Nielson 
Hanne Riis Nielson 
Department of Computer Science 
University of Aarhus, Bldg. 540 
Ny Munkegade, DK-8000 Aarhus C 
Denmark 
E-mail: {fn.hrn}@daimi.au.dk 
WWW: http://www.daimi.au.dkrfn 
Chris Hankin 
Department of Computing 
The Imperial College of Science, Technology, and Medicine 
180 Queen's Gate, London SW7 2BZ 
UK 
E-mail: clh@doc.ic.ac.uk 
Cataloging-in-Publication data applied for 
Die Deutsche Bibliothek - CIP-Einheitsaufnahme 
Nielson, Flemming: 
Principles of program analysis: with 51 tables/F. Nielson; H.R. 
Nielson; C. Hankin. 
ACM Subject Classification (1998): F.3.2, D.3, F.3, D.2 
ISBN 978-3-642-08474-4 
This work is subject to copyright. All rights are reserved, whether the whole or part of the 
material is concerned, specifically the rights of translation, reprinting, reuse of illustrations, 
recitation, broadcasting, reproduction on microfilm or in any other way, and storage in data 
banks. Duplication of this publication or parts thereof is permitted only under the provisions 
of the German Copyright Law of September 9, 1965, in its current version, and permission for 
use must always be obtained from Springer-Verlag Berlin Heidelberg GmbH. 
Violations are liable for prosecution under the German Copyright Law. 
Â© Springer-Verlag Berlin Heidelberg 1999 
Originally published by Springer-Verlag Berlin Heidelberg New York in 1999 
Softcover reprint ofthe hardcover 1 st edition 1999 
The use of general descriptive names, trademarks, etc. in this publication does not imply, even in 
the absence of a specific statement, that such names are exempt from the relevant protective laws 
and regulations and therefore free for general use. 
Typesetting: Camera -ready by the authors 
Cover Design: Erich Kirchner, Heidelberg 
Printed on acid-free paper SPIN 10706501 - 06/3142SR - 543210 
ISBN 978-3-642-08474-4
ISBN 978-3-662-03811-6 (eBook)
DOI 10.1007/978-3-662-03811-6

Preface 
Aims of the book. Our motivation for writing this book is twofold. 
One is that for quite some time we have lacked a textbook for our own courses 
on program analysis; instead we have been forced to base the courses on 
conference papers supplemented with the occasional journal paper or chapter 
from a text book out of print. The other is the growing feeling that the 
various subcommunities in the field often study similar problems without 
being sufficiently aware of insights or developments generally known in other 
subcommunities. The idea then emerged that perhaps a text book could be 
written that both would be useful for advanced courses on program analysis 
and that would help increase the awareness in the field about the many 
similarities between the various approaches. 
There is an important analogy to complexity theory here. Consider re-
searchers or students looking through the literature for a clever algorithm 
or heuristics to solve a problem on graphs. They might come across papers 
on clever algorithms or heuristics for solving problems on boolean formulae 
and might dismiss them on the grounds that graphs and boolean formulae 
surely are quite different things. Yet complexity theory tells us that this is 
quite a mistaken point of view. The notions of log-space reduction between 
problems and of NP-complete problems lead to realising that problems may 
appear unrelated at first sight and nonetheless be so closely related that a 
good algorithm or heuristics for one will give rise to a good algorithm or 
heuristics for the other. We believe it to be a sad fact that students of 
programming languages in general, and program analysis in particular, are 
much too often allowed to get away with making similarly naive decisions 
and arguments. Program analysis is still far from being able to precisely re-
late ingredients of different approaches to one another but we hope that this 
book will help to bring this about. In fact, we hope to shock quite a number 
of readers by convincing them that there are important similarities between 
the approaches of their own work and other approaches deemed to be of no 
relevance. 
This book concentrates on what we believe to be the four main approaches to 
program analysis: Data Flow Analysis; Constraint Based Analysis; Abstract 

VI 
Preface 
Interpretation; and Type and Effect Systems. For each approach we aim 
at identifying and describing the important general principles, rather than 
presenting a cook-book of techniques and language constructs, with a view 
to how the principles scale up for more complex programming languages and 
analyses. 
As a consequence we have deliberately decided that this book should not 
treat a number of interesting approaches, some of which are dear to the 
heart of the authors, in order to be able to cover the four main approaches 
to some depth; the approaches not covered include denotationally based pro-
gram analysis, projection analysis, and logical formulations based on Stone 
dualities. For reasons of space, we have also had to omit material that would 
have had a natural place in the book; this includes a deeper treatment of 
set constraints, fast techniques for implementing type based analyses, single 
static assignment form, a broader treatment of pointer analyses, and the in-
terplay between analysis and transformation and how to efficiently recompute 
analysis information invalidated by the transformation. 
How to read the book. The book is relatively self-contained although 
the reader will benefit from previous exposure to discrete mathematics and 
compilers. The main chapters are generally rigorous in the early parts where 
the basic techniques are covered, but less so in the later parts where more 
advanced techniques are covered. 
Chapter 1 is intended to be read quickly. The purpose of the chapter is to 
give an overview of the main approaches to program analysis, to stress the 
approximate nature of program analysis, and to make it clear that seemingly 
different approaches may yet have profound similarities. We recommend 
reading through all of Chapter 1 even though the reader might want to spe-
cialise in only parts of the book. 
Chapter 2 introduces Data Flow Analysis. Sections 2.1 to 2.4 cover the basic 
techniques for intraprocedural analysis, including the Monotone Frameworks 
as generalisations of the Bit Vector Frameworks, and a worklist algorithm 
for computing the information efficiently. Section 2.2 covers more theoretical 
properties (semantic correctness) and can be omitted on a first reading. The 
presentation makes use of notions from lattice theory and for this we refer to 
Appendix A.I, A.2 and parts of A.3. -
Section 2.5 is a more advanced section 
that gives an overview of interprocedural analysis including a treatment of call 
string based methods and methods based on assumption sets; since Section 
2.5 is used as a stepping stone to Chapter 3 we recommend to read at least 
up to Subsection 2.5.2. Section 2.6 is an advanced section that illustrates 
how the relatively simple techniques introduced so far can be combined to 
develop a very complex shape analysis, but the material is not essential for 
the remainder of the book. 

Preface 
VII 
Chapter 3 covers Constraint Based Analysis. The treatment makes a clear 
distinction between determining the safety of an analysis result and how to 
compute the best safe result; it also stresses the need to analyse open systems. 
Sections 3.1,3.3 and 304 cover the basic techniques; these include coinduction 
which is likely to be new to most readers and we refer to the treatment in 
Appendix B (building upon Tarski's theorem as covered in Appendix Ao4). 
Section 3.2 covers more theoretical properties (semantic correctness and the 
existence of best solutions) and can be omitted on a first reading. -
Sections 
3.5 and 3.6 extend the development so as to link up with the treatment 
of Data Flow Analysis. Section 3.5 shows how to incorporate Monotone 
Frameworks (Section 2.3) and Section 3.6 shows how to add context in the 
manner of call strings and assumption sets (Section 2.5). 
Chapter 4 covers Abstract Interpretation in a programming language inde-
pendent fashion in order to stress that it can be integrated both with Data 
Flow Analysis and Constraint Based Analysis. Section 4.1 introduces some 
of the key considerations and is used to motivate some of the technical def-
initions in later sections. Section 4.2 deals with the use of widening and 
narrowing for approximating fixed points and Sections 4.3 deals with Galois 
connections; this order of presentation has been chosen to stress the funda-
mental nature played by widenings but the sections are largely independent 
of one another. -
Sections 4.4 and 4.5 study how to build Galois connec-
tions in a systematic manner and how to use them for inducing approximate 
analyses; the material is not essential for the remainder of the book. 
Chapter 5 covers Type and Effect Systems which is an approach to program 
analysis that is often viewed as having a quite different flavour from the ap-
proaches covered so far. Section 5.1 presents the basic approach (by linking 
back to the Constraint Based Analyses studied in Chapter 3) and suffices for 
getting an impression of the approach. Section 5.2 studies more theoretical 
properties (semantic correctness) and Section 5.3 studies algorithmic issues 
(soundness and completeness of a variation of algorithm W) and these sec-
tions can be omitted on a first reading. -
Sections 5.4 and 5.5 gradually 
introduce more and more advanced Type and Effect Systems. 
Chapter 6 presents algorithms for Data Flow Analysis and Constraint Based 
Analysis. The treatment concentrates on general techniques for solving sys-
tems of inequations. We emphasise the fact that, to a large extent, the same 
set of techniques can be used for a number of different approaches to pro-
gram analysis. Section 6.1 presents a general worklist algorithm, where the 
operations on the worklist constitute an abstract data type, and its correct-
ness and complexity is established. Section 6.2 organises the worklist so that 
iteration takes place in reverse postorder and the Round Robin Algorithm is 
obtained as a special case. Section 6.3 then further identifies strong compo-
nents and iterates through each strong component in reverse postorder before 
considering the next. 

VIII 
Preface 
Appendices A and C review the concepts from partially ordered sets, graphs 
and regular expressions that are used throughout the book. Appendix B is 
more tutorial in nature since coinduction is likely to be a new concept for 
most readers. 
To help the reader when navigating through the book we provide a table of 
contents, a list of tables, and a list of figures at the front of the book and an 
index of the main concepts and an index of notation at the end of the book. 
The index of notation is divided into three parts: first the mathematical 
symbols (with an indication of where they take their parameters), then the 
notation beginning with a greek letter, and finally the notation beginning 
with a letter in the latin alphabet (regardless of the font used). The book 
concludes with a bibliography. 
Our notation is mostly standard and is explained when introduced. However, 
it may be helpful to point out that we will be using "iff" as an abbreviation 
for "if and only if", that we will be usingÂ· .. [ ... f--+ â¢â¢â¢ J to mean both syntactic 
substitution as well as update of an environment or store, and that we will 
be writing ... ---+fin â¢â¢â¢ for the set of finitary functions: these are the partial 
functions with a finite domain. We also use A-notation for functions when it 
improves the clarity of presentation: AX.Â·Â· . x ... stands for the unary function 
f defined by f(x) = ... xÂ·Â·Â·. 
Most proofs and some technical lemmas and facts are in small print in order 
to aid the reader in navigating through the book. 
How to teach from the book. The book contains more material 
than can be covered in a one semester course. The pace naturally depends 
on the background of the studentsj we have taught the course at different 
paces to students in their fourth year as well as to Ph.D.-students with a 
variety of backgrounds. Below we summarise our experiences on how many 
lectures are needed for covering the various parts of the bookj it supplements 
the guide-lines presented above for how to read the book. 
Two or three lectures should suffice for covering all of Chapter 1 and possibly 
some of the simpler concepts from Appendix A.l and A.2. 
Sections 2.1, 2.3 and 2.4 are likely to be central to any course dealing with 
data flow analysis. Three to four lectures should suffice for covering Sections 
2.1 to 2.4 but five lectures may be needed if the students lack the necessary 
background in operational semantics or lattice theory (Appendix A.l, A.2 
and parts of A.3). -
Sections 2.5 and 2.6 are more advanced. One or two 
lectures suffice for covering Section 2.5 but it is hard to pay justice to Section 
2.6 in less than two lectures. 
Four or five lectures should suffice for a complete treatment of both Chapter 
3 and Appendix Bj however, the concept of coinduction takes some time to 
get used to and should be explained more than once. 

Preface 
IX 
Four or five lectures should suffice for a complete treatment of Chapter 4 as 
well as Appendix A.4. 
About four lectures should suffice for a complete treatment of Chapter 5. 
Two lectures should suffice for Chapter 6 but three lectures may be needed 
if large parts of Appendix C need to be reviewed. 
We have always covered the appendices as an integrated part. of the other 
chapters; indeed, some of the foundations for partial orders are introduced 
gently in Chapter 1 and most of our students have had some prior exposure 
to partial orders, graphs and regular expressions. 
The book contains numerous exercises and several mini projects which are 
small projects dealing with practical or theoretical aspects of the develop-
ment. Many of the important links and analogies between the various chap-
ters are studied in the exercises and mini projects. Some of the harder exer-
cises are starred. 
Acknowledgements. We should like to thank Reinhard Wilhelm for 
his long and lasting interest in this project and for his many encouraging and 
constructive remarks. We have also profited greatly from discussions with 
Alan Mycroft, Mooly Sagiv and Helmut Seidl about their perspective on 
selected parts of the manuscript. Many other colleagues have influenced the 
writing of the book and we should like to thank them all; in particular Alex 
Aiken, Torben Amtoft, Patrick Cousot, Laurie Hendren, Suresh Jagannathan, 
Florian Martin, Barbara Ryder, Bernhard Steffen. We are grateful to Schloss 
Dagstuhl for having hosted two key meetings: a one-week meeting among the 
authors in March of 1997 (during which we discovered the soothing nature 
of Vangelis' 1492) and our advanced course in November of 1998. Warm 
thanks go to the many students attending the advanced course in Dagstuhl, 
and to the many students in Aarhus, London, Saarbriicken and Tel Aviv that 
have tested the book as it evolved ever so slowly. Finally, we should like to 
thank Alfred Hofmann at Springer for a very satisfactory contract and Rene 
Rydhof Hansen for his help in tuning the 1J\'IE;X commands. 
Official web page. Further information about the book is available at 
the web page http://www.daimi.au . dkrhrn/PPA/ppa. html. We intend to 
provide information about availability of the book, a list of misprints (initially 
empty), pointers to web based tools that can be used in conjunction with the 
book, and transparencies and other supplementary material as they become 
available. 
Aarhus and London 
August, 1999 
Flemming Nielson 
Hanne Riis Nielson 
Chris Hankin 

Contents 
1 Introduction 
1.1 
The Nature of Program Analysis 
1.2 Setting the Scene . . . . . . . . . 
1.3 Data Flow Analysis. . . . . . . . 
1.3.1 
The Equational Approach. 
1.3.2 
The Constraint Based Approach 
1.4 Constraint Based Analysis . 
1.5 Abstract Interpretation .. 
1.6 Type and Effect Systems. . 
1.6.1 
Annotated Type Systems 
1.6.2 
Effect Systems 
1.7 Algorithms ... 
1.8 Transformations 
Concluding Remarks 
Mini Projects 
Exercises .. 
2 Data Flow Analysis 
2.1 
Intraprocedural Analysis . . . . . . . . 
2.1.1 
Available Expressions Analysis 
2.1.2 
Reaching Definitions Analysis. 
2.1.3 
Very Busy Expressions Analysis 
2.1.4 
Live Variables Analysis ..... 
2.1.5 
Derived Data Flow Information. 
1 
1 
3 
5 
5 
8 
10 
13 
17 
18 
22 
25 
26 
29 
29 
31 
33 
33 
37 
41 
44 
47 
50 

XII 
Contents 
2.2 
Theoretical Properties . . . . . . . . . . . . . 
52 
2.2.1 
Structural Operational Semantics . . . 
52 
2.2.2 
Correctness of Live Variables Analysis 
57 
2.3 
Monotone Frameworks . 
63 
2.3.1 
Basic Definitions 
65 
2.3.2 
The Examples Revisited . 
68 
2.3.3 
A Non-distributive Example. 
70 
2.4 Equation Solving . . . . . . 
72 
2.4.1 
The MFP Solution . 
72 
2.4.2 
The MOP Solution . 
76 
2.5 
Interprocedural Analysis . . 
80 
2.5.1 
Structural Operational Semantics . 
83 
2.5.2 
Intraprocedural versus Interprocedural Analysis. 
86 
2.5.3 
Making Context Explicit. . . 
88 
2.5.4 
Call Strings as Context ... 
93 
2.5.5 
Assumption Sets as Context . 
97 
2.5.6 
Flow-Sensitivity versus Flow-Insensitivity 
99 
2.6 
Shape Analysis ............... 
102 
2.6.1 
Structural Operational Semantics . 
103 
2.6.2 
Shape Graphs . 
107 
2.6.3 
The Analysis 
113 
Concluding Remarks 
126 
Mini Projects 
130 
Exercises .. 
133 
3 Constraint Based Analysis 
139 
3.1 
Abstract O-CFA Analysis 
139 
3.1.1 
The Analysis ... 
141 
3.1.2 
Well-definedness of the Analysis 
148 
3.2 
Theoretical Properties .......... 
151 
3.2.1 
Structural Operational Semantics . 
151 
3.2.2 
Semantic Correctness 
156 
3.2.3 
Existence of Solutions 
160 

Contents 
3.2.4 
Coinduction versus Induction 
3.3 Syntax Directed O-CFA Analysis .. 
3.3.1 
Syntax Directed Specification 
3.3.2 
Preservation of Solutions . 
3.4 
Constraint Based O-CFA Analysis. 
3.4.1 
Preservation of Solutions 
3.4.2 
Solving the Constraints 
3.5 
Adding Data Flow Analysis . . 
3.5.1 
Abstract Values as Powersets 
3.5.2 
Abstract Values as Complete Lattices 
3.6 Adding Context Information ... 
3.6.1 
Uniform k-CFA Analysis. 
3.6.2 
The Cartesian Product Algorithm 
Concluding Remarks 
Mini Projects 
Exercises .. 
4 Abstract Interpretation 
4.1 
A Mundane Approach to Correctness. 
4.1.1 
Correctness Relations .. 
4.1.2 
Representation Functions 
4.1.3 
A Modest Generalisation 
4.2 
Approximation of Fixed Points 
4.2.1 
Widening Operators . 
4.2.2 
Narrowing Operators. 
4.3 
Galois Connections . . . . . . 
4.3.1 
Properties of Galois Connections 
4.3.2 
Galois Insertions ........ . 
4.4 Systematic Design of Galois Connections . 
4.4.1 
Component-wise Combinations 
4.4.2 
Other Combinations 
4.5 
Induced Operations ..... 
4.5.1 
Inducing along the Abstraction Function. 
XIII 
163 
166 
167 
169 
171 
173 
174 
180 
180 
183 
187 
189 
194 
196 
200 
203 
209 
209 
212 
214 
217 
219 
222 
228 
231 
237 
240 
244 
247 
251 
256 
256 

XIV 
4.5.2 
Application to Data Flow Analysis . . . . . 
4.5.3 
Inducing along the Concretisation Function 
Concluding Remarks 
Mini Projects 
Exercises .. 
5 Type and Effect Systems 
5.1 
Control Flow Analysis ....... . 
5.1.1 
The Underlying Type System 
5.1.2 
The Analysis ... 
5.2 Theoretical Properties . . 
5.2.1 
Natural Semantics 
5.2.2 
Semantic Correctness 
5.2.3 
Existence of Solutions 
Contents 
260 
265 
268 
272 
274 
281 
281 
282 
285 
289 
290 
292 
295 
5.3 Inference Algorithms . . . . . 
298 
5.3.1 
An Algorithm for the Underlying Type System 
298 
5.3.2 
An Algorithm for Control Flow Analysis . 
304 
5.3.3 
Syntactic Soundness and Completeness 
310 
5.3.4 
Existence of Solutions 
5.4 Effects............ 
5.4.1 
Side Effect Analysis 
5.4.2 
Exception Analysis. 
5.4.3 
Region Inference . . 
5.5 
Behaviours ........ . 
5.5.1 
Communication Analysis 
Concluding Remarks 
Mini Projects 
Exercises .. 
6 Algorithms 
6.1 
Worklist Algorithms 
6.1.1 
The Structure of Worklist Algorithms 
6.1.2 
Iterating in LIFO and FIFO. 
6.2 Iterating in Reverse Postorder .... 
315 
317 
317 
323 
328 
337 
337 
347 
351 
357 
363 
363 
366 
370 
372 

Contents 
6.2.1 
The Round Robin Algorithm . 
6.3 Iterating Through Strong Components 
Concluding Remarks 
Mini Projects 
Exercises .. 
A Partially Ordered Sets 
A.1 Basic Definitions .. 
A.2 Construction of Complete Lattices 
A.3 Chains .... 
AA Fixed Points 
Concluding Remarks 
B Induction and Coinduction 
B.1 Proof by Induction .... 
B.2 Introducing Coinduction 
B.3 Proof by Coinduction 
Concluding Remarks .... 
C Graphs and Regular Expressions 
C.1 Graphs and Forests. 
C.2 Reverse Postorder . 
C.3 Regular Expressions 
Concluding Remarks 
Index of Notation 
Index 
Bibliography 
xv 
376 
379 
382 
385 
387 
391 
391 
395 
396 
400 
402 
403 
403 
405 
409 
413 
415 
415 
419 
424 
425 
427 
431 
437 

List of Tables 
1.1 
Reaching Definitions information for the factorial program. 
4 
1.2 Reaching Definitions: annotated base types. . . . . 
18 
1.3 Reaching Definitions: annotated type constructors. 
1.4 Call-tracking Analysis: Effect System. . .. 
1.5 Chaotic Iteration for Reaching Definitions .. 
1.6 Constant Folding transformation. 
2.1 
Available Expressions Analysis. 
2.2 
Reaching Definitions Analysis. 
2.3 Very Busy Expressions Analysis. 
2.4 Live Variables Analysis. . .... 
2.5 
Semantics of expressions in WHILE .. 
2.6 The Structural Operational Semantics of WHILE. 
2.7 Constant Propagation Analysis. . . . . . . 
2.8 
Algorithm for solving data flow equations. 
2.9 Iteration steps of the worklist algorithm .. 
2.10 The instrumented semantics of WHILE ... 
20 
24 
25 
27 
38 
42 
45 
48 
53 
54 
71 
73 
74 
131 
3.1 
Abstract Control Flow Analysis (Subsections 3.1.1 and 3.1.2). 144 
3.2 The Structural Operational Semantics of FUN (part 1). ... 153 
3.3 The Structural Operational Semantics of FUN (part 2). ... 154 
3.4 Abstract Control Flow Analysis for intermediate expressions. 
156 
3.5 Syntax directed Control Flow Analysis. 
168 
3.6 Constraint based Control Flow Analysis. . 
3.7 Algorithm for solving constraints. . . . . . 
172 
176 

XVIII 
List of Tables 
3.8 Abstract values as powersets. . . . . 
182 
3.9 Abstract values as complete lattices. 
186 
3.10 Uniform k-CFA analysis. . . . . 
190 
5.1 
The Underlying Type System. . 
283 
5.2 
Control Flow Analysis. . . . 
286 
5.3 Equality of Annotations. . . 
288 
5.4 Natural Semantics for FUN. 
290 
5.5 
Algorithm WUL for the underlying type system. 
301 
5.6 
Unification of underlying types. . . . . . . . 
303 
5.7 Unification of simple types. . . . . . . . . . 
305 
5.8 Algorithm WCFA for Control Flow Analysis. 
308 
5.9 
Side Effect Analysis. . . . . . . . . . . . . . 
320 
5.10 Exception Analysis. ............. 
326 
5.11 Natural Semantics for extended expressions. . 
332 
5.12 Region Inference Analysis and Thanslation (part 1). . 
335 
5.13 Region Inference Analysis and Thanslation (part 2). . 
336 
5.14 The sequential semantics. . . . . . 
339 
5.15 The concurrent semantics. . . . . . 
341 
5.16 Communication Analysis (part 1). 
344 
5.17 Communication Analysis (part 2). 
345 
5.18 Ordering on behaviours. . . . . . . 
346 
6.1 
The Abstract Worklist Algorithm. 
367 
6.2 Iterating in last-in first-out order (LIFO). 
371 
6.3 Iterating in reverse postorder. . . . . . 
374 
6.4 The Round Robin Algorithm. . . . . . 
377 
6.5 
Pseudocode for constraint numbering. 
380 
6.6 Iterating through strong components. 
382 
C.1 The DFSF Algorithm ......... . 
419 

List of Figures 
1.1 The nature of approximation: erring on the safe side. . 
2 
1.2 Flow graph for the factorial program. . 
6 
1.3 The adjunction (0:, 1'). . . . . . . . 
15 
2.1 
Flow graph for the power program. 
2.2 
A schematic flow graph. . . . . . . 
2.3 A schematic flow graph (in reverse). 
2.4 Preservation of analysis result. .. . 
2.5 The correctness result for live . ... . 
36 
39 
46 
59 
60 
2.6 Instances for the four classical analyses. 
68 
2.7 Flow graph for the Fibonacci program. . 
83 
2.8 Analysis of procedure call: the forward case. . 
91 
2.9 Analysis of procedure call: ignoring calling context. . 
92 
2.10 Analysis of procedure call: merging of context. 
93 
2.11 Procedure call graph for example program. 
101 
2.12 Reversal of a list of five elements. . . . . . . 
104 
2.13 Shape graphs corresponding to Figure 2.12. 
110 
2.14 Sharing information. . . . . . . . . . . . . . 
111 
2.15 The single shape graph in the extremal value Â£ for the list 
reversal program. . . . . . . . . . 
113 
2.16 The effect of [x: =nn]i. ............. 
114 
2.17 The effect of [x: =y]l when x,#y. . . . . . . . . . 
116 
2.18 The effect of [x: =y.set]l in Case 2 when x ,#y. . 
118 
2.19 The effect of [x:=y.selJl in a special case (part 1). . 
121 
2.20 The effect of [x: =y.sel]l in a special case (part 2). . 
122 

XX 
List of Figures 
2.21 The effect of [x.sel:=nil)l when #into(nu, H') ::;1. 
124 
2.22 The effect of [x.sel: =y)l when #into(ny, H') < 1. 
125 
2.23 du-and ud-chains.. . . . . . . . . . . . . . . . . . 
130 
3.1 
Pictorial illustration of the clauses [let) and [var). 
146 
3.2 
Pictorial illustration ofthe clauses lapp), [fn) and [var). 
147 
3.3 
Preservation of analysis result. ............. 
157 
3.4 Initialisation of data structures for example program. . 
177 
3.5 
Iteration steps of example program. .......... 
178 
3.6 
Control Flow and Data Flow Analysis for example program.. 183 
4.1 
Correctness relation R generated by representation function (3. 216 
4.2 
The complete lattice Interval = (Interval, ~). 
219 
4.3 
Fixed points of f. . . . . . . . . . . . . . 
222 
4.4 The widening operator V' applied to f. . 
224 
4.5 
The narrowing operator 6. applied to/. 
4.6 The Galois connection (L, ex, ,,(, M) . ... 
4.7 The complete lattice P(Sign) = (P(Sign) , ~). 
4.8 The Galois insertion (L,ex,,,(,M) . .. 
4.9 
The reduction operator c; : M -+ M. 
4.10 The complete lattice (Sign', ~). .. 
229 
233 
236 
241 
243 
274 
5.1 
The memory model for the stack-based implementation of FUN.329 
5.2 
The pipeline produced by pipe [f1 , f2] inp out. ..... 338 
6.1 
Example: LIFO iteration. . . . . . . . . . . . . . . . . . .. 
372 
6.2 
(a) Graphical representation. (b) Depth-first spanning tree. 
374 
6.3 
Example: Reverse postorder iteration. . . . 
375 
6.4 Example: Round Robin iteration. . . . . . . 
377 
6.5 
(a) Strong components. (b) Reduced graph. 
380 
6.6 
Example: Strong component iteration. 
382 
A.l Two complete lattices. . .. 
A.2 Two partially ordered sets. 
A.3 Fixed points of f. . . . . . . 
392 
398 
401 

List of Figures 
C.1 A flow graph .............. . 
C.2 A DFSF for the graph in Figure C.l. . 
C.3 An irreducible graph .......... . 
XXI 
420 
421 
423 

Chapter 1 
Introduction 
In this book we shall introduce four of the main approaches to program 
analysis: Data Flow Analysis, Constraint Based Analysis, Abstract Interpre-
tation, and Type and Effect Systems. Each of Chapters 2 to 5 deals with one 
of these approaches at some length and generally treats the more advanced 
material in later sections. Throughout the book we aim at stressing the many 
similarities between what may at a first glance appear to be very unrelated 
approaches. To help to get this idea across, and to serve as a gentle intro-
duction, this chapter treats all of the approaches at the level of examples. 
The technical details are worked out but it may be difficult to apply the 
techniques to related examples until some of the material of later chapters 
has been studied. 
1.1 
The Nature of Program Analysis 
Program analysis offers static compile-time techniques for predicting safe 
and computable approximations to the set of values or behaviours arising 
dynamically at run-time when executing a program on a computer. A main 
application is to allow compilers to generate code avoiding redundant com-
putations, e.g. by reusing available results or by moving loop invariant com-
putations out of loops, or avoiding superfluous computations, e.g. of results 
known to be not needed or of results known already at compile-time. Among 
the more recent applications is the validation of software (possibly purchased 
from sub-contractors) to reduce the likelihood of malicious or unintended be-
haviour. Common for these applications is the need to combine information 
from different parts of the program. 
A main aim of this book is to give an overview of a number of approaches to 
program analysis, all of which have a quite extensive literature, and to show 

2 
true answer 
{d1,Â·Â·Â· ,dn } 
.. 
v 
v 
{d1,Â·Â·Â·, dn ,Â·Â·Â·, dn+m } 
safe answer 
{dn+1 ,Â·Â·Â·, dN} 
/ 
/ 
/ 
/ 
/ 
... 
1 Introduction 
, 
Figure 1.1: The nature of approximation: erring on the safe side. 
that there is a large amount of commonality among the approaches. This 
should help in cultivating the ability to choose the right approach for the 
right task and in exploiting insights developed in one approach to enhance 
the power of other approaches. 
One common theme behind all approaches to program analysis is that in 
order to remain computable one can only provide approximate answers. As 
an example consider a simple language of statements and the program 
read(x); (if x>O then y:=l else (y:=2;S)); z:=y 
where S is some statement that does not contain an assignment to y. Intu-
itively, the values of y that can reach z: =y will be 1 or 2. 
Now suppose an analysis claims that the only value for y that can reach z: =y 
is in fact 1. While this seems intuitively wrong, it is in fact correct in the 
case where S is known never to terminate for x :S 0 and y = 2. But since 
it is undecidable whether or not S terminates, we normally do not expect 
our analysis to attempt to detect this situation. So in general, we expect the 
program analysis to produce a possibly larger set of possibilities than what 
will ever happen during execution of the program. This means that we shall 
also accept a program analysis claiming that the values of y reaching z: =y 
are among 1, 2 or 27, although we will clearly prefer the analysis that gives 
the more precise answer that the values are among 1 or 2. This notion of 
safe approximation is illustrated in Figure 1.1. Clearly the challenge is not to 

1.2 Setting the Scene 
3 
produce the safe "{ d1 , ... ,dN}" too often as the analysis will then be utterly 
useless. Note, that although the analysis does not give precise information it 
may still give useful information: knowing that the value of y is one of 1, 2 
and 27 just before the assignment z: =y still tells us that z will be positive, 
and that z will fit within 1 byte of storage etc. To avoid confusion it may 
help to be precise in the use of terminology: it is better to say "the values 
of y possible at z: =y are among 1 and 2" than the slightly shorter and more 
frequently used "the values of y possible at z: =y are 1 and 2" . 
Another common theme, to be stressed throughout this book, is that all 
program analyses should be semantics based: this means that the information 
obtained from the analysis can be proved to be safe (or correct) with respect 
to a semantics of the programming language. It is a sad fact that new program 
analyses often contain subtle bugs, and a formal justification of the program 
analysis will help finding these bugs sooner rather than later. However, we 
should stress that we do not suggest that program analyses be semantics 
directed: this would mean that the structure of the program analysis should 
reflect the structure of the semantics and this will be the case only for a few 
approaches which are not covered in this book. 
1.2 
Setting the Scene 
Syntax of the WHILE language. We shall consider a simple im-
perative language called WHILE. A program in WHILE is just a statement 
which may be, and normally will be, a sequence of statements. In the interest 
of simplicity, we will associate data flow information with single assignment 
statements, the tests that appear in conditionals and loops, and skip state-
ments. We will require a method to identify these. The most convenient 
way of doing this is to work with a labelled program - as indicated in the 
syntax below. We will often refer to the labelled items (assignments, tests 
and skip statements) as elementary blocks. In this chapter we will assume 
that distinct elementary blocks are initially assigned distinct labels; we could 
drop this requirement, in which case some of the examples would need to be 
slightly reformulated and the resultant analyses would be less accurate. 
We use the following syntactic categories: 
a 
E AExp arithmetic expressions 
b 
E BExp boolean expressions 
S 
E Stmt 
statements 
We assume some countable set of variables is given; numerals and labels will 
not be further defined and neither will the operators: 
x, y 
E Var 
variables 
n 
E Num numerals 
f 
E Lab 
labels 

4 
1 Introduction 
Â£ 
RDentry(Â£) 
RDexit(Â£) 
1 
(x, ?),(y, ?),(z,?) 
(x, ?),(y,l),(z,?) 
2 
(x, ?),(y,l),(z,?) 
(x, ?), (y, 1), (z,2) 
3 
(x, ?),(y,1),(y,5),(z,2),(z,4) 
(x, ?),(y,1),(y,5),(z,2),(z,4) 
4 
(x, ?),(y,1),(y,5), (z,2), (z,4) 
(x, ?),(y,1),(y,5),(z,4) 
5 
(x, ?),(y,l), (y,5), (z,4) 
(x,?), (y,5), (z,4) 
6 
(x, ?), (y, 1), (y,5), (z,2),(z,4) 
(x, ?), (y,6), (z,2), (z,4) 
Table 1.1: Reaching Definitions information for the factorial program. 
0Pa 
E 
OPa arithmetic operators 
OPb 
E 
OPb 
boolean operators 
oPr 
E 
OPr relational operators 
The syntax of the language is given by the following abstract syntax: 
a 
b 
8 
.. -
x I n I a1 0Pa a2 
true I false I not b I b1 OPb b2 I a1 OPr a2 
[x := ali I [skip]i I 8 1 ; 82 I 
if [W then 81 else 82 I while [W do 8 
One way to think of the abstract syntax is as specifying the parse trees of 
the language; it will then be the purpose of the concrete syntax to provide 
sufficient information to enable unique parse trees to be constructed. In this 
book we shall not be concerned with concrete syntax: whenever we talk about 
some syntactic entity we will always be talking about the abstract syntax so 
there will be no ambiguity with respect to the form of the entity. We shall 
use a textual representation of the abstract syntax and to disambiguate it 
we shall use parentheses. For statements one often writes begin ... end or 
{ ... } for this but we shall feel free to use ( ... ). Similarly, we use brackets 
( ... ) to resolve ambiguities in other syntactic categories. To cut down on the 
number of brackets needed we shall use the familiar relative precedences of 
arithmetic, boolean and relational operators. 
Example 1.1 An example of a program written in this language is the 
following which computes the factorial of the number stored in x and leaves 
the result in z: 
Reaching Definitions Analysis. The use of distinct labels allows 
us to identify the primitive constructs of a program without explicitly con-
structing a flow graph (or flow chart). It also allows us to introduce a program 
analysis to be used throughout the chapter: Reaching Definitions Analysis, 
or as it should be called more properly, reaching assignments analysis: 

1.3 Data Flow Analysis 
An assignment (called a definition in the classical literature) of 
the form [x := a]l may reach a certain program point (typically 
the entry or exit of an elementary block) if there is an execution 
of the program where x was last assigned a value at Â£ when the 
program point is reached. 
5 
Consider the factorial program of Example 1.1. Here [y:=xl1 reaches the 
entry to [z:=112j to allow a more succinct presentation we shall say that 
(y,l) reaches the entry to 2. Also we shall say that (x,?) reaches the entry to 
2j here "?" is a special label not appearing in the program and it is used to 
record the possibility of an uninitialised variable reaching a certain program 
point. 
Full information about reaching definitions for the factorial program is then 
given by the pair RD = (RDentry, RDe:z;it) of functions in Table 1.1. Careful 
inspection of this table reveals that the entry and exit information agree for 
elementary blocks of the form [W whereas for elementary blocks of the form 
[x := a]l they may differ on pairs (x, Â£'). We shall come back to this when 
formulating the analysis in subsequent sections. 
Returning to the discussion of safe approximation note that if we modify 
Table 1.1 to include the pair (z,2) in RDentry(5) and RDe:Z;it(5) we still have 
safe information about reaching definitions but the information is more ap-
proximate. However, if we remove (z,2) from RDentry(6) and RDe:Z;it(6) then 
the information will no longer be safe - there exists a run of the factorial pro-
gram where the set {(x,?),(y,6),(z,4)} does not correctly describe the reaching 
definitions at the exit of label 6. 
1.3 
Data Flow Analysis 
In Data Flow Analysis it is customary to think of a program as a graph: the 
nodes are the elementary blocks and the edges describe how control might 
pass from one elementary block to another. Figure 1.2 shows the flow graph 
for the factorial program of Example 1.1. We shall first illustrate the more 
common equational approach to Data Flow Analysis and then a constraint 
based approach that will serve as a stepping stone to Section 1.4. 
1.3.1 
The Equational Approach 
The equation system. An analysis like Reaching Definitions can be 
specified by extracting a number of equations from a program. There are two 
classes of equations. One class of equations relate exit information of a node 
to entry information for the same node. For the factorial program 
[y:=xpj [z:=1Fj while [y>113 do ([z:=z*y14 j [y:=y-115)j [y:=016 

6 
1 Introduction 
I [Y:=X]lI 
! 
I [z:=1J2I 
! 
I [Y>1J31-=no~-+-'1 [y:=O]61 
!yes 
! 
I [z: =z*y]41 
! 
I [y: =y-1]51 
Figure 1.2: Flow graph for the factorial program. 
we obtain the following six equations: 
RDexit(I) 
RDexit (2) 
RDexit(3) 
RDexit(4) 
RDexit(5) 
RDexit(6) 
(RDentry(I)\{(y,Â£) 1Â£ E Lab}) U {(y, I)} 
(RD entry (2) \ {(z, Â£) I Â£ E Lab}) U {(z, 2)} 
RDentry(3) 
(RDentry(4)\{(z,Â£) 1Â£ E Lab}) U {(z,4)} 
(RDentry(5)\{(y,Â£) 1Â£ E Lab}) U {(y,5)} 
(RDentry(6)\{(y,Â£) 1Â£ E Lab}) U {(y,6)} 
These are instances of the following schema: for an assignment [x :=a]l' 
we exclude all pairs (x,Â£) from RDentry(Â£') and add (x,Â£') in order to obtain 
RDexit(Â£') - this reflects that x is redefined at Â£. For all other elementary 
blocks [ .. f we let RDexit (Â£') equal RDentry(Â£') - reflecting that no variables 
are changed. 
The other class of equations relate entry information of a node to exit in-
formation of nodes from which there is an edge to the node of interest; that 
is, entry information is obtained from all the exit information where control 
could have come from. For the example program we obtain the following 
equations: 
RDentry(2) 

1.3 Data Flow Analysis 
7 
RDentry(3) 
RDexit(2) U RDexit(5) 
RDentry(4) = RDexit(3) 
RDentry(5) = RDexit(4) 
RDentry(6) = RDexit(3) 
In general, we write RDentry(f) = RDexit(fl) U ... U RDexit(fn) if f1,Â·Â·Â·,fn 
are all the labels from which control might pass to f. We shall consider more 
precise ways of explaining this in Chapter 2. Finally, let us consider the 
equation 
RDentry(I) = {(x,?) I x is a variable in the program} 
that makes it clear that the label "?" is to be used for uninitialised variables; 
so in our case 
RDentry(I) = {(x, ?), (y, ?), (z,?)} 
The least solution. The above system of equations defines the twelve 
sets 
RDentry(I),Â·Â·Â·, RDexit(6) 
in terms of each other. Writing RO for this twelve-tuple of sets we can regard 
the equation system as defining a function F and demanding that: 
To be more specific we can write 
where e.g.: 
Fentry(3)(Â·Â·Â·, RDexu(2),Â·Â·Â·, RDexit(5),Â·Â·Â·) = RDexit(2) U RDexit(5) 
It should be clear that F operates over twelve-tuples of sets of pairs of vari-
ables and labels; this can be written as 
where it might be natural to take Var* = Var and Lab* = Lab. However, 
it will simplify the presentation in this chapter to let Var* be a finite subset 
of Var that contains the variables occurring in the program S* of interest 
and similarly for Lab*. So for the example program we might have Var* = 
{x,y,z} and Lab* = {I"" ,6, ?}. 
It is immediate that (P(Var* x Lab*))12 can be partially ordered by setting 
RO !; RO' iff Vi: RDi ~ RD~ 

8 
1 Introduction 
where Rj) = (RD1,"', RD12) and similarly Rj)1 = (RD~,Â·Â·Â·, RD~2)' This 
turns (P(Var* x Lab*Â»12 into a complete lattice (see Appendix A) with 
least element 
0= (0",Â·,0) 
and binary least upper bounds given by: 
It is easy to show that P is in fact a monotone function (see Appendix A) 
meaning that: 
Rj) !:;; Rj)1 implies P(Rj)!:;; p(Rj)/) 
This involves calculations like 
RDexit(2) ~ RD~xit(2) and RDexit(5) ~ RD~xit(5) 
imply 
RDexit(2) U RDexit(5) ~ RD~xit(2) U RD~xit(5) 
and the details are left to the reader. 
Consider the sequence (pn(0Â»n and note that 0 !:;; P(0). Since P is mono-
tone, a straightforward mathematical induction (see Appendix B) gives that 
pn(0) !:;; pn+l(0) for all n. All the elements of the sequence will be in 
(P(Var* x Lab*Â»12 and since this is a finite set it cannot be the case that 
all elements of the sequence are distinct so there must be some n such that: 
pn+1(0) = pn(0) 
But since pn+1(0) = F(pn(0Â» this just says that pn(0) is a fixed point of P 
and hence that pn(0) is a solution to the above equation system. 
In fact we have obtained the least solution to the equation system. To see 
this suppose that Rj) is some other solution, i.e. RD = P(RD). Then a 
straightforward mathematical induction shows that pn(0) !:;; RD. Hence 
the solution pn(0) contains the fewest pairs of reaching definitions that is 
consistent with the program, and intuitively, this is also the solution we want: 
while we can add additional pairs of reaching definitions without making 
the analysis semantically unsound, this will make the analysis less usable as 
discussed in Section 1.1. In Exercise 1.7 we shall see that the least solution 
is in fact the one displayed in Table 1.1. 
1.3.2 
The Constraint Based Approach 
The constraint system. An alternative to the equational approach 
above is to use a constraint based approach. The idea is here to extract a 
number of inclusions (or inequations or constraints) out of a program. We 

1.3 Data Flow Analysis 
9 
shall present the constraint system for Reaching Definitions in such a way 
that the relationship to the equational approach becomes apparent; however, 
it is not a general phenomenon that the constraints are naturally divided into 
two classes as was the case for the equations. 
For the factorial program 
[y:=x]l; [z:=lF; while [y>l]3 do ([z:=z*y]4; [y:=y-1P); [y:=O]6 
we obtain the following constraints for expressing the effect of elementary 
blocks: 
RDexit(I) 
::J 
RDentry(I)\{(y,Â£) 1Â£ E Lab} 
RDexit(I) 
::J 
{(y, I)} 
RDexit(2) 
::J 
RDentry(2)\{(z,Â£) 1Â£ E Lab} 
RDexit (2) 
::J 
{(z,2)} 
RDexit(3) 
::J 
RDentry(3) 
RDexit(4) 
::J 
RDentry(4)\{(z,Â£) 1Â£ E Lab} 
RDexit(4) 
::J 
{(z,4)} 
RDexit(5) 
::J 
RDentry(5)\{(y,Â£) 1Â£ E Lab} 
RDexit(5) 
::J 
{(y,5)} 
RDexit(6) 
::J 
RDentry(6)\{(y,Â£) 1Â£ E Lab} 
RDexit(6) 
::J 
{(y,6)} 
By considering this system a certain methodology emerges: for an assignment 
[x := at we have one constraint that excludes all pairs (x, Â£) from RDentry (Â£') 
in reaching RDexit(C') and we have one constraint for incorporating (x, C'); 
for all other elementary blocks [ ... Jf' we just have one constraint that allows 
everything in RDentry (C') to reach RDexit(Â£'). 
Next consider the constraints for more directly expressing how control may 
flow through the program. For the example program we obtain the con-
straints: 
RDentry(2) 
::J 
RDexit(1) 
RDentry(3) 
::J 
RDexit(2) 
RDentry(3) 
::J 
RD exit (5) 
RDentry(5) 
::J 
RD exit (4) 
RDentry(6) 
::J 
RDexit(3) 
In general, we have a constraint RDentry(Â£) ~ RDexit (Â£') if it is possible for 
control to pass from Â£' to Â£. Finally, the constraint 
RDentry(I) ~ {(x, ?), (y, ?), (z,?)} 
records that we cannot be sure about the definition point of uninitialised 
variables. 

10 
1 Introduction 
The least solution revisited. It is not hard to see that a solution 
to the equation system presented previously will also be a solution to the 
above constraint system. To make this connection more transparent we can 
rearrange the constraints by collecting all constraints with the same left hand 
side. This means that for example 
RD exit (1) 
::J 
RDexit(I) 
::J 
will be replaced by 
RDentry(I)\{(y,i) Ii E Lab} 
{(y, I)} 
RDexit(I) 2 (RDentry(I)\{(y,i) Ii E Lab}) U {(y, I)} 
and clearly this has no consequence for whether or not RD is a solution. In 
other words we obtain a version of the previous equation system except that 
all equalities have been replaced by inclusions. Formally, whereas the equa-
tional approach demands that RD = F(RD), the constraint based approach 
demands that RD ;;;! F(RD) for the same function F. It is therefore immedi-
ate that a solution to the equation system is also a solution to the constraint 
system whereas the converse is not necessarily the case. 
Luckily we can show that both the equation system and the constraint system 
have the same least sl!!ution. Recall that the least solu!ion to RD =: F(RD) 
is constructed as Fn(0) for a value of n such that Fn(0) = Fn+1(0). If RD 
is a solution to the constraint system, that is RD ;;;! F(RD), then 0!; RD is 
immediate and the monotonicity of F and mathematical induction then gives 
Fn(0) !; RD. Since Fn(0) is a solution to the constraint system this shows 
that it is also the least solution to the constraint system. 
In summary, we have thus seen a very strong connection between the equa-
tional approach and the constraint based approach. This connection is not 
always as apparent as it is here: one of the characteristics of the constraint 
based approach is that often constraints with the same left hand side are gen-
erated at many different places in the program and therefore it may require 
serious work to collect them. 
1.4 
Constraint Based Analysis 
The purpose of Control Flow Analysis is to determine information about 
what "elementary blocks" may lead to what other "elementary blocks". This 
information is immediately available for the WHILE language unlike what is 
the case for more advanced imperative, functional and object-oriented lan-
guages. Often Control Flow Analysis is expressed as a Constraint Based 
Analysis as will be illustrated in this section. 
Consider the following functional program: 

1.4 Constraint Based Analysis 
11 
let 
f = fn x => x 1; 
g 
fn y => y+2; 
h 
fn z => z+3 
in 
(f g) + (f h) 
It defines a higher-order function f with formal parameter x and body x 1; 
then it defines two functions g and h that are given as actual parameters to 
f in the body of the let-construct. Semantically, x will be bound to each 
of these two functions in turn so both g and h will be applied to 1 and the 
result of the computation will be the value 7. 
An application of f will transfer control to the body of f, i.e. to x 1, and 
this application of x will transfer control to the body of x. The problem is 
that we cannot immediately point to the body of x: we need to know what 
parameters f will be called with. This is exactly the information that the 
Control Flow Analysis gives us: 
For each function application, which functions may be applied. 
As is typical of functional languages, the labelling scheme used would seem 
to have a very different character than the one employed for imperative lan-
guages because the "elementary blocks" may be nested. We shall therefore 
label all sub expressions as in the following simple program that will be used 
to illustrate the analysis. 
Example 1.2 Consider the program: 
It calls the identity function fn x => x on the argument fn y => Y and 
clearly evaluates to fn y => Y itself (omitting all [ .. Â·le). 
_ 
We shall now be interested in associating information with the labels them-
selves, rather than with the entries and exits of the labels - thereby we exploit 
the fact that there are no side-effects in our simple functional language. The 
Control Flow Analysis will be specified by a pair (, p) of functions where 
Â«(f) is supposed to contain the values that the sub expression (or "elemen-
tary block") labelled f may evaluate to and p(x) contain the values that the 
variable x can be bound to. 
The constraint system. One way to specify the Control Flow Anal-
ysis then is by means of a collection of constraints and we shall illustrate this 
for the program of Example 1.2. There are three classes of constraints. One 
class of constraints relate the values of function abstractions to their labels: 
{fn x => [xll} ~ Â«(2) 
{fn y => [yP} ~ Â«(4) 

12 
1 Introduction 
These constraints state that a function abstraction evaluates to a closure 
containing the abstraction itself. So the general pattern is that an occurrence 
of [fn x => e]l in the program gives rise to a constraint {fn x => e} ~ Â«( Â£). 
The second class of constraints relate the values of variables to their labels: 
p(x) ~ Â«(1) 
p(y) ~ Â«(3) 
The constraints state that a variable always evaluates to its value. So for 
each occurrence of [xJl in the program we will have a constraint p(x) ~ Â«(Â£). 
The third class of constraints concerns function application: for each applica-
tion point [e! e2]l, and for each possible function [fn x => eJl' that could be 
called at this point, we will have: (i) a constraint expressing that the formal 
parameter of the function is bound to the actual parameter at the application 
point, and (ii) a constraint expressing that the result obtained by evaluating 
the body of the function is a possible result of the application. 
Our example program has just one application [[ ... ]2 [ ... ]4]5, but there are 
two candidates for the function, i.e. Â«(2) is a subset of the set {fn x => [xj1, 
fn y => [yj3}. If the function fn x => [x]! is applied then the two con-
straints are Â«(4) ~ p(x) and Â«(1) ~ Â«(5). We express this as conditional 
constraints: 
{fn x => [xj1} ~ Â«(2) => Â«(4) ~ p(x) 
{fn x => [x]!} ~ Â«(2) => Â«(1) ~ Â«(5) 
Alternatively, the function being applied could be fn y => [yj3 and the cor-
responding conditional constraints are: 
{fn y => [yJ3} ~ Â«(2) => Â«(4) ~ p(y) 
{fn y => [yJ3} ~ Â«(2) => Â«(3) ~ Â«(5) 
The least solution. As in Section 1.3 we shall be interested in the 
least solution to this set of constraints: the smaller the sets of values given 
by ( and p, the more precise the analysis is in predicting which functions are 
applied. In Exercise 1.2 we show that the following choice of ( and p gives a 
solution to the above constraints: 
Â«(1) 
{fn y => [y]3} 
Â«(2) 
{fn x => [x]!} 
Â«(3) 
0 
Â«(4) 
{fn y => [yj3} 
Â«(5) 
{fn y => [yj3} 
p(x) 
{fn y => [yj3} 
p(y) 
0 

1.5 Abstract Interpretation 
13 
Among other things this tells us that the function abstraction in y => Y is 
never applied (since jJ(y) = 0) and that the program may only evaluate to 
the function abstraction in y => Y (since ((5) = {in y => [yj3}). 
Note the similarities between the constraint based approaches to Data Flow 
Analysis and Constraint Based Analysis: in both cases the syntactic structure 
of the program gives rise to a set of constraints whose least solution is desired. 
The main difference is that the constraints for the Constraint Based Analysis 
have a more complex structure than those for the Data Flow Analysis. 
1.5 
Abstract Interpretation 
The theory of Abstract Interpretation is a general methodology for calculat-
ing analyses rather than just specifying them and then relying on a posteriori 
validation. To some extent the application of Abstract Interpretation is in-
dependent of the specification style used for presenting the program analysis 
and so applies not only to the Data Flow Analysis formulation to be used 
here. 
Collecting semantics. As a preliminary step we shall formulate a so-
called collecting semantics that records the set of traces tr that can reach a 
given program point: 
tr E Trace = (Var x Lab)' 
Intuitively, a trace will record where the variables have obtained their values 
in the course of the computation. So for the factorial program 
we will for example have the trace 
((x, ?), (y, ?), (z, ?), (y, 1), (z, 2), (z, 4), (y, 5), (z, 4), (y, 5), (y, 6)) 
corresponding to a run of the program where the body of the While-loop is 
executed twice. 
The traces contain sufficient information that we can extract a set of seman-
tically reaching definitions: 
SRD(tr)(x) = f 
iff 
the rightmost pair (x,f') in tr has f = f' 
In order for the Reaching Definitions Analysis to be correct (or safe) we shall 
require that it captures the semantic reaching definitions, that is, if tr is a 
possible trace just before entering the elementary block labelled f then we 
shall demand that 
't/x E Var: (x,SRD(tr)(x)) E RDentry(f) 

14 
1 Introduction 
in order to trust the information in RDentry(C) about the set of definitions 
that may reach the entry to C. In later chapters, we will conduct proofs of 
results like this. 
The collecting semantics will specify a superset of the possible traces at the 
various program points. We shall specify the collecting semantics CS in the 
style of the Reaching Definitions Analysis in Section 1.3; more precisely, we 
shall specify a twelve-tuple of elements from (P(Thace))12 by means of a set 
of equations. First we have 
CS exit(l) 
CSexit (2) 
CSexit(3) 
CSexit(4) 
CSexit(5) 
CSexit (6) 
{tr: (y,l) I tr E CSentry(l)} 
{tr : (z,2) I tr E CSentry (2)} 
CSentry(3) 
{tr : (z,4) I tr E CSentry (4)} 
{tr : (y, 5) I tr E CSentry (5)} 
{tr: (y,6) I tr E CSentry(6)} 
showing how the assignment statements give rise to extensions of the traces. 
Here we write tr : (x, C) for appending an element (x, C) to a list tr, that 
is ((Xl'C1)"", (xn,Cn)) : (X, C) equals ((Xl,C1)"", (xn,Cn), (X, C)). Further-
more, we have 
CSentry(2) 
CSexit (l) 
CSentry(3) 
CSexit(2) U CSexit(5) 
CSentry (4) 
CSexit(3) 
CSentry(5) = CSexit(4) 
CSentry (6) 
CSexit (3) 
corresponding to the flow of control in the program; more detailed infor-
mation about the value of the variables would allow us to define the sets 
CSentry(4) and CSentry(6) more precisely but the above definitions are suffi-
cient for illustrating the approach. Finally, we take 
CSentry(l) 
= 
{((x, ?), (y, ?), (z, ?))} 
corresponding to the fact that all variables are uninitialised in the beginning. 
In the manner of the previous sections we can rewrite the above system of 
equations in the form 
where B is a twelve-tuple of elements from (P(Thace))12 and where G is a 
monotone function of functionality: 
G: (P(Thace))12 -+ (P(Thace))12 

1.5 Abstract Interpretation 
15 
Figure 1.3: The adjunction (a,,),). 
As is explained in Appendix A there is a body of general theory that ensures 
that the equation in fact has a least solution; we shall write it as Ifp(G). 
However, since (P(Trace))12 is not finite we cannot simply use the methods 
of the previous sections in order to construct Ifp( G) . 
Galois connections. As we have seen the collecting semantics operates 
on sets of traces whereas the Reaching Definitions Analysis operates on sets of 
pairs of variables and labels. To relate these "worlds" we define an abstraction 
function a and a concretisation function ')' as illustrated in: 
')' --
--+-
P(Trace) 
P(Var x Lab) 
a 
The idea is that the abstraction junction a extracts the reachability informa-
tion present in a set of traces; it is natural to define 
a(X) = {(x, SRD(tr)(x)) I x E Var 1\ tr EX} 
where we exploit the notion of semantically reaching definitions. 
The concretisation junction ')' then produces all traces tr that are consistent 
with the given reachability information: 
')'(Y) = {tr I 'Ix E Var: (x, SRD(tr) (x)) E Y} 
Often it is demanded that a and')' satisfy the condition 
a(X) ~ Y Â¢} X ~ ')'(Y) 
and we shall say that (a, ')') is an adjunction, or a Galois connection, whenever 
this condition is satisfied; this is illustrated on Figure 1.3. We shall leave it 
to the reader to verify that (a, ')') as defined above does in fact fulfil this 
condition. 

16 
1 Introduction 
Induced analysis. We shall now show how the collecting semantics 
can be used to calculate (as opposed to "guess") an analysis like the one in 
Section 1.3; we shall say that the analysis is an induced analysis. For this we 
define 
O7(Xl'Â·Â·Â·' X12) 
= 
(a(Xd,Â·Â·Â·, a(X12)) 
1(Y1,Â·Â·Â·, Yd = 
(-y(Y1 ),Â·Â·Â· ,1'(Y12 )) 
where a and l' are as above and we consider the function a 0 G 0 1 of func-
tionality: 
(a 0 G 0 1) : (P(Var x Lab))12 -+ (P(Var x Lab))12 
This function defines a Reaching Definitions analysis in an indirect way. Since 
G is specified by a set of equations (over P(Trace)) we can use 070 G 0 1 to 
calculate a new set of equations (over P(Var x Lab)). We shall illustrate 
this for one of the equations: 
(Sexit (4) = {tr : (z,4) I tr E (Sentry (4)} 
The corresponding clause in the definition of Gis: 
G exit( 4)(Â· .. , (Sentry (4), ... ) = {tr : (z,4) I tr E (Sentry (4)} 
We can now calculate the corresponding clause in the definition of a 0 G 0 1: 
a( G exit ( 4) (1(Â· .. , RDentry (4), ... ))) 
= a({tr: (z,4) I tr E 1'(RDentry(4))}) 
= {(x,SRD(tr: (z,4))(x)) 
I x E Var, Vy E Var : (y, SRD(tr)(y)) E RDentry(4)} 
= {(x, SRD(tr)(x)) 
I x E Var,x =I z, (x, SRD(tr)(x)) E RDentry(4)} U {(z,4)} 
= (RDentry(4) \ ((z,Â£) 1Â£ E Lab}) U {(z,4)} 
This clause is equivalent to the equation for RDexit(4) in Section 1.3; similar 
calculations can be performed for the other clauses. 
The least solution. As explained in Appendix A the equation system 
RD = (07oGo1)(RD) 
has a least solution; we shall write it as Ifp(07 0 G 0 1). It is interesting to 
note that if one replaces the infinite sets Var and Lab with finite sets Var* 
and Lab* as before, then the least fixed point of a 0 G 0 1 can be obtained 
as (a 0 G 0 1)n(0) just as was the case for F previously. 
In Exercise 1.4 we shall show that 070 G 0 1 ~ F and that O7(Gn(0)) c: 
(a 0 G 0 1)n(0) ~ Fn(0) holds for all n. In fact it will be the case that 
O7(1fp(G)) ~ Ifp(07 0 G 0 1) ~ Ifp(F) 

1.6 Type and Effect Systems 
17 
and this just says that the least solution to the equation system defined by 
a. 0 G 0 1 is correct with respect to the collecting semantics, and similarly that 
the least solution to the equation system of Section 1.3 is also correct with 
respect to the collecting semantics. Thus it follows that we will only need to 
show that the collecting semantics is correct - the correctness of the induced 
analysis will follow for free. 
For some analyses one is able to prove the stronger result a. 0 G 0 1 = F. 
Then the analysis is optimal (given the choice of approximate properties it 
operates on) and clearly Ifp(a. 0 G 0 1) = Ifp(F). In Exercise 1.4 we shall 
study whether or not this is the case here. 
1.6 
Type and Effect Systems 
A simple type system. The ideal setting for explaining Type and 
Effect Systems is to consider a typed functional or imperative language. 
However, even our simple toy language can be considered to be typed: a 
statement 8 maps a state to a state (in case it terminates) and may therefore 
be considered to have type 1:: -t 1:: where 1:: denotes the type of states; we 
write this as the judgement: 
One way to formalise this is by the following utterly trivial system of axioms 
and inference rules: 
[x := all : 1:: -t :E 
[skip]l : 1:: -t 1:: 
81 : 1:: -t 1:: 
82 : 1:: -t 1:: 
81; 82 : 1:: -t 1:: 
if [b]l then 81 else 82 : 1:: -t 1:: 
8::E-t:E 
while [b]l do 8 : 1:: -t 1:: 
Often a Type and Effect System can be viewed as the amalgamation of two 
ingredients: an Effect System and an Annotated Type System. -In an Effect 
System we typically have judgements of the form S : :E ~ 1:: where the effect 
'P tells something about what happens when 8 is executed: for example this 
may be which errors might occur, which exceptions might be raised, or which 
files might be modified. In an Annotated Type System we typically have 
judgements of the form 8 : 1::1 -t :E2 where the 1::i describe certain properties 
of states: for example this may be that a variable is positive or that a certain 

18 
[ass] 
[skip] 
[seq] 
[ ij) 
[wh] 
[sub] 
1 Introduction 
[x := at : RD -t ((RD\{(x,Â£) 1Â£ E Lab}) U {(x,Â£')}) 
[skiPY' : RD -t RD 
81 : RD1 -t RD2 
82 : RD2 -t RD3 
81; 82 : RD1 -t RD3 
81 : RD1 -t RD2 
82 : RD1 -t RD2 
if [W then 81 else 82 : RD1 -t RD2 
8: RD -t RD 
while [W do 8 : RD -t RD 
8: RD2 -t RD3 if RD1 ~ RD2 and RD3 ~ RD4 
8: RD1 -t RD4 
Table 1.2: Reaching Definitions: annotated base types. 
invariant is maintained. We shall first illustrate the latter approach for the 
WHILE language and then illustrate the Effect Systems using the functional 
language. 
1.6.1 
Annotated Type Systems 
Annotated base types. To obtain our first specification of Reaching 
Definitions we shall focus on a formulation where the base types are anno-
tated. Here we will have judgements of the form 
8: RD1 -t RD2 
where RD1, RD2 E P(Var x Lab) are sets of reaching definitions. Based 
on the trivial axioms and rules displayed above we then obtain the more 
interesting ones in Table 1.2. 
To explain these rules let us first explain the meaning of 8: RD1 -t RD2 in 
terms of the developments performed in Section 1.3. For this we first observe 
that any statement 8 will have one elementary block at its entry, denoted 
init(8), and one or more elementary blocks at its exit, denoted fina1(S); for a 
statement like if [X<y]1 then [x: =y]2 else [y: =x]3 we thus get init(Â· .. ) = 1 
and fina1(Â·Â·Â·) = {2,3}. 
Our first (and not quite successful) attempt at explaining the meaning of 
8 : RD1 -t RD2 then is to say that: 
RD1 = RDentry(init(S)) 
U{RDexit(Â£) 1Â£ E fina1(S)} = RD2 

1.6 Type and Effect Systems 
19 
This suffices for explaining the axioms for assignment and skip: here the 
formulae after the arrows correspond exactly to the similar equations in the 
equational formulation of the analysis in Section 1.3. Also the rule for se-
quencing now seems rather natural. However, the rule for conditional is more 
dubious: considering the statement if [X<y]l then [x: =y]2 else [y: =xj3 once 
more, it seems impossible to achieve that the then-branch gives rise to the 
same set of reaching definitions as the else-branch does. 
Our second (and successful) attempt at explaining the intended meaning of 
S : RDI -+ RD2 then is to say that: 
RDI ~ RDentry(init(S)) 
ve E finaJ(S) : RDexit(e) ~ RD2 
This formulation is somewhat closer to the development in the constraint 
based formulation of the analysis in Section 1.3 and it explains why the last 
rule, called a subsumption rule, is unproblematic. Actually, the subsumption 
rule will solve our problem with the conditional because even when the then-
branch gives a different set of reaching definitions than the else-branch we 
can enlarge both results to a common set of reaching definitions. Finally, 
consider the rule for the iterative construct. Here we simply express that RD 
is a consistent guess concerning what may reach the entry and exits of S -
this expresses a fixed point property. 
Example 1.3 To analyse the factorial program 
of Example 1.1 we will proceed as follows. We shall write RDf for the set 
{(x, 7), (y, 1), (y, 5), (z, 2), (z, 4)} and consider the body of the while-loop. 
The axiom [ass] gives 
[z: =z*y]4: RDf -+ {(x, 7), (y, 1), (y, 5), (z, 4)} 
[y:=y-l]5: {(x, 7), (y, 1), (y, 5), (z,4)} -+ {(x, 7), (y, 5), (z,4)} 
so the rule [seq] gives: 
([Z:=Z*y]4; [y:=y-l]5): RDf -+ {(x, 7), (y,5), (z,4)} 
Now {(x, 7), (y, 5), (z, 4)} ~ RDf so the subsumption rule gives: 
We can now apply the rule [wh] and get: 

20 
1 Introduction 
[ass] 
[x:= all : ~ {(~~})}) ~ 
[skip] 
[skip]l: ~ -+ ~ 
[seq] 
[ ifl 
[wh] 
[sub] 
81 : ~ ~ 
~ 
82 : ~ ~ 
~ 
8 .8 . '" 
X1UX 2 
) 
'" 
1, 2Â·.0 (RDl \X2) URD2 
.0 
81 : ~ ;611 ) ~ 8 2 : ~ ;622) 
~ 
if [b]l then 81 else 8 2 : ~ R~~8:62) ~ 
8: ~ ;6) ~ 
while [by do 8: ~ ~D) ~ 
8:~+O+~ 
x' 
8: ~ RD/) 
~ 
if X' ~ X and RD ~ RD' 
Table 1.3: Reaching Definitions: annotated type constructors. 
Using the axiom [ass] we get: 
[Y:=X]1: {(x, ?), (y, ?), (z,?)} --t {(x, ?), (y, 1), (z,?)} 
[z:=1]2: {(x, ?), (y, 1), (z,?)} --t {(x, ?), (y, 1), (z,2)} 
[Y:=0]6: RDf --t {(x, ?), (y,6), (z,2), (z,4)} 
Since {(x, ?), (y, 1), (z, 2)} ~ RDf we can apply the rules [seq] and [sub] to get 
([y:=x]\ [z:=1j2; while [y>1]3 do ([z:=Z*y]4; [y:=y-1]5); [y:=0]6): 
{(x, ?),(y, ?),(z, ?)} --t {(x, ?),(y,6),(z,2),(z,4)} 
corresponding to the result in Table 1.1. 
â¢ 
The system in Table 1.2 suffices for manually analysing a given program. To 
obtain an implementation it will be natural to extract a set of constraints 
similar to those considered in Section 1.3, and then solve them in the same 
way as before. This will be the idea behind the approach taken in Chapter 5. 
Annotated type constructors. Another approach to Reaching Def-
initions has a little bit of the flavour of Effect Systems in that it is the type 
constructors (arrow in our case) that are annotated. Here we will have judge-
ments of the form 
x 
8:~----t~ 
RD 

1.6 Type and Effect Systems 
21 
where X denotes the set of variables that definitely will be assigned in 8 and 
RD denotes the set of reaching definitions that 8 might produce. The axioms 
and rules are shown in Table 1.3 and are explained below. 
The axiom for assignment simply expresses that the variable x definitely will 
be assigned and that the reaching definition (x, C) is produced. In the rule 
for sequencing the notation RD \ X means {(x,C) E RD I x <I X}. The 
rule expresses that we take the union of the reaching definitions after having 
removed entries from 8 1 that are definitely redefined in 82 . Also we take the 
union of the two sets of assigned variables. In the rule for conditional we 
take the union of information about reaching definitions whereas we take the 
intersection (rather than the union) of the assigned variables because we are 
not completely sure what path was taken through the conditional. A similar 
comment holds for the rule for the while-loop; here we can think of 0 as the 
intersection between 0 (when the body is not executed) and X. 
We have included a subsumption rule because this is normally the case for 
such systems as we shall see in Chapter 5. However, in the system above there 
is little need for it, and if one excludes it then implementation becomes very 
straightforward: simply perform a syntax directed traversal of the program 
where the sets X and RD are computed for each subprogram. 
Example 1.4 Let us once again consider the analysis of the factorial pro-
gram 
For the body of the while-loop we get 
so the rule [seq] gives: 
[z: =Z*y]4: ~ {(~:l)}) ~ 
[y :=y-1]5: ~ 
{y}) ~ 
{(y,5)} 
([z:=z*y]4; [y:=y-1]5): ~ 
{y,z}) ~ 
{(y,5),(z,4)} 
We can now apply the rule [wh] and get: 
while [y>1]3 do ([z:=Z*y]4; [y:=y-1]5): ~ 
0 
) ~ 
{(y,5),(z,4)} 
In a similar way we get 
([Y:=X]l; [z:=1j2): ~ 
{y,z}) ~ 
{(y,1),(z,2)} 
[y: =0]6: ~ {(~~J)}) ~ 
so using the rule [seq] we obtain 

22 
1 Introduction 
([Y:=X]1 j [z:=lJ2j while [y>1]3 do ([z:=z*y]4j [y:=y-1]5)j [Y:=0]6): 
E 
{y,z} 
) E 
{(y,6) ,(z,2) ,(z,4)} 
showing that the program definitely will assign to y and z and that the final 
value of y will be assigned at 6 and the final value of z at 2 or 4. 
â¢ 
Compared with the previous specifications of Reaching Definitions analy-
sis the flavour of Table 1.3 is rather different: the analysis of a statement 
expresses how information present at the entry will be modified by the state-
ment - we may therefore view the specification as a higher-order formulation 
of Reaching Definitions analysis. 
1.6.2 
Effect Systems 
A simple type system. To give the flavour of Effect Systems let 
us once more turn to the functional language. As above, the idea is to 
annotate a traditional type system with analysis information, so let us start 
by presenting a simple type system for a language with variables x, function 
abstraction fn,.. x => e (where 7r is the name of the abstraction), and function 
application e1 e2. The judgements have the form 
rf-e:7 
where r is a type environment that gives types to all free variables of e and 
7 is the type of e. For simplicity we shall assume that types are either base 
types such as int and bool or they are function types written 71 -+ 72. The 
type system is given by the following axioms and rules: 
r f- x: 7", 
if r(x) = 7", 
r f- fn,.. x => e : 7", -+ 7 
r f- el : 72 -+ 7, r f- e2 : 72 
r f- el e2 : 7 
So the axiom for variables just expresses that the type of x is obtained from 
the assumptions of the type environment. The rule for function abstrac-
tion requires that we "guess" a type 7", for the formal parameter x and we 
determine the type of the body of the abstraction under that additional as-
sumption. The rule for function application requires that we determine the 
type of the operator as well as the argument and it implicitly expresses that 
the operator must have a function type by requiring the type of el to have 
the form 72 -+ 7. Furthermore the two occurrences of 72 in the rule implicitly 

1.6 Type and Effect Systems 
23 
express that the type of the actual parameter must equal the type expected 
by the formal parameter of the function. 
Example 1.5 Consider the following version of the program of Exam-
ple 1.2 
(fnx x => x) (fny y => y) 
where we now have given fn x => x the name X and fn y => Y the name 
Y. To see that this program has type int -+ int we first observe that 
[y 1-+ int] I- y : int so: 
[ ]1- fny y => y: int -+ int 
Similarly, we have [x 1-+ int -+ int] I- x : int -+ int so: 
[ ]1- fnx x => x: (int -+ int) -+ (int -+ int) 
The rule for application then gives: 
[ ]1- (fnx x => x) (fny y => y) : int -+ int 
Effects. The analysis we shall consider is a Call-Tracking Analysis: 
For each sub expression, which function abstractions may be ap-
plied during its evaluation. 
â¢ 
The set of function names constitutes the effect of the subexpression. To 
determine this information we shall annotate the function types with their 
latent effect so for example we shall write int ~ int for the type of a 
function mapping integers to integers and with effect {X} meaning that when 
executing the function it may apply the function named X. More generally, 
the annotated types r will either be base types or they will have the form 
where <p is the effect, i.e. the names of the function abstractions that we 
might apply when applying a function of this type. 
We specify the analysis using judgements of the form 
rl-e:r&<p 
where r is the type environment that now gives the annotated type of all 
free variables, r is the annotated type of e, and <p is the effect of evaluating 
e. The analysis is specified by the axioms and rules in Table 1.4 which will 
be explained below. 

24 
1 Introduction 
[var] r f- x: 7", & 0 if r(x) = 7", 
[In] 
r[x f-+ 7",] f- e : 7 & t.p 
r f- fn,.. X => e : 7", 
cpU{,..l) 7 & 0 
Table 1.4: Call-tracking Analysis: Effect System. 
In the axiom [var] for variables we produce an empty effect because we assume 
that the parameter mechanism is call-by-value and therefore no evaluation 
takes place when mentioning a variable. Similarly, in the rule [fn] for function 
abstractions we produce an empty effect: no evaluation takes place because 
we only construct a closure. The body of the abstraction is analysed in 
order to determine its annotated type and effect. This ip.formation is needed 
to annotate the function arrow: all the names of functions in the effect of 
the body and the name of the abstraction itself may be involved when this 
particular abstraction is applied. 
Next, consider the rule [app] for function application el e2. Here we obtain 
annotated types and effects from the operator el as well as the operand e2. 
The effect of the application will contain the effect t.p1 of the operator (because 
we have to evaluate it before the application can take place), the effect t.p2 of 
the operand (because we employ a call-by-value semantics so this expression 
has to be evaluated too) and finally we need the effect t.p of the function being 
applied. But this is exactly the information given by the annotation of the 
arrow in the type 72 ~ 
7 of the operand. Hence we produce the union of 
these three sets as the overall effect of the application. 
Example 1.6 Returning to the program of Example 1.5 we have: 
[ ] f- fny y => y: int {Yl) int & 0 
[] f- fnx x => x: (int {Yl) int) {Xl) (int {Yl) int) & 0 
[] f- (fnx x => x) (fny y => y) : int {Yl) int & {X} 
This shows that our example program may (in fact it will) apply the function 
fn x => x but that it will not apply the function fn y => y. 
â¢ 
For a more general language we will also need to introduce some form of 
subsumption rule in the manner of Tables 1.2 and 1.3; there are different 

1. 7 Algorithms 
25 
INPUT: 
Example equations for Reaching Definitions 
OUTPUT: 
The least solution: RD = (RDI,Â·Â·Â·, RDI2) 
METHOD: 
Step 1: 
Initialisation 
RDI:=0;Â·Â·Â·; RDI2:=0 
Step 2: 
Iteration 
while RDj =i Fj(RDI ,Â·Â·Â·, RD12) for some j 
do RDj:=Fj(RDI,Â·Â·Â·, RDI2) 
Table 1.5: Chaotic Iteration for Reaching Definitions. 
approaches to this and we shall return to this later. Effect Systems are 
often implemented as extensions of type inference algorithms and, depending 
on the form of the effects, it may be possible to calculate them on the fly; 
alternatively, sets of constraints can be generated and solved subsequently. 
We refer to Chapter 5 for more details. 
1. 7 
Algorithms 
Let us now reconsider the problem of computing the least solution to the 
program analysis problems considered in Data Flow Analysis and Constraint 
Based Analysis. 
Recall from Section 1.3 that we consider twelve-tuples RD E (P(Var* x 
Lab*))12 of pairs of variables and labels where each label indicates an ele-
mentary block in which the corresponding variable was last assigned. The 
equation or constraint system gives rise to demanding the least solution to an 
equation RD = F(RD) or inclusion RD ;) F(RD) where F is a monotone nmc-
tion over (P(Var* x Lab*))12. Due to the finiteness of (P(Var* x Lab*))12 
the desired solution is in both cases obtainable as F n (0) for any n such that 
Fn+1 (0) = Fn(0) and we know that such an n does in fact exist. 
Chaotic Iteration. Naively implementing the above procedure soon 
turns out to require an overwhelming amount of work. In later chapters we 
shall see much more efficient algorithms and in this section we shall illustrate 
the principle of Chaotic Iteration that lies at the heart of many of them. For 
this let us write 
RD 
= 
(RDI,Â·Â·Â·, RDI2) 
F(RD) 
= 
(FI (RD), ... ,F12(RD)) 

26 
1 Introduction 
and consider the non-deterministic algorithm in Table 1.5. It is immediate 
that there exists j such that ROj =1= Fj(R01,Â·Â·Â·, R012) if and only if RD =1= 
F(RD). Hence if the algorithm terminates it will produce a fixed point of F; 
that is, a solution to the desired equations or constraints. 
Properties of the algorithm. To further analyse the algorithm we 
shall exploit that 
o 
~ RD ~ F(RD) ~ Fn(0) 
holds at all points in the algorithm (where n is determined by Fn+1(0) 
Fn(0)): it clearly holds initially and, as will be shown in Exercise 1.6, it is 
maintained during iteration. This means that if the algorithm terminates we 
will have obtained not only a fixed point of F but in fact the least fixed point 
(i.e. Fn(0)). 
To see that the algorithm terminates note that if j satisfies 
ROj =1= Fj(RO!,Â·Â·Â·, R012) 
then in fact ROj C Fj(R01,Â·Â·Â·, R012) and hence the size of RD increases by 
at least one as we perform each iteration. This ensures termination since we 
assumed that (P(Var* x Lab*))12 is finite. 
The above algorithm is suitable for manually solving data flow equations and 
constraint systems. To obtain an algorithm that is suitable for implemen-
tation we need to give more details about the choice of j so as to avoid an 
extensive search for the value; we shall return to this in Chapters 2 and 6. 
1.8 
Transformations 
A major application of program analysis is to transform the program (at the 
source level or at some intermediate level inside a compiler) so as to obtain 
better performance. To illustrate the ideas we shall show how Reaching Defi-
nitions can be used to perform a transformation known as Constant Folding. 
There are two ingredients in this. One is to replace the use of a variable in 
some expression by a constant if it is known that the value of the variable will 
always be that constant. The other is to simplify an expression by partially 
evaluating it: subexpressions that contain no variables can be evaluated. 
Source to source transformation. Consider a program 8* and let 
RO be a solution (preferable the least) to the Reaching Definitions Analysis 
for 8*. For a sub-statement 8 of 8* we shall now describe how to transform 
it into a "better" statement 8'. We do so by means of judgements of the 
form 
RO I- 8 I> 8' 

1.8 Transformations 
27 
[aSSl] 
RD f- [x := a]i [> [x:= a[y I--t nW 
.f { y E FV(a) /\ (y,?) ~ RDentry(Â£) /\ 
1 
Y(Z,Â£') E RDentry(Â£) : (Z = y::} [ .. . ]i' is [y := n]l') 
[ass2] 
RD f- [x := a]i [> [x:= n]l 
if FV(a) = 0 /\ a ~ Num /\ a evaluates to n 
RD f- 81 [> 8~ 
RD f- 81; 8 2 [> 8I;82 
[ selJ2] 
RD f- 8 2 [> 8~ 
RD f- 81; 8 2 [> 81;8~ 
RD f- 81 [> 8~ 
RD f- if [bY then 8 1 else 8 2 [> if [W then 8I else 8 2 
[iII] 
[i!2] 
RD f- if [bY then 81 else 8 2 [> if [W then 81 else 8~ 
[wh] 
RD f- 8 [> 8' 
RD f- while [bY do 8 [> while [b]L do 8' 
Table 1.6: Constant Folding transformation. 
expressing one step of the transformation process. We may define the trans-
formation using the axioms and rules in Table 1.6; they are explained below. 
The first axiom [assl] expresses the first ingredient in Constant Folding as 
explained above - the use of a variable can be replaced with a constant if it is 
known that the variable always will be that constant; here we write a[y I--t n] 
for the expression that is as a except that all occurrences of y have been 
replaced by n; also we write FV(a) for the set of variables occurring in a. 
The second axiom [ass2] expresses the second ingredient of the transformation 
- expressions can be partially evaluated; it uses the fact that if an expression 
contains no variables then it will always evaluate to the same value. 
The last five rules in Table 1.6 simply say that if we can transform a sub-
statement then we can transform the statement itself. Note that the rules 
(e.g. [seqd and [seq2]) do not prescribe a specific transformation order and 
hence many different transformation sequences may exist. Also note that the 
relation RD f-. [> 
â¢ is neither reflexive nor transitive because there are no 
rules that forces it to be so. Hence we shall often want to perform an entire 
sequence of transformations. 

28 
1 Introduction 
Example 1.7 To illustrate the use of the transformation consider the pro-
gram: 
[x:=10)\ [y:=x+10F; [z:=y+10)3 
The least solution to the Reaching Definitions Analysis for this program is: 
RDentry (1) 
RDexit (1) 
RDentry(2) 
RDexit(2) 
RDentry(3) 
RDexit(3) 
{(x, ?),(y, ?), (z, ?)} 
{(x, 1), (y,?), (z,?)} 
{(x,l),(y, ?),(z, ?)} 
{(x,1),(y,2),(z, ?)} 
{(x, 1), (y,2), (z, ?)} 
{(x,1),(y,2),(z,3)} 
Let us now see how to transform the program. From the axiom [ass1) we 
have 
RD f- [y: =x+10F I> [y: =10+10)2 
and therefore the rules for sequencing gives: 
RD f- [x: =10)1; [y: =X+10)2; [z: =y+10)3 I> [x: =10)1; [y: =10+10)2; [z: =y+10P 
We can now continue and obtain the following transformation sequence: 
RD 
f-
[x:=10f; [y:=x+10F; [z:=y+10P 
I> 
[x:=10)l; [y:=10+10F; [z:=y+10]3 
I> 
[x:=10)\ [y:=20]2; [z:=y+10)3 
I> 
[x:=10]\ [y:=20F; [z:=20+10)3 
I> 
[x:=10]1; [y:=20]2; [z:=30P 
after which no more steps are possible. Another transformation sequence is 
RD 
f-
[X:=10]1; [y:=x+10)2; [z:=y+10]3 
I> 
[x: =10]1; [y: =10+10]2; [z: =y+10]3 
I> 
[x: =10)\ [y: =10+10)2; [z: =10+10+10)3 
I> 
[x:=10P; [y:=10+10F; [z:=30P 
I> 
[x:=10P; [y:=20F; [z:=30]3 
which, in this particular case, yields the same resulting program. 
_ 
Successive transformations. The above example shows that we 
shall want to perform many successive transformations: 
RD f- 51 I> 52 I> ... I> 5n+1 
This could be costly because once 51 has been transformed into 52 we might 
have to recompute Reaching Definitions Analysis for 52 before the trans-
formation can be used to transform it into 53 etc. It turns out that it is 

Concluding Remarks 
29 
sometimes possible to use the analysis for 81 to obtain a reasonable analysis 
for 8 2 without performing the analysis from scratch. In the case of Reaching 
Definitions and Constant Folding this is very easy: if RD is a solution to 
Reaching Definitions for 8i and RD I- 8i [> 8i +1 then RD is also a solution to 
Reaching Definitions for 8i+1 - intuitively, the reason is that the transforma-
tion only changed things that were not observed by the Reaching Definitions 
Analysis. 
Concluding Remarks 
In this chapter we have briefly illustrated a few approaches (but by no means 
all) to program analysis. Clearly there are many differences between the four 
approaches. However, the main aim of the chapter has been to suggest that 
there are also more similarities than one would perhaps have expected at 
first sight: in particular, the interplay between the use of equations versus 
constraints. It is also interesting to note that some of the techniques touched 
upon in this chapter have close connections to other approaches to reasoning 
about programs; especially, some versions of Annotated Type Systems are 
closely related to Hoare's logic for partial correctness assertions. 
As mentioned earlier, the approaches to program analysis covered in this book 
are semantics based rather than semantics directed. The semantics directed 
approaches include the denotational based approaches [27, 86, 115, 117] and 
logic based approaches [19, 20, 81, 82]. 
Mini Projects 
Mini Project 1.1 Correctness of Reaching Definitions 
In this mini project we shall increase our faith in the Type and Effect System 
of Table 1.3 by proving that it is correct. This requires knowledge of regular 
expressions and homomorphisms to the extent covered in Appendix C. 
First we shall show how to associate a regular expression with each state-
ment. We define a function S such that S(8) is a regular expression for each 
statement 8 E Stmt. It is defined by structural induction (see Appendix B) 
as follows: 
S([x:=a]i) 
S([skip]i) 
S(81 ; 82 ) 
S(if [b]i then 8 1 else 8 2) 
A 
S(8d Â·S(82 ) 
= S(81 ) + S(82 ) 

30 
1 Introduction 
S(while b do 8) 
= 
(S(8))* 
The alphabet is {I; I x E Va:r*,i E Lab*} where Va:r* and Lab* are finite 
and non-empty sets that contain all the variables and labels, respectively, of 
the statement S* of interest. As an example, for 8* being 
if [x>ojI then [x:=x+l]2 else ([x:=x+2]3; [x:=x+3]4) 
we have S(8*) =!~ + (!~ . !~). 
Correctness of X. To show the correctness of the X component in 
8: E :0) E we shall for each y E Va:r* define a homomorphism 
hy : {I; I x E Var*,i E Lab*} -t {!}* 
as follows: 
h (Ii) _ {! 
if y = x 
y.", -
A ify #- x 
As an example hx(S(8*)) = ! + (! . !) and hy(S(8*)) = A using that AÂ· A = A 
and A + A = A. Next we write 
hy(S(8)) ~ ! . !* 
to mean that the language Â£[hy(S(8))] defined by the regular expression 
hy(S(8)) is a subset of the language Â£[! . !*] defined by! . !*; this is equivalent 
to 
-,3w E Â£[hy(S{8))] : hy{w) = A 
and intuitively says that y is always assigned in 8. Prove that 
if S : E :0) E and y E X then hy{S(8)) ~ ! . !* 
by induction on the shape of the inference tree establishing 8 : E :0 ) E 
(see Appendix B for an introduction to the proof principle). 
Correctness of RD. 
To show the correctness of the RD component 
in 8 : E :0) E we shall for each y E Var* and i' E Lab* define a 
homomorphism 
as follows: 
{
I 
ify=x A i=i' 
ht (!;) = ? if y = x Ai#- i' 
A ify #- x 
As an example h~(S(S*)) = ! + (? . ?) and h~{S{8*)) = A. Next 
ht (S(8)) ~ ((!+?)* . ?) + A 

Exercises 
31 
is equivalent to 
..,3w E Â£[S(8)] : h~ (w) ends in ! 
and intuitively means than the last assignment to y could not have been 
performed at the statement labelled i'. Prove that 
if S: ~ :0) ~ and (y,i') f/. RD then h; (S(8)) ~ ((!+?)* . ?) + A 
by induction on the shape of the inference tree establishing 8 : ~ :0) 
~. _ 
Exercises 
Exercise 1.1 A variant of Reaching Definitions replaces RD E P(Var x 
Lab) by RL E P(Lab); the idea is that given the program, a label should 
suffice for finding the variables that may be assigned in some elementary 
block bearing that label. Use this as the basis for modifying the equation 
system given in Section 1.3 for RD to an equation system for Rl. (Hint: 
It may be appropriate to think of RD = {(Xl, ?), ... , (xn ,?)) as meaning 
RD = {(Xl, ?"'1)'Â·Â·Â·' (xn, ?"'n)} and then use RL = {?",uÂ·Â·Â·, ?"'n}Â·) 
-
Exercise 1.2 Show that the solution displayed for the Control Flow Anal-
ysis in Section 1.4 is a solution. Also show that it is in fact the least solution. 
(Hint: Consider the demands on Â«2), Â«4), p(x), Â«1) and Â«5).) 
_ 
Exercise 1.3 Let (a, ')') be an adjunction, or a Galois connection, as ex-
plained in Section 1.5; this just means that a(X) ~ Y Â¢:} X ~ ')'(Y) holds 
for all X and Y. Show that a uniquely determines 'Y in the sense that')' = ')" 
whenever (a, ')") is an adjunction. Also show that 'Y uniquely determines a 
for (a, ')') being an adjunction. 
_ 
Exercise 1.4 For F as in Section 1.3 and a, a, 'Y, ;:.; and G as in Section 
1.5 show that a 0 Go;:'; ~ F; this involves showing that 
a(Gj(')'(RD1)'Â·Â·Â·' ,),(RDl2))) ~ Fj(RDl,Â·Â·Â·, RDl2) 
for all j and (RDbÂ·Â·Â·' RD12). Determine whether or not F = aoGo;:';. Prove 
by numerical induction on n that (a 0 G 0 ;:.;)n(0') !; Fn(0). Also prove that 
a(Gn(0)) ~ (a 0 G 0 ;:.;)n(0) using that a(0) = 0 and G ~ Go;:'; 0 a. 
_ 
Exercise 1.5 Consider the Annotated Type System for Reaching Defini-
tions defined in Table 1.2 in Section 1.6 and suppose that we want to stick 
to the first (and unsuccessful) explanation of what 8 : RDl -+ RD2 means in 
terms of Data Flow Analysis. Can you change Table 1.2 (by modifying or 
removing axioms and rules) such that this becomes possible? 
_ 

32 
1 Introduction 
Exercise 1.6 Consider the Chaotic Iteration algorithm of Section 1. 7 and 
suppose that 
holds immediately before the assignment to RDjj show that is also holds 
afterwards. (Hint: Write RD' for (RD1,Â·Â·Â·, Fj(RD), ... , RD12 ) and use the 
monotonicity of F and RD 1; F(RD) to establish that RD 1; RD' 1; F(RD) 1; 
F(RD').) 
â¢ 
Exercise 1. 7 Use the Chaotic Iteration scheme of Section 1. 7 to show 
that the information displayed in Table 1.1 is in fact the least fixed point of 
the function F defined in Section 1.3. 
â¢ 
Exercise 1.8 Consider the following program 
[z:=1]\while [x>O]2 do ([z:=z*y]3 j [x:=x_1]4) 
computing the x-th power of the number stored in y. Formulate a system 
of data flow equations in the manner of Section 1.3. Next use the Chaotic 
Iteration strategy of Section 1.7 to compute the least solution and present it 
in a table (like Table 1.1). 
â¢ 
Exercise 1.9 Perform Constant Folding upon the program 
[x:=10]l j [y:=x+10]2 j [z:=y+x]3 
so as to obtain 
[X:=10]l j [y:=20]2 j [z:=30]3 
How many ways of obtaining the result are there? 
â¢ 
Exercise 1.10 The specification of Constant Folding in Section 1.8 only 
considers arithmetic expressions. Extend it to deal also with boolean expres-
sions. Consider adding axioms like 
RD I- ([skip]i; 8) I> 8 
RD I- (if [trueJi then 81 else 82) I> 81 
and discuss what complications arise. 
Exercise 1.11 Consider adding the axiom 
RD I- [x := a]i I> [x:= a[y t-+ a'JJl 
.f { y E FV(a) A Cy,?) ~ RDentry(i) A 
1 
V(z,i') E RDentry(i) : (y = z =} [ â¢â¢ â¢ ]i' is [y := a,]i') 
â¢ 
to the specification of Constant Folding given in Section 1.8 and discuss 
whether Or not this is a good idea. 
â¢ 

Chapter 2 
Data Flow Analysis 
In this chapter we introduce techniques for Data Flow Analysis. Data Flow 
Analysis is the traditional form of program analysis which is described in 
many textbooks on compiler writing. We will present analyses for the simple 
imperative language WHILE that was introduced in Chapter 1. This includes 
a number of classical Data Flow Analyses: Available Expressions, Reaching 
Definitions, Very Busy Expressions and Live Variables. We introduce an 
operational semantics for WHILE and demonstrate the correctness of the Live 
Variables Analysis. We then present the notion of Monotone Frameworks 
and show how the examples may be recast as such frameworks. We continue 
by presenting a worklist algorithm for solving flow equations and we study 
its termination and correctness properties. The chapter concludes with a 
presentation of some advanced topics, including Interprocedural Data Flow 
Analysis and Shape Analysis. 
Throughout the chapter we will clarify the distinctions between intraproce-
dural and interprocedural analyses, between forward and backward analyses, 
between may and must analyses (or union and intersection analyses), between 
flow sensitive and flow insensitive analyses, and between context sensitive and 
context insensitive analyses. 
2.1 
Intraprocedural Analysis 
In this section we present a number of example Data Flow Analyses for 
the WHILE language. The analyses are each defined by pairs of functions 
that map labels to the appropriate sets; one function in each pair specifies 
information that is true on entry to the block, the second specifies information 
that is true at the exit. 

34 
2 Data Flow Analysis 
Initial and final labels. When presenting examples of Data Flow 
Analyses we will use a number of operations on programs and labels. The 
first of these is 
init : Stmt -+ Lab 
which returns the initial label of a statement: 
init([x := all) 
init([ skip]l) 
init(81 ; 82 ) 
init(if [bY then 8 1 else 8 2) 
init(while [b]l do 8) 
We will also need a function 
= 
= 
= 
= 
= 
final: Stmt -+ P(Lab) 
Â£ 
Â£ 
init(8d 
Â£ 
Â£ 
which returns the set of final labels in a statement; whereas a sequence of 
statements has a single entry, it may have multiple exits (as for example in 
the conditional): 
final([x := all) 
{Â£} 
final([skip]l) = {Â£} 
final(81 ; 82) 
= 
final(82 ) 
fina1(if [b]l then 81 else 82 ) 
= 
final(81 ) U fina1(82 ) 
final(while [b]l do 8) = 
{Â£} 
Note that the while-loop terminates immediately after the test has evaluated 
to false. 
Blocks. To access the statements or tests associated with a label in a 
program we use the function 
blocks: Stmt -+ P(Blocks) 
where Blocks is the set of statements, or elementary blocks, of the form 
[x:=a]l or [skip]l as well as the set of tests of the form [bY. It is defined as 
follows: 
blocks([x := a]t) 
blocks([skip]t) 
blocks(81 ; 82 ) 
blocks(if [b]l then 81 else 82 ) 
blocks(while [b]l do 8) 
{[x := all} 
= 
{[skip]l} 
= 
blocks( 8d U blocks( 82) 
{[b]t} U blocks(81 ) U blocks(82 ) 
= 
{[bY} U blocks(8) 

2.1 Intraprocedural Analysis 
Then the set of labels occurring in a program is given by 
labels: Stmt -+ P(Lab) 
where 
labels(S) = {i I [B]L E blocks(S)} 
Clearly init(S) E labels(S) and finaleS) ~ labels(S). 
35 
Flows and reverse flows. We will need to operate on edges, or flows, 
between labels in a statement. We define a function 
Bow: Stmt -+ P(Lab x Lab) 
which maps statements to sets of flows: 
Bow([x := all) 
= 0 
Bow([ skip]L) 
0 
BOW(Sl; S2) 
Bow{while [W do S) 
fIOW(Sl) U fIOW(S2) 
U{(i,init(S2)) liE final(Sl)} 
fIOW(Sl) U fIOW(S2) 
U{ (i, init(Sl)), (i, init(S2))} 
= fIow{S) U {(i, init{S))} 
u{(Â£',i) IÂ£' E finaleS)} 
Thus labels(S) and Bow{S) will be a representation of the flow graph of S. 
Example 2.1 Consider the following program, power, computing the x-th 
power of the number stored in y: 
[Z:=l]l;while [x>O]2 do ([z:=Z*y]3; [x:=x_l]4) 
We have init{power) = 1, final{power) = {2} and labels{power) = {1, 2, 3, 4}. 
The function Bow produces the following set 
{{1,2), (2,3),{3,4), (4,2)} 
which corresponds to the flow graph in Figure 2.1. 
â¢ 
The function Bow is used in the formulation of forward analyses. Clearly 
init{S) is the (unique) entry node for the flow graph with nodes labels{S) 
and edges Bow{S). Also 
labels{S) = {init{S)} U {i I (i,Â£') E Bow{S)} U {Â£' I (i,Â£') E fIow(S)} 
and for composite statements (meaning those not simply of the form [BY) 
the equation remains true when removing the {init(S)} component. 

36 
2 Data Flow Analysis 
! [Z:=Z*y]3! 
! 
Figure 2.1: Flow graph for the power program. 
In order to formulate backward analyses we require a function that computes 
reverse flows: 
Bo~: Stmt -+ P(Lab x Lab) 
It is defined by: 
Bo~(S) = ((i,t) I (t,i) E Bow(S)} 
Example 2.2 For the power program of Example 2.1, Bo~ produces 
{(2,1),(2,4),(3,2),(4,3)} 
which corresponds to a modified version of the flow graph in Figure 2.1 where 
the direction of the arcs has been reversed. 
_ 
In case fina1(S) contains just one element that will be the unique entry node 
for the flow graph with nodes JabeJs(S) and edges Bo~(S). Also 
JabeJs(S) = fina1(S) U {il (i,i') E Bo~(S)} U {t I (i,t) E Bo~(S)} 
and for composite statements the equation remains true when removing the 
fina1(S) component. 
The program of interest. We will use the notation S* to repre-
sent the program that we are analysing (the "top-level" statement), Lab* to 
represent the labels (labeJs(S*)) appearing in S*, Var* to represent the vari-
ables (FV(S*)) appearing in S*, Blocks* to represent the elementary blocks 

2.1 Intraprocedural Analysis 
37 
(blocks(S*)) occurring in S*, and AExp* to represent the set of non-trivial 
arithmetic sub expressions in S*; an expression is trivial if it is a single vari-
able or constant. We will also write AExp(a) and AExp(b) to refer to the 
set of non-trivial arithmetic sub expressions of a given arithmetic, respectively 
boolean, expression. 
To simplify the presentation of the analyses, and to follow the traditions of 
the literature, we shall frequently assume that the program S* has isolated 
entries; this means that: 
This is the case whenever S* does not start with a while-loop. Similarly, we 
shall frequently assume that the program S* has isolated exits; this means 
that: 
A statement, S, is label consistent if and only if: 
[BI]f, [B2]f E blocks(S) implies Bl = B2 
Clearly, if all blocks in S are uniquely labelled (meaning that each label occurs 
only once), then S is label consistent. When S is label consistent the clause 
"where [B]f E blocks(S)" is unambiguous in defining a partial function from 
labels to elementary blocks; we shall then say that e labels the block B. We 
shall exploit this when defining the example analyses below. 
Example 2.3 The power program of Example 2.1 has isolated entries but 
not isolated exits. It is clearly label consistent as well as uniquely labelled. _ 
2.1.1 
Available Expressions Analysis 
The Available Expressions Analysis will determine: 
For each program point, which expressions must have already 
been computed, and not later modified, on all paths to the pro-
gram point. 
This information can be used to avoid the re-computation of an expression. 
For clarity, we will concentrate on arithmetic expressions. 
Example 2.4 Consider the following program: 
[x: =a+b] 1 ; [y: =a*bF; while [y>a+bp do ([a: =a+1]4; [x: =a+b]5) 
It should be clear that the expression a+b is available every time execution 
reaches the test (label 3) in the loop; as a consequence, the expression need 
not be recomputed. 
_ 

38 
2 Data Flow Analysis 
kill and gen functions 
killAE([x := a]~) = 
{a' E AExp* I x E FV(a')} 
killAE([skip]l) 
0 
killAE ([W) = 0 
genAE([x := all) 
= 
{a' E AExp(a) I x f/. FV(a')} 
genAE ([skip]l) = 0 
genAE ([W) = AExp(b) 
data flow equations: AE= 
AEentry(f) 
A Ee",it (f) = (AEentry(i)\killAE(Bl)) U genAE (Bl) 
where Bl E blocks(S*) 
if f = init(S*) 
otherwise 
Table 2.1: Available Expressions Analysis. 
The analysis is defined in Table 2.1 and explained below. An expression is 
killed in a block if any of the variables used in the expression are modified in 
the block; we use the function 
to produce the set of non-trivial expressions killed in the block. Test and 
skip blocks do not kill any expressions and assignments kill any expression 
that uses the variable that appears in the left hand side of the assignment. 
Note that in the clause for [x: =a]l we have used the notation a' E AExp* 
to denote the fact that a' is a non-trivial arithmetic expression appearing in 
the program. 
A generated expression is an expression that is evaluated in the block and 
where nOne of the variables used in the expression are later modified in the 
block. The set of non-trivial generated expressions is produced by the func-
tion: 
genAE: Blocks* --t P(AExp*) 
The analysis itself is nOw defined by the functions AEentry and AEe",it that 
each map labels to sets of expressions: 
For a label consistent program S* (with isolated entries) the functions can 
be defined as in Table 2.1. 

2.1 Intraprocedural Analysis 
39 
Figure 2.2: A schematic flow graph. 
The analysis is a forward analysis and, as we shall see, we are interested 
in the largest sets satisfying the equation for AEentry - an expression will 
be considered available if no path kills it. No expression is available at the 
start of the program. Subsequently, the expressions that are available at the 
entry to a block are any expressions that are available at all of the exits 
from blocks that flow to the block; if there are no such blocks the formula 
evaluates to AExp*. Given a set of expressions that are available at the 
entry, the expressions available at the exit of the block are computed by 
removing killed expressions and adding any new expression generated by the 
block. 
To see why we require the largest solution, consider Figure 2.2 which shows 
the flow graph for a program in a schematic way. Such a flow graph might 
correspond to the following program: 
[z:=x+y]L;while [truet do [skip( 
The set of expressions generated by the first assignment is {x+y}; the other 
blocks do not generate expressions and no block kills any expressions. The 
equations for AEentry and AEea:it are as follows: 
AEentry(Â£) = 0 
AEentry(e') = AEea:it(Â£) n A Eexit (e") 
AEentry (.e") = AEea:it(e') 
AEea:it(Â£) = AEentry(Â£) U {x+y} 
A Eea:it (.e') 
AEentry(e') 
AEea:it(.e") = AEentry(Â£") 

40 
2 Data Flow Analysis 
After some simplification, we find that: 
AEentry(l') = {x+y} n AEentry(l') 
There are two solutions to this equation: {x+y} and 0. Consideration of 
the example and the definition of available expressions shows that the most 
informative solution is {x+y} - the expression is available every time we enter 
i'. Thus we require the largest solution to the equations. 
Example 2.5 For the program 
[x:=a+b]lj [y:=a*b]2 jwhile [y>a+b]3 do ([a:=a+l]4 j [x:=a+b]5) 
of Example 2.4, killAE and genAE are defined as follows: 
i 
killAE(i) 
genAE (i) 
1 
0 
{a+b} 
2 
0 
{Mb} 
3 
0 
{a+b} 
4 {a+b, a*b, a+l} 
0 
5 
0 
{a+b} 
We get the following equations: 
AEentry(l) 
= 0 
AEentry(2) 
= AEexit(l) 
AEentry(3) 
= 
AEexit(2) n A Eexit (5) 
AEentry(4) 
AEexit(3) 
AEentry(5) 
AEexit(4) 
AEe",it(l) 
= 
AEentry(l) U {a+b} 
AEe",it(2) 
= 
AEentry (2) U { a*b} 
AEe"'it(3) 
= 
A Eentry (3) U {a+b } 
AEexit(4) 
= 
AEentry (4) \ {a+b, a*b, a+1} 
AEe"'it(5) 
= 
AEentry (5) U { a+b} 
Using an analogue of the Chaotic Iteration discussed in Chapter 1 (starting 
with AExp* rather than 0) we can compute the following solution: 
i 
AEentry(i) 
AEexit(i) 
1 
0 
{a+b} 
2 
{a+b} 
{a+b, Mb} 
3 
{a+b} 
{a+b} 
4 
{a+b} 
0 
5 
0 
{a+b} 

2.1 Intraprocedural Analysis 
41 
Note that, even though a is redefined in the loop, the expression a+b is re-
evaluated in the loop and so it is always available on entry to the loop. On 
the other hand, a*b is available on the first entry to the loop but is killed 
before the next iteration. 
_ 
2.1.2 
Reaching Definitions Analysis 
As mentioned in Chapter 1, the Reaching Definitions Analysis should more 
properly be called reaching assignments but we will use the traditional name. 
This analysis is analogous to the previous one except that we are interested 
In: 
For each program point, which assignments may have been made 
and not overwritten, when program execution reaches this point 
along some path. 
A main application of Reaching Definitions Analysis is in the construction 
of direct links between blocks that produce values and blocks that use them; 
we shall return to this in Subsection 2.1.5. 
Example 2.6 Consider the following program: 
[x:=5]1; [y:=1]2;while [x>lP do ([y:=x*y]4; [x:=x-l]5) 
All of the assignments reach the entry of 4 (the assignments labelled 1 and 
2 reach there on the first iteration); only the assignments labelled 1, 4 and 5 
reach the entry of 5. 
_ 
The analysis is specified in Table 2.2. The function 
killRD : Blocks* -+ P(Var* x Lab:) 
produces the set of pairs of variables and labels of assignments that are 
destroyed by the block. An assignment is destroyed if the block assigns a 
new value to the variable, i.e. the left hand side of the assignment. To deal 
with uninitialised variables we shall, as in Chapter 1, use the special label 
? 
"?" and we set Lab~ = Lab* U {?}. 
The function 
genRD : Blocks* -+ P(Var* x Lab:) 
produces the set of pairs of variables and labels of assignments generated by 
the block; only assignments generate definitions. 
The analysis itself is now defined by the pair of functions RD entry and RD exit 
mapping labels to sets of pairs of variables and labels (of assignment blocks): 
RDentry, RDexit : Lab* -+ P(Var* x Lab:) 

42 
2 Data Flow Analysis 
kill and gen functions 
{(x, ?)} 
killRO([skip]l) 
killRo([W) 
U{(x,f') I Bl' is an assignment to x in S*} 
o 
= 0 
genRO([x := all) = 
genRO([skip]l) 
genRO([W) = 
{(x, fn 
o o 
= { 
data flow equations: RD= 
{(x, ?) I x E FV(S*)} 
U{RDexit(f') I (.e',f) E :!:low(S*)} 
RDexit(f) = (RDentry(f)\killRO(Bl)) U genRo(Bl) 
where Bl E blocks(S*) 
if f = init(S*) 
otherwise 
Table 2.2: Reaching Definitions Analysis. 
For a label consistent program S* (with isolated entries) the functions are 
defined as in Table 2.2. 
Similar to the previous example, this is a forward analysis but, as we shall 
see, we are interested in the smallest sets satisfying the equation for RDentry. 
An assignment reaches the entry of a block if it reaches the exit of any of 
the blocks which precede itj if there are none the formula evaluates to 0. 
The computation of the set of assignments reaching the exit of a block is 
analogous to the Available Expressions Analysis. 
We motivate the requirement for the smallest solution by consideration of 
the program [z:=x+y]ljwhile [trueJi' do [skipy" corresponding to Figure 
2.2 again. The equations for RDentry and RDexit are as follows: 
RDentry(f) = {(x, ?),(y, ?),(z, ?)} 
RDentry(f') = RD exit (f) U RD exit (fll) 
RDentry(fll) = RD exit (f') 
RD exit (f) = (RDentry(f) \ {(z, ?)})U{(z,f)} 
RD exit (f') = RDentry(f') 
RD exit (fll) 
RD entry (fll) 

2.1 Intraprocedural Analysis 
43 
Once again, we concentrate on the entry of the block labelled Â£1, RDentry(Â£I); 
after some simplification we get 
RDentry(Â£') = {(x, ?), (y, ?), (z,Â£)} U RDentry(Â£') 
but this equation has many solutions: we can take RD entry (Â£') to be any 
superset of {(x, ?), (y, ?), (z, Â£)}. However, since Â£' does not generate any new 
definitions, the most precise solution is {(x, ?), (y, ?), (z,Â£)} - we require the 
smallest solution to the equations. 
Sometimes, when the Reaching Definitions Analysis is presented in the liter-
ature, one has RDentry(init(S*)) = 0 rather than RDentry(init(S*)) = ((x,?) I 
x E FV(S*)}. This is correct only for programs that always assign variables 
before their first use; incorrect optimisations may result if this is not the case. 
The advantage of our formulation, as will emerge from Mini Project 2.2, is 
that it is always semantically sound. 
Example 2.1 The following table summarises the assignments killed and 
generated by each of the blocks in the program 
of Example 2.6: 
Â£ 
killRD(Â£) 
genRD(Â£) 
1 {(x, ?),(x,I),(x,5)} 
{(x, I)} 
2 
{(y, ?),(y,2),(y,4)} 
{(y,2)} 
3 
0 
0 
4 
{(y, ?),(y,2),(y,4)} 
{(y,4)} 
5 {(x, ?),(x,I),(x,5)} 
{(x,5)} 
The analysis gives rise to the following equations: 
RDentry(l) = {(x, ?), (y,?)} 
RDentry(2) 
RDexit(l) 
RDentry(3) 
RDexit(2) U RDexit(5) 
RDentry(4) 
RDexit(3) 
RDentry(5) = RD exit (4) 
RDexit(l) 
(RD entry (1) \ {(x, ?), (x, 1), (x, 5)}) U {(x, I)} 
RDexit (2) = (RD entry (2) \ {(y, ?), (y, 2), (y, 4)}) U {(y, 2)} 
RDexit(3) = RDentry(3) 
RDexit(4) 
(RD entry (4) \ {(y, ?), (y, 2), (y, 4)}) U {(y, 4)} 
RDexit(5) = (RD entry (5) \ {(x, ?), (x, 1), (x, 5)}) U {(x, 5)} 

44 
2 Data Flow Analysis 
Using Chaotic Iteration we may compute the solution: 
P. 
RDentry(P.) 
RDexit(P.) 
1 
{(x, ?), (y, ?)} 
{(y, ?), (x, I)} 
2 
{(y, ?), (x, I)} 
{(x, 1), (y, 2)} 
3 {(x, I),(y,2),(y,4), (x,5)} {(x, 1), (y,2), (y,4), (x,5)} 
4 {(x, 1), (y,2), (y,4),(x,5)} 
{(x, 1), (y,4), (x,5)} 
5 
{(x, 1), (y,4), (x,5)} 
{(y, 4), (x, 5)} 
-
2.1.3 
Very Busy Expressions Analysis 
An expression is very busy at the exit from a label if, no matter what path 
is taken from the label, the expression must always be used before any of the 
variables occurring in it are redefined. The aim of the Very Busy Expressions 
Analysis is to determine: 
For each program point, which expressions must be very busy at 
the exit from the point. 
A possible optimisation based on this information is to evaluate the expres-
sion at the block and store its value for later use; this optimisation is some-
times called hoisting the expression. 
Example 2.8 Consider the program: 
The expressions a-b and b-a are both very busy at the start of the condi-
tional; they can be hoisted to the start of the conditional resulting in a space 
saving in the size of the code generated for this program. 
_ 
The analysis is specified in Table 2.3. We have already defined the notion 
of an expression being killed when we presented the Available Expressions 
Analysis; we use an equivalent function here: 
By analogy with the previous analyses, we also need to define how a block 
generates additional very busy expressions. For this we use: 
All of the expressions that appear in a block are very busy at the entry to 
the block (unlike what was the case for Available Expressions). 

2.1 Intraprocedural Analysis 
VBentTY(Â£) 
kill and gen functions 
killvB([X := a]f) 
{a' E AExp* I x E FV(a')} 
killvB ([ skip]l) 
0 
killvB([b]f) 
0 
genVB([X := a]l) 
genVB([skip]f) 
genVB([W) 
AExp(a) 
o 
AExp(b) 
data flow equations: VB= 
{ 0 
ifÂ£Efinal(S*) 
n{VBentTY(Â£') I (Â£',Â£) E flo~(S*)} otherwise 
(V B exit ( Â£) \ killvB (Bf)) U genVB (BÂ£) 
where Bf E blocks(S*) 
Table 2.3: Very Busy Expressions Analysis. 
45 
The analysis itself is defined by the pair of functions VB entry and VB exit 
mapping labels to sets of expressions: 
For a label consistent program S* (with isolated exits) they are defined as in 
Table 2.3. 
The analysis is a backward analysis and, as we shall see, we are interested in 
the largest sets satisfying the equation for VB exit. The functions propagate 
information against the flow of the program: an expression is very busy at 
the exit from a block if it is very busy at the entry to every block that follows; 
if there are none the formula evaluates to AExp*. However, no expressions 
are very busy at the exit from any final block. 
To motivate the fact that we require the largest set, we consider the situation 
where we have a flow graph as shown in Figure 2.3; this flow graph might 
correspond to the program: 
(while [x>l]f do [skip(); [x:=x+1]f" 
The equations for this program are 
VBentry(Â£) 
VBentry(Â£') 
VB entry (Â£") 
VBexit(Â£) 
VBexit(Â£') 
{x+1} 

46 
2 Data Flow Analysis 
Figure 2.3: A schematic flow graph (in reverse). 
VBexit(J:) 
VBentry(Â£') n VB entry (Â£") 
VBexit(Â£') 
VBentry(Â£) 
VB exit (Â£") = 0 
and, for the exit conditions of Â£, we calculate: 
VBexit(Â£) 
= 
VBexit(Â£) n {x+l} 
Any subset of {x+l} is a solution but {x+l} is the most informative. Hence 
we want the largest solution to the equations. 
Example 2.9 To analyse the program 
if [a>b]l then ([x:=b-a]2; [y:=a-bn else ([y:=b-a]4; [x:=a-b]5) 
of Example 2.8, we calculate the following killed and generated sets: 
Â£ killv8(Â£) 
genV8(Â£) 
1 
0 
0 
2 
0 
{b-a} 
3 
0 
{a-b} 
4 
0 
{b-a} 
5 
0 
{a-b} 
We get the following equations: 
VB entry (1) 
VBexit(l) 
VB entry (2) 
VBexit (2) U {b-a} 
VB entry (3) 
{a-b} 
VB entry (4) 
= 
VB exit (4) U {b-a} 
VB entry (5) 
= 
{a-b} 

2.1 Intraprocedural Analysis 
47 
VBexit(l) = VB entry (2) n VB entry (4) 
VBexit(2) = VB entry (3) 
VBexit(3) = 0 
VB exit (4) = VB entry (5) 
VBexu(5) = 0 
We can then use an analogue of Chaotic Iteration (starting with AExp* 
rather than 0) to compute: 
Â£ VBentry(Â£) 
VBexit(Â£) 
1 {a-b,b-a} {a-b, b-a} 
2 {a-b, b-a} 
{a-b} 
3 
{a-b} 
0 
4 {a-b, b-a} 
{a-b} 
5 
{a-b} 
0 
â¢ 
2.1.4 
Live Variables Analysis 
A variable is live at the exit from a label if there exists a path from the 
label to a use of the variable that does not re-define the variable. The Live 
Variables Analysis will determine: 
For each program point, which variables may be live at the exit 
from the point. 
This analysis might be used as the basis for Dead Code Elimination. If the 
variable is not live at the exit from a label then, if the elementary block is 
an assignment to the variable, the elementary block can be eliminated. 
Example 2.10 Consider the following expression: 
The variable x is not live at the exit from label 1; the first assignment of the 
program is redundant. Both x and y are live at the exit from label 3. 
â¢ 
The analysis is defined in Table 2.4. The variable that appears on the left 
hand side of an assignment is killed by the assignment; tests and skip state-
ments do not kill variables. This is expressed by the function: 
The function 

48 
LVentry(Â£) 
kill and gen functions 
killlv([x := ali) 
{x} 
killlv([skip]Â£) 
0 
killlv([W) = 0 
genlV([x := a]l) 
= 
FV(a) 
genlv([skip]Â£) = 0 
genlV([W) = FV(b) 
data flow equations: LV= 
2 Data Flow Analysis 
{ 0 
if Â£ E nna1(S*) 
U{LVentry(Â£') I (Â£',Â£) E .f:lo~(S*)} otherwise 
= (LVexit(Â£)\killlv(BÂ£)) U genlv(BÂ£) 
where Bl E blocks(S*) 
Table 2.4: Live Variables Analysis. 
genlV : Blocks* ~ P(Var*) 
produces the set of variables that appear in the block. 
The analysis itself is defined by the pair of functions LVentry and LVexit map-
ping labels to sets of variables: 
LVerit, LVentry : Lab* ~ P(Var*) 
For a label consistent program S* (with isolated exits) they can be defined 
as in Table 2.4. The equation for LVexit(Â£) includes a variable in the set of 
live variables (at the exit from Â£) if it is live at the entry to any of the blocks 
that follow Â£j if there are none then the formula evaluates to 0. 
The analysis is a backward analysis and, as we shall see, we are interested in 
the smallest sets satisfying the equation for LVexit. To see why we require 
the smallest set, consider once again the program 
(while [x>1]Â£ do [skip()j [x:=x+1t' 
corresponding to the flow graph in Figure 2.3. The equations for the program 
are: 
LVentry(Â£) 
LVentry (Â£') = 
LV entry (Â£") 
LVexit(Â£) U {x} 
LVexit(Â£') 
{x} 

2.1 Intraprocedural Analysis 
49 
LVexit(l) = LVentry(e')ULVentry(l") 
LV exit (e') = LVentry(l) 
LV exit (l") = 0 
Suppose that we are interested in LVexit(l)j after some calculation we get: 
LVexit(l) = LVexit(l) U {x} 
Any superset of {x} is a solution. Optimisations based on this analysis are 
based on "dead" variables - the smaller the set of live variables, the more 
optimisations are possible. Hence we shall be interested in the smallest solu-
tion {x} to the equations. Correctness of the analysis will be established in 
Section 2.2. 
Example 2.11 Returning to the program 
[x:=2]lj [y:=4]2 j [x:=1]3 j (if [y>x]4 then [z:=y]5 else [z:=y*y]6)j [x:=zj1 
of Example 2.10, we can compute killlv and genlV as: 
l 
killlV(l) genlv(l) 
1 
{x} 
0 
2 
{y} 
0 
3 
{x} 
0 
4 
0 
{x,y} 
5 
{z} 
{y} 
6 
{z} 
{y} 
7 
{x} 
{z} 
We get the following equations: 
LV entry (1) = LVexit(1)\{x} 
LV entry (2) = LV exit (2)\{y} 
LV entry (3) = LVexit(3)\{x} 
LV entry (4) = LVexit(4) U {x,y} 
LV entry (5) 
(LVexit(5)\{z}) U {y} 
LV entry (6) 
(LV exit (6)\{z}) U {y} 
LV entry (7) = {z} 
LV exit (1) 
LV entry (2) 
LV exit (2) = LV entry (3) 
LV exit (3) = LV entry (4) 
LV exit (4) = LV entry (5) U LVentry (6) 
LV exit (5) 
LV entry (7) 
LV exit (6) = LV entry (7) 
LV exit (7) 
0 

50 
2 Data Flow Analysis 
We can then use Chaotic Iteration to compute the solution: 
e LV entry (e) 
LVexit(e) 
1 
0 
0 
2 
0 
{y} 
3 
{y} 
{x,y} 
4 
{x,y} 
{y} 
5 
{y} 
{z} 
6 
{y} 
{z} 
7 
{z} 
0 
Note that we have assumed that all variables are dead at the end of the 
program. Some authors assume that the variables of interest are output at 
the end of the program; in that case LV exit (7) should be {X, y, z} which means 
that LVentry(7), LVexit (5) and LVexit(6) should all be {y,z}. 
â¢ 
2.1.5 
Derived Data Flow Information 
It is often convenient to directly link labels of statements that produce values 
to the labels of statements that use them. Links that, for each use of a vari-
able, associate all assignments that reach that use are called Use-Definition 
chains or ud-chains. Links that, for each assignment, associate all uses are 
called Definition-Use chains or du-chains. 
In order to make these definitions more precise, we will use the notion of a 
definition clear path with respect to some variable. The idea is that e 1, ... , en 
is a definition clear path for x if none of the blocks labelled e1,Â· .. ,en assigns 
a value to x and if en uses x. Formally, for a label consistent program S* we 
define the predicate clear: 
clear(x,e,e') 
= 
3e1,Â·Â·Â·,en : 
(e1 = e) 1\ (en = e') 1\ (n > 0) 1\ 
(Vi E {I,Â·Â·Â· ,n -I} : (ei,eH1 ) E Bow(S*)) 1\ 
(Vi E {I, ... ,n - I} : -,def(x, ei )) 1\ use(x, en) 
Here the predicate use checks whether the variable is used in a block 
and the predicate def checks whether the variable is assigned in a block: 
Armed with these definitions, we can define the functions 

2.1 Intraprocedural Analysis 
as follows: 
ud(x, Â£') 
du(x,Â£) 
= {Â£ I def(x,Â£) 1\ 3Â£" : (Â£,Â£") E Bow(S*) 1\ c1ear(x,Â£",Â£'n 
U {? I c1ear(x, init(S*), Â£'n 
= 
{ 
{Â£' I def(x,Â£) 1\ 3Â£": (Â£,Â£") E Bow(S*) 1\ c1ear(x,Â£",Â£'n 
iff;f ? 
{Â£' I c1ear(x,init(S*),Â£'n 
iff= ? 
51 
So ud(x, Â£') will return the labels where an occurrence of x at Â£' might have 
obtained its value; this may be at a label Â£ in S* or x may be uninitialised as 
indicated by the occurrence of "?". And du(x, Â£) will return the labels where 
the value assigned to x at Â£ might be used; again we distinguish between 
the case where x gets its value within the program and the case where it is 
uninitialised. It turns out that: 
du(x, Â£) = {Â£' I Â£ E ud(x, Â£'n 
Before showing how ud- and du-chains can be used, we illustrate the functions 
by a simple example. 
Example 2.12 Consider the program: 
[x: =0]\ [x: =3]2; (if [z=x]3 then [Z:=0]4 else [z:=x]5); [Y:=X]6; [x:=y+zf 
Then we get: 
ud(x,Â£) 
x 
Y 
z 
du(x, Â£) 
x 
Y 
z 
1 
0 
0 
0 
1 
0 
0 
0 
2 
0 
0 
0 
2 
{3,5,6} 
0 
0 
3 
{2} 
0 
{?} 
3 
0 
0 
0 
4 
0 
0 
0 
4 
0 
0 
{7} 
5 
{2} 
0 
0 
5 
0 
0 
{7} 
6 
{2} 
0 
0 
6 
0 
{7} 
0 
7 
0 
{6} 
{4,5} 
7 
0 
0 
0 
? 
0 
0 
{3} 
The table for ud shows that the occurrence of x in block 3 will get its value 
in block 2 and the table for du shows that the value assigned to x in block 2 
may be used in block 3, 5 and 6. 
â¢ 
One application of ud- and du-chains is for Dead Code Elimination; for the 
program of Example 2.12 we may remove the block labelled 1 for example 
because there will be no use of the value assigned to x before it is reassigned 
in the next block. Another application is in Code Motion; in the example 
program the block labelled 6 can be moved to just in front of the conditional 

52 
2 Data Flow Analysis 
because it only uses variables assigned in earlier blocks and the conditional 
does not use the variable assigned in block 6. 
The definitions of ud- and du-chains do not give any hints as to how to 
compute the chains - the definitions are not constructive. It is possible to 
give constructive definitions which re-use some of the functions that we have 
defined in the earlier examples. In order to define ud-chains we can use 
RDentry , which records the assignments reaching a block and define 
UD : Var* x Lab* -+ P(Lab*) 
by: 
UD(x i) = { {i' I (x,i') E RDentry(i)} if x E l5,enLv(Bi ) 
, 
0 
otherWIse 
Similarly, we can define a function DU : Var* x Lab* -+ P(Lab*) for du-
chains based on the functions we have seen previously. We shall leave this to 
Mini Project 2.1 where we also consider the formal relationship between the 
UD and DU functions and the functions ud and duo 
2.2 
Theoretical Properties 
In this section we will show that the Live Variables Analysis of Subsection 
2.1.4 is indeed correct; the correctness of the Reaching Definitions Analysis is 
the topic of Mini Project 2.2. We shall begin by presenting a formal semantics 
for WHILE. 
The material of this section may be skimmed through on a first reading; 
however, it is frequently when conducting the correctness proof that the final 
and subtle errors in the analysis are found and corrected! In other words, 
proving the semantic correctness of the analysis should not be considered a 
dispensable development that is merely of interest for theoreticians. 
2.2.1 
Structural Operational Semantics 
We choose to use a (so-called small step) Structuml Opemtional Semantics 
because it allows us to reason about intermediate stages in a program execu-
tion and it also allows us to deal with non-terminating programs. 
Configurations and transitions. First define a state as a mapping 
from variables to integers: 
(J' EState = Var -+ Z 
A configumtion of the semantics is either a pair consisting of a statement 
and a state or it is a state; a terminal configuration is a configuration that 
simply is a state. The tmnsitions of the semantics are of the form 

2.2 Theoretical Properties 
A : AExp -+ (State -+ Z) 
A[x]O' 
O'(x) 
A[n]O' 
N[n] 
A[a1 0Pa a2]O' = A[adO' 0Pa A[a2]O' 
l3 : BExp -+ (State -+ T) 
l3[not b]O' = -,l3[b]O' 
l3[b1 OPb b2]O' 
l3[a1 OPr a2]O' 
l3[bdO' OPb B[b2]O' 
A[adO' OPr A[a2]O' 
Table 2.5: Semantics of expressions in WHILE. 
(8, a) -+ a' 
and (8, a) -+ (8', a') 
53 
and express how the configuration is changed by one step of computation. 
So in the configuration (8, a) one of two things may happen: 
â¢ the execution terminates after one step and we record that by giving 
the resulting state a', or 
â¢ the execution does not terminate after one step and we record that by 
a new configuration (8', a') where 8' is the rest of the program and a' 
is the updated state. 
To deal with arithmetic and boolean expressions we require the semantic 
functions 
A: AExp -+ (State -+ Z) 
l3: BExp -+ (State -+ T) 
whose definition are given in Table 2.5. Here we assume that OPa, OPb and 
oPr are the semantic counterparts of the corresponding syntax. We have 
also assumed the existence of N : Num -+ Z which defines the semantics 
of numerals. For simplicity we have assumed that no errors can occur; this 
means that division by 0 will have to produce an integer for example. One 
can modify the definition so as to allow errors but this will complicate the 
correctness proof to be performed below. Note that the value of an expression 
is only affected by the variables appearing in it, that is: 
if 'Ix E FV(a) : 0'1 (x) = 0'2 (x) then A[a]O'l = A[a]O'2 
if 'Ix E FV(b) : 0'1 (x) = 0'2 (x) then l3[b]O'l = l3[b]O'2 
These results can easily be proved by structural induction on expressions 
(or by mathematical induction on their size); see Appendix B for a brief 
introduction to these proof principles. 

54 
2 Data Flow Analysis 
[ass] 
([x := a]l, CT) --+ CT[X I--t A[a]CT] 
[skip] 
([ skip]! , CT) --+ CT 
[seq1] 
(81 , CT) --+ (8~, CT') 
(81 ; 82, CT) --+ (81; 82, CT') 
[seq2] 
(81, CT) --+ CT' 
(81 ; 82, CT) --+ (82 , CT') 
riAl 
(if [W then 81 else 82, CT) --+ (81 , CT) 
if B[b]CT = true 
[if2] 
(if [W then 81 else 82, CT) --+ (82 , CT) 
if B[b]CT = false 
[Wh1] 
(while [by do 8, CT) --+ ((8; while [W do 8), CT} 
if B[b]CT = true 
[w~] (while [W do 8, CT) --+ CT 
if B[b]CT = false 
Table 2.6: The Structural Operational Semantics of WHILE. 
The detailed definition of the semantics of the statements may be found in 
Table 2.6; it is explained below. 
The clause [ass] specifies that the assignment x := a is executed in one step; 
here we write CT[X I--t A[a]CT] for the state that is as CT except that x is mapped 
to A[a]CT, i.e. the value that a will evaluate to in the state CT. Formally: 
( [ 
A[ ]]) 
{A[a]CT if x = y 
CT X I--t 
a CT y = 
CT(y) 
otherwise 
The semantics of sequencing is given by the two rules [seq1] and [seq2] and 
the idea is as follows. The first step of executing 8 1 ; 82 is the first step of 
executing 8 1 . It may be that only one step is needed for 8 1 to terminate and 
then the rule [seq2] applies and says that the new configuration is (82 , CT') 
reflecting that we are ready to start executing 82 in the next step. Alter-
natively, 81 may not terminate in just one step but gives rise to some other 
configuration (81, CT'); then the rule [seq1] applies and it expresses that the 
rest of 8 1 and all of 82 still have to be executed: the next configuration is 
(81; 82,CT'}. 
The semantics of the conditional is given by the two axioms riAl and [if2] ex-
pressing that the first step of computation will select the appropriate branch 
based on the current value of the boolean expression. 
Finally, the semantics of the while-construct is given by the two axioms [Wh1] 
and [w~]; the first axiom expresses that if the boolean expressions evaluates 

2.2 Theoretical Properties 
55 
to true then the first step is to unroll the loop and the second axiom expresses 
that the execution terminates if the boolean expression evaluates to false. 
Derivation sequences. A derivation sequence for a statement 8 1 and 
a state 0"1 can take one of two forms: 
â¢ It is a finite sequence of configurations (81,0"1), ..â¢ , (8n ,O"n),O"n+1 sat-
isfying (8i , O"i) -+ (8i+1, O"i+1) for i = 1, ... , n -1 and (8n , O"n) -+ O"n+1; 
this corresponds to a terminating computation . 
â¢ It is an infinite sequence of configurations (81,0"1), ..â¢ , (8i, O"i),Â·Â·Â· satis-
fying (8i , O"i) -+ (8i+1, O"i+1) for all i ~ 1; this corresponds to a looping 
computation. 
Example 2.13 We illustrate the semantics by showing an execution of 
the factorial program of Example 1.1. In the following we assume that the 
state O"nznynz maps x to n"" y to ny and z to n z. We then get the following 
finite derivation sequence: 
([Y:=X]l; [z:=1]2;while [y>1]3 do ([z:=z*y]4; [y:=y_1]5); [y:=0]6,0"300) 
-+ 
([z: =1]2; while [y>1]3 do ([z: =z*y]4; [y: =y-1]5); [y: =0]6,0"330) 
-+ 
(while [y>1]3 do ([Z:=Z*y]4; [y:=y_1]5); [y:=0]6,0"331) 
-+ 
([z:=z*y]4; [y:=y-1]5; 
while [y>1]3 do ([z:=z*y]4; [y:=y-1]5); [y:=0]6,0"331) 
-+ 
([y:=y-1]5;while [Y>1]3 do ([z:=z*y]4; [y:=y_1]5); [y: =0]6, 0"333) 
-+ 
(while [y>1]3 do ([Z:=Z*y]4; [y:=y_1]5); [y:=0]6,0"323) 
-+ 
([z:=z*y]4; [y:=y-1P; 
while [y>1]3 do ([z:=z*y]4; [y:=y-1]5); [y:=0]6,0"323) 
-+ 
([y:=y-1]5;while [y>1]3 do ([z:=z*y]4; [y:=y-1]5); [y:=0]6,0"326) 
-+ 
(while [y>lP do ([z:=z*y]4;[y:=y-1]5);[y:=0]6,0"316) 
-+ 
([y: =0]6,0"316) 
-+ 
0"306 
Note that labels have no impact on the semantics: they are merely carried 
along and never inspected. 
_ 
Properties of the semantics. We shall first establish a number of 
properties of the operations on programs and labels that we have used in 
the formulation of the analyses. In the course of the computation the set of 

56 
2 Data Flow Analysis 
flows, the set of final labels and the set of elementary blocks of the statements 
of the configurations will be modified; Lemma 2.14 shows that the sets will 
decrease: 
Lemma 2.14 
(i) If (S, 0') -+ 0" then finaJ(S) = {init(S)}. 
(ii) If (S,o') -+ (S', 0") then finaJ(S) ;2 finaJ(S'). 
(iii) If (S,o') -+ (S',o") then Bow(S) ;2 Bow(S'). 
(iv) If (S,o') -+ (S', 0") then blocks(S) ;2 bJocks(S') and if S is label con-
sistent then so is S'. 
â¢ 
Proof The proof of (i) is by induction on the shape of the inference tree used to 
establish (8,0') -+ 0"; we refer to Appendix B for a brief introduction to the proof 
principle. Consulting Table 2.6 we see that there are three non-vacuous cases: 
The case [ass]. Then ([x := a]l, 0') -+ O'[x I-t A[a]O'] and we get: 
tinal([x := all) = {Â£} = {init([x := all)} 
The case [skip]. Then ([skip]l,O') -+ 0' and we get: 
tinal([skip]l) = {Â£} = {init([skip]l)} 
The case [Wh2]. Then (while [b]l do 8,0') -+ 0' because B[b]O' = false and we get: 
tinal(while [b]l do 8) = {/!} = {init(while [b]l do 8)} 
This completes the proof of (i). 
The proof of (ii) is by induction on the shape of the inference tree used to establish 
(8, a) -+ (8', a'). There are five non-vacuous cases: 
The case [seq1]. Then (81;82,0') -+ (8~;82,O") because (81,0') -+ (8~,O") and we 
get: 
tinal(81; 82) = tinal(82) = tinal(8~; 82) 
The case [seq2]. Then (81;82,0') -+ (82,0") because (81,0') -+ a' and we get: 
tinal(81; 8 2 ) = tinal(82) 
The case riAl. Then (if [b]' then 81 else 82, a) -+ (81, a) because B[b]O' = true 
and we get: 
The case [if2] is similar to the previous case. 
The case [Wh1]. Then (while [b]l do 8, a) -+ Â«8; while [b]' do 8), a) because B[b]O' 
= true and we get: 
tinal(8; while [b]l do 8) = tinal(while [b]l do 8) 

2.2 Theoretical Properties 
57 
This completes the proof of (ii). 
The proof of (iii) is by induction on the shape of the inference tree used to establish 
(S, a) -t (S', a'). There are five non-vacuous cases: 
The case [seqd. Then (S1;S2,a) -t (S~;S2,a') because (S1,a) -t (S~,a') and we 
get 
HOW(S1;S2) 
= 
HOW(S1) U HOW(S2) U {(Â£,init(S2Â» 1Â£ E final(St}} 
:2 
How(SD U HOW(S2) U {(Â£, init(S2Â» I Â£ E final(S1)} 
:2 
How(SD U HOW(S2) u {(Â£, init(S2Â» I Â£ E final(SD} 
= 
How(S~; S2) 
where we have used the induction hypothesis and (ii). 
The case [seq2]' Then (S1; S2, a) -t (S2, a') because (S1,a) -t a' and we get: 
HOW(S1; S2) 
= 
How(St} u HOW(S2) U {(Â£, init(S2Â» 1Â£ E final(S1)} 
:2 
HOW(S2) 
The case [ifd. Then (if [by then S1 else S2,a) -t (S1,a) because B[b]a = true 
and we get: 
How(if [b]l then S1 else S2) 
= 
flOW(S1) u flOW(S2) 
u {(Â£, init(S1Â», (Â£, init(S2Â»} 
:2 
HOW(S1) 
The case [if2] is similar to the previous case. 
The case [wh2]. Then (while [b]t do S, a) -t (S; while [b]l do S, a) because B[b]a 
= true and we get: 
How(S; while [b]l do S) 
= 
How(S) U flow(while [b]l do S) 
U {(Â£', Â£) I Â£' E finaleS)} 
How(S) U How(S) U {(Â£, init(SÂ»} 
U {(Â£', Â£) I Â£' E finaleS)} U {(Â£', Â£) I Â£' E finaleS)} 
How(S) U {(Â£,init(SÂ»} U ((Â£',Â£) IÂ£' E finaleS)} 
= 
How(while [b]t do S) 
This completes the proof of (iii). 
The proof of (iv) is similar to that of (iii) and we omit the details. 
â¢ 
2.2.2 
Correctness of Live Variables Analysis 
Preservation of solutions. Subsection 2.1.4 shows how to define an 
equation system for a label consistent program S* (with isolated exits); we will 

58 
2 Data Flow Analysis 
refer to this system as LV=(S*). The construction of LV=(S*) can be modified 
to give a constraint system LV~(S*) of the form studied in Subsection 1.3.2: 
LV exit (f) 
LV entry (f) 
:J 
(LV exit (f) \killLV (BÂ£)) U genLv(BÂ£) 
where BÂ£ E blocks(S*) 
if f E final(S*) 
otherwise 
We make this definition because in the correctness proof we will want to use 
the same solution for all statements derived from S*; this will be possible for 
LV~(S*) but not for LV=(S*). 
Now consider a collection live of functions: 
We say that live solves LV=(S), and write 
if the functions satisfy the equations; similarly we write 
live F LV~ (S) 
if live solves LV~ (S*). The following result shows that any solution of the 
equation system is also a solution of the constraint system and that the least 
solutions of the two systems coincide. 
Lemma 2.15 Consider a label consistent program S*. If live F LV=(S*) 
then live F LV~(S*). The least solution of LV=(S*) coincides with the least 
solution of LV~(S*). 
â¢ 
Proof If live F LV=(S*) then clearly live F LV<;;(S*) because "2" includes the 
case of "=". 
Next let us prove that LV<;;(S*) and LV=(S*) have the same least solution. We 
gave a constructive proof of a related result in Chapter 1 (under some assumptions 
about finiteness) so let us here give a more abstract proof using more advanced 
fixed point theory (as covered in Appendix A). In the manner of Chapter 1 we 
construct a function F& such that: 
liveFLV<;;(S) 
iff 
live;;;JF&(live) 
live F LV=(S) 
iff 
live = F&(live) 
Using Tarski's Fixed Point Theorem (Proposition A.lO) we now have that F& has 
a least fixed point Ifp(F&) such that 
lfp( F&) = n {live I live ;;;J F& (live)} = n {live I live = FL~ (live)} 

2.2 Theoretical Properties 
59 
(S,a1) 
~ 
(S', aD 
~ ... 
~ 
(S",af) 
~ 
a"' 
1 
I 
F LV' I 
FLV' 
I 
FLV< 
live 
live 
live 
Figure 2.4: Preservation of analysis result. 
and since Jfp(F&,) = F&,(1fp(F&,)) as well as Ifp(F&,) ;;;! F&,(Ifp(F&)) this proves 
the result. 
_ 
The next result shows that if we have a solution to the constraint system 
corresponding to some statement Sl then it will also be a solution to the 
constraint system obtained from a sub-statement 82; this result will be es-
sential in the proof of the correctness result. 
Lemma 2.16 If live 1= LV~(Sd (with Sl being label consistent) and 
!lOW(Sl) ;2 !lOW(S2) and blocks(Sl) ;2 blocks(S2) then live 1= LV~(S2) (with 
S2 being label consistent). 
-
Proof If 81 is label consistent and blocks(81) ;2 blocks(82 ) then also 82 is label 
consistent. If live F LV!;; (81 ) then live also satisfies each constraint in LV!;; (82 ) and 
hence live F LV!;; (82). 
-
We now have the following corollary expressing that the solution to the con-
straints of LV~ is preserved during computation; this is illustrated in Figure 
2.4 for finite computations. 
Corollary 2.17 If live 1= LV~(S) (for S being label consistent) and if 
(S, a) ~ (S', a') then also live 1= LV~ (8'). 
-
Proof Follows from Lemma 2.14 and 2.16. 
-
We also have an easy result relating entry and exit components of a solution. 
Lemma 2.18 If live 1= LV~(S) (with 8 being label consistent) then for 
all (l, l') E !loweS) we have liveea;it(l) ;2 liveentry(f'). 
-
Proof The result follows immediately from the construction of LV!;; (8). 
_ 
Correctness relation. Intuitively, the correctness result for the Live 
Variables Analysis should express that the sets of live variables computed by 

60 
2 Data Flow Analysis 
(B, Ul) 
--+ 
(B',O"i) 
--+ ... --+ 
(B",O"n 
--+ 
O"t 
I 
~v 
I 
~V' 
I 
~V" I 
~X(t) 
(B,0"2) 
--+ 
(B',O"~) 
--+ 
--+ 
(B",O"~) 
-+ 
U'" 
2 
V = N(init(SÂ» 
V' = N(init(S'Â» 
V" = N(init(S"Â» 
i E nna1(S) 
Figure 2.5: The correctness result for live. 
the analysis are correct throughout the computation. Only the values of the 
live variables are of interest for the computation: if a variable is not live then 
its value in the state is irrelevant - its value cannot affect the interesting 
parts of the result of the computation. Assume now that V is a set of live 
variables and define the correctness relation: 
The relation expresses that for all practical purposes the two states Ul and 
0"2 are equal: only the values of the live variables matters and here the two 
states are equal. 
Example 2.19 Consider the statement [x:=y+z]i and let VI = {y,z} and 
V2 = {x}. Then 0"1 "'Vl 0"2 means O"I(Y) = 0"2(y) 1\ Ul(Z) = 0"2(Z) and 
0"1 "'V2 0"2 means 0"1 (x) = 0"2(X). 
Next suppose that ([x:=y+z]i,O"I) -+ O"f and ([x:=y+zjl,0"2) -+ O"~. Clearly 
0"1 "'Vl U2 ensures that O"f "'V2 O"~. So if V2 is the set of variables live after 
[x:=y+z]i then VI is the set of variables live before [x:=y+zt 
_ 
The correctness result will express that the relation ""," is an invariant under 
the computation. This is illustrated in Figure 2.5 for finite computations and 
it is formally expressed by Corollary 2.22 below; to improve the legibility of 
formulae we write: 
fV(f) 
liveentry(f) 
X (f) 
Jiveexit(f) 
Since the live variables at the exit from a label are defined to be (a superset 
of) the union of the live variables at the entries of all of it successor labels, 
we have the following result. 

2.2 Theoretical Properties 
61 
Lemma 2.20 Assume live F lV~(8) with 8 being label consistent. Then 
0'1 ""X(i) 0'2 implies 0'1 ""N(Â£') 0'2 for all (Â£,Â£') E flow(8). 
â¢ 
Proof Follows directly from Lemma 2.18 and the definition of """v. 
â¢ 
Correctness result. We are now ready for the main result. It states 
how semantically correct liveness information is preserved under each step of 
the execution: (i) in the case where we do not immediately terminate and 
(ii) in the case where we do immediately terminate. 
Theorem 2.21 
If live F lV~(8) (with 8 being label consistent) then: 
(i) if (8,0'1) -+ (8',O'D and 0'1 "'N(init(SÂ» 0'2 then there exists 
O'~ such that (8,0'2) -+ (8',0'~) and O'~ "'N(init(SIÂ» 
O'~, and 
(ii) if (8,0'1) -+ O'~ and 0'1 rvN(init(SÂ» 0'2 then there exists O'~ 
such that (8,0'2) -+ O'~ and O'~ rv X(init(SÂ» O'~ 
Proof The proof is by induction on the shape of the inference tree used to establish 
(S, 0'1) -+ (S', O'D and (S,O'l) -t O'~, respectively. 
The case [ass]. Then ([a: := ay,O'l) -t 0'1[a: t-+ A[a]O'I] and from the specification 
of the constraint system we have 
N(i) = liveentry(i) = (liveexit(i)\{a:}) U FV(a) = (X(i)\{a:}) U FV(a) 
and thus 
0'1 '" N(t) 0'2 implies A[a]O'l = A[a]0'2 
because the value of a is only affected by the variables occurring in it. Therefore, 
taking 
O'~ = 0'2[a: t-+ A[a]0'2] 
we have that O'Ha:) = O'~(a:) and thus O'~ """X(t) O'~ as required. 
The case [skip]. Then ([skip]l, 0'1) -t 0'1 and from the specification of the constraint 
system 
N(i) = Jive.ntry(i) = (live.z;t(i)\0) U 0 = live.,,;t(i) = XCi) 
and we take O'~ to be 0'2. 
The case [seq1]' Then (Sl; S2,0'1) -t (S~; S2,O'D because (Sl,O'l) -t (S~,O'D. By 
construction we have BOW(Sl; S2) 2 BOW(Sl) and also bJocks(Sl; S2) 2 bJocks(Sl). 
Thus by Lemma 2.16, Jive is a solution to LV~(Sl) and thus by the induction 
hypothesis there exists O'~ such that 
(Sl,0'2) -t (S~, O'~) and O'~ """N(init(S~Â» O'~ 
and the result follows. 

62 
2 Data Flow Analysis 
The case [seQ2]' Then (81;82,0"1) -t (82,O"D because (81,0"1) -t O"~. Once again 
by Lemma 2.16, live is a solution to LV~ (8d and thus by the induction hypothesis 
there exists O"~ such that: 
Now 
{(Â£,init(82)) 1Â£ E final(8d} <;:;; flow(81; 82) 
and by Lemma 2.14, final(81) = {init(8d}. Thus by Lemma 2.20 
I 
I 
0"1 '" N(init(S2)) 0"2 
and the result follows. 
The case [ifd. Then (if [b]t then 8 1 else 82,0"1) -t (81,0"1) because B[b]O"l = 
true. 
Since 0"1 "'N(t) 0"2 and N(Â£) = liveentry(Â£) ;2 FV(b), we also have that 
B[b]0"2 = true (the value of b is only affected by the variables occurring in it) and 
thus: 
(if [b]t then 8 1 else 82,0"2) -t (81,0"2) 
From the specification of the constraint system, N (Â£) = live entry (Â£) ;2 liveexit (Â£) = 
X(Â£) and hence 0"1 "'XU) 0"2. 
Since (Â£, init(81)) E flow(8), Lemma 2.22 gives 
0"1 "'N(init(Sl)) 0"2 as required. 
The case [if2] is similar to the previous case. 
The case [whd. Then(while [by do 8,0"1) -t (8; while [b]t do 8,0"1) because B[b]O"l 
= true. Since 0"1 "'NU) 0"2 and N(Â£) ;2 FV(b), we also have that B[b]0"2 = true and 
thus 
(while [b]t do 8,0"2) -t (8; while [b]t do 8,0"2) 
and again, since N(Â£) = liveentry(Â£) ;2 liveexit(Â£) = X(Â£) we have 0"1 "'X(t) 0"2 and 
then 
0"1 '" N(init(S)) 0"2 
follows from Lemma 2.20 because (Â£, init(8)) E flow(while [b]t do 8). 
The case [wh2]. Then (while [b]t do 8,0"1) -t 0"1 because B[b]O"l = false. Since 
0"1 "'N(t) 0"2 and N(Â£) ;2 FV(b), we also have that B[b]0"2 = false and thus: 
(while [b]t do 8,0"2) -t 0"2 
From the specification of LV~(8), we have N(Â£) = liveentry(Â£) ;2 liveexit(Â£) = X(Â£) 
and thus 0"1 '" XCi) 0"2Â· 
This completes the proof. 
â¢ 
Finally, we have an important corollary which lifts the previous result to 
program executions: (i) in the case where the derivation sequence has not 
yet terminated and (ii) in the case it has terminated: 
Corollary 2.22 If live F LV~(8) (with 8 being label consistent) then: 
(i) if (8, (1) -+ * (8', aD and a1 '" N(init(S)) a2 then there exists a~ such 
that (8,a2) -t* (8',a~) and a~ "'N(init(S')) a~, and 

2.3 Monotone Frameworks 
63 
(ii) if (S,O'l) ~* O'~ and 0'1 "'N(init(SÂ» 0'2 then there exists O'~ such that 
(S, 0'2) ~* O'~ and O'~ "'X(L) O'~ for some i E final(S). 
_ 
Proof The proof is by induction on the length of the derivation sequence and uses 
Theorem 2.21. 
â¢ 
Remark. We have now proved the correctness of Live Variables Analysis 
with respect to a small step operational semantics. Obviously, the correctness 
of the analysis can also be proved with respect to other kinds of semantics. 
However, note that if one relies on, say, a big step (or natural) semantics then 
it is not so obvious how to express (and prove) the correctness of looping 
computations: in a big step semantics a looping computation is modelled by 
the absence of an inference tree - in contrast to the small step semantics 
where it is modelled by an infinite derivation sequence. 
_ 
2.3 
Monotone Frameworks 
Despite the differences between the analyses presented in Section 2.1, there 
are sufficient similarities to make it plausible that there might be an un-
derlying framework. The advantages that accrue from identifying such a 
framework include the possibility of designing generic algorithms for solving 
the data flow equations, as we will see in Section 2.4. 
The overall pattern. Each of the four classical analyses (presented in 
Subsection 2.1.1 to 2.1.4) considers equations for a label consistent program 
S* and they take the form 
Analysiso (i) 
{
I, 
iUEE 
U{Analysis.(i') I (i',i) E F} otherwise 
Analysis. (i) 
= 
i.e (Analysiso (i)) 
where 
â¢ U is n or U (and U is U or n), 
â¢ F is either Row(S*) or Raif(S*), 
â¢ E is {init(S*)} or final(S*), 
â¢ 
Â£ specifies the initial or final analysis information, and 
â¢ i.e is the transfer function associated with BL E blocks(S*). 
We now have the following characterisation: 

64 
2 Data Flow Analysis 
â¢ The forward analyses have F to be flow(S*) and then Analysiso con-
cerns entry conditions and Analysis. concerns exit conditions; also the 
equation system presupposes that S* has isolated entries. 
â¢ The backward analyses have F to be flo~(S*) and then Analysiso 
concerns exit conditions and Analysis. concerns entry conditions; also 
the equation system presupposes that S* has isolated exits. 
The principle we have seen emerging in Section 2.1 is that: 
â¢ When U is n we require the greatest sets that solve the equations 
and we are able to detect properties satisfied by all paths of execution 
reaching (or leaving) the entry (or exit) of a label; these analyses are 
often called must analyses. 
â¢ When U is U we require the least sets that solve the equations and we 
are able to detect properties satisfied by at least one execution path to 
(or from) the entry (or exit) of a label; these analyses are often called 
may analyses. 
Remark. Some authors propose a typology for Data Flow Analysis, char-
acterising each analysis by a triple from 
where -+ means forwards, +- means backwards, .j. means smallest and t means 
largest. This leads to eight possible types of analysis - a cube. In fact, given 
our association of n with t and U with .j., the cube collapses to a square. 
We have presented analyses of the following four types: (n, -+, t), (U, -+, .j.), 
(n, +-, t) and (U, +-, .j.). 
â¢ 
It is occasionally awkward to have to assume that forward analyses have iso-
lated entries and that backward analyses have isolated exits. This motivates 
reformulating the above equations to be of the form 
Analysiso ( Â£) 
Analysis. ( Â£) 
U{Analysis.(Â£') I (Â£',Â£) E F}Ul~ 
I. {l 
where lE = 
..L 
iUEE 
iUtJ-E 
where ..L satisfies 1 U ..L = 1 (hence ..L is not really there). This formulation 
makes sense also for analyses that do not have isolated entries and exits. 
In this section, we present a more formal approach to defining data flow 
frameworks that exploits the similarities that we have identified above. Noth-
ing that we present in this section is dependent on the definition of elementary 

2.3 Monotone Frameworks 
65 
blocks, or the programming language constructs; however, the techniques do 
not directly apply to languages with procedures (which will be addressed in 
Section 2.5). The view that we take here is that a program is a transition 
system; the nodes represent blocks and each block has a transfer function as-
sociated with it that specifies how the block acts on the "input" state. (Note 
that for forward analyses, the input state is the entry state, and for backward 
analyses, it is the exit state.) 
2.3.1 
Basic Definitions 
Property spaces. One important ingredient in the framework is the 
property space, L, used to represent the data flow information as well as 
the combination operator, U : P(L) -+ L, that combines information from 
different paths; as usual U : L x L -+ L is defined by hUb = U{h, b} 
and we write 1. for U 0. It is customary to demand that this property space 
is in fact a complete lattice; as discussed in Appendix A this just means 
that it is a partially ordered set, (L, ~), such that each subset, Y, has a 
least upper bound, U Y. Looking ahead to the task of implementing the 
analysis one often requires that L satisfies the Ascending Chain Condition; 
as discussed in Appendix A this means that each ascending chain, (In)n, 
i.e. h !;;; 12 ~ 13 ~ ... , eventually stabilises, i.e. 3n : In = In+! = .... 
Example 2.23 For Reaching Definitions we have L = P(Var* x Lab:) 
and it is partially ordered by subset inclusion, i.e. "~" is "~". Similarly, U Y 
is U Y, h U 12 is h U 12, and 1. is 0. That L satisfies the Ascending Chain 
Condition, i.e. that h ~ 12 ~ ... implies 3n : In = In+! = ... , follows because 
Var* x Lab: is finite (unlike Var x Lab). 
_ 
Example 2.24 For Available Expressions we have L = P(AExp*) and 
it is partially ordered by superset inclusion, i.e. "~" is "2". Similarly, U Y 
is ny, h U 12 is h n 12, and 1. is AExp*. That L satisfies the Ascending 
Chain Condition, i.e. that h 212 2 ... implies 3n : In = In+! = ... , follows 
because AExp* is finite (unlike AExp). 
_ 
Remark. Historically, the demands on the property space, L, have often 
been expressed in a different way. A join semi-lattice is a non-empty set, 
L, with a binary join operation, U, which is idempotent, commutative and 
associative, i.e. I U I = I, h U 12 = b U h and (h U b) U h = h U (12 U 13). 
The commutativity and associativity of the operation mean that it does not 
matter in which order we combine information from different paths. The join 
operation induces a partial ordering, ~, on the elements by taking h ~ 12 
if and only if h U 12 = 12 â¢ It is not hard to show that this in fact defines 
a partial ordering and that h U 12 is the least upper bound (with respect to 
~). A unit for the join operation is an element, 1., such that 1. U I = I. It is 

66 
2 Data Flow Analysis 
not hard to show that the unit is in fact the least element (with respect to 
!;). It has been customary to demand that the property space, L, is a join 
semi-lattice with a unit and that it satisfies the Ascending Chain Condition. 
As proved in Lemma A.8 of Appendix A this is equivalent to our assumption 
that the property space, L, is a complete lattice satisfying the Ascending 
Chain Condition. 
-
Some formulations of Monotone Frameworks are expressed in terms of prop-
erty spaces satisfying a Descending Chain Condition and using a combination 
operator n. It follows from the principle of lattice duality (see the Conclud-
ing Remarks of Chapter 4) that this does not change the notion of Monotone 
Framework. 
Transfer functions. Another important ingredient in the framework is 
the set of transfer functions, /Â£ : L -t L for f E Lab*. It is natural to demand 
that each transfer function is monotone, i.e. 1 !; l' implies /Â£(l) !; /Â£(l'). 
Intuitively, this says that an increase in our knowledge about the input must 
give rise to an increase in our knowledge about the output (or at the least 
that we know the same as before). Formally, we shall see that monotonicity 
is of importance for the algorithms we develop. To control the set of transfer 
functions we demand that there is a set F of monotone functions over L, 
fulfilling the following conditions: 
â¢ F contains all the transfer functions /Â£ in question, 
â¢ F contains the identity function id, and 
â¢ F is closed under composition of functions. 
The condition on the identity function is natural because of the skip state-
ment and the condition on composition of functions is natural because of 
the sequencing of statements. Clearly one can take F to be the space of 
monotone functions over L but it is occasionally advantageous to consider a 
smaller set because it makes it easier to find compact representations of the 
functions. 
Some formulations of Monotone Frameworks associate transfer functions with 
edges (or flows) rather than nodes (or labels). A similar effect can be obtained 
using the approach of Exercise 2.1l. 
Frameworks. In summary, a Monotone Framework consists of: 
â¢ a complete lattice, L, that satisfies the Ascending Chain Condition, 
and we write U for the least upper bound operator; and 
â¢ a set F of monotone functions from L to L that contains the identity 
function and that is closed under function composition. 

2.3 Monotone Frameworks 
67 
Note that we do not demand that F is a complete lattice or even a partially 
ordered set although this is the case for the set of all monotone functions 
from L to L (see Appendix A). 
A somewhat stronger concept is that of a Distributive Framework. This is a 
Monotone Framework where additionally all functions f in F are required to 
be distributive: 
f(ll u 12) = f(lr) U f(12) 
Since I(h U 12) ;! f(lr) U 1(12) follows from monotonicity, the only additional 
demand is that f(ll U 12) !;;;; I(lr) U f(12). When this condition is fulfilled it 
is sometimes possible to get more efficient algorithms. 
Instances. The data flow equations make it clear that more than just 
a Monotone (or Distributive) Framework is needed in order to specify an 
analysis. To this end we define an instance, Analysis, of a Monotone (or 
Distributive) Framework to consist of: 
â¢ the complete lattice, L, of the framework; 
â¢ the space of functions, F, of the framework; 
â¢ a finite flow, F, that typically is How(S*) or Ho~(S*); 
â¢ a finite set of so-called extremal labels, E, that typically is {init(S*)} 
or final( S* ) ; 
â¢ an extremal value, tEL, for the extremal labels; and 
â¢ a mapping, f., from the labels Lab* of F and E to transfer functions 
in Y. 
The instance then gives rise to a set of equations, Analysis=, of the form 
considered earlier: 
Analysiso(Â£) = U{Analysis.(Â£') I (l',l) E F} U t~ 
l {t 
where tE = 
.1 
Analysis. (Â£) = It (Analysiso (Â£)) 
iff E E 
iff f/. E 
It also gives rise to a set of constraints, Analysis!;, defined by: 
Analysiso(l) ;! U{Analysis.(Â£') I (l',l) E F} U t~ 
where t~ = { ~ 
Analysis. (l) 
::J It (Analysiso (l)) 
iff E E 
iff f/. E 

68 
2 Data Flow Analysis 
Available 
Reaching 
Very Busy 
Live 
Expressions 
Definitions 
Expressions 
Variables 
L 
P(AExp*) 
P(Var* x Lab:) 
P(AExp*) 
P(Var*) 
L 
::::> 
C 
::::> 
c 
-
-
-
-
-
u 
n 
u 
n 
u 
..1 
AExp* 
0 
AExp* 
0 
t 
0 
{(x,?) I XEFV(S*)} 
0 
0 
E 
{init(S*) } 
{init(S*)} 
final(S*) 
final(S*) 
F 
fIow(S*) 
fIow(S*) 
fIo~(S*) 
fIo~(S*) 
:F 
{f : L ~ L I 3lk, 19 : f(l) = (l \ lk) U 19} 
!t 
!tel) = (l \ kil1([B]l)) U gen([B]l) where [B]l E blocks(S*) 
Figure 2.6: Instances for the four classical analyses. 
2.3.2 
The Examples Revisited 
We now return to the four classical analyses from Section 2.1 and show how 
the analyses of a label consistent program, S*, can be recast as an instance of 
a Monotone (in fact Distributive) Framework. We refer to Figure 2.6 for all 
the data needed to specify the Monotone Framework as well as the instance. 
It is immediate that the property space, L, is a complete lattice in all cases. 
Given the choice of a partial ordering, ~, the information about the least 
element, ..1, and the least upper bound operation, U, is immediate. Note 
that we define ~ to be ~ for those analyses where we used U (and required 
the least solution) in Section 2.1, and similarly, that we define ~ to be ~ 
for those analyses where we used n (and required the greatest solution) in 
Section 2.1. To ensure that L satisfies the Ascending Chain Condition we have 
restricted our attention to the finite sets of expressions, labels and variables 
occurring in the program, S*, under consideration. 
The definition of the flow, F, is as one should expect: it is fIow(S*) for forward 
analyses and fIo~(S*) for backward analyses. Similarly, the extremal labels, 
E are {init(S*)} for forward analyses and final(S*) for backward analyses. 
The only thing to note about the extremal value, t, is that there seems to be 
no general pattern concerning how to define it: it is not always T L (nor is it 
always ..1L). 
It remains to show that the conditions on the set :F of transfer functions are 
satisfied. 

2.3 Monotone Frameworks 
69 
Lemma 2.25 Each of the four data flow analyses in Figure 2.6 is a Mono-
tone Framework as well as a Distributive Framework. 
_ 
Proof To prove that the analyses are Monotone Frameworks we just have to 
confirm that F has the necessary properties: 
The functions of F are monotone: Assume that l I;;;; l'. Then (l \ lk) I;;;; (l' \ lk) 
and, therefore ((l \ lk) U 19) I;;;; ((l' \ lk) U 19) and thus f(l) I;;;; f(l') as required. 
Note that this calculation is valid regardless of whether [;;; is ~ or ;2. 
The identity function is in F: It is obtained by taking both lk and 19 to be 0. 
The functions of F are closed under composition: Suppose f(l) = (l \ lk) U 19 and 
f'(l) = (l \ l~) U 
l~. Then we calculate: 
(f 0 f')(l) 
(((l \ l~) U l~) \ lk) U 19 
(l \ (l~ U lk)) U ((l~ \ lk) U 19) 
So (f 0 f')(l) = (l \ l%) U 
l~ where l% = l~ U lk and l~ = (l~ \ lk) U 19. This 
completes the proof of the first part of the lemma. 
To prove that the analyses are Distributive Frameworks consider f E F given by 
f(l) = (l \ lk) U 19. Then we have: 
f(l U l') 
((l U l') \ lk) U 19 
((l \ lk) U (l' \ lk)) U 19 
((l \ lk) U 19) U ((l' \ lk) U 19) 
f(l) U f(l') 
Note that the above calculation is valid regardless of whether U is U or n. This 
completes the proof. 
_ 
It is worth pointing out that in order to get this result we have made the 
frameworks dependent upon the actual program - this is needed to enforce 
that the Ascending Chain Condition is fulfilled. 
Example 2.26 Let us return to the Available Expressions Analysis of the 
program 
of Examples 2.4 and 2.5 and let us specify it as an instance of the associated 
Monotone Framework. The complete lattice of interest is 
(P( {a+b, a*b, a+l}), 2) 
with least element {a+b, a*b, a+l}. The set of transfer functions has the form 
shown in Figure 2.6. 
The instance of the framework additionally has the flow {(I, 2), (2,3), (3,4), 
(4,5), (5, 3)} and the set of extremal labels is {I}. The extremal value is 0 

70 
2 Data Flow Analysis 
and the transfer functions associated with the labels are 
ftE(y) = 
f:E(y) 
gE(y) = 
ftE(y) = 
ftE(y) = 
for Y ~ {a+b, a*b, a+1}. 
YU {a+b} 
YU {a*b} 
YU {a+b} 
y \ {a+b, a*b, a+l} 
Yu {a+b} 
2.3.3 
A Non-distributive Example 
â¢ 
Lest the reader should imagine that all Monotone Frameworks are Distribu-
tive Frameworks, here we present One that is not. The Constant Propagation 
Analysis will determine: 
For each program point, whether or not a variable has a constant 
value whenever execution reaches that point. 
Such information can be used as the basis for an optimisation known as 
Constant Folding: all uses of the variable may be replaced by the constant 
value. 
The Constant Propagation framework. The complete lattice 
used for Constant Propagation Analysis of a program, S*, is 
----
T 
Statecp = ((Var* ~ z h,!;;;,U,n,..1,AX.T) 
where Var * is the set of variables appearing in the program and ZT = ZU{T} 
is partially ordered as follows: 
Vz E ZT: z!;;; T 
Vz1, Z2 E Z : (ZI !;;; Z2) {:} (ZI = Z2) 
The top element of ZT is used to indicate that a variable is non-constant and 
all other elements indicate that the value is that particular constant. The 
idea is that an element a of Var * ~ ZT is a property state: for each variable 
x, a(x) will give information about whether or not x is a constant and in the 
latter case which constant. 
To capture the case where nO information is available we extend Var* ~ ZT 
with a least element ..1, written (Var* ~ ZTh. The partial ordering!;;; On 
Statecp = (Var* ~ ZTh is defined by 
Va E (Var* ~ ZTh: 
..1!;;; a 
Val,a2 E Var* ~ ZT: al!;;; a2 iff Vx : al(x) !;;; a2(x) 

2.3 Monotone Frameworks 
71 
Acp : AExp -t (SWecp -t zD 
Acp[x]i7 
{ .l 
ifi7=.l 
i7(x) 
otherwise 
Acp[n]i7 
{ n.l 
if i7 = .1 
otherwise 
transfer functions: fiP 
[x := all : fY(i7) 
{ 
.1 
if i7 = .1 
i7[x N Acp[a]i7] 
otherwise 
[skip]l : 
fY(i7) 
= 
a 
[b]l : 
fi P (0') 
0' 
Table 2.7: Constant Propagation Analysis. 
and the binary least upper bound operation is then: 
VO' E (Var* -t ZTh: 
0' U.l = 0' = .1 U 0' 
VO'l,0'2 E Var* -t ZT: Vx: (0'1 U 0'2) (x) = 0'1 (x) U 0'2 (x) 
In contrast to the earlier examples, we define the transfer functions as follows: 
:Fcp = {j I f is a monotone function on SWecp} 
It is easy to verify that SWecp and :Fcp satisfy the requirements of being a 
Monotone Framework (see Exercise 2.8). 
Constant Propagation is a forward analysis, so for the program S* we take 
the flow, F, to be fIow(S*) , the extremal labels, E, to be {init(S*)} , the 
extremal value, tcp, to be AX. T, and the mapping, f CP , of labels to transfer 
functions is given in Table 2.7. The specification of the transfer functions 
uses the function 
-
T 
Acp : AExp -t (Statecp -t Z~) 
for analysing expressions. Here the operations on Z are lifted to ZI = 
Z U {.l, T} by taking Zl Qpa Z2 = Zl 0Pa Z2 if Zl,Z2 E Z (and where 0Pa is 
the corresponding arithmetic operation on Z), Zl CPa Z2 = .1 if Zl = .1 or 
Z2 = .l and Zl Qpa Z2 = T otherwise. 
Lemma 2.27 Constant Propagation is a Monotone Framework that is not 
a Distributive Framework. 
_ 
Proof The proof that Constant Propagation is a Monotone Framework is left for 
Exercise 2.8. To show that it is not a Distributive Framework consider the transfer 

72 
2 Data Flow Analysis 
function fY for [y:=x*xjl and let 0'1 and 0'2 be such that 0'1 (x) = 1 and 0'2 (x) = -1. 
Then 0'1 U 0'2 maps x to T and thus fY(O'l U 0'2) maps y to T and hence fails to 
record that y has the constant value 1. However, both fY(O'1) and fY(O'2) map y 
to 1 and so does fY(O'r) U fY(O'2). 
â¢ 
Correctness of the analysis will be established in Section 4.5. 
2.4 
Equation Solving 
Having set up a framework, there remains the question of how to use the 
framework to obtain an analysis result. In this section we shall consider two 
approaches. One is an iterative algorithm in the spirit of Chaotic Iteration 
as presented in Section 1.7. The other more directly propagates analysis 
information along paths in the program. 
2.4.1 
The MFP Solution 
We first present a general iterative algorithm for Monotone Frameworks that 
computes the least solution to the data flow equations. Historically, this 
is called the MFP solution (for Maximal Fixed Point) although it in fact 
computes the least fixed point; the reason is that the classical literature 
tends to focus on analyses where U is n (and because the least fixed point 
with respect to [;;; or ;2 then equals the greatest fixed point with respect to 
~). 
The algorithm, written in pseudo-code in Table 2.8, takes as input an instance 
of a Monotone Framework. It uses an array, Analysis, which contains the 
Analysiso information for each elementary block; the array is indexed by 
labels. It also uses a worklist W which is a list of pairs; each pair is an element 
of the flow relation F. The presence of a pair in the worklist indicates that 
the analysis has changed at the exit of (or entry to - for backward analyses) 
the block labelled by the first component and so must be recomputed at the 
entry to (or exit from) the block labelled by the second component. As a 
final stage the algorithm presents the result (MFPo , MFP.) of the analysis 
in a form close to the formulation of the data flow equations. 
Example 2.28 To illustrate how the algorithms works let us return to 
Example 2.26 where we consider the program 
[x:=a+b]l; [y:=a*b]2;while [y>a+b]3 do ([a:=a+1]4; [x:=a+b]5) 
Writing W for the list Â«2,3),(3,4),(4,5),(5,3)) and U for the set {a+b, a*b, 
a+1}, step 1 of the algorithm will initialise the data structures as in the 
first row in Table 2.9. Step 2 will inspect the first element of the worklist 

2.4 Equation Solving 
INPUT: 
An instance of a Monotone Framework: 
(L,F,F,E,Â£,j.) 
OUTPUT: 
MFPo,MFP. 
METHOD: 
Step 1: 
Initialisation (of Wand Analysis) 
W:=nil; 
for all (l, l') in F do 
W := consÂ«l,l'),W); 
for all l in F or E do 
if lEE then Analysis[l] := Â£ 
else Analysis[l] := J.L; 
Step 2: 
Iteration (updating W and Analysis) 
while W =f. nil do 
l := fst(head(W)); l' = snd(head(W)); 
W := tail(W); 
73 
if !Â£(Analysis[l]) g Analysis[l'] then 
Analysis[l'] := Analysis[l'] U ft(Analysis[l]); 
for alii" with (l',l") in F do 
W := consÂ«l',l"),W); 
Step 3: 
Presenting the result (MFPo and MFP.) 
for all l in F or E do 
MFPo(l) := Analysis[l]; 
MFP.(l) := ft(Analysis[l]) 
Table 2.8: Algorithm for solving data flow equations. 
and rows 2-7 represent cases where there is a change in the array Analysis 
and hence a new pair is placed on top of the worklistj it is inspected in the 
next iteration. Rows 8-12 represent cases where no modification is made in 
the array and hence the worklist is getting smaller - the elements of W are 
merely inspected. Step 3 will then produce the solution we already saw in 
Example 2.5. 
_ 
Properties of the algorithm. We shall first show that the algorithm 
computes the expected solution to the equation system. 
Lemma 2.29 The worklist algorithm in Table 2.8 always terminates and 
it computes the least (or MFP) solution to the instance of the framework 
given as input. 
_ 
Proof First we prove the termination result. Step 1 and 3 are bounded loops over 
finite sets and thus trivially terminate. Next consider step 2. Assume that there 

74 
2 Data Flow Analysis 
Analysis[f] for f being 
W 
1 
2 
3 
4 
5 
1 
((1,2),W) 
0 
U 
U 
U 
U 
2 
((2,3),W) 
0 
{a+b} 
U 
U 
U 
3 
((3,4),W) 
0 
{a+b} 
{a+b,a*b} 
U 
U 
4 
((4,5),W) 
0 
{a+b} 
{a+b,a*b} 
{a+b,a*b} 
U 
5 
((5,3),W) 
0 
{a+b} 
{a+b,a*b} 
{a+b,a*b} 
0 
6 
((3,4),W) 
0 
{a+b} 
{a+b} 
{a+b,a*b} 
0 
7 
((4,5),W) 
0 
{a+b} 
{a+b} 
{a+b} 
0 
8 
((2,3),Â·Â· .) 
0 
{a+b} 
{a+b} 
{a+b} 
0 
9 
((3,4),Â·Â· .) 
0 
{a+b} 
{a+b} 
{a+b} 
0 
10 
((4,5),Â·Â· .) 
0 
{a+b} 
{a+b} 
{a+b} 
0 
11 
((5,3)) 
0 
{a+b} 
{a+b} 
{a+b} 
0 
12 
0 
0 
{a+b} 
{a+b} 
{a+b} 
0 
Table 2.9: Iteration steps of the worklist algorithm. 
are b labels in the program. Then the worklist initially has at most b2 elementsj 
the worst case is that F associates every label to every label. Each iteration either 
deletes an element from the worklist or adds up to b new elements. New elements 
are added iffor the pair selected in this iteration, (Â£,Â£'), we have ii(Analysis[Â£]) g 
Analysis[Â£']j that is, ii(Analysis[Â£]) ::J Analysis[.e'] or they are incomparable. In either 
case, the new value of Analysis[Â£'] is strictly greater than the previous one. Since 
the set of values satisfies the Ascending Chain Condition, this can only happen a 
finite number of times. Thus the worklist will eventually be exhausted. 
Next we prove the correctness result. Let Analysiso and Analysis. be the least 
solution to the instance given as input to the algorithm. The proof is now in three 
parts: (i) first we show that on each iteration the values in Analysis are approxi-
mations to the corresponding values of Analysiso , (ii) then we show that Analysiso 
is an approximation to Analysis at the termination of step 2 of the algorithm, and 
(iii) finally we combine these results. 
Part (i). We show that 
'i.e : Analysis[Â£] !;;; Analysiso (Â£) 
is an invariant ofthe loop of step 2. After step 1 we have Analysis[Â£] !;;; Analysiso(Â£) 
for all Â£ because Analysiso (Â£) ;;;! L whenever Â£ E E. After each iteration through the 
loop either there is no change because the iteration just deletes an element from 
the worklist or else Analysis[Â£"] is unchanged for all Â£" except for some Â£'. In that 
case there is some Â£ such that (Â£, Â£') E F and 
new Ana Iysis[.e'] 
oldAnalysis[.e'] u ii (oldAna Iysis[ Â£]) 
C 
Analysiso (Â£') U Ii (Analysiso (Â£) ) 
Analysiso (.e') 

2.4 Equation Solving 
75 
The inequality follows since h is monotone and the last equation follows from 
(Analysiso, Analysis.) being a solution to the instance. 
Part (ii). On termination of the loop, the worklist is empty. We show that 
ve,e' : (e,i) E F => Analysis[i] ;;;) ft(Analysis[e]) 
by contradiction. So suppose that Analysis[e'] :f! ft(Analysis[e]) for some (e, e') E 
F and let us obtain a contradiction. Consider the last time that Analysis[e] was 
updated. If this was in step 1 we considered (e,l') in step 2 and ensured that 
Analysis[i] ;;;) ft(Analysis[e]) 
and this invariant has been maintained ever since; hence this case cannot apply. 
It follows that Analysis[e] was last updated in step 2. But at that time (e, l') was 
placed in the worklist once again. When considering (e, l') in step 2 we then ensured 
that 
Analysis[e'] ;;;) ft(Analysis[e]) 
and this invariant has been maintained ever since; hence this case cannot apply 
either. This completes the proof by contradiction. 
On termination of the loop we have: 
ve E E : Analysis[e] ;;;) Â£ 
This follows because it was established in step 1 and it is maintained ever since. 
Thus it follows that at termination of step 2: 
\If : Analysis[e] ;;;) (U{ff' (Analysis[i]) I (i, e) E F}) u Â£~ 
Part (iii). By our assumptions and Proposition A.l0 we have 
ve: MFPo(e) ;;;) Analysiso(e) 
since Analysiso(e) is the least solution to the above constraint system and MFPo 
equals the final value of Analysis. Together with part (i) this proves that 
\If: MFPo[e] = Analysiso(e) 
upon termination of step 2. 
â¢ 
Based on the proof of termination in Lemma 2.29 we can determine an upper 
bound on the number of basic operations (for example an application of !Â£, 
an application of u, or an update of Analysis) performed by the algorithm. 
For this we shall assume that the flow F is represented in such a way (for 
example an array of lists) that all (Â£', Â£/1) emanating from Â£' can be found in 
time proportional to their number. Suppose that E and F contain at most 
b 2: 1 distinct labels, that F contains at most e ~ b pairs, and that L has 
finite height at most h 2: 1. Then steps 1 and 3 perform at most O(b + e) 
basic operations. Concerning step 2 a pair is placed on the worklist at most 
O(h) times, and each time it takes only a constant number of basic steps to 

76 
2 Data Flow Analysis 
process it - not counting the time needed to add new pairs to W; this yields 
at most O(e . h) basic operations for step 2. Since h ~ 1 and e ~ b this 
gives at most O(e . h) basic operations for the algorithm. (Since e ::; b2 a 
potentially coarser bound is O(b2 . h).) 
Example 2.30 Consider the Reaching Definitions Analysis and suppose 
that there are at most v ~ 1 variables and b ~ 1 labels in the program, S*, 
being analysed. Since L = P(Var* x Lab:), it follows that h ::; vÂ· b and thus 
we have an O(v . b3 ) upper bound on the number of basic operations. 
Actually we can do better. If S* is label consistent then the variable of the 
pairs (x,f) of P(Var* x Lab:) will always be uniquely determined by the 
label f so we get an O(b3 ) upper bound on the number of basic operations. 
Furthermore, F is flow( S*) and inspection of the equations for flow( S*) shows 
that for each label Â£ we construct at most two pairs with f in the first com-
ponent. This means that e ::; 2Â· b and we get an O(b2 ) upper bound on the 
number of basic operations. 
_ 
2.4.2 
The MOP Solution 
Let us now consider the other solution method for Monotone Frameworks 
where we more directly propagate analysis information along paths in the 
program. Historically, this is called the MOP solution (for Meet Over all 
Paths) although we do in fact take the join (or least upper bound) over 
all paths leading to an elementary block; once again the reason is that the 
classical literature tends to focus on analyses where U is n. 
Paths. For the moment, we adopt the informal notion of a path to the 
entry of a block as the list of blocks traversed from the start of the program 
up to that block (but not including it); analogously, we can define a path 
from an exit of the block. Data Flow Analyses determine properties of such 
paths. Forward analyses concern paths from the initial block to the entry 
of a block; backward analyses concern paths from the exit of a block to a 
final block. The effect of a path on the state can be computed by composing 
the transfer functions associated with the individual blocks in the path. In 
the forward case we collect information about the state of affairs before the 
block is executed and in the backward case we collect information about the 
state of affairs immediately after the block has been executed. This informal 
description contrasts with the approach taken in Section 2.1 and earlier in 
this section; there we presented equations which were defined in terms of the 
immediate predecessors (successors) of a block (as defined by the flow and 
flo~ functions). We will see later that, for a large class of analyses, these 
two approaches coincide. 
For the formal development let us consider an instance (L,F,F,E,~,f.) of 
a Monotone Framework. We shall use the notation l = [f1 ,Â·Â·Â·, fn] for a 

2.4 Equation Solving 
77 
sequence of n 2: 0 labels. We then define two sets of paths. The paths up to 
but not including fare 
and the paths up to and including fare: 
For a path f = [f 1, ... ,fn] we define the transfer function 
so that for the empty path we have i[ 1 = id where id is the identity function. 
By analogy with the definition of solutions to the equation system, in par-
ticular MFPo(f) and MFP.(f), we now define two components of the MOP 
solution. The solution up to but not including f is 
and the solution up to and including f is: 
MOP.(f) = U{fi\L) If E path.(f)} 
Unfortunately, the MOP solution is sometimes uncomputable (meaning that 
it is undecidable) even though the MFP solution is always easily computable 
(because of the property space satisfying the Ascending Chain Condition); 
the following result establishes one such result: 
Lemma 2.31 The MOP solution for Constant Propagation is undecid-
able. 
â¢ 
Proof Let Ul,Â·Â·Â·, Un and Vl,Â·Â·Â·, Vn be strings over the alphabet {I,Â·Â· Â·,9} (see Ap-
pendix C). The Modified Post Correspondence Problem is to determine whether or 
not there exists a sequence i 1 , ... , im with il = 1 such that Uil ... Ui~ = Vi l ... Vin â¢ 
Let I u I denote the length of the string u and let [u] be its value interpreted as a 
natural number. Consider the program (omitting most labels) 
x:=[ul]j y:=[vl]j 
while [ ... j do 
(if [ ... j then x:=x * 101''11 + [Ul]j y:=y * 10lvlI + [vl] else 
if [ ... j then x:=x * 10lunl + [un]j y:=y * 10Ivnl + [vn] else skip) 
[z: =sign((x-y)*(x-y))jl 

78 
2 Data Flow Analysis 
where sign gives the sign (which is 1 for a positive argument and 0 or -1 otherwise) 
and where the details of [ ... ] are of no concern to us. 
Then MOP.(Â£) will map z to 1 if and only if the Modified Post Correspondence 
Problem has no solution. Since the Modified Post Correspondence problem is un-
decidable [76] so is the MOP solution for Constant Propagation (assuming that our 
selection of arithmetic operations does indeed allow those used to be defined). 
_ 
MOP versus MFP solutions. We shall shortly prove that the MFP 
solution safely approximates the MOP solution (informally, MFP :;;;) MOP). 
In the case of a (n, -+, t) or (n, f-, t) analysis, the MFP solution is a subset of 
the MOP solution (:;;;) is ~); in the case of a (U, -+, -l-) or (U, f-, -l-) analysis, 
the MFP solution is a superset of the MOP solution. We can also show 
that, in the case of Distributive Frameworks, the MOP and MFP solutions 
coincide. 
Lemma 2.32 Consider the MFP and MOP solutions to an instance (L, F, 
F, B, t, f.) of a Monotone Framework; then: 
MFPo :;;;) MOPo and MFP. :;;;) MOP. 
If the framework is distributive and if patho (Â£) "I 0 for all Â£ in E and F then: 
MFPo = MOPo and MFP. = MOP. 
Proof It is straightforward to show that: 
VÂ£: MOP.(Â£) ~ h(MOPo(Â£)) 
VÂ£: MFP.(Â£) = h(MFPo(Â£)) 
For the first part of the lemma it therefore suffices to prove that: 
VÂ£: MOPo(Â£) ~ MFPo(Â£) 
Note that MFP 0 is the least fixed point of the functional F defined by: 
-
Next let us restrict the length of the paths used to compute MOPo; for n ;:::: 0 
define: 
MOP~(Â£) = U{h(L) liE patho(Â£), III < n} 
Clearly, MOPo(Â£) = Un MOP~(Â£) and to prove MFPo ;;;;! MOPo is therefore suffices 
to prove 
Vn: MFPo ;;;;! MOP~ 
and we do so by numerical induction. The basis, MFPo ;;;;! MOP~, is trivial. The 
inductive step proceeds as follows: 
MFPo(Â£) 
= 
F(MFPo)(Â£) 

2.4 Equation Solving 
(U{fdMFPo(Â£')) I (Â£',C) E F}) U ~1, 
::J 
(U{fdMOP~(Â£'Â» I (Â£',C) E F}) U ~1, 
(U{ftl (U{ft(~) If E patho(Â£'), If I < n}) I (Â£', C) E F}) U ~1, 
::J 
(U({U{fdft(~Â» If E patho(Â£'), If I < n} I (Â£',C) E F}) U ~1, 
= 
U({ft(~) If E patho(C), 1:::; If I :::; n}) U ~1, 
MOP~+1(C) 
79 
where we have used the induction hypothesis to get the first inequality. This com-
pletes the proof of MFPo ~ MOPo and MFP. ~ MOP â¢. 
To prove the second part of the lemma we now assume that the framework is 
distributive. Consider C in E or F. By assumption It is distributive, that is 
It(h U l2) = It(h) U It(h), and from Lemma A.9 of Appendix A it follows that 
!t(U Y) = U{ft(l) ICE Y} 
whenever Y is non-empty. By assumption we also have patho(C) "# 0 and it follows 
that 
and this shows that: 
Next we calculate: 
U{ft(ft(~Â» If E patho(Cn 
U{ft(~) leE path.(Cn 
VC: !t(MOPo(CÂ» = MOP.(C) 
MOPo(C) 
= 
U{ft(~) If E patho(Cn 
= 
U{ft(~) If E U{path.(Â£') I (Â£',C) E F} U {[ 11 C E E}} 
= 
U({fttCft(~Â» If E patho(Â£'), (Â£',f) E F} U {~I C E E}) 
(U{ftl (U{ft(~) If E patho(Â£'n I (Â£', C) E F}) U ~1, 
(U{ftl(MOPo(Â£'Â» I (Â£',C) E F}) U ~1, 
Together this shows that (MOPo, MOP.) is a solution to the data flow equations. 
Using Proposition A.IO of Appendix A and the fact that (MFPo, MFP.) is the least 
solution we get MOPo ~ MFPo and MOP. ~ MFP â¢. Together with the results of 
the first part of the lemma we get MOPo = MFPo and MOP. = MFP.. 
â¢ 
We shall leave it to Exercise 2.13 to show that the condition that patho (Â£) =f- 0 
(for e in E and F) does hold when the Monotone Framework is constructed 
from a program S* in the manner of the earlier sections. 
It is sometimes stated that the MOP solution is the desired solution and 
that one only uses the MFP solution because the MOP solution might not be 

80 
2 Data Flow Analysis 
computable. In order to validate this belief we would need to prove that the 
MOP solution is semantically correct as was proved for the MFP solution in 
Section 2.2 in the case of Live Variables Analysis - in the case of Live Variables 
this is of course immediate since it is a Distributive Framework. We shall not 
do so because it is always possible to formulate the MOP solution as an MFP 
solution over a different property space (like P(L)) and therefore little is lost 
by focusing on the fixed point approach to Monotone Frameworks. (Also 
note that P(L) satisfies the Ascending Chain Condition when L is finite.) 
2.5 
Interprocedural Analysis 
The Data Flow Analysis techniques that have been presented in the previous 
sections are called intraproceduml analyses because they deal with simple 
languages without functions or procedures. It is somewhat more demand-
ing to perform interproceduml analyses where functions and procedures are 
taken into account. Complications arise when ensuring that calls and returns 
match one another, when dealing with parameter mechanisms (and the alias-
ing that may result from call-by-reference) and when allowing procedures as 
parameters. 
In this section we shall introduce some of the key techniques of interproce-
dural analysis. To keep things simple we just extend the WHILE language 
with top-level declarations of global mutually recursive procedures having a 
call-by-value parameter and a call-by-result parameter. The extension of the 
techniques to a language where procedures may have multiple call-by-value, 
call-by-result and call-by-value-result parameters is straightforward and so 
is the extension with local variable declarations (see Exercise 2.20); we shall 
freely use these extensions in examples. 
Syntax of the procedure language. A program, P*, in the ex-
tended WHILE-language has the form 
begin D* S* end 
where D* is a sequence of procedure declarations: 
D ::= proc p(val x,res y) istn S endt~ I D D 
Procedure names (denoted p) are syntactically distinct from variables (de-
noted x and y). The label f!n of is marks the entry to the procedure body 
and the label Cre of end marks the exit from the procedure body. The syntax 
of statements is extended with: 
S ::= ... I [call p(a, z)l~~ 
The call statement has two labels: f!c will be used for the call of the procedure 
and f!r will be used for the associated return; the actual parameters are a 
and z. 

2.5 Interprocedural Analysis 
81 
The language is statically scoped, the parameter mechanism is call-by-value 
for the first parameter and call-by-result for the second parameter and the 
procedures may be mutually recursive. We shall assume throughout that 
the program is uniquely labelled (and hence label consistent); also we shall 
assume that only procedures that have been declared in D* are ever called 
and that D* does not contain two definitions of the same procedure name. 
Example 2.33 Consider the following program calculating the Fibonacci 
number of the positive integer stored in x and returning it in y: 
begin proc fib(val z, u, res v) isl 
if [z<3]2 then [v:=u+l]3 
else ([call fib(z-l,u,v)]t; [call fib(z-2,v,v)]~) 
endS; 
[call fib(x,O,y)]~o 
end 
It uses the procedure fib that returns in v the Fibonacci number of z plus 
the value of u. Both x and y are global variables whereas z, u and v are 
formal parameters and hence local variables. 
_ 
Flow graphs for statements. The next step is to extend the defi-
nitions of the functions init, final, blocks, labels, and flow to specify the flow 
graphs also for the procedure language. For the new statement we take: 
init([ call p( a, z) ]~~) 
final([call p(a, z)]~~) 
blocks([call p(a, z)]~~) = 
labels([call p(a,z)]~~) = 
flow([call p(a,z)]~~) 
{I!r} 
{[call p(a, z)]~~} 
{ie,l!r} 
{(ie;in),(ix;ir)} 
if proc p(val x, res y) isl" S endlz 
is in D* 
Here (le; in) and (lx; lr) are new kinds of flows: 
â¢ (le; in) is the flow corresponding to calling a procedure at ie and with 
in being the entry point for the procedure body, and 
â¢ (lx; lr) is the flow corresponding to exiting a procedure body at lx and 
returning to the call at lr. 
The definition of flow([call p(a, z)]~~) exploits the fact that the syntax of 
procedure calls only allows us to use the (constant) name of a procedure 

82 
2 Data Flow Analysis 
defined in the programj had we been allowed to use a variable that denotes 
a procedure (e.g. because it was a formal parameter to some procedure or 
because it was a variable being assigned some procedure) then it would be 
much harder to define flow([call pea, z)l~~). This is often called the dynamic 
dispatch problem and we shall deal with it in Chapter 3. 
Flow graphs for programs. Next consider the program P* of the 
form begin D* S* end. For each procedure declaration proc p(val x, res y) 
isfn S endf~ we set 
init(p) 
final(p) = 
blocks(P) = 
labels(p) 
flow(p) 
en 
{e",} 
{isi n , ende~} U blocks(S) 
{en, e",} U labels(S) 
{(en, init(S))} U flow(S) U {(e, e",) leE finaleS)} 
and for the entire program P* we set 
init* 
final* 
blocks* 
labels* 
flow* 
= 
= 
= 
init(S*) 
final(S*) 
U{blocks(p) I proc p(val x,res y) isfn S endf~ is in D*} 
U blocks(S*) 
U{labels(p) I proc p(val x,res y) isin S endix is in D*} 
U labels( S*) 
U{flow(p) I proc p(val x, res y) isfn S endix is in D*} 
U flow(S*) 
as well as Lab* = labels*. 
We shall also need to define a notion of interproceduml flow 
inter-flow* = ((ec,en,e""er ) I P* contains [call p(a,z)l~~ 
as well as proc p( val x, res y) isln S endix } 
that clearly indicates the relationship between the labels of a procedure call 
and the corresponding procedure body. This information will be used later to 
analyse procedure calls and returns more precisely than is otherwise possible. 
Indeed, suppose that inter-flow* contains (e~, en, e"" e~) for i = 1,2 in which 
case flow* contains (e~jen) and (e",je~) for i = 1,2. But this "gives rise to" 
the four tuples (e~, en, ex, et) for i = 1,2 and j = 1,2 and only the tuples 
with i = j match the return with the call: these tuples are exactly the ones 
in inter-flow*. 

2.5 Interprocedural Analysis 
83 
proc fib (val z, u, res v) 
isl 
â¢ 
! 
! [z<3j2! 
no 
I 
! 
!yes 
I [call fib(x,O,y)]~o I I [V:=U+l]31 
I [call fib(z-l,u,v)]~ I: -
-
! 
! 
I [call fib(Z-2,v,v)]~I: 
I 
~ 
ends 
Figure 2.7: Flow graph for the Fibonacci program. 
Example 2.34 For the Fibonacci program considered in Example 2.33 
we have 
flow* = {(I, 2), (2,3), (3,8), 
(2,4), (4; 1), (8; 5), (5,6), (6; 1), (8; 7), (7,8), 
(9; 1), (8; IOn 
inter-flow* 
= 
{(9, 1,8,10), (4, 1,8,5), (6,1,8, 7n 
and init* = 9 and fina1* = {1O}. The corresponding flow graph is illustrated 
in Figure 2.7. 
â¢ 
For a forward analysis we use F = flow* and E = {init*} much as before 
and we introduce a new "metavariable" IF = inter-flow* for the interpro-
cedural flow; for a backward analysis we use F = flow!:, E = fin~ and 
IF = inter-flow!:. Most of the explanations in the sequel will focus on for-
ward analyses. 
2.5.1 
Structural Operational Semantics 
We shall now show how the semantics of WHILE can be extended to cope 
with the new constructs. To ensure that the language allows local data in 

84 
2 Data Flow Analysis 
procedures we shall need to distinguish between the values assigned to dif-
ferent incarnations of the same variable and for this we introduce an infinite 
set of locations (or addresses): 
e E Loc 
locations 
An environment, p, will map the variables in the current scope to their loca-
tions, and a store, t;, will then specify the values of these locations: 
P 
E Env 
t; 
E Store 
Var* ~ Loc 
Loc ~fin Z 
environments 
stores 
Here Var* is the (finite) set of variables occurring in the program and 
Loc ~fin Z denotes the set of partial functions from Loc to Z that have 
a finite domain. Thus the previously used states (T EState = Var* ~ Z 
have been replaced by the two mappings p and t; and can be reconstructed 
as (T = t; 0 p: to determine the value of a variable x we first determine its 
location e = p(x) and next the value t;(e) stored in that location. For this 
to work it is essential that t; 0 p : Var * ~ Z is a total function rather than 
a partial function; in other words, we demand that ran(p) ~ dom(t;) where 
ran(p) = {p(x) I x E Var*} and dom(t;) = {e I t; is defined on O. 
The locations of the global variables of the program P* are given by a top-level 
environment denoted p*; we shall assume that it maps all variables to unique 
locations. The semantics of statements is now given relative to modifications 
of this environment. The transitions have the general form 
in case that the computation does not terminate in one step, and the form 
in case that it does terminate in one step. It is fairly straightforward to 
rewrite the semantics of WHILE given in Table 2.6 to have this form; as an 
example the clause [ass] for assignments becomes: 
p 1-* (x:=a,t;) ~ t;[p(x) I--t A[a](t; 0 p)] 
if t; 0 p is total 
Note that there is no need to modify the semantics of arithmetic and boolean 
expressions. 
For procedure calls we make use of the top-level environment, P*, and we 
take: 

2.5 Interprocedural Analysis 
85 
p 1-* ([call p(a,z)]~~,c;;) -t 
(bind p*[x J---t 6,y J---t e2] in 8 then z:=y,c;;[6 t-+ A[a](c;; 0 p),6 t-+ v]) 
where eb6 Â¢. dom(p),v E Z 
and proc p(val x, res y) isin 8 endiz is in D* 
The idea is that we allocate new locations 6 and 6 for the formal parameters 
x and y, and we then make use of a bind-construct to combine the procedure 
body 8 with the environment p*[x J---t el, y J---t 6] in which it must be executed 
and we also record that the final value of y must be returned in the actual 
parameter z. At the same time the store is updated such that the new location 
for x is mapped to the value of the actual parameter a whereas we do not 
control the initial value, v, of the new location for y. The bind-construct is 
only needed to ensure that we have static scope rules and its semantics is as 
follows: 
pI 1-* (8, c;;) -t (81, C;;/) 
p 1-* (bind pI in 8 then z:=y,c;;) -t (bind pI in 8 1 then Z:=y,C;;/) 
pI 1-* (8,c;;) -t C;;I 
p 1-* (bind pI in 8 then z:=y,C;;) -t C;;/[p(Z) J---t C;;/(pl(y))] 
The first rule expresses that executing one step of the body of the construct 
amounts to executing one step of the construct itselfj note that we use the 
local environment when executing the body. The second rule expresses that 
when the execution of the body finishes then so does execution of the con-
struct itself and we update the value of the global variable z to be that of the 
local variable Yj furthermore, there is no need for the local environment pI to 
be retained as subsequent computations will use the previous environment p. 
Remark. 
Although the semantics works with two mappings, an environ-
ment and a store, it is often the case that the analysis abstracts the state, 
i.e. the composition of the environment and the store. The correctness of the 
analysis will then have to relate the abstract state both to the environment 
and the store. 
The correctness result will often be expressed in the style of Section 2.2: 
information obtained by analysing the original program will remain correct 
under execution of the program. The semantics presented above deviates 
from that of the WHILE-language in that it introduces the bind-construct 
which is only used in the intermediate configurations. So in order to prove 
the correctness result we will also need to specify how to analyse the bind-
construct. We refer to Chapter 3 for an illustration of how to do this. 
â¢ 

86 
2 Data Flow Analysis 
2.5.2 
Intraprocedural versus Interprocedural Analysis 
To appreciate why interprocedural analysis is harder than intraprocedural 
analysis let us begin by just naively using the techniques from the previous 
sections. For this we suppose that: 
â¢ for each procedure call [call p( a, z) ]~~ we have two transfer functions 
he and hr corresponding to calling the procedure and returning from 
the call, and 
â¢ for each procedure definition proc p(val x, res y) is1n S end1", we 
have two transfer functions hn and h", corresponding to entering and 
exiting the procedure body. 
A naive formulation. Given an instance (Â£, F, F, E, L, f.) of a Mono-
tone Framework we shall now treat the two kinds of flow ((f1,f2) versus 
(fe;fn) and (f",;fr)) in the same way: we interpret the semi-colon as stand-
ing for a comma. While a Monotone Framework is allowed to interpret all 
transfer functions freely, we shall for now naively assume that the two trans-
fer functions associated with procedure definitions are the identity functions, 
and that the two transfer functions associated with each procedure call are 
also the identity functions, thus effectively ignoring the parameter-passing. 
We now obtain an equation system of the form considered in the previous 
sections: 
h(Ao(f)) 
A.(f) 
Ao(f) 
U{A.(f') I (f',f) E F or (f';f) E F} U L~ 
Here L~ is as in Section 2.3: 
iffEE 
iff tt E 
When inspecting this equation system is should be apparent that both proce-
dure calls (fe; fn) and procedure returns (fx; fr) are treated like goto's: there 
is no mechanism for ensuring that information flowing along (fe; fn) from a 
call to a procedure only flows back along (fx; fr) from the procedure to the 
same call. (Indeed, nowhere does the formulation consult the interprocedu-
ral flow, IF.) Expressed in terms of the flow graph in Figure 2.7, there is 
nothing preventing us from considering a path like [9,1,2,4,1,2,3,8,10] that 
does not correspond to a run of the program. Intuitively, the equation system 
considers a much too large set of "paths" through the program and hence 
will be grossly imprecise (although formally on the safe side). 

2.5 Interprocedural Analysis 
87 
Valid paths. A natural way to overcome this shortcoming is to somehow 
restrict the attention to paths that have the proper nesting of procedure calls 
and exits. We shall explore this idea in the context of redefining the MOP 
solution of Section 2.4 to only take the proper set of paths into account, 
thereby defining an MVP solution (for Meet over all Valid Paths). 
So consider a program P* of the form begin D* S* end. A path is said to be 
a complete path from .e1 to .e2 in P* if it is has proper nesting of procedure 
entries and exits and such that a procedure returns to the point where it was 
called. These paths are generated by the nonterminal CPl1h according to 
the following productions: 
CPl1 ,Â£2 --t .e1 
whenever.e1 =.e2 
CPl1 ,l3 --t .e1 , CPl2 ,l3 
whenever (.e1,.e2 ) E F; 
for a forward analysis this means 
that (f1,f2) E flow* 
CPÂ£c'Â£ --t fe, CPln,l", , CPl.,l 
whenever (fe,fn,fx,.er) ElF; 
for a forward analysis this means 
that P* contains [call p( a, z) ]~~ 
and proc p(val x,res y) isln S endl " 
The matching of calls and returns is ensured by the last kind of productions: 
the flows (.ee; .en) and (.ex;.er) are forced to obey a parenthesis structure in 
that .ee,.en will be in the generated path only if there is a matching occur-
rence of .ex, fr - and vice versa. Hence for a forward analysis, a terminating 
computation will give rise to a complete path from init* to one of the labels 
of final*. Note that the grammar constructed above will only have a finite 
set of nonterminals because there is only a finite set of labels in P*. 
Example 2.35 For the Fibonacci program of Example 2.33 we obtain the 
following grammar (using forward flow and ignoring the parts not reachable 
from CP9,1O): 
CP9 ,10 
--t 9, CP1,8, CPlO,10 
CP3,8 
--t 3, CP8,8 
CPlO,10 
--t 10 
CP8,8 
--t 8 
CP1,8 
--t 1, CP2,8 
CP4 ,8 
--t 4, CP1,8, CP5,8 
CP2,8 
--t 2, CP3,8 
CP5,8 
--t 5, CP6,8 
CP2 ,8 
--t 2, CP4 ,8 
CP6,8 
--t 6, CP1,8, CP7,8 
CP7 ,8 
--t 7, CP8,8 
It is now easy to verify that the path [9,1,2,4,1,2,3,8,5,6,1,2,3,8,7,8,10] 
is generated by CP9 ,10 whereas the path [9,1,2,4,1,2,3,8,10] is not. 
â¢ 
A path is said to be a valid path if it starts at an extremal node of P* and 
if all the procedure exits match the procedure entries but it is possible that 

88 
2 Data Flow Analysis 
some procedures are entered but not yet exited. This will obviously include 
all prefixes of the complete paths starting in E but we also have to take into 
account prefixes of computations that might not terminate. To specify the 
valid paths we therefore construct another grammar with productions: 
VP* --+ VPll,l2 
whenever Â£1 E E and Â£2 E Lab* 
VPl1 ,l2 --+ Â£1 
whenever Â£1 = Â£2 
VPl1,l3 --+ Â£1, VPl2 ,l3 
whenever (Â£1,Â£2) E F 
VPle,l --+ Â£e, CPln,lx' VPlr,l 
whenever (Â£e,Â£n,Â£",,Â£r) E IF 
VPle,l --+ Â£e, VPln,l 
whenever (Â£e,Â£n,Â£",,Â£r) E IF 
The valid paths will then be generated by the nonterminal VP*. For a for-
ward analysis, to go from the label Â£e of a procedure call to the program point 
Â£ there are two possibilities. One is that the call initiated at Â£e terminates 
before reaching Â£ and this corresponds to the second last kind of production 
where we use the nonterminal CPln,lx to generate the complete path corre-
sponding to executing the procedure body. The other possibility is that Â£ 
is reached before the call terminates and this corresponds to the last kind 
of production where we simply use VPln,l to generate a valid path in the 
procedure body. 
We can now modify the two sets of paths defined in Section 2.3 as follows 
(keeping in mind that the definitions are implicitly parameterised on F, E 
and IF): 
vpatho(Â£) 
{[Â£1, ... , Â£n-1] I n ~ 1/\ Â£n = Â£ /\ [Â£1, ... , Â£n] is a valid path} 
vpath.(Â£) 
Clearly the sets of paths are smaller than what would have resulted if we 
had merely regarded (Â£1; Â£2) as standing for (Â£1, Â£2) and had used the notions 
patho(Â£) and path.(Â£) of Subsection 2.4.2. 
Using valid paths we now define the MVP solution as follows: 
MVPo(Â£) = U{fi\L) If E vpatho(Â£)} 
Mvp.(e) = U{fi\L) If E vpath.(e)} 
Since the sets of paths are smaller than in the similar definitions in Subsection 
2.4.2, we clearly have MVPo(Â£) [; MOPo(Â£) and MVP.(Â£) [; MOP.(Â£) for 
all e. 
2.5.3 
Making Context Explicit 
The MVP solution may be undecidable for lattices of finite height, just as was 
the case for the MOP solution, so we now have to reconsider the MFP solution 

2.5 Interprocedural Analysis 
89 
and how to avoid taking too many invalid paths. An obvious approach is 
to encode information about the paths taken into the data flow properties 
themselves; to this end we introduce context information: 
context information 
The context may simply be an encoding of the path taken but we shall see 
in Subsection 2.5.5 that there are other possibilities. We shall now show how 
an instance of a Monotone Framework (as introduced in Section 2.3) can be 
extended to take context into account. 
The intraprocedural fragment. Consider an instance (L,:F, F, E, 
L, f.) of a Monotone Framework. We shall now construct an instance 
(L, F, F, E,t;, 1.) 
of an embellished Monotone Framework that takes context into account. We 
begin with the parts of its definition that are independent of the actual choice 
of b., Le. the parts that correspond to the intraprocedural analysis: 
â¢ L - b. -+ L' 
-
, 
â¢ the transfer functions in F are monotone; and 
â¢ each transfer function h is given by h(l) (8) = h(l (8)). 
In other words, the new instance applies the transfer functions ofthe original 
instance in a pointwise fashion. 
Ignoring procedures, the data flow equations will take the form displayed 
earlier: 
A.(Â£) 
h (Ao(Â£)) 
for all labels that do not label a procedure call 
(Le. that do not occur as first or fourth components 
of a tuple in IF) 
U{A.(Â£') I (Â£',Â£) E F or (Â£';e) E F} U ~ 
for all labels (including those that label procedure calls) 
Example 2.36 Let (Lsign, :Fsign, F, E, Lsign, pign) be an instance of a Mono-
tone Framework specifying a Detection of Signs Analysis (see Exercise 2.15) 
and assume that 
Lsign = P(Var* -+ Sign) 

90 
2 Data Flow Analysis 
where Sign = {-, 0, +}. Thus Lsign describes sets of abstract states a sign map-
ping variables to their possible signs. The transfer function f;ign associated 
with the assignment [x := ajt will now be written as 
f;ign (Y) = U{ <p~ign (asign) I a sign E Y} 
where Y ~ Var* -t Sign and 
<p~ign(aSign) = {asign[x f--t sjl s E Asign[a](asignn 
Here Asign : AExp -t (Var* -t Sign) -t P(Sign) specifies the analysis of 
arithmetic expressions. The transfer functions for tests and skip-statements 
are the identity functions. 
Given a set 6. of contexts, the embellished Monotone Framework will have 
t:; = 6. -t Lsign 
but we shall prefer the following isomorphic definition 
t:;, = P(6. x (Var* -t Sign)) 
Thus t:;, describes sets of pairs of context and abstract states. The transfer 
function associated with the assignment [x := ajt will now be: 
In subsequent examples we shall further develop this analysis. 
â¢ 
The interprocedural fragment. It remains to formulate the data 
flow equations corresponding to procedures. 
For a procedure definition proc p(val x, res y) istn S endt", we have two 
transfer functions: 
r;:, r;: : (6. -t L) -t (6. -t L) 
In the case of our simple language we shall prefer to take both of these transfer 
functions to be the identity function; i.e. 
r;:(f) = l 
r;:(l) 
l 
for all l E L. Hence the effect of procedure entry is handled by the trans-
fer function for procedure call (considered below) and similarly the effect of 
procedure exit is handled by the transfer function for procedure return (also 

2.5 Interprocedural Analysis 
91 
proc p(val x,res y) 
f I 
[call p(a, z)l~~ 
f 
Figure 2.8: Analysis of procedure call: the forward case. 
considered below). For more advanced languages where many semantic ac-
tions take place at procedure entry or exit it may be preferable to reconsider 
this decision. 
For a procedure call (Re, Rn, Rx, Rr) E IF we shall define two transfer functions. 
In our explanation we shall concentrate on the case of forward analyses where 
P* contains [call p(a,z)l~~ as well as proe p(val x,res y) isCn S endc~. 
Corresponding to the actual call we have the transfer function 
1Â£ : (~ -t L) -t (~ -t L) 
and it is used in the equation: -fL (Ao(fe)) for all (fe,fn,ex,fr) ElF 
In other words, the transfer function modifies the data flow properties (and 
the context) as required for passing to the procedure entry. 
Corresponding to the return we have the transfer function 
-
fLcr : (~ -t L) x (~ -t L) -t (~ -t L) 
and it is used in the equation: 
-
A.(Rr) = fLcr (Ao(Re),Ao(Rr)) for all (fe, f n, Rx,Rr) ElF 
-
The first parameter of rt Â£ 
describes the data flow properties at the call 
point for the procedure an."d the second parameter describes the properties at 
the exit from the procedure body. Ignoring the first parameter, the transfer 
function modifies the data flow properties (and the context) as required for 
passing back from the procedure exit. The purpose of the first parameter is 

92 
2 Data Flow Analysis 
proc p(val x, res y) 
I [call p(a, z)]lc I 
I [call p(a, z)k I 
Figure 2.9: Analysis of procedure call: ignoring calling context. 
to recover some of the information (data flow properties as well as context 
information) that was available before the actual call; how this is done de-
pends on the actual choice of the set, .6., of context information and we shall 
return to this shortly. Figure 2.8 illustrates the flow of data in the analysis 
of the procedure call. 
Variations. The functionality and use of i[;r (as well as Figure 2.8) is 
sufficiently general that it allows us to deal with most of the scenarios found 
in the literature. A simple example being the possibility to define 
thereby completely ignoring the information before the call; this is illustrated 
in Figure 2.9. 
A somewhat more interesting example is the ability to define 
thereby allowing a simple combination of the information coming back from 
the call with the information pertaining before the call. This form is illus-
trated in Figure 2.10 and is often motivated on the gr~ds that if?ir copies 
data that is local to the calling procedure whereas Jlc~lr copies~ormation 
that is global. (It may be worth noticing that the function f'Llr is com-
pletely additive if and only if it can be written in this form with if5r and 
if!{r being completely additive.) 
Context-sensitive versus context-insensitive. So far we have 
criticised the naive approach because it was unable to maintain the proper 

2.5 Interprocedural Analysis 
93 
proc p(val x,res y) 
I [call pea, Z)]lc I 
! 
I [call pea, Z)]lr I 
Figure 2.10: Analysis of procedure call: merging of context. 
relationship between procedure calls and procedure returns. A related criti-
cism of the naive approach is that it cannot distinguish between the different 
calls of a procedure. The information about calling states is combined for 
all call sites, the procedure body is analysed only once using this combined 
information, and the resulting information about the set of return states is 
used at all return points. The phrase context-insensitive is often used to refer 
to this shortcoming. 
The use of non-trivial context information not only helps to avoid the first 
criticism but also the second: if there are two different calls but they are 
reached with different contexts, 81 and 82 , then all information obtained from 
the procedure will be clearly related to 81 or 82 and no undesired combination 
or "cross-over" will take place. The phrase context-sensitive is often used to 
refer to this ability. 
Clearly a context-sensitive analysis is more precise than a context-insensitive 
analysis but at the same time it is also likely to be more costly. The choice be-
tween which technique to use amounts to a careful balance between precision 
and efficiency. 
2.5.4 
Call Strings as Context 
To complete the design of the analysis of the program we must choose the set, 
~, of context information and also specify the extremal value, t, and define 
the two transfer functions associated with procedure calls. In this subsection 
we shall consider two approaches based on call strings and our explanation 
will be in terms of forward analyses. 
Call strings of unbounded length. As the first possibility we 
simply encode the path taken; however, since our main interest is with pro-

94 
2 Data Flow Analysis 
cedure calls we shall only record flows of the form (Â£e; en) corresponding to 
a procedure call. Formally we take 
6. = Lab* 
where the most recent label Â£e of a procedure call is at the right end (just as 
was the case for valid paths and paths); elements of 6. are called call strings. 
We then define the extremal value t: by the formula 
~(8) {L 
if 8 = A 
L 
= 
...L 
otherwise 
where A is the empty sequence corresponding to the fact that there are no 
pending procedure calls when the program starts execution; L is the extremal 
value available from the underlying Monotone Framework. 
Example 2.37 For the Fibonacci program of Example 2.33 the following 
call strings will be of interest: 
A, [9], [9,4], [9,6], [9,4,4], [9,4,6], [9,6,4], [9,6,6],Â·Â·Â· 
corresponding to the cases with 0,1,2,3, ... pending procedure calls. 
_ 
For a procedure call (Â£e,en,e""er ) E IF, amounting to [call.!(a,z)]~~ in 
the case of a forward analysis, we define the transfer function fL such that 
i[ (Z)([8,Â£e]) = flJf(8)) where [8,Â£e] denotes the path obtained by append-
ing ee to 8 (so as to reflect that now we enter the body of the procedure) 
and the function fL : L --+ L describes how the property is modified. This is 
achieved by setting 
F (f) (8') = {flJf(8)) 
when?' = [8,ee] 
lc 
...L 
otherwIse 
which takes care of the special case of empty paths. 
Next we define the transfer function fLr corresponding to returning from 
the procedure call: 
-
................ 
2 
................ 
fLlr (l,l')(8) = h,lr(l(8),l'([8,ee])) 
Here the information f from the original call is combined with information fi 
from the procedure exit using the function fLlr : L x L --+ L. However, only 
information corresponding to the same contexts for call point Â£e is combined: 
this is ensured by the two occurrences of 8 in the right hand side of the above 
formula. 

2.5 Interprocedural Analysis 
95 
Example 2.38 Let us return to the Detection of Signs Analysis of Ex-
ample 2.36. For a procedure call [call p(a, z)]~~ where p is declared by 
proc p(val x,res y) isLn S endL", we may take: 
jffl (Z) = UH8'} X 4>~i;nl(aSign) I (8',asign) E Z 1\ 8' = [8,lc]} 
4>~~nl(aSign) 
{asign[x 1-+ s][y 1-+ s']1 s E Asign[a](asign) 1\ s' E {-,O,+}} 
When returning from the procedure call we take: 
-
pign2 (Z Z') 
ic,ir 
' 
,/..sign2 (sign 
sign) 
'l'ic,l.r a l 
, a2 
UH 8} x 4>~~:t (a~ign, a~ign) I (8, a~ign) E Z 1\ 
1\ (8',a~ign) E Z' 1\ 8' = [8,lc]} 
Thus we extract all the information from the procedure body except for the 
information about the formal parameters x and y and the actual parameter 
z. For the formal parameters we rely on the information available before the 
current call which is still correct and for the actual parameter we perform the 
required update of the information. ~e that to facilitate this definition it 
is crucial that the transfer function flc,l.r takes two arguments: information 
from the call point as well as from the procedure exit. 
_ 
Call strings of bounded length. Clearly the call strings can be-
come arbitrarily long because the procedures may be recursive. It is therefore 
customary to restrict their length to be at most k for some number k ~ OJ 
the idea being that only the last k calls are recorded. We write this as 
~ = Lab9 
and we still take the extremal value t: to be defined by the formula 
~(8) {t 
if 8 = A 
t 
= 
1. otherwise 
Note that in the case k = 0 we have ~ = {A} which is equivalent to having 
no context information. 
Example 2.39 Consider the Fibonacci program of Example 2.33 and as-
sume that we are only interested in recording the last call, i.e. k = 1. Then 
the call strings of interest are: 
A, [9], [4], [6] 
Alternatively, we may choose to record the last two calls, i.e. k = 2, in which 
case the following call strings are of interest: 

96 
2 Data Flow Analysis 
In general, we would expect an analysis using these 8 contexts to be more 
precise than one using the 4 different contexts displayed above. 
_ 
We shall now present the transfer functions for the g~eral case where call 
strings have length at most k. The transfer function fL for procedure call 
is redefined by 
f[ (l)(c5') = UUIJ1(c5)) 115' = fc5,Â£eld 
where f 15, Â£e h denotes the call string [15, Â£el but possibly truncated (by omit-
ting elements on the left) so as to have length at most k. Since the function 
mapping 15 to fc5,Â£eh is not injective (unlike the one mapping 15 to [c5,Â£e)) we 
need to take the least upper bound over all 15 that can be mapped to the 
relevant context 15'. 
-
Similarly, the transfer function fL'r for procedure return is redefined by 
as should be expected. 
Example 2.40 Let us consider Detection of Signs Analysis in the special 
case where k = 0, i.e. where ~ = {A} and hence ~ x (Var* -+ Sign) is 
isomorphic to Var* -+ Sign. Using this isomorphism the formulae defining 
the transfer functions for procedure call can be simplified to 
pign2 (Y y') 
Â£c,Â£r 
' 
where Y, Y' ~ Var* -+ Sign. It is now easy to see that the analysis is context-
insensitive: at procedure return it is not possible to distinguish between the 
different call points. 
Let us next consider the case where k = 1. Here ~ = Lab U {A} and the 
transfer functions for procedure call are: 
f~l (Z) 
U{{Â£e} x Â¢~~nl(aSign) I (c5,asign) E Z} 
iEt (Z, Z') = U{{ c5} x Â¢~i;,~~ (a~ign, a~ign) I (15, a~ign) E Z 
1\ (Â£e, a~ign) E Z'} 
-
Now the transfer function f::gn1 will mark all data from th~ll point Â£e with 
that label. Thus it does not harm that the information f::gn1 (Z) is merged 

2.5 Interprocedural Analysis 
97 
with similar information jffl (Z) from another procedure call. At the return 
from the call the transfer function ffij~ selects those pairs (fe, (j~ign) E Z' 
that are relevant for the current call and combines them with those pairs 
(6, (j~ign) E Z that describe the situation before the call; in particular, this 
allows us to reset the context to be that of the call point. 
_ 
2.5.5 
Assumption Sets as Context 
An alternative to describing a path directly in terms of the calls being per-
formed is to record information about the state in which the call was made; 
these methods can clearly be combined but in the interest of simplicity we 
shall abstain from doing so. 
Large assumption sets. Throughout this subsection we shall make 
the simplifying assumption that 
L = P(D) 
as is the case for the Detection of Signs Analysis. Much as in Examples 2.36 
and 2.38 the property space L = ~ ~ L is then isomorphic to 
L = P(~ x D) 
and we shall use this definition throughout this subsection. Restricting the 
attention to only recording information about the last call (corresponding to 
taking k = 1 above), one possibility is to take 
~ =P(D) 
and we then take the extremal value to be 
1:= {({t},t)} 
meaning that the initial context is described by the initial abstract state. This 
kind of context information is often called an assumption set and expresses 
a dependency on data (as opposed to a dependency on control as in the case 
of call strings). 
Example 2.41 Assume that we want to perform a Detection of Signs 
Analysis (Example 2.36) of the Fibonacci program of Example 2.33 and that 
the extremal value tsign is the singleton [x I-t +, y I-t -, z I-t -j. Then the 
contexts of primary interest will be sets consisting of some of the following 
abstract states 
[x I-t +, y I-t 0, z I-t -j, [x I-t +, y I-t 0, z I-t OJ, 
[x I-t +, y I-t 0, z I-t +j, 
[x I-t +, y I-t +, z I-t -], [x I-t +, y I-t +, z I-t OJ, 
[x I-t +, y I-t +, z I-t +j 
corresponding to the states in which the call-statements may be encoun-
tered. 
_ 

98 
2 Data Flow Analysis 
For a procedure call (Â£e,Â£n,Â£",,Â£r) E IF, i.e. rca:,: p(a,z)l~~ in the case of 
forward analysis, we define the transfer function fIe for procedure call by: 
f[ (Z) = U{{8'} x Â¢}Jd) I (8, d) E Z /\ 
8' = {d" I (8,d") E Z}} 
where Â¢t : D --t P(D). The idea is as follows: a pair (8, d) E Z describes a 
context and an abstract state for the current call. We now have to modify 
the context to take the call into account, i.e. we have to determine the set of 
possible abstract states in which the call could happen in the current context 
and this is 8' = {d" I (8, d") E Z}. Given this context we proceed as in 
the call string formulations presented above and mark the data flow property 
with this context. 
Next we shall consider the transfer function fLÂ£r for procedure return 
f[;r (Z, Z') = U{{ 8} X Â¢LÂ£r (d, d') I (8, d) E Z /\ (8', d') E Z'/\ 
8' = {d"I(8,d") E Z}} 
where Â¢LÂ£r : D x D --t P(D). Here (8, d) E Z describes the situation 
before the call and (8', cQ E Z' describes the situation at the procedure exit. 
From the definition of fIe we know that the context matching (8, d) will be 
8' = {d" I (8, d") E Z} so we impose that condition. We can now combine 
information from before the call with that at the procedure exit much as in 
the call string approach; in particular, we can reset the context to be that of 
the call point. 
There is one important snag with the definitions of the transfer functions 
iF and fT:: they are in general not monotone! One way to overcome 
this proble~ r is to consider more general techniques for solving systems of 
equations where the transfer functions satisfy a weaker condition than mono-
tonicity; we provide references to this approach in the Concluding Remarks. 
Another way to overcome the problem is to use more approximate defini-
tions that are indeed monotone; one possibility is to replace the condition 
8' = {d" I (8, d") E Z} by 8' ~ {d" I (8, d") E Z}. An even more ap-
proximate, but computationally more tractable, solution is to use smaller 
assumption sets as detailed below. 
Small assumption sets. As a simpler version of using assumption 
sets one may take 
and then use t: = {( t, t)} as the extremal value. So rather than basing the 
embellished Monotone Framework on P(D) x D as above we now base it on 
D x D. Of course, this is much less precise but, on the positive side, the size 
of the data flow properties has been reduced dramatically. 

2.5 Interprocedural Analysis 
99 
For a procedure call (Re, Rn, Rx, Rr) E IF, i.e. [call pea, z)l~~ for forward anal-
yses, the transfer function 1Â£ is now defined by 
1Â£ (Z) = U{{ d} x 4>t (d) I (8, d) E Z} 
where, as before, 4{ : D -t P(D). Here the individual pieces of information 
concerning the abstract state of the call have their own local contexts; we 
have no way of grouping the abstract states corresponding to 8 as we did in 
the approach with large assumption sets. 
--
The corresponding definition of the transfer function fLÂ£r for procedure 
return then is 
J[;r (Z,Z') = U{{8} X Â¢{,Â£Jd,d') I (8, d) E Z 1\ (d,d') E Z'} 
where again Â¢{,lr : D x D -t P(D). Examples of how to use assumption 
sets will be considered in the exercises. 
2.5.6 
Flow-Sensitivity versus Flow-Insensitivity 
All of the data flow analyses we have considered so far have been flow-
sensitive: this just means that in general we would expect the analysis of 
a program Sl; S2 to differ from the analysis of the program S2; Sl where the 
statements come in a different order. 
Sometimes one considers flow-insensitive analyses where the order of state-
ments is of no importance for the analysis being performed. This may sound 
weird at first, but suppose that the analysis being performed is like the ones 
considered in Section 2.1 except that for simplicity all kill components are 
empty sets. Given these assumptions one might expect that the programs 
Sl; S2 and S2; Sl give rise to the same analysis. Clearly a flow-insensitive 
analysis may be much less precise than its flow-sensitive analogue but also it 
is likely to be much cheaper; since interprocedural data flow analyses tend 
to be very costly, it is therefore useful to have a repertoire of techniques for 
reducing the cost. 
Sets of assigned variables. We shall now present an example of a 
flow-insensitive analysis. Consider a program P* of the form begin D* S* 
end. For each procedure 
proc p(val x, res y) isÂ£n S endl .. 
in D*, the aim is to determine the set IAV(P) of global variables that might 
be assigned directly or indirectly when p is called. 
To compute these sets we need two auxiliary notions. The set AV(S) of 
directly assigned variables gives for each statement S the set of variables 

100 
2 Data Flow Analysis 
that could be assigned in S - but ignoring the effect of procedure calls. It is 
defined inductively upon the structure of S: 
AV([skip]l) 
AV([x := all) = 
AV(Slj S2) 
AV(if [b]l then SI else S2) 
AV(while [b]L do S) = 
AV([call p(a,z)]~~) 
o 
{x} 
AV(SI) u AV(S2) 
AV(St) u AV(S2) 
AV(S) 
{z} 
Similarly we shall need the set CP(S) of immediately called procedures that 
gives for each statement S the set of procedure names that could be directly 
called in S - but ignoring the effect of procedure calls. It is defined inductively 
upon the structure of S: 
CP([skip]L) 
0 
CP([x := all) 
= 
0 
CP(Slj S2) 
CP(if [W then SI else S2) = 
CP(while [W do S) = 
CP([call p(a, z)]~~) = 
CP(SI) U CP(S2) 
CP(SI) U CP(S2) 
CP(S) 
{p} 
Both the sets AV(S) and CP(S) are well-defined by induction on the structure 
of Sj also it should be clear that they are context-insensitive in the sense that 
any rearrangement of the statements inside S would have given the same 
result. The information in CP( .. Â·) can be presented graphically: let the 
graph have a node for each procedure name as well as a node called main* 
for the program itself, and let the graph have an edge from p (respectively 
main*) to p' whenever the procedure body S of p has p' E CP(S) (respectively 
p' E CP( S*)). This graph is usually called the procedure call graph. 
We can now formulate a system of data flow equations that specifies how to 
obtain the desired sets IAV(P): 
IAV(P) = (AV(S) \ {x}) U U{IAV(Pl) I p' E CP(S)} 
where proc p(val x, res y) isLn S endlm is in D* 
By analogy with the considerations in Section 2.1 we want the least solution 
of this system of equations. 

2.5 Interprocedural Analysis 
101 
Figure 2.11: Procedure call graph for example program. 
Example 2.42 Let us now consider the following version of the Fibonacci 
program (omitting labels): 
begin proc fib (val z) is if z<3 then call add(1) 
end 
else (call fib(z-1); call fib(z-2)) 
end; 
proc add(val u) is (y:=y+u; u:=O) 
end; 
y:=O; call fib(x) 
We then get the following equations 
IAV(fib) 
IAV(add) 
= (0 \ {z}) U IAV(fib) U IAV(add) 
= {y,u} \ {u} 
The associated procedure call graph is shown in Figure 2.11. The least solu-
tion to the equation system is 
IAV(fib) = IAV(add) = {y} 
showing that only the variable y will be assigned by the procedure calls. 
(Had we instead taken the greatest solution to the equations we would have 
IAV(fib) = IAV(add) = Var* for any set Var* of variables that contains 
those used in the program and this would be completely unusable.) 
â¢ 
Note that the formulation of the example analysis did not associate infor-
mation with entries and exits of blocks but rather with the blocks (or more 
generally the statements) themselves. This is a rather natural space saving 
approach for a context-insensitive analysis. It also relates to the discussion 
of Type and Effect Systems in Section 1.6: the "annotated base types" in 
Table 1.2 versus the "annotated type constructors" in Table 1.3. 

102 
2 Data Flow Analysis 
2.6 
Shape Analysis 
We shall now study an extension of the WHILE-language with heap allocated 
data structures and an intraprocedural Shape Analysis that gives a finite 
characterisation of the shapes of these data structures. So while the aim of 
the previous sections has been to present the basic techniques of Data Flow 
Analysis, the aim of this section is to show how the techniques can be used 
to specify a rather complex analysis. 
Shape analysis information is not only useful for classical compiler optimisa-
tions but also for software development tools: the Shape Analysis will allow 
us to statically detect errors like dereferencing a nil-pointer - this is guar-
anteed to give rise to a dynamic error and a warning can be issued. Perhaps 
more surprisingly, the analysis allows us to validate certain properties of the 
shape of the data structures manipulated by the program; we can for exam-
ple validate that a program for in-situ list reversal does indeed transform a 
non-cyclic list into a non-cyclic list. 
Syntax of the pointer language. We shall study an extension of 
WHILE that allows us to create cells in the heap; the cells are structured and 
may contain values as well as pointers to other cells. The data stored in a 
cell is accessed via selectors so we assume that a finite and non-empty set 
Sel of selector names are given: 
sel E Sel 
selector names 
As an example Sel may include the Lisp-like selectors car and cdr for select-
ing the first and second components of pairs. The cells of the heap can be 
addressed by expressions like x.cdr: this will first determine the cell pointed 
to by the variable x and then return the value of the cdr field. For the sake of 
simplicity we shall only allow one level of selectors although the development 
generalises to several levels. Formally the pointer expressions 
P 
E PExp 
are given by: 
p 
::= x I x.sel 
The syntax of the WHILE-language is now extended to have: 
a 
.. -
pin I al OPa a2 I nil 
b .. -
true I false I not b I b1 oPb b2 I al oPr a2 I oPp P 
8 
.. -
[p:=a]Â£ I [skip]Â£ 181 ; 82 I 
if [W then 81 else 82 I while [W do 8 I 
[malloc PY 
Arithmetic expressions are extended to use pointer expressions rather than 
just variables, and an arithmetic expression can also be the constant nil. 

2.6 Shape Analysis 
103 
The binary operations oPa are as before, that is, they are the standard arith-
metic operations and in particular they do not allow pointer arithmetic. The 
boolean expressions are extended such that the relational operators oPr now 
allow testing for the equality of pointers and also we shall allow unary opera-
tions oPp on pointers (as for example is-nil and has-sel for each sel E Sel). 
Note that arithmetic as well as boolean expressions can only access cells in 
the heap, they cannot create new cells nor update existing cells. 
The assignment statement takes the general form p:=a where P is a pointer 
expression. In the case where p is just a variable we have an extension of the 
ordinary assignment of the WHILE language and in the case where P contains 
a selector we have a destructive update of the heap. The statements of the 
extended language also contain a statement malloc p for creating a new cell 
pointed to by p. 
Example 2.43 The following program reverses the list pointed to by x 
and leaves the result in y: 
[y:=nil]l; 
while [not is-nil(x)j2 do 
([z: =y]3; [y: =x]4; [x: =x.cdr]5; [y.cdr: =z]6); 
[z:=nilf 
Figure 2.12 illustrates the effect of the program when x points to a five 
element list and y and z are initially undefined. Row 0 shows the heap just 
before entering the while-loop: x points to the list and y is nil (denoted 
by 0); to avoid cluttering the figure we do not draw the car-pointers. After 
having executed the statements of the body of the loop the situation is as in 
row 1: x now points to the tail of the list, y points to the head of the list and 
z is nil. In general the n'th row illustrates the situation just before entering 
the loop the n + 1 'th time so in row 5 we see that x points to nil and the 
execution of the loop terminates and y points to the reversed list. The final 
statement z: =nil simply removes the pointer from z to ~4 and sets it to the 
nil-value. 
_ 
2.6.1 
Structural Operational Semantics 
To model the scenario described above we shall introduce an infinite set Loc 
of locations (or addresses) for the heap cells: 
~ E Loc 
locations 
The value of a variable will now either be an integer (as before), a location 
(Le. a pointer) or the special constant 0 reflecting that it is the nil value. 
Thus the states are given by 
a EState = Var * -+ (Z + Loc + {o}) 

104 
2 Data Flow Analysis 
x 
0: 
Y -<> 
z 
x 
1: 
Y~<> 
z -<> 
x 
2: 
:~<> 
3: 
Y 
z 
x~<> 
4: 
Y 
z 
x -<> 
5: 
Y 
z 
Figure 2.12: Reversal of a list of five elements. 
where as usual Var* is the (finite) set of variables occurring in the program 
of interest. As mentioned above the cells of the heap have multiple fields and 
they are accessed using the selectors. Each field can either be an integer, a 
pointer to another cell or it can be nil. We formalise this by taking 
1-Â£ E Heap = (Loc x Sel) --+fin (Z + Loc + {<>}) 

2.6 Shape Analysis 
105 
where the use of partial functions with finite domain reflects that not all 
selector fields need to be defined; as we shall see later, a newly created cell 
with location ~ will have all its fields to be uninitialised and hence the corre-
sponding heap 1Â£ will have 1Â£(~, sel) to be undefined for all sel E Sel. 
Pointer expressions. Given a state and a heap we need to determine 
the value of a pointer expression p as an element of Z + Loc + {<> }. For this 
we introduce the function 
p : PExp* -+ (State x Heap) -+fin (Z + {<>} + Loc) 
where PExp* denotes pointer expressions with variables in Var*. It is de-
fined by: 
p[X](U,1Â£) = 17 (x) 
p[x.sel] (17, 1Â£) = { 
1Â£(U(X), sel) 
if u(x) E Loc and 1Â£ is defined on (17 (x) , sel) 
undef 
if u(x) ~ Loc or 1Â£ is undefined on (17 (x) , sel) 
The first clause takes care of the situation where p is a simple variable and 
using the state we determine its value - note that this may be an integer, 
a location or the special nil-value <>. The second clause takes care of the 
case where the pointer expression has the form x.sel. Here we first have to 
determine the value of x; it only makes sense to inspect the sel-field in the 
case x evaluates to a location that has a sel-field and hence the clause is split 
into two cases. In the case where x evaluates to a location we simply inspect 
the heap 1Â£ to determine the value of the sel-field - again we may note that 
this can be an integer, a location or the special value <>. 
Example 2.44 In Figure 2.12 the oval nodes model the cells of the heap 
1Â£ and they are labelled with their location (or address). The unlabelled edges 
denote the state 17: an edge from a variable x to some node labelled ~ means 
that u(x) = ~; an edge from x to the symbol <> means that u(x) = <>. The 
labelled edges model the heap 1Â£: an edge labelled sel from a node labelled 
~ to a node labelled ~' means that there is a sel pointer between the two 
cells, that is 1Â£(~, sel) = ~'; an edge labelled sel from a node labelled ~ to the 
symbol <> means that the pointer is a nil-pointer, that is 1Â£(~, sel) = <>. 
Consider the pointer expression x.cdr and assume that 17 and 1Â£ are as 
in row 0 of Figure 2.12, that is u(x) = 6 and 1Â£(6, cdr) = 6. Then 
p[x.cdr](u,1Â£) = 6. 
â¢ 
Arithmetic and boolean expressions. It is now straightforward 
to extend the semantics of arithmetic and boolean expressions to handle 
pointer expressions and the nil-constant. Obviously the functionality of the 

106 
2 Data Flow Analysis 
semantic functions A and B has to be changed to take the heap into account: 
A: AExp -t (State x Heap) -tfin (Z + Loc + {o}) 
B: BExp -t (State x Heap) -tfin T 
The clauses for arithmetic expressions are 
A[p](a,1Â£) 
p[p](a,1Â£) 
A[n] (a, 1Â£) = N[n] 
A[al 0Pa a2](a,1Â£) 
A[ad(a,1Â£) 0Pa A[a2] (a, 1Â£) 
A[nil] (a, 1Â£) 
0 
where we use p to determine the value of pointer expressions and we explicitly 
write that the meaning of nil is o. Also the meaning 0Pa of the binary oper-
ation oPa has to be suitably modified to be undefined unless both arguments 
are integers in which case the results are as for the WHILE-language. 
The definition of the semantics of boolean expressions is similar so we only 
give two of the clauses: 
B[al oPr a2](a,1Â£) = A[al] (a, 1Â£) oPr A[a2] (a, 1Â£) 
B[ oPp p] (a, 1Â£) = oPp (p[P] (a, 1Â£)) 
Analogously to above, the meaning oPr of the binary relation operator oPr 
has to be suitably modified to give undefined in case the arguments are not 
both integers or both pointers (in which case the equality operation tests for 
the equality of the pointers). The meaning of the unary operation oPp is 
defined by 0Pp; as an example: 
. 
'l() 
{tt if v = 0 
ls-nl v 
= 
ff 
th 
. 
o erWlse 
Statements. Finally, the semantics of statements is extended to cope 
with the heap component. The configurations will now contain a state as 
well as a heap so we have 
([x:=alt ,a, 1Â£} -t (a[x f-t A[a] (a, 1Â£)], 1Â£) 
if A[a] (a, 1Â£) is defined 
reflecting that for the assignment x: =a the state is updated as usual and the 
heap is left unchanged. In the case where we assign to a pointer expression 
containing a selector field we shall leave the state unchanged and update the 
heap as follows: 
([x.sel:=aY,a, 1Â£} -t (a,1Â£[(a(x),sel) f-t A[a](a,1Â£)l} 
if a(x) E Loc and A[a] (a, 1Â£) is defined 

2.6 Shape Analysis 
107 
Here the side condition ensures that the left hand side of the assignment does 
indeed evaluate to a location. 
The construct malloe p is responsible for creating a new cell. We have two 
clauses depending on the form of p: 
([maUoe x]i, 0', 1Â£) --+ (O'[x I--t ~], 1Â£) 
where ~ does not occur in 0' or 1Â£ 
([maUoe (x.sel)jl,O', 1Â£) --+ (O',1Â£[(O'(X),sel) I--t~]) 
where ~ does not occur in 0' or 1Â£ and O'(x) E Loc 
Note that in both cases we introduce a fresh location ~ but we do not specify 
any values for 1Â£(~, sel) - as discussed before we have settled for a semantics 
where the fields of ~ are undefined; obviously other choices are possible. Also 
note that in the last clause the side condition ensures that we already have a 
location corresponding to x and hence can create an edge to the new location. 
Remark. The semantics only allows a limited reuse of garbage locations. 
For a statement like [maUoe x]1; [x:=nilj2; [maUoe y]3 we will assign some 
location to x at the statement with label 1 and since it neither occurs in 
the state nor the heap after the assignment labelled 2 we are free to reuse 
it in the statement labelled 3 (but we do not have to). For a statement like 
[maUoe xJ1; [x.edr:=nilj2; [x:=nil]3; [maUoe y]4 we would not be able to 
reuse the location allocated at 1 although it will be unreachable (and hence 
garbage) after the statement labelled 3. 
â¢ 
2.6.2 
Shape Graphs 
It should be evident that there are programs for which the heap can grow ar-
bitrarily large. Therefore the aim of the analysis will be to come up with finite 
representations of it. To do so we shall introduce a method for combining the 
locations of the semantics into a finite number of abstract locations. We then 
introduce an abstract state 5 mapping variables to abstract locations (rather 
than locations) and an abstract heap H specifying the links between the ab-
stract locations (rather than the locations). More precisely, the analysis will 
operate on shape graphs (5, H, is) consisting of: 
â¢ an abstract state, 5, 
â¢ an abstract heap, H, and 
â¢ sharing information, is, for the abstract locations. 
The last component allows us to recover some of the imprecision introduced 
by combining many locations into one abstract location. We shall now de-
scribe how a given state 0' and heap 1Â£ give rise to a shape graph (5, H, is); in 
doing so we shall specify the functionality of 5, H and is in detail as well as 
formulate a total of five invariants. 

108 
2 Data Flow Analysis 
Abstract locations. The abstract locations have the form nx where 
X is a subset of the variables of Var*: 
abstract locations 
Since Var* is finite it is clear that ALoe is finite and a given shape graph 
will contain a subset of the abstract locations of ALoe. 
The idea is that if x E X then the abstract location nx will represent the lo-
cation a(x). The abstract location n0 is called the abstract summary location 
and will represent all the locations that cannot be reached directly from the 
state without consulting the heap. Clearly nx and n0 will represent disjoint 
sets of locations when X f= 0. 
In general, we shall enforce the invariant that two distinct abstract locations 
nx and ny always represent disjoint sets of locations. As a consequence, for 
any two abstract locations nx and ny it is either the case that X = Y or 
that X n Y = 0. To prove this assume by way of contradiction that X f= Y 
and that z E X n Y. From z E X we get that a(z) is represented by nx and 
similarly z E Y gives that a(z) is represented by ny. But then a(z) must be 
distinct from a(z) and we have the desired contradiction. 
The invariant can be formulated as follows: 
Invariant 1. If two abstract locations nx and ny occur in the same 
shape graph then either X = Y or X n Y = 0. 
Example 2.45 Consider the state and heap in row 2 of Figure 2.12. The 
variables x, y and z point to different locations (e3, 6, and 6, respectively) 
so in the shape graph they will be represented by different abstract locations 
named nix}, n{y} and n{z}' The two locations e4 and e5 cannot be reached 
directly from the state so they will be represented by the abstract summary 
location n0' 
â¢ 
Abstract states. One of the components of a shape graph is the ab-
stract state,S, that maps variables to abstract locations. To maintain the 
naming convention for abstract locations we shall ensure that: 
Invariant 2. If x is mapped to nx by the abstract state then x E X. 
From Invariant 1 it follows that there will be at most one abstract location 
in the shape graph containing a given variable. 
We shall only be interested in the shape of the heap so we shall not distinguish 
between integer values, nil-pointers and uninitialised fields; hence we can 
view the abstract state as an element of 
5 E AState = P(Var* x ALoe) 

2.6 Shape Analysis 
109 
where we have chosen to use powersets so as to simplify the notation in later 
parts of the development. We shall write ALoc(5) = {nx 13x: (x,nx) E 5} 
for the set of abstract locations occurring in 5. (Note that AState is too 
large in the sense that it contains elements that do not satisfy the invariants.) 
Abstract heaps. Another component of the shape graph is the abstract 
heap, H, that specifies the links between the abstract locations (just as the 
heap specifies the links between the locations in the semantics). The links 
will be specified by triples (nv, sel, nw) and formally we take the abstract 
heap as an element of 
HE AHeap = P(ALoe x Sel x ALoe) 
where we again do not distinguish between integers, nil-pointers and unini-
tialised fields. We shall write ALoc(H) = {nv, nw I 3sel : (nv, sel, nw) E H} 
for the set of abstract locations occurring in H. 
The intention is that if 1-Â£(6, sel) = 6 and 6 and 6 are represented by nv 
and nw, respectively, then (nv,sel,nw) E H. 
In the heap 1-Â£ there will be at most one location ~2 such that 1-Â£(6, sel) = 
6. The abstract heap only partly shares this property because the abstract 
location n0 can represent several locations pointing to different locations. 
However, the abstract heap must satisfy: 
Invariant 3. Whenever (nv,sel,nw) and (nv,sel,nw') are in the 
abstract heap then either V = 0 or W = W'. 
Thus the target of a selector field will be uniquely determined by the source 
unless the source is the abstract summary location n0. 
Example 2.46 Continuing Example 2.45 we can now see that the ab-
stract state 52 corresponding to the state of row 2 of Figure 2.12 will be 
The abstract heap H2 corresponding to row 2 has 
H2 = {(n{x} , cdr, n0), (n0, cdr, n0), (n{y}, cdr, n{z})} 
The first triple reflects that the heap maps 6 and cdr to ~4' 6 is represented 
by n{x} and ~4 is represented by n0. The second triple reflects that the heap 
maps ~4 and cdr to ~5 and both ~4 and ~5 are represented by n0. The final 
triple reflects that the heap maps 6 and cdr to 6, 6 is represented by n{y} 
and 6 is represented by n{ z}. Note that there is no triple (n{ z}, cdr, n0) 
because the heap maps 6 and cdr to <> rather than a location. 
The resulting abstract state and abstract heap is illustrated in Figure 2.13 
together with similar information for the other states and heaps of Figure 

110 
2 Data Flow Analysis 
cdr 
cdr 
O:X~ 
x~ 
1: 
cdr 
x~ 
x 
2: :~ 
3: 
y 
z 
4: 
5: 
y 
z 
Figure 2.13: Shape graphs corresponding to Figure 2.12. 
2.12. The square nodes model abstract locations; the unlabelled edges from 
variables to square nodes model the abstract state and the labelled edges 
between square nodes model the abstract heap. If the abstract state does 
not associate an abstract location with some variable then that variable does 
not occur in the picture. 
Note that even if the semantics uses the same locations throughout the 
computation it need not be the case that the locations are associated with 
the same abstract locations at all points in the analysis. Consider Figures 
2.12 and 2.13: the abstract location n0 will in turn represent the locations 
{6,6'~4'~5}, {6'~4,~5}, {~4'~5}, {6,~5}, {6,6} and {6,6,6}Â· 
â¢ 
Sharing information. We are now ready to introduce the third and 
final component of the shape graphs. Consider the top row of Figure 2.14. 
The abstract state and abstract heap to the right represent the state and the 
heap to the left but they also represent the state and the heap shown in the 
second row. We shall now show how to distinguish between these two cases. 
The idea is to specify a subset, is, of the abstract locations that represent 
locations that are shared due to pointers in the heap: an abstract location 
nx will be included in is if it does represent a location that is the target of 
more than one pointer in the heap. In the top row of Figure 2.14, the abstract 
location n{y} represents the location ~5 and it is not shared (by two or more 
heap pointers) so n{y} fj. is; the fat box indicates that the abstract location is 
unshared. On the other hand, in the second row ~5 is shared (both 6 and ~4 
point to it) so n{y} E is; the double box indicates that the abstract location 
might be shared. 

2.6 Shape Analysis 
111 
x 
x 
<> 
y 
y 
Figure 2.14: Sharing information. 
Obviously, the abstract heaps themselves also contain some implicit sharing 
information: this is illustrated in the bottom row of Figure 2.14 where there 
are two distinct edges with target n{y}. We shall ensure that this implicit 
sharing information is consistent with the explicit sharing information (as 
given by is) by imposing two invariants. The first ensures that information 
in the sharing component is also reflected in the abstract heap: 
Invariant 4. If nx E is then either 
(a) (n0, sel, nx) is in the abstract heap for some sel, or 
(b) there exists two distinct triples (nv,seh,nx) and (nw,seh,nx) 
in the abstract heap (that is either Sell =F sel2 or V =F W). 
Case 4(a) takes care of the situation where there might be several locations 
represented by n0 that point to nx (as in the second and third rows of 
Figure 2.14). Case 4(b) takes care of the case where two distinct pointers 
(with different source or different selectors) point to nx (as in the bottom 
row of Figure 2.14). 
The second invariant ensures that sharing information present in the abstract 
heap is also reflected in the sharing component: 
Invariant 5. Whenever there are two distinct triples (nv,seh,nx) 
and (nw, sel2' nx) in the abstract heap and nx =F n0 then nx E is. 

112 
2 Data Flow Analysis 
This takes care of the case where nx represents a single location being the 
target of two or more heap pointers (as in the bottom row of Figure 2.14). 
Note that invariant 5 is the "inverse" of invariant 4(b) and that we have no 
"inverse" of invariant 4(a) - the presence of a pointer from n0 to nx gives 
no information about sharing properties of nx. 
In the case of the abstract summary location the explicit sharing information 
clearly gives extra information: if n0 E is then there might be a location 
represented by n0 that is the target of two or more heap pointers, whereas 
if n0 tf. is then all the locations represented by n0 will be the target of at 
most one heap pointer. The explicit sharing information may also give extra 
information for abstract locations nx where X Â¥- 0: from 4(a) alone we 
cannot deduce that nx is shared - this is clearly illustrated for the node n{y} 
by the top two rows of Figure 2.14. 
The complete lattice of shape graphs. To summarise, a shape 
graph is a triple consisting of an abstract state 5, an abstract heap H, and a 
set is of abstract locations that are shared: 
5 
E AState 
H 
E AHeap 
is 
E IsShared 
P(Var* x ALoe) 
P(ALoe x Sel x ALoe) 
P(ALoe) 
where ALoe = {nz I Z ~ Var*}. A shape graph (5, H, is) is a compatible 
shape graph if it fulfils the five invariants presented above: 
1. Vnv, nw E ALoc(5) U ALoc(H) U is : (V = W) V (V n W = 0) 
2. Vex, nx) E 5 : x E X 
3. V(nv,sel,nw), (nv,sel,nw') E H: (V = 0) V (W = W') 
4. Vnx E is: (3sel: (n0, sel, nx) E H) V 
(3(nv,seh,nx), (nw,sel2,nx) E H: 
seh Â¥- sel2 V V Â¥- W) 
5. V(nv, seh,nx), (nw,sel2,nx) E H: 
((sell Â¥- sel2 V VÂ¥- W) 1\ XÂ¥- 0) * nx E is 
The set of compatible shape graphs is denoted 
SG = {(5, H, is) I (5, H, is) is compatible} 
and the analysis, to be called Shape, will operate over sets of compatible 
shape graphs, i.e. elements of P(SG). Since P(SG) is a powerset, it is 
trivially a complete lattice with U being U and!;.;; being~. Furthermore, 
P(SG) is finite because SG ~ AState x AHeap x IsShared and all of 
AState, AHeap and IsShared are finite. 

2.6 Shape Analysis 
113 
Figure 2.15: The single shape graph in the extremal value t for the list 
reversal program. 
2.6.3 
The Analysis 
The analysis will be specified as an instance of a Monotone Framework with 
the complete lattice of properties being P(SG). For each label consistent 
program S* with isolated entries we obtain a set of equations of the form 
{ 
t 
if Â£ = init(S*) 
U {Shape. (Â£') I (Â£1, Â£) E flow( S*)} otherwise 
Shape.(Â£) 
fjA(Shapeo(f)) 
where t E P(SG) is the extremal value holding at entry to S* and f'jA are 
the transfer functions to be developed below. 
The analysis is a forward analysis since it is defined in terms of the set 
flow(S*) , and it is a may analysis since we are using U as the combina-
tion operation. However, there are also aspects of a must analysis because 
each individual shape graph must not contain any superfluous information. 
This will be useful for achieving strong update and strong nullification; here 
"strong" means that an update or nullification of a pointer expression allows 
one to remove the existing binding before adding a new one. This in turn 
leads to a very powerful analysis. 
Example 2.47 Consider again the list reversal program of Example 2.43 
and assume that x initially points to an unshared list with at least two el-
ements and that y and z are initially undefined; the singleton shape graph 
corresponding to this state and heap is illustrated in Figure 2.15 and will be 
the extremal value t used throughout this development. 
The Shape Analysis computes the sets Shapeo(Â£) and Shape.(Â£) of shape 
graphs describing the state and heap before and after executing the elemen-
tary block labelled Â£. The equations for Shape.(Â£) are 
Shape. (1) 
Shape. (2) 
Shape. (3) 
Shape. (4) 
Shape. (5) 
f~A(Shapeo(l)) 
fiA(Shapeo(2)) 
f~A(Shapeo(3)) 
fiA (Shapeo ( 4) ) 
f~A(Shapeo(5)) 
f~A(t) 
fiA (Shape. (1) U Shape. (6)) 
f~A(Shape.(2)) 
fiA(Shape.(3)) 
f~A(Shape.(4)) 

114 
x 
2 Data Flow Analysis 
(5, H, is) 
(5', H', is') 
Figure 2.16: The effect of [x: =nil)l. 
Shape. (6) 
Shape. (7) 
f~A(Shapeo(6)) 
= 
fr(Shapeo(7)) 
f~A(Shape.(5)) 
fiA(Shape.(2)) 
where the transfer functions fjA will be specified below. In the examples 
that follow we will provide further information about the equations but the 
computation of Shape.(l),Â·Â·Â·, Shape. (7) is left to Exercise 2.21. 
â¢ 
The transfer function {fA: P(SG) -+ P(SG) associated with a label, P, has 
the form: 
fjA(SG) = U{ rP~A((5, H, is)) I (5, H, is) E SG} 
where rP~A : SG -+ P(SG) specifies how a single shape graph (in Shapeo(P)) 
may be transformed into a set of shape graphs (in Shape. (P)) by the elemen-
tary block labelled f. We shall now inspect the various forms of elementary 
block and specify rPZA in each case. We shall first consider the boolean ex-
pressions and the skip-statement, then the different forms of assignments 
and finally the malloe-statement. 
Transfer function for [b]Â£ and [skip]l. We are only interested in 
the shape of the heap and the boolean tests do not modify the heap. Hence 
we take 
rP~A((5, H, is)) = {(5, H, is)} 
so that the transfer function {fA will be the identity function. Similarly for 
the skip-statement. 
Example 2.48 This case is illustrated by the test [not is-nil(x)j2 of 
the list reversal program of Example 2.43: the transfer function fiA is the 
identity function. Hence Shape.(2) = Shape.(l) U Shape.(6) as shown in 
Example 2.47. 
â¢ 

2.6 Shape Analysis 
115 
Thansfer function for [x: =a]Â£ where a is of the form n, al 0Pa a2 or 
nil. The effect of this assignment will be to remove the binding to x, and 
to rename all abstract locations so that they do not include x in their name. 
The renaming of abstract locations is specified by the function 
k",(nz) = nZ\{",} 
and we then take 
cP;A((S, H, is)) = {kill",((S, H, is))} 
where kill",((S,H,is)) = (S',H',is') is given by 
S' 
H' 
= 
= 
{(z, k",(nz)) I (z, nz) E S /\ z i x} 
((k",(nv), sel, k",(nw)) I (nv, sel, nw) E H} 
is' 
{k",(nx) I nx E is} 
so that we obtain strong nullification. It is easy to check that if (S, H, is) is 
compatible then so is (S', H', is'). 
Example 2.49 The statement [y:=nilj1 of the list reversal program of 
Example 2.43 is of the form considered here. Since there is no occurrence of 
y in the single shape graph in ~ of Figure 2.15, the shape graph Shape. (1) in 
Example 2.47 is equal to to 
â¢ 
An interesting case is when (x, n{ "'}) E S since this will cause the two abstract 
locations n{x} and n0 to be merged. The sharing information is then updated 
to capture that we can only be sure that n0 is unshared in the updated shape 
graph if both n0 and n{ "'} were unshared in the original shape graph. This is 
illustrated in Figure 2.16: the left hand picture shows the interesting parts of 
the shape graph (S, H, is) and the right hand picture shows the corresponding 
parts of (S', H', is'). We shall assume that the square boxes represent distinct 
abstract locations so in particular V, {x}, Wand 0 are all distinct sets. 
The fat boxes represent unshared abstract locations as before, the thin boxes 
represent abstract locations whose sharing information is not affected by the 
transfer function, and unlabelled edges between abstract locations represent 
pointers that are unaffected by the transfer function. 
Example 2.50 The statement [z:=nilF of the list reversal program of 
Example 2.43 illustrates this case: for each of the shape graphs of Shape. (2) 
the abstract location n{z} is merged with n0 to produce one of the shape 
graphs of Shape. (7). 
â¢ 
Remark. 
The analysis does not perform garbage collection: it might be 
the case that there are no heap pointers to n{",} and then the corresponding 
location in the heap will be unreachable after the assignment. Nonetheless 
the analysis will merge the two abstract locations n{ "'} and n0 and insist on 
a pointer from n0 to any abstract location that n{",} might point to. 
â¢ 

116 
2 Data Flow Analysis 
jJ 
X --I nx I----D 
X 
y 
y 
(5, H, is) 
(5", H" , is") 
Figure 2.17: The effect of [x:=yli when x"#y. 
Transfer function for [x: =y]l. If X = Y then the transfer function 
J'fA is just the identity. 
Next suppose that x "# y. The first effect of the assignment is to remove 
the old bindings to X; for this we use the killrIJ operation introduced above. 
Then the new binding to X is recorded; this includes renaming the abstract 
location that includes y in its variable set to also include x. The renaming 
of the abstract locations is specified by the function: 
Y(n ) _ {nZU{rIJ} if y E Z 
grIJ 
Z 
-
nz 
otherwise 
We shall then take 
Â¢~A((5,H,is)) = {(5",H",is")} 
where (5', H', is') = killrIJ((5, H, is)) and 
5" = {(z, g~ (nz)) I (z, nz) E 5'} 
u {(x,g~(ny)) I (y',ny) E 5' I\y' =y} 
H" = 
{(g~(nv),sel,g~(nw)) I (nv,sel,nw) E H'} 
is" = 
{g~(nz) I nz E is'} 
so that we obtain strong update. Here the second clause in the formula for 
5" adds the new binding to x. Again we note that if (5, H, is) is compatible 
then so is (5",H",is"). 
The clause is illustrated in Figure 2.17 where we assume that nodes represent 
distinct abstract locations; it follows from the invariants that y E Y but y fJ. V 
and y fJ. W. Note that nyu{rIJ} inherits the sharing properties of ny although 

2.6 Shape Analysis 
117 
both x and y will point to the same cell; the reason is that the sharing 
information only records sharing in the heap - not sharing via the state. 
Example 2.51 The statement [Y:=X]4 of the list reversal program of Ex-
ample 2.43 is of the form considered here: each of the shape graphs of 
Shape.(3) in Example 2.47 is transformed into one of the shape graphs of 
Shape. (4). 
Also the statement [z: =y]3 is of the form considered here: each of the shape 
graphs of Shape. (2) is transformed into one of those of Shape. (3). 
_ 
Transfer function for [x:=y.se!y. First assume that x = y; then 
the assignment is semantically equivalent to the following sequence of assign-
ments 
[t: =y.sel]i1 ; [x: =t]i2; [t: =nil]lS 
where t is a fresh variable and Â£1, Â£2 and f3 are fresh labels. The transfer 
function J'fA can therefore be obtained as 
f 5A = f5A 0 f5A 0 f5A 
l 
I.s 
1.2 
1.1 
where the transfer functions fi2A and fisA follow the pattern described above. 
We shall therefore concentrate on the transfer function filA, or equivalently, 
ffA in the case where x "I- y. 
Example 2.52 The statement [x:=x.cdr]5 of the list reversal program of 
Example 2.43 is transformed into [t: =x.cdr]51; [x: =t ]52; [t : =nil] 53 . We shall 
return to the analysis of [t:=x.cdr]51 later. 
_ 
So assume that x "I- y and let (5, H, is) be a compatible shape graph before 
the analysis of the statement. As in the previous case, the first step will be 
to remove the old binding for x and again we use the auxiliary function kill",: 
(5',H',is') = kill",((5,H,is)) 
The next step will be to rename the abstract location corresponding to y.sel 
to include x in its name and to establish the binding of x to that abstract 
location. We can now identify three possibilities: 
1. There is no abstract location ny such that (y, ny) E 5' or there is 
an abstract location ny such that (y,ny) E 5' but no nz such that 
(ny, sel, nz) E H'; in this case the shape graph will represent a state 
and a heap where y or y.sel is an integer, nil or undefined. 
2. There is an abstract location ny such that (y, ny) E 5' and there is 
an abstract location nu "I- n0 such that (ny, sel,nu) E H'; in this case 
the shape graph will represent a state and a heap where the location 
pointed to by y.sel will also be pointed to by some other variable (in U). 

118 
2 Data Flow Analysis 
x--__ 
y 
y 
(5, H, is) 
(5" , H", is") 
Figure 2.18: The effect of [x:=y.selji in Case 2 when x f.y. 
3. There is an abstract location ny such that (y, ny) E 5' and (ny, sel, n0) 
E H'; in this case the shape graph will represent a state and a heap 
where no other variable points to the location pointed to by y.sel. 
Case 1. First consider the statement [x:=y.selJl (where x f. y) in the case 
where there is no abstract location ny such that (y,ny) E 5'. Then there is 
no abstract location for y.sel and hence no abstract location to rename and 
no binding to establish. Thus we take: 
Â¢>IAÂ«5, H, is)) = {kill",Â«5, H, is))} 
Note that this situation captures the case where an attempt is made to deref-
erence a nil-pointer. 
Alternatively, there is an abstract location ny such that (y, ny) E 5' but there 
is no abstract location n such that (ny,sel,n) E H'. From the invariants it 
follows that ny is unique but still there is no abstract location to rename and 
no binding to establish. So again we take: 
Â¢>IAÂ«5, H, is)) = {kill",Â«5, H, is))} 
This situation captures the case where an attempt is made to dereference a 
non-existing selector field of pointer. 
Case 2. We consider the statement [x:=y.sel]l (where x f. y) in the case 
where there is an abstract location ny such that (y, ny) E 5' and there is an 
abstract location nu f. n0 such that (ny, sel, nu) E H'. Both ny and nu will 
be uniquely determined because of the invariants (and they might be equal). 
The abstract location nu will be renamed to include the variable x using the 
function: 
hU (n ) = {nuu{",} 
if Z = ~ 
'" 
z 
nz 
otherwlse 

2.6 Shape Analysis 
We shall then take 
Â¢;A((5, H, is)) = {(5", H", is")} 
where (5',H',is') = kill.,((5,H,is)) and 
5" = {(z, h~(nz)) I (z, nz) E 5'} u {(x, h~(nu))} 
H" = 
{(h~(nv),sel',h~(nw)) I (nv,sel',nw) E H'} 
is" = {h~(nz) I nz E is'} 
119 
The inclusion of (x, h~ (nu)) in 5" reflects the assignment. The definition of 
is" ensures that sharing is preserved by the operationj in particular, nUu{.,} 
is shared in H" if and only if nu is shared in H'. 
The effect of the assignment is illustrated in Figure 2.18 in the case where 
nu E is. As before we assume that the abstract locations shown on the figure 
are distinct so in particular Y, V and W are all distinct from U. 
Case 3. We now consider the statement [x:=y.seW (where x =I y) in the case 
where there is an abstract location ny such that (y, ny) E 5' and furthermore 
(ny, sel, n0) E H'. As before the invariants ensure that ny is uniquely deter-
mined. The location n0 describes the location for y.sel as well as a (possibly 
empty) set of other locations. We now have to materialise a new abstract 
location n{.,} from n0j then n{.,} will describe the location for y.sel and n0 
will continue to represent the remaining locations. Having introduced a new 
abstract location we will have to modify the abstract heap accordingly. 
This is a potentially difficult operation, so let us consider the following se-
quence of assignments: 
[x:=nil]Â·Â·Â·j [x:=y.sei]Lj [x:=nil]Â·Â·Â· 
t 
1 
(5", J,., ;s") 
1 
(5, H, is) 
(5', H', is') 
(5'" , H"', is"') 
Clearly [x:=nil]Â·Â·Â·j [x:=y.sel]l is equivalent to [x:=y.sei]L both in terms of the 
analysis and the semantics. Indeed, (5', H', is') = kill., ((5, H, is)) represents 
the effect of removing the binding to x. We are trying to determine candidate 
shape graphs (5", H", is") holding after the assignment [x:=y.sei]L (where 
x =1= y) but let us first study our expectations of (5"', H"', is"'). It is immediate 
that (5"', H"', is"') = kill., ((5" , H", is")). Furthermore, the states and heaps 
possible at the point described by (5', H' ,is') should be the same as those 
possible at the point described by (5"', H"', is"'). This suggests demanding 
that 
(5'" , H"', is"') = (5', H' , is') 

120 
2 Data Flow Analysis 
which means that kill x ((5", H", is")) = (5', H', is'). It is also immediate that 
(x,n{x}) E S" and that (ny,sel,n{x}) E H". 
We shall then take 
<PZA((S, H, is)) = {(S", H", is") I (5", H", is") is compatible 1\ 
killx ((5", H" , is")) = (5', H', is') 1\ 
(x,n{x}) E 5" 1\ (ny,sel,n{x}) E H"} 
where (5', H' , is') = killx ((5, H , is)). 
It is hopefully clear that we have not missed any shape graphs (5", H", is") 
that might be the result of the assignment. What might be a worry is that 
we have included an excessive amount of irrelevant shape graphs. (Indeed 
producing all compatible shape graphs would be trivially sound but also 
utterly useless.) Although it is possible to do slightly better (see Exercise 
2.23) we shall now argue that amount of imprecision in the above definition 
is not excessive. 
We first establish that 
5" = 5' U {(x,n{x})} 
showing that the abstract state is fully determined. Consider (z, nz) E 5". If 
z = x it follows from the compatibility of (5", H", is") that nz = n{x}. If z :I 
x it follows from (x, n{x}) E 5" and the compatibility of (5", H", is") that x (j. 
Z and hence (z, n z) = (z, kx (n z )) (where kx is the auxiliary function used to 
define the killx operation). This establishes that 5" ~ 5' U {(x,n{x})}. Next 
consider (u, nu) E 5'. We know that u :I x and x (j. U from the definition 
of 5' and from compatibility of (S',H',is'). There must exist (u,nu) E 5" 
such that kx(nu) = nu but since x :I u this gives nu = nu. It follows that 
5" ;2 5' U {(x,n{x})} and we have proved the required equality. 
We next establish that 
is' \ {n0} 
is" \ {n0,n{x}} 
n0 E is' 
iff n0 E is" V n{x} E is" 
showing that 
â¢ abstract locations apart from n0 retain their sharing information, 
â¢ if n0 is shared then that sharing cannot go away but must give rise to 
sharing of at least one of n0 or n{x}, and 
â¢ if n0 is not shared then no sharing can be introduced for n0 or n{x}. 
Since both (5' , H', is') and (5", H", is") are compatible shape graphs it follows 
that if nu E is' then x (j. U and if nu E is" then x (j. U V {x} = U. 
Hence is' = {kx(nu) I nu E is"} establishes is' \ {n01 = is" \ {n0,n{x}} 

2.6 Shape Analysis 
121 
y 
(5, H, is) 
Figure 2.19: The effect of [x:=y.set]l in a special case (part 1). 
because kx (nu) = nu :f. n0 for all nu E is" \ {n0, n{ x} }. Furthermore, 
." 
." . 
., 
d 
d'" 
d'''' 
d" 
n0 E IS 
V nix} E IS gIves n0 E IS, an n0 'F IS 
/\ nix} 'F IS gIves n0 'F IS. 
Thus we have established the required relationship. 
We now turn to the abstract heap. We shall classify the labelled edges 
(nv, sel', nw) into four groups depending on whether or not the source or 
target may be one of the nodes n0 or n{ x}: 
(nv,sel',nw) is external iff {nv,nw} n {n0,n{x}} = 0 
(nv,sel',nw) is internal iff {nv,nw} ~ {n0,n{x}} 
(nv,sel',nw) is going-out 
iff nv E {n0,n{x}} /\ nw Â¢ {n0,n{x}} 
(nv, sel',nw) is going-in 
iff nv Â¢ {n0,n{x}} /\ nw E {n0,n{x}} 
We shall also say that two edges (nv, sel', nw) and (n;,r, sel", n~) are related 
if and only if kx(nv) = kx(n;,r), sel' = sel" and kx(nw) = kx(n~). Clearly 
an external edge is related only to itself. 
Reasoning as above one can show that: 
â¢ H' and H" have the same external edges, 
â¢ each internal edge in H' is related to an internal edge in H" and vice 
versa, 
â¢ each edge going-out in H' is related to an edge going-out in H" and vice 
versa, and 
â¢ each edge going-in in H' is related to an edge going-in in H" and vice 
versa. 

122 
2 Data Flow Analysis 
x 
x 
y 
y 
(5" H" Â·5") 
1, 1,1 1 
(5" H" . ") 
2, 
2,152 
L 
~ 
x ~ 
x 
y 
y 
~ 
nv 
sel 
n0 
el 
nW 
~el3 
nV 
sel 
n0 
(5" H" Â·5") 
3, 3,1 3 
(5" H" is") 
4' 4' 
4 
L 
L 
x 
x 
y 
y 
Figure 2.20: The effect of [X: =y.seW in a special case (part 2). 
One consideration is that the going-in edge (ny, sel, n0) E HI should be 
changed into the going-in edge (ny,sel,n}x}) E HII. We clearly demanded 
that (ny,sel,n{x}) E H" and because (5' ,H",is") is compatible it follows 
that (ny, sel, n0) f/. H". 
As a more concrete illustration consider the scenario in Figure 2.19. Here 
neither n0 nor nw is shared and we assume that both nv and nw are distinct 
from n0; we also assume that x :I y and seh :I sel3. The result of the 

2.6 Shape Analysis 
123 
transfer function is shown in Figure 2.20. First note that the going-in edge 
(ny, sel, n0) E H is changed to (ny, sel, n{x}) E H~' in all shape graphs. Next 
note that the going-in edge labelled selt can only point to n0 because n{x} is 
not shared (as n0 is not) and ny points to n{x}. The going-out edge labelled 
seh can start at both n0 and n{ x} but it cannot do so simultaneously because 
nw is not shared. The internal edge labelled sel3 can only point to n0 because 
n{x} is not shared and ny points to n{x}; but it can start at both n0 and n{x} 
and can even do so simultaneously. This explains why there are six shape 
graphs in Â¢7A((5, H, is)), all of which are clearly needed. 
Example 2.53 The statement [t:=x.cdr]51 introduced in Example 2.52 
is of the form considered here: the transfer function will transform each of 
the shape graphs of Shape. (4) and subsequent transformations will produce 
Shape. (5). 
_ 
Transfer function for [x.sel: =aJ1~ where a is of the form n, al 0Pa a2 
or nil. Again we consider a compatible shape graph (5, H, is). First assume 
that there is no nx such that (x, nx) E 5; then x will not point to a cell 
in the heap and the statement will have no effect on the shape of the heap 
so the transfer function {fA is just the identity. Next assume that there is 
a (necessarily unique) nx such that (x, nx) E 5 but that there is no nu 
such that (n x, sel, nu) E H; then the cell pointed to by sel does not point to 
another cell so the statement will not change the shape of the heap and also 
in this case the transfer function fr will be the identity. 
The interesting case is when there are abstract locations nx and nu such that 
(x, n x) E 5 and (n x, sel, nu) E H; these abstract locations will be unique 
because of the invariants. The effect of the assignment will be to remove the 
triple (nx, sel, nu) from H: 
Â¢7A((5, H, is)) = {killx .sel((5, H, is))} 
where kill x .sel((5,H,is)) = (5',H',is') is given by: 
5' 
5 
H' 
= 
{(ny, sel', nw) I (ny, sel', nw) E H /\ -,(X = V ,/\ sel = sel')} 
{ 
is\{nu} if nu E is /\ #into(nu, H') :::; 1 /\ 
is' = 
-,3sel' : (n0, sel', nu) E H' 
is 
otherwise 
The sharing information is as before except that we may be able to do better 
for the node nu - we have removed one of the pointers to it and in the case 
where there is at most one pointer left and it does not have source n0 the 
corresponding location will be unshared. This is yet another aspect of strong 
update. Here we write #into(nu, H') for the number of pointers to nu in H'. 
This clause is illustrated in Figure 2.21. 

124 
2 Data Flow Analysis 
x 
(5, H, is) 
(5', H', is') 
Figure 2.21: The effect of [x.sel: =nillc when #into(nu, H') :::;1. 
Remark. Again we shall note that the analysis does not incorporate garbage 
collection: it might be the case that there is only one pointer to the abstract 
location nu and that after the assignment x.sel: =nil the corresponding lo-
cation will be unreachable. However, the abstract location may still be part 
of the shape graph. 
_ 
Transfer function for [x.sel: =yt First assume that x = y. The 
statement is then semantically equivalent to 
[t:=y]Â£lj [x.sel:=t]Â£2j [t:=nil]Â£3 
where t is a fresh variable and Â£1, Â£2 and Â£3 are fresh labels. The transfer 
function f'fA is then given by 
f SA = f SA 0 f SA 0 f5A 
Â£ 
Â£3 
Â£2 
Â£1 
The transfer functions filA and fi3A follow the pattern we have seen before 
so we shall concentrate on the clause for fi2A, or equivalently, fiA in the case 
where x Â¥- y. 
So assume that x Â¥- y and that (S, H, is) is a compatible shape graph. It may 
be the case that there is no nx such that (x, nx) E S and in that case the 
transfer function will be the identity since the statement cannot affect the 
shape of the heap. 
So assume that nx satisfies (x, nx) E S. The case where there is no ny such 
that (y, ny) E S corresponds to a situation where the value of y is an integer, 
the nil-value or undefined and is therefore similar to the case [x.sel: =nil]l: 
c/>7A((S, H, is)) = {kill",.sel((5, H, is))} 
The interesting case is when x Â¥- y, (x, nx) E 5 and (y, ny) E 5. The first 
step will be to remove the binding for x.sel and for this we can use the kill",.sel 
function. The second step will be to establish the new binding. So we take 
c/>7A((5,H,is)) = {(51 ,HI,is")} 

2.6 Shape Analysis 
125 
x 
x 
y 
(5,H,is) 
(5', H", is") 
Figure 2.22: The effect of [x.sel:=y]Â£ when #into(ny, H') < 1. 
where (5',H',is') = kill x .sel((5,H,is)) and 
S" 
5' (= 5) 
H" 
H'U{(nx,sel,ny)} 
'11 
IS 
= { is' U {ny} if #into(ny, H') 2: 1 
is' 
otherwise 
Note that the node ny might become shared when we add a new pointer to 
it. The effect of the transfer function is illustrated in Figure 2.22. 
Example 2.54 This transfer function is illustrated by the assignment 
[y.cdr:=z]6 of the list reversal program of Example 2.43: each of the shape 
graphs of Shape. (5) in Example 2.47 are transformed into one of the shape 
graphs of Shape. (6). 
_ 
Transfer function for [x. sel : =y. selT~ . This statement is equivalent 
to the sequence of statement 
[t:=y.sel']Â£l; [x.sel:=t]Â£2; [t:=nil]Â£3 
for t being a fresh variable and Â£1, Â£2 and Â£3 being fresh labels. Thus the 
transfer function JiA satisfies 
f SA = f SA 0 f SA 0 f SA 
Â£ 
Â£3 
Â£2 
Â£1 
The transfer functions fit, fi2
A and fi3A all follow the patterns we have seen 
before so this completes the specification of the transfer function. 
Transfer function for [malloc p]Â£. 
We first consider the statement 
[malloc X]i where we have to remove the binding for x and then introduce a 
new (unshared) location pointed to by x. Thus we define 
<pEA((S, H, is)) = {(5' U {(x,n{x})}, H', is')} 
where (5', H', is') = kill x (5, H, is). 

126 
2 Data Flow Analysis 
The statement [malloe (x.sel)Jl is equivalent to the sequence 
[malloe t]Â£1; [x.sel: =t]Â£2; [t: =nil]Â£3 
where t is a fresh variable and Â£1, Â£2 and Â£3 are fresh labels. The transfer 
function J'fA is then 
f 5A = f5A 0 f5A 0 f5A 
Â£ 
Â£3 
Â£2 
Â£1 
The transfer functions J't, 1I2A and 1I3A all follow the patterns we have seen 
before so this completes the specification of the transfer function. 
Concluding Remarks 
Data Flow Analysis for imperative languages. As mentioned 
in the beginning of this chapter, Data Flow Analysis has a long tradition. 
Most compiler textbooks contain sections on optimisation which mainly dis-
cuss Data Flow Analyses and their implementation [5, 55,181]. The emphasis 
in these books is often on practical implementations of data flow analyses. A 
classic textbook which provides a more theoretical treatment of the subject 
is by Hecht [69]; the book contains a detailed discussion of the four example 
Data Flow Analyses in Section 2.1, and also presents a more traditional treat-
ment of Monotone Frameworks based on the use of semi-lattices as well as a 
number of algorithms (see Chapter 6 for a more thorough treatment of algo-
rithms). Marlowe and Ryder [103] provide a survey of data flow frameworks. 
Steffen [164] and Schmidt [151] express data flow analyses using modal logic 
(rather than equations) thereby opening up the possibility of using model 
checking techniques for program analysis. 
The examples presented in Section 2.1 are fairly standard. Alternative treat-
ments of this material can be found in any of the books already cited. The 
examples may all be represented as Bit Vector Frameworks (see Exercise 2.9): 
the lattice elements may be represented by a vector of bits and the lattice op-
erations efficiently implemented as boolean operations. The method used in 
Section 2.2 to prove the correctness of the Live Variables Analysis is adapted 
from [112] and is expressed by means of an operational semantics [140, 130]. 
The notion of faint variables, introduced in Exercise 2.4, was first introduced 
by Giegerich, Moncke and Wilhelm [65]. 
The use of semi-lattices in Data Flow Analysis was first proposed in [96]. The 
notion of Monotone Frameworks is due to Kam and Ullman [93]. These early 
papers, and much of the later literature, use the dual notions (meets and 
maximal fixed points) to our presentation. Kam and Ullman [93] prove that 
the existence of a general algorithm to compute MOP solutions would imply 
the decidability of the Modified Post Correspondence Problem [76]. Cousot 
and Cousot [37] model abstract program properties by complete semi-lattices 
in their paper on Abstract Interpretation (see Chapter 4). 

Concluding Remarks 
127 
We have associated transfer functions with elementary blocks. It would be 
possible to associate transfer functions with flows instead as e.g. in [147]. 
These two approaches have equal power: to go from the first to the second, 
the transfer functions may be moved from the blocks to their outgoing flows; 
to go from the second to the first, we can introduce artificial blocks. In fact 
artificial blocks can be avoided as shown in Exercise 2.11. 
Most of the papers that we have cited so far concentrate on intraprocedural 
analysis. An early, and influential, paper on interprocedural analysis is [155] 
that studies two approaches to establishing context. One is based on call 
strings and expresses aspects of the dynamic calling context; our presenta-
tion is inspired by [178]. The other is the "functional approach" that is based 
on data and that shares some of the aims of assumption sets [99, 138, 145]; 
the technical formulation is different because [155] obtains the effect by cal-
culating the transfer functions for the call statement. Most of the subsequent 
papers in the literature can be seen as variations and combinations over this 
theme; a substantial effort in this direction may be found in [44]. As men-
tioned in Section 2.5.5, the use of large assumption sets may lead to equation 
systems where the transfer functions are not monotone; we refer to [51, 52] 
for a modern presentation of techniques that allow the solution of so-called 
weakly monotonic systems of equations. 
Pointer analysis. There is an extensive literature on the analysis of 
alias problems for languages with pointers. Following [62] we can distinguish 
between analyses of pointers to (1) statically allocated data (typically on 
the stack) and (2) dynamically allocated data (typically in the heap). The 
analysis of pointers to statically allocated data is the simplest: typically the 
data will have compile-time names and the analysis result can be presented 
as a set of points-to pairs of the form (p, x) meaning that the pointer p points 
to the data x or as alias pairs of the form (*p, x) meaning that *p and x are 
aliased. Analyses in this category include [47, 100, 145, 182, 162, 153]. 
The analysis of dynamically allocated data is more complicated since the 
objects of interest are inherently anonymous. The simplest analyses [38, 
61] study the connectivity of the heap: they attempt to split the heap into 
disjoint parts and do not keep any information about the internal structure 
of the individual parts. These analyses have been found quite useful for many 
applications. 
The more complex analyses of dynamically allocated data give more precise 
information about the shape of the heap. A number of approaches use graphs 
to represent the heap. A main distinction between these approaches is how 
they map a heap of potentially unbounded size to a graph of bounded size: 
some bound the length of paths in the heap [84, 165], others merge heap cells 
created at the same program point [85, 30], and yet others merge heap cells 
that cannot be kept apart by the set of pointer variables pointing to them 
[148, 149]. Another group of analyses obtain information about the shape 

128 
2 Data Flow Analysis 
of the heap by more directly approximating the access paths. Here a main 
distinction is the kind of properties of the access paths that are recorded: 
some focus on simple connectivity properties [62], others use some limited 
form of regular expressions [101], and yet others use monomial relations [45]. 
The analysis presented in Section 2.6 is based on the work of Sagiv, Reps 
and Wilhelm [148, 149]. In contrast to [148, 149] it uses sets of compatible 
shape graphs; [148, 149] merge sets of compatible shape graphs into a single 
summary shape graph and then use various mechanisms for extracting parts 
of the individual compatible shape graphs and in this wayan exponential 
factor in the cost of the analysis can be avoided. The sharing component of 
the shape graphs is designed to detect list-like properties; it can be replaced 
by other components detecting other shape properties [150]. 
Static Single Assignments. Some program analyses can be per-
formed more efficiently or more accurately when the program is transformed 
into an intermediate form called static single assignment (SSA) form [42]. 
The benefit of SSA form is that the definition-use chains of the program are 
explicit in the representation: each variable has at most one definition (mean-
ing that it is assigned at most once). As a consequence some optimisations 
can be performed more efficiently using this representation [12, 180, 109, 110]. 
The transformation to SSA form amounts to renaming the variables and 
introducing special assignments at the points where flow of control might 
join. The assignments use so-called Â¢>-functions; each argument position of 
the Â¢>-function identifies one of the program points where flow of control might 
come from. The special statements have the form x : = Â¢>(Xl' ... ,xn ) and the 
idea is that the value of x will equal the value of Xi whenever control comes 
from the i'th predecessor. The algorithms for transforming to SSA form 
often proceed in two stages: the first stage identifies the points where flow 
of control might join and where the special assignments are to be inserted, 
and the second stage renames the variables to ensure that each of them is 
assigned at most once. To obtain a compact representation of the program 
one wants to minimise the number of extra statements (and variables) and 
there are techniques based on additional data flow information for achieving 
this. 
Data Flow Analysis for other language paradigms. The 
analysis techniques that we have studied assume the existence of some rep-
resentation of the flow of control in the program. For the class of imperative 
languages that we have studied, it is relatively easy to determine this control 
flow information. For many languages, for example functional programming 
languages, this is not the case. The next chapter presents techniques for de-
termining control flow information for such languages and shows how Data 
Flow Analysis can be integrated with control flow analysis. 

Concluding Remarks 
129 
The techniques we have presented can be applied directly to other language 
paradigms. Two examples are in object-oriented programming and a commu-
nicating processes language. Vitek, Horspool and Uhl [178] present an analy-
sis for object-oriented languages which determines classes of objects and their 
lifetimes. Their analysis is an interprocedural analysis that uses a graph-
based representation of the memory as data flow values. Reif and Smolka 
[142] apply Data Flow Analysis techniques to distributed communicating 
processes to detect unreachable code and to determine the values of program 
expressions. They apply their analysis to a language with asynchronous com-
munication. Their reachability analysis is based on an algorithm that builds 
a spanning tree for each process flow graph and links matching transmits 
and receives between processes. They construct a Monotone Framework for 
determining value sets. 
Intraprocedural control flow analysis. Many compilers trans-
form the source program into an intermediate form consisting of sequences 
of fairly low-level instructions (like three address code) and then perform 
the optimisations based on this representation. For this to work more in-
formation is needed about the flow of control within the individual program 
segments; in our terminology we need the flow (or fIo~) relation in order 
to apply the data flow analysis techniques. It is the task of intraprocedural 
control flow analysis to provide this information. 
More refined forms of intraprocedural control flow analysis include struc-
tural analysis [154, 110] that aims at discovering a wider variety of control 
structures in the code - control structures like conditionals, while-loops and 
repeat-loops that resemble those of the source language. The starting point 
for structural analysis is the flow graph; it is examined to identify instances of 
the various control structures, the instances are replaced by abstract nodes 
and the connecting edges are collapsed; this process is repeated until the 
flow graph has been collapsed into a single abstract node. The approach 
described above is a refinement of classical techniques based on identifying 
natural loops (or interval analysis [69, 5, 110]) intended to provide more 
meaningful program structure. 
We refer to the Concluding Remarks of Chapter 6 for a discussion of systems 
implementing data flow analysers. 

130 
2 Data Flow Analysis 
du-chain 
Figure 2.23: du-and ud-chains. 
Mini Projects 
Mini Project 2.1 ud- and du-chains 
The aim of this mini project is to develop a more thorough understanding of 
the concepts of ud- and du-chains introduced in Subsection 2.1.5. 
1. The function ud is specified in terms of definition clear paths, whilst 
UD re-uses functions introduced for the Reaching Definitions and Live 
Variables Analyses. Prove that the two functions compute the same 
information. 
2. DU can be defined by analogy with UD. Starting from the definition of 
du, develop an equational definition of DU and verify its correctness. 
3. A Constant Propagation Analysis is presented in Subsection 2.3.3; an 
alternative approach would be to use du- and ud-chains. Suppose there 
is a block [x := njl that assigns a constant n to a variable x. By 
following the du-chain it is possible to find all blocks using the variable 
x. It is only safe to replace a use of x by the constant n in a block Â£' 
if all other definitions that reach Â£' also assign the same constant n to 
x. This can be determined by using the ud-chain. This is illustrated 
in Figure 2.23. Considering the program of Example 2.12, Constant 
Folding (followed by Dead Code Elimination) can be used to produce 
the following program: 
(if [z=313 then [z:=014 else [z:=315); [y:=316 ; [x:=3+zf 
Develop a formal description of this analysis. 
â¢ 

Mini Projects 
[ ass] 
[ skip] 
([x := a]l, (7, tr) -+ Â«(7[X I---t A[a](7], tr : (x, e)) 
([skip]l, (7, tr) -+ Â«(7, tr) 
(81 , (7, tr) -+ (8~, (71, tr/) 
(81 ; 82 , (7, tr) -+ (82 , (71, tr/) 
(if [W then 81 else 82 , (7, tr) -+ (81 , (7, tr) 
(if [W then 81 else 82,(7, tr) -+ (82,(7, tr) 
131 
if B[b](7 = true 
if B[b](7 = false 
[whIJ 
(while [W do 8, (7, tr) -+ Â«(8; while [W do 8), (7, tr) 
if B[b](7 = true 
[wh2 ] 
(while [b]l do 8, (7, tr) -+ Â«(7, tr) 
if B[b](7 = false 
Table 2.10: The instrumented semantics of WHILE. 
Mini Project 2.2 Correctness of Reaching Definitions 
The aim of this mini project is to prove the correctness of Reaching Definitions 
with respect to the notion of semantic reaching definitions introduced in 
Section 1.5. To get a precise definition of the set of traces of interest we shall 
begin by introducing a so-called instrumented semantics: an extension of a 
more traditional semantics that keeps track of additional information that is 
mainly of interest for the program analysis. 
The instrumented semantics has transitions of the forms: 
(8, (7, tr) -+ Â«(71, tr/) and (8, (7, tr) -+ (8' , (71, tr/) 
All configurations include a trace tr E Trace = (Var x Lab)* that records 
the elementary block in which a variable is being assigned. The detailed 
definition of the instrumented semantics is given in Table 2.10. 
Given a program 8* and an initial state (7* E State it is natural to construct 
the trace 
where Xl, ... ,Xn are the variables in 8* and to consider the finite derivation 

132 
2 Data Flow Analysis 
sequence: 
(8*,0'*, tr *) --t * (0", tr') 
Intuitively, there should be a similar derivation sequence (8*, a*) --t* a' in the 
Structural Operational Semantics. Similar remarks apply to infinite deriva-
tion sequences. 
As in Section 2.2 we shall study the constraint system RD~(8*) corresponding 
to the equation system RD=(8*). Let reach be a collection of functions: 
reachentry,reachexit : Lab* --t P(Vax* x Lab*) 
We say that reach solves RD~(8), and write 
reach 1= RD~(8) 
if the functions satisfy the constraints; similarly for reach 1= RD=(8). 
1. Formulate and prove results corresponding to Lemmas 2.15, 2.16 and 
2.18. 
The correctness relation I"V will relate traces tr E Trace to the information 
obtained by the analysis. Let Y ~ P(Var* x Lab*) and define 
tr I"V Y 
iff \Ix E Vax* : (x, SRD(tr)(x)) E Y 
meaning that Y contains at least the semantically reaching definitions ob-
tained from the trace tr by the function SRD introduced in Section 1.5. 
2. Formulate and prove results corresponding to Lemma 2.20, Theorem 
2.21 and Corollary 2.22. 
â¢ 
Mini Project 2.3 A Prototype Implementation 
In this mini project we shall implement one of the program analyses consid-
ered in Section 2.1. As implementation language we shall choose a functional 
language such as Standard ML or Haskell. We can then define a suitable 
data type for WHILE programs as follows: 
type var 
string 
type label 
int 
datatype aexp = Var of var I Const of int 
Op of string * aexp * aexp 
and bexp 
= True I False 
Not of bexp I Boolop of string * bexp * bexp 
Relop of string * aexp * aexp 
datatype stat = Assign of var * aexp * label I Skip of label 
Seq of stat * stat I If of bexp * label * stat * stat 
While of bexp * label * stat 

Exercises 
133 
Now proceed as follows: 
1. Implement the operations init, final, flow, flovJl and blocks. 
2. Generate the data flow equations for the Live Variables Analysis of 
Subsection 2.1.4. 
3. Solve the data flow equations; the function should be based on the 
algorithm of Section 2.4. 
For the more ambitious: generalise your program to accept an instance of a 
Monotone Framework as input. 
_ 
Exercises 
Exercise 2.1 Formulate data flow equations for the Reaching Definitions 
Analysis of the program studied in Example 1.1 of Chapter 1 and in particular 
define the appropriate gen and kill functions. 
_ 
Exercise 2.2 Consider the following program: 
[x:=lF; (while [y>O]2 do [x:=x-l]3); [x:=2]4 
Perform a Live Variables Analysis for this program using the equations of 
Section 2.1.4. 
_ 
Exercise 2.3 A modification of the Available Expressions Analysis de-
tects when an expression is available in a particular variable: a non-trivial 
expression a is available in x at a label Â£ if it has been evaluated and assigned 
to x on all paths leading to Â£ and if the values of x and the variables in the 
expression have not changed since then. Write down the data flow equations 
and any auxiliary functions for this analysis. 
_ 
Exercise 2.4 Consider the following program: 
[x:=l]l; [x:=x-1F; [x:=2]3 
Clearly x is dead at the exits from 2 and 3. But x is live at the exit of 1 even 
though its only use is to calculate a new value for a variable that turns out to 
be dead. We shall say that a variable is a faint variable if it is dead or if it is 
only used to calculate new values for faint variables; otherwise it is strongly 
live. In the example x is faint at the exits from 1, 2 and 3. Define a Data 
Flow Analysis that detects strongly live variables. (Hint: For an assignment 
[x := all the definition of !t(l) should be by cases on whether x is in 1 or 
not.) 
_ 

134 
2 Data Flow Analysis 
Exercise 2.5 A basic block is often taken to be a maximal group of state-
ments such that all transfers to the block are to the first statement in the 
group and, once the block has been entered, all statements in the group are 
executed sequentially. In this exercise we shall consider basic blocks of the 
form 
where n ? 0 and B is x := a, skip or b. Reformulate the analyses of Section 
2.1 for this more general notion of basic block. 
â¢ 
Exercise 2.6 Consider the analyses Available Expressions and Reaching 
Definitions. Which of the equations make sense for programs that do not 
have isolated entries (and how can this be improved)? Similarly, which of 
the equations for Very Busy Expressions and Live Variables make sense for 
programs that do not have isolated exits (and how can this be improved)? 
(Hint: See the beginning of Section 2.3.) 
â¢ 
Exercise 2.7 Consider the correctness proof for the Live Variables Anal-
ysis in Section 2.2. Give a compositional definition of LV=(Â·Â·Â·) for a label 
consistent statement using 
as one of the clauses and observe that a similar development is possible for 
LVC;;( ... ). Give a formal definition of live F C where C is a set of equalities 
or inclusions as might have been produced by LV=(S) or LVC;;(S). 
Prove that {live !live F LVC;;(S)} is a Moore family in the sense of Appendix 
A (with n being n and determine whether or not a similar result holds for 
{live !live F LV=(S)}. 
â¢ 
Exercise 2.8 Show that Constant Propagation is a Monotone Framework 
with the set :Fcp as defined in Section 2.3.3. 
â¢ 
Exercise 2.9 A Bit Vector Framework is a special instance of a Monotone 
Framework where 
â¢ L = (P(D),!;) for some finite set D and where!; is either ~ or ;2, and 
â¢ :F = {f : P(D) -+ P(D)! 3YJ, Yl ~ D : W ~ D : 
f(Y) = (Y n YJ) U Yl} 
Show that the four classical analyses of Section 2.1 are Bit Vector Frame-
works. Show that all Bit Vector Frameworks are indeed Distributive Frame-
works. Devise a Distributive Framework that is not also a Bit Vector Frame-
work. 
â¢ 

Exercises 
135 
Exercise 2.10 Consider the Constant Propagation Analysis of Section 
2.3.3 and the program 
(if [ ... ]1 then [x:=-1]2 j [y:=1]3 else [x:=1]4 j [y:=-1]5)j [z:=x*y]6 
Show that MFP.(6) differs from MOP.(6). 
-
Exercise 2.11 In our formulation of Monotone Frameworks we associate 
transfer functions with basic blocks. In a statement of the form 
if [b]Â£ then 81 else 82 
this prevents us from using the result of the test to pass different information 
to 8 1 and S2j as an example suppose that x is known to be positive or negative 
and that b is the test x>O, then x is always positive at the entry to 8 1 and 
always negative at the entry to 8 2 â¢ To remedy this deficiency consider writing 
[b]l as [b]ll,l2 where Â£1 corresponds to b evaluating to true and Â£2 corresponds 
to b evaluating to false. Make the necessary changes to the development in 
Sections 2.1 and 2.3. (Begin by considering forward analyses.) 
_ 
Exercise 2.12 Consider one of the analyses Available Expressions, Very 
Busy Expressions and Live Variables Analysis and perform a complexity anal-
ysis in the manner of Example 2.30. 
_ 
Exercise 2.13 Let F be Bow(8*) and E be {init(S*)} for a label consis-
tent program 8*. Show that 
Prove a similar result when F is Bo~(8*) and E is final(8*). 
-
Exercise 2.14 In a Detection of Signs Analysis one models all negative 
numbers by the symbol -, zero by the symbol 0, and all positive numbers 
by the symbol +. As an example, the set {-2, -1, I} is modelled by the set 
{-, +}, that is an element of the powerset P( {-, 0, +}). 
Let 8* be a program and Var * be the finite set of variables in 8*. Take 
L to be Var* --t P({-,O,+}) and define an instance (L,F,F,E,t,j.) of a 
Monotone Framework for performing Detection of Signs Analysis. 
Similarly, take L' to be P (Var * x { -, 0, + }) and define an instance (L', :F' , F' , 
E' , t' ,F) of a Monotone Framework for Detection of Signs Analysis. Is there 
any difference in the precision obtained by the two approaches? 
_ 

136 
2 Data Flow Analysis 
Exercise 2.15 In the previous exercise we defined a Detection of Signs 
Analysis that could not record the interdependencies between signs of vari-
ables (e.g. that two variables x and y always will have the same sign); this is 
sometimes called an independent attribute analysis. In this exercise we shall 
consider a variant of the analysis that is able to record the interdependencies 
between signs of variables; this is sometimes called a relational analysis. To do 
so take L to be P(Var* -+ {-,O,+}) and define an instance (L,F,F,E,~,f.) 
of a Monotone Framework for performing Detection of Signs Analysis. Con-
struct an example showing that the result of this relational analysis may be 
more informative than that of the independent attribute analysis. The dis-
tinction between independent attribute methods and relational methods is 
further discussed in Chapter 4. 
_ 
Exercise 2.16 The interprocedural analysis using bounded call strings 
uses contexts to record the last k call sites. Reformulate the analysis for a 
notion of context that records the last k distinct call sites. Discuss whether 
or not this analysis is useful for distinguishing between the call of a procedure 
and subsequent recursive calls. 
_ 
Exercise 2.17 Consider the Fibonacci program of Example 2.33 and the 
Detection of Signs Analysis of Exercise 2.15 and Example 2.36. Construct 
the data flow equations corresponding to using large and small assumption 
sets, respectively. 
_ 
Exercise 2.18 Choose one of the four classical analyses from Section 2.1 
and formulate it as an interprocedural analysis based on call strings. (Hint: 
Some may be easier than others.) 
_ 
Exercise 2.19 Extend the syntax of programs to have the form 
begin D*; input x; S*; output y end 
so that it maps integers to integers rather than states to states. Consider the 
Detection of Signs Analysis and define the transfer functions for the input 
and output statements. 
_ 
Exercise 2.20 Consider extending the procedure language such that pro-
cedures can have multiple call-by-value, call-by-result and call-by-value-result 
parameters as well as local variables and reconsider the Detection of Signs 
Analysis. How should one define the transfer functions associated with pro-
cedure call, procedure entry, procedure exit, and procedure return? 
_ 
Exercise* 2.21 Compute the shape graphs Shape. (1), ... , Shape. (7) of 
Example 2.47 using the information supplied in Examples 2.48, 
2.54. 
(Warning: there will be more than 50 shape graphs.) 
_ 

Exercises 
137 
Exercise 2.22 In the Shape Analysis of Section 2.6 work out direct def-
initions of the transfer functions for elementary statements of the forms 
[x: =x.set]l, [x.sel: =x]l, [x.sel: =x.sel']l and [malloc (x.sel)t 
_ 
Exercise* 2.23 Consider Case 3 in the definition of the transfer function 
for [x:=y.selJl (where x =1= y) in the Shape Analysis. Make a careful analy-
sis of internal, going-in and going-out edges and determine whether or not 
some of the shape graphs (5 11 , H", isll) in cP7A((5, H, is)) can be removed by 
placing stronger demands on the edges in H" compared to those in H' (where 
(5', H', is') = kill., ((5, H, is))). 
_ 
Exercise* 2.24 The Shape Analysis as presented in Section 2.6 does not 
take garbage collection into account. Modify the Structural Operational Se-
mantics of the pointer language to perform garbage collection and subse-
quently modify the analysis to reflect this. 
_ 
Exercise* 2.25 The use of a single abstract summary location leads to a 
certain amount of inaccuracy in the Shape Analysis. A more accurate anal-
ysis could associate allocation sites with the abstract locations. An abstract 
location would then have the form nl,X where e is an allocation site (a label 
of a malloc-statement) and X is a set of variables as before. Develop the 
transfer functions for the new analysis. 
_ 

Chapter 3 
Constraint Based Analysis 
In this chapter we present the technique of Constraint Based Analysis us-
ing a simple functional language, FUN. We begin by presenting an abstract 
specification of a Control Flow Analysis and then study its theoretical prop-
erties: it is correct with respect to a Structural Operational Semantics and it 
can be used to analyse all programs. This specification of the analysis does 
not immediately lend itself to an efficient algorithm for computing a solution 
so we proceed by developing first a syntax directed specification and then 
a constraint based formulation and finally we show how the constraints can 
be solved. We conclude by illustrating how the precision of the analysis can 
be improved by combining it with Data Flow Analysis and by incorporating 
context information thereby linking up with the development of the previous 
chapter. 
3.1 
Abstract O-CFA Analysis 
In Chapter 2 we saw how properties of data could be propagated through a 
program. In developing the specification we relied on the ability to identify for 
each program fragment all the possible successor (and predecessor) fragments 
via the operator flow (and flo~) and the interprocedural flow inter-flow* 
(and inter-floW:;). The usefulness of the resulting specification was due to 
the number of successors and predecessors being small (usually just one or two 
except for procedure exits). This is a typical feature of imperative programs 
without procedures but it usually fails for more general languages, whether 
imperative languages with procedures as parameters, functional languages, 
or object-oriented languages. In particular, the interprocedural techniques of 
Section 2.5 provide a solution for the simpler cases where the program text 
allows one to limit the number of successors, as is the case when a proce-

140 
3 Constraint Based Analysis 
dure call is performed by explicitly mentioning the name of the procedure. 
However, these techniques are not powerful enough to handle the dynamic 
dispatch problem where variables can denote procedures. In Section 1.4 we 
illustrated this by the functional program 
let f = fn x => x 1; 
g = fn y => y+2; 
h = fn z => z+3 
in (f g) + (f h) 
where the function application x 1 in the body of f will transfer control 
to the body of the function x, and here it is not so obvious what program 
fragment this actually is, since x is the formal parameter of f. The Control 
Flow Analysis of the present chapter will provide a solution to the dynamic 
dispatch problem by determining for each subexpression a hopefully small 
number of functions that it may evaluate to; thereby it will determine where 
the flow of control may be transferred to in the case where the subexpression 
is the operator of a function application. In short, Control Flow Analysis 
will determine the interprocedural flow information (inter-fIow* or IF) upon 
which the development of Section 2.5 is based. 
Syntax of the FUN language. For the main part of this chapter 
we shall concentrate on a small functional language: the untyped lambda 
calculus extended with explicit operators for recursion, conditional and local 
definitions. The purpose of the Control Flow Analysis will be to compute 
for each sub expression the set of functions that it could evaluate to, and to 
express this it is important that we are able to label all program fragments. 
We shall be very explicit about this: a program fragment with a label is 
called an expression whereas a program fragment without a label is called a 
term. So we use the following syntactic categories: 
e 
E Exp 
expressions (or labelled terms) 
t 
E Term terms (or unlabelled expressions) 
We assume that a countable set of variables is given and that constants 
(including the truth values), binary operators (including the usual arithmetic, 
boolean and relational operators) and labels are left unspecified: 
f,x 
E Var 
variables 
c 
E Const 
op 
E Op 
i 
E Lab 
constants 
binary operators 
labels 
The abstract syntax of the language is now given by: 
e .. _ tL 
t .. -
c I x I fn x => eo I fun f x => eo I el e2 
if eo then el else e2 I let x = el in e2 I el op e2 

3.1 Abstract O-CFA Analysis 
141 
Here fn x => eo is a function definition (or function abstraction) whereas 
fun f x => eo is a recursive variant of fn x => eo where all free occurrences of 
f in eo refer to fun f x => eo itself. The construct let x = e1 in e2 is a non-
recursive local definition that is semantically equivalent to (fn x => e2)(e1). 
As usual we shall use parentheses to disambiguate the parsing whenever 
needed. Also we shall assume throughout that in all occurrences of fun f x => 
eo, f and x are distinct variables. 
We shall need the notion of free variables of expressions and terms so we 
define the function 
FV: (TermUExp) -t P(Var) 
in the following standard way. The abstractions fn x => eo and fun f x => eo 
contain binding occurrences of variables so FV( fn x => eo) = FV( eo) \ {x} and 
similarly FV(fun f x => eo) = FV(eo) \ {f,x}. Similarly, let x = e1 in e2 
contains a binding occurrence of x so we have FV(let x = e1 in e2) = 
FV(e1) U (FV(e2) \ {x}) reflecting that free occurrences of x in e1 are bound 
outside the construct. The remaining clauses for FVare straightforward. 
Example 3.1 The functional program (fn x => x) (fn y => y) consid-
ered in Section 1.4 is now written as: 
Compared with the notation of Example 1.2 we have omitted the square 
brackets. 
_ 
Example 3.2 Consider the following expression, loop, of FUN: 
(let g = (fun f x => (f1 (fn y => y2)3)4)5 
in (g6 (fn z => z7)8)9)10 
It defines a function g that is applied to the identity function fn z => z7. 
The function g is defined recursively: f is its local name and x is the formal 
parameter. Hence the function will ignore its actual parameter and call itself 
recursively with the argument fn y => y2. This will happen again and again 
so the program loops. 
_ 
3.1.1 
The Analysis 
Abstract domains. We shall now show how to specify O-CFA analyses. 
These may be regarded as the simplest possible form of Control Flow Analysis 
in that no context information is taken into account. (As will become clear 
in Section 3.6, this is what the number 0 is indicating.) 
The result of a O-CFA analysis is a pair (e, p) where: 

142 
3 Constraint Based Analysis 
â¢ C is the abstract cache associating abstract values with each labelled 
program point. 
â¢ P is the abstract environment associating abstract values with each 
variable. 
This is made precise by: 
v E Val 
P(Term) 
abstract values 
-
-
P E Env 
Var ~ Val abstract environments 
C E C;clle 
Lab ~ Val abstract caches 
Here an abstract value v is an abstraction of a set of functions: it is a set of 
terms of the form fn x => eo or fun f x => eo. We will not be recording any 
constants in the abstract values because the analysis we shall specify is a pure 
Control Flow Analysis with no Data Flow Analysis component; in Section 
3.5 we shall show how to extend it with Data Flow Analysis components. 
Furthermore, we do not need to assume that all bound variables are distinct 
and that all labels are distinct, but clearly, greater precision is achieved if 
this is the case; this means that semantically equivalent programs can have 
different analysis results and this is a common feature of all approaches to 
program analysis. As we shall see an abstract environment is an abstraction 
of a set of environments occurring in closures at run-time (see the semantics 
in Subsection 3.2.1). In a similar wayan abstract cache might be considered 
as an abstraction of a set of execution profiles: as discussed below some texts 
prefer to combine the abstract environment with the abstract cache. 
Example 3.3 Consider the expression ((fn x => xl)2 (fn y => y3)4)5 
of Example 3.1. The following table contains three guesses of a O-CFA anal-
ysis: 
(Ce, Pe) 
(C~, P'e) 
(C~, p:n 
1 
{fn y => y<S} 
{fn y => y<S} 
{fn x => Xl, fn y => y<S} 
2 
{fn x => Xl} 
{fn x => xl} 
{in x => xl, fn y => y3} 
3 
0 
0 
{fn x => xl, fn y => y3} 
4 
{fn y => y3} 
{fn y => y3} 
{fn x => xl,fn y => y3} 
5 
{fn y => y3} 
{fn y => y3} 
{fn x => xl, fn y => y3} 
x 
{fn y => y3} 
0 
{fn x => xl, fn y => y3} 
Y 
0 
0 
{fn x => Xl, fn y => y3} 
Intuitively, the guess (Ce, Pe) of the first column is acceptable whereas the 
guess (C~, P'e) of the second column is wrong: it would seem that fn x => 
xl is never called since P'e(x) = 0 indicates that x will never be bound to 
any closures. Also the guess (C~,~) of the third column would seem to be 
acceptable although clearly more imprecise than (Ce,Pe). 
â¢ 

3.1 Abstract O-CFA Analysis 
143 
Example 3.4 Let us consider the expression, loop, of Example 3.2 and 
introduce the following abbreviations for abstract values: 
f 
= 
idy 
= 
idz 
= 
fun f x => (fl (fn y => y2)3)4 
fn y => y2 
fn z => z7 
One guess of a O-CFA analysis for this program is (Clp, Pip) defined by: 
Clp(l) 
if} 
C1P(6) 
if} 
PiP (f) 
= 
if} 
CIP(2) 
= 
0 
Clp(7) 
= 
0 
Pip (g) 
if} 
CIP(3) 
= 
{idyl 
Clp(8) 
= 
{idz } 
PiP (x) 
= 
{idy, idz } 
Clp(4) 
= 
0 
Clp(9) 
= 
0 
Plp(y) 
= 
0 
C1p(5) 
if} 
Clp(lO) 
0 
Plp(z) 
= 
0 
= 
Intuitively, this is an acceptable guess. The choice of Pip (g) = if} reflects that 
g will evaluate to a closure constructed from that abstraction. The choice of 
Plp(x) = {idy, idz } reflects that x will be bound to closures constructed from 
both abstractions in the course of the evaluation. The choice of Clp(lO) = 0 
reflects that the evaluation of the expression will never terminate. 
_ 
We have already said that Control Flow Analysis computes the interproce-
dural flow information used in Section 2.5. It is also instructive to point 
out the similarity between Control Flow Analysis and Definition- Use chains 
(du-chains) for imperative languages (see Subsection 2.1.5): in both cases we 
attempt to trace how definition points reach points of use. In the case of 
Control Flow Analysis the definition points are the points where the function 
abstractions are created, and the use points are the points where functions 
are applied; in the case of Definition-Use chains the definition points are the 
points where variables are assigned a value, and the use points are the points 
where values of variables are accessed. 
Remark. Clearly an abstract cache C : Lab -t Val and an abstract environ-
ment Ii: Var -t "Vai can be combined into an entity of type (Var U Lab) -t 
"Vai. Some texts dispense with the labels altogether, simply using an abstract 
environment and no abstract cache, by ensuring that all subterms are prop-
erly "labelled" by variables. This type of expression frequently occurs in the 
internals of compilers in the form of "continuation passing style", "A-normal 
form" or "three address code". We have abstained from doing so in order to 
illustrate that the techniques not only work for compiler intermediate forms 
but also for general programming languages and calculi for computation; this 
flexibility is useful when dealing with non-standard applications (as discussed 
in the Concluding Remarks). 
_ 

144 
3 Constraint Based Analysis 
[con] 
(C, ji) 1= cl always 
[var] 
(C,ji) 1= xl iff ji(x) ~ C(f) 
[fn] 
(C,ji) 1= (fn x => eo)l iff {fn x => eo} ~ C(f) 
[fun] 
(C, ji) 1= (fun f x => eo)l iff {fun f x => eo} ~ C(f) 
[app] 
(C, ji) 1= (til t~2)l 
iff (C, ji) 1= til /\ (C, ji) 1= t~2 /\ 
(V(fn x => t~O) E C(f1) : 
(C, ji) 1= t~O /\ 
C(f2) ~ ji(x) /\ C(fo) ~ C(f)) /\ 
(V(fun f x => t~O) E C(f1) : 
(C, ji) 1= t~O /\ 
C(f2) ~ ji(x) /\ C(fo) ~ C(f) /\ 
{fun f x => t~O} ~ ji(J)) 
[ijJ 
(C, ji) 1= (if t~O then til else t~2)l 
iff (C, ji) 1= t~O /\ 
(C, ji) 1= til /\ (C, ji) 1= t~2 /\ 
C(f1) ~ C(f) /\ C(f2) ~ C(f) 
[let] 
(C, ji) 1= (let x = til in t~2)l 
iff (C, ji) 1= til /\ (C, ji) 1= t~2 /\ 
C(f1) ~ ji(x) /\ C(f2) ~ C(f) 
[op] 
(C, ji) 1= (til op t~2)l iff (C, ji) 1= til /\ (C, ji) 1= t~2 
Table 3.1: Abstract Control Flow Analysis (Subsections 3.1.1 and 3.1.2). 
Acceptability relation. It remains to determine whether or not a 
proposed guess (C, ji) of an analysis results is in fact an acceptable O-CFA 
analysis for the program considered. We shall give an abstract specification 
of what this means; having studied its theoretical properties (in Section 3.2) 
we then consider how to compute the desired analysis (in Sections 3.3 and 
3.4). 
It is instructive to point out that the abstract specification corresponds to an 
implicit formulation of the data flow equations of Chapter 2; it will be used 
to determine whether or not a a guess is indeed an acceptable solution to 
the analysis problem. The syntax directed and constraint based formulations 
(of Sections 3.3 and 3.4) correspond to explicit formulations of the data flow 
equations from which an iterative algorithm in the spirit of Chaotic Iteration 
(Section 1.7) can be used to compute an analysis result. 

3.1 Abstract O-CFA Analysis 
145 
For the formulation of the abstract O-CFA analysis we shall write 
(C,P) Fe 
for when (C, p) is an acceptable Control Flow Analysis of the expression e. 
Thus the relation "F" has functionality 
F : (C~e x ~ 
x Exp) -+ {true, false} 
and its defining clauses are given in Table 3.1 (writing "always" for "iff true"); 
the clauses are explained below but the relation F will not be formally defined 
until Subsection 3.1.2. 
The clause [con] places no demands on C(l) because we are not tracking 
any data values in the pure O-CFA analysis considered here and because we 
assume that there are no functions among the constants; the clause can be 
reformulated as 
(C, P) F ct iff 0 ~ C(l) 
thereby highlighting this point. 
The clause [var] is responsible for linking the abstract environment into the 
abstract cache: so in order for (C, p) to be an acceptable analysis, everything 
the variable x can evaluate to has to be included in what may be observed 
at the program point l: p(x) ~ C(l). 
The clauses [fn] and [fun] simply demand that in order for (C, p) to be an 
acceptable analysis, the functional term (fn x => eo or fun f x => eo) must 
be included in C(l); this says that the term is part of a closure that can arise 
at pro~ram point l during evaluation. Note that these clauses do not demand 
that (C, p) is an acceptable analysis result for the bodies of the functions; the 
clause for function application will take care of that. 
Before turning to the more complicated clause [app] let us consider the clauses 
[if] and [let]. They contain "recursive calls" demanding that subexpressions 
must be analysed in consistent ways using (C, p); additionally, the clauses 
explicitly link the values produced by sub expressions to the value of the 
overall expression, and in the case of [let] also the abstract cache is linked 
into the abstract environment. The interplay between the clauses [var] and 
[let] is illustrated in Figure 3.1; as in Chapter 2 an arrow indicates a flow of 
information. The clause [op] follows the same overall pattern. 
Clause [app] also contains "recursive calls" demanding that the operator til 
and the operand t~2 can be analysed using (C, p). For each term fn x => t~O 
that may reach the operator position (ld, i.e. where 
(;fn x => t~O) E C(ld 
it further demands that the actual parameter (labelled (2) is linked to the 
formal parameter (x) 

146 
3 Constraint Based Analysis 
C 
P 
I (let x = :il in t~2)ll 
,"-. 
~E( 
'~ 
l 
I t!21 
~ 
A 
I 
, 
I 
, 
L 
_ 
_ 
- - .. 
L 
_ 
_ 
Figure 3.1: Pictorial illustration of the clauses [let] and [var]. 
and that the result of the function evaluation (labelled Â£0) is linked to the 
result of the application itself (labelled Â£) 
C(Â£o) ~ C(Â£) 
and finally, that the function body itself can be analysed using (C,P): 
(C,P) F= t~O 
This is illustrated in Figure 3.2. For terms fun f x => t~O the demands are 
much the same except that the term itself additionally needs to be included 
in ji(f) in order to reflect the recursive nature of fun f x => t~O . 
Example 3.5 Consider Example 3.3 and the guesses of a O-CFA analysis 
for the expression. First we show that (Ce, Pe) is an acceptable guess: 
(Ce,Pe) F= ((fn x => XI)2 (fn y => y3)4)5 
Using clause [app] and Ce(2) = {fn x => Xl} we must check: 
(Ce, Pe) F= (fn x => XI)2 
(Ce, Pe) F= (fn y => y3)4 
~ 
I 
(Ce,Pe) 1= x 
Ce(4) ~ iJe(x) 
Ce(l) ~ Ce(5) 

3.1 Abstract O-CFA Analysis 
147 
r:tl 
' , 
L$I------'---->-----' 
I t!21----...,. 
~ 
, , 
, , 
.. - -
- - .. 
L 
_ 
_ 
- - .. 
.. - -
XL/~-~------------------~ 
Figure 3.2: Pictorial illustration of the clauses lapp], [fn] and [var]. 
All of these are easily checked using the clauses [fn] and [var]. 
Next we show that (C~,~) is not an acceptable guess: 
We do so by proceeding as above and observing that C~(4) ~ ~(x). 
â¢ 
Note that the clauses contain a number of inclusions of the form 
lhs ~ rhs 
where rhs is of the form C(Â£) or p(x) and where lhs is of the form C(Â£), p(x), or 
{t}. These inclusions express how the higher-order entities may flow through 
the expression. 
It is important to observe that the clauses [fn] and [.fun] do not contain 
"recursive calls" demanding that sub expressions must be analysed. Instead 
one relies on the clause [app] demanding this for all "sub expressions" that 
may eventually be applied. This is a phenomenon common in program anal-
ysis, where one does not want to analyse unreachable program fragments: 
occasionally results obtained from these parts of the program can suppress 
transformations in the reachable part of the program. It also allows us to 
deal with open systems where functions may be supplied by the environment; 
this is particularly important for languages involving concurrency. However, 

148 
3 Constraint Based Analysis 
note that this perspective is different from that of type inference, where even 
unreachable fragments must be correctly typed. 
In the terminology of Section 2.5 the analysis is flow-insensitive because FUN 
contains no side effects and because we analyse the operand to a function call 
even when the operator cannot evaluate to any function; see Exercise 3.3 for 
how to improve on this. Also the analysis is context-insensitive because it 
treats all function calls in the same way; we refer to Section 3.6 for how to 
improve on this. 
3.1.2 
Well-definedness of the Analysis 
Finally, we need to clarify that the clauses of Table 3.1 do indeed define a 
relation. The difficulty here is that the clause [appj is not in a form that 
allows us to define (C, p) 1= e by structural induction in the expression e -
it requires checking the acceptability of ~C, p) for an expression t~O that is 
not a subexpression of the application (tIl t~2)t. This leads to defining the 
relation "1=" of Table 3.1 by coinduction, that is as the greatest fixed point of 
a certain functional. An alternative will be to define the analysis as the least 
fixed point of the functional but, as we shall see in Example 3.6 and more 
formally in Proposition 3.16, this is not always appropriate. 
The formal definition of p. Following the approach of Appendix 
B we shall view Table 3.1 as defining a function: 
Q : (( Cicli.e x :Ex;; x Exp) -t {true,false}) 
-t (( Cicli.e x :Ex;; x Exp) -t {true,false}) 
As an example we have: 
Q(Q)(C,p, (let x = til in t~2)t) 
=Q(C,p,til ) 1\ Q(C,p,t~2) 1\ C(fl)~p(X) 1\ C(f2)~C(f) 
We frequently refer to Q as a functional because its argument and result are 
themselves functions. 
Now by inspecting Table 3.1 it is easy to verify that the functional Q con-
structed this way is a monotone function on the complete lattice 
(( Cicli.e x :Ex;; x Exp) -t {true, false}, !;) 
where the ordering !; is defined by: 
Hence Q has fixed points and we shall define "1=" coinductively: 

3.1 Abstract O-CFA Analysis 
149 
F is the greatest fixed point of Q 
The following example intuitively motivates the use of a coinductive (i.e. a 
greatest fixed point) definition as opposed to an inductive (i.e. a least fixed 
point) definition; a more formal explanation will be given in Subsection 3.2.4. 
Example 3.6 Consider the expression loop of Example 3.4 
(let g = (fun f x => (fl (fn y => y2)3)4)5 
in (g6 (fn z => z7)8)9)lO 
and the suggested analysis result (Clp, Pip). To show (Clp, Pip) F loop it is, 
according to the clause [let], sufficient to verify that 
(Clp, Pip) F (fun f x => (fl (fn y => y2)3)4)5 
(Clp, Pip) F (g6 (fn z => z7)8)9 
because Clp (5) ~ Pip(g) and Clp(9) ~ Clp(lO). The first clause follows from 
[fun] and for the second clause we use that CIP(6) = if} so it is, according to 
[app], sufficient to verify that 
because CIP(8) ~ Pip(x), Clp(4) ~ Clp(9) and f E Pip(f). The first two clauses 
now follow from [var] and [fn]. For the third clause we proceed as above and 
since Clp(l) = if} it is sufficient to verify 
(CIP ' Pip) F fl 
(Clp, Pip) F (fn y => y2)3 
(CIP ' Pip) F (fl (fn y => y2)3)4 
because Clp(3) ~ Plp(X), Clp(4) ~ CIP(4) and f E Pip(f). 
Again the first two clauses are straightforward but in the last clause we 
encounter a circularity: to verify (Clp, Pip) F (fl (fn y => y2)3)4 we have 
to verify (Clp, Pip) F (fl (fn y => y2)3)4! 
To solve this we use coinduction: basically this amounts to assuming that 
(Clp, Pip) F (fl (fn y => y2)3)4 holds at the "inner level" and proving that 
it also holds at the "outer level". This will give us the required proof. 
_ 

150 
3 Constraint Based Analysis 
Analogy. The use of coinduction and induction, or greatest and least 
fixed points, may be confusing at first. We therefore offer the following anal-
ogy. Imagine a company that is about to buy a large number of computers. 
To comply with national and international rules for commerce the deal must 
be offered in such a way that all vendors have a chance to make a bid. To 
this effect the company first makes a specification of the requirements to the 
computers (like the ability to execute certain benchmarks efficiently); how-
ever, the specification is necessarily loose meaning that there might be many 
ways of fulfilling the requirements. Among the bids made the company then 
chooses the bid believed to be best (in the sense of offering the required func-
tionalityas cheaply as possible or offering as much functionality as possible). 
Then suppose that when the actual computers are delivered the company 
suspects that they are not fulfilling the promises. It then starts arbitration 
in order to persuade the vendor that they have not fulfilled their obligations 
(and must take back the computers, or upgrade them or sell them at reduced 
price). In order for the company to be successful it must be able to use 
the details of the specification, and not any other information about what 
other vendors would have been able to provide, in order to prove that the 
computers delivered fail to fulfil one or more demands explicitly put forward 
in the specification. If the specification is too loose, like just demanding an 
Intel-based PC, there is no way that the inability to run Linux (or some other 
operating system) can be used against the vendor. -
Coming back to the 
analysis in Table 3.1 its primary purpose is to provide a specification of when 
a potential analysis result is acceptable. The specification takes the form of 
defining a relation 
1=: (C~e x &v x Exp) -t {true, false} 
and since the specification is intended to be loose it must be defined coin-
ductively: something only fails to live up to the specification if it can be 
demonstrated to follow from the clauses; this line of reasoning is in fact 
a fundamental ingredient in algebraic specification theory. As an example, 
suppose we were silly and merely defined 
(a,p) 1= P 
iff (a,p) 1= P 
then any solution (a, p) would in fact live up to the specification; it is a simple 
mathematical fact that only the coinductive interpretation of the definition of 
1= will live up to this demand (and in particular the inductive definition will 
not as it will not accept any solutions at all). Having clarified the meaning of 
the specification we can then look for the best solution (a, p): this is typically 
the least solution that satisfies the specification. So to summarise, the actual 
specification itself is defined by a greatest fixed point (namely coinductively) 
whereas algorithms for computing the intended solution are normally defined 
by least fixed points. 

3.2 Theoretical Properties 
151 
3.2 
Theoretical Properties 
In this section we shall investigate some more theoretical properties of the 
Control Flow Analysis, namely: 
â¢ semantic correctness, and 
â¢ the existence of least solutions. 
The semantic correctness result is important since it ensures that the infor-
mation from the analysis is indeed a safe description of what will happen 
during the evaluation of the program. The result about the existerice of least 
solutions ensures that all programs can be analysed and furthermore that 
there is a "best" or "most precise" analysis result. 
As in Section 2.2, the material of this section may be skimmed through on a 
first reading; however, we reiterate that it is frequently when conducting the 
correctness proof that the final and subtle errors in the analysis are found 
and corrected! 
3.2.1 
Structural Operational Semantics 
Configurations. We shall equip the language FUN with a Structural 
Operational Semantics. We shall choose an approach based on explicit envi-
ronments rather than substitutions because (as discussed in the Concluding 
Remarks) a substitution based semantics does not preserve the identity of 
functions (and hence abstract values) during evaluation. So a function defini-
tion will evaluate to a closure containing the syntax of the function definition 
together with an environment mapping its free variables to their values. For 
this we introduce the following categories 
defined by: 
v 
E Val 
values 
p 
E Env environments 
v 
p .. -
c 1 close t in p 
[ 11 p[x I-? v1 
A function abstraction fn x => eo will then evaluate to a closure, written 
close (fn x => eo) in p; similarly, the abstraction fun f x => eo will evaluate 
to close (fun f x => eo) in p. Our definitions do not demand that all terms t 
occurring in some close t in p in the semantics will be of the form fn x => eo 
or fun f x => eo; however, it will be the case for the semantics presented 
below. 

152 
3 Constraint Based Analysis 
As in Section 2.5 we shall need intermediate configurations to handle the 
binding of local variables. We therefore introduce syntactic categories for 
intermediate expressions and intermediate terms 
ie 
E IExp 
intermediate expressions 
it E ITerm intermediate terms 
that extend the syntax of expressions and terms as follows: 
ie .. _ 
itl 
it .. -
c' x , fn x => eo , fun f x => eo , iel ie2 
if ieo then el else e2 'let x = iel in e2 , iel op ie2 
bind p in ie , close t in p 
The role of the bind-construct is much as in Section 2.5: bind p in ie records 
that the intermediate expression ie has to be evaluated in an environment 
with the bindings p. (The sequence of environments of nested bind-constructs 
may be viewed as an encoding of the frames of a run-time stacle) So while 
close t in p is a fully evaluated value this is not the case for bind p in ie. 
We shall need these intermediate terms because we define a small step se-
mantics; only the close-constructs will be needed for a big step variant of 
the semantics. 
Alternatively, the definitions of Val and Env could have been written Val = 
Const + (Term x Env) and Env = Var -7fin Val (for a finite mapping) but 
it is important to stress that all entities are defined mutually recursively in 
the manner of context-free grammars. Formally, we defined an environment p 
as a list but nevertheless we shall feel free to regard it as a finite mapping: we 
write dom{p) for {x , p contains [x I-t .. Â·n; we write p{x) = v if x E dom{p) 
and the rightmost occurrence of [x I-t ... ] in p is [x I-t v], and we write p' X 
for the environment obtained from p by removing all occurrences of [x I-t ... ] 
with x rt. X. For the sake ofreadability we shall write [x I-t v] for [][x I-t v]. 
We have been very deliberate in when to use intermediate expressions and 
when to use expressions although it is evident that all expressions are also 
intermediate expressions. Since we do not evaluate the body of a function 
before it is applied we continue to let the body be an expression rather 
than an intermediate expression. Similar remarks apply to the branches of 
the conditional and the body of the local definitions. Note that although 
an environment only records the terms fn x => eo and fun f x => eo in the 
closures bound into it, we do not lose the identity of the function abstractions 
as eo will be of the form t~O and hence .eo may be used as the "unique" 
identification of the function abstraction. 
Thansitions. We are now ready to define the transition rules of the Struc-
tural Operational Semantics by means of judgements of the form 
p f- iel -7 ie2 

3.2 Theoretical Properties 
[var] 
[In] 
[Iun] 
P f- xÂ£ -+ vÂ£ if x E dom(p) and v = p(x) 
P f- (fn x => eo)Â£ -+ (close (fn x => eo) in Po)Â£ 
where Po = P I FV(fn x => eo) 
p f- (fun I x => eo)Â£ -+ (close (fun I x => eo) in po)Â£ 
where Po = P I FV(fun I x => eo) 
P f- (iel ie2)Â£ -+ (iei ie2)l 
p f- ie2 -+ ie~ 
p f- (vil ie2)Â£ -+ (vil ie~)Â£ 
p f- ((close (fn x => ed in pd l v~2)l -+ 
(bind pdx I-t V2] in edl 
p f- ((close (fun I x => el) in pd l v~2)f -+ 
(bind P2[X I-t V2] in ed 
153 
where P2 = pdl I-t close (fun I x => el) in Pl] 
[bindd 
p f- (bind Pl in iel)i -+ (bind Pl in iei)Â£ 
p f- (bind Pl in vil)f -+ vi 
Table 3.2: The Structural Operational Semantics of FUN (part 1). 
given by the axioms and inference rules of Tables 3.2 and 3.3; they are ex-
plained below. The idea is that one step of computation of the expression 
iel in the environment P will transform it into ie2. 
The value of a variable is obtained from the environment as expressed by the 
axiom [var]. The axioms [fn] and [Iun] construct the appropriate closures; 
they restrict the environment p to the free variables of the abstraction. Note 
that in [fun] it is only recorded that we have a recursively defined function; 
the unfolding of the recursion will not happen until it is called. 
The clauses for application shows that the semantics is a call-by-value seman-
tics: In an application we first evaluate the operator in a number of steps 
using the rule [apPl] and then we evaluate the operand in a number of steps 
using the rule [apP2]. The next stage is to use one of the rules [apPjn] or 
[appjun] to bind the actual parameter to the formal parameter and, in the 
case of [apPjun] , to unfold the recursive function so that subsequent recursive 
calls will be bound correctly. We shall use a bind-construct to contain the 
body of the function together with the appropriate environment. Finally, we 

154 
[ifd 
[lei:!] 
3 Constraint Based Analysis 
P f- ieo -t ie~ 
P f- (if ieo then el else e2)l -t (if ie~ then el else e2)i 
P f- (if trueio then tfl else t~2)i -t tf 
P f- (if falseio then tfl else t~2)i -t t~ 
P f- iel -t ie~ 
p f- (let x = iel in e2)l -t (let x = ie~ in e2)i 
p f- (let x = vil in e2)i -t (bind po[x 1--+ v] in e2)i 
where Po = P I FV(e2) 
if v = VI op V2 
Table 3.3: The Structural Operational Semantics of FUN (part 2). 
evaluate the bind-construct using rule [bindl ] a number of times, and we get 
the result of the application by using rule [bind:!]. The interplay between 
these rules is illustrated by the following example. 
Example 3.7 Consider the expression ((fn x => xl)2 (fn y => y3)4)5 of 
Example 3.1. It has the following derivation sequence (explained below): 
[] 
f-
((fn x => xl)2 (fn y => y3)4)5 
-t 
(( close (fn x => xl) in [])2 (fn y => y3)4)5 
-t 
((close (fn x => xl) in [])2 (close (fn y => y3) in [])4)5 
-t 
(bind [x 1--+ (close (fn y => y3) in [])] in xl )5 
-t 
(bind [x 1--+ (close (fn y => y3) in [ ])] in 
(close (fn y => y3) in [ ])1)5 
-t 
(close (fn y => y3) in [ ])5 
First [apPl] and [In] are used to evaluate the operator, then [apP2] and [In] 
are used to evaluate the operand and [apPfn] introduces the bind-construct 
containing the local environment [x 1--+ (close (fn y => y3) in [ ])] needed 
to evaluate its body. So Xl is evaluated using [bindl ] and [var] , and finally 
[bind:!] is used to get rid of the local environment. 
_ 

3.2 Theoretical Properties 
155 
The semantics of the conditional is the usual one: first the condition is eval-
uated in a number of steps using rule [ift] and then the appropriate branch 
is selected by rules [ih] and [ih]. For the local definitions we first compute 
the value of the bound variable in a number of steps using rule [letl] and 
then we introduce a bind-construct using rule [le~] reflecting that the body 
of the let-construct has to be evaluated in an extended environment. The 
rules [bindl ] and [bin~] are now used to compute the result. For binary ex-
pressions we first evaluate the arguments using [OPI] and [OP2] and then the 
operation itself, denoted op, is performed using [OP3]' 
As in Chapter 2 the labels have no impact on the semantics but are merely 
carried along. It is important to note that the outermost label never changes 
while inner labels may disappear; see for example the rules [ih] and [bin~]. 
This is an important property of the semantics that is exploited by the O-CFA 
analysis. 
Example 3.8 Let us consider the expression, loop 
(let g = (fun f x => (il (in y => y2)3)4)5 
in (g6 (in z => z7)8)9)IO 
of Example 3.2 and see how the informal explanation of its semantics is 
captured in the formal semantics. First we introduce abbreviations for three 
closures: 
f 
idy 
idz 
= 
= 
= 
close (iun i x => (il (in y => y2)3)4) in [ ] 
close (in y => y2) in [ ] 
close (fn z => z7) in [] 
Then we have the following derivation sequence 
[] 
I-
loop 
-t 
(let g = f5 in (g6 (in z => z7)8)9)IO 
-t 
(bind [g t-t f] in (g6 (in z => z7)8)9)IO 
-t 
(bind [g t-t f] in (rB (in z => z7)8)9)IO 
-t 
(bind [g t-t f] in (f6 id~)9)IO 
-t 
(bind [g t-t f] in 
(bind [i t-t f][x t-t idz] in (il (in y => y2)3)4)9)IO 
-t * 
(bind [g t-t f] in 
(bind [i t-t f][x t-t idz] in 
(bind [i t-t f][x t-t idyl in (il (in y => y2)3)4)4)9)IO 
-t* 
showing that the program does indeed loop. 
â¢ 

156 
3 Constraint Based Analysis 
[bind] 
((, p) 1= (bind p in it~O)Â£ 
iff ((, p) 1= it~O 1\ ((Po) <; ((P) 1\ P n P 
[close] 
((, p) 1= (close to in p)Â£ 
iff {to} <; ((P) 1\ P n P 
Table 3.4: Abstract Control Flow Analysis for intermediate expressions. 
3.2.2 
Semantic Correctness 
We shall formulate semantic correctness of the Control Flow Analysis as a 
subject reduction result; this is an approach borrowed from type theory and 
merely says that an acceptable result of the analysis remains acceptable under 
evaluation. However, in order to do that we need to extend the analysis to 
intermediate expressions. 
Analysis of intermediate expressions. The clauses for the con-
structs bind p in ie and close to in p are given in Table 3.4; the remaining 
clauses are as in Table 3.1 (with the obvious replacements of expressions with 
intermediate expressions). 
The clause [bind] reflects that its body will be executed and hence whatever 
it evaluates to will also be a possible value for the construct. Additionally, it 
expresses that there is a certain relationship n between the local environment 
(of the semantics) and the abstract environment (of the analysis). The clause 
[close] is similar in spirit to the clauses for function abstraction: the term of 
the closure is a possible value of the construct. Additionally, there has to be 
a relationship n between the two environments. 
Correctness relation. The purpose of the global abstract environ-
ment, p, is to model all of the local environments arising during evaluation. 
We formalise this by defining the correctness relation 
n : (Env x ~) -t {true, false} 
and demanding that p n p for all local environments, p, occurring in the 
intermediate expressions. We then define: 
p n p iff dom(p) <; dom(p) 1\ Vx E dom(p) Vtx Vpx : 
(p(x) = close tx in Px) => (tx E p(x) 1\ Px n p) 
This clearly demands that the function abstraction, tx , in p(x) must be an 
element of p(x). It also shows that all local environments reachable from p, 
e.g. p"" must be modelled by p as well. Note that the relation n is well-
defined because each recursive call is performed on a local environment that 

3.2 Theoretical Properties 
157 
P 
I-
ie 
~ 
ie' 
~ 
ie" 
~ 
~ 
vi. 
In 
IF 
IF 
IF 
IF 
p 
(C,P) 
(C,P) 
(C,P) 
(C,P) 
Figure 3.3: Preservation of analysis result. 
is strictly smaller than that of the call itself; thus a simple proof by well-
founded induction (Appendix B) suffices for showing the well-definedness of 
n. 
Example 3.9 Suppose that: 
P = 
[x ~ close h in PI][Y ~ close t2 in P2l 
PI 
= [ 1 
P2 
= 
[x ~ close tg in pgl 
pg 
= [ 1 
Then P n P amounts to {tl, tg} ~ p(x) /\ {t2} ~ p(y). 
â¢ 
We shall sometimes find it helpful to split the definition of n into two com-
ponents. For this we make use of the auxiliary relation 
V: (Val x (~ x Vai)) ~ {true, false} 
and define V and n by mutual recursion: 
v V (p,v) 
iff 
Vt Vp: (v = close tin p) => (t E v /\ p n P) 
P n P iff 
dom(p) ~ dom(p) /\ "Ix E dom(p) : p(x) V (p, p(x)) 
Clearly the two definitions of n are equivalent. 
Correctness result. The correctness result is now expressed by: 
Theorem 3.10 
IT p n p and p I- ie ~ ie' then (C, P) F ie implies (C, P) Fie'. 
This is illustrated in Figure 3.3 for a terminating evaluation sequence p I-
ie ~ * vi; note that the result is analogous to that of Corollary 2.17 for the 
Live Variables Analysis in Chapter 2. 

158 
3 Constraint Based Analysis 
The intuitive content of the result is as follows: 
If there is a possible evaluation of the program such that the 
function at a call point evaluates to some abstraction, then this 
abstraction has to be in the set of possible abstractions computed 
by the analysis. 
To see this assume that P I- t'- -+ * (close to in po)l and that (C, p) 1= tt as 
well as p n p. Then Theorem 3.10 (and an immediate numerical induction) 
gives that (C, p) 1= (close to in Po)l. Now from the clause [close] of Table 
3.4 we get that to E C(l) as was claimed. It is worth noticing that if the 
program is closed, i.e. if it does not contain free variables, then p will be [ ] 
and the condition p n p is trivially fulfilled. 
Note that the theorem expresses that all acceptable analysis results remain 
acceptable under evaluation. One advantage of this is that we do not need to 
rely on the existence of a least or "best" solution (to be proved in Subsection 
3.2.3) in order to formulate the result. Indeed the result does not say that 
the "best" solution remains "best" - merely that it remains acceptable. More 
importantly, the result opens up the possibility that the efficient realisation 
of Sections 3.3 and 3.4 computes a more approximate solution than the least 
(perhaps using the techniques of Chapter 4). Finally, note that the formula-
tion of the theorem crucially depends on having defined the analysis for all 
intermediate expressions rather than just all ordinary expressions. 
We shall now turn to the proof of Theorem 3.10. We first state an important 
observation: 
Proof By cases on the clauses for "1=". 
â¢ 
We then prove Theorem 3.10: 
Proof We assume that p 'R. P and (C, p) 1= ie and prove (C, p) 1= ie' by induction 
on the structure of the inference tree for p I- ie -+ ie'. Most cases simply amount 
to inspecting the defining clause for (C, (i) 1= iej note that this method of proof 
applies to all fixed points of a recursive definition and in particular also to the 
greatest fixed point. We only give the proofs for some of the more interesting cases. 
The case [var]. Here pI- ie -+ ie' is: 
p I- Xl -+ vi because x E dom(p) and v = p(x) 
If v = c there is nothing to prove so suppose that v = close to in po. From p n p 
we get v V (p,{i(xÂ» and hence to E p(x) and po n p. From (C,P) 1= ie we get 
p(x) ~ C(l), and hence to E C(l). Since to E C(l) and Po n pwe have established 
(C, p) 1= ie'. 

3.2 Theoretical Properties 
The case Ifn]. Here p f- ie ~ ie' is: 
p f- (fn x => eo)l ~ (close (fn x => eo) in PO)l 
where po = p I FV(fn x => eo) 
159 
From (C,P) 1= ie we get (fn x => eo) E C(f)j from p R P it is immediate to get 
po n pj this then establishes (C, p) 1= ie'. 
The case [apPl]. Here p f- ie ~ ie' is: 
p f- (iel ie2)l ~ 
(ie~ ie2)l because p f- iel ~ 
ie~ 
The defining clauses of (C, p) 1= ie and (C, 'ji) 1= ie' are equal except that the former 
has (C,P) 1= iel where the latter has (C,'ji) 1= iei. From the induction hypothesis 
applied to 
we get (C, 'ji) 1= iei and the desired result then follows. 
The case [apPjn]. Here p f- ie ~ ie' is: 
p f- Â«close (fn x => t~O) in pd1 V~2)l ~ (bind PI [x t-+ V2] in t~O)l 
From (C,P) 1= ie we have (C,P) 1= (close (fn x => t~O) in pd1 which yields: 
Further we have (C, 'ji) 1= V~2 j in the case where V2 = c, it is immediate that 
and in the case where V2 = close t2 in P2 it follows from the definition of (C, 'ji) 1= 
V~2. Finally, the first universally quantified formula of the definition of (C, 'ji) 1= ie 
gives: 
(C, p) 1= t~O, C(f2) ~ p(x), and C(fo) ~ C(f) 
Now observe that V2 V (p,p(xÂ» since C(f2) ~ p(x) follows from the clause (apPjn). 
Since PI n P we now have 
and this establishes the desired (C, 'ji) 1= ie'. 
The case [bind2]. Here p f- ie ~ ie' is: 
p f- (bind PI in V~l)l ~ vf 
From (C, p) 1= ie we have (C, 'ji) 1= V~l as well as C(f1) ~ C(f) and the desired 
(C, p) 1= vf follows from Fact 3.11. 
This completes the proof. 
â¢ 

160 
3 Constraint Based Analysis 
Example 3.12 From Example 3.7 we have: 
[ 1 f- ((fn x => x1)2 (fn y => y3)4)5 ----+ * (close (fn y => y3) in [ ])5 
Next let (Ce,Pe) be as in Example 3.3. Clearly [ 1 R Pe and from Example 
3.5 we have: 
According to Theorem 3.10 we can now conclude: 
Using Table 3.4 it is easy to check that this is indeed the case. 
â¢ 
3.2.3 
Existence of Solutions 
Having defined the analysis in Table 3.1 it is natural to ask the following 
question: Does each expression e admit a Control Flow Analysis, i.e. does 
there exist (C, (i) such that (C, p) 1= e? We shall show that the answer to this 
question is yes. 
However, this does not exclude the possibility of having many different analy-
ses for the same expression so an additional question is: Does each expression 
e have a "least" Control Flow Analysis, i.e. does there exists (Co, Po) such 
that (Co, Po) 1= e and such that whenever (C, p) 1= e then (Co, Po) is "less 
than" (C, p)? Again, the answer will be yes. 
Here "least" is with respect to the partial order defined by: 
(C1,P1) ~ (C2,P2) 
iff 
(ve E Lab: C1(Â£) ~ C2 (Â£)) 1\ 
(Vx E Var : P1(X) ~ P2(X)) 
It will be the topic of Sections 3.3 and 3.4 (and Mini Project 3.1) to show that 
the least solution can be computed efficiently for all expressions. However, it 
may be instructive to give a general proof for the existence of least solutions 
also for intermediate expressions. To this end we recall the notion of a Moore 
family (see Appendix A and Exercise 2.7): 
A subset Y of a complete lattice L = (L,~) is a Moore family if 
and only if (n Y') E Y for all Y' ~ Y. 
This property is also called the model intersection property because whenever 
we take the "intersection" of a number of "models" we still get a "model". 

3.2 Theoretical Properties 
161 
Proposition 3.13 
For all ie E IExp the set {(e, p) I (e, P) F ie} is a Moore family. 
It is an immediate corollary that all intermediate expressions ie admit a 
Control Flow Analysis: Let Y' be the empty set; then ny' is an element of 
{(e, p) I (e, ji) F ie} showing that there exists at least one analysis of ie. 
It is also an immediate corollary that all intermediate expressions have a least 
Control Flow Analysis: Let Y' be the set {(e, p) I (e, ji) F ie}; then ny' is 
an element of {(e, ji) I (C, ji) F ie} so it will also be an analysis of ie. Clearly 
ny' ~ (e, ji) for all other analyses (e, p) of ie so it is the least analysis result. 
In preparation for the proof of Proposition 3.13 we shall first establish an 
auxiliary result for n and V: 
Lemma 3.14 
(i) 
For all P E Env the set {ji I p n p} is a Moore family. 
(ii) 
For all v E Val the set {(p, iJ) I v V (ji, iJ)} is a Moore family. 
_ 
Proof To prove (i) we proceed by well-founded induction on p (which is also the 
manner in which the existence of the predicate was proved). Now assume that 
Vi E I : p n (i; 
for some index set I and let us show that p n (niPi). For this consider x, t"" and 
p", such that: 
p(X) = close t", in p", 
We then know 
Vi E I : t", E pi(X) II p", n Pi 
and using the induction hypothesis it follows that 
t", E (lliPi)(X) II p", n (niPi) 
(taking care when 1= 0). 
To prove (ii) we simply expand the definition of V and note that the result then 
follows from (i). 
â¢ 
We now prove Proposition 3.13 using coinduction (see Appendix B): 
Proof The ternary relation p of Tables 3.1 and 3.4 is the greatest fixed point of 
a function Q as explained in Section 3.1. Now assume that 
Vi E I: (Ci, Pi) pie 
and let us prove that ni(Ci, Pi) pie. We shall proceed by coinduction (see Ap-
pendix B) so we start by defining the ternary relation Q' by: 
(C',p)Q'ie' iff (C',p)=ni(Ci,pi) II ViEI:(Ci,pi)pie' 

162 
3 Constraint Based Analysis 
It is then immediate that we have: 
The coinduction proof principle requires that we prove 
Q' 1; Q(Q') 
and this amounts to assuming (C',?) Q' ie' and proving that (C',?) (Q(Q')) ie'. 
So let us assume that 
and let us show that: 
ni(Ci,Pi) (Q(Q')) ie' 
For this we consider each of the clauses for ie' in turn. 
Here we shall only deal with the more complicated choice ie' = (it~l it~2)L. From 
Vi E I: (Ci,Pi) 1= (it~l it~2)l 
we get Vi E I: (Ci,Pi) 1= it~l and hence: 
ni(Ci, Pi) Q' it~l 
Similarly we get: 
ni(Ci,Pi) Q' it~2 
Next consider (fn x => t~O) E ni(Ci(l!)) and let us prove that: 
For all i E I we have that (Ci, Pi) 1= ie' and since (fn x => t~O) E Ci (l d we have 
and this then gives (3.1) as desired (taking care when I = 0). 
The case of 
(fun f x => t~O) E ni(Ci(lI)) is similar. This completes the proof. 
_ 
Example 3.15 Let us return to Example 3.5 and consider the following 
potential analysis results for ((fn x => XI)2 (fn y => y3)4)5: 
(Ce, Pe) 
(C~,~) 
(C~,~) 
1 
{fn y => yil} 
{fn y => yil} 
{fn y => yil} 
2 
{fn x => xl} 
{fn x => xl} 
{fn x => xl} 
3 
0 
{fn x => xl} 
{fn y => y3} 
4 
{fn y => y3} 
{fn y => y3} 
{fn y => y3} 
5 
{fn y => y3} 
{fn y => y3} 
{fn y => y3} 
x 
{fn y => y3} 
{fn y => y3} 
{fn y => y3} 
Y 
0 
{fn x => xl} 
{fn y => y3} 

3.2 Theoretical Properties 
It is straightforward to verify that 
(C~,~) F ((fn x => xl)2 (fn y => y3)4)5 
(C~,P::) F ((fn x => xl)2 (fn y => y3)4)5 
Now Proposition 3.13 ensures that also: 
(C~ n C~, ~ n P::) F ((fn x => xl)2 (fn y => y3)4)5 
163 
Neither (C~,~) nor (C~, P::) is a least solution. Their "intersection" (C~ n 
C~, ~ n P::) is smaller and equals (Ce, fie) which turns out to be the least 
analysis result for the expression. 
_ 
3.2.4 
Coinduction versus Induction 
One of the important aspects of the development of the abstract Control 
Flow Analysis in Table 3.1 is the coinductive definition of the acceptability 
relation: 
F= as the greatest fixed point of a functional Q 
An alternative might be an inductive definition of an acceptability relation: 
F' as the least fixed point of the functional Q. 
However, in Example 3.6 we argued that this might be inappropriate and 
here we are going to demonstrate that an important part of the development 
of the previous subsection actually fails for the least fixed point (F') of Q. 
Proposition 3.16 
There exists e* E Exp such that {(C, p) I (C, P) F' e*} is not a 
Moore family. 
Proof (sketch) This proof is rather demanding and is best omitted on a first 
reading. To make the proof tractable we consider 
and take: 
e* 
= 
t~ 
t* 
= 
(fn x => (xl xl)l)l (fn x => (xl xl)l)l 
Labe 
= 
{Â£} 
Yare 
= 
{x} 
Terltle 
= 
{t*,fn x => (xl xl)t,xl xt,x} 
IExPe 
= 
{tl It E Terltle} 
Val.. 
= 
P( {fn x => (xl xl)l}) = {0, {fn x => (xl xl)l}} 

164 
3 Constraint Based Analysis 
This is in line with Exercise 3.2 (and the development of Subsection 3.3.2) and as 
we shall see the proof of Proposition 3.13 is not invalidated. 
Next let Q be the functional defined by Table 3.1 and let Q be in the domain of Q. 
The condition 
Q = Q(Q) 
is equivalent to: 
By considering the four possibilities of t E TerIIle this is then equivalent to the 
conjunction ofthe following four conditions (where (C, p) is universally quantified): 
(C, P) Q (in x => (xl xl)l)l iff 
(C, P) Q (xl xl)l iff 
(C, p) Q t~ iff 
{in x => (Xl Xl)l} ~ C(l) 
~ 
l 
(C,P)Qx /\ 
C(I!) =1=0 => 
Â«C,P) Q (xl xl)l/\ 
C(I!) ~ p(xÂ» 
(C, P) Q (in x => (xl xl)l)l /\ 
C(I!) =1= 0 => 
((C, P) Q (xl xl)l /\ 
C(I!) ~ p(xÂ» 
Here we have used that C(I!) =1= 0 implies that C(I!) = {in x => (xl xl)l} as follows 
from the definition of VaL, in the beginning of this proof. 
The conjunction of the above four conditions implies the conjunction of the following 
four conditions: 
(C,P)Qxl 
iff 
pcx)~C(I!) 
(C,P) Q (in x => (xl xl)l)l iff {in x => (xl xl)l} ~ C(I!) 
(C, P) Q (Xl Xl)l 
iff p(x) ~ C(I!) /\ 
(C(I!) =1= 0 => (C, P) Q (xl xl)l) /\ 
C(I!) ~ p(x) 
(C,P)Qt~ iff 
{inx=>(xlxl)l}~C(I!) /\ 
(C, P) Q (Xl xl)l /\ 
C(I!) ~ p(x) 
This implication can be reversed and this shows that also the conjunct of the above 
four conditions is equivalent to Q = Q(Q). 
Using that p(x) can only be 0 or {in x => (xl xl)l}, and similarly for C(I!), the 
above four conditions are equivalent to the following: 

3.2 Theoretical Properties 
165 
(C,P) Q (fn x => (xL xl)L)L 
iff {fn x => (xL XL)L} = C(l) 
(C, P) Q (xL xL)l 
iff p(x) = C(l) 1\ 
(C(l) i= 0 '* (C, P) Q (xl xl)L) 
(C, P) Q t~ 
iff {fn x => (xl xln = C(l) = p(x) 1\ 
(C, P) Q (xl xL)l 
It follows that the conjunct of the above four conditions is once more equivalent to 
Q = Q(Q). 
The crucial case in the definition of (C, P) Q e is for e = (xL xl)L as this determines 
the truth or falsity of all other cases. We shall now try to get a handle on the 
candidates Ql,Â·Â·Â·, Qn for satisfying Qi = Q(Qi). Concentrating on the condition 
for (xl xl)l it follows that (C, Ii> Qi (XL xL)l must demand that C(l) = p(x). Since 
each of C(l) and p(x) can only be {fn x => (xl xL)L} or 0 there are at most the 
following four candidates for Qi: 
(C, P) Ql (xL xl)l 
iff C(l) = p(x) 
(C, P) Q2 (xL xL)l 
iff C(l) = p(x) = 0 
(C, P) Q3 (xL xL)l 
iff C(l) = p(x) i= 0 
(C, P) Q4 (xl xL)L 
iff 
false 
Verifying the condition 
V(C, P) : 
(e, Ii> Qi (x x) 
( 
~ 
l 
l l 
for i E {I, 2,3, 4} it follows that Ql and Q2 satisfy the condition whereas Q3 and 
Q4 do not. 
It is now straightforward to verify also the remaining three conditions and it follows 
that: 
Qi = Q(Qi) for i = 1,2 
This means that Ql equals p: (the greatest fixed point of Q) and that Q2 equals 
p:' (the least fixed point of Q). One can then calculate that 
(C, P) Ql t~ iff C(l) = p(x) i= 0 
(C, P) Q2 t~ iff 
false 
and this shows that 
which is a singleton set and in fact a Moore family, whereas 
which cannot be a Moore family (since a Moore family is never empty). This 
completes the proof. 
_ 

166 
3 Constraint Based Analysis 
3.3 
Syntax Directed O-CFA Analysis 
We shall now show how to obtain efficient realisations of O-CFA analyses. So 
assume throughout this section that e* E Exp is the expression of interest 
and that we want to find a "good" solution (C, p) satisfying (C, p) F e*. 
This entails finding a solution that is as small as possible with respect to the 
partial order ~ defined in Section 3.2 by: 
(C 1,pd ~ (C2,P2) iff (Vi: C1(Â£) ~ C2 (Â£)) 1\ ('Ix: PI (x) ~ P2(X)) 
Proposition 3.13 shows that a least solution does exist; however, the algo-
rithm that is implicit in the proof does not have tractable (Le. polynomial) 
complexity: it involves enumerating all candidate solutions, determining if 
they are indeed solutions, and if so taking the greatest lower bound with 
respect to the others found so far. 
An alternative approach is somehow to obtain a finite set of constraints, say 
of the form lhs ~ rhs (where lhs and rhs are much as described in Section 
3.1), and then take the least solution to this system of constraints. The 
most obvious method is to expand the formula (C, p) F e* by unfolding 
all "recursive calls", using memoisation to keep track of all the expansions 
that have been performed so far, and stopping the expansion whenever a 
previously expanded call is re-encountered. 
Three phases. We shall take a more direct route motivated by the above 
considerations; it has three phases: 
(i) The specification of Table 3.1 is reformulated in a syntax directed man-
ner (Subsection 3.3.1). 
(ii) The syntax directed specification is turned into an algorithm for con-
structing a finite set of constraints (Subsection 3.4.1). 
(iii) The least solution of this set of constraints is computed (Subsection 
3.4.2). 
This is indeed a common phenomenon: a specification "FA" is reformulated 
into a specification "FB" ensuring that 
so that "FB" is a safe approximation to "FA" and in particular the best 
(Le. least) solution to "FB e/' is also a solution to "FA e/'. This also 
ensures that all solutions to "FB" are semantically correct (assuming that 
this has already been established for all solutions to "FA"); however, we do 
not claim that a subject reduction result holds for FB (even though it has 
been established for FA). 

3.3 Syntax Directed O-CFA Analysis 
167 
If additionally 
(C,p) FA e* :::} (C,p) FE e* 
then we can be assured that no solutions are lost and hence the best (Le. least) 
solution to "FE e/' will also be the best (Le. least) solution to "PA e*". As 
we shall see, it may be necessary to restrict attention to only solutions (C, p) 
satisfying some additional properties (e.g. that only program fragments of e* 
appear in the range of C and p). 
3.3.1 
Syntax Directed Specification 
In reformulating the specification of "P e/' into a more computationally 
oriented specification "Fs e*" we shall ensure that each function body is 
analysed at most once rather than each time the function could be applied. 
One way to achieve this is to analyse each function body exactly once as is 
done in the syntax directed O-CPA analysis of Table 3.5; a better alterna-
tive would be to analyse only reachable function bodies and we refer to Mini 
Project 3.1 for how to achieve this. In Table 3.5 each function body is there-
fore analysed in the relevant clause for function abstraction rather than in 
the clause for function application; thus we now risk analysing unreachable 
program fragments. 
The formal definition of Fs. Since semantic correctness was dealt 
with in Section 3.2 there is no longer any need to consider intermediate 
expressions and consequently our specification of 
(C, p) Ps e 
in Table 3.5 considers ordinary expressions only. We shall take "ps" to be 
the largest relation that satisfies the specification; however, given the syntax 
directed nature of the specification there is in fact only one relation that 
satisfies the specification (see Exercise 3.9). Hence it would be technically 
correct, but intuitively misleading, to claim that we take the least relation 
that satisfies the specification. In other words, whether or not ps is defined 
inductively or coinductively, the same relation is defined; this is a common 
phenomenon whenever the clauses are defined in a syntax directed manner. 
Example 3.17 Consider the expression loop 
(let g = (fun f x => (f1 (fn y => y2)3)4)5 
in (g6 (fn z => z7)8)9)10 
of Example 3.4. We shall verify that (Clp, Pip) ps loop where Clp and Pip are 
as in Example 3.4. Using the clause [let], it is sufficient to show 
(CIP ' Pip) Fs 
(fun f x => (f1 (fn y => y2)3)4)5 
(3.2) 
(Clp, Pip) Fs 
(g6 (fn z => z7)8)9 
(3.3) 

168 
[con] 
[var] 
[fn] 
[fun] 
[app] 
[ if] 
[let] 
[op] 
(c, p) 1=8 c'- always 
(C,P) 1=8 xl iff p(x) ~ C(f) 
(C, P) 1=8 (fn x => eo)l 
iff {fn x => eo} ~ C(f) 1\ 
(C, p) 1=8 eo 
(C, P) 1=8 (fun f x => eo)l 
3 Constraint Based Analysis 
iff {fun f x => eo} ~ C(f) 1\ 
(C,P) 1=8 eo 1\ {fun f x => eo} ~ P(f) 
(C, P) 1=8 (t~l t~2)l 
iff (C, p) 1=8 tfl 1\ (C, p) 1=8 t~2 1\ 
('v'(fn x => t&O) E C(f1) : 
C(f2) ~ p(x) 1\ C(fo) ~ C(f)) 1\ 
('v'(fun f x => t&O) E C(f1) : 
C(f2) ~ p(x) 1\ C(fo) ~ C(f)) 
(C,P) 1=8 (if t&o then t~l else t~2)l 
iff (C, p) 1=8 t&O 1\ 
(C, p) 1=8 t~l 1\ (C, p) 1=8 t~2 1\ 
C(f1) ~ C(f) 1\ C(f2) ~ C(f) 
(C, p) 1=8 (let x = t~l in t~2)l 
iff (C, p) 1=8 t~l 1\ (C, p) 1=8 t~2 1\ 
C(f1) ~ p(x) 1\ C(f2) ~ C(f) 
(C,P) 1=8 (t~l op t~2)l iff (C,P) 1=8 t~l 1\ (C,P) 1=8 t~2 
Table 3.5: Syntax directed Control Flow Analysis. 
since we have Clp(5) ~ /Jip(g) and CIP(9) ~ Clp(lO). To show (3.2) we use the 
clause [fun] and it is sufficient to show 
since f E Clp(5) and f E /Jip(f). Now Clp(l) = {f} so, according to clause [app] 
this follows from 

3.3 Syntax Directed O-CFA Analysis 
169 
since Clp(3) ~ Pip(x) and Clp(4) ~ Clp(4). The first clause follows from [var] 
since Pip(f) ~ Clp(l) and for the last clause we observe that idy E Clp(3) and 
~ 
2 
~ 
(Clp, Pip) Fs y as follows from Pip(y) ~ Clp(2). 
To show (3.3) we observe that Clp(6) = {f} so using [app] it is sufficient to 
show 
~ 
6 
(Clp, Pip) Fs g 
~ 
~ 
78 
(Clp, Pip) Fs (fn z=> z ) 
since CIP(B) ~ Pip(x) and Clp(4) ~ Clp(B). This is straightforward except for 
the last clause where we observe that idz E CIP(8) and (CIP'Pip) Fs z7 as 
follows from Pip(y) ~ Clp(7). 
Note that because the analysis is syntax directed we have had no need for 
coinduction, unlike what was the case in Example 3.6. 
â¢ 
3.3.2 
Preservation of Solutions 
The specification of the analysis in Table 3.5 uses potentially infinite value 
spaces although this is not really necessary (as Exercise 3.2 demonstrates 
for Table 3.1). We can easily restrict ourselves to entities occurring in the 
original expression and this forms the basis for relating the results of the 
analysis of Table 3.5 to those of the analysis of Table 3.1. 
So let Lab* ~ Lab be the finite set of labels occurring in the program e* of 
interest, let Var* ~ Var be the finite set of variables occurrin$ in e* and let 
Term* be the finite set of subterms occurring in e*. Define (C;, pn by: 
C;(l) ={ 0 
if l i Lab* 
Term* 
if l E Lab* 
p;(x) ={ 0 
if xi Var* 
Term* 
if x E Var* 
Then the claim 
~ 
~T T 
(C, p) !;;;; (C* , p* ) 
intuitively expresses that (C, p) is concerned only with subterms occurring in 
the expression e*; obviously, we are primarily interested in analysis results 
with that property. Actually, this condition can be "reformulated" as the 
technically more manageable 
(C, p) E CiclJ.e* x :Er;V* 
where we define C~e* = Lab* --+ Vai*,:Er;V* = Var* --+ Vai* and Vai* = 
P(Ter~). 

170 
3 Constraint Based Analysis 
We can now show that all the solutions to "1=8 e/' that are "less than" 
(ey, pn are solutions to "1= e*" as well: 
Proposition 3.18 
-
................T 
T 
""" 
If (e, (J) 1=8 e* and (e, (J) !; (e* ,p*) then (e, (J) 1= e*. 
Proof Assume that (C, p) Fs e* and that (C, p) 1; (c"I, PI). Furthermore let 
Exp* be the set of expressions occurring in e* and note that 
Ve E Exp* : (C,P) Fs e 
(3.4) 
is an immediate consequence of the syntax directed nature of the definition of 1= â¢. 
To show that (C, p) 1= e* we proceed by coinduction. We know that "1=" is defined 
coinductively by the specification of Table 3.1, i.e. "1= = gtp(Q)" where Q is the 
function (implicitly) defined by Table 3.1. Similarly, we know that "1=. = gtp(Q.)" 
where Q. is the function (implicitly) defined by Table 3.5. 
Next write (C',?) 1=* e' for (C',?) = (C,P) A e' E Exp*. It now suffices to show 
(3.5) 
because then "(1=. n 1=*) 
~ Q(I=. n 1=*)" follows and hence by coinduction 
"(1=. n 1=*) ~ 1=" and since (C, P) 1=. e* as well as (C, P) 1=* e* we then have the 
required (C, p) 1= e*. 
The proof of (3.5) amounts to a comparison of the right hand sides of Table 3.5 
and Table 3.1: for each clause we shall assume that the right hand side of Table 
3.5 holds for (C, p, e) and that e E Exp* and we shall show that the corresponding 
right hand side of Table 3.1 holds when all occurrences of "1=" are replaced by 
"1=. n 1=*". 
The clauses [con], [var], [if], [let] and fop] are trivial as the right hand sides of Tables 
3.5 and 3.1 are similar and the subterms will all be in Exp*. The clauses (fn] and 
[fun] are straightforward as the right hand sides of Table 3.5 imply the right-hand 
sides of Table 3.1. Finally, we consider the clause lapp]. For (fn x => t~O) E C(Â£I) 
we need to show that (C, P) 1=. t~O; but since (C, P) 1; (c"I, p"I) this follows from 
(3.4). For (fun f x => t~O) E C(Â£I) we need to show that (C, P) 1=. t~O and that 
(fun f x => t~O) E (i(f); the first follows from (3.4) and the second is an immediate 
~ 
l 
l 
consequence of (C, P) 1=. (fun f x => toO) (for some Â£) that again follows from 
(3.4). 
â¢ 
We show an analogue of Proposition 3.13 for the syntax directed analysis: 
Proposition 3.19 
{(e, (J) E C'iclte* x EfnV * I (e, (J) 1= s e*} is a Moore family. 
This result has as immediate corollaries that: 

3.4 Constraint Based O-CFA Analysis 
171 
â¢ each expression e* has a Control Flow Analysis that is "less than" 
~T T 
(c* ,p*), and 
â¢ each eX.l?ression e* has a "least" Control Flow Analysis that is "less 
than" (CI,pJ:). 
This means that the properties obtained for the analysis of Table 3.1 in 
Subsection 3.2.3 also hold for the analysis of Table 3.5 with the additional 
restriction on the range of the analysis functions. In particular, any analysis 
result that is acceptable with respect to Table 3.5 (and properly restricted to 
C~e* x :Eti.V *) is also an acceptable analysis result with respect to Table 
3.1. The converse relationship is studied in Exercise 3.11 and Mini Project 
3.1. 
Proof We shall write (C~, pOI) also for the greatest element of C-;cl!e* x :Er;v*. 
It is immediate to show that 
for all sub expressions e of e* by means of structural induction on e. This establishes 
( a) and (b) also for e = e*. Next consider some 
a~ote th~ one can write Y = {(Ci,pi) liE {I,Â·Â·Â· ,n}} for some n 2': 0 since 
Cache* x Env * is finite. That 
ny E {(C,P) E C-;cl!e* x:Er;v* I (C,P) 1=. e*} 
then follows from (a) and (b) because ny = (C~, p~) n (C1 , PI) n ... n (Cn, Pn) .â¢ 
3.4 
Constraint Based O-CFA Analysis 
'Ye are now read)'" to consider efficient ways of finding the least solution 
(C, p) such that (C, j5) 1=8 e*. To do so we first construct a finite set C*[e*] 
of constraints and conditional constraints of the form 
Ihs ~ rhs 
{t} ~ rhs' => Ihs ~ rhs 
(3.6) 
(3.7) 
where rhs is of the form C(e) or r(x), and Ihs is ofthe form C(e), r(x), or {t}, 
and all occurrences of t are of the form fn x => eo or fun f x => eo. To 
simplify the technical development we shall read (3.7) as 
({ t} ~ rhs' => Ihs) ~ rhs 
and we shall write Is for Ihs as well as {t} ~ rhs' => Ihs. 

172 
3 Constraint Based Analysis 
[con] 
C*[d] = 0 
[var] 
C*[xt] = {rex) ~ C(in 
[In] 
C*[(fn x => eo)t] = {{fn x => eo} ~ C(in 
u C*[eo] 
[fun] 
C*[(fun f x => eo)t] = {{fun f x => eo} ~ C(in 
U C*[eo] U {{fun f x => eo} ~ rUn 
[app] 
c*[(til t~2 )l] = c*[til] U C*[t~2] 
U {{t} ~ C(it} => C(i2) ~ rex) 
I t = (fn x => t~O) E Term*} 
U {{t} ~ C(i1) => C(io) ~ C(i) 
I t = (fn x => t~O) E Ter~} 
U {{t} ~ C(it} => C(i2) ~ rex) 
I t = (fun f x => t~O) E Ter~} 
U {{t} ~ C(it} => C(io) ~ C(i) 
It = (fun f x => t~O) E Ter~} 
[if] 
C*[(if t~O then til else t~2)~] = C*[t~O] U c*[ti1 ] U C*[t~2] 
U {C(i1) ~ C(in 
U{C(i2) ~ C(in 
[let] 
C*[(let x = til in t~2)t] = c*[til] UC*[t~2] 
U {C(i1) ~ r(xn U {C(i2) ~ C(in 
[op] 
c*[(til op t~2)l] = c*[til] U C*[t~2] 
Table 3.6: Constraint based Control Flow Analysis. 
fuformally, the constraints are obtained by expanding the clauses defining 
(e, P> Fs e* into a finite set of constraints of the above form and then letting 
C*[e*] be the set of individual conjuncts. One caveat is that all occurrences 
of "e" are changed into "e" and that all occurrences of "fJ' are changed into 
"r" to avoid confusion: e(i) will be a set of terms whereas C(i) is pure syntax 
and similarly for (.i(x) and rex). 
Formally, the constraint based O-CFA analysis is defined by the function C* 
of Table 3.6: it makes use of the set Ter~ of subterms occurring in the 
expression e* in order to generate only a finite number of constraints in the 
clause for application; this is justified by Propositions 3.18 and 3.19. 
If the size of the expression e* is n then it might seem that there could be 
O(n2 ) constraints of the form (3.6) and O(n4) constraints of the form (3.7). 

3.4 Constraint Based O-CFA Analysis 
173 
However, inspection of the definition of C* ensures that at most O(n) con-
straints of the form (3.6) and 0(n2 ) constraints of the form (3.7) are ever 
generated: each of the O(n) constituents only generate 0(1) constraints of 
the form (3.6) and O(n) constraints of the form (3.7). 
Example 3.20 Consider the expression 
of Example 3.7. We generate the following set of constraints 
C*[((fn x => xl)2 (fn y => y3)4)5] = 
{{fn x => Xl} <;;; C(2), 
rex) <;;; C(1), 
{fn y => y3} <;;; C(4), 
r(y) <;;; C(3), 
{fn x => xl} <;;; C(2) * C(4) <;;; rex), 
{fn x => xl} <;;; C(2) * C(1) <;;; C(5), 
{fn y => y3} <;;; C(2) * C(4) <;;; r(y), 
{fn y => y3} <;;; C(2) * C(3) <;;; C(5) } 
where we use that fn x => xl and fn y => y3 are the only abstractions in 
~~. 
. 
3.4.1 
Preservation of Solutions 
~ 
It is important to stress that while (C, ji) 1=8 e* is a logical formula, C*[e*] is 
a set of syntactic entities. To give meaning to the syntax we first translate 
the "C" and "r" symbols into the sets "e" and "fl': 
(e, P)[CCl)] = eel) 
(e, p)[r(x)] = p(x) 
To deal with the possible forms of Is we additionally take: 
(e,p)[{tH = it} 
(e,p)[{t}<;;;rhs'*lhs] = {~e,p)[lhS] if{t} <;;; (e,ji)[rhs'] 
otherwise 
Next we define a satisfaction relation (e, p) I=e (Is <;;; rhs) on the individual 
constraints: 
(e, ji) I=e (ls <;;; rhs) 
iff (e, P)[ls] <;;; (C, p)[rhs] 

174 
3 Constraint Based Analysis 
This definition can be lifted to a set C of constraints by: 
(C,p) Fe C iff V(ls ~ rhs) E C: (C,p) Fe (ls ~ rhs) 
We then have the following result showing that all solutions to the set C*[e*] 
of constraints also satisfy the syntax directed specification of the Control 
Flow Analysis and vice versa: 
Proposition 3.21 
~ 
~T T 
If (C, p) !;;; (C* ,p* ) then 
(C, p) F8 e* if and only if (C, p) Fe C*[e*] 
Thus the least solution (C, p) to (C, p) F8 e* equals the least solution to 
(C, p) Fe C*[e*]. 
Proof A simple structural induction on e shows that 
for all sub expressions e of e*; in the case of the function application (tfl t~2)l the 
assumption (C, p) [;;; (C;, PI) is used to ensure that C(Â£1) ~ Term*. 
_ 
3.4.2 
Solving the Constraints 
We shall present two approaches to solving the set of constraints C*[e*]. First 
we shall show that finding the least solution to C*[e*] is equivalent to finding 
the least fixed point of a certain function; straightforward techniques allow 
us to compute the least fixed point in time O(n5 ) when the size of the ex-
pression e* is n. Improvements upon this are possible, but to obtain the best 
known result we shall consider a graph representation of the problem; this 
will give us a O(n3 ) algorithm. This is indeed a common phenomenon in 
program analysis: syntax directed specifications are appropriate for correct-
ness considerations but often they need to be "massaged" in order to obtain 
efficient implementations. 
Fixed point formulation. To show that finding the solution of the 
set C*[e*] of constraints is a fixed point problem we shall define a function 
and show that it has a least fixed point Ifp( F*) that is indeed the least solution 
whose existence is guaranteed by Propositions 3.18 and 3.21. 

3.4 Constraint Based O-CFA Analysis 
We define the function F* by 
where: 
F*(C,p) = (Fl(C,p),F2(C,P)) 
H (C, p)(i) 
= U{(C, P)[ls] I (ls ~ C(i)) E C*[e*]} 
F2(C,P)(X) = U{(C,P)[ls] I (ls ~ r(x)) E C*[e*]} 
175 
To see that this defines a monotone function it suffices to consider a constraint 
lhs' ~ rhs' :::} lhs ~ rhs 
in C*[e*] and to observe that lhs' is of the form {t}j this ensures that 
(CbPl) !; (C2,P2) implies Fi(Cl,Pl) !; Fi(C2,P2) (for i = 1,2) because if 
{t} ~ (Cl,pI)[rhs'] then also {t} ~ (C2,P2)[rhs'l Since C~e* x ~* is 
a complete lattice this means that F* has a least fixed point and it turns out 
also to be the least solution to the set C*[e*] of constraints: 
Proposition 3.22 
Ifp(F*) = n{(C,p) I (C,p) Fe C*[e*]} 
Proof It is easy to verify that: 
F*(C,p) [;;; (C,P) iff (C,P) Fe C*[e*] 
Using the formula Ifp(f) = n{x I f(x) [;;; x} (see Appendix A) the result then 
follows. 
â¢ 
If the size of e* is n then an element (C, P) of C~e* x ~ 
* may be viewed 
as an O(n)-tuple of values from Vai*. Since Vai* is a lattice of height O(n) 
this means that C~e* x Vai* has height O(n2 ) and hence the formula 
may be used to compute the least fixed point in at most O(n2 ) iterations. 
A naive approach will need to consider all O(n2 ) constraints to determine 
the value of each of the O(n) components of the new iterantj this yields an 
overall O(n5 ) bound on the cost. 
Graph formulation. An alternative method for computing the least 
solution to the set C*[e*] of constraints is to use a graph formulation of 
the constraints. The graph will have nodes C(i) and r(x) for i E Lab* and 
x E Var*. Associated with each node p we have a data field D[p] that initially 
is given by: 
D[p] = {t I ({t} ~p) E C*[e*]} 
The graph will have edges for a subset ofthe constraints in C*[e*]j each edge 
will be decorated with the constraint that gives rise to it: 

176 
3 Constraint Based Analysis 
INPUT: 
C*[e*] 
OUTPUT: 
(C, p) 
METHOD: 
Step 1: Initialisation 
W:= nil; 
USING: 
for q in Nodes do D[q] := 0; 
for q in Nodes do E[q] := nil; 
Step 2: Building the graph 
for cc in C*[e*] do 
case cc of 
{t} c;;. p: add(p,{t}); 
PI c;;. P2: E[Pd := cons(cc,E[Pd); 
{ t} c;;. P :::} PI c;;. P2: 
Step 3: Iteration 
E[PI] := cons(cc,E[PI]); 
E[P] := cons(cc,E[P]); 
while W i= nil do 
q := head(W); W := tail(W); 
for cc in E[q] do 
case cc of 
PI c;;. P2: add(p2' D[PI]); 
{t} c;;. p:::} PI c;;. P2: 
if t E D[P] then add(p2' D[Pd); 
Step 4: Recording the solution 
for Â£ in Lab* do C(Â£) := D[C(Â£)]; 
for x in Var* do p(x) := D[r(x)]; 
procedure add( q,d) is 
if --, (d c;;. D[q]) 
then D[q]:= D[q] U d; 
W := cons(q,W); 
Table 3.7: Algorithm for solving constraints. 
â¢ a constraint PI c;;. P2 gives rise to an edge from PI to P2, and 
â¢ a constraint {t} c;;. P:::} PI c;;. P2 gives rise to an edge from PI to P2 and 
an edge from P to P2. 
Having constructed the graph we now traverse all edges in order to propagate 
information from one data field to another. We make certain only to traverse 

3.4 Constraint Based O-CFA Analysis 
177 
P 
D[P] 
E[P] 
C(1) 
0 
[id", ~ C(2) :::} C(1) ~ C(5)] 
C(2) 
id", 
tidy ~ C(2) :::} C(3) ~ C(5), idy ~ C(2) :::} C(4) ~ r(y), 
id", ~ C(2) :::} C(1) ~ C(5), id", ~ C(2) :::} C(4) ~ r(x)] 
C(3) 
0 
tidy ~ C(2) :::} C(3) ~ C(5)] 
C(4) 
idy 
tidy ~ C(2) :::} C(4) ~ r(y), id", ~ C(2) :::} C(4) ~ r(x)] 
C(5) 
0 
[ ] 
r(x) 
0 
[r(x) ~ C(l)] 
r(y) 
0 
[r(y) ~ C(3)] 
Figure 3.4: Initialisation of data structures for example program. 
an edge from Pl to P2 when D[Pl] is extended with a term not previously 
there (and this incorporates the situation where D[Pl] is initially set to a 
non-empty set). Furthermore, an edge decorated with it} ~ P:::} Pl ~ P2 is 
only traversed if in fact t E D[p]. 
To be more specific consider the algorithm of Figure 3.7. It takes as input a 
set C*[e*] of constraints and produces as output a solution (C, jJ) E C;clie* x 
~*. It operates on the following main data structures: 
â¢ a worklist W, i.e. a list of nodes whose outgoing edges should be tra-
versed; 
â¢ a data array D that for each node gives an element of Vai*; and 
â¢ an edge array E that for each node gives a list of constraints from which 
a list of the successor nodes can be computed. 
The set Nodes consists of C(.e) for all .e in Lab* and r(x) for all x in Var*. 
The first step of the algorithm is to initialise the data structures. The second 
step is to build the graph and to perform the initial assignments to the data 
fields. This is established using the procedure add(q,d) that incorporates d 
into D[q] and adds q to the worklist if d was not part of D[q]. The third step 
is to continue propagating contributions along edges as long as the worklist 
is non-empty. The fourth and final step is to record the solution in a more 
familiar form. 
Example 3.23 Let us consider how the algorithm operates on the ex-
pression ((fn x => Xl)2 (fn y => y3)4)5 of Example 3.20. After step 2 the 
data structure W has been initialised to 
W = [C(4) , C(2)], 

178 
3 Constraint Based Analysis 
w 
[C(4),C(2)] 
[r(x),C(2)] 
[C(1),C(2)] 
[C(5),C(2)] 
[C(2)] 
[ ] 
p 
D[P] 
D[P] 
D[P] 
D[P] 
D[P] 
D[P] 
C(1) 
0 
0 
idy 
idy 
idy 
idy 
C(2) 
id", 
id", 
id", 
id", 
id", 
id", 
C(3) 
0 
0 
0 
0 
0 
0 
C(4) 
idy 
idy 
idy 
idy 
idy 
idy 
C(5) 
0 
0 
0 
idy 
idy 
idy 
r(x) 
0 
idy 
idy 
idy 
idy 
idy 
r(y) 
0 
0 
0 
0 
0 
0 
Figure 3.5: Iteration steps of example program. 
and the data structures D and E have been initialised as in Figure 3.4 where 
we have written id", for {fn x => xl} and idy for {fn y => y3}. The algo-
rithm will now iterate through the worklist and update the data structures W 
and D as described by step 3. The various intermediate stages are recorded 
in Figure 3.5. The algorithm computes the solution in the last column and 
this agrees with the solution presented in Example 3.5. 
â¢ 
The following result shows that the algorithm of Table 3.7 does indeed com-
pute the solution we want: 
Proposition 3.24 
Given input C*[e*] the algorithm of Table 3.7 terminates and the 
result (C, PJ produced by the algorithm satisfies 
and hence it is the least solution to C*[e*]. 
Proof It is immediate that steps 1, 2 and 4 terminate, and this leaves us with 
step 3. It is immediate that the values of D[q] never decrease and that they can be 
increased at most a finite number of times. It is also immediate that a node q is 
added to the worklist only if some value of D[q] actually increased. For each node 
placed on the worklist only a finite amount of calculation (bounded by the number 
of outgoing edges) needs to be performed in order to remove the node from the 
worklist. This guarantees termination. 
Next let (C',p) be a solution to (C',p) Fe C*[e*l It is possible to show that the 
following invariant 

3.4 Constraint Based D-CFA Analysis 
Vf E Lab* : D[C(Â£)] s;; e'(Â£) 
'Vx E Var* : D[r(x)] s;; (J(x) 
179 
is maintained at all points after step 1. It follows that (e, f)) !; (e', (J) upon 
completion of the algorithm. 
We prove that (e, f)) Fe C*[e*] by contradiction. So suppose there exists cc E C*[e*] 
such that (e, f)) Fe CC does not hold. If cc is {t} s;; p then step 2 ensures that {t} s;; 
D(P] and this is maintained throughout the algorithm; hence cc cannot have this 
form. If cc is Pl s;; P2 it must be the case that the final value of D satisfies D(Pl] 
=I 0 since otherwise (e, f)) Fe CC would hold; now consider the last time D(Pl] was 
modified and note that Pl was placed on the worklist at that time (by the procedure 
add); since the final worklist is empty we must have considered the constraint cc 
(which is in E(Pl]) and updated D(P2] accordingly; hence cc cannot have this form. 
If cc is {t} s;; P ::::} Pl s;; P2 it must be the case that the final value of D satisfies 
D(P] =I 0 as well as D(Pl] =I 0; now consider the last time one of D(P] and D(Pl] was 
modified and note that P or Pl was placed on the worklist at that time; since the 
final worklist is empty we must have considered the constraint cc and updated D(P2] 
accordingly; hence cc cannot have this form. Thus (e, PJ Fe CC for all cc E C*[e*l 
We have now shown that (e,f)) Fe C*[e*] and that (e,f)) !; (e',{J) whenever 
(<=', (J) Fe C*[e*]. It now follows that 
(e, f)) = n{(e', p) I (e', p) Fe C*[e*]} 
as required. 
â¢ 
The proof showing that the algorithm terminates can be refined to show that 
it takes at most O(n3 ) steps if the original expression e* has size n. To see 
this recall that C* [e*] contains at most 0 (n) constraints of the form {t} ~ P 
or Pl ~ P2, and at most O(n2) constraints of the form {t} ~ P ::::} Pl ~ P2. 
We therefore know that the graph has at most O(n) nodes and O(n2 ) edges 
and that each data field can be enlarged at most O(n) times. Assuming 
that the operations upon D[P] take unit time we can perform the following 
calculations: step 1 takes time O(n), step 2 takes time O(n2), and step 4 
takes time O(n); step 3 traverses each of the O(n2 ) edges at most O(n) times 
and hence takes time O(n3 ); it follows that the overall algorithm takes no 
more than O(n3 ) basic steps. 
Combining the three phases. From Proposition 3.24 we get that 
the pair (C, p> computed by the algorithm of Table 3.7 is the least solution 
to C[e*], so in particular (C, PJ Fe C[e*]. Proposition 3.21 shows that a 
solution to the constraints will also be an acceptable analysis result for the 
syntax directed specification, hence (C, PJ F8 e*. Proposition 3.18 shows that 
a solution that only involves program fragments of e* and that is acceptable 
for the syntax directed specification, also is acceptable for the abstract spec-
ification, and therefore (C, PJ F e*. Thus we have the following important 
corollary: 

180 
3 Constraint Based Analysis 
Corollary 3.25 Assume that (C, ji) is the solution to the constraints 
C[e*] computed by the algorithm of Table 3.7; then (C, ji) 1= e*. 
_ 
It is not the case that any (C, ji) satisfying (C, ji) 1= e* can be obtained using 
the above approach - see Exercise 3.11 and Mini Project 3.1. 
For many applications it is the ability to compute the least (C, ji) satis-
fying (C, ji) 1= e* that is of primary interest, rather than the ability to 
check (C,ji) 1= e* for a proposed guess (C,ji). However, the ability to check 
(C, ji) 1= e* is indispensable for open systems where the environment e.g. pro-
vides a library to be used with e*. When analysing and optimising e* it is 
(C, ji) that expresses the assumptions about the environment; indeed if an 
existing library e satisfies (C, ji) 1= e for the (C, ji) 1= e* used to optimise e* 
then one can exchange the library e with any other e', as long as (C, ji) 1= e' 
continues to hold, and the optimisation made in e* will continue to hold. 
3.5 
Adding Data Flow Analysis 
In Section 3.1 we indicated that our Control Flow Analysis could be extended 
with Data Flow Analysis components. Basically, this amounts to extending 
the set Vai to contain abstract values other than just abstractions. We shall 
first see how this can be done when the data flow component is a powerset 
and next we shall see how it can be generalised to complete lattices. We shall 
present the two approaches as abstract specifications only (in the manner of 
Section 3.1), leaving the syntax directed formulations to the exercises. 
3.5.1 
Abstract Values as Powersets 
Abstract domains. There are several ways to extend the value domain 
Vai so as to specify both Control Flow Analysis and Data Flow Analysis. 
A particularly simple approach is to use a set Data of abstract data values 
(Le. abstract properties of booleans and integers) since this allows us to define: 
v E Vaid = P(Term U Data) abstract values 
For each constant c E Const we need an element de E Data specifying the 
abstract property of c. Similarly, for each operator op E Op we need a total 
function 
OJ) : Vaid X Vaid -+ Vaid 
telling how op operates on abstract properties. Typically, OJ) will have a 
definition of the form 
V1 OJ) 112 = U{d op(d1,d2 ) I d1 E V1 n Data,d2 E V2 n Data} 

3.5 Adding Data Flow Analysis 
181 
for some function dop : Data x Data -t P(Data) specifying how the 
operator computes with the abstract properties of integers and booleans. 
Example 3.26 For a Detection of Signs Analysis we take Datasign = {tt, 
ff, -, 0, +} where tt and ff stand for the two truth values and -, 0, and + for 
the negative numbers, the number 0, and the positive numbers, respectively. 
It is then natural to define dtrue = tt and d7 = + and similarly for the other 
constants. Taking + as an example, we can base its definition on the following 
table 
d+ 
tt 
ff 
-
Â° 
+ 
tt 
0 0 
0 
0 
0 
ff 
0 0 
0 
0 
0 
-
0 0 
{-} 
{-} 
{-, 0, +} 
Â° 
0 0 
{-} 
{o} 
{+} 
+ 
0 0 
{-, 0, +} 
{+} 
{+} 
and similarly for the other operators. 
â¢ 
Acceptability relation. The acceptability relation of the combined 
analysis has the form 
and is presented in Table 3.B. Compared with the analysis of Table 3.1 the 
clause [con] now records that de is a possible value of c and the clause [op] 
makes use of the function Op described above. In the case of [iJl we have 
made sure only to analyse those branches of the conditional that the analysis 
of the condition indicates the need for; hence we can be more precise than 
in the pure Control Flow Analysis - the Data Flow Analysis component of 
the analysis can influence the outcome of the Control Flow Analysis. In 
the manner of Exercise 3.3 similar improvements can be made to many of 
the clauses (see Exercise 3.14) and thereby the specification becomes more 
flow-sensitive. 
Example 3.27 Consider the expression: 
(let f = (fn x => (if (Xl > 02)3 then (fn y => y4)5 
else (fn z => 256)1)8)9 
in ((flO 311)12 013)14)15 
A pure O-CFA analysis will not be able to discover that the else-branch of 
the conditional will never be executed so it will conclude that the subterm 
with label 12 may evaluate to fn y => y4 as well as fn z => 256 as shown 
in the first column of Figure 3.6. The second column of Figure 3.6 shows that 
when we combine the analysis with a Detection of Signs Analysis (outlined 

182 
[can] 
[var] 
ffn] 
[!un] 
[app] 
[ iJl 
[let] 
[op] 
Â«,P) Fd d iff {de} ~ Â«i) 
Â«,P) Fd xl. iff p(x) ~ Â«i) 
3 Constraint Based Analysis 
Â«,P) Fd (fn X => eo)Â£ iff {fn X => eo} ~ Â«i) 
Â«,P) Fd (fun f X => eo)Â£ iff {fun f x => eo} ~ Â«i) 
Â«, P) Fd (t~l t~2)Â£ 
iff Â«, p) Fd t~l A Â«, P) Fd t~2 A 
('v'(fn x => t~O) E Â«id : 
Â«, P) Fd t~O A 
Â«i2) ~ p(x) A Â«io) ~ Â«i)) A 
('v'(fun f x => t~O) E Â«i1) : 
Â«, P) Fd t~O A 
Â«i2) ~ p(x) A Â«io) ~ Â«i) A 
{fun f x => t~O} ~ P(f)) 
Â«, P) Fd (if t~O then t~l else t~2)Â£ 
iff Â«, P) Fd t~O A 
(dtrue E Â«io) => Â«(,P) Fd t~l A Â«i1) ~ Â«i))) A 
(dfalse E Â«io) => Â«(, P) Fd t~2 A Â«i2) ~ Â«i))) 
Â«,P) Fd (let x = t~l in t~2)Â£ 
iff Â«, P) Fd t~l A Â«, p) Fd t~2 A 
Â«it) ~ p(x) A Â«i2) ~ Â«i) 
(C, P) Fd (t~l op t~2)l 
iff Â«, P) Fd t~l A Â«, P) Fd t~2 A 
Â«id Op Â«i2) ~ Â«i) 
Table 3.8: Abstract values as powersets. 
in Example 3.26) then the analysis can determine that only fn y => y4 is 
a possible abstraction at label 12. Note that the Detection of Signs Analy-
sis (correctly) determines that the expression will evaluate to a value with 
property {o}. 
â¢ 
The proof techniques introduced in Section 3.2 should suffice for proving the 
correctness of the analysis with respect to the operational semantics. A slight 
extension of the algorithmic techniques presented in Sections 3.3 and 3.4 (and 
in Mini Project 3.1) suffices for obtaining an implementation of the analysis 
provided that the set Data is finite. 

3.5 Adding Data Flow Analysis 
183 
I 
Section 3.1 
Subsection 3.5.1 
Subsection 3.5.2 
(C,p) 
(C,p) 
(C,p) 
(6,8) 
1 
0 
{+} 
0 
{+} 
2 
0 
{o} 
0 
{o} 
3 
0 
{ttl 
0 
{ttl 
4 
0 
{o} 
0 
{o} 
5 
{fn y => y4} 
{fn y => y4} 
{fn y => y4} 
0 
6 
0 
0 
0 
0 
7 
{fn z => 256} 
0 
0 
0 
8 
{fn y => y\ 
{fn y => y4} 
{fn y => y4} 
0 
fn z => 256 } 
9 
{fn x => ( ... )8} 
{fn x => ( ... )8} 
{fn x => ( ... )8} 
0 
10 
{fn x => ( ... )8} 
{fn x => ( ... )8} 
{fn x => (oo .)8} 
0 
11 
0 
{+} 
0 
{+} 
12 
{fn y => y4, 
{fn y => y4} 
{fn y => y4} 
0 
fn z => 256} 
13 
0 
{o} 
0 
{o} 
14 
0 
{o} 
0 
{o} 
15 
0 
{o} 
0 
{o} 
f 
{fn x => ( ... )8} 
{fn x => ( ... )8} 
{fn x => (oo .)8} 
0 
X 
0 
{+} 
0 
{+} 
y 
0 
{o} 
0 
{o} 
z 
0 
0 
0 
0 
Figure 3.6: Control Flow and Data Flow Analysis for example program. 
Finally, we should stress that a solution to the analysis of Table 3.8 does 
not immediately give a solution to the analysis of Table 3.1. More precisely, 
(C,p) Fd e does not guarantee that (C',{l) Fe where Vi: C'(f) = C(f) n 
Term and Yx : {lex) = p(x) n Term. The reason is that the Control Flow 
Analysis part of Table 3.8 is influenced by the Data Flow Analysis part in 
the clause [iJl: if for example the abstract value of the condition does not 
include dtrue then the then-branch will not be analysed. 
3.5.2 
Abstract Values as Complete Lattices 
Abstract domains. Clearly VaId = P(TermUData) is isomorphic to 
P(Term) x P(Data). This suggests that the abstract cache C : Lab -+ VaId 

184 
3 Constraint Based Analysis 
could be split into a term component and a data component and similarly 
for the abstract environment p: Var -t VaId. 
Having decoupled P(Term) and P(Data) we can now consider replacing 
P(Data) by a more general collection of properties. An obvious possibility 
is to replace P(Data) by a complete lattice L and perform a development 
closely related to that of the (forward) Monotone Frameworks of Chapter 2. 
So let us define a monotone structure to consist of: 
â¢ a complete lattice L, and 
â¢ a set F of monotone functions of L x L -t L. 
An instance of a monotone structure then consists of the structure (L, F) 
and 
â¢ a mapping L from the constants c E Const to values in L, and 
â¢ a mapping f. from the binary operators op E Op to functions of F. 
Compared with the instances of the Monotone Frameworks of Section 2.3 we 
omit the flow component since it will be the responsibility of the Control 
Flow Analysis to determine this. The component Â£ has been replaced by the 
mapping L giving the extremal value for all the constants and the component 
f. mapping labels to transfer functions has been replaced by a mapping of 
the binary operators to their interpretation. 
Example 3.28 A monotone structure corresponding to the development 
of Subsection 3.5.1 will have L to be P(Data) and F to be the monotone 
functions of P(Data) x P(Data) -t P(Data). 
An instance of the monotone structure is then obtained by taking 
for all constants c (and with de E Data as above) and 
for all binary operators op (and where dop : Data x Data -t P(Data) is as 
above). 
_ 
Example 3.29 A monotone structure for Constant Propagation Analysis 
will have L to be ZI x P({tt,ff}) and F to be the monotone functions of 
LxL-tL. 

3.5 Adding Data Flow Analysis 
185 
An instance of the monotone structure is obtained by taking e.g. Â£7 = (7,0) 
and Â£true = (.1, {tt} ). For a binary operator such as + we can take: 
I 
(Zl + Z2, 0) if h = (Zl," .),12 = (Z2," .), 
and Zl,Z2 E Z 
f+(h,12) = 
(.1,0) 
if h = (Zl," ,),12 = (Z2,"')' 
and Zl = .1 or Z2 = .1 
(T,0) 
otherwise 
_ 
We can now define the following abstract domains 
v E V;} 
= P(Term) 
abstract values 
p E :EnV 
= Var -t Vai abstract environments 
C 
C~e 
-
E 
= Lab -t Val abstract caches 
to take care of the Control Flow Analysis and furthermore 
d E D7ta 
= 
L 
abstract data values 
'6 
E DEnv 
= Var -t D7ta abstract data environments 
6 E 
D~he = Lab -t rlata abstract data caches 
to take care of the Data Flow Analysis. 
Acceptability relation. The acceptability relation now has the form 
(C, 6, p,8) I=D e 
and it is defined by the clauses of Table 3.9. In the clause [con] we see that the 
Â£. component of the instance is used to restrict the value of the 6 component 
of the analysis and in the clause [op] we see how the f. component is used. The 
clause [ifJ has explicit tests for the two branches as in the previous approach 
thereby allowing the Control Flow Analysis to benefit from results obtained 
by the Data Flow Analysis component. As in the previous subsection, similar 
improvements can be made to many of the other clauses so as to produce a 
more flow-sensitive analysis. 
Example 3.30 Returning to the expression of Example 3.27 and the De-
tection of Signs Analysis we now get the analysis result of the last column of 
Figure 3.6. So we see that the result is as before. 
_ 
The proof techniques introduced in Section 3.2 should suffice for proving the 
correctness of the analysis with respect to the operational semantics. A slight 
extension of the algorithmic techniques presented in Sections 3.3 and 3.4 (and 
in Mini Project 3.1) suffices for obtaining an implementation ofthe analysis 
provided that L satisfies the Ascending Chain Condition (as is the case for 
Monotone Frameworks). 

186 
3 Constraint Based Analysis 
[con] 
(C, 5,p,J) I=D d iff tc !; 5(f) 
[var] 
fin] 
[!un] 
[app] 
[iJJ 
[let] 
[op] 
(C, 5, p, J) I=D xl iff p(x) ~ C(f) A 8(x)!; 5(f) 
(C, 5, p, 8) I=D (fn x => eo)l iff {fn x => eo} ~ C(f) 
(C,5,p,8) I=D (fun f x => eo)l iff {fun f x => eo} ~ C(f) 
~ ~ 
~ 
l 
l 
l 
(C,D,p,t5) I=D (t}l t22) 
....... _....... 
Â£. 
-_....... 
i 
iff (C, 0, p, (5) I=D t}l A (C, 0, p, (5) I=D t22 A 
(\I(fn x => t~O) E C(f}) : 
.................... 
l. 
(C, 0, p, (5) I=D toO A 
C(f2) ~ p(x) A 5(f2) !; 8(x) A 
C(fo) ~ C(f) A 5(fo)!; 5(f)) A 
(\I(fun f x => t~O) E C(fJ) : 
..................... 
l 
(C, 0, p, (5) I=D toO A 
C(f2) ~ p(x) A 5(f2) !; 6(x) A 
C(fo) ~ C(f) A 5(fo)!; 5(f) A 
{fun f x => t~O} ~ p(fÂ» 
(C, 0, p}) 1= D (if t~O then tfl else t~2)l 
..................... 
1 
iff (C, 0, P, (5) I=D toO A 
....... 
........-....... 
t. 
(ttruB !; D(fo) => (C, D,p, (5) I=D t}l A 
C(f}) ~ C(f)A 
5(f}) !; 5(fÂ» A 
....... 
....... -........ 
L 
('-false!; D(fo) => (C, 0, p, (5) I=D t22 A 
C(f2) ~ C(f)A 
0(f2) !; O(fÂ» 
(C, O,p}) I=D (let x = tfl in t~2)l 
--
-. 
l 
-_....... 
I. 
iff (C,D,p,t5) I=D t}l A (C,D,p,t5) I=D t22 A 
C(f}) ~ p(x) A 5(f}) !; 8(x) A 
C(f2) ~ C(f) A 5(f2) !; 5(f) 
Table 3.9: Abstract values as complete lattices. 

3.6 Adding Context Information 
187 
Staging the specification. Let us briefly consider the following al-
ternative clause for [ifJ where the data flow component cannot influence the 
control flow component because we always make sure that the analysis result 
is acceptable for both branches: 
(e, 0, p,8) FD (if t~O then til else t~2)Â£ 
iff (e, 0, p,8) FD t~O 1\ 
(e, 0, p, 8) FD til 1\ C(f~l) ~ C(e) 1\ O(el) !;;; o(e) 1\ 
(C, 0, p, 8) FD t~2 1\ c(e2 ) ~ c(e) 1\ 0(Â£2) !;;; o(e) 
Unlike what was the case for the analyses of Tables 3.8 and 3.9, a solution to 
the analysis modified in this way does give rise to a solution to the analysis 
of Table 3.1; to be precise (C, 0, p,8) FD e guarantees (C, p) Fe. 
In terms of implementation this modification means that the constraints for 
the control flow component (C and P) can be solved first, and based on this 
the constraints for the data flow component (0 and ;5) can be solved next. If 
both sets of constraints are solved for their least solution this will still yield 
the least solution of the combined set of constraints. 
Example 3.31 Let us return to Example 3.30. If we modify the clause 
for [ifJ as discussed above then the resulting analysis will have C and p as in 
the column for the pure analysis from Section 3.1 and 0 and 8 will associate 
slightly larger sets with some of the labels and variables: 
0(6) = {+} 
0(14) 
{O,+} 
0(15) = {O,+} 
8(z) = {a} 
This analysis is less precise than those of Tables 3.8 and 3.9: it will only 
determine that the expression will evaluate to a value with the property 
{O, +}. 
â¢ 
Imperative constructs and data structures. Mini Project 3.2 
shows one way of extending the development to track creation points of data 
structures. Mini Project 3.4 shows how to deal with imperative constructs 
in the manner of the imperative language WHILE of Chapter 2. 
3.6 
Adding Context Information 
The Control Flow Analyses presented so far are imprecise in that they cannot 
distinguish the various instances of function calls from one another. In the 

188 
3 Constraint Based Analysis 
terminology of Section 2.5 the O-CFA analysis is context-insensitive and in 
the terminology of Control Flow Analysis it is monovariant. 
Example 3.32 Consider the expression: 
(let f = (fn x => xl)2 
in ((f3 f4)5 (fn y => y6)7)8)9 
The least O-CFA analysis is given by (Cid,Pid): 
Cid(l) 
{fn x => xl, fn y => y6} 
Cid(2) 
{fn x => xl} 
Cid (3) 
{fn x => xl} 
Cid (4) = {fn x => xl} 
Cid (5) = {fn x => xl,fn y => y6} 
Cid (6) = {fn y => y6} 
Cid (7) = {fn y => y6} 
Cid(8) = {fn x => xl,fn y => y6} 
Cid(9) 
{fn x => xl, fn y => y6} 
Pid(f) 
{fn x => xl} 
Pid(X) 
{fn x => xl, fn y => y6} 
Pid (y) 
{fn y => y6} 
So we see that x can be bound to fn x => xl as well as fn y => y6 and 
hence the overall expression (label 9) may evaluate to either of these two 
abstractions. However, it is easy to see that in fact only fn y => y6 is a 
possible result. 
_ 
To get a more precise analysis it is useful to introduce a mechanism that 
distinguishes different dynamic instances of variables and labels from one 
another. This results in a context-sensitive analysis and in the terminology 
of Control Flow Analysis the term polyvariant is used. There are several 
approaches to how this can be done. One simple possibility is to expand the 
program such that the problem does not arise. 
Example 3.33 For the expression of Example 3.32 we could for example 
consider 
let f1 = (fn x1 => x1) 
in let f2 = (fn x2 => x2) 
in (f1 f2) (fn y => y) 

3.6 Adding Context Information 
189 
and then analyse the expanded expression: the O-CFA analysis is now able 
to deduce that xi can only be bound to fn x2 => x2 and that x2 can only 
be bound to fn y => Y so the overall expression will evaluate to fn y => Y 
only. 
_ 
A more satisfactory solution to the problem is to extend the analysis with 
context information allowing it to distinguish between the various instances 
of variables and program points and still analyse the original expression. 
Examples of such analyses include k-CFA analyses, uniform k-CFA analyses, 
polynomial k-CFA analyses (mainly of interest for k > 0) and the Cartesian 
Product Algorithm. 
3.6.1 
Uniform k-CFA Analysis 
Abstract domains. A key idea is to introduce context to distinguish 
between the various dynamic instances of variables and program points. 
There are many choices concerning how to model contexts and how they 
can be modified in the course of the analysis. In a uniform k-CFA analy-
sis (as well as in a k-CF A analysis) a context /j records the last k dynamic 
call points; hence in this case contexts will be sequences of labels of length at 
most k and they will be updated whenever a function application is analysed. 
This is modelled by taking: 
/j 
E 
Ll 
= 
Lab~k context information 
Since the contexts will be used to distinguish between the various instances 
of the variables we will need a context environment to determine the context 
associated with the current instance of a variable: 
ce 
E CEnv = 
Var -t Ll 
context environments 
The context environment will play a role similar to the environment of the 
semantics; in particular, this means that we shall extend the abstract values 
to contain a context environment: 
V E 'Vai = P(Term x CEnv) abstract values 
So in addition to recording the abstractions (fn x => e and fun f x => e) 
we will also record the context environment at the definition point for the 
free variables of the term. This should be compared with the Structural 
Operational Semantics of Section 3.2 where the closures contain information 
about the abstraction as well as the environment determining the values of 
the free variables at the definition point. 
The abstract environment p will now map a variable and a context to an 
abstract value: 
p E ~ = (Var x Ll) -t'Vai abstract environments 

190 
3 Constraint Based Analysis 
[con] 
(C, p) F6e d always 
[var] 
(C,P) F6e xt iff p(x,ce(x)) ~ C(l,o) 
ffn] 
(C, p) F6e (fn x => eo)t iff {(fn x => eo, ceo)} ~ C(l, 0) 
where ceo = ce I FV(fn x => eo) 
[fun] 
(C, p) F6e (fun I x => eo)t iff {(fun I x => eo, ceo)} ~ C(l,o) 
where ceo = ce I FV(fun I x => eo) 
lapp] 
(C,P) F6e (tfl t~2)t 
iff (C, p) F6e tfl 1\ (C, p) F6e t~2 1\ 
(V(fn x => t&o, ceo) E C(ll, 0) : 
(C,P) F~:~ t&o 1\ 
C(l2, 0) ~ pcx, 00) 1\ C(lo, 00) ~ C(l, 0) 
where 00 = ro, ilk 
and ce~ = ceo[x I-t 00]) 1\ 
(V(fun I x => t&o, ceo) E C(lI, 0) : 
(C, p) F~:~ t&o 1\ 
C(l2, 0) ~ p(x, 00) 1\ C(lo, 00) ~ C(l,o) 1\ 
{(fun I x => t&o, ceo)} ~ PCI, 00) 
where 00 = ro,llk 
and ce~ = ceo[1 I-t 00, x I-t 00]) 
[ifJ 
(C, p) F6e (if t&o then tfl else t~2)t 
iff (C, p) F6e t&o 1\ (C, P) F6e tfl 1\ (C, P) F6e t~2 1\ 
C(ll'O) ~ C(l,o) 1\ C(l2'0) ~ C(l,o) 
[let] 
(C, p) F6e (let x = tfl in t~2)t 
iff (C, p) F6e tfl 1\ (C, P) F6el t~2 1\ 
C(ll, 0) ~ p(x, 0) 1\ C(l2, 0) ~ C(l,o) 
where ee' = ce[x I-t 0] 
[op] 
(C, p) F6e (tfl op t~2)t iff (C, P) F6e tfl 1\ (C, P) F6e t~2 
Table 3.10: Uniform k-CFA analysis. 
Typically we will use a context environment to find the context associated 
with the variable of interest and then use it together with the variable to 
access the abstract environment. This means that indirectly we get the effect 
of having local abstract environments in the abstract values although pis still 
a global entity as in the previous sections. 

3.6 Adding Context Information 
191 
The uniform k-CFA analysis differs from the k-CFA analysis in performing 
a similar development for the abstract cache that now maps a label and a 
context to an abstract value: 
C E C;clie = (Lab x ~) -t Vai abstract caches 
Given information about the context of interest we can determine the abstract 
value associated with a label. Again we indirectly get the effect of having a 
cache for each possible context although it is still a global entity. (In k-CFA 
one has C;clie = (Lab x CEnv) -t Vai.) 
Acceptability relation. The acceptability relation for uniform k-
CFA is presented in Table 3.10. It is defined by formulae of the form 
(C,p) I=~e e 
where ce is the current context environment and 6 is the current context. 
The formula expresses that (C, p) is an acceptable analysis of e in the context 
specified by ce and 6. The clauses for the various constructs are very much 
as those in Table 3.1 and will be explained below. 
In the clause [var] we use the current context environment ce to determine the 
context ce(x) of the current instance of the variable x and then the abstract 
value of the variable is given by p(x, ce(x)). The current context is 6 so we 
have to ensure that p(x,ce(x)) ~ Â«l,6). 
In the clause fin] we record the current context environment as part of the 
abstract value and (as in the Structural Operational Semantics of Table 3.2) 
we restrict the context environment to the set of variables of interest for the 
abstraction. The clause [fun] is similar. 
In the clause [app] we analyse the two subexpressions using the same context 
and context environment as the composite expression. When we find a po-
tential abstract value, say (fn x => t~O, ceo), that the operator may evaluate 
to, it will contain a local context environment ceo that was created at its 
definition point. When analysing t~O we will have passed through the appli-
cation point l, so the current context will be updated to include l and this 
will also be the context associated with the variable x in the updated version 
of the context environment ceo used for the analysis of t~o. The new context 
is r6,lh which (as in Section 2.5) denotes the sequence [6,ll but possibly 
truncated (by omitting elements on the left) so as have length at most k. In 
the case where the operator has the form (fun f x => t~O, ceo) we proceed in 
a similar way and note that f as well as x will be associated with the new 
context in the analysis of the body of the function. 
The clauses for [ij), [let] and [op] are fairly straightforward modifications of 
those of Table 3.1; however, note that the context ofthe bound variable ofthe 
let-construct is the current context (as no application point is passed). We 

192 
3 Constraint Based Analysis 
shall dispense with proving the correctness of the analysis and with showing 
how it can be implemented. 
Example 3.34 We shall now specify a uniform 1-CFA analysis for the 
expression of Example 3.33: 
The initial context will be A, the empty sequence of labels. In the course 
of the analysis the current context will be modified at the two application 
points with labels 5 and 8; since we only records call strings of length at most 
one the only contexts of interest will therefore be A, 5 and 8. There are four 
context environments of interest: 
ceo=[] 
the initial (empty) context environment, 
cel = ceo[f r+ A] 
the context environment for the analysis of the body 
of the let-construct, 
ce2 = ceo [x r+ 5] 
ce3 = ceo[x r+ 8] 
the context environment used for the analysis of the 
body of f initiated at the application point 5, and 
the context environment used for the analysis of the 
body of f initiated at the application point 8. 
Let us take Cid and Pid to be: 
Cid (1,5) = {(fn x => Xl,ceO)} 
Cid(2,A)={(fn X => Xl,ceO)} 
Cid(4, A) = {(fn X => xl,ceo)} 
Cid(7,A) = {(fn y => y6,ceo)} 
Cid(9,A) = {(fn y => y6,ceO)} 
Pid(f,A) = {(fn x => xl,ceo)} 
Pid(x,5)={(fn x => xl,ceO)} 
Cid(1,8)={(fn y => y6,ceO)} 
Cid(3,A) = {(fn x => xl,ceo)} 
Cid (5, A) = {(fn x => xl,ceO)} 
Cid(8,A) = {(fn y => y6,ceO)} 
We shall now show that this is an acceptable analysis result for the example 
expression: 
(Cid,Pid) p~o (let f = (fn x => xl)2 in ((f3 f4)5 (fn y => y6f)8)9 
According to clause [let], it is sufficient to verify that 
(Cid,Pid) p~o (fn x => xl)2 
(Cid,Pid) p~l ((f3 f4)5 (fn y => y6)7)8 
because Cid(2,A) ~ Pid(f,A) and Cid(8,A) ~ Cid(9,A). This is straightfor-
ward except for the last clause. Since Cid(5,A) = {(fn x => xl,ceo)} it is, 
according to [app], sufficient to verify that 

3.6 Adding Context Information 
193 
because Cld(7,A) ~ Prd(x,8) and Cld(1,8) ~ Cld(8,A). This is straightforward 
except for the first clause. Proceeding as above we see that Cld(3,A) = {(in 
x => xl ,ceo)} and it is sufficient to verify 
because Cld(4, A) ~ Prd(x, 5) and Cld(l, 5) ~ Cld(5, A). This is straightforward. 
The importance of this example is that it shows that the uniform l-CFA 
analysis is strong enough to determine that in y => y6 is the only result 
possible for the overall expression unlike what was the case for the O-CFA 
analysis in Example 3.32. We can also see that, since Prd (y, 8) = 0 for all 
8 E {A, 5, 8} it follows that in y => y6 is never called upon a function. 
_ 
The resulting analysis will have exponential worst case complexity even for 
the case where k = 1. To see this assume that the expression has size nand 
that it has p different variables. Then A has O(n) elements and hence there 
will be O(p . n) different pairs (x,8) and O(n2 ) different pairs (f,8). This 
means that (C, p) can be seen as an O(n2 ) tuple of values from Vai. Since 
Val itself is a powerset of pairs of the form (t, ce) and there are O(n . nP ) 
such pairs it follows that Val has height O(nÂ· nP ). Since p = O(n) we have 
the exponential worst case complexity claimed above. 
This should be contrasted with the O-CFA analysis developed in the previous 
sections. It corresponds to letting A be a singleton. Repeating the above 
calculations we can see (C, p) as an O(p+n) tuple ofvalues from Vai, and Vai 
will be a lattice of height O(n). In total this gives us a polynomial analysis 
as we already saw in Section 3.4. 
The worst case complexity of the uniform k-CFA analysis (as well as the k-
CFA analysis) can be improved in different ways. One possibility is to reduce 
the height of the lattice Val using the techniques of Chapter 4. Another 
possibility is to replace all context environments with contexts, i.e. to have 
Val = P(Term x A); clearly this will give a lattice of polynomial height. 
This idea is closely related to the so-called polynomial k-CFA analysis where 
the analogues of context environments are forced to be constant functions, 
i.e. to map all variables to the same context. In the case of polynomiall-CFA 
the analysis is of complexity O(n6 ). 

194 
3 Constraint Based Analysis 
Interprocedural analysis revisited. Let us compare the above 
development with that of Section 2.5 where we considered interprocedural 
analysis for a simple imperative procedure language. 
Recall that the abstract domain of interest in Section 2.5 has the form 
t:J.-+L 
where t:J. is the context information and L is the complete lattice of abstract 
values of interest. For each label '- the analysis will determine two elements 
A o {'-) and A.{'-) of t:J. -+ L describing the situation before and after the 
elementary block labelled '- is executed. So we have 
Ao , A. : Lab -+ (t:J. -+ L) 
and in the terminology of the present chapter we may regard these functions 
as abstract caches. There is no analogue of the abstract environment in 
Section 2.5 - the reason is that the procedure language is so simple that it is 
not needed: the abstract environment records the context of the free variables 
and since all free variables in the procedures are global variables there is no 
need for this component. 
We can now reformulate the above development as follows. We can take the 
abstract domain of interest to be 
t:J. -+ P{Term x CEnv) 
and reformulate the abstract cache and the abstract environment as having 
the functionalities: 
C: Lab -+ t:J. -+ P{Term x CEnv) 
ji: Var -+ t:J. -+ P{Term x CEnv) 
Thus the abstract caches of the interprocedural analysis and the uniform k-
CFA analysis (using call strings) have the same overall functionality and we 
may conclude that the two analyses are variations over a theme. 
3.6.2 
The Cartesian Product Algorithm 
The Cartesian Product Algorithm, abbreviated CPA, has been developed for 
object-oriented languages but the main ideas can be expressed using a varia-
tion of our functional language in which functions take m arguments (m > 0): 
t ::=Â·Â·Â·1 fn Xl,'" ,Xm => eb 1 eo{el,'" ,em) 
For notational simplicity we shall dispense with recursive functions through-
out this subsection. To be faithful to the official description of CPA we shall 
furthermore impose the well-formedness condition that all function abstrac-
tions are closed, i.e. FV{fn Xl,'" ,Xm => eb) = 0, much as was the case for 
the procedural language considered in Section 2.5. We leave a more general 
treatment to Exercise 3.17. 

3.6 Adding Context Information 
195 
Rephrasing the O-CFA analysis. The first step is to adapt the 
specification of the O-CFA analysis to deal with functions taking m argu-
ments. Even though CPA is a very practical oriented algorithm it will be 
appropriate to consider the abstract specification in Table 3.1 (rather than 
the syntax directed specification in Table 3.5). We modify it as follows: 
~ 
Â£ 
~ 
(C,p) 1= (fn Xl,Â·Â·Â· ,Xm => eb) iff {fn Xl,Â·Â·Â· ,Xm => eb} ~ C(e) 
(C, p) 1= (t~O (til, ... , t;;:' ))Â£ 
iff (C, p) 1= t~O /\ (C, p) 1= til 1\ ... 1\ (C, p) 1= t;;:' 1\ 
V(fn Xl,Â·Â·Â·, Xm => tib ) E C(eO) : 
C(el ) x ... x C(em) ~ p(xd x ... x P(Xm) 1\ 
(C, p) 1= tib 1\ 
C(eb) ~ C(e) 
The third last conjunct in the clause for function application can be written 
in the case where no C(ei) is empty. 
The Cartesian Product Algorithm. We next extend the analysis 
to take context into account. This will be in the form of the actual arguments 
supplied to the function: 
8 E Ll = Termm = Term x ... x Term 
(m times) 
Recalling that Val = P(Term) we then redefine the abstract domains as 
follows: 
P E 
En:v 
= (Var x Ll) -+ Val 
C E C~e 
(Lab x Ll) -+ Val 
The key clauses in the CPA analysis then are: 
~ 
Ii 
Â£. 
~ 
(C,ji) I=CPA (fn Xl,Â·Â·Â· ,Xm => eb) Iff {fn Xl,Â·Â·Â· ,Xm => eb} ~ C(e,8) 
(C, p) I=~PA (t~O (til, ... , t;;:' )Â£ 
iff (C, p) I=~PA t~O 1\ (C, p) I=~PA til /\ ... 1\ (C, ji) I=~PA t;;:' 1\ 
Â£ 
~ 
V(fn Xl,Â·Â·Â· ,Xm => tbb) E C(eo, 8) 
V8b E C(el , 8) x ... x C(em , 8) : 
{8b } ~ p(xd x ... X P(Xm) 1\ 
(C, p) l=~bpA tib /\ 
C(eb, 8b) ~ C(e, 8) 

196 
3 Constraint Based Analysis 
It is clear from this specification that the body of the function is analysed sep-
arately for each possible tuple of arguments and that no merging of data takes 
place. To be practical we need to implement the analysis using memoisation 
so that the bodies are only analysed once. This can be done by organis-
ing each (, PJ I=~~A ttb into so-called templates and to maintain them in a 
global pool; when a template is created it is only added to the pool if it is 
not already present. 
The Cartesian Product Algorithm derives its name from the cartesian product 
Â«(Â£1,8) xÂ· .. x Â«(Â£m' 8) over which 8b ranges. The analysis can be implemented 
in a straightforward lazy manner because the product grows monotonically: 
Â«(Â£1 ,8) x .. Â· x Â«( Â£m, 8) will increase each time one of the Â«( Â£i, 8) increases 
(assuming that none is empty). A mild generalisation is studied in Exercise 
3.17. 
Interprocedural analysis revisited. Let us once more compare 
the development to Section 2.5 but this time to the development based on 
assumption sets. The development in Section 2.5 is based on 
Ao,A. : Lab --t (~~ L) 
where ~ is the context information and L = P(D) is the powerset of interest, 
and the current development can be reformulated as operating on 
( : Lab --t ~ --t -Vai 
p: Var --t ~ --t -Vai 
where -Vai = P(Term) is the powerset of interest. Clearly D and Term can 
be taken to be equal and the fact that ~ = Term then shows that ~ = D and 
we may conclude that "small assumption sets" and the Cartesian Product 
Algorithm are variations over a theme. 
Concluding Remarks 
Control Flow Analysis for functional languages. Many of 
the key ideas for Control Flow Analysis have been developed within the 
context of functional languages. The concept of k-CFA analysis seems due to 
Shivers [156, 157, 158]; other works on O-CFA-like analyses include [163, 136, 
57,56]. The ideas behind k-CFA and polynomial k-CFA analysis were further 
clarified in [79] that also established the exponential complexity of k-CFA 
analysis for k > 0; it also related a form of Set Based Analysis [70] to O-CFA. 
The uniform k-CFA analyses were introduced in [122] as a simplification of 
the k-CFA analyses; an obvious variation over this is to record the set of the 
last k distinct call points (see Exercise 3.15). Yet another variation over the 

Concluding Remarks 
197 
same theme is Closure Analysis; an early and often neglected development 
may be found in [152]. 
The formulation of the analyses (as well as the one presented in Table 3.1) 
would often seem to be more appropriate for a dynamically scoped than for a 
statically scoped language: The O-CFA analysis coalesces information about 
variables having several defining occurrences even if they differ in their scope; 
clearly the analysis can easily be modified so that it more directly models 
static scope (see Exercise 3.7) rather than relying on no variable having more 
than one defining occurrence. 
To the extent these developments go beyond O-CFA they establish additional 
context (called mementoes, tokens or contours) for representing information 
concerning the dynamic call chain. The most common approach links back 
to the use of call strings in Section 2.5. The Cartesian Product Algorithm 
[2] was originally developed for object-oriented programs and amounts to the 
use of "small assumption sets" in Section 2.5. 
Another way to establish context is to represent the static call chain. This 
seems first to be described by [80] as part of their so-called "polymorphic 
splitting" analysis. A more general set-up was formulated in [122] that also 
argued for the need to base abstract specifications on coinductive methods -
bearing in mind that coinductive and inductive methods may coincide as in 
the case of syntax directed specifications. 
Clearly Control Flow Analysis should be combined with Data Flow Analysis 
to strengthen the quality of the control information as well as providing the 
data flow information of interest. Our treatment in Section 3.5 only presents 
the first steps in this direction: a more ambitious approach is outlined in 
Mini Project 3.4 which is based in [126]. In fact, some authors would claim 
that the analysis presented in Section 3.1 should not be regarded as a O-CFA 
analysis since it does not include data flow analysis (as in Section 3.5) and 
does not take evaluation order into account (as in Exercises 3.3 and 3.14); in 
our view these developments are all variations over a theme. 
Most of the papers cited above directly formulate a syntax directed specifi-
cation, perhaps proving it semantically sound, and perhaps showing how to 
generate constraints so as to obtain an efficient implementation. The use of 
abstract specifications first appeared in [80, 122] and has the advantage of 
being more directly applicable to open systems (that allow to interface with 
the library routines provided by the environment) and also to the ideas of 
Abstract Interpretation of Chapter 4. In particular, the notion of reachability 
suggests itself rather naturally [21, 60], it becomes clearer how to integrate 
ideas from Abstract Interpretation into Control Flow Analysis, and one does 
not inadvertently restrict oneself to closed systems only. 
(The notion of 
reachability is considered in Mini Project 3.1 which is based on [60].) 

198 
3 Constraint Based Analysis 
Only few papers [125] discuss the interplay between the choice of specification 
style for the analysis and the choice of semantics. We have used a small-step 
Structural Operational Semantics rather than a big-step semantics in order 
to express the semantic correctness also of looping programs. We have used 
an environment based semantics in order to ensure that we do not "modify" 
the bodies of functions before they are called, so that function abstractions 
can meaningfully be used in the value domains of our analysis [125]. As a 
consequence we have had to introduce intermediate expressions (closures and 
bindings) and have had to specify the abstract analysis also for intermediate 
expressions; for the syntax directed specification and the constraint based 
analysis this was not necessary given that semantic correctness had already 
been dealt with. Alternative choices are clearly possible but are likely to 
sacrifice at least some of the generality offered by the present approach. 
Control Flow Analysis for other language paradigms. An-
other main application of Control Flow Analysis has been for object-oriented 
languages: one simply tracks objects rather than functions [3, 124, 137]. As 
a reminder of the close links between Data Flow Analysis and Control Flow 
Analysis we should also mention that some approaches [178, 139] are closer to 
the presentation of Chapter 2. A common theme among the more advanced 
studies is the incorporation of context (related to k-CFA) and an abstract 
store [133] (to deal with imperative aspects like method update). To increase 
the precision, local versions of the abstract store need to exist at all program 
points, and abstract reference counts are needed to incorporate a "kill" com-
ponent (in the manner of Chapter 2). We refer to the above literature for 
further details of how to formulate such analyses and how to choose a proper 
balance between precision and cost. 
Control Flow Analysis for concurrent languages has received relatively little 
attention [22]. However, variations of the techniques presented in this chapter 
have been used to analyse functional languages extended with concurrency 
primitives allowing processes to be created dynamically and to communicate 
via shared locations or channels [78, 57, 60, 22, 23]. 
In this book we do not consider logic programming languages. However, we 
should point out that Control Flow Analysis also has applications for logic 
programming languages and that set based analysis was first developed for 
this class of languages [72, 73]. 
Set-Constraint Based Analysis. Control Flow Analysis is just one 
approach to program analysis where the use of constraints pays off. In this 
chapter we have have taken the following approach: (i) first we have given 
an abstract specification of when a proposed solution is acceptable, (ii) then 
we have developed an algorithm for generating a set of constraints expressing 
that a proposed solution is acceptable, and (iii) finally we have solved the 
set of constraints for the least solution. For the solution of set constraints 

Concluding Remarks 
199 
in step (iii), it is unimportant how the constraints were in fact obtained. 
For this reason, it is often said that set constraints allow the separation the 
specification of an analysis from its implementation and that set constraints 
are able to deal with forward analyses as well as backward analyses and 
indeed mixtures of these. 
Set constraints [72, 7] have a long history [143, 84]. They allow us to express 
general inclusions of the form 
where set expressions, 8, set variables, V, and set constructors, C, may be 
built as follows: 
8 
V I 0 I 8 1 U 8 2 I 8 1 n 8 2 I C(81 , .â¢. ,8n ) 
(81 ~ 8 2 ) =} 8 3 I (81 i- 0) =} 8 2 I C-i (8) I ..,8 I ... 
V 
XIYIÂ·Â·Â· 
C 
true I false I 0 I ... I cons I nil I ... 
Set constraints allow the consideration of constructors that are not just 
nullary and this allows us to record also the shape of data structures, so 
for example cons(81 , 8 2 ) U nil expresses possibly empty lists whose heads 
come from 8 1 and whose tails come from 8 2 . The associated projection selects 
those terms (if any) having the required shape, e.g. cons-1(8) produces the 
heads that may be present in 8. We have seen conditional constraints before 
and it turns out that projection is so powerful that it can be used to code 
conditional constraints. Finally, it is sometimes possible to explicitly take 
the complement of a set but this adds to the complexity of the development. 
(It means that solutions can no longer be guaranteed using Tarski's Theorem 
and sometimes a version of Banach's Theorem may be used instead.) 
The complexity of solving a system of set constraints depends rather dra-
matically on the set forming operations allowed and therefore many versions 
have been considered in the literature. We refer to [6, 135] for an overview 
of what is known in this area; here we just mention [28] for a general result 
and [70, 10] for some cubic time fragments. 
. 
However, it is worth pointing out that many of these results are worst-case 
results; benchmark results of Jaganathan and Wright [80] shows e.g. that in 
practice a l-CFA analysis may be faster than a O-CFA analysis despite the 
fact that the former has exponential worst-case complexity and the latter 
polynomial worst-case complexity. The reason seems to be that the O-CFA 
analysis explores most of its polynomial sized state space whereas the l-CFA 
analysis is so precise that it only explores a fraction of its exponentially sized 
state space. 
The basic idea behind many of the solution procedures for set constraints is 
roughly as follows [9]: 

200 
3 Constraint Based Analysis 
1. Dynamically expand conditional constraints, based on the condition 
being fulfilled, until no more expansion is possible. 
2. Remove all conditional constraints and combine the remaining con-
straints to obtain the least solution. 
This is not quite the algorithm used in Section 3.4 where we were only in-
terested in solving a rather limited class of constraints for O-CFA analysis. 
When generating the constraints in Table 3.6 we were able to "guess a uni-
verse" TerIr4 that was sufficiently large and this allowed us to generate 
explicit versions of the conditional constraints; in fact a superset of all those 
to be considered in step 1 of the above algorithm. Therefore our subsequent 
constraint solving algorithm in Table 3.7 merely needed to check the already 
existing constraints and to determine whether or not they could contribute 
to the solution. In practice, the above "lazy" algorithm is likely to perform 
much better than the "eager" algorithm of Tables 3.6 and 3.7. 
Mini Projects 
Mini Project 3.1 Reachability Analysis 
The syntax directed analysis of Table 3.5 analyses each subexpression of e* 
"exactly once" rather than "at most once" as really called for. In this mini 
project we shall study one way to amend this. 
The idea is to introduce an abstract reachability component 
R E Reach = Lab ---t P( {on}) 
and to modify the syntax directed analysis to have a relation of the form 
(C,p, R) F~ e 
The idea is that fn x => t~O has {on} ~ R(â¬o) if and only if the function is 
indeed applied somewhere, and that the "recursive call" (C, p, R) Fs t~O is 
performed if and only if {on} ~ R(â¬o). 
1. Modify Table 3.5 to incorporate this idea. 
2. Show the following analogue of Proposition 3.18: If (C, p, R) F~ t;*, 
-. 
-. ........ 
-T -T 
-. 
l. 
{on} ~ R(â¬*) and (C, p) ~ (C*, p*) then (C, P) F t**. 
3. Determine whether or not the statements 
if (C, PJ F e* then (C, p, R) F~ e* for some R 
-
--
-T 
T 
-. 
-. 
.-
if (C,PJ F e* and (C,P) ~ (C* ,p*) then (C,p, R) F~ e* for some R 
hold in general. 
â¢ 

Mini Projects 
201 
Mini Project 3.2 Data Structures 
The language considered so far only includes simple data like integers and 
booleans. In this mini project we shall extend the language with more general 
data structures: 
Here C E Constr denotes an n-ary data constructor. A data element is 
constructed by C(el,Â·Â·Â·, en): it has tag C and its components are the values 
of el, ... , en. The case construct will first determine the value Vo of eo, if 
Vo has the tag C then Xl, ... ,Xn will be bound to the components of Vo and 
el is evaluated. If Vo does not have tag C then X is bound to Vo and e2 is 
evaluated. 
As an example we may have Constr = {cons, nil} so we have the following 
expression (omitting labels) for reversing a list: 
let append = fun app xs => fn ys => 
case xs of cons(z,zs) => cons(z,app zs ys) 
or xs => ys 
in fun rev xs => case xs of cons(y,ys) => 
append (rev ys) (cons(y,nil(Â») 
or xs => nilO 
To specify a O-CFA analysis for this language we shall take 
Val = P(TermU {C(fl ,Â·Â·Â· ,fn) ICE Constr,fl ,Â·Â·Â· ,fn E Lab}) 
As before the terms of interest are fn x => eo and fun f x => eo for recording 
the abstractions. The new contribution is a number of elements of the form 
C(fl ,Â·Â·Â·, fn) denoting a data element C(Vl,Â·Â·Â·, vn) whose i'th component 
might have been created by the expression at program point f i â¢ 
1. Develop a syntax directed analogue of the analysis in Table 3.5. 
2. Modify the constraint generation algorithm of Table 3.6 to handle the 
new constructs and make the necessary changes to the constraint solv-
ing algorithm of Table 3.7. 
For the more ambitious: are there any difficulties in developing an abstract 
analogue of the analysis in Table 3.1? 
â¢ 

202 
3 Constraint Based Analysis 
Mini Project 3.3 A Prototype Implementation 
In this mini project we shall implement the pure O-CFA analysis considered 
in Section 3.3. As implementation language we shall choose a functional 
language such as Standard ML or Haskell. We can then define a suitable 
data type for FUN expressions as follows: 
type var 
string 
type label 
= int 
datatype canst = Num of int I True I False 
datatype exp 
La bel of term * label 
and term 
= Const of canst I Var of var 
Now proceed as follows: 
Fn of var * exp I Fun of var * var * exp 
App of exp * exp I If of exp * exp * exp 
Let of var * exp * exp lOp of string * exp * exp 
1. Implement the constraint based control flow analysis of Section 3.4; this 
includes defining an appropriate data structure constraints for (condi-
tional) constraints. 
2. Implement the graph based algorithm of Section 3.4 for solving con-
straints; this involves choosing appropriate data structures for the work-
list and the two arrays used by the algorithm. 
For the more ambitious: generalise your program to perform some of the more 
advanced analyses, e.g. by incorporating data flow information or context 
information. 
_ 
Mini Project 3.4 Imperative Constructs 
In Section 3.5 we showed how to incorporate Data Flow Analysis into our 
Control Flow Analysis; this becomes more challenging when imperative con-
structs are added to the language: 
Here new1r x : = el in e2 creates a new reference variable x to be used in e2; 
it is initialised to el, its content is obtained by ! x, and it may be updated 
by x : = eo. The creation point for the reference variable is indicated by the 
program point 7r E Pnt and the construct el ; e2 merely sequences el and 
e2 (and is equivalent to let x = el in e2 when x does not occur in e2). 

Exercises 
203 
One approach to extending the development of Section 3.5 might be to work 
with an acceptability relation of the form 
(C, p, So, S.) Fme e 
where 
â¢ C : Lab -7 Vai and C{Â£) describes the values that the subexpression 
labelled Â£ may evaluate to, 
â¢ Ii: Var -7 Vai and p{x) describes the values that x might be bound 
to, 
â¢ So : Lab -7 (Pnt -7 Vai) and So{Â£) describes the states that may be 
possible before the sub expression labelled Â£ is evaluated, 
â¢ 5. : Lab -7 (Pnt -7 Vai) and S.{Â£) describes the states that may be 
possible after the subexpression labelled Â£ is evaluated, and 
â¢ me: Var -7 Pnt and me (x) indicates the creation point for the refer-
ence variable x. 
One choice of Vai is P{Term) x L where the first component tracks functions 
and the second component tracks abstract values. It may be helpful to devise 
a Structural Operational Semantics for the extended language and to use it 
as a guide when defining the acceptability relation. 
A more advanced treatment would involve context information in the manner 
of Section 3.6. 
â¢ 
Exercises 
Exercise 3.1 Consider the following expression (omitting labels): 
let f = fn x => x 1 
in let g = fn y => y+2 
in let h = fn z => z+3 
in (f g) + (f h) 
Add labels to the program and guess an analysis result. Use Table 3.1 to 
verify that it is indeed an acceptable guess.Â· 
â¢ 

204 
3 Constraint Based Analysis 
Exercise 3.2 The specification of the Control Flow Analysis in Table 3.1 
uses potentially infinite value spaces and this is not really necessary. To see 
this choose some expression e* E Exp that is to be analysed. Let Var* ~ Var 
be the finite set of variables occurring in e*, let Lab* ~ Lab be the finite set 
of labels occurring in e*, and let TerI14 be the finite set of subterms of e*. 
Next define 
---
if E Val* 
= P(TerIl4) 
p E ~* 
= Var* ~ -Vai* 
( 
E 
CiclJ.e* 
Lab* -t -Vai* 
and note that these value spaces are finite. Show that the specification of 
the analysis in Table 3.1 still makes sense when ((, ji) is restricted to be in 
CiclJ.e* x ~*. 
_ 
Exercise 3.3 Modify the Control Flow Analysis of Table 3.1 to take ac-
count of the left to right evaluation order imposed by a call-by-value se-
mantics: in the clause [app] there is no need to analyse the operand if the 
operator cannot produced any closures. Try to find a program where the 
modified analysis accepts analysis results ((, p) rejected by Table 3.1. 
_ 
Exercise 3.4 So far we have defined "((, ji) is an acceptable solution for 
e" to mean that 
(3.8) 
but an alternative condition is that 
(3.9) 
Show that (3.8) implies (3.9) but not vice versa. Discuss which of (3.8) or 
(3.9) is the preferable definition. 
_ 
Exercise 3.5 Consider an alternative specification of the analysis in Table 
3.1 where the condition 
in [app] is replaced by 
((il) ~ ji(f) 
also in lapp]. Show that the proof of Theorem 3.10 can be modified accord-
ingly. Discuss the relative precision of the two analyses. 
_ 
Exercise 3.6 Reconsider our decision to use -Vai = P(Term) and con-
sider using -Vai = P(Exp) instead. Show that the specification of the Con-
trol Flow Analysis may be modified accordingly but that then Fact 3.11 (and 
hence the correctness result) would fail. 
_ 

Exercises 
205 
Exercise 3.7 The operational semantics allow us to rename bound vari-
ables without changing the semantics; this is in accord with the language 
being statically scoped (or lexically scoped) rather than dynamically scoped. 
As an example 
and clearly the two programs have the same semantics. 
However, renaming bound variables changes the acceptability of a solution as 
well as influences the precision of the analysis specified in Table 3.1. Develop 
an abstract specification of a O-CFA analysis that is more faithful to the static 
scoping than that of Table 3.1; it should agree with the specification of Table 
3.1 for expressions that do not have multiple defining occurrences. 
_ 
Exercise 3.8 In Section 3.2 we equipped FUN with a call-by-value seman-
tics. An alternative would be to use a call-by-name or lazy semantics. It can 
be obtained as a simple modification of the semantics of Tables 3.2 and 3.3 by 
allowing the environments p E Env to map variables to intermediate terms 
(and not just values), by deleting the rules [apP2] and [letl] and then make 
some obvious modifications to the axioms [var], [apPjn], [apPjun] and [le~]; 
in the case of [var] we will take: 
p I- xl -+ itl 
if x E dom(p) and it = p(x) 
Complete the specification of the semantics and show that the correctness 
result (Theorem 3.10) still holds for the analysis of Table 3.1. 
What does that tell us about the precision of the analysis? 
-
Exercise 3.9 Let I=~ and I=~ be two relations satisfying the specification 
of Table 3.5. Show that 
by structural induction on e. 
-
Exercise 3.10 Consider Proposition 3.18 and determine whether or not 
the statement 
holds in general. 
Exercise 3.11 Give an example showing that both of the statements 
if (C, p) 1= e* then (C, p) 1=8 e* 
..-
..-
-T:r 
-
if (C, p) 1= e* and (C, p) !; (C* ,p* ) then (C, p) 1=8 e* 
fail in general. 
-
-

206 
3 Constraint Based Analysis 
Exercise 3.12 Give a direct proof of the correctness of the syntax di-
rected analysis of Table 3.5, i.e. establish an analogue of Theorem 3.10. This 
involves first extending the syntax directed analysis to the bind- and close-
constructs and next proving that if p n p, pI- ie ~ ie' and (C, PJ t=s ie then 
also (C, PJ t=s ie'. 
-
Exercise 3.13 Consider the system C;[e*] that contains a constraint 
ISlUÂ·Â·Â·Ulsn=rhs 
whenever C*[e*] contains the n ~ 1 constraints 
lSi ~ rhs 
Show that 
(C, PJ t=c C:'[e*] implies (C, PJ t=c C*[e*] 
where (C,PJ t=c (Is = rhs) is defined in the obvious way. Also show that 
(C, PJ t=c C*[e*] implies (C, PJ t=c C;[e*] 
holds in the special case where (C, PJ is least such that (C, p) t=c C*[e*]. 
_ 
Exercise 3.14 Use the ideas of Exercise 3.3 to develop an improvement 
of Table 3.8 where expressions are only analysed when absolutely needed. 
Next develop a syntax directed analysis using the same ideas. Discuss the 
relationship between the two specifications: are they more closely related 
than is the case for t= and t=s of Table 3.1 and 3.5 (see Exercises 3.10 and 
3.11)? 
_ 
Exercise 3.15 Modify the abstract specification of the uniform k-CFA 
analysis so that it does not record the last k function calls but the last k 
times we called a different function than in the preceding call: if the calling 
sequence is [1,2,2,1,1] then 2-CFA records [1,1] but the modified analysis 
records [2,1]. Discuss which of the two analyses (say for k = 2) is likely to 
be most useful in practice. 
_ 
Exercise 3.16 Let us consider a language of first-order recursion equation 
schemes: the programs have the form 
where D* is a sequence of function definitions of the form: 
f(x)= e 

Exercises 
207 
Here f is a function name, x is the formal parameter and e is the body of 
the function; the functions defined in D may be mutually recursive and the 
parameter mechanism is call-by-value. The expressions are given by 
e 
tÂ£ 
t 
C I x I f e I if eo then el else e2 I el op e2 
where c E Const and op E Op as before; we shall assume that f and x 
belong to distinct syntactic categories. As an example we may define the 
Fibonacci function by the following expression (omitting labels): 
define fib(z) 
if z<3 then 0 
else fib (z-l) + fib (z-2) 
in 
fib x 
Define a uniform k-CFA analysis for this language. For k = 0 and k = 1 
compare the development with that for the procedure language in Section 
2.5. 
_ 
Exercise 3.17 In Subsection 3.6.2 we covered the Cartesian Production 
Algorithm based on the assumption that all function abstractions are closed. 
In this exercise we do not make this simplifying assumption and also we 
wish to deal with recursive functions. Develop an abstract specification of an 
analysis 
where 8 E ~ is as in Subsection 3.6.2 and 
ce E CEnv = Var -+ ~ 
Hint: Modify Table 3.10 to deal with functions taking m arguments and 
to perform the appropriate determination of new contexts in the clause for 
function application. 
_ 

Chapter 4 
Abstract Interpretation 
The purpose of this chapter is to convey some of the essential ideas of Ab-
stract Interpretation. We shall mainly do so in a programming language 
independent way and thus focus on the design of the property spaces, the 
functions and computations upon them, and the relationships between them. 
We first formulate a notion of correctness for a restricted class of analyses 
as this will allow us to motivate better some of the key definitions in the 
development. Then we cover the widening and narrowing techniques that can 
be used to obtain approximations of the least fixed point and for limiting the 
number of computation steps needed. Next we consider Galois connections 
and Galois insertions that allow a costly space of properties to be replaced 
with one that is less costly. Galois connections can be constructed in a 
systematic way and can be used to induce one specification of an analysis 
from another. 
4.1 
A Mundane Approach to Correctness 
To set the scene, imagine some programming language. Its semantics iden-
tifies some set V of values (like states, closures, double precision reals) and 
specifies how a program p transforms one value Vl to another V2; we may 
write 
(4.1) 
for this without committing ourselves to the details of the semantics and 
without necessarily imposing determinacy (that p f- Vl "-t V2 and p f- Vl "-t Va 
imply V2 = va). 
In a similar way, a program analysis identifies the set L of properties (like 
shapes of states, abstract closures, lower and upper bounds for reals) and 

210 
4 Abstract Interpretation 
specifies how a program p transforms one property II to another l2; we may 
write 
(4.2) 
for this without committing ourselves to the method used for specification 
of the analysis. However, unlike what is the case for the semantics, it is 
customary to require I> to be deterministic and thereby define a function; 
this will allow us to write jp(h) = l2 to mean p f- h I> l2. 
In the rest of this section we shall show how to relate the semantics to the 
analysis. We shall present two approaches based on correctness relations and 
representation functions, respectively. In both cases we shall define a notion 
of correctness of the analysis with respect to the semantics and we shall show 
that the two notions are equivalent. 
This is a mundane approach in the sense that it only applies to analyses 
where properties directly describe sets of values. This is the case for the 
Constant Propagation Analysis of Section 2.3, the Shape Analysis of Section 
2.6 and the Control Flow Analysis of Chapter 3 but it is not the case for 
the Live Variable Analysis of Section 2.1 where properties are related to 
relations between values. In the literature, the terms first-order analyses 
versus second-order analyses have been used to differentiate between these 
classes of analyses. It is important to stress that the development of Sections 
4.2 to 4.5 apply equally well to both classes. 
We begin by showing how the development of Chapters 2 and 3 can be 
rephrased in the style of (4.1) and (4.2). 
Example 4.1 Consider the WHILE language of Chapter 2. Recall that 
the semantics is a Structural Operational Semantics with transitions of the 
forms (8, a) --+ (8', a') and (8, a) --+ a', where 8 and 8' are statements of 
Stmt and a and a' are states of State = Var --+ Z. With 8* being the 
program of interest we shall now write 
for the reflexive transitive closure of the transition relation, i.e. for: 
Note that the set V of values is the set State. 
We shall now consider the Constant Propagation Analysis of Section 2.3. 
Recall that the analysis of 8* gives rise to a set of equations CP= formulated 
in terms of an instance of a Monotone Framework: the properties L of interest 
are given by SWeep = (Var* --+ ZTh, E is {init(8*)}, F is flow(8*), and t 
is Ax.T. Further recall that a solution to the equations is a pair (CPo,CP.) 

4.1 A Mundane Approach to Corr~ctness 
211 
of mappings CPo: Lab* ~ Statecp and CP. : Lab* ~ statecp satisfying 
the equations. Given a solution (CPo, CP.) to Cp= we take 
S* I- 0'1 [> 0'2 
to mean that: 
Thus for a program S* with isolated entries, 0'1 is the abstract state associated 
with the entry point of S*, and 0'2 is the abstract state associated with the 
exit points; we use the least upper bound operation (U) on the complete 
lattice Statecp to combine the contributions from the (possibly several) exit 
points of S*. 
â¢ 
Example 4.2 Consider the FUN language of Chapter 3. Recall that the 
semantics is given by a Structural Operational Semantics with transitions 
of the form p I- ie ~ ie' where p is an environment (an element of Env = 
Var ~fin Val) and ie and ie' are intermediate expressions from IExp. Now 
let e* be the closed expression of interest. We shall write 
to mean that e* when given the argument VI will evaluate to the value V2, 
i.e. that 
[ ] I- (e* Vfl )Â£2 -7 * V~2 
where Â£1 and Â£2 are fresh labels. Note that the set V of values now is the set 
Val. 
We shall next consider the pure Control Flow Analysis of Section 3.1. Recall 
that the result of analysing the expression e* is a pair (C, jJ) satisfying (C, jJ) 1= 
e* as defined in Table 3.1. Here C is an element of Ciclle = Lab* -7 "Vai 
and P is an element of ~ 
= Var* ~ "Vai where "Vai = P(TerII4). For 
this analysis we shall take the properties L of interest to be pairs (p, v) of 
~ 
x 'Vai. and assume that (C, jJ) 1= (e* Cll )l2 for some constant c. Then 
we define 
e* I- (Pi, VI) [> (p2, V2) 
to mean that when e* is given an argument with property (PI, VI) then the 
result of the application will have property (p2, V2): 
C(Â£l) = VI A C(Â£2) = V2 A PI = P2 = P 
Note that the "dummy" constant c used as an argument to e* is used as a 
place holder for all potential arguments being described by VI; for this idea 
to work it is important that the analysis of c puts no constraints on (C, jJ) as 
in indeed the case for the specification in Table 3.1. 
â¢ 

212 
4 Abstract Interpretation 
4.1.1 
Correctness Relations 
Every program analysis should be correct with respect to the semantics. For a 
class of (so-called first-order) program analyses this is established by directly 
relating properties to values using a correctness relation: 
R: V x L ~ {true, false} 
The intention is that v R 1 formalises our claim that the value v is described 
by the property 1. 
Correctness formulation. To be useful one has to prove that the 
correctness relation R is preserved under computation: if the relation holds 
between the initial value and the initial property then it also holds between 
the final value and the final property. This may be formulated as the impli-
cation 
and is also expressed by the following diagram: 
p f-
R 
R 
p 
f-
[> 
A relation R satisfying a condition like this is often called a logical relation 
and the implication is sometimes written (p f- . ~ Â·)(R ~ R)(p f- . [> .). 
The theory of Abstract Interpretation comes to life when we augment the set 
of properties L with a preorder structure and relate this to the correctness 
relation R. The most common scenario is when L = (L,!;;;;, U, n, 1.., T) is a 
complete lattice with partial ordering!;;;; (see Appendix A). We then impose 
the following relationship between Rand L: 
v R h 1\ h !;;;; 12 
=> 
v R 12 
("11 E L' ~ L : v R l) 
=> 
v R (nL') 
(4.4) 
(4.5) 
Condition (4.4) says that the smaller the property is with respect to the 
partial ordering, the better (Le. more precise) it is. This is an "arbitrary" 
decision in the sense that we could instead have decided that the larger the 
property is, the better it is, as is indeed the case in much of the literature on 

4.1 A Mundane Approach to Correctness 
213 
Data Flow Analysis; luckily the principle of duality from lattice theory (see 
the Concluding Remarks) tells us that this difference is only a cosmetic one. 
Condition (4.5) says that there is always a best property for describing a 
value. This is important for having to perform only one analysis (using the 
best property, Le. the greatest lower bound of the candidates) instead of 
several analyses (one for each of the candidates). Recall from Appendix A 
that a subset Y of L is a Moore family if and only if (ny') E Y for all subsets 
Y' of Y. We can then see that condition (4.5) is equivalent to the demand 
that {II v R I} is a Moore family. 
Condition (4.5) has two immediate consequences: 
vRT 
v R h 1\ v R 12 
=> v R (h n l2) 
The first formula says that T describes any value and the second formula says 
that if we have two descriptions of a value then their greatest lower bound is 
also a description of the value. 
Example 4.3 Returning to the Constant Propagation Analysis of Exam-
ple 4.1 we can now specify the relation 
Rcp : State x SWecp -t {true, false} 
between the values (Le. the states) and the properties (i.e. the abstract 
states): 
u Rcp u iff 'Vx E FV(S*) : (u(x) = T V u(x) = u(x)) 
Thus u may map some variables to T but if u maps a variable x to an element 
in Z then this must also be the value of u(x). 
Let us observe that the conditions (4.4) and (4.5) are fulfilled by the Con-
stant Propagation Analysis. Recall from Section 2.3 that (SWecp, !;cp) is 
a complete lattice with the ordering !;cP. It is then straightforward to verify 
that (4.4) and (4.5) do indeed hold. (Also compare with Exercise 2.7.) 
â¢ 
Example 4.4 For the Control Flow Analysis mentioned in Example 4.2 
we shall define 
RCFA : Val x (:En; x 'Vai) -t {true, false} 
to be the relation V of Section 3.2: 
V RCFA (p, v) iff v V (p, v) 

214 
4 Abstract Interpretation 
Recall that we have two kinds of values v E Val, constants c and closures 
close t in p, and that V is given by: 
V C ~) . ff { true 
v 
p,v I 
t E V 1\ \/x E dom(p) : p(x) V (p,p(x)) 
if V = c 
if v = close t in p 
The correctness condition (4.3) can be reformulated as 
(VI V (p,VI) 1\ []I- (e* Vf1)Â£2 -t* V~2 1\ (C,p) 1= (e* C(1)Â£2 1\ 
C(Â£t) = VI 1\ C(Â£2) = V2) :::} 
V2 V (p, V2) 
and it follows from the correctness result established by Theorem 3.10 in 
Section 3.2 (see Exercise 4.3). 
Finally, let us observe that the Control Flow Analysis also satisfies the con-
ditions (4.4) and (4.5). For this we shall equip &v x Val with the partial 
ordering ~CFA defined by: 
(PI,Vt) ~CFA (P2,V2) iff VI ~ V2 1\ \/x: PI (x) ~ P2(X) 
This will turn &v x Val into a complete lattice. By induction on v E Val 
one can then easily prove that (4.4) and (4.5) are fulfilled. 
_ 
4.1.2 
Representation Functions 
An alternative approach to the use of a correctness relation R : V x L -t 
{true, false} between values and properties is to use a representation function: 
(3:V-tL 
The idea is that (3 maps a value to the best property describing it. The 
correctness criterion for the analysis will then be formulated as follows: 
This is also expressed by the following diagram: 
p 
I-
(3 1 
1 (3 
p 
I-
I> 
Thus the idea is that if the initial value VI is safely described by h then the 
final value V2 will be safely described by the result h of the analysis. 

4.1 A Mundane Approach to Correctness 
215 
Equivalence of correctness formulations. Lemma 4.5 below 
shows that the formulations (4.3) and (4.6) of the correctness of the analysis 
are indeed equivalent (when Rand (3 are suitably related). To establish 
this we shall first show how to define a correctness relation R{3 from a given 
representation function (3: 
v R{3 l iff (3( v) I;;;; l 
Next we show how to define a representation function (3 R from a correctness 
relation R: 
Lemma 4.5 
(i) Given (3 : V -t L, then the relation R{3 : V x L -t {true, false} satisfies 
conditions (4.4) and (4.5), and furthermore (3Rp = (3. 
(ii) Given R : V x L -t {true,false} satisfying conditions (4.4) and (4.5), 
then (3R is well-defined and R{3R = R. 
Hence the two formulations (4.3) and (4.6) of correctness are equivalent. 
_ 
Proof To prove (i) we first observe that condition (4.4) is immediate since [;;; is 
transitive. Condition (4.5) is immediate because when f3(v) is a lower bound for L' 
we have f3(v) [;;; nL'. The calculation f3Rp(V) = n{ll v R~ l} = n{ll f3(v) [;;; l} = 
f3(v) then concludes the proof of (i). 
To prove (ii) we observe that from v R l we get f3R(V) [;;; l and hence v R(3R l. 
Conversely, from v R~R l we get f3R(V) [;;; lj writing L' = {ll v R l} it is clear that 
(4.5) gives v R (nL') and this amounts to v R (f3R(V))j we then get the desired 
result v R l by (4.4). 
â¢ 
Motivated by these results we shall say that the relation R is generated by 
the representation function (3 whenever v R l is equivalent to (3(v) I;;;; l. This 
relationship is illustrated in Figure 4.1: The relation R expresses that v is 
described by all the properties above (3(v) and (3 expresses that among all 
the properties that describe v, (3(v) is the best. 
Example 4.6 For the Constant Propagation Analysis studied in Exam-
ples 4.1 and 4.3 we shall define 
(3cp : State -t SWecp 
as the injection of State into SWecp: (3cp(cr) = >.x.cr(x). It is straightfor-
ward to verify that Rcp is generated by (3cp, i.e. 
a Rcp (j Â¢} (3cp (cr) I;;;;cp (j 
using the definition of the ordering I;;;;cp on SWeep. 
-

216 
4 Abstract Interpretation 
R 
- -
Figure 4.1: Correctness relation R generated by representation function (3. 
Example 4.7 For the Control Flow Analysis studied in Examples 4.2 and 
4.4 we shall define 
(3CFA : Val -+ ~ 
x Vai 
inductively ~n the structure of values v E Val: 
(3 
() 
{ (Ax.0,0) 
if v = C 
CFA V = 
{(3 tFA (p), {t}) if v = close tin p 
The first clause reflects that we do not collect constants in a pure O-CFA 
analysis. In the second clause we only have one closure so the abstract value 
will be a singleton set and we construct the associated "minimal" abstract 
environment by extending (3CFA to operate on environments. To do that 
we shall "merge" all the abstract environments occurring in U{(3CFA{p{X)) I 
x E Var*}i this reflects that the O-CFA analysis uses one global abstract 
environment to describe all the possible local environments of the semantics. 
So we define (3tFA : Env -+ ~ 
by: 
(3tFA{P) (x) = U{py{x) I (3CFA{p{y)) = (jJy,1ly) and y E dom{p)} 
U {V., if x E dom{p) and (3CFA{p{X)) = (p." V.,) 
o otherwise 
To show that RCFA is generated by (3CFA we have to show that: 
V RCFA (p, v) {:} (3CFA (v) ~CFA (p, v) 
This follows by induction on v E Val and we leave the details to Exercise 
4.4. 
â¢ 

4.1 A Mundane Approach to Correctness 
217 
4.1.3 
A Modest Generalisation 
We shall conclude this section by performing a modest generalisation of the 
development performed so far. A program p specifies how one value V1 is 
transformed into another value V2: 
Here V1 E V1 and V2 E V2 and we shall subsequently refrain from imposing 
the condition that V1 = V2; thus we shall allow the programs to have differ-
ent "argument" and "result" types - for example, this will be the case for 
most functional programs. The analysis of p specifies how a property h is 
transformed into a property l2: 
Here h E L1 and l2 E L2 and again we shall refrain from imposing the 
restriction that L1 = L2. As previously argued it is natural to demand that 
p I- h I> l2 specifies a function 
fp: L1 -t L2 
given by fp(h) = l2 iff pI- h I> h. 
Turning to the correctness conditions we shall now assume that we have two 
correctness relations, one for V1 and L1 and one for V2 and L 2 : 
R1 : V1 x L1 -t {true,false} generated by (31 : V1 -t L1 
R2 : V2 x L2 -t {true, false} generated by (32 : V2 -t L2 
Correctness of fp now amounts to 
for all V1 E V1, V2 E V2 and h E L1. Using the concept of logical relations 
(briefly mentioned above) this can be written as: 
To be precise, "-+ (R1 """* R2) f means that: 
Higher-order formulation. We can now ask whether the relation 
R1 """* R2 defined above is a correctness relation. Lemma 4.8 below shows 
that this is indeed the case and furthermore that we can find a representation 
function (3 such that R1 """* R2 is generated by (3. The representation function 

218 
4 Abstract Interpretation 
{3 can be defined from the representation functions {31 and {32 and it will be 
denoted {31 --* {32: 
({31 --* (32)(~ ) = '\h. U{{32(V2) I {31 (VI) ~ 11 /\ VI ~ V2} 
Lemma 4.8 If Ri is a correctness relation for Vi and Li that is generated 
by the representation function {3i : Vi -t Li (for i = 1,2) then Rl --* R2 
is a correctness relation and it is generated by the representation function 
{31 --* {32. 
â¢ 
Proof We shall prove ~ (Rl -+ R2) f {::} ({:h -+ (82)(~) !;;; f. We calculate: 
Â«(31 -+ (32)(~) !;;; f 
{::} VII: U{,82(V2) I ,81 (VI) !;;; II /I. VI ~ V2} !;;; f(lI) 
{::} 
Vh,Vl,V2: (,8I(VI) !;;; h /I. VI ~ V2 =? ,82(V2) !;;; f(lI)) 
{::} 
VII, VI, V2 : (VI RI h /I. VI ~ V2 => V2 R2 f(h)) 
{::} 
~ (Rl -+ R2) f 
Note that it now follows (from Lemma 4.5) that if each Hi satisfies conditions (4.4) 
and (4.5) then so does RI -+ R2. 
â¢ 
Example 4.9 Consider the program plus with the semantics given by 
plus I- (ZI, Z2) ~ ZI + Z2 
where ZI, Z2 E Z. A very precise analysis might use the complete lattices 
(P(Z),~) and (P(Z x Z),~) as follows: 
fplus(ZZ) = {ZI +z21 (ZI,Z2) E ZZ} 
where ZZ ~ Z x Z. Consider now the correctness relations Rz and Rzxz 
generated by the representation functions: 
(3z(Z) = {z} 
(3ZXZ(ZI, Z2) 
= 
{(ZI' Z2)} 
The correctness of the analysis of plus can now be expressed by 
VZ1,Z2,Z,ZZ: plus I- (ZI,Z2) ~ Z /\ (ZI,Z2) Rzxz ZZ =? Z Rz fplus(ZZ) 
or more succinctly 
(plus I- . ~ .) (Rzxz --* Rz) fplus 
The representation function {3zxz --* {3z satisfies 
({3zxz --* (3z)(P 1-. ~.) = '\ZZ.{z I (ZI,Z2) E ZZ /\ pI- (Z},Z2) ~ z} 
so the correctness can also be expressed as ({3zxz --* Ih)(plus I- . ~ .) ~ 
fplus' 
â¢ 

4.2 Approximation of Fixed Points 
219 
"'~'" 
[-oo,oo[ 
.': 
'. 
.' 
..::'~_l'OOJ 
."" 
[O,ooJ 
.â¢â¢...... [l,ooJ 
[2,2J 
Figure 4.2: The complete lattice Interval = (Interval, ~). 
The above example illustrates how the abstract concepts can give a more 
succinct formulation of the correctness of the analysis. In the following we 
shall see several cases where we move freely between what we may call a 
"concrete" formulation of a property and an "abstract" formulation of the 
same property. And we shall see that the latter often will allow us to reuse 
general results so that we do not have to redevelop parts of the theory for 
each application considered. 
4.2 
Approximation of Fixed Points 
It should be clear by now that complete lattices playa crucial role in program 
analysis and in the remainder of this chapter we shall tacitly assume that 
property spaces such as L and M are indeed complete lattices. We refer to 
Appendix A for the basic notions of complete lattices and monotone func-
tions. The following example introduces an interesting complete lattice that 
forms the basis of many analyses over the integers. 
Example 4.10 We shall now present a complete lattice that may be used 
for Array Bound Analysis, i.e. for determining if an array index is always 
within the bounds of the array - if this is the case then a number of run-time 
checks can be eliminated. 
The lattice (Interval,~) of intervals over Z may be described as follows. 

220 
4 Abstract Interpretation 
The elements are 
where the ordering $ on Z is extended to an ordering on Z' = Z U { -oo,oo} 
by setting -00 $ z, Z $ 00, and -00 $ 00 (for all Z E Z). Intuitively, .1. 
denotes the empty interval and [Zl, Z2] is the interval from Zl to Z2 including 
the end points if they are in Z. We shall use int to range over elements of 
Interval. 
The partial ordering!; on Interval is depicted in Figure 4.2; the idea is that 
intl !; in~ corresponds to {z I Z is in intd <; {z I Z is in in~} where the 
meaning of "is in" should be immediate. To give a succinct definition of the 
partial ordering we define the infimum and supremum operations on intervals 
as follows: 
inf(int) 
{ 
00 if int = .1. 
= 
if int = [Zl, Z2] 
Zl 
sup(int) 
{ 
-00 if int = .1. 
= 
if int = [Zl' Z2] 
Z2 
This allows us to define: 
intl !; in~ iff 
inf(in~) $ inf(intl) 1\ sUP(intl) $ sup(in~) 
We claim that (Interval, !;) is indeed a complete lattice. We shall prove this 
by showing that each subset of Interval has a least upper bound and then 
refer to Lemma A.2 of Appendix A to get that (Interval,!;) is a complete 
lattice. So let Y be a subset of Interval. The idea is that each interval int 
of Y should be "contained in" the interval U Y defined by: 
{ 
.1. 
if Y <; {.1.} 
U Y = [ inf' {inf( int) lint E Y} , sup' {sup( int) lint E Y} ] 
otherwise 
where inf' and sup' are the infimum and supremum operators on Z' corre-
sponding to the ordering $ on Z'; they are given by inf'(0) = 00, inf'(Z) = z' 
if z' E Z is the least element of Z, and inf'(Z) = -00 otherwise; and simi-
larly sup'(0) = -00, sup'(Z) = z' if z' E Z is the greatest element of Z, and 
sup'(Z) = 00 otherwise. It is now straightforward to show that U Y is indeed 
the least upper hound of Y. 
â¢ 
Given a complete lattice L = (L, !;, U, n, .1., T) the effect of a program, p, in 
transforming one property, h, into another, 12 , i.e. p I- h I> 12 , is normally 
given by an equation 

4.2 Approximation of Fixed Points 
221 
for a monotone function I : L -t L dependent on the program p. Note 
that the demand that I is monotone is very natural for program analysis; it 
merely says that if l~ describes at least the values that h does then also I(lD 
describes at least the values that l(lt) does. 
For recursive or iterative program constructs we ideally want to obtain the 
least fixed point, Jfp(f), as the result of a finite iterative process. However, 
the iterative sequence (r(1.))n need not eventually stabilise nor need its least 
upper bound necessarily equal Jfp(f). This might suggest considering the 
iterative sequence (fn(T))n and, even when it does not eventually stabilise, 
we can always terminate the iteration at an arbitrary point in time. While 
this is safe (thanks to condition (4.4) of Section 4.1) it turns out to be grossly 
imprecise in practice. 
Fixed points. We shall begin by recalling some of the properties of fixed 
points of monotone functions over complete lattices; we refer to Appendix 
A for the details of this development. So consider a monotone function I : 
L -t L on a complete lattice L = (L,!;, U, n, 1., T). A fixed point of I is an 
element 1 E L such that 1(1) = 1 and we write 
Fix(f) = {I I 1(1) = I} 
for the set of fixed points. The function I is reductive at 1 if and only if 
1(1) !; 1 and we write 
Red(f) = {II 1(1) !; I} 
for the set of elements upon which I is reductive; we shall say that I itself 
is reductive if Red(f) = L. Similarly, the function I is extensive at I if and 
only if 1(1) ~ 1 and we write 
Ext(f) = {I I 1(1) ~ I} 
for the set of elements upon which I is extensive; we shall say that I itself is 
extensive if Ext(f) = L. 
Since L is a complete lattice it is always the case that the set Fix(f) will have 
a greatest lower bound in L and we denote it by Jfp(f); this is actually the 
least fixed point of I because Tarski's Theorem (Proposition A.I0) ensures 
that: 
Ifp(f) = n 
Fix(f) = n 
Red(f) E Fix(f) ~ Red(f) 
Similarly, the set Fix(f) will have a least upper bound in L and we denote 
it by gfp(f); this is actually the greatest fixed point of I because Tarski's 
Theorem ensures that: 
gfp(f) = U Fix(f) = U Ext(f) E Fix(f) ~ Ext(f) 
In Denotational Semantics it is customary to iterate to the least fixed point by 
taking the least upper bound ofthe sequence (r(.l))n. However, we have not 

222 
4 Abstract Interpretation 
T 
gfp(f) 
Fix(f) - -
Ifp(f) 
Unr(J..) 
Ext(f) - - - - -
J.. 
Figure 4.3: Fixed points of f. 
imposed any continuity requirements on f (e.g. that f(Un In) = Un(f(ln)) 
for all ascending chains (In)n) and consequently we cannot be sure to actually 
reach the fixed point. In a similar way one could consider the greatest lower 
bound of the sequence (r(T))n. One can show that 
as is illustrated in Figure 4.3; indeed all inequalities (Le. !;) can be strict 
(Le. f:.). 
4.2.1 
Widening Operators 
Since we cannot guarantee that the iterative sequence (fn(J..))n eventually 
stabilises nor that its least upper bound necessarily equals Ifp(f), we must 
consider another way of approximating Jfp(f). The idea is now to replace 
it by a new sequence (fv)n that is known to eventually stabilise and to do 
so with a value that is a safe (upper) approximation of the least fixed point. 
The construction of the new sequence is parameterised on the operator '\7, 
called a widening operator; the precision of the approximated fixed point as 
well as the cost of computing it depends on the actual choice of widening 
operator. 

4.2 Approximation of Fixed Points 
223 
Upper bound operators. In preparation for the development, an 
operator 0 : L x L -t L On a complete lattice L = (L,!;) is called an upper 
bound operator if 
for all h,12 E L, i.e. it always returns an element larger than both its ar-
guments. Note that we do not require 0 to be monotone, commutative, 
associative, nor absorptive (i.e. that I 0 I = 1). 
Let (In)n be a sequence of elements of L and let Â¢J : L x L -t L be a total 
function on L. We shall now use Â¢J to construct a new sequence (It)n defined 
by: 
ifn = Â° 
'" 
{In 
I = 
'" 
n 
In-l Â¢J In 
if n > Â° 
The following result expresses that any sequence can be turned into an as-
cending chain by an upper bound operator: 
Fact 4.11 If (In)n is a sequence and 0 is an upper bound operator then 
(l~)n is an ascending chain; furthermore l~ ;:;;! U{lo, h,Â· .. , In} for all n. 
_ 
Proof To prove that (l~)n is an ascending chain it is sufficient to prove that 
l~ ~ I~H for all n. If n = 0 we calculate l~ = 10 !;;; 10 0 h = l~. For the inductive 
step we have l~ ~ l~ 0 In+l = I~H as required. 
To prove that l~ ~ U{ 10, h, ... , In} we first observe that it holds trivially for 
n = O. For the inductive step we have l~+! = l~ 0 In+! ;;;J U{lo, h, ... ,In} U In+! = 
U{lo, h,Â·Â·Â· ,In, In+d and the result follows. 
â¢ 
Example 4.12 Consider the complete lattice (Interval,!;) of Figure 4.2 
and let int be an arbitrary but fixed element of Interval. Consider the 
following operator oint defined On Interval: 
. t oint . +_ 
{intl U in~ if intl C int V in~ !; intl 
m 1 
mv.o! = 
[-00,00] 
'otherwi;e 
Note that the operation is not symmetric: for int = [0,2] we e.g. have 
[1, 2]oint[2, 3] = [1,3] whereas [2, 3] oint [1, 2] = [-00,00]. 
It is immediate that oint is an upper bound operator. Consider nOw the 
sequence: 
[0,0], [1, 1], [2,2], [3,3], [4,4], [5,5],Â·Â·Â· 
If int = [0,00], then the upper bound operator will transform the above 
sequence into the ascending chain: 
[0,0], [0, 1], [0, 2], [0,3], [0,4], [0,5],Â·Â·Â· 

224 
4 Abstract Interpretation 
Red(f) -
Jfp(f) 
f~ 
f~ 
f~ = 1.. 
Figure 4.4: The widening operator V applied to f. 
However, if int = [0,2], then we will get the following ascending chain 
[0,0], [0, 1], [0,2], [0,3], [-00,00], [-00,00],'" 
which eventually stabilises. 
â¢ 
Widening operators. We can now introduce a special class of upper 
bound operators that will help us to approximate the least fixed points: An 
operator V : L x L -+ L is a widening operator if and only if: 
â¢ it is an upper bound operator, and 
â¢ for all ascending chains (In)n the ascending chain (l~)n eventually sta-
bilises. 
Note that it follows from Fact 4.11 that (l~)n is indeed an ascending chain. 
The idea is as follows: Given a monotone function f : L -+ L on a com-
plete lattice L and given a widening operator V on L, we shall calculate the 
sequence (fv)n defined by 
ifn = Â° 
if n > Â° 
1\ f(f~-l) !;;;; f~-l 
otherwise 

4.2 Approximation of Fixed Points 
225 
As in Fact 4.11 it follows that this is an ascending chain and Proposition 4.13 
below will ensure that this sequence eventually stabilises. From Fact 4.14 
below we shall see that this means that we will eventually have f (fW) ~ fW 
for some value of m (corresponding to the second clause in the definition 
of I~). This means that I is reductive at fW and from Tarski's Theorem 
(Proposition A.10) we then know that fW :;;;! Ifp(f) must be the case; hence 
we take 
lfpv(f) = fW 
as the desired safe approximation of Jfp(f). This is illustrated in Figure 4.4. 
We shall now establish the necessary results. 
Proposition 4.13 
If \l is a widening operator then the ascending chain (f~)n even-
tually stabilises. 
In preparation for the proof of Proposition 4.13 we shall first show the fol-
lowing technical result: 
Fact 4.14 If \l is a widening operator then: 
(i) the sequence (f~)n is an ascending chain; 
(ii) if l(fv) ~ fv for some m then the sequence (f~)n eventually stabilises 
and furthermore 'fin > m : f~ = fW and Un f~ = fW; 
(iii) if (f~)n eventually stabilises then there exists an m such that f(fW) ~ 
Iv; and 
(iv) if (f~)n eventually stabilises then Un f~ :;;;! Ifp(f). 
These claims also hold if \l is just an upper bound operator. 
â¢ 
Proof The proof of (i) is analogous to that of Fact 4.11 so we omit the details. 
To prove (ii) we assume that l(fv) ~ Iv for some m. By induction on n > m 
we prove that Iv = Iv: for n = m + 1 it follows from the assumption and for the 
inductive step we note that l(fv) ~ Iv will be the case so 1~+1 = Iv. Thus (fv) .. 
eventually stabilises and it follows that Un Iv = Iv. 
To prove (iii) we assume that (fv)n eventually stabilises. This means that there 
exists m such that \In > m : Iv = Iv. By way of contradiction assume that 
l(fv) ~ Iv does not hold; then Iv = 1;+1 = Iv V l(fv) ;! l(tv) and we have 
the desired contradiction. 
To prove (iv) we observe that (ii) and (iii) give Un Iv = Iv for some m where 
l(fv) ~ IvÂ· Hence Iv E Red(f) and by Tarski's Theorem (Proposition A.I0) 
this shows Iv ;! Jfp(t). 
â¢ 

226 
4 Abstract Interpretation 
We now turn to the proof of Proposition 4.13: 
Proof By way of contradiction we shall assume that the ascending chain (fv)n 
never stabilises; i.e.: 
\lno : 3n ~ no : Iv =1= I~o 
It follows that 1(f~-1) !; 1~-1 never holds for any n > 0; because if it did, 
then Fact 4.14 gives that (fv)n eventually stabilises and our hypothesis would be 
contradicted. This means that the definition of (fv)n specialises to: 
Iv = { ~~-1 V' f(/~-1) 
ifn = 0 
otherwise 
Now define the sequence {In)n by 
ifn = 0 
ifn> 0 
and note that {In)n is an ascending chain because (fv)n is an ascending chain (Fact 
4.14) and I is monotone. We shall now prove that 
\In: l~ = Iv 
by induction on n: for n = 0 it is immediate and for n > 0 we have l~ = 1~_1 V'ln = 
1~-1 V' 1(f~-1) = Iv. Since {In)n is an ascending chain and V' is a widening op-
erator it follows that the sequence {l~)n eventually stabilises, i.e. (fv)n eventually 
stabilises. This provides the desired contradiction and proves the result. 
_ 
Example 4.15 Consider the complete lattice (Interval, !;) of Figure 4.2. 
Let K be a finite set of integers, e.g. the set of integers explicitly mentioned 
in a given program. We shall now define a widening operator \1K based on 
K. The idea is that [Zl,Z2] \1K [Z3,Z4] is something like 
where LB(zl, zs) E {zd UK U {-oo} is the best possible lower bound and 
UB(Z2, Z4) E {Z2} UK U {oo} is the best possible upper bound. In this way 
a change in any of the bounds of the interval [Zl, Z2] can only take place in a 
finite number of steps (corresponding to the elements of K). 
For the precise definition we let Zi E Z' = Z U { -oo,oo} and write: 
{ 
Zl 
if Zl ~ Z3 
LBK(Zl,Z3) 
k 
if Z3 < Zl A k = max{k E K I k ~ Z3} 
-00 if Z3 < Zl A Vk E K: Z3 < k 
{ 
Z2 
if Z4 ~ Z2 
UBK(Z2, Z4) 
k 
ifz2 <z4 A k = min{ k E K I Z4 ~ k} 
00 if Z2 < Z4 A Vk E K: k < Z4 

4.2 Approximation of Fixed Points 
We can now define \7 = \7K by: 
{ 
1.. 
if intl = in~ = 1.. 
intl \7 in~ = 
[LBK(inf(intd,inf(in~)),UBK(sup(intl),sup(in~))] 
otherwise 
As an example consider the ascending chain (intn)n: 
[0,1], [0, 2], [0,3], [0,4], [0,5], [0,6], [0,7],Â· .. 
and assume that K = {3,5}. Then (int~)n will be the chain 
[0,1], [0,3], [0,3], [0,5], [0, 5], [0, 00], [0,00], ... 
227 
It is straightforward to show that \7 is indeed an upper bound operator. To 
show that it is a widening operator we consider an ascending chain (intn)n 
and must show that the ascending chain (int~)n eventually stabilises. By way 
of contradiction suppose that (int~)n does not eventually stabilise. Then at 
least one of the following properties will hold: 
(Vn : inf( int~) > -00) 
1\ 
inf(U n int~) = -00 
(Vn : sup( int~) < 00) 
1\ sup(U n int~) = 00 
Without loss of generality we can assume that the second property holds. 
Hence there must exist an infinite sequence nl < n2 < ... such that 
Vi: 00 > sup(int~+1) > sup(int~J 
and by finiteness of K there must be some j such that 
Vi ~ j : 00 > sup(int~;+1) > sup(int~J > max(K) 
where for the purpose of this definition we set max(0) = -00. Since we also 
have 
Â·tV 
Â·tV "Â·,,, 
zn nj+l = zn nj 
V zn"nj+l 
it must be the case that sup(intnj+1) > sup(int~) as otherwise sup(int~j+1) 
= sup(int~.). But then the construction yields 
3 
and we have the desired contradiction. This shows that \7 is a widening 
operator on the complete lattice of intervals. 
_ 

228 
4 Abstract Interpretation 
4.2.2 
Narrowing Operators 
Using the technique of widening we managed to arrive at an upper approx-
imation Iv of the least fixed point of 1. However, we have 1 (Jv) ~ Iv 
so 1 is reductive at Iv and this immediately suggests a way of improving 
the approximation by considering the iterative sequence (r(JV))n' Since 
Iv E Red(J) this will be a descending chain with r(Jv) E Red(J) and 
hence r (Jv) ~ 1Â£p(J) for all n. Once again we have no reason to believe 
that this descending chain eventually stabilises although it is of course safe 
to stop at an arbitrary point. This scenario is not quite the one we saw above 
but inspired by the notion of widening we can define the notion of narrowing 
that encapsulates a termination criterion. This development can safely be 
omitted on a first reading. 
An operator ~ : L x L --t L is a narrowing opemtor if: 
â¢ I2 ~ h => I2 ~ (II ~ I2) ~ h for all h, I2 E L, and 
â¢ for all descending chains (In)n the sequence (I~)n eventually stabilises. 
Note that we do not require ~ to be monotone, commutative, associative or 
absorptive. One can show that (I~)n is a descending chain when (In)n is a 
descending chain (Exercise 4.10). 
The idea is as follows: For Iv satisfying 1 (Jv) ~ Iv, i.e. Ifpv (J) = Iv, we 
now construct the sequence ([I]bJn by 
{ Iv 
ifn=O 
[1]1 = 
[1];;.-1 ~ 1([1];;.-1) if n > 0 
Lemma 4.16 below guarantees that this is a descending chain where all ele-
ments satisfy JÂ£p(J) ~ [J]1. Proposition 4.17 below tells us that this chain 
eventually stabilises so [J]~' = [J]~'+1 for some value m'. We shall therefore 
take 
1Â£p~(J) = [1]~' 
as the desired approximation of 1Â£p(J). The complete development is illus-
trated in Figures 4.4 and 4.5. We shall now establish the required results. 
Lemma 4.16 IT ~ is a narrowing operator and l(Jv) ~ Iv then ([I]:;Un 
is a descending chain in Red(J) and 
[1]1 ~ r(Jv) ~ Ifp(J) 
for all n. 
â¢ 
Proof By induction on n we prove: 
(4.7) 

4.2 Approximation of Fixed Points 
/ 
......... . 
... -. --.. - _ ... _. 
Red(f) 
[fl~ = fv 
[fl~ 
[!l~'-l 
229 
[!l~' = [fl~' +1 = ... 
Ifp(f) 
Figure 4.5: The narrowing operator ~ applied to f. 
For the basis (n = 0) it is immediate that 
r+1 (tv) ~ f([fl~) ~ [fl~ 
using that f(tv) ~ fv. By construction of [fl~+1 we then get 
and together these two results establish the basis of the induction proof. 
For the inductive step we may apply f to the induction hypothesis (4.7) and get 
since f is assumed to be monotone. Using the induction hypothesis we also have 
f([fl~) ~ [fl~+1 so we obtain: 
r+2(tv) ~ f([fl~+1) ~ [Jl~+l 
By construction of [fl~+2 we get 
and together these two results complete the proof of (4.7). 
From (4.7) it now follows that ([fl:;'.)n is a descending chain in Red(t). It also 
follows that r(tv) ~ [fl~ holds for n > 0 and it clearly also holds for n = O. 

230 
4 Abstract Interpretation 
From the assumption f(f~) !;;;; f~ it is immediate that f(fn(f~)) !;;;; r(f~) for 
n ~ 0 and hence r(f~) E Red(f). But then r(f~) ;! Ifp(f). This completes the 
proof. 
_ 
Proposition 4.17 
IT A is a narrowing operator and I (Iv) ~ Iv then the descending 
chain ([f]~On eventually stabilises. 
Proof Define the sequence (In)n by 
I - { f~ 
n -
f([I1:;.-l) 
ifn=O 
ifn> 0 
and note that this defines a descending chain because ([f]~)n is a descending chain 
and because f([f]?z..) !;;;; f~. Therefore the sequence (l~)n eventually stabilises. We 
now prove by induction on n that: 
l~ = [f]~ 
The basis (n = 0) is immediate. For the inductive step we calculate 1~+1 = 
l~ ~ In+1 = 
[f]~ ~f([f]~) = [/]:;.+1. It thus follows that ([fl~)n eventually 
stabilises. 
_ 
It is important to stress that narrowing operators are not the dual concept 
of widening operators. In particular, the sequence (lv)n may step outside 
Ext(l) in order to end in Red(l) , whereas the sequence ([J]a)n stays in 
Red(l) all the time. 
Example 4.18 Consider the complete lattice (Interval, ~) of Figure 4.2. 
Basically there are two kinds of infinite descending chains in Interval: those 
with elements of the form [-00, z] and those with elements of the form [z, 00] 
where Z E Z. Consider an infinite sequence of the latter form; it will have 
elements 
[Zl' 00], [Z2, 00], [Z3, 00],'" 
where Zl < Z2 < Z3 < .... The idea is now to define a narrowing operator 
AN that will force the sequence to stabilise when Zi 2: N for some fixed non-
negative integer N. Similarly, for a descending chain with elements of the 
form [-00, Zi] the narrowing operator will force it to stabilise when Zi ~ - N. 
Formally, we shall define A = AN by 

4.3 Galois Connections 
231 
where 
ZI = { 
inf(intl) if N < inf(in~) 1\ sup(in~) = 00 
inf(in~) otherwise 
Z2 = { 
sup(intl) if inf(in~) = -00 1\ sup(in~) < -N 
sup(in~) otherwise 
So consider e.g. the infinite descending chain ([n,oo])n 
[0,00], [1,00], [2,00], [3,00], [4,00], [5,00],Â· .. 
and assume that N = 3. Then the operator will give the sequence ([n, oo].:l)n: 
[0,00], [1, 00], [2, 00], [3,00], [3,00], [3,00],Â·Â· . 
Let us show that .6. is indeed a narrowing operator. It is immediate to verify 
that 
in~ !;;; intI implies in~ !;;; intI .6. in~ !;;; intI 
by cases on whether in~ = 1. or in~ '# l.. We shall then show that if (intn)n 
is a descending chain then the sequence (in~)n eventually stabilises. So 
assume that (intn)n is a descending chain. One can then show that (in~)n 
is a descending chain (Exercise 4.9). Next suppose by way of contradiction 
that (in~)n never eventually stabilises. It follows that there exists nl ~ Â° 
such that: 
in~ '# [-00,00] for all n ~ nl 
It furthermore follows for all n ~ nl that in~ must have: 
sup(in~) = 00 or inf(in~) = -00 
Without loss of generality let us assume that all n ~ nl have sup( in~) = 00 
and inf( in~) E Z. Hence there exists n2 ~ nl such that: 
inf(in~) > N for all n ~ n2 
But then in~ = in~2 for all n ~ n2 and we have the desired contradiction. 
This shows that .6. is a narrowing operator. 
â¢ 
4.3 
Galois Connections 
Sometimes calculations on a complete lattice L may be too costly or even 
uncomputable and this may motivate replacing L by a simpler lattice M. An 
example is when L is the powerset of integers and M is a lattice of intervals. 
So rather than performing the analysis p I- h I> h in L, the idea will be to 
find a description of the elements of L in M and to perform the analysis 

232 
4 Abstract Interpretation 
p I- ml [> m2 in M. To express the relationship between L and M it is 
customary to use an abstraction function 
a:L-+M 
giving a representation of the elements of L as elements of M and a concreti-
sation function 
"{:M-+L 
that expresses the meaning of elements of M in terms of elements of L. We 
shall write 
or 
L 
(L,a, ,,(,M) 
"{ ---a 
M 
for this setup and would expect that a and "{ should be somehow related. 
We shall study this relationship in the present section and then return to the 
connection between p I- h [> l2 and p I- ml [> m2 in Section 4.5; in Section 4.4 
we shall study the systematic construction of such relationships. 
We define (L, a, ,,(, M) to be a Galois connection between the complete lat-
tices (L,!;) and (M,!;) if and only if 
a : L -+ M and "( : M -+ L are monotone functions 
that satisfy: 
ao"{ 
C 
Am.m 
(4.8) 
(4.9) 
Conditions (4.8) and (4.9) express that we do not lose safety by going back 
and forth between the two lattices although we may lose precision. In the 
case of (4.8) this ensures that if we start with an element 1 E L we can first 
find a description a(l) of it in M and next determine which element "((a(l)) 
of L that describes a(l); this need not be 1 but it will be a safe approximation 
to l, i.e. 1 !; "((a(lÂ». This is illustrated in Figure 4.6. 
Example 4.19 Let P(Z) = (P(Z),~) be the complete lattice of sets of 
integers and let Interval = (Interval, !;) be the complete lattice of Figure 
4.2. We shall now define a Galois connection 
(P(Z), aZI, "(ZI, Interval) 
between P(Z) and Interval. 

4.3 Galois Connections 
233 
Figure 4.6: The Galois connection (L, a, '1, M). 
The concretisation function 'YZI : Interval..-.+ P(Z) is defined by 
'YzI(int) = {z E Z I inf(int) ~ z ~ sup(int)} 
where inf and sup are as in Example 4.10. Thus'YZI will extract the set of ele-
ments described by the interval, e.g. 'YZI([O, 3]) = {a, 1,2, 3} and 'YZI([O, 00]) = 
{z E Z I z ~ O}. 
The abstraction function aZI : P(Z) ..-.+ Interval is defined by 
Z 
{..L 
ifZ=0 
aZI( ) = 
[inf'(Z), sup'(Z)] otherwise 
where inf' and sup' are as in Example 4.10. Thus aZI will determine the 
smallest interval that includes all the elements of the set, e.g. aZI ( {a, 1, 3}) = 
[0,3] and aZI({2 * z I z > a}) = [2,00]. 
Let us verify that (P(Z), aZI, 'YZh Interval) is indeed a Galois connection. It 
is easy to see that aZI and 'YZI are monotone functions. We shall next prove 
that (4.8) holds, i.e. that 'YZI 0 aZI ;;:J AZ.Z. If Z t 0 we have: 
'YZI(aZI(ZÂ» = 
'YzI([inf'(Z),sup'(Z)]) 
= 
{z E Z I inf'(Z) ~ z ~ sup'(Z)} 
;2 Z 
For Z = 0 we trivially have 'YzI(azI(0Â» = 'YZI(..L) = 0 so we have proved 
(4.8). Intuitively, this condition expresses that if we start with a subset of Z, 
find the smallest interval containing it, and next determine the corresponding 
subset of Z, then we will get a (possibly) larger subset of Z than the one we 
started with. 

234 
4 Abstract Interpretation 
Finally, we shall prove (4.9), i.e. that aZI Â° 'YZI !; >.int.int. Consider first 
int = [Zl' Z2] where we have: 
aZI(-YzI([ZbZ2])) = aZI({z E Z I Zl ~ Z ~ Z2}) 
= [Zl,Z2] 
For int = 1.. we trivially have aZI(-YzI(1..)) = aZI(0) = 1.. so we have proved 
(4.9). Intuitively, the condition expresses that if we start with an interval, 
determine the corresponding subset of Z, and next find the smallest interval 
containing this set, then the resulting interval will include the interval we 
started with; actually we showed that the two intervals are equal. 
_ 
Adjunctions. There is an alternative formulation of the Galois connec-
tion (L, a, 'Y, M) that is frequently easier to work with. We define (L, a, 'Y, M) 
to be an adjunction between complete lattices L = (L,!;) and M = (M,!;) 
if and only if 
a : L -+ M and 'Y : M -+ L are total functions 
that satisfy 
a(l) !; m 
Â¢:} l!; 'Y(m) 
(4.10) 
for all l ELand m EM. 
Condition (4.10) expresses that a and 'Y "respect" the orderings of the two 
lattices: IT an element l E L is safely described by the element m EM, 
i.e. a(l) !; m, then it is also the case that the element described by m is safe 
with respect to l, i.e. l !; 'Y(m). 
Proposition 4.20 
(L, a, 'Y, M) is an adjunction if and only if (L, a, 'Y, M) is a Galois 
connection. 
Proof First assume that {L, 0:", M) is a Galois connection and let us show that 
it is an adjunction. So assume first that o:{l) I; mj since , is monotone we get 
,(o:{lÂ» I; ,(m)j using that ,oo:;;;J Al.l we then get II; ,(o:{lÂ» I; ,(m) as required. 
The proof showing that II; ,(m) implies o:{l) I; m is analogous. 
Next assume that (L,o:",M) is an adjunction and let us prove that it is a Galois 
connection. First we prove that, 0 0: ;;;J AU: for I E L we trivially have o:{l) I; o:{l) 
and using that o:{l) I; m => I I; ,(m) we get I I; ,(o:{lÂ» as required. The proof 
showing that 0: 0, I; Am.m is analogous. To complete the proof we also have to 
show that 0: and, are monotone. To see that 0: is monotone suppose that Ii I; hj 
we have already proved that, 0 0: ;;;J Al.l so we have Ii I; 12 I; ,(0:(12Â»j using that 
I I; ,(m) => o:{l) I; m we then get o:{h) I; 0:(12). The proof showing that, is 
monotone is analogous. 
â¢ 

4.3 Galois Connections 
235 
Galois connections defined by extraction functions. We 
shall now see that representation functions (introduced in Section 4.1) can be 
used to define Galois connections. So consider once again the representation 
function (3 : V -+ L mapping the values of V to the properties of the complete 
lattice L. It gives rise to a Galois connection 
(P(V), a, 'Y, L) 
between P(V) and L where the abstraction and concretisation functions are 
defined by 
a(V') = U{(3(v) I v E V'} 
'Y(l) 
= 
{v E V I (3(v) !;; l} 
for V' ~ V and l E L. Let us pause for a minute to see that this indeed 
defines an adjunction: 
a(V/) !;; l 
Â¢:> U{(3(v) I v E V'} !;; l 
Â¢:> Vv E V' : (3(v) !;; l 
Â¢:> V' ~ 'Y(l) 
It follows from Proposition 4.20 that we also have a Galois connection. It is 
also immediate that a( {v}) = (3( v) as illustrated by the diagram: 
'Y 
V 
A special case of the above construction that is frequently useful is when 
L = (P(D),~) for some set D and we have an extraction function 
'TJ: V -+ D 
mapping the values of V to their descriptions in D. We will then define the 
representation function (31/ : V -+ P(D) by (31/(v) = {'TJ(v)} and the Galois 
connection between P(V) and P(D) will now be written 
where 
(P(V),a1/,'Y1/' P(D)) 
a1/(V') = U{(31/(v) I v E V'} 
= {'TJ(v) I v E V'} 
'Y1/(D') = {v E V I (31/(v) ~ D/} = {v I 'TJ(v) E D/} 
for V' ~ V and D' ~ D. The relationship between 'TJ, (31/' a1/ and 'Y1/ is 
illustrated by the diagram: 

236 
4 Abstract Interpretation 
{O,+} 
{+} 
Figure 4.7: The complete lattice P{Sign) = (P{Sign), ~). 
P{V) â¢ 
â¢ P{D) 
oj/(jo 
v 
rJ 
D 
Example 4.21 Let us consider the two complete lattices (P(Z),~) and 
(P(Sign), ~) where Sign = {-, 0, +}; see Figure 4.7. The extraction function 
sign : Z -+ Sign 
simply defines the signs of the integers and is specified by: 
{ 
-
ifz<O 
sign(z) = Â° if z = 0 
+ ifz>O 
The above construction then gives us a Galois connection 
(P(Z),CÂ¥sign,')'sign, P{Sign)) 
with 
CÂ¥sign(Z) = {sign{z) I z E Z} 
')'sign{S) 
{z E Z I sign(z) E S} 
where Z ~ Z and S ~ Sign. 
_ 

4.3 Galois Connections 
237 
4.3.1 
Properties of Galois Connections 
We shall present three interesting results. The first result says that a Galois 
connection is fully determined by either one of the abstraction and concreti-
sation functions: 
Lemma 4.22 If (L, a, ,,(, M) is a Galois connection then: 
(i) a uniquely determines "( by "((m) = U{l I a(l) !;;;; m} and"( uniquely 
determines a by a(l) = n{m II !;;;; "((m)}. 
(ii) a is completely additive and "( is completely multiplicative. 
In particular a(1-) = 1- and "((T) = T. 
-
Proof To show (i) we shall first show that 'Y is determined by 0:. Since (L, 0:, 'Y, M) 
is an adjunction by Proposition 4.20 we have 'Y(m) = U{l I I r; 'Y(m)} = U{l I 
0:(1) r; m}. This shows that 0: uniquely determines 'Y: if both (L,0:,'Y1,M) and 
(L,0:,'Y2,M) are Galois connection then 'Y1(m) = U{ll a(l) r; m} = 'Y2(m) for all 
m and hence 'Y1 = 'Y2. 
Similarly, we have 0:(1) = n{m I 0:(1) r; m} = n{m II r; 'Y(m)} and this shows 
that 'Y uniquely determines 0:. 
To show (ii) consider L' ~ Lj using Proposition 4.20 we then have 
o:(U L') r; m 
{:} U L' r; 'Y(m) 
{:} 
VI E L' : I r; 'Y(m) 
{:} 
VI E L' : 0:(1) r; m 
{:} 
U{o:(l) II E L'} r; m 
and it follows that o:(U L') = U{ o:(l) II E L'}. 
The proof that 'Y(nM') = nh(m) 1m EM'} is analogous. 
â¢ 
Motivated by Lemma 4.22 we shall say that if (L, 0:, "(, M) is a Galois con-
nection then a is the lower adjoint (or left adjoint) of "( and that 'Y is the 
upper adjoint (or right adjoint) of a. 
The next result shows that it suffices to specify either a completely additive 
abstraction function or a completely multiplicative concretisation function in 
order to obtain a Galois connection: 
Lemma 4.23 If a : L -+ M is completely additive then there exists 
"( : M -+ L such that (L, a, ,,(, M) is a Galois connection. Similarly, if 
'Y : M -+ L is completely multiplicative then there exists a : L -+ M such 
that (L, a, ,,(, M) is a Galois connection. 
_ 

238 
4 Abstract Interpretation 
Proof Consider the claim for a and define "( by: 
"((m) 
= 
U{l' I a(l') ~ m} 
Then we have a(l) ~ m :::} l E {l' I a(l') ~ m} :::} l ~ "((m) where the last 
implication follows from the definition of "(. For the other direction we first observe 
that l ~ "((m) :::} a(l) ~ a("((m)) because a is completely additive and hence 
monotone. Now 
a ("((m)) 
a(U{l' I a(l') ~ m}) 
= 
U{a(l') I a(l') ~ m} 
~ m 
so l ~ "((m) :::} a(l) ~ m. It follows that (L, a, ,,(, M) is an adjunction and hence a 
Galois connection by Proposition 4.20. 
The proof of the claim for "( is analogous. 
â¢ 
The final result shows that we neither lose nor gain precision by iterating 
abstraction and concretisation: 
Fact 4.24 If (L,a,,,(,M) is a Galois connection then a 0 "( 0 a = a and 
"( 0 a 0 "( = "(. 
â¢ 
Proof We have Al.l ~ "( 0 a and since a is monotone we get a ~ a 0 ("( 0 a). 
Similarly (a 0 "() 0 a ~ a follows from a 0 "( ~ Am.m. Thus a 0 "( 0 a = a. 
The proof of "( 0 a 0 "( = "( is analogous. 
â¢ 
Example 4.25 As a somewhat more complex example consider the com-
plete lattices Interval = (Interval,~) and P(Sign) = (P(Sign),~) of Fig-
ures 4.2 and 4.7, respectively. 
Let us define a concretisation function ')'Is : P(Sign) -+ Interval by: 
')'IS({-,O,+}) 
')'Is({-,+}) 
')'Is ({-}) 
')'Is ({+}) 
= 
= 
= 
[-00,00] 
[-00,00] 
[-00, -1] 
[1,00] 
')'IS({-,O}) 
')'Is({O,+}) 
')'Is ( {O}) 
')'Is (0) 
[-00,0] 
= 
[0,00] 
= 
[0,0] 
.1 
To determine whether or not there exists an abstraction function 
aIS : Interval -+ P(Sign) 
such that (Interval, aIS, ')'Is, P(Sign)) is a Galois connection, we shall sim-
ply determine whether or not ')'Is is completely multiplicative: If it is, then 
Lemma 4.23 guarantees that there does indeed exist a Galois connection and 
Lemma 4.22 tells us how to construct the abstraction function. IT ')'Is is 

4.3 Galois Connections 
239 
not completely multiplicative then Lemma 4.22 guarantees that there cannot 
exist a Galois connection. In order to determine whether or not ')'Is is com-
pletely multiplicative we shall use Lemma A.4 of Appendix A: It is immediate 
to verify that P(Sign) is finite and that ')'Is satisfies conditions (i) and (ii) 
of Lemma A.4. To verify condition (iii) we need to compare ')'Is(81 n 82) to 
')'Is (81 ) n ')'Is (82 ) for all pairs (81,82) E P(Sign) x P(Sign) of incomparable 
sets of signs, i.e. all pairs in the following list: 
({-,O},{-,+}), ({-,O},{O,+}), ({-,O},{+}), 
({-,+},{O,+}), ({-,+},{O}), 
({O,+},{-}), 
({-},{O}), 
({-},{+}), 
({O},{+}) 
In checking the pair ({ - , O}, {-, + }) we calculate 
')'Is ( { -,O} n { -, + } ) 
')'Is ( {-, O}) n ')'Is ( {-, +}) = 
')'Is ({-}) 
= [-00,-1] 
[-00,0] n [-00,00] = 
[-00,0] 
and we deduce that ')'Is is not completely multiplicative. Hence according 
to Lemma 4.22 we cannot find any alS : Interval ---t P(Sign) such that 
(Interval, alS, ')'Is, P(Sign)) is a Galois connection. 
_ 
The mundane approach to correctness. We shall now return 
to further motivating that the connection between L and M should be a 
Galois connection. Recall from Section 4.1 that the semantic correctness of 
an analysis may be expressed by a correctness relation R between the values 
of V and the properties of L or it may be expressed by a representation 
function {3 mapping the values of V to their description in L. When we 
replace L with some other complete lattice M we would obviously like the 
correctness results still to hold. We shall now see that if there is a Galois 
connection (L, a, 'Y, M) between L and M then we can construct a correctness 
relation between V and M and a representation function from V to M. 
Let us first focus on the correctness relation. So let R : V x L ---t {true, false} 
be a correctness relation that satisfies the conditions (4.4) and (4.5) of Section 
4.1. Further let (L, a, 'Y, M) be a Galois connection between the complete 
lattices L and M. It is then natural to define 8 : V x M ---t {true, false} by 
v 8 m iff v R (-y(m)) 
and we shall now argue that this is a correctness relation between V and M 
fulfilling conditions corresponding to (4.4) and (4.5). We have 
(v 8 m1) 1\ m1 !; m2 
~ v R ('Y(m1)) 1\ 'Y(m1) !; 'Y(m2) 
~ v R ('Y(m2)) 
~ V8 m 2 

240 
4 Abstract Interpretation 
using that "( is monotone and that R satisfies condition (4.4); this shows that 
S also satisfies condition (4.4). Also for all M' ~ M we have 
("1m E M' : v S m) * (Vm E M': v R ("((m))) 
* vR(nh(m)lmEM'}) 
* v R b(nM')) 
* v S (nM') 
using that "( is completely multiplicative (Lemma 4.22) and that R satisfies 
condition (4.5); this shows that S also satisfies condition (4.5). Hence S 
defines a correctness relation between V and M. 
Continuing the above line of reasoning assume now that R is generated by the 
representation junction (3: V --+ L, i.e. v R 1 {:} (3(v) ~ l. Since (L,a,,,(,M) 
is a Galois connection and hence an adjunction (Proposition 4.20) we may 
calculate 
v S m {:} 
v R b(m)) 
{:} (3(v) ~ "((m) 
{:} 
(a Â° (3) (v) ~ m 
showing that S is generated by a Â° (3 : V --+ M. This shows how the Galois 
connection facilitates the definition of correctness relations and representa-
tion functions and concludes our motivation for why it is natural to require 
that the connection between Land M is specified by a Galois connection. 
4.3.2 
Galois Insertions 
For a Galois connection (L, a, ,,(, M) there may be several elements of M that 
describe the same element of L, i.e. "( need not be injective, and this means 
that M may contain elements that are not relevant for the approximation of 
L. 
The concept of Galois insertion is intended to rectify this: (L, a, ,,(, M) is a 
Galois insertion between the complete lattices L = (L,~) and M = (M,~) 
if and only if 
a : L --+ M and "( : M --+ L are monotone functions 
that satisfy: 
ao"( 
)..m.m 
Thus we now require that we do not lose precision by first doing a concretisa-
tion and then an abstraction. As a consequence M cannot contain elements 

4.3 Galois Connections 
241 
Figure 4.8: The Galois insertion (L, a, "I, M). 
that do not describe elements of L, i.e. M does not contain superfluous ele-
ments. 
Example 4.26 The calculations of Example 4.19 show that 
(P(Z), aZI, "IZI, Interval) 
is indeed a Galois insertion: we start with an interval, use 'YZI to determine 
the set of integers it describes and next use aZI to determine the smallest 
interval containing this set and we get exactly the same interval as we started 
with. 
â¢ 
The concept of a Galois insertion is illustrated in Figure 4.8 and is further 
clarified by: 
Lemma 4.27 For a Galois connection (L,a,"I,M) the following claims 
are equivalent: 
(i) 
(L, a, "I, M) is a Galois insertionj 
(ii) 
a is surjective: "1m EM: 3l E L : a(l) = mj 
(iii) 
"I is injective: Vml,m2 EM: "I(md = "I(m2) => ml = m2j and 
(iv) 
"I is an order-similarity: "1mb m2 EM: 
"I(ml) !;;;; "I(m2) Â¢:} ml !;;;; m2Â· 
â¢ 
Proof First we prove that (i) => (iii). For this we calculate 
where the last step follows from (i). 

242 
4 Abstract Interpretation 
Next we prove that (iii) => (ii). For m E M we have ')'(a{y(mÂ» = ')'(m) by Fact 
4.24 and hence a(')'(mÂ» = m by (iii). 
Now we prove that (ii) => (iv). That ml !; m2 => ')'(md !; ')'(m2) is immediate 
since,), is monotone. Next suppose that ')'(ml) !; ')'(m2)j by monotonicity of a this 
gives a(')'(mlÂ» !; a(')'(m2Â»j using (ii) we can write ml = a(h) and m2 = a(12) for 
some h, 12 E Lj using Fact 4.24 this then gives ml !; m2 as desired. 
Finally we prove that (iv) => (i). For this we calculate 
a(')'(mlÂ» !; m2 {:} ')'(md!; ')'(m2) {:} ml !; m2 
where the first step is using Proposition 4.20 and the last step is using (iv). This 
shows that a(')'(mtÂ» and ml have the same upper bounds and hence are equal. _ 
Lemma 4.27 has an interesting consequence for a Galois connection given by 
an extraction function 17 : V -t D: it is a Galois insertion if and only if 17 is 
surjective. 
Example 4.28 Consider the complete lattices (P{Z),~) and (P{Sign x 
Parity),~) where Sign = {-,O,+} as before and Parity = {odd, even}. 
Define the extraction function sign parity : Z -t Sign x Parity by: 
, 
't 
{ (sign{z), odd) 
if z is odd 
sIgn pan y(z) = (' () 
) 'f ' 
sIgn z , even 
1 z IS even 
This gives rise to a Galois connection (P{Z), asignparity, ')'signparity, P{Sign x 
Parity)). The property (O, odd) describes no integers so clearly sign parity is 
not surjective and we have an example of a Galois connection that is not a 
Galois insertion. 
_ 
Construction of Galois insertions. Given a Galois connection 
(L, a, ')', M) it is always possible to obtain a Galois insertion by enforcing 
that the concretisation function ')' is injective, Basically, this amounts to 
removing elements from the complete lattice M using a reduction operator 
c;:M-tM 
defined from the Galois connection. We have the following result: 
Proposition 4.29 
Let {L, a,,),, M) be a Galois connection and define the reduction 
operator c; : M -t M by 
c;{m) = n{m' I ')'{m) = ')'{m')} 
Then c;[M] = ({c;{m) I m EM}, !;M) is a complete lattice and 
(L, a, ')', c;[M]) is a Galois insertion. 

4.3 Galois Connections 
243 
Figure 4.9: The reduction operator ~: M -t M. 
This is illustrated in Figure 4.9: The idea is that two elements of M are 
identified by ~ if they map to the same value by 'Yj in particular, m and 
a(')'(m)) will be identified. 
In preparation for the proof we shall first show the following two results: 
Fact 4.30 Let (L,a,"/,M) be a Galois connection. Then 
alL] = ({a(l) Il E L},!;M) 
is a complete lattice. 
â¢ 
Proof Clearly a[L] ~ M is partially ordered by the ordering !;;;M of M = (M,!;;;M). 
We now want to show for M' ~ a[L] ~ M that 
Ua[L)M' = UMM' 
We first show that UM M' E a[L] which means that there exists l E L such that 
a(l) = UM M'. For this take l = UL 'Y[M'] which clearly exists in L. Since 
a 0 'Y 0 a = a (Fact 4.24) and a is completely additive (Lemma 4.22) we then have 
thus showing UM M' E a[L]. Since UM M' is the least upper bound of M' in M 
it follows that it also is in a[L]. By Lemma A.2 we then have the result. 
_ 
Fact 4.31 If (L, a, "/, M) is a Galois connection and 
~(m) = n{m' I ,,/(m') = 'Y(m)} 

244 
4 Abstract Interpretation 
then 
~(m) = ah(m)) 
and hence a[L] = ~[M]. 
-
Proof We have ~(m) !;;; a(-y(mÂ» because -y(m) = -y(a(-y(m))) using Fact 4.24. 
That a(-y(mÂ» !;;; ~(m) is equivalent to -y(m) !;;; -y(~(mÂ» using Proposition 4.20 and 
-y(~(mÂ» = nh(m') I -y(m') = -y(m)} = -y(m) follows from Lemma 4.22. 
Next consider a(l) for l E Lj by Fact 4.24 we have a(l) = a(-y(a(l))) and hence 
a(l) = ~(a(lÂ»j this shows alL] ~ ~[M]. Finally consider ~(m) for m E Mj then 
~(m) = a(-y(mÂ»j this shows ~[M] ~ alL]. 
_ 
We now turn to the proof of Proposition 4.29: 
Proof Facts 4.30 and 4.31 give that ~[M] = alL] is a complete lattice. Since 
(L, a, -y, M) is an adjunction (Proposition 4.20) it follows that also (L, a, -y, alL]) 
is. Since a is surjective on alL] it follows from Lemma 4.27 that we have a Galois 
insertion. 
â¢ 
Reduction operators defined by extraction functions. We 
shall now specialise the construction of Proposition 4.29 to the setting where 
a Galois connection (P(V),a'1'-Y'1' P(D)) is given by an extraction function 
'f/ : V -7 D. Then the reduction operator ~'1 is given by 
~'1(D') = D' n 'f/[V] 
where 'f/[V] = {'f/(v) I v E V} denotes the image of'f/ and furthermore ~'1[P(D)] 
is isomorphic (see Appendix A) to P('f/[V)) (see Exercise 4.14). The result-
ing Galois insertion will therefore be isomorphic to (P(V), a'1' 'Y'1' P('f/[V))): 
Formally, the Galois connection (Ll,al,-Y1,M1) is isomorphic to the Ga-
lois connection (L2,a2,-Y2,M2) when there are isomorphisms (h : L1 -7 L2 
and ()M : M1 -t M2 (see Appendix A) such that a2 = ()M 0 a1 0 ()i1 and 
'Y2 = ()L 01'1 0 ()AlÂ· 
Example 4.32 Returning to Example 4.28 we can use the above tech-
nique to construct a Galois insertion. Now 
signparity[Z] = {( -, odd), (-, even), (0, even), (+, odd), (+, even)} 
showing that (P(Z), asignparity, -Ysignparity, P(signparity[Z))) is the resulting Ga-
lois insertion. 
_ 
4.4 
Systematic Design of Galois Connections 
Sequential composition. When developing a program analysis it is 
often useful to do so in stages: The starting point will typically be a com-
plete lattice (Lo,~) fairly closely related to the semantics; an example is 

4.4 Systematic Design of Galois Connections 
245 
(P(V), ~). We may then decide to use a more approximate set of prop-
erties and introduce the complete lattice (L1,!;;;) related to Lo by a Galois 
connection (Lo, a1 , 'Y1, L1). This step can then be repeated any number of 
times: We replace one complete lattice Li of properties with a more ap-
proximate complete lattice (Li+1,!;;;) related to Li by a Galois connection 
(Li,ai+1,'Yi+1,Li+1). This process will stop when we have an analysis with 
the required computational properties. So the situation can be depicted as 
follows: 
The above sequence of approximations of the analysis could as well have been 
done in one step, i.e. the "functional composition" of two Galois connections 
is also a Galois connection. Formally, if (LO,a1,'Y!,L1) and (L1,a2,'Y2,L2) 
are Galois connections, then 
is also a Galois connection. To verify this we simply observe that a2(a1 (10Â» !;;; 
12 Â¢} a1(lO) !;;; 'Y2(12) Â¢} 10 !;;; 'Y1(-Y2(12Â» and, using Proposition 4.20, this 
shows the result. A similar result holds for Galois insertions because the 
functional composition of surjective functions yields a surjective function. 
Each of the Galois connections (Li, ai+1, 'Yi+1, Li+1) may have been obtained 
by combining other Galois connections and we shall shortly introduce a num-
ber of techniques for doing so. We shall illustrate these techniques in a se-
quence of examples that in the end will give us a finite complete lattice that 
has turned out to be very useful in practice for performing an Array Bound 
Analysis. 
Example 4.33 One of the components in the Array Bound Analysis is 
concerned with approximating the difference in magnitude between two num-
bers (typically the bound and the index). We shall proceed in two stages: 
First we shall approximate pairs (Z1,Z2) of integers by their difference in 
magnitude IZ11-lz21 and next we shall further approximate this difference 
using a finite lattice. The two Galois connections will be defined by extrac-
tion functions and they will then be combined by taking their functional 
composition. 
The first stage is specified by the Galois connection 
(P(Z x Z),adiff,'Ydiff, P(ZÂ» 
where diff : Z x Z -+ Z is the extraction function calculating the difference 
in magnitude: 

246 
4 Abstract Interpretation 
The abstraction and concretisation functions O!diff and "Ydiff will then be 
O!diff(ZZ) 
"Ydiff(Z) 
for ZZ ~ Z x Z and Z ~ Z. 
= 
{IZII-lz211(Zl,Z2)EZZ} 
{(Zb Z2) I IZII-lz21 E Z} 
The second stage is specified by the Galois connection 
(P(Z), O!range, "Yrange, P(RangeÂ» 
where Range = {<-1, -1, 0, +1, >+1}. The extraction function range: Z -t 
Range clarifies the meaning of the elements of Range: 
-1 
ifz=-1 
range(z) = 
0 
if Z = 0 
I 
<-1 if Z < -1 
+1 
if z = 1 
>+1 
if Z > 1 
The abstraction and concretisation functions O!range and "Yrange will then be 
O!range(Z) 
= 
{range(z) I z E Z} 
"Yrange(R) 
= 
{z I range(z) E R} 
for Z ~ Z and R ~ Range. 
We then have that the functional composition 
(P(Z x Z),O!R,"YR, P(RangeÂ» 
where O!R = O!range 0 O!diff and "YR = "Ydiff 0 "Yrange, is a Galois connection. We 
obtain the following formulae for the abstraction and concretisation functions: 
O!R(ZZ) 
= 
{range(lzII-lz2i) I (Zl' Z2) E ZZ} 
"YR(R) 
= 
{(z!, Z2) I range(lzII-lz2i) E R} 
Using Exercise 4.15 we see that this is the Galois connection specified by the 
extraction function range 0 diff: Z x Z -t Range. 
_ 
A catalogue of combination techniques. We have seen that the 
"functional (or sequential) composition" of Galois connections gives rise to 
a new Galois connection. It is also useful to be able to do "parallel com-
binations" and this will be the topic for the remainder of this section. We 
may have analyses for the individual components of a composite structure 
and may want to combine them into an analysis for the whole structure; we 
shall see two techniques for that: the independent attribute method and the 
relational method. We shall also show how Galois connections can be used 

4.4 Systematic Design of Galois Connections 
247 
to approximate the total function spaces and the monotone function spaces. 
Alternatively, we may have several analyses of the same structure and may 
want to combine them into one analysis and here the direct product and the 
direct tensor product allow us to do that. Using the notion of Galois inser-
tions this leads to a study of the reduced product and the reduced tensor 
product. 
The benefit of having such a catalogue of techniques is that a relative small 
set of "basic" analyses, whose correctness have been established, can be used 
to construct rather sophisticated analyses and, from an implementation point 
of view, opens up the possibility of reusing existing implementations. Finally, 
it is worth stressing that the complete lattices used in program analysis ob-
viously can be constructed without using these techniques but that it often 
provides additional insights to view them as combinations of simpler Galois 
connections. 
4.4.1 
Component-wise Combinations 
The first techniques we shall consider are applicable when we have several 
analyses of individual components of a structure and we want to combine 
them into a single analysis. 
Independent attribute method. Let (L 1 , 0::1, 1'1, M1) and (L2 ,0::2, 
1'2, M2) be Galois connections. The independent attribute method will then 
give rise to a Galois connection 
where: 
To see that this indeed does define a Galois connection we simply calculate 
0::(h,l2) ~ (m1,m2) 
Â¢} 
(0::1 (h), 0::2 (l2)) ~ (m1,m2) 
Â¢} O::l(h) ~m1 /\ 0::2(l2) ~m2 
Â¢} h ~'Y1(mt} /\ l2 ~'Y2(m2) 
Â¢} (h,l2) ~ (1'1 (mt), 1'2 (m2)) 
Â¢} (h,h)!; 'Y(ml,m2) 
and use Proposition 4.20. A similar result holds for Galois insertions (Exercise 
4.17). 

248 
4 Abstract Interpretation 
Example 4.34 The Array Bound Analysis will contain a component that 
performs a Detection of Signs Analysis on pairs of integers. As a starting 
point, we take the Galois connection 
(P(Z), asign, 'Ysign, P(Sign)) 
specified by the extraction function sign in Example 4.21. It can be used 
to analyse both components of a pair of integers so using the independent 
attribute method we will get a Galois connection 
(P(Z) x P(Z), ass, 'Yss, P(Sign) x P(Sign)) 
where ass and 'Yss are given by 
ass(ZI,Z2) 
'Yss(81,82) 
({sign(z) I z E ZI}, {sign(z) I z E Z2}) 
({z I sign(z) E 8 1}, {z I sign(z) E 8 2 }) 
where Zi ~ Z and 8i ~ Sign. 
This Galois connection cannot be described using an extraction function be-
cause neither P(Z) x P(Z) nor P(Sign) x P(Sign) is a powerset. However, 
they are both isomorphic to powersets: 
P(Z) x P(Z) 
~ P( {I, 2} x Z) 
P(Sign) x P(Sign) 
~ P( {I, 2} x Sign) 
By defining the extraction function twosigns : {1,2} x Z --+ {1,2} x Sign 
using the formula twosigns(i, z) = (i, sign(z)) we obtain a Galois connection 
(P({1,2} x Z),atwosigns,'Ytwosigns, P({1,2} x Sign)) 
that is isomorphic to (P(Z) x P(Z),ass,'Yss, P(Sign) x P(Sign)). 
In general the independent attribute method often leads to imprecision. An 
expression like (x,-x) in the source language may have a value in {(z, -z) I 
z E Z} but in the present setting where we use P(Z) x P(Z) to represent sets 
of pairs of integers we cannot do better than representing {(z, -z) I z E Z} 
by (Z, Z) and hence the best property describing it in the analysis of Example 
4.34 will be ass(Z,Z) = ({-,O,+},{-,O,+}). Thus we lose all information 
about the relative signs of the two components. 
_ 
Relational method. In the independent attribute method there is ab-
solutely no interplay between the two pairs of abstraction and concretisation 
functions. It is possible to do better by allowing the two components of the 
analysis to interact with one another so as to get more precise descriptions. 
Let (P(V1),al,I'I,P(Dd) and (P(V2),a2,'Y2,P(D2)) be Galois connections. 
The relational method will give rise to the Galois connection 

4.4 Systematic Design of Galois Connections 
where 
a(VV) 
'Y(DD) 
U{al({vI}) x a2({V2}) I (VI,V2) E VV} 
((VI,V2) I al({VI}) x a2({V2}) ~ DD} 
249 
where VV ~ VI X V2 and D D ~ DI X D2. Let us check that this does indeed 
define a Galois connection. From the definition it is immediate that a is 
completely additive and hence that there exists a Galois connection (Lemma 
4.23). It remains to show that 'Y (as defined above) is the upper adjoint of 
a. For this we can use Lemma 4.22 and calculate 
'Y(DD) 
((VI,V2) I al({vI}) x a2({v2}) ~ DD} 
((VI,V2) I a({(vI,v2)}) ~ DD} 
U{VVI a(VV) ~ DD} 
where we have used that a is completely additive. This shows the required 
result. 
It is instructive to see how the relational method is simplified if the Galois 
connections (P(Vi), ai, 'Yi, P(Di)) are given by extraction functions 'T}i : Vi -+ 
Di, i.e. if ai(Vi') = {'T}i(Vi) I Vi E Vn and 'Yi(DD = {Vi I 'T}i(Vi) E DaÂ· We 
then have 
a(VV) 
'Y(DD) 
{('I]I(Vt},'T}2(V2)) I (VI,V2) E VV} 
{(VI,V2) I ('T}I(VI),'T}2(V2)) E DD} 
which also can be obtained directly from the extraction function 'T} : VI x V2 -+ 
DI X D2 defined by 'I](VI,V2) = ('T}I(Vt},'T}2(V2)). 
Example 4.35 Let us return to Example 4.34 and show how the rela-
tional method can be used to construct a more precise analysis. We will now 
get a Galois connection 
(P(Z x Z),ass/,,/,sS/, P(Sign x Sign)) 
where aSS' and 'YSS' are given by 
ass/(ZZ) 
'YSS/(SS) 
{(sign(zt},sign(z2)) I (Zl, Z2) E ZZ} 
{(z!, Z2) I (sign(zt},sign(z2)) E SS} 
where ZZ ~ Z x Z and SS ~ Sign x Sign. This corresponds to using an ex-
traction function twosigns' : Z x Z -+ Sign x Sign given by twosigns' (ZI, Z2) = 
(sign(zl),sign(z2))' 
Once again consider the expression (x, -x) in the source language that has 
a value in {(z, -z) I Z E Z}. In the present setting {(z, -z) I z E Z} is an 

250 
4 Abstract Interpretation 
element of P(Z x Z) and it is described by the set ass, ({(z, -z) I z E Z}) = 
{(-, +), (0,0), (+, -)} of P(Sign x Sign). Hence the information about the 
relative signs of the two components is preserved. This will be the Galois 
connection that we will use in our further development of the Array Bound 
Analysis. 
_ 
The above treatment of the relational method can be extended in a very 
general way. Let us write P(VdÂ®P(V2) for P(VI x V2) and similarly P(DdÂ® 
P(D2) for P(D1 x D2). It is possible to perform a more general development 
using a notion of tensor product for which Ll Â® L2 exists even when the 
complete lattices Ll and L2 are not powersets. We refer to the Concluding 
Remarks for information about this. 
Total function space. In Appendix A it is established that if L is a 
complete lattice then so is the total function space S -+ L for S being a set. 
We have a similar result for Galois connections: 
Let (L, a, ,,(, M) be a Galois connection and let S be a set. Then we obtain 
a Galois connection 
(S -+ L, a', ,,(', S -+ M) 
by taking 
a'(f) 
a 0 f 
"(' (g) 
"( 0 g 
To see this we first observe that a' and "(' are monotone functions because a 
and "( are; furthermore 
"(' ( a' (f)) = "( 0 a 0 f 
=:J f 
a' ("(' (g) ) 
a 0 "( 0 g C g 
follow since (L, a, ,,(, M) is a Galois connection. A similar result holds for 
Galois insertions. The construction is illustrated by the following commuting 
diagrams: 
L 
M 
L 
"( 
M 
S 

4.4 Systematic Design of Galois Connections 
251 
Example 4.36 Assume that we have some analysis mapping the program 
variables to properties from the complete lattice L, i.e. operating on the ab-
stract states Var -+ L. Given a Galois connection (L, a, "(, M) the above 
construction will show us how the abstract states of Var -+ L are approxi-
mated by the abstract states of Var -+ M. 
â¢ 
Monotone function space. In Appendix A it is established that the 
monotone function space between two complete lattices is a complete lattice. 
We have a similar result for Galois connections: 
Let (L1,al,"(I,M1 ) and (L2,a2,"(2,M2) be Galois connections. Then we 
obtain the Galois connection 
by taking 
a(f) = a2 Â° 1 Â° "(1 
"(g) = "(2 Â° go al 
To check this we first observe that the functions a and "( are monotone 
because a2 and "(2 are; next we calculate 
"(a(f)) 
("(2 oa2) 01 0("(1 oal) 
::J 1 
a("(g)) = (a2 0"(2) ogo(al 0"(1) 
C 
9 
using the monotonicity of 1 : Ll -+ L2 and 9 : Ml -+ M2 together with (4.8) 
and (4.9). A similar result holds for Galois insertions (Exercise 4.17). 
This construction is illustrated by the following commuting diagrams: 
1 
"(g) 
Ll 
â¢ L2 
Ll 
â¢ L2 
?, 1 
I 
a, 
a, I 
1 
?, 
Ml 
â¢ M2 
Ml 
â¢ M2 
a(f) 
9 
4.4.2 
Other Combinations 
So far our constructions have shown how to combine Galois connections deal-
ing with individual components of the data into Galois connections dealing 
with composite data. We shall now show how two analyses dealing with the 

252 
4 Abstract Interpretation 
same data can be combined into one analysis; this amounts to performing two 
analyses in parallel. We shall consider two variants of this analysis, one "cor-
responding" to the independent attribute method and one "corresponding" 
to the relational method. 
Direct product. Let (L,a1,'Y1,Mt) and (L,a2,'/'2,M2) be Galois con-
nections. The direct product of the two Galois connections will be the Galois 
connection 
where a and,/, are given by: 
a(l) = (a1(l),a2(l)) 
,/,(ml,m2) 
'/'1 (m1) n '/'2 (m2) 
To see that this indeed defines a Galois connection we calculate 
a(l) ~ (ml, m2) 
{:} 
a1 (l) ~ m1 J\ a2 (l) ~ m2 
{:} l ~ ,/,l(mt} J\ l ~ '/'2(m2) 
{:} l ~ ,/,(m1,m2) 
and then use Proposition 4.20 to get the result. 
Example 4.37 Let us consider how this construction can be used to com-
bine the detection of signs analysis for pairs of integers given in Example 4.35 
with the analysis of difference in magnitude given in Example 4.33. We get 
the Galois connection 
(P(Z x Z), aSSR, '/'SSR, P(Sign x Sign) x P(Range)) 
where aSSR and '/'SSR are given by: 
aSSR(ZZ) = ({(sign(zt), sign(z2)) I (Zl, Z2) E ZZ}, 
{range(lzll-lz21) I (Zl,Z2) E ZZ}) 
'/'SSR(SS, R) = {(Zl, Z2) I (sign(zl),sign(z2)) E SS} 
n{(Zl, Z2) I range(lzll-lz21) E R} 
Note that the expression (x, 3*x) in the source language has a value in 
{(z,3 * z) I Z E Z} which is described by assR( {(z, 3 * z) I Z E Z}) = 
({(-,-), (0,0), (+,+)},{0,<-1}). Thus we do not exploit the fact that if the 
pair is described by (0,0) then the difference in magnitude will indeed be 
described by 0 whereas if the pair is described by (-, -) or (+, +) then the 
difference in magnitude will indeed be described by <-1. 
â¢ 

4.4 Systematic Design of Galois Connections 
253 
Direct tensor product. In the direct product there is no interplay 
between the two abstraction functions and as we saw above this gives rise 
to the same loss of precision as in the independent attribute method. It 
is possible to do better by letting the two components interact with one 
another. Again we shall only consider the simple case of powersets so let 
(P(V), ai, 'Yi, P(Di)) be Galois connections. Then the direct tensor product 
is the Galois connection 
where a and 'Yare defined by: 
a(V') 
'Y(DD) 
U{al({v}) x a2({v}) I v E V'} 
{v I al({v}) x a2({v}) ~ DD} 
where V' ~ V and DD ~ Dl x D2 . To verify that this defines a Galois 
connection we calculate 
a(V') ~ DD {:} 
Vv E V': al({v}) x a2({v}) ~ DD 
{:} 
Vv E V' : v E 'Y(DD) 
{:} V' ~ 'Y(DD) 
and then use Proposition 4.20. 
The construction can be simplified if the two Galois connections (P(V), ai, 
'Yi, P(Di)) are given by extraction functions "Ii : V ~ Di, i.e. if ai(V') = 
{1Ji(V) I v E V'} and 'Yi(DD = {v I 1]i(V) E Da. Then we have 
a(V') = {(1]l(V),1]2(V)) I v E V'} 
'Y(DD) 
= 
{v I (1]l(V),1]2(V)) E DD} 
which also can be obtained directly from the extraction function 1] : V ~ 
Dl X D2 defined by 1](v) = (1]l (v), 1]2(V)). 
Example 4.38 Let us return to Example 4.37 and show how the direct 
tensor product gives a more precise analysis. We will now get a Galois con-
nection 
(P(Z x Z),assR','YssR',P(Sign x Sign x Range)) 
where 
aSSR'(ZZ) 
'YSSR' (SSR) 
((sign(Zl)' sign(z2), range(lzll-lz21)) I (Zl, Z2) E ZZ} 
{(Zl, Z2) I (sign(zl), sign(z2)' range(lzll-lz21)) E SSR} 
for ZZ ~ Z x Z and SSR ~ Sign x Sign x Range. 

254 
4 Abstract Interpretation 
It is worth pointing out that this Galois connection is also obtainable from 
the extraction function 
twosignsrange : Z x Z -+ Sign x Sign x Range 
defined by twosignsrange(zl, Z2) = (sign(zd, sign(z2)' range(lzll-lz21)). 
Returning to the precision of the analysis we will now have aSSR' ( { (z, 3 * z) I 
Z E Z}) = {( -, -, <-1), (0,0,0), (+, +, <-1n and hence have a more precise 
description than in Example 4.37. 
However, it is worth noticing that the above Galois connection is not a Ga-
lois insertion. To see this consider the two elements 0 and {(O, 0, <-1n of 
P(Sign x Sign x Range) and observe that 
1'SSR' (0) = 0 = 1'SSR' ({(O, 0, <-1n) 
Thus 1'SSR' is not injective and hence Lemma 4.27 shows that we do not have 
a Galois insertion. 
_ 
Reduced product and reduced tensor product. The con-
struction of Proposition 4.29 gives us a general method for turning a Galois 
connection into a Galois insertion. This technique can now be combined 
with the other techniques for combining Galois connections and this is of 
particular interest for the direct product and the direct tensor product. 
Let (L,al,1'l,M1) and (L,a2,1'2,M2) be Galois connections. Then the re-
duced product is the Galois insertion 
(L, a, 1', <;[Ml x M2]) 
where 
a(l) = (al (l), a2 (l)) 
1'(ml,m2) = 1'l(ml)n1'2(m2) 
<;(ml,m2) = n{(m~,m~) I 1'1 (ml) n 1'2 (m2) = 1'l(mD n 1'2(m~n 
To see that this is indeed a Galois insertion recall that we already know 
that the direct product (L, a, 1', Ml x M2) is a Galois connection and that 
Proposition 4.29 then shows that (L, a, 1', <;[Ml x M2]) is a Galois insertion. 
Next let (P(V),ai,1'i,P(Di)) be Galois connections for i = 1,2. Then the 
reduced tensor product is the Galois insertion 
where 
a(V') 
1'(DD) 
c;(DD) 
U{al({V}) x a2({v}) I v E V'} 
{v I al({v}) x a2({v}) ~ DD} 
n{DD' 11'(DD) = 1'(DD'n 

4.4 Systematic Design of Galois Connections 
255 
Again it follows from Proposition 4.29 that this is indeed a Galois insertion. 
Example 4.39 Let us return to Example 4.38 where we noted that the 
complete lattice P(Sign x Sign x Range) contains more than one element 
that describes the empty set of P(Z x Z). The superfluous elements will 
be removed by the construction of Proposition 4.29. The function ~SSR' will 
amount to 
~SSR/(SSR) = n{SSR' I "/ssR/(SSR) = ,,/ssR/(SSR')} 
where SSR, SSR' ~ Sign x Sign x Range. In particular, ~SSR' will map the 
singleton sets constructed from the 16 elements 
(-,0, <-1), (-,0,-1), (-,0,0), 
(0, -, 0), 
(0, -, +1), 
(0, -, >+1), 
(0,0, <-1), (0,0, -1), (0,0,+1), 
(0,0, >+1), 
(0,+,0), 
(0, +, +1), (0,+,>+1), 
(+,0, <-1), (+,0, -1), (+,0,0) 
to the empty set. The remaining 29 elements of Sign x Sign x Range are 
(- - <-1) 
" 
, (- - -1) 
" 
, 
(-,-,0), (-,-,+1), 
(-,-,>+1), 
(-,0,+1), 
(-,0, >+1), 
(-,+,<-1), (-, +, -1), 
(-,+,0), (-,+,+1), 
(-,+, >+1), 
(0, -, <-1), (0, -, -1), 
(0,0,0), (0, +, <-1), 
(0, +, -1), 
(+, -, <-1), (+, -, -1), 
(+,-,0), (+, -, +1), 
(+, -, >+1), 
(+,0,+1), 
(+,0, >+1), 
(+,+,<-1), (+, +, -1), 
(+,+,0), (+,+,+1), 
(+,+,>+1) 
and they describe disjoint subsets of Z x Z. Let us call the above set of 29 
elements for AB (for Array Bound); then ~SSR' [P(Sign x Sign x Range)] is 
isomorphic to P (AB). 
To conclude the development of the complete lattice and the associated Ga-
lois connection for the Array Bound Analysis we shall simply construct the 
reduced tensor product of the Galois connections of Examples 4.35 and 4.33. 
This will yield a Galois insertion isomorphic to 
(P(Z x Z), aSSR/, "/SSR/, P(AB)) 
Note that from an implementation point of view the last step of the con-
struction has paid off: if we had stopped with the direct tensor product in 
Example 4.38 then the properties would need 45 bits for their representation 
whereas now 29 bits suffice. 
Summary. The Array Bound Analysis has been designed from three simple 
Galois connections specified by extraction functions: 

256 
4 Abstract Interpretation 
(i) an analysis approximating integers by their sign (Example 4.21), 
(ii) an analysis approximating pairs of integers by their difference in mag-
nitude (Example 4.33), and 
(iii) an analysis approximating integers by their closeness to 0, 1 and -1 
(Example 4.33). 
We have illustrated different ways of combining these analyses: 
(iv) the relational product of analysis (i) with itself, 
(v) the functional composition of analysis (ii) and (iii), and 
(vi) the reduced tensor product of analysis (iv) and (v). 
It is worth noting that because the resulting complete lattice P(AB) is a 
powerset then it is indeed possible to obtain the very same Galois insertion 
using an extraction function twosignsrange' : Z x Z -t AB. 
â¢ 
4.5 
Induced Operations 
We shall now show that Galois connections are indeed useful for transforming 
computations into more approximate computations that have better time, 
space, or termination behaviour. We can do so in two different ways. In both 
cases we assume that we have an analysis using the complete lattice Land 
that we have a Galois connection (L, a, "1, M). 
One possibility is to replace the analysis using L with an analysis using M. 
In Subsection 4.5.1 we shall show that if the analysis using M is an upper 
approximation to the analysis induced from L then the correctness properties 
are preserved. We shall illustrate this approach in Subsection 4.5.2 for the 
Monotone Frameworks considered in Chapter 2. 
An alternative is only to use the complete lattice M for approximating the 
fixed point computations in L. So rather than performing all computations 
on the more approximate lattice M the idea is only to use M to ensure con-
vergence of fixed point computations and not needlessly reduce the precision 
of all other operations. We shall illustrate this in Subsection 4.5.3. 
4.5.1 
Inducing along the Abstraction Function 
Now suppose that we have Galois connections (Li , ai, 'Yi, M i ) such that each 
Mi is a more approximate version of Li (for i = 1,2). One way to make use 

4.5 Induced Operations 
257 
of this is to replace an existing analysis jp : L1 -+ L2 with a new and more 
approximate analysis 9p : M1 -+ M 2 â¢ We already saw in Section 4.4 that 
a2 0 jp 0/'1 is a candidate for 9p 
(just as /'2 o9p 0 a1 would be a candidate for jp). The analysis a2 0 jp 0/'1 is 
said to be induced by jp and the two Galois connections. This is illustrated 
by the diagram: 
Example 4.40 Let us return to Example 4.9 where we studied the simple 
program plus and specified the very precise analysis 
jplus(ZZ) = {Zl + Z2 I (Z1, Z2) E ZZ} 
using the complete lattices (P(Z),~) and (P(Z x Z), ~). In Example 4.21 
we introduced the Galois connection 
(P(Z), asign, ,/,sign, P(Sign)) 
for approximating sets of integers by sets of signs. In Example 4.35 we used 
the relational method to get the Galois connection 
(P(Z x Z),assl,/,sSI, P(Sign x Sign)) 
operating on pairs of integers. We now want to induce a more approximate 
analysis for the plus program 
9plus : P(Sign x Sign) -+ P(Sign) 
from the existing analysis jplus. To do so we take 
9plus = asign 0 jplus o/,ss' 
and simply calculate (for SS ~ Sign x Sign) 
9plus(SS) = asign(Jplusbssl(SSÂ»)) 
a sign(Jplus({(Zl,Z2) E Z x Z I (sign(zl),sign(z2)) E SS})) 
= a sign({Zl + Z2 I Zl,Z2 E Z, (sign(zl),sign(z2)) E SS}) 
{sign(zl +Z2) I Z1,Z2 E Z,(sign(zl),sign(z2)) E SS} 
u{ 81 EB 82 I (81, 82) E SS} 
where EB : Sign x Sign -+ P(Sign) is the "addition" operator on signs (so 
e.g. +EB+ = {+} and +EB - = {-,O,+}). 
-

258 
4 Abstract Interpretation 
The mundane approach to correctness. We shall now follow 
the approach of Section 4.1 and show that correctness of fp carries over to 
gpo For this assume that: 
R;, : Vi x Li --t {true, false} is generated by /3i : Vi --t Li 
The correctness of the analysis fp : L1 --t L2 is then expressed by 
where R1 ~ R2 is generated by /31 ~ /32 (Lemma 4.8). As argued in Section 
4.3 we get a correctness relation 8i for Vi and Mi by letting 
8i : Vi x Mi --t {true, false} be generated by ai 0 /3i : Vi --t Mi 
which is equivalent to saying that Vi 8i mi {:} Vi Ri ('Yi(mi)). The correctness 
relation for the analysis using M1 and M2 will be 8 1 ~ 82 which will be 
generated by (a1 0 (31) ~ (a2 0 (32) (Lemma 4.8). We now have the following 
useful result: 
Lemma 4.41 If (Li' ai, 'Yi, M i ) are Galois connections, and /3i : Vi --t Li 
are representation functions then 
holds for all ~. 
â¢ 
Proof To see this we simply calculate 
Â«01 0 ,BI)~(02 0 ,82))(~)(ml) = 
U{02(,82(V2)) I 01 (,81 (vI)) !; ml/\ VI "-+ V2} 
= 
02(U{,82(V2) I ,81(VI)!; 'Yl(ml) /\Vl ~ V2}) 
= 
02((,81 ~,82)(~ )(-yl(ml))) 
and the result follows. 
â¢ 
We shall now show that Lemma 4.41 yields: 
This just means that if fp is correct and if gp is an upper approximation to 
the induced analysis then also gp is correct. So suppose that 
and that a2 0 f p0'Y1 !;;;; gpo Since (Li' ai, 'Yi, Mi ) are Galois connections and fp 
and gp are monotone we get fp !;;;; 'Y2 0 gp 0 a1 as illustrated in the diagrams: 

4.5 Induced Operations 
fp 
Ip 
L1 
â¢ L2 
Ll 
â¢ L2 
~, 1 
I 
a2 
a, j 
In 
172 
In 
Ml 
â¢ M2 
Ml 
â¢ M2 
gp 
gp 
It follows that (f31 --Â»)- (32)(P f- . "-+ .) !; 72 0 gp 0 al and hence 
a2 0 (f31 --Â»)- (32)(P f- . "-+ .) 071 !; gp 
By Lemma 4.41 this is the desired result. 
259 
We shall say that a function fp : Ll --+ L2 is optimal for the program p if 
and only if correctness of a function f' : Ll --+ L2 amounts to fp !; f'. An 
equivalent formulation is that fp is optimal if and only if 
Lemma 4.41 may then be read as saying that if fp : Ll --+ L2 is optimal then 
so is a2 0 fp 0 71 : M1 --+ M2. 
Fixed points in the induced analysis. Let us next consider the 
situation where the analysis fp : L1 --+ L2 requires the computation of the 
least fixed point of a monotone function F : (L1 --+ L2) --+ (L1 --+ L2) so 
that fp = Jfp(F). The Galois connections (Li , ai, 7i, Mi ) give rise to a Galois 
connection (L1 --+ L2, a, 7, Ml --+ M2) between the monotone function spaces 
as shown in Section 4.4. We can now apply our technique of inducing and 
let G : (M1 -+ M2) --+ (Ml -+ M2) be an upper approximation to a 0 F 0 7. 
It will be natural to take gp : Ml --+ M2 to be gp = lfp( G). That correctness 
of fp carries over to gp follows from the following general result: 
Lemma 4.42 Assume that (L, a, 7, M) is a Galois connection and let f : 
L -+ L and g : M --+ M be monotone functions satisfying that g is an upper 
approximation to the function induced by f, i.e.: 
aof 0 7!;g 
Then for all m EM: 
gem) !; m => fC7(mÂ» !; 7(m) 
and furthermore Jfp(f) !; 7(Ifp(gÂ» and a(1fp(fÂ» !; lfp(g). 
â¢ 
Proof First assume g(m) !; m. The assumption ao f 07!; 9 gives a(f('Y(m)))!; 
g(m) and hence a(f(-y(m))) !; m. Using that a Galois connection is an adjunction 
(Proposition 4.20) we get f(-y(m)) !; 'Y(m) as required. 

260 
4 Abstract Interpretation 
To prove the second result we observe that hem) I gem) ~ m} ~ {l I f(l) ~ l} 
follows from the previous result. Hence we get (using Lemma 4.22): 
-y(n{m I gem) ~ m}) = nh(m) I gem) ~ m};! n{ll f(l) ~ l} 
Using Tarski's Theorem (Proposition A.10) twice we have Ifp(g) = nRed(g) = 
n{m I gem) !; m} and lfp(f) = nRed(f) = n{ll f(l) ~ l} so it follows that 
-y{lfp(gÂ» ;! lfp(f) as required. Then a{lfp(fÂ» 
~ Ifp(g) follows because a Galois 
connection is an adjunction. 
â¢ 
4.5.2 
Application to Data Flow Analysis 
Generalised Monotone Frameworks. To illustrate how these 
techniques can be applied we shall now consider a generalisation of the Mono-
tone Frameworks of Section 2.3. So let a generalised Monotone Framework 
consist of: 
â¢ a complete lattice L = (L, ~). 
Here we do not demand that L satisfies the Ascending Chain Condition and 
we do not specify the space :F of transfer functions as we shall be taking 
:F to be the entire space of monotone functions from L to L (which clearly 
contains the identity function and is closed under composition of functions). 
An instance A of a generalised Monotone Framework then consists of: 
â¢ the complete lattice, L, of the framework; 
â¢ a finite flow, F ~ Lab x Lab; 
â¢ a finite set of extremal labels, E ~ Lab; 
â¢ an extremal value, Â£ E L; and 
â¢ a mapping, 1., from the labels Lab of F and E to monotone transfer 
functions from L to L. 
As in Chapter 2 this gives rise to a set A-;;J of constraints 
:::J 
U{A.(t) I (t,i) E F} U Â£~ 
A.(i) 
:::J 
/L(Ao(i)) 
where Â£~ = { ~ iUEE 
iUÂ¢ E 
where i ranges over the labels Lab of F and E. We write (Ao, A.) 1= A-;;J 
whenever Ao , A. : Lab -t L is a solution to the constraints A-;;J. It is useful 
to consider the associated monotone function 
I: (Lab -t L) x (Lab -t L) -t (Lab -t L) x (Lab -t L) 

4.5 Induced Operations 
261 
defined by: 
... 
U{" 
R. 
f(Ao, A.) = ( )..Â£. 
A.(Â£ ) I (Â£ ,Â£) E F} U tE , )..Â£Â·h(Ao(Â£)) ) 
We then have the following important result (in the manner of Section 4.4): 
(Ao, A.) ;;;;J RAo, A.) is equivalent to (Ao, A.) F= A~ 
Galois connections and Monotone Frameworks. Let now 
(L, a, "I, M) be a Galois connection and consider an instance B of the gener-
alised Monotone Framework M that satisfies 
â¢ the mapping 9. from the labels Lab of F and E to monotone transfer 
functions of M -+ M satisfies 91. ;;;;J a 0 h 0 "I for all Â£j and 
â¢ the extremal value J satisfies J ;;;;J a(t,) 
and otherwise B is as A, i.e. has the same F and E. 
As above we get a set of constraints B~ and we write (Bo, B.) F= B~ when-
ever Bo, B. : Lab -+ M is a solution to the constraints. The alternative 
formulation is (Bo, B.) ;;;;J jj(Bo, B.) where g: (Lab -t M) x (Lab -t M) -+ 
(Lab -+ M) x (Lab -+ M) is the monotone function associated with the 
constraints. 
We shall now see that whenever we have a solution to the constraints obtained 
from B then we also have a solution to the constraints obtained from A. This 
can be expressed by: 
(Bo,B.)F=B~ implies boBo,'YoB.)F=A~ 
We can give a direct ("concrete") proof of this result but it is instructive to 
see how it follows from the general ("abstract") results established earlier. 
The idea is to "lift" the Galois connection (L, a, "I, M) to a Galois connection 
Â«Lab -+ L) x (Lab -+ L), a' , i, (Lab -+ M) x (Lab -+ M)) 
using the techniques for total function spaces and the independent attribute 
method presented in Section 4.4. The assumptions 91. ;;;;J a 0 h 0"1 (for all Â£) 
and J;;;;J a(t,) can then be used to establish 
jj;;;;J a' 0 j 0 "I' 
as the following calculations show 
(a' oj0'Y')(Bo,B.) = 
()..Â£.U{a('Y(B.(Â£'))) I (Â£',Â£) E F}Ua(t,~), 
)..Â£.a(hb(Bo(Â£))))) 
C 
jj(Bo,B.) 

262 
4 Abstract Interpretation 
where we have used Lemma 4.22. We can now use Lemma 4.42 to obtain 
iJ(Bo, B.) !; (Bo, B.) implies /(r'(Bo,B.))!; ,'(Bo,B.) 
and it follows that if (Bo, B.) F= B~ then (r 0 Bo" 
0 B.) F= A~ as stated 
above. 
The mundane approach to correctness. The above result shows 
that any solution to the constraints obtained for B also is a solution to the 
constraints obtained from A. We shall now show that semantic correctness of 
A implies semantic correctness of B. 
Let us reconsider the approach taken to semantic correctness in Section 4.1; 
here F = Bow(8*) and E = {init(8*)}. For the analysis A this calls for using 
a representation function 
(3 : State -+ L 
and the correctness of all solutions to A~ then amounts to the claim: 
Assume that (Ao, A.) F= A~ and (8*, Ul) -+* U2; 
then {J(Ul) !; t implies (3(U2) !; U{A.(f) If E fina1(8*)}. 
(4.11) 
For the analysis B it follows from Section 4.3 that it is natural to use the 
representation function 
. 
a 0 (3 : State -+ M 
and the correctness of all solutions to B~ then amounts to the claim: 
Assume that (Bo,B.) F= B~ and (8*,Ul) -+* U2; 
(412) 
then (a 0 (3)(ud !; 3 implies (a 0 (3)(U2) !; U{B.(f) If E fina1(8*)}. 
. 
We know that B is an upper approximation of the analysis induced from 
A and shall now prove that (4.11) implies (4.12). To do so we shall need 
to strengthen the relationship between the extremal values of A and B by 
assuming that 3 satisfies 
,(3) = t 
from which 3 ;! a(t) readily follows. For the proof that (4.11) implies (4.12) 
suppose that: 
It follows that: 
From (4.11) we get (3(U2) !; Uh 0 B.(f) I f E fina1(8*)} and hence (3(U2) !; 
-y(U{B .. (f) If E fina1(8*)}) showing the desired (a 0 (3)(U2) !; U{B.(f) If E 
fina1(8*)}. 

4.5 Induced Operations 
263 
A Worked Example 
As a concrete example we shall now consider an analysis SS for the WHILE 
language that approximates how sets of states are transformed into sets of 
states. First we prove that it is correct. Then we show that the Constant 
Propagation Analysis of Section 2.3 is an upper approximation of an analysis 
induced from SS. This will then establish the correctness of the Constant 
Propagation Analysis. 
Sets of states analysis. The analysis SS approximating the sets of 
states will be a generalised Monotone Framework with: 
â¢ the complete lattice (P(State), ~). 
Given a label consistent statement S* in Stmt we can now specify the in-
stance as follows: 
â¢ the flow F is Bow( S*); 
â¢ the set E of extremal labels is {init( S*)}; 
â¢ the extremal value Â£ is State; and 
â¢ the transfer functions are given by f.ss: 
{O'[x 1-7 A[a] 0'] I 0' E~} if [x := all is in S* 
fiSCÂ£.) 
= 
fiS(~) = ~ 
JiS(~) = ~ 
where ~ ~ State. 
if [skip]l is in S* 
if [W is in S* 
Correctness. The following result shows that this analysis is correct in 
the sense explained above: 
Lemma 4.43 Assume that (SSo, SS.) F SS2 and (S*,0'1) -+* 0'2; then 
0'1 E State implies 0'2 E U{SS.(l) Il E fina1(S*)}. 
_ 
Proof From Section 2.2 we have: 
(S,O') -+ (S', 0") implies fina1(S) ;2 fina1(S') /\ :How(S) ;2 :How(S') 
and as in Chapter 2 it is immediate that 
:How(S);2 :How(S') /\ (SSo,SS.) F SS2(S) implies (SSo,SS.) F SS2(S') 
It then suffices to show 
(SSo,SS.) F SS2(S) /\ (S,O') -+ 0" 
/\ 0' E SSo(init(S)) 
implies 0" E U{SS.(t) It E fina1(S)} 
(SSo, SS.) F SS2 (S) /\ (S,O') -+ (S', 0") /\ 0' E SSo (init(S)) 
implies 0" E SSo(init(S')) 

264 
4 Abstract Interpretation 
since then an induction on the length of the derivation sequence (8*, Ul) --+* U2 will 
give the result. The proof proceeds by induction on the inference in the semantics. 
We only consider a few of the interesting cases. 
The case ([a; := ajL, u) --+ u[a; t-+ A(a]u]. Then 552 (8) will contain the equation 
SS.(Â£) 2 {u[a; t-+ A[a]u] I u E SSo(Â£)} 
and since init([a; := all) = Â£ and fina1([a; := all) = {Â£} we see that that the required 
relationship holds: if u E SSo(Â£) then u[a; t-+ A[a]u] E SS.(Â£). 
The case (81; S2, u) --+ (8~; 82, u') because (81, u) --+ (8L u'). From the assumption 
u E SSo(init(SI;82Â» we get u E SSo(init(81Â» and the induction hypothesis gives 
u' E SSo(init(SD). But then u' E SSo(init(8~; 82Â» as required. 
The case (81;82,U) --+ (82,U') because (81,U) --+ u'. From the assumption u E 
SSo(init(81; 82Â» we get u E SSo(init(81Â» and the induction hypothesis gives u' E 
U{SS.(Â£) 1Â£ E fina1(81)}. We have 
{(Â£,init(82Â» 1Â£ E fina1(8t}} ~ flow(81;82) 
and since we have the constraints 
for all Â£ we get 
SSo(init(82Â» 2 U{SS.(Â£) 1Â£ E fina1(8t}} 
and hence u' E SSo(init(82Â» as required. 
The remaining cases follow the same pattern and are omitted. 
â¢ 
Remark. The SS analysis is unlikely to be optimal and hence is unlikely 
to equal the collecting semantics (see Section 1.5 or Exercise 4.5). This may 
be demonstrated by exhibiting an example where U{SS.(l) I l E flnaI(8*)} 
is strictly larger than {u' I (8*, u) --+>1< U' 1\ u EState} and it is fairly easy 
to do so. To obtain a specification of the collecting semantics we should let 
transfer functions be associated with edges rather than nodes as this would 
allow us to record the outcome of tests (see Exercise 2.11). 
â¢ 
Constant Propagation Analysis. The analysis of Section 2.3 is 
specified by a generalised Monotone Framework consisting of 
â¢ the complete lattice SWecp = ((Var -+ ZTh, !;). 
The instance for the statement 8* is determined by 
â¢ the flow F is Bow(8*)j 
â¢ the set E of extremal labels is {init(8*)}j 
â¢ the extremal value" is Ax.Tj and 
â¢ the transfer functions are given by the mapping f.cp defined in Table 
2.7. 

4.5 Induced Operations 
265 
Galois connection. The relationship between the two analyses is es-
tablished by defining the representation function 
-
(3cp : State -+ Statecp 
by (3cp (cr) = cr (as in Example 4.7). As in Section 4.3 this gives rise to a Galois 
connection (P(State),acP,'Ycp,Statecp) where acp(E) = U{(3cp(cr) I cr E 
~} and 'ycp (0') = {cr I (3cp ( cr) !;;; a}. One can now show that for all labels e 
f cP --, 
fSS 
l 
:::! acp 0 
l 
0 'YcP 
as well as 'Ycp(Ax.T) = State. Let us only consider the case where [x := all 
occurs in S* and calculate 
acp (fis ('Ycp (a))) = acp (fis ( {cr I cr !;;; a} )) 
acp ({ cr[x 1-7 A[a]crll cr !;;; a}) 
U{ cr[x 1-7 A[a]crll cr !;;; a} 
C 
a[x 1-7 U{A[a]cr I cr !;;; all 
C fY(a) 
and where the last step follows from U{A[a]cr I cr ~ a} !;;; Acp[a]a which 
can be proved by a straightforward structural induction on a. 
Thus we conclude that CP is an upper approximation to the analysis induced 
from SS by the Galois connection and hence it is correct. 
4.5.3 
Inducing along the Concretisation Function 
Widening operator induced by Galois connection. Suppose 
that we have a Galois connection (L, a, 'Y, M) between the complete lattices 
Land M, and also a monotone function f : L -+ L. Often the motivation 
for approximating f arises because a fixed point of f is desired, and the 
ascending chain (r(..L))n does not eventually stabilise (or may do so in too 
many iterations). Instead of using a 0 f 0 'Y : M -+ M to remedy this situation 
it is often possible to consider a widening operator \1M : M x M -+ M and 
use it to define '1Â£ : L x L -+ L by the formula: 
If \1L turns out to be a widening operator we then know how to approximate 
the least fixed point of f : L -+ L while calculating over L. This has the 
advantage that the coarser structure of M is only used to ensure convergence 
and does not needlessly reduce the precision of all other operations. The 
following result gives sufficient criteria for this approach to work: 

266 
4 Abstract Interpretation 
Proposition 4.44 
Let (L, a, 'Y, M) be a Galois connection and let \1M : M x M -+ M 
be an upper bound operator. Then the formula 
defines an upper bound operator \1L : L x L -+ L. It defines 
a widening operator if one of the following two conditions are 
fulfilled: 
(i) M satisfies the Ascending Chain Condition, or 
(ii) (L, a, 'Y, M) is a Galois insertion and \1M : M x M -+ M is 
a widening operator. 
Proof First we prove that "i1Â£ is an upper bound operator. Since V'M is an upper 
bound operator we have a(li) !; a(h)V'Ma(l2). Using that a Galois connection is 
an adjunction we get Ii !; 'Y(a(it)V'Ma(12Â», i.e. Ii !; it V'L l2. 
Assume now that condition (i) is fulfilled and consider an ascending chain (In)n 
in L. We know that also (l~L)n is an ascending chain and that I~L E 'Y[ M] for 
n > o. Hence (a(I~LÂ»n is an ascending chain and since M satisfies the Ascending 
Chain Condition there exists no ?: 1 such that a(l~L) = a(l~~) for all n ?: no. So 
'Y(a(I~LÂ» = 'Y(a(I~~Â» for all n ?: no and using that 'Y 0 a O'Y = 'Y (Fact 4.24) we 
get I~L = l~~ for all n ?: no. This completes the proof. 
Assume next that condition (ii) is fulfilled and consider again an ascending chain 
(In)n in L. Since a is monotone it follows that (a(ln)n) is an ascending chain in M. 
Now, V'M is a widening operator on M so there exists no such that (a(lnÂ»V'M = 
(a(lnoÂ»V'M for n?: no. We shall now prove that 
(a(lnÂ» V'M = 
a(I~L) 
(4.13) 
for all n?: o. The case n = 0 is immediate since (a~loÂ»V'M = a(lo) = a(lciL). For 
the induction step we assume that (a(lnÂ»V'M = a(lnL). Then 
and 
(a(lnHÂ» V'M 
(a(lnÂ» V'M V'M a(ln+l) 
= 
a(I:;L) V'M a(ln+l) 
a(l~tl) = a(I:;L V'L InH) 
= aC'Y(a(l:;L) V'M a(lnHÂ») 
= a(l:;L) V'M a(ln+l) 
since (L, a, 'Y, M) is a Galois insertion. 
Using (4.13) we thus have that there exists no such that a(l~L) = a(l~~) for all 
n ?: no. But then 'Y(a(I~LÂ» = 'Y(a(I~~Â» and hence l~L = l~~ for all n ?: no 
because (L, a, 'Y, M) is a Galois insertion. This completes the proof. 
_ 

4.5 Induced Operations 
267 
Precision of induced widening operator. The following result 
compares the precision of using the widening operator "1Â£ with the precision 
of using the widening operator \1M. 
Lemma 4.45 If (L, a, '1, M) is a Galois insertion such that 'Y(-'-M) = -'-L, 
and if \1M : M x M -? M is a widening operator, then the widening operator 
\1L : L x L -? L defined by it \1L l2 = 'Y(a(h) \1M a(b)) satisfies 
IfpvL (f) = 'Y(JfpVM (a 0 f 0 '1)) 
for all monotone functions f : L -? L. 
â¢ 
Proof By Proposition 4.44 we already know that '\1Â£ is a widening operator. Hence 
there exists nf 2: 0 such that IfpvL (f) = I;~ = IVL for all n 2: nf. Next write 
9 = a 0 1 0 "I and recall that "ilM is a widening operator. Hence there exists ng 2: 0 
such that IfpvM (g) = g~~ = gVM for all n 2: ng â¢ To obtain the desired result it 
therefore suffices to prove 
(4.14) 
by induction on n. The base case (n = 0) is immediate since I~L = ..lL and 
g'f,M = -'-M and we assumed that ..lL = 'Y(..lM). 
To prepare for the induction step we prove that (4.14) implies that: 
(4.15) 
For "=>" we calculate (using (4.14) and that (L, a, "I, M) is a Galois connection): 
l(fvL) !; IVL 
=> 
a(f(fvLÂ»!; a(fvL) 
=> a(f( 'Y(gVM))) !; a ("{(gVM Â» 
=> 
g(gVM)!; a( 'Y(gVMÂ» 
=> 
g(gVL)!; gVM 
For "{:::" we calculate (using (4.14) and that (L,a,'Y,M) is a Galois connection): 
g(gVM) !; gVM 
=> 
"I (g(gVM Â» !; "I (gVM ) 
=> 
'Y(a(f('Y(gvMÂ»)))!; 'Y(gVM) 
=> 
'Y(a(f(fvL)))!; IVL 
=> 
I(fVL)!; IVL 
Returning to the induction step (n> 0) of the proof of (4.14) we calculate: 
= { 1;;;1 
if 1(f;;;1) !; 1;;;1 
I;;; 1 "ilL 1 (f;;; 1 ) 
otherwise 
{ 1;;;1 
if g(g~-;'/) !; g~"';} 
1;;;1 "ilL IU;;;l) 
otherwise 
= 
{ 
'Y(g~~l) 
if g(g~~l) !; g~~l 
'Y(a("{(g~~lÂ» "ilM a(f("{(g~~lÂ»))) otherwise 

268 
= -y(gVM) 
n-l 
gVM 
g~;./ V'M g(g~;}) 
4 Abstract Interpretation 
if g(g~;}) ~ g~;./ 
otherwise 
) 
if g(g~~:/) ~ g~;:/ 
otherwise 
) 
In this calculation we have used that (4.14) and (4.15) hold for n -1, the definition 
of V'L, and that (L, a, -y, M) is a Galois insertion. 
_ 
This result then provides the formal justification for the motivating remarks 
in the beginning of the subsection. To be specific let M be of finite height, 
let (L, a, 'Y, M) be a Galois insertion satisfying 'Y(J .. M) = 1..L, and let 'VM be 
the least upper bound operator UM. Then the above lemma shows that 
which means that IfpvL (I) equals the result we would have obtained if we 
decided to work with a Â° f Â° 'Y : M -+ M instead of the given f : L -+ Lj 
furthermore the number of iterations needed turn out to be the same. Since 
the greater precision of Lover M is available for all other operations, this 
suggests that the use of widening operators is often preferable to the approach 
of Subsection 4.5.1. 
Concluding Remarks 
In this chapter we have only been able to touch upon a few of the central 
concepts of Abstract Interpretationj this has mainly been based on [37, 39, 
35, 40]. Mini Project 4.1 and the series of examples leading up to Example 
4.39 are based on [179]. Mini Project 4.3 is inspired by [126, 68]. Much 
more can be said both about the development of the theory and about its 
applications. In this section we briefly discuss some of the more important 
concepts that have been omitted so far. 
Upper closure operators. An upper closure operator p : L -+ Lis 
a monotone function that is extensive (Le. satisfies p ;;;J Al.l) and idempotent 
(Le. pop = p). Such operators arise naturally in Abstract Interpretation [39] 
because whenever (L,a,'Y,M) is a Galois connection the function 'Y0a: L-+ 
L is easily seen to be an upper closure operator. Furthermore, if p : L -+ L 
is an upper closure operator then the image p[L] = {p(l) Il E L} of Lunder 
p equals the set Fix(p) = {l ELI 1 = p(l)} of fixed points of p and is a 
complete lattice under the partial ordering of Lj in fact (L, p, Al.l, p[L]) is a 
Galois insertion. 
It follows that upper closure operators may be used to represent Galois con-
nections by simply demanding that the more approximate space M is actually 

Concluding Remarks 
269 
a subset of L and that no essential features are lost by doing this. This then 
opens up the possibility for directly comparing the precision of various Galois 
connections over the same complete lattice L by simply relating the closure 
operators. The relation PI !;;; P2 is naturally defined byVl E L : Pl(l) !;;; P2(l) 
but turns out to be equivalent to the condition that P2(L) ~ Pl(L) and 
represents the fact that P2 is more approximate than Pl. 
Having defined an ordering on the set of upper closure operators on Lone 
can next show that the set is a complete lattice: the least element is given 
by the upper closure operator )".l.l and the greatest element by )"l. T. The 
binary greatest lower bound operator n is of special interest: it gives rise to 
the construction of the reduced product [39] (see Section 4.3.) 
A number of additional constructs can be explained by means of upper clo-
sure operators: we just mention reduced cardinal power [39] and disjunctive 
completion [39]. By "inverting" some of these constructions it may then be 
possible to find the "optimal bases" with respect to a given combination: for 
reduced product the notion of pseudo-complementation has been used to find 
the minimal factors [33, 34], and for disjunctive completion one can identify 
the basic irreducible properties that are inherently necessary for the analysis 
[63,64]. 
Stronger properties on the complete lattices. Being a com-
plete lattice is a rather weak notion compared to being a powerset. By 
considering more structure on the complete lattices, say distributivity, and 
identifying the elements that correspond to singletons, e.g. atoms or join irre-
ducible elements, it is frequently possible to lift some of the stronger results 
that hold for powersets to a larger class of complete lattices. 
Since powersets are isomorphic to bit vectors this gives a way of finding more 
general conditions on analyses for when they are as efficient as the Bit Vector 
Frameworks. This is of particular interest in the case of fixed points, where 
one can use properties of distributive lattices and distributive analyses to 
give rather low bounds on the number of iterations needed for the ascending 
chain (rC..l))n to stabilise [129, 118]. 
Another line of work concerns the development of the tensor product for 
complete lattices that are not also powersets [111, 113, 115]. Several notions 
of tensor product have been studied in lattice theory but the development of 
tensor products suitable for program analysis was first done in [111]. 
Concrete analyses. In this chapter we have concentrated on introduc-
ing some of the key notions in the theory of Abstract Interpretation and only 
occasionally have we hinted at concrete applications. 
One of the main applications of Abstract Interpretation has been in the area 
of logic programming. To implement a program efficiently it is important to 
have precise information about the substitutions that may reach the various 

270 
4 Abstract Interpretation 
program points; a central question to be asked for a substitution is whether or 
not it is ground. A number of analyses have been designed for this and most 
of these build upon the framework of Abstract Interpretation. This includes 
the design of iteration strategies based on widening, and the decomposition 
of base domains using the techniques mentioned above under upper closure 
operators. 
Another main application of Abstract Interpretation has been to approxi-
mate subsets of n-dimensional vector spaces over integers or rationals. For 
the purpose of this discussion we shall limit ourselves to at most two di-
mensions (the line and the plane). In the case of one dimension there are 
two main techniques. One we already illustrated: the lattice of intervals, 
and it may be generalised to consider (possibly finite) unions of intervals. 
The other technique records sets of numbers modulo some base value, e.g. 
{x I x mod kl = k2 }. Clearly these two analyses can be combined. In the case 
of two (or more) dimensions it is straightforward to perform an independent 
attribute analysis where the techniques above are applied component-wise 
for each dimension. 
A substantial effort has been devoted to developing more interesting rela-
tional analyses for two ( or more) dimensions where the choice of axes is of 
less importance for the ability to approximate subsets of vector space. An 
early method was the affine subspaces of Karr [94] where sets of the form 
{(x,y) I k1x + k2y = k3} can be described. The generalisation from equality 
to inequality, and allowing to take intersections of such subsets, was consid-
ered by Cousot and Halbwachs [41] and resulted in a study of convex poly-
gons. Generalisations and combinations of these ideas have been developed 
by Grang~r [66, 67] and by Masdupuy [105]. 
An interesting line of work pioneered by Deutsch [44, 45] is to change the 
problem of describing regular sets of words over a finite alphabet to the 
problem of describing sets of integer vectors. This is by no means trivial but 
once it has been achieved it opens up the possibility for using all of the above 
techniques to represent also regular sets of words. This is very important for 
the analysis of higher-order and concurrent programs, as shown by Deutsch 
and Colby [31, 32], since it can describe the shape of activation records and 
communication patterns in much greater precision than other comparative 
techniques. 
We should also mention techniques for building the abstract space of prop-
erties "dynamically" [24] and for using widening and narrowing to improve 
the performance of chaotic iteration [25]. 
Duality. The dual ~d of a partial ordering ~ is obtained by defining 
h ~d l2 if and only if l2 ~ h; thus we could write ~d as;:]. Any concept 
defined in terms of partial orderings can be dualised by replacing all partial 
orderings by their dual. In this way the dual least element is the greatest 

Concluding Remarks 
271 
element, and the dual least upper bound is the greatest lower bound etc. The 
principle of lattice duality of Lattice Theory says that if any statement about 
partially ordered sets is true then so is the dual statement. (Interestingly the 
concept of monotonicity is its own dual.) However, we should like to point 
out that the dual of a complete lattice may of course differ from the complete 
lattice itself; pictorially we represent this by drawing the complete lattice 
"up-side down" . 
The principle of lattice duality is important for program analysis because it 
gives an easy way of relating the literature on Abstract Interpretation to the 
"classical" literature on Data Flow Analysis: simply dualise the complete 
lattices. So in Abstract Interpretation the greatest element is trivially safe 
and conveys no information whereas in "classical" Data Flow Analysis it is 
the least element that has this role. Similarly, in Abstract Interpretation we 
are interested in least fixed points whereas in "classical" Data Flow Analysis 
we are interested in greatest fixed points. 
Staying within the basic approach of Abstract Interpretation, that going up 
in the complete lattice means losing information, it is still possible to dualise 
much of the development: in particular we can define the notion of dual Ga-
lois connections. To see why this may be worthwhile consider the following 
scenario. In program analysis we aim at establishing an element lÂ£ E L for 
describing the set of values that may reach a given program point i. In pro-
gram transformation it is frequently the case that a certain transformation 
B is valid only if the set of values that reach a certain point have certain 
properties; we may formulate this as the condition lÂ£ [;;; ls. Now if we want 
to be more approximate we approximate lÂ£ to l~ and ls to lk and formulate 
the approximate condition l~ [;;; lk. To ensure that l~ [;;; lk implies li [;;; ls we 
demand that It [;;; l~ and that lk [;;; ls. Thus properties of program points are 
approximated by going up in the complete lattice, for which Galois connec-
tions are useful, whereas enabling conditions for program transformations are 
approximated by going down in the complete lattice, and for this the concept 
of dual Galois connections is useful. 
A final word of advice concerns the interplay between Abstract Interpretation 
and Denotational Semantics. In Denotational Semantics the least element 
conveys absolutely no information, and we learn more when things get larger 
according to the partial order; had there been a greatest element it would 
have denoted conflicting information. This is quite opposite to the situation 
in Abstract Interpretation where the greatest element conveys absolutely no 
information and we learn more when things get smaller according to the par-
tial order; the least element often denotes non-reachability. Hence it would 
be dangerous to simply apply too many of the intuitions from Denotational 
Semantics when performing Abstract Interpretation not least because both 
formalisms ask for least fixed points and therefore are not duals of one an-
other. 

272 
4 Abstract Interpretation 
Mini Projects 
Mini Project 4.1 A Galois Connection for Lists 
In a series of examples leading up to Example 4.39 we constructed a Galois 
insertion for recording the relationship between pairs of integers; it was given 
by 
(P(Z X Z),aSSRf,'YSSRf, P(AB)) 
where AB ~ Sign x Sign x Range contained only 29 elements (out of the 
45 possibilities). 
In this mini project we are going to construct a Galois insertion for recording 
the relationship between pairs of lists. Let V be the domain of lists of finite 
length over some simple data type. We write x = [Xl, ... , xn] for a list with 
n elements whose first element is Xl; when n = 0 we write X = []. Next let 
X = [Xl, ... , xn] and Y = [YI, ... , Yrn] be two lists. They have the same head 
if and only if n > 0, m > 0 and Xl = YI. The list X is a suffix of Y if and only 
if there exists k ~ 0 such that n + k = m and Xi = Yi+k for i E {I,Â· .. , n}. 
Finally we write length(x) = n and length(y) = m. 
The Galois insertion should have the form 
(P(V x V), a, 'Y, P(LR)) 
where LR ~ P( {H, S}) x Range. Here H means that the lists have the same 
head, S means that the shorter list is a suffix of the other, and the range 
components describe length(x) - length(y) where X is the first list and Y the 
second list. 
Complete the details of the specification. 
â¢ 
Mini Project 4.2 Correctness of the Shape Analysis 
We shall now return to the Shape Analysis of Section 2.6 and show how it 
gives rise to a Galois connection. Recall that the semantics uses configura-
tions with a state U E State and a heap component 11. E Heap and that the 
analysis works on shape graphs consisting of an abstract state S, an abstract 
heap H and a sharing component is. 
We shall begin by defining a function vars that given a location and a state 
will determine the associated abstract location: 
vars(e)(u) = nx where X = {x I u(x) = e} 
Proceed as follows: 

Mini Projects 
273 
1. Define a representation function 
/3SA : State x Heap --+ P(SG) 
that to each state and heap associates a singleton set consisting of a 
compatible shape graph (as defined in Section 2.6) and construct the 
associated Galois connection 
(P(State x Heap), aSA, 'YSA, P(SG)) 
Is it a Galois insertion? 
To establish the correctness of the analysis we shall follow the approach of 
Subsection 4.5.2: 
2. Specify an analysis SH approximating the sets of pairs of states and 
heaps as a generalised Monotone Framework over the complete lat-
tice (P(State x Heap), ~)j write fiH for the associated transfer nmc-
tions. Prove the correctness of the analysis SH (i.e. prove an analogue 
of Lemma 4.43). 
3. Show that fiA ;J aSA 0 fiH 0 'YSA for all transfer functions and conclude 
that the Shape Analysis is correct. Determine whether or not fr = 
aSA 0 fiH 0 'YSA holds for all transfer functions. (See Exercise 2.23.) -
Mini Project 4.3 Application to Control Flow Analysis 
In this mini project we shall perform an analogue of the development of 
Subsection 4.5.2 for the Control and Data Flow Analysis of Section 3.5. 
1. Specify a "sets of values" analysis 
(Csv, psv) I=sv e 
in the manner of Subsection 3.5.1 (by taking Data = Val where Val is 
as in Section 3.2). Formulate and prove a semantic correctness result 
in the manner of Example 4.40 and Theorem 3.10. 
2. Let a monotone structure (L, F) be given as in Subsection 3.5.2 and 
consider a Galois connection (P(Val) , a, 'Y, L). Motivated by the judge-
ments of the acceptability relation (C, 6, p, d) I=D e construct a Galois 
connection: 
({(Csv, psv) I ... }, a', 'Y', {(C, 6, p, d) I ... }) 
Formulate and prove a result intended to establish 
(C, 6, p, d) I=D e 
=? 
'Y'((C, 6, p, d)) I=sv e 
and argue that this shows the semantic correctness of the Control and 
Data Flow Analysis. 
-

274 
4 Abstract Interpretation 
0+ 
+ 
Figure 4.10: The complete lattice (Sign', !;). 
Exercises 
Exercise 4.1 For the complete lattice (Sign', !;) of Figure 4.10 define a 
correctness relation Rzsl : Z x Sign' -4 {true,false}. Verify that it indeed has 
the properties (4.4) and (4.5). Next define a representation function (3zs' : 
Z -4 Sign' and show that the Rzsl constructed above is indeed generated by 
(3ZS/. 
-
Exercise 4.2 Show that if (4.4) and (4.5) hold for R and L then we also 
have: 
and more generally: 
(VI E L' "# 0 : v R I) 
:::} v R (U L') 
Give an example showing that v R ..L fails even though (4.4) and (4.5) are 
fulfilled. 
_ 
Exercise 4.3 Show that the Control Flow Analysis of Chapter 3 is indeed 
correct in the sense of condition (4.3) as claimed in Example 4.4. To do so 
first show that 
(C, p) 1= Vi iff v V (p, C(l)) 
whenever v is a value (Le. a closure or a constant). Next show that 
is a corollary of Theorem 3.7. Finally assume the premise of (4.3) and use 
the above results to obtain the desired result. 
_ 

Exercises 
275 
Exercise 4.4 Show that the relation RCFA defined in Example 4.4 is gen-
erated by the representation function (3CFA also defined in Example 4.7. To 
do so prove that 
V RCFA (p, v) iff (3CFA (v) ~CFA (p, v) 
by induction on the size of v; only the case where v is close t in p is non-
trivial. 
-
Exercise 4.5 Define Li = (P(Vi),~) (for i = 1,2) and define i p : Ll -+ 
L2 by 
ip(h) = {V2 E V2 I::IVI E h : p f- VI ~ V2} 
Show that i p is monotone. Next show that (p f- . ~ Â·)(Rl -7> R2) i p where 
Vi Ri li is defined by Vi E li. Also, show that for l' : Ll -+ L2 we have 
(p f- . '"'-+ Â·)(Rl -7> R2 ) l' if and only if i p ~ 1'. A semantics that associates a 
program p with i p as defined here is sometimes called a collecting semantics. 
Finally, note that R is generated by (3i defined by (h(vi) = {Vi}; show that 
i p = ((31 -7> (32)(p f- . '"'-+ .). 
-
Exercise 4.6 Show that all of 
â¢ u 
â¢ A(h,l2).T 
â¢ A(h,l2). { ~ if h ~ h 
otherwise 
{ 
l2 
if h = 1. 
â¢ A(h,l2). 
lTl 
if h ~ h /\ h Â¥- 1. 
otherwise 
â¢ A(h,h).{ h Ul2 ifh ~l'Vl2~h 
T 
otherwise 
are upper bound operators (where l' is some element of L). Determine which 
of them that are also widening operators. Try to find sufficient conditions on 
l' such that the operator involving l' is a widening operator. 
_ 
Exercise 4.7 Show that if L satisfies the Ascending Chain Condition then 
an operator on L is a widening operator if and only if it is an upper bound 
operator. Conclude that if L satisfies the Ascending Chain Condition then 
the least upper bound operator U : L x L -+ L is a widening operator. 
_ 
Exercise 4.8 Consider changing the definition of i~ from i~ = 1. to 
i~ = lo for some lo E L. Possible assumptions on lo are: 

276 
4 Abstract Interpretation 
â¢ 10 = I(.l.)j 
â¢ 10 = j27(.l.)j 
â¢ 10 E Ext(f) j 
â¢ 10 arbitrary. 
Which of these suffice for proving Fact 4.14 and Proposition 4.13? 
_ 
Exercise 4.9 Let "VK be as in Example 4.15 and define 
. t " . +_ 
_ 
{inti U in~ 
if inti r;; int' V in~ r;; inti 
an 1 v anv.,! 
-
. t" . +_ 
th 
. 
m I v K znv.,! 
0 
erWlse 
where int' is an interval satisfying inf( int') > -00 and sup( int') < 00. Show 
that 
Vintl, in~ : inti Vin~ r;; intI VKin~ 
and that the inequality may be strict. Show that V is an upper bound 
operator. Determine whether or not V is a widening operator. 
_ 
Exercise 4.10 Let (In)n be a descending chain and let ~ : Lx L -+ L be 
a total function that satisfies l~ r;; li => 
l~ r;; (li ~ l~) r;; li for allli, l~ E L. 
Show that the sequence (l~)n is a descending chain and that l~ ~ In for all 
n. 
-
Exercise 4.11 Consider the following alternative strategy to narrowing 
for improving the approximation Iv E Red(f) to the fixed point Ifp(f) of 
the function I : L -t L. A descending chain truncator is a function T that 
maps descending chains (In)n to a non-negative number such that 
if (In)n and (l~)n are descending chains and "In :::; T((ln)n) : In = l~ 
then TÂ«ln)n) = T((l~)n). 
This ensures that T is finitely calculable. The truncated descending chain 
then is 
(fv,Â·Â·Â· ,r(fv),Â·Â·Â· ,1m ' (fv)) 
where m' = T(Cr(fv))n) and the desired approximation to Ifp(f) is 
Prove that this development can be used as an alternative to narrowing and 
try to determine the relationship between the two concepts. 
_ 

Exercises 
277 
Exercise 4.12 Show that if L satisfies the Descending Chain Condition 
then the binary greatest lower bound operator n : L x L -+ L is a narrowing 
operator. 
_ 
Exercise 4.13 Consider the complete lattice Interval of Figure 4.2 and 
the complete lattice Sign' of Figure 4.10 and define "(IS' by 
/'Is' (T) 
/'IS'(O+) = 
/'IS' (0) 
= 
/'Is' (1..) 
[-00,00] 
[0,00] 
[0,0] 
1.. 
/'IS' (-0) 
/'IS' (-) = 
/'IS' (+) = 
[-00,0] 
[-00, -1] 
[1,00] 
Show that there exists a Galois connection between Interval and Sign' with 
/'Is' as the upper adjoint. 
_ 
Exercise 4.14 Let (P(V),a1)'/'1),P(D)) be a Galois connection that is 
given by an extraction function 'fJ : V -+ D. Show that a1) is surjective if and 
only if'fJ is. Conclude that (P(V),a1)'''(1),P(D)) is a Galois insertion if and 
only if'fJ is surjective. Next show that <;"1)[P(D)] = a1)[P(V)] is isomorphic 
(see Appendix A) to P('fJ[V]) where 'fJ[V] is the image of'fJ and finally verify 
that <;"1)(V') = V' n 'fJ[V]. 
-
Exercise 4.15 Assume that the Galois connections (P(Di),a1)i+1l/'1)i+1' 
P(DHd) are given by the extraction functions 'fJH1 : Di -+ Di+1' Show that 
the composition of the Galois connections, (P(Do), a, /" P(D2)), will have 
a = a1)2 0 a1)l = a1)201)1 and/, = 11)1 0/'1)2 = /''120Tji ; i.e. the Galois connection 
is given by the extraction function 'fJ2 0 'fJ1. 
-
Exercise 4.16 Let (P(V1), a1, /'1, P(Dd) and (P(V2) , a2,"(2, P(D2)) be 
Galois connections given by the extraction functions 'fJ1 : V1 -+ D1 and 'fJ2 : 
V2 -+ D2. Furthermore assume that Vi and V2 are disjoint and similarly for 
D1 and D 2 â¢ Define an extraction function 
by 
(v) _ { 'fJ1 ( V ) if v E V1 
'fJ 
-
'fJ2(V) 
if v E V2 
Show that this defines a Galois connection 
and reformulate it as an isomorphic Galois connection 
in the manner of the independent attribute method. How important is it 
that V1 and V2 are disjoint and that D1 and D2 are disjoint? 
_ 

278 
4 Abstract Interpretation 
Exercise 4.17 Let (Ll.al,'Yl,Md and (L2,a2,'Y2,M2) be Galois inser-
tions. First define 
a(h,l2) = (al(l1),a2(l2Â» 
'Y(ml. m2) 
= 
(-rl(mr),'Y2(m2Â» 
and show that (Ll x L2, a, 'Y, Ml x M2) is a Galois insertion. Then define 
aU) = a2 0 f 0 'Yl 
'Y(g) = 'Y2 0 goal 
and show that (Ll -i L2,a,'Y,Ml -i M2) is a Galois insertion. 
-
Exercise 4.18 Let (P(Vl),al,'Yl,P(DlÂ» and (P(V2),a2,'Y2,P(D2Â» be 
Galois insertions. Define 
a(VV) = U{al({vr}) x a2({v2}) I (Vl.V2) E VV} 
'Y(DD) = ((Vl.V2) I al({vl}) x a2({v2}) ~ DD} 
and determine whether or not (P(Vi x V2), a, 'Y, P(DI x D2Â» is a Galois 
insertion. 
_ 
Exercise 4.19 Let (L, aIm, Ml ) and (L, a2,'Y2, M2) be Galois insertions. 
Define 
a(l) = (al(l), a2(lÂ» 
'Y(ml, m2) = 'Yl (ml) n 'Y2(m2) 
and determine whether or not (L, a, 'Y, Ml x M2) is a Galois insertion. 
_ 
Exercise 4.20 Let (Ll. al, 'Y1. Ml) be a Galois connection and define 
aU) = al 0 f 0 'Yl 
'Y(g) = 'Yl 0 goal 
(in the manner of Section 4.4). Do any of the following equalities 
a(Al.l) = Am.m 
'Y{Am.m) 
= 
AU 
aUl 0 h) = a(h) 0 a(h) 
'Y(gl 0 g2) 
= 'Y(gr) 0 'Y(92) 
necessarily hold? Which of the equalities hold when (Ll , al,'Yl, Ml) is known 
to be a Galois insertion? 
_ 

Exercises 
279 
Exercise 4.21 Consider the Galois insertion 
(P(Z x Z),aSSR/,')'SSR/P(AB)) 
developed in Example 4.39. Determine for each of the sets 
{(x,y) I x = y} 
{(x,y) I x = -y} 
{(x,y)lx=y+1} 
{(x,y) Ix=y+3} 
{(x,y) I x 2': y} 
{(x,y) I x 2': y + I} 
{(x, y) I X2 + y2 ::; lOG} 
the best description in P(AB). 
â¢ 
Exercise 4.22 Let (Li' ai, ')'i, Mi) be Galois connections for i = 1,2,3. 
Use the approach of Section 4.4 to define 
aU) 
')'(g) 
such that ((Ll x L 2 ) -t L 3 , a, ,)" (Ml x M2 ) -t M3 ) is a Galois connection. 
Next let all of (Li , ai, ')'i, M i ) be the Galois connection 
(P(Z), aZI, ')'zr, Interval) 
of Example 4.19 that relates the set of integers to the intervals. Let plus: 
P(Z) x P(Z) -t P(Z) be the "pointwise" application of addition defined by: 
A(ZI' Z2).{ZI + Z2 I ZI E ZI 1\ Z2 E Z2} 
Next define 
a(plus) = A(intl' in0.).Â·Â·Â· 
and supply the details of the definition. 
â¢ 
Exercise 4.23 Let L be the complete lattice of sets of integers, let M 
be the complete lattice of intervals of Example 4.10, let (L, a, ,)" M) be the 
Galois insertion of Example 4.19, and let \lM : M x M -t M be the widening 
operator of Example 4.15. Observe that the formula 
defines a widening operator \lL : Lx L -t L and develop a formula for it (in 
the manner of the formula for \lM of Example 4.15). 
â¢ 

280 
4 Abstract Interpretation 
Exercise 4.24 Suppose that (L, a, ,,(, M) is a Galois connection or a Ga-
lois insertion and that possibly M satisfies the Descending Chain Condition. 
Let tlM : M x M -t M be a narrowing operator and try to determine if the 
formula 
htld2 = "((a(h)tlMa(l2)) 
defines a narrowing operator tlL : L x L -t L. 
â¢ 

Chapter 5 
Type and Effect Systems 
So far our techniques have applied equally well to typed and untyped pro-
gramming languages. This flexibility does not apply to the development to 
be performed in this chapter: here we demand that our programming lan-
guage is typed because we will use the syntax for types in order to express the 
program analysis properties of interest (as was already illustrated in Section 
1.6). 
We shall first present an Annotated Type System for Control Flow Analy-
sis in Section 5.1, demonstrate its semantic soundness and other theoretical 
properties in Section 5.2, and then in Section 5.3 show how to obtain an 
algorithm for computing the annotated types (and prove that it is sound and 
complete). In Sections 5.4 and 5.5 we give examples of other analyses spec-
ified by Type and Effect Systems. In Section 5.4 we study Type and Effect 
Systems with rules for subtyping, polymorphism and polymorphic recursion 
and illustrate their use in an analysis for tracking Side Effects, an Exception 
Analysis and an analysis for Region Inference. Finally, in Section 5.5 we show 
that the annotations can be given more structure and we illustrate this for a 
Communication Analysis. 
5.1 
Control Flow Analysis 
Syntax of the FUN language. To illustrate the approach we shall 
make use of the functional language FUN also considered in Chapter 3; that 
the approach also applies to the imperative language of Chapter 2 was briefly 
sketched in Section 1.6. However, in this chapter we shall use a slightly 
different labelling scheme from the one in Chapter 3; the syntactic category 
of interest is 
e 
E Exp expressions 

282 
5 Type and Effect Systems 
and it is defined by: 
e 
.. -
c I x I fn.,.. x => eo I fun'lr J x => eo I el e2 
if eo then el else e2 I let x = el in e2 I el op e2 
The program points, 7r E Pnt, are used to name the function abstractions in 
the program; this could also be done using the notion of labelled terms from 
Chapter 3 but for our present purposes we do not need the full generality 
of this machinery - the reason is that now we will use the types to record 
information that was previously associated with labels. Hence our syntax 
just makes use of expressions and dispenses with terms. 
As in the previous chapters we shall assume that a countable set of variables 
is given and that constants (including the truth values), binary operators 
(including the natural arithmetic, boolean and relational operators) and pro-
gram points are left unspecified: 
c 
E Const constants 
op 
E Op 
binary operators 
J, x 
E Var 
variables 
7r 
E Pnt 
program points 
Example 5.1 The functional program (fn x ,;,,> x) (fn y => y) con-
sidered in Chapters 1 and 3 is now written as 
(fnx x => x) (fny y => y) 
just as we did in Example 1.5. 
Example 5.2 The expression loop of Example 3.2 is now written: 
let g = (funF f x => f (fny y => y)) 
in g (fnz z => z) 
â¢ 
Recall that this is a looping program: g is first applied to the identity function 
fnz z => z but it ignores its argument and calls itself recursively with the 
function fny y => y. 
â¢ 
5.1.1 
The Underlying Type System 
The analyses will be specified as extensions of the ordinary type system in 
order to record the program properties of interest. For this reason the or-
dinary type system is sometimes called the underlying type system and we 
shall start by specifying it. 

5.1 Control Flow Analysis 
[con] 
r I-UL c: Tc 
[var] 
r I-UL x : T 
ifr(x)=T 
[In] 
[!un] 
[app] 
[ifl 
[let] 
[op] 
r[x t-+ T",]I-UL eo : TO 
r I-UL fn'll" x => eo : T", -+ TO 
r[J t-+ T", -+ TO][X t-+ T",] I-UL eo : TO 
r I-UL fun.r f x => eo : T", -+ TO 
r I-UL el : T2 -+ TO 
r I-UL e2 : T2 
r I-UL el e2 : TO 
r I-UL eo : bool r I-UL el : T 
r I-UL e2 : T 
r I-UL if eo then el else e2 : T 
r I-UL el : Tl 
r[x t-+ Tl] I-UL e2 : T2 
r I-UL let x = el in e2 : T2 
r I-UL el : T!p 
r I-UL e2 : T;p 
r I-UL el op e2 : Top 
Table 5.1: The Underlying Type System. 
283 
Types. Let us first introduce the notions of types and type environments: 
T 
E Type 
types 
r 
E TEnv type environments 
We shall assume that the types are given by 
T ::= int 1 bool 1 Tl -+ T2 
where int and bool are the only two kinds of base types and as usual we 
use arrows for function types. Each constant c E Const has a type that we 
shall denote Tc so e.g. true has type Ttrue = bool and 7 has type T7 = into 
Each binary operator op will expect two arguments of type T!p and T;p, 
respectively, and give a result of type Top - an example is the relational 
operation ~ that expects two arguments of type int and gives a result of 
type bool. For the sake of simplicity we shall assume that all the constants 
have base types and that all binary operators expect arguments of base types 
and return values of base types. 
The type environments are given by: 
r ::= [ ]1 r[x t-+ T] 

284 
5 Type and Effect Systems 
Formally, r is a list but nevertheless we shall feel free to regard it as a finite 
mapping: we write dom(r) for {x I r contains [x I-t ... ]}; we write rex) = 7 
if x E dom(r) and the rightmost occurrence of [x I-t ... ] in r is [x I-t 7], 
and we write r I X for the type environment obtained from r by removing 
all occurrences of [x I-t ... ] with x tJ. X. For the sake of readability we shall 
write [x I-t 7] for [ ][x I-t 7]. 
Typing judgements. The general form of a typing is given by 
r I-UL e : 7 
that says that the expression e has type 7 assuming that any free variable 
has type given by r. The axioms and rules for the judgements are listed in 
Table 5.1 and are explained below. 
The axioms [con] and [var] are straightforward: the first uses the predefined 
type for the constant and the second consults the type environment. In the 
rules [fn] and [fun] we guess a type for the bound variables and determine 
the types of the bodies under the additional assumptions; the rule [fun] has 
the implicit requirement that the guess of the type for f matches that of the 
resulting function. As a consequence of these two rules the type system is 
nondeterministic in the sense that r I-UL e : 71 and r I-UL e : 72 does not 
necessarily imply that 71 = 72. 
The rule [app] requires that the operator and the operand of the application 
can be typed and implicitly it requires that the type of the operator is a 
function type where the type before the arrow equals that of the operand 
- in this way we express that the types of the formal and actual parameter 
must be equal. 
The rules [ij], [let] and fop] are straightforward. In particular, the let-
construct let x = e1 in e2 admits exactly the same typings as the application 
(fn1r x => e2) e1 and regardless of the choice of IT. In Sections 5.4 and 5.5 
we shall consider a polymorphic let-construct where let x = e1 in e2 may 
admit more typings than (fn1r x => e2) e1. 
Example 5.3 Let us show that the expression loop 
let g = (funF f x => f (fny y => y)) 
in g (fnz z => z) 
of Example 5.2 has type 7 -+ 7 for each type 7. We shall first consider the 
expression funF f x => f (fny y => y) where we write rfx for the type 
environment [f I-t (7 -+ 7) -+ (7 -+ 7)][X I-t 7 -+ 7]. Then we get 
r fx I-UL f : (7 -+ 7) -+ (7 -+ 7) 
rfx I-UL fny y => y: 7 -+ 7 

5.1 Control Flow Analysis 
using the axiom [var] and the rule [fn]. The rule lapp] then gives 
rfx I-UL f (fny y => y) : 7 -7 7 
285 
and we have a judgement matching the premise of the rule [fun]. Hence we 
get 
[]I-UL funF f x => f (fny y => y) : (7 -7 7) -7 (7 -77) 
Taking r g to be the type environment [g t-t (7 -7 7) -7 (7 -7 7)] we get 
r g I-UL g (fnz z => z) : 7 -7 7 
using the axiom [var] and the rules [in] and lapp]. The rule [let] can now be 
used to show that the expression loop has type 7 -7 7. 
_ 
5.1.2 
The Analysis 
Annotated types. That a function has type 71 -7 72 means that given 
an argument of type 71 it will return a value of type 72 in case it terminates. 
To get a Control Flow Analysis we shall annotate the type with information, 
<P E Ann, about which function it might be. The annotations are given by 
<P 
E 
Ann annotations 
where: 
<P ::= {1f} 1 <PI U 'P2 10 
So 'P will be a set of function names - describing the set of function definitions 
that can result in a function of a given type; as will be discussed below, we 
shall feel free to write {1fl, ... , 1f n} for {1ft} U ... U {1f n}. We now take 
-
T 
E 
Type 
annotated types 
r 
-
E 
TEnv annotated type environments 
and define: 
7 
.. -
int 1 bool 1 Tl -'4 72 
r 
.. -
[ ]1 r[x t-t T] 
We shall write LTJ for the underlying type corresponding to the annotated 
type T; it is defined as follows: 
LintJ = int 
LboolJ 
bool 
LTI 4 
T2J 
LTd -7 LT2J 
As an example we have Lint ~ 
intJ = (int -7 int). Furthermore, we 
extend the notation to operate on type environments so LrJ (x) = Lr(x)J for 
all x. 

286 
5 Type and Effect Systems 
[con] 
I' f-CFA C : Tc 
[var] 
I' f-CFAX: 7 
ifr(x) =7 
[In] 
[fun] 
lapp] 
[ iJJ 
[let] 
fop] 
1'[1 I-t 7", 
{1r}U'P) 70][X I-t 7",] f-CFA eo : 70 
I' f-CFA fun.,. 1 x => eo : 7", 
{1r}U'P) 70 
I' f-CFA el : 72 -.!4 70 I' f-CFA e2 : 72 
I' f-CFA el e2 : 70 
I' f-CFA eo : bool I' f-CFA el : 7 I' f-CFA e2 : 7 
I' f-CFA if eo then el else e2 : 7 
I' f-CFA el : 71 
r[x I-t 71] f-CFA e2 : 72 
I' f-CFA let x = el in e2 : 72 
...... 
1""" 
2 
r f-CFA el : Top r f-CFA e2 : Top 
I' f-CFA el op e2 : Top 
Table 5.2: Control Flow Analysis. 
Judgements. The judgements of the Control Flow Analysis have the 
form 
I' f-CFA e : 7 
and are defined by the axioms and rules of Table 5.2. The clauses [In] and [!un] 
annotate the arrow of the resulting function type with the information that 
the abstraction named 1r should be included in the set of possible functions; 
the use of {1r} U cp indicates that we may want to include other names as well 
and we shall say that the Type and Effect System allows subeffecting (see 
the Concluding Remarks). In Example 5.5 below we shall give an example 
where sub effecting is indeed needed to analyse the expression. The remaining 
clauses of Table 5.2 are straightforward modifications of the similar clauses 
of Table 5.1. 
Example 5.4 Let us return to the expression 
(fnx x => x) (fny y => y) 

5.1 Control Flow Analysis 
287 
of Example 5.1. Writing Tv for int ~ 
int we have the following inference 
tree: 
[x I-t Tv]l-cFAx: Tv 
[ ]I-CFA fnx x => x : Tv ~ 
Tv 
[y I-t int]l-cFA y : int 
[ ]I-CFA fny y => Y : Tv 
[ ]I-CFA (fnx x => x) (fny y => y) : Tv 
Note that the whole inference tree is needed to get full information about the 
control flow properties of the expression. IT we label all the subexpressions 
as in Chapter 3 then we can list the types of the subexpressions as follows 
and we are close to the information supplied by C in Chapter 3. The infor-
mation corresponding to p can be obtained by "merging" information from 
the various type environments of the inference tree (see Exercise 5.4). 
â¢ 
Example 5.5 Consider once again the expression loop 
let g = (funF f x => f (fny y => yÂ» 
in g (fnz z => z) 
and let us write fifx for [f I-t (T ~ 
r) .J.!:4 (T ~ r)][x ~ r ~ 
r]. 
Using the clause ffn] we have 
~ I-
~ {y,Z} 
~ 
rfx 
CFA fny y => Y : T ~ 
T 
where we exploit that subeffecting allows us to enlarge the annotation from 
the minimal {V} to {V, Z}. Using this we can construct an inference tree for: 
[ ] 
() 
(~ {y,Z} 
~) {F} 
(~ 0 
~) 
I-CFA funF f x => f fny y => Y : T ~ 
T ~ 
T --t T 
Next let fig be [g I-t (T ~ 
r) .J.!:4 (T ~ r)]. We now exploit that the 
annotation {Z} can be enlarged to {Z, Y} in the clause ffn] so that: 
r
~ I-
~ {Z,Y} 
~ 
g 
CFA fnz z => z : T ~ 
T 
Since {Z, Y} = {V, Z} we can use the clause lapp] and get 
fig I-CFA g (fnz z => z) : r ~ r 

288 
5 Type and Effect Systems 
[unit] 
tp=tpu0 
[idem] 
tp=tpU'P 
[com] 
'PI U 'P2 = 'P2 U 'PI 
[ass] 
'PI U ('P2 U 'P3) = ('PI U 'P2) U 'P3 
[ref] 
tp='P 
[trans] 
'PI = 'P2 'P2 = 'P3 
[cong] 
'PI = 'P~ 'P2 = 'P~ 
'PI = 'P3 
'PI U 'P2 = 'P~ U 'P~ 
Table 5.3: Equality of Annotations. 
and eventually [ ] f-CFA loop : 1" ~ 1". This can be interpreted as saying 
that the expression loop does not terminate: the type is 1" ~ 1" but the 
annotation 0 indicates that there will be no function abstractions with the 
given type. 
Actually we can show 
[ ] L 
( )  
(~ {Y,Z} 
~) {F} 
(~ 'P 
~) 
'-CFA funF f x => f 
fny y => Y : 7 --'----'-+ 7 
......:......:. 
7 ----t 7 
for every annotation 'P and hence we have [ ] f-CFA loop : 1" 34 1" for every 
annotation 'P; clearly the judgement with 'P = 0 is the most informative. 
_ 
Equivalence of annotations. There are a few subtleties involved 
in using this simple-minded system for Control Flow Analysis. One is the 
implicit decision to regard a type like 1" ~ 
1" as being equal to 1" ~ 
1". 
Concerning this subtlety, we already explained that we feel free to write 
{1f1,Â·Â· . ,1fn } for {1fd U ... U {1fn }. To be utterly formal we should really say 
that we write {1f1, ... ,1fn } for ((0 U {1f1}) UÂ·Â·Â·) U {1fn }. 
Next we allow to replace 71 ....'e.!.t 72 by 71 ....'e4 72 whenever 'PI and 'P2 are 
"equal as sets". To be utterly formal this can be axiomatised by the axioms 
and rules of Table 5.3: the axioms [unit], [idem], [com] and [ass] express that 
set union has a unit and is idempotent, commutative and associative, and the 
axioms and rules [trans], [reJl and [cong] ensure that equality is an equivalence 
relation as well as a congruence. 
Finally, we allow to replace 1"1 by 1"2 if they have the same underlying types 
and all annotations on corresponding function arrows are "equal as sets". To 
be utterly formal we could axiomatise this by: 
ri=r{ 
T2=T~ 
c.p=<p' 
(7'1 -.!4 T'z) = (7'{ ~ 
1"~) 
o 
It is customary to be informal about these fine technical points. But to 
avoid confusion one should at the very least point out that annotations are 

5.2 Theoretical Properties 
289 
considered equal modulo the existence of a unit, commutativity, associativity, 
and idempotencej the abbreviation UCAI is often used for this. 
Conservative extension. Another subtlety is the ability to give the 
abstraction fny y => Y the type T ~ 
T. Suppose for a moment that the 
two rules for function abstraction did not have a {7r} U cp annotation on the 
function arrows but only a {7r }. Then fny y => Y would have type T ~ 
T 
but not T ~ 
Tj consequently the program from Example 5.5 would have 
no type in the Annotated Type System for Control Flow Analysis! This is a 
very undesirable property: we ought to be able to analyse all programs. 
To ensure that our system does not have the above deficiency we shall for-
mally prove that the Control Flow Analysis of Table 5.2 is a conservative 
extension of the underlying type system of Table 5.1. This is expressed by: 
Fact 5.6 
(i) 
If r I-CFA e: T then LrJ I-UL e: LTJ. 
(ii) 
If r I-UL e : T then there exists rand T such that 
r I-CFA e : T, LrJ = r and LTJ = T. 
â¢ 
Proof The proof of (i) is straightforward. For the proof of (ii) one annotates all 
arrows with the set of all program points in e. 
_ 
This result paves the way for extending LÂ·J to operate on entire typings: 
applied to the typing r I-CFA e : T it produces a typing of the form LrJ f-UL 
e : LT J. As an example, the typing of Example 5.5 is transformed into the 
typing of Example 5.3 (assuming that LTJ = T). 
In Section 5.4 we shall study explicit inference rules for subeffecting: this is 
a related technique for ensuring that the analysis is a conservative extension 
of the underlying type system. 
5.2 
Theoretical Properties 
Having specified the analysis we shall now ensure that it is semantically 
correct. Furthermore, the fact that the analysis is a conservative extension 
of the underlying type system motivates the following result: whenever we 
have a typing in the underlying type system then the set of typings of the 
Control Flow Analysis constitutes a Moore family. So as in Section 3.2 (and 
Exercise 2.7) every acceptable expression can be analysed and it has a best 
analysis. 
As in Sections 2.2 and 3.2, the material of this section may be skimmed 
through on a first readingj however, we re-iterate that it is frequently when 

290 
[con] 
[In] 
[!un] 
[app] 
[iII] 
[let] 
[op] 
5 Type and Effect Systems 
I-c--+c 
I- (fn". x => eo) --+ (fn". x => eo) 
I- (fun". f x => eo) --+ fn". x => (eo[f I-t fun". f x => eo]) 
I- el --+ (fn". x => eo) 
I- e2 --+ V2 
I- eo[x I-t V2] --+ Vo 
I- el e2 --+ Vo 
I- eo --+ true I- el --+ VI 
I- if eo then el else e2 --+ VI 
I- eo --+ false I- e2 --+ V2 
I- if eo then el else e2 --+ V2 
I- el --+ VI 
I- e2[x t-+ VI] --+ V2 
I- let x = el in e2 --+ V2 
I- el --+ VI 
I- e2 --+ V2 
I- el op e2 --+ V 
if VI op V2 = V 
Table 5.4: Natural Semantics for FUN. 
conducting the correctness proof that the final and subtle errors in the anal-
ysis are found and corrected! 
5.2.1 
Natural Semantics 
To prove the semantic correctness of the analysis we need to define the se-
mantics. Many kinds of semantics would be appropriate but among the 
operational semantics we shall now prefer a Natural Semantics (Le. big-step 
operational semantics) without environments because this makes semantic 
correctness somewhat easier to establish. This is related to the discussion in 
the Concluding Remarks of Chapter 3 about the difference between using a 
big-step operational semantics without environments and a small-step oper-
ational semantics with environments; thus our correctness statement will be 
somewhat weak in case of looping programs. 
Transitions. The Natural Semantics will have transitions of the form 
I-e--+v 
meaning that the expression e evaluates to the value v. We shall assume that 
e E Exp is a closed expression, Le. FV(e) = 0, meaning that e does not have 

5.2 Theoretical Properties 
291 
any free variables. The values 
v E Val values 
will be a subset of the expressions given by the syntax 
v ::= c I fn". x => eo 
provided that FV(fn". x => eo) = 0 
where we demand that values are closed expressions. Compared to the Struc-
tural Operational Semantics of Section 3.2, it is not necessary to introduce 
environments since the bound variables will be syntactically replaced by their 
value as soon as they become free. Hence there is neither need for a close-
construct nor for a bind-construct. 
As usual we shall write el [x I-t e2] for substituting e2 for all free occurrences of 
x in el. It will be the case that e2 is closed whenever we use this notation and 
therefore there is no risk of variable capture and hence no need to rename 
bound variables. Throughout we shall assume that fun". f x => eo uses 
distinct variables for f and x. The semantics is given by Table 5.4 and is 
explained below. 
The axioms [con] and (fn] express that the constant and the function abstrac-
tion, respectively, evaluate to themselves. For recursive function abstractions 
we shall unfold the recursion one level as expressed by the axiom (fun]; note 
that the function abstraction being created inherits the program point of the 
recursive function abstraction. The rule [app] for application expresses that 
first we evaluate the operator, then the operand, and next we substitute the 
actual parameter for the formal parameter in the body of the abstraction and 
evaluate the body. We have only one rule for application because the axiom 
(fun] ensures that all function abstractions will be of the form fn". x => eo. 
The rules for the conditional, the let-construct and the binary operators 
should be straightforward. 
Example 5.7 Consider the expression (fnx x =) x) (fny y => y) of Ex-
ample 5.1. Using the axiom (fn] we have 
f- fnx x => x --+ fnx x => x 
f- fny y => Y --+ fny y => Y 
f- x[x I-t fny y => y] --+ fny y => Y 
and we can apply the rule [app] to get: 
f- (fnx x => x) (fny y => y) --+ fny y => Y 
In Example 3.7 we showed how the Structural Operational Semantics deals 
with this expression. 
_ 

292 
5 Type and Effect Systems 
Example 5.8 Next consider the expression loop 
let g = (funF f x => f (fny y => y)) 
in g (fnz z => z) 
and let us see how this looping program is modelled in the Natural Semantics. 
First we observe that the axiom [fun] gives 
I- funF f x => f (fny y => y) ---t 
fnF x => ((funF f x => f (fny y => y)) (fny y => y)) 
so we have replaced the recursive call of f with the recursive function defini-
tion itself. Turning to the body of the let-construct we have to replace the 
occurrence of g with the abstraction: 
fnF x => ((funF f x => f (fny y => y)) (fny y => y)) 
The operator will now evaluate to this value and the operand fnz z => z 
will evaluate to itself. So the next step is to determine a value v such that 
we have an inference tree for 
I- (funF f x => f (fny y => y)) (fny y => y) ---t v 
(5.1) 
and after that we are in a position to use the rule for application. The eval-
uation of the operator in (5.1) proceeds as before and so does the evaluation 
of the operand and once more we are left with the obligation to construct an 
inference tree for the judgement (5.1). Thus we have encountered a circular-
ity and we see that the looping of the program is modelled in the semantics 
by the absence of an inference tree. In Example 3.8 we showed how the 
Structural Operational Semantics deals with the expression loop. 
â¢ 
It is immediate to verify that if e is a closed expression and I- e ---t v then 
all I- e' ---t v' occurring in the corresponding inference tree (in particular 
I- e ---t v itself) will have both e' and v'to be closed. 
5.2.2 
Semantic Correctness 
To be able to express the semantic correctness of the analysis we need to 
assume that the types of the binary operators op and their semantics are 
suitably related. Recall that for the underlying type system we assume that 
op takes two arguments with the base types T!p and T;p and gives a result 
of type Top and since base types do not have any annotations we shall now 
assume that: 
If [ ]I-CFA VI : T!p and [ ]I-CFA V2 : T;p then [ ] I-CFA v : Top 
where v = VI op V2. 

5.2 Theoretical Properties 
293 
This ensures that when given arguments of appropriate types the operator 
will return a value of the expected type. 
The semantic correctness of the analysis now expresses that the annotated 
type foreseen by the analysis will also be the annotated type of the value 
obtained by evaluating the expression; this is expressed by the following 
subject reduction result: 
Theorem 5.9 
If [ ]I-CFA e : 7, and I- e -+ v then [ ]I-CFA v : 7. 
It follows that if [ ]1- e : 71 ~ 
72 and I- e -+ fn,.. x => eo then 7r E CPo; 
hence the analysis correctly tracks the closures that can result from a given 
expression. Also note that if [ ]1- e : 71 ~ ~ then e cannot terminate. 
In preparation for the proof of the theorem we need a few technical results 
of the sort that are standard for type systems. The first result expresses 
that the type environment may be extended with information that does not 
influence the analysis result: 
Fact 5.10 If i\ I-CFA e : 7 and r\(x) = i\(x) for all x E FV(e) then 
i\ I-CFA e : 7. 
â¢ 
Proof The proof is by induction on the inference tree for r\ I-CFA e : T using that 
it is the rightmost occurrence of a variable in a type environment that determines 
its type. 
_ 
The next result shows that we can safely substitute an expression of the 
correct annotated type for a variable: 
Lemma 5.11 Assume [ ] I-CFA eo : TO and r[x I--t 70]I-CFA e : 7. Then 
r I-CFA e[x !--to eo] : 7. 
â¢ 
Proof The proof is by structural induction on e. Most cases are straightforward 
so we shall only present the cases of variables and function abstractions. 
The case y. We assume that 
f[x I--t TO]I-CFA y : T 
so from the axiom [var] of Table 5.2 we have (f[x I--t TO])(Y) = T. If x = Y then 
y[x I--t eo] = eo and T = TO, and from [ ] I-CFA eo : To and Fact 5.10 we get the 
required I' I-CFA eo : T. If x =1= y then y[x I--t eo] = y and it is easy to see that 
I' I-CFA y: T. 
The case fn,.. y => e. We assume that 
f[x I--t To]I-CFA fn,.. y => e : T 

294 
5 Type and Effect Systems 
so, according to rule [In] of Table 5.2, it must be the case that T = Ty ~ 
T' 
and: 
f[x ~ 'To][y ~ 'Ty]I-CFA e : T' 
If x = y then it follows from Fact 5.10 that fry ~ Ty] I-CFA e : T' and since 
(fn ... y => e)[x ~ eo] = fn ... y => e the result follows. So assume that x =1= Yj 
then (fn ... y => e)[x ~ eo] = fn ... y => (e[x ~ eo]). It follows from Fact 5.10 that 
fry ~ Ty][X ~ To]l-cFA e : T' and then we get fry ~ Ty]I-CFA e[x ~ eo] : T' from 
the induction hypothesis. Now the result follows using the rule [In]. 
â¢ 
We now turn to the proof of Theorem 5.9: 
Proof The proof proceeds by induction on the inference tree for I- e --t v. 
The cases [con] and [In] are immediate. 
The case [fun]. We assume that 
I- fun ... f x => eo --t fn ... x => eo[f ~ fun ... f x => eo] 
where f and x are distinct variables. Also we have 
[ ]I-CFA fun ... f x => eo : r., { ... }U'Po) TO 
and according to Table 5.2 this is because [f ~ T., { ... }U'fo) TO][X ~ r.,]I-CFA eo : TO. 
Since [f ~ T., 
{",}U'Po) TO][X ~:r..] equals [x ~ T.,J[f ~ r., ~ 
TO] (because 
we assumed that f and x are distinct) it follows from Lemma 5.11 that 
[x ~ T.,]I-CFA eo[f ~ fun ... f x => eo] : TO 
and hence []I-CFA fn ... x => eo[f ~ fun.". f x => eo] : r., 
{",}U'fo) TO which is the 
desired result. 
The case lapp]. We assume that 
because I- el --t fn ... x => eo, I- e2 --t V2 and I- eo[x ~ 
V2] --t Vo. Also we have 
[ ]I-CFA el e2 : TO 
and according to Table 5.2 this is because []I-CFA el : T2 -'4 To and []I-CFA e2 : T2. 
The induction hypothesis applied to the inference tree for el gives: 
[ ]I-CFA fn ... x => eo : T2 4 
TO 
According to Table 5.2 this can only be the case if 7r E cp and [x ~ T2]I-CFA eo : TO. 
The induction hypothesis applied to the inference tree for e2 gives 
[ ]I-CFA V2 : T2 
and by Lemma 5.11 we now get []I-CFA eo[x ~ 
V2] : TO. The induction hypothesis 
applied to the inference tree for eo[x ~ V2] now gives 
[ ] I-CFA Vo : TO 

5.2 Theoretical Properties 
and this is the desired result. 
The case [ifl]. We assume that 
f- if eo then el else e2 --+ VI 
because f- eo --+ true and f- el --+ VI. Also we have 
[ ] f-CFA if eo then el else e2 : 7 
295 
and from Table 5.2 we see that this is because [ ] f-CFA eo : bool, [ ] f-CFA el : 7 and 
[ ] f-CFA e2 : 7. The induction hypothesis gives 
and this is the desired result. 
The case [ih] is analogous. 
The case [let]. We assume that 
[ ] f-CFA VI : 7 
because f- el --+ VI and f- e2[x f-t vI] --+ V2. Also we have 
[ ] f-CFA let x = el in e2 : 72 
and this is because [ ] f-CFA el : 71 and [x f-t 7I] f-CFA e2 
T2. The induction 
hypothesis now gives: 
[ ] f-CFA VI : 71 
From Lemma 5.11 we then get [ ] f-CFA e2[x f-t vI] : 72 and the induction hypothesis 
gives 
as required. 
The case lop]. We assume that 
f- el op e2 --+ V 
because f- el --+ VI, f- e2 --+ V2 and VI op V2 = v. Also we assume 
[ ] f-CFA el op e2 : Top 
and this can only be because [ ] f-CFA el : T;p and [ ] f-CFA e2 : T;p. The induction 
hypothesis gives [ ] f-CFA VI : T;p and [ ] f-CFA V2 : T;p and the desired result that 
[ ] f-CFA V : Top then follows from the stated assumptions about the relationship 
between the semantics and the types of the binary operators. 
_ 
5.2.3 
Existence of Solutions 
In Chapter 3 we showed that the set of solutions to the Control Flow Analysis 
constituted a Moore family: the greatest lower bound of a set of solutions is 
also a solution. (Also see Exercise 2.7.) From that result it then follows that 
all programs can be analysed and that there is a best analysis. 

296 
5 Type and Effect Systems 
Complete lattice of annotations. A similar development is pos-
sible here except that special care has to be taken concerning the nature of 
annotations. We would like to be able to regard the set Ann of annotations 
as a complete lattice, and this necessitates a partial ordering 
<P1 ~ <P2 
or 
<P1 ~ <P2 
that intuitively means that the set <P1 of program points is included in the 
set <P2 of program points. One way to formalise this is to say 
3<p' : <P1 U <p' = <P2 
where we rely on the axiomatisation of "equal as sets" presented in Table 5.3. 
Another way is by means of an explicit system of rules and axioms for in-
ferring judgements of the form <P1 ~ <P2; we shall dispense with these details 
(but see Exercise 5.3). 
One way to ensure that (Ann,~) is a complete lattice is to demand that 
all annotations are subsets of a given finite set; for this one might replace 
Pnt by the finite set Pnt* of program points occurring in the expression of 
interest. Another possibility will be to change the syntax of annotations to 
allow expressing an arbitrary subset of Pnt. Either approach works, since 
all that suffices for the subsequent development is to assume that: 
(Ann,~) is a complete lattice isomorphic to (P(Pnt), ~). 
The partial ordering on Ann will sometimes be written as ~ and sometimes 
as ~. 
Complete lattice of annotated types. We can now extend the 
partial ordering on annotations to operate on annotated types that have the 
same underlying type. For this let rEType be a type and write 
Type[r] 
for the set of annotated types 7 with underlying type r, i.e. such that L7J = r. 
Next define 71 ~ 72 for 71,72 E TYPe[r] to mean that whenever ~ has the 
annotation <Pi in a given position then <P1 ~ <P2. Formally this is achieved 
by: 
I 
1) ...!4 72 ~ 7{ 1-t 7~ 
As an example, (int 11t int)..!Â£4 int ~ (int --'Â£4 int) -!Â£4 int will 
be the case if and only if <P1 ~ <P3 and <P2 ~ <P4' (Note that this ordering 
is not contravariant unlike what will be the case for the subtyping to be 
introduced in Section 5.4.) Clearly the least element in Type[r] will have 0 
on all function arrows occurring in r and the greatest element will have Pnt 
on all function arrows. 

5.2 Theoretical Properties 
297 
It is straightforward to prove that each (Type[T], !;;;) is a complete lattice. 
In a similar way the partial ordering can be extended to operate also on type 
environments having the same underlying structure. Finally, suppose that 
r rUL e : T and write 
JUDGCFA[r rUL e : T] 
for the set of typings r rCFA e : T such that lÂ·J maps r rCFA e : T to 
r rUL e : T (and hence lrJ = rand lTJ = T). This facilitates extending the 
partial ordering!;;; to operate on the set JUDGCFA[r rUL e : T] of typings. 
Moore family result. We are now ready to state the result about 
Moore families: 
Proposition 5.12 
JUDGcFA[r rUL e : T] is a Moore family whenever r rUL e : T. 
Proof We assume that r r-UL e : T and prove that JUDGCFA[r r-uL e : T) is a 
Moore family. For this we proceed by induction on the shape of the inference tree 
for r r-UL e : T. We shall only give the proof for the cases [var), (fn] and lapp); the 
other cases follow the same overall pattern. In all cases let 
y = {(ri r-CFA e : 7') liE I} 
be a subset of JUDGCFA[r r-UL e : T). Using that (Type[T'],!;;;) is a complete lattice 
for all choices of T' E Type we get that ny exists and that it is defined in a 
pointwise manner. (Note that if I = 0 then ny is obtained from r r-UL e : T by 
placing n0 = Pnt as annotation on all function arrows.) It remains to show that 
ny E JUDGcFA[r r-UL e : T). 
The case [var]. The result follows from (n;fii)(x) = ni(ri(xÂ». 
The case (fn]. We have r r-UL fn" x => eo : T" -t TO because r[x t-+ T,,] r-UL eo : TO. 
For i E I we have fi r-CFA fn" x => eo : T;, 
{,,}u<p~ ~ because of 
-. 
~ 
-::::i 
and clearly r'[x t-+ T,,) r-CFA eo : TO is an element of JUDGCFA[r(X t-+ T,,] r-UL eo : TO]. 
By the induction hypothesis we get (nj;i)[x t-+ niT;,) f-CFA eo : ni~ and hence 
where <p = ni <pi. 
The case lapp). We now have r r-UL el e2 : TO because r r-UL el : T2 -t TO and 
r r-UL e2 : 72. For all i E I we have fi r-CFA el e2 : ~ because 

298 
5 Type and Effect Systems 
and clearly ri I-CFA el : ~ ~ 
T(i is an element of JUDGcFA[r I-UL el : T2 ~ 
TO] and 
ri I-CFA e2 : ~ is an element of JUDGCFA[r l-uL e2 : n]. By the induction hypothesis 
we get niri I- el : ni~ ...!4 niT(i and niri I- e2 : ni~ where cp = ni cpi and hence 
niri I- el e2 : ni~ 
which is the desired result. 
Example 5.13 Consider the expression e: 
f (fnx x => x+l) + f (fny y => y+2) + (fnz z => z+3) (4) 
In the underlying type system we have: 
[I I-t (int ~ int) -+ int] I- e : int 
In the Control Flow Analysis we have 
[f I-t (int ~ int) ~ 
int] I- e: int 
â¢ 
whenever {X, Y} ~ CPl. The least solution therefore has CPl = {X, Y} and 
CP2 = 0. This clearly tells us that f is only applied to (fnx x => x+1) 
and (fny y => y+2) and not to (fnz z => z+3). A larger solution like 
CPl = {X, Y, Z} and CP2 = {V} would not convey this information. Hence it 
seems sensible to ask for the least solution with respect to [;;; and the existence 
of this solution is guaranteed by Proposition 5.12. 
â¢ 
5.3 
Inference Algorithms 
The main difference between an analysis expressed in the form of an inference 
system (as in Table 5.2) and in the form of an algorithm is that the user of 
the inference system is expected to have sufficient foresight to be able to 
guess the right types and annotations whereas the implementation of the 
algorithm will make use of a mechanism for making tentative guesses that 
are later refined. Let us first consider the simple case corresponding to the 
underlying type system of Table 5.1. 
5.3.1 
An Algorithm for the Underlying Type System 
Augmented types. The algorithm corresponding to the type system 
of Table 5.1 will work on augmented types that allow the use of type variables 
to reflect that the details of a type are not fully determined yet: 
T 
E AType augmented types 
Q 
E TVar 
type variables 

5.3 Inference Algorithms 
We shall take: 
7 
.. -
int I boo 1 I 71 -t 72 I a 
a 
.. -
'a I 'b I 'e IÂ·Â·Â· 
299 
Substitutions. A substitution is a finite and partial mapping from type 
variables to augmented types, we write 
() : TVar -tfin AType 
and note that the domain dom( ()) = {a I () is defined on a} is finite. We 
shall allow to view a substitution as a total function from type variables to 
augmented types, setting () a = a whenever a ~ dom(()). We shall say that 
a substitution () is defined on a if and only if a E dom(()). 
The substitution () is called a ground substitution if and only if it maps all type 
variables in its domain to ordinary types, i.e. if Va E dom(()) : () a E Type. 
The substitution () is said to cover f, respectively 7, if and only if it is defined 
on all the type variables in f, respectively 7. Substitutions can be applied to 
augmented types in a pointwise manner: 
() int 
int 
() bool 
= 
bool 
()(71 -t 72) 
= 
(() 7d -t (() 72) 
()a 
7 if()a=7 
We shall write ()1 o()2 for the composition of ()1 and ()2, Le. (()1 O()2)7 = ()1 (()2 7) 
for all augmented types 7. 
The idea. The type reconstruction algorithm, called WUL, is given two 
arguments: an augmented type environment f (mapping program variables 
to augmented types) and an expression e. If it succeeds in finding a typing 
for the expression then it will return its augmented type 7 and a substitution 
() telling how the type environment has to be refined in order to obtain a 
typing. As an example we will have 
WUL([X I-t 'a], 1 + (x 2)) = (int, ['a f-t int -t int]) 
because during the inspection of the expression 1 + (x 2) it becomes clear 
that x must be a function from integers to integers if the expression is to be 
correctly typed. So the idea is that if 
WUL(f,e) = (7,()) 
then 
()a(() f) f-UL e: ()a 7 
for every ground substitution ()a that covers () f and 7, Le. whenever we 
replace all the type variables with ordinary types in a consistent way. When 

300 
5 Type and Effect Systems 
this property holds we shall say that the algorithm is syntactically sound. 
In order for the algorithm to be syntactically complete it is also required 
that all typings obtainable in the inference system can be reconstructed from 
the results of the algorithm. We shall discuss these properties at length in 
Subsection 5.3.3. 
The algorithm. The algorithm WUL is specified in Table 5.5 and is 
explained below. The algorithm asks for fresh type variables at several places. 
By this is meant type variables that do not occur in the argument to WUL 
and that have not been generated as fresh variables elsewhere; this could be 
formalised by supplying WUL with yet another parameter for tracking the 
type variables that remain fresh but it is customary not to do so. There is a 
small amount of nondeterminism in WUL in that there may be many choices 
for the fresh variables; this could be made precise by assuming they are all 
numbered and by always supplying the candidate with the smallest number 
but again it is customary not to do so. 
In the clause for constants we simply note that the type of c is 7c and there 
is no need to adjust our assumptions r so we return the identity substitution 
id: we demand that id a = a for all a and could take id to be the empty 
mapping. The clause for variables is similar except that now we consult the 
type environment r to determine the augmented type of x. 
For function abstraction we assume that the formal parameter has type ax 
for ax being a fresh type variable - so far we have no constraints on the type 
of the formal parameter. Then we call WUL recursively on the function body 
to determine its type under the assumption that x has type ax. The resulting 
type 70 and substitution (}o are then used to construct the overall type; in 
particular, (}o is used to replace the type variable ax with a more refined 
type since the analysis of the function body may have provided additional 
information about the type of the formal parameter. 
The clause for recursive function definition is somewhat more complicated. 
It starts out in a way similar to the previous clause and requires fresh type 
variables ax and ao so that we can supply augmented types ax -+ ao and ax 
for the occurrences of f and x, respectively, in the analysis of the function 
body. However, this will result in two possible types for the function body: 
one is the type variable ao (modified by the substitution (}o obtained by 
analysing the function body) and the other is the type 70 obtained from the 
analysis of the function body. These two types have to be equal according to 
the rule [In] of Table 5.1 and to ensure this we shall use a unification procedure 
UUL; its definition will be discussed in detail below but the idea is that given 
two augmented types 71 and 72, UUL (71,72) will return a substitution (} that 
makes them equal i.e. such that (} 71 = (} 72. In the clause for recursive 
function definition we get that (}1 (70) = (}1 ((}o ao) so the overall type will 
be (}1 ((}o ax) -+ (}1 70. Also we record that the assumptions of r have to be 
modified by (}o as well as (}1. 

5.3 Inference Algorithms 
WUL(f, c) = (Te, id) 
Wudf, x) = (f(x), id) 
WUL(f, fn.,.. x => eo) = let a., be fresh 
(TO, ( 0 ) = WUL(r[X t-+ a.,j,eo) 
in 
((90 a.,) ~ TO, ( 0 ) 
WUL(f, fun ... ! x => eo) = 
let 
a." ao be fresh 
(TO, ( 0 ) = WUL(r[! t-+ a., ~ ao][x t-+ a.,j,eo) 
91 = UUL(TO,90 ao) 
in 
(91 (90 a.,) ~ 91 TO, 91 0(0 ) 
WUL(f,el e2) = let (Tl,91) = WUL(f,el) 
(T2,92) = Wud91 f,e2) 
a be fresh 
93 = Uud92 Tl,T2 ~ a) 
in 
(93 a, 930920 ( 1) 
WUL(f, if eo then el else e2) = let 
(TO, ( 0) = WUL(f,eo) 
(T1,9t) = WUL(90 f,el) 
(T2,92) = WUL(91 (90 f),e2) 
93 = Uud(Jz(91 TO), bool) 
94 = Uud93 T2,93(92 Tl)) 
in 
(94(93 T2), 940930920 9t) 
WUL(f,let x = el in e2) = let (Tl,9t) = WUL(r,el) 
(T2,92) = Wud(91 f)[x t-+ Tlj,e2) 
in 
(T2, 92 0 9t) 
WUL(f,el Op e2) = let 
(Tl,91)=Wudf,et) 
(T2,92) = WUL(91 f,e2) 
93 = UUL(92 Tl, T!p) 
94 = UUL(93 T2, T;p) 
in 
(Top, 940930920 ( 1 ) 
Table 5.5: Algorithm WUL for the underlying type system. 
301 
Also the clause for function application relies on the unification procedure. 
In this clause we call WUL recursively on the operator and the operand and 
we use unification to ensure that the type of the operator Tl (modified by 
( 2 ) is a function type with an argument type that equals the type T2 of the 
operand: 92 Tl has to have the form T2 ~ a. Again we have to record that 
the assumptions of f have to be modified by all three substitutions that have 
been constructed. 

302 
5 Type and Effect Systems 
By now the clause for conditional is straightforward and similarly the clauses 
for the let-construct and the binary operator. (Note that the let-construct 
is not polymorphic and therefore no special action is needed to determine the 
type of the bound variable; we shall return to the problem of polymorphism 
later.) 
Example 5.14 Consider the expression (fnx x => x) (fny y => y) of 
Example 5.1. The call 
WUL([ ], (fnx x => x) (fny y => y)) 
gives rise to the call 
WUL([ ], fnx x => x) 
which will create the fresh type variable 'a and return the pair ('a -+ 'a, id). 
We now have the call 
WUL([ ], fny y => y) 
that creates the fresh type variable 'b and returns ('b -+ 'b, id). Thus we get 
the following call of UUL 
UUL('a -+ 'a, ('b -+ 'b) -+ 'e) 
where 'e is a fresh type variable. As we shall see in Example 5.15 below this 
gives rise to the substitution ['a H 'b -+ 'b]['e H 'b -+ 'b] and the initial call 
of WUL will return 'b -+'b and ['a H'b -+ 'b]['e H 'b -+ 'b]. 
â¢ 
Analogy. The placement of substitutions in Table 5.5 may seem ad hoc 
at first sight. As an aid to the intuition we shall therefore offer the following 
analogy. Consider a society where a number of laws have been passed. One 
such law might stipulate that when the owner of a personal computer sells 
it, the owner is obliged to comply with the law and in particular provide the 
buyer with the original disks for all software that remains on the computer. 
Let us focus our attention on a particular owner preparing to sell a computer 
and who is determining an acceptable selling price. Then parliament passes 
a law stating that whenever original software disks are passed on from one 
private person to another, the previous owner has to pay a fee of ten percent of 
the original buying price to a government agency combating software piracy. 
From this day on the owner of the computer needs to reconsider all the 
considerations made in order to determine the acceptable selling price. In 
short: whenever a new law is passed all existing considerations need to be 
reconsidered in order to remain valid. -
Coming back to Table 5.5, the 
typings determined by WUL correspond to the considerations of the owner, 
and the substitutions produced by UUL to the new laws being passed by 
parliament: whenever a new substitution ("law") is constructed all existing 

5.3 Inference Algorithms 
UUL(int, int) 
UUL(bool, bool) 
= 
id 
id 
let (h = UUL(71 , 7{) 
()2 = UUL(()1 72,{)1 7~) 
in 
{)2 0 {)1 
{ 
[a f-t 7] 
if a does not occur in 7 
or if a equals 7 
fail 
otherwise 
{ 
[a f-t 7] 
if a does not occur in 7 
or if a equals 7 
fail 
otherwise 
fail 
in all other cases 
Table 5.6: Unification of underlying types. 
303 
typings ("considerations") must be suitably modified so as to remain valid; 
this is exactly what is achieved by the carefully chosen use of substitutions 
in Table 5.5. 
0 
Unification. The algorithm UUL for unifying two augmented types is 
shown in Table 5.6. It takes two augmented types 71 and 72 as arguments 
and if it succeeds it returns a substitution () such that () 71 = () 72. 
In the clause for unifying the two function types 71 -+ 72 and 7{ -+ 7~ the 
desired result is obtained in two stages: {)1 ensures that {)1 71 = {)1 7{ and 
hence (e2 0 ()1) 71 = ({)2 0 ()t) 7{ and {)2 ensures that {)2 ({)1 72) = {)2 ({)1 7~); it 
follows that {)2 0 {)1 succeeds in unifying the two function types. Note that 
the algorithm only fails at top-level if: 
â¢ two types with different top-level constructors (by which we mean int, 
bool, or -+) are to be unified, or 
â¢ a type variable is to be unified with a function type containing that 
type variable. 
Example 5.15 In Example 5.14 we had the following call of the unifica-
tion procedure: 
UUL(/a -+ la, ('b -+ Ib) -+ Ie) 
It will first give rise to the call UUL (la, Ib -+ Ib) which returns the substitution 
[/a f-t Ib -+ Ib]. Then we will have the callUUL(/b -+ Ib, Ie) and the substitution 

304 
5 Type and Effect Systems 
['e f-t 'b ---t 'b] is returned. Thus the overall result will be the substitution 
['a f-t 'b ---t 'b]['e f-t 'b ---t 'b] as already used in Example 5.14. 
â¢ 
5.3.2 
An Algorithm for Control Flow Analysis 
In applying the above ideas to the inference system for Control Flow Analysis 
presented in Table 5.2 we are going to face a difficulty. In the underlying type 
system two types are equal if and only if their syntactic representations are 
the same; we say that types constitute a free algebra. For annotated types, 
two annotated types may be equal even when their syntactic representations 
are different: int {,..dU{"'2}) 
int equals int 
{"'2}U{,..d) 
int because 
annotations and types are considered equal modulo UCAl as discussed in 
Subsection 5.1.2 - we say that annotated types constitute a non-free algebra. 
The difficulty we encounter when transferring the development of the previous 
subsection to the Control Flow Analysis is that the algorithm WUL relies 
on the procedure UUL for unifying two types, and that this algorithm only 
applies to types in a free algebra. One way out of this difficulty would be to 
use instead an algorithm for unifying modulo UCAI; such algorithms exist 
but their properties are not so nice as those of UUL. Another way out of the 
difficulty, and the approach we shall take, is to arrange it such that a variant 
of UUL can still be used by introducing additional mechanisms for dealing 
with the annotations: one is the notion of simple types and annotations and 
the other is the use of constraints. 
Simple types and annotations. The first step will be to restrict 
the form of the annotated types so that only annotation variables are allowed 
on the function arrows; later we shall combine this with a set of constraints 
restricting the values of the annotation variables. 
A simple type is an augmented annotated type where the only annotations 
allowed on function arrows are annotation variables and where type variables 
are allowed in types, and a simple annotation is an annotation where also 
annotation variables are allowed: 
T 
E 
SType simple types 
Q 
E 
TVar 
f3 
E 
AVar 
type variables 
annotation variables 
cp 
E SAnn 
simple annotations 
Formally, the syntactic categories are given by: 
7 
.. -
int I boo 1 I 71 ~ 72 I Q 
Q 
â¢â¢ -
'a I'b I'e I ... 
f3 
'11'21'31Â·Â·Â· 

5.3 Inference Algorithms 
305 
UCFA(int, int) = id 
UCFA(bool, bool) = id 
UCFA(7i J4 T2 T' -.i4 7,' ) 
, 1 
2 
= 
let 
()o = [.8' I-t .8] 
()1 = UCFA (()o Tl, ()o Tn 
()2 = UCFA(()1 (()o T2), ()1 (()o T~)) 
in 
()2 0 ()1 0 ()o 
{ 
[a I-t T] if a does not occur in T 
UCFA(7,a) = 
or if a equals T 
fail 
otherwise 
{ 
[a I-t T] 
if a does not occur in T 
UCFA(a,T) 
or if a equals T 
fail 
otherwise 
UCFA (71, T2) 
fail 
in all other cases 
Table 5.7: Unification of simple types. 
A simple type environment f then is a mapping from variables to simple 
types. 
Unification of simple types. Simple types constitute a free algebra 
and so we can apply the same technique as in Subsection 5.3.1 to define a 
function UCFA of the following form: 
UCFA(71,T2) = () 
Here Tl and T2 are simple types and () will be a simple substitution: A simple 
substitution is a substitution that maps type variables to simple types, and 
that maps annotation variables to annotation variables only. Thus a simple 
substitution applied to a simple type still gives a simple type. As for UUL 
the intention is that () unifies Tl and T2, i.e. () Tl = () T2. In case this is not 
possible the outcome of UCFA (71, T2) will be a failure. The unification function 
UCFA is defined in Table 5.7 and is explained below. 
As before id is the identity substitution and ()' 0 ()" is the composition of two 
substitutions. In the clause for unifying the two function types Tl J4 T2 
and T{ 1-t T2 the desired result is obtained by first ensuring that the 
two annotation variables are equal and then proceeding as in the unification 
algorithm for the underlying types. Note that UCFA (71, T2) will fail if and only 
if UUL (LTd, LT2J) fails. 

306 
5 Type and Effect Systems 
Example 5.16 Consider the following call of the unification procedure: 
'1 
'2 
'3 
UCFA('a -t 'a, ('b -t 'b) -t 'e) 
We construct the substitution [/3 t-t '1] and then we perform the call 
(' 
I 
'2 ') 
UCFA a, b -t b 
which returns the substitution [/a t-t 'b -4 'b]. Then we make the call 
( I 
'2 
I 
') 
UCFA b -t b, e 
and the substitution ['e t-t 'b -4 'b] is returned. Thus the overall result will 
be [/3 t-t 11][/a t-t 'b -4 'b][/e t-t 'b -4 'b]. 
â¢ 
The following fact, whose proof we leave to Exercise 5.8, expresses that the 
algorithm is correct. The first part of the result says that the algorithm is 
syntactically sound: if it succeeds then it produces the desired result. The 
second part says that the algorithm is syntactically complete: if there is some 
way of unifying the two simple types then the algorithm will succeed in doing 
so. 
Fact 5.17 Let 7\ and 72 be two simple types . 
â¢ If UCFA (71,72) = () then () is a simple substitution such that () 71 = () 72. 
â¢ If there exists a substitution ()" such that ()1I71 = ()1I72 then there exists 
substitutions () and ()' such that UCFA (71,72) = () and ()" = ()' 0 (). 
â¢ 
Constraints. Annotated types can contain arbitrary annotations and 
simple types can only contain annotation variables. To make up for this 
deficiency we shall introduce constraints on the annotation variables. A con-
straint is an inclusion of the form 
where j3 is an annotation variable and c.p is a simple annotation. A constraint 
set C is a finite set of such constraints. 
We shall write () C for the set of inclusions obtained by applying the substi-
tution () to all the individual constraints of C: if j3 :2 c.p is in C then () j3 :2 () c.p 
is in () C. If C is a constraint set and () is a simple substitution then also () C 
is a constraint set. 
A type substitution is a substitution that is defined on type variables only 
and that maps them to types in Type (Le. to annotated types without type 

5.3 Inference Algorithms 
307 
and annotation variables); we shall say that it covers r, respectively r, if it 
is defined on all type variables in r, respectively r. Similarly, an annotation 
substitution is a substitution that is defined on annotation variables only 
and that maps them to annotations in Ann (Le. to annotations without 
annotation variables); it covers r, respectively r or C, if it is defined on all 
annotation variables in r, respectively r or C. An annotation substitution 
(J A solves a constraint set C, written 
if and only if it covers C and for each /3 2 t.p in C it is the case that (J A /3 is 
a superset of, or is equal to, (J A t.p. 
A ground substitution is an annotation substitution on annotation variables 
and a type substitution on type variables. A substitution (J is a ground 
validation of Cr, r, C) if and only if it is a ground substitution that covers 
r, r and C and such that (J F C (or more precisely, (J A F C where (J A is the 
restriction of (J to the annotation variables). 
The algorithm. We are now ready to define an analogue of the type 
reconstruction algorithm WUL. It has the form 
WCFA(r, e) = (r, (J, C) 
where we demand that r is a simple type environment (Le. that it maps 
variables to simple types), and it will be the case that r is a simple type, (J is 
a simple substitution, and C is a constraint set of a very special form: it only 
contains constraints of the form /3 2 {7r}. Since WCFA will make use of UCFA 
there is also the possibility that WCFA fails. The algorithm WCFA is defined 
by the clauses of Table 5.8 and is explained below. 
In the clauses for constants and variables we proceed as before and do not 
impose any constraints. In the case of function abstraction we shall addi-
tionally use a fresh annotation variable /30; it will be the top-level annotation 
of the overall type and we shall add a constraint /30 2 {7r} requiring that 
it includes the label 7r of the function definition - this corresponds to the 
annotation {7r} U t.p used in the rule [In] of Table 5.2. 
The clause for recursive function definition is modified in a similar way. Here 
the annotation variable /3 will be used to annotate the function type of f and 
the call of WCFA on the body of the function may cause it to be modified to 
(Jo /30. Next the call of the unification procedure may cause it to be further 
modified to (Jl ((Jo /30). Since both (Jo and (Jl are simple substitutions we 
know that (Jl ((Jo /30) is an annotation variable so the resulting type will still 
be simple. The recursive call of WCFA gives rise to a constraint set Co that 
has to be modified using the substitution (Jl and as in the clause for ordinary 
function abstraction we have to add a new constraint expressing that the 

308 
5 Type and Effect Systems 
WCFA(r,C) = (Te, id, 0) 
WCFA(r, x) = (r(x), id, 0) 
WCFA (r, fn'll" x => eo) = let a., be fresh 
(70,00,00) = WCFA(r[X I--t a.,], eo) 
{30 be fresh 
in 
Â«00 are) .At 70, 00, 00 U {(30 2 {-II"}}) 
WCFA(r, fun'll" f x => eo) = 
let are, ao, (30 be fresh 
(70,00,00) = WCFA(r[f I--t are .At ao][x I--t are], eo) 
01 = UCFA(70,00 ao) 
in 
(01 (00 are) 
(h(60 ,80Â» 
01 70, 01 000 , 
(01 00) U {01 (00 (30) 2 {11"}}) 
WCFA(r,fh e2) = let (1),01,01 ) = WCFA(r,eIl 
(~,02,02) = WCFA(Ol r,e2) 
a, (3 be fresh 
03 = UCFA(02 71,72 J4 a) 
in 
(03 a, 03002 001 , 03 (02 0 1) U 03 O2) 
WCFA(r, if eo then e1 else e2) = 
let (70,00,00) = WCFA(r,eo) 
(71,01,01) = WCFA(OO r,e1) 
(72,02,02) = WCFA(Ol (00 r),e2) 
03 = UCFA(02 (01 70), bool) 
04 = UCFA(03 ~,03 (02 Til) 
in 
(04 (03 72), 04 003 0 02001 000 , 
04 (03 (02 (01 0 0))) U 04 (03 (02 0 1Â» U 04 (03 O2)) 
WCFA(r, let x = e1 in e2) = 
let (71,01,01) = WCFA(r,e1) 
(72,02,02) = WCFAÂ«Ol r)[x I--t 71],e2) 
in 
(72, 02001, (02 0 1) U 02) 
WCFA(r,e1 op e2) = let (71,Ol,Od = WCFA(r,e1) 
(72,02,02) = WCFA(Ol r,e2) 
03 = UCFA(02 71, T!p) 
04 = UCFA(03 72, T;p) 
in 
(Top, 04003 0 02 0011 
04 (03 (02 Oil) U 04 (03 O2)) 
Table 5.8: Algorithm WCFA for Control Flow Analysis. 

5.3 Inference Algorithms 
309 
annotation variable (h ((}o (30) has to contain the label 71' of the function 
definition. 
The clauses for the remaining constructs are fairly straightforward modifica-
tions of the similar clauses for WUL. Note that the substitutions are applied 
to the constraint sets as well as the types and type environments. 
Example 5.18 Returning to the expression (fnx x => x) (fny y => y) of 
Example 5.1 we shall now consider the call: 
WCFA([], (fnx x => x) (fny y => y)) 
It gives rise to the call WCFA([ j, fnx x => x) which creates a fresh type variable 
'a and a fresh annotation variable '1 and returns ('a ~ 'a, id, {'1 2 {X}}). 
Then we have the call WCFA([ j, fny y => y) that creates a fresh type variable 
'b and a fresh annotation variable '2 and returns ('b ~ 'b, id, {'2 2 {Y}}). 
Thus we get the following call of UCFA 
'1 
'2 
'3 
UCFA('a -+ 'a, ('b -+ 'b) -+ 'e) 
where 'e is a fresh type variable and '3 is a fresh annotation variable. This 
gives rise to ['3 I-t '1]['a I-t 'b ~ 'b]['e I-t 'b ~ 'bj as shown in Example 5.16 
and the initial call of WCFA will return the type 'b ~ 'b, the substitution 
['3 I-t '1]['a I-t' b ~ 'b]['e I-t 'b ~ 'bj and the set {'1 2 {X}, '2 2 {Y}} of 
constraints. This corresponds to the typing obtained in Example 5.4. 
â¢ 
Example 5.19 Consider the program loop 
let g = (funF f x => f (fny y => y)) 
in g (fnz z => z) 
of Example 5.2 and the call WCFA([ ], loop). This will first give rise to a call 
WCFA([ j, funF f x => f (fny y => y)) 
that returns the type ('a -4 'a) -4 'b and the set {'1 2 {F}, '2 2 {Y}} of 
constraints. Then we have a call of WCFA on the body of the let-construct: 
'2 
'1 
WCFA([g I-t ('a -+ 'a) -+ 'bj,g (fnz z => z)) 
The call of WCFA on fnz z => z will give the type 'e ~ 'e and the constraint 
set {'3 2 {Z}}. The unification procedure will then be called with the types 
'2'1 
' 
I 
('a -::.t 'a) -::.t 'b and ('e -4 'e) --4 'd and the resulting substitution 
is ['1 I-t '4, '2 I-t '3, 'a I-t 'e, 'b I-t 'dj. Thus the application will have type 'd 
and the constraint set will be {'3 2 {Z}}. The initial call of WCFA on loop 
returns the type 'd and the constraint set {'4 2 {F}, '3 2 {V}, '3 2 {Z}}. 
This corresponds to the typing obtained in Example 5.5. 
â¢ 

310 
5 Type and Effect Systems 
As illustrated in the above examples, when the overall call of WCFA (I', e) gives 
(7, (), C) we are only interested in the effect of () on the type and annotation 
variables occurring in r. 
5.3.3 
Syntactic Soundness and Completeness 
We are now ready to prove the correctness of the algorithm; this will take 
the form of a syntactic soundness result and a syntactic completeness result. 
Syntactic soundness is a rather straightforward result to prove. This nor-
mally also holds for Type and Effect Systems that are more complex than 
the Control Flow Analysis considered here, although the actual details of 
the Type and Effect Systems may of course require special treatment going 
beyond the techniques covered here. 
By contrast, syntactic completeness is a somewhat harder result to prove. For 
a complex Type and Effect System it frequently involves establishing a result 
about proof normalisation: that the rules of the inference system that are 
not syntax directed need only be used at certain places (see Exercise 5.13). 
However, in the case of Control Flow Analysis the proof is going to be un-
characteristically simple because both the algorithm WCFA and the inference 
system for Control Flow Analysis are defined in syntax directed ways. Thus 
the present development does not indicate the breath of techniques needed 
for Type and Effect Systems in general and the the Concluding Remarks will 
contain references to the more general situation. 
Syntactic soundness. The soundness result expresses that any infor-
mation obtained from the algorithm is indeed correct with respect the the 
inference system: 
Theorem 5.20 
If WCFA (I', e) = (r, (), C) and ()a is a ground validation of () 1', r 
and C then ()a(() 1') f-CFA e : ()a r. 
This theorem may be reformulated as follows: if WCFA (I', e) = (r, (), C) and 
()T is a type substitution that covers () I' and r, and if () A is an annota-
tion substition that covers () 1', rand C and that satisfies () A F C, then 
()A(()T(() 1')) f-CFA e : ()A(()T r). To see this first note that ()A 0 ()T = ()T 0 ()A 
is a ground substitution; next note that given a ground substitution ()a we 
may obtain an annotation substitution () A by restricting ()a to annotation 
variables and similarly we may obtain a type substitution ()T by restricting 
()a to type variables and clearly ()a = () A 0 ()T = ()T 0 () A. 
Proof The proof proceeds by structural induction on e (because WCFA is defined 
by structural induction on e). 

5.3 Inference Algorithms 
311 
The case c. We have WCFA(r,C) = (Tc ,id,0). Next let (Ja be a ground validation of 
rj clearly it also covers Tc (because Tc is a base type) and it also satisfies (Ja 1= 0. 
From the axiom [con] of Table 5.2 it is immediate that 
(Ja (r) I-CFA c : Tc 
and since (Ja Tc = Tc this is the desired result. 
The case x. We have WCFA(r, x) = (r(x),id,0). Next let (Ja be aground validation 
of rj clearly it also covers rex) and it satisfies (Ja 1= 0. From the axiom [var] of 
Table 5.2 it is immediate that 
(Ja f I-CFAX: (Ja(r(xÂ» 
and this is the desired result. 
The case fn,.. x => eo. We shall use the notation established in the clause for 
WCFA(r, fn,.. x => eo). So let (Ja be a ground validation of (Jo r, (Jo ax ~ 
To, and 
Co U {./10 ~ {11"}}. Then (Ja is a ground validation of (Jo(r[x f-t ax]), To, and Co. 
Hence by the induction hypothesis we get: 
(JaÂ«(Jo f>[x f-t (JaÂ«(Jo ax)]l-cFA eo : (Ja TO 
Since (Ja 1= Co U Wo ~ {11"}} we have (Ja ./10 ~ {11"} so we can apply the rule [fn] of 
Table 5.2 and get 
(JaÂ«(Jo r) I-CFA fn,.. x => eo : (JaÂ«(Jo a",) 
60 f30) (Ja To 
which is the desired result. 
The case fun,.. f x => eo. We shall use the notation already established in the 
clause for WCFA (r, fun,.. f x => eo). So let (Ja be a ground validation of (Jl Â«(Jo r), 
(JlÂ«(JO a",) 
61(110 f30) 
(Jl To, and Â«(Jl Co) U {(JlÂ«(JO ./10) ~ {11"}}. Then (Ja 0 (Jl is a 
ground validation of (Jo r, (Jo a", ~ 
To and Co. Since (Jl To = (JlÂ«(JO ao) by Fact 
5.17 we also have that (Ja 0 (Jl is a ground validation of (Jo r, (Jo ax ~ 
(Jo ao 
and Co. Hence we can apply the induction hypothesis and get: 
(JaÂ«(JlÂ«(Jo (r[j f-t a", ~ 
ao][x f-t a",])) I-CFA eo: (JaÂ«(Jl TO) 
Since (Jl To = (Jl Â«(Jo ao) and (Ja 1= Â«(Jl Co)U{ (Jl Â«(Jo ./10) ;2 {11"}} we get (JaÂ«(Jl Â«(Jo ./10Â» ;2 
{11"} so we can apply the rule [fun] of Table 5.2 and get 
(JaÂ«(JlÂ«(Jo fÂ» I-CFA fun,.. f x => eo : (JaÂ«(JlÂ«(Jo a",Â» 
90(91(90 f30)\ (JaÂ«(Jl TO) 
and this is the desired result. 
The case el e2. We shall use the notation already established in the clause for 
WCFA(r, el e2)' So let (Ja be a ground validation of (J3Â«(J2Â«(JI rÂ» and (J3 a and 
(J3 Â«(J2 Cl) U (J3 C2. Let (Jo be a ground extension of (Ja upon (J3Â«(J2 TI) and (J3 T2. 
Then (Jo 0 (J3 0 (J2 is a ground validation of (Jl r, Tl and Cl. Hence we can apply 
the induction hypothesis to el and get: 
(JOÂ«(J3Â«(J2Â«(JI r))) I-CFA el : (JOÂ«(J3Â«(J2 TI) 

312 
5 Type and Effect Systems 
Similarly 80083 is a ground validation of 82(81 1'),72 and C2. Hence we can apply 
the induction hypothesis to e2 and get: 
80(83(82(81 1'))) f-CFA e2 : 80(83 72) 
Since 83(82 7I) = (83 72) ~ 
(83 a) follows from Fact 5.17 we can use the rule 
[app] of Table 5.2 and get 
80(83(82(81 1'))) f-CFA e1 e2 : 80(83 a) 
which is equivalent to 80(83(82(81 1'))) f-CFA e1 e2 : 80(83 a) and this is the desired 
result. 
The cases if eo then e1 else e2, let x = e1 in e2 and e1 op e2 are analogous. 
_ 
Syntactic completeness. It is not enough to show that WCFA is syn-
tactically sound: an algorithm that always fails will indeed be a syntactically 
sound implementation of our analysis. We shall therefore be interested in a 
result saying that any judgement of the Annotated Type System can in fact 
be obtained by the algorithm: 
Theorem 5.21 
Assume that r is a simple type environment and ()' r f-CFA e : 1'" 
holds for some ground substitution ()' that covers r. Then there 
exists 1", (), C and ()a such that 
â¢ WCFA(r,e) = (T,(),C), 
â¢ ()a is a ground validation of () r, 1" and C, 
â¢ ()a 0 () = ()' except on fresh type and annotation variables (as 
created by WCFA(r,e)), and 
â¢ ()a 1" = 7". 
Proof The proof is by induction on the shape of the inference tree; since the 
Annotated Type System of Table 5.2 is syntax directed this means that the proof 
follows the syntactic structure of e. Without loss of generality we may assume that 
8' is not d~fined on type and annotation variables that are freshly generated in the 
call WCFA(r, e). 
The case c. We have 8' I' f-CFA c : T' and T' = Te. Clearly WCFA(f, c) = (Te, id, O) so 
it suffices to set 80 = 8' and clearly 80 T' = T'. 
The case x. We have 8' I' f-CFAX: T' because T' = 8'(f(x)). Clearly WCFA(f,x) = 
(f(x), id, 0) so it suffices to set 80 = 8'. 
The case fn,.. x => eo. We have 
, ~ 
"" 
{,..}Ucp' 
"" 
8 r f-CFA fn,.. x => eo : T., 
) TO 

5.3 Inference Algorithms 
313 
where cp' is an annotation (i.e. it does not contain any annotation variables) and 
from Table 5.2 we get that also: 
Â«()' r)[x I-t T;,]rCFA eo : ~ 
Let now 0:., be a fresh type variable and note that 0:., f}. domÂ«()'). Define ()" by 
()" ( = {~ if ( = 0:., 
() ( 
otherwise 
where ( can either be a type variable or an annotation variable. Then we also have: 
()" (r[x I-t 0:.,]) rCFA eo : ~ 
By the induction hypothesis there exists To, ()o, Go and ()a such that: 
WCFA(r[X I-t 0:.,], eo) = (To, ()o, Go), 
()a is a ground validation of Â«()o r)[x I-t ()o(o:.,)], TO, and Go, 
()a 0 ()o = ()" except on fresh type and annotation variables 
created by WCFA(r[X I-t 0:.,], eo) 
Next let (30 be a fresh annotation variable and define: 
()a ( -
{ 
{orr} U cp' 
if ( = (30 
-
()a ( 
otherwise 
Then we get: 
()a is a ground validation of ()o r, Â«()o 0:",) ~ 
To and Go U {(3;2 {orr}}, 
()a 0 ()o = ()' except on fresh type and annotation variables 
created by WCFA(r, fn" x => eo), 
()aÂ«()o 0:., ~ 
TO) =T;, 
{,,}u<p') Th 
This is the desired result. 
The case fun" f x => eo. We have 
, ~ 
-"4 
{,,}u<p' 
-"4 
() r rCFA fun"fx => eo : T., ~ 
TO 
where cp' does not contain annotation variables and according to Table 5.2 this is 
because: 
Â«()' ~)[f 
-"4 
{,,}u<p' 
'][ 
-"4] 
-"4 
r 
I-t T", 
) 
TO X I-t T", rCFA eo : TO 
Let 0:"" 0:0 and (30 be fresh type and annotation variables and note that they are 
not in domÂ«()'). Define ()" by: 
()"(= 
,f}Ucp' 
{
? 
TO 
()' ( 
if(=o:", 
if(=(3o 
if(=o:o 
otherwise 

314 
5 Type and Effect Systems 
Then we also have: 
By the induction hypothesis there exists 70, 00, Co and 06 such that: 
WCFA(1'[1 f-t ax ~ 
ao][x f-t ax], eo) = (70,00, Co) 
06 is a ground validation of 00(1'[1 f-t a", ~ 
ao][x f-t ax]), 70, and Co 
06 0 00 = 0" except on fresh type and annotation variables 
created by WCFA ( ... , eo) 
Since 06(00 ao) = 0" ao = TfJ = 0670 it follows from Fact 5.17 that there exists 01 
and Oa such that UCFA (00 ao, 70) = 01 and 06 = Oa 0 01. Hence 
Oa is a ground validation of 01(00 1'),01(00 ax) 
111(110 ,80Â») 01 70, 
and (01 Co) U {Ol(Oo,Bo);2 fir}} 
Oa 0 01 0 00 = 0' except on fresh type and annotation variables 
created by WCFA(" " fun,.. 1 x => eo) 
Oa(Ol(Oo ax) IIdllo ,80Â») 01(00 ao)) =:;:;:, 
{,..}u<p') 76 
and this is the desired result. 
The case e1 e2. We have 
()' f I-CFA el e2 : ~ 
and according to Table 5.2 this is because: 
By the induction hypothesis applied to e1 there exists 71, 01, C1 and o"b such that: 
WCFA(1', e1) = (71,01, C1) 
o"b is a ground validation of 01 1',71, and C1 
O"b 0 01 = 0' except on fresh type and annotation variables 
created by WCFA(O' 1', e1) 
01 
~ 
~,<p',~, 
a 71 = 72 ~ 
70 
Then we have 
1 
~ 
-"/ 
Oa(Ol r) I-CFA e2 : 72 
so by the induction hypothesis applied to e2 there exists 72, O2, C2 and 0& such 
that: 
WCFA(Ol 1', e2) = (72, O2, C2) 
0& is a ground validation of O2 (01 1'),72 and C2 
0& 0 02 = e"b except on fresh type and annotation variables 
created by WCFA(" " e2) 

5.3 Inference Algorithms 
It then follows that: 
6'2, is a ground validation of 62(61 f), 62 71,62 G1,72, and G2 
6'2, 0 62 0 61 = 6' except on fresh type and annotation variables 
created by WCFA("" e1) and WCFA(" ., e2) 
112 (ll 
~) 
~, <P'. -., 
Ua U2 71 = 72 """""""""""t 70 
6'2, 72 = 7~ 
Next let a and f3 be fresh and define: 
if (= a 
if ( = f3 
otherwise 
315 
SÂ· 
113 (ll ~) 
112 (ll 
~) 
-., 
<p', 
~I 
112 ~ 
<p',~, 
113 (~ 
B. 
) 
1nce ua U2 71 = ua U2 71 = 72 """""""""""t 
TO = ua 72 """""""""""t 
TO = ua 72 ""'"-t a it follows 
from Fact 5.17 that there exists 63 and 6a such that UCFA(62 71,72 -4 a) = 63 and 
6~ = 6a 0 63. It follows that 
6a is a ground validation of 63(62(61 f)), 63 a, and 63(62 G1 ) U 63 G2 
6a 0 63 0 62 0 61 = 6' except on fresh type and annotation variables 
created by WCFA('" ,el e2) 
6a(63 a) = 6~ a = 7~ 
and this is the desired result. 
The cases if eo then e1 else e2, let x = e1 in e2 and e1 op e2 are analogous. 
_ 
5.3.4 
Existence of Solutions 
Since WCFA generates a set of constraints the statement of syntactic soundness 
(Theorem 5.20) is a little weaker than usual: if the constraints cannot be 
solved then we cannot use the soundness result to guarantee that the result 
produced by WCFA can be inferred in the inference system. 
This suggests showing that the constraints always have solutions; in line with 
previous developments in this book we shall prove a stronger result. For this 
let AV( C) be the set of annotation variables in C. 
Lemma 5.22 If WCFA(r, e) = (7, B, C) and X is a finite set of annotation 
variables such that X 2 AV( C), then 
{BA I BA 1= C /\ dom((}A) = X /\ BA is an annotation substitution} 
is a Moore family. 
â¢ 
Proof Let G be a finite set of constraints of the form f3 ;2 cp where cp E SAnn 
and such that X ;2 A V( G) where X is a finite set of annotation variables; it will 
not be of importance that C is generated by WCFA. Let Y be a possibly empty 

316 
5 Type and Effect Systems 
subset of the set displayed in the lemma. Each element of Y will be an annotation 
substitution with domain X and that satisfies O. By setting 
for f3 E X 
we define an annotation substitution 8 with dom(8) = X. For each f3 ;2 <p in 0 and 
8A in Y we have 
8A f3 ;2 8A <p ;2 8 <p 
(since <p is monotone in any free annotation variables) and hence: 
This establishes the result. 
â¢ 
Consider now the call WCFA (r, e) = (7, (J, C) and the problem of finding a 
ground substitution (Ja that covers (J r, T and C. After the statement of 
Theorem 5.20 we made it clear that (Ja can always be written as (J A 0 (JT for 
an annotation substitution (J A covering (J r, T and C and a type substitution 
(JT covering (J r and T. The choice of the type substitution (JT must be 
performed by the user of WCFAi one possibility is to let (JT a = int for all 
type variables a in (J rand T. The existence of an annotation substitution 
(JAnow follows from Lemma 5.22. 
Corollary 5.23 If WCFA(r, e) = (7, (J, C) then there exists a ground val-
idation (Ja of (J r, T and C. 
â¢ 
Proof Let 8T be given by 8T a = int for all type variables a in 8 f and Tj clearly 
8T covers 8 f and 'T. Next let X be the set of annotation variables in 8 f, 'T and OJ 
then Lemma 5.22 guarantees the existence of an annotation substitution 8A that 
covers 8 f, 'T and 0 and such that 8A 1= O. Taking 8G = 8A 08T we obtain a 
ground validation of 8 f, 'T and O. 
â¢ 
The result obtained by choosing a type substitution is only unique in case 
there are no type variables present in (J r and T. If there is at least one 
type variable present then the result of using (JT displayed above will differ 
from the result of using (JT given by (JT a = bool for all type variables a 
in (J r and T. In general a type substitution (J!} may have (J!} a E TYPe[TaJ 
for an arbitrary underlying type Ta. However, in case TYPe[TaJ has more 
than one element one is likely to prefer the least element since it has empty 
annotations on all function arrows (but see Exercise 5.9). 
In a similar way one is likely to prefer the least annotation substitution 
guaranteed by Lemma 5.22. Keeping in mind that all constraints in C have 
the form f3 ;2 {7r} for f3 E AVar and 7r E Pnt, we simply set: 
(J 
f3 - { {7r I f3;2 {7r} is in C} if f3 E AV(C) 
A 
-
undefined 
otherwise 

5.4 Effects 
317 
It is immediate that dom((}A) = AV(C) and that (}A F= C. If also dom((}) = 
AV( C) and () F= C then it is immediate that V (3 : () (3 ;2 () A (3 which we may 
write as () ~ () A. This shows that () A as constructed above is indeed the least 
element of the Moore family displayed in Lemma 5.22. 
5.4 
Effects 
The Type and Effect System for Control Flow Analysis is fairly simple: it is 
a syntax directed system using a form of subeffecting and the annotations are 
just sets. Much more powerful Type and Effect Systems can be constructed 
by allowing subtyping, let-polymorphism or polymorphic recursion; the re-
sulting analyses will be more powerful and, not surprisingly, the techniques 
required for the implementation will be more demanding. 
Subtyping and the various notions of polymorphism can be combined but for 
the sake of simplicity we shall present them one at a time. We shall first 
present a Side Effect Analysis for an extension of the FUN language with 
assignments; it will use sub effecting and subtyping. Then we shall present 
an Exception Analysis for an extension of FUN with exceptions; it will use 
subeffecting, subtyping as well as polymorphism. Finally, we shall present 
a Region Analysis for the FUN language; it will be based on polymorphic 
recursion. 
5.4.1 
Side Effect Analysis 
Syntax. Let us consider an extension of the language FUN (Section 5.1) 
with imperative constructs for creating reference variables and for accessing 
and updating their values: 
e ::= Â·Â·Â·1 new,.. x := el in e2 1 !x 1 x := eo 1 el ; e2 
The idea is that new,.. x : = el in e2 creates a new reference variable called x 
and initialises it to the value of el; the scope of the reference variable is e2 but 
we shall want the creation of the reference variable to be visible also outside 
its scope so as to be able to determine whether or not functions may need 
to allocate additional memory. The value of the reference variable x can be 
obtained by writing ! x and it may be set to a new value by the assignment 
x : = eo. The sequencing construct el ; e2 first evaluates el (for its side 
effects) and then e2. 
Example 5.24 The following program computes the Fibonacci number 
of a positive number x and leaves the result in the reference variable r: 

318 
5 Type and Effect Systems 
newR r:=O 
in 
let fib = funF f z => if z<3 then r:=!r+l 
else f(z-l); f(z-2) 
in fib x; !r 
The program creates a new reference variable r and initialises it to 0, then it 
defines the function fib, applies it to the value of x and returns the value of r. 
The statement r:=!r+l in the body of the recursive function will increment 
the value of r each time it is executed and each call of fib x will increase 
the value of r with the Fibonacci number of x. 
The aim of the Side Effect Analysis is to record: 
For each subexpression which locations have been created, ac-
cessed and assigned. 
â¢ 
So for the function fib in Example 5.24 the analysis will record that it 
accesses and assigns the reference variable created at program point R. 
Semantics. Before presenting the analysis let us briefly sketch the se-
mantics of the language. To distinguish between the various incarnations 
of the new-construct we shall introduce locations (or references) and, as for 
the imperative languages of Chapter 2, the configurations will then contain 
a store component mapping locations to their values: 
., E Store = Loc -+fin Val 
The values of Val include the constants e, the (closed) function abstractions 
of the form fn,. x => e and the locations e E Loc. The semantic clauses of 
Table 5.4 are now modified to trace the store in a left-to-right evaluation as 
for example in the following clause for the let-construct: 
I- (eI, "1) --t (VI, '>2) 
I- (e2[x t-+ VI], '>2) --t (V2, "3) 
I- (let x = el in e2,"1) --t (V2,"3) 
For the new constructs we then have the following axioms and rules (explained 
below): 
I- (el,"l) --t (Vl,"2) 
I- (e2[x t-+ e],'>2[e t-+ vt}) --t (V2,'>3) 
I- (new,. x := el in e2,"1) --t (V2,"3) 
where e does not occur in the domain of"2 
I- (el,"l) --t (Vl,"2) 
I- (e2,'>2) --t (V2,"3) 
I- (eli e2, '>1) --t (V2, "3) 

5.4 Effects 
319 
In the rule for new we evaluate el, create a new location ~ which is initialised 
to the value of el, and then we syntactically replace all occurrences of x 
with that location so that all subsequent references to x will be to~. We 
exploit this when defining the semantics for the constructs for accessing and 
updating the reference. Note that the value returned by the assignment will 
be the value being assigned. 
Annotated types. In the Side Effect Analysis a location will be rep-
resented by the program point where it could be created. We shall therefore 
define the annotations (or effects) t.p E AnnsE by: 
t.p ::= {!11"} I {11":=} I {neW7r} I t.pl ut.p210 
The annotation ! 11" means that the value of a location created at 11" is accessed, 
11": = means that a location created at 11" is assigned, and new1I" that a new 
location has been created at 11". As for the Control Flow Analysis we shall 
consider annotations equal modulo UCAI. We shall need a set ti7 of program 
points defined by 
ti7 ::= 11" I ti71 U ti72 I 0 
also to be interpreted modulo UCAI; we shall write ti7 = {1I"bÂ·Â·Â·,1I"n} for a 
typical element. 
The annotated types T E TypesE are now given by: 
~ 
. I 
I~ '" 
~ I 
~ 
7 ::= l.nt bool 71 -t 72 ref.., 7 
Here ref.., T is the type of a location created at one of the program points in 
ti7 and that will contain values of the annotated type T. As before the type 
environment r will map variables to annotated types. 
Example 5.25 Consider the Fibonacci program 
neWR r:=O 
in 
let fib = funF f z => if z<3 then r:=!r+1 
else f(z-l)j f(z-2) 
in fib Xj !r 
of Example 5.24. The .variable r has the annotated type ref{R} int and the 
variable fib has type int {!R,R:=}) int since it maps integers to integers 
and whenever executed it may, as a side effect, access and update a reference 
created at the program point R. 
â¢ 
Typing judgements. The typing judgements for the Side Effect Anal-
ysis will be of the form: 
r I-SE e : T & t.p 

320 
[con] 
[var] 
[In] 
[fun] 
lapp] 
[iJ] 
[let] 
fop] 
r f-SE c: Tc & 0 
r hE x : T & 0 
ifr(x)=T 
r[x t-+ Ta:] f-SE eo : TO & <Po 
r f-SE fn.". x => eo : Ta: ~ 
TO & 0 
5 Type and Effect Systems 
r[J t-+ Ta: ~ 
To][X t-+ Ta:] hE eo: TO & <Po 
r hE fun.". f x => eo : T;" ~ 
TO & 0 
r f-SE el : T2 ~ 
To & <PI r f-SE e2 : T2 & <P2 
r f-SE el e2 : TO & <PI U <P2 U <Po 
r hE eo : bool & <Po 
r hE el : T & <PI 
r hE e2 : T & <P2 
r f-SE if eo then el else e2 : T & <Po U <PI U <P2 
r hE el : Tl & <PI 
r[x t-+ Tl] f-SE e2 : T2 & <P2 
r f-SE let x = el in e2 : T2 & <PI U <P2 
~ 
1 
~ 
2 
r f-SE el : Top & <PI r f-SE e2 : Top & <P2 
r f-SE el op e2 : Top & <PI U <P2 
[dereJ] 
r hE ! x : T & {! 1rl, ... , ! 1r n} 
[ass] 
[new] 
[seq] 
[sub] 
r f-SE el : Tl & <PI 
r[x t-+ ref{.".}Tl] f-SE e2 : T2 & <P2 
r hE new.". x ;= el in e2 : T2 & (<PI U <P2 U {new1r}) 
r f-SE el : Ti & <PI r f-SE e2 : T2 & <P2 
r f-SE el ; e2 : T2 & <PI U <P2 
r hE e : T & <P 
r f-SE e : T' & <p' 
if T ::; T' and <P ~ <p' 
Table 5.9: Side Effect Analysis. 

5.4 Effects 
321 
The idea is that under the assumption r, the expression e will evaluate into a 
value with the annotated type T and that during computation the side effects 
expressed by <p might take place. The analysis is specified by the axioms and 
rules of Table 5.9. 
In the clauses [con] and [var] we record that there are no side effects so we use 
o for the overall effect. The premise of the clause [In] gives the effect of the 
body of the function and we use that to annotate the arrow of the function 
type whereas we use 0 as the overall effect of the function definition itself: 
no side effects can be observed by simply defining the function. A similar 
explanation holds for the recursive function definition, the only difference 
being that the assumption about f in the premise has to use the same effect 
as the one determined from the function body. In the rule [app] we see how 
the information comes together: the overall effect is what we can observe 
from evaluating the argument el, what we can observe from evaluating the 
argument e2, and what we get from evaluating the body of the function called. 
The rules [i.f], [let] and top] are straightforward. 
Turning to the axioms and rules involving reference variables we make sure 
that we only assign values of the appropriate type to the variable. Also, 
in each of the cases we make sure to record that a location at the relevant 
program point has been created, referenced or assigned. The rule [seq] is 
straightforward and the final rule [sub] will be explained below. 
Example 5.26 The following program 
newA x:=l 
in 
(news y:=!x in (x:=!y+l; !y+3Â» 
+ (newe x:=!x in (x:=!x+l; !x+lÂ» 
evaluates to 8 because both summands evaluate to 4. The first summand has 
type and effect 
int & {newB, !A,A:=, !B} 
and the second summand has type and effect 
int & {newC, !A, C:=, !C} 
because the reference variable that is updated is the local one. Hence 
int & {newA,A:=, !A,newB, !B,newC,C:=,!C} 
is the type and effect of the overall program. It follows from the effect that 
the variable being created at B (y in the program) is never reassigned after 
its creation. This might suggest transforming the news-construct into a let-
construct (i.e. let y=!x in (x:=y+l; y+3Â»). 
_ 

322 
5 Type and Effect Systems 
Subeffecting and subtyping. The purpose of the rule [sub] in Table 
5.9 is to ensure that we obtain a conservative extension of the underlying type 
system. The rule is really a combination of a separate rule for subeffecting 
r hE e : 7' & cp 
if cp ~ cp' 
r f-SE e : 7' & cp' 
and a separate rule for subtyping: 
r f-SE e : 7' & cp 
r f-SE e : 7" & cp 
Here cp ~ cp' means that cp is "a subset" of cp' (modulo UCAI) as discussed in 
Subsection 5.2.3 (and Exercise 5.3). The ordering 7':::; 7" on annotated types 
is derived from the ordering on annotations as follows: 
~:::;7'1 
7'2:::;7~ 
cp~cp' 
7'1-47'2:::;~-4~ 
7' :::; 7" 
T':::; 7' ro ~ ro' 
ref tv 7' :::; ref tv' 7" 
Note that the order of the comparison is reversed for arguments to functions; 
we say that 7'1 -4 7'2 is contravariant in 7'1 but covariant in cp and 7'2. (To 
familiarise oneself with this idea just pretend that the types are really logical 
propositions and that -4 as well as :::; mean logical implication; then it 
should be clear that the rule is the right one.) Also ref tv 7' is covariant in ro 
and both covariant in 7 (when the reference variable is used for accessing its 
value as in Ix) and contravariant in 7' (when the reference variable is used 
for assignments as in x : = ... ). This turns out to be essential for semantic 
correctness to hold. 
This form of subtyping we shall call shape conformant subtyping because 
7'1 :::; 72 implies that the two annotated types have the same underlying types, 
i.e. L7'IJ = L7'2J and hence that 7'1 and 7'2 have the same "shape". (There are 
more permissive notions of subtyping than this, and we shall return to this 
issue in the Concluding Remarks.) 
Example 5.27 Consider the following program 
neWA x:=l 
in (fn f => f (fn y => Ix) + f (fn z => (x:=z; zÂ») 
(fn g => g 1) 
where we have omitted the labels on the function definitions. The program 
evaluates to 2 because each summand evaluates to 1. 
The type and effect of the two arguments to f are 
int {!A}) int 
& 
. 
{A:=} 
l.nt~ int 
& 
o 
o 

5.4 Effects 
323 
and when f has the type 
C. 
{!A,A:=} 
~nt 
) 
. 
) 
{!A,A:=} 
l.nt 
) int 
the application of f to the arguments will be well-typed: we have 
( . 
{!A} 
) 
< ( 
{!A,A:=} 
) 
~nt ~ 
int 
int 
) int 
{ AÂ·-} 
(int 
.- ) int) < 
{!A,A:=} 
(int 
) int) 
and may use the rule for subtyping to change the types of the arguments to 
the type expected by f. 
If instead we had only used the rule for subeffecting we would be obliged to 
let the two arguments to f have type and effect: 
{A:=,!A} 
int 
) int 
& 
0 
{A:=,!A} 
int 
) int 
& 
0 
This is indeed possible by using the rule for sub effecting just before the rule 
for function abstraction. 
_ 
The combined rule [sub] for subeffecting and subtyping gives rise to a conser-
vative extension of the underlying type system. This would also have been 
the case if we had adopted either the rule for subeffecting or the rule for 
subtyping. However, had we incorporated no additional rule then this would 
not be the case. 
Remark. One can make a distinction between "Annotated Type Systems" 
and "Effect Systems" but it is a subtle one and it is hardly fruitful to distin-
guish them in a formal way; yet intuitively there is a difference. The analysis 
presented in this subsection is truly an Effect System because the annotations 
relate neither to the input nor the output of functions; rather they relate to 
the internal steps of the computation. By contrast the analysis of Subsection 
5.1 is an Annotated Type System because the annotations relate to inten-
sional aspects of the semantic values: what function abstraction might be the 
result of evaluating the expression. 
_ 
5.4.2 
Exception Analysis 
Syntax. As our next analysis we consider an Exception Analysis; the aim 
of this analysis is to determine: 
For each expression, what exceptions might result from evaluating 
the expression. 

324 
5 Type and Effect Systems 
These exceptions may be raised by primitive operators (like division by zero) 
or they may be explicitly raised in the program. Furthermore, there may be 
the possibility of trapping an exception by means of executing an expression 
designed to overcome the source of the abnormal situation. To illustrate this 
scenario we extend the syntax of expressions in FUN (Section 5.1) as follows: 
e ::= ... I raise 8 I handle 8 as e1 in e2 
The exception is raised by the raise-construct. If e2 raises some exception 
81 then handle 82 as e1 in e2 will trap the exception in case 81 = 82 and 
this means that e1 will be executed; if 81 =f. 82 we will continue propagating 
the exception 81. We take a simple-minded approach to exceptions and use 
strings to denote their identity. 
Example 5.28 Consider the following program computing the combina-
torial ( : ) of the values x and y of the variables x and y: 
let comb = fun f x => fn y => 
if x<O then raise x-out-of-range 
else if y<O or y>x then raise y-out-of-range 
else if y=O or y=x then 1 
else f (x-1) y + f (x-1) (y-1) 
in handle x-out-of-range as 0 in comb x y 
The program raises the exception x-out-of-range if x is negative and in 
the body of the let-construct this exception is trapped and the value 0 is 
returned. The program raises the exception y-out-of-range if the value of 
y is negative or larger than the value of x and this exception is not trapped 
in the program. 
_ 
Semantics. Before presenting the analysis let us briefly sketch the se-
mantics of the language. An expression can now give rise to an exception so 
we shall extend the set Val of values to include entities of the form raise 
8. The semantic clauses of Table 5.4 then have to be extended to take care 
of the new kind of values. For function application we shall for example add 
three rules 
f- e1 ----+ raise 8 
f- e1 e2 ----+ raise 8 
f- e1 ----+ (fn,.. x => eo) 
f- e2 ~ raise 8 
f- e1 e2 ----+ raise 8 
f- e1 ----+ (fn,.. x => eo) 
f- e2 ~ 
V2 
f- eo[x t-+ V2] ----+ raise 8 
f- e1 e2 ----+ raise 8 

5.4 Effects 
325 
reflecting that an exception can be raised in any of the sub computations in 
the premise of rule [app]. Similar rules are added for the other constructs. 
We can then specify the meaning of the new constructs by the following 
axioms and rules: 
I- raise s ---t raise s 
if V2 =j:. raise s 
I- e2 ---t raise s 
I- el ---t VI 
I- handle s as el in e2 ---t VI 
Note that the expression e2 in handle s as el in e2 may also raise an ex-
ception other than s in which case it will be propagated out of the handle-
construct. Similarly, the expression el may raise an exception which will then 
be propagated out of the handle-construct. 
Annotated types. The purpose of Exception Analysis is to determine 
which exceptions might be raised and not trapped in the program. We shall 
therefore take the annotations to be sets of exceptions. To get a more flexible 
type system and a more powerful analysis we shall use a polymorphic type 
system. This means that we shall allow the annotated types to contain 
type variables and also we shall allow the annotations to contain annotation 
variables. So the annotations (or effects) cp E AnnEs will be given by 
cp ::= is} I CPl ucp2101 (3 
and the annotated types 7 E TypeEs will be given by: 
7 ::= int I bool I 71 4 72 I a 
As usual effects will be considered equal modulo VCAL To express the poly-
morphism concisely we introduce type schemes. They have the form 
a ::= YÂ«(I,"', (n).7 
where (1, .. " (n is a (possible empty) list of type variables and annotation 
variables; if the list is empty we simply write 7 for YO.7. 
Example 5.29 Consider the function fnF f => fnx x => f x. We can give 
it the type schema 
'1 
0 
'1 
Y 'a, 'b, '1. ('a -+ 'b) --+ ('a -+ 'b) 
which has instances like 
( . 
{x-out-of-range}.) 0 
(. 
{x-out-of-range}.) 
l.nt 
I l.nt --+ 
l.nt -'-----....:;...:-+) l.nt 
and (int ~ bool) ~ (int ~ bool). 
â¢ 

326 
[con] 
[var] 
(fn] 
(fun] 
lapp] 
[ ijj 
[let] 
top] 
[raise] 
[handle] 
[sub] 
[gen] 
[ins] 
r rES c : Tc & 0 
r rES x : a & 0 
if r(x) = a 
r[x I-t T",] rES eo : TO & 'Po 
r rES fnll" x => eo : T", ~ 
TO & 0 
5 Type and Effect Systems 
r(J I-t r., ~ 
TO][X I-t T",] rES eo : To & 'Po 
r rES funll" f x => eo : T", ~ 
TO & 0 
r rES e1 : T2 ~ 
TO & 'PI r rES e2 : T2 & 'P2 
r rES e1 e2 : TO & 'PI U 'P2 U 'Po 
r rES eo : boo 1 & 'Po 
r rES e1 : T & 'PI 
r rES e2 : T & 'P2 
r rES if eo then e1 else e2 : T & 'Po U 'PI U 'P2 
r rES e1 : a1 & 'PI r[x I-t a1] rES e2 : T2 & 'P2 
r rES let x = e1 in e2 : T2 & 'PI U 'P2 
~ 
1 
~ 
2 
r rES e1 : Top & 'PI 
r rES e2 : Top & 'P2 
r rES e1 op e2 : Top & 'PI U 'P2 
r rES raise s: T & {s} 
r rES e1 : T & 'PI r rES e2 : T & 'P2 
r rES handle s as e1 in e2 : T & 'PI U ('P2 \ { S } ) 
r rES e : T & 'P 
r rES e : T' & 'P' 
r rES e : T & 'P 
r rES e : V((l,Â·Â·Â·, (n).T & 'P 
if (1, ... ,(n do not occur free in rand 'P 
r rES e : V((l,Â·Â·Â·, (n).T & 'P 
r rES e : (0 T) & 'P 
if e has dom(O) ~ {(I, ... ,(n} 
Table 5.10: Exception Analysis. 

5.4 Effects 
327 
Typing judgements. 
ysis will be of the form 
The typing judgements for the Exception Anal-
r f-ES e : (j & <p 
where the type environment r now maps variables to type schemes. As ex-
pected (j denotes the type schema of e, and <p is the set of exceptions that may 
be raised during evaluation of e. The analysis is specified by the axioms and 
rules of Table 5.10. Most of the axioms and rules are straightforward modifi-
cations of those we have seen before; in the rule [let] the use of a type schema 
for the let-bound variable is going to give us the required polymorphism. 
The axiom [raise] ensures that a raised exception can have any type and 
the effect records that the exception s might be raised. The rule [handle] 
then determines the effects of its two subexpressions and records that any 
exception raised by el and any exception except s raised by e2 is a possible 
exception for the construct. Formally, <p \ {s } is defined as follows: {s} \ {s} = 
0, {s'} \ {s} = {s'} if s' =/; s, (<pU<p') \ {s} = (<p\ {s})U(<p'\ {s}), 0\ {s} = 0 
and (3 \ {s} = (3. (We shall consider alternative definitions shortly.) 
The rule [sub] is the rule for subeffecting and subtyping and the ordering 
f ~ f' on types is given by 
as was also the case in Subsection 5.4.l. 
The rules [gen] and [ins] are responsible for the polymorphism. The generali-
sation rule [gen] is used to construct type schemes: we can quantify over any 
type or annotation variable that does not occur free in the assumptions or in 
the effect of the construct; this rule is usually used before applications of the 
rule [let]. The instantiation rule [ins] can then be used to turn type schemes 
into annotated types: we just apply a substitution in order to replace the 
bound type and annotation variables with other types and annotations (pos-
sibly containing type variables and annotation variables); this rule is usually 
used after applications of the axiom [var]. 
Example 5.30 The program 
let f = fn g => fn x => g x 
in f (fn y => if Y < 0 then raise neg else y) (3-2) 
+ f (fn z => if z > 0 then raise pas else O-z) (2-3) 
evaluates to 2 because each of the summands evaluates to 1. 
We may analyse f so as to obtain the type schema 
'0 
0 
'0 
V'a, 'b, 'O.('a ---t 'b) --t ('a ---t 'b) 

328 
5 Type and Effect Systems 
and we may analyse the two functional arguments to f so as to obtain: 
int {neg})Â· int & 0 
int {POS}) int & 0 
We can now take two instances of the type schema for f: to match the first 
argument off we use the substitution [/a 1-7 inti Ib 1-7 int;IO 1-7 {neg}] and to 
match the second argument we use [/a 1-7 inti Ib 1-7 int;IO 1-7 {pos }]. Hence 
the type and effect of each summand is 
and therefore 
int & {neg} 
int & {pos} 
int & {neg, pos} 
is the overall type and effect of the entire program. 
If we did not avail ourselves of polymorphism we would have to rely on 
sub effecting and subtyping and let f have the type 
( . 
{neg,pos}.) 0 
(. 
{neg,pos}.) 
~nt 
) 
~nt ---t 
~nt 
) 
~nt 
which is more imprecise than the type scheme displayed above, although we 
would still be able to obtain int & {neg, pos} as the overall type and effect 
of the entire program. 
_ 
Remark. 
The judgements of the Exception Analyses were of the form 
f rES e : (j & <p but nonetheless most axioms and rules of Table 5.10 have 
conclusions of the form f rES e : T & <po By changing rules [ijJ, [let], [raise], 
[handle] and possibly [sub] one can obtain a more liberal system for Exception 
Analysis. We leave the details for Exercise 5.15. 
In the rule [handle] we make use of the notation <p \ {s} and we next defined 
(3\ {s} = (3 even though (3 might later be instantiated to {s }. Since (3\ {s} ~ (3 
should hold for all instantiations of (3, this is semantically sound (correspond-
ing to what happens in the rule [sub]). To define a less approximate system 
it would be natural to let 
<p ::= . .. I <p \ {s} 
and then to extend the axiomatisation of VCAI to deal with set difference. _ 
5.4.3 
Region Inference 
Let us once again consider the language FUN as introduced in Section 5.l. 
The purpose of Region Inference is to facilitate implementing FUN in a stack-
based as opposed to a heap-based regime as is usually the case; since the 

5.4 Effects 
329 
(rl,Ol) 
(rl,l) 
(r2;",) D 
(r2,1) 
(r3;"') 0 
(r3,1) 
rl 
r2 
r3 
Figure 5.1: The memory model for the stack-based implementation of FUN. 
stack-based regime is quite efficient in reusing dead memory locations without 
relying on explicit garbage collection this might lead to an efficient implemen-
tation. But functions are statically scoped and can produce other functions 
as results, so it is by no means clear that a stack-based regime should be 
possible; the purpose of region inference is to analyse how far locally allo-
cated data can be passed around so that the allocation of memory can be 
performed in an appropriate manner. This leads to a memory model for the 
stack-based regime where the memory is a stack of dynamic regions (rl, r2, 
r3, ... ) and each dynamic region is an indexed list (or array) of values as 
illustrated in Figure 5.1. 
Syntax. To make this feasible we shall introduce a notion of extended 
expressions that contain explicit information about regions. To this end we 
introduce region names, region variables, and static regions 
rn 
E RName region names 
{! 
E RVar 
region variables 
r 
E RegRI 
regions 
as follows: 
rn .. -
rll r21 r31Â·Â·Â· 
{! 
.. -
"1 1"2 1"3 I ... 
r 
.. -
{! I rn 
The syntax of extended expressions 
ee E EExp 
is given by: 

330 
ee 
5 Type and Effect Systems 
c at r I x I fn7l" x => eeo at r I fun7l" f [t1J x => eeo at r I eel ee2 
if eeo then eel else ee2 I let x = eel in ee2 I eel op ee2 at r 
ee[f1 at r Iletregion ff in ee 
Here we write ff for a possibly empty sequence {ll,' .. ,{lk of region variables 
and similarly r for a possible empty sequence rl,"', rk of regions. The 
main point of the extended expressions is that if a sub expression explicitly 
produces a new value then it has an explicit placement component, "at r", 
that indicates the region in which the value is (to be) placed. The letregion-
construct explicitly allows the deallocation of the regions, ff, that are no longer 
needed and the placement construct, ee[f1 at r, allows us to explicitly place a 
"copy" of ee (typically a recursive function) in the region r. The construct for 
recursive function definitions explicitly takes a sequence of region variables 
as parameters; intuitively, this ensures that the various incarnations of the 
recursive function can have their local data in different regions. 
Example 5.31 The relationship between expressions and extended ex-
pressions will be clarified when presenting the typing judgements below. For 
now we merely state that the expression e given by 
(let x = 7 in fny y => y+x) 9 
will give rise to the extended expression ee: 
letregion {!l. {l3. {!4 
in (let x = (7 at (!l) 
in (fny y => (y+x) at (l2) at (l3) (9 at (l4) 
Here the value 7 is placed in the region {!l, the value of x+y in region {l2, 
the function named Y in the region {l3 and the argument 9 in region {l4' 
The final value (which is 7 + 9 = 16) is therefore to be found in region {l2. 
All other regions ({ll, {l3, and (l4) no longer serve any purpose and can be 
deallocated; this is made explicit by the letregion-construct. In the version 
of the program that is actually run we should replace the metavariables {ll, 
{l3 and {l4 by region variables "1, "2, and "3 and furthermore the free region 
variable {l2 should be replaced by a region name such as r1. 
_ 
Semantics. The Natural Semantics of expressions is given by Table 5.4 
and we now devise a Natural Semantics for the extended expressions so as to 
make the role of regions clear. The transitions takes the form 
p I- (ee, <;) --+ (v, <;') 
where p is an environment, ee is an extended expression, <; and <;' are stores 
(as in Figure 5.1) and v is an expressible value. Formally we shall use the 

5.4 Effects 
331 
domains: 
p 
E Env 
= Var* -t EVal 
v 
E 
EVaI 
= RName x Offset 
0 
E 
Offset = 
N 
~ 
E Store 
= RName -tfin (Offset -tfin SVaI) 
w 
E 
SVal 
Here Var* is the finite set of variables in the extended expression ee* of 
interest, a store is a stack of regions (where the stack is modelled as a finitary 
mapping from region names), a region is a list of storable values (where the 
list is modelled as a finitary mapping from indices called offsets), and a 
storable value is given by 
w ::= c I (x, ee, p) I (ff, x, ee, p) 
consisting of ordinary constants, closures, and so-called region polymorphic 
closures. In addition to the formal parameter, the body of the function, 
and the environment at the definition point, a region polymorphic closure 
also contains a list of formal region parameters that have to be instantiated 
whenever the function is called - in this way it is explicityly ensured that 
the local data can be properly placed in the store for each function call. 
To simplify the notation we shall view a store as an element of RName x 
Offset -tfin SVal and so write ~(r,o) for ~(r)(o) etc. All ordinary values are 
"boxed" which means that they are always placed in the store and hence an 
expressible value is always a pair consisting of a region name and an offset. 
The semantics is defined in Table 5.11 and is explained below. We intend 
that an extended expression does not contain free region variables when eval-
uated and hence many r's have become rn's. The axiom [con] for constants 
explicitly allocates a new offset in the relevant region and places the constant 
in that cell. The axiom [var] for variables does not involve the allocation of 
a new offset and merely performs a lookup in the environment. 
The axiom fin] for function abstraction allocates a new offset where it stores 
an ordinary closure consisting of the formal parameter, the body of the func-
tion, and the current environment. The axiom [fun] for recursive functions 
constructs a region polymorphic closure that records the list of formal region 
variables to be instantiated by means of the placement construct explained 
below; also note that recursion is handled by updating the current environ-
ment with a reference to the function itself. 
The rules lapp], [ifd, [if2] and [let] should be straightforward and the rule lop] 
for binary operations allocates a new offset for the result. 
The rule [place] takes care of the situation where an extended expression 
(such as a variable or a recursive function definition) evaluates to a region 

332 
[can] 
5 Type and Effect Systems 
pI- (c at rn,c;) -+ ((rn,o),c;[(rn,o) f-t c]) 
if of/. dom(c;(rn)) 
[var] 
pI- (x,c;) -+ (p(x),c;) 
[In] 
pI- ((fn1C x => eeo) at rn, c;} -+ 
((rn,o),c;[(rn,o) f-t (x, eeo, p}]) 
if Â° f/. dom(c;(rn)) 
[fun] 
pI- ((fun1C IlI?1 x => eeo) at rn, c;} -+ 
lapp] 
[ifd 
[let] 
fop] 
[place] 
[region] 
((rn, 0), c;[(rn, 0) f-t (ff,x,eeo,p[1 f-t (rn,o)])]) 
if of/. dom(c;(rn)) 
pI- (eel,C;l) -+ ((rnl,Ol),C;2) pI- (ee2,C;2) -+ (V2,C;3) 
po[x f-t V2]1- (eeo, C;3) -+ (vo, C;4) 
P I- (eel ee2, C;l) -+ (vo, C;4) 
ifC;3(rnl,ol) = (x,eeo,po) 
pI- (eeo,C;l) -+ ((rn,0),C;2) pI- (eel,C;2) -+ (Vl,C;3) 
p I- (if eeo then eel else ee2,C;l) -+ (Vl,C;3) 
if C;2(rn, 0) = true 
pI- (eeo,C;l) -+ ((rn,0),C;2) pI- (ee2,C;2) -+ (V2,C;3) 
p I- (if eeo then eel else ee2,C;l) -+ (V2,C;3) 
if C;2(rn, 0) = false 
pI- (eel,C;l) -+ (Vl,C;2) p[x f-t Vl]1- (ee2,C;2) -+ (V2,C;3) 
p I- (let x = eel in ee2,C;l) -+ (V2,C;3) 
pI- (eel,C;l) -+ ((rnl,Ol),C;2) pI- (ee2,C;2) -+ ((rn2,02),C;3) 
p I- ((eel op ee2) at rn,C;l} -+ ((rn, 0), C;3[(rn, 0) f-t w]) 
if C;3(rnl, od op C;3(rn2, 02) = wand 0 f/. dom(C;3(rn)) 
pI- (ee,C;l) -+ ((rn',o'),C;2) 
pI-
(ee[rn] at rn, C;l) 
-t ((rn, 0), C;2[(rn, 0) f-t (x,eeo[fff-t rn],po}]) 
if of/. dom(C;2(rn)) and C;2(rn',o') = (ff,x,eeo,po) 
pI- (ee[fff-t rn],c;drn f-t [J]) -+ (V,C;2) 
pI- (letregion ff in ee, C;l) -+ (v, C;2 ~rn) 
if {rn} n dom(c;) = 0 
Table 5.11: Natural Semantics for extended expressions. 

5.4 Effects 
333 
polymorphic closure. The placement construct ee[f1 at r then allocates a 
new cell in the region r and stores a copy of the region polymorphic closure 
in the cell except that it ensures that the list of formal region parameters is 
replaced by a list of the actual region names; this is an important feature for 
allowing each recursive call of a function to allocate its auxiliary data locally 
on the stack rather than being lumped together (in the heap) with data from 
other recursive calls. 
Finally, the rule [region] for the letregion-construct allocates new unused 
region names to be used instead of the region variables, evaluates the enclosed 
extended expression, and finally deallocates the newly allocated region names; 
formally, dom(t;~rn) = dom(t;) \ {rn} and V(rn,o) E dom(t;~rn) : t;(rn, 0) = 
(t;~rn)(rn, 0). 
Annotated types. When analysing the extended expressions we shall 
want to keep track of those regions that may be affected during evaluation: 
in what regions do we place (or put) data and from what regions do we 
access (or get) data. This is somewhat analogous to the Side Effect Analysis 
of Subsection 5.4.1 and will be taken care of using the effects to be defined 
below. Another purpose of the analysis is to keep track of the regions in 
which the values reside. To this end we shall say that an extended type is a 
pair 
r@r 
consisting of an annotated type and the region where the value resides. For-
mally, annotations (or effects), annotated types and type schemes 
are given by: 
cp 
E 
AnnRI 
r E 
TypeRI 
effects 
annotated types 
(j 
E 
SchemeRI type schemes 
cp 
.. -
{put r} I {get r} I CPI U CP2 I 0 I P 
int I bool I (71 @rt} ~ 
(r2@r2) I a 
V(al,Â·Â·Â· ,an), (PI,Â·Â·Â· ,Pm), [el,Â·Â·Â· ,ek].r 
V(al,Â·Â·Â·, an), (PI,Â·Â·Â·, Pm}.r 
Here we distinguish between two kinds of type schemes: compound type 
schemes (with [i!1) to be used for recursive functions and ordinary type 
schemes (without reD. We shall write r for YO, O.r whereas we insist on 
writing V[ ].r for yo, 0, [ ].r; in this way the two kinds of type schemes can 
always be distinguished. The use of an annotation variable, p, in the anno-
tation placed on the function arrow, p.cp, is related to the use of simple types 
in Section 5.3 and is mainly of interest for the inference algorithm; for our 

334 
5 Type and Effect Systems 
present purposes (3.<p can be read as (3 U <p although the inference algorithm 
will view it as the constraint f3 ;2 (3 U <p. 
Example 5.32 Returning to the extended expression 
letregion el, e3, e4 
in (let x = (7 at ed 
in (fny y => (y+x) at e2) at e3) (9 at e4) 
of Example 5.31, the sub expression 7 at el will have the annotated type 
int@el, the function abstraction (fny y => (y+x) at e2) at e3 will have the 
annotated type 
where <p = {get e4, get el, put e2}, and the overall expression will have the 
extended type int@e2' 
-
Typing judgements. It would be quite feasible to define typing judge-
ments for verifying that extended expressions are correctly typed. However, 
the main use of region inference is to facilitate the implementation of FUN 
and for this reason we shall touch upon the discussion initiated in Section 1.8 
and let the typing judgements also describe how to translate expressions into 
extended expressions. This suggests using typing judgements of the form 
r f-RI e ~ ee : 7'@r & <p 
where r is a type environment mapping variables into extended type schemes 
that are pairs consisting of a type scheme and a region. 
The analysis is defined in Tables 5.12 and 5.13 and is explained below. The 
axiom [con] for constants inserts an explicit placement component and records 
the placement in the effect. The axiom [var] for variables is straightforward 
as it involves no explicit placements. 
The rule (fn] for ordinary function abstraction transforms the body of the 
function and then inserts an explicit placement component for the function 
itself. The rule (fun] for recursive functions involves a restricted form of poly-
morphic recursion (excluding type variables as this would be undecidable): 
polymorphic recursion means that when analysing the body of the function 
we are allowed to use the recursive function polymorphically. This general-
ity gives much more precise type information and is essential for the success 
of region inference. For conciseness the rule has been presented in a manner 
where the rule for ordinary functions is needed to analyse the premise; clearly 
the rule could have been expanded so as to be syntax directed. The rules for 
application, conditional, local definitions and operators are straightforward. 
Then we have a rule [sub] for subeffecting and subtyping; clearly one could de-
cide to dispense with subtyping in which case sub effecting could be integrated 
with function abstraction as illustrated in Section 5.1. 

5.4 Effects 
[con] r I-RI c~ c at r: (rc@r) & {put r} 
[var] r I-RI x~ x: 0' & 0 
ifr(x)=a 
(fn] 
(fun] 
lapp] 
[ iJ] 
[let] 
top] 
r I-RI fn7l" x => eo ~ (fn7l" x => eeo) at r : 
Â«1'",@r", ~ 
1'o@ro)@r) & {put r} 
ru I-t (vjJ, W'].1')@r]I-RI fn7l" x => eo ~ 
(fn7l" x => eeo) at r : (T@r) & 'P 
r I-RI fun7l" f x => eo ~ (fun1T f W'j x => eeo) at r : 
Â«(vjJ, [g'J.1')@r) & 'P 
if jJ and if do not occur free in rand 'P 
r I-RI el ~ eel: Â«1'2@r2 ~ 
1'o@rO)@rl) & 'PI 
r I-RI e2 ~ ee2 : (1'2@r2) & 'P2 
r I-RI eo ~ eeo : (bool@ro) & 'Po 
r I-RI el ~ eel: (T@r) & 'PI r I-RI e2"-t ee2 : (T@r) & 'P2 
r I-RI if eo then el else e2 "-t if eeo then eel else ee2 : 
(T@r) & 'Po U 'PI U 'P2 U {get ro} 
r I-RI el "-t eel : (al @rl) & 'PI 
r[x I-t al @rl]I-RI e2"-t ee2 : (1'2@r2) & 'P2 
r I-RI el ~ eel : (r;p@rt) & 'PI 
r I-RI e2 ~ ee2 : (r;p@r2) & 'P2 
r rRI el op e2 ~ (eel op ee2) at r : 
(rop@r) & 'PI U 'P2 U {get rl, get r2, put r} 
Table 5.12: Region Inference Analysis and Translation (part 1). 
335 
There are two rules [genl] and [ge~] for generalising over type variables. One 
applies to ordinary types and the other to compound type schemes. In both 
cases we could have adapted the rule so as also to generalise over (additional) 
region and effect variables; however, even when doing so, the type inference 
is quite separate from the region and effect inference. 

336 
[sub] 
[region] 
r r-RI e "-+ ee : (T@r) & cp 
r r-RI e"-+ ee : (T'@r) & cp' 
if T ::::; T' and cp ~ cp' 
r r-RI e "-+ ee : (T@r) & cp 
r r-RI e"-+ ee : Â«V&T)@r) & cp 
if a do not occur free in rand cp 
r r-RI e"-+ ee : Â«vil, [~.T)@r) & cp 
r r-RI e"-+ ee : Â«Va, il, [~.T)@r) & cp 
if a do not occur free in rand cp 
r r-RI e"-+ ee : Â«Va, il.T)@r) & cp 
r r-RI e'V) ee : Â«B T)@r) & cp 
if B has dom(B) ~ {a,il} 
5 Type and Effect Systems 
r r-RI e"-+ ee : Â«Va, il, [~.T)@r) & cp 
r r-RI e "-+ ee[B~ at r' : Â«B T)@r') & cp u {get r, put r'} 
if B has dom(B) ~ {a, il, Q} 
r r-RI e "-+ ee : (T@r) & cp 
r r-RI e"-+ letregion e in ee : (T@r) & cp' 
if cp' = Observe(r, T, r) (cp) and e occurs in cp but not in cp' 
Table 5.13: Region Inference Analysis and Translation (part 2). 
There are two inference rules [insIJ and [ins2] for instantiating a type scheme. 
One is for ordinary types schemes and is invisible as far as the syntax of the 
extended expression is concerned. The other is for compound type schemes 
and is visible in the extended expression in that an explicit placement con-
struct is introduced; additionally the effect records that the value has been 
accessed and placed again. Both rules would typically be used immediately 
after the axiom for variables. 
The rule [region] for the letregion-construct uses an auxiliary function, 
Observe, to reduce the effect to what is visible from the outside; region vari-
ables that are no longer visible can then be encapsulated within the program. 
The auxiliary function may be defined as follows: 
Observe(r, T, r')( {put r}) 
= { 
{put r} if r occurs in r, T, or r' 
0 
otherwise 
Observe(r, T, r')( {get r}) 
= { 
{get r} if r occurs in r, T, or r' 
0 
otherwise 

5.5 Behaviours 
337 
Observe(r, T, r')(<PI U <P2) = Observe(r, T, r')(<PI) U Observe(r, T, r')(<P2) 
Observe(r, T, r')(0) 
= 0 
Observe(r, T, r')({3) = 
if {3 occurs in r, T, or r' 
otherwise 
Example 5.33 In Example 5.31 we considered the expression e 
(let x = 7 in fny y => y+x) 9 
and the extended expression ee: 
letregion elf ea. e4 
in (let x = (7 at ed 
in (fny y => (y+x) at e2) at ea) (9 at e4) 
One can now show that [] f-RI e ~ ee : (int@e2) & {put e2}. 
5.5 
Behaviours 
â¢ 
So far the effects have had a rather simple structure in that they merely 
denote sets of atomic actions like accessing a value or raising an exception. 
The effects have not attempted to capture the temporal order of these atomic 
actions. Often such information would be useful, for example to check that 
a variable is not accessed until after it has been assigned a value. In this 
section we shall show how to devise a Type and Effect System where effects 
(called behaviours) are able to record such temporal ordering in the context 
of a Communication Analysis for a fragment of Concurrent ML. 
5.5.1 
Communication Analysis 
Syntax. Let us consider an extension of the language FUN with con-
structs for generating new processes, for communicating between processes 
over typed channels, and for creating new channels. The syntactic category 
e E Exp of expressions is now given by: 
e ::= ... I channel,.. I spawn eo I send el on e2 I receive eo I eli e2 I ch 
Here channel,.. creates a new channel identifier (denoted ch above), spawn eo 
generates a new parallel process that executes eo, and send von ch sends the 
value v to another process ready to receive a value by means of receive chi 
sequential composition is as before. Channel identifiers 
ch E Chan channel identifiers 

338 
5 Type and Effect Systems 
i~ f, 
Figure 5.2: The pipeline produced by pipe [fl, f2] inp out. 
are created dynamically and are given by: 
ch ::= chan1 I chan2 I ... 
Also we shall assume that the constants, c E Const, not only include integers 
and booleans but also a special value called unit and denoted by 0; this is 
the value to be returned by the spawn and send constructs. 
Example 5.34 In this example we shall imagine that the expressions are 
extended with operations on lists: isnil e tests whether or not the list e 
is empty, hd e selects the first element and tl e selects the remainder of 
the list. We now define a function pipe that takes a list of functions, an 
input channel and an output channel as arguments; it constructs a pipeline 
of processes that apply the functions to the data arriving at the input channel 
and returns the results on the output channel (see Figure 5.2). It makes use 
of the function node that takes a function, an input channel and an output 
channel as arguments and it is defined as follows: 
let node = fnF f => fnl inp => fno out => 
spawn Â«funH h d => let v = receive inp 
in send (f v) on out; 
h d) 0) 
in funp pipe fs => fnl inp => fno out => 
if isnil fs then node (fnx x =>x) inp out 
else let ch = channelc 
in (node (hd fs) inp ch; pipe (tl fs) ch out) 
To deal with the empty list the function produces a process that just applies 
the identity function (denoted id in Figure 5.2). 
â¢ 
Semantics. We shall begin by defining the operational semantics of the 
sequential fragment of the language and then show how to incorporate it into 
the concurrent fragment. This will make use of a notion of evaluation context 
in order to obtain a succinct specification of the semantics. The sequential 
fragment is evaluated in an eager left to right manner and does not need any 

5.5 Behaviours 
339 
(fn1r x => e) v -+ e[x I-t v] 
let x = v in e -+ e[x I-t v] 
fu~ f x => e -+ (fn1r x => e)[f I-t (fun1r f x => e)] 
if true then el else e2 -+ el 
if false then el else e2 -+ e2 
v; e -+ e 
Table 5.14: The sequential semantics. 
notion of a store, a state or an environment. A fully evaluated expression 
gives rise to a value which is a constant, a channel identifier or a function 
abstraction. This may be modelled by 
v E Val values 
defined by: 
v ::= c I ch I fn1r x => eo 
There is no need to package the function abstraction with an environment 
because the semantics will treat a function application (fn1r x => eo) v by 
substituting v for x in eo yielding eo[x I-t v]. 
Part of the sequential semantics is specified in Table 5.14. In the manner of 
a Structural Operational Semantics it axiomatises the relation 
for when the expression el evaluates to e2 in one step. However, it does not 
describe how el may evaluate to e2 as a result of a subcomponent eu of el 
evaluating to e12 unlike what has been the case for the Structural Operational 
Semantics used so far. As an example Table 5.14 cannot be used to show 
that (1 + 2) + 4 -+ 3 + 4 although it can be used to show that 1 + 2 -+ 3. 
To recover from this shortcoming we shall make use of evaluation contexts; 
these are expressions containing one hole which is written []. Formally, 
evaluation contexts E are given by: 
E 
[ ] lEe I v E I let x = E in e 
if E then el else e2 I E op e I v op E 
send E on e I send v on E I receive E I E; e 

340 
5 Type and Effect Systems 
Here the syntax: ensures that E is an expression containing exactly one hole 
while e is an ordinary expression without any holes. The definition of E may 
be read as follows: you are allowed to evaluate an expression on its own, 
you may evaluate the function part of an application, you may evaluate the 
argument part of an application only after the function part has been fully 
evaluated etc. We shall write E[e] for the expression obtained by replacing 
the hole of E with e; so for example if E is [ ] + 4 then E[e] is e + 4. We 
dispense with the detailed definition of E[e] because it is straightforward: 
since the hole in E never occurs in the scope of a bound variable there is no 
risk of variable capture. 
The basic idea is to stipulate that el evaluates to e2 in one step (el => e2) if 
there exists E, elO and e20 such that el = E[elO], elO -+ e20 and e2 = E[e20]. 
As an example, (1+2)+4 => 3+4 by taking E to be []+4, elO to be 1+2 and 
e20 to be 3. Note that having E op e as an evaluation context corresponds 
to having an inference rule 
as in Table 3.3. As already said, an advantage of using evaluation contexts is 
that the description of the semantics often becomes more succinct; we shall 
benefit from this below. 
The concurrent semantics operates on a finite pool, PP, of processes and a 
finite set, CP, of channels. The set of channels keeps track of those chan-
nels that have been generated so far; this allows us to evaluate channel,.. 
by generating a new channel that has never been used before. The pool of 
processes both keeps track of the processes spawned so far and of the expres-
sion residing on each process; this allows us to allocate new processes when 
an expression is spawned and to let distinct processes communicate with one 
another. Formally 
P E Proc processes 
is defined by 
p ::= proc11 proc21Â·Â·Â· 
and we take 
CP 
E Pfin(Chan) 
PP 
E Proc -+fin Exp 
where it will be the case that each P P(P) is a closed expression. We shall 
write PP[p : e] for PP' given by dom(PP') = dom(PP) U {p}, PP'(P) = e 
and PP'(q) = PP(q) for q i: p. 
The concurrent semantics is specified in Table 5.15. It is a Structural Oper-
ational Semantics that axiomatises the relation CPl, PPl => CP2 , PP2 for 
when one configuration CPl, PPl in one step evolves into another configu-
ration CP2 , PP2â¢ Thanks to the use of evaluation contexts all clauses can 

5.5 Behaviours 
341 
[seq] 
CP, PPfp: E[el]] => CP, PPfp: E[e2]] 
if el ~ e2 
[chan] 
CP, PPfp: E[channel".]] => CP U {ch}, PPfp: E[ch]] 
if ch ~ CP 
[spawn] 
CP, PPfp : E[spawn eo]] => CP, PPfp: E[O]]fpo : eo] 
if Po ~ dom(PP) U {p} 
[comm] 
CP,PPfpl: EI[send von Ch]]fp2: E2[receive ch]] 
=> CP,PPfpl : EdO]][P2 :E2[V]] 
if PI =I P2 
Table 5.15: The concurrent semantics. 
be written succinctly. The clause [seq] incorporates the sequential semantics 
into the concurrent semantics (and corresponds to the discussion of el => e2 
above). The clause [chan] takes care of the allocation of channels: channel". 
is replaced by a fresh channel identifier ch. The clause [spawn] generates a 
new process, initialises it to the expression to be executed, and replaces the 
spawn-construct by the unit value. Finally, the clause [comm] allows syn-
chronous communication between distinct processes: the receive-construct 
is replaced by the value being sent and the send-construct is replaced by the 
unit value. 
Annotated types. The purpose of the Communication Analysis is to 
determine the communication behaviour of each expression: what channels 
will be allocated, what type of entities will be sent and received over channels, 
and what is the behaviour of the processes being generated. Furthermore, 
we are interested in recording the temporal order (or "causality") among 
these actions: what takes place before what. There are several ways to 
formalise this and we shall choose one where the inference system is not 
overly complicated (but where instead the inference algorithm presents some 
challenges) . 
To formalise our idea we introduce the following syntactic categories: 
T 
E 
TypecA 
types 
cp 
E 
AnncA 
annotations (or behaviours) 
r 
E 
RegcA 
regions 
(j 
E 
SchemecA type schemes 
Types are much as before but extended with a unit type (for the unit value) 
and a type for channels: 
T::=alboollint lunitlTl--4 T21Tchanr 

342 
5 Type and Effect Systems 
Type variables, a E TVar, are as before. 
Behaviours differ from the annotations and effects used so far in that we shall 
not merely use union for combining them: 
<p 
.. -
{3 I A I <P1; <P2 I <P1 + <P2 I rec{3.<p 
r chan r I spawn <P I r!r I r?r 
Behaviour variables, {3 E AVar, are as before and the behaviour A is used for 
atomic actions that do not involve communication; in a sense it corresponds 
to the empty set in previous annotations although it will be more intuitive to 
think of it as the empty string in regular expressions or as the silent action 
in process calculi. The behaviour <P1; <P2 says that <P1 takes place before <P2 
whereas <P1 + <P2 indicates a choice between <P1 and <P2 (as will be the case 
for the conditional); this is reminiscent of constructs in regular expressions 
as well as in process algebras. The construct rec/3.<p indicates a recursive 
behaviour that acts as given by <P except that any occurrence of /3 stands 
for rec{3.<p itself; it is typically used whenever there is explicit or implicit 
recursion in the program. 
The behaviour r chan r indicates that a new channel has been allocated over 
which entities of type r can be communicated; the region r indicates the set 
of program points (see below) where the creation could have taken place. 
The behaviour spawn <P indicates that a new process has been generated and 
that it operates as described by <po Next r!r indicates that a value is sent 
over a channel of type r chan rand r?r indicates that a value is received 
over a channel of that type; this is reminiscent of constructs in most process 
algebras (in particular CSP). 
Regions have a bit more structure than in Subsection 5.4.3 because now we 
shall also be able to take the union of regions: 
r ::= {-II"} I (J I r1 U r2 I 0 
Region variables, (J E RVar, are as before. In Subsection 5.4.3 the static 
regions were used to ensure that at run-time data would be allocated in the 
same dynamic regions; here regions are used to identify the set {11"1,Â·Â·Â·, 11" n} 
of program points where a channel might have been created. (So program 
points now play the role of region names.) However, these regions will not 
appear explicitly in the syntax of expressions, types or behaviours. 
Type schemes have the form 
where (1, ... ,(n is a (possibly empty) list of type variables, behaviour vari-
ables and region variables; if the list is empty we simply write r for \lO.r. 

5.5 Behaviours 
343 
Example 5.35 Returning to the program of Example 5.34, the node func-
tion is intended to have the type schema 
V'a, 'b, '1, "1, ''2. ('a -4 'b) ..14 ('a chan "1) ..14 ('b chan ''2) -!4 unit 
where cp = spawn(rec '2. ("l?'a; '1; ''2I'b; '2)) 
corresponding to the function argument having type 'a ~ 'b, the input 
channel having type 'a chan "1 and the output channel having type 'b chan ''2. 
When supplied with these arguments the node function will spawn a process 
that recursively will read on the input channel, execute the function supplied 
as its parameter, and write on the output channel - this is exactly what is 
expressed by cpo 
The pipe function is intended to have the type schema 
V'a, '1, "1, ''2. (('a -4 'a) list -14 ('a chan ("1 U {C})) 
..14 ('a chan ''2) ~ unit 
where cp' = rec '2. (spawn(rec '3. (("1 U {C} )?'a; A; ''2I'a; '3)) 
+ 'a chan C; spawn(rec '4. (("1 U {C})?'a; '1; Cl'a; '4)); '2) 
where the first summand in the body of cp' corresponds to the then-branch 
where node is called with the identity function (which has behaviour A) and 
the second to the else-branch of the conditional. Here we see that a channel 
is created, a process is spawned and then the overall behaviour recurses. We 
shall return to these types schemes after presenting the typing rules. 
_ 
Typing judgements. The typing judgements for the Communication 
Analysis will be of the form 
f I-CA e : (1 & cp 
where the type environment f maps variables to type schemes (or types), (1 
is the type scheme (or type) for the expression e, and cp is the behaviour that 
may arise during evaluation of e. The analysis is specified by the axiom and 
rules of Tables 5.16 and 5.17 and have many points in common with those 
we have seen before; the differences are explained below. 
The axioms [con] and [var] for constants and variables differ from the similar 
axioms in Table 5.10 in that A is used instead of 0. A similar remark holds 
for the two rules [fn] and ffun] for functions; note that [fun] does not explic-
itly demand CPo to be of the form rec{3' .cp' although this will frequently be 
necessary in order to satisfy the demands (using the ordering on behaviours 
to be introduced below). In the rule [app] for function application we now 
use sequencing to express that we first evaluate the function part, then the 
argument and finally the body of the function. In the rule [ifl for conditional 
we additionally use choice to express that only one of the then- and else-
branches are taken. Then the rules [let] and [op] should be straightforward. 

344 
5 Type and Effect Systems 
[con] r rCA c : Tc & A 
[var] r rCA x : a & A 
if r(x) = a 
[jn] 
[fun] 
lapp] 
[ ill 
[let] 
lop] 
r[x t-t 7",] rCA eo : 70 & 'Po 
r rCA fn7r x => eo : 7", ~ 
70 & A 
ru t-t 7", ~ 
70][X t-t 7",] rCA eo : 70 & 'Po 
r rCA fun7r f x => eo : 7", ~ 
70 & A 
r rCA e1 : 72 ....'e4 70 & 'P1 r rCA e2 : 72 & 'P2 
r rCA e1 e2 : 70 & 'P1; 'P2; 'Po 
r rCA eo : bool & 'Po r rCA e1 : 7 & 'P1 r rCA e2 : 7 & 'P2 
r rCA if eo then e1 else e2 : 7 & 'Po; ('P1 + 'P2) 
r rCA e1 : 0'1 & 'P1 r[x t-t 0'1] rCA e2 : ~ & 'P2 
r rCA let x = e1 in e2 : 72 & 'P1; 'P2 
~ 
1 
~ 
2 
r rCA e1 : Top & 'P1 r rCA e2 : Top & 'P2 
r rCA e1 op e2 : Top & 'P1; 'P2; A 
Table 5.16: Communication Analysis (part 1). 
The axiom [chan] for chan~el creation makes sure to record the program 
point in the type as well as the behaviour, the rule [spawn] encapsulates the 
behaviour of the spawned process in the behaviour of the construct itself and 
the rules [send] and [receive] for sending and receiving values over channels 
indicate the order in which the arguments are evaluated and then produce the 
behaviour for the axiom taken. The rules [seq] and [ch] are straightforward 
and the rules [gen] and [ins] are much as in Table 5.10. 
Example 5.36 Consider the following example program: 
let ch = channelA 
in (send 1 on chi send true on ch) 
Intuitively, this program should be rejected because the channel ch is used 
for communicating values of two different types thereby violating type safety. 
To see that the program is indeed rejected, first observe that the rule [chan] 
gives: 
[ ] rCA channelA: 'a chan {A} & 'a chan {A} 
Then the generalisation rule [gen] does not allow us to generalise over the 
type variable 'a since it occurs in the behaviour 'a chan {A}; it follows that 

5.5 Behaviours 
[chan] 
[spawn] 
[send] 
[receive] 
[seq] 
[ch] 
[sub] 
[gen] 
[ins] 
r r-CA channel1r
: l' chan {7r} & l' chan {7r} 
r r-CA eo : To & CPo 
r r-CA spawn eo : unit & spawn CPo 
r r-CA e1 : l' & CP1 
r r-CA e2 : l' chan r2 & CP2 
r r-CA send e1 on e2 : unit & CPI; CP2; r2!T 
r r-CA eo : l' chan ro & CPo 
r r-CA receive eo: l' & cpo;ro?T 
r r-CA eI : 1'1 & CPI r r-CA e2 : 1'2 & CP2 
r r-CA e1; e2 : 1'2 & CP1; CP2 
r r-CA ch : l' chan r & A 
r r-CA e : l' & cP 
r r-CA e : 1" & cP' 
r r-CA e : l' & cP 
r r-CA e : \1(1'Â·Â·Â·' (n).T & cP 
if l' chan r = r(ch) 
if l' :::; 1" and cP !;;; cp' 
if (1, ... ,(n do not occur free in rand cP 
f t-CA e : \1(1'Â·Â·Â·' (n).T & cP 
r r-CA e : (f) 1') & cP 
if f) has dom(f)) <;;; {(I,Â·Â·Â· ,en} 
Table 5.17: Communication Analysis (part 2). 
345 
ch can "only" be given the type 'a chan {A} and therefore the program fails 
to type check (because 'a cannot be equal to int and bool at the same time). 
However, if the generalisation rule [gen] were only to require that (1,Â· .. , (n 
do not occur free in r, then it would be possible to give the channel ch the 
type schema \I'a. 'a chan {A}; as a consequence we would be able to give types 
to the two occurrences of send in the body of the let construct and the type 
system would not be semantically correct. 
_ 
The rule [sub] for subeffecting and subtyping looks like the one in Table 5.10 
but involves a few subtleties. The ordering l' :::; 1" on types is given by 
1':::;1" 
1":::;1' r<;;;r' 
l' chan r :::; 1" chan r' 

346 
5 Type and Effect Systems 
'PI C 'P2 
'P2!;;; 'P3 
'PI !;;; 'P3 
'PI !;;; 'P2 
'P3!;;; 'P4 
~c~ 
~!;;;~ 
spawn 'PI !;;; spawn 'P2 
recfi.'P1 !;;; recfiÂ·'P2 
'P !;;; A; 'P 
A; 'P !;;; 'P 
'P !;;; 'P; A 
'P; A!;;; 'P 
recfiÂ·'P !;;; 'P[fi t-+ recfiÂ·'Pl 
'P[fi t-+ recfiÂ·'Pl !;;; recfi.'P 
75,7' 7'5,7 rc;;,r' 
7 chan r !;;; 7' chan r' 
rl c;;, r2 
71:::; 72 
rl!7i !;;; r2 !72 
rl c;;, r2 
72 5, 71 
rl ?71 !;;; r2?~ 
Table 5.18: Ordering on behaviours. 
and is similar to the definition in Subsection 5.4.1: 71 ...!4 72 is contravariant 
in 7i but covariant in 'P and 72, and 7 chan r is both covariant in 7 (for 
when a value is sent) and contravariant in 7 (for when a value is received) 
and it is covariant in r. The ordering r c;;, r' means that r is "a subset of" 
of r' (modulo VCAl) just as what was the case for the effects in Section 
5.4. Finally, the ordering 'P !;;; 'P' on behaviours in more complex than before 
because of the rich structure possessed by behaviours. The definition is given 
in Table 5.18 and will be explained below. Since the syntactic categories of 
types and behaviours are mutually recursive also the definitions of 7 5, 7' 
and 'P !;;; 'P' need to be interpreted recursively. 
The axiomatisation of'P !;;; 'P' in Table 5.18 ensures that we obtain a preorder 
that is a congruence with respect to the operations for combining behaviours. 
Furthermore, sequencing is an associative operation with A as identity and 
we have a distributive law with respect to choice. It follows that choice is 
associative and commutative. Next the axioms for recursion allows us to 
unfold the rec-construct. The final three rules clarify how behaviours depend 
upon types and regions: 7 chan r is both contravariant and covariant in 7 and 

Concluding Remarks 
347 
is covariant in r (just as was the case for the type r chan r); r!r is covariant 
in both rand r (because a value is sent) whereas r?r is covariant in rand 
contravariant in r (because a value is received). There is no explicit law for 
renaming bound behaviour variables as we shall regard rec{3.cp as being equal 
to rec{3'.cp' whenever they are a-equivalent. 
The Communication Analysis in Tables 5.16 and 5.17 differs from the Region 
Inference Analysis of Tables 5.12 and 5.13 in that there is no analogue of the 
rule [region] where an Observe function is used to reduce the annotation to 
what is visible from the outside. The reason is that the Communication Anal-
ysis is intended to record all the side effects (in the form of communications) 
that take place during computation. 
Example 5.37 Returning to the program of Example 5.35 we can now 
verify that the node function can be given the type: 
(1a 4 'b) -J4 (1a chan 111) -J4 (1b chan 112) ..!4 unit 
where cp = spawn(rec '2. C'l?'a; '1; 112!'b; '2)) 
We use the rules [fun] and [sub] of Tables 5.16 and 5.17 together with the 
axiom cp[{3 t---t rec{3.cp] ~ rec{3.cp of Table 5.18. Then the rule [gen] allows us 
to obtain the type schema given in Example 5.35. 
Turning to the pipe function we first note that it can be given a type of the 
form 
'1 
A 
A 
' 
(('a -+ 'a) list) -+ ('a chan (111 U {C})) -+ ('a chan 112) ~ unit 
where the regions for the input and local channels are "merged" because they 
can both be used as input channels in a call to pipe whereas the region for 
the output channel is always kept separate. The behaviour cp' is of the form 
rec '2. (spawn(rec '3. ((111 U {C})?'a; A; 112!'a; '3)) 
+ 'a chan C; spawn(rec '4. ((111 U {C} )?'a; '1; e!'a; '4)); '2) 
because in the then-branch the input channel for node has type 'a chan (111 U 
{C}) and the output channel has type 'a chan 112, whereas in the else-branch 
the input channel for node has type'a chan (1I1U{C}) and the output channel 
has type 'a chan {C} (as well as 'a chan (111 U {C})). The rule [gen] allows us 
to obtain the type schema displayed in Example 5.35. 
â¢ 
Concluding Remarks 
Control Flow Analysis. The literature [49, 50, 71, 18] contains many 
formulations of non-standard type systems aiming at performing Control 

348 
5 Type and Effect Systems 
Flow Analysis. The formulation presented in Section 5.1 uses a particularly 
simple set of techniques where types are annotated but there is no additional 
effect component nor is there any coverage of polymorphism or subtyping. 
Although there is no explicit clause for sub effecting in the manner of Section 
5.4, we regard the formulation as a subeJJecting analysis because the rules 
for function abstraction allow us to increase the annotation on the function 
arrow in much the same way as is the case in sub effecting (and in much more 
restricted ways than holds for subtyping; see also Exercise 5.13). References 
for type systems with subtyping include [59, 58, 107] as well as the more ad-
vanced [83, 160, 161] that also deal with polymorphism. To allow a general 
treatment of subtyping, these papers generally demand constraints to be an 
explicit part of the inference system unlike what was done in Section 5.1. In-
deed, the formulation of Section 5.1 allows only shape conform ant subtyping, 
where the underlying type system does not make use of any form of subtyp-
ing, and is thus somewhat simpler than atomic sub typing, where an ordering 
is imposed upon base types, and general subtyping, where an ordering may 
be imposed between arbitrary types. 
The semantic correctness of the Control Flow Analysis established in Section 
5.2 is expressed as a subject reduction result but formulated for a Natural Se-
mantics [91] (rather than a Structural Operational Semantics); this approach 
to semantic correctness has a rather long history [114, 116, 184]. The Moore 
family result about the set of typings is inspired by the ideas of [114] and is 
included so as to stress the fundamental role played by partial orders in all 
of the approaches to program analysis considered in this book; it also relates 
to the study of principal types in type systems. 
The development of a syntactically sound and complete algorithm for the 
Control Flow Analysis in Section 5.3 is based on the ideas in [90, 183, 167, 
168, 169]; the call-tracking analysis of Mini Project 5.1 is based on [169]. 
The basic idea is to ensure that the algorithm operates on a free algebra by 
restricting annotations to be annotation variables only (the concept of simple 
types) and by recording a set of constraints for the meaning of the annotation 
variables; in our case this is particularly straightforward because the Control 
Flow Analysis does not deal with polymorphism. Our development differs 
somewhat from that of [58, 107] that deal with the more advanced notions of 
atomic subtyping and general subtyping. A different approach, not studied 
here, would be to dispense with simple types and constraints and instead use 
techniques for unifying types in a non-free algebra [159]. 
Restricted effects. The Type and Effect Systems presented in Sec-
tion 5.4 all share the important property (also holding for the type system 
in Section 5.1) that no type information is recorded in the effects and that 
the shape of the type information cannot be influenced by the effects. All 
systems included a proper effect component and thereby illustrated the di-
versity of effects; some pioneering papers in Type and Effect Systems are 

Concluding Remarks 
349 
[88, 102, 89, 90]. At the same time we illustrated a number of design consid-
erations to be taken into account when devising a Type and Effect System: 
whether or not to incorporate sub effecting, subtyping, polymorphism, poly-
morphic recursion, whether or not types are allowed to be influenced by 
effects (which is not the case in Sections 5.4 and 5.1), and whether or not 
constraints are an explicit part of the inference system (as is implicitly the 
case in Subsection 5.4.3). However, it would be incorrect to surmise that the 
selection of components are inherently linked to the example analysis where 
they were illustrated. Rather, the techniques needed for semantic correctness 
and for syntactic soundness and completeness depend heavily on the partic-
ular selection of components; some are straightforward to deal with whereas 
others are beyond state-of-the-art. 
The Side Effect Analysis presented in Subsection 5.4.1 illustrated the use 
of sub effecting and subtyping, but did not incorporate polymorphism, there 
were no constraints in the inference system, and the effects did not influence 
the types. This system is sufficiently simple that semantic soundness may 
be established using the techniques of Section 5.2. If the rule for subtyping 
was omitted then also the techniques developed in Section 5.3 would suffice 
for obtaining a sound and complete inference algorithm. The presence of 
the rule for subtyping naturally leads to a two stage implementation process: 
first the underlying types are inferred and next the constraints on effects 
(or annotations) are determined [169, 174]. This works because we restrict 
ourselves to shape conformant subtyping where effects do not influence the 
type information. However, adding polymorphism to this development would 
dramatically increase the complexity of the development (see below). 
The Exception Analysis presented in Subsection 5.4.2 illustrated the use of 
sub effecting, subtyping and polymorphism, but there were no constraints in 
the inference system, and the effects did not influence the types. Semantic 
soundness is a bit more complex than in Section 5.2 because of the polymor-
phism but the techniques of [173, 167, 168, 15] suffice. For the development 
of a syntactically sound and complete inference algorithm one may take the 
two stage approach described above [169, 174]; as before, it works because 
we restrict ourselves to shape conformant subtyping where effects do not 
influence the type information. Alternatively, one may use more powerful 
techniques [183, 167, 168, 127] that even allow to include type information 
inside effects; this amounts to an extension of the approach of Section 5.3 
and will be explained below. 
The Region Inference analysis presented in Subsection 5.4.3 illustrated the 
use of polymorphic recursion as far as the effects are concerned, there were 
implicitly constraints in the inference system (via the dot notation on func-
tion arrows), but still the effects cannot influence the types. The presentation 
is mainly based on [175] but adapted to the FUN language and the style of 
presentation used elsewhere in this chapter. To obtain effects that are as 

350 
5 Type and Effect Systems 
small as possible the inference system uses "effect masking" (developed in 
[102, 167, 168]) for removing internal components of the effect: effect com-
ponents that only deal with regions that are not externally visible. Semantic 
correctness of the inference system can be shown using the approach of [176]. 
For the development of a syntactically sound inference algorithm one may 
once more take the two stage approach described above; the first stage (or-
dinary type inference) is standard and the second stage is considered in [174] 
where algorithm S generates effect and region variables and algorithm n 
deals with the complications due to polymorphic recursion (for effects and 
regions only). The inference algorithm is proved syntactically sound but is 
known not to be syntactically complete; indeed, obtaining an algorithm that 
is syntactically sound as well as complete, seems beyond state-of-the-art. 
General effects. 
One way to make effects more expressive is to allow 
type information inside the effects so that the shape of the type information 
can be influenced by the effects. This idea occurred already in [183, 167, 168] 
for an extended Side Effect Analysis making use of polymorphism and subef-
fecting (but not sUbtyping); this work attempted to "generalise" previous 
work based on the idea of expansive expressions and imperative versus ap-
plicative type variables [173, 166]. As already indicated, semantic soundness 
amounts to an extension of the techniques of Section 5.2 as presented in 
[173,167, 168, 15]. 
The two stage approach no longer works for obtaining an inference algorithm 
because the effects are used to control the shape of the underlying types 
in the form of which type variables are included in a polymorphic type. 
This suggests extending the techniques of Section 5.3 in that special care 
needs to be taken when deciding the variables over which to generalise when 
constructing a polymorphic type. The main idea is that the algorithm needs 
to consult the constraints in order to determine a larger set of forbidden 
variables than those directly occurring in the type environment or the effect; 
this can be formulated as a downwards closure with respect to the constraint 
set [183, 127] or by taking a principal solution of the constraints into account 
[167, 168]. 
Adding subtyping to this development dramatically increases the complexity 
of the development. The integration of shape conformant subtyping, poly-
morphism and sub effecting is done in [127, 134, 15] that establishes semantic 
soundness and develops an inference algorithm that is proved syntactically 
sound; extensions of this development incorporate a syntactic completeness 
result (see the discussion of [13] below). This work went a long way to-
wards integrating the techniques for polymorphism and sub effecting (but no 
subtyping) from Effect Systems [183, 167, 168] with the techniques for poly-
morphism and subtyping (but no effects) from Type Systems [83, 160, 161]. 
Another way to make effects more expressive is to let them contain infor-
mation about the temporal order and causality of actions, rather than just 

Mini Projects 
351 
being an unordered set of possibilities. In Section 5.5 we considered the task 
of extracting behaviours (reminiscent of terms in a process algebra) from 
programs in Concurrent ML by means of a Type and Effect System; here 
effects (the behaviours) have structure, they may influence the type informa-
tion, there are no explicit constraints in the inference system (although there 
are in more advanced developments [13]), and there are inference rules for 
sub effecting and shape conformant subtyping. These ideas first occurred in 
[119, 120] (not involving polymorphism) and in [131, 132, 14] (involving poly-
morphism); our presentation in Section 5.5 is mainly based on [131, 132] with 
ingredients from [119, 120]. We refer to [13] for a comprehensive account of a 
more ambitious development where the inference system is massaged so as to 
facilitate developing a syntactically sound and complete inference algorithm; 
this includes having explicit constraints in the inference system as is usually 
the case in type systems that make use of subtyping. An application to the 
validation of embedded systems is presented in [128]. 
Other developments. All of the formulations presented in this chap-
ter have had a number of common features: to the extent that polymorphism 
has been incorporated it has been based on the Hindley /MiIner polymor-
phism also found in Standard ML, there has been no subtyping involved in 
the underlying type system, and there has been no treatment of conjunction 
or disjunction types as in [19, 20, 81, 82]. Also all of the formulations have 
expressed safety properties: if a certain point is reached then certain infor-
mation will hold; liveness properties in the form of adding annotations that 
indicate whether or not functions can be assumed to be total was considered 
in [121]. 
Finally, linking up with the development of Chapter 4 on Abstract Interpre-
tation, it is possible to allow annotations to be elements of a complete lattice 
(that is possibly of finite height as in Monotone Frameworks) as outlined in 
Mini Project 5.4; s this mini project also discusses how to deal with binding 
time analyses (inpired by [75, 114, 123]) and security analyses (inspired by 
[74, 1]). In the other direction it may be profitable to describe Type and 
Effect Systems using the framework of Abstract Interpretation [108, 36]. 
Mini Project 5.5 on units and the year 2000 problem (Y2K) was inspired by 
[95, 46, 141]. 
Mini Projects 
Mini Project 5.1 A Call-Tracking Analysis 
Consider a Type and Effect System for Call-Tracking Analysis: it has judge-
ments of the form 
r I-CT e : f & cp 

352 
5 Type and Effect Systems 
where !.p denotes the set of functions that may be called during the evaluation 
of e (and similarly for the annotations on function arrows). 
1. Formulate an inference system with subeffectingj next add subtyping 
and finally add polymorphism. 
Next consider the inference system with sub effecting only: 
2. Modify the Natural Semantics of Table 5.4 such that semantic correct-
ness can be stated for the analysis and prove that the result holds. 
3. Devise an algorithm for Call-Tracking Analysis and prove that it is 
syntactically sound (and complete). 
For the more ambitious: can you also deal with subtyping and/or polymor-
phism? 
_ 
Mini Project 5.2 Data Structures 
As in Mini Project 3.2 we shall now extend the language with more general 
data structures and consider how to modify the Control Flow Analysis (Table 
5.2) so as to track the creation points. 
Pairs. To accommodate pairs we extend the syntax as follows: 
7 .. -
... 171 x'P 72 
e 
.. -
... I Pair7r(el,e2) I (case eo of Pair(xl,x2) => et} 
Here Pair is a binary constructor and the corresponding case-expression does 
not need an or-component as in Mini Project 3.2. As an example, consider 
the following program for "sorting" a pair of integers: 
let srt = fnx x => case x of Pair(y,z) => 
if y<z then x else PairB(z,y) 
in srt(PairA(n,mÂ» 
Here the pair returned with be constructed at A if the value of n is smaller 
than the value ofm and at B otherwise. The overall type is int X{A,B} into 
1. Modify the Control Flow Analysis of Table 5.2 to track the creation 
points of pairs. 
2. Extend the Natural Semantics of Table 5.4 and augment the proof of 
semantic correctness (Theorem 5.9). 
3. Extend the algorithms WCFA and UCFA and augment the proof of syn-
tactic soundness and completeness (Theorems 5.20 and 5.21). 

Mini Projects 
353 
Lists. To accommodate lists we extend the syntax as follows: 
T .. -
... 1 T list'!' 
e 
.. -
Â·Â·Â·1 Cons'IT(el,e2) 1 Nil'IT 1 (case eo of Cons(xl,x2) => el or e2) 
Now perform a similar development as the one you performed for pairs. 
For the more ambitious: can you give a more general treatment of algebraic 
types in the manner of Mini Project 3.2? 
â¢ 
Mini Project 5.3 A Prototype Implementation 
In this mini project we shall implement the Control Flow Analysis consid-
ered in Sections 5.1 and 5.3. As implementation language we shall choose a 
functional language such as Standard ML or Haskell. We can then define a 
suitable data type for FUN expressions as follows: 
type var 
type point 
= 
string 
= int 
datatype const = 
Num of int 1 True 1 False 
datatype exp 
= Const of const 1 Var of var 1 Fn of point * var * exp 
Fun of point * var * var * exp 1 App of exp * exp 
If of exp * exp * exp 1 Let of var * exp * exp 
Op of string * exp * exp 
Now proceed as follows: 
1. Define data types for simple types and simple substitutions and imple-
ment the function UCFA of Table 5.7. 
2. Define data types for simple type environments and constraint sets and 
implement the function WCFA of Table 5.8. 
3. Define data types for types and type environments and implement a 
function that pretty prints the result in the manner of Subsection 5.3.4: 
type variables must get instantiated to int and annotation variables to 
the least solution to the constraints. 
Test your implementation on selected examples. 
â¢ 
Mini Project 5.4 Monotone Type Systems 
Consider an instance of a monotone structure as defined in Subsection 3.5.2 
(and motivated by the Monotone Frameworks of Section 2.3); examples in-
clude the Constant Propagation Analysis of Example 3.29. 

354 
5 Type and Effect Systems 
Define an Annotated Type System with sub effecting (and possibly subtyp-
ing) for performing the analysis specified by the monotone structure (in the 
manner of the annotated base types of Table 1.2). Design the system such 
that it becomes possible to specify the following analyses: 
â¢ A binding time analysis where data can be static (i.e. available at 
compile-time) or dynamic (i.e. available at run-time), denoted 5 and 
D, respectively. Define a partial ordering by setting 5 !; D to indicate 
that static data can also be used for operating upon dynamic data. 
â¢ A security analysis where data can be classified at several clearance 
levels C1,Â· .. , Ck. Define a partial ordering by setting C1 !; ... !; Ck to 
indicate that data at low levels of clearance can also be used at high 
levels of clearance. 
How much of the development in Sections 5.2 and 5.3 can be adapted to 
this scenario? (Feel free to impose further conditions if needed, e.g. that the 
property space satisfies the Ascending Chain Condition.) 
For the more ambitious: can you deal with both subtyping and polymor-
phism? 
_ 
Mini Project 5.5 Units of Measure (and Y2K) 
Even the most advanced programming languages, allowing user defined data 
types and enforcing strong typing, hardly ever record units of measure (for 
example meters, degrees Centigrade, US dollars). Each unit of measure may 
involve two ingredients: the scaling factor and the base of origin: 
â¢ An example of different scaling factors concerns the measure of length 
which can be measured in feet (FT) or in meters (M)j in this case there 
is a simple conversion between the two units: x FT = (x * 0.3048) 
M. A related example concerns the measure of currency which can be 
measured in US dollars (USD) or European currency units (EURO)j 
here the conversion is slightly more complex: x USD = (x* rate) EURO 
where rate is the exchange rate which is likely to vary over time. 
â¢ An example of different bases of origins concerns the measure of time 
which can be measured relative to the birth of Christ (AD) or relative to 
the beginning of the 20'th Century (YY)j there is a simple conversion: 
x YY = (1900 + x) AD. The so-called year 2000 problem (Y2K) arises 
when there are only two digits available for x and one needs to represent 
a year that is not in the 20'th Century. 
â¢ An example of a measure involving both ingredients is the measure of 
temperature which can be measured in degrees Fahrenheit (F) and de-

Mini Projects 
355 
grees Centigrade (C); in this case the conversion between temperatures 
is more complex: x F = ((x - 32) * 5)/9 C. 
There are many reasons for why one would like to extend the programming 
language to be precise about the units of measurement; perhaps one needs 
to port the software to a new environment where a different unit of mea-
surement is used, perhaps one is operating on data presented in a mixture of 
units of measurement, perhaps one wants to reduce the likelihood of incor-
rect computations (like adding US dollars and European currency units), or 
perhaps one wants to prepare for a change in data formats. 
Let us assume that we are given a program that is correctly typed in some un-
derlying type system. One approach to incorporating units into the program 
proceeds as follows: 
1. The type system is extended to include unit annotations on numbers. 
2. The program is analysed according to the extended type system; most 
likely a number of type errors are identified. 
3. The type errors are corrected by inserting explicit conversion functions 
based on the above discussion. 
4. It is checked that the resulting program is type correct in the extended 
type system. 
5. Possibly data formats are changed based on the extended type infor-
mation. 
In this mini project we focus on the first task and for our purposes it suffices 
to let the syntax of the underlying types be given by: 
T ::= num I T -+ T 
A suitable extended type system might be based around the following types 
and annotations: 
f 
<p 
scale 
base 
num'P If -+ f 
scale BASE base 
u I unit I scale. scale I scale-1 I 
feet I meter I usd I euro I year I kelvin I '" 
f3 I none I ad I 20th I freezing I ... 
We model meters (M) as meter BASE none and feet (FT) as feet BASE none; 
US dollars (USD) as usd BASE none and European currency units (EURO) as 
euro BASE none; years relative to the birth of Christ (AD) as year BASE ad 
and years relative to the 20'th Century (YY) as year BASE 20th; degrees 

356 
5 Type and Effect Systems 
Centigrade (C) as kelvin BASE freezing etc. "Ordinary" numbers without 
units now have the annotation unit BASE none. 
One way to associate units with numbers is to use functions like 
asC : numunitBASEnone -+ numkelvinBASEfreezing 
so that asC 7 denotes 7 degrees Centigrade. To compute with numbers 
having units we shall give the arithmetic operations for multiplication and 
division the following polymorphic types: 
* 
VU1,u2,u[with u = U1.U2] : 
numO"l BASE none -+ num0"2 BASE none -+ numO" BASE none 
/ 
VU1,u2,u[with u = udu21)]: 
numO"l BASE none -+ num0"2 BASE none -+ numO" BASE none 
This reflects the fact that multiplication and division works on relative units 
of measurement (Le. having no base). One needs to be careful about the the 
laws imposed upon annotations so as to ensure that e.g. meter.feet is treated 
in the same way as feet.meter in accordance with the usual conventions of 
Physics; this amounts to an axiomatisation that is more refined than the 
UCAl axiomatisation. 
Turning to the operations for addition and substraction: 
+ 
if /32 = none ] 
if /31 = none 
Vu,(31,(32,(3 [With (3 = { ~~ne 
undefined 
if /32 = none 1 
if (31 = (32 
otherwise 
This reflects the fact that addition can be used on units of measurement 
where at most one has a base and that subtraction can be used to find the 
relative unit of measurement between two based measurements as well as to 
decrease a based measurement by a relative unit. 
Define an annotated type system for a version of the FUN language using 
a selection of the ideas above. It may be helpful to allow polymorphism 
only for the scale annotations (using the scale variables u) and the base 
annotations (using the base variables (3). Try to establish a notion of semantic 
correctness for the annotated type system. Develop an inference algorithm 
and prove it syntactically sound. Finally investigate whether or not the 
inference algorithm is syntactically complete. 

Exercises 
357 
In some cases it is possible to extend this scheme with explicit rules for 
conversion between units; e.g. x FT = x * 30.48 CM which involves both a 
conversion between feet (FT) and meters (M) and between meters (M) and 
centimeters (CM). This is not always feasible (e.g. in the case of currency) 
and we shall leave the extension to the interested reader. 
â¢ 
Exercises 
Exercise 5.1 Consider the following expression: 
let f = fnx x => x 1; 
g = fny y => y+2; 
h = fnz z => z+3 
in (f g) + (f h) 
Use Table 5.1 (the underlying type system) to obtain a type for this expres-
sion; what types do you use for f, g and h? Next use Table 5.2 (Control Flow 
Analysis) to obtain the annotated type of this expression; what annotated 
types do you use for f, g and h? 
â¢ 
Exercise 5.2 Consider the following variation of FUN where function def-
initions explicitly involve type information as in fn1l" x : 7 x => eo and fun1l" f : 
(Tx -4 TO) X => eo- Modify Table 5.1 accordingly and prove that the resulting 
type system is deterministic: r rUL e : 71 and r rUL e : 72 imply 71 = 72. 
â¢ 
Exercise 5.3 Consider the inclusion 4'1 ~ 4'2 between effects that was 
discussed in Subsections 5.4.1 and 5.2.3. Give an axiomatisation of 4'1 ~ 4'2 
such that 'PI ~ 'P2 holds if and only if the set of elements mentioned in 4'1 is 
a subset of the set of elements mentioned in 4'2. 
â¢ 
Exercise* 5.4 In Example 5.4 we showed how to record the annotated 
type information in a form close to that considered in Chapter 3. To make 
this precise suppose that expressions of FUN are simultaneously labelled as 
in Chapters 3 and 5: 
e 
t.e 
t 
c I x I fn1l" x => eo I fun1l" f x => eo I el e2 
if eo then el else e2 I let x = el in e2 I el op e2 
Next modify Table 5.2 so as to define a judgement 
p,c;r rCFA e: T 
(5.2) 
where the idea is that 

358 
5 Type and Effect Systems 
â¢ 6(Â£) = Ti ensures that all judgements p, 6; fl I-CFA tl : TI in (5.2) have 
TI = Ti, and 
â¢ p(x) = To: ensures that all judgements p, 6; fl I-CFAxi : T in (5.2) have 
T' = Tze 
Check that (5.2) holds for the expression in Example 5.4 when we take 6(1) = 
Tv, 6(2) = Tv ~ 
Tv, 6(3) = int, 6(4) = Tv, 6(5) = Tv, p(x) = Tv and 
p(y) = into 
_ 
Exercise 5.5 Consider adding the following inference rule to Table 5.2 
(Control Flow Analysis) 
f I-CFA e1 : T2 .Jet TO f I-CFA e2 : T2. 
0 
~ 
1f<p= 
r I-CFA e1 e2 : .1~ 
TO 
where .1;0 is the least element of TYPe [TO J and TO = LToJ. Explain what this 
rule does and determine whether or not Theorem 5.9 (semantic correctness) 
continues to hold. 
_ 
Exercise 5.6 We shall now extend the Control Flow Analysis of Section 
5.1 to include information about where functions are called. To do so modify 
the syntax of expressions by adding labels to all application points (in the 
manner of Chapter 3): 
Also define a syntactic category of label annotations 'l/J E LAnn by 
'l/J ::= Â£ I 'l/J1 U 'l/J2 I 0 
to be interpreted modulo UCAI. Finally modify the syntax of annotated types 
as follows: 
In this system it should be possible to type the expression 
Â«fnx x => x) (fny y => yÂ»1 
(see Example 5.4) such that fnx x => x gets the type 
(int {V}) int) {X}) (int {V}) int)Â· 
o 
{1} 
0 
indicating that it is called at the application labelled 1; more generally it 
should have the type 
(~ {Y}ucf>y 
~) {X}uq,x 
(~ {Y}uq,y 
~) 
T 
) T 
) 
T 
) 
T 
.py 
{1 }u.px 
.py 
for all <Px, <Pv E Ann, 'l/Jx, 'l/Jy E LAnn, and T E TYPe. Modify the analysis 
of Table 5.2 so as to specify this analysis. 
_ 

Exercises 
359 
Exercise 5.7 In Section 5.2 we equipped FUN with a call-by-value se-
mantics. An alternative would be to use a call-by-name semantics. It can be 
obtained as a simple modification of the semantics of Table 5.4 by changing 
rule [app] such that the argument is not evaluated before the substitution 
takes place and similarly changing rule [let]. Make these changes to Table 5.4 
and show that the correctness result (Theorem 5.9) still holds for the analysis 
of Table 5.2. 
What does that tell us about the precision of the analysis? 
-
Exercise 5.8 Prove Fact 5.17 (the syntactic soundness and completeness 
of Robinson unification). 
_ 
Exercise 5.9 Consider the partial ordering 7 ::; 7' on the annotated types 
of Section 5.1 that is defined by: 
......... 
1..fJ,"""'" 
-, 
cp', -, 
71 --"-t 72::; 71 -'-t 72 
We shall say that this ordering treats 71 ..!4 72 covariantly in r.p and 72 but 
contravariantly in 71; this is in line with the subtype ordering considered in 
Section 5.4 but differs form the partial ordering 7 I;;:; 7' considered in Section 
5.2. 
Show that (TYPe[7J, ::;) is a complete lattice for all choices of the underlying 
type 7 E Type. Next investigate whether or not an analogue of Proposition 
5.12 holds for this ordering. 
Finally reconsider the decision to let Br(a) in Subsection 5.3.4 be the least 
element of (Type[7], 1;;:;); would it be preferable to let Br(a) be the least 
element of (TYPe[7],::;) or (Type[7], 2:)? 
-
Exercise* 5.10 Suppose that 
WUL([ ],funF f x => eo) = (ax -t 0:0, B) 
where ax and ao are distinct type variables. Let e be an arbitrary correctly 
typed closed expression, i.e. [] f-UL e : 7 for some 7 and show that the call 
(funF f x => eo) e 
cannot terminate. (Hint: use Fact 5.6, Theorems 5.9, 5.20 and 5.21 and that 
WUL is syntactically sound.) 
_ 
Exercise 5.11 Formulate what it means for the Side Effect Analysis of 
Table 5.9 to be semantically correct; this involves modifying the Natural 
Semantics of Table 5.4 to deal with the store and to record the side effects. 
(Proving the result would require quite some work.) 
_ 

360 
5 Type and Effect Systems 
Exercise 5.12 Suppose that the language of Subsection 5.4.1 has a call-
by-name semantics rather than a call-by-value semantics. Modify the Side 
Effect Analysis of Table 5.9 accordingly. 
_ 
Exercise 5.13 We shall now illustrate the concept of proof normalisation 
for the Side Effect Analysis of Subsection 5.4.1; for this we shall assume that 
Table 5.9 does not include the combined rule for subeffecting and subtyping 
but only the rule for subeffecting. 
For this system one can dispense with an explicit rule for sub effecting by 
integrating its effects into all other rules; this can be done by adding a "Uc,o'" 
to all effects occurring in the conclusions of all axioms and rules. Do this and 
argue that exactly the same judgements are provable in the two systems. 
A further variation is only to incorporate "Uc,o'" where it is really needed: in 
all axioms and in the rules for function abstraction. Do this and argue that 
once more exactly the same judgements are provable in the two systems. 
What you have performed amounts to proof normalisation: whenever one 
has an inference system with a number of syntax directed rules and axioms 
and at least one rule that is not syntax directed, it is frequently possible to 
restrict the use of the non syntax directed rules. In this way the structure 
of the inference trees comes closer to the structure of the syntax trees and 
this is often helpful for proving semantic correctness and is a useful starting 
point for developing inference algorithms. 
_ 
Exercise 5.14 Consider the rule [handle] in the Exception Analysis of 
Table 5.10. Would the analysis still be semantically correct if we replaced it 
by the following two rules: 
f f-ES e1 : T & c,ol f f-ES e2 : T & c,02 
f f-ES handle s as e1 in e2 : T & c,02 
if sf/. c,02 and AV(c,02) = 0 
r f-ES e1 : T & c,ol f f-ES e2 : T & c,02 
f f-ES handle s as e1 in e2 : T & c,ol U (c,02 \ { s} ) 
if s E c,02 or AV(c,02) # 0 
Here s E c,02 means {s} ~ c,02 and A V (c,02) is the set of annotation variables 
in c,02. 
-
Exercise 5.15 Consider the Exception Analysis of Table 5.10 and change 
the rule [let] to 
f f-Es e1 : 0'1 & c,ol 
f[x 1--+ 0'1] f-ES e2 : 0'2 & c,02 
f f-ES let x = e1 in e2 : 0'2 & c,ol U c,02 

Exercises 
361 
and perform similar changes in [ijJ, [raise], [handle] and [sub]; do not forget to 
define the meaning of T ~ r. Clearly the new system is at least as powerful 
as Table 5.10 but is it more powerful? (Hint: Consider the places where [gen] 
and [ins] can be used in Table 5.10.) 
â¢ 

Chapter 6 
Algorithms 
In previous chapters we have studied several algorithms for obtaining solu-
tions of program analyses. In this chapter we shall explore further the sim-
ilarities between the different approaches to program analysis by studying 
general algorithms for solving equation or inequation systems. 
6.1 
Worklist Algorithms 
We will abstract away from the details of a particular analysis by considering 
equations or inequations in a set of flow variables for which we want to 
solve the system. As an illustration, in Data Flow Analysis there might be 
separate flow variables for the entry and exit values at each program point, 
whilst in Constraint Based Analysis there would be a separate flow variable 
for the cache at each program point and for the environment at each program 
variable. 
Example 6.1 Consider the following WHILE program 
if [bl]l then (while [b2F do [x := al]3) 
else (while [b3]4 do [x : = a2]5) ; 
[x := a3]6 
where we leave the expressions ai and bi unspecified. The equations generated 
by the Reaching Definitions Analysis of Section 2.1.2 take the form: 
RDentry(l) = X? 
RDexit(l) = RDentry(l) 
RD entry (2) = RD exit (1) U RD exit (3) 
RD exit (2) = RD entry (2) 
RDentry(3) = RDexit(2) 
RDexit(3) = (RDentry(3)\X356?) UX3 
RDentry(4) = RDexit(l) U RDexit(5) RDexit(4) = RDentry(4) 
RDentry(5) = RDexit(4) 
RDexit(5) = (RDentry(5)\X356?) U X5 
RDentry(6) = RDexit(2) U RDexit(4) RDexit(6) = (RDentry(6)\X356?) UX6 

364 
6 Algorithms 
Here Xl = {(x, Â£)} and we also allow a string of subscripts on Xj for example, 
X 356? = {(x, 3), (x, 5), (x, 6), (x, ?)}. 
When expressed as a constraint system in the flow variables {Xl"'" Xl2} it 
takes the form 
Xl 
= 
X? 
X7 
= 
Xl 
X2 
= 
X7U X9 
Xs 
= 
X2 
X3 
= 
Xs 
X9 
= 
(X3 \X356?) U X3 
X4 
= 
X7 UXu 
XIO 
= 
X4 
X5 
= 
XlO 
Xu = 
(Xs \X356? ) U X5 
X6 
= 
Xs UXlO 
Xl2 
= 
(X6 \X356?) U X6 
where Xl,'" ,X6 correspond to RDentry(l),"', RDentry(6) and X7,'" ,Xl2 cor-
respond to RDezit(l)," " RDezit(6). 
Since we are generally interested in the solution for RDentry, we shall in this 
and subsequent examples consider the following simplified equation system: 
Xl 
= 
X? 
X2 
Xl U (X3 \X356?) U X3 
X3 
= 
X2 
X4 
= 
Xl U (X5 \X356?) U X5 
X5 
= 
X4 
X6 
= 
X2 UX4 
Clearly nothing is lost by these changes in representation. 
â¢ 
Equations versus inequations. An apparent difference between 
the settings of Chapters 2 and 3 is that we solve equation systems (Xi = ti)~l 
in the former but inequation systems (Xi ;! ti)~l in the latter. However, al-
ready in Section 2.2, we observed that a solution of an equation system is also 
a solution of the inequation system resulting from replacing all occurrences 
of "=" by";!". In fact, the following inequation system (where all left hand 
sides are the same) 
and the equation 
X = X U tl U ... U tn 
have the same solutions: any solution of the former is also a solution of the 
latter and vice versa. Furthermore, the least solution of the above systems is 
also the least solution of 
X = tl uÂ·Â·Â· U tn 
(where the X component has been removed on the right hand side). Given 
these observations, it should be clear that it does not much matter whether 
our algorithms are supposed to solve inequation or equation systems - but see 
Exercise 6.5 for the flexibility of inequation systems in obtaining efficient al-
gorithms. Throughout this chapter we will concentrate on constraint systems 
with multiple inequations for the same left hand side. 

6.1 Worklist Algorithms 
365 
Assumptions. We make the following assumptions: 
â¢ There is a finite constraint system S of the form 
for N 2: 1 where the left hand sides are not necessarily distinct. 
â¢ The set FV( ti) of flow variables contained in a right hand side ti is a 
subset of the finite set X = {Xi 11 ::; i ::; N}. 
â¢ A solution is a total function, 'ljJ : X -+ L, assigning each flow variable 
a value in the complete lattice (L,~) satisfying the Ascending Chain 
Condition. 
â¢ The terms are interpreted with respect to solutions, 'ljJ : X -+ L, and 
we write [t]'ljJ E L to represent the interpretation of t with respect to 
'ljJ. 
â¢ The interpretation [t]'ljJ of a term t is monotone in 'ljJ and its value only 
depends on the values {'ljJ(x) I X E FV(t)} of the solution on the flow 
variables occurring in the term. 
In the interest of generality, we leave the nature of the right hand sides, ti, 
unspecified. 
Example 6.2 The constraints used in this chapter may appear to be sim-
pler than those used in Chapter 3 in that apparently we do not allow con-
ditional constraints. However, conditional constraints can be dealt with by 
allowing the terms to contain conditionals. Consider the following expression 
(from Example 3.20): 
Using the notation above, the constraints generated by the Constraint Based 
O-CFA Analysis are: 
Xl ;2 X6 
Xg ;2 X7 
X5 ;2 if {fn x => xl} ~ X2 then Xl 
X6 ;2 if {fn x => Xl} ~ X2 then X4 
X2 ;2 {fn x => xl} 
X4 ;2 {fn y => y3} 
X5 ;2 if {fn y => y3} ~ X2 then X3 
X7 ;2 if {fn y => y3} ~ X2 then X4 
Here Xl to X5 correspond to C(1) to C(5), X6 corresponds to r(x) and X7 
corresponds to r(y). 
_ 

366 
6 Algorithms 
6.1.1 
The Structure of Worklist Algorithms 
In Chapters 2 and 3, we already presented algorithms for solving the sys-
tems which arise from Intraprocedural Data Flow Analysis and Control Flow 
Analysis. The common feature of those algorithms is that they both use a 
worklist to control the iteration. Some representation of the work to be done 
is stored in the worklistj the iteration selects a task from the worklist and 
removes it - the processing of the task may cause new tasks to be added to 
the worklist. This process is iterated until there is no more work to be done 
- the worklist is empty. 
Operations on worklists. Our starting point in this section is an 
abstract variant of those previous algorithms. It is abstract because it is 
parameterised on the details of the worklist and on the associated operations 
and values: 
â¢ empty is the empty worklistj 
â¢ insertÂ«x;;;! t),W) returns a new worklist that is as W except that a new 
constraint x ;;;! t has been addedj it is normally used as in 
W := insertÂ«(x ;;;! t),W) 
so as to update the worklist W to contain the new constraint x ;;;! tj 
â¢ extract(W) returns a pair whose first component is a constraint x ;;;! t 
in the worklist and whose second component is the smaller worklist 
obtained by removing an occurrence of x ;;;! tj it is normally used as in 
Â«(x;;;! t),W) := extract(W) 
so as to select and remove a constraint from W. 
In its most abstract form the worklist could be viewed as a set of constraints 
with the following operations: 
empty = 0 
function insertÂ«(x ;;;! t),W) 
return W U {x;;;! t} 
function extract(W) 
return Â«(x;;;! t),W\ {x ;;;! t}) for some x ;;;! t in W 
However, it may be more appropriate to regard the worklist as a multiset 
(thereby allowing constraints to occur more than once on the worklist), as 
a list with additional structure, or as a combination of other structuresj the 
structure will be used by the function extract to extract the appropriate 
constraint. Later in this chapter we shall illustrate how a judicious choice 

6.1 Worklist Algorithms 
INPUT: 
A system S of constraints: Xl;;:) h, ... ,XN ;;:) tN 
OUTPUT: 
The least solution: Analysis 
METHOD: 
Step 1: 
Initialisation (of W, Analysis and infl) 
W := empty; 
USING: 
for all X ;;;! t in S do 
W := insert((x ;;:) t),W) 
Analysis[x] := .1; 
infl[x] := 0; 
for all x ;;;! t in S do 
for all Xl in FV(t) do 
infl[x/] := infl[x/] U {x;;;! t}; 
Step 2: 
Iteration (updating Wand Analysis) 
while W =1= empty do 
((x;;:) t),W) := extract(W); 
new := eval(t,Analysis); 
if Analysis[x] i1 new then 
Analysis[x] := Analysis[x] U new; 
for all Xl ;;;! f in infl[x] do 
W := insert((xl ;;;! f),w); 
function eval(t,Analysis) 
return [t](Analysis) 
value empty 
function insert((x ;;;! t),w) 
returnÂ·Â· . 
function extract(W) 
returnÂ·Â· . 
Table 6.1: The Abstract Worklist Algorithm. 
367 
of "structure" will produce worklist algorithms with good practical perfor-
mance. 
Abstract worklist algorithm. The abstract worklist algorithm is 
shown in Table 6.1. Since the solution is a function with finite domain, 
we have represented it as an array, Analysis, as we did in Section 2.4. The 
algorithm uses a worklist, W, and an auxiliary array of sets of constraints, 
infl, that records the constraints whose values are influenced by a given flow 
variable; after step 1 we have for each x in X: 
infl[x] = {(Xl;;;! n in S I x E FV(e)} 

368 
6 Algorithms 
Initially the worklist contains all constraints from S, the influence sets are 
generated and the Analysis array has every flow variable set to..L. During 
the iteration a constraint x ;;J t is selected from the worklist using the extract 
function. If the Analysis array is assigned during the iteration, all constraints 
influenced by the updated variable are added to the worklist using the insert 
function. The algorithm terminates when the worklist equals the value empty. 
Example 6.3 Consider the simplified equation system from Example 6.l. 
After step 1 of the abstract worklist algorithm of Table 6.1, the worklist W 
contains all equations and the influence sets are as follows (where we identify 
the equations with the flow variables on the left hand side): 
Xl 
X2 
X3 
X4 
X5 
X6 
infl 
{X2,X4} 
{X3, Xa} 
{X2} 
{X5,X6} 
{X4} 
0 
Additionally Analysis is set to 0 for all flow variables. We shall continue this 
example later. 
â¢ 
Properties of the algorithm. Despite the abstract presentation of 
the algorithm we can give a proof of its correctness and an upper bound on its 
complexity that will remain true even when more specialised representations 
of the worklist are used. 
Given a system of constraints, S = (Xi ;;J ti)f:ll we define a function 
Fs : (X -+ L) -+ (X -+ L) 
by: 
Fs(t/J)(x) = U{[t]t/J I x ;;J t in S} 
This defines a monotone function over the complete lattice X -+ L and it 
follows from Tarski's Fixed Point Theorem (see Proposition A.lO) that Fs 
has a least fixed point, J.ts, which is the least solution to the constraints S. 
Since L by assumption satisfies the Ascending Chain Condition and since X 
is finite it follows that also X -+ L satisfies the Ascending Chain Condition; 
therefore J.ts is given by 
J.ts = Jfp(Fs) = U F1(.l) 
j?O 
and the chain (FS(.l))n eventually stabilises. 
Lemma 6.4 Given the assumptions, the algorithm of Table 6.1 computes 
the least solution of the given constraint system, S. 
â¢ 

6.1 Worklist Algorithms 
369 
Proof We write Analysisi[X] to represent the value of Analysis[x] after the i-th 
iteration of the loop considered. 
First we prove termination. The loops in step 1 are all for-loopsi thus termination 
of step 1 is trivially proved. The body of the while-loop of step 2 removes an 
element from the worklist, it then either adds at most N elements to the worklist 
(if Analysis[x] is assigned to) or else it leaves the worklist unchanged (if Analysis[x] 
is not assigned to). If Analysis[x] is assigned to it gets a strictly larger value. Since 
L satisfies the Ascending Chain Condition, Analysis[x] (for each of the finitely many 
x E X) can only change a finite number of times. Thus the worklist will eventually 
be exhausted. 
The correctness proof is in three parts: (i) first we show that on each iteration the 
values in Analysis are less than or equal to the corresponding values of p,s, (ii) then 
we show that p,s is less than or equal to Analysis at the termination of step 2, and 
(iii) finally we combine these results. 
Part (i). We show that 
\/x EX: Analysis.[x] [;;; p,s(x) 
is an invariant of the while-loop of step 2: After step 1 the invariant is trivially 
established. We have to show that the while-loop preserves the invariant. For each 
iteration of the while-loop, either there is no assignment or else for some constraint 
x ;;;J t in S, we perform an assignment to Analysis[x] so that: 
Analysisi+l [x] 
Analysisi[X] U eval(t, Analysisi) 
Analysisi[X] U [t](AnalysisJ 
c: 
p,s(x) U [t](p,s) 
c: 
p,s(x) U Fs(p,s)(x) 
p,s(x) 
The third step follows from the induction hypothesis and the monotonicity of [t] 
and the fourth step follows because x ;;;J t is one of the constraints considered in 
the definition of Fs. So each iteration of the while-loop preserves the invariant. 
Part (ii). On termination of the loop, the worklist is empty. By contradiction we 
establish that 
Fs(Analysis) [;;; Analysis 
thereby showing the reductiveness of Fs on Analysis. For the proof by contradiction 
suppose that Analysis[x] ~ Fs (Analysis)(x) for some x E X and further suppose that 
one reason is because the constraint x ;;;J t is not fulfilled. Consider the last time 
that Analysis[y] was assigned to, for any variable y E FV(t). 
If this was in step 1 then, since all constraints are in W at the beginning of step 2, 
for some i ~ 1 it is ensured that 
Analysis.[x] ;;;J Analysisi_l[X] U [t](Analysisi_l) 
where Analysisi_l [y] contains the final value for Yi hence [t](Analysisj ) remains 
stable for j ~ i - 1 showing that this case cannot apply. 

370 
6 Algorithms 
It follows that Analysis[y] was last assigned in step 2. This must have been in the 
context of dealing with a constraint y ;;;! t'. But then, since (x ;;;! t) E infl[y], the 
constraint x ;;;! t was added to the worklist and then we re-established, for some 
later i ~ 1, that 
Analysisi[x] ;;;! Analysisi_l[X] U [t](Analysisi_l) 
As before, [t](Analysisj) remains stable for j ~ i-I showing that this case cannot 
apply either. This completes the proof by contradiction. 
Thus Fs is reductive on Analysis and by Tarski's Fixed Point Theorem (see Propo-
sition A.lO): 
p,s = Ifp(Fs) !;;; Analysis 
Part (iii). That p,s = Analysis on termination of step 2 follows from the combination 
of parts (i) and (ii). 
â¢ 
Assume that the size of the right hand sides of constraints is at most M ~ 1 
and that the evaluation of a right hand side takes O(M) steps; further assume 
that each assignment takes 0(1) step. Each constraint is influenced by at 
most M flow variables and therefore the initialisation of the influence sets 
takes O(N + N . M) steps. Writing N", for the number of constraints in 
infl[x] we note that L:"'EX N", :::; M . N. Assuming that L is of finite height 
at most h ~ 1, the algorithm assigns to Analysis[x] at most h times, adding 
N", constraints to the worklist, for each flow variable x EX. Thus, the total 
number of constraints added to the worklist is bounded from above by: 
N + (hÂ· L N",) :::; N + (hÂ· M . N) 
"'EX 
Since each element on the worklist causes a call to eval, the cost of the calls 
is O(N . M + hÂ· M2 . N). This gives an overall complexity of O(h . M2 . N). 
6.1.2 
Iterating in LIFO and FIFO 
Extraction based on LIFO. The algorithm of Table 6.1 is an ab-
stract algorithm because it does not provide the details of the worklist nor 
of the associated operations; a concrete algorithm is only obtained once this 
information is supplied. We now show that the algorithm abstracts the algo-
rithms that we studied in Tables 2.8 and 3.7 of Chapters 2 and 3, respectively. 
In both of the earlier algorithms the worklist was implemented by a list that 
was used as a stack, i.e. in a LIFO manner (meaning last-in first-out), as 
specified by the operations in Table 6.2. However, the two algorithms differ 
in the way constraints and influence sets are represented. 
Example 6.5 A constraint of the form Analysis[Â£'] ;) h(Analysis[Â£]) is 
represented on the worklist W of Table 2.8 by the pair (Â£, f'); this is possible 

6.1 Worklist Algorithms 
empty = nil 
function insert((x ;! t),W) 
return cons((x ;! t),W) 
function extract(W) 
return (head(W), tail(WÂ» 
Table 6.2: Iterating in last-in first-out order (LIFO). 
371 
since each Â£ uniquely identifies the transfer function h. The influence sets 
were indirectly represented through the flow, F; to be more precise, infl[Â£'] = 
{(Â£', Â£") E F I Â£" E Lab}. 
The number N of constraints in a system generated from Intraprocedural 
Data Flow Analysis is proportional to the number b of elementary blocks. 
Furthermore, it is usual to take M = 1. Thus the upper bound on the 
complexity of the abstract algorithm specialises to O(h . b); for an analysis 
such as Reaching Definitions Analysis where h is proportional to b this gives 
O(b2 ). This agrees with the bound obtained in Example 2.30. 
â¢ 
Example 6.6 In Table 3.7 constraints of the form it} ~ p, Pi ~ P or 
it} ~ P2 :::} Pi ~ P are represented on the worklist W by anyone of the 
flow variables Pi or P2 occurring on the left hand side. The influence sets are 
represented using the edge array, E; to be more precise, infl[p] = E[P] (viewed 
as a set). The initialisation of the influence sets in step 1 of the algorithm 
of Table 6.1 corresponds to step 2 of the algorithm in Table 3.7; also the 
inclusion on the worklist of all the constraints in infl[p] is replaced by the 
for-loop in step 3 of Table 3.7. Finally, note that Analysis[p] is written as 
O[P]. 
The number N of constraints in a system generated from Control Flow Anal-
ysis is O(n2 ) where n is the size of the expression (see the discussion about 
the number of constraints in Section 3.4). Also, h is bounded by n and, once 
again, M = 1. Thus the upper bound on the complexity of the abstract algo-
rithm specialises to O(n3 ). This agrees with the bound obtained in Section 
3.4. 
â¢ 
One disadvantage of the LIFO strategy as presented above, is that we do 
not check for the presence of a constraint when adding it to the worklist. 
Hence the worklist may evolve so as to contain multiple copies of the same 
constraint and this may lead to unnecessarily recalculating terms before their 
free variables have had much chance of getting new values. This is illustrated 
in the following example; obviously a remedy is to modify the LIFO strategy 
such that it never inserts a constraint when it is already present. 

372 
6 Algorithms 
W 
Xl 
X2 
Xs 
'4 
Xl) 
X6 
[Xl, X2, XS, '4, X5, X6] 
0 
0 
0 
0 
0 
0 
[X2, '4, X2, Xs, X4, X5, X6] x? 
-
-
-
-
-
[XS,X6,'4, X2,XS, '4,X5,X6] 
-
X S? 
-
-
-
-
[X2, X6, '4, X2, Xs, '4, X5,X6] 
-
-
X S? 
-
-
-
[X6, '4, X2, Xs, '4, X5, X6] 
-
-
-
-
-
-
['4, X2, Xs, '4, X5, X6] 
-
-
-
-
-
X S? 
[X5,Xs,X2,XS,'4, X5, X6] 
-
-
-
x 5? -
-
['4, X6, X2, XS,X4, Xl), X6] 
-
-
-
-
x 5? -
[X6, X2, Xs, '4, X5, X6] 
-
-
-
-
-
-
[X2, Xg,'4, X5, X6] 
-
-
-
-
-
X S5? 
[XS,X4,X5,Xs] 
-
-
-
-
-
-
[X4,Xl),Xs] 
-
-
-
-
-
-
[X5,Xs] 
-
-
-
-
-
-
[Xs] 
-
-
-
-
-
-
[ ] -
-
-
-
-
-
Figure 6.1: Example: LIFO iteration. 
Example 6.7 Continuing Examples 6.1 and 6.3, the LIFO worklist algo-
rithm obtained from Tables 6.1 and 6.2 operates as shown in Figure 6.1. The 
first column is the worklist where we identify the equations by the variables 
on the left hand side as in Example 6.3. The remaining columns are the 
values of Analysis[xd; "-" means that the value is unchanged and hence is 
equal to that in the previous row. The first row of the table is the result 
of the initialisation of step 1; each of the remaining rows corresponds to one 
iteration through the loop of step 2. The improved strategy, where a con-
straint is never inserted when it is already present, is considered in Exercise 
6.3. 
â¢ 
Extraction based on FIFO. An obvious alternative to the use of a 
LIFO strategy is to use a FIFO strategy (meaning first-in first-out) where the 
list is used as a queue. Again it may be worthwhile not to insert a constraint 
into a worklist when it is already present. However, rather than going deeper 
into the LIFO and FIFO strategies, we shall embark on a treatment of more 
advanced insertion and extraction strategies. 
6.2 
Iterating in Reverse Postorder 
A careful organisation of the worklist may lead to algorithms that perform 
better in practice than simply using the LIFO or FIFO strategies discussed 

6.2 Iterating in Reverse Postorder 
373 
above; however, in general we will not be able to improve our estimation of 
the worst case complexity to reflect this. 
In this section we explore the idea that changes should be propagated through-
out the rest of the program before returning to re-evaluate a constraint. One 
way of ensuring that every other constraint is evaluated before re-evaluating 
the constraint which caused the change is to impose some total order on the 
constraints. To obtain a suitable ordering we shall impose a graph structure 
on the constraints (see below) and then use an iteration order based on re-
verse postorder (see Appendix C). This approach has been very successful 
for Intraprocedural Data Flow Analysis and we shall show how the Round 
Robin Algorithm can be obtained by fully implementing these ideas. 
The graph structure of a constraint system. Given a con-
straint system S = (Xi;;! ti)f::l we can construct a graphical representation 
G s of the dependencies between the constraints in the following way: 
â¢ there is a node for each constraint Xi ;;! ti, and 
â¢ there is a directed edge from the node for Xi ;;! ti to the node for Xj ;;! tj 
if Xi appears in tj (Le. if Xj ;;! tj appears in infl[xd). 
This constructs a directed graph. Sometimes it has a root, Le. a node from 
which every other node is reachable through a directed path (see Appendix 
C). This will generally be the case for constraint systems corresponding to 
forward analyses of WHILE programsj in the case of Example 6.1 the root is 
Xl. It will not in general be the case for constraint systems corresponding 
to backward analyses for WHILE programs nor for constraint systems con-
structed for Constraint Based Analysis. We therefore need a generalisation of 
the concept of root. One obvious remedy is to add a dummy root and enough 
dummy edges from the dummy root to ordinary nodes that the dummy root 
in fact becomes a root. A more elegant formulation, that avoids cluttering 
the graph with dummy nodes and edges, is to consider a handle, Le. a set of 
nodes such that each node in the graph is reachable through a directed path 
starting from one of the nodes in the handle (see Appendix C). Indeed, a 
graph G has a root r if and only if G has {r} as a handle. In the case of Ex-
ample 6.2 a minimal handle is {X2' X4}. One can take the entire set of nodes 
of a graph as a handle but it is more useful to choose a minimal handle: a 
handle such that no proper subset is also a handle; as discussed in Appendix 
C, minimal handles always exist (although they need not be unique). 
We can then construct a depth-first spanning forest from the graph Gs and 
handle Hs using the algorithm of Table C.l. This also produces an array, 
rPostorder, that associates each node (Le. each constraint X ;;! t) with its 
number in a reverse postorder traversal of the spanning forest. To lighten 
the notation, particularly when presenting the Round Robin Algorithm later, 
we sometimes demand that a constraint system (Xi ~ ti)f::l is listed in reverse 

374 
6 Algorithms 
Xl 
X6 
/ j\ Â§] 
(a) 
(b) 
Figure 6.2: (a) Graphical representation. (b) Depth-first spanning tree. 
empty = (nil,0) 
function insertÂ«x ;;:;! t),(W.e,W.pÂ» 
return (W.e,(W.p U {x;;:;! t}Â» 
function extraetÂ«W.e,W.pÂ» 
if W.e = nil then 
W.e := sort_rPostorder(W.p); 
W.p :=0 
return (head(W.e), (tail(W.e),W.pÂ» 
Table 6.3: Iterating in reverse postorder. 
postorder. The advantages of reverse postorder over other orderings, such as 
preorder and breadth-first order, are discussed in Appendix C. 
Example 6.8 Figure 6.2(a) shows the graphical representation of the con-
straints of Example 6.1; again we use the left hand side of the equation as 
the name of the equation. The node for Xl is the root of the graph. Figure 
6.2(b) shows a depth-first spanning tree for the graph; the associated reverse 
postorder is Xl, â¢â¢â¢ ,X6. 
â¢ 
Extraction based on reverse postorder. . Conceptually, we now 
modify step 2 of the worklist algorithm of Table 6.1 so Â·that the iteration 
amounts to an outer iteration that contains an inner iteration that visits the 
nodes in reverse postorder. 
To achieve this, without actually changing Table 6.1, we shall organise the 
worklist W as a pair (W.e,W.p) of two structures. The first component, W.e, 
is a list of current nodes to be visited in the current inner iteration. The 

6.2 Iterating in Reverse Postorder 
375 
w.c 
W.p 
Xl 
X2 
X3 
><4 
X5 
X6 
[ ] 
{Xl'Â·Â·Â· ,X6} 
0 
0 
0 
0 
0 
0 
[X2, X3, X4, Xs, X6] 
{X2, X4} x? 
-
-
-
-
-
[X3, X4, X5, X6] 
{X2,X3,X4,X6} 
-
x 3? -
-
-
-
[X4,X5,X6] 
{X2,X3,X4,X6} 
-
-
x 3? -
-
-
[Xs,X6] 
{X2,Â· â¢â¢ , X6} 
-
-
-
x 5? -
-
[X6] 
{X2,Â·Â·Â· , X6} 
-
-
-
-
x 5? -
[X2, X3, X4, X5, X6] 
0 -
-
-
-
-
X 35? 
[X3, ><4, X5, X6] 
0 -
-
-
-
-
-
[X4,X5,X6] 
0 -
-
-
-
-
-
[X5,X6] 
0 -
-
-
-
-
-
[X6] 
0 -
-
-
-
-
-
[ ] 
0 -
-
-
-
-
-
Figure 6.3: Example: Reverse postorder iteration. 
second component, W.p, is a set of pending nodes to be visited in a later 
inner iteration. Nodes are always inserted into W.p and always extracted 
from W.c; when W.c is exhausted the current inner iteration has finished 
and in preparation for the next we must sort W.p in the reverse postorder 
given by rPostorder and assign the result to W.c. The details are provided by 
the operations of Table 6.3; here (tail(W.c),W.p) denotes the pair whose first 
component is the tail of the list W.c and whose second component is W.p. 
Clearly one could dispense with sorting the set W.p if it was implemented by 
a suitable data structure that allows the insertion and extraction of elements 
in their appropriate order. This is facilitated by a priority queue where 
each element has a priority associated with it; in our case the priority is 
the rPostorder number. One of the better ways of implementing a priority 
queue is as a 2-3 tree and in this representation an element can be inserted 
or extracted in O(log2 N) steps where the priority queue contains at most N 
elements. 
However, the amortised complexity of our scheme is equally good. To see 
this note that clearly a list of N elements can be sorted in O(N Â·log2(N)) 
steps. Further suppose that we use a linked list representation of lists; then 
inserting an element to the front of a list, as well as extracting the head of a 
list, can be done in constant time. Thus the overall complexity for processing 
N insertions and N extractions is O(N Â·log2(N)) in both schemes. 
Actually we can do somewhat better under the special circumstances of our 
algorithm. Recall that there are at most N constraints that are all known 
from the outset. Let us represent W.p as a bit vector of length N and agree 
that it takes constant time to access or modify a component. Then insertion 
into W.p is performed by setting a bit to 1 and sorting of W.p is performed 

376 
6 Algorithms 
by a for-loop running through all N constraints, and recording a constraint 
in the sorted list if and only if the bit is 1. Then we can guarantee that at 
most N insertions and N extractions can be done in O(N) steps; hence the 
complexity estimation of Section 6.1.1 still holds. 
Also the overall correctness of the algorithm carries over from Lemma 6.4 
because we still have an abstract worklist; simply take W = W.c U W.p. 
Example 6.9 Returning to Examples 6.1 and 6.3, we shall consider the 
reverse postorder Xl,Â·Â·Â· ,x6 of Example 6.8. Using the algorithm of Table 
6.3, we get the iterations shown in Figure 6.3. Note that we perform fewer 
iterations than when using the LIFO strategy (see Example 6.7). Improved 
versions of Table 6.3 are considered in Exercises 6.6 and 6.7. 
â¢ 
6.2.1 
The Round Robin Algorithm 
Now suppose that we change the above algorithm such that each time W.c is 
exhausted we assign it the list [1,Â·Â·Â·, N] rather than the potentially shorter 
list obtained by sorting W.p. Clearly this may lead to more evaluations of 
right hand sides of constraints but it simplifies some of the book-keeping 
details. Indeed, now our only interest in W.p is whether or not it is empty; 
let us introduce a boolean, change, that is false whenever W.p is empty. Also 
let us split the iterations into an overall outer iteration having an explicit 
inner iteration; each inner iteration will then be a simple iteration through 
all constraints in reverse postorder. 
We thus arrive at the Round Robin Algorithm shown in Table 6.4. The 
algorithm starts by initialising Analysis and change (step 1). The outer itera-
tion (step 2) is a while-loop that begins by modifying change (corresponding 
to what would have happened in extract); it then has an inner iteration (a 
for-loop) that updates Analysis by recomputing the right hand sides of con-
straints. The algorithm continues iterating as long as any part of the solution 
changes (indicated by change being set to true). 
Example 6.10 Using the reverse postorder X!,Â·Â·Â· ,X6 of Example 6.8 to 
solve the equations of Example 6.1, the Round Robin Algorithm of Table 6.4 
operates as shown in Figure 6.4. The lines marked * record the assignment 
of false to change at the beginning of the body of the while-loop. 
â¢ 
Theoretical properties. We first study the correctness of the Round 
Robin Algorithm and then establish a striking bound on its complexity in the 
special case of Bit Vector Frameworks (as studied in Exercise 2.9 of Chapter 
2). 
Lemma 6.11 Given the assumptions, the algorithm of Table 6.4 com-
putes the least solution of the given constraint system, S. 
â¢ 

6.2 Iterating in Reverse Postorder 
INPUT: 
A system S of constraints: Xl;;;! tl,'" ,XN ;;;! tN 
ordered 1 to N in reverse postorder 
OUTPUT: 
The least solution: Analysis 
METHOD: 
Step 1: 
Initialisation 
USING: 
for all X E X do 
Analysis[x] := ..L 
change := true; 
Step 2: 
Iteration (updating Analysis) 
while change do 
change := false; 
for i := 1 to N do 
new := eval(ti,Analysis); 
if Analysis[xi] ~ new then 
change := true; 
Analysis[xi] := Analysis[xi] U new; 
function eval(t,Analysis) 
return [t](Analysis) 
Table 6.4: The Round Robin Algorithm. 
change 
Xl 
X2 
X3 
X4 
Xs 
X6 
true 
0 
0 
0 
0 
0 
0 
* false 
true X? 
-
-
-
-
-
true 
-
X 3? 
-
-
-
-
true 
-
-
X 3? 
-
-
-
true 
-
-
-
XS? 
-
-
true 
-
-
-
-
XS? 
-
true 
-
-
-
-
-
X3S? 
* false 
false 
-
-
-
-
-
-
false 
-
-
-
-
-
-
false 
-
-
-
-
-
-
false 
-
-
-
-
-
-
false 
-
-
-
-
-
-
false 
-
-
-
-
-
-
Figure 6.4: Example: Round Robin iteration. 
377 

378 
6 Algorithms 
Proof Since the algorithm of Table 6.4 is obtained in a systematic manner from 
that of Table 6.1, the correctness proof can be obtained in a systematic manner 
from that of Lemma 6.4; we leave the details to Exercise 6.9. 
_ 
We shall say that the constraint system (Xi ;;;;J ti)f::l is an instance of a Bit 
Vector Framework when L = P(D) for some finite set D and when each right 
hand side ti is of the form (xj;nYl )UY? for sets Yik ~ D and variablexji EX. 
Clearly the classical Data Flow Analyses of Section 2.1 produce constraint 
systems of this form (possibly after expansion of a composite constraint Xi ;;;;J 
tt u ... U t;; into the individual constraints Xi ;;;;J tt, .. " Xi ;;;;J t:i). 
Consider a depth-first spanning forest T and a reverse postorder rPostorder 
constructed for the graph G s with handle Hs. We know from Appendix C 
that the loop connectedness parameter d( G s, T) ~ 0 is defined as the largest 
number of so-called back edges found on any cycle-free path of G s. We also 
know that the back edges are exactly those edges that are not topologically 
sorted by rPostorder, i.e. for which the target of the edge does not have an 
rPostorder number that is strictly larger than that of the source. 
Let us say that the algorithm of Table 6.4 has iterated n ~ 1 times if the 
loop of step 1 has been executed once and the while-loop of step 2 has been 
executed n - 1 times. We then have the following result: 
Lemma 6.12 Under the assumptions stated above, the algorithm of Table 
6.4 halts after at most d( G s, T) + 3 iterations. It therefore performs at most 
O((d(Gs, T) + 1) . N) assignments. 
_ 
Proof If a path contains d back edges, the while-loop of step 2 takes at most d + 1 
iterations to propagate a change throughout the path. To be specific, it takes one 
iteration for the value to arrive at the source of the first back edge; this follows 
since up to this point the nodes in the path are numbered in increasing sequence. 
After that, it takes one iteration of the while-loop for the value to reach the source 
of the next back edge, and so on. So the algorithm needs at most d + 1 iterations 
to propagate the information. 
One more iteration of the while-loop suffices for detecting that there are no further 
changes. We also have to count one for the iteration of the for-loop in step 1. This 
gives an upper bound of d + 3 ~ d(Gs, T) + 3 on the number of iterations. 
Clearly each iteration can perform at most O(N) assignments or evaluations of right 
hand sides. This gives an upper bound of OÂ«d(Gs, T) + 1)Â· N) on the number of 
assignments. 
_ 
For WHILE programs we know from Appendix C that the loop connectedness 
parameter is independent of the choice of depth first spanning forest, and 
hence of the choice of the reverse postorder recorded in rPostorder, and that 
it equals the maximal nesting depth d of while-loops. It follows that Lemma 
6.12 gives an overall complexity of O((d + 1) . b) where b is the number of 
elementary blocks. We would normally expect this bound to be significantly 
smaller than the O(b2 ) obtained in Examples 2.8 and 6.5. 

6.3 Iterating Through Strong Components 
379 
It is worth mentioning that an empirical study of Fortran programs once 
reported that the loop connectedness parameter seldom exceeds 3; this gave 
rise to the Folk Theorem that the Round Robin Algorithm usually has linear 
time complexity. 
Example 6.13 The WHILE program of Example 6.1 has a loop connect-
edness parameter of 1. According to Lemma 6.12 the Round Robin Algorithm 
will perform at most 4 iterations (1 for step 1 and 3 for step 2 of Table 6.4). 
However, Example 6.10 managed to succeed in only 3 iterations (1 for step 
1 and 2 for step 2). 
â¢ 
6.3 Iterating Through Strong Components 
As was said above, a careful organisation of the worklist is a key factor in 
obtaining algorithms having a good practical performance. Iterating through 
the entire system of constraints in reverse postorder (as in Section 6.2) is a 
first step in this direction. A further step that often pays off in practice 
is to identify the so-called strong components in the system of constraints 
and to process them one by one; the processing of each strong component 
means iterating through the constraints of that strong component in reverse 
postorder (in the manner of Section 6.2). 
Strong Components. Once again, we exploit the graph structure on 
constraints introduced in Section 6.2. Recall that a graph is strongly con-
nected if every node is reachable from every other node (see Appendix C). 
The strong components of a graph are its maximal strongly connected sub-
graphs. The strong components partition the nodes in the graph (Fact C.3). 
The interconnections between components can be represented by a reduced 
graph: each strong component is represented by a node in the reduced graph 
and there is an edge from one strong component to another if there is an edge 
in the original graph from some node in the first strong component to a node 
in the second strong component and provided that the two strong compo-
nents are not the same. The reduced graph is always a DAG, i.e. a directed, 
acyclic graph (Lemma C.5). As a consequence, the strong components can 
be linearly ordered in topological order: SCl $ SC2 (where SCl and SC2 
are nodes in the reduced graph) whenever there is an edge from SCl to SC2 ; 
such a topological order can be obtained by constructing a reverse postorder 
for the reduced graph (as follows from Corollary C.lI). 
Example 6.14 Consider once again the equation system of Example 6.1. 
Its strong components and the reduced graph are shown in Figure 6.5. There 
are two possible topological orderings of the strong components. One is 
{xd, {X2,X3}, {X4,Xs}, {xt;} and the other is {xt}, {X4,X5}, {X2,X3}, {xt;}. 

380 
6 Algorithms 
~ 
(a) 
(b) 
Figure 6.5: (a) Strong components. (b) Reduced graph. 
INPUT: 
OUTPUT: 
METHOD: 
A graph partitioned into strong components 
srPostorder 
scc:= 1; 
for each sec in topological order do 
rp:= 1; 
for each x;) t in the strong component sec 
in local reverse postorder do 
srPostorder[x;) t] := (scc,rp); 
rp := rp + 1 
sec := sec + 1; 
Table 6.5: Pseudocode for constraint numbering. 
In this example each strong component was an outermost loop. This holds 
in general for both forward and backward flow graphs for programs in the 
WHILE language. 
_ 
For each constraint we need to record both the strong component it oc-
curs in and its number in the local reverse postorder for that strong com-
ponent. We shall do so by means of a numbering srPostorder that to each 
constraint x ;) t assigns a pair (sec, rp) consisting of the number sec of the 
strong component and the number rp of its reverse postorder numbering in-
side that strong component. When srPostorder[x ;) t] = (sec, rp) we shall 
write fst(srPostorder[x ;) t]) for sec, and snd(srPostorder[x ;) t]) for rp. One 
way to obtain srPostorder is using the algorithm of Table 6.5. 

6.3 Iterating Through Strong Components 
381 
The Algorithm. The basic method of the new algorithm is that strong 
components are visited in topological order with nodes being visited in reverse 
postorder within each strong component. 
Conceptually, we now modify step 2 of the worklist algorithm of Table 6.1. 
so that the iteration amounts to three levels of iteration; the outermost level 
deals with the strong components one by one; the intermediate level performs 
a number of passes over the constraints in the current strong component; and 
the inner level performs one pass in reverse postorder over the appropriate 
constraints. 
To achieve this, without actually changing Table 6.1, we shall again organise 
the worklist W as a pair (W.c,W.p) of two structures. The first component, 
W.c, is a list of current nodes to be visited in the current inner iteration. 
The second component, W.p, is a set of pending nodes to be visited in a 
later intermediate or outer iteration. Nodes are always inserted into W.p 
and always extracted from W.c; when W.c is exhausted the current inner 
iteration has finished and in preparation for the next we must extract a 
strong component from W.p, sort it and assign the result to W.c. The details 
are provided by the operations of Table 6.6. 
Intuitively, an inner iteration ends when W.c is exhausted, an intermediate 
iteration ends when scc gets a higher value than last time it was computed, 
and the outer iteration ends when both W.C and W.p are exhausted. 
Again the lists can be organised as priority queues. This time the priority of 
x ~ t is the srPostorder information about the strong component, sec, and the 
local reverse postorder number, rp; rather than directly using pairs, (sec, rp), 
it may be helpful to use a linearised representation with numbers such that 
the lexicographic ordering on the pairs corresponds to the arithmetic ordering 
on the numbers. 
Example 6.15 Consider the ordering {Xl}, {X2,X3}, {X4' X5}, {X6} of the 
strong components of Example 6.14. The algorithm iterating through strong 
components (Table 6.6) produces the walkthrough of Figure 6.6 when solving 
the system. Note that even on this small example the algorithm performs 
slightly better than the others (ignoring the cost of maintaining the worklist). 
Improved versions of Table 6.6 are considered in Exercises 6.11 and 6.12. â¢ 
It is also possible to split the set W.p of pending constraints into two col-
lections: W.pc for those that relate to the current strong component and 
W.pf for those that relate to future strong components. (Since the reduced 
graph is a DAG there cannot be any constraints relating to earlier strong 
components.) We leave these details to Exercise 6.12. 
Once more the overall correctness of the algorithm carries over from Lemma 
6.4; also the estimation of the complexity carries over from Section 6.1.1. 

382 
empty = (nil,0) 
function insertÂ«x ~ t),(W.c,W.pÂ» 
return (W.c,(W.p U {x ~ t}) 
function extractÂ«W.c,W.pÂ» 
local variables: scc, W...scc 
if W.c = nil then 
6 Algorithms 
sce := min{fst(srPostorder[x ~ t]) I (x ~ t) E W.p}; 
W...scc := {(x ~ t) E W.p I fst(srPostorder[x ~ t]) = scc}; 
W.e := sort...srPostorder(W...scc); 
W.p := W.p \ W...sec; 
return (head(W.c), (tail(W.c),W.pÂ» 
Table 6.6: Iterating through strong components. 
W.e 
W.p 
Xl 
X2 
X3 
X4 
X5 
X6 
[ ] {Xl,Â·Â·Â· ,X6} 
0 
0 
0 
0 
0 
0 
[ ] {X2,Â· .. , X6} 
x? 
-
-
-
-
-
[X3] 
{X3,Â·Â·Â·,X6} 
-
X 3? 
-
-
-
-
[ ] {X2,Â·Â·Â· , X6} 
-
-
X 3? 
-
-
-
[X3] 
{X4,X5,X6} 
-
-
-
-
-
-
[ ] 
{X4,X5,X6} 
-
-
-
-
-
-
[X5] 
{X5,X6} 
-
-
-
X 5? 
-
-
[ ] 
{X4,X5,X6} 
-
-
-
-
X 5? 
-
[X5] 
{X6} 
-
-
-
-
-
-
[ ] 
{X6} 
-
-
-
-
-
-
[ ] 
0 -
-
-
-
-
X35? 
Figure 6.6: Example: Strong component iteration. 
Concluding Remarks 
Iterative solvers. Worklist algorithms have a long history; an early 
presentation of a general worklist algorithm for Data Flow Analysis may be 
found in [96]. A number of special purpose iterative algorithms are pre-
sented in [69]; this covers versions for forward Data Flow Analysis problems 
(based on reverse postorder traversal of the depth-first spanning forest) and 
backward Data Flow Analysis problems (based on postorder traversal of the 
depth-first spanning forest); it also covers versions in so-called "integrated 
form" (roughly meaning that constraints are placed on the worklist) and 
"segregated form" (roughly meaning that flow variables are placed on the 

Concluding Remarks 
383 
worklist); it also deals with the Round Robin Algorithm. A theoretical study 
of Round Robin Algorithms may be found in [92]. The empirical study of 
the loop connectedness parameter for Fortran programs, giving rise to the 
Folk Theorem that the Round Robin Algorithm operates in linear time in 
practice, is contained in [98]. 
The use of strong components to speed up iterative algorithms has been 
considered by a number of authors (e.g. [77,87]). The treatment in Section 
6.3 and Exercise 6.12 is based on [77] where the worklists W.e, W.pe, and 
W.pf are represented by priority queues eurrentQ, pendingQ, and futureQ, 
respectively. We refer to [77] for examples for which iteration over strong 
components performs better than either the basic worklist algorithm or the 
Round Robin Algorithm. There are many ways to implement priority queues 
as balanced search trees; the use of 2-3 trees is explained in [4]. 
Local solvers. Suppose that the set of constraints is very large but that 
we are only interested in the values of a small subset of the flow variables. 
In this situation the classical worklist solvers may compute information in 
which we have no interest; hence it may be better to perform a local fixed 
point computation where one finds a partial solution for the flow variables of 
interest (and of course all flow variables which directly or indirectly influence 
them). For best performance the local fixed point solvers perform a dynamic 
determination of what is essentially the strong components of the constraints 
of interest. 
One such algorithm is the top down solver [29]; this is a general fixed point 
algorithm that was originally developed for program analysis tools for logic 
programming languages. The algorithm proceeds top down; as it meets a 
new flow variable, it attempts to produce a solution for that flow variable. 
Its practical performance is very good and is competitive with the more 
sophisticated versions of the worklist algorithm. 
Another algorithm is the worklist based local fixed point solver [51, 54] that 
uses time stamps to impose an ordering on the elements of the worklist. The 
worklist is now a maximum priority queue - the priority of an entry being 
given by its time stamp. Elements are selected from the worklist in an order 
which gives a run-time approximation to strong components (as with the top 
down solver). 
A further refinement is the differential worklist based local fixed point solver 
[52, 53]. The differential worklist algorithm aims to minimise the amount 
of recomputation that occurs when a value changes; it does this by only 
recomputing the actual sub-terms influenced by the changed value and by 
only computing the incremental change due to the difference between the old 
and new values. This algorithm compares well in practice with a number of 
special purpose algorithms reported in the literature. (See Exercise 6.5 for a 
key insight.) 

384 
6 Algorithms 
Efficient algorithms for restricted forms of systems. Elim-
ination methods have been proposed as an alternative to the iterative ap-
proaches for solving systems. Most of this development has concentrated on 
the kind of systems generated for Data Flow Analysis. Ryder and Paull [146] 
provide a survey of different elimination methods. 
A very successful elimination method is structural analysis [154]. The first 
phase of structural analysis performs a postorder search of the graph rep-
resenting the constraint system in order to identify control structures; here 
postorder is with respect to a depth-first spanning forest for the graph and as 
a consequence, deeper nested control structures are identified earlier. Having 
identified a control structure, its nodes are reduced to a single node to pro-
duce a new, derived graph. This process terminates when there is only one 
node. The result of the first phase is a control tree which records the reduc-
tion sequence; this can be viewed as a technique for translating machine code 
into high-level programs (where the primitives are those control structures 
considered in structural analysis). The second phase of structural analysis 
then traverses the control tree in order to solve the constraint system; this 
takes the form of a bottom-up traversal followed by a top-down traversal. 
The bottom-up traversal associates a transfer function with each node in the 
control tree; the transfer function describes the effect of analysing that part 
of the program. The top-down traversal evaluates the transfer functions upon 
appropriate arguments. The second phase of structural analysis essentially 
amounts to high-level Data Flow Analysis [144] which again has some of the 
flavour of Section 1.6. 
An early elimination algorithm for Data Flow Analysis was based on interval 
analysis [11]. An interval can briefly be described as a single entry cycle 
together with an acyclic extension. The algorithm performs in a similar way 
to structural analysis. The first phase constructs a derived sequence of graphs 
that is usually coarser than the control tree produced by structural analysis. 
The second phase consists of a backward traversal of the derived sequence 
followed by a forward traversal. 
We conclude by mentioning another elimination method based on path al-
gebras [170, 171]. For intraprocedural analyses, path expressions amount to 
regular expressions, and some very fast algorithms exist for solving analyses 
over such structures. 
Practical systems. A number of generic tools for implementing pro-
gram analyses have been constructed. The analyses are specified using a 
specification language; in some systems this is a special purpose language 
(e.g. a functional language) whereas in others it is the implementation lan-
guage ofthe tool (e.g. C++). The tools then provide one or more algorithms 
for solving equations or constraints; the details of these algorithms depend 
on the scope of the tools; often the tools are restricted to certain classes of 
languages (e.g. they might not support languages with dynamic dispatch) or 

Mini Projects 
385 
certain classes of analyses (e.g. they may not support interprocedural anal-
yses). Internally, some of the systems work on abstract syntax trees, others 
work on flow graphs and yet others work directly on the equations or con-
straints. 
The specification languages of tools like Spare [177], System Z [185] and PAG 
[104] are based on eager functional languages and they support high-level 
definitions of complete lattices and the associated analysis functions; Spare 
and System Z are inspired by a denotational approach to program analysis 
(see e.g. [117]) whereas PAG is closer to the traditional Data Flow Analysis 
approach. The above tools build on the ideas of Abstract Interpretation 
by allowing the user to control the complexity of the analyses by specifying 
operations related to widening operators (see Section 4.2 and Exercise 6.13). 
Other program analysis tools are based on other specification mechanisms 
like graph rewriting [17] or modal logic [97]. 
To improve the efficiency of the generated analysers, some of the tools can 
take advantage of special properties of the analyses. An example is PAG 
[104] which supports several implementations of sets including bit vectors, 
AVL trees (see [4]) and BDD's [26] (binary decision diagrams). Another ex-
ample is Sharlit [172] which provides an equation solver based on elimination 
techniques for Bit Vector Frameworks. 
BANE [8] is a general constraint solver for a very general class of set con-
straints [7] (see the Concluding Remarks to Chapter 3). The constraints 
are sufficiently general that the system has been used successfully for imple-
menting both Constraint Based Analyses and Type Systems. It is written in 
Standard ML but also admits a simple textual interface in which a system of 
constraints can be specified. 
Mini Projects 
Mini Project 6.1 Comparison of Algorithms 
Recall that a Bit Vector Framework (see Section 6.2 and Exercise 2.9) is a 
special case of a Monotone Framework with 
â¢ L = (P(D),~) where D is a finite set and ~ is either ~ or 2, and 
â¢ :F = {J : P(D) ~ P(D) I 3Y], Yl ~ D : W ~ D : 
f(Y) = (Y n Y]) U YJ} 
Implement a system that will accept a description of a Bit Vector Framework 
and a description of a constraint system that is an instance of a Bit Vector 
Framework and that will produce the least solution. It should be possible to 

386 
6 Algorithms 
direct the system to use (i) the Round Robin Algorithm (see Table 6.4), or 
the worklist algorithm of Table 6.1 based on (ii) last-in first-out (see Table 
6.2 and Exercise 6.3), (iii) reverse postorder (see Table 6.3 and Exercise 6.6) 
and (iv) strong components with local reverse postorders (see Table 6.6 and 
Exercise 6.11). 
Design suitable experiments to allow an empirical comparison of the worst-
case and average-case performance of the four algorithms. 
For the more ambitious: Generalise your system to accept descriptions of 
Monotone Frameworks and more general instances of constraint systems. 
Apply the system to the equations and constraints generated in Chapters 
2 and 3. 
_ 
Mini Project 6.2 Algorithms for Conditional Constraints 
Consider the following simplified form of set constraints (as discussed in the 
Concluding Remarks to Chapter 3) where the terms on the right hand sides 
of the constraints are given by: 
t 
::= x I .1 I c I tl U t2 I tl n t2 I 
if tl "I- .1 then t2 I if c !; tl then t2 
Here x stands for a flow variable, c for a primitive constant (like {fn x => xl} 
of Example 6.2), and if ... thenÂ·Â·Â· denotes a conditional constraint as con-
sidered in Example 6.2. 
1. Verify that the assumptions ofthis chapter still hold; in particular, that 
the evaluation of terms is still monotone. 
2. Observe that once a condition becomes true, it remains so. Modify the 
LIFO worklist algorithm (see Tables 6.1 and 6.2) such that it simplifies 
constraints as conditions become true. 
3. State and prove a correctness result for the new algorithm. What can 
you say about its complexity? 
4. Modify the new algorithm to implement the optimisation suggested by 
Exercise 6.3. 
5. Investigate the application of these ideas to the material of Chapter 3. 
For the more ambitious: Explore the extent to which similar optimisations 
are possible for the other worklist algorithms of this chapter. Give particular 
attention to the maintenance of the ordering on the constraints; what are the 
implications for the complexity of the modified algorithms? 
_ 

Exercises 
387 
Exercises 
Exercise 6.1 In Example 6.7 the result of step 1 produced the worklist 
W = [Xl.' Â·.X6]. Redo the example assuming that step 1 instead produced 
the worklist W = [X6.Â· Â·.xd. Compare the number of iterations. 
_ 
Exercise 6.2 Consider the program Â«fn x => x) (fn y => y)) from 
Example 6.2. Give a walkthrough of the worklist algorithm of Table 6.1 to 
solve these constraints using the LIFO extraction strategy of Table 6.2. 
_ 
Exercise 6.3 Modify the LIFO extraction strategy of Table 6.2 so that a 
constraint is never inserted into the worklist W when it is already present. 
Then redo Example 6.7 using the modified strategy and compare the number 
of iterations. 
_ 
Exercise 6.4 In Appendix C we explained that the set Dom(n) of domi-
nators of a node n in a graph (N, A) with handle H can be described as the 
greatest solution of the following equation system: 
Dom(n) _ { in} 
-
{n} U n(n' ,n)EA Dom(n') 
ifn E H 
otherwise 
(This can be regarded as an intersection based analysis in the sense of Section 
2.1.) Develop a worklist algorithm for computing dominator sets. 
_ 
Exercise 6.5 Consider a constraint system 
(Xi ;:;;J t} U ... U t{'1)f::1 
where each t{ has size 0(1) and can be evaluated in 0(1) steps. Show that 
the worklist algorithm can solve this system in O(h . M2 . N) steps. Next 
show that the constraint system is equivalent to the constraint system 
in the sense that they have the same solutions. Finally show that the worklist 
algorithm can solve this system in O(hÂ· M . N) steps. 
_ 
Exercise 6.6 Modify the reverse postorder extraction strategy of Table 
6.3 so that a constraint is never inserted into W.p when it is already present 
in either W.p or W.e. Then redo Example 6.9 using the modified strategy 
and compare the number of iterations. 
_ 

388 
6 Algorithms 
Exercise 6.7 In Section 6.2 we discussed using two priority queues for 
the worklist algorithm based on reverse postorder: W.e for the constraints 
to be considered in the current "inner" iteration and W.p for those to be 
considered in the next "inner" iteration. 
Develop a concrete algorithm based on these ideas so that as few operations 
as possible need to be performed. Explore the idea of adding a constraint 
x' ;;;;! t' to W.e or W.p depending on how rPostorder[x' ;;;;! t'] compares to 
rPostorder[x ;;;;! t] where x ;;;;! t is the constraint just considered in step 2 of 
Table 6.1. The objective should be to keep W.e as large as possible without 
destroying the iteration order. Then redo Example 6.9 and compare the 
number of iterations with the result from Exercise 6.6. 
_ 
Exercise 6.8 Consider the equations used to introduce Available Expres-
sions Analysis in Example 2.5 of Chapter 2; when expressed as a constraint 
system in the flow variables {x I, .â¢â¢ , X 1O} it takes the form 
Xl = 0 
X6 
xIU{a+b} 
X2 = X6 
X7 = x2 U {a*b} 
X3 = X7 nXlO 
Xs = x3 U {a+b} 
X4 = Xs 
X9 = X4\{a+b, a*b, a+l} 
X5 = X9 
XlO = X5 U {a+b} 
where Xl,Â·Â· . ,X5 correspond to AEentry(l), ... ,AEentry(5) and X6, ... , XlO cor-
respond to AEe",it(l),Â·Â· ., AEe"'it(5). 
Draw the graph of this system as described in Section 6.2. Use the depth-first 
search algorithm of Appendix C to assign a reverse postorder numbering to 
the nodes in the graph. Give a walkthrough of the Round Robin Algorithm 
to produce a solution to the system. 
_ 
Exercise 6.9 Complete the details of the proof of Lemma 6.11 concerning 
the correctness of the Round Robin Algorithm. 
_ 
Exercise 6.10 Draw the graph of the constraint system in Exercise 6.8. 
Identify the strong components of the graph (see Appendix C) and write 
down a srPostorder numbering for the nodes in the graph. Use the algorithm 
of Section 6.3 to solve the system. 
_ 
Exercise 6.11 Modify the strong component extraction strategy of Table 
6.6 so that a constraint is never inserted into W.p when it is already present 
in either W.p or W.e. Then redo Example 6.15 using the modified strategy 
and compare the number of iterations. 
_ 

Exercises 
389 
Exercise 6.12 In Section 6.3 we discussed using two priority queues for 
the worklist algorithm based on strong components: W.e for the constraints to 
be considered in the current "inner" iteration, W.pe for those to be considered 
in the next "inner" iteration, and W.pf for those to be considered in future 
"outer" iterations. 
Develop a concrete algorithm based on these ideas so that as few operations 
as possible need to be performed. Explore the idea of adding a constraint 
x' ;;;;! t' to W.e, W.pe or W.pf depending on how srPostorder[x' ;;;;! t'l compares 
to srPostorder[x ;;;;! tl where x ;;;;! t is the constraint just considered in step 
2 of Table 6.1. The objective should be to keep W.e and W.pe as large as 
possible without destroying the iteration order. Then redo Example 6.15 and 
compare the number of iterations with the result from Exercise 6.11. 
_ 
Exercise 6.13 Modify the worklist algorithm of Table 6.1 to use a widen-
ing operator (see Section 4.2) over L. Does the widening operator over L give 
rise to a widening operator over X -t L? Prove that the resulting worklist 
algorithm computes an upper approximation to the least solution of the con-
straint system. Also prove that it always terminates when L is a complete 
lattice (even when it does not satisfy the Ascending Chain Condition). 
_ 
Exercise 6.14 A monotone function f is fast whenever f 0 f ~ f u id and 
a Monotone Framework (L, F) is fast whenever all functions in F are fast. 
Notice that if a monotone function is idempotent then it is also fast; use this 
to show that all Bit Vector Frameworks (see Exercise 2.9) are fast. 
It is frequently possible to approximate a function by a fast function. Let f 
be a monotone function and define the following sequence (for n 2': 0): 
f(n) = (f U id)n 
Assuming that there exists a number if such that f{it) = f{it H ) we define 
the fastness closure of f to be: 
f = f{it) 
Show that f(it) = f(j) for j 2': if and conclude that the the fastness closure 
is well-defined. Next prove that f is idempotent and hence fast. Finally 
show that f;;;;! f U id and that they are equal when f is fast and distributive 
(Le. additive). 
Suppose that f is distributive (and hence monotone) and consider the func-
tional F defined by 
F(g)=idUgof 
as might arise for a simple while-loop where the body is described by f. Prove 
that 
n 
FnH(1-) = (f U id)n = U fi 
j=O 

390 
6 Algorithms 
holds for n ~ O. Conclude that if! is the fastness closure of f then Jfp( F) := f. 
This shows that for fast Distributive Frameworks (including all Bit Vector 
Frameworks) there is a simple non-iterative way of solving data flow equations 
for programs with a simple loop structure. 
_ 

Appendix A 
Partially Ordered Sets 
Partially ordered sets and complete lattices play a crucial role in program 
analysis and in this appendix we shall summarise some of their properties. 
We review the basic approaches for how to construct complete lattices from 
other complete lattices and state the central properties of partially ordered 
sets satisfying the Ascending Chain and Descending Chain Conditions. We 
then review the classical results about least and greatest fixed points. 
A.1 
Basic Definitions 
Partially ordered set. A partial ordering is a relation ~: L x L -t 
{ true,false} that is reflexive (i.e. VI : I ~ 1), transitive (Le. Vit, 12,13 : h ~ 
12 /\ 12 ~ 13 =? it ~ la), and anti-symmetric (i.e. Vit,12 : it ~ h 1\ h ~ it =? 
it = 12)' A partially ordered set (L,~) is a set L equipped with a partial 
ordering ~ (sometimes written ~L)' We shall write h ;::;) it for it ~ 12 and 
h C 12 for h ~ 12 /\ h =f. 12, 
A subset Y of L has 1 E L as an upper bound if Vl' E Y : l' ~ 1 and as 
a lower bound if VI' E Y : l' ;::;) I. A least upper bound I of Y is an upper 
bound of Y that satisfies 1 ~ 10 whenever 10 is another upper bound of Y j 
similarly, a greatest lower bound 1 of Y is a lower bound of Y that satisfies 
10 ~ 1 whenever 10 is another lower bound of Y. Note that subsets Y of a 
partially ordered set L need not have least upper bounds nor greatest lower 
bounds but when they exist they are unique (since ~ is anti-symmetric) and 
they are denoted U Y and nY, respectively. Sometimes U is called the join 
operator and n the meet operator and we shall write it U 12 for U{it, h} and 
similarly 11 n 12 for n{h, 12}. 
Complete lattice. A complete lattice L = (L,~) = (L,~, u,n, 1.., T) 
is a partially ordered set (L, ~) such that all subsets have least upper bounds 

392 
A Partially Ordered Sets 
{2,3} 
{3} 
{3} 
{2,3} 
(a) 
Figure A.I: Two complete lattices. 
as well as greatest lower bounds. Furthermore, .1 = U 0 = n L is the least 
element and T = n 0 = U L is the greatest element. 
Example A.1 If L = (P(S),~) for some set S then I; is ~ and U Y = 
UY, ny = ny, .1 = 0 and T = S. If L = (P(S),2) then I; is 2 and 
UY = nY, ny = UY,.l = Sand T = 0. 
Hence (P(S), ~) as well as (P(S), 2) are complete lattices. In the case where 
S = {I, 2, 3} the two complete lattices are shown on Figure A.I; these draw-
ings are often called Hasse diagrams. Here a line "going upwards" from some 
h to some l2 means that h I; l2; we do not draw lines that follow from 
reflexivity or transitivity of the partial ordering. 
_ 
Lemma A.2 For a partially ordered set L = (L,I;) the claims 
(i) L is a complete lattice, 
(ii) every subset of L has a least upper bound, and 
(iii) every subset of L has a greatest lower bound 
are equivalent. 
-
Proof Clearly (i) implies (ii) and (iii). To show that (ii) implies (i) let Y ~ Land 
define 
(A.1) 
and let us prove that this indeed defines a greatest lower bound. All the elements 
of the set on the right hand side of (A.1) are lower bounds of Y so clearly (A.1) 
defines a lower bound of Y. Since any lower bound of Y will be in the set it follows 
that (A.1) defines the greatest lower bound of Y. Thus (i) holds. 

A.I Basic Definitions 
393 
To show that (iii) implies (i) we define U Y = n {l ELI Vl' E Y : l' i;;; l}. 
Arguments analogous to those above show that this defines a least upper bound 
and that (i) holds. 
â¢ 
Moore family. A Moore family is a subset Y of a complete lattice 
L = (L,~) that is closed under greatest lower bounds: VY' ~ Y : ny' E Y. 
It follows that a Moore family always contains a least element, nY, and 
a greatest element, n0, which equals the greatest element, T, from L; in 
particular, a Moore family is never empty. 
Example A.3 Consider the complete lattice (P(S),~) of Figure A.l (a). 
The subsets 
{{2}, {I, 2}, {2, 3}, {I, 2, 3}} and {0, {I, 2, 3}} 
are both Moore families, whereas neither of 
{ {I}, {2}} and {0, {I}, {2}, {I, 2}} 
are. 
â¢ 
Properties of functions. 
A function f : Ll --+ L2 between partially 
ordered sets Ll = (Ll' ~d and L2 = (L2' ~2) is surjective (or onto or epic) 
if 
and it is injective (or 1-1 or monic) if 
Vl, l' E Ll : f(l) = f(l') =? l = l' 
The function f is monotone (or isotone or order-preserving) if 
VI, I' E Ll : I ~l l' 
=? 
f(l) ~2 f(l')Â· 
It is an additive function (or a join morphism, sometimes called a distributive 
function) if 
and it is called a multiplicative function (or a meet morphism) if 
The function f is a completely additive function (or a complete join mor-
phism) if for all Y ~ L 1 : 

394 
A Partially Ordered Sets 
and it is completely multiplicative (or a complete meet morphism) if for all 
Y~L1: 
l(n1Y) = n2{I(l') Il' E Y} whenever n1Y exists 
Clearly U1 Y and n1Y always exist when L1 is a complete lattice; when L2 
is not a complete lattice the above statements also require the appropriate 
least upper bounds and greatest lower bounds to exist in L 2 â¢ The function 
1 is affine if for all non-empty Y ~ L1 
I(U 1Y ) = U 2{1(l') Il' E Y} whenever U 1Y exists (and Y # 0) 
and it is strict if 1(.1-1) = ..l2; note that a function is completely additive if 
and only if it is both affine and strict. 
Lemma AA If L = (L, I;;;, u, n,..l, T) and M = (M, I;;;, u, n,..l, T) are 
complete lattices and M is finite then the three conditions 
(i) 'Y: M -+ L is monotone, 
(ii) 'Y(T) = T, and 
(iii) 'Y(m1 n m2) = 'Y(m1) n 'Y(m2) whenever m1 It: m2 /\ m2 It: m1 
are jointly equivalent to 'Y : M -t L being completely multiplicative. 
_ 
Proof First note that if"( is completely multiplicative then (i), (ii) and (iii) hold. 
For the converse note that by monotonicity of"( we have "(mInm2) = "(mI)n"(m2) 
also when mI k m2 V m2 k mI. We then prove by induction on the (finite) 
cardinality of M' ~ M that: 
(A.2) 
If the cardinality of M' is 0 then (A.2) follows from (ii). If the cardinality of M' is 
larger than 0 then we write M' = M" U {m"} where m" 1. M"; this ensures that 
the cardinality of M" is strictly less than that of M'; hence: 
"(n M ') 
= "((11 M ") n m") 
= 
"(nM") n ,,(m") 
This proves the result. 
= 
(Ilb(m) 1m EM"}) n"(m") 
= 
I-Ib(m) 1m EM'} 
-
Lemma A.5 A function 1 : (P(D),~) -t (P(E),~) is affine if and only 
if there exists a function <p : D -t P(E) and an element <P0 E P(E) such that 
I(Y) = U{<p(d) IdE Y} U <P0 
The function 1 is completely additive if and only if additionally <P0 = 0. 
_ 

A.2 Construction of Complete Lattices 
395 
Proof Suppose that f is of the form displayed and let y be a non-empty set; Then 
UU(Y) lYE Y} 
U{U{cp(d) IdE Y} U CP0 lYE Y} 
U{U{cp(d) IdE Y} lYE Y} U CP0 
U{cp(d) IdE UY} U CP0 
f(Uy) 
showing that f is affine. 
Next suppose that f is affine and define cp(d) = f( {d}) and CP0 = f(0). For 
Y E P(D) let Y = {{d} IdE Y} U {0} and note that Y = UY and Y =1= 0. Then 
f(Y) 
f(Uy) 
U(U({d}) IdE Y} U {f(0)}) 
U({cp(d)) IdE Y} U {CP0}) 
= 
U{cp(d) IdE Y} U CP0 
so f can be written in the required form. The additional statement about com-
pletely additivity is straightforward. 
â¢ 
An isomorphism from a partially ordered set (L 1 , !;;;;1) to a partially ordered 
set (L2, !;;;;2) is a monotone function B : L1 -+ L2 such that there exists a 
(necessarily unique) monotone function B-1 : L2 -+ L1 with eo 0-1 = id2 
and B-1 0 B = id1 (where idi is the identity function over Li , i = 1,2). 
A.2 
Construction of Complete Lattices 
Complete lattices can be combined to construct new complete lattices. We 
shall first see how to construct products and then two kinds offunction spaces. 
Cartesian product. Let L1 = (L1, !;;;;1) and L2 = (L2, !;;;;2) be partially 
ordered sets. Define L = (L,~) by 
and 
(lll,h1) ~ (h2,l22) iff lll!;;;;l h2 /\ l21!;;;;2 h2 
It is then straightforward to verify that L is a partially ordered set. If ad-
ditionally each Li = (Li,!;;;;i,Ui,ni,-1i,Ti) is a complete lattice then so is 
L = (L, ~, U, n, -1, T) and furthermore 

396 
A Partially Ordered Sets 
and ..i = (..i1,..i2) and similarly for ny and T. We often write L1 x L2 for 
L and call it the cartesian product of L1 and L2. 
A variant of the cartesian product called the smash product is obtained if we 
require that all the pairs (h,12) of the lattice satisfy h =..i1 {:} 12 = ..i2. 
Total function space. Let L1 = (L1, ~d be a partially ordered set 
and let S be a set. Define L = (L,~) by 
L = {J : S ---+ L1 I f is a total function} 
and 
f ~ I' iff "Is E S : f(s) ~1 I'(s) 
It is then straightforward to verify that L is a partially ordered set. If 
additionally L1 = (L 1 , ~1, U1' n 1, ..i1, T d is a complete lattice then so is 
L = (L, ~, U, n,..i, T) and furthermore 
Uy = AsÂ·Udf(s) If E Y} 
and ..i = AS . ..i1 and similarly for ny and T. We often write S ---+ L1 for L 
and call it the total function space from S to L 1 . 
Monotone function space. Again let L1 = (L1, ~d and L2 = 
(L2, ~2) be partially ordered sets. Now define L = (L,~) by 
L = {f : L1 ---+ L2 I f is a monotone function} 
and 
f ~ I' iff Vh E L1 : f(h) ~2 I'(h) 
It is then straightforward to verify that L is a partially ordered set. If ad-
ditionally each Li = (Li, ~i, Ui,ni, ..ii, Ti) is a complete lattice then so is 
L = (L, ~, U, n, ..i, T) and furthermore 
and ..i = Ah . ..i2 and similarly for ny and T. We often write L1 ---+ L2 for L 
and call it the monotone function space from L1 to L2. 
A.3 
Chains 
The ordering ~ on a complete lattice L = (L,~) expresses when one prop-
erty is better (or more precise) than another property. When performing a 
program analysis we will typically construct a sequence of elements in Land 
it is the general properties of such sequences that we shall study now. In the 
next section we will be more explicit and consider the sequences obtained 
during a fixed point computation. 

A.3 Chains 
397 
Chains. A subset Y ~ L of a partially ordered set L = (L,~) is a chain 
if 
Vit, hEY : (h ~ lz) V (lz ~ h) 
Thus a chain is a (possibly empty) subset of L that is totally ordered. We 
shall say that it is a finite chain if it is a finite subset of L. 
A sequence (In)n = (In)nEN of elements in L is an ascending chain if 
Writing (In)n also for {In I n E N} it is clear that an ascending chain also is 
a chain. Similarly, a sequence (In)n is a descending chain if 
and clearly a descending chain is also a chain. 
We shall say that a sequence (In)n eventually stabilises if and only if 
::Ino EN: Vn EN: n ;::: no => In = lno 
For the sequence (In)n we write Un In for U{ln I n E N} and similarly we 
write nnln for n{ln I n EN}. 
Ascending Chain and Descending Chain Conditions. We 
shall say that a partially ordered set L = (L, ~) has finite height if and only 
if all chains are finite. It has finite height at most h if all chains contain at 
most h + 1 elements; it has finite height h if additionally there is a chain with 
h + 1 elements. The partially ordered set L satisfies the Ascending Chain 
Condition if and only if all ascending chains eventually stabilise. Similarly, it 
satisfies the Descending Chain Condition if and only if all descending chains 
eventually stabilise. These concepts are related as follows: 
Lemma A.6 A partially ordered set L = (L,~) has finite height if and 
only if it satisfies both the Ascending and Descending Chain Conditions. _ 
Proof First assume that L has finite height. If (In)n is an ascending chain then it 
must be a finite chain and hence eventually stabilise; thus L satisfies the Ascending 
Chain Condition. In a similar way it is shown that L satisfies the Descending Chain 
Condition. 
Next assume that L satisfies the Ascending Chain Condition as well as the Descend-
ing Chain Condition and consider a chain Y ~ L. We shall prove that Y is a finite 
chain. This is obvious if Y is empty so assume that it is not. Then also (Y,~) is 
a non-empty partially ordered set satisfying the Ascending and Descending Chain 
Conditions. 
As an auxiliary result we shall now show that 
each non-empty subset y' of Y contains a least element 
(A.3) 

398 
A Partially Ordered Sets 
! 
0 
T 
00 
-1 
-2 
! 
2 
1 
~ -00 
0 
(a) 
(b) 
Figure A.2: Two partially ordered sets. 
To see this we shall construct a descending chain (l~)n in yl as follows: first let l~ 
be an arbitrary element of y'. For the inductive step let 1~+1 = l~ if l~ is the least 
element of yl; otherwise we can find 1~+1 E y' such that 1~+1 ~ l~ A 
1~+1 # l~. 
Clearly (l~)n is a descending chain in y; since Y satisfies the Descending Chain 
Condition the chain will eventually stabilise, i.e. 3n~ : 'in > n~ : l~ = I', and the 
-
no 
construction is such that I', is the least element of yl. 
no 
Returning to the main proof obligation we shall now construct an ascending chain 
(In)n in y. Using (A.3) each In is chosen as the least element of the set Y \ 
{lo,Â·Â·Â·,ln-I} as long as the latter set is non-empty, and this yields In-I ~ In A 
In-I # In; when Y \ {lo,Â·Â·Â·, In-I} is empty we set In = In-I, and since Y is non-
empty we know that n > o. Thus we have an ascending chain in Y and using the 
Ascending Chain Condition we have 3no : 'in ~ no : In = lno. But this means that 
y \ {lo,Â· .. , lno} = 0 since this is the only way we can achieve that lno+! = lno. It 
follows that y is finite. 
_ 
Example A.7 The partially ordered set of Figure A.2 (a) satisfies the 
Ascending Chain Condition but does not have finite height; the partially 
ordered set of Figure A.2 (b) satisfies the Descending Chain Condition but 
does not have finite height. 
_ 
One can show that each of the three conditions finite height, ascending chain, 
and descending chain, is preserved under the construction of cartesian prod-
uct: if L1 and L2 satisfies one of the conditions then L1 x L2 will also satisfy 
that condition. The construction of total function spaces S -t L only pre-
serves the conditions of L if S is finite and the construction of monotone 
function spaces L1 -t L2 does not in general preserve the conditions. 
An alternative characterisation of complete lattices satisfying the Ascending 
Chain Condition is given by the following result: 

A.3 Chains 
399 
Lemma A.8 For a partially ordered set L = (L,~) the conditions 
(i) L is a complete lattice satisfying the Ascending Chain Condition, and 
(ii) L has a least element, -1., and binary least upper bounds and satisfies 
the Ascending Chain Condition 
are equivalent. 
â¢ 
Proof It is immediate that (i) implies (ii) so let us prove that (ii) implies (i). Using 
Lemma A.6 it suffices to prove that all subsets Y of L have a least upper bound 
U Y. If Y is empty clearly U Y = -1.. If Y is finite and non-empty then we can 
write Y = {Yl,Â·Â· ., Yn} for n ~ 1 and it follows that U Y = ( ... (Yl U Y2) uÂ· .. ) U Yn. 
If Y is infinite then we construct a sequence Cln)n of elements of L: let lo be an 
arbitrary element yo of Y and given In take In+l = In in the case where Vy E Y : Y !;;; 
In and take In+l = In U Yn+l in the case where some Yn+1 E Y satisfies Yn+l Ib In. 
Clearly this sequence is an ascending chain. Since L satisfies the Ascending Chain 
Condition it follows that the chain eventually stabilises, i.e. there exists n such that 
In = In+1 = . . .. This means that Vy E Y : Y !;;; In because if Y Ib In then In =f In U Y 
and we have a contradiction. So we have constructed an upper bound for Y. Since 
it is actually the least upper bound of the subset {yo,Â·Â·Â·, Yn} of Y it follows that 
it is also the least upper bound of Y. 
â¢ 
A related result is the following: 
Lemma A.9 For a complete lattice L = (L,~) satisfying the Ascending 
Chain Condition and a total function 1 : L -+ L, the conditions 
(i) 1 is additive, i.e. VIt, l2 : 1(It U l2) = 1(ld u 1(l2), and 
(ii) 1 is affine, i.e. VY ~ L, Y :f. 0 : 1(U Y) = U{J(l) Il E Y} 
are equivalent and in both cases 1 is a monotone function. 
â¢ 
Proof It is immediate that (ii) implies (i): take Y = {h, 12}. It is also immediate 
that (i) implies that f is monotone since h !;;; l2 is equivalent to huh = 12. 
Next suppose that f satisfies (i) and let us prove (ii). If Y is finite we can write 
Y = {Yl, ... , Yn} for n ~ 1 and 
fCU Y) = f(Yl U ... U Yn) = f(Yl) U ... U f(Yn) !;;; UU(l) Il E Y} 
If Y is infinite then the construction of the proof of Lemma A.S gives U Y = In 
and In = Yn U ... U yo for some Yi E Y and 0 ::; i ::; n. We then have 
feU Y) = fCln) = f(Yn U ... U yo) = f(Yn) U ... U f(yo) !;;; UU(l) II E Y} 
Furthermore 
feU Y) ~ UU(l) II E Y} 
follows from the monotonicity of f. This completes the proof. 
â¢ 

400 
A Partially Ordered Sets 
A.4 
Fixed Points 
Reductive and extensive functions. Consider a monotone func-
tion f : L --+ L on a complete lattice L = (L,~, u,n, 1.., T). A fixed point of 
f is an element 1 E L such that f(l) = 1 and we write 
Fix(f) = {l I f(l) = l} 
for the set of fixed points. The function f is reductive at 1 if and only if 
f(l) ~ 1 and we write 
Red(f) = {l I f(l) ~ l} 
for the set of elements upon which f is reductive; we shall say that f itself 
is reductive if Red(f) = L. Similarly, the function f is extensive at 1 if and 
only if f(l) ;) 1 and we write 
Ext(f) = {ll f(l) ;) l} 
for the set of elements upon which f is extensive; we shall say that f itself is 
extensive if Ext(f) = L. 
Since L is a complete lattice it is always the case that the set Fix(f) will 
have a greatest lower bound in L and we denote it by JÂ£p(f): 
Jfp(f) == n 
Fix(f) 
Similarly, the set Fix(f) will have a least upper bound in L and we denote it 
by gfp(f): 
gfp(f) = U Fix(f) 
We then have the following result, known as Tarski's Fixed Point Theorem, 
showing that Jfp(f) is the least fixed point of f and that gfp(f) is the greatest 
fixed point of f: 
Proposition A.I0 
Let L = (L,~, u,n, 1.., T) be a complete lattice. If f : L --+ Lis 
a monotone function then Jfp(f) and gÂ£p(f) satisfy: 
1Â£p(f) 
= n Red(f) 
E Fix(f) 
gfp(f) = U Ext(f) 
E Fix(f) 
Proof To prove the claim for Ifp(f) we define lo = nRed(f). We shall first show 
that f(lo) !;;; lo so that lo E Red(f). Since lo !;;; l for all l E Red(f) and f is 
monotone we have 
f(lo) !;;; f(l) !;;; l for alll E Red(f) 

A.4 Fixed Points 
401 
T 
Red(f) - - - - -
gfp(f) 
Fix(f) - -
Ifp(J) 
Ext(f) - - - - -
..L 
Figure A.3: Fixed points of f. 
and hence I(lo) 1; lo. To prove that lo 1; I(lo) we observe that l(f(lo)) 1; I(lo) 
showing that /(lo) E Red(f) and hence lo 1; I(lo) by definition of lo. Together this 
shows that lo is a fixed point of 1 so lo E Fix(f). To see that lo is le8$t in Fix(f) 
simply note that Fix(f) ~ Red(f). It follows that Jfp(f) = lo. 
The claim for gfp(f) is proved in a similar way. 
â¢ 
In denotational semantics it is customary to iterate to the least fixed point by 
taking the least upper bound ofthe sequence (r(..L))n. However, we have not 
imposed any continuity requirements on f (e.g. that f(Un in) = Un(f(ln)) 
for all ascending chains (in)n) and consequently we cannot be sure to actually 
reach the fixed point. In a similar way one could consider the greatest lower 
bound of the sequence (r (T))n. One can show that 
..L !; r(..L) !; U nr(..L) 
c: Ifp(f) 
c: gfp(f) !; nnr(T) !; r(T) !; T 
as is illustrated in Figure A.3; indeed all inequalities (Le. !;) can be strict 
(Le. c:). However, if L satisfies the Ascending Chain Condition then there 
exists n such that r(..L) = r+1(..L) and hence Ifp(J) = r(..L). (Indeed any 
monotone function f over a partially ordered set satisfying the Ascending 
Chain Condition is also continuous.) Similarly, if L satisfies the Descending 
Chain Condition then there exists n such that r(T) = r+1(T) and hence 
gfp(f) = r(T). 

402 
A Partially Ordered Sets 
Remark (for readers familiar with ordinal numbers). It is possible always 
to obtain lfp(J) as the limit of an ascending (transfinite) sequence but one 
may have to iterate through the ordinals. To this effect define ftt< E L for 
an ordinal /'i, by the equation 
and note that for a natural number n we have ft n = r+1 (..L). Then Jfp(J) = 
ftt< whenever /'i, is a cardinal number strictly greater than the cardinality of 
L, e.g. /'i, may be taken to be the cardinality of P(L). A similar construction 
allows to obtain gfp(J) as the limit of a descending (transfinite) chain. 
_ 
Concluding Remarks 
For more information on partially ordered sets consult a text book (e.g. [43]). 

Appendix B 
Induction and Coinduction 
We begin by reviewing a number of techniques for conducting inductive 
proofs. We then motivate the concept of coinduction and finally formulate 
a general proof principle for coinduction. This makes heavy use of Tarski's 
Fixed Point Theorem (Proposition A.IO). 
B.l Proof by Induction 
Mathematical induction. Perhaps the best known induction princi-
ple is that of mathematical induction. To prove that a property, Q(n), holds 
for all natural numbers, n, we establish 
Q(O) 
Vn : Q(n) => Q(n + 1) 
and conclude 
Vn: Q(n) 
Formally, the correctness of mathematical induction can be related to the fact 
that each natural number is either 0 or the successor, n + 1, of some other 
natural number, n. Thus the proof principle reflects the way the natural 
numbers are constructed. 
Structural induction. Mathematical induction allows us to perform 
induction on the size of any structure for which a notion of size can be defined; 
this is just a mapping from the structure into the natural numbers. As an 
example consider an algebraic data type given by 
dED 
d ::= Base I ConI (d) I Con2 (d, d) 

404 
B Induction and Coinduction 
where Base is a base case, ConI is a unary constructor and Con2 is a binary 
constructor. To prove that a certain property, Q(d), holds for all elements, 
d, of D we can define a size measure: 
size(Base) = 0 
size(Conl(d) 
= 1 + size(d) 
size( Con2 (dl , d2Â» 
1 + size( dl ) + size( d2) 
and then proceed by mathematical induction on size(d) to prove Q(d). 
Alternatively we can conceal the mathematical induction within a principle 
of structural induction: we must then show 
Q(Base) 
'Vd: Q(d) =? Q(Conl(dÂ» 
'Vdl , d2 : Q(dt} 1\ Q(d2) =? Q(Con2(dl , d2Â» 
from which we conclude 
'Vd: Q(d) 
Once again the proof principle reflects the way the data are constructed. 
Induction on the shape. Now suppose that Base represents 0, that 
Conl(d) represents d + 1, and that Con2(dl , d2) represents dl + d2. We can 
then define a Natural Semantics 
d-+n 
for evaluating d into the number, n, it represents: 
[base] 
Base -+ 0 
d-+n 
[conI] 
Conl(d) -+ n + 1 
dl -+ nl d2 -+ n2 
[con2] 
Con2(dl ,d2) -+ nl +n2 
This defines a notion of evaluation trees, d ~ n: there is one base Gase 
([base]) and two constructors ([conI] and [con2]). Again we can perform 
induction on the size of the evaluation trees but as above it is helpful to 
conceal the mathematical induction wIthin a principle of induction on the 
shape of inference trees: we must show 
Q(Base -+ 0) 

B.2 Introducing Coinduction 
405 
from which we conclude 
V(d ~ n) : Q(d ~ n) 
As is to be expected, the proof principle once again reflects the way evaluation 
trees are constructed. 
Course of values induction. All of the above induction principles 
have been constructive in the sense that we establish a predicate for the base 
cases and then show that it is maintained by all constructors. A variant of 
mathematical induction with a different flavour requires proving 
Vn: (Vm < n : Q(m)) :::::} Q(n) 
from which we conclude 
Vn: Q(n) 
Here the base case is dealt with in the same manner as the induction step. 
This induction principle is called course of values induction . 
. 
Well-founded induction. Course of values induction is an instance 
of a very powerful induction principle called well-founded induction. Given a 
partially ordered set (D, :j), the partial ordering is a well-founded ordering 
if there is no infinite decreasing sequence 
dl )- d2 )- d3 )- â¢â¢â¢ 
where d )- d' means d' :j d 1\ d =f. d' - this amounts to the Descending Chain 
Condition studied in Appendix A. The principle of well-founded induction 
then says: if we show 
Vd: (Vd' -< d: Q(d')) :::::} Q(d) 
we may then conclude 
Vd: Q(d) 
(The proof of correctness of this principle is along the lines of the proof of 
(A.3) in Lemma A.6 and can be found also in the literature referenced below.) 
B.2 
Introducing Coinduction 
To explain the difference between induction and coinduction, and to motivate 
the need for coinductive methods, let us consider a small example. Consider 
the program 

406 
B Induction and Coinduction 
if 1(27, m) then "something good" else "something bad" 
where 1 is a function from pairs of natural numbers (Le. pairs of non-negative 
integers) to truth values. 
We want to ensure that the program never undertakes to do "something 
bad". Since the value of m is not known it is not feasible to prove that 
1(27, m) "I false by merely evaluating 1(27, m)j we therefore need to perform 
some kind of proof. For this it is natural to define the predicate Qf as follows 
Qf(n) iff '<1m : I(n, m) "I false 
where it is implicit that m, n ~ 0. 
Perhaps the most obvious approach is to use mathematical induction to prove 
'<In: Qf(n). This amounts to proving 
Qf(O) 
'<In: Qf(n) * Qf(n + 1) 
and then concluding 
'<In: Qf(n) 
from which the desired Qf(27) follows. 
An alternative presentation of essentially the same idea is to establish the 
validity of the axiom and rule 
Qf(O) 
and then deduce that 
'<In: Qf(n) 
Here the basic steps in the mathematical induction have been couched in 
terms of an inductive definition of the predicate Qf. 
The approach outlined above works nicely for the function 10 defined by 
10(0, m) = true 
10(n+I,m) = lo(n,m) 
but what about the functions h, hand 13 defined by 
h(O,m) = h(O,m) 
h(n + I,m) = h(n,m) 
12(0,m) 
true 
h(n + I,m) = 
h(n + I,m) 
13(0, m) 
13(0, m) 
h(n + I,m) = h(n + I,m) 

B.2 Introducing Coinduction 
407 
where Ii (27, m) never terminates? Intuitively, they should be acceptable in 
the sense that "something bad" never happens. However, we cannot prove 
this by induction because we cannot establish the base case (for h and h) 
and/or we cannot establish the inductive step (for hand fg). 
An intuitive argument for why Ii is acceptable might go as follows: assume 
that all occurrences of Ii on the right hand sides of the above definitions 
satisfy Qi; then it follows that also the Ii on the left hand side does. Hence 
Ii satisfies Qi, i.e. 'in : Qi(n). This sounds very dangerous: we assume the 
desired result in order to prove it. However, with due care and given the 
proper definition of Qi, this is a valid proof: it is a coinductive proof. 
Obtaining a functional. Let us rewrite the defining clauses of Ji into 
clauses for Qi so as to clarify the relationship between when Qi holds on the 
left hand side and on the right hand side of the definitions of Ii: 
Qo(O) 
iff 
true 
Q1 (0) 
iff 
Q1 (0) 
Qo(n + 1) 
iff Qo(n) 
Q1(n + 1) 
iff Q1(n) 
Q2 (0) 
iff true 
Q2(n + 1) 
iff Q2(n + 1) 
Q3 (0) 
iff 
Q3 (0) 
Q3(n + 1) 
iff Q3(n + 1) 
(B.1) 
Here the clauses for Qo look just like our principle for mathematical induction 
whereas the others involve some amount of circularity. To make this evident 
let us rewrite the above as 
Qi = Qi(Qi) 
(B.2) 
where 
QO(Q')(O) 
true 
Q1(QI)(0) = Q/(O) 
Qo(Q')(n + 1) = Q/(n) 
Q1(QI)(n + 1) = QI(n) 
(B.3) 
Q2(QI)(0) 
true 
Q3(QI)(0) = Q/(O) 
Q2(Q')(n + 1) = QI(n + 1) 
Q3(QI)(n + 1) = Q/(n + 1) 
Clearly Qi satisfies (B.1) if and only if it satisfies (B.2) with Qi as in (B.3). 
It is immediate that each Qi is a monotone function on the complete lattice 
(N -+ {true,false}, ~) 
of predicates where Q1 ~ Q2 means that 'in : Q1(n) => Q2(n) and where the 
least element -l is given by 'in : -len) = false and the greatest element T is 
given by 'in : T(n) = true. Using Tarski's Fixed Point Theorem (Proposition 
A.10) it follows that each Qi has a least fixed point Jfp(Qi) and a greatest 
fixed fixed point gfp(Qi); these are possibly different predicates in (N -+ 
{ true, false}, ~). 
We frequently refer to Qi as a functional by which we mean a function whose 
argument and result are themselves functions (or that are more elaborate 
structures containing functions in them). 

408 
B Induction and Coinduction 
Least fixed point. Let us begin by looking at the least fixed points. 
It follows from Appendix A that 
and given that the clauses for Qo(Q) only use a finite number of Q's on the 
right hand sides (in fact zero or one), Qo satisfies a continuity property that 
ensures that 
This is good news because our previous proof by mathematical induction 
essentially defines the predicate Uk Q~(1.): Q~(1.)(n) holds if and only if at 
most k axioms and rules suffice for proving Qo(n). Thus it would seem that 
a proof by induction "corresponds" to taking the least fixed point of Qo. 
Next let us look at Q3' Here 
because Q3(1.) = 1. so that 1. is a fixed point. This explains why we have 
Ifp( Q3) (27) = false and why an inductive proof will not work for establishing 
Q3(27). Somewhat similar arguments can be given for Ql and Q2. 
Greatest fixed point. Let us next look at the greatest fixed points. 
Here 
because Q3 (T) = T so that T is a fixed point. This explains why we have 
gfp(Q3)(27) = true and thus provides a formal underpinning for our belief 
that h will not cause any harm in the example program. Somewhat similar 
arguments can be given for Ql and Q2. 
Also for Qo it will be the case that gfp(Qo)(27) = true. This is of course 
not surprising since Ifp(Qo)(27) = true and Ifp(Qo) ~ gfp(Qo). However, 
it is more interesting to note that for Qo there is no difference between the 
inductive and the coinductive approach (unlike what is the case for Ql, Q2 
and Q3): 
Jfp(Qo) = gfp(Qo) 
because mathematical induction on n suffices for proving that Ifp(Qo)(n) = 
gfp(Qo) (n). 
Remark. To the mathematically inclined reader we should point out that 
the fact that Ifp(Qo) = gfp(Qo) is related to Banach's Fixed Point Theorem: 
a contractive operator on a metric space has a unique fixed point. Contrac-
tiveness of Qo (as opposed to Ql, Q2 and Q3) follows because the clause for 
Qo(Q)(n) only performs calls to Q on arguments smaller than n. 
_ 

B.3 Proof by Coinduction 
409 
B.3 
Proof by Coinduction 
Consider once again the algebraic data type 
dE D 
d ::= Base I Con1 (d) I Con2(d, d) 
with one base case, one unary constructor and one binary constructor. Next 
consider the definition of a predicate 
Q : D -+ {true, false} 
by means of clauses of the form: 
Q(Base) 
iff 
Q(Con1(d)) 
iff ... Q(d')Â·Â·Â· 
Q(Con2(d1, d2)) 
iff ... Q(d~)Â·Â·Â· Q(d~) ... 
We can use this as the basis for defining a functional Q by cases as in: 
\Ve note that 
Q(Q')(Base) 
Q(Q')(Con1(d)) 
Q(Q')(Con2(d1, d2 )) 
... Q'(d')Â·Â·Â· 
... Q' (d~) ... Q' (d~) ... 
(D -+ {true, false}, ~) 
is a complete lattice under the ordering given by Q1 ~ Q2 if and only if 
Vd: Q1(d) =? Q2(d). We also 
assume that Q is monotone 
and this means that e.g. a clause like "Q(Con1(d)) iff -,Q(d)" will not be 
acceptable. From Proposition A.I0 it follows that Q has a least as well as a 
greatest fixed point. 
Induction (or least fixed point). Consider first the inductive def-
inition: 
Q = lfp(Q) 
(B.4) 
This is more commonly written as: 
Q(Base) 
... Q(d')Â·Â·Â· 
Q(Con1(d)) 
... Q(dD ... Q(d~) ... 
Q(Con2(d1,d2)) 
(B.5) 
It is often the case that each rule only has a finite number of calls to Q 
and then the two definitions are equivalent: the predicate in (B.5) amounts 

410 
B Induction and Coinduction 
to Uk Qk(-L) and by a continuity property as discussed above, this agrees 
with the predicate of (BA). A proof by induction then simply amounts to 
establishing the validity of the axioms and rules in (B.5). Such a proof has a 
very constructive flavour: we take nothing for granted and only believe what 
can be demonstrated to hold. This proof strategy is often used to reason 
about semantics because the semantics of a program should not allow any 
spurious behaviour that is not forced by the semantics. 
Coind uction (or greatest fixed point). Consider next the coin-
ductive definition 
Q = gfp(Q) 
A proof by coinduction then amounts to using the proof rule 
Q' C Q(Q') 
Q'!;;;Q 
( . 
Q' C Q(Q')) 
I.e. Q' !;;; gfp( Q) 
as follows from the formula for gfp(Q) given in Proposition A.IO. So to prove 
Q(d) one needs to 
find some Q' such that 
Q'{d) 
Vd' : Q'(d') => Q(Q')(d') 
Such a proof has a very optimistic flavour: we can assume everything we like 
as long as it cannot be demonstrated that we have violated any facts. It is 
commonly used for checking that a specification holds because the specifica-
tion should not forbid some behaviour unless explicitly called for. 
It sometimes saves a bit of work to use the derived proof rule 
Q'!;;; Q(QUQ') 
Q'!;;;Q 
To see that this is a valid proof rule suppose that Q' !;;; Q(Q U Q'). By 
definition of Q we also have Q !;;; Q(Q) and by monotonicity of Q this gives 
Q !;;; Q(Q U Q'). Hence 
Q U Q' !;;; Q(Q U Q') 
and Q U Q' !;;; Q follows by definition of Q. It is then immediate that Q' !;;; Q 
as required. 
Clearly this explanation can be generalised to algebraic data types with ad-
ditional base cases and constructors, and from predicates to relations. 

B.3 Proof by Coinduction 
411 
Example B.1 Consider the relations 
defined by: 
Rl : Dl1 x D12 -+ {true,false} 
R2 : D21 x D22 -+ {true, false} 
Rl = R1(R1, R2) 
R2 = R2(R1, R2) 
This is intended to define 
where R(R~,R~) = (Rl(R~,R~),R2(R~,R~)) is assumed to be monotone. 
Next write R' U RII for the relation defined by 
and write R' !;;;; RII for the truth value defined by: 
We then have the following version of the coinduction principle: we establish 
and conclude: 
R~ !;;;; Rl and R~ !;;;; R2 
By analogy with the previous discussion, it sometimes saves a bit of work 
only to show 
R~ c:: 
Rl(RlUR~,R2UR~) 
R~ c:: 
R 2(R1 U R~, R2 U R~) 
because we also have 
c:: 
Rl (Rl' R2) 
c:: 
R 2 (R1 , R2 ) 
and this allows us to conclude 
c:: 
Rl(RlUR~,R2UR~) 
c:: 
R2(RIUR~,R2UR~) 
using the definition of (Rl' R2) = gfp(R). 
â¢ 

412 
B Induction and Coinduction 
Example B.2 For an example of a somewhat different flavour consider 
the universe 
{a, 1, 2}OO = {a, 1, 2}* U {a, 1, 2}W 
where {a, 1, 2}* consists of finite strings (ai)i=1 = a1 ... an of symbols ai E 
{a, 1, 2} and where {a, 1, 2}W consists of infinite strings (ai)~1 = a1 ... an' .. 
of symbols ai E {a, 1, 2}. Concatenation of strings is defined as follows 
(ai)i=1 (bj )';:1 
( )n+m 
where 
Ck = { ak 
if k ~ n 
= 
Ck k=1 
bk-n if k > n 
(ai)i=1 (bj )';:1 = (Ck)k::1 
where 
Ck = { ak 
if k ~ n 
bk-n if k > n 
(ai)~1 (bj)}";"'1 = (Ck)k::1 
where 
Ck = ak 
(whereÂ· .. denotes "m" or "00" in the last formula). 
Next consider a context free grammar with start symbol S and productions: 
S--+Â°1110SllS 
It can be rephrased as an inductive definition 
Â° 
E S 
1 E S 
xES 
Ox E S 
xES 
Ix E S 
for defining a subset S ~ {a, 1, 2}oo. If an inductive interpretation is taken, 
as is usually the case for context free grammars, the set S is {0,1}+ that 
consists of all strings (ai)i=1 that are finite and nonempty (so n > 0) and 
without any occurrence of 2's (Le. ai E {a, I}). 
If a coinductive interpretation is taken the set S becomes {a, I} + U {a, I}OO 
where also all infinite strings of O's and l's occur. To see this note that the 
empty string cannot be in S because it is not of one of the forms 0, 1, Ox or 
Ix. Also note that no string containing 2 can be in S: a string containing 2 
can be written a1 ... an2x where ai E {a, I}; for n = Â° 
the result is immediate 
(since none of 0, 1, Ox or Ix starts with a 2) and for n > Â° 
we note that if 
a1 ... an2x E S then a2 ... an2x E S and by assumption this cannot be the 
case. Finally note that there is no way to exclude infinite strings of o's and 
1 's to be in S because they can always be written in one of the forms Ox or 
Ix (where x E {O,I}OO). 
Now suppose we add the production 
corresponding to the rule: 
S--+S 
xES 
xES 

B.3 Proof by Coinduction 
413 
When interpreted inductively this does not change the set S; it is still {O, 1} + . 
However, when interpreted coinductively the set S changes dramatically; it is 
now {a, 1, 2}OO and thus includes the empty string as well as strings containing 
2. To see this simply note that any string x in {a, 1, 2}OO can be written as 
x = x and hence no string can be excluded from being in S. 
â¢ 
Concluding Remarks 
For more information on induction principles consult a text book (e.g. [16]). 
It is harder to find good introductory material on coinduction; one possibility 
is to study the work on strong bisimulation in CCS (e.g. Chapter 4 of [106]). 

Appendix C 
Graphs and Regular 
Expressions 
C.l 
Graphs and Forests 
Directed graphs. A directed graph (or digraph) G = (N, A) consists of 
a finite set N of nodes (or vertices) and a set A ~ N x N of edges (or arcs). 
An edge (n}, n2) has source nl and target n2 and is said to go from nl to 
n2 j if nl = n2 it is a self-loop. We shall sometimes refer to a directed graph 
simply as a graph. 
A directed path (or just path) from a node no to a node nm is a sequence 
of edges (no,nl), (n},n2),Â·Â·Â·, (nm-2,nm-d, (nm-l,nm) where the target of 
the edge (ni-l, ni) equals the source of the edge (ni, niH). The path is said 
to have source no, target nm and length m ~ OJ if m = 0 the path is said to be 
trivial and then consists of the empty sequence of edges. The concatenation of 
the path (no, nd,Â·Â·Â·, (nm-l,nm) with the path (nm,nm+l),Â·Â·Â·' (nk-l, nk) is 
the path (no, nd, ... , (nk-l, nk). We say that n' is reachable from n whenever 
there is a (possibly trivial) directed path from n to n'. 
Example C.l Let S be a statement from the WHILE language of Chapter 
2. The forward flow graph (labeJs(S),flow(S)) and the backward flow graph 
(labeJs(S),flo~(S)) are both directed graphs. Next let Lab be a finite set 
of labels and F ~ Lab x Lab a flow relation in the manner of Section 2.3. 
Then G = (Lab, F) is a directed graph. 
_ 
Example C.2 Let e* be a program in the FUN language of Chapter 3 and 
let N* be the finite set of nodes p of the form ql) or rex) where l E Lab* is 
a label in the program and x E Var* is a variable in the program. Let C be 

416 
C Graphs and Regular Expressions 
a set of constraints of the form considered in Section 3.4 and let A* contain 
the edge (PI,P2) for each constraint PI ~ P2 in C and the edges (P,P2) and 
(PI,P2) for each constraint {t} ~ P =} PI ~ P2 in C. Then G = (N*,A*) is a 
directed graph. 
-
In keeping with the notation used in Chapter 2 we shall usually write a 
path P = (nO,nl),(nl,n2),Â·Â·Â·,(nm -2,nm-I),(nm-l,nm) as the sequence 
P = [no, nl, n2,"', nm-2, nm-I, nml of nodes visited. In this notation the 
sequence of nodes is never empty and the trivial path from no to no is 
written [nol whereas a self-loop is written [no, nolo The concatenation of 
the path [no, nl,'" ,nm-I,nml with the path [nm,nm+1,"" nk-I,nkl is the 
path [no, nl,"', nk-I, nkl (where nm does not occur twice). 
We shall define the set of paths from n to n' as follows: 
paths.(n,n') = {[no,'" ,nml I m 2: 0 A no = n A nm = n' A 
Vi < m: (ni,ni+r) E A} 
Cycles. A cycle is a non-trivial path from a node to itself (and may take 
the form of a self-loop); the set of cycles from a node n to itself is defined as 
follows: 
cyc1es(n) = {[no,Â·Â·Â·,nml I m 2: 1 A no = n A nm = n A 
Vi < m : (ni, ni+l) E A} 
and we observe that cyc1es(n) = {p E paths.(n,n) I P :f; [n]}. A cycle 
P = [no, nl,"', nmJ is multiple entry if it contains two distinct nodes (not 
merely distinct indices), ni :f; nj, such that there are (not necessarily distinct) 
nodes n~ and nj external to the cycle with (n~, ni), (nj, nj) E A. 
A graph that contains no cycles (so Vn EN: cyc1es(n) = 0) is said to be 
acyclic. A directed, acyclic graph is often called a DAG. 
A topological sort of a directed graph is a total ordering of the nodes such 
that if (n, n') is an edge then n is ordered strictly before n'. A directed graph 
has a topological sort if and only if it is acyclic. 
Strongly connected components. Two nodes nand n' are said 
to be strongly connected whenever there is a (possibly trivial) directed path 
from n to n' and a (possibly trivial) directed path from n' to n. Defining 
SC = {(n, n') I nand n' are strongly connected} 
we obtain a binary relation SC ~ N x N. 
Fact C.3 SC is an equivalence relation. 
-
Proof Reflexivity follows from the fact that there is a trivial path from any node 
to itself. Symmetry follows immediately from the definition. Transitivity follows 

C.1 Graphs and Forests 
417 
from observing that if P12 is a path from nl to n2 and P23 is a path from n2 to n3 
then P12P23 is a path from nl to n3. 
_ 
The equivalence classes of SC are called the strong components (or strongly 
connected components) of the graph G = (N, A). A graph is said to be 
strongly cO'TJ,nected whenever it contains exactly one strongly connected com-
ponent. 
Example C.4 In uniquely labelled statements of the WHILE language of 
Chapter 2, the nodes corresponding to the outermost loop in any nested 
sequence of loops would constitute a strongly connected component; this is 
true for both forward and backward flow graphs. 
_ 
The interconnections between strong components can be represented by the 
reduced graph. Each strongly connected component is represented by a node 
in the reduced graph and there is an edge from one node to another distinct 
node if and only if there is an edge from some node in the first strongly 
connected component to a node in the second in the original graph. Hence, 
the reduced graph contains no self-loops. 
Lemma C.5 For any graph G the reduced graph is a DAG. 
-
Proof Suppose, by way of contradiction, that the reduced graph contains a cycle 
[SCo,Â·Â·Â·, SCm} where SCm = SCo. Since we already observed that the reduced 
graph contains no self-loops this means that the cycle contains distinct nodes SCi 
and SCj and we may without loss of generality assume that i < j. 
That there is an edge from SCk to SCHl in the reduced graph (0 ~ k < m) means 
that there is an edge (nk' n~+l) in the original graph from a node nk E SCk to a 
node n~+l E SCHl. It is also immediate that there is a path Pk in the original 
graph from n~ to nk and a path p~ from n:n to no. 
We can now construct a path (ni,n:+l)PHlÂ·Â·Â· (nj-l,nj)pj in the original graph 
from ni to nj and a path (nj, nJ+l)pj+1 ... (nm-l, n:n)p~(no, ni)Pl ... (ni-l, n;)pi 
from nj to ni. But then ni and nj should be in the same equivalence class thereby 
contradicting our assumption that SCi =F SCj. 
_ 
Handles and roots. A handle for a directed graph G = (N, A) is a 
set H ~ N of nodes such that for all nodes n E N there exists a node h E H 
such that there is a (possibly trivial) directed path from h to n. A root is 
a node r E N such that for all nodes n E N there is a (possibly trivial) 
directed path from r to n. If r is a root for G = (N, A) then {r} is handle 
for G; in fact, {n} is a handle for G if and only if n is a root. Conversely, 
if H is a handle for G = (N, A) and r f/. N then r is a root for the graph 
(N U {r}, Au {(r, h) I h E H}); we sometimes refer to r as a dummy root 
and to each (r, h) as a dummy edge. 

418 
C Graphs and Regular Expressions 
A directed graph G = (N, A) always has a handle since one can use H = N as 
the handle. A handle H is minimal if no proper subset is a handle, that is, if 
H' c H then H' is not a handle. One can show that minimal handles always 
exist since the set of nodes is finite. The minimal handle is a singleton if and 
only if the graph has a rootj in this case we say that the graph is rooted. 
Example C.6 Let S be a statement from the WHILE language of Chap-
ter 2. Then init(S) is a root for (labels(S),How(S)) and {init(S)} is a min-
imal handle. Furthermore, fina1(S) is a handle for the backward flow graph 
(labels(S), Ho~(S)). 
â¢ 
A path from a handle H to a node n is a path from a node h E H to nj 
similarly for a path from the root r to a node n. Given a handle H, possibly 
in the form of a root r (corresponding to H = {r}), the set of paths from H 
to n is defined by: 
path~ (n) = U{paths.(h,n) I hE H} 
When H is implicit we write path.(n) for path~ (n). 
Forests and trees. A node n in a graph G = (N, A) is said to have 
in-degree m if the set of predecessors, {n' I (n',n) E A}, has cardinality m. 
Similarly, one can define the concept of out-degree. 
A forest (or unordered, directed forest) is a directed, acyclic graph G = 
(N, A) where nodes have in-degree at most 1. One can show that the set 
of nodes with in-degree 0 constitutes a minimal handle for the forest. IT 
(n, n') E A then n is said to be the parent of n' and n' is a said to be a 
child of nj ancestor and descendant are the reflexive, transitive closures of 
the parent and child relations, respectively. The node n is a proper ancestor 
of n' if it is an ancestor and n i= n' and similarly for proper descendants. 
A tree (or unordered, directed tree) is a forest that has a rootj in other words, 
a tree is a forest where exactly one node has in-degree 0 and this is then the 
root. Given a forest (N, A) with minimal handle H, each node h E H will be 
a root for the tree consisting of all the nodes n E N reachable from h and all 
the edges (nl,n2) E E whose source and target are both in the treej thus a 
forest can be viewed as a set of trees. 
Sometimes we will be interested in ordered forests and ordered trees which 
are forests and trees where additionally all children of a node are linearly 
ordered (unlike what is normally the case for graphs, forests and trees). 
Dominators. Given a directed graph G = (N,A) with handle H, possi-
bly in the form of a root r (corresponding to H = {r}), a node n' is said to 
be a dominator of a node n whenever every (possibly trivial) path from H 
to n contains n'. We also say that n' dominates n. As a consequence of the 

C.2 Reverse Postorder 
INPUT: 
A directed graph (N, A) with k nodes and handle H 
OUTPUT: 
(1) A DFSF T = (N, AT), and 
(2) a numbering rPostorder of the nodes indicating the 
reverse order in which each node was last visited 
and represented as an element of array [N] of int 
METHOD: 
i:= k; 
USING: 
mark all nodes of N as unvisited; 
let AT be empty; 
while unvisited nodes in H exists do 
choose a node h in H; 
DFS(h); 
procedure DFS(n) is 
mark n as visited; 
while (n, n') E A and n' has not been visited do 
add the edge (n, n') to AT; 
DFS(n'); 
rPostorder[n] := i; 
i:= i-I; 
Table C.l: The DFSF Algorithm. 
419 
definition of paths, the only dominator of an element of a minimal handle 
(such as a root) is the element itself. 
For any node n the set of dominators can be specified as the greatest solution 
to the following equation: 
Dom(n) = { in} 
ifnEH 
in} u n{Dom(n') I (n',n) E A} otherwise 
The node n' properly dominates the node n if the two nodes are distinct 
and n' dominates n. The node n' directly dominates the node n if it is 
the "closest" proper dominator of nj that is n' E Dom(n)\{n} and for all 
nil E Dom(n)\{n} we have nil E Dom(n'). 
C.2 
Reverse Postorder 
Spanning forests. A spanning forest for a graph is a forest, with the 
same nodes as the graph and a subset of the edges of the graph (see below) as 

420 
C Graphs and Regular Expressions 
Figure C.I: A flow graph. 
edges. The algorit~m of Table C.I non-deterministically constructs a depth-
first spanning forest (abbreviated DFSF). In parallel with constructing the 
forest, the algorithm also generates a numbering of the nodes which is the 
reverse of the order in which the nodes are last visited in the construction of 
the tree; this ordering is called the -reverse postorder and in the algorithm it 
is represented as an array indexed by the nodes of the graph. If the DFSF is 
a tree it is called a depth-first spanning tree (abbreviated DFST). Note that 
the algorithm does not specify which unvisited node to select at each stage; 
consequently, the depth-first spanning forest for a graph is not unique. 
Given a spanning forest one can categorise the edges in the original graph as 
follows: 
â¢ Tree edges: edges present in the spanning forest. 
â¢ Forward edges: edges that are not tree edges and that go from a node 
to a proper descendant in the tree. 
â¢ Back edges: edges that go from descendants to ancestors (including 
self-loops) . 
â¢ Cross edges: edges that go between nodes that are unrelated by the 
ancestor and descendant relations. 
The algorithm of Table C.I ensures that cross edges always go from nodes 
visited later (Le. with lower numbers in the reverse postorder) to nodes visited 
earlier (Le. with higher numbers). 
Example C.7 To illustrate the operation of the algorithm, consider the 
graph in Figure C.I with root b1 . The algorithm may produce the tree shown 

C.2 Reverse Postorder 
421 
Figure C.2: A DFSF for the graph in Figure C.l. 
in Figure C.2. The reverse postorder of the nodes is b1 , b3 , 8 3 , b2 , 8 1 , 8 2 and 
the second number annotating each node in Figure C.2 reflects this ordering. 
The edges from 8 2 to b2 and from 83 to b3 in Figure C.1 are back edges. The 
edge from b3 to 8 1 is a cross edge. 
_ 
Example C.S Let 8 be a uniquely labelled statement from WHILE and 
consider the forward flow graph (labels( 8), flow( 8)) with root init( 8). First 
consider a while loop while b' do 8' in 8. It gives rise to one tree edge 
and one or more back edges; more precisely as many back edges as there are 
elements in fina1(8'). Next consider a conditional if b' then 8t else 8~ in 8 
that is not the last statement of 8 and not the last statement of the body of a 
while loop in 8. It gives rise to three tree edges and one or more cross edges; 
more precisely there will be one fewer cross edges than there are elements in 
final(8D U fina1(8~). Note that no forward edges can arise for statements in 
WHILE; however, an extension of WHILE with a one-branch conditional if b 
then 8 can give rise to forward edges. 
_ 
Properties. We now establish some properties of reverse postorder. The 
first lemma shows that the node at the source of a back edge comes later than 
the target (unless it is a self-loop). 
Lemma C.9 Let G = (N, A) be a directed graph, T a depth-first spanning 
forest of G and rPostorder the associated ordering computed by the algorithm 
of Table C.l. An edge (n, n') E A is a back edge if and only if rPostorder[n] ~ 
rPostorder[n'] and is a self-loop if and only if rPostorder[n] = rPostorder[n']. _ 
Proof The statement about self-loops is immediate. Next let (n, n') E A. 
(::::} ): If (n, n') is a back edge then n is a descendant of n' in T. Consequently, n' 
is visited before n in the depth first search and the call DFS(n') is pending during 
the entire call of DFS(n) and consequently rPostorder[n] ~ rPostorder[n']. 

422 
C Graphs and Regular Expressions 
({::): If rPostorder[n] ~ rPostorder[n'] then, either n is a descendant of n' or n' is 
in a subtree that was constructed later than the subtree that n appears in. In the 
second case, since (n, n') E A, it would be a cross edge going from a node with a 
high reverse postorder number to a node with a low reverse postorder number -
since our algorithm does not admit such cross edges, n must be a descendant of n' 
and thus (n, n') is a back edge. 
-
Next, we establish that every cycle in a directed graph must contain at least 
one back edge. 
Corollary C.IO Let G = (N, A) be a directed graph, T a depth-first 
spanning forest of G and rPostorder the associated ordering computed by the 
algorithm of Table C.l. Any cycle of G contains at least one back edge. 
_ 
Proof A cycle is a path [no,Â·Â·Â·, n,.,.] with no = n,.,. and m ~ 1. Then, since 
rPostorder[no] = rPostorder[nm ], we must have rPostorder[ni] ~ rPostorder[ni+l] for 
some 0 $ i < m. Thus Lemma C.9 applies. 
_ 
The ordering rPostorder topologically sorts the depth-first spanning forest. 
Corollary C.II Let G = (N,A) be a directed graph, T a depth-first 
spanning forest of G and rPostorder the associated ordering computed by the 
algorithm of Table C.l. Then rPostorder topologically sorts T as well as the 
forward and cross edges. 
_ 
Proof By Lemma C.9, for any edge (n, n'), rPostorder[n] < rPostorder[n'] if and 
only if the edge is not a back edge, that is, if and only if the edge is a tree edge in 
T or a forward edge or a cross edge. Thus rPostorder topologically sorts T as well 
as the forward and cross edges. 
_ 
Loop connectedness. Let G = (N, A) be a directed graph with handle 
H. The loop connectedness parameter of G with respect to a depth-first 
spanning forest T constructed by the algorithm of Table C.I, is the largest 
number of back edges found in any cycle-free path of G; we write d(G, T), 
or d( G) when T is clear from the context, to denote the loop connectedness 
parameter of a graph. The .value of d(G) for the graph in Figure C.I is 1. 
Let a dominator-back edge be an edge (nl' n2) where the target n2 dominates 
the source nl. A dominator-back edge (nl' n2) is a back edge regardless of 
the choice of spanning forest T because the path from H to nl in T must 
contain n2 (given that n2 dominates nl). The graph shown in Figure C.3 
shows that there may be more back edges than there are dominator-back 
edges; indeed, it has root r and no dominator-back edges, but any spanning 
forest T will characterise one of the edges between nl and n2 as a back edge. 
The literature contains many definitions of the concept of reducible graph. 
We shall say that a directed graph G = (N, A) with handle H is reducible 

C.2 Reverse Postorder 
423 
Figure C.3: An irreducible graph. 
if and only if the graph (N, A \ A db ) obtained by removing the set Adb of 
dominator-back edges is acyclic and still has H as handle. The simplest 
example of an irreducible graph is shown in Figure C.3. 
Reducible graphs are of interest because for a reducible graph G with handle 
H and an arbitrary depth-first spanning forest T, it will be the case (as shown 
below) that an edge is characterised as a back edge with respect to T if and 
only if it is a dominator-back edge. It follows that the loop connectedness 
parameter is then independent of the choice of depth-first spanning forest. 
Lemma C.12 Let G = (N,A) be a reducible graph with handle H, T 
a depth first spanning forest for G and H, and rPostorder the associated 
ordering computed by the algorithm of Table C.l. Then an edge is a back 
edge if and only if it is a dominator-back edge. 
_ 
Proof We already argued that a dominator-back edge necessarily is a back edge 
with respect to T. Next suppose by way of contradiction that there is a back edge 
(ns, nt) that is not a dominator-back edge. It is clear that the tree edges (i.e. the 
edges in T) are not back edges and hence not dominator-back edges and therefore 
they all occur in the graph (N, A \ Adb) constructed above. Next let T' be the 
graph obtained by adding the edge (ns, nt) to T. Then also all edges in T' occur 
in the graph (N, A \ Adb). The path in T' from nt to ns , followed by the back edge 
(ns, nt), constitutes a cycle in T' and hence in (N, A \ Adb). This contradicts the 
acyclicity of (N, A \ Adb) and hence the reducibility of G. 
_ 
Example C.13 Once more let S be a uniquely labelled statement from 
WHILE and consider the flow graph (Jabels(S) , fIow(S)) with root init(S). It 
is reducible: Each while loop can be entered only at the test node which then 
dominates all the nodes for the body of the loop and hence all the back edges 
introduced by the while loop are also dominator-back edges. Clearly the 
graph consisting of tree edges, forward edges and cross edges will be acyclic. 
The loop connectedness parameter equals the maximum number of back edges 
(and hence dominator-back edges) of a cycle-free path in the flow graph; it 

424 
C Graphs and Regular Expressions 
follows that it is equal to the maximum number of nested while loops in the 
program. 
â¢ 
Another interesting property of rPostorder for reducible graphs is the following 
result. 
Corollary C.14 Let G = (N, A) be a reducible graph with handle H. 
Any cycle-free path in G beginning with a node in the handle, is monoton-
ically increasing by the ordering rPostorder computed by the algorithm of 
Table C.l. 
â¢ 
Proof Any such path must contain no dominator-back edges and thus by Lemma 
C.12 is either a path in the depth-first spanning forest or a sequence of paths 
from the forest connected by forward or cross edges. The result then follows from 
Corollary C.lI. 
â¢ 
Other orders. As we have seen, reverse postorder numbers the nodes 
in the reverse of the order in which they were last visited in the construction 
of the depth-first spanning forest. Two commonly used alternative orderings 
on nodes are preorder and breadth-first order; as an example, the preorder 
for the tree in Figure C.2 is b1,b2,S2,Sl,b3,S3 and the breadth-first order is 
b1 , b2 , b3, S2, Sl, S3. Like reverse postorder, both orders topologically sort the 
depth-first spanning forest. They also topologically sort forward edges but 
they do not necessarily topologically sort cross edges. (For preorder, consider 
the edge from b3 to Sl in the example; for breadth-first order, modify the 
example to have an additional node between b1 and b3 .) This observation 
makes reverse postorder the better choice for a number of iterative algorithms. 
C.3 
Regular Expressions 
An alphabet is a finite and non-empty set ~ of symbols; we shall assume 
that it is disjoint from the set {A, 0, (,), +, ., *} of special symbols. A regular 
expression over ~ is any expression constructed by the following inductive 
definition': 
1. A and 0 are (so-called atomic) regular expressions. For' any a E ~, a is 
a (so-called atomic) regular expression. 
2. If R1 and R2 are regular expressions then (R1 + R2), (R1 . R2 ) and 
(R1)* are (so-called compound) regular expressions. 
The above definition constructs expressions that are fully parenthesised; by 
placing a precedence on the operators so that '*'binds more tightly than '.', 
and '.' binds more tightly than '+', most parentheses can be omitted. 

Concluding Remarks 
425 
A string W E ~* is a sequence of symbols from ~. A language L over ~ is a 
set of strings, i.e. L ~ ~*. The language defined by a regular expression R is 
C[R] defined by: 
C[A] = {A} 
C[0] = 0 
C[a] = 
{a}forallaE~ 
C[RI + R2] = C[Rd U C[R2] 
C[RI . R2] = C[Rd . C[R2] 
00 
C[R~] = U (C[Rd)k 
k=O 
where Ll . L2 = {WIW2 I WI ELI and W2 E L2}, LO = {A} and Li+l = L i â¢ L. 
Two regular expressions Rl and R2 are equivalent, denoted Rl = R2, if their 
languages are equal, that is C[Rd = C[R2]. 
A homomorphism from ~i to ~2 is a function h : ~1 -+ ~2 that is extended 
to operate on words h(al ... am) = h(al) ... h(am) and on languages h(L) = 
{h(w) I wE L}. It may be extended to operate on regular expressions: 
h(0) = 0 
h(A) = A 
h(a) = { 
b1 Â· â¢.. Â· bm if h(a) = b1 .â¢. bm 1\ m > 0 
A 
if h(a) = A 
h(Rl +R2) = h(Rt} +h(R2) 
h(RI . R2) = h(R1 ) . h(R2) 
h(R*) = h(R)* 
If R is a regular expression over ~l then it is immediate that h(R) is a regular 
expression over ~2 and that C(h(R)) = h(C(R)). 
Concluding Remarks 
There are a number of good books on graph theoretic concepts; our presen-
tation is based on [48], Chapter 5 of [4], Chapters 3 and 4 of [69], Chapter 
10 of [5] and Chapter 7 of [110]. For more information on regular languages 
consult a text book such as [76]. 

Index of Notation 
---* 
,212,217,218 
-t ,339 
-tfin 
, VIII 
-t 
, 17, 18,20 
~ , 322, 327, 345 
1= 
,307 
\7 
,224 
I , 152,284 
[ 
] 
,365 
,60 
C 
,296,346 
:::J 
,365 
C 
,296 
:::) 
,306 
I-
: 
,22 
1-: &,23 
I-
~ ,209,217 
I-
I> 
, 26, 210, 217 
I-
-t 
, 152 
I-* 
-t 
, 84 
L J, 285 
0,103 
a, 15, 298, 304 
a.", 235 
{3, 214, 304 
{3 ;2 <p, 306 
{3I ---* {32, 218 
{3R,215 
{3.", 235 
a, 89,94,95,97,98, 189,228 
8, 89, 189 
[8,Â£],94 
f8,Â£h, 96,191 
8, 185 
11, 235 
11[V], 244 
r, 22, 283 
r I- e : T, 22 
rlx,284 
r I- e : T & <P, 23 
r, 23, 285, 305,319 
"{, 15 
"{'I'll 235 
Â£,67 
Â£1,;, 64, 67 
Â£., 184 
Â£, 94, 95, 97, 98 
A, 342 
.x, VIII 
,J,.SA 114 
'1'Â£ , 
<p, 23,285, 304,319,325,333,341 
<p!;;;<p', 346 
<PI !;;; <P2, 296 
<PI ~ <P2, 296 
7l', 282 
t/J, 365 
p, 84, 151, 331 
p 1-* (S,<;) -t (S',c;'), 84 
p 1-* (S,<;) -t c;', 84 
pIX, 152 
p, 11, 142, 185, 189 
p*, 84 
{!, 329 
e, 330 
E,17 
a, 52, 103 
al "'v a2, 60 
a, 325, 333, 341 
c;, 84, 242, 318, 331 
c;[M], 242 

428 
~'1' 244 
T, 283, 298 
T ~ T', 322, 327, 345 
T1 !; T2, 296 
T, 23, 285, 304,319, 325, 333, 341 
(},299 
(}A 1= C, 307 
e, 84, 103, 318 
(, 325 
A, 53, 106 
Acp, 71 
a,3 
AEentry , 38 
AEe:z:it' 38 
AExp*, 37 
AExp, 3 
AExp(Â·),37 
AHeap, 109, 112 
ALoe, 108 
ALoc(H), 109 
ALoc(5),109 
Analysis=, 67 
Analysis!;, 67 
Analysis, 367 
Analysis, 63 
AnncA,341 
Ann, 285 
AnnEs, 325 
AnnRI,333 
AnnsE,319 
AState, 108, 112 
AType, 298 
AV, 99, 315 
AVar, 304 
B, 53,106 
b,3 
BExp, 3 
BExp(Â·),37 
bind-construct, 85, 152 
[B]l, 35 
Blocks*, 36 
blocks*, 82 
blocks, 34, 81 
C,306 
Index of Notation 
C, 11, 142, 185, 191 
C*, 172 
c, 140, 282 
C;dhe, 142, 185, 191 
ce,189 
CEnv, 189 
ch,337 
Chan, 337 
clear, 50 
close-construct, 151 
Const, 140, 282 
CP,100 
CF,340 
(5, 14 
cycles, 416 
!2*,80 
D,185 
d, 185 
dc, 180 
dop , 181 
Data, 180 
Ilata,185 
D~he,185 
deE, 50 
DEnv, 185 
dOIn, 84, 152, 284, 299 
DU,52 
du, 50 
E, 67, 339 
e1 -t e2, 339 
e, 140,281,337 
ee,329 
EExp, 329 
empty, 366 
Env, 84, 151, 331 
~, 142, 185, 189 
EVal, 331 
Exp, 140, 281, 337 
Ext, 221, 400 
extract, 366 
F,66 
:F,89 
Fcp, 71 
[f]a, 228 

Index of Notation 
i[, 91, 94, 96, 98, 99 
i[;r ' 91, 94, 96, 98, 99 
7;, 89 
r;:,90 
-r;:, 90 
lv, 224 
J'fA, 113, 114 
j., 184 
fl,66 
I p , 210,217 
IjP,71 
final*, 82 
final, 34, 81 
Fix, 221, 400 
Bow*, 82 
Bow, 35, 81 
Bo~, 36 
FV, 141,365 
genAE,38 
genLV, 48 
genRD,41 
genvB,44 
gfp, 221, 400 
H, 109,112 
11., 104 
Heap, 104 
lAV, 99 
id,66,300 
ie, 152 
IExp, 152 
inf,220 
infl, 367 
init*, 82 
init, 34, 81 
insert, 366 
inter-Bow*, 82 
Interval, 219 
is, 110, 112 
IsShared, 112 
it, 152 
ITerm, 152 
JUDGCFA[r I-UL e : r], 297 
killAE,38 
killLV, 47 
killRD,41 
killvB,44 
kill"" 115 
kill"'osel, 123 
L,89 
l, 3,140 
It,223 
Lab*, 36, 169 
Lab, 3, 140 
labe1t4, 82 
labels, 35, 81 
lip~, 228 
lip"",225 
lip, 221,400 
Loc, 84, 103, 318 
LV=, 58 
LV~, 58 
LV entry , 48 
LVe'JJit' 48 
MFP, 72 
MOP, 77 
MVP, 88 
.N,53 
nx,108 
n0,108 
n,3 
Num, 3 
0, 331 
Observe, 336 
Offset, 331 
Op, 140,282 
op, 180 
OP, 140,282 
oPa , 53, 71 
OPa , 4 
oPb,53 
oPb,4 
OPr, 53 
OPr , 4 
P*, 80 
p,105 
pI- h I> h, 210, 217 
pI- Vl ~ 
V2, 209, 217 
Parity, 242 
429 

430 
path, 77 
Pnt, 282 
PP,340 
Proc, 340 
R ~ R, 212 
Rl ~ R2 , 217 
R{3, 215 
R,212 
n,156 
r, 330 
r, 329, 341 
ran, 84 
Range, 246 
RDf-S [> S',26 
RD,5 
RDentry, 41 
RDexit, 41 
Red, 221, 400 
RegcA,341 
RegRI,329 
rn,329 
RName, 329 
rPostorder, 373 
RVar, 329 
S: RDI -+ RD2 , 18 
S: ~ :0) ~,20 
S: ~ -+~, 17 
S*, 36 
' 
S,3 
S, 108, 112 
S,365 
SAnn, 304 
SchemecA, 341 
SchemeR I , 333 
Sel,102 
sel, 102 
SG,112 
Shape, 113 
Sign, 236 
SRD,13 
State, 52, 103 
SWecp, 70 
Stmt, 3 
Store, 84, 318,331 
SType, 304 
sup, 220 
SVal,331 
[t]'I/!, 365 
t, 140,365 
T~v, 283 
TEnv, 285 
Term*, 169 
Term, 140 
tr, 13 
Trace, 13 
TVar, 298,304 
Type, 283 
TypecA,341 
TypeES' 325 
TypeRI,333 
TypesE,319 
Type[r], 296 
Type, 285 
UCFA,305 
UUL, 300, 303 
UD,52 
ud, 50 
use, 50 
V,157 
Index of Notation 
V, 142, 180, 185, 189 
v, 151, 291, 331, 339 
Val, 151, 291, 339 
Vaid,180 
Vai, 142, 185, 189 
Var*, 36,169 
Var, 3, 140, 282 
VBentry, 45 
VBexit, 45 
vpath, 88 
WCFA,307 
W,72 
WUL,299 
W,331 
(Xi ;;;) ti)~l' 365 

Index 
abstract O-CFA, 145 
abstract cache, 142, 191 
abstract data value, 180 
abstract environment, 142, 189 
abstract heap, 109 
Abstract Interpretation, 13, 209 
abstraction function, 15, 232 
abstract location, 107, 108 
abstract reachability component, 200 
abstract state, 108 
abstract summary location, 108 
abstract syntax, 4, 140 
abstract value, 142, 189 
abstract worklist algorithm, 367 
acceptable O-CFA, 144 
acyclic, 416 
additive, 393 
adjoint, 237 
adjunction, 15, 234 
affine, 394 
alphabet, 424 
ancestor, 418 
annotated type, 285, 319, 325, 333 
annotated type environment, 285 
Annotated Type System, 17, 281 
annotation, 285, 319, 325, 333 
annotation substitution, 307 
annotation variable, 304 
are, 415 
Array Bound Analysis, 219, 245 
ascending chain, 397 
Ascending Chain Condition, 65, 397 
assigned variable, 99 
assumption set, 97, 196 
atomic subtyping, 348 
augmented type, 298 
Available Expressions Analysis, 37 
back edge, 420 
backward analysis, 64, 83 
BANE, 385 
base of origin, 354 
basic block, 134 
behaviour, 342 
behaviour variable, 342 
binding time analysis, 354 
Bit Vector Framework, 134, 378, 
385 
call-by-name, 205 
call-by-result, 80 
call-by-value, 80 
Call-Tracking Analysis, 23 
called procedure, 100 
call string, 94, 194 
cartesian product, 396 
Cartesian Product Algorithm, 194 
chain, 397 
channel, 338 
channel identifier, 338 
channel pool, 340 
Chaotic Iteration, 25 
child,418 
closure, 151 
Closure Analysis, 197 
Code Motion, 51 
coinductive definition, 148, 163,410 
collecting semantics, 13, 264, 275 
combination operator, 65 
Communication Analysis, 337, 341 
compatible shape graph, 112 
complete lattice, 391 

432 
completely additive, 393 
completely multiplicative, 394 
complete path, 87 
concrete syntax, 4 
concretisation function, 15, 232 
conditional constraint, 12, 171,365 
conservative extension, 289 
Constant Folding, 26, 70, 130 
Constant Propagation Analysis, 70, 
130,184,210,263,264 
constraint, 171, 306 
constraint based O-CFA, 172 
Constraint Based Analysis, 139 
constraint based approach, 8, 10, 
11 
constraint system, 58, 67, 365 
context-insensitive, 93, 148, 188 
context-sensitive, 93, 188 
context environment, 189 
context information, 89, 189 
contravariant, 322 
Control Flow Analysis, 10, 211, 285, 
286 
correctness relation, 60, 156, 212, 
239, 258 
course of values induction, 405 
covariant, 322 
cover, 299, 307 
CPA, 194 
cross edge, 420 
cycle, 416 
DAG,416 
data array, 177 
Data Flow Analysis, 5, 33 
Dead Code Elimination, 47, 51,130 
Definition-Use chain, 50, 143 
definition clear path, 50 
Denotational Semantics, 221, 271 
depth-first spanning forest, 373, 420 
depth-first spanning tree, 420 
derivation sequence, 55 
descendant, 418 
descending chain, 397 
Index 
Descending Chain Condition, 66, 
397 
Detection of Signs Analysis, 89, 181 
DFSF, 420 
DFST, 420 
digraph, 415 
directed graph, 373, 415 
directed path, 415 
directly dominates, 419 
direct product, 252 . 
direct tensor product, 253 
distributive, 67, 393 
Distributive Framework, 67 
dominates, 418 
dominator, 418 
du-chain, 50, 130, 143 
duality, 271 
dynamic dispatch problem, 82,140 
edge, 415 
edge array, 177 
effect, 23, 319, 325, 333 
Effect System, 17 
elementary block, 3, 34 
embellished Monotone Framework, 
89 
environment, 84, 151, 330 
equality of annotations, 288 
equational approach, 5 
equation system, 57, 63, 64, 67, 
364 
equivalent, 425 
evaluation context, 340 
eventually stabilises, 397 
Exception Analysis, 323 
expression, 140,281,338 
extended expression, 329 
extended type, 333 
extensive, 221,400 
extraction function, 235, 244 
extremal label, 67 
extremal value, 67, 184 
faint variable, 133 
fast, 389 

Index 
fastness closure, 389 
FIFO,372 
final label, 34 
finite chain, 397 
first-in first-out, 372 
first-order analysis, 210 
fixed point, 221, 400 
flow, 35, 67 
flow-insensitive, 99, 148 
flow-sensitive, 99 
flow graph, 35 
flow variable, 363 
forest, 418 
forward analysis, 64, 83, 113 
forward edge, 420 
free algebra, 304 
free variable, 141 
fresh type variable, 300 
functional, 407 
functional composition, 245 
Galois connection, 15, 232 
Galois insertion, 240 
generalisation, 327, 335 
generalised Monotone Framework, 
260 
general subtyping, 348 
generated by, 215 
graph, 415 
graph formulation of the constraints, 
175,373 
greatest element, 392 
greatest fixed point, 221, 400 
greatest lower bound, 391 
ground substitution, 299, 307 
ground validation, 307 
handle, 373, 417 
heap, 104 
height, 397 
hoisting, 44 
in-degree, 418 
independent attribute analysis, 136, 
247 
induced analysis, 16, 256 
induction on the shape, 404 
inductive definition, 163, 406 
inequation system, 364 
influence set, 367 
initial label, 34 
injective, 393 
433 
instance, 67, 184, 260 
instantiation, 327, 337 
instrumented semantics, 131 
intermediate expression, 152 
intermediate term, 152 
interprocedural analysis, 80, 194 
interprocedural flow, 82, 140 
intraprocedural analysis, 80 
isolated entries, 37 
isolated exits, 37 
isomorphism, 395 
join semi-lattice, 65 
k-CFA, 189, 196 
label, 35 
label consistent, 37 
last-in first-out, 370 
latent effect, 23 
lazy, 205 
least element, 392 
least fixed point, 221, 259, 400 
least upper bound, 391 
LIFO,370 
live variable, 47 
Live Variables Analysis, 47 
location, 84, 103, 318 
logical relation, 212, 217 
loop connectedness, 378,422 
lower adjoint, 237 
lower bound, 391 
materialise, 119 
mathematical induction, 403 
Maximal Fixed Point, 72 
may analysis, 64, 113 
Meet Over all Paths, 76 

434 
Meet over all Valid Paths, 87 
MFP solution, 72 
model intersection property, 160 
Modified Post Correspondence Prob-
lem, 77 
monotone, 393 
Monotone Framework, 66, 389 
monotone function space, 251, 396 
monotone structure, 184, 273, 354 
Monotone Type System, 354 
monovariant, 188 
Moore family, 134, 160, 170, 213, 
297, 393 
MOP solution, 76 
multiple entry, 416 
multiplicative, 393 
must analysis, 64, 113 
MVP solution, 87, 88 
narrowing operator, 228 
Natural Semantics, 290 
node, 415 
non-free algebra, 304 
non-trivial expression, 37 
offset, 331 
open system, 147, 180 
optimal analysis, 17, 259 
ordered forest, 418 
ordered tree, 418 
out-degree, 418 
PAG,385 
parent, 418 
partially ordered set, 391 
partial ordering, 391 
path, 76, 415 
pointer expression, 102 
polymorphic recursion, 334 
polymorphism, 325, 327 
polynomial k-CFA, 193, 196 
polyvariant, 188 
procedure call graph, 100 
process, 338 
process pool, 340 
Index 
program point, 282 
proof by coinduction, 410 
proof by induction, 410 
proof normalisation, 310, 360 
proper, 418 
properly dominates, 419 
property space, 65 
Reaching Definitions Analysis, 4, 
41 
reduced graph, 379, 417 
reduced product, 254 
reduced tensor product, 254 
reducible, 422 
reduction operator, 242 
reductive, 221, 400 
reference variable, 317 
region, 329, 331, 343 
Region Inference, 328 
region name, 329 
region polymorphic closure, 331 
region variable, 329 
regular expression, 424 
relational analysis, 136, 248 
representation function, 214, 235, 
240, 258, 262 
reverse flow, 36 
reverse postorder, 373, 420 
root, 373, 417 
Round Robin Algorithm, 377 
safe approximation, 166 
scaling factor, 354 
second-order analysis, 210 
security analysis, 354 
selector name, 102 
self-loop, 415 
semantically reaching definitions, 
13 
semantic correctness, 293 
semantics based, 3, 29 
semantics directed, 3, 29 
sequential composition, 245 
Set Based Analysis, 196 
set constraints, 199, 385 

Index 
sets of states analysis, 263 
Shape Analysis, 102, 272 
shape conformant subtyping, 322, 
348 
shape graph, 107, 112 
sharing information, 110 
Sharlit, 385 
Side Effect Analysis, 318 
simple annotation, 304 
simple substitution, 305 
simple type, 304 
simple type environment, 305 
smash product, 396 
Spare, 385 
state, 52, 103 
storable value, 331 
store, 84, 318, 331 
strict, 394 
strong component, 379, 417 
strongly connected, 379, 417 
structural induction, 404 
Structural Operational Semantics, 
52, 151 
subeffecting, 286, 322, 327, 335, 
346 
subject reduction result, 156 
substitution, 299 
subsumption rule, 19, 21 
subtyping, 322,327,335, 346 
surjective, 393 
syntactic completeness, 306, 312 
syntactic soundness, 306, 310 
syntax directed O-CFA, 167 
System Z, 385 
Tarski's Fixed Point Theorem, 400 
temporal order, 337 
tensor product, 250, 269 
term, 140 
topological sort, 416 
total function space, 250, 396 
trace, 13, 131 
transfer function, 66 
transition, 52 
tree, 418 
tree edge, 420 
type, 22, 283, 342 
435 
Type and Effect System, 17, 281 
type environment, 22, 283 
type reconstruction, 299, 307 
type schema, 325, 333, 343 
type substitution, 306 
type variable, 298, 304 
typing judgement, 284, 286, 319, 
327, 334, 344 
UCAl, 289, 304 
ud-chain, 50, 130 
underlying type, 285 
underlying type system, 282 
unification procedure, 300, 303, 305 
uniform k-CFA, 189, 191, 196 
unit, 354 
upper adjoint, 237 
upper bound, 391 
upper bound operator, 223 
upper closure operator, 268 
Use-Definition chain, 50 
valid path, 87 
value, 151, 291, 339 
vertex, 415 
Very Busy Expressions Analysis, 
44 
well-founded induction, 405 
well-founded ordering, 405 
widening operator, 224, 265, 389 
worklist, 72, 177,366,367 
Y2K,354 

Bibliography 
[1] M. Abadi, A. Banerjee, N. Heintze, and J. G. Riecke. A core calculus 
of dependency. In Proc. POPL '99, pages 147-160. ACM Press, 1999. 
[2] O. Agesen. The cartesian product algorithm. In Proc. ECOOP'95, 
volume 952 of Lecture Notes in Computer Science, pages 2-26. Springer, 
1995. 
[3] O. Agesen, J. Palsberg, and M. Schwartzbach. 
Type inference of 
SELF: Analysis of objects with dynamic and multiple inheritance. In 
Proc. ECOOP'93, volume 707 of Lecture Notes in Computer Science. 
Springer, 1993. 
[4] A. V. Aho, J. E. Hopcroft, and J. D. Ullman. The Design and Analysis 
of Computer Algorithms. Addison Wesley, 1974. 
[5] A. V. Aho, R. Sethi, and J. D. Ullman. Compilers: Principles, Tech-
niques, and Tools. Addison Wesley, 1986. 
[6] A. Aiken. Set constraints: Results, applications and future directions. 
In Proc. Second Workshop on the Principles and Practice of Constraint 
Programming, volume 874 of Lecture Notes in Computer Science, pages 
326-335. Springer, 1994. 
[7] A. Aiken. An introduction to set constraint-based program analysis. 
Science of Computer Programming, to appear. 
[8] A. Aiken, M. Fiihndrich, J. S. Foster, and Z. Suo A toolkit for con-
structing type- and constraint-based program analyses. In Proc. Types 
in Compilation, volume 1473 of Lecture Notes in Computer Science, 
pages 78-96. Springer, 1998. 
[9] A. Aiken and E. Wimmers. Type inclusion constraints and type infer-
ence. In Proc. FPCA '93, pages 31-41. ACM Press, 1993. 
[10] A. Aiken, E. Wimmers, and T.K. Lakshman. Soft typing with condi-
tional types. In Proc. POPL '94, pages 163-173. ACM Press, 1994. 

438 
Bibliography 
[11] F. E. Allen and J. A. Cocke. A program data flow analysis procedure. 
Communications of the ACM, 19(3):137-147,1976. 
[12] B. Alpern, M. M. Wegman, and F. K. Zadeck. Detecting equality of 
variables in programs. In Proc. POPL '88, pages 1-11. ACM Press, 
1988. 
[13] T. Amtoft, F. Nielson, and H. R. Nielson. Type and Effect Systems: 
Behaviours for Concurrency. Imperial College Press, 1999. 
[14] T. Amtoft, F. Nielson, and H.R. Nielson. Type and behaviour recon-
struction for higher-order concurrent programs. Journal of Functional 
Programming, 7(3):321-347,1997. 
[15] T. Amtoft, F. Nielson, H.R. Nielson, and J. Ammann. Polymorphic 
subtyping for effect analysis: The dynamic semantics. In Analysis and 
Verification of Multiple-Agent Languages, volume 1192 of Lecture Notes 
in Computer Science, pages 172-206. Springer, 1997. 
[16] A. Arnold and 1. Guessarian. Mathematics for Computer Science. Pren-
tice Hall International, 1996. 
[17] U. Assmann. How to uniformly specify program analysis and transfor-
mation. In Proc. CC '96, volume 1060 of Lecture Notes in Computer 
Science, pages 121-135. Springer, 1996. 
[18] A. Banerjee. A modular, polyvariant, and type-based closure analysis. 
In Proc. IeFP '97, pages 1-10. ACM Press, 1997. 
[19] P. N. Benton. Strictness logic and polymorphic invariance. In Proc. Sec-
ond International Symposium on Logical Foundations of Computer Sci-
ence, volume 620 of Lecture Notes in Computer Science, pages 33-44. 
Springer, 1992. 
[20] P. N. Benton. Strictness properties of lazy algebraic datatypes. In 
Proc. WSA '93, volume 724 of Lecture Notes in Computer Science, 
pages 206-217. Springer, 1993. 
[21] S. K. Biswas. A demand-driven set-based analysis. In Proc. POPL '97, 
pages 372-385. ACM Press, 1997. 
[22] C. Bodei, P. Degano, F. Nielson, and H. R. Nielson. Control flow 
analysis for the 'if-calculus. In Proc. CONCUR '98, number 1466 in 
Lecture Notes in Computer Science, pages 84-98. Springer, 1998. 
[23] C. Bodei, P. Degano, F. Nielson, and H. R. Nielson. Static analysis of 
processes for no read-up and no write-down. In Proc. FOSSACS'99, 
number 1578 in Lecture Notes in Computer Science, pages 120-134. 
Springer, 1999. 

Bibliography 
439 
[24] F. Bourdoncle. Abstract interpretation by dynamic partitioning. Jour-
nal of Functional Programming, 10:407-435,1992. 
[25] F. Bourdoncle. Efficient chaotic iteration strategies with widenings. In 
Proc. Formal Methods in Programming and Their Applications, volume 
735 of Lecture Notes in Computer Science, pages 128-141. Springer, 
1993. 
[26] R. E. Bryant. Symbolic boolean manipulation with ordered binary 
decision diagrams. Computing Surveys, 24(3), 1992. 
[27] G. L. Burn, C. Hankin, and S. Abramsky. Strictness Analysis for 
Higher-Order Functions. Science of Computer Programming, 7:249-
278, 1986. 
[28] W. Charatonik and L. Pacholski. Set constraints with projections are 
in NEXPTIME. In Proc. FOCS 'g4, pages 642-653, 1994. 
[29] B. Le Charlier and P. Van Hentenryck. Experimental evaluation of a 
generic abstract interpretation algorithm for prolog. ACM TOPLAS, 
16(1):35-101,1994. 
[30] D. Chase, M. Wegman, and F. Zadeck. Analysis of pointers and struc-
tures. In Proc. PLDI '90, pages 296-310. ACM Press, 1990. 
[31] C. Colby. Analyzing the communication topology of concurrent pro-
grams. In Proc. PEPM '95, pages 202-214. ACM Press, 1995. 
[32] C. Colby. Determining storage properties of sequential and concurrent 
programs with assignment and structured data. In Proc. SAS '95, vol-
ume 983 of Lecture Notes in Computer Science, pages 64-81. Springer, 
1995. 
[33] A. Cortesi, G. File, R. Giacobazzi, C. Palamidessi, and F. Ranzato. 
Complementation in abstract interpretation. In Proc. SAS '95, Lecture 
Notes in Computer Science, pages 100-117. Springer, 1995. 
[34] A. Cortesi, G. File, R. Giacobazzi, C. Palamidessi, and F. Ranzato. 
Complementation in abstract interpretation. ACM TOPLAS, 19(1):7-
47,1997. 
[35] P. Cousot. Semantics Foundation of Program Analysis. In S. S. Much-
nick and N. D. Jones, editors, Program Flow Analysis: Theory and 
Applications, chapter 10, pages 303-342. Prentice Hall International, 
1981. 
[36] P. Cousot. Types as abstract interpretations. In Proc. POPL '97, pages 
316-331. ACM Press, 1997. 

440 
Bibliography 
[37] P. Cousot and R. Cousot. Abstract Interpretation: a Unified Lattice 
Model for Static Analysis of Programs by Construction or Approxi-
mation of Fixpoints. In Pmc. POPL '77, pages 238-252. ACM Press, 
1977. 
[38] P. Cousot and R. Cousot. Static determination of dynamic properties of 
generalised type unions. In Conference on Language Design for Reliable 
Software, volume 12(3) of ACM SIGPLAN Notices, pages 77-94, 1977. 
[39] P. Cousot and R. Cousot. Systematic Design of Program Analysis 
Frameworks. In Pmc. POPL '79, pages 269-282, 1979. 
[40] P. Cousot and R. Cousot. 
Comparing the Galois Connection and 
Widening/Narrowing Approaches to Abstract Interpretation. 
In 
Pmc. PLILP '92, volume 631 of Lecture Notes in Computer Science, 
pages 269-295. Springer, 1992. 
[41] P. Cousot and N. Halbwachs. Automatic Discovery of Linear Restraints 
Among Variables of a Program. In Pmc. POPL '78, pages 84-97. ACM 
Press, 1978. 
[42] R. Cytron, J. Ferrante, B. K. Rosen, M. N. Wegman, and F. K. Zadek. 
Efficiently computing static single assignment form and the control 
dependence graph. ACM TOPLAS, 13(4):451-490,1991. 
[43] B. A. Davey and H. A. Priestley. Introduction to Lattices and Or-
der. Cambridge Mathematical Textbooks. Cambridge University Press, 
1990. 
[44] A. Deutsch. On Determining Lifetime and Aliasing of Dynamically Al-
located Data in Higher Order Functional Specifications. In Pmc. POPL 
'90, pages 157-169. ACM Press, 1990. 
[45] A. Deutsch. Interprocedural may-alias analysis for pointers: Beyond 
k-limiting. In Pmc. PLDI '94, pages 230-241. ACM Press, 1994. 
[46] P. H. Eidorf, F. Henglein, C. Mossin, H. Niss, M. H. S0rensen, and 
M. Tofte. AnnoDomini: From type theory to year 2000 conversion 
tool. In Pmc. POPL '99, pages 1-14. ACM Press, 1999. 
[47] M. Emami, R. Ghiya, and L. J. Hendren. Context-sensitive inter-
procedural points-to analysis in the presence of function pointers. In 
Pmc. PLDI '94, pages 242-256. ACM Press, 1994. 
[48] S. Even. Graph Algorithms. Pitman, 1979. 
[49] K.-F. Faxen. Optimizing lazy functional programs using flow inference. 
In Pmc. SAS '95, volume 983 of Lecture Notes in Computer Science, 
pages 136-153. Springer, 1995. 

Bibliography 
441 
[50] K.-F. Faxen. 
Polyvariance, polymorphism, and flow analysis. 
In 
Proc. Analysis and Verification of Multiple-Agent Languages, volume 
1192 of Lecture Notes in Computer Science, pages 260-278. Springer, 
1997. 
[51] C. Fecht and H. Seidl. An even faster solver for general systems of 
equations. In Proc. SAS '96, volume 1145 of Lecture Notes in Computer 
Science, pages 189-204. Springer, 1996. 
[52] C. Fecht and H. Seidl. Propagating differences: An efficient new fix-
point algorithm for distributive constraint systems. In Proc. ESOP 
'98, volume 1381 of Lecture Notes in Computer Science, pages 90-104. 
Springer, 1998. 
[53] C. Fecht and H. Seidl. Propagating differences: An efficient new fix-
point algorithm for distributive constraint systems. Nordic Journal of 
Computing, 5:304-329, 1998. 
[54] C. Fecht and H. Seidl. A faster solver for general systems of equations. 
Science of Computer Programming, to appear. 
[55] C. N. Fischer and Jr. R. J. LeBlanc. 
Crafting a Compiler. 
Ben-
jamin/Cummings, 1988. 
[56] C. Flanagan and M. Felleisen. Well-founded touch optimizations for fu-
tures. Technical Report Rice COMP TR94-239, Rice University, 1994. 
[57] C. Flanagan and M. Felleisen. The semantics of future and its use in 
program optimization. In Proc. POPL '95, pages 209-220. ACM Press, 
1995. 
[58] Y.-C. Fuh and P. Mishra. Polymorphic subtype inference: Closing the 
theory-practice gap. In Proc. TAPSOFT '89, volume 352 of Lecture 
Notes in Computer Science, pages 167-183. Springer, 1989. 
[59] Y.-C. Fuh and P. Mishra. Type inference with subtypes. Theoretical 
Computer Science, 73:155-175, 1990. 
[60] K. L. S. Gasser, F. Nielson, and H. R. Nielson. Systematic realisation of 
control flow analyses for CML. In Proc. JCFP '97, pages 38-51. ACM 
Press, 1997. 
[61] R. Ghiya and L. Hendren. Connection analysis: a practical interproce-
dural analysis for C. In Proc. of the eight workshop on languages and 
compilers for parallel computing, 1995. 
[62] R. Ghiya and L. J. Hendren. Is it a tree, a dag, or a cyclic graph? 
a shape analysis for heap-directed pointers in C. In G. Kahn, editor, 
Proc. POPL '96, pages 1-15. ACM Press, 1996. 

442 
Bibliography 
[63] R. Giacobazzi and F. Ranzato. Compositional optimization of dis-
junctive abstract interpretations. In Pmc. ESOP '96, volume 1058 of 
Lecture Notes in Computer Science, pages 141-155. Springer, 1996. 
[64] R. Giacobazzi and F. Ranzato. Optimal domains for disjunctive ab-
stract interpretation. Science of Computer Programming, 32:177-210, 
1998. 
[65] R. Giegerich, U. M6ncke, and R. Wilhelm. Invariance of approximative 
semantics with respect to program transformations. In Pmc. GI-
11. Jahrestagung, volume 50 of Informatik Fachberichte, pages 1-10. 
Springer, 1981. 
[66] P. Granger. Static analysis of arithmetical congruences. International 
Journal of Computer Mathematics, 30:165-190,1989. 
[67] P. Granger. Static Analysis of Linear Congruence Equalities among 
Variables of a Program. In Pmc. TAPSOFT '91, volume 493 of Lecture 
Notes in Computer Science, pages 169-192. Springer, 1991. 
[68] R. R. Hansen, J. G. Jensen, F. Nielson, and H. R. Nielson. Abstract 
interpretation of mobile ambients. In Pmc. SAS '99, Lecture Notes in 
Computer Science. Springer, 1999. 
[69] M. S. Hecht. Flow Analysis of Computer Programs. North Holland, 
1977. 
[70] N. Heintze. Set-based analysis of ML programs. In Pmc. LFP '94, 
pages 306-317,1994. 
[71] N. Heintze. Control-flow analysis and type systems. In Proc. SAS 
'95, volume 983 of Lecture Notes in Computer Science, pages 189-206. 
Springer, 1995. 
[72] N. Heintze and J. Jaffar. A decision procedure for a class of Herbrand 
set constraints. In Pmc. LICS '90, pages 42-51, 1990. 
[73] N. Heintze and J. Jaffar. An engine for logic program analysis. In 
Pmc. LICS '92, pages 318-328, 1992. 
[74] Nevin Heintze and Jon G. Riecke. The SLam calculus: Programming 
with Secrecy and Integrity. In Pmc. POPL '98, pages 365-377. ACM 
Press, 1998. 
[75] F. Henglein and C. Mossin. Polymorphic binding-time analysis. In 
Pmc. ESOP '94, volume 788 of Lecture Notes in Computer Science, 
pages 287-301. Springer, 1994. 
[76] J. E. Hopcroft and J. D. Ullman. Introduction to Automata Theory, 
Languages and Computation. Addison Wesley, 1979. 

Bibliography 
443 
[77] S. Horwitz, A. Demers, and T. Teitelbaum. An efficient general iterative 
algorithm for dataflow analysis. Acta Informatica, 24:679-694, 1987. 
[78] S. Jagannathan and S. Weeks. Analyzing Stores and References in a 
Parallel Symbolic Language. In Proc. LFP '94, pages 294-305, 1994. 
[79] S. Jagannathan and S. Weeks. A unified treatment of flow analysis in 
higher-order languages. In Proc. POPL '95. ACM Press, 1995. 
[80] S. Jagannathan and A. Wright. Effective flow analysis for avoiding 
run-time checks. In Proc. SAS '95, volume 983 of Lecture Notes in 
Computer Science, pages 207-224. Springer, 1995. 
[81] T. P. Jensen. Strictness analysis in logical form. 
In Proc. FPC A 
'91, volume 523 of Lecture Notes in Computer Science, pages 352-366. 
Springer, 1991. 
[82] T. P. Jensen. Disjunctive strictness analysis. In Proc. LICS '92, pages 
174-185,1992. 
[83] M. P. Jones. A theory of qualified types. In Proc. ESOP '92, volume 
582 of Lecture Notes in Computer Science, pages 287-306. Springer, 
1992. 
[84] N. D. Jones and S. S. Muchnick. Flow analysis and optimization of Lisp-
like structures. In S. S. Muchnick and N. D. Jones, editors, Program 
Flow Analysis: Theory and Applications, chapter 4, pages 102-131. 
Prentice Hall International, 1981. 
[85] N. D. Jones and S. S. Muchnick. A flexible approach to interprocedural 
data flow analysis and programs with recursive data structures. In 
Proc. POPL '82, pages 66-74. ACM Press, 1982. 
[86] N. D. Jones and F. Nielson. Abstract Interpretation: a Semantics-
Based Tool for Program Analysis. In Handbook of Logic in Computer 
Science volume 4. Oxford University Press, 1995. 
[87] M. Jourdan and D. Parigot. Techniques for improving grammar flow 
analysis. In Proc. ESOP '90, volume 432 of Lecture Notes in Computer 
Science, pages 240-255. Springer, 1990. 
[88] P. Jouvelot. Semantic Parallelization: a practical exercise in abstract 
interpretation. In Proc. POPL '87, pages 39-48, 1987. 
[89] P. Jouvelot and D. K. Gifford. Reasoning about continuations with 
control effects. In Proc. PLDI '89, ACM SIGPLAN Notices, pages 
218-226. ACM Press, 1989. 
[90] P. Jouvelot and D. K. Gifford. Algebraic reconstruction of types and 
effects. In Proc. POPL '91, pages 303-310. ACM Press, 1990. 

444 
Bibliography 
[91] G. Kahn. Natural semantics. In Pmc. STACS'87, volume 247 of Lecture 
Notes in Computer Science, pages 22-39. Springer, 1987. 
[92] J. B. Kam and J. D. Ullman. Global data flow analysis and iterative 
algorithms. Journal of the ACM, 23:158-171, 1976. 
[93] J. B. Kam and J. D. Ullman. Monotone data flow analysis frameworks. 
Acta Informatica, 7:305-317,1977. 
[94] M. Karr. Affine Relationships among Variables of a Program. Acta 
Informatica, 6(2):133-151, 1976. 
[95] A. Kennedy. Dimension types. In Proc. ESOP '94, volume 788 of 
Lecture Notes in Computer Science, pages 348-362. Springer, 1994. 
[96] G. Kildall. A Unified Approach to Global Program Optimization. In 
Pmc. POPL '73, pages 194-206. ACM Press, 1973. 
[97] M. Klein, J. Knoop, D. Koschiitzki, and B. Steffen. DFA & OPT-
METAFrame: A toolkit for program analysis and optimisation. In 
Pmc. TACAS '96, volume 1055 of Lecture Notes in Computer Science, 
pages 422-426. Springer, 1996. 
[98] D. E. Knuth. An empirical study of Fortran programs. Software-
Practice and Experience, 1:105-133, 1971. 
[99] W. Landi and B. G. Ryder. Pointer-Induced Aliasing: A Problem 
Classification. In Proc. POPL '91, pages 93-103. ACM Press, 1991. 
[100] W. Landi and B. G. Ryder. A safe approximate algorithm for inter-
procedural pointer aliasing. In Pmc. PLDI '92, pages 235-248. ACM 
Press, 1992. 
[101] J. Larus and P. Hilfinger. Detecting conflicts between structure ac-
cesses. In Proc. PLDI '88, pages 21-34. ACM Press, 1988. 
[102] J. M. Lucassen and D. K. Gifford. Polymorphic effect analysis. In 
Proc. POPL '88, pages 47-57. ACM Press, 1988. 
[103] T. J. Marlowe and B. G. Ryder. Properties of data flow frameworks -
a unified model. Acta Informatica, 28(2):121-163, 1990. 
[104] F. Martin. Pag - an efficient program analyzer generator. Journal of 
Software Tools for Technology Transfer, 2(1):46-67, 1998. 
[105] F. Masdupuy. Using Abstract Interpretation to Detect Array Data 
Dependencies. In Proc. International Symposium on Supercomputing, 
pages 19-27, 1991. 

Bibliography 
445 
[106] R. Milner. Communication and Concurrency. Prentice Hall Interna-
tional, 1989. 
[107] J. Mitchell. Type inference with simple subtypes. Journal of Functional 
Programming, 1(3):245-285, 1991. 
[108] B. Monsuez. Polymorphic types and widening operators. In Proc. Static 
Analysis (WSA '93), volume 724 of Lecture Notes in Computer Science, 
pages 267-281. Springer, 1993. 
[109] R. Morgan. Building an Optimisng Compiler. Digital Press, 1998. 
[110] S. Muchnick. Advanced Compiler Design and Implementation. Morgan 
Kaufmann Publishers, 1997 (third printing). 
[111] F. Nielson. Abstract Interpretation using Domain Theory. PhD thesis, 
University of Edinburgh, Scotland, 1984. 
[112] F. Nielson. Program Transformations in a denotational setting. ACM 
TOPLAS, 7:359-379, 1985. 
[113] F. Nielson. Tensor Products Generalize the Relational Data Flow Anal-
ysis Method. In Proc. 4th Hungarian Computer Science Conference, 
pages 211-225, 1985. 
[114] F. Nielson. A formal type system for comparing partial evaluators. 
In D. Bj!llrner, A. P. Ershov, and N. D. Jones, editors, Proc. Partial 
Evaluation and Mixed Computation, pages 349-384. North Holland, 
1988. 
[115] F. Nielson. Two-Level Semantics and Abstract Interpretation. Theo-
retical Computer Science -
Fundamental Studies, 69:117-242, 1989. 
[116] F. Nielson. 
The typed A-calculus with first-class processes. 
In 
Proc. PARLE'B9, volume 366 of Lecture Notes in Computer Science, 
pages 355-373. Springer, 1989. 
[117] F. Nielson. Semantics-directed program analysis: a tool-maker's per-
spective. In Proc. Static Analysis Symposium (SAS), number 1145 in 
Lecture Notes in Computer Science, pages 2-21. Springer, 1996. 
[118] F. Nielson and H. R. Nielson. Finiteness Conditions for Fixed Point 
Iteration. In Proc. LFP '92, pages 96-108. ACM Press, 1992. 
[119] F. Nielson and H. R. Nielson. From CML to process algebras. In Proc. 
CONCUR '93, volume 715 of Lecture Notes in Computer Science, pages 
493-508. Springer, 1993. 
[120] F. Nielson and H. R. Nielson. From CML to its process algebra. The-
oretical Computer Science, 155:179-219, 1996. 

446 
Bibliography 
[121] F. Nielson and H. R. Nielson. Operational semantics of termination 
types. Nordic Journal of Computing, pages 144-187, 1996. 
[122] F. Nielson and H. R. Nielson. Infinitary Control Flow Analysis: a 
Collecting Semantics for Closure Analysis. In Proc. POPL '97. ACM 
Press, 1997. 
[123] F. Nielson and H. R. Nielson. A prescriptive framework for designing 
multi-level lambda-calculi. In Proc. PEPM'97, pages 193-202. ACM 
Press, 1997. 
[124] F. Nielson and H. R. Nielson. The flow logic of imperative objects. In 
Proc. MFCS'98, number 1450 in Lecture Notes in Computer Science, 
pages 220-228. Springer, 1998. 
[125] F. Nielson and H. R. Nielson. Flow logics and operational semantics. 
Electronic Notes of Theoretical Computer Science, 10, 1998. 
[126] F. Nielson and H. R. Nielson. Interprocedural control flow analysis. In 
Proc. ESOP '99, number 1576 in Lecture Notes in Computer Science, 
pages 20-39. Springer, 1999. 
[127] F. Nielson, H.R. Nielson, and T. Amtoft. Polymorphic subtyping for 
effect analysis: The algorithm. In Analysis and Verification of Multiple-
Agent Languages, volume 1192 of Lecture Notes in Computer Science, 
pages 207-243. Springer, 1997. 
[128] H. R. Nielson, T. Amtoft, and F. Nielson. Behaviour analysis and safety 
conditions: a case study in CML. In Proc. FASE '98, number 1382 in 
Lecture Notes in Computer Science, pages 255-269. Springer, 1998. 
[129] H. R. Nielson and F. Nielson. Bounded fixed-point iteration. Journal 
of Logic and Computation, 2(4):441-464, 1992. 
[130] H. R. Nielson and F. Nielson. Semantics with Applications: A Formal 
Introduction. Wiley, 1992. (See http://www.daimi.au.dk/rvhrn for 
the availability of an on-line version.). 
[131] H. R. Nielson and F. Nielson. Higher-Order Concurrent Programs with 
Finite Communication Topology. In Proc. POPL '94. Springer, 1994. 
[132] H. R. Nielson and F. Nielson. Communication analysis for Concur-
rent ML. In F. Nielson, editor, ML with Concurrency, Monographs in 
Computer Science, pages 185-235. Springer, 1997. 
[133] H. R. Nielson and F. Nielson. Flow logics for constraint based analysis. 
In Proc. CC '98, volume 1383 of Lecture Notes in Computer Science, 
pages 109-127. Springer, 1998. 

Bibliography 
447 
[134] H.R. Nielson, F. Nielson, and T. Amtoft. Polymorphic subtyping for 
effect analysis: The static semantics. In Analysis and Verification of 
Multiple-Agent Languages, volume 1192 of Lecture Notes in Computer 
Science, pages 141-171. Springer, 1997. 
[135] L. Pacholski and A. Podelski. Set constraints: A pearl in research on 
constraints. In Proc. Third International Conference on the Principles 
and Practice of Constraint Programming, volume 1330 of Lecture Notes 
in Computer Science, pages 549-561. Springer, 1997. 
[136] J. Palsberg. Closure analysis in constraint form. ACM TOPLAS, 17 
(1):47-62, 1995. 
[137] J. Palsberg and M. I. Schwartzbach. Object-Oriented Type Systems. 
Wiley, 1994. 
[138] H. D. Pande and B. G. Ryder. Data-flow-based virtual function reso-
lution. In Proc. SAS '96, volume 1145 of Lecture Notes in Computer 
Science, pages 238-254. Springer, 1996. 
[139] J. Plevyak and A. A. Chien. Precise concrete type inference of object-
oriented programs. In Proc. OOPSLA '94, 1994. 
[140] G. D. Plotkin. A structural approach to operational semantics. Tech-
nical Report FN-19, DAIMI, Aarhus University, Denmark, 1981. 
[141] G. Ramalingam, J. Field, andF. Tip. Aggregate structure identification 
and its application to program analysis. In Proc. POPL '99, pages 119-
132. ACM Press, 1999. 
[142] J. H. Reif and S. A. Smolka. Data Flow Analysis of Distributed Com-
municating Processes. International Journal of Parallel Programming, 
19(1):1-30, 1990. 
[143] J. Reynolds. Automatic computation of data set definitions. In Infor-
mation Processing, volume 68, pages 456-461. North Holland, 1969. 
[144] B. K. Rosen. High-level data flow analysis. Communications of the 
ACM, 20(10):141-156,1977. 
[145] E. Ruf. Context-insensitive alias analysis reconsidered. In Proc. PLDI 
'95, pages 13-22. ACM Press, 1995. 
[146] B. G. Ryder and M. C. Paull. Elimination algorithms for data flow 
analysis. ACM Computing Surveys, 18(3):275-316,1986. 
[147] M. Sagiv, T. Reps, and S. Horwitz. Precise interprocedural dataflow 
analysis with applications to constant propagation. In Proc. TAPSOFT 
'95, volume 915 of Lecture Notes in Computer Science, pages 651-665, 
1995. 

448 
Bibliography 
[148] M. Sagiv, T. Reps, and R. Wilhelm. Solving shape-analysis problems in 
languages with destructive updating. In Proc. POPL '96, pages 16-3l. 
ACM Press, 1996. 
[149] M. Sagiv, T. Reps, and R. Wilhelm. Solving shape-analysis problems 
in languages with destructive updating. ACM TOPLAS, 20(1):1-50, 
1998. 
[150] M. Sagiv, T. Reps, and R. Wilhelm. Parametric shape analysis via 
3-valued logic. In Proc. POPL '99, pages 105-118. ACM Press, 1999. 
[151] D. Schmidt. Data flow analysis is model checking of abstract interpre-
tations. In Proc. POPL '98, pages 38-48. ACM Press, 1998. 
[152] P. Sestoft. Replacing function parameters by global variables. Master's 
thesis, Department of Computer Science, University of Copenhagen, 
Denmark, 1988. 
[153] M. Shapiro and S. Horwitz. Fast and accurate flow-insensitive points-to 
analysis. In Proc. POPL '97, pages 1-14. ACM Press, 1997. 
[154] M. Sharir. Structural Analysis: a New Approach to Flow Analysis in 
Optimising Compilers. Computer Lan9uages, 5:141-153, 1980. 
[155] M. Sharir and A. Pnueli. Two approaches to interprocedural data flow 
analysis. In S. S. Muchnick and N. D. Jones, editors, Program Flow 
Analysis. Prentice Hall International, 1981. 
[156] O. Shivers. Control flow analysis in Scheme. In Proc. PLDI '88, volume 
7 (1) of ACM SIGPLAN Notices, pages 164-174. ACM Press, 1988. 
[157] O. Shivers. Data-flow analysis and type recovery in Scheme. In P.Lee, 
editor, In Topics in Advanced Language Implementation, pages 47-87. 
MIT Press, 1991. 
[158] O. Shivers. 
The semantics of Scheme control-flow analysis. 
In 
Proc. PEPM '91, volume 26 (9) of ACM SIGPLAN Notices. ACM 
Press, 1991. 
[159] J. H. Siekmann. Unification theory. Journal of Symbolic Computation, 
7:207-274, 1989. 
[160] G. S. Smith. Polymorphic type inference with overloading and subtyp-
ing. In Proc. TAPSOFT '93, volume 668 of Lecture Notes in Computer 
Science, pages 671-685. Springer, 1993. 
[161] G. S. Smith. Principal type schemes for functional programs with over-
loading and subtyping. Science of Computer Programming, 23:197-226, 
1994. 

Bibliography 
449 
[162] B. Steensgaard. Points-to analysis in almost linear time. In Proc. POPL 
'96, pages 32-41. ACM Press, 1996. 
[163] D. Stefanescu and Y. Zhou. An equational framework for the flow 
analysis of higher order functional programs. In Proc. LFP '94, pages 
318-327, 1994. 
[164] B. Steffen. Generating data flow analysis algorithms from modal spec-
ifications. Science of Computer Programming, 21:115-239,1993. 
[165] J. Stransky. A lattice for abstract interpretation of dynamic (lisp-like) 
structures. Information and Computation, 1990. 
[166] J .-P. Talpin and P. Jouvelot. Polymorphic Type, Region and Effect 
Inference. Journal of Functional Programming, 2(3):245-271, 1992. 
[167] J.-P. Talpin and P. Jouvelot. 
The type and effect discipline. 
In 
Proc. LICS '92, pages 162-173, 1992. 
[168] J.-P. Talpin and P. Jouvelot. The type and effect discipline. Information 
and Computation, 111(2):245-296, 1994. 
[169] Y.-M. Tang. Control-Flow Analysis by Effect Systems and Abstract 
Interpretation. PhD thesis, Ecole des Mines de Paris, 1994. 
[170] R. E. Tarjan. Fast algorithms for solving path problems. Journal of 
the ACM, 28(3):594-614, 1981. 
[171] R. E. Tarjan. A unified approach to path programs. Journal of the 
ACM, 28(3):577-593, 1981. 
[172] S. Tjiang and J. Hennessy. Sharlit - a tool for building optimizers. In 
Proc. PLDI '92. ACM Press, 1992. 
[173] M. Tofte. Type inference for polymorphic references. Information and 
Computation, 89:1-34, 1990. 
[174] M. Tofte and L. Birkedal. 
A region inference algorithm. 
ACM 
TOPLAS, 20(3):1-44, 1998. 
[175] M. Tofte and J.-P. Talpin. Implementing the call-by-value lambda-
calculus using a stack of regions. In Proc. POPL '94, pages 188-201. 
ACM Press, 1994. 
[176] M. Tofte and J.-P. Talpin. Region-based memory management. Infor-
mation and Computation, 132:109-176,1997. 
[177] G. V. Venkatesh and C. N. Fischer. Spare: A development environ-
ment for program analysis algorithms. IEEE Transactions on Software 
Engineering, 1992. 

450 
Bibliography 
[178] J. Vitek, R. N. Horspool, and J. S. Uhl. Compile-Time Analysis of 
Object-Oriented Programs. In Proc. CC '92, volume 641 of Lecture 
Notes in Computer Science, pages 236-250. Springer, 1992. 
[179] A. B. Webber. Program analysis using binary relations. In Proc. PLDI 
'97, volume 32 (5) of ACM SIGPLAN Notices, pages 249-260. ACM 
Press, 1997. 
[180] M. N. Wegman and F. K. Zadeck. Constant propagation with condi-
tional branches. ACM TOPLAS, pages 181-210, 1991. 
[181] R. Wilhelm and D. Maurer. Compiler Design. Addison-Wesley, 1995. 
[182] R. P. Wilson and M. S. Lam. Efficient context-sensitive pointer analysis 
for C programs. In Proc. PLDI '95, pages 1-12. ACM Press, 1995. 
[183] A. K. Wright. Typing references by effect inference. In Proc. ESOP 
'92, volume 582 of Lecture Notes in Computer Science, pages 473-491. 
Springer, 1992. 
[184] A. K. Wright and M. Felleisen. A syntactic approach to type soundness. 
Information and Computation, 115:38-94, 1994. 
[185] K. Yi and W. L. Harrison III. Automatic generation and management 
of interprocedural program analyses. In Proc. POPL '93, pages 246-
259. ACM Press, 1993. 


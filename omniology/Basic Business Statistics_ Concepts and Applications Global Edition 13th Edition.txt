Global 
edition
Basic Business Statistics
Concepts and Applications
THIRTEENTH edition
Berenson • Levine • Szabat

MyStatLab™
for Business Statistics
MyStatLab is a course management system that provides engaging learning experiences and delivers 
proven results while helping students succeed. Tools are embedded which make it easy to integrate 
statistical software into the course. And, MyStatLab comes from an experienced partner with 
educational expertise and an eye on the future.
Tutorial Exercises
MyStatLab homework and practice exercises correlated to the exercises in the textbook are generated 
algorithmically, giving students unlimited opportunity for practice and mastery. MyStatLab grades 
homework and provides feedback and guidance. 
Powerful Homework and Test Manager
Create, import, and manage online homework assignments, quizzes, and tests that are automatically 
graded, allowing you to spend less time grading and more time teaching. Thousands of high-quality and  
algorithmic exercises of all types and difficulty levels are available to meet the needs of students with  
diverse mathematical backgrounds. 
Help Me Solve This breaks the problem 
into manageable steps. Students enter 
answers along the way.
View an Example walks students 
through a problem similar to the one 
assigned.
Textbook links to the appropriate  
section in the etext.
Tech Help is a suite of Technology  
Tutorial videos that show how to 
perform statistical calculations using 
popular software.

Adaptive Learning
An Adaptive Study Plan serves as a personalized tutor for your students. When enabled, Knewton in 
MyStatLab monitors student performance and provides personalized recommendations. It gathers information 
about learning preferences and is continuously adaptive, guiding students though the Study Plan one  
objective at a time.
Integrated Statistical Software
Copy our data sets, from the eText and the MyStatLab questions, into software such as StatCrunch, 
Minitab, Excel, and more. Students have access to support tools—videos, Study Cards, and manuals for 
select titles—to learn how to use statistical software.

StatCrunch
MyStatLab includes web-based statistical software, StatCrunch, within the online assessment platform so 
that students can analyze data sets from exercises and the text. In addition, MyStatLab includes access to 
www.StatCrunch.com, the full web-based program where users can access thousands of shared data 
sets, create and conduct online surveys, perform complex analyses using the powerful statistical software, 
and generate compelling reports. 
Engaging Video Resources
•	Business Insight Videos are 10 engaging videos showing managers at top companies using statistics in 
their everyday work. Assignable questions encourage discussion.
• StatTalk Videos, hosted by fun-loving statistician Andrew Vickers, demonstrate important statistical  
concepts through interesting stories and real-life events. This series of 24 videos includes available  
assessment questions and an instructor’s guide.
PHStat™ (access code required)
PHStat is a statistics add-in for Microsoft Excel that simplifies the task of operating Excel,  
creating real Excel worksheets that use in-worksheet calculations. Download PHStat by visiting  
www.pearsonhighered.com/phstat or through a link in MyStatLab’s Tools for Success, access code  
required.
This book features PHStat version 4 which is compatible with all current Microsoft Windows and  
(Mac) OS X Excel versions.

A Roadmap for Selecting  
a Statistical Method
Data Analysis Task
For Numerical Variables
For Categorical Variables
Describing a group or 
several groups
Ordered array, stem-and-leaf display, frequency distribution, ­relative 
frequency distribution, percentage distribution, cumulative percentage 
distribution, histogram, polygon, cumulative percentage polygon, bullet 
maps, sparklines, gauges, treemaps (Sections 2.2, 2.4, 17.1)
Mean, median, mode, geometric mean, quartiles, range, interquartile range, 
standard deviation, variance, coefficient of variation, skewness, kurtosis, 
boxplot, normal probability plot (Sections 3.1, 3.2, 3.3, 6.3) 
Index numbers (online Section 16.8) 
Gauges, bullet graphs, and treemaps ­(Section 17.1)
Summary table, bar chart, pie chart, Pareto chart 
(Sections 2.1 and 2.3)
Inference about one group
Confidence interval estimate of the mean (Sections 8.1 and 8.2)
t test for the mean (Section 9.2)
Chi-square test for a variance or standard deviation (online Section 12.7)
Confidence interval estimate of the proportion 
(Section 8.3)
Z test for the proportion ­(Section 9.4)
Comparing two groups
Tests for the difference in the means of two independent ­populations  
(Section 10.1)
Wilcoxon rank sum test (Section 12.4)
Paired t test (Section 10.2)
F test for the difference between two variances (Section 10.4)
Wilcoxon signed ranks test (online Section 12.8)
Z test for the difference ­between two proportions 
­(Section 10.3)
Chi-square test for the ­difference between two 
proportions (Section 12.1)
McNemar test for two related samples (online  
Section 12.6)
Comparing more than two 
groups
One-way analysis of variance for comparing several means (Section 11.1)
Kruskal-Wallis test (Section 12.5)
Randomized block design (Section 11.2)
Two-way analysis of variance (Section 11.3)
Friedman rank test (online Section 12.9)
Chi-square test for differences among more than two 
proportions (Section 12.2)
Analyzing the ­relationship 
­between two ­variables
Scatter plot, time series plot (Section 2.5)
Covariance, coefficient of correlation (Section 3.5)
Simple linear regression (Chapter 13)
t test of correlation (Section 13.7)
Time-series forecasting (Chapter 16) 
Sparklines (Section 17.1)
Contingency table, side-by-side bar chart,  
PivotTables ­(Sections 2.1, 2.3, 2.6)
Chi-square test of ­independence (Section 12.3)
Analyzing the ­relationship 
­between two or more 
variables
Multiple regression (Chapters 14 and 15)
Regression trees (Section 17.3)
Neural nets (Section 17.4)
Cluster analysis (Section 17.5)
Multidimensional scaling (Section 17.6)
Multidimensional contingency tables (Section 2.7)
Drilldown and slicers (Section 17.1)
Logistic regression (Section 14.7)
Classification trees (Section 17.3)
Neural nets (Section 17.4)

Mark L. Berenson
Department of Information and Operations Management 
School of Business, Montclair State University
David M. Levine
Department of Statistics and Computer Information Systems 
Zicklin School of Business, Baruch College, City University of New York
Kathryn A. Szabat
Department of Business Systems and Analytics 
School of Business, La Salle University
Basic Business 
Statistics
Concepts and Applications
Thirteenth Edition
Global Edition
Boston  Columbus  Indianapolis  New York  San Francisco  Upper Saddle River 
Amsterdam  Cape Town  Dubai  London  Madrid  Milan  Munich  Paris  Montreal  Toronto 
Delhi  Mexico City  S~ao Paulo  Sydney  Hong Kong  Seoul  Singapore  Taipei  Tokyo

MICROSOFT® AND WINDOWS® ARE REGISTERED TRADEMARKS OF THE MICROSOFT CORPORATION IN THE U.S.A. AND OTHER 
COUNTRIES. THIS BOOK IS NOT SPONSORED OR ENDORSED BY OR AFFILIATED WITH THE MICROSOFT CORPORATION. Illustrations  
of Microsoft Excel in this book have been taken from Microsoft Excel 2013, unless otherwise indicated.
MICROSOFT AND/OR ITS RESPECTIVE SUPPLIERS MAKE NO REPRESENTATIONS ABOUT THE SUITABILITY OF THE INFORMATION 
CONTAINED IN THE DOCUMENTS AND RELATED GRAPHICS PUBLISHED AS PART OF THE SERVICES FOR ANY PURPOSE. ALL SUCH 
DOCUMENTS AND RELATED GRAPHICS ARE PROVIDED “AS IS” WITHOUT WARRANTY OF ANY KIND. MICROSOFT AND/OR ITS RESPECTIVE 
SUPPLIERS HEREBY DISCLAIM ALL WARRANTIES AND CONDITIONS WITH REGARD TO THIS INFORMATION, INCLUDING ALL WARRANTIES 
AND CONDITIONS OF MERCHANTABILITY, WHETHER EXPRESS, IMPLIED OR STATUTORY, FITNESS FOR A PARTICULAR PURPOSE, TITLE 
AND NON-INFRINGEMENT. IN NO EVENT SHALL MICROSOFT AND/OR ITS RESPECTIVE SUPPLIERS BE LIABLE FOR ANY SPECIAL, INDIRECT 
OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER 
IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF OR IN CONNECTION WITH THE USE OR 
PERFORMANCE OF INFORMATION AVAILABLE FROM THE SERVICES.
THE DOCUMENTS AND RELATED GRAPHICS CONTAINED HEREIN COULD INCLUDE TECHNICAL INACCURACIES OR TYPOGRAPHICAL 
ERRORS. CHANGES ARE PERIODICALLY ADDED TO THE INFORMATION HEREIN. MICROSOFT AND/OR ITS RESPECTIVE SUPPLIERS MAY 
MAKE IMPROVEMENTS AND/OR CHANGES IN THE PRODUCT(S) AND/OR THE PROGRAM(S) DESCRIBED HEREIN AT ANY TIME.  PARTIAL 
SCREEN SHOTS MAY BE VIEWED IN FULL WITHIN THE SOFTWARE VERSION SPECIFIED.
Minitab © 2013.  Portions of information contained in this publication/book are printed with permission of Minitab Inc. All such material remains the exclusive 
property and copyright of Minitab Inc. All rights reserved.
The contents, descriptions, and characters of WaldoLands and Waldowood are Copyright © 2014, 2011 Waldowood Productions, and used with permission.
Pearson Education Limited
Edinburgh Gate Harlow
Essex CM20 2JE 
England
and Associated Companies throughout the world
Visit us on the World Wide Web at:
www.pearsonglobaleditions.com
© Pearson Education Limited 2015
The rights of Mark L. Berenson, David M. Levine and Kathryn A. Szabat to be identified as the authors of this work have been asserted by them in accordance 
with the Copyright, Designs and Patents Act 1988.
Authorized adaptation from the United States edition, entitled Basic Business Statistics: Concepts and Applications, 13th edition, ISBN 978-0-321-87002-5, by 
Mark L. Berenson, David M. Levine and Kathryn A. Szabat, published by Pearson Education © 2015.
All rights reserved. No part of this publication may be reproduced, stored in a retrieval system, or transmitted in any form or by any means, electronic, mechanical, 
photocopying, recording or otherwise, without either the prior written permission of the publisher or a license permitting restricted copying in the United Kingdom 
issued by the Copyright Licensing Agency Ltd, Saffron House, 6–10 Kirby Street, London EC1N 8TS.
All trademarks used herein are the property of their respective owners. The use of any trademark in this text does not vest in the author or publisher any trademark 
ownership rights in such trademarks, nor does the use of such trademarks imply any affiliation with or endorsement of this book by such owners.
Many of the designations used by manufacturers and sellers to distinguish their products are claimed as trademarks. Where those designations appear in this book, 
and Pearson was aware of a trademark claim, the designations have been printed in initial caps or all caps.
British Library Cataloguing-in-Publication Data
A catalogue record for this book is available from the British Library
10  9  8  7  6  5  4  3  2  1
15  14  13  12  11
ISBN 10:	
1-292-06902-3
ISBN 13: 978-1-292-06902-9
Typeset in Times LT Std by Lumina Datamatics 
Printed by Courier Kendallville in the United States of America
Editor in Chief: Deirdre Lynch
Head of Learning Asset Acquisition, Global Editions: Laura Dent
Acquisitions Editor: Marianne Stepanian
Acquisitions Editor, Global Editions: Subhasree Patra
Project Editor: Dana Bettez
Assistant Editor: Sonia Ashraf
Senior Managing Editor: Karen Wernholm
Senior Project Editor, Global Editions: Vaijyanti
Senior Production Supervisor: Kathleen A. Manley
Senior Manufacturing Production Controller, Global Editions:  
  Trudy Kimber
Digital Assets Manager: Marianne Groth
Manager, Multimedia Production: Christine Stavrou
Media Production Manager, Global Editions: M Vikram Kumar
Software Development: John Flanagan, MathXL; Marty Wright, TestGen
Senior Marketing Manager: Erin Lane
Marketing Assistant: Kathleen DeChavez
Senior Author Support/Technology Specialist: Joe Vetere
Rights and Permissions Advisor: Cathy Pare
Image Manager: Rachel Youdelman
Procurement Specialist: Debbie Rossi
Art Direction: Barbara Atkinson
Cover Design: Lumina Datamatics
Text Design, Production Coordination, Composition,  
and Illustrations: PreMediaGlobal
Cover photo: ©Sergey Nivens/Shutterstock

6
The authors of this book: Kathryn Szabat, David 
Levine, and Mark Berenson at a Decision Sciences 
Institute meeting.
About the Authors
Mark L. Berenson is Professor of Management and Information Systems at Montclair 
State University (Montclair, New Jersey) and also Professor Emeritus of Statistics and Computer 
Information Systems at Bernard M. Baruch College (City University of New York). He currently 
teaches graduate and undergraduate courses in statistics and in operations management in the 
School of Business and an undergraduate course in international justice and human rights that he 
co-developed in the College of Humanities and Social Sciences.
Berenson received a B.A. in economic statistics and an M.B.A. in business statistics from  
City College of New York and a Ph.D. in business from the City University of New York.  
Berenson’s research has been published in Decision Sciences Journal of Innovative Education, 
Review of Business Research, The American Statistician, Communications in Statistics, 
Psychometrika, Educational and Psychological Measurement, Journal of Management Sciences 
and Applied Cybernetics, Research Quarterly, Stats Magazine, The New York Statistician, Journal 
of Health Administration Education, Journal of Behavioral Medicine, and Journal of Surgical 
Oncology. His invited articles have appeared in The Encyclopedia of Measurement & Statistics and 
Encyclopedia of Statistical Sciences. He is co-author of 11 statistics texts published by Prentice 
Hall, including Statistics for Managers Using Microsoft Excel, Basic Business Statistics: Concepts 
and Applications, and Business Statistics: A First Course.
Over the years, Berenson has received several awards for teaching and for innovative contributions 
to statistics education. In 2005, he was the first recipient of the Catherine A. Becker Service for 
Educational Excellence Award at Montclair State University and, in 2012, he was the recipient of 
the Khubani/Telebrands Faculty Research Fellowship in the School of Business.
David M. Levine is Professor Emeritus of Statistics and Computer Information Systems 
at Baruch College (City University of New York). He received B.B.A. and M.B.A. degrees in statis-
tics from City College of New York and a Ph.D. from New York University in industrial engineering 
and operations research. He is nationally recognized as a leading innovator in statistics education 
and is the co-author of 14 books, including such best-selling statistics textbooks as Statistics for 
Managers Using Microsoft Excel, Basic Business Statistics: Concepts and Applications, Business 
Statistics: A First Course, and Applied Statistics for Engineers and Scientists Using Microsoft Excel 
and Minitab.

	
ABOUT THE AUTHORS	
7
He also is the co-author of Even You Can Learn Statistics: A Guide for Everyone Who Has Ever 
Been Afraid of Statistics, currently in its second edition, Six Sigma for Green Belts and Champions 
and Design for Six Sigma for Green Belts and Champions, and the author of Statistics for Six Sigma 
Green Belts, all published by FT Press, a Pearson imprint, and Quality Management, third edi-
tion, McGraw-Hill/Irwin. He is also the author of Video Review of Statistics and Video Review 
of Probability, both published by Video Aided Instruction, and the statistics module of the MBA 
primer published by Cengage Learning. He has published articles in various journals, including 
Psychometrika, The American Statistician, Communications in Statistics, Decision Sciences Journal 
of Innovative Education, Multivariate Behavioral Research, Journal of Systems Management, 
Quality Progress, and The American Anthropologist, and he has given numerous talks at the 
Decision Sciences Institute (DSI), American Statistical Association (ASA), and Making Statistics 
More Effective in Schools and Business (MSMESB) conferences. Levine has also received several 
awards for outstanding teaching and curriculum development from Baruch College.
Kathryn A. Szabat is Associate Professor and Chair of Business Systems and 
Analytics at LaSalle University. She teaches undergraduate and graduate courses in business statis-
tics and operations management.
Szabat’s research has been published in International Journal of Applied Decision Sciences, 
Accounting Education, Journal of Applied Business and Economics, Journal of Healthcare 
Management, and Journal of Management Studies. Scholarly chapters have appeared in Managing 
Adaptability, Intervention, and People in Enterprise Information Systems; Managing, Trade, 
Economies and International Business; Encyclopedia of Statistics in Behavioral Science; and 
Statistical Methods in Longitudinal Research.
Szabat has provided statistical advice to numerous business, nonbusiness, and academic commu-
nities. Her more recent involvement has been in the areas of education, medicine, and nonprofit 
capacity building.
Szabat received a B.S. in mathematics from State University of New York at Albany and M.S. and 
Ph.D. degrees in statistics, with a cognate in operations research, from the Wharton School of the 
University of Pennsylvania.

8
Brief Contents
Preface  19
Getting Started: Important Things to Learn First  29
	1	 Defining and Collecting Data  41
	2	 Organizing and Visualizing Variables  64
	3	 Numerical Descriptive Measures  129
	4	 Basic Probability  179
	5	 Discrete Probability Distributions  213
	6	 The Normal Distribution and Other Continuous Distributions  247
	7	 Sampling Distributions  278
	8	 Confidence Interval Estimation  300
	9	 Fundamentals of Hypothesis Testing: One-Sample Tests  336
	10	 Two-Sample Tests  375
	11	 Analysis of Variance  422
	12	 Chi-Square and Nonparametric Tests  475
	13	 Simple Linear Regression  519
	14	 Introduction to Multiple Regression  571
	15	 Multiple Regression Model Building  624
	16	 Time-Series Forecasting  657
	17	 Business Analytics  702
	18	 A Roadmap for Analyzing Data  735
	19	 Statistical Applications in Quality Management (online)
	20	 Decision Making (online)
Appendices A–G  743
Self-Test Solutions and Answers to Selected Even-Numbered Problems  795
Index  831

9
Contents
Preface  19
Getting Started: Important 
Things to Learn First	
29
Using Statistics: “You Cannot Escape from Data”  29
GS.1   Statistics: A Way of Thinking  30
GS.2   Data: What Is It?  31
GS.3   Business Analytics: The Changing Face of  
  Statistics  32
    “Big Data”  32
    Statistics: An Important Part of Your Business  
    Education  33
GS.4   Software and Statistics  34
  Excel and Minitab Guides  34
References  35
Key Terms  35
Excel Guide  36
  EG1. Getting Started with Microsoft Excel  36
  EG2. Entering Data  36
  EG3. Opening and Saving Workbooks  37
  EG4. Creating and Copying Worksheets  38
  EG5. Printing Worksheets  38
Minitab Guide  39
  MG1. Getting Started with Minitab  39
  MG2. Entering Data  39
  MG3. Opening and Saving Worksheets and Projects  39
  MG4. Creating and Copying Worksheets  40
  MG5. Printing Parts of a Project  40
1  Defining and  
Collecting Data	
41
Using Statistics: Beginning of the End … Or the End  
of the Beginning?  41
1.1	
Defining Data  42
Establishing the Variable Type  42
1.2	
Measurement Scales for Variables  43
Nominal and Ordinal Scales  43
Interval and Ratio Scales  44
1.3	
Collecting Data  46
Data Sources  46
Populations and Samples  47
Data Formatting  47
Data Cleaning  48
Recoding Variables  48
1.4	
Types of Sampling Methods  49
Simple Random Sample  50
Systematic Sample  51
Stratified Sample  51
Cluster Sample  51
1.5	
Types of Survey Errors  52
Coverage Error  53
Nonresponse Error  53
Sampling Error  53
Measurement Error  53
Ethical Issues About Surveys  54
Think About This: New Media Surveys/Old Sampling 
Problems  54
Using Statistics: Beginning of the End … Revisited  55
Summary  56
REFERENCES  56
Key Terms  56
Checking Your Understanding  57
Chapter Review Problems  57
Cases for Chapter 1  58
	
Managing Ashland MultiComm Services  58
	
CardioGood Fitness  58
	
Clear Mountain State Student Surveys  59
	
Learning with the Digital Cases  59
Chapter 1 Excel Guide  61
  EG1.1 Defining Data  61
  EG1.2 Measurement Scales for Variables  61
  EG1.3 Collecting Data  61
  EG1.4 Types of Sampling Methods  61
Chapter 1 Minitab Guide  62
  MG1.1 Defining Data  62
  MG1.2 Measurement Scales for Variables  62
  MG1.3 Collecting Data  63
  MG1.4 Types of Sampling Methods  63
2  Organizing and  
Visualizing Variables	
64
Using Statistics: The Choice Is Yours  64
	
How to Proceed with This Chapter  65
2.1	
Organizing Categorical Variables  66
The Summary Table  66
The Contingency Table  67
2.2	
Organizing Numerical Variables  70
The Ordered Array  70
The Frequency Distribution  71
Classes and Excel Bins  73
The Relative Frequency Distribution and the Percentage 
Distribution  73
The Cumulative Distribution  75
Stacked and Unstacked Data  77

10	
contents
2.3	
Visualizing Categorical Variables  79
The Bar Chart  79
The Pie Chart  80
The Pareto Chart  81
The Side-by-Side Bar Chart  83
2.4	
Visualizing Numerical Variables  85
The Stem-and-Leaf Display  85
The Histogram  87
The Percentage Polygon  88
The Cumulative Percentage Polygon (Ogive)  89
2.5	
Visualizing Two Numerical Variables  93
The Scatter Plot  93
The Time-Series Plot  94
2.6	
Organizing Many Categorical Variables  96
2.7	
Challenges in Organizing and Visualizing Variables  98
Obscuring Data  98
Creating False Impressions  99
Chartjunk  100
Guidelines for Constructing Visualizations  102
Using Statistics: The Choice Is Yours, Revisited  103
Summary  103
REFERENCES  104
Key Equations  104
Key Terms  105
Checking Your Understanding  105
Chapter Review Problems  105
Cases for Chapter 2  110
	
Managing Ashland MultiComm Services  110
	
Digital Case  111
	
CardioGood Fitness  111
	
The Choice Is Yours Follow-Up  111
	
Clear Mountain State Student Surveys  112
Chapter 2 Excel Guide  113
  EG2.1 Organizing Categorical Variables  113
  EG2.2 Organizing Numerical Variables  115
  EG2.3 Visualizing Categorical Variables  117
  EG2.4 Visualizing Numerical Variables  119
  EG2.5 Visualizing Two Numerical Variables  122
  EG2.6 Organizing Many Categorical Variables  122
Chapter 2 Minitab Guide  123
  MG2.1 Organizing Categorical Variables  123
  MG2.2 Organizing Numerical Variables  124
  MG2.3 Visualizing Categorical Variables  124
  MG2.4 Visualizing Numerical Variables  126
  MG2.5 Visualizing Two Numerical Variables  128
  MG2.6 Organizing Many Categorical Variables  128
3  Numerical Descriptive 
Measures	
129
Using Statistics: More Descriptive Choices  129
3.1	
Central Tendency  130
The Mean  130
The Median  132
The Mode  133
The Geometric Mean  134
3.2	
Variation and Shape  135
The Range  135
The Variance and the Standard Deviation  136
The Coefficient of Variation  140
Z Scores  141
Shape: Skewness and Kurtosis  142
Visual Explorations: Exploring Descriptive Statistics  145
3.3	
Exploring Numerical Data  148
Quartiles  148
The Interquartile Range  150
The Five-Number Summary  151
The Boxplot  152
3.4	
Numerical Descriptive Measures for a Population  155
The Population Mean  155
The Population Variance and Standard Deviation  156
The Empirical Rule  157
The Chebyshev Rule  158
3.5	
The Covariance and the Coefficient of Correlation  159
The Covariance  160
The Coefficient of Correlation  161
3.6	
Descriptive Statistics: Pitfalls and Ethical Issues  165
Using Statistics: More Descriptive Choices, Revisited  166
Summary  166
References  167
Key Equations  167
Key Terms  168
Checking Your Understanding  168
Chapter Review Problems  169
Cases for Chapter 3  172
	
Managing Ashland MultiComm Services  172
	
Digital Case  172
	
CardioGood Fitness  172
	
More Descriptive Choices Follow-up  172
	
Clear Mountain State Student Surveys  172
Chapter 3 Excel Guide  173
  EG3.1 Central Tendency  173
  EG3.2 Variation and Shape  173
  EG3.3 Exploring Numerical Data  174
  EG3.4 Numerical Descriptive Measures for a Population  175
  EG3.5 The Covariance and the Coefficient of Correlation  175
Chapter 3 Minitab Guide  176
  MG3.1 Central Tendency  176
  MG3.2 Variation and Shape  176
  MG3.3 Exploring Numerical Data  177
  MG3.4 Numerical Descriptive Measures for a Population  177
  MG3.5 The Covariance and the Coefficient of Correlation  177
4  Basic Probability	
179
Using Statistics: Possibilities at M&R Electronics 
World  179
4.1	
Basic Probability Concepts  180
Events and Sample Spaces  181
Contingency Tables and Venn Diagrams  183
Simple Probability  183
Joint Probability  184

To our spouses and children,  
Rhoda, Marilyn, Kathy, Lori, and Sharyn
and to our parents, in loving memory,  
Nat, Ethel, Lee, Reuben, Mary, and William

	
contents	
11
Marginal Probability  185
General Addition Rule  186
4.2	
Conditional Probability  189
Computing Conditional Probabilities  189
Decision Trees  191
Independence  193
Multiplication Rules  194
Marginal Probability Using the General Multiplication 
Rule  195
4.3	
Bayes’ Theorem  197
Think About This: Divine Providence and Spam  200
4.4	
Counting Rules  202
4.5	
Ethical Issues and Probability  205
Using Statistics:  Possibilities at M&R Electronics World, 
Revisited  206
Summary  206
References  206
Key Equations  207
Key Terms  207
Checking Your Understanding  208
Chapter Review Problems  208
Cases for Chapter 4  210
	
Digital Case  210
	
CardioGood Fitness  210
	
The Choice Is Yours Follow-Up  210
	
Clear Mountain State Student Surveys  210
Chapter 4 Excel Guide  211
  EG4.1 Basic Probability Concepts  211
  EG4.2 Conditional Probability  211
  EG4.3 Bayes’ Theorem  211
  EG4.4 Counting Rules  211
Chapter 4 Minitab Guide  212
  MG4.1 Basic Probability Concepts  212
  MG4.2 Conditional Probability  212
  MG4.3 Bayes’ Theorem  212
  MG4.4 Counting Rules  212
5  Discrete Probability 
Distributions	
213
Using Statistics: Events of Interest at Ricknel Home 
Centers  213
5.1	
The Probability Distribution for a Discrete Variable  214
Expected Value of a Discrete Variable  214
Variance and Standard Deviation of a Discrete Variable  215
5.2	
Covariance of a Probability Distribution and Its 
Application in Finance  217
Covariance  218
Expected Value, Variance, and Standard Deviation of the 
Sum of Two Variables  219
Portfolio Expected Return and Portfolio Risk  219
5.3	
Binomial Distribution  223
5.4	
Poisson Distribution  230
5.5	
Hypergeometric Distribution  234
5.6	
Using the Poisson Distribution to Approximate  
the Binomial Distribution (online)  237
Using Statistics: Events of Interest at Ricknel Homecenters, 
Revisited  237
Summary  237
References  237
Key Equations  238
Key Terms  238
Checking Your Understanding  239
Chapter Review Problems  239
Cases for Chapter 5  241
Managing Ashland MultiComm Services  241
Digital Case  242
Chapter 5 Excel Guide  243
  EG5.1 The Probability Distribution for a Discrete Variable  243
  EG5.2 Covariance of a Probability Distribution and its Application 
in Finance  243
  EG5.3 Binomial Distribution  243
  EG5.4 Poisson Distribution  244
  EG5.5 Hypgeometric Distribution  244
Chapter 5 Minitab Guide  245
  MG5.1 The Probability Distribution for a Discrete Variable  245
  MG5.2 Covariance and its Application in Finance  245
  MG5.3 Binomial Distribution  245
  MG5.4 Poisson Distribution  245
  MG5.5 Hypergeometric Distribution  246
6  The Normal Distribution 
and Other Continuous 
Distributions	
247
Using Statistics: Normal Downloading at MyTVLab  247
6.1	
Continuous Probability Distributions  248
6.2	
The Normal Distribution  248
Computing Normal Probabilities  250
Finding X Values  255
Visual Explorations: Exploring the Normal 
Distribution  259
Think About This: What Is Normal?  259
6.3	
Evaluating Normality  261
Comparing Data Characteristics to Theoretical 
Properties  261
Constructing the Normal Probability Plot  263
6.4	
The Uniform Distribution  265
6.5	
The Exponential Distribution  268
6.6	
The Normal Approximation to the Binomial Distribution 
(online)  270
Using Statistics: Normal Downloading at MyTVLab, 
Revisited  270

12	
contents
Summary  271
References  271
Key Equations  271
Key Terms  272
Checking Your Understanding  272
Chapter Review Problems  272
Cases for Chapter 6  273
Managing Ashland MultiComm Services  273
Digital Case  274
CardioGood Fitness  274
More Descriptive Choices Follow-up  274
Clear Mountain State Student Surveys  274
Chapter 6 Excel Guide  275
  EG6.1 Continuous Probability Distributions  275
  EG6.2 The Normal Distribution  275
  EG6.3 Evaluating Normality  275
  EG6.4 The Uniform Distribution  276
  EG6.5 The Exponential Distribution  276
Chapter 6 Minitab Guide  276
  MG6.1 Continuous Probability Distributions  276
  MG6.2 The Normal Distribution  276
  MG6.3 Evaluating Normality  276
  MG6.4 The Uniform Distribution  277
  MG6.5 The Exponential Distribution  277
7  Sampling Distributions	
278
Using Statistics: Sampling Oxford Cereals  278
7.1	
Sampling Distributions  279
7.2	
Sampling Distribution of the Mean  279
The Unbiased Property of the Sample Mean  279
Standard Error of the Mean  281
Sampling from Normally Distributed Populations  282
Sampling from Non-normally Distributed Populations—The 
Central Limit Theorem  285
Visual Explorations: Exploring Sampling Distributions  289
7.3	
Sampling Distribution of the Proportion  290
7.4	
Sampling from Finite Populations (online)  293
Using Statistics:  Sampling Oxford Cereals, Revisited  294
Summary  294
References  294
Key Equations  294
Key Terms  295
Checking Your Understanding  295
Chapter Review Problems  295
Cases for Chapter 7  297
Managing Ashland MultiComm Services  297
Digital Case  297
Chapter 7 Excel Guide  298
  EG7.1 Sampling Distributions  298
  EG7.2 Sampling Distribution of the Mean  298
  EG7.3 Sampling Distribution of the Proportion  298
Chapter 7 Minitab Guide  299
  MG7.1 Sampling Distributions  299
  MG7.2 Sampling Distribution of the Mean  299
  MG7.3 Sampling Distribution of the Proportion  299
8  Confidence Interval 
Estimation	
300
Using Statistics: Getting Estimates at Ricknel Home 
Centers  300
8.1	
Confidence Interval Estimate for the Mean  
(s Known)  301
Can You Ever Know the Population Standard 
Deviation?  306
8.2	
Confidence Interval Estimate for the Mean  
(s Unknown)  307
Student’s t Distribution  307
Properties of the t Distribution  308
The Concept of Degrees of Freedom  309
The Confidence Interval Statement  310
8.3	
Confidence Interval Estimate for the Proportion  315
8.4	
Determining Sample Size  318
Sample Size Determination for the Mean  318
Sample Size Determination for the Proportion  320
8.5	
Confidence Interval Estimation and Ethical Issues  323
8.6	
Application of Confidence Interval Estimation in 
Auditing (online)  324
8.7	
Estimation and Sample Size Estimation for Finite 
Populations (online)  324
8.8	
Bootstrapping (online)  324
Using Statistics: Getting Estimates at Ricknel Home 
Centers, Revisited  324
Summary  325
References  325
Key Equations  325
Key Terms  326
Checking Your Understanding  326
Chapter Review Problems  326
Cases for Chapter 8  329
Managing Ashland MultiComm Services  329
Digital Case  330
Sure Value Convenience Stores  331
CardioGood Fitness  331
More Descriptive Choices Follow-Up  331
Clear Mountain State Student Surveys  331
Chapter 8 Excel Guide  332
  EG8.1 Confidence Interval Estimate for the Mean (s Known)  332
  EG8.2 Confidence Interval Estimate for the Mean (s Unknown)  332
  EG8.3 Confidence Interval Estimate for the Proportion  333
  EG8.4 Determining Sample Size  333
Chapter 8 Minitab Guide  334
  MG8.1 Confidence Interval Estimate for the Mean (s Known)  334
  MG8.2 Confidence Interval Estimate for the Mean (s Unknown)  334
  MG8.3 Confidence Interval Estimate for the Proportion  334
  MG8.4 Determining Sample Size  335

	
contents	
13
9  Fundamentals of  
Hypothesis Testing:  
One-Sample Tests	
336
Using Statistics: Significant Testing at Oxford Cereals  336
9.1	
Fundamentals of Hypothesis-Testing Methodology  337
The Null and Alternative Hypotheses  337
The Critical Value of the Test Statistic  338
Regions of Rejection and Nonrejection  339
Risks in Decision Making Using Hypothesis Testing  339
Z Test for the Mean (s Known)  342
Hypothesis Testing Using the Critical Value Approach  342
Hypothesis Testing Using the p-Value Approach  345
A Connection Between Confidence Interval Estimation and 
Hypothesis Testing  347
Can You Ever Know the Population Standard 
Deviation?  348
9.2	
t Test of Hypothesis for the Mean (s Unknown)  349
The Critical Value Approach  350
The p-Value Approach  352
Checking the Normality Assumption  352
9.3	
One-Tail Tests  356
The Critical Value Approach  356
The p-Value Approach  357
9.4	
Z Test of Hypothesis for the Proportion  360
The Critical Value Approach  361
The p-Value Approach  362
9.5	
Potential Hypothesis-Testing Pitfalls and Ethical 
Issues  364
Statistical Significance Versus Practical Significance  364
Statistical Insignificance Versus Importance  365
Reporting of Findings  365
Ethical Issues  365
9.6	
Power of a Test (online)  365
Using Statistics: Significant Testing at Oxford Cereals, 
Revisited  366
Summary  366
References  366
Key Equations  367
Key Terms  367
Checking Your Understanding  367
Chapter Review Problems  367
Cases for Chapter 9  369
Managing Ashland MultiComm Services  369
Digital Case  370
Sure Value Convenience Stores  370
Chapter 9 Excel Guide  371
  EG9.1 Fundamentals of Hypothesis-Testing Methodology  371
  EG9.2 t Test of Hypothesis for the Mean (s Unknown)  371
  EG9.3 One-Tail Tests  372
  EG9.4 Z Test of Hypothesis for the Proportion  372
Chapter 9 Minitab Guide  373
  MG9.1 Fundamentals of Hypothesis-Testing Methodology  373
  MG9.2 t Test of Hypothesis for the Mean (s Unknown)  373
  MG9.3 One-Tail Tests  373
  MG9.4 Z Test of Hypothesis for the Proportion  374
10  Two-Sample Tests	
375
Using Statistics: For North Fork, Are There Different 
Means to the Ends?  375
10.1	 Comparing the Means of Two Independent 
Populations  376
Pooled-Variance t Test for the Difference Between Two 
Means  376
Confidence Interval Estimate for the Difference Between 
Two Means  381
t Test for the Difference Between Two Means, Assuming 
Unequal Variances  382
Do People Really Do This?  384
10.2	 Comparing the Means of Two Related Populations  387
Paired t Test  388
Confidence Interval Estimate for the Mean Difference  393
10.3	 Comparing the Proportions of Two Independent 
Populations  395
Z Test for the Difference Between Two Proportions  395
Confidence Interval Estimate for the Difference 
Between Two Proportions  399
10.4	 F Test for the Ratio of Two Variances  401
10.5	 Effect Size (online) 
Using Statistics: For North Fork, Are There Different 
Means to the Ends? Revisited  406
Summary  407
References  408
Key Equations  408
Key Terms  409
Checking Your Understanding  409
Chapter Review Problems  409
Cases for Chapter 10  411
Managing Ashland MultiComm Services  411
Digital Case  412
Sure Value Convenience Stores  412
CardioGood Fitness  412
More Descriptive Choices Follow-Up  413
Clear Mountain State Student Surveys  413
Chapter 10 Excel Guide  414
  EG10.1 Comparing the Means of Two Independent  
Populations  414
  EG10.2 Comparing the Means of Two Related Populations  416
  EG10.3 Comparing the Proportions of Two Independent 
Populations  417
  EG10.4 f Test for the Ratio of Two Variances  417
Chapter 10 Minitab Guide  419
  MG10.1 Comparing the Means of Two Independent 
Populations  419
  MG10.2 Comparing the Means of Two Related Populations  419
  MG10.3 Comparing the Proportions of Two Independent 
Populations  420
  MG10.4 f Test for the Ratio of Two Variances  420

14	
contents
11  Analysis of Variance	
422
Using Statistics: The Means to Find Differences at 
Arlington’s  422
11.1	 The Completely Randomized Design: One-Way 
ANOVA  423
Analyzing Variation in One-Way ANOVA  424
F Test for Differences Among More Than Two Means  426
Multiple Comparisons: The Tukey-Kramer Procedure  430
The Analysis of Means (ANOM) (online)  432
ANOVA Assumptions  433
Levene Test for Homogeneity of Variance  433
11.2	 The Randomized Block Design  438
Testing for Factor and Block Effects  438
Multiple Comparisons: The Tukey Procedure  443
11.3	 The Factorial Design: Two-Way ANOVA  446
Factor and Interaction Effects  447
Testing for Factor and Interaction Effects  449
Multiple Comparisons: The Tukey Procedure  452
Visualizing Interaction Effects: The Cell Means Plot  454
Interpreting Interaction Effects  454
11.4	 Fixed Effects, Random Effects, and Mixed Effects 
Models (online)  459
Using Statistics: The Means to Find Differences  
at Arlington’s Revisited  459
Summary  459
References  460
Key Equations  460
Key Terms  461
Checking Your Understanding  462
Chapter Review Problems  462
Cases for Chapter 11  465
Managing Ashland MultiComm Services  465
Digital Case  465
Sure Value Convenience Stores  466
CardioGood Fitness  466
More Descriptive Choices Follow-Up  466
Clear Mountain State Student Surveys  466
Chapter 11 Excel Guide  468
  EG11.1 The Completely Randomized Design: One-Way 
ANOVA  468
  EG11.2 The Randomized Block Design  470
  EG11.3 The Factorial Design: Two-Way ANOVA  471
Chapter 11 Minitab Guide  472
  MG11.1 The Completely Randomized Design: One-Way 
ANOVA  472
  MG11.2 The Randomized Block Design  473
  MG11.3 The Factorial Design: Two-Way ANOVA  473
12  Chi-Square and 
Nonparametric Tests	
475
Using Statistics: Avoiding Guesswork About Resort 
Guests  475
12.1	 Chi-Square Test for the Difference Between Two 
Proportions  476
12.2	 Chi-Square Test for Differences Among More Than Two 
Proportions  483
The Marascuilo Procedure  486
The Analysis of Proportions (ANOP) (online)  488
12.3	 Chi-Square Test of Independence  489
12.4	 Wilcoxon Rank Sum Test: A Nonparametric Method  
for Two Independent Populations  495
12.5	 Kruskal-Wallis Rank Test: A Nonparametric Method  
for the One-Way ANOVA  501
Assumptions  504
12.6	 McNemar Test for the Difference Between Two 
Proportions (Related Samples) (online)  505
12.7	 Chi-Square Test for the Variance or Standard Deviation 
(online)  506
12.8	 Wilcoxon Signed Ranks Test: A Nonparametric Test for 
Two Related Populations (online)  506
12.9	 Friedman Rank Test: A Nonparametric Test for the 
Randomized Block Design (online)  506
Using Statistics: Avoiding Guesswork About Resort 
Guests, Revisited  506
Summary  507
References  507
Key Equations  508
Key Terms  508
Checking Your Understanding  508
Chapter Review Problems  508
Cases for Chapter 12  510
Managing Ashland MultiComm Services  510
Digital Case  511
Sure Value Convenience Stores  511
CardioGood Fitness  512
More Descriptive Choices Follow-Up  512
Clear Mountain State Student Surveys  512
Chapter 12 Excel Guide  514
  EG12.1 Chi-Square Test for the Difference Between Two 
Proportions  514
  EG12.2 Chi-Square Test for Differences Among More Than Two 
Proportions  514
  EG12.3 Chi-Square Test of Independence  515
  EG12.4 Wilcoxon Rank Sum Test: a Nonparametric Method for 
Two Independent Populations  515
  EG12.5 Kruskal-Wallis Rank Test: a Nonparametric Method for 
the One-Way ANOVA  516
Chapter 12 Minitab Guide  517
  MG12.1 Chi-Square Test for the Difference Between Two 
Proportions  517
  MG12.2 Chi-Square Test for Differences Among More Than Two 
Proportions  517
  MG12.3 Chi-Square Test of Independence  517
  MG12.4 Wilcoxon Rank Sum Test: a Nonparametric method for 
Two Independent Populations  517
  MG12.5 Kruskal-Wallis Rank Test: a Nonparametric method for 
the One-Way ANOVA  518

	
contents	
15
13  Simple Linear Regression	 519
Using Statistics: Knowing Customers at Sunflowers 
Apparel  519
13.1	 Types of Regression Models  520
Simple Linear Regression Models  521
13.2	 Determining the Simple Linear Regression 
Equation  522
The Least-Squares Method  522
Predictions in Regression Analysis: Interpolation Versus 
Extrapolation  525
Computing the Y Intercept, b0, and the Slope, b1  525
Visual Explorations: Exploring Simple Linear Regression 
Coefficients  528
13.3	 Measures of Variation  530
Computing the Sum of Squares  530
The Coefficient of Determination  531
Standard Error of the Estimate  533
13.4	 Assumptions of Regression  535
13.5	 Residual Analysis  535
Evaluating the Assumptions  535
13.6	 Measuring Autocorrelation: The Durbin-Watson 
Statistic  539
Residual Plots to Detect Autocorrelation  539
The Durbin-Watson Statistic  540
13.7	 Inferences About the Slope and Correlation 
Coefficient  543
t Test for the Slope  544
F Test for the Slope  545
Confidence Interval Estimate for the Slope  547
t Test for the Correlation Coefficient  547
13.8	 Estimation of Mean Values and Prediction of Individual 
Values  551
The Confidence Interval Estimate for the Mean 
Response  551
The Prediction Interval for an Individual Response  552
13.9	 Potential Pitfalls in Regression  555
Six Steps for Avoiding the Potential Pitfalls   557
Using Statistics: Knowing Customers at Sunflowers 
Apparel, Revisited  557
Summary  557
References  558
Key Equations  559
Key Terms  560
Checking Your Understanding  560
Chapter Review Problems  560
Cases for Chapter 13  564
Managing Ashland MultiComm Services  564
Digital Case  564
Brynne Packaging  564
Chapter 13 Excel Guide  566
  EG13.1 Types of Regression Models  566
  EG13.2 Determining the Simple Linear Regression Equation  566
  EG13.3 Measures of Variation  567
  EG13.4 Assumptions of Regression  567
  EG13.5 Residual Analysis  567
  EG13.6 Measuring Autocorrelation: the Durbin-Watson 
Statistic  568
  EG13.7 Inferences About the Slope and Correlation Coefficient  568
  EG13.8 Estimation of Mean Values and Prediction of Individual 
Values  568
Chapter 13 Minitab Guide  569
  MG13.1 Types of Regression Models  569
  MG13.2 Determining the Simple Linear Regression Equation  569
  MG13.3 Measures of Variation  569
  MG13.4 Assumptions  569
  MG13.5 Residual Analysis  569
  MG13.6 Measuring Autocorrelation: the Durbin-Watson Statistic  570
  MG13.7 Inferences About the Slope and Correlation 
Coefficient  570
  MG13.8 Estimation of Mean Values and Prediction of Individual 
Values  570
14  Introduction to  
Multiple Regression	
571
Using Statistics: The Multiple Effects of OmniPower 
Bars  571
14.1	 Developing a Multiple Regression Model  572
Interpreting the Regression Coefficients  573
Predicting the Dependent Variable Y  575
14.2	 r 2, Adjusted r 2, and the Overall F Test  578
Coefficient of Multiple Determination  578
Adjusted r 2  578
Test for the Significance of the Overall Multiple  
Regression Model  579
14.3	 Residual Analysis for the Multiple Regression 
Model  581
14.4	 Inferences Concerning the Population Regression 
Coefficients  583
Tests of Hypothesis  583
Confidence Interval Estimation  584
14.5	 Testing Portions of the Multiple Regression Model  586
Coefficients of Partial Determination  590
14.6	 Using Dummy Variables and Interaction Terms  
in Regression Models  591
Dummy Variables  592
Interactions  594
14.7	 Logistic Regression  601
14.8	 Influence Analysis  606
The Hat Matrix Elements, hi  607
The Studentized Deleted Residuals, ti  607
Cook’s Distance Statistic, Di  607
Comparison of Statistics  608
Using Statistics: The Multiple Effects of Omnipower Bars, 
Revisited  609
Summary  609
References  611
Key Equations  611
Key Terms  612
Checking Your Understanding  612
Chapter Review Problems  612
Cases for Chapter 14  615
Managing Ashland MultiComm Services  615
Digital Case  616

16	
contents
Chapter 14 Excel Guide  617
  EG14.1 Developing a Multiple Regression Model  617
  EG14.2 r 2, Adjusted r 2, and the Overall F Test  618
  EG14.3 Residual Analysis for the Multiple Regression  
Model  618
  EG14.4 Inferences Concerning the Population Regression 
Coefficients  619
  EG14.5 Testing Portions of the Multiple Regression Model  619
  EG14.6 Using Dummy Variables and Interaction Terms in 
Regression Models  619
  EG14.7 Logistic Regression  619
  EG14.8 Influence Analysis  620
Chapter 14 Minitab Guide  620
  MG14.1 Developing a Multiple Regression Model  620
  MG14.2 r 2, Adjusted r 2, and the Overall F Test  621
  MG14.3 Residual Analysis for the Multiple Regression Model  621
  MG14.4 Inferences Concerning the Population Regression 
Coefficients  621
  MG14.5 Testing Portions of the Multiple Regression Model  622
  MG14.6 Using Dummy Variables and Interaction Terms in 
Regression Models  622
  MG14.7 Logistic Regression  622
  MG14.8 Influence Analysis  623
15  Multiple Regression  
Model Building	
624
Using Statistics: Valuing Parsimony at WSTA-TV  624
15.1	 The Quadratic Regression Model  625
Finding the Regression Coefficients and Predicting Y  625
Testing for the Significance of the Quadratic Model  627
Testing the Quadratic Effect  628
The Coefficient of Multiple Determination  630
15.2	 Using Transformations in Regression Models  632
The Square-Root Transformation  633
The Log Transformation  633
15.3	 Collinearity  636
15.4	 Model Building  637
The Stepwise Regression Approach to Model Building  639
The Best-Subsets Approach to Model Building  640
Model Validation  644
Steps for Successful Model Building  644
15.5	 Pitfalls in Multiple Regression and Ethical Issues  646
Pitfalls in Multiple Regression  646
Ethical Issues  646
Using Statistics: Valuing Parsimony at Wsta-Tv, 
Revisited  647
Summary  647
Key Equations  647
References  649
Key Terms  649
Checking Your Understanding  649
Chapter Review Problems  649
Cases for Chapter 15  651
The Mountain States Potato Company  651
Sure Value Convenience Stores  651
Digital Case  651
The Craybill Instrumentation Company Case  652
More Descriptive Choices Follow-Up  652
Chapter 15 Excel Guide  653
  EG15.1 The Quadratic Regression Model  653
  EG15.2 Using Transformations in Regression Models  653
  EG15.3 Collinearity  653
  EG15.4 Model Building  654
Chapter 15 Minitab Guide  654
  MG15.1 The Quadratic Regression Model  654
  MG15.2 Using Transformations in Regression Models  655
  MG15.3 Collinearity  655
  MG15.4 Model Building  655
16  Time-Series Forecasting	 657
Using Statistics: Principled Forecasting  657
16.1	 The Importance of Business Forecasting  658
16.2	 Component Factors of Time-Series Models  658
16.3	 Smoothing an Annual Time Series  659
Moving Averages  660
Exponential Smoothing  662
16.4	 Least-Squares Trend Fitting and Forecasting  665
The Linear Trend Model  665
The Quadratic Trend Model  667
The Exponential Trend Model  669
Model Selection Using First, Second, and Percentage 
Differences  671
16.5	 Autoregressive Modeling for Trend Fitting and 
Forecasting  675
Selecting an Appropriate Autoregressive Model  676
Determining the Appropriateness of a Selected Model  677
16.6	 Choosing an Appropriate Forecasting Model  683
Performing a Residual Analysis  683
Measuring the Magnitude of the Residuals Through Squared 
or Absolute Differences  683
Using the Principle of Parsimony  684
A Comparison of Four Forecasting Methods  684
16.7	 Time-Series Forecasting of Seasonal Data  686
Least-Squares Forecasting with Monthly or Quarterly 
Data  687
16.8	 Index Numbers (online)  692
Think About This: Let the Model User Beware  692
Using Statistics: Principled Forecasting, Revisited  692
Summary  693
References  693
Key Equations  694
Key Terms  694
Checking Your Understanding  695
Chapter Review Problems  695
Cases for Chapter 16  696
Managing Ashland MultiComm Services  696
Digital Case  696
Chapter 16 Excel Guide  697
  EG16.1 The Importance of Business Forecasting  697
  EG16.2 Component Factors of Time-Series Models  697
  EG16.3 Smoothing an Annual Time Series  697

	
contents	
17
  EG16.4 Least-Squares Trend Fitting and Forecasting   670
  EG16.5 Autoregressive Modeling for Trend Fitting and 
Forecasting  698
  EG16.6 Choosing an Appropriate Forecasting Model  699
  EG16.7 Time-Series Forecasting of Seasonal Data  699
Chapter 16 Minitab Guide  700
  MG16.1 The Importance of Business Forecasting  700
  MG16.2 Component Factors of Time-Series Models  700
  MG16.3 Smoothing an Annual Time Series  700
  MG16.4 Least-Squares Trend Fitting and Forecasting  701
  MG16.5 Autoregressive Modeling for Trend Fitting and 
Forecasting  701
  MG16.6 Choosing an Appropriate Forecasting Model  701
  MG16.7 Time-Series Forecasting of Seasonal Data  701
17  Business Analytics	
702
Using Statistics: Finding the Right Lines  
at WaldoLands  702
17.1	 Descriptive Analytics  703
Dashboards  704
Data Discovery  706
17.2	 Predictive Analytics  710
17.3	 Classification and Regression Trees  711
Regression Tree Example  713
17.4	 Neural Networks  716
Multilayer Perceptrons  716
17.5	 Cluster Analysis  719
17.6	 Multidimensional Scaling  721
Using Statistics: Finding the Right Lines at Waldolands, 
Revisited  724
References  725
Key Equations  725
Key Terms  725
Checking Your Understanding  726
Chapter Review Problems  726
Case for Chapter 17 
The Mountain States Potato Company  727
Chapter 17 Software Guide  728
  SG17.1 Descriptive Analytics  728
  SG17.2 Predictive Analytics  732
  SG17.3 Classification and Regression Trees  732
  SG17.4 Neural Networks  733
  SG17.5 Cluster Analysis  734
  SG17.6 Multidimensional Scaling  734
18  A Roadmap for  
Analyzing Data	
735
Using Statistics: Mounting Future Analyses  735
18.1	 Analyzing Numerical Variables  737
Describing the Characteristics of a Numerical Variable  738
Reaching Conclusions About the Population Mean and/or 
Standard Deviation  738
Determining Whether the Mean and/or Standard Deviation 
Differs Depending on the Group  738
Determining Which Factors Affect the Value of a 
Variable  739
Predicting the Value of a Variable Based on the Values  
of Other Variables  739
Determining Whether the Values of a Variable Are Stable 
Over Time  739
18.2	 Analyzing Categorical Variables  739
Describing the Proportion of Items of Interest in Each 
Category  740
Reaching Conclusions About the Proportion of Items  
of Interest  740
Determining Whether the Proportion of Items  
of Interest Differs Depending on the Group  740
Predicting the Proportion of Items of Interest Based  
on the Values of Other Variables  740
Determining Whether the Proportion of Items of Interest  
Is Stable Over Time  740
Using Statistics:  Mounting Future Analyses, Revisited  741
Digital Case  741
Chapter Review Problems  741
19  Statistical Applications  
in Quality Management 
(online)
Using Statistics: Finding Quality at the Beachcomber
19.1	 The Theory of Control Charts
19.2	 Control Chart for the Proportion: The p Chart
19.3	 The Red Bead Experiment: Understanding Process 
Variability
19.4	 Control Chart for an Area of Opportunity:  
The c Chart
19.5	 Control Charts for the Range and the Mean
The R Chart
The 
_
X Chart
19.6	 Process Capability
Customer Satisfaction and Specification Limits
Capability Indices
CPL, CPU, and Cpk
19.7	 Total Quality Management
19.8	 Six Sigma
The DMAIC Model
Roles in a Six Sigma Organization
Lean Six Sigma
Using Statistics: Finding Quality at the Beachcomber, 
Revisited
Summary
References
Key Equations
Key Terms
CHECKING YOUR UNDERSTANdING
Chapter Review Problems

18	
contents
Cases for Chapter 19
The Harnswell Sewing Machine Company Case
Managing Ashland Multicomm Services
Chapter 19 Excel Guide
  EG19.1 The Theory of Control Charts
  EG19.2 Control Chart for the Proportion: The p Chart
  EG19.3 The Red Bead Experiment: Understanding Process 
Variability
  EG19.4 Control Chart for an Area of Opportunity: The c Chart
  EG19.5 Control Charts for the Range and the Mean
  EG19.6 Process Capability
20  Decision Making  
(online)
Using Statistics: Reliable Decision Making
20.1	 Payoff Tables and Decision Trees
20.2	 Criteria for Decision Making
Maximax Payoff
Maximin Payoff
Expected Monetary Value
Expected Opportunity Loss
Return-to-Risk Ratio
20.3	 Decision Making with Sample Information
20.4	 Utility
Think About This: Risky Business
Using Statistics: Reliable Decision-Making, Revisited
Summary
References
Key Equations
Key Terms
Chapter Review Problems
Chapter 20 Excel Guide
  EG20.1 Payoff Tables and Decision Trees
  EG20.2 Criteria for Decision Making
Appendices  743
A.  Basic Math Concepts and Symbols  744
A.1    Rules for Arithmetic Operations  744
A.2    Rules for Algebra: Exponents and Square Roots  744
A.3    Rules for Logarithms  745
A.4    Summation Notation  746
A.5    Statistical Symbols  749
A.6    Greek Alphabet  749
B.  Required Excel Skills  750
B.1    Worksheet Entries and References  750
B.2    Absolute and Relative Cell References  751
B.3    Entering Formulas into Worksheets  751
B.4    Pasting with Paste Special  752
B.5    Basic Worksheet Cell Formatting  752
B.6    Chart Formatting  754
B.7    Selecting Cell Ranges for Charts  755
B.8    Deleting the “Extra” Bar from a Histogram  755
B.9    Creating Histograms for Discrete Probability 
Distributions  756
C.  Online Resources  757
C.1    About the Online Resources for This Book  757
C.2    Accessing the MyStatLab Course Online  757
C.3    Details of Downloadable Files  757
C.4    PHStat  765
D.  Configuring Microsoft Excel  766
D.1    Getting Microsoft Excel Ready for Use (ALL)  766
D.2    Getting PHStat Ready for Use (ALL)  767
D.3    Configuring Excel Security for Add-In Usage 
(WIN)  767
D.4    Opening PHStat (ALL)  768
D.5    Using a Visual Explorations Add-in Workbook 
(ALL)  769
D.6    Checking for the Presence of the Analysis ToolPak 
or Solver Add-Ins (ALL)  769
E.  Tables  770
E.1    Table of Random Numbers  770
E.2    The Cumulative Standardized Normal 
Distribution  772
E.3    Critical Values of t  774
E.4    Critical Values of x2  776
E.5    Critical Values of F  777
E.6    Lower and Upper Critical Values, T1, of the 
Wilcoxon Rank Sum Test  781
E.7    Critical Values of the Studentized Range, Q  782
E.8    Critical Values, dI and dU, of the Durbin–Watson 
Statistic, D (Critical Values Are One-Sided)  784
E.9    Control Chart Factors  785
E.10  The Standardized Normal Distribution  786
F.  Useful Excel Knowledge  787
F.1    Useful Keyboard Shortcuts  787
F.2    Verifying Formulas and Worksheets  788
F.3    New Function Names  788
F.4    Understanding the Nonstatistical Functions  790
G.   Software FAQs  792
G.1    PHStat FAQs  792
G.2    Microsoft Excel FAQs  793
G.3    FAQs for New Users of Microsoft Excel 2013  794
G.4    Minitab FAQs  794
Self-Test Solutions and Answers to Selected 
Even-Numbered Problems	 795
Index  831

19
Preface
Over a generation ago, advances in “data processing” led to new business opportunities as first 
centralized and then desktop computing proliferated. The Information Age was born. Computer sci-
ence became much more than just an adjunct to a mathematics curriculum, and whole new fields of 
studies, such as computer information systems, emerged.
More recently, further advances in information technologies have combined with data analysis 
techniques to create new opportunities in what is more data science than data processing or com-
puter science. The world of business statistics has grown larger, bumping into other disciplines. 
And, in a reprise of something that occurred a generation ago, new fields of study, this time with 
names such as informatics, data analytics, and decision science, have emerged.
This time of change makes what is taught in business statistics and how it is taught all the 
more critical. These new fields of study all share statistics as a foundation for further learning. We 
are accustomed to thinking about change, as seeking ways to continuously improve the teaching 
of business statistics have always guided our efforts. We actively participate in Decision Sciences 
Institute (DSI), American Statistical Association (ASA), and Making Statistics More Effective in 
Schools and Business (MSMESB) conferences. We use the ASA’s Guidelines for Assessment and 
Instruction (GAISE) reports and combine them with our experiences teaching business statistics to 
a diverse student body at several large universities.
What to teach and how to teach it are particularly significant questions to ask during a time of 
change. As an author team, we bring a unique collection of experiences that we believe helps us find 
the proper perspective in balancing the old and the new. Our two lead authors, Mark L. Berenson 
and David M. Levine, were the first educators to create a business statistics textbook that discussed 
using statistical software and incorporated “computer output” as illustrations—just the first of many 
teaching and curricular innovations in their many years of teaching business statistics. Our newest 
co-author, Kathryn A. Szabat, has provided statistical advice to various business and nonbusiness 
communities. Her background in statistics and operations research and her experiences interacting 
with professionals in practice have guided her, as departmental chair, in developing a new, interdis-
ciplinary academic department, Business Systems and Analytics, in response to the technology- and 
data-driven changes in business today.
All three of us benefit from our many years teaching undergraduate business subjects and the 
diversity of interests and efforts of our past co-author, Timothy Krehbiel. We are pleased to offer the 
innovations and new content that are itemized starting on the next page. As in prior editions, we are 
guided by these key learning principles:
• Help students see the relevance of statistics to their own careers by providing examples 
drawn from the functional areas in which they may be specializing.
• Emphasize interpretation of statistical results over mathematical computation.
• Give students ample practice in understanding how to apply statistics to business.
• Familiarize students with how to use statistical software to assist business decision making.
• Provide clear instructions to students for using statistical applications.
Read more about these principles on page 27.
What’s New and Innovative in This Edition?
This thirteenth edition of Basic Business Statistics contains both new and innovative features and 
content, while refining and extending the use of the DCOVA (Define, Collect, Organize, Visualize, 
and Analyze) framework, first introduced in the twelfth edition as an integrated approach for apply-
ing statistics to help solve business problems.

20	
Preface
Innovations
Getting Started: Important Things to Learn First—In a time of change, you can never know exactly 
what knowledge and background students bring into an introductory business statistics classroom. 
Add that to the need to curb the fear factor about learning statistics that so many students begin with, 
and there’s a lot to cover even before you teach your first statistical concept.
We created “Getting Started: Important Things to Learn First” to meet this challenge. This 
unit sets the context for explaining what statistics is (not what students may think!) while ensur-
ing that all students share an understanding of the forces that make learning business statistics 
critically important today. Especially designed for instructors teaching with course management 
tools, including those teaching hybrid or online courses, “Getting Started” has been developed to 
be posted online or otherwise distributed before the first class section begins and is available for 
download as explained in Appendix C.
Student Tips—In-margin notes reinforce hard-to-master concepts and provide quick study tips for 
mastering important details.
Discussion of Business Analytics—“Getting Started: Important Things to Learn First” quickly 
defines business analytics and big data and notes how these things are changing the face of  
statistics.
This material serves as an introduction to the new “Business Analytics” chapter (Chapter 17).  
This new chapter begins with a scenario that uses the management of a theme park to introduce 
applications of business analytics. The chapter begins by discussing descriptive visualization 
methods used for general oversight and applies them to issues raised in the scenario. Using other 
examples, the chapter then discusses the predictive analytics methods classification and regres-
sion trees, neural nets, cluster analysis, and multidimensional scaling that are in common use 
today.
Because standard Microsoft Excel and Minitab offer little or no support for the methods dis-
cussed, the chapter uses results created using JMP, the interactive data analysis software from the 
SAS Institute, and Tableau Public, the Web-based data visualization tool from Tableau Software, 
where appropriate. For those interested, a special Software Guide located at the end of the chapter 
explains how to use these two programs (and Microsoft Excel) to construct the results shown in 
the chapter.
PHStat version 4—For Microsoft Excel users, this new version of the Pearson Education statistics 
add-in contains several new and enhanced procedures, simpler set up, and is compatible with both 
Microsoft Windows and (Mac) OS X Excel versions.
Chapter Short Takes Online PDF documents (available for download as explained in Appendix C) 
that supply additional insights or explanations to important statistical concepts or details about 
the results presented in this book.
Revised and Enhanced Content
New Continuing End-of-Chapter Cases—This thirteenth edition features several new end-of-chapter 
cases. New and recurring throughout the book is a case that concerns analysis of sales and market-
ing data for home fitness equipment (CardioGood Fitness), a case that concerns pricing decisions 
made by a retailer (Sure Value Convenience Stores), and the More Descriptive Choices Follow-Up 
case, which extends the use of the retirement funds sample first introduced in Chapter 2. Also recur-
ring is the Clear Mountain State Student Surveys case, which uses data collected from surveys of 
undergraduate and graduate students to practice and reinforce statistical methods learned in vari-
ous chapters. This case replaces end-of-chapter questions related to the student survey database in 
the previous edition. Joining the Mountain States Potato Company regression case of the previous 
edition are new cases in simple linear regression (Brynne Packaging) and multiple regression (The 
Craybill Instrumentation Company).
Many New Applied Examples and Problems—Many of the applied examples throughout this 
book use new problems or revised data. Approximately 44% of the problems are new to this 
­edition. The ends-of-section and ends-of-chapter problem sets contain many new problems that 
use data from The Wall Street Journal, USA Today, and other sources.
Revised Using Statistics Scenarios—There are new or revised Using Statistics scenarios in five 
chapters.

	
Preface	
21
Checklist for Preparing to Use Microsoft Excel or Minitab with This Book—Found in Section 
GS.4 of “Getting Started: Important Things to Learn First,” this checklist explains for students 
which skills they will need and where they will find information about those skills in the book.
Revised Appendices Keyed to the Preparing to Use Microsoft Excel Checklist—The revised 
Appendix B discusses the Excel skills that readers need to make best use of the In-Depth Excel 
instructions in this book. Appendix F presents useful Excel knowledge, including a discussion 
of the new worksheet function names that were introduced in Excel 2010. Appendix G presents 
FAQs about using Excel and Minitab with this book.
Configuring Microsoft Excel Appendix—This revised Appendix D discusses the procedures and 
practices that will help readers that use Microsoft Excel to avoid common technical problems that 
might otherwise arise as they learn business statistics with this book.
Distinctive Features
We have continued many of the traditions of past editions and have highlighted some of these 
­features below.
Using Statistics Business Scenarios—Each chapter begins with a Using Statistics example that 
shows how statistics is used in the functional areas of business—accounting, finance, information 
systems, management, and marketing. Each scenario is used throughout the chapter to provide an 
applied ­context for the concepts. The chapter concludes with a Using Statistics, Revisited section 
that ­reinforces the statistical methods and applications discussed in each chapter.
Emphasis on Data Analysis and Interpretation of Excel and Minitab Results—We believe 
that the use of computer software is an integral part of learning statistics. Our focus ­emphasizes 
­analyzing data by interpreting results while reducing emphasis on doing computations. For 
­example, in the coverage of tables and charts in Chapter 2, the focus is on the interpretation of 
various charts and on when to use each chart. In our coverage of hypothesis testing in Chapters 9  
through 12, and regression and multiple regression in Chapters 13 through 15, extensive com-
puter results have been included so that the p-value approach can be emphasized.
Pedagogical Aids—An active writing style is used, with boxed numbered equations, set-off 
­examples to provide reinforcement for learning concepts, student tips, problems divided into 
“Learning the Basics” and “Applying the Concepts,” key equations, and key terms.
Digital Cases—In the Digital Cases, available for download as explained in Appendix C, learners 
must examine interactive PDF documents to sift through various claims and information in order 
to discover the data most relevant to a business case scenario. Learners then determine whether 
the conclusions and claims are supported by the data. In doing so, learners discover and learn how 
to identify common misuses of statistical information. (Instructional tips for using the Digital 
Cases and solutions to the Digital Cases are included in the Instructor’s Solutions Manual.)
Answers—Most answers to the even-numbered exercises are included at the end of the book.
Flexibility Using Excel—For almost every statistical method discussed, this book presents more 
than one way of using Excel. Students can use In-Depth Excel instructions to directly work with 
worksheet solution details or they can use either the PHStat instructions or the Analysis ToolPak 
instructions to automate the creation of those worksheet solutions.
PHStat—PHStat is the Pearson Education statistics add-in that you use with Microsoft Excel to 
help build solutions to statistical problems. With PHStat, you fill in simple-to-use dialog boxes 
and watch as PHStat creates a worksheet solution for you. PHStat allows you to use the Microsoft 
Excel statistical functions without having to first learn advanced Excel techniques or worrying 
about building worksheets from scratch. As a student studying statistics, you can focus mainly on 
learning statistics and not worry about having to fully master Excel as well.
Unlike other programs, PHStat solutions are real worksheets that contain real Excel calculations 
(called formulas in Excel). You can examine the contents of worksheet solutions to learn the 
appropriate functions and calculations necessary to apply a particular statistical method. With 
most of these worksheet solutions, you can change worksheet data and immediately see how 
those changes affect the results. This book uses PHStat version 4 which includes over 60 proce-
dures that create Excel worksheets and charts for these statistical methods:
Descriptive Statistics: boxplot, descriptive summary, dot scale diagram, frequency distribution, 
histogram & polygons, Pareto diagram, scatter plot, stem-and-leaf display, one-way tables & 
charts, and two-way tables & charts

22	
Preface
Probability and probability distributions: simple & joint probabilities, normal probability plot, 
and binomial, exponential, hypergeometric, and Poisson probability distributions
Sampling: sampling distributions simulation
Confidence interval estimation: for the mean, sigma unknown; for the mean, sigma known, for 
the population variance, for the proportion, and for the total difference
Sample size determination: for the mean and the proportion
One-sample tests: Z test for the mean, sigma known; t test for the mean, sigma unknown;  
chi-square test for the variance; and Z test for the proportion
Two-sample tests (unsummarized data): pooled-variance t test, separate-variance t test, paired t 
test, F test for differences in two variances, and Wilcoxon rank sum test
Two-sample tests (summarized data): pooled-variance t test, separate-variance t test, paired t test, 
Z test for the differences in two means, F test for differences in two variances, chi-square test for 
differences in two proportions, Z test for the difference in two proportions, and McNemar test
Multiple-sample tests: chi-square test, Marascuilo procedure Kruskal-Wallis rank test, Levene 
test, one-way ANOVA, Tukey-Kramer procedure randomized block design, and two-way 
ANOVA with replication
Regression: simple linear regression, multiple regression, best subsets, stepwise regression, and 
logistic regression
Control charts: p chart, c chart, and R and Xbar charts.
Decision-making: covariance and portfolio management, expected monetary value, expected 
opportunity loss, and opportunity loss
Data preparation: stack and unstack data
See Appendix Section C.4 for more information about PHStat.
Visual Explorations—The series of Excel workbooks that allow students to interactively explore 
important statistical concepts in descriptive statistics, the normal distribution, sampling distribu-
tions, and regression analysis. For example, in descriptive statistics, students observe the effect 
of changes in the data on the mean, median, quartiles, and standard deviation. With the normal 
distribution, students see the effect of changes in the mean and standard deviation on the areas 
under the normal curve. In sampling distributions, students use simulation to explore the effect of 
sample size on a sampling distribution. In regression analysis, students have the opportunity to fit 
a line and observe how changes in the slope and intercept affect the goodness of fit. The Visual 
Explorations workbooks are available for download as explained in Appendix C. (See Appendix 
Section C.4 to learn more about the workbooks that comprise Visual Explorations.)
Chapter-by-Chapter Changes Made for This Edition
Besides the new and innovative content described in “What’s New and Innovative in This Edition?” the 
thirteenth edition of Basic Business Statistics contains the following specific changes to each chap-
ter. Highlights of the changes to the individual chapters are as follows:
Getting Started: Important Things to Learn First—This all-new chapter includes new material on 
business analytics and introduces the DCOVA framework and a basic vocabulary of statistics, both of 
which were introduced in Chapter 1 of the twelfth edition.
Chapter 1—Collecting data has been relocated to this chapter from Section 2.1. Sampling meth-
ods and types of survey errors have been relocated from Sections 7.1 and 7.2. There is a new  
subsection on data cleaning. The CardioGood Fitness and Clear Mountain State Surveys cases 
are included.
Chapter 2—Section 2.1, “Data Collection,” has been moved to Chapter 1. The chapter uses a new 
data set that contains a sample of 316 mutual funds and a new set of restaurant cost data. The 
CardioGood Fitness, The Choice Is Yours Follow-up, and Clear Mountain State Surveys cases 
are included.
Chapter 3—For many examples, this chapter uses the new mutual funds data set that is introduced 
in Chapter 2. There is increased coverage of skewness and kurtosis. There is a new example on 

	
Preface	
23
computing descriptive measures from a population using “Dogs of the Dow.” The CardioGood 
Fitness, More Descriptive Choices Follow-up, and Clear Mountain State Surveys cases are 
included.
Chapter 4—The chapter example has been updated. There are new problems throughout the chap-
ter. The CardioGood Fitness, The Choice Is Yours Follow-up, and Clear Mountain State Surveys 
cases are included.
Chapter 5—There is an additional example on applying probability distributions in finance, and 
there are many new problems throughout the chapter. The notation used has been made more 
consistent.
Chapter 6—This chapter has an updated Using Statistics scenario and some new problems. The 
CardioGood Fitness, More Descriptive Choices Follow-up, and Clear Mountain State Surveys 
cases are included.
Chapter 7—Sections 7.1 and 7.2 have been moved to Chapter 1. An additional example of sam-
pling distributions from a larger population has been included.
Chapter 8—This chapter includes an updated Using Statistics scenario and new examples and 
exercises throughout the chapter. The Sure Value Convenience Stores, CardioGood Fitness, More 
Descriptive Choices Follow-up, and Clear Mountain State Surveys cases are included. The sec-
tion “Applications of Confidence Interval Estimation in Auditing” has been moved online. There 
is an online section on bootstrapping.
Chapter 9—This chapter includes additional coverage of the pitfalls of hypothesis testing. The 
Sure Value Convenience Stores case is included.
Chapter 10—This chapter has an updated Using Statistics scenario, a new example on the paired 
t-test on textbook prices, and a new example on the Z-test for the difference between two pro-
portions. The Sure Value Convenience Stores, CardioGood Fitness, More Descriptive Choices 
Follow-up, and Clear Mountain State Surveys cases are included. There is a new online section 
on Effect Size.
Chapter 11—The chapter has a new Using Statistics scenario that relates to a mobile electron-
ics merchandiser that replaces the Perfect Parachutes scenario. This chapter includes the Sure 
Value Convenience Stores, CardioGood Fitness, More Descriptive Choices Follow-up, and Clear 
Mountain State Surveys cases. It now includes an online section on fixed effects, random effects, 
and mixed effects models.
Chapter 12—The chapter includes many new problems. This chapter includes the Sure Value 
Convenience Stores, CardioGood Fitness, More Descriptive Choices Follow-up, and Clear 
Mountain State Surveys cases. The McNemar test and the Chi-square test for a standard deviation 
or variance are now online sections.
Chapter 13—The Using Statistics scenario has been updated and changed, with new data used 
throughout the chapter. This chapter includes the Brynne Packaging case.
Chapter 14—The online section on influence analysis has been moved into the text.
Chapter 15—This chapter includes the Sure Value Convenience Stores, Craybill Instrumentation, 
and More Descriptive Choices Follow-up cases.
Chapter 16—This chapter includes new data involving movie attendance in Section 16.3 and 
updated data for The Coca-Cola Company in Sections 16.4 through 16.6 and Wal-Mart Stores, 
Inc., in Section 16.7. In addition, most of the problems are new or updated.
Chapter 17—This is the new business analytics chapter already discussed in Innovations on 
page 24. This chapter has been designed so that the descriptive methods or any of the predictive 
analytics methods can be taught separately and apart from the rest of the chapter should time not 
permit coverage of the entire chapter.
Chapter 18—This chapter now includes some new problems.
Chapter 19—The “Statistical Applications in Quality Management” chapter has been renumbered 
as Chapter 19 and moved online, where it is available for download as explained in Appendix C.
Chapter 20—The “Decision Making” chapter has been renumbered as Chapter 20 and remains 
available for download as explained in Appendix C.

24	
Preface
About Our Educational Philosophy
In Our Starting Point at the beginning of this preface, we stated that we are guided by these key learn-
ing principles:
• Help students see the relevance of statistics to their own careers by providing examples 
drawn from the functional areas in which they may be specializing.
• Emphasize interpretation of statistical results over mathematical computation.
• Give students ample practice in understanding how to apply statistics to business.
• Familiarize students with how to use statistical software to assist business decision making.
• Provide clear instructions to students for using statistical applications.
The following further explains these principles:
1.	
Help students see the relevance of statistics to their own careers by providing examples 
drawn from the functional areas in which they may be specializing. Students need a 
frame of reference when learning statistics, especially when statistics is not their major. That 
frame of reference for business students should be the functional areas of business, such as 
accounting, finance, information systems, management, and marketing. Each statistics topic 
needs to be presented in an applied context related to at least one of these functional areas. 
The focus in teaching each topic should be on its application in business, the interpretation 
of results, the evaluation of the assumptions, and the discussion of what should be done if 
the assumptions are violated.
2.	
Emphasize interpretation of statistical results over mathematical computation. 
Introductory business statistics courses should recognize the growing need to interpret statisti-
cal results that computerized processes create. This makes the interpretation of results more 
important than knowing how to execute the tedious hand calculations required to produce them.
3.	
Give students ample practice in understanding how to apply statistics to business. Both 
classroom examples and homework exercises should involve actual or realistic data as much 
as possible. Students should work with data sets, both small and large, and be encouraged 
to look beyond the statistical analysis of data to the interpretation of results in a managerial 
context.
4.	
Familiarize students with how to use statistical software to assist business decision 
making. Introductory business statistics courses should recognize that programs with sta-
tistical functions are commonly found on a business decision maker’s desktop computer. 
Integrating statistical software into all aspects of an introductory statistics course allows the 
course to focus on interpretation of results instead of computations (see point 2).
5.	
Provide clear instructions to students for using statistical applications. Books should explain 
clearly how to use programs such as Microsoft Excel and Minitab with the study of statistics, 
without having those instructions dominate the book or distract from the learning of statistical 
concepts.
Student Resources
Student Solutions Manual, by Professor Pin Tian Ng of Northern Arizona University and accu-
racy checked by Annie Puciloski, provides detailed solutions to virtually all the even-numbered  
exercises and worked-out solutions to the self-test problems.
Online resources—The complete set of online resources are discussed fully in Appendix C, which 
also explains how to download these resources. These resources include the Excel and Minitab 
Data Files that contain the data used in chapter examples or named in problems and end-of- 
chapter cases; the Excel Guide Workbooks that contain templates or model solutions for applying 
Excel to a particular statistical method; the Digital Cases PDF files that support the end-of-­chapter 
Digital Cases; the Visual Explorations Workbooks that interactively demonstrate various key 
statistical concepts; and the PHStat add-in that simplifies the use of Microsoft Windows or OS X 
Microsoft Excel with this book, as explained in Section EG.1.
The online resources also include the Chapter Short Takes and Online Topic Sections that 
expand and extend the discussion of statistical concepts worksheet-based solutions as well as 
the full text of two additional chapters, “Statistical Applications in Quality Management” and 
“Decision Making.”

	
Preface	
25
Instructor Resources
The following supplements are among the resources available to adopting instructors at the Instructor’s 
Resource Center, located at www.pearsonglobaleditions.com/Berenson.
• Instructor’s Solutions Manual, by Professor Pin Tian Ng of Northern Arizona University and 
accuracy checked by Annie Puciloski, includes solutions for end-of-section and end-of-chapter 
problems, answers to case questions, where applicable, and teaching tips for each chapter.
• Lecture PowerPoint Presentations, by Professor Patrick Schur of Miami University and 
accuracy checked by David Levine and Kathryn Szabat, are available for each chapter. The 
PowerPoint slides provide an instructor with individual lecture outlines to accompany the 
text. The slides include many of the figures and tables from the text. Instructors can use these 
lecture notes as is or can easily modify the notes to reflect specific presentation needs.
• Test Bank, by Professor Pin Tian Ng of Northern Arizona University, contains true/false, 
multiple-choice, fill-in, and problem-solving questions based on the definitions, concepts, 
and ideas developed in each chapter of the text. 
• TestGen® (www.pearsoned.com/testgen) enables instructors to build, edit, print, and 
administer tests using a computerized bank of questions developed to cover all the objec-
tives of the text. TestGen is algorithmically based, allowing instructors to create multiple but 
equivalent versions of the same question or test with the click of a button. Instructors can also 
modify test bank questions or add new questions. The software and test bank are available for 
download from Pearson Education’s online catalog.
MyStatLab™ Online Course (access code required) MyStatLab from Pearson is the world’s 
leading online resource for statistics learning, integrating interactive homework, assessment, and 
media in a flexible, easy to use format. MyStatLab is a course management systems that delivers 
proven results in helping individual students succeed.
• MyStatLab can be successfully implemented in any environment—lab-based, hybrid, fully 
online, traditional—and demonstrates the quantifiable difference that integrated usage has on 
student retention, subsequent success, and overall achievement.
• MyStatLab’s comprehensive online gradebook automatically tracks students’ results on tests, 
quizzes, and homework and in the study plan. Instructors can use the gradebook to pro-
vide positive feedback or intervene if students have trouble. Gradebook data can be easily 
exported to a variety of spreadsheet programs, such as Microsoft Excel. You can determine 
which points of data you want to export and then analyze the results to determine success.
MyStatLab provides engaging experiences that personalize, stimulate, and measure learning for 
each student. In addition to the resources below, each course includes a full interactive online ver-
sion of the accompanying textbook.
• Tutorial Exercises with Multimedia Learning Aids: The homework and practice exercises 
in MyStatLab align with the exercises in the textbook, and they regenerate algorithmically 
to give students unlimited opportunity for practice and mastery. Exercises offer immediate 
helpful feedback, guided solutions, sample problems, animations, videos, and eText clips for 
extra help at the point of use.
• MyStatLab Accessibility: MyStatLab is compatible with the JAWS 12/13 screen reader and 
enables multiple-choice and free-response problem types to be read and interacted with via 
keyboard controls and math notation input.
• StatTalk Videos: Fun-loving statistician Andrew Vickers takes to the streets of Brooklyn, 
NY to demonstrate important statistical concepts through interesting stories and real-life 
events. This series of 24 fun and engaging videos will help students actually understand sta-
tistical concepts. Available with an instructor’s user guide and assessment questions.
• Business Insight Videos: Ten engaging videos show managers at top companies using statis-
tics in their everyday work. Assignable question encourage debate and discussion.
• Additional Question Libraries: In addition to algorithmically regenerated questions that are 
aligned with your textbook, the MyStatLab courses come with two additional question libraries:
	 450 Getting Ready for Statistics covers the developmental math topics students need 
for the course. These can be assigned as a prerequisite to other assignments, if desired.
	 1000 Conceptual Question Library requires students to apply their statistical understanding.
MyStatLab™

26	
Preface
• Integration of Statistical Software: We make it easy to copy our data sets, both from the 
eText and the MyStatLab questions, into software such as StatCrunch, Minitab, Excel, and 
more. Students have access to a variety of support tools—Technology Tutorial Videos, 
Technology Study Cards, and Technology Manuals for select titles—to learn how to effec-
tively use statistical software.
• StatCrunch®: MyStatLab integrates the web-based statistical software StatCrunch within 
the online assessment platform so that students can easily analyze data sets from exercises 
and the text. In addition, MyStatLab includes access to www.statcrunch.com, a website 
where users can access tens of thousands of shared data sets, conduct online surveys, perform 
complex analyses using the powerful statistical software, and generate compelling reports.
And, MyStatLab comes from an experienced partner with educational expertise and an eye on the 
future.
• Knowing that you are using a Pearson product means knowing that you are using quality 
content. That means that our eTexts are accurate and our assessment tools work. 
• Whether you are just getting started with MyStatLab, or have a question along the way, we’re 
here to help you learn about our technologies and how to incorporate them into your course.
To learn more about how MyStatLab combines proven learning applications with powerful assess-
ment, visit www.mystatlab.com or contact your Pearson representative.
StatCrunch® is powerful web-based statistical software that allows users to perform complex anal-
yses, share data sets, and generate compelling reports of their data. The vibrant online community 
offers tens of thousands shared data sets for students to analyze.
Full access to StatCrunch is available with a MyStatLab access kit, and StatCrunch is available by 
itself to qualified adopters. StatCrunch is now compatible with most mobile devices. To access, visit 
www.statcrunch.com/mobile from the browser on your smartphone or tablet. For more informa-
tion, visit our website at www.statcrunch.com, or contact your Pearson representative.
Acknowledgments
We are extremely grateful to the RAND Corporation and the American Society for Testing and 
Materials for their kind permission to publish various tables in Appendix E, and to the American 
Statistical Association for its permission to publish diagrams from the American Statistician.
A Note of Thanks
We thank Mohammad Ahmadi, University of Tennessee Chattanooga; Shubbo Bandyopadhyay, 
University of Florida; William Borders, Troy University; Steven Garren, James Madison University; 
Jersy Kamburowski, University of Toledo; M. B. Khan, California State University Long Beach; 
Hui Min Li, West Chester University; Nelson Modeste, Tennessee State University; Chris Morgan, 
Purdue University; Patricia Mullins, University of Wisconsin; Yvonne Sandoval, University of 
Arizona; and Yan Yu, University of Cincinnati for their comments, which have made this a better 
book. We also appreciate and acknowledge the assistance of Jian Cao and Curt Hinrichs of the SAS 
Institute in helping us prepare some of the contents of the new Chapter 17.
Creating a new edition of a textbook is a team effort, and we would like to thank our Pearson 
Education editorial, marketing, and production teammates: Sonia Ashraf, Dana Bettez, Kathleen 
DeChavez, Erin Lane, Deirdre Lynch, Kathy Manley, Christine Stavrou, Marianne Stepanian, and 
Joe Vetere. We also thank our statistical reader and accuracy checker Annie Puciloski for her dil-
igence in checking our work and Nancy Kincade for overseeing and managing these efforts on 
behalf of PreMediaGlobal.
Finally, we would like to thank our families for their patience, understanding, love, and assis-
tance in making this book a reality. It is to them that we dedicate this book.

	
Preface	
27
Concluding Remarks
Please email us at authors@davidlevinestatistics.com if you have a question or require clarifica-
tion about something discussed in this book. We also invite you to communicate any suggestions 
you may have for a future edition of this book. And while we have strived to make this book both 
pedagogically sound and error-free, we encourage you to contact us if you discover an error. When 
contacting us electronically, please include “BBS edition 13” in the subject line of your message.
You can also visit davidlevinestatistics.com, where you will find an email contact form and 
links to additional information about this book. For technical assistance using Microsoft Excel or 
any of the Excel add-ins that you can use with this book including PHStat, review Appendices D 
and G and follow the technical support links discussed in Appendix Section G.1, if necessary.
Mark L. Berenson 
David M. Levine  
Kathryn A. Szabat
Pearson would like to thank and acknowledge the following contributors and reviewers for their 
work on the Global Edition. 
Contributors:
Kanika Gupta, European University Business School; Walid Alwagfi, Gulf University for Science 
and Technology. 
Reviewers:
Santosh Gopalkrishnan, Symbiosis Institute of Business Management; Rumki Bandyopadhyay, 
Institute for International Management and Technology; Ajender Kumar Malik, BITS Pilani.


29
U s i n g  S tat i s t i c s
“You Cannot Escape from Data”
Not so long ago, business students were unfamiliar with the word data and 
had little experience handling data. Today, every time you visit a search engine 
­website or “ask” your mobile device a question, you are handling data. And 
if you “check in” to a location or indicate that you “like” something, you are 
­creating data as well.
You accept as almost true the premises of stories in which characters 
­collect “a lot of data” to uncover conspiracies, to foretell disasters, or to catch a 
­criminal. You hear concerns about how the government or business might be able 
to “spy” on you in some ways or how large social media companies “mine” your 
personal data for profit.
You hear the word data everywhere and may even have a “data plan” for 
your smartphone. You know, in a general way, that data are facts about the  
world and that most data seem to be, ultimately, a set of numbers—that 49% of  
­students recently polled dreaded taking a business statistics course, or that 
50% of citizens believe the country is headed in the right direction, or that 
­unemployment is down 3%, or that your best friend’s social media account has 
835 friends and 202 recent posts.
You cannot escape from data in this digital world. What, then, should you 
do? You could try to ignore data and conduct business by relying on hunches or 
your “gut feelings.” However, if you only want to use gut feelings, then you prob-
ably shouldn’t be reading this book or taking business courses in the first place.
You could note that there is so much data in the world—or just in your 
own little part of the world—that you couldn’t possibly get a handle on it. 
You could accept other 
people’s data summaries 
and their conclusions 
without first reviewing 
the data yourself. That, 
of course, would expose 
yourself to fraudulent 
practices.
Or, you could 
do things the proper 
way and realize that 
you cannot escape 
­learning the methods of 
­statistics, the subject of 
this book . . .
contents
GS.1	 Statistics: A Way of Thinking
GS.2	 Data: What Is It?
GS.3	 Business Analytics: The 
Changing Face of Statistics
		
”Big Data” 
		
Statistics: An Important Part 
of Your Business Education
How to Use This Book
GS.4	 Software and Statistics
Excel Guide
EG.1	 Getting Started with 
Microsoft Excel
EG.2	 Entering Data
EG.3	 Opening and Saving 
Workbooks
EG.4	 Creating and Copying 
Worksheets
EG.5	 Printing Worksheets
Minitab Guide
MG.1	 Getting Started with Minitab
MG.2	 Entering Data
MG.3	 Opening and Saving 
Worksheets and Projects
MG.4	 Creating and Copying 
Worksheets
MG.5	 Printing Parts of a Project
Objectives
That the volume of data that exists 
in the world makes learning about 
statistics critically important
That statistics is a way of thinking 
that can help you make better 
decisions
How the DCOVA framework for 
applying statistics can help you 
solve business problems
What business analytics is and 
how these techniques represent 
an opportunity for you
How to make best use of this book
How to prepare for using Microsoft 
Excel or Minitab with this book
Important Things  
to Learn First
Getting 
Started
Angela Waye/Shutterstock

30	
Getting Started  Important Things to Learn First 
GS.1  Statistics: A Way of Thinking
Statistics is a way of thinking that can help you make better decisions. Statistics helps you 
solve problems that involve decisions that are based on data that have been collected. You may 
have had some statistics instruction in the past. If you ever created a chart to summarize data 
or calculated values such as averages to summarize data, you have used statistics. But there’s 
even more to statistics than these commonly taught techniques, as the detailed table of contents 
shows.
Statistics is undergoing important changes today. There are new ways of visualizing data 
that either did not exist, were not practical to do, or were not widely known until recently. And, 
more and more, statistics today is being used to “listen” to what the data might be telling you 
(the subject of Chapter 17) rather than just being a way to use data to prove something you 
want to say.
If you associate statistics with doing a lot of mathematical calculations, you will quickly 
learn that business statistics uses software to perform the calculations for you (and, generally, 
the software calculates with more precision and efficiency than you could do manually). But 
while you do not need to be a good manual calculator to apply statistics, because statistics is 
a way of thinking, you do need to follow a framework, or plan, to minimize possible errors of 
thinking and analysis. The DCOVA framework is one such framework.
The DCOVA Framework
The DCOVA framework consists of the following tasks:
• Define the data that you want to study in order to solve a problem or meet an objective.
• Collect the data from appropriate sources.
• Organize the data collected by developing tables.
• Visualize the data collected by developing charts.
• Analyze the data collected to reach conclusions and present those results.
The DCOVA framework uses the five tasks Define, Collect, Organize, Visualize, and Analyze 
to help apply statistics to business decision making. Typically, you do the tasks in the order 
listed. You must always do the first two tasks to have meaningful outcomes, but, in practice, 
the order of the other three can change or appear inseparable. Certain ways of visualizing data 
help you to organize your data while performing preliminary analysis as well. In any case, 
when you apply statistics to decision making, you should be able to identify all five tasks, and 
you should verify that you have done the first two tasks before the other three.
Using the DCOVA framework helps you to apply statistics to these four broad categories 
of business activities:
 • Summarize and visualize business data
 • Reach conclusions from those data
 • Make reliable forecasts about business activities
 • Improve business processes
Throughout this book, and especially in the Using Statistics scenarios that begin the chapters, 
you will discover specific examples of how DCOVA helps you apply statistics. For example, in 
one chapter, you will learn how to demonstrate whether a marketing campaign has increased 
sales of a product, while in another you will learn how a television station can reduce unneces-
sary labor expenses.

	
GS.2  Data: What Is It?	
31
GS.2  Data: What Is It?
Defining data in a general way as “facts about the world,” to quote the opening essay, can 
prove confusing as such facts could be singular, a value associated with something, or col-
lective, a list of values associated with something. For example, “David Levine” is a singular 
fact, a coauthor of this book, whereas “Mark, David, and Kathy” is the collective list of au-
thors of this book. Furthermore, if everything is data, how do you distinguish “David Levine” 
from “Basic Business Statistics,” two very different facts (coauthor and title) about this book. 
Statisticians avoid this confusion by using a more specific definition of data and by defining a 
second word, variable.
Data are “the values associated with a trait or property that help distinguish the occur-
rences of something.” For example, the names “David Levine” and “Kathryn Szabat” are data 
because they are both values that help distinguish one of the authors of this book from another. 
In this book, data is always plural to remind you that data are a collection, or set, of values. 
While one could say that a single value, such as “David Levine,” is a datum, the phrases data 
point, observation, response, and single data value are more typically encountered.
The trait or property of something that values (data) are associated with is what statisti-
cians define as a variable. For example, you might define the variables “coauthor” and “title” 
if you were defining data about a set of textbooks.
Substituting the word characteristic for the phrase “trait or property” and using the phrase 
“an item or individual” instead of the vague word “something” produces the definitions of 
variable and data used in this book.
Student Tip
Business convention 
places the data, the set 
of values, for a variable 
in a column when using 
a worksheet or similar 
object. The Excel and 
Minitab data worksheets 
used in this book follow 
this convention. Be-
cause of this convention, 
people sometimes use 
the word column as a 
substitute for variable.
Variable
A characteristic of an item or individual.
Data
The set of individual values associated with a variable.
Think about characteristics that distinguish individuals in a human population. Name, 
height, weight, eye color, marital status, adjusted gross income, and place of residence are all 
characteristics of an individual. All of these traits are possible variables that describe people.
Defining a variable called author-name to be the first and last names of the authors of this 
text makes it clear that valid values would be “Mark Berenson,” “David Levine,” and “Kathryn 
Szabat” and not, say, “Berenson,” “Levine,” and “Szabat.” Be careful of cultural or other 
assumptions in definitions—for example, is “last name” a family name, as is common usage 
in North America, or an individual’s own unique name, as is common usage in most Asian 
countries?
Having defined data and variable, you can define the subject of this book, statistics.
Statistics
The methods that help transform data into useful information for decision makers.
Statistics allows you to determine whether your data represent information that could be 
used in making better decisions. Therefore, statistics helps you determine whether differences 
in the numbers are meaningful in a significant way or are due to chance. To illustrate, consider 
the following news reports about various data findings:
 • “Acceptable Online Ad Length Before Seeing Free Content” (USA Today, February 
16, 2012, p. 1B) A survey of 1,179 adults 18 and over reported that 54% thought that  
15 seconds was an acceptable online ad length before seeing free content.

32	
Getting Started  Important Things to Learn First 
 • “First Two Years of College Wasted?” (M. Marklein, USA Today, January 18, 2011, 
p. 3A) A survey of more than 3,000 full-time, traditional-age students found that the 
students spent 51% of their time on socializing, recreation, and other activities; 9% of 
their time attending class/lab; and 7% of their time studying.
 • “Follow the Tweets” (H. Rui, A. Whinston, and E. Winkler, The Wall Street Journal, 
November 30, 2009, p. R4) In this study, the authors found that the number of times 
a specific product was mentioned in comments in the Twitter social messaging service 
could be used to make accurate predictions of sales trends for that product.
Without statistics, you cannot determine whether the “numbers” in these stories represent use-
ful information. Without statistics, you cannot validate claims such as the claim that the num-
ber of tweets can be used to predict the sales of certain products. And without statistics, you 
cannot see patterns that large amounts of data sometimes reveal.
When talking about statistics, you use the term descriptive statistics to refer to methods 
that primarily help summarize and present data. Counting physical objects in a kindergarten 
class may have been the first time you used a descriptive method. You use the term inferential 
statistics to refer to methods that use data collected from a small group to reach conclusions 
about a larger group. If you had formal statistics instruction in a lower grade, you were prob-
ably mostly taught descriptive methods, the focus of the early chapters of this book, and you 
may be unfamiliar with many of the inferential methods discussed in later chapters.
GS.3  Business Analytics: The Changing Face of Statistics
The Using Statistics scenario that opens this chapter notes the increasing use of new statistical 
techniques that either did not exist, were not practical to do, or were not widely known in the 
past. Of all these new techniques, business analytics best reflects the changing face of statis-
tics. These methods combine traditional statistical methods with methods from management 
science and information systems to form an interdisciplinary tool that supports fact-based 
management decision making. Business analytics enables you to
 • Use statistical methods to analyze and explore data to uncover unforeseen relationships.
 • Use management science methods to develop optimization models that impact an orga-
nization’s strategy, planning, and operations.
 • Use information systems methods to collect and process data sets of all sizes, including 
very large data sets that would otherwise be hard to examine efficiently.
Business analytics allows you to interpret data, reach conclusions, and make decisions 
and, in doing that, it combines many of the tasks of the DCOVA framework into one integrated 
process. And because you apply business analytics in the context of organizational decision 
making and problem solving (see reference 7), successful application of business analytics 
requires an understanding of a business and its operations. Chapter 17 examines business ana-
lytics more closely, including its implications for the future.
“Big Data”
Relatively recent advances in information technology allow businesses to collect, process, and 
analyze very large volumes of data. Because the operational definition of “very large” can be par-
tially dependent on the context of a business—what might be “very large” for a sole proprietorship 
might be commonplace and small for a multinational corporation—many use the term big data.
Big data is more of a fuzzy concept than a term with a precise operational definition, 
but it implies data that are being collected in huge volumes and at very fast rates (typically 
in real time) and data that arrive in a variety of forms, organized and unorganized. These 
attributes of “volume, velocity, and variety,” first identified in 2001 (see reference 5), make big 
data different from any of the data sets used in this book.
Big data increases the use of business analytics because the sheer size of these very large 
data sets makes preliminary exploration of the data using older techniques impractical to do. 
This effect is explored in Chapter 17.

	
GS.3  Business Analytics: The Changing Face of Statistics	
33
Statistics: An Important Part of Your Business Education
As business analytics becomes increasingly important in business, and especially as the use of 
big data increases, statistics, an essential component of business analytics, becomes increas-
ingly important to your business education. In the current data-driven environment of business, 
you need general analytical skills that allow you to manipulate data, interpret analytical re-
sults, and incorporate results in a variety of decision-making applications, such as accounting, 
finance, HR management, marketing, strategy/planning, and supply chain management.
The decisions you make will be increasingly based on data and not on gut or intuition sup-
ported by personal experience. Data-guided practice is proving to be successful; studies have 
shown an increase in productivity, innovation, and competition for organizations that embrace 
business analytics. The use of data and data analysis to drive business decisions cannot be 
ignored. Having a well-balanced mix of technical skills—such as statistics, modeling, and ba-
sic information technology skills—and managerial skills—such as business acumen, problem-
solving skills, and communication skills—will best prepare you for today’s, and tomorrow’s, 
workplace (see reference 1).
If you thought that you could artificially separate statistics from other business subjects, 
take a statistics course, and then forget about statistics, you have overlooked the changing face 
of statistics. The changing face is the reason that Hal Varian, the chief economist at Google, 
Inc., noted as early as 2009, “the sexy job in the next 10 years will be statisticians. And I’m not 
kidding” (see references 8 and 9).
This book helps you develop the skills necessary to use the DCOVA 
framework to apply statistics to the four types of business activities 
listed on page 30. Chapter 1 discusses the Define and Collect tasks, 
the necessary starting point for all statistical activities. Chapters 2 
and 3 explain the Organize and Visualize tasks and present methods 
that summarize and visualize business data (the first activity listed in 
Section GS.1). Chapter 3 also presents statistics used in the Analyze 
task. Chapters 4 through 12 discuss methods that use sample data 
to reach conclusions about populations (the second activity listed). 
Chapters 13 through 16 review methods to make reliable forecasts 
(the third activity). The online Chapter 19 introduces methods that 
you can use to improve business processes (the fourth activity) and 
the online Chapter 20 introduces decision-making methods. (As 
previously noted, Chapter 17 discusses business analytics.) Chapter 
18 summarizes the methods of this book and provides you with a 
roadmap for analyzing data.
Each chapter begins with a Using Statistics scenario that puts 
you in a realistic business situation. You will face problems that the 
statistical concepts and methods introduced in the chapter will help 
solve. Later, near the end of the chapter, a Using Statistics Revisited 
section reviews how the statistical methods discussed in the chapter 
can be applied to help solve the problems you faced.
Each chapter ends with a variety of features that help you re-
view what you have learned in the chapter. Summary, Key Equations, 
and Key Terms concisely present the important points of a chapter. 
Checking Your Understanding tests your understanding of basic con-
cepts, and Chapter Review Problems allow you to practice what you 
have learned.
Throughout this book, you will find Excel and Minitab solutions 
to example problems. You will also find many Student Tips, margin 
notes that help clarify and reinforce significant details about particu-
lar statistical concepts. Selected chapters include Visual Explorations 
features that allow you to interactively explore statistical concepts. 
And many chapters include a “Think About This” essay that explains 
important statistical concepts in further depth.
This book contains numerous case studies that give you an op-
portunity to enhance your analytic and communication skills. Appear-
ing in most chapters is the continuing case study Managing Ashland 
MultiComm Services that details problems managers of a residential 
telecommunications provider face and a Digital Case, which asks you 
to sort through information in electronic documents and then apply 
your statistical knowledge to resolve a business problem or issue. 
Besides these two cases, you will find a number of other cases, in-
cluding some that reoccur in several chapters, in this book.
Don’t worry if your instructor does not cover every section of 
every chapter. Introductory business statistics courses vary in terms 
of scope, length, and number of college credits earned. Your func-
tional area of study (accounting, management, finance, marketing, 
etc.) may also affect what you learn in class or what you are 
assigned to read in this book.
How to Use This Book

34	
Getting Started  Important Things to Learn First 
In later chapters, these guides are keyed to the in-chapter section numbers and present 
detailed Excel and Minitab instructions for performing the statistical methods discussed in 
chapter sections. Table GS.2 presents the typographic conventions that the guides use to pres-
ent computer operations. Excel Guides additionally identify the key Excel technique that is 
used for a statistical method and include instructions for using PHStat, the Pearson Education 
statistics add-in that simplifies the operation of Microsoft Excel.
GS.4  Software and Statistics
You use software to assist you in applying statistical methods to business decision making. 
Microsoft Excel and Minitab are examples of applications that people use for statistics. Excel 
is the Microsoft Office data analysis application that evolved from earlier electronic spread-
sheets used in accounting and financial applications. Minitab, a dedicated statistical applica-
tion, or statistical package, was developed from the ground up to perform statistical analysis 
as accurately as possible. Versions of Minitab run on larger computer systems and can perform 
sophisticated analyses of large data sets.
Although you are probably more familiar with Excel than with Minitab, both programs 
share many similarities, starting with their shared use of worksheets (or spreadsheets) to store 
data for analysis. Worksheets are tabular arrangements of data, in which the intersections of 
rows and columns form cells, boxes into which you make entries. In Minitab, the data for 
each variable are placed in separate columns, and this is also the standard practice when using 
Excel. Generally, to perform a statistical analysis in either program, you select one or more 
columns of data and then apply the appropriate command.
Both Excel and Minitab allow you to save worksheets, programming information, and 
results as one file, called a workbook in Excel and a project in Minitab. In Excel, workbooks 
are collections of worksheets and chart sheets. You save a workbook when you save “an ­Excel 
file” (either as an .xlsx or .xls file). In Minitab, a project includes data worksheets, all the 
results shown in a session window, and all graphs created for the data. Unlike in Excel, in 
Minitab you can save individual worksheets (as .mtw worksheet files) as well as save the en-
tire project (as an .mpj project file).
Excel and Minitab Guides
You can use either Excel or Minitab to learn and practice the statistical methods learned 
in this book. Immediately following each chapter are Excel and Minitab Guides. For this 
chapter, special guides explain how the guides have been designed to support your learning 
with this book. To prepare for using Excel or Minitab, review and complete the checklist in 
Table GS.1 below.
T a b l e  G S . 1
Checklist for Preparing 
to Use Excel or 
Minitab with This Book
❑  Determine which program, Excel or Minitab, you will use with this book.
❑  Read and review the Excel or Minitab Guide for this chapter to verify your knowledge of required 
basic skills.
❑  Read Appendix C to learn about the online resources you need to make best use of this book. 
Appendix C includes a complete list of the data files that are used in the examples and problems 
found in this book. Names of data files appear in this distinctive type face— Retirement Funds —
throughout this book.
❑  Download the online resources that you will need to use this book, using the instructions in Appendix C.
❑  Check for updates to the program that you plan to use with this book, using the Appendix Section 
D.1 instructions.
❑  If you plan to use Excel with PHStat, the Visual Explorations add-in workbooks, or the Analysis 
ToolPak and you maintain your own computer system, read the special instructions in Appendix D.
❑  Examine Appendix G to learn answers to frequently asked questions (FAQs).

	
Key Terms	
35
Operation and Examples
Notes
Keyboard keys 
  Enter Ctrl Shift
Names of keys are always the object of the verb press, as in 
“press Enter.”
Keystroke combinations 
  Ctrl+C 
  Ctrl+Shift+Enter 
  Command+Enter
Keyboarding actions that require you to press more than one key 
at the same time. Ctrl+C means press C while holding down 
Ctrl. Ctrl+Shift+Enter means press Enter while holding down 
both Ctrl and Shift.
Click or select operations
  click OK
  select the first 2-D Bar  
    gallery item
Mouse pointer actions that require you to single click an 
onscreen object. This book uses the verb select when the object 
is either a worksheet cell or an item in a gallery, menu, list, or 
Ribbon tab.
Menu or ribbon selection
  File ➔ New
  Layout ➔ Legend ➔ None
A sequence of Ribbon or menu selections. File ➔ New means 
first select the File tab and then select New from the list that 
appears.
Placeholder object 
  variable 1 cell range 
  bins cell range
An italicized boldfaced phrase is a placeholder for an object 
reference. In making entries, you enter the reference, e.g., 
A1:A10, and not the placeholder.
T a b l e  G S . 2
Computing Conventions 
Used in This Book
The guides presume that you have knowledge of the basic computing skills listed in Table 
GS.3. If you have not mastered these skills, you can read the online pamphlet Basic Computing 
Skills. (Appendix C explains how you can download a copy of this and other online sections.)
T a b l e  G S . 3
Basic Computing Skills
Basic Skill
Specifics
Identification of 
application window 
objects
Title bar, minimize/resize/close buttons, scroll bars, formula bar, 
workbook area, cell pointer, shortcut menu. For Excel only, panes 
and these Ribbon parts: tab, group, gallery, and launcher button
Knowledge of mouse 
operations
Click (also called select), check and clear, double-click, right-click, 
drag/drag-and-drop
Identification of dialog 
box objects
Command button, list box, drop-down list, edit box, option button, 
check box
Refere n c e s
	 1.	Advani, D. “Preparing Students for the Jobs of the Future.” 
University Business (2011), www.universitybusiness.com 
/article/preparing-students-jobs-future.
	 2.	Davenport, T., and J. Harris. Competing on Analytics: The 
New Science of Winning. Boston: Harvard Business School 
Press, 2007.
	 3.	Davenport, T., J. Harris, and R. Morison. Analytics at Work. 
Boston: Harvard Business School Press, 2010.
	 4.	Keeling, K., and R. Pavur. “Statistical Accuracy of Spreadsheet 
Software.” The American Statistician 65 (2011): 265–273.
	 5.	 Laney, D. 3D Data Management: Controlling Data Volume, Veloc-
ity, and Variety. Stamford, CT: META Group. February 6, 2001.
	 6.	Levine, D., and D. Stephan. “Teaching Introductory Business 
Statistics Using the DCOVA Framework.” Decision Sciences 
Journal of Innovative Education 9 (September 2011): 393–
398.
	 7.	Liberatore, M., and W. Luo. “The Analytics Movement.” 
­Interfaces 40 (2010): 313–324.
	 8.	Varian, H. “For Today’s Graduate, Just One Word: Statis-
tics.” The New York Times, August 6, 2009, www.nytimes 
.com/2009/08/06/technology/06stats.html.
	 9.	Varian, H. “Hal Varian and the Sexy Profession.” Significance, 
March 2011.
K ey Te r ms
big data  32
cells  34
data  31
DCOVA framework  30
descriptive statistics  32
inferential statistics  32
project  34
statistical package  34
statistics  31
template  36
variable  31
workbook  34
worksheet  34

36	
Getting Started  Important Things to Learn First 
E x c e l  G u i d e
EG.1  Getting Started with Microsoft Excel
You can use Excel to learn and apply the statistical methods discussed in this book and as an aid in 
solving end-of-section and end-of-chapter problems. How you use Excel is up to you (or perhaps your 
instructor), and the Excel Guides give you two complementary ways to use Excel.
If you are focused more on getting results as quickly as possible, consider using PHStat. PHStat, 
available for users of this book, is an example of an add-in, an application that extends the functionality 
of Microsoft Excel. The PHStat add-in simplifies the task of operating Excel while creating real Excel 
worksheets that use in-worksheet calculations. With PHStat, you can create worksheets that are identical 
to the ones featured in this book while minimizing the potential for making worksheet entry errors. In 
contrast, most other add-ins create results that are mostly text pasted into an empty worksheet.
For many topics, you may choose to use the In-Depth Excel instructions. These instructions use pre-con-
structed worksheets as models or templates for a statistical solution. You learn how to adapt these worksheets 
to construct your own solutions. Many of these sections feature a specific Excel Guide workbook that contains 
worksheets that are identical to the worksheets that PHStat creates. Because both of these ways create the same 
results and the same worksheets, you can use a combination of both ways as you read through this book. 
The In-Depth Excel instructions and the Excel Guide workbooks work best with the latest 
­versions of Microsoft Excel, including Excel 2010 and Excel 2013 (Microsoft Windows), Excel 
2011 (OS X), and Office 365. Where incompatibilities arise with versions older than Excel 2010, 
the incompatibilities are noted and alternative worksheets are provided for use. (Excel Guides 
also contain instructions for using the Analysis ToolPak add-in that is included with some 
­Microsoft Excel versions, when appropriate.)
You will want to master the Table EG.A basic skills before you begin using Excel to understand 
statistical concepts and solve problems. If you plan to use the In-Depth Excel instructions, you will also 
need to master the skills listed in the second half of the table. While you do not necessarily need these 
skills if you plan to use PHStat, knowing them will be useful if you expect to customize the Excel work-
sheets that PHStat creates or expect to be using Excel beyond the course that uses this book.
T a b l e  E G . A
Skills Set for Using 
Microsoft Excel with 
This Book
Basic Microsoft Office Skill
Specifics
Excel data entry
Organizing worksheet data in columns, entering numerical and 
categorical data
File operations
Open, save, print
Worksheet operations
Create, copy
In-Depth Excel Skill
Specifics
Formula skills
Concept of a formula, cell references, absolute and relative cell 
references, how to enter a formula, how to enter an array formula
Workbook presentation
How to apply format changes that affect the display of 
worksheet cell contents
Chart formatting correction
How to correct the formatting of charts that Excel improperly creates
Discrete histogram creation
How to create a properly formatted histogram for a discrete 
probability distribution
This guide reviews the basic Microsoft Office skills and Appendix B teaches you the In-Depth Excel 
skills. If you start by studying Sections B.1 through B.4 of that appendix, you will have the skills you 
need to make effective use of the In-Depth Excel instructions when you first encounter them in Chapter 1.  
(You can read other sections in Appendix B as needed.)
EG.2  Entering Data
As noted in Section GS.4, you enter data into the rows and columns of a worksheet. By convention, and 
the style used in this book, when you enter data for a set of variables, you enter the name of each variable 
into the cells of the first row, beginning with column A. Then you enter the data for the variable in the 
subsequent rows to create a DATA worksheet similar to the one shown in Figure EG.1.

	
Excel Guide	
37
F i g u r e  E G . 1
An example of a DATA 
worksheet
Student Tip
Most of the Excel data 
workbooks that you can 
download and use with 
this book (see Appen-
dix C) contain a DATA 
worksheet that follows 
the rules of this sec-
tion. You can use any of 
those worksheets as an 
additional model for data 
entry.
F i g u r e  E G . 2
Excel 2013 Open and 
Save As dialog boxes
You select the storage folder by using the drop-down list at the top of either of these dialog boxes. You 
enter, or select from the list box, a file name for the workbook in the File name box. You click Open or Save to 
complete the task. Sometimes when saving files, you may want to change the file type before you click Save.
In Microsoft Windows Excel versions, to save your workbook in the format used by versions older 
than Excel 2007, select Excel 97-2003 Workbook (*.xls) from the Save as type drop-down list before 
you click Save.
To save data in a form that can be opened by programs that cannot open Excel workbooks, you 
might select either Text (Tab delimited) (*.txt) or CSV (Comma delimited) (*.csv) as the save type. 
In OS X Excel versions, the equivalent selections are to select Excel 97–2004 Workbook (.xls), Tab 
Delimited Text (.txt), or Windows Comma Separated (.csv) from the Format drop-down list before 
you click Save.
When you want to open a file and cannot find its name in the list box, double-check that the current 
folder being searched is the proper folder. If it is, change the file type to All Files (*.*) (All Files in OS 
X Excel) to see all files in the current folder. This technique can help you discover inadvertent misspell-
ings or missing file extensions that otherwise prevent the file from being displayed.
Although all versions of Microsoft Excel include a Save command, you should avoid this choice 
until you gain experience. Using Save makes it too easy to inadvertently overwrite your work. Also, you 
cannot use the Save command for any open workbook that Excel has marked as read-only. (Use Save As 
to save such workbooks.)
To enter data in a specific cell, either use the cursor keys to move the cell pointer to the cell or use 
your mouse to select the cell directly. As you type, what you type appears in the formula bar. Complete 
your data entry by pressing Tab or Enter or by clicking the checkmark button in the formula bar.
When you enter data, never skip any rows in a column, and as a general rule, also avoid skip-
ping any columns. Also try to avoid using numbers as row 1 variable headings; if you cannot avoid 
their use, precede such headings with apostrophes. Pay attention to any special instructions that oc-
cur throughout the book for the order of the entry of your data. For some statistical methods, enter-
ing your data in an order that Excel does not expect will lead to incorrect results.
EG.3  Opening and Saving Workbooks
You open and save a workbook by first selecting the folder that stores the workbook and then speci-
fying the file name of the workbook. In most Excel versions, select File ➔ Open to open a workbook 
file and File ➔ Save As to save a workbook. (In Excel 2007, select Office Button ➔ Open to open a 
workbook file and Office Button ➔ Save As to save a workbook.) Open and Save As display nearly 
identical dialog boxes that vary only slightly among the different Excel versions. Figure EG.2 shows 
the Excel 2013 Open and Save As dialog boxes. To see these dialog boxes in Excel 2013, double-
click Computer in the Open or Save As panels, a step that other Excel versions do not require.

38	
Getting Started  Important Things to Learn First 
You can also make a copy of a worksheet or move a worksheet to another position in the same work-
book or to a second workbook. Right-click the sheet tab and select Move or Copy from the shortcut menu 
that appears. In the To book drop-down list of the Move or Copy dialog box (see Figure EG.3), first select 
(new book) (or the name of the pre-existing target workbook), check Create a copy, and then click OK.
EG.5  Printing Worksheets
To print a worksheet (or a chart sheet), click its sheet tab to open to the sheet. Then, in all Excel versions 
except Excel 2007, select File ➔ Print. If the print preview (partially obscured in Figure EG.4) is ac-
ceptable to you, click the Print button. To return to the worksheet, press Esc (Excel 2013), click File 
(Excel 2010), or Cancel (OS X Excel 2011).
If necessary, you can adjust print formatting while in print preview by clicking Page Setup to dis-
play the Page Setup dialog box (see Figure EG.4 inset). For example, to print your worksheet with 
gridlines and numbered row and lettered column headings (similar to the appearance of the worksheet 
onscreen), click the Sheet tab in the Page Setup dialog box, check Gridlines and Row and column 
headings, and click OK.
In Excel 2007, printing requires additional mouse clicks. First click Office Button and then move the 
mouse pointer over (but do not click) Print. In the Preview and Print gallery, click Print Preview. If the ­preview 
contains errors or displays the worksheet in an undesirable manner, click Close Print Preview, make the 
­necessary changes, and reselect the Print Preview. After completing all corrections and adjustments, click 
Print in the Print Preview window to display the Print dialog box. Select the printer to be used from the Name 
drop-down list, click All and Active sheet(s), adjust the Number of copies, and click OK.
F i g u r e  E G . 3
Worksheet tab shortcut 
menu (left) and the Move 
or Copy dialog box 
(right)
F i g u r e  E G . 4
Excel 2013 Print Preview 
and Page Setup (inset) 
dialog boxes
Student Tip
Although every version 
of Excel offers the (print) 
Entire workbook choice, 
you get the best results if 
you print each worksheet 
separately when you 
need to print more than 
one worksheet (or chart 
sheet).
EG.4  Creating and Copying Worksheets
You create new worksheets by either creating a new workbook or by inserting a new worksheet in an 
open workbook. In Microsoft Windows Excel versions, select File ➔ New (Office Button ➔ New in 
Excel 2007) and in the pane that appears, double-click the Blank workbook icon. In OS X Excel 2011, 
select File ➔ New Workbook.
New workbooks are created with a fixed number of worksheets. To delete extra worksheets or insert 
more sheets, right-click a sheet tab and click either Delete or Insert (see Figure EG.3). By default, Excel 
names a worksheet serially, in the form Sheet1, Sheet2, and so on. You should change these names to 
better reflect the content of your worksheets. To rename a worksheet, double-click the sheet tab of the 
worksheet, type the new name, and press Enter.

	
Minitab Guide	
39
M i n i ta b  G u i d e
Student Tip
The Minitab worksheets 
that you can download 
and use with this book 
(see Appendix C) follow 
the rules of this section. 
You can use any of those 
worksheets as an addi-
tional model for data entry.
F i g u r e  M G . 1
Minitab main worksheet 
with overlapping session, 
worksheet, and  
Project Manager 
windows
MG.2  Entering Data
Minitab uses the standard business convention, expecting data for a variable to be entered into a column. In 
this book, data are entered in columns, left to right, starting with the first column. Column names take the 
form Cn, such that the first column is named C1, the second column is C2, and the tenth column is C10. Col-
umn names appear in the top border of a Minitab worksheet. Columns that contain non-numerical data have 
names that include “-T” (C2-T and C3-T in Figure MG.1). Columns that contain data that Minitab interprets 
as either dates or times have names that include “-D” (not seen in Figure MG.1).
When entering data, you use the first, unnumbered and shaded row to enter variable names. You can 
then refer to the column by that name or its Cn name in Minitab procedures. If a variable name contains 
spaces or other special characters, such as Market Cap, Minitab will display that name in dialog boxes 
using a pair of single quotation marks ('Market Cap'). You must include those quotation marks any time 
you enter such a variable name in a dialog box.
To enter or edit data in a specific cell, either use the cursor keys to move the cell pointer to the cell 
or use your mouse to select the cell directly. Never skip any rows in a column, and as a general rule, also 
avoid skipping any columns. Minitab interprets a skipped row as holding a “missing” value in the sense 
discussed in Section 1.3.
MG.3  Opening and Saving Worksheets and Projects
You open and save Minitab worksheet or project files by first selecting the folder that stores a workbook 
and then specifying the file name of the workbook. To open a worksheet, select File ➔ Open Worksheet. 
To open a project, select File ➔ Open Project. To save a worksheet, select File ➔ Save Current Work-
sheet As. To save a project, select File ➔ Save Project As. Both pairs of open and save commands dis-
play nearly identical dialog boxes. Figure MG.2 shows the Minitab 16 Open Worksheet and Save Current 
Worksheet As dialog boxes.
Inside the Open or Save dialog boxes, you select the storage folder by using the drop-down list at 
the top of either dialog box. You enter or select from the list box a file name for the workbook in the File 
name box. You click Open or Save to complete the task. Sometimes when saving files, you might want 
to change the file type before you click Save. If you want to save your data as an Excel worksheet, select 
Excel from the Save as type drop-down list before you click Save. If you want to save data in a form that 
MG.1  Getting Started with MINITAB
You can use Minitab to learn and apply the statistical methods discussed in this book and as an aid in 
solving end-of-section and end-of-chapter problems. As explained in Section GS.4, in Minitab you cre-
ate and use project files that contain worksheets and other components.
When you start Minitab, you typically see a new project that contains only the session window and 
one worksheet window. These windows appear inside the Minitab window, and sometimes you may 
need to adjust the size of the Minitab window to see the entire window of a project component. In Figure 
MG.1, the Project Manager window that summarizes the content of the current project overlaps the 
session and DATA worksheet windows. You can arrange or hide these windows as you see fit. To view a 
particular window that may be obscured or hidden, select Window from the Minitab menu bar, and then 
select the name of the window you want to make visible.
Student Tip
When you save a project, 
you can click Options 
in the Save Project As 
dialog box to open the 
Save Project - Options 
dialog box to selectively 
save parts of a project.

40	
Getting Started  Important Things to Learn First 
can be opened by programs that cannot open Excel workbooks, you might select one of the Text or CSV 
choices as the Save as type type.
When you want to open a file and cannot find its name in the list box, double-check that the current 
Look in folder is the folder you intend. If it is, change the file type to All (*.*) to see all files in the cur-
rent folder. This technique can help you discover inadvertent misspellings or missing file extensions that 
otherwise prevent the file from being displayed.
Although Minitab includes Save Current Worksheet and a Save Project commands (commands 
without the “As”), you should avoid this choice until you gain experience. Using Save makes it too 
easy to inadvertently overwrite your work. Also, you cannot use the Save command for any open 
workbook that Minitab has marked as read-only. (Use Save As to save such workbooks.)
Individual graphs and a project’s session window can also be opened and saved separately in 
Minitab, although these operations are never used in this book.
F i g u r e  M G . 2
Minitab 16 Open 
Worksheet and Save 
Current Worksheet As 
dialog boxes
MG.4  Creating and Copying 
Worksheets
You create new worksheets by either creating a new project or by 
inserting a new worksheet in an open project. To create a new proj-
ect, select File ➔ New and in the New dialog box, click Minitab 
Project and then click OK. To insert a new worksheet, also select 
File ➔ New but in the New dialog box click Minitab Worksheet 
and then click OK.
A new project is created with one new worksheet. To insert 
another worksheet, select File ➔ New and in the New dialog box 
click Minitab Worksheet and then click OK. You can also insert 
a copy of a worksheet from another project into the current proj-
ect. Select File ➔ Open Worksheet and select the project that 
contains the worksheet to be copied. Selecting a project (and not 
a worksheet) causes an additional dialog box to be displayed, in 
which you can specify which worksheets of that second project 
are to be copied and inserted into the current project.
By default, Minitab names a worksheet serially in the form 
Worksheet1, Worksheet2, and so on. You should change these 
names to better reflect the content of your worksheets. To rename a 
worksheet, open the Project Manager window (see Figure MG.1), 
right-click the worksheet name in the left pane, select Rename 
from the shortcut menu, type in the new name, and press Enter. 
You can also use the Save Current Worksheet As command dis-
cussed in Section MG.3, although this command also saves the 
worksheet as a separate file.
MG.5  Printing Parts of a Project
To print a worksheet, a graph, or the contents of a session, first select 
the window that corresponds to the object you want to print. Then se-
lect File ➔ Print object, where object is either Worksheet, Graph, 
or Session Window, depending on which object you first selected.
If you are printing a graph or a session window, selecting the 
Print command displays the Print dialog box. The Print dialog box 
contains settings to select the printer to be used, what pages to print, 
and the number of copies to produce. If you need to change these set-
tings, change them before clicking OK to create your printout.
If you are printing a worksheet, selecting Print Worksheet 
displays the Data Window Print Options dialog box (shown  
below). In this dialog box, you specify the formatting options for 
your printout (the default selections should be fine), enter a Title, 
and click OK. Minitab then 
presents the Print dialog box 
discussed in the previous 
paragraph.
If you need to change 
the paper size or paper orien-
tation of your printout, select 
File ➔ Print Setup before 
you select the Print com-
mand, make the appropriate 
selections in the dialog box 
that appears, and click OK.

41
U s i n g  S tat i s t i c s
Beginning of the End … Or the End of the 
Beginning?
The past few years have been challenging for Good Tunes & More (GT&M), a 
business that traces its roots to Good Tunes, a store that exclusively sold music 
CDs and vinyl records.
GT&M first broadened its merchandise to include home entertainment 
and computer systems (the “More”), and then undertook an expansion to take 
advantage of prime locations left empty by bankrupt former competitors.  
Today, GT&M finds itself at a crossroads. Hoped-for increases in revenues 
that have failed to occur and declining profit margins due to the competitive 
pressures of online sellers have led management to reconsider the future of  
the business.
While some investors in the business have argued for an orderly retreat, 
­closing stores and limiting the variety of merchandise, GT&M CEO Emma ­Levia 
has decided in a time of uncertainty to “double down” and expand the ­business 
by purchasing Whitney Wireless, a successful three-store chain that sells 
­smartphones and other mobile devices.
Levia foresees creating a brand new “A-to-Z” electronics retailer but 
first must establish a fair and reasonable price for the privately held Whitney 
­Wireless. To do so, she has asked a group of analysts to identify, define, and 
collect the data that would be helpful in setting a price for the wireless business. 
As part of that group, you quickly realize that you need the data that would help 
to verify the contents of the wireless company’s basic financial statements.
You focus on data associated with the company’s profit and loss statement 
and quickly realize the need for sales and expense-­related ­variables. You begin to 
think about what the 
data for such variables 
would look like and 
how to collect those 
data. You realize that 
you are starting to 
apply the DCOVA 
framework to the 
objective of helping 
Levia acquire Whitney 
Wireless.
contents
1.1	 Defining Data
1.2	 Measurement Scales for 
Variables
1.3	 Collecting Data
1.4	 Types of Sampling Methods
1.5	 Types of Survey Errors
Think About This: New Media 
Surveys/Old Sampling Problems
Using Statistics: Beginning of 
the End … Revisited
Chapter 1 Excel Guide
Chapter 1 Minitab Guide
Objectives
To understand the types of 
variables used in statistics
To know the different 
measurement scales 
To know how to collect data
To know the different ways to 
collect a sample
To understand the types of  
survey errors
Chapter
Defining and  
Collecting Data
1
Tyler Olson/Shutterstock

42	
Chapter 1  Defining and Collecting Data 
D
efining a business objective is only the beginning of the process of business decision 
making. In the GT&M scenario, the objective is to establish a fair and reasonable 
price for the company to be acquired. Establishing a business objective always pre-
cedes the application of statistics to business decision making. Business objectives can arise 
from any level of management and can be as varied as the following:
 • A marketing analyst needs to assess the effectiveness of a new television advertisement.
 • A pharmaceutical company needs to determine whether a new drug is more effective 
than those currently in use.
 • An operations manager wants to improve a manufacturing or service process.
 • An auditor wants to review the financial transactions of a company in order to determine 
whether the company is in compliance with generally accepted accounting principles.
Establishing an objective is the end of what some would label problem definition, the formal 
beginning of any business decision-making process. But establishing the objective also marks 
a beginning—of applying the DCOVA framework to the task at hand.
Recall from Section GS.1 that the DCOVA framework uses the five tasks Define, Collect, 
Organize, Visualize, and Analyze to help apply statistics to business decision making. Restated, 
using the definition of a variable on page 30, the DCOVA framework consists of these tasks:
 • Define the variables that you want to study in order to solve a problem or meet an 
­objective.
 • Collect the data for those variables from appropriate sources.
 • Organize the data collected by developing tables.
 • Visualize the data collected by developing charts.
 • Analyze the data collected to reach conclusions and present those results.
In this chapter, you will learn more about the Define and Collect tasks.
Defining the variables that you want to study in order to solve a problem or meet an objective, 
the D task in the DCOVA framework, involves more than just making a list of things to study. 
For each variable of interest that you identify you must supply an operational definition, a 
universally accepted meaning that is clear to all associated with an analysis. This definition 
should clearly identify the values of the variable necessary to ensure that collected data are ac-
ceptable and appropriate for analysis.
For example, in the “Beginning of the End …” scenario, sales per year data might be of 
interest to you as you focus on the data associated with Whitney Wireless. But the variable 
yearly sales might be subject to miscommunication: Does the variable refer to sales per year 
for the entire chain or just to one individual store? Are the values of the variable defined as dol-
lar sales or unit sales? Providing an operational definition assures that such miscommunication 
does not occur.
When defining categorical variables, even individual values for a variable may need to be 
defined. For example, in a famous example, researchers collecting demographic data asked 
persons to fill in a form, one line of which asked about sex. More than one person supplied 
the answer yes and not the male or female value that the researchers intended. (Perhaps this is 
the reason that such a variable is more typically today named Gender—gender’s operational 
definition is more self-apparent.)
Establishing the Variable Type
As part of defining a variable, you establish whether the variable is a categorical or numerical 
variable. Categorical variables (also known as qualitative variables) have values that can 
only be placed into categories such as yes and no. Gender (male or female) is a categorical 
1.1  Defining Data
Student Tip
Providing operational 
definitions are important 
in other contexts such as 
when writing a book about 
business statistics. Go 
back to page 31 and dis-
cover the first operational 
definitions of this book, for 
the easily misconstrued 
words variable and data! 
Like those definitions, 
all important operational 
definitions in this book are 
highlighted in boxes similar 
to one found on page 31.

	
1.2  Measurement Scales for Variables	
43
variable. So, too are “Do you have a Facebook profile?” (yes or no) and Student class designa-
tion (Freshman, Sophomore, Junior, or Senior). Numerical variables (also known as quanti-
tative variables) have values that represent a counted or measured quantity.
Numerical variables are further identified as being either discrete or continuous variables. 
Discrete variables have numerical values that arise from a counting process. “Number of 
items purchased” is a discrete numerical variable because its values represent the count of the 
number of items purchased. Continuous variables have numerical values that arise from a 
measuring process. “The time spent waiting on a checkout line” is an example of a continuous 
numerical variable because its values can represent a measurement with a stopwatch.
Values of a continuous variable can take on any value within a continuum or an interval, 
depending on the precision of the measuring instrument. For example, the waiting time could 
be 1 minute, 1.1 minutes, 1.11 minutes, or 1.113 minutes, depending on the precision of the 
stopwatch used. (Theoretically, a perfectly precise measuring device would never generate two 
identical continuous values for the same variable. However, because no measuring device is so 
precise, identical continuous values can occur.)
At first glance, identifying the variable type may seem easy, but some variables that you 
might want to study could be either categorical or numerical, depending on how you define 
them. For example, “age” would seem to be an obvious numerical variable, but what if you are 
interested in comparing the buying habits of children, young adults, middle-aged persons, and 
retirement-age people? In that case, defining “age” as a categorical variable would make better 
sense. Again, this illustrates the earlier point that without operational definitions, variables are 
meaningless.
Asking questions about the variables you have identified for study can often be a great 
help in determining the type of variable you have. Table 1.1 illustrates the process.
Learn More
Read the Short Takes for 
Chapter 1 to learn more 
about determining the type 
of variable.
T a b l e  1 . 1
Identifying Types of 
Variables
Question
Responses
Data Type
Do you have a Facebook  
profile?
❑ Yes  ❑ No
Categorical
How many text messages have  
you sent in the past three days?
______
Numerical 
(discrete)
How long did it take to  
download the update for your  
newest mobile app?
______ seconds
Numerical 
(continuous)
Variables can be further identified by the level of measurement, or measurement scale. Stat-
isticians use the terms nominal scale and ordinal scale to describe the values for a categorical 
variable and use the terms interval scale and ratio scale to describe the values for a numerical 
variable.
Nominal and Ordinal Scales
Values for a categorical variable are measured on a nominal scale or on an ordinal scale.  
A nominal scale (see Table 1.2) classifies data into distinct categories in which no ranking 
is implied. Examples of a nominal scaled variable are your favorite soft drink, your political 
party affiliation, and your gender. Nominal scaling is the weakest form of measurement be-
cause you cannot specify any ranking across the various categories.
An ordinal scale classifies values into distinct categories in which ranking is implied. 
For example, suppose that GT&M conducted a survey of customers who made a purchase and 
asked the question “How do you rate the overall service provided by Good Tunes & More dur-
ing your most recent purchase?” to which the responses were “excellent,” “very good,” “fair,” 
1.2  Measurement Scales for Variables
Learn More
Read the Short Takes for 
Chapter 1 to learn more 
about nominal and ordinal 
scales.

44	
Chapter 1  Defining and Collecting Data 
and “poor.” The answers to this question represent an ordinal scaled variable because the  
responses “excellent,” “very good,” “fair,” and “poor” are ranked in order of satisfaction.  
Table 1.3 lists other examples of ordinal scaled variables.
T a b l e  1 . 2
Examples of Nominal 
Scales
Categorical Variable
Categories
Do you have a  
Facebook profile?
❑ Yes  ❑ No
Type of investment
❑ Cash  ❑ Mutual funds  ❑ Other
Cellular provider
❑ AT&T  ❑ Sprint  ❑ Verizon  ❑ Other  ❑ None
Ordinal scaling is a stronger form of measurement than nominal scaling because an ob-
served value classified into one category possesses more of a property than does an observed 
value classified into another category. However, ordinal scaling is still a relatively weak form 
of measurement because the scale does not account for the amount of the differences between 
the categories. The ordering implies only which category is “greater,” “better,” or “more pre-
ferred”—not by how much.
Interval and Ratio Scales
Values for a numerical variable are measured on an interval scale or a ratio scale. An interval 
scale (see Table 1.4) is an ordered scale in which the difference between measurements is a 
meaningful quantity but does not involve a true zero point. For example, a noontime tempera-
ture reading of 67° Fahrenheit is 2 degrees warmer than a noontime reading of 65°. In addition, 
the 2° Fahrenheit difference in the noontime temperature readings is the same as if the two 
noontime temperature readings were 74° and 76° Fahrenheit because the difference has the 
same meaning anywhere on the scale.
T a b l e  1 . 3
Examples of Ordinal 
Scales
Categorical Variable
Ordered Categories
Student class designation
Freshman  Sophomore  Junior  Senior
Product satisfaction
Very unsatisfied  Fairly unsatisfied
Neutral  Fairly satisfied Very satisfied
Faculty rank
Professor  Associate Professor
Assistant Professor  Instructor
Standard & Poor’s investment grade ratings
AAA  AA+  AA  AA−  A+ A  BBB
Course grade
A  B  C  D  F
T a b l e  1 . 4
Examples of Interval 
and Ratio Scales
Numerical Variable
Level of Measurement
Temperature (in degrees  
Celsius or Fahrenheit)
Interval
ACT or SAT standardized  
exam score
Interval
File download time  
(in seconds)
Ratio
Age (in years  
or days)
Ratio
Cost of a computer system  
(in U.S. dollars)
Ratio

	
1.2  Measurement Scales for Variables	
45
A ratio scale is an ordered scale in which the difference between the measurements in-
volves a true zero point, as in height, weight, age, or salary measurements. If GT&M con-
ducted a survey and asked how much money you expected to spend on audio equipment in 
the next year, the responses to such a question would be an example of a ratio-scaled variable. 
A person who expects to spend $1,000 on audio equipment expects to spend twice as much 
money as someone who expects to spend $500. As another example, a person who weighs 
240 pounds is twice as heavy as someone who weighs 120 pounds.
Temperature is a trickier case: Fahrenheit and Celsius scales are interval but not ratio 
scales; the “zero” value is arbitrary, not real. You cannot say that a noontime temperature read-
ing of 4° Fahrenheit is twice as hot as 2° Fahrenheit. In contrast, a Kelvin temperature reading 
is ratio scaled. In this scale, the temperature 0° Kelvin means no molecular motion.
Data measured on an interval scale or on a ratio scale constitute the highest levels of mea-
surement. They are stronger forms of measurement than an ordinal scale because you can de-
termine not only which observed value is the largest but also by how much.
Learn More
Read the Short Takes for 
Chapter 1 to learn more 
about interval and ratio 
scales.
Problems for Sections 1.1 and 1.2
Learning the Basics
1.1  A home delivery restaurant has segmented its delivery in 
north, south, east and west zones.
a.	 Explain why the four zones are an example of a categorical 
variable.
b.	 Explain why the four zones are an example of a nominal scaled 
variable.
1.2  A hotel offers comfort, deluxe, and luxury rooms. Explain why 
the type of hotel rooms is an example of an ordinal scaled variable.
1.3  The winners in an Olympic race are decided according to the 
recorded time up to the hundredth decimal point.
a.	 Explain why the recorded time is a continuous numerical  
variable.
b.	 Explain why the recorded time is a ratio-scaled variable.
Applying the Concepts
SELF 
Test 
1.4  For each of the following variables, determine 
whether the variable is categorical or numerical. If the 
variable is numerical, determine whether the variable is discrete or 
continuous. In addition, determine the measurement scale.
a.	 Number of cellphones in the household
b.	 Monthly data usage (in MB)
c.	 Number of text messages exchanged per month
d.	 Voice usage per month (in minutes)
e.	 Whether the cellphone is used for email
1.5  The following information is collected from students upon 
exiting the campus bookstore during the first week of classes.
a.	 Amount of time spent shopping in the bookstore
b.	 Number of textbooks purchased
c.	 Academic major
d.	 Gender
Classify each of these variables as categorical or numerical. If the 
variable is numerical, determine whether the variable is discrete or 
continuous. In addition, determine the measurement scale for each 
of these variables.
1.6  For each of the following variables, determine whether the 
variable is categorical or numerical. If the variable is numerical, 
determine whether the variable is discrete or continuous. In addi-
tion, determine the measurement scale for each variable.
a.	 Name of Internet service provider
b.	 Time, in hours, spent surfing the Internet per week
c.	 Whether the individual uses a mobile phone to connect to the 
Internet
d.	 Number of online purchases made in a month
e.	 Where the individual uses social networks to find sought-after 
information
1.7  For each of the following variables, determine whether the 
variable is categorical or numerical. If the variable is numerical, 
determine whether the variable is discrete or continuous. In addi-
tion, determine the measurement scale for each variable.
a.	 Amount of money spent on clothing in the past month
b.	 Favorite department store
c.	 Most likely time period during which shopping for clothing 
takes place (weekday, weeknight, or weekend)
d.	 Number of pairs of shoes owned
1.8  Suppose the following information is collected from Robert 
Keeler on his application for a home mortgage loan at the Metro 
County Savings and Loan Association.
a.	 Monthly payments: $2,227
b.	 Number of jobs in past 10 years: 1
c.	 Annual family income: $96,000
d.	 Marital status: Married
Classify each of the responses by type of data and measurement scale.
1.9  One of the variables most often included in surveys is in-
come. Sometimes the question is phrased “What is your income 
(in thousands of dollars)?” In other surveys, the respondent is 
asked to “Select the circle corresponding to your income level” 
and is given a number of income ranges to choose from.
a.	 In the first format, explain why income might be considered 
either discrete or continuous.
b.	 Which of these two formats would you prefer to use if you 
were conducting a survey? Why?

46	
Chapter 1  Defining and Collecting Data 
1.10  If two students score a 90 on the same examination, what 
arguments could be used to show that the underlying variable—
test score—is continuous?
1.11  The director of market research at a large department store 
chain wanted to conduct a survey throughout a metropolitan area 
to determine the amount of time working women spend shopping 
for clothing in a typical month.
a.	 Indicate the type of data the director might want to collect.
b.	 Develop a first draft of the questionnaire needed in (a) by writ-
ing three categorical questions and three numerical questions 
that you feel would be appropriate for this survey.
1.3  Collecting Data
After defining the variables that you want to study, you can proceed with the data collection 
task. Collecting data is a critical task because if you collect data that are flawed by biases, 
ambiguities, or other types of errors, the results you will get from using such data with even 
the most sophisticated statistical methods will be suspect or in error. (For a famous example of 
flawed data collection leading to incorrect results, read the Think About This essay on page 54.)
Data collection consists of identifying data sources, deciding whether the data you collect 
will be from a population or a sample, cleaning your data, and sometimes recoding variables. 
The rest of this section explains these aspects of data collection.
Data Sources
You collect data from either primary or secondary data sources. You are using a primary data 
source if you collect your own data for analysis. You are using a secondary data source if the 
data for your analysis have been collected by someone else.
You collect data by using any of the following:
 • Data distributed by an organization or individual
 • The outcomes of a designed experiment
 • The responses from a survey
 • The results of conducting an observational study
 • Data collected by ongoing business activities
Market research companies and trade associations distribute data pertaining to specific indus-
tries or markets. Investment services such as Mergent, Inc., provide business and financial data on 
publicly listed companies. Syndicated services sold by The Nielsen Company provide consumer 
research data to telecom and mobile media companies. Print and online media companies also 
distribute data that they may have collected themselves or may be republishing from other sources.
The outcomes of a designed experiment are a second data source. For example, a con-
sumer goods company might conduct an experiment that compares the stain-removing abilities 
of several laundry detergents. Note that developing a proper experimental design is mostly 
beyond the scope of this book, but Chapters 10 and 11 discuss some of the fundamental experi-
mental design concepts.
Survey responses represent a third type of data source. People being surveyed are asked 
questions about their beliefs, attitudes, behaviors, and other characteristics. For example, peo-
ple could be asked which laundry detergent has the best stain-removing abilities. (Such a sur-
vey could lead to data that differ from the data collected from the outcomes of the designed 
experiment of the previous paragraph.) Surveys can be affected by any of the four types of 
errors that are discussed in Section 1.5.
Observational study results are a fourth data source. A researcher collects data by directly 
observing a behavior, usually in a natural or neutral setting. Observational studies are a com-
mon tool for data collection in business. For example, market researchers use focus groups 
to elicit unstructured responses to open-ended questions posed by a moderator to a target au-
dience. Observational studies are also commonly used to enhance teamwork or improve the 
quality of products and services.
Data collected by ongoing business activities are a fifth data source. Such data can be col-
lected from operational and transactional systems that exist in both physical “bricks-and-mortar” 
and online settings but can also be gathered from secondary sources such as third-party social 

	
1.3  Collecting Data	
47
media networks and online apps and website services that collect tracking and usage data. For ex-
ample, a bank might analyze a decade’s worth of financial transaction data to identify patterns of 
fraud, and a marketer might use tracking data to determine the effectiveness of a website.
Sources for “big data” (see Section GS.3) tend to be a mix of primary and secondary 
sources of this last type. For example, a retailer interested in increasing sales might mine 
­Facebook and Twitter accounts to identify sentiment about certain products or to pinpoint top 
influencers and then match those data to its own data collected during customer transactions.
Populations and Samples
You collect your data from either a population or a sample. A population consists of all the 
items or individuals about which you want to reach conclusions. All the GT&M sales trans-
actions for a specific year, all the customers who shopped at GT&M this weekend, all the 
full-time students enrolled in a college, and all the registered voters in Ohio are examples of 
populations. In Chapter 3, you will learn that when you analyze data from a population you 
compute parameters.
A sample is a portion of a population selected for analysis. The results of analyzing a 
sample are used to estimate characteristics of the entire population. From the four examples of 
populations just given, you could select a sample of 200 GT&M sales transactions randomly 
selected by an auditor for study, a sample of 30 GT&M customers asked to complete a cus-
tomer satisfaction survey, a sample of 50 full-time students selected for a marketing study, and 
a sample of 500 registered voters in Ohio contacted via telephone for a political poll. In each 
of these examples, the transactions or people in the sample represent a portion of the items or 
individuals that make up the population. In Chapter 3, you will learn that when you analyze 
data from a sample you compute statistics.
Data collection will involve collecting data from a sample when any of the following con-
ditions hold:
 • Selecting a sample is less time consuming than selecting every item in the population.
 • Selecting a sample is less costly than selecting every item in the population.
 • Analyzing a sample is less cumbersome and more practical than analyzing the entire 
population.
Data Formatting
The data you collect may be formatted in more than one way. For example, suppose that you 
wanted to collect electronic financial data about a sample of companies. The data you seek to 
collect could be formatted in any number of ways, including the following:
 • Tables of data
 • Contents of standard forms
 • A continuous data stream, such as a stock ticker
 • Messages delivered from social media websites and networks
These examples illustrate that data can exist either in a structured or unstructured form. Struc-
tured data is data that follows some organizing principle or plan, typically a repeating pattern. 
For example, a simple stock ticker is structured because each entry would have the name of a 
company, the number of shares last traded, the bid price, and percent change in the stock price 
that the transaction represents. Due to their inherent organization, tables and forms are also 
structured. In a table, each row contains a set of values for the same columns (i.e., variables), 
and in a set of forms, each form contains the same set of entries. For example, once we identify 
that the second column of a table or the second entry on a form contains the family name of an 
individual, then we know that all entries in the second column of the table or all of the second 
entries in all copies of the form contain the family name of an individual.
In contrast, unstructured data follows no repeating pattern. For example, if five different 
persons sent you an email message concerning the stock trades of a specific company, that data 
could be anywhere in the message. You could not reliably count on the name of the company 
being the first words of each message (as in a stock ticker entry), and the pricing, volume, and 
Learn More
Read the Short Takes  
for Chapter 1 for a further 
discussion about data 
sources.
Student Tip
To help remember the 
difference between a 
sample and a popula-
tion, think of a pie. The 
entire pie represents the 
population, and the pie 
slice that you select is 
the sample.

48	
Chapter 1  Defining and Collecting Data 
percent change data could appear in any order. In the Getting Started chapter, big data was 
defined, in part, as data that arrive in a variety of forms, organized and unorganized. You can 
restate that definition as big data exists as both structured and unstructured data.
The ability to handle unstructured data represents an advance in information technology. 
Chapter 17 discusses business analytics methods that can analyze structured data as well as 
unstructured data or semistructured data. (Think of an application form that contains structured 
form fills but also contains an unstructured free-response portion.)
With the exception of some of the methods discussed in Chapter 17, the methods taught, 
and the software techniques used in this book, involve structured data. Your beginning point 
will always be tabular data, and for many problems and examples, you can begin with that data 
in the form of a Microsoft Excel or Minitab worksheet that you can download and use (see 
­Appendix C).
Electronic Formats and Encodings  Data can exist in more than one electronic format. 
This affects data formatting as some electronic formats are more immediately usable than oth-
ers. For example, which data would you like to use: data in an electronic worksheet file or data 
in a scanned image file that contains one of the worksheet illustrations in this book? Unless 
you like to do extra “busy work,” you chose the first format because the second format would 
require you to employ a translation process—perhaps a character-scanning program that can 
recognize numbers in an image.
Data can also be encoded in more than one way, as you may have learned in an informa-
tion systems course. Different encodings can affect the precision of values for numerical vari-
ables, and that can make some data not fully compatible with other data you have collected. 
While beyond the scope of this book to fully explain, the Short Takes for Chapter 1 includes 
an experiment that you can perform in either Microsoft Excel or Minitab that illustrates how 
data encoding can affect precision.
Data Cleaning
Whatever ways you choose to collect data, you may find irregularities in the values you collect 
such as undefined or impossible values. For a categorical variable, an undefined value would 
be a value that does not represent one of the categories defined for the variable. For a numeri-
cal variable, an impossible value would be a value that falls outside a defined range of possible 
values for the variable. For a numerical variable without a defined range of possible values, 
you might also find outliers, values that seem excessively different from most of the rest of the 
values. Such values may or may not be errors, but they demand a second review.
Values that are missing are another type of irregularity. A missing value is a value that 
was not able to be collected (and therefore not available to analysis). For example, you would 
record a nonresponse to a survey question as a missing value. You can represent missing values 
in Minitab by using an asterisk value for a numerical variable or by using a blank value for a 
categorical variable, and such values will be properly excluded from analysis. The more lim-
ited Excel has no special values that represent a missing value. When using Excel, you must 
find and then exclude missing values manually.
When you spot an irregularity, you may have to “clean” the data you have collected. Al-
though a full discussion of data cleaning is beyond the scope of this book (see reference 8), 
you can learn more about the ways you can use Excel or Minitab for data cleaning in the 
Short Takes for Chapter 1. If you only use the data files designed for use with this book and 
available online (see Appendix C), you will not need to worry about data cleaning as none of 
those data files contain any irregularities.
Recoding Variables
After you have collected data, you may discover that you need to reconsider the categories that 
you have defined for a categorical variable or that you need to transform a numerical variable 
into a categorical variable by assigning the individual numeric data values to one of several 
groups. In either case, you can define a recoded variable that supplements or replaces the 
original variable in your analysis.

	
1.4  Types of Sampling Methods	
49
For example, having defined the variable student class designation to be one of the four cat-
egories shown in Table 1.3 on page 44, you realize that you are more interested in investigating 
the differences between lowerclassmen (defined as freshman or sophomore) and upperclassmen 
(junior or senior). You can create a new variable UpperLower and assign the value Upper if a stu-
dent is a junior or senior and assign the value Lower if the student is a freshman or sophomore.
When recoding variables, be sure that the category definitions cause each data value to 
be placed in one and only one category, a property known as being mutually exclusive. Also 
ensure that the set of categories you create for the new, recoded variables include all the data 
values being recoded, a property known as being collectively exhaustive. If you are recoding 
a categorical variable, you can preserve one or more of the original categories, as long as your 
recodings are both mutually exclusive and collectively exhaustive.
When recoding numerical variables, pay particular attention to the operational definitions 
of the categories you create for the recoded variable, especially if the categories are not self-
defining ranges. For example, while the recoded categories Under 12, 12–20, 21–34, 35–54, 
and 55 and Over are self-defining for age, the categories Child, Youth, Young Adult, Middle 
Aged, and Senior need their own operational definitions.
Problems for Section 1.3
Applying the Concepts
1.12  Assume that a research has been carried out to estimate the 
rate of return given by all the Initial Public Offerings (IPOs) in 
the U.K. if they are sold on the first day of listing. The researcher 
analyzed the returns given by 250 IPOs in the U.K. Categorize the 
data for population and sample.
1.13  With reference to the case in 1.12, explain why the 
­researcher chose to collect the returns for 250 IPOs, rather than 
considering all IPOs. 
1.14  Assume that the recorded heights of 10 students are 120, 
122, 128, 176, 124, 127, 121, 125, 127, and 129 centimeters. 
Which number do you think will be the outlier while calculating 
the average height of students in the class and why? How would 
you deal with this outlier?
1.15  Transportation engineers and planners want to address the 
dynamic properties of travel behavior by describing in detail the 
driving characteristics of drivers over the course of a month. What 
type of data collection source do you think the transportation engi-
neers and planners should use?
1.16  Visit the website of NASDAQ. Enter the symbol of 2-3 
companies one by one. Observe the format in which the results 
appear for each of these companies. In which format do you think 
the data appears?
When you collect data by selecting a sample, you begin by defining the frame. The frame is 
a complete or partial listing of the items that make up the population from which the sample 
will be selected. Inaccurate or biased results can occur if a frame excludes certain groups, or 
portions of the population. Using different frames to collect data can lead to different, even op-
posite, conclusions.
Using your frame, you select either a nonprobability sample or a probability sample. In 
a nonprobability sample, you select the items or individuals without knowing their prob-
abilities of selection. In a probability sample, you select items based on known probabilities. 
Whenever possible, you should use a probability sample as such a sample will allow you to 
make inferences about the population being analyzed.
Nonprobability samples can have certain advantages, such as convenience, speed, and low 
cost. Such samples are typically used to obtain informal approximations or as small-scale ini-
tial or pilot analyses. However, because the theory of statistical inference depends on prob-
ability sampling, nonprobability samples cannot be used for statistical inference and this more 
than offsets those advantages in more formal analyses.
Figure 1.1 shows the subcategories of the two types of sampling. A nonprobability sample 
can be either a convenience sample or a judgment sample. To collect a convenience sample, 
you select items that are easy, inexpensive, or convenient to sample. For example, in a ware-
house of stacked items, selecting only the items located on the tops of each stack and within 
1.4  Types of Sampling Methods

50	
Chapter 1  Defining and Collecting Data 
easy reach would create a convenience sample. So, too, would be the responses to surveys that 
the websites of many companies offer visitors. While such surveys can provide large amounts 
of data quickly and inexpensively, the convenience samples selected from these responses will 
consist of self-selected website visitors. (Read the Think About This essay on page 54 for a 
related story.)
To collect a judgment sample, you collect the opinions of preselected experts in the sub-
ject matter. Although the experts may be well informed, you cannot generalize their results to 
the population.
The types of probability samples most commonly used include simple random, system-
atic, stratified, and cluster samples. These four types of probability samples vary in terms of 
cost, accuracy, and complexity, and they are the subject of the rest of this section.
Simple Random Sample
In a simple random sample, every item from a frame has the same chance of selection as ev-
ery other item, and every sample of a fixed size has the same chance of selection as every other 
sample of that size. Simple random sampling is the most elementary random sampling tech-
nique. It forms the basis for the other random sampling techniques. However, simple random 
sampling has its disadvantages. Its results are often subject to more variation than other sam-
pling methods. In addition, when the frame used is very large, carrying out a simple random 
sample may be time consuming and expensive.
With simple random sampling, you use n to represent the sample size and N to represent 
the frame size. You number every item in the frame from 1 to N. The chance that you will se-
lect any particular member of the frame on the first selection is 1>N.
You select samples with replacement or without replacement. Sampling with replace-
ment means that after you select an item, you return it to the frame, where it has the same 
probability of being selected again. Imagine that you have a fishbowl containing N business 
cards, one card for each person. On the first selection, you select the card for Grace Kim. You 
record pertinent information and replace the business card in the bowl. You then mix up the 
cards in the bowl and select a second card. On the second selection, Grace Kim has the same 
probability of being selected again, 1>N. You repeat this process until you have selected the 
desired sample size, n.
Typically, you do not want the same item or individual to be selected again in a sam-
ple. Sampling without replacement means that once you select an item, you cannot select 
it again. The chance that you will select any particular item in the frame—for example, the 
business card for Grace Kim—on the first selection is 1>N. The chance that you will select any 
card not previously chosen on the second selection is now 1 out of N - 1. This process contin-
ues until you have selected the desired sample of size n.
When creating a simple random sample, you should avoid the “fishbowl” method of se-
lecting a sample because this method lacks the ability to thoroughly mix the cards and, there-
fore, randomly select a sample. You should use a more rigorous selection method.
One such method is to use a table of random numbers, such as Table E.1 in Appendix 
E, for selecting the sample. A table of random numbers consists of a series of digits listed in 
a randomly generated sequence. To use a random number table for selecting a sample, you 
first need to assign code numbers to the individual items of the frame. Then you generate the 
random sample by reading the table of random numbers and selecting those individuals from 
the frame whose assigned code numbers match the digits found in the table. Because the num-
ber system uses 10 digits 10, 1, 2, c , 92, the chance that you will randomly generate any  
F i g u r e  1 . 1
Types of samples
Nonprobability Samples
Judgment
Sample
Systematic
Sample
Stratiﬁed
Sample
Simple
Random
Sample
Cluster
Sample
Probability Samples
Convenience
Sample

	
1.4  Types of Sampling Methods	
51
particular digit is equal to the probability of generating any other digit. This probability is 1 
out of 10. Hence, if you generate a sequence of 800 digits, you would expect about 80 to be the 
digit 0, 80 to be the digit 1, and so on. Because every digit or sequence of digits in the table is 
random, the table can be read either horizontally or vertically. The margins of the table desig-
nate row numbers and column numbers. The digits themselves are grouped into sequences of 
five in order to make reading the table easier.
Systematic Sample
In a systematic sample, you partition the N items in the frame into n groups of k items, where
k = N
n
You round k to the nearest integer. To select a systematic sample, you choose the first item to 
be selected at random from the first k items in the frame. Then, you select the remaining n - 1 
items by taking every kth item thereafter from the entire frame.
If the frame consists of a list of prenumbered checks, sales receipts, or invoices, taking a 
systematic sample is faster and easier than taking a simple random sample. A systematic sam-
ple is also a convenient mechanism for collecting data from membership directories, electoral 
registers, class rosters, and consecutive items coming off an assembly line.
To take a systematic sample of n = 40 from the population of N = 800 full-time employ-
ees, you partition the frame of 800 into 40 groups, each of which contains 20 employees. You 
then select a random number from the first 20 individuals and include every twentieth indi-
vidual after the first selection in the sample. For example, if the first random number you select 
is 008, your subsequent selections are 028, 048, 068, 088, 108, c , 768, and 788.
Simple random sampling and systematic sampling are simpler than other, more sophisti-
cated, probability sampling methods, but they generally require a larger sample size. In addi-
tion, systematic sampling is prone to selection bias that can occur when there is a pattern in 
the frame. To overcome the inefficiency of simple random sampling and the potential selec-
tion bias involved with systematic sampling, you can use either stratified sampling methods or 
cluster sampling methods.
Stratified Sample
In a stratified sample, you first subdivide the N items in the frame into separate subpopula-
tions, or strata. A stratum is defined by some common characteristic, such as gender or year 
in school. You select a simple random sample within each of the strata and combine the results 
from the separate simple random samples. Stratified sampling is more efficient than either 
simple random sampling or systematic sampling because you are ensured of the representation 
of items across the entire population. The homogeneity of items within each stratum provides 
greater precision in the estimates of underlying population parameters. In addition, stratified 
sampling enables you to reach conclusions about each strata in the frame. However, using a 
stratified sample requires that you can determine the variable(s) on which to base the stratifica-
tion and can also be expensive to implement.
Cluster Sample
In a cluster sample, you divide the N items in the frame into clusters that contain several 
items. Clusters are often naturally occurring groups, such as counties, election districts, city 
blocks, households, or sales territories. You then take a random sample of one or more clusters 
and study all items in each selected cluster.
Cluster sampling is often more cost-effective than simple random sampling, particularly 
if the population is spread over a wide geographic region. However, cluster sampling often re-
quires a larger sample size to produce results as precise as those from simple random sampling 
or stratified sampling. A detailed discussion of systematic sampling, stratified sampling, and 
cluster sampling procedures can be found in references 2, 4, and 5.
Learn More
Learn to use a table of 
random numbers to select a 
simple random sample in a 
Chapter 1 online section.
Learn More
Learn how to select a  
stratified sample in a  
Chapter 1 online section.

52	
Chapter 1  Defining and Collecting Data 
Problems for Section 1.4
Learning the Basics
1.17  For a population containing N = 902 individuals, what 
code number would you assign for
a.	 the first person on the list?
b.	 the fortieth person on the list?
c.	 the last person on the list?
1.18  The principal wants to assess the overall efficiency of teach-
ers by asking 30 students selected using the table of random num-
bers (Table E.1) to rate the teachers’ presentations. Which kind of 
sampling is it?
1.19  Given a population of N = 93, starting in row 29, column 01 
of the table of random numbers (Table E.1), and reading across the 
row, select a sample of N = 15
a.	 without replacement.
b.	 with replacement.
Applying the Concepts
1.20  For a study that consists of personal interviews with partici-
pants (rather than mail or phone surveys), explain why simple random 
sampling might be less practical than some other sampling methods.
1.21  The human resources manager of a company is concerned about 
low productivity. The low level of motivation among workers seems to 
be responsible for this. In order to find out about the motivation level 
in each department, the manager plans to take a random sample of 120 
employees from different departments. Explain why this is an example 
of stratified sampling and not simple random sampling.
1.22  A population has four members (called A, B, C, and D). You 
would like to select a random sample of n = 2, which you decide 
to do in the following way: Flip a coin; if it is heads, the sample will 
be items A and B; if it is tails, the sample will be items C and D. 
Although this is a random sample, it is not a simple random sam-
ple. Explain why. (Compare the procedure described in Problem 
1.21 with the procedure described in this problem.)
1.23  The registrar of a college with a population of N = 4,000 
full-time students is asked by the president to conduct a survey 
to measure satisfaction with the quality of life on campus. The 
following table contains a breakdown of the 4,000 registered  
full-time students, by gender and class designation:
The registrar intends to take a probability sample of n = 200 stu-
dents and project the results from the sample to the entire popula-
tion of full-time students.
a.	 If the frame available from the registrar’s files is an alphabeti-
cal listing of the names of all N = 4,000 registered full-time 
students, what type of sample could you take? Discuss.
b.	 What is the advantage of selecting a simple random sample  
in (a)?
c.	 What is the advantage of selecting a systematic sample in (a)?
d.	 If the frame available from the registrar’s files is a list of the 
names of all N = 4,000 registered full-time students compiled 
from eight separate alphabetical lists, based on the gender and 
class designation breakdowns shown in the class designation 
table, what type of sample should you take? Discuss.
e.	 Suppose that each of the N = 4,000 registered full-time stu-
dents lived in one of the 10 campus dormitories. Each dormi-
tory accommodates 400 students. It is college policy to fully 
integrate students by gender and class designation in each dor-
mitory. If the registrar is able to compile a listing of all students 
by dormitory, explain how you could take a cluster sample.
SELF 
Test 
1.24  Prenumbered sales invoices are kept in a  
sales journal. The invoices are numbered from 0001  
to 5000.
a.	 Beginning in row 16, column 01, and proceeding horizontally 
in a table of random numbers (Table E.1), select a simple ran-
dom sample of 50 invoice numbers.
b.	 Select a systematic sample of 50 invoice numbers. Use the ran-
dom numbers in row 20, columns 05–07, as the starting point 
for your selection.
c.	 Are the invoices selected in (a) the same as those selected in 
(b)? Why or why not?
1.25  Suppose that 5,000 sales invoices are separated into four 
strata. Stratum 1 contains 50 invoices, stratum 2 contains 500 in-
voices, stratum 3 contains 1,000 invoices, and stratum 4 contains 
3,450 invoices. A sample of 500 sales invoices is needed.
a.	 What type of sampling should you do? Why?
b.	 Explain how you would carry out the sampling according to the 
method stated in (a).
c.	 Why is the sampling in (a) not simple random sampling?
Class Designation
Gender
Fr.
So.
Jr.
Sr.
Total
Female
700
520
500
480
2,200
Male
 560
460
400
380
1,800
Total
1,260
980
900
860
4,000
1.5  Types of Survey Errors
As you learned in Section 1.3, responses from a survey represent a source of data. Nearly 
every day, you read or hear about survey or opinion poll results in newspapers, on the Inter-
net, or on radio or television. To identify surveys that lack objectivity or credibility, you must 
critically evaluate what you read and hear by examining the validity of the survey results. 

	
1.5  Types of Survey Errors	
53
First, you must evaluate the purpose of the survey, why it was conducted, and for whom it 
was conducted.
The second step in evaluating the validity of a survey is to determine whether it was based 
on a probability or nonprobability sample (as discussed in Section 1.4). You need to remember 
that the only way to make valid statistical inferences from a sample to a population is by using 
a probability sample. Surveys that use nonprobability sampling methods are subject to serious 
biases that may make the results meaningless.
Even when surveys use probability sampling methods, they are subject to four types of 
potential survey errors:
 • Coverage error
 • Nonresponse error
 • Sampling error
 • Measurement error
Well-designed surveys reduce or minimize these four types of errors, often at considerable cost.
Coverage Error
The key to proper sample selection is having an adequate frame. Coverage error occurs if 
certain groups of items are excluded from the frame so that they have no chance of being se-
lected in the sample or if items are included from outside the frame. Coverage error results in 
a selection bias. If the frame is inadequate because certain groups of items in the population 
were not properly included, any probability sample selected will provide only an estimate of 
the characteristics of the frame, not the actual population.
Nonresponse Error
Not everyone is willing to respond to a survey. Nonresponse error arises from failure to col-
lect data on all items in the sample and results in a nonresponse bias. Because you cannot al-
ways assume that persons who do not respond to surveys are similar to those who do, you need 
to follow up on the nonresponses after a specified period of time. You should make several 
attempts to convince such individuals to complete the survey and possibly offer an incentive 
to participate. The follow-up responses are then compared to the initial responses in order to 
make valid inferences from the survey (see references 2, 4, and 5). The mode of response you 
use, such as face-to-face interview, telephone interview, paper questionnaire, or computerized 
questionnaire, affects the rate of response. Personal interviews and telephone interviews usu-
ally produce a higher response rate than do mail surveys—but at a higher cost.
Sampling Error
When conducting a probability sample, chance dictates which individuals or items will or will 
not be included in the sample. Sampling error reflects the variation, or “chance differences,” 
from sample to sample, based on the probability of particular individuals or items being se-
lected in the particular samples.
When you read about the results of surveys or polls in newspapers or on the Internet, there 
is often a statement regarding a margin of error, such as “the results of this poll are expected 
to be within {4 percentage points of the actual value.” This margin of error is the sampling 
­error. You can reduce sampling error by using larger sample sizes. Of course, doing so in-
creases the cost of conducting the survey.
Measurement Error
In the practice of good survey research, you design surveys with the intention of gathering 
meaningful and accurate information. Unfortunately, the survey results you get are often only a 
proxy for the ones you really desire. Unlike height or weight, certain information about behav-
iors and psychological states is impossible or impractical to obtain directly.
When surveys rely on self-reported information, the mode of data collection, the respon-
dent to the survey, and or the survey itself can be possible sources of measurement error. 

54	
Chapter 1  Defining and Collecting Data 
Satisficing, social desirability, reading ability, and/or interviewer effects can be dependent on 
the mode. The social desirability bias or cognitive/memory limitations of a respondent can af-
fect the results. And vague questions, double-barreled questions that ask about multiple issues 
but require a single response, or questions that ask the respondent to report something that oc-
curs over time but fail to clearly define the extent of time about which the question asks (the 
reference period) are some of the survey flaws that can cause errors.
To minimize measurement error, you need to standardize survey administration and re-
spondent understanding of questions, but there are many barriers to this (see references 1, 3, 
and 10).
Ethical Issues About Surveys
Ethical considerations arise with respect to the four types of survey error. Coverage error can 
result in selection bias and becomes an ethical issue if particular groups or individuals are pur-
posely excluded from the frame so that the survey results are more favorable to the survey’s 
sponsor. Nonresponse error can lead to nonresponse bias and becomes an ethical issue if the 
sponsor knowingly designs the survey so that particular groups or individuals are less likely 
than others to respond. Sampling error becomes an ethical issue if the findings are purposely 
presented without reference to sample size and margin of error so that the sponsor can promote 
a viewpoint that might otherwise be inappropriate. Measurement error can become an ethical 
issue in one of three ways: (1) a survey sponsor chooses leading questions that guide the re-
spondent in a particular direction; (2) an interviewer, through mannerisms and tone, purposely 
makes a respondent obligated to please the interviewer or otherwise guides the respondent in a 
particular direction; or (3) a respondent willfully provides false information.
Ethical issues also arise when the results of nonprobability samples are used to form con-
clusions about the entire population. When you use a nonprobability sampling method, you 
need to explain the sampling procedures and state that the results cannot be generalized be-
yond the sample.
T h i n k  A b o u t  T h i s  New Media Surveys/Old Sampling Problems
Imagine that you work for a software distributor that 
has decided to create a “customer experience im-
provement program” to record how your customers 
are using your products, with the goal of using the 
collected data to make product enhancements. Or 
say that you are an editor of an online news web-
site who decides to create an instant poll to ask 
website visitors about important political issues. Or 
you’re a marketer of products aimed at a specific 
demographic and decide to use a social network-
ing site to collect consumer feedback. What might 
you have in common with a dead-tree publication 
that went out of business over 70 years ago?
By 1932, before there was ever an  
Internet—or even commercial television—a “straw 
poll” conducted by the magazine Literary Digest 
had successfully predicted five U.S. presiden-
tial elections in a row. For the 1936 election, the 
magazine promised its largest poll ever and sent 
about 10 million ballots to people all across the 
country. After receiving and tabulating more than  
2.3 million ballots, the Digest confidently pro-
claimed that Alf Landon would be an easy winner 
over Franklin D. Roosevelt. As things turned out, 
FDR won in a landslide, with Landon receiving the 
fewest electoral votes in U.S. history. The reputa-
tion of Literary Digest was ruined; the magazine 
would cease publication less than two years later.
The failure of the Literary Digest poll was a 
watershed event in the history of sample surveys 
and polls. This failure refuted the notion that the 
larger the sample is, the better. (Remember this 
the next time someone complains about a political 
survey’s “small” sample size.) The failure opened 
the door to new and more modern methods of 
sampling discussed in this chapter. Today’s Gallup 
polls of political opinion (www.gallup.com) or GfK 
Roper Reports about consumer behavior (www 
.gfkamerica.com/practice_areas/roper_­
consulting) arose, in part, due to this failure. 
George Gallup, the “Gallup” of the poll, and Elmo 
Roper, of the eponymous reports, both first gained 
widespread public notice for their correct “scien-
tific” predictions of the 1936 election.
The failed Literary Digest poll became fod-
der for several postmortems, and the reason for 
the failure became almost an urban legend. Typi-
cally, the explanation is coverage error: The bal-
lots were sent mostly to “rich people,” and this 
created a frame that excluded poorer citizens 
(presumably more inclined to vote for the Dem-
ocrat Roosevelt than the Republican Landon). 
However, later analyses suggest that this was not 
true; instead, low rates of response (2.3 million 
ballots represented less than 25% of the ballots 
distributed) and/or nonresponse error (Roosevelt 
voters were less likely to mail in a ballot than 
Landon voters) were significant reasons for the 
failure (see reference 9).
When Microsoft revealed its Office Ribbon 
for Office 2007, a program manager explained how  
Microsoft had applied data collected from its “Cus-
tomer Experience Improvement Program” to the user 

	
Using Statistics	
55
interface redesign. This led others to speculate that 
the data were biased toward beginners—who might 
be less likely to decline participation in the program—
and that, in turn, had led Microsoft to create a user 
interface that ended up perplexing more experienced 
users. This was another case of nonresponse error!
The editor’s instant poll mentioned earlier is 
targeted to the visitors of the online news website, 
and the social network–based survey is aimed at 
“friends” of a product; such polls can also suffer 
from nonresponse error, and this fact is often over-
looked by users of these new media. Often, mar-
keters extol how much they “know” about survey 
respondents, thanks to data that can be collected 
from a social network community. But no amount 
of information about the respondents can tell mar-
keters who the nonrespondents are. Therefore, 
new media surveys fall prey to the same old type 
of error that may have been fatal to Literary Digest 
way back when.
Today, companies establish formal surveys 
based on probability sampling and go to great 
lengths—and spend large sums—to deal with 
coverage error, nonresponse error, sampling error, 
and measurement error. Instant polling and tell-a-
friend surveys can be interesting and fun, but they 
are not replacements for the methods discussed in 
this chapter.
Problems for Section 1.5
Applying the Concepts
1.26  While collecting data using a survey, only 40% of the re-
spondents gave feedback. What does this tell you about survey 
methods? And how can a researcher increase the response rate? 
1.27  A simple random sample of n = 300 full-time employ-
ees is selected from a company list containing the names of  
all N = 5,000 full-time employees in order to evaluate job  
satisfaction.
a.	 Give an example of possible coverage error.
b.	 Give an example of possible nonresponse error.
c.	 Give an example of possible sampling error.
d.	 Give an example of possible measurement error.
SELF 
Test 
1.28  The results of a 2013 Adobe Systems study on 
retail apps and buying habits reveal insights on percep-
tions and attitudes toward mobile shopping using retail apps and 
browsers, providing new direction for retailers to develop their 
digital publishing strategies (adobe.ly/11gt8Rq). Increased con-
sumer interest in using shopping applications means retailers 
must adapt to meet the rising expectations for specialized mobile 
shopping experiences. The results indicate that tablet users (55%) 
are almost twice as likely as smartphone users (28%) to use their 
device to purchase products and services. The findings also reveal 
that retail and catalog apps are rapidly catching up to mobile 
browsers as a viable shopping channel: nearly half of all mobile 
shoppers are interested in using apps instead of a mobile browser 
(45% of tablet shoppers and 49% of smartphone shoppers). The 
research is based on an online survey with a sample of 1,003 con-
sumers. Identify potential concerns with coverage, nonresponse, 
sampling, and measurement errors.
1.29  A recent PwC Supply Global Chain survey indicated that 
companies that acknowledge the supply chain as a strategic asset 
achieve 70% higher performance (pwc.to/VaFpGz). The “Lead-
ers” in the survey point to next-generation supply chains, which 
are fast, flexible, and responsive. They are more concerned with 
skills that separate a company from the crowd: 51% say differ-
entiating capabilities is the real key to success. What additional 
information would you want to know about the survey before you 
accepted the results of the study?
1.30  A recent survey points to a next generation of consumers 
seeking a more mobile TV experience. The 2013 KPMG Inter-
national Consumer Media Behavior study found that while TV is 
still the most popular media activity with 88% of U.S. consum-
ers watching TV, a relatively high proportion of U.S. consumers, 
14%, now prefer to watch TV via their mobile device or tablet for 
greater flexibility (bit.ly/Wb8Jv9). What additional information 
would you want to know about the survey before you accepted the 
results of the study?
T
he analysts charged by GT&M CEO Emma Levia to 
identify, define, and collect the data that would be help-
ful in setting a price for Whitney Wireless have completed 
their task. The group has identified a number of variables 
to analyze. In the course of doing this work, the group real-
ized that most of the variables to study would be discrete 
numerical variables based on data that (ac)counts the finan-
cials of the business. These data would mostly be from the 
primary source of 
the business itself, 
but some supple-
mental variables 
about economic conditions and other factors that might 
affect the long-term prospects of the business might 
come from a secondary data source, such as an economic  
agency.
U s i n g  S tat i s t i c s
Beginning of the End… Revisited
Tyler Olson/Shutterstock

56	
Chapter 1  Defining and Collecting Data 
S u m m a r y
In this chapter, you learned about the various types of vari-
ables used in business and their measurement scales. In 
addition, you learned about different methods of collect-
ing data, several statistical sampling methods, and issues 
­involved in taking samples. In the next two chapters, you 
will study a variety of tables and charts and descriptive mea-
sures that are used to present and analyze data.
Referen c e s
	 1.	Biemer, P. B., R. M. Graves, L. E. Lyberg, A. Mathiowetz, and 
S. Sudman. Measurement Errors in Surveys. New York: Wiley 
Interscience, 2004.
	 2.	Cochran, W. G. Sampling Techniques, 3rd ed. New York: 
­Wiley, 1977.
	 3.	Fowler, F. J. Improving Survey Questions: Design and Evalu-
ation, Applied Special Research Methods Series, Vol. 38, 
­Thousand Oaks, CA: Sage Publications, 1995.
	 4.	Groves R. M., F. J. Fowler, M. P. Couper, J. M. Lepkowski, 
E. Singer, and R. Tourangeau. Survey Methodology, 2nd ed. 
New York: John Wiley, 2009.
	 5.	Lohr, S. L. Sampling Design and Analysis, 2nd ed. Boston, 
MA: Brooks/Cole Cengage Learning, 2010.
	 6.	Microsoft Excel 2013. Redmond, WA: Microsoft Corporation, 
2012.
	 7.	Minitab Release 16. State College, PA: Minitab, Inc., 2010.
	 8.	Osbourne, J. Best Practices in Data Cleaning. Thousand Oaks, 
CA: Sage Publications, 2012.
	 9.	Squire, P. “Why the 1936 Literary Digest Poll Failed.” Public 
Opinion Quarterly 52 (1988): 125–133.
	10.	Sudman, S., N. M. Bradburn, and N. Schwarz. Thinking About 
Answers: The Application of Cognitive Processes to Survey 
Methodology. San Francisco, CA: Jossey-Bass, 1993.
K e y  Term s
categorical variable  42
cluster  51
cluster sample  51
collect  42
collectively exhaustive  49
continuous variable  43
convenience sample  49
coverage error  53
define  42
discrete variable  43
frame  49
interval scale  44
judgment sample  50
margin of error  53
measurement error  53
measurement scale  43
missing value  48
mutually exclusive  49
nominal scale  43
nonprobability sample  49
nonresponse bias  53
nonresponse error  53
numerical variable  43
operational definition  42
ordinal scale  43
outlier  48
parameter  47
population  47
primary data source  46
probability sample  49
qualitative variable  42
quantitative variable  43
ratio scale  45
recoded variable  48
sample  47
sampling error  53
sampling with replacement  50
sampling without replacement  50
secondary data source  46
selection bias  53
simple random sample  50
statistics  47
strata  51
stratified sample  51
structured data  47
systematic sample  51
table of random numbers  50
unstructured data  47
The group foresaw that examining several categorical vari-
ables related to the customers of both GT&M and Whitney 
Wireless would be necessary. The group discovered that the af-
finity (“shopper’s card”) programs of both firms had already 
collected demographic data of interest when customers en-
rolled in those programs. That primary source, when combined 
with secondary data gleaned from the social media networks 
to which the business belongs, might prove useful in getting a 
rough approximation of the profile of a typical customer that 
might be interested in doing business with an “A-to-Z” elec-
tronics retailer.

	
Chapter Review Problems	
57
C h ec ki n g  Yo ur  U n de r s ta nding
1.31  What is the fundamental purpose of sampling?
1.32  Is sampling required while conducting a census?
1.33  How can you assess bias in a survey report that uses the prob-
ability sampling technique?
1.34  What ethical issues are involved in conducting a survey using 
probability and non-probability sampling?
1.35  What is the difference between a nominal scaled variable and 
an ordinal scaled variable?
1.36  What is the difference between an interval-scaled variable 
and a ratio-scaled variable?
1.37  What is the difference between probability sampling and non-
probability sampling?
C h a pte r  R e vi e w P r ob le ms
1.38  Visit the official website for either Excel (www.office 
.microsoft.com/excel) or Minitab (www.minitab.com/products/
minitab). Read about the program you chose and then think about 
the ways the program could be useful in statistical analysis.
1.39  Results of a 2013 Adobe Systems study on retail apps and 
buying habits reveals insights on perceptions and attitudes toward 
mobile shopping using retail apps and browsers, providing new di-
rection for retailers to develop their digital publishing strategies. 
Increased consumer interest in using shopping applications means 
retailers must adapt to meet the rising expectations for specialized 
mobile shopping experiences. The results indicate that tablet us-
ers (55%) are almost twice as likely as smartphone users (28%) to 
use their device to purchase products and services. The findings 
also reveal that retail and catalog apps are rapidly catching up to 
mobile browsers as a viable shopping channel: Nearly half of all 
mobile shoppers are interested in using apps instead of a mobile 
browser (45% of tablet shoppers and 49% of smartphone shop-
pers). The research is based on an online survey with a sample 
of 1,003 18–54 year olds who currently own a smartphone and/or 
tablet; it includes consumers who use and do not use these devices 
to shop (adobe.ly/11gt8Rq).
a.	 Describe the population of interest.
b.	 Describe the sample that was collected.
c.	 Describe a parameter of interest.
d.	 Describe the statistic used to estimate the parameter in (c).
1.40  The Gallup organization releases the results of recent polls 
at its website, www.gallup.com. Visit this site and read an article 
of interest.
a.	 Describe the population of interest.
b.	 Describe the sample that was collected.
c.	 Describe a parameter of interest.
d.	 Describe the statistic used to estimate the parameter in (c).
1.41  A recent PwC Supply Global Chain survey indicated that com-
panies that acknowledge the supply chain as a strategic asset achieve 
70% higher performance. The “Leaders” in the survey point to next-
generation supply chains, which are fast, flexible, and responsive. They 
are more concerned with skills that separate a company from the crowd: 
51% say differentiating capabilities is the real key to success (pwc.to 
/VaFpGz). The results are based on a survey of 503 supply chain 
­executives in a wide range of industries representing a mix of com-
pany sizes from across three global regions: Asia, Europe, and the 
Americas.
a.	 Describe the population of interest.
b.	 Describe the sample that was collected.
c.	 Describe a parameter of interest.
d.	 Describe the statistic used to estimate the parameter in (c).
1.42  The Data and Story Library (DASL) is an online library of 
data files and stories that illustrate the use of basic statistical meth-
ods. Visit lib.stat.cmu.edu/index.php, click DASL, and explore a 
data set of interest to you.
a.	 Describe a variable in the data set you selected.
b.	 Is the variable categorical or numerical?
c.	 If the variable is numerical, is it discrete or continuous?
1.43  Download and examine the U.S. Census Bureau’s “Business and 
Professional Classification Survey (SQ-CLASS),” available through 
the Get Help with Your Form link at www.census.gov/econ/.
a.	 Give an example of a categorical variable included in the survey.
b.	 Give an example of a numerical variable included in the survey.
1.44  Three professors examined awareness of four widely dissemi-
nated retirement rules among employees at the University of Utah. 
These rules provide simple answers to questions about retirement plan-
ning (R. N. Mayer, C. D. Zick, and M. Glaittle, “Public Awareness of 
Retirement Planning Rules of Thumb,” Journal of Personal Finance, 
2011 10(1), 12–35). At the time of the investigation, there were ap-
proximately 10,000 benefited employees, and 3,095 participated in the 
study. Demographic data collected on these 3,095 employees included 
gender, age (years), education level (years completed), marital status, 
household income ($), and employment category.
a.	 Describe the population of interest.
b.	 Describe the sample that was collected.
c.	 Indicate whether each of the demographic variables mentioned 
is categorical or numerical.
1.45  A manufacturer of cat food is planning to survey households 
in the United States to determine purchasing habits of cat owners. 
Among the variables to be collected are the following:
	
i.	 The primary place of purchase for cat food
	
ii.	 Whether dry or moist cat food is purchased
	
iii.	The number of cats living in the household
	
iv.	Whether any cat living in the household is pedigreed
a.	 For each of the four items listed, indicate whether the variable 
is categorical or numerical. If it is numerical, is it discrete or 
continuous?
b.	 Develop five categorical questions for the survey.
c.	 Develop five numerical questions for the survey.

58	
Chapter 1  Defining and Collecting Data 
C a s e s  f o r  C h a p t e r  1
Managing Ashland MultiComm Services
Ashland MultiComm Services (AMS) provides high-­quality 
communications networks in the Greater Ashland area. 
AMS traces its roots to Ashland Community Access Tele-
vision (ACATV), a small company that redistributed the 
broadcast television signals from nearby major metropoli-
tan areas but has evolved into a provider of a wide range of 
broadband services for residential customers.
AMS offers subscription-based services for digital ca-
ble video programming, local and long-distance telephone 
services, and high-speed Internet access. Recently, AMS has 
faced competition from other network providers that have 
expanded into the Ashland area. AMS has also seen de-
creases in the number of new digital cable installations and 
the rate of digital cable renewals.
AMS management believes that a combination of in-
creased promotional expenditures, adjustment in subscrip-
tion fees, and improved customer service will allow AMS 
to successfully face the competition from other network 
providers. However, AMS management worries about the 
possible effects that new Internet-based methods of program 
delivery may have had on their digital cable business. They 
decide that they need to conduct some research and organize 
a team of research specialists to examine the current status 
of the business and the marketplace in which it competes.
The managers suggest that the research team examine 
the company’s own historical data for number of subscrib-
ers, revenues, and subscription renewal rates for the past 
few years. They direct the team to examine year-to-date data 
as well, as the managers suspect that some of the changes 
they have seen have been a relatively recent phenomena.
1.	What type of data source would the company’s own 
­historical data be? Identify other possible data sources 
that the research team might use to examine the current 
marketplace for residential broadband services in a city 
such as Ashland.
2.	What type of data collection techniques might the team 
employ?
3.	In their suggestions and directions, the AMS managers 
have named a number of possible variables to study, but 
offered no operational definitions for those variables. 
What types of possible misunderstandings could arise if 
the team and managers do not first properly define each 
variable cited?
CardioGood Fitness
CardioGood Fitness is a developer of high-quality cardio-
vascular exercise equipment. Its products include treadmills, 
fitness bikes, elliptical machines, and e-glides. CardioGood 
Fitness looks to increase the sales of its treadmill products 
and has hired The AdRight Agency, a small advertising 
firm, to create and implement an advertising program. The 
AdRight Agency plans to identify particular market seg-
ments that are most likely to buy their clients’ goods and 
services and then locates advertising outlets that will reach 
that market group. This activity includes collecting data on 
clients’ actual sales and on the customers who make the 
purchases, with the goal of determining whether there is a 
distinct profile of the typical customer for a particular prod-
uct or service. If a distinct profile emerges, efforts are made 
to match that profile to advertising outlets known to reflect 
the particular profile, thus targeting advertising directly to 
high-potential customers.
CardioGood Fitness sells three different lines of tread-
mills. The TM195 is an entry-level treadmill. It is as de-
pendable as other models offered by CardioGood Fitness, 
but with fewer programs and features. It is suitable for indi-
viduals who thrive on minimal programming and the desire 
for simplicity to initiate their walk or hike. The TM195 sells 
for $1,500.
The middle-line TM498 adds to the features of the 
entry-level model two user programs and up to 15% eleva-
tion upgrade. The TM498 is suitable for individuals who are 
walkers at a transitional stage from walking to running or 
midlevel runners. The TM498 sells for $1,750.
The top-of-the-line TM798 is structurally larger and 
heavier and has more features than the other models. Its 
unique features include a bright blue backlit LCD console, 
quick speed and incline keys, a wireless heart rate monitor 
with a telemetric chest strap, remote speed and incline con-
trols, and an anatomical figure that specifies which muscles 
are minimally and maximally activated. This model features 
a nonfolding platform base that is designed to handle rig-
orous, frequent running; the TM798 is therefore appealing 
to someone who is a power walker or a runner. The selling 
price is $2,500.
As a first step, the market research team at AdRight is 
assigned the task of identifying the profile of the typical 
customer for each treadmill product offered by CardioGood 
Fitness. The market research team decides to investigate 

	
Cases for Chapter 1	
59
Clear Mountain State Student Surveys
1.	The Student News Service at Clear Mountain State 
University (CMSU) has decided to gather data about 
the undergraduate students who attend CMSU. They 
create and distribute a survey of 14 questions and 
receive responses from 62 undergraduates (stored  
in  UndergradSurvey ). Download (see Appendix C) and 
review the survey document CMUndergradSurvey.
pdf. For each question asked in the survey, determine 
whether the variable is categorical or numerical. If 
you determine that the variable is numerical, identify 
whether it is discrete or continuous.
2.	 The dean of students at CMSU has learned about the 
­undergraduate survey and has decided to undertake a simi-
lar survey for graduate students at CMSU. She ­creates and 
distributes a survey of 14 questions and receives responses 
from 44 graduate students (stored in  GradSurvey ). Down-
load (see Appendix C) and review the survey document 
CMGradSurvey.pdf. For each question asked in the sur-
vey, determine whether the variable is categorical or nu-
merical. If you determine that the variable is numerical, 
identify whether it is discrete or continuous.
whether there are differences across the product lines with 
respect to customer characteristics. The team decides to col-
lect data on individuals who purchased a treadmill at a Car-
dioGood Fitness retail store during the prior three months.
The team decides to use both business transactional 
data and the results of a personal profile survey that ev-
ery purchaser completes as their sources of data. The team 
identifies the following customer variables to study: prod-
uct purchased—TM195, TM498, or TM798; gender; age, 
in years; education, in years; relationship status, single or 
partnered; annual household income ($); average number 
of times the customer plans to use the treadmill each week; 
average number of miles the customer expects to walk/run 
each week; and self-rated fitness on an 1-to-5 ordinal scale, 
where 1 is poor shape and 5 is excellent shape. For this set 
of variables:
1.	Which variables in the survey are categorical?
2.	Which variables in the survey are numerical?
3.	Which variables are discrete numerical variables?
Learning with the Digital Cases
As you have already learned in this book, decision makers 
use statistical methods to help analyze data and communi-
cate results. Every day, somewhere, someone misuses these 
techniques either by accident or intentional choice. Identify-
ing and preventing such misuses of statistics is an important 
responsibility for all managers. The Digital Cases give you 
the practice you need to help develop the skills necessary 
for this important task.
Each chapter’s Digital Case tests your understanding of 
how to apply an important statistical concept taught in the 
chapter. For each case, you review the contents of one or 
more electronic documents, which may contain internal and 
confidential information to an organization as well as pub-
licly stated facts and claims, seeking to identify and correct 
misuses of statistics. Unlike in a traditional case study, but 
like in many business situations, not all of the information 
you encounter will be relevant to your task, and you may 
occasionally discover conflicting information that you have 
to resolve in order to complete the case.
To assist your learning, each Digital Case begins with 
a learning objective and a summary of the problem or is-
sue at hand. Each case directs you to the information neces-
sary to reach your own conclusions and to answer the case 
questions. Many cases, such as the sample case worked out 
next, extend a chapter’s Using Statistics scenario. You can 
download digital case files for later use or retrieve them on-
line from a MyStatLab course for this book, as explained in 
Appendix C.
Sample Digital Case
To illustrate learning with a Digital Case, open the Digi-
tal Case file WhitneyWireless.pdf that contains summary 
information about the Whitney Wireless business. Recall 
from the Using Statistics scenario for this chapter that Good 
Tunes & More (GT&M) is a retailer seeking to expand by 
purchasing Whitney Wireless, a small chain that sells mo-
bile media devices. Apparently, from the claim on the title 
page, this business is celebrating its “best sales year ever.”
Review the Who We Are, What We Do, and What We 
Plan to Do sections on the second page. Do these sections 
contain any useful information? What questions does this 
passage raise? Did you notice that while many facts are pre-
sented, no data that would support the claim of “best sales 
year ever” are presented? And were those mobile “mobile-
mobiles” used solely for promotion? Or did they generate 

60	
Chapter 1  Defining and Collecting Data 
any sales? Do you think that a talk-with-your-mouth-full 
event, however novel, would be a success?
Continue to the third page and the Our Best Sales Year 
Ever! section. How would you support such a claim? With 
a table of numbers? Remarks attributed to a knowledgeable 
source? Whitney Wireless has used a chart to present “two 
years ago” and “latest twelve months” sales data by cate-
gory. Are there any problems with what the company has 
done? Absolutely!
First, note that there are no scales for the symbols used, 
so you cannot know what the actual sales volumes are. In 
fact, as you will learn in Section 2.7, charts that incorporate 
icons as shown on the third page are considered examples 
of chartjunk and would never be used by people seeking to 
properly visualize data. The use of chartjunk symbols cre-
ates the impression that unit sales data are being presented. 
If the data are unit sales, does such data best support the 
claim being made, or would something else, such as dollar 
volumes, be a better indicator of sales at the retailer?
For the moment, let’s assume that unit sales are be-
ing visualized. What are you to make of the second row, 
in which the three icons on the right side are much wider 
than the three on the left? Does that row represent a newer 
(wider) model being sold or a greater sales volume? Exam-
ine the fourth row. Does that row represent a decline in sales 
or an increase? (Do two partial icons represent more than 
one whole icon?) As for the fifth row, what are we to think? 
Is a black icon worth more than a red icon or vice versa?
At least the third row seems to tell some sort of tale of 
increased sales, and the sixth row tells a tale of constant 
sales. But what is the “story” about the seventh row? There, 
the partial icon is so small that we have no idea what prod-
uct category the icon represents.
Perhaps a more serious issue is those curious chart la-
bels. “Latest twelve months” is ambiguous; it could include 
months from the current year as well as months from one 
year ago and therefore may not be an equivalent time period 
to “two years ago.” But the business was established in 2001, 
and the claim being made is “best sales year ever,” so why 
hasn’t management included sales figures for every year?
Are the Whitney Wireless managers hiding something, 
or are they just unaware of the proper use of statistics? Ei-
ther way, they have failed to properly organize and visualize 
their data and therefore have failed to communicate a vital 
aspect of their story.
In subsequent Digital Cases, you will be asked to pro-
vide this type of analysis, using the open-ended case ques-
tions as your guide. Not all the cases are as straightforward 
as this example, and some cases include perfectly appropri-
ate applications of statistical methods.

	
Chapter 1 Excel Guide	
61
C h a p t e r  1  E x c e l  G u i d e
EG1.1  Defining Data
Establishing the Variable Type
Microsoft Excel infers the variable type from the data you enter 
into a column. If Excel discovers a column that contains numbers, 
for example, it treats the column as a numerical variable. If Excel 
discovers a column that contains words or alphanumeric entries, it 
treats the column as a non-numerical (categorical) variable.
This imperfect method works most of the time, especially if 
you make sure that the categories for your categorical variables are 
words or phrases such as “yes” and “no.” However, because you 
cannot explicitly define the variable type, Excel can mistakenly 
offer or allow you to do nonsensical things such as using a statisti-
cal method that is designed for numerical variables on categorical 
variables. If you must use coded values such as 1, 2, or 3, enter 
them preceded with an apostrophe, as Excel treats all values that 
begin with an apostrophe as non-numerical data. (You can check 
whether a cell entry includes a leading apostrophe by selecting a 
cell and viewing the contents of the cell in the formula bar.)
EG1.2  Measurement Scales for 
Variables
There are no Excel Guide instructions for this section.
EG1.3  Collecting Data
Recoding Variables
Key Technique  To recode a categorical variable, you first copy 
the original variable’s column of data and then use the find-and-
replace function on the copied data. To recode a numerical vari-
able, enter a formula that returns a recoded value in a new column.
Example  Using the DATA worksheet of the Recoded work-
book, create the recoded variable UpperLower from the categori-
cal variable Class and create the recoded Variable Dean’s List 
from the numerical variable GPA.
In-Depth Excel  Use the RECODED worksheet of the 
­Recoded workbook as a model.
The worksheet already contains UpperLower, a recoded ver-
sion of Class that uses the operational definitions on page 49, and 
Dean’s List, a recoded version of GPA, in which the value No re-
codes all GPA values less than 3.3 and Yes recodes all values 3.3 
or greater than 3.3. The RECODED_FORMULAS worksheet in 
the same workbook shows how formulas in column I use the IF 
function to recode GPA as the Dean’s List variable.
These recoded variables were created by first opening to the 
DATA worksheet in the same workbook and then following these 
steps:
	 1.	 Right-click column D (right-click over the shaded “D” at the 
top of column D) and click Copy in the shortcut menu.
	 2.	 Right-click column H and click the first choice in the Paste 
Options gallery.
	 3.	 Enter UpperLower in cell H1.
	 4.	 Select column H. With column H selected, click Home ➔ 
Find & Select ➔ Replace.
In the Replace tab of the Find and Replace dialog box:
	 5.	 Enter Senior as Find what, Upper as Replace with, and 
then click Replace All.
	 6.	 Click OK to close the dialog box that reports the results of 
the replacement command.
	 7.	 Still in the Find and Replace dialog box, enter Junior as 
Find what (replacing Senior), and then click Replace All.
	 8.	 Click OK to close the dialog box that reports the results of 
the replacement command.
	 9.	 Still in the Find and Replace dialog box, enter Sophomore 
as Find what, Lower as Replace with, and then click  
Replace All.
	10.	 Click OK to close the dialog box that reports the results of 
the replacement command.
	11.	 Still in the Find and Replace dialog box, enter Freshman as 
Find what and then click Replace All.
	12.	 Click OK to close the dialog box that reports the results of 
the replacement command.
(This creates the recoded variable UpperLower in column H.)
	13.	 Enter Dean’s List in cell I1.
	14.	 Enter the formula =IF(G2 < 3.3, "No", "Yes") in cell I2.
	15.	 Copy this formula down the column to the last row that con-
tains student data (row 63).
(This creates the recoded variable Dean’s List in column I.)
The RECODED worksheet uses the IF function to recode the 
numerical variable into two categories (see Appendix Section F.4). 
Numerical variables can also be recoded into multiple categories 
by using the VLOOKUP function. Read the Short Takes for 
Chapter 1 to learn more about this advanced recoding technique.
EG1.4  Types of Sampling Methods
Simple Random Sample
Key Technique  Use the RANDBETWEEN(smallest integer, 
largest integer) function to generate a random integer that can 
then be used to select an item from a frame.
Example  Create a simple random sample with replacement of 
size 40 from a population of 800 items.
In-Depth Excel  Enter a formula that uses this function and 
then copy the formula down a column for as many rows as is nec-
essary. For example, to create a simple random sample with re-
placement of size 40 from a population of 800 items, open to a 
new ­worksheet. Enter Sample in cell A1 and enter the formula 
=RANDBETWEEN(1, 800) in cell A2. Then copy the formula 
down the column to cell A41.

62	
Chapter 1  Defining and Collecting Data 
Excel contains no functions to select a random sample with-
out replacement. Such samples are most easily created using an 
add-in such as PHStat or the Analysis ToolPak, as described in the 
following paragraphs.
Analysis ToolPak  Use Sampling to create a random sample 
with replacement.
For the example, open to the worksheet that contains the pop-
ulation of 800 items in column A and that contains a column head-
ing in cell A1. Select Data ➔ Data Analysis. In the Data Analysis 
dialog box, select Sampling from the Analysis Tools list and then 
click OK. In the procedure’s dialog box (shown below):
	 1.	 Enter A1:A801 as the Input Range and check Labels.
	 2.	 Click Random and enter 40 as the Number of Samples.
	 3.	 Click New Worksheet Ply and then click OK.
Example  Create a simple random sample without replacement 
of size 40 from a population of 800 items.
PHStat  Use Random Sample Generation.
For the example, select PHStat ➔ Sampling ➔ Random Sample 
Generation. In the procedure’s dialog box (shown below):
	 1.	 Enter 40 as the Sample Size.
	 2.	 Click Generate list of random numbers and enter 800 as 
the Population Size.
	 3.	 Enter a Title and click OK.
Unlike most other PHStat results worksheets, the worksheet cre-
ated contains no formulas.
In-Depth Excel  Use the COMPUTE worksheet of the 
­Random workbook as a template.
The worksheet already contains 40 copies of the formula 
=RANDBETWEEN(1, 800) in column B. Because the RAND-
BETWEEN function samples with replacement as discussed at 
the start of this section, you may need to add additional copies 
of the formula in new column B rows until you have 40 unique 
values.
If your intended sample size is large, you may find it difficult 
to spot duplicates. Read the Short Takes for Chapter 1 to learn 
more about an advanced technique that uses formulas to detect du-
plicate values.
MG1.1  Defining Data
Establishing the Variable Type
As Section MG.2 mentions on page 39, worksheet columns that 
contain non-numerical data have names that include “-T” and 
worksheet columns that contain data that Minitab interprets as ei-
ther dates or times have names that include “-D.” When Minitab 
adds a “-T” suffix, it is classifying the column as a categorical, or 
text, variable. When Minitab does not add a suffix, it is classifying 
the column as a numerical variable. (A column with the “-D” suf-
fix is a date variable, a special type of a numerical variable.)
Sometimes, Minitab will misclassify a variable, for example, 
mistaking a numerical variable for a categorical (text) variable. In 
such cases, select the column, then select Data ➔ Change Data 
Type, and then select one of the choices, for example, Text to Nu-
meric for the case of when Minitab has mistaken a numerical vari-
able as a categorical variable.
MG1.2  Measurement Scales for 
Variables
There are no Minitab Guide instructions for this section.
C h a p t e r  1  M i n i ta b  G u i d e

	
Chapter 1 Minitab Guide	
63
MG1.3  Collecting Data
Recoding Variables
Example  Using the DATA worksheet of the Recoded project, cre-
ate the recoded variable UpperLower from the categorical variable 
Class (C4-T) and create the recoded variable Dean’s List from the nu-
merical variable GPA (C7) to indicate if the GPA value is at least 3.3.
Instructions  Use the Replace command to recode a categorical 
variable. For the example, open to the DATA worksheet of the 
Recode project and:
	 1.	 Select the Class column (C4-T).
	 2.	 Select Editor ➔ Replace.
In the Replace in Data Window dialog box:
	 3.	 Enter Senior as Find what, Upper as Replace with, and 
then click Replace All.
	 4.	 Click OK to close the dialog box that reports the results of 
the replacement command.
	 5.	 Still in the Find and Replace dialog box, enter Junior as 
Find what (replacing Senior), and then click Replace All.
	 6.	 Click OK to close the dialog box that reports the results of 
the replacement command.
	 7.	 Still in the Find and Replace dialog box, enter Sophomore as 
Find what, Lower as Replace with, and then click Replace All.
	 8.	 Click OK to close the dialog box that reports the results of 
the replacement command.
	 9.	 Still in the Find and Replace dialog box, enter Freshman as 
Find what, and then click Replace All.
	10.	 Click OK to close the dialog box that reports the results of 
the replacement command.
To create the recoded variable Dean’s List from the numerical 
variable GPA (C7), with the DATA worksheet of the Recode proj-
ect still open:
	 1.	 Enter Dean’s List as the name of the empty column C8.
	 2.	 Select Calc ➔ Calculator.
In the Calculator dialog box (shown below):
	 3.	 Enter C8 in the Store result in variable box.
	 4.	 Enter IF(GPA < 3.3, "No", "Yes") in the Expression box.
	 5.	 Click OK.
Variables can also be recoded into multiple categories by using the 
Data ➔ Code command. Read the Short Takes for Chapter 1 to 
learn more about this advanced recoding technique.
MG1.4  Types of Sampling Methods
Simple Random Samples
Example  Create a simple random sample with replacement of 
size 40 from a population of 800 items.
Instructions  Use Sample From Columns to create the ran-
dom sample. For the example, first create the list of 800 employee 
numbers in column C1. Select Calc ➔ Make Patterned Data ➔ 
Simple Set of Numbers. In the Simple Set of Numbers dialog box 
(shown below):
	 1.	 Enter C1 in the Store patterned data in box.
	 2.	 Enter 1 in the From first value box.
	 3.	 Enter 800 in the To last value box.
	 4.	 Click OK.
With the worksheet containing the column C1 list still open:
	 5.	 Select Calc ➔ Random Data ➔ Sample from Columns.
In the Sample From Columns dialog box (shown below):
	 6.	 Enter 40 in the Number of rows to sample box.
	 7.	 Enter C1 in the From columns box.
	 8.	 Enter C2 in the Store samples in box.
	 9.	 Click OK.

64
U s i n g  S tat i s t i c s
The Choice Is Yours
Even though he is still in his 20s, Tom Sanchez realizes that he needs to start 
funding his 401(k) retirement plan now because you can never start too early to 
save for retirement. Based on research he has already done, Sanchez seeks to in-
vest his money in one or more retirement funds. He decides to contact the Choice 
Is Yours investment service that a business professor had once said was noted for 
its ethical behavior and fairness toward younger investors.
What Sanchez did not know is that Choice Is Yours has already been think-
ing about studying a wide variety of retirement funds, with the business objective 
of being able to suggest appropriate funds for its younger investors. A company 
task force has already selected 316 retirement funds that may prove appropriate 
for younger investors. You have been asked to define, collect, organize, and visu-
alize data about these funds in ways that could assist prospective clients making 
decisions about the funds in which they will invest. What facts about each fund 
would you collect to help customers compare and contrast the many funds?
You decide that a good starting point would be to define the variables for key 
characteristics of each fund, including each fund’s past performance. You also decide 
to define variables such as the amount of assets that a fund manages and whether the 
goal of a fund is to invest in companies whose earnings are expected to substantially 
increase in future years (a “growth” fund) or invest in companies whose stock price is 
undervalued, priced low relative to their earnings potential (a “value” fund).
You collect data from appropriate sources and use the business convention of plac-
ing the data for each variable in its own column in a worksheet. As you think more about 
your task, you realize 
that 316 rows of data, 
one for each fund in the 
sample, would be hard 
for prospective clients 
such as Tom Sanchez to 
review easily.
Is there some-
thing else you can do? 
Can you organize and 
present these data to 
prospective clients in a 
more helpful and com-
prehensible manner?
Contents
2.1	 Organizing Categorical 
Variables
2.2	 Organizing Numerical 
Variables
Classes and Excel Bins
Stacked and Unstacked Data
2.3	 Visualizing Categorical 
Variables
2.4	 Visualizing Numerical 
Variables
2.5	 Visualizing Two Numerical 
Variables
2.6	 Organizing Many Categorical 
Variables
2.7	 Challenges in Organizing and 
Visualizing Variables
Guidelines for Constructing 
Visualizations
Using Statistics: The Choice Is 
Yours, Revisited
chapter 2 Excel Guide
chapter 2 Minitab Guide
Objectives
To construct tables and charts for 
categorical variables
To construct tables and charts for 
numerical variables
To learn the principles of properly 
presenting graphs
To organize and analyze many 
variables
Chapter
Organizing and 
Visualizing Variables
2
Ryan R Fox/Shutterstock

	
	﻿	65
	
CHAPTER 2  Organizing and Visualizing Variables	
65
A
rranging data into columns marks the beginning of the third task of the DCOVA 
framework, Organizing the data collected into tables. While a worksheet full of data 
columns is a table in the simplest sense, you need to do more for the reasons noted in 
the scenario.
Designers of the first business computing systems faced a similar problem. Operating un-
der the presumption that the more data shown to decision makers, the better, they created pro-
grams that listed all the data collected, one line at a time, in lengthy reports that consumed 
much paper and that could weigh many pounds. Such reports often failed to facilitate decision 
making as most decision makers did not have the time to read through a report that could be 
dozens or hundreds of pages long.
What those decision makers needed was information that summarized the detailed data. 
Likewise, you need to take the detailed worksheets containing the variables and organize tabu-
lar or visual summaries of the data. Many times you do both: You first construct tables that 
summarize variables and then construct charts and other visual displays based on those tables. 
In other words, the DCOVA third and fourth tasks, Organize and Visualize the variables, are 
often done in tandem or together. When so combined, the Organize and Visualize tasks can 
sometimes help jumpstart the Analysis task by enabling a decision maker reach preliminary 
conclusions about data that can be tested during the Analyze task.
(Later, in Chapter 17, you will learn that recent advances in computing technology have 
made practical methods from the interdisciplinary field of business analytics that enables you 
to combine the organizing and visualizing tasks with the fifth DCOVA task, Analyze the data 
collected to reach conclusions and present results, for even very large or “big” data sets.)
For its examples, this chapter makes extensive use of  Retirement Funds , the Excel workbook 
or Minitab worksheet file that contains the sample of 316 funds mentioned in the scenario. This 
is one of many files that you can download and use with this book as explained in Appendix C.
How to Proceed with This Chapter
Table 2.1 presents the methods used to organize and visualize data that are discussed in this 
book. This table includes methods for summarizing and describing variables that some instruc-
tors prefer to group with the methods of this chapter but which this book discusses in other 
chapters.
Student Tip
Because of this jumpstart 
effect, you will find your-
self repeating some of 
the organizing and visual-
izing methods discussed 
in this chapter when you 
study later chapters that 
focus on methods that 
help you to analyze the  
variables.
T a b l e  2 . 1
Methods to Organize 
and Visualize Variables
For Categorical Variables:
Summary table, contingency table (in Section 2.1)
Bar chart, pie chart, Pareto chart, side-by-side bar chart (in Section 2.3)
For Numerical Variables:
Ordered array, frequency distribution, relative frequency distribution, percentage distribution, 
cumulative percentage distribution (in Section 2.2)
Stem-and-leaf display, histogram, polygon, cumulative percentage polygon (in Section 2.4)
Boxplot (in Section 3.3)
Normal probability plot (in Section 6.3)
Mean, median, mode, quartiles, geometric mean, range, interquartile range, standard deviation, 
variance, coefficient of variation, skewness, kurtosis (in Sections 3.1, 3.2, and 3.3)
Index numbers (in Section 16.8)
For Two Numerical Variables:
Scatter plot, time-series plot (in Section 2.5)
Sparklines (in Section 17.1)
For Categorical and Numerical Variables Considered Together:
Organizing Many Categorical Variables (in Section 2.6)
Multidimensional contingency tables, PivotTables, gauges, bullet graphs, and treemaps (in Section 17.1)
Cluster analysis (in Section 17.5)
Multidimensional scaling (in Section 17.6)
Learn More
Learn more about retire-
ment funds in general as well 
as the variables found in the 
Retirement Funds worksheet 
in AllAboutRetirement-
Funds.pdf online section.

66	
Chapter 2  Organizing and Visualizing Variables
When you organize the data, you sometimes begin to discover patterns or relationships in 
the data, as examples in Section 2.1 and 2.2 illustrate. To better explore and discover patterns and 
relationships, you can visualize your data by creating various charts and special displays.
Because the methods used to organize and visualize the data collected for categorical vari-
ables differ from the methods used to organize and visualize the data collected for numerical 
variables, this chapter discusses them in separate sections. You will always need to first deter-
mine the type of variable, numerical or categorical, you seek to organize and visualize, in order 
to choose appropriate methods.
This chapter also contains a section on common errors that people make when visualizing 
variables. When learning methods to visualize variables, you should be aware of such possible 
errors because of the potential of such errors to mislead and misinform decision makers about 
the data you have collected.
2.1  Organizing Categorical Variables
You organize categorical variables by tallying the values of a variable by categories and plac-
ing the results in tables. Typically, you construct a summary table to organize the data for a 
single categorical variable and you construct a contingency table to organize the data from two 
or more categorical variables.
The Summary Table
A summary table tallies the values as frequencies or percentages for each category. A sum-
mary table helps you see the differences among the categories by displaying the frequency, 
amount, or percentage of items in a set of categories in a separate column. Table 2.2 presents a 
summary table that tallies responses to a recent survey that asked young adults about the main 
reason that they shop online. From this table, stored in  Online Shopping , you can conclude that 
37% shop online mainly for better prices and convenience and that 29% shop online mainly to 
avoid holiday crowds and hassles.
T a b l e  2 . 2
Main Reason Young 
Adults Shop Online
Demand
Percentage
Better prices
37%
Avoiding holiday crowds or 
hassles
29%
Convenience
18%
Better selection
13%
Ships directly
  3%
Source: Data extracted and adapted from “Main Reason Young Adults Shop Online?” 
USA Today, December 5, 2012, p. 1A.
Example 2.1
Summary Table of 
Levels of Risk of 
Retirement Funds
The sample of 316 retirement funds for the Choice Is Yours scenario (see page 64) includes the 
variable risk that has the defined categories low, average, and high. Construct a summary table 
of the retirement funds, categorized by risk.
Solution  From Table 2.3, you can see that about two-thirds of the funds have low risk. 
About 30% of the funds have average risk. Very few funds have high risk.
T a b l e  2 . 3
Frequency and 
Percentage Summary 
Table of Risk Level for 
316 Retirement Funds
Fund Risk Level
Number of Funds
Percentage of Funds
Low
212
67.09%
Average
  91
28.80%
High
  13
    4.11%
Total
316
100.00%

	
2.1  Organizing Categorical Variables	
67
The Contingency Table
A contingency table cross-tabulates, or tallies jointly, the values of two or more categorical 
variables, allowing you to study patterns that may exist between the variables. Tallies can be 
shown as a frequency, a percentage of the overall total, a percentage of the row total, or a per-
centage of the column total, depending on the type of contingency table you use. Each tally 
appears in its own cell, and there is a cell for each joint response, a unique combination of 
values for the variables being tallied. In the simplest contingency table, one that contains only 
two categorical variables, the joint responses appear in a table such that the tallies of one vari-
able are located in the rows and the tallies of the other variable are located in the columns.
For the sample of 316 retirement funds for the Choice Is Yours scenario, you might create 
a contingency table to examine whether there is any pattern between the fund type variable and 
the risk level variable. Because the fund type is one of two possible values (Growth or Value) 
and the risk level is one of three possible values (Low, Average, or High), there would be six 
possible joint responses for this table. You could create the table by hand tallying the joint re-
sponses for each of the retirement funds in the sample. For example, for the first fund listed in 
the sample you would add to the tally in the cell that is the intersection of the Growth row and 
the Low column because the first fund is of type Growth and risk level Low. However, a better 
choice is to use one of the ways described in the Chapter 2 Excel Guide or Minitab Guide to 
automate this process.
Table 2.4 presents the completed contingency table after all 316 funds have been tallied. 
In this table, you can see that there are 143 retirement funds that have the value Growth for the 
fund type variable and the value Low for the risk level variable and that Growth and Low was 
the most frequent joint response for the fund type and risk level variables.
T a b l e  2 . 5
Contingency Table 
Displaying Fund Type 
and Risk Level, Based 
on Percentage of 
Overall Total
T a b l e  2 . 6
Contingency Table 
Displaying Fund Type 
and Risk Level, Based 
on Percentage of Row 
Total
T a b l e  2 . 4
Contingency Table 
Displaying Fund Type 
and Risk Level
RISK LEVEL
FUND TYPE
Low
Average
High
Total
Growth
143
74
10
227
Value
  69
17
  3
  89
Total
212
91
13
316
Contingency tables that display cell values as a percentage of a total can help show patterns 
between variables. Table 2.5 shows a contingency table that displays values as a percentage of 
the Table 2.4 overall total (316), Table 2.6 shows a contingency table that displays values as a 
percentage of the Table 2.4 row totals (227 and 89), and Table 2.7 shows a contingency table 
that displays values as a percentage of the Table 2.4 column totals (212, 91, and 13).
RISK LEVEL
FUND TYPE
Low
Average
High
Total
Growth
45.25%
23.42%
3.16%
  71.84%
Value
21.84%
  5.38%
0.95%
  28.16%
Total
67.09%
28.80%
4.11%
100.00%
RISK LEVEL
FUND TYPE
Low
Average
High
Total
Growth
63.00%
32.60%
4.41%
100.00%
Value
77.53%
19.10%
3.37%
100.00%
Total
67.09%
28.80%
4.11%
100.00%
Student Tip
Remember, each joint 
response gets tallied into 
only one cell.
Like worksheet cells,  
contingency table cells  
are the intersections of rows 
and columns, but unlike  
in a worksheet, both the  
rows and the columns  
represent variables. To  
identify placement, the  
terms row variable and  
column variable are often 
used.

68	
Chapter 2  Organizing and Visualizing Variables
Table 2.5 shows that 71.84% of the funds sampled are growth funds, 28.16% are value 
funds, and 45.25% are growth funds that have low risk. Table 2.6 shows that 63% of the growth 
funds have low risk, while 77.53% of the value funds have low risk. Table 2.7 shows that of  
the funds that have low risk, 67.45% are growth funds. From Tables 2.5–2.7, you see that 
growth funds are less likely than value funds to have low risk.
Problems for Section 2.1
Learning the Basics
2.1  A categorical variable has three categories, with the follow-
ing frequencies of occurrence:
Category
Frequency
A
13
B
28
C
  9
a.	 Compute the percentage of values in each category.
b.	 What conclusions can you reach concerning the categories?
2.2  The following data represent the responses to two questions 
asked in a survey of 40 college students majoring in business:  
What is your gender? 1M = male; F = female2  and What  
is your major? (A = Accounting; C = Computer Information 
Systems; M = Marketing):
Gender:	 M	 M	 M	
F	
M	
F	
F	
M	
F	
M 
Major:	
A	
 C	
C	
M	
A	
C	
A	
A	
C	
C
Gender:	 F	
M	 M	 M	 M	
F	
F	
M	
F	
F 
Major:	
A	
A	
A	
M	
C	
M	
A	
A	
A	
C
Gender:	 M	 M	 M	 M	
F	
M	
F	
F	
M	
M 
Major:	
C	
 C	  A	
A	
M	 M	
C	
A	
A	
A
Gender:	 F	
M	 M	 M	 M	
F	
M	
F	
M	
M 
Major:	
C	
C	
A	
A	
A	
A	
C	
C	
A	
C
a.	 Tally the data into a contingency table where the two rows rep-
resent the gender categories and the three columns represent 
the academic major categories.
b.	 Construct contingency tables based on percentages of all 40 
student responses, based on row percentages and based on col-
umn percentages.
Applying the Concepts
2.3  The following table, stored in  Smartphone Sales , represents 
the annual percentage of smartphones sold in 2011, 2012, and 
2013 (projected).
Type
2011
2012
2013
Android
47%
60%
58%
iOS
19%
22%
23%
Symbian
19%
  5%
  1%
RIM
11%
  6%
  4%
Microsoft
  2%
  4%
10%
Bada
  2%
  3%
  3%
Source: Data extracted from “How High Can Apple Fly,” USA 
Today, October 5, 2012, p. 1A, 2A.
a.	 What conclusions can you reach about the market for smart-
phones in 2011, 2012, and 2013?
b.	 What differences are there in the market for smartphones in 
2011, 2012, and 2013?
2.4  The Edmunds.com NHTSA Complaints Activity Report 
contains consumer vehicle complaint submissions by automaker, 
brand, and category (data extracted from edmu.in/Ybmpuz). The 
following tables, stored in  Automaker1  and  Automaker2 , rep-
resent complaints received by automaker and complaints received 
by category for January 2013.
Automaker
Number
American Honda
169
Chrysler LLC
439
Ford Motor Company
440
General Motors
551
Nissan Motors Corporation
467
Toyota Motor Sales
332
Other
516
RISK LEVEL
FUND TYPE
Low
Average
High
Total
Growth
  67.45%
  81.32%
  76.92%
  71.84%
Value
  32.55%
  18.68%
  23.08%
  28.16%
Total
100.00%
100.00%
100.00%
100.00%
T a b l e  2 . 7
Contingency Table 
Displaying Fund Type 
and Risk Level, Based 
on Percentage of 
Column Total

	
2.1  Organizing Categorical Variables	
69
a.	 Compute the percentage of complaints for each automaker.
b.	 What conclusions can you reach about the complaints for the 
different automakers?
Category
Number
Airbags and seatbelts
   201
Body and glass
   182
Brakes
   163
Fuel/emission/exhaust system
   240
Interior electronics/hardware
   279
Powertrain
1,148
Steering
   397
Tires and wheels
     71
c.	 Compute the percentage of complaints for each category.
d.	 What conclusions can you reach about the complaints for  
different categories?
2.5  The 2013 Mortimer Spinks and Computer Weekly Technol-
ogy Survey reflect the views of technology and digital experts 
across the United Kingdom (bit.ly/WS4jg3). Respondents were 
asked, “What is the most important factor influencing the success 
of a tech start-up?” Assume the following results:
Most Important Factor
Frequency
Leadership
400
Marketing
346
Product
464
Technology
  86
a.	 Compute the percentage of values for each factor.
b.	 What conclusions can you reach concerning factors influencing 
successful tech start-ups?
2.6  The following table represents world oil production 
in millions of barrels a day in the third quarter of 2011:
Region
Oil Production (millions 
of barrels a day)
Iran
  3.53
Saudi Arabia
  9.34
Other OPEC countries
22.87
Non-OPEC countries
52.52
Source: International Energy Agency, 2012.
a.	 Compute the percentage of values in each category.
b.	 What conclusions can you reach concerning the production of 
oil in the third quarter of 2011?
2.7  Visier’s Survey of Employers explores how North Ameri-
can organizations are solving the challenges of delivering work-
force analytics. Employers were asked what would help them be  
SELF 
Test 
successful with human resources metrics and reports. The  
responses (stored in  Needs ) were as follows:
Needs
Frequency
Easier-to-use analytic tools
127
Faster access to data
  41
Improved ability to present and interpret  
data
 
123
Improved ability to plan actions
  33
Improved ability to predict impacts of my 
actions
  49
Improved relationships to the business line 
organizations
  37
Source: Data extracted from bit.ly/YuWYXc.
a.	 Compute the percentage of values for each response need.
b.	 What conclusions can you reach concerning needs for em-
ployer success with human resources metrics and reports?
2.8  A survey of 1,085 adults asked “Do you enjoy shopping for 
clothing for yourself?” The results indicated that 51% of the fe-
males enjoyed shopping for clothing for themselves as compared 
to 44% of the males. (Data extracted from “Split Decision on 
Clothes Shopping,” USA Today, January 28, 2011, p. 1B.) The 
sample sizes of males and females were not provided. Suppose 
that the results were as shown in the following table:
GENDER
ENJOY SHOPPING
Male
Female
Total
Yes
238
276
  514
No
304
267
  571
Total
542
543
1,085
a.	 Construct contingency tables based on total percentages, row 
percentages, and column percentages.
b.	 What conclusions can you reach from these analyses?
2.9  Each day at a large hospital, hundreds of laboratory tests are 
performed. The rate of “nonconformances,” tests that were done 
improperly (and therefore need to be redone), has seemed to be 
steady, at about 4%. In an effort to get to the root cause of the non-
conformances, the director of the lab decided to study the results 
for a single day. The laboratory tests were subdivided by the shift 
of workers who performed the lab tests. The results are as follows:
SHIFT
LAB TESTS PERFORMED
Day
Evening
Total
Nonconforming
  16
  24
    40
Conforming
654
306
  960
Total
670
330
1,000
a.	 Construct contingency tables based on total percentages, row 
percentages, and column percentages.

70	
Chapter 2  Organizing and Visualizing Variables
b.	 Which type of percentage—row, column, or total—do you 
think is most informative for these data? Explain.
c.	 What conclusions concerning the pattern of nonconforming 
laboratory tests can the laboratory director reach?
2.10  Do social recommendations increase ad effectiveness? A 
study of online video viewers compared viewers who arrived at an 
advertising video for a particular brand by following a social me-
dia recommendation link to viewers who arrived at the same video 
by web browsing. Data were collected on whether the viewer 
could correctly recall the brand being advertised after seeing the 
video. The results were:
CORRECTLY RECALLED THE 
BRAND
ARRIVAL METHOD
Yes
No
Recommendation
407
150
Browsing
193
  91
Source: Data extracted from “Social Ad Effectiveness: An Unruly 
White Paper,” www.unrulymedia.com, January 2012, p. 3.
What do these results tell you about social recommendations?
You organize numerical variables by creating ordered arrays of one or more variables. This sec-
tion uses the numerical variable meal cost, which represents the cost of a meal at a restaurant, 
as the basis for its examples. Because the meal cost data has been collected from a sample of  
100 restaurants that can be further categorized by their locations as either “city” or “suburban” res-
taurants, the variable meal cost raises the common question about how data should be organized in 
a worksheet when a numerical variable represents data from more than one group. This question is 
answered at the end of this section, after ordered arrays and distributions are first discussed.
The Ordered Array
An ordered array arranges the values of a numerical variable in rank order, from the smallest 
value to the largest value. An ordered array helps you get a better sense of the range of values in 
your data and is particularly useful when you have more than a few values. For example, finan-
cial analysts reviewing travel and entertainment costs might have the business objective of de-
termining whether meal costs at city restaurants differ from meal costs at suburban restaurants. 
They collect data from a sample of 50 city restaurants and from a sample of 50 suburban restau-
rants for the cost of one meal (in $). Table 2.8A shows the unordered data (stored in  Restaurants ).  
The lack of ordering prevents you from reaching any quick conclusions about meal costs.
2.2  Organizing Numerical Variables
T a b l e  2 . 8 A
Meal Cost at 50 City 
Restaurants and 50 
Suburban Restaurants
City Restaurant Meal Costs
33   26   43   32   44   44   50   42   44   36   61   50   51   50   76   53   44   77   57   43   29   34   77   50   74
56   67   57   66   80   68   42   48   60   35   45   32   25   74   43   39   55   65   35   61   37   54   41   33   27
Suburban Restaurant Meal Costs
47   48   35   59   44   51   37   36   43   52   34   38   51   34   51   34   51   56   26   34   34   44   40   31   54
41   50   71   60   37   27   34   48   39   44   41   37   47   67   68   49   29   33   39   39   28   46   70   60   52
In contrast, Table 2.8B, the ordered array version of the same data, enables you to quickly 
see that the cost of a meal at the city restaurants is between $25 and $80 and that the cost of a 
meal at the suburban restaurants is between $26 and $71.
T a b l e  2 . 8 B
Ordered Arrays of 
Meal Costs at 50 City 
Restaurants and 50 
Suburban Restaurants
City Restaurant Meal Cost
25   26   27   29   32   32   33   33   34   35   35   36   37   39   41   42   42   43   43   43   44   44   44   44   45
48   50   50   50   50   51   53   54   55   56   57   57   60   61   61   65   66   67   68   74   74   76   77   77   80
Suburban Restaurant Meal Cost
26   27   28   29   31   33   34   34   34   34   34   34   35   36   37   37   37   38   39   39   39   40   41   41   43
44   44   44   46   47   47   48   48   49   50   51   51   51   51   52   52   54   56   59   60   60   67   68   70   71

	
2.2  Organizing Numerical Variables	
71
When your collected data contains a large number of values, reaching conclusions from an 
ordered array can be difficult. In such cases, creating one of the distributions discussed in the 
following pages would be a better choice.
The Frequency Distribution
A frequency distribution tallies the values of a numerical variable into a set of numerically 
ordered classes. Each class groups a mutually exclusive range of values, called a class interval. 
Each value can be assigned to only one class, and every value must be contained in one of the 
class intervals.
To create a useful frequency distribution, you must consider how many classes would 
be appropriate for your data as well as determine a suitable width for each class interval. In 
general, a frequency distribution should have at least 5 and no more than 15 classes because 
having too few or too many classes provides little new information. To determine the class 
interval width [see Equation (2.1)], you subtract the lowest value from the highest value and 
divide that result by the number of classes you want the frequency distribution to have.
Determining the Class Interval Width
	
Interval width = highest value - lowest value
number of classes
	
(2.1)
For the city restaurant meal cost data shown in Tables 2.8A and 2.8B, between 5 and 10 
classes are acceptable, given the size (50) of that sample. From the city restaurant meal costs 
ordered array in Table 2.8B, the difference between the highest value of $80 and the lowest 
value of $25 is $55. Using Equation (2.1), you approximate the class interval width as follows:
55
10 = 5.5
This result suggests that you should choose an interval width of $5.50. However, your 
width should always be an amount that simplifies the reading and interpretation of the fre-
quency distribution. In this example, such an amount would be either $5 or $10, and you 
should choose $10, which creates 7 classes, and not $5, which creates 13 classes, too many for 
the sample size of 50.
Because each value can appear in only one class, you must establish proper and clearly 
defined class boundaries for each class. For example, if you chose $10 as the class interval for 
the restaurant data, you would need to establish boundaries that would include all the values 
and simplify the reading and interpretation of the frequency distribution. Because the cost of 
a city restaurant meal varies from $25 to $80, establishing the first class interval as $20 to less 
than $30, the second as $30 to less than $40, and so on, until the last class interval is $80 to less 
than $90, would meet the requirements. Table 2.9 contains frequency distributions of the cost 
per meal for the 50 city restaurants and the 50 suburban restaurants using these class intervals.
T a b l e  2 . 9
Frequency 
Distributions of the 
Meal Costs for 50 City 
Restaurants and 50 
Suburban Restaurants
Meal Cost ($)
City Frequency
Suburban Frequency
20 but less than 30
  4
  4
30 but less than 40
10
17
40 but less than 50
12
13
50 but less than 60
11
10
60 but less than 70
  7
  4
70 but less than 80
  5
  2
80 but less than 90
  1
  0
Total
50
50

72	
Chapter 2  Organizing and Visualizing Variables
The frequency distribution allows you to reach some preliminary conclusions about the 
data. For example, Table 2.9 shows that the cost of city restaurant meals is concentrated be-
tween $30 and $60, as is the cost of suburban restaurant meals. However, many more meals at 
suburban restaurants cost between $30 and $40 than at city restaurants.
For some charts discussed later in this chapter, class intervals are identified by their class 
midpoints, the values that are halfway between the lower and upper boundaries of each class. 
For the frequency distributions shown in Table 2.9, the class midpoints are $25, $35, $45, $55, 
$65, $75, $85, and $95. Note that well-chosen class intervals lead to class midpoints that are 
simple to read and interpret, as in this example.
If the data you have collected do not contain a large number of values, different sets of 
class intervals can create different impressions of the data. Such perceived changes will dimin-
ish as you collect more data. Likewise, choosing different lower and upper class boundaries 
can also affect impressions.
Student Tip
The total of the frequency  
column must always 
equal the total number  
of values.
T a b l e  2 . 1 0
Frequency Distributions 
of the One-Year Return 
Percentage for Growth 
and Value Funds
Example 2.2
Frequency  
Distributions of  
the One-Year  
Return Percentages 
for Growth and 
Value Funds
As a member of the company task force in The Choice Is Yours scenario (see page 64), you are 
examining the sample of 316 retirement funds stored in  Retirement Funds .You want to com-
pare the numerical variable 1YrReturn%, the one-year percentage return of a fund, for the two 
subgroups that are defined by the categorical variable Type (Growth and Value). You construct 
separate frequency distributions for the growth funds and the value funds.
Solution  The one-year percentage returns for both the growth and value funds are con-
centrated between 10 and 20 (see Table 2.10).
One-Year Return Percentage
Growth Frequency
Value Frequency
−15 but less than −10
    1
  0
−10 but less than −5
    0
  0
−5 but less than 0
    0
  0
0 but less than 5
    6
  2
5 but less than 10
  23
12
10 but less than 15
104
29
15 but less than 20
  75
37
20 but less than 25
  12
  8
25 but less than 30
    3
  1
30 but less than 35
    3
  0
Total
227
89
In the solution for Example 2.2, the total frequency is different for each group (227 and 
89). When such totals differ among the groups being compared, you cannot compare the distri-
butions directly as was done in Table 2.9 because of the chance that the table will be misinter-
preted. For example, the frequencies for the class interval “5 but less than 10” for growth and 
“10 but less than 15” for value look similar—23 and 29—but represent two very different parts 
of a whole: 23 out of 227 and 29 out of 89, or about 10% and 33%, respectively. When the total 
frequency differs among the groups being compared, you construct either a relative frequency 
distribution or a percentage distribution.

	
2.2  Organizing Numerical Variables	
73
The proportion, or relative frequency, in each group is equal to the number of values in 
each class divided by the total number of values. The percentage in each group is its proportion 
multiplied by 100%.
The Relative Frequency Distribution  
and the Percentage Distribution
Relative frequency and percentage distributions present tallies in ways other than as frequen-
cies. A relative frequency distribution presents the relative frequency, or proportion, of 
the total for each group that each class represents. A percentage distribution presents the 
percentage of the total for each group that each class represents. When you compare two or 
more groups, knowing the proportion (or percentage) of the total for each group is more useful 
than knowing the frequency for each group, as Table 2.11 demonstrates. Compare this table to  
Table 2.9 on page 71, which displays frequencies. Table 2.11 organizes the meal cost data in a 
manner that facilitates comparisons.
Classes and Excel Bins
To make use of Microsoft Excel features that can 
help you construct a frequency distribution, or any 
of the other types of distributions discussed in this 
chapter, you must implement your set of classes 
as a set of Excel bins. While bins and classes are 
both ranges of values, bins do not have explicitly 
stated intervals.
You establish bins by creating a column that 
contains a list of bin numbers arranged in ascend-
ing order. Each bin number explicitly states the up-
per boundary of its bin. Bins’ lower boundaries are 
defined implicitly: A bin’s lower boundary is the first 
value greater than the previous bin number. For 
the column of bin numbers 4.99, 9.99, and 14.99, 
the second bin has the explicit upper boundary of 
9.99 and has the implicit lower boundary of “val-
ues greater than 4.99.” Compare this to a class 
interval, which defines both the lower and upper 
boundaries of the class, such as in “0 (lower) but 
less than 5 (upper).”
Because the first bin number does not have 
a “previous” bin number, the first bin always has 
negative infinity as its lower boundary. A common 
workaround to this problem, used in the examples 
throughout this book (and in PHStat, too), is to de-
fine an extra bin, using a bin number that is slightly 
lower than the lower boundary value of the first 
class. This extra bin number, appearing first, will al-
low the now-second bin number to better approxi-
mate the first class, though at the cost of adding an 
unwanted bin to the results.
In this chapter, Tables 2.9 through 2.13 use 
class groupings in the form “valueA but less than 
valueB.” You can translate class groupings in this 
form into nearly equivalent bins by creating a list of 
bin numbers that are slightly lower than each valueB 
that appears in the class groupings. For example, the 
Table 2.10 classes on page 72 could be translated 
into nearly equivalent bins by using this bin number 
list: −15.01 (the extra bin number is slightly lower 
than the first lower boundary value −15), −10.01 
(slightly less than −10, −5.01, −0.01, 4.99, 9.99, 
14.99, 19.99, 24.99, 29.99, and 34.99.
For class groupings in the form “all val-
ues from valueA to valueB,” such as the set 0.0 
through 4.9, 5.0 through 9.9, 10.0 through 14.9, 
and 15.0 through 19.9, you can approximate 
each class grouping by choosing a bin number 
slightly more than each valueB, as in this list of bin  
numbers: −0.01 (the extra bin number), 4.99 
(slightly more than 4.9), 9.99, 14.99, and 19.99.
T a b l e  2 . 1 1
Relative Frequency 
Distributions 
and Percentage 
Distributions of the 
Meal Costs at City and 
Suburban Restaurants
City
Suburban
Meal Cost ($)
Relative 
Frequency
Percentage 
Relative 
Frequency
Percentage 
20 but less than 30
0.08
  8.0%
0.08
    8.0%
30 but less than 40
0.20
20.0%
0.34
  34.0%
40 but less than 50
0.24
24.0%
0.26
  26.0%
50 but less than 60
0.22
22.0%
0.20
  20.0%
60 but less than 70
0.14
14.0%
0.08
    8.0%
70 but less than 80
0.10
10.0%
0.04
    4.0%
80 but less than 90
0.02
    2.0%
0.00
    0.0%
Total
1.00
100.0%
1.00
100.0%

74	
Chapter 2  Organizing and Visualizing Variables
If there are 80 values and the frequency in a certain class is 20, the proportion of values in that 
class is
20
80 = 0.25
and the percentage is
0.25 * 100% = 25%
You construct a relative frequency distribution by first determining the relative frequency in 
each class. For example, in Table 2.9 on page 71, there are 50 city restaurants, and the cost per 
meal at 11 of these restaurants is between $50 and $60. Therefore, as shown in Table 2.11, the 
proportion (or relative frequency) of meals that cost between $50 and $60 at city restaurants is
11
50 = 0.22
You construct a percentage distribution by multiplying each proportion (or relative fre-
quency) by 100%. Thus, the proportion of meals at city restaurants that cost between $50 and 
$60 is 11 divided by 50, or 0.22, and the percentage is 22%. Table 2.11 on page 73 presents 
the relative frequency distribution and percentage distribution of the cost of meals at city and 
suburban restaurants.
From Table 2.11, you conclude that meal cost is slightly more at city restaurants than at 
suburban restaurants. You note that 14% of the city restaurant meals cost between $60 and $70 
as compared to 8% of the suburban restaurant meals and that 20% of the city restaurant meals 
cost between $30 and $40 as compared to 34% of the suburban restaurant meals.
Computing the Proportion or Relative Frequency
The proportion, or relative frequency, is the number of values in each class divided by the 
total number of values:
	
Proportion = relative frequency = number of values in each class
total number of values
	
(2.2)
Student Tip
The total of the relative 
frequency column must 
always be 1.00. The 
total of the percentage 
column must always  
be 100.
Example 2.3
Relative Frequency 
Distributions and 
Percentage Dis-
tributions of the 
One-Year Return 
Percentage for 
Growth and Value 
Funds
As a member of the company task force in The Choice Is Yours scenario (see page 64), you 
want to properly compare the one-year return percentages for the growth and value retirement 
funds. You construct relative frequency distributions and percentage distributions for these 
funds.
Solution  From Table 2.12, you conclude that the one-year return percentage for the 
growth funds is lower than the one-year return percentage for the value funds. For example, 
45.81% of the growth funds have returns between 10 and 15, while 32.58% of the value funds 
have returns between 10 and 15. Of the growth funds, 33.04% have returns between 15 and 20 
as compared to 41.57% of the value funds.

	
2.2  Organizing Numerical Variables	
75
The Cumulative Distribution
The cumulative percentage distribution provides a way of presenting information about the 
percentage of values that are less than a specific amount. You use a percentage distribution as 
the basis to construct a cumulative percentage distribution.
For example, you might want to know what percentage of the city restaurant meals cost 
less than $40 or what percentage cost less than $50. Starting with the Table 2.11 meal cost 
percentage distribution for city restaurant meal costs on page 73, you combine the percentages 
of individual class intervals to form the cumulative percentage distribution. Table 2.13 presents 
the necessary calculations. From this table, you see that none (0%) of the meals cost less than 
$20, 8% of meals cost less than $30, 28% of meals cost less than $40 (because 20% of the 
meals cost between $30 and $40), and so on, until all 100% of the meals cost less than $90.
T a b l e  2 . 1 3
Developing the 
Cumulative Percentage 
Distribution for City 
Restaurant Meal Costs
From Table 2.11:
Percentage (%) of Meal Costs That Are Less Than  
the Class Interval Lower Boundary
Class Interval
Percentage (%)
20 but less than 30
  8
0 (there are no meals that cost less than 20)
30 but less than 40
20
    8 = 0 + 8
40 but less than 50
24
  28 = 8 + 20
50 but less than 60
22
  52 = 8 + 20 + 24
60 but less than 70
14
  74 = 8 + 20 + 24 + 22
70 but less than 80
10
  88 = 8 + 20 + 24 + 22 + 14
80 but less than 90
  2
  98 = 8 + 20 + 24 + 22 + 14 + 10
90 but less than 100
  0
100 = 8 + 20 + 24 + 22 + 14 + 10 + 2
Table 2.14 is the cumulative percentage distribution for meal costs that uses cumulative 
calculations for the city restaurants (shown in Table 2.13) as well as cumulative calculations 
for the suburban restaurants (which are not shown). The cumulative distribution shows that the 
cost of suburban restaurant meals is lower than the cost of meals in city restaurants. This distri-
bution shows that 42% of the suburban restaurant meals cost less than $40 as compared to 28% 
of the meals at city restaurants; 68% of the suburban restaurant meals cost less than $50, but 
only 52% of the city restaurant meals do; and 88% of the suburban restaurant meals cost less 
than $60 as compared to 74% of such meals at the city restaurants.
Growth
Value
One-Year Return 
Percentage
Relative 
Frequency
 
Percentage
Relative 
Frequency
 
Percentage
−15 but less than −10
0.0044
0.44
0.0000
0.00
−10 but less than −5
0.0000
0.00
0.0000
0.00
−5 but less than 0
0.0000
0.00
0.000
0.00
0 but less than 5
0.0264
2.64
0.0225
2.25
5 but less than 10
0.1013
10.13
0.1348
13.48
10 but less than 15
0.4581
45.81
0.3258
32.58
15 but less than 20
0.3304
33.04
0.4157
41.57
20 but less than 25
0.0529
5.29
0.0899
8.99
25 but less than 30
0.0132
1.32
0.0112
1.12
30 but less than 35
0.0132
1.32
0.0000
0.00
Total
1.0000
100.00
1.0000
100.00
T a b l e  2 . 1 2
Relative Frequency 
Distributions 
and Percentage 
Distributions of the 
One-Year Return 
Percentage for 
Growth and Value 
Funds

76	
Chapter 2  Organizing and Visualizing Variables
Unlike in other distributions, the rows of a cumulative distribution do not correspond to 
class intervals. (Recall that class intervals are mutually exclusive. The rows of cumulative dis-
tributions are not: the next row “down” includes all of the rows above it.) To identify a row, 
you use the lower class boundaries from the class intervals of the percentage distribution as is 
done in Table 2.14.
T a b l e  2 . 1 4
Cumulative 
Percentage 
Distributions of 
the Meal Costs for 
City and Suburban 
Restaurants
Meal Cost ($)
Percentage of City  
Restaurants Meals That Cost  
Less Than Indicated Amount
Percentage of Suburban  
Restaurants Meals That Cost  
Less Than Indicated Amount
  20
    0
    0
  30
    8
    8
  40
  28
  42
  50
  52
  68
  60
  74
  88
  70
  88
  96
  80
  98
100
  90
100
100
100
100
100
Example 2.4
Cumulative Per-
centage Distri-
butions of the 
One-Year Return 
Percentage for 
Growth and Value 
Funds
As a member of the company task force in The Choice Is Yours scenario (see page 64), you 
want to continue comparing the one-year return percentages for the growth and value retire-
ment funds. You construct cumulative percentage distributions for the growth and value funds.
Solution  The cumulative distribution in Table 2.15 indicates that returns are lower for the 
growth funds than for the value funds. The table shows that 59.03% of the growth funds and 
48.31% of the value funds have returns below 15%.The table also reveals that 92.07% of the 
growth funds have returns below 20 as compared to 89.89% of the value funds.
T a b l e  2 . 1 5
Cumulative 
Percentage 
Distributions of the 
One-Year Return 
Percentages for 
Growth and Value 
Funds
One-Year Return 
Percentages
Growth Percentage Less 
Than Indicated Value
Value Percentage Less Than 
Indicated Value
−15
    0.00
    0.00
−10
    0.44
    0.00
−5
    0.44
    0.00
0
    0.44
    0.00
5
    3.08
    2.25
10
  13.22
  15.73
15
  59.03
  48.31
20
  92.07
  89.89
25
  97.36
  98.88
30
  98.68
100.00
35
100.00
100.00

	
2.2  Organizing Numerical Variables	
77
Stacked and Unstacked Data
When data for a numerical variable have been 
collected for more than one group, you can enter 
those data in a worksheet as either unstacked or 
stacked data.
In an unstacked format, you create separate 
numerical variables for each group. For example, 
if you entered the meal cost data used in the ex-
amples in this section in unstacked format, you 
would create two numerical variables—city meal 
cost and suburban meal cost—enter the top data 
in Table 2.8A on page 70 as the city meal cost 
data, and enter the bottom data in Table 2.8A as 
the suburban meal cost data.
In a stacked format, you pair a numerical 
variable that contains all of the values with a sec-
ond, separate categorical variable that contains 
values that identify to which group each numeri-
cal value belongs. For example, if you entered the 
meal cost data used in the examples in this sec-
tion in stacked format, you would create a meal 
cost numerical variable to hold the 100 meal cost 
values shown in Table 2.8A and create a second 
location (categorical) variable that would take 
the value “City” or “Suburban,” depending upon 
whether a particular value came from a city or 
suburban restaurant (the top half or bottom half 
of Table 2.8A).
Sometimes a particular procedure in a data 
analysis program will require data to be either 
stacked (or unstacked), and instructions in the 
Excel and Minitab Guides note such require-
ments when they arise. (Both PHStat and Minitab 
have commands that allow you to automate the 
stacking or unstacking of data as discussed in 
the Excel and Minitab Guides for this chapter.) 
Otherwise, it makes little difference whether your 
data are stacked or unstacked. However, if you 
have multiple numerical variables that represent 
data from the same set of groups, stacking your 
data will be the more efficient choice. For this 
reason, the DATA worksheet in  Restaurants  
contains the numerical variable Cost and the 
categorical variable Location to store the meal 
cost data for the sample of 100 restaurants as 
stacked data.
Problems for Section 2.2
Learning the Basics
2.11  Construct an ordered array, given the following data from a 
sample of exam scores in Mathematics:
88 78 78 73 91 78 85
2.12  Construct an ordered array for 30 students’ GPA:
5  4  6  7  9  8  5  6  8  6  5  4  3  6  8   
9  7  6  5  9  5  7  8  9  4  7  9  4  5  8
Can you draw any meaningful conclusions? Why or why not?
2.13  In late 2011 and early 2012, the Universal Health Care 
Foundation of Connecticut surveyed small business owners across 
the state that employed 50 or fewer employees. The purpose of the 
study was to gain insight on the current small business health-care 
environment. Small business owners were asked if they offered 
health-care plans to their employees and if so, what portion (%) of 
the employee monthly health-care premium the business paid. The 
following frequency distribution was formed to summarize the 
portion of premium paid for 89 small businesses who offer health-
care plans to employees:
Portion of Premium Paid (%)
Frequency
less than 1%
  2
1% but less than 26%
  4
26% but less than 51%
16
51% but less than 76%
21
76% but less than 100%
23
100%
23
Source: Data extracted from “Small Business Owners Need 
Affordable Health Care: A Small Business Health Care 
Survey,” Universal Health Care Foundation of Connecticut, 
April 2012, p. 15.
a.	 What percentage of small businesses pays less than 26% of the 
employee monthly health-care premium?
b.	 What percentage of small businesses pays between 26% and 
75% of the employee monthly health-care premium?
c.	 What percentage of small businesses pays more than 75% of 
the employee monthly health-care premium?
2.14  Data were collected on the Facebook website about the 
most “liked” fast food brands. The data values (the number of 
“likes” for each fast food brand) for the brands named ranged from 
1.0 million to 29.2 million.
a.	 If these values are grouped into six class intervals, indicate the 
class boundaries.
b.	 What class interval width did you choose?
c.	 What are the six class midpoints?
Applying the Concepts
2.15  The file  BBCost2012  contains the total cost ($) for four 
tickets, two beers, four soft drinks, four hot dogs, two game pro-
grams, two baseball caps, and parking for one vehicle at each 
of the 30 Major League Baseball parks during the 2012 season. 
These costs were
176   337   223   174   233   185   160   225   324   187   196   153 
184   217   146   172   300   166   184   224   213   242   172   230 
257   152   225   151   224   198
Source: Data extracted from fancostexperience.com/pages/fcx/fci_/
pdfs8.pdf
a.	 Organize these costs as an ordered array.
b.	 Construct a frequency distribution and a percentage distribu-
tion for these costs.
c.	 Around which class grouping, if any, are the costs of attending 
a baseball game concentrated? Explain.

78	
Chapter 2  Organizing and Visualizing Variables
SELF 
Test 
2.16  The file  Utility  contains the following data about 
the cost of electricity (in $) during July 2013 for a random 
sample of 50 one-bedroom apartments in a large city.
96 171 202 178 147 102 153 197 127 82
157 185 90 116 172 111 148 213 130 165
141 149 206 175 123 128 144 168 109 167
95 163 150 154 130 143 187 166 139 149
108 119 183 151 114 135 191 137 129 158
a.	 Construct a frequency distribution and a percentage distribu-
tion that have class intervals with the upper class boundaries 
$99, $119, and so on.
b.	 Construct a cumulative percentage distribution.
c.	 Around what amount does the monthly electricity cost seem to 
be concentrated?
2.17  How much time do commuters living in or near cities spend 
waiting in traffic, and how much does this waiting cost them per 
year? The file  Congestion  contains the time spent waiting in traf-
fic and the yearly cost associated with that waiting for commuters in  
31 U.S. cities. (Source: Data extracted from “The High Cost of Con-
gestion,” Time, October 17, 2011, p. 18.) For both the time spent waiting 
in traffic and the yearly cost associated with that waiting data,
a.	 Construct a frequency distribution and a percentage distribution.
b.	 Construct a cumulative percentage distribution.
c.	 What conclusions can you reach concerning the time ­Americans 
living in or near cities spend sitting in traffic?
d.	 What conclusions can you reach concerning the time and cost 
of waiting in traffic per year?
2.18  How do the average credit scores of people living in differ-
ent American cities differ? The data in  Credit Scores  is an ordered 
array of the average credit scores of 143 American cities. (Data 
extracted from usat.ly/109hZAR.)
a.	 Construct a frequency distribution and a percentage distribution.
b.	 Construct a cumulative percentage distribution.
c.	 What conclusions can you reach concerning the average credit 
scores of people living in different American cities?
2.19  One operation of a mill is to cut pieces of steel into parts 
that will later be used as the frame for front seats in an automo-
bile. The steel is cut with a diamond saw and requires the resulting 
parts to be within {0.005 inch of the length specified by the au-
tomobile company. Data are collected from a sample of 100 steel 
parts and stored in  Steel . The measurement reported is the dif-
ference in inches between the actual length of the steel part, as 
measured by a laser measurement device, and the specified length 
of the steel part. For example, the first value, -0.002 represents a 
steel part that is 0.002 inch shorter than the specified length.
a.	 Construct a frequency distribution and a percentage distribu-
tion.
b.	 Construct a cumulative percentage distribution.
c.	 Is the steel mill doing a good job meeting the requirements set 
by the automobile company? Explain.
2.20  A manufacturing company produces steel housings for elec-
trical equipment. The main component part of the housing is a 
steel trough that is made out of a 14-gauge steel coil. It is produced 
using a 250-ton progressive punch press with a wipe-down opera-
tion that puts two 90-degree forms in the flat steel to make the 
trough. The distance from one side of the form to the other is criti-
cal because of weatherproofing in outdoor applications. The com-
pany requires that the width of the trough be between 8.31 inches 
and 8.61 inches. The widths of the troughs, in inches, collected 
from a sample of 49 troughs and stored in  Trough , are:
8.312  8.343  8.317  8.383  8.348  8.410  8.351  8.373
8.481  8.422  8.476  8.382  8.484  8.403  8.414  8.419
8.385  8.465  8.498  8.447  8.436  8.413  8.489  8.414
8.481  8.415  8.479  8.429  8.458  8.462  8.460  8.444
8.429  8.460  8.412  8.420  8.410  8.405  8.323  8.420
8.396  8.447  8.405  8.439  8.411  8.427  8.420  8.498
8.409
a.	 Construct a frequency distribution and a percentage distribution.
b.	 Construct a cumulative percentage distribution.
c.	 What can you conclude about the number of troughs that will 
meet the company’s requirements of troughs being between 
8.31 and 8.61 inches wide?
2.21  The manufacturing company in Problem 2.20 also produces 
electric insulators. If the insulators break when in use, a short  
circuit is likely to occur. To test the strength of the insulators,  
destructive testing in high-powered labs is carried out to determine 
how much force is required to break the insulators. Force is mea-
sured by observing how many pounds must be applied to the insu-
lator before it breaks. The force measurements, collected from a 
sample of 30 insulators and stored in  Force , are:
1,870  1,728  1,656  1,610  1,634  1,784  1,522  1,696
1,592  1,662  1,866  1,764  1,734  1,662  1,734  1,774
1,550  1,756  1,762  1,866  1,820  1,744  1,788  1,688
1,810  1,752  1,680  1,810  1,652  1,736
a.	 Construct a frequency distribution and a percentage distribution.
b.	 Construct a cumulative percentage distribution.
c.	 What can you conclude about the strength of the insulators if 
the company requires a force measurement of at least 1,500 
pounds before the insulator breaks?
2.22  The file  Bulbs  contains the life (in hours) of a sample of 
40 20-watt compact fluorescent light bulbs produced by Manufac-
turer A and a sample of 40 20-watt compact fluorescent light bulbs 
produced by Manufacturer B.
a.	 Construct a frequency distribution and a percentage distribu-
tion for each manufacturer, using the following class interval 
widths for each distribution:
Manufacturer A: 6,500 but less than 7,500, 7,500 but less than 
8,500, and so on.
Manufacturer B: 7,500 but less than 8,500, 8,500 but less than 
9,500, and so on.
b.	 Construct cumulative percentage distributions.
c.	 Which bulbs have a longer life—those from Manufacturer A or 
Manufacturer B? Explain.

	
2.3  Visualizing Categorical Variables	
79
2.23  The file  Drink  contains the following data for the amount 
of soft drink (in liters) in a sample of 50 2-liter bottles:
2.109  2.086  2.066  2.075  2.065  2.057  2.052  2.044  2.036  2.038
2.031  2.029  2.025  2.029  2.023  2.020  2.015  2.014  2.013  2.014
2.012  2.012  2.012  2.010  2.005  2.003  1.999  1.996  1.997  1.992
1.994  1.986  1.984  1.981  1.973  1.975  1.971  1.969  1.966  1.967
1.963  1.957  1.951  1.951  1.947  1.941  1.941  1.938  1.908  1.894
a.	 Construct a cumulative percentage distribution.
b.	 On the basis of the results of (a), does the amount of soft drink 
filled in the bottles concentrate around specific values?
2.3  Visualizing Categorical Variables
The chart you choose to visualize the data for a single categorical variable depends on whether 
you seek to emphasize how categories directly compare to each other (bar chart) or how cat-
egories form parts of a whole (pie chart), or whether you have data that are concentrated in 
only a few of your categories (Pareto chart). To visualize the data for two categorical variables, 
you use a side-by-side bar chart.
The Bar Chart
A bar chart visualizes a categorical variable as a series of bars, with each bar representing 
the tallies for a single category. In a bar chart, the length of each bar represents either the fre-
quency or percentage of values for a category and each bar is separated by space, called a gap.
The left illustration in Figure 2.1 displays the bar chart for the Table 2.2 summary table on 
page 66 that tallies responses to a recent survey that asked young adults the main reason they 
shop online. Reviewing Figure 2.1, you see that respondents are most likely to say because of 
better prices, followed by avoiding holiday crowds or hassles. Very few respondents mentioned 
ships directly.
F i g u r e  2 . 1
Excel bar chart (left) and pie chart (right) for reasons for shopping online

80	
Chapter 2  Organizing and Visualizing Variables
The Pie Chart
A pie chart uses parts of a circle to represent the tallies of each category. The size of each part, 
or pie slice, varies according to the percentage in each category. For example, in Table 2.2 on 
page 66, 37% of the respondents stated that they shop online mainly because of better prices. 
To represent this category as a pie slice, you multiply 37% by the 360 degrees that makes up a 
circle to get a pie slice that takes up 133.2 degrees of the 360 degrees of the circle, as shown in 
Figure 2.1 on page 79. From the Figure 2.1 pie chart, you can see that the second largest slice 
is avoiding holiday crowd and hassles, which contains 29% of the pie.
Example 2.5
Bar Chart of Levels 
of Risk of Retire-
ment Funds
As a member of the company task force in The Choice Is Yours scenario (see page 64), you 
want to first construct a bar chart of the risk of the funds that is based on Table 2.3 on page 66 
and then interpret the results.
Solution  Reviewing Figure 2.2, you see that low risk is the largest category, followed by 
average risk. Very few of the funds have high risk.
F i g u r e  2 . 2
Excel bar chart of 
the levels of risk of 
retirement funds
Example 2.6
Pie Chart of  
Levels of Risk of 
Retirement Funds
As a member of the company task force in The Choice Is Yours scenario (see page 64), you 
want to visualize the risk level of the funds by constructing a pie chart based on Table 2.3 (see 
page 66) for the risk variable and then interpret the results.
Solution  Reviewing Figure 2.3, you see that more than two-thirds of the funds are low 
risk, about 30% are average risk, and only about 4% are high risk.
F i g u r e  2 . 3
Excel pie chart of 
the risk of retirement 
funds

	
2.3  Visualizing Categorical Variables	
81
Today, some assert that pie charts should never be used. Others argue that they offer an 
easily comprehended way to visualize parts of a whole. All commentators agree that varia-
tions such as 3D perspective pies and “exploded” pie charts, in which one or more slices are 
pulled away from the center of a pie, should not be used because of the visual distortions they  
introduce.
The Pareto Chart
In a Pareto chart, the tallies for each category are plotted as vertical bars in descending order, 
according to their frequencies, and are combined with a cumulative percentage line on the 
same chart. Pareto charts get their name from the Pareto principle, the observation that in 
many data sets, a few categories of a categorical variable represent the majority of the data, 
while many other categories represent a relatively small, or trivial, amount of the data.
Pareto charts help you to visually identify the “vital few” categories from the “trivial 
many” categories so that you can focus on the important categories. Pareto charts are also pow-
erful tools for prioritizing improvement efforts, such as when data are collected that identify 
defective or nonconforming items.
A Pareto chart presents the bars vertically, along with a cumulative percentage line. The 
cumulative line is plotted at the midpoint of each category, at a height equal to the cumulative 
percentage. In order for a Pareto chart to include all categories, even those with few defects, in 
some situations, you need to include a category labeled Other or Miscellaneous. If you include 
such a category, you place the bar that represents that category at the far end (to the right) of 
the X axis.
Using Pareto charts can be an effective way to visualize data for many studies that seek 
causes for an observed phenomenon. For example, consider a bank study team that wants to 
enhance the user experience of automated teller machines (ATMs). During this study, the team 
identifies incomplete ATM transactions as a significant issue and decides to collect data about 
the causes of such transactions. Using the bank’s own processing systems as a primary data 
source, causes of incomplete transactions are collected, stored in  ATM Transactions , and then 
organized in the Table 2.16 summary table.
The informal “80/20” rule, 
which states that often 80% 
of results are from 20% of 
some thing, such as “80% 
of the work is done by 20% 
of the employees,” derives 
from the Pareto principle.
Cause
Frequency
Percentage
ATM malfunctions
32
4.42%
ATM out of cash
28
3.87%
Invalid amount requested
23
3.18%
Lack of funds in account
19
2.62%
Card unreadable
234
32.32%
Warped card jammed
365
50.41%
Wrong keystroke
23
3.18%
Total
724
100.00%
Source: Data extracted from A. Bhalla, “Don’t Misuse the Pareto Principle,” Six Sigma Forum 
Magazine, May 2009, pp. 15–18.
T a b l e  2 . 1 6
Summary Table of 
Causes of Incomplete 
ATM Transactions
To separate out the “vital few” causes from the “trivial many” causes, the bank study team 
creates the Table 2.17 summary table, in which the causes of incomplete transactions appear in 
descending order by frequency, as required for constructing a Pareto chart. The table includes 
the percentages and cumulative percentages for the reordered causes, which the team then uses 
to construct the Pareto chart shown in Figure 2.4. In Figure 2.4, the vertical axis on the left 
represents the percentage due to each cause and the vertical axis on the right represents the 
cumulative percentage.

82	
Chapter 2  Organizing and Visualizing Variables
Because the categories in a Pareto chart are ordered by decreasing frequency of occur-
rence, the team can quickly see which causes contribute the most to the problem of incom-
plete transactions. (Those causes would be the “vital few,” and figuring out ways to avoid such 
causes would be, presumably, a starting point for improving the user experience of ATMs.) 
By following the cumulative percentage line in Figure 2.4, you see that the first two causes, 
warped card jammed (50.44%) and card unreadable (32.3%), account for 82.7% of the incom-
plete transactions. Attempts to reduce incomplete ATM transactions due to warped or unread-
able cards should produce the greatest payoff.
T a b l e  2 . 1 7
Ordered Summary 
Table of Causes of 
Incomplete ATM 
Transactions
 
Cause
 
Frequency
 
Percentage
Cumulative 
Percentage
Warped card jammed
365
50.41%
50.41%
Card unreadable
234
32.32%
82.73%
ATM malfunctions
32
4.42%
87.15%
ATM out of cash
28
3.87%
91.02%
Invalid amount requested
23
3.18%
94.20%
Wrong keystroke
23
3.18%
97.38%
Lack of funds in account
   19  
    2.62%
100.00%
Total
724
100.00%
F i g u r e  2 . 4
Minitab Pareto chart 
of incomplete ATM 
transactions
Example 2.7
Pareto Chart of the 
Main Reason for 
Shopping Online
Construct a Pareto chart from Table 2.2 (see page 66), which summarizes the main reason 
young adults shop online.
Solution  First, create a new table from Table 2.2 in which the categories are ordered by 
descending frequency and columns for percentages and cumulative percentages for the ordered 
categories are included (not shown). From that table, create the Pareto chart in Figure 2.5.  

	
2.3  Visualizing Categorical Variables	
83
The Side-by-Side Bar Chart
A side-by-side bar chart uses sets of bars to show the joint responses from two categorical 
variables. For example, the Figure 2.6 side-by-side chart visualizes the data for the levels of 
risk for growth and value funds shown in Table 2.4 on page 67. In Figure 2.6, you see that a 
substantial portion of the growth funds and the value funds have low risk. However, a larger 
portion of the growth funds have average risk.
F i g u r e  2 . 5
Excel Pareto chart of 
the main reason for 
shopping online
F i g u r e  2 . 6
Side-by-side bar chart of 
fund type and risk level
From Figure 2.5, you see that better prices and avoiding holiday crowds and hassles  
accounted for 66% of the responses and better prices, avoiding holiday crowds and hassles, 
convenience, and better selection accounted for 97% of the responses.

84	
Chapter 2  Organizing and Visualizing Variables
Problems for Section 2.3
Applying the Concepts
2.24  An online survey commissioned by Vizu, a Nielsen com-
pany, of digital marketing and media professionals on current at-
titudes and practices regarding paid social media advertising was 
conducted by Digiday in fall 2012. Advertisers were asked to in-
dicate the primary purpose of their paid social media ads. The sur-
vey results were as follows:
Paid Social Media Advertising 
Objective
Percentage
Primarily branding related, e.g. raising  
  awareness, influencing brand opinions
 
45%
Primarily direct-response related, e.g.  
  driving product trials or site visits
 
16%
Mix—more than half is branding
25%
Mix—more than half is direct-response
14%
Source: Data extracted from www.nielsen.com/us/en/reports/2013/
the-paid-social-media-advertising-report-2013.html.
a.	 Construct a bar chart, a pie chart, and a Pareto chart.
b.	 Which graphical method do you think is best for portraying 
these data?
c.	 What conclusions can you reach concerning the purpose of 
paid media advertising?
2.25  What do college students do with their time? A survey of 
3,000 traditional-age students was taken, with the results as follows:
Activity
Percentage
Attending class/lab
  9%
Sleeping
24%
Socializing, recreation, other
51%
Studying
  7%
Working, volunteering, student clubs
  9%
Source: Data extracted from M. Marklein, “First Two Years of 
College Wasted?” USA Today, January 18, 2011, p. 3A.
a.	 Construct a bar chart, a pie chart, and a Pareto chart.
b.	 Which graphical method do you think is best for portraying 
these data?
c.	 What conclusions can you reach concerning what college stu-
dents do with their time?
2.26  The following data has been recorded of the consumer 
­complaints in a hotel:
Complaint type
Number of  
consumer complaints
Heating
30
Cleaning
100
Towels
50
Theft
2
Noise
10
Room Service
20
a.	 Construct a Pareto chart.
b.	 What were the top and bottom 50% complaints received for?
c.	 Based on results of a and b, what would you advise the hotel 
management to prioritize?
2.27  The Edmunds.com NHTSA Complaints Activity Report 
contains consumer vehicle complaint submissions by automaker, 
brand, and category (data extracted from edmu.in/Ybmpuz.) 
The following tables, stored in  Automaker1  and  Automaker2 , 
represent complaints received by automaker and complaints  
received by category for January 2013.
Automaker
Number
American Honda
169
Chrysler LLC
439
Ford Motor Company
440
General Motors
551
Nissan Motors Corporation
467
Toyota Motor Sales
332
Other
516
a.	 Construct a bar chart and a pie chart for the complaints re-
ceived by automaker.
b.	 Which graphical method do you think is best for portraying 
these data?
Category
Number
Airbags and seatbelts
201
Body and glass
182
Brakes
63
Fuel/emission/exhaust system
240
Interior electronics/hardware
279
Powertrain
  1,148
Steering
397
Tires and wheels
71
c.	 Construct a Pareto chart for the categories of complaints.
d.	 Discuss the “vital few” and “trivial many” reasons for the  
categories of complaints.
2.28  The following table indicates the percentage of residential 
electricity consumption in the United States, in a recent year orga-
nized by type of use
Type of Appliance
Percentage
Cooking
  4%
Cooling
  9%
Electronics
  6%
Heating
45%
Lighting
  6%
Refrigeration
  4%
Water heating
18%
Wet cleaning
  3%
Other
  5%
Source: Department of Energy

	
2.4  Visualizing Numerical Variables	
85
a.	 Construct a bar chart, a pie chart, and a Pareto chart.
b.	 Which graphical method do you think is best for portraying 
these data?
c.	 What conclusions can you reach concerning residential elec-
tricity consumption in the United States?
2.29  Visier’s Survey of Employers explores how North American 
organizations are solving the challenges of delivering workforce 
analytics. Employers were asked what would help them be suc-
cessful with human resources metrics and reports. The responses 
were as follows (stored in  Needs ):
Needs
Frequency
Easier-to-use analytic tools
127
Faster access to data
  41
Improved ability to present and interpret data
123
Improved ability to plan actions
  33
Improved ability to predict impacts of my  
  actions
  49
Improved relationships to the business line  
  organizations
  37
Source: Data extracted from bit.ly/YuWYXc.
a.	 Construct a bar chart and a pie chart.
b.	 What conclusions can you reach concerning needs for em-
ployer success with human resource metrics and reports?
2.30  A study has been conducted on the prevalence of depres-
sion with respect to demographic features such as age, race, and 
­gender. The survey was administered to 155 patients and it was 
found that women are more likely to be depressed compared to 
men, as shown in the table below. 
Male
Female
Total
Depressed
  54
21
  75
Not Depressed
  68
12
  80
Total
122
33
155
Source: Data extracted from Gottlieb SS, Khatta M, Friedmann E, 
et al., “The Influence of Age, Gender, and Race on the Prevalence 
of Depression in Heart Failure Patients.”
a.	 Which type of graph will be most suitable for the given data?
b.	 Draw the graph so meaningful conclusions can be drawn from it.
2.31  A research report states that an adolescent's  overall health 
impacts their physical and psychological well being. The report 
looks at the direct relationship between nutritional intake and 
academic achievement. Milk intake was looked at specifically, as 
it is rich in calcium and contributes to bone growth. However, it 
was found that teens consume twice as much soft drink as milk. 
­Assume the following data is collected:
Drink milk
Do not drink milk
Good PE grades
  67
   33
Not good PE grades
 22 
   48
a.	 Construct contingency tables based on total percentages, row 
percentages, and column percentages.
b.	 Draw a side-by-side bar chart.
c.	 What conclusion can you draw related to drinking milk and PE 
grades? Which method of analysis is better, contingency tables 
or side-by-side graphs?
2.32  A research was conducted to find if dogs resemble their 
owners. The finding of the research was that people tend to se-
lect dogs that in some way resemble them and the resemblance 
increases with the duration of ownership. Assume that this finding 
is specific to a particular breed of dogs and that the following data 
has been collected:
Resemble Owner
Do Not  
Resemble Owner
Specific Breed
20
11
Other Dogs
12
17
a.	 Draw a side-by-side chart to project whether only dogs of 
a specific breed resemble their owners, or dogs of all breeds  
do so.
b.	 What conclusions can you draw from the chart?
2.4  Visualizing Numerical Variables
You visualize the data for a numerical variable through a variety of techniques that show the 
distribution of values. These techniques include the stem-and-leaf display, the histogram, the per-
centage polygon, and the cumulative percentage polygon (ogive), all discussed in this section, as 
well as the boxplot, which requires descriptive summary measures, as explained in Section 3.3.
The Stem-and-Leaf Display
A stem-and-leaf display visualizes data by presenting the data as one or more row-wise stems 
that represent a range of values. In turn, each stem has one or more leaves that branch out to 
the right of their stem and represent the values found in that stem. For stems with more than 
one leaf, the leaves are arranged in ascending order.

86	
Chapter 2  Organizing and Visualizing Variables
Student Tip
If you turn a stem-and-
leaf display sideways, 
the display looks like a 
histogram.
Example 2.8
Stem-and-Leaf 
Display of the 
One-Year Return 
Percentage for the 
Value Funds
As a member of the company task force in The Choice Is Yours scenario (see page 64), you 
want to study the past performance of the value funds. One measure of past performance is the 
numerical variable 1YrReturn%, the one-year return percentage. Using the data from the 89 
value funds, you want to visualize this variable as a stem-and-leaf display.
Solution  Figure 2.7 illustrates the stem-and-leaf display of the one-year return percentage 
for value funds.
F i g u r e  2 . 7
Minitab stem-and-leaf 
display of the one-year 
return percentage for 
value funds
Using Excel with PHStat 
will create an equivalent 
display that contains a 
­different set of stems.
Figure 2.7 allows you to conclude:
 • The lowest one-year return was approximately 1.
 • The highest one-year return was 28.
 • The one-year returns were concentrated between 12 and 19.
 • Very few of the one-year returns were above 21.
Stem-and-leaf displays allow you to see how the data are distributed and where concentra-
tions of data exist. Leaves typically present the last significant digit of each value, but some-
times you round values. For example, suppose you collect the following meal costs (in $) for 
15 classmates who had lunch at a fast-food restaurant (stored in  FastFood ):
7.42 6.29 5.83 6.50 8.34 9.51 7.10 6.80 5.90 4.89 6.50 5.52 7.90 8.30 9.60
To construct the stem-and-leaf display, you use whole dollar amounts as the stems and 
round the cents to one decimal place to use as the leaves. For the first value, 7.42, the stem 
would be 7 and its leaf would be 4. For the second value, 6.29, the stem would be 6 and its leaf 3. 
The completed stem-and-leaf display for these data is
4
9
5
589
6
3558
7
149
8
33
9
56

	
2.4  Visualizing Numerical Variables	
87
The Histogram
A histogram visualizes data as a vertical bar chart in which each bar represents a class interval 
from a frequency or percentage distribution. In a histogram, you display the numerical variable 
along the horizontal (X) axis and use the vertical (Y) axis to represent either the frequency or 
the percentage of values per class interval. There are never any gaps between adjacent bars in 
a histogram.
Figure 2.8 visualizes the data of Table 2.9 on page 71, meal costs at city and suburban 
restaurants, as a pair of frequency histograms. The histogram for city restaurants shows that 
the cost of meals is concentrated between approximately $30 and $60. Only one meal at city 
restaurants cost more than $80. The histogram for suburban restaurants shows that the cost 
of meals is also concentrated between $30 and $60. However, many more meals at suburban 
restaurants cost between $30 and $40 than at city restaurants. Very few meals at suburban  
restaurants cost more than $70.
F i g u r e  2 . 8
Minitab frequency histograms for meal costs at city and suburban restaurants
Example 2.9
Histograms of the 
One-Year Return 
Percentages for the 
Growth and Value 
Funds
As a member of the company task force in The Choice Is Yours scenario (see page 64), you 
seek to compare the past performance of the growth funds and the value funds, using the one-
year return percentage variable. Using the data from the sample of 316 funds, you construct 
histograms for the growth and the value funds to create a visual comparison.
Solution  Figure 2.9 displays frequency histograms for the one-year return percentages 
for the growth and value funds.
Reviewing the histograms in Figure 2.9 leads you to conclude that the returns were lower 
for the growth funds than for value funds. The return for both the growth funds and the value 
funds is concentrated between 10 and 20, but the return for the value funds is more concen-
trated between 15 and 20 while the return for the growth funds is more concentrated between 
10 and 15.
(continued)

88	
Chapter 2  Organizing and Visualizing Variables
The Percentage Polygon
When using a categorical variable to divide the data of a numerical variable into two or more 
groups, you visualize data by constructing a percentage polygon. This chart uses the mid-
points of each class interval to represent the data of each class and then plots the midpoints, at 
their respective class percentages, as points on a line along the X axis. While you can construct 
two or more histograms, as was done in Figures 2.8 and 2.9, a percentage polygon allows you 
to make a direct comparison that is easier to interpret. (You cannot, of course, combine two 
histograms into one chart as bars from the two groups would overlap and obscure data.)
Figure 2.10 displays percentage polygons for the cost of meals at city and suburban res-
taurants. Compare this figure to the pair of histograms in Figure 2.8 on page 87. Reviewing 
the polygons in Figure 2.10 allows you to make the same observations as were made when 
examining Figure 2.8, including the fact that while city restaurant meal costs are both concen-
trated between $30 and $60, suburban restaurants have a much higher concentration between 
$30 and $40. However, unlike the pair of histograms, the polygons allow you to more easily 
identify which class intervals have similar percentages for the two groups and which do not.
The polygons in Figure 2.10 have points whose values on the X axis represent the mid-
point of the class interval. For example, look at the points plotted at X = 35 (+35). The point 
for meal costs at city restaurants (the lower one) show that 20% of the meals cost between $30 
and $40, while the point for the meal costs at suburban restaurants (the higher one) shows that 
34% of meals at these restaurants cost between $30 and $40.
F i g u r e  2 . 9
Excel frequency histograms for the one-year return percentages for the growth and value funds
F i g u r e  2 . 1 0
Minitab percentage 
polygons of meal costs 
for city and suburban 
restaurants

	
2.4  Visualizing Numerical Variables	
89
When you construct polygons or histograms, the vertical (Y) axis should include zero to 
avoid distorting the character of the data. The horizontal (X) axis does not need to show the 
zero point for the numerical variable, but a major portion of the axis should be devoted to the 
entire range of values for the variable.
The Cumulative Percentage Polygon (Ogive)
The cumulative percentage polygon, or ogive, uses the cumulative percentage distribution 
discussed in Section 2.2 to plot the cumulative percentages along the Y axis. Unlike the per-
centage polygon, the lower boundary of the class interval for the numerical variable are plot-
ted, at their respective class percentages, as points on a line along the X axis.
Figure 2.12 shows cumulative percentage polygons of meal costs for city and suburban 
restaurants. In this chart, the lower boundaries of the class intervals (20, 30, 40, etc.) are ap-
proximated by the upper boundaries of the previous bins (19.99, 29.99, 39.99, etc.). Reviewing 
the curves leads you to conclude that the curve of the cost of meals at the city restaurants is 
located to the right of the curve for the suburban restaurants. This indicates that the city restau-
rants have fewer meals that cost less than a particular value. For example, 52% of the meals at 
city restaurants cost less than $50, as compared to 68% of the meals at suburban restaurants.
Example 2.10
Percentage 
Polygons of the 
One-Year Return 
Percentage for the 
Growth and Value 
Funds
As a member of the company task force in The Choice Is Yours scenario (see page 64), you 
seek to compare the past performance of the growth funds and the value funds using the one-
year return percentage variable. Using the data from the sample of 316 funds, you construct 
percentage polygons for the growth and value funds to create a visual comparison.
Solution  Figure 2.11 displays percentage polygons of the one-year return percentage for 
the growth and value funds.
F i g u r e  2 . 1 1
Excel percentage 
polygons of the one-year 
return percentages for the 
growth and value funds
Figure 2.11 shows that the value funds polygon is to the right of the growth funds polygon.  
This allows you to conclude that the one-year return percentage is higher for value funds 
than for growth funds. The polygons also show that the return for value funds is concentrated  
between 15 and 20, and the return for the growth funds is concentrated between 10 and 15.

90	
Chapter 2  Organizing and Visualizing Variables
The cumulative percentage polygons in Figure 2.13 show that the curve for the one-year 
return percentage for the growth funds is located slightly to the left of the curve for the value 
funds. This allows you to conclude that the growth funds have fewer one-year return percent-
ages that are higher than a particular value. For example, 59.03% of the growth funds had 
one-year return percentages below 15, as compared to 48.31% of the value funds. You can 
conclude that, in general, the value funds slightly outperformed the growth funds in their one-
year returns.
F i g u r e  2 . 1 2
Minitab cumulative 
percentage polygons of 
meal costs for city and 
suburban restaurants
Example 2.11
Cumulative Per-
centage Polygons 
of the One-Year 
Return Percentages 
for the Growth and 
Value Funds
As a member of the company task force in The Choice Is Yours scenario (see page 64), you 
seek to compare the past performance of the growth funds and the value funds using the one-
year return percentage variable. Using the data from the sample of 316 funds, you construct 
cumulative percentage polygons for the growth and the value funds.
Solution  Figure 2.13 displays cumulative percentage polygons of the one-year return per-
centages for the growth and value funds.
In Microsoft Excel, you 
approximate the lower 
boundary by using the upper 
boundary of the previous bin.
F i g u r e  2 . 1 3
Excel cumulative 
percentage polygons 
of the one-year return 
percentages for the 
growth and value funds

	
2.4  Visualizing Numerical Variables	
91
Problems for Section 2.4
Learning the Basics
2.33  Construct a stem-and-leaf display, given the following data 
from a sample of midterm exam scores in finance:
54 69 98 93 53 74
2.34  Construct an ordered array, given the following stem-and-
leaf display from a sample of n = 7 midterm exam scores in in-
formation systems:
5
0
6
7
446
8
19
9
2
Applying the Concepts
2.35  The following is a stem-and-leaf display representing the 
amount of gasoline purchased, in gallons (with leaves in tenths of 
gallons), for a sample of 25 cars that use a particular service sta-
tion on the New Jersey Turnpike:
9
147
10
02238
11
125566777
12
223489
13
02
a.	 Construct an ordered array.
b.	 Which of these two displays seems to provide more informa-
tion? Discuss.
c.	 What amount of gasoline (in gallons) is most likely to be pur-
chased?
d.	 Is there a concentration of the purchase amounts in the center 
of the distribution?
SELF 
Test 
2.36  The file  BBCost2012  contains the total cost (in $) 
for four tickets, two beers, four soft drinks, four hot dogs, 
two game programs, two baseball caps, and parking for one vehicle at 
each of the 30 Major League Baseball parks during the 2012 season.
Source: Data extracted from fancostexperience.com/pages/fcx 
/fci_pdfs//8.pdfs.
a.	 Construct a stem-and-leaf display.
b.	 Around what value, if any, are the costs of attending a baseball 
game concentrated? Explain.
2.37  The file  Caffeine  contains the caffeine content (in milli-
grams per ounce) for a sample of 26 energy drinks:
  3.2       1.5     4.6     8.9     7.1      9.0     9.4    31.2   10.0   10.1    
  9.9     11.5   11.8   11.7   13.8     14.0   16.1   74.5   10.8   26.3   
17.7   113.3   32.5   14.0   91.6   127.4
Source: Data extracted from “The Buzz on Energy-Drink Caffeine,” 
Consumer Reports, December 2012.
a.	 Construct an ordered array.
b.	 Construct a stem-and-leaf display.
c.	 Does the ordered array or the stem-and-leaf display provide 
more information? Discuss.
d.	 Around what value, if any, is the amount of caffeine in energy 
drinks concentrated? Explain.
2.38  The file  Utility  contains the following data about the cost 
of electricity during July 2013 for a random sample of 50 one-
bedroom apartments in a large city:
96 171 202 178 147 102 153 197 127 82
157 185  90 116 172 111 148 213 130 165
141 149 206 175 123 128 144 168 109 167
95 163 150 154 130 143 187 166 139 149
108 119 183 151 114 135 191 137 129 158
a.	 Construct a histogram and a percentage polygon.
b.	 Construct a cumulative percentage polygon.
c.	 Around what amount does the monthly electricity cost seem to 
be concentrated?
2.39  As player salaries have increased, the cost of attending 
baseball games has increased dramatically. The following histo-
gram and cumulative percentage polygon visualizes the total cost 
(in $) for four tickets, two beers, four soft drinks, four hot dogs, 
two game programs, two baseball caps, and parking for one ve-
hicle at each of the 30 Major League Baseball parks during the 
2012 season that is stored in  BBCost2012 .
What conclusions can you reach concerning the cost of attending a 
baseball game at different ballparks?

92	
Chapter 2  Organizing and Visualizing Variables
2.40  The following histogram and cumulative percentage poly-
gon visualize the data about the property taxes per capita($) for the 
50 states and the District of Columbia, stored in  Property Taxes .
What conclusions can you reach concerning the property taxes per 
capita?
2.41  How much time do Americans living in or near cities spend 
waiting in traffic, and how much does waiting in traffic cost them 
per year? The data in the file  Congestion  include this cost for 31 
cities. (Source: Data extracted from “The High Cost of Conges-
tion,” Time, October 17, 2011, p. 18.) For the time Americans liv-
ing in or near cities spend waiting in traffic and the cost of waiting 
in traffic per year,
a.	 Construct a percentage histogram.
b.	 Construct a cumulative percentage polygon.
c.	 What conclusions can you reach concerning the time Ameri-
cans living in or near cities spend waiting in traffic?
d.	 What conclusions can you reach concerning the cost of waiting 
in traffic per year?
2.42  How do the average credit scores of people living in various 
cities differ? The file  Credit Scores  contains an ordered array of 
the average credit scores of 143 American cities. (Data extracted 
from usat.ly/17a1fA6.)
a.	 Construct a percentage histogram.
b.	 Construct a cumulative percentage polygon.
c.	 What conclusions can you reach concerning the average credit 
scores of people living in different American cities?
2.43  One operation of a mill is to cut pieces of steel into parts 
that will later be used as the frame for front seats in an automo-
bile. The steel is cut with a diamond saw and requires the result-
ing parts to be within { 0.005 inch of the length specified by the 
automobile company. The data are collected from a sample of 100 
steel parts and stored in  Steel . The measurement reported is the 
difference in inches between the actual length of the steel part, as 
measured by a laser measurement device, and the specified length 
of the steel part. For example, the first value,-0.002 represents a 
steel part that is 0.002 inch shorter than the specified length.
a.	 Construct a percentage histogram.
b.	 Is the steel mill doing a good job meeting the requirements set 
by the automobile company? Explain.
2.44  A manufacturing company produces steel housings for 
electrical equipment. The main component part of the housing is 
a steel trough that is made out of a 14-gauge steel coil. It is pro-
duced using a 250-ton progressive punch press with a wipe-down 
operation that puts two 90-degree forms in the flat steel to make 
the trough. The distance from one side of the form to the other 
is critical because of weatherproofing in outdoor applications. 
The company requires that the width of the trough be between  
8.31 inches and 8.61 inches. The widths of the troughs, in inches, 
collected from a sample of 49 troughs, are stored in  Trough .
a.	 Construct a percentage histogram and a percentage polygon.
b.	 Plot a cumulative percentage polygon.
c.	 What can you conclude about the number of troughs that will 
meet the company’s requirements of troughs being between 
8.31 and 8.61 inches wide?
2.45  The manufacturing company in Problem 2.44 also produces 
electric insulators. If the insulators break when in use, a short cir-
cuit is likely to occur. To test the strength of the insulators, de-
structive testing in high-powered labs is carried out to determine 
how much force is required to break the insulators. Force is mea-
sured by observing how many pounds must be applied to the insu-
lator before it breaks. The force measurements, collected from a 
sample of 30 insulators, are stored in  Force .
a.	 Construct a percentage histogram and a percentage polygon.
b.	 Construct a cumulative percentage polygon.
c.	 What can you conclude about the strengths of the insulators 
if the company requires a force measurement of at least 1,500 
pounds before the insulator breaks?
2.46  The file  Bulbs  contains the life (in hours) of a sample of 
40 20-watt compact fluorescent light bulbs produced by Manufac-
turer A and a sample of 40 20-watt compact fluorescent light bulbs 
produced by Manufacturer B.
Use the following class interval widths for each distribution:
Manufacturer A: 6,500 but less than 7,500, 7,500 but less 
than 8,500, and so on.
Manufacturer B: 7,500 but less than 8,500, 8,500 but less 
than 9,500, and so on.
a.	 Construct percentage histograms on separate graphs and plot 
the percentage polygons on one graph.
b.	 Plot cumulative percentage polygons on one graph.
c.	 Which manufacturer has bulbs with a longer life—Manufacturer 
A or Manufacturer B? Explain.
2.47  The data stored in  Drink  represents the amount of soft 
drink in a sample of 50 2-liter bottles.
a.	 Construct a histogram and a percentage polygon.
b.	 Construct a cumulative percentage polygon.
c.	 On the basis of the results in (a) and (b), does the amount of soft 
drink filled in the bottles concentrate around specific values?

	
2.5  Visualizing Two Numerical Variables	
93
2.5  Visualizing Two Numerical Variables
Visualizing two numerical variables together can reveal possible relationships between two 
variables and serve as a basis for applying the methods discussed in Chapters 13 through 17. 
To visualize two numerical variables, you construct a scatter plot. For the special case in which 
one of the two variables represents the passage of time, you construct a time-series plot.
The Scatter Plot
A scatter plot explores the possible relationship between two numerical variables by plotting 
the values of one numerical variable on the horizontal, or X, axis and the values of a second 
numerical variable on the vertical, or Y, axis. For example, a marketing analyst could study 
the effectiveness of advertising by comparing advertising expenses and sales revenues of  
50 stores by using the X axis to represent advertising expenses and the Y axis to represent 
sales revenues.
Example 2.12
Scatter Plot for 
NBA Investment 
Analysis
Suppose that you are an investment analyst who has been asked to review the valuations of 
the 30 NBA professional basketball teams. You seek to know if the value of a team reflects 
its revenues. You collect revenue and valuation data (both in $millions) for all 30 NBA teams, 
organize the data as Table 2.18, and store the data in  NBAValues .
To quickly visualize a possible relationship between team revenues and valuations, you 
construct a scatter plot as shown in Figure 2.14, in which you plot the revenues on the X axis 
and the value of the team on the Y axis.
Team  
Code
Revenue 
($millions)
Value 
($millions)
Team  
Code
Revenue 
($millions)
Value 
($millions)
Team  
Code
Revenue 
($millions)
Value 
($millions)
ATL
  99
316
HOU
135
   568
OKC
127
475
BOS
143
730
IND
  98
   383
ORL
126
470
BRK
  84
530
LAC
108
   430
PHI
107
418
CHA
  93
315
LAL
197
1,000
PHX
121
474
CHI
162
800
MEM
  96
   377
POR
117
457
CLE
128
434
MIA
150
   625
SAC
  96
525
DAL
137
685
MIL
  87
   312
SAS
135
527
DEN
110
427
MIN
  96
   364
TOR
121
405
DET
125
400
NOH
100
   340
UTA
111
432
GSW
127
555
NYK
243
1,100
WAS
102
397
Source: Data extracted from www.forbes.com/nba-valuations.
Solution  From Figure 2.14, you see that there appears to be a strong increasing (positive) 
relationship between revenues and the value of a team. In other words, teams that generate 
a smaller amount of revenues have a lower value, while teams that generate higher revenues 
have a higher value. This relationship has been highlighted by the addition of a linear regres-
sion prediction line that will be discussed in Chapter 13.
T a b l e  2 . 1 8
Revenues and Values for NBA Teams
(continued)

94	
Chapter 2  Organizing and Visualizing Variables
F i g u r e  2 . 1 4
Scatter plot of revenue 
and value for NBA teams
Other pairs of variables may have a decreasing (negative) relationship in which one vari-
able decreases as the other increases. In other situations, there may be a weak or no relation-
ship between the variables.
The Time-Series Plot
A time-series plot plots the values of a numerical variable on the Y axis and plots the time 
period associated with each numerical value on the X axis. A time-series plot can help you 
visualize trends in data that occur over time.
Learn More
Read the Short Takes for 
Chapter 2 for an example 
that illustrates a negative 
relationship.
Example 2.13
Time-Series Plot for 
Movie Revenues
As an investment analyst who specializes in the entertainment industry, you are interested  
in discovering any long-term trends in movie revenues. You collect the annual revenues  
(in $billions) for movies released from 1995 to 2012, and organize the data as Table 2.19, and 
store the data in  Movie Revenues .
To see if there is a trend over time, you construct the time-series plot shown in Figure 2.15.
Year
Revenue ($billions)
Year
Revenue ($billions)
1995
5.29
2004
  9.27
1996
5.59
2005
  8.95
1997
6.51
2006
  9.25
1998
6.77
2007
  9.63
1999
7.30
2008
  9.95
2000
7.48
2009
10.65
2001
8.13
2010
10.50
2002
9.19
2011
10.28
2003
9.35
2012
10.71
Source: Data extracted from www.the-numbers.com/market, March 18, 2013.
Solution  From Figure 2.15, you see that there was a steady increase in the revenue of 
movies between 1995 and 2003, a leveling off from 2003 and 2006, followed by a further 
increase from 2007 to 2009, followed by another leveling off from 2010 to 2012. During that 
time, the revenue increased from under $6 billion in 1995 to more than $10 billion in 2009  
to 2012.
T a b l e  2 . 1 9
Movie Revenues (in 
$billions) from 1995  
to 2012

	
2.5  Visualizing Two Numerical Variables	
95
F i g u r e  2 . 1 5
Time-series plot of movie 
revenue per year from 
1995 to 2012
Problems for Section 2.5
Learning the Basics
2.48  The following is a set of data from a sample of n = 11 items:
X: 7 5 8 3 6 0 2 4 9 5 8
Y: 1 5 4 9 8 0 6 2 7 5 4
a.	 Construct a scatter plot.
b.	 Is there a relationship between X and Y? Explain.
2.49  The following is a series of annual sales (in $millions) over 
an 11-year period (2002 to 2012):
 Year:  2002  2003  2004  2005  2006  2007  2008  2009  2010  2011  2012
Sales:  13.0  17.0   19.0   20.0   20.5   20.5   20.5   20.0   19.0  17.0   13.0
a.	 Construct a time-series plot.
b.	 Does there appear to be any change in annual sales over time? 
Explain.
Applying the Concepts
2.50  Movie companies need to predict the gross re-
ceipts of individual movies once a movie has debuted. 
The following results, stored in  PotterMovies , are the first week-
end gross, the U.S. gross, and the worldwide gross (in $millions) 
of the eight Harry Potter movies:
Title
First 
Weekend 
($millions)
U.S. Gross 
($millions)
Worldwide 
Gross 
($millions)
Sorcerer’s Stone
  90.295
317.558
   976.458
Chamber of Secrets
  88.357
261.988
   878.988
Prisoner of Azkaban
  93.687
249.539
   795.539
Goblet of Fire
102.335
290.013
   896.013
Order of the Phoenix
  77.108
292.005
   938.469
Half-Blood Prince
  77.836
301.460
  934.601
Deathly Hallows Part I
125.017
295.001
  955.417
Deathly Hallows Part II
169.189
381.011
1,328.111
Source: Data extracted from www.the-numbers.com/interactive/
comp-HarryPotter.php.
a.	 Construct a scatter plot with first weekend gross on the X axis 
and U.S. gross on the Y axis.
b.	 Construct a scatter plot with first weekend gross on the X axis 
and worldwide gross on the Y axis.
c.	 What can you say about the relationship between first weekend 
gross and U.S. gross and first weekend gross and worldwide gross?
2.51  Data were collected on the typical cost of dining at  
American-cuisine restaurants within a 1-mile walking distance of 
a hotel located in a large city. The file  Bundle  contains the typi-
cal cost (a per transaction cost in $) as well as a Bundle score, a 
measure of overall popularity and customer loyalty, for each of 40 
selected restaurants. (Data extracted from www.bundle.com via 
the link on-msn.com/MnlBxo.)
a.	 Construct a scatter plot with Bundle score on the X axis and 
typical cost on the Y axis.
b.	 What conclusions can you reach about the relationship between 
Bundle score and typical cost?
2.52  College football is big business, with coaches’ pay and rev-
enues in millions of dollars. The file  College Football  contains 
the coaches’ total pay and net revenue for college football at 105 
schools (Data extracted from “College Football Coaches Continue 
to See Salary Explosion,” USA Today, November 20, 2012, p. 1C.)
a.	 Do you think schools with higher net revenues also have higher 
coaches’ pay?
b.	 Construct a scatter plot with net revenue on the X axis and 
coaches’ pay on the Y axis.
c.	 Does the scatter plot confirm or contradict your answer to (a)?
2.53  A Pew Research Center survey found that social network-
ing is popular in many nations around the world. The file  Global 
SocialMedia  contains the level of social media networking (mea-
sured as the percentage of individuals polled who use social net-
working sites) and the GDP at purchasing power parity (PPP) per 
capita for each of 25 selected countries. (Data extracted from Pew 
Research Center, “Global Digital Communication: Texting, Social 
Networking Popular Worldwide,” updated February 29, 2012, via 
the link bit.ly/sNjsmq.)
SELF 
Test 

96	
Chapter 2  Organizing and Visualizing Variables
a.	 Construct a scatterplot with GDP (PPP) per capita on the X axis 
and social media usage on the Y axis.
b.	 What conclusions can your reach about the relationship be-
tween GDP and social media usage?
2.54  How have stocks performed in the past? The following table 
presents the data stored in  Stock Performance  and shows the per-
formance of a broad measure of stocks (by percentage) for each 
decade from the 1830s through the 2000s:
  Decade
Performance (%)
1830s
2.8
1840s
12.8
1850s
6.6
1860s
12.5
1870s
7.5
1880s
6.0
1890s
5.5
1900s
10.9
1910s
2.2
1920s
13.3
1930s
-2.2
1940s
9.6
1950s
18.2
1960s
8.3
1970s
6.6
1980s
16.6
1990s
17.6
2000s*
-0.5
*Through December 15, 2009.
Source: Data extracted from T. Lauricella, 
“Investors Hope the '10s Beat the '00s,” The Wall 
Street Journal, December 21, 2009, pp. C1, C2.
a.	 Construct a time-series plot of the stock performance from the 
1830s to the 2000s.
b.	 Does there appear to be any pattern in the data?
2.55  The data in  NewHomeSales  represent number and median 
sales price of new single-family houses sold in the United States 
recorded at the end of each month from January 2000 through 
­December 2012. (Data extracted from www.census.gov, February 
24, 2013.)
a.	 Construct a times series plot of new home sales prices.
b.	 What pattern, if any, is present in the data?
2.56  The file  Movie Attendance  contains the yearly movie 
­attendance (in billions) from 2001 through 2012:
  Year
Attendance (billions)
2001
1.44
2002
1.58
2003
1.55
2004
1.49
2005
1.40
2006
1.41
2007
1.40
2008
1.39
2009
1.42
2010
1.33
2011
1.30
2012
1.37
Source: Data extracted from the-numbers.com/market.
a.	 Construct a time-series plot for the movie attendance (in 
­billions).
b.	 What pattern, if any, is present in the data?
2.57  The file  Audits  contains the number of audits of corpo-
rations with assets of more than $250 million conducted by the 
Internal Revenue Service between 2001 and 2012. (Data extracted 
from www.irs.gov.)
a.	 Construct a time-series plot.
b.	 What pattern, if any, is present in the data?
2.6  Organizing Many Categorical Variables
You construct a multidimensional contingency table to tally the responses of three or more 
categorical variables. In the simplest case of three categorical variables, each cell in the table 
contains the tallies of the third variable, organized by the subgroups represented by the row 
and column variables.
Both Excel and Minitab can organize many variables at the same time, but the two pro-
grams have different strengths. Using Excel, you can create a PivotTable, an interactive table 
that facilitates exploring multidimensional data. A PivotTable summarizes the variables as a mul-
tidimensional table and allows you to interactively change the level of summarization and the 
arrangement and formatting of the variables. PivotTables also allow you to interactively “slice” 
your data to summarize subsets of data that meet specified criteria as discussed in Section 17.1.
Methods designed spe-
cifically to visualize many 
categorical variables are 
beyond the scope of this 
book to discuss.

	
2.6  Organizing Many Categorical Variables	
97
Using Minitab, you can also create multidimensional tables, but unlike Excel PivotTables, 
the Minitab tables are not interactive. However, Minitab, unlike Excel, contains a number of 
specialized statistical and graphing procedures (beyond the scope of this book to discuss) that 
can be used to analyze and visualize multidimensional data.
Consider the Table 2.5 contingency table on page 67 that jointly tallies the type and 
risk variables for the sample of 316 retirement funds as percentages of the overall total. For  
convenience, this table is shown as a two-dimensional PivotTable in the left illustration of  
Figure 2.16. This table shows, among other things, that there are many more growth funds of 
low risk than of average or high risk.
F i g u r e  2 . 1 6
PivotTables for the 
retirement funds sample 
showing percentage of 
overall total for fund type 
and showing percentage 
of overall total risk (left) 
and for fund type, market 
cap, and risk (right)
Adding a third categorical variable, the market cap of the fund, creates the multidimen-
sional contingency table shown at right in Figure 2.16. This new PivotTable reveals the follow-
ing patterns that cannot be seen in the original Table 2.5 contingency table:
 • For the growth funds, the pattern of risk differs depending on the market cap of the 
fund. Large cap funds are most likely to have low risk and are very unlikely to have high 
risk. Mid-cap funds are equally likely to have low or average risk. Small cap funds are 
most likely to have average risk and are less likely to have high risk.
 • The value funds show a pattern of risk that is different from the pattern seen in the 
growth funds. Mid-cap funds are more likely to have low risk. Almost all of large value 
funds are low risk, and the small value funds are equally likely to have low or average risk.
Based on these results, the market cap of the mutual fund (small cap, mid-cap, large cap) is 
an example of a lurking variable, a variable that is affecting the results of the other variables. 
The relationship between the type of fund (growth or value) and the level of risk is clearly  
affected by the market cap of the mutual fund (small cap, mid-cap, or large cap).
Problems for Section 2.6
Applying the Concepts
2.58  Using the sample of retirement funds stored in  
 RetirementFunds :
a.	 Construct a table that tallies type, market cap, and rating.
b.	 What conclusions can you reach concerning differences among 
the types of retirement funds (growth and value), based on mar-
ket cap (small, mid-cap, and large) and the rating (one, two, 
three, four, and five)?
2.59  Using the sample of retirement funds stored in  
 RetirementFunds :
a.	 Construct a table that tallies market cap, risk, and rating.
b.	 What conclusions can you reach concerning differences among 
the types of funds based on market cap (small, mid-cap, and 
large), risk (low, average, and high), and the rating (one, two, 
three, four, and five)?
2.60  Using the sample of retirement funds stored in  
 RetirementFunds :
a.	 Construct a table that tallies type, risk, and rating.
b.	 What conclusions can you reach concerning differences among 
the types of retirement funds (growth and value), based on the 
risk (low, average, and high), and the rating (one, two, three, 
four, and five)?
2.61  Using the sample of retirement funds stored in  
 RetirementFunds :
a.	 Construct a table that tallies type, market cap, risk, and rating.
b.	 What conclusions can you reach concerning differences among 
the types of funds based on market cap (small, mid-cap, and 
large), based on type (growth and value), risk (low, average, 
and high), and rating (one, two, three, four, and five)?

98	
Chapter 2  Organizing and Visualizing Variables
Visualizations can also be subject to information overload. Figure 2.18 presents a side-
by-side bar chart that is based on the obscured data of Figure 2.17 and is typical of charts 
that sometimes get constructed when using large or complex sets of data, including the “big 
data” discussed in Chapter 17. As a bar chart, this visualization can highlight certain charac-
teristics of the sample data, consistent with the discussion earlier in the chapter. (For example, 
when you examine Figure 2.18, you can notice more quickly than you would when examining  
Figure 2.17 that there are more large-cap retirement funds with low risk and a three-star rating 
than any other combination of risk and star rating.) However, other details are less obvious, 
and an overly complex legend poses its own problems even for people who do not suffer from 
color perception problems.
2.7  Challenges in Organizing and Visualizing Variables
As noted throughout this chapter, organizing and visualizing variables can provide useful sum-
maries that can jumpstart the analysis of the variables. However, you must be mindful of the 
limits of the information technology being used to collect, store, and analyze data as well as 
the limits of others to be able to perceive and comprehend your results. Many people make a 
mistake of being overly worried about the former limits—over which, in a typical business 
environment, they have no control—and forgetting or being naïve about the presentation issues 
that are often much more critical. You can sometimes easily create summaries and visualiza-
tions that obscure the data or create false impressions of the data that lead to misleading or 
unproductive analysis. The challenge in organizing and visualizing variables is to avoid these 
complications.
Obscuring Data
Management specialists have long known that information overload, presenting too many de-
tails, can obscure data and hamper decision making (see reference 2). Figure 2.17 presents an 
expanded version of the multidimensional contingency table shown in Figure 2.16 on page 97. 
This table, broken up into two parts by Minitab, illustrates that too many variables as well as 
data poorly formatted and presented can obscure the data.
Even though Figure 2.17 uses 
an example constructed by 
Minitab, the principle being 
illustrated holds for Excel as 
well. The equivalent Excel 
PivotTable is even more  
obscuring than the  
Figure 2.17 table!
F i g u r e  2 . 1 7
Expanded 
multidimensional 
contingency table for the 
retirement funds sample 
showing percentage of 
overall total for fund 
type, market cap, risk, 
and star rating

	
2.7  Challenges in Organizing and Visualizing Variables	
99
Creating False Impressions
As you organize and visualize variables, you must be careful not to create false impressions 
that could affect preliminary conclusions about the data. Selective summarizations and im-
properly constructed visualizations often create false impressions.
A selective summarization is the presentation of only part of the data that have been col-
lected. Frequently, selective summarization occurs when data collected over a long period of 
times are summarized as percentage changes for a shorter period. This type of summarization 
undoes the help that a time-series plot (see page 94) can provide in visualizing trends in data 
that occur over time. For example, Table 2.20A presents the one-year difference in sales of five 
auto industry companies for the month of April. That selective summarization tells a different 
story, particularly for company G, than does Table 2.20B, which shows the year to year differ-
ences for three consecutive years.
Improperly constructed charts can also create false impressions. Take a second look at 
Figure 2.19 above. Because of their relative positions and colorings, many people will perceive 
the dark blue company in the left chart to have a smaller market share than the dark red com-
pany in the right chart even though both pie slices each represent 27%, as can be seen in  False 
Impressions . Differently scaled axes in charts visualizing the same data and a Y axis that either 
does not begin at the origin or is a “broken” axis that is missing intermediate values are other 
common ways to create false impressions.
F i g u r e  2 . 1 8
Side-by-side bar chart 
for the retirement 
funds sample showing 
percentage of overall 
total for fund type, 
market cap, risk, and  
star rating
F i g u r e  2 . 1 9
Market shares of 
companies in “two” 
industries
If you are having a hard time 
believing that the dark blue 
slice in the left pie chart is 
equal to the dark red slice 
in the right pie, open to the 
TwoPies worksheet in the 
Challenging workbook and 
verify the underlying data.
Visualizations can obscure data in other ways as well. Figure 2.19 shows two pie charts 
that display the market shares of companies in two industries. Use these charts to compare the 
two industries. How quickly did you notice that both pie charts represent identical data? The 
ordering or coloring of parts of a chart (pie slices in this example) can obscure data if done 
carelessly.

100	
Chapter 2  Organizing and Visualizing Variables
Chartjunk
Seeking to construct a visualization that can more effectively convey an important point, some 
people add decorative elements to enhance or replace the simple bar and line shapes of the 
visualizations discussed in this chapter. While judicious use of such elements may aid in the 
memorability of a chart (see reference 1), most often such elements either obscure the data or, 
worse, create a false impression of the data. Such elements are called chartjunk.
Figure 2.20 visualizes Australian wine exports to the United States for four years. The 
chartjunk version on the left uses wine glasses in a histogram-like display in lieu of a proper 
time-series plot, such as the one shown on the right. Because the years between measurements 
are not equally spaced, the four wine glasses create a false impression about the ever increasing 
trend in wine exports. The wine glasses also distort the data by using an object with a three-
dimensional volume. (While the height of wine in the 1997 glass is a bit more than 6 times the 
height of the 1989 glass, the volume of that filled 1997 wine glass would be much more than the  
almost empty 1989 glass.)
F i g u r e  2 . 2 0
Two visualizations of Australian wine exports to the United States, in millions of gallons
Chartjunk adapted from S. Watterson, “Liquid Gold—Australians Are Changing the World of Wine. Even the French Seem Grateful,” Time,  
November 22, 1999, p. 68.
Figure 2.21 presents another visual used in the same magazine article. In the chartjunk 
version, the grape vine with its leaves and bunch of grapes convey no useful information while 
creating false impressions about the amount of acreage and the passage of time that obscures 
the trend easily seen in the time-series plot.
T a b l e  2 . 2 0
(A) One-year 
percentage change in 
year-to-year sales for 
the month of April  
(B) Percentage change 
for three consecutive 
years
Change from 
Prior Year
Change from Prior Year
Company
Company
Year 1
Year 2
Year 3
A
  +7.2
A
−22.6
−33.2
  +7.2
B
+24.4
B
  −4.5
−41.9
+24.4
C
+24.9
C
−18.5
−31.5
+24.9
D
+24.8
D
−29.4
−48.1
+24.8
E
+12.5
E
  −1.9
−25.3
+12.5
F
+35.1
F
  −1.6
−37.8
+35.1
G
+29.7
G
  +7.4
−13.6
+29.7
Table 2.20 data is based on 
sales trends of U.S. automakers 
during the period 2007–2010 
that included the 2008  
economic downturn. A  
times-series plot of these data 
for a larger period of time 
would allow you to see trends 
not evident in Table 2.20B.

	
2.7  Challenges in Organizing and Visualizing Variables	
101
…they’re growing more…
Amount of land planted with grapes for the wine industry
1949–1950
135,326
acres
1959–1960
130,201
acres
1969–1970
150,300
acres
1979–1980
172,075
acres
1989–1990
146,204
acres
1997–1998
243,644
acres
F i g u r e  2 . 2 1
Two visualizations of the 
amount of land planted 
with grapes for the wine 
industry
Chartjunk adapted from S.  
Watterson, “Liquid Gold— 
Australians Are Changing the World 
of Wine. Even the French Seem 
Grateful,” Time, November 22, 1999,  
pp. 68–69.
Coke still has most fizz
Carbonated soft drinks with the biggest 
share of the $58 billion market last year:
Coke Classic
20%
Pepsi-Cola
14%
Mountain
Dew
7%
Diet
Coke
9%
Sprite
7%
Dr Pepper
6%
F i g u r e  2 . 2 2
Two visualizations of market share of soft drinks
Chartjunk adapted from Anne B. Carey and Sam Ward, “Coke Still Has Most Fizz,” USA Today, May 10, 2000, p. 1B.
Figure 2.22 visualizes the market share for selected soft drink brands. The chartjunk version 
fails to convey any more information than a simple bar or pie chart would, and the soft drink bottle 
tops included in the chart obscure and distort the data. The side-by-side bar chart at right shows 
how the market shares as represented by the height of the “fizzy” elements overstate the actual 
market shares of the five lesser brands, using the height of the first bottle and fizz to represent 20%.

102	
Chapter 2  Organizing and Visualizing Variables
Guidelines for Constructing Visualizations
To avoid distortions and to create a visualiza-
tion that best conveys the data, use the following 
guidelines:
•  Use the simplest possible visualization
•  Include a title
•  Label all axes
•  Include a scale for each axis if the chart 
contains axes
•  Begin the scale for a vertical axis at zero
•  Use a constant scale
•  Avoid 3D effects
•  Avoid chartjunk
The grapevine portion of Figure 2.21 on page 
101 violates a number of these guidelines besides 
not avoiding the use of chartjunk. There are no axes 
present, and there is no clear zero point on the verti-
cal axis. The 1949–1950 acreage, 135,326, is plot-
ted above the higher 1969–1970 acreage, 150,300. 
Both the horizontal axis and the vertical axis do not 
contain a constant scale, and neither axis is labeled.
When using Microsoft Excel, beware of such 
types of distortions. Excel can construct charts in 
which the vertical axis does not begin at zero and 
may tempt you to restyle simple charts in an in-
appropriate manner or may tempt you to use un-
common chart choices such as doughnut, radar, 
surface, bubble, cone, and pyramid charts. You 
should resist these temptations as they will often 
result in a visualization that obscures the data or 
creates a false impression or both.
Problems for Section 2.7
Applying the Concepts
2.62  (Student Project) Bring to class a chart from a website, 
newspaper, or magazine published recently that you believe to be a 
poorly drawn representation of a numerical variable. Be prepared 
to submit the chart to the instructor with comments about why you 
believe it is inappropriate. Do you believe that the intent of the 
chart is to purposely mislead the reader? Also, be prepared to pres-
ent and comment on this in class.
2.63  (Student Project) Bring to class a chart from a website, 
newspaper, or magazine published this month that you believe to 
be a poorly drawn representation of a categorical variable. Be pre-
pared to submit the chart to the instructor with comments about 
why you consider it inappropriate. Do you believe that the intent 
of the chart is to purposely mislead the reader? Also, be prepared 
to present and comment on this in class.
2.64  (Student Project) The Data and Story Library (DASL) is 
an online library of data files and stories that illustrate the use 
of basic statistical methods. Go to lib.stat.cmu.edu/index.php, 
click DASL, and explore some of the various graphical  
displays.
a.	 Select a graphical display that you think does a good job re-
vealing what the data convey. Discuss why you think it is a 
good graphical display.
b.	 Select a graphical display that you think needs a lot of improve-
ment. Discuss why you think that it is a poorly constructed 
graphical display.
2.65  Examine the visualization at the top of the next column, 
adapted from one that appeared in a post in a digital marketing 
blog.
a.	 Describe at least one good feature of this visual display.
b.	 Describe at least one bad feature of this visual display.
c.	 Redraw the graph, using the guidelines above.
2.66  Examine the following visualization, adapted from one that 
appeared in the post “Who Are the Comic Book Fans on Face-
book?” on February 2, 2013, as reported by graphicspolicy.com.
a.	 Describe at least one good feature of this visual display.
b.	 Describe at least one bad feature of this visual display.
c.	 Redraw the graph, using the guidelines given on page 102.

	
Summary	
103
2.67  Examine the following visualization, adapted a manage-
ment consulting white paper.
a.	 Describe at least one good feature of this visual display.
b.	 Describe at least one bad feature of this visual display.
c.	 Redraw the graph, using the guidelines given on page 102.
2.68  Professor Deanna Oxender Burgess of Florida Gulf Coast 
University conducted research on annual reports of corpora-
tions (see D. Rosato, “Worried About the Numbers? How About 
the Charts?” The New York Times, September 15, 2002, p. B7).  
Burgess found that even slight distortions in a chart changed read-
ers’ perception of the information. Using online or library sources, 
select a corporation and study its most recent annual report. Find 
at least one chart in the report that you think needs improvement 
and develop an improved version of the chart. Explain why you 
believe the improved chart is better than the one included in the 
annual report.
2.69  Figure 2.1 shows a bar chart and a pie chart for the main 
reason young adults shop online (see page 79).
a.	 Create an exploded pie chart, a doughnut chart, a cone chart, or 
a pyramid chart that shows the main reason young adults shop 
online.
b.	 Which graphs do you prefer—the bar chart or pie chart or the 
exploded pie chart, doughnut chart, cone chart, and pyramid 
chart? Explain.
2.70  Figures 2.2 and 2.3 show a bar chart and a pie chart for the 
risk level for the retirement fund data (see page 80).
a.	 Create an exploded pie chart, a doughnut chart, a cone chart, 
and a pyramid chart that shows the risk level of retirement 
funds.
b.	 Which graphs do you prefer—the bar chart or pie chart or the 
exploded pie chart, doughnut chart, cone chart, and pyramid 
chart? Explain.
I
n the Using Statistics scenario, you were hired by the 
Choice Is Yours investment company to assist clients who 
seek to invest in retirement funds. A sample of 316 retire-
ment funds was selected, and information on the funds and 
past performance history was recorded. For each of the 316 
funds, data were collected on 13 variables. With so much in-
formation, visualizing all these numbers required the use of 
properly selected graphical displays.
From bar charts and pie charts, you were able to see that 
about two-thirds of the funds were classified as having low risk, 
about 30% had average risk, and about 4% had high risk. Con-
tingency tables of the fund type and risk revealed that more of 
the value funds have low risk as compared to average or high. 
After constructing histograms and percentage polygons of 
the one-year returns, you were able to conclude that the one-
year return was slightly higher for the value funds than for the 
growth funds. The return for both the growth and value funds is 
concentrated between 10 
and 20, the return for the 
growth funds is more con-
centrated between 10 and 
15, and the return for the 
value funds is more concentrated between 15 and 20.
From a multidimensional contingency table, you dis-
covered more complex relationships; for example, for the 
growth funds, the pattern of risk differs depending on the 
market cap of the fund.
With these insights, you can inform your clients about 
how the different funds performed. Of course, the past per-
formance of a fund does not guarantee its future perfor-
mance. You might also want to analyze the differences in 
return in the past three years, in the past 5 years, and the past 
10 years to see how the growth funds, the value funds, and 
the small, mid-cap, and large market cap funds performed.
U s i n g  S tat i s t i c s
The Choice Is Yours, Revisited
Dmitriy Shironosov/Shutterstock
S u m m a r y
Organizing and visualizing data are the third and fourth 
tasks of the DCOVA framework. How you accomplish 
these tasks varies by the type of variable, categorical or  
numerical, as well as the number of variables you seek to or-
ganize and visualize at the same time. Table 2.20 on page 104 
summarizes the appropriate methods to do these tasks.

104	
Chapter 2  Organizing and Visualizing Variables
Referen c e s
	 1.	Batemen, S., R. Mandryk, C. Gutwin, A. Genest, D. McDine, 
and C. Brooks. “Useful Junk? The Effects of Visual Embel-
lishment on Comprehension and Memorability of Charts.” 
April 10, 2010, www.hci.usask.ca/uploads/173-pap0297-
bateman.pdf.
	 2.	Gross, Bertram. The Managing of Organizations: The Admin-
istrative Struggle, Vols. I & II. New York:The Free Press of 
Glencoe, 1964.
	 3.	Huff, D. How to Lie with Statistics. New York: Norton, 1954.
	 4.	Microsoft Excel 2013. Redmond, WA: Microsoft Corporation, 
2012.
	 5.	Minitab Release 16. State College, PA: Minitab, 2010.
	 6.	Tufte, E. R. Beautiful Evidence. Cheshire, CT: Graphics Press, 
2006.
	 7.	Tufte, E. R. Envisioning Information. Cheshire, CT: Graphics 
Press, 1990.
	 8.	Tufte, E. R. The Visual Display of Quantitative Information, 
2nd ed. Cheshire, CT: Graphics Press, 2002.
	 9.	Tufte, E. R. Visual Explanations. Cheshire, CT: Graphics 
Press, 1997.
	10.	Wainer, H. Visual Revelations: Graphical Tales of Fate and 
Deception from Napoleon Bonaparte to Ross Perot. New York: 
Copernicus/Springer-Verlag, 1997.
K e y  Eq u at i o n s
Determining the Class Interval Width
	
Interval width = highest value - lowest value
number of classes
	
(2.1)
Computing the Proportion or Relative Frequency
	 Proportion = relative frequency = number of values in each class
total number of values
	 (2.2)
Type of Variable
Methods
Categorical variables
Organize
Summary table, contingency table (Section 2.1)
Visualize one variable
Bar chart, pie chart, Pareto chart (Section 2.3)
Visualize two variables
Side-by-side chart (Section 2.3)
Numerical variables
Organize
Ordered array, frequency distribution, relative 
frequency distribution, percentage distribution, 
cumulative percentage distribution (Section 2.2)
Visualize one variable
Stem-and-leaf display, histogram, percentage polygon, 
cumulative percentage polygon (ogive) (Section 2.4)
Visualize two variables
Scatter plot, time-series plot (Section 2.5)
Many variables together
Organize
Multidimensional tables (Section 2.6)
Using the appropriate methods to organize and visual-
ize your data allows you to reach preliminary conclusions 
about the data. In several different chapter examples, tables 
and charts helped you reach conclusions about the main rea-
son that young adults shop online and about the cost of res-
taurant meals in a city and its suburbs; they also provided 
some insights about the sample of retirement funds in The 
Choice Is Yours scenario.
Using the appropriate methods to visualize your data 
may help you reach preliminary conclusions as well as cause 
you to ask additional questions about your data that may lead 
to further analysis at a later time. If used improperly, meth-
ods to organize and visualize the variables can obscure data 
or create false impressions, as Section 2.7 discusses.
Methods to organize and visualize data help summa-
rize data. For numerical variables, there are many additional 
ways to summarize data that involve computing sample 
statistics or population parameters. The most common ex-
amples of these, numerical descriptive measures, are the 
subject of Chapter 3.
T a b l e  2 . 2 0
Organizing and 
Visualizing Data

	
Chapter Review Problems	
105
K ey Te r ms
bar chart  79
bins  73
cell  67
chartjunk  100
class boundaries  71
class interval  71
class interval width  71
class midpoints  72
classes  71
contingency table  67
cumulative percentage distribution  75
cumulative percentage polygon (ogive)  89
frequency distribution  71
histogram  87
joint response  67
lurking variables  97
multidimensional contingency table  96
ogive (cumulative percentage polygon)  89
ordered array  70
Pareto chart  81
Pareto principle  81
percentage distribution  73
percentage polygon  88
pie chart  80
PivotTable  96
proportion  73
relative frequency  73
relative frequency distribution  73
scatter plot  93
side-by-side bar chart  83
stacked  77
stem-and-leaf display  86
summary table  66
time-series plot  94
unstacked  77
C h ec ki n g  Yo ur  U n de r s ta nding
2.71  Is diagrammatic presentation of data better than tabulation of 
data? Why or why not?
2.72  How do you determine class intervals in frequency distribution?
2.73  How is a Pareto chart a better representation of data in com-
parison to a pie chart? 
2.74  Compare and contrast the bar chart for categorical data  
with the histogram for numerical data.
2.75  How does a side-by-side bar chart present data better than a 
contingencies table?
2.76  Why is it said that the main feature of a Pareto chart is its 
ability to separate the “vital few” from the “trivial many”?
2.77  What are the class boundaries in frequency distribution? Why 
it is important to properly define class boundaries?
2.78  How can a multidimensional table differ from a two-variable 
contingency table?
2.79  What type of insights can you gain from a contingency table 
that contains three variables that you cannot gain from a contin-
gency table that contains two variables?
C h a pte r  R e vi e w P r ob le ms
a.	 Using the four categories of publisher, bookstore, author, and 
freight, construct a bar chart, a pie chart, and a Pareto chart.
b.	 Using the four subcategories of publisher and three subcatego-
ries of bookstore, along with the author and freight categories, 
construct a Pareto chart.
c.	 Based on the results of (a) and (b), what conclusions can 
you reach concerning who gets the revenue from the sales of 
new college textbooks? Do any of these results surprise you?  
Explain.
2.80  The following summary, table, presents the breakdown of the 
price of a new college textbook:
Revenue Category
Percentage (%)
Publisher
64.8
Manufacturing costs
32.3
Marketing and promotion
15.4
Administrative costs and taxes
10.0
After-tax profit
  7.1
Bookstore
22.4
Employee salaries and benefits
11.3
Operations
  6.6
Pretax profit
  4.5
Author
11.6
Freight
  1.2
Source: Data extracted from T. Lewin, “When Books Break the Bank,” 
The New York Times, September 16, 2003, pp. B1, B4.

106	
Chapter 2  Organizing and Visualizing Variables
2.81  The following table represents the market share (in num-
ber of movies, gross in millions of dollars, and millions of tickets 
sold) of each type of movie in 2012:
 
Type
Number
Gross 
($millions)
Tickets 
(millions)
 
Based on fiction  
  book/short story
 
80
 
 3181.1
 
  541.8
Based on comic/ 
  graphic novel
 
9
 
 1,552.3
 
198.2
Based on factual  
  book/article
 
17
 
328.1
 
41.9
Based on game
3
125.1
15.9
Based on real life  
  events
 
168
 
371.5
 
47.4
Based on TV
9
261.4
33.4
Original screenplay
367
4,245.0
541.8
Remake
14
194.3
24.7
Based on folk tale/ 
  legend/fairytale
 
4
 
304.5
 
38.9
Spin-off
2
39.9
5.1
Source: Data extracted from www.the-numbers.com/market 
/Sources2012.summary.
a.	 Construct a bar chart, a pie chart, and a Pareto chart for the 
number of movies, gross (in $millions), and number of tickets 
sold (in millions).
b.	 What conclusions can you reach about the market shares of the 
different types of movies in 2012?
2.82  A survey was conducted from 665 consumer magazines on 
the practices of their websites. The results are summarized in a 
copyediting table and a fact-checking table:
 
Copyediting as Compared  
to Print Content
 
Percentage (%)
 
As rigorous
41
 
 
Less rigorous
48
 
 
Not copyedited
11
  
 
Source: Data extracted from S. Clifford, “Columbia Survey Finds a 
Slack Editing Process of Magazine Web Sites,” The New York Times, 
March 1, 2010, p. B6.
 
a.	 For copyediting, construct a bar chart, a pie chart, and a Pareto chart.
b.	 Which graphical method do you think is best for portraying 
these data?
 
Fact Checking as Compared  
to Print Content
Percentage  
(%)
 
 
Same
57
 
 
Less rigorous
27
 
 
Online not fact-checked
  8
 
 
Neither online nor print is fact-checked
  8
 
 
Source: Data extracted from S. Clifford, “Columbia Survey Finds 
a Slack Editing Process of Magazine Web Sites,” The New York 
Times, March 1, 2010, p. B6.
 
c.	 For fact checking, construct a bar chart, a pie chart, and a  
Pareto chart.
d.	 Which graphical method do you think is best for portraying the 
fact checking data?
e.	 What conclusions can you reach concerning copyediting and 
fact checking of print and online consumer magazines?
2.83  The owner of a restaurant that serves Continental-style en-
trées has the business objective of learning more about the patterns 
of patron demand during the Friday-to-Sunday weekend time pe-
riod. Data were collected from 630 customers on the type of en-
trée ordered and organized in the following table (and stored in  
 Entree ):
Type of Entrée
Number Served
Beef
187
Chicken
103
Mixed
  30
Duck
  25
Fish
122
Pasta
  63
Shellfish
  74
Veal
  26
Total
630
a.	 Construct a percentage summary table for the types of entrées 
ordered.
b.	 Construct a bar chart, a pie chart, and a Pareto chart for the 
types of entrées ordered.
c.	 Do you prefer using a Pareto chart or a pie chart for these data? Why?
d.	 What conclusions can the restaurant owner reach concerning 
demand for different types of entrées?
2.84  Suppose that the owner of the restaurant in Problem 2.83 
also wants to study the demand for dessert during the same time 
period. She decides that in addition to studying whether a dessert 
was ordered, she will also study the gender of the individual and 
whether a beef entrée was ordered. Data were collected from 630 
customers and organized in the following contingency tables:
GENDER
DESSERT ORDERED
Male
Female
Total
Yes
  50
  96
146
No
250
234
484
Total
300
330
630
BEEF ENTRÉE
DESSERT ORDERED
Yes
No
Total
Yes
  74
  68
142
No
123
365
488
Total
197
433
630
a.	 For each of the two contingency tables, construct contingency 
tables of row percentages, column percentages, and total per-
centages.

b.	 Which type of percentage (row, column, or total) do you think 
is most informative for each gender? For beef entrée? Explain.
c.	 What conclusions concerning the pattern of dessert ordering 
can the restaurant owner reach?
2.85  The following data represent the pounds per capita of fresh 
food and packaged food consumed in the United States, Japan, and 
Russia in a recent year:
COUNTRY
FRESH FOOD
United 
States
Japan
Russia
Eggs, nuts, and beans
  88
94
88
Fruit
124
126
88
Meat and seafood
197
146
125
Vegetables
194
278
335
PACKAGED FOOD
Bakery goods
108
53
144
Dairy products
298
147
127
Pasta
  12
32
16
Processed, frozen,  
  dried and chilled  
  food, and ready-to- 
  eat meals
 
 
 
183
 
 
 
251
 
 
 
70
Sauces, dressings,  
  and condiments
 
   63
 
75
 
49
Snacks and candy
  47
19
24
Soup and canned  
  food
 
  77
 
17
 
25
Source: Data extracted from H. Fairfield, “Factory Food,” The New 
York Times, April 4, 2010, p. BU5.
a.	 For the United States, Japan, and Russia, construct a bar chart, 
a pie chart, and a Pareto chart for different types of fresh foods 
consumed.
b.	 For the United States, Japan, and Russia, construct a bar chart, 
a pie chart, and a Pareto chart for different types of packaged 
foods consumed.
c.	 What conclusions can you reach concerning differences be-
tween the United States, Japan, and Russia in the fresh foods 
and packaged foods consumed?
2.86  The Air Travel Consumer Report, a monthly product of the 
Department of Transportation’s Office of Aviation Enforcement and 
Proceedings (OAEP), is designed to assist consumers with informa-
tion on the quality of services provided by airlines. The report in-
cludes a summary of consumer complaints by industry group and by 
complaint category. A breakdown of 987 November 2012 consumer 
complaints based on industry group is given in the following table:
Industry Group
Number of Consumer 
Complaints
Airlines
922
Travel agents
23
Tour operators
24
Miscellaneous
18
Industry total
987
Source: Data extracted from “The Travel Consumer Report,” Office 
of Aviation Enforcement and Proceedings, January 2013.
a.	 Construct a Pareto chart for the number of complaints by in-
dustry group. What industry group accounts for most of the 
complaints?
The 922 consumer complaints against airlines fall into one of two 
groups: complaints against U.S. airlines and complaints against 
foreign airlines. The following table summarizes these 922 com-
plaints by complaint type:
Complaint Type
Against U.S. 
Airlines
Against 
Foreign 
Airlines
Flight problems
201
38
Oversales
7
5
Reservation/ticketing/ 
  boarding
 
103
 
39
Fares
48
31
Refunds
47
13
Baggage
98
51
Customer service
100
38
Disability
48
6
Advertising
4
2
Discrimination
3
4
Animals
0
0
Other
29
7
Total
688
234
b.	 Construct pie charts to display the percentage of complaints by 
type against U.S. airlines and foreign airlines.
c.	 Construct a Pareto chart for the complaint categories against 
U.S. airlines. Does a certain complaint category account for 
most of the complaints?
d.	 Construct a Pareto chart for the complaint categories against 
foreign airlines. Does a certain complaint category account for 
most of the complaints?
2.87  One of the major measures of the quality of service provided 
by an organization is the speed with which the organization responds 
to customer complaints. A large family-held department store sell-
ing furniture and flooring, including carpet, had undergone a major 
expansion in the past several years. In particular, the flooring depart-
ment had expanded from 2 installation crews to an installation super-
visor, a measurer, and 15 installation crews. A business objective of 
the company was to reduce the time between when the complaint is 
received and when it is resolved. During a recent year, the company 
received 50 complaints concerning carpet installation. The number 
of days between the receipt of the complaint and the resolution of the 
complaint for the 50 complaints, stored in  Furniture , are:
54  5 35 137 31 27 152 2 123 81 74 27
11 19 126 110 110 29  61 35   94  31 26    5
12  4 165  32 29 28  29 26  25  1 14 13
13 10  5  27  4 52  30 22    36 26 20 23
33 68
a.	 Construct a frequency distribution and a percentage distribution.
b.	 Construct a histogram and a percentage polygon.
c.	 Construct a cumulative percentage distribution and plot a cu-
mulative percentage polygon (ogive).
	
Chapter Review Problems	
107

108	
Chapter 2  Organizing and Visualizing Variables
d.	 On the basis of the results of (a) through (c), if you had to tell 
the president of the company how long a customer should ex-
pect to wait to have a complaint resolved, what would you say? 
Explain.
2.88  The file  DomesticBeer  contains the percentage alcohol, 
number of calories per 12 ounces, and number of carbohydrates 
(in grams) per 12 ounces for 152 of the best-selling domestic beers 
in the United States.
Source: Data extracted from www.beer100.com/beercalories.
htm, March 20, 2013.
a.	 Construct a percentage histogram for percentage alcohol, num-
ber of calories per 12 ounces, and number of carbohydrates (in 
grams) per 12 ounces.
b.	 Construct three scatter plots: percentage alcohol versus calo-
ries, percentage alcohol versus carbohydrates, and calories ver-
sus carbohydrates.
c.	 Discuss what you learn from studying the graphs in (a)  
and (b).
2.89  The file  CigaretteTax  contains the state cigarette tax ($) for 
each state as of January 1, 2013.
a.	 Construct an ordered array.
b.	 Plot a percentage histogram.
c.	 What conclusions can you reach about the differences in the 
state cigarette tax between the states?
2.90  The file  CDRate  contains the yields for one-year certifi-
cates of deposit (CDs) and a five-year CDs for 23 banks in the 
United States, as of March 20, 2013.
Source: Data extracted and compiled from www.Bankrate.com, 
March 20, 2013.
a.	 Construct a stem-and-leaf display for one-year CDs and five-
year CDs.
b.	 Construct a scatter plot of one-year CDs versus five-year CDs.
c.	 What is the relationship between the one-year CD rate and the 
five-year CD rate?
2.91  The file  CEO-Compensation  includes the total compensa-
tion (in $millions) for CEOs of 170 large public companies and 
the investment return in 2012. (Data extracted from “CEO Pay 
Skyrockets as Economy, Stocks Recover,” USA Today, March 27, 
2013, p. B1.) For total compensation:
a.	 Construct a frequency distribution and a percentage distribution.
b.	 Construct a histogram and a percentage polygon.
c.	 Construct a cumulative percentage distribution and plot a  
cumulative percentage polygon (ogive).
d.	 Based on (a) through (c), what conclusions can you reach  
concerning CEO compensation in 2012?
e.	 Construct a scatter plot of total compensation and investment 
return in 2012.
f.	 What is the relationship between the total compensation and 
investment return in 2012?
2.92  Studies conducted by a manufacturer of Boston and  
Vermont asphalt shingles have shown product weight to be a 
major factor in customers’ perception of quality. Moreover, the 
weight represents the amount of raw materials being used and is 
therefore very important to the company from a cost standpoint. 
The last stage of the assembly line packages the shingles before 
the packages are placed on wooden pallets. The variable of inter-
est is the weight in pounds of the pallet, which for most brands 
holds 16 squares of shingles. The company expects pallets of 
its Boston brand-name shingles to weigh at least 3,050 pounds 
but less than 3,260 pounds. For the company’s Vermont brand-
name shingles, pallets should weigh at least 3,600 pounds but 
less than 3,800. Data, collected from a sample of 368 pallets of 
Boston shingles and 330 pallets of Vermont shingles, are stored  
in  Pallet .
a.	 For the Boston shingles, construct a frequency distribution 
and a percentage distribution having eight class intervals, us-
ing 3,015, 3,050, 3,085, 3,120, 3,155, 3,190, 3,225, 3,260, and 
3,295 as the class boundaries.
b.	 For the Vermont shingles, construct a frequency distribution 
and a percentage distribution having seven class intervals, us-
ing 3,550, 3,600, 3,650, 3,700, 3,750, 3,800, 3,850, and 3,900 
as the class boundaries.
c.	 Construct percentage histograms for the Boston shingles and 
for the Vermont shingles.
d.	 Comment on the distribution of pallet weights for the Boston 
and Vermont shingles. Be sure to identify the percentages of 
pallets that are underweight and overweight.
2.93  What was the average price of a room at two-star, three-star, 
and four-star hotels in cities around the world in 2012? The file  
  HotelPrices  contains the prices in English pounds (about US$1.56 
as of July 2012). (Data extracted from bit.ly/Q0qxe4.) Complete 
the following for two-star, three-star, and four-star hotels:
a.	 Construct a frequency distribution and a percentage distribution.
b.	 Construct a histogram and a percentage polygon.
c.	 Construct a cumulative percentage distribution and plot a cu-
mulative percentage polygon (ogive).
d.	 What conclusions can you reach about the cost of two-star, 
three-star, and four-star hotels?
e.	 Construct separate scatter plots of the cost of two-star hotels 
versus three-star hotels, two-star hotels versus four-star hotels, 
and three-star hotels versus four-star hotels.
f.	 What conclusions can you reach about the relationship of the 
price of two-star, three-star, and four-star hotels?
2.94  The file  Protein  contains calorie and cholesterol  
information for popular protein foods (fresh red meats, poultry, 
and fish).
Source: U.S. Department of Agriculture.
a.	 Construct a percentage histogram for the number of calories.
b.	 Construct a percentage histogram for the amount of cholesterol.
c.	 What conclusions can you reach from your analyses in (a)  
and (b)?
2.95  The file  Natural Gas  contains the monthly average well-
head and residential price for natural gas (dollars per thousand 
cubic feet) in the United States from January 1, 2008, to January 
1, 2013. (Data extracted from “U.S. Natural Gas Prices,” 1.usa.
gov/qHDWNz, March 1, 2013.) For the wellhead price and the 
residential price:
a.	 Construct a time-series plot.
b.	 What pattern, if any, is present in the data?

c.	 Construct a scatter plot of the wellhead price and the residential 
price.
d.	 What conclusion can you reach about the relationship between 
the wellhead price and the residential price?
2.96  The following data (stored in  Drink ) represent the amount 
of soft drink in a sample of 50 consecutively filled 2-liter bottles. 
The results are listed horizontally in the order of being filled:
2.109  2.086  2.066  2.075  2.065  2.057  2.052  2.044  2.036  2.038
2.031  2.029  2.025  2.029  2.023  2.020  2.015  2.014  2.013  2.014
2.012  2.012  2.012  2.010  2.005  2.003  1.999  1.996  1.997  1.992
1.994  1.986  1.984  1.981  1.973  1.975  1.971  1.969  1.966  1.967
1.963  1.957  1.951  1.951  1.947  1.941  1.941  1.938  1.908  1.894
a.	 Construct a time-series plot for the amount of soft drink on the 
Y axis and the bottle number (going consecutively from 1 to 
50) on the X axis.
b.	 What pattern, if any, is present in these data?
c.	 If you had to make a prediction about the amount of soft drink 
filled in the next bottle, what would you predict?
d.	 Based on the results of (a) through (c), explain why it is impor-
tant to construct a time-series plot and not just a histogram, as 
was done in Problem 2.47 on page 92.
2.97  The file  Currency  contains the exchange rates of the  
Canadian dollar, the Japanese yen, and the English pound from 
1980 to 2012, where the Canadian dollar, the Japanese yen, and 
the English pound are expressed in units per U.S. dollar.
a.	 Construct time-series plots for the yearly closing values of the 
Canadian dollar, the Japanese yen, and the English pound.
b.	 Explain any patterns present in the plots.
c.	 Write a short summary of your findings.
d.	 Construct separate scatter plots of the value of the Canadian 
dollar versus the Japanese yen, the Canadian dollar versus 
the English pound, and the Japanese yen versus the English 
pound.
e.	 What conclusions can you reach concerning the value of the 
Canadian dollar, Japanese yen, and English pound in terms of 
the U.S. dollar?
2.98  A/B testing is a method used by businesses to test differ-
ent designs and formats of a webpage to determine if a new web 
page is more effective than a current web page. Web designers 
tested a new call to action button on its web page. Every visitor 
to the web page was randomly shown either the original call-to-
action button (the control) or the new variation. The metric used to  
measure success was the download rate: the number of people 
who downloaded the file divided by the number of people who 
saw that particular call-to-action button. Results of the experiment 
yielded the following:
Variations
Downloads
Visitors
Original call to action button
351
3,642
New call to action button
485
3,556
a.	 Compute the percentage of downloads for the original call-to-
action button and the new call-to-action button.
b.	 Construct a bar chart of the percentage of downloads for 
the original call-to-action button and the new call-to-action  
button.
c.	 What conclusions can you reach concerning the original call-
to-action button and the new call-to-action button?
Web designers tested a new web design on its web page. Every 
visitor to the web page was randomly shown either the original 
web design (the control) or the new variation. The metric used 
to measure success was the download rate: the number of people 
who downloaded the file divided by the number of people who 
saw that particular web design. Results of the experiment yielded 
the following:
Variations
Downloads
Visitors
Original web design
305
3,427
New web design
353
3,751
d.	 Compute the percentage of downloads for the original web de-
sign and the new web design.
e.	 Construct a bar chart of the percentage of downloads for the 
original web design and the new web design.
f.	 What conclusions can you reach concerning the original web 
design and the new web design?
g.	 Compare your conclusions in (f) with those in (c).
Web designers now tested two factors simultaneously—the 
­call-to-action button and the new web design. Every visitor to the 
web page was randomly shown one of the following:
	
Old call to action button with original web design
	
New call to action button with original web design
	
Old call to action button with new web design
	
New call to action button with new web design
Again, the metric used to measure success was the download rate: 
the number of people who downloaded the file divided by the 
number of people who saw that particular call-to-action button and 
web design. Results of the experiment yielded the following:
Call to 
Action 
Button
Web 
Design
Downloaded
Declined
Total
Original
Original
83
917
1,000
New
Original
137
863
1,000
Original
New
95
905
1,000
New
New
170
830
1,000
Total
485
3,515
4,000
h.	 Compute the percentage of downloads for each combination of 
call-to-action button and web design.
i.	 What conclusions can you reach concerning the original call to 
action button and the new call to action button and the original 
web design and the new web design?
j.	 Compare your conclusions in (i) with those in (c) and (g).
	
Chapter Review Problems	
109

110	
Chapter 2  Organizing and Visualizing Variables
2.99  (Class Project) Have each student in the class respond to 
the question “Which carbonated soft drink do you most prefer?” 
so that the instructor can tally the results into a summary table.
a.	 Convert the data to percentages and construct a Pareto chart.
b.	 Analyze the findings.
2.100  (Class Project) Cross-classify each student in the class by 
gender (male, female) and current employment status (yes, no), so 
that the instructor can tally the results.
a.	 Construct a table with either row or column percentages, de-
pending on which you think is more informative.
b.	 What would you conclude from this study?
c.	 What other variables would you want to know regarding em-
ployment in order to enhance your findings?
Report Writing Exercises
2.101  Referring to the results from Problem 2.92 on page 108 
concerning the weights of Boston and Vermont shingles, write a 
report that evaluates whether the weights of the pallets of the two 
types of shingles are what the company expects. Be sure to incor-
porate tables and charts into the report.
C a s e s  f o r  C h a p t e r  2
Managing Ashland MultiComm Services
Recently, Ashland MultiComm Services has been criticized 
for its inadequate customer service in responding to ques-
tions and problems about its telephone, cable television, and 
Internet services. Senior management has established a task 
force charged with the business objective of improving cus-
tomer service. In response to this charge, the task force col-
lected data about the types of customer service errors, the 
cost of customer service errors, and the cost of wrong bill-
ing errors. It found the following data:
Types of Customer Service Errors
Type of Errors
Frequency
Incorrect accessory
27
Incorrect address
42
Incorrect contact phone
31
Invalid wiring
9
On-demand programming error
14
Subscription not ordered
8
Suspension error
15
Termination error
22
Website access error
30
Wrong billing
137
Wrong end date
17
Wrong number of connections
19
Wrong price quoted
20
Wrong start date
24
Wrong subscription type
33
Total
448
Cost of Customer Service Errors in the Past Year
Type of Errors
Cost ($ thousands)
Incorrect accessory
17.3
Incorrect address
62.4
Incorrect contact phone
21.3
Invalid wiring
40.8
On-demand programming errors
38.8
Subscription not ordered
20.3
Suspension error
46.8
Termination error
50.9
Website access errors
60.7
Wrong billing
121.7
Wrong end date
40.9
Wrong number of connections
28.1
Wrong price quoted
50.3
Wrong start date
40.8
Wrong subscription type
60.1
Total
701.2
Type and Cost of Wrong Billing Errors
Type of Wrong Billing Errors
Cost ($ thousands)
Declined or held transactions
7.6
Incorrect account number
104.3
Invalid verification
9.8
Total
121.7

1.	Review these data (stored in  AMS2-1 ). Identify the vari-
ables that are important in describing the customer ser-
vice problems. For each variable you identify, construct 
the graphical representation you think is most appropri-
ate and explain your choice. Also, suggest what other 
information concerning the different types of errors 
would be useful to examine. Offer possible courses of 
action for either the task force or management to take 
that would support the goal of improving customer  
service.
2.	As a follow-up activity, the task force decides to collect 
data to study the pattern of calls to the help desk (stored 
in  AMS2-2 ). Analyze these data and present your conclu-
sions in a report.
Digital Case
In the Using Statistics scenario, you were asked to gather 
information to help make wise investment choices. Sources 
for such information include brokerage firms, investment 
counselors, and other financial services firms. Apply your 
knowledge about the proper use of tables and charts in this 
Digital Case about the claims of foresight and excellence by 
an Ashland-area financial services firm.
Open EndRunGuide.pdf, which contains the EndRun  
Financial Services “Guide to Investing.” Review the guide, 
paying close attention to the company’s investment claims 
and supporting data and then answer the following.
1.	How does the presentation of the general information 
about EndRun in this guide affect your perception of the 
business?
2.	Is EndRun’s claim about having more winners than losers 
a fair and accurate reflection of the quality of its invest-
ment service? If you do not think that the claim is a fair 
and accurate one, provide an alternate presentation that 
you think is fair and accurate.
3.	Review the discussion about EndRun’s “Big Eight Differ-
ence” and then open and examine the attached sample of 
mutual funds. Are there any other relevant data from that 
file that could have been included in the Big Eight table? 
How would the new data alter your perception of End-
Run’s claims?
4.	 EndRun is proud that all Big Eight funds have gained in 
value over the past five years. Do you agree that EndRun 
should be proud of its selections? Why or why not?
CardioGood Fitness
The market research team at AdRight is assigned the task to 
identify the profile of the typical customer for each tread-
mill product offered by CardioGood Fitness. The market 
research team decides to investigate whether there are dif-
ferences across the product lines with respect to customer 
characteristics. The team decides to collect data on indi-
viduals who purchased a treadmill at a CardioGood Fit-
ness retail store during the prior three months. The data are 
stored in the  CardioGood Fitness  file. The team identifies the 
following customer variables to study: product purchased, 
TM195, TM498, or TM798; gender; age, in years; educa-
tion, in years; relationship status, single or partnered; annual 
household income ($); average number of times the cus-
tomer plans to use the treadmill each week; average number 
of miles the customer expects to walk/run each week; and 
self-rated fitness on an 1-to-5 ordinal scale, where 1 is poor 
shape and 5 is excellent shape.
1.  Create a customer profile for each CardioGood Fitness 
treadmill product line by developing appropriate tables 
and charts.
2.  Write a report to be presented to the management of Car-
dioGood Fitness detailing your findings.
	
Cases for Chapter 2	
111
The Choice Is Yours Follow-Up
Follow up the Using Statistics Revisited section on  
page 103 by analyzing the differences in 3-year return per-
centages, 5-year return percentages, and 10-year return per-
centages for the sample of 316 retirement funds stored in  
 Retirement Funds . In your analysis, examine differences be-
tween the growth and value funds as well as the differences 
among the small, mid-cap, and large market cap funds.

112	
Chapter 2  Organizing and Visualizing Variables
Clear Mountain State Student Surveys
1.	The student news service at Clear Mountain State Uni-
versity (CMSU) has decided to gather data about the 
undergraduate students that attend CMSU. They cre-
ate and distribute a survey of 14 questions and re-
ceive responses from 62 undergraduates (stored in  
 UndergradSurvey ). For each question asked in the survey, 
construct all the appropriate tables and charts and write a 
report summarizing your conclusions.
2.	The dean of students at CMSU has learned about the 
undergraduate survey and has decided to undertake 
a similar survey for graduate students at CMSU. She 
creates and distributes a survey of 14 questions and re-
ceives responses from 44 graduate students (stored in  
 GradSurvey ). For each question asked in the ­survey, con­
struct all the appropriate tables and charts and write a 
report summarizing your conclusions.

EG2.1  Organizing Categorical 
Variables
The Summary Table
Key Technique  Use the PivotTable feature to create a summary 
table for untallied data.
Example  Create a frequency and percentage summary table 
similar to Table 2.3 on page 66.
PHStat  Use One-Way Tables & Charts.
For the example, open to the DATA worksheet of the Retirement 
Funds workbook. Select PHStat ➔ Descriptive Statistics ➔ 
One-Way Tables & Charts. In the procedure’s dialog box (shown 
below):
	 1.	 Click Raw Categorical Data (because the worksheet con-
tains untallied data).
	 2.	 Enter H1:H317 as the Raw Data Cell Range and check 
First cell contains label.
	 3.	 Enter a Title, check Percentage Column, and click OK.
PHStat creates a PivotTable summary table on a new worksheet. 
For data that have already been tallied into categories, click Table 
of Frequencies in step 1.
In the PivotTable, risk categories appear in alphabetical order 
and not in the order low, average, and high as would normally be 
expected. To change to the expected order, use steps 14 and 15 of 
the In-Depth Excel instructions but change all references to cell 
A6 to cell A7 and drop the Low label over cell A5, not cell A4.
In-Depth Excel (untallied data)  Use the Summary Table 
workbook as a model.
For the example, open to the DATA worksheet of the Retirement 
Funds workbook and select Insert ➔ PivotTable. In the Create 
PivotTable dialog box (shown at top in right column):
	 1.	 Click Select a table or range and enter H1:H317 as the  
Table/Range cell range.
	 2.	 Click New Worksheet and then click OK.
In the Excel 2013 PivotTable Fields task pane (shown below) or in 
the similar PivotTable Field List task pane in other Excels:
	 3.	 Drag Risk in the Choose fields to add to report box and 
drop it in the ROWS (or Row Labels) box.
	 4.	 Drag Risk in the Choose fields to add to report box a 
second time and drop it in the Σ Values box. This sec-
ond label changes to Count of Risk to indicate that a 
count, or tally, of the risk categories will be displayed in the 
­PivotTable.
In the PivotTable being created:
	 5.	 Enter Risk in cell A3 to replace the heading Row Labels.
	 6.	 Right-click cell A3 and then click PivotTable Options in the 
shortcut menu that appears.
C h a p t e r  2  E x c e l  G u i d e
	
Chapter 2 Excel Guide	
113

114	
Chapter 2  Organizing and Visualizing Variables
In the PivotTable Options dialog box (shown below):
	 7.	 Click the Layout & Format tab.
	 8.	 Check For empty cells show and enter 0 as its value. Leave 
all other settings unchanged.
	 9.	 Click OK to complete the PivotTable.
To add a column for the percentage frequency:
	10.	 Enter Percentage in cell C3. Enter the formula = B4>B+7 in 
cell C4 and copy it down through row 7.
	11.	 Select cell range C4:C7, right-click, and select Format 
Cells in the shortcut menu.
	12.	 In the Number tab of the Format Cells dialog box, select  
Percentage as the Category and click OK.
	13.	 Adjust the worksheet formatting, if appropriate (see Appendix B)  
and enter a title in cell A1.
In the PivotTable, risk categories appear in alphabetical order 
and not in the order low, average, and high, as would normally be  
expected. To change to the expected order:
	14.	 Click the Low label in cell A6 to highlight cell A6. Move 
the mouse pointer to the top edge of the cell until the mouse 
pointer changes to a four-way arrow.
	15.	 Drag the Low label and drop the label over cell A4. The risk 
categories now appear in the order Low, Average, and High 
in the summary table.
In-Depth Excel (tallied data)  Use the SUMMARY_ 
SIMPLE worksheet of the Summary Table workbook as a 
model for creating a summary table.
The Contingency Table
Key Technique  Use the PivotTable feature to create a contin-
gency table for untallied data.
Example  Construct a contingency table displaying fund type 
and risk level similar to Table 2.4 on page 67.
PHStat (untallied data)  Use Two-Way Tables & Charts. 
For the example, open to the DATA worksheet of the Retirement 
Funds workbook. Select PHStat ➔ Descriptive Statistics ➔ 
Two-Way Tables & Charts. In the procedure’s dialog box (shown 
below):
	 1.	 Enter C1:C317 as the Row Variable Cell Range.
	 2.	 Enter H1:H317 as the Column Variable Cell Range.
	 3.	 Check First cell in each range contains label.
	 4.	 Enter a Title and click OK.
In the PivotTable, risk categories appear in alphabetical order 
and not in the order low, average, and high as would normally be 
expected. To change the expected order, use steps 14 and 15 of the 
In-Depth Excel instructions.
In-Depth Excel (untallied data)  Use the Contingency  
Table workbook as a model.
For the example, open to the DATA worksheet of the Retirement 
Funds workbook. Select Insert ➔ PivotTable. In the Create  
PivotTable dialog box:
	 1.	 Click Select a table or range and enter A1:N317 as the  
Table/Range cell range.
	 2.	 Click New Worksheet and then click OK.
In the PivotTable Fields (called the PivotTable Field List in some 
Excel versions) task pane:
	 3.	 Drag Type from Choose fields to add to report and drop it 
in the ROWS (or Row Labels) box.
	 4.	 Drag Risk from Choose fields to add to report and drop it 
in the COLUMNS (or Column Labels) box.
	 5.	 Drag Type from Choose fields to add to report a second 
time and drop it in the Σ Values box. (Type changes to 
Count of Type.)
In the PivotTable being created:
	 6.	 Select cell A3 and enter a space character to clear the label 
Count of Type.
	 7.	 Enter Type in cell A4 to replace the heading Row Labels.
	 8.	 Enter Risk in cell B3 to replace the heading Column Labels.
	 9.	 Click the Low label in cell D4 to highlight cell D4. Move 
the mouse pointer to the left edge of the cell until the mouse 
pointer changes to a four-way arrow.
10.	 Drag the Low label to the left and drop the label when an  
I-beam appears between columns A and B. The Low label  
appears in B4 and column B now contains the low risk 
­tallies.

	11.	 Right-click over the PivotTable and then click PivotTable  
Options in the shortcut menu that appears.
In the PivotTable Options dialog box:
	12.	 Click the Layout & Format tab.
	13.	 Check For empty cells show and enter 0 as its value. Leave 
all other settings unchanged.
	14.	 Click the Total & Filters tab.
	15.	 Check Show grand totals for columns and Show grand  
totals for rows.
	16.	 Click OK to complete the table.
In-Depth Excel (tallied data)  Use the CONTINGENCY_
SIMPLE worksheet of the Contingency Table workbook as a 
model for creating a contingency table.
EG2.2  ORGANIZING NUMERICAL 
Variables
Stacked and Unstacked Data
PHStat  Use Stack Data or Unstack Data.
For example, to unstack the 3YrReturn% variable by the Type 
variable in the retirement funds sample, open to the DATA work-
sheet of the Retirement Funds workbook. Select Data Prepa-
ration ➔ Unstack Data. In that procedure’s dialog box, enter 
C1:C317 (the Type variable cell range) as the Grouping Vari-
able Cell Range and enter J1:J317 (the 3YrReturn% variable cell 
range) as the Stacked Data Cell Range. Check First cells in both 
ranges contain label and click OK. The unstacked data appear on 
a new worksheet.
The Ordered Array
In-Depth Excel  To create an ordered array, first select the 
numerical variable to be sorted. Then select Home ➔ Sort &  
Filter (in the Editing group) and in the drop-down menu click Sort 
Smallest to Largest. (You will see Sort A to Z as the first drop-
down choice if you did not select a cell range of numerical data.)
The Frequency Distribution
Key Technique  Establish bins (see Classes and Excel Bins in 
Section 2.2) and then use the FREQUENCY(untallied data cell 
range, bins cell range) array function to tally data.
Example  Create a frequency, percentage, and cumulative per-
centage distribution for the restaurant meal cost data that contains 
the information found in Tables 2.9, 2.11, and 2.14, in Section 2.2.
PHStat (untallied data)  Use Frequency Distribution. (Use 
Histogram & Polygons, discussed in Section EG2.4, if you plan 
to construct a histogram or polygon in addition to a frequency  
distribution.) For the example, open to the DATA worksheet of 
the Restaurants workbook. This worksheet contains the meal 
cost data in stacked format in column G and a set of bin numbers 
appropriate for those data in column H. Select PHStat ➔ Descrip-
tive Statistics ➔ Frequency Distribution. In the procedure’s dia-
log box (shown below):
	 1.	 Enter G1:G101 as the Variable Cell Range, enter I1:I9 as 
the Bins Cell Range, and check First cell in each range 
contains label.
	 2.	 Click Multiple Groups - Stacked and enter A1:A101 as the 
Grouping Variable Cell Range. (The cell range A1:A101 
contains the Location variable.)
	 3.	 Enter a Title and click OK.
Click Single Group Variable in step 2 if constructing a distribu-
tion from a single group of untallied data. Click Multiple Groups -  
Unstacked in step 2 if the Variable Cell Range contains two or 
more columns of unstacked, untallied data.
Frequency distributions for the two groups appear on separate 
worksheets. To display the information for the two groups on one 
worksheet, select the cell range B3:D11 on one of the worksheets. 
Right-click that range and click Copy in the shortcut menu. Open 
to the other worksheet. In that other worksheet, right-click cell E3 
and click Paste Special in the shortcut menu. In the Paste Special 
dialog box, click Values and numbers format and click OK. Ad-
just the worksheet title as necessary. (Learn more about the Paste 
Special command in Appendix B.)
In-Depth Excel (untallied data)  Use the Distributions 
workbook as a model.
For the example, open to the UNSTACKED worksheet of the 
Restaurants workbook. This worksheet contains the meal cost 
data unstacked in columns A and B and a set of bin numbers  
appropriate for those data in column D. Then:
	 1.	 Right-click the UNSTACKED sheet tab and click Insert in 
the shortcut menu.
	 2.	 In the General tab of the Insert dialog box, click Worksheet 
and then click OK.
In the new worksheet:
	 3.	 Enter a title in cell A1, Bins in cell A3, and Frequency in  
cell B3.
	 4.	 Copy the bin number list in the cell range D2:D9 of the  
UNSTACKED worksheet and paste this list into cell A4 of 
the new worksheet.
	 5.	 Select the cell range B4:B12 that will hold the array formula.
	
Chapter 2 Excel Guide	
115

116	
Chapter 2  Organizing and Visualizing Variables
	 6.	 Type, but do not press the Enter or Tab key, the formula  
=FREQUENCY(UNSTACKED!$A$1:$A$51, $A$4:$A$11). 
Then, while holding down the Ctrl and Shift keys, press 
the Enter key to enter the array formula into the cell range 
B4:B12. (Learn more about array formulas in Appendix B.)
	 7.	 Adjust the worksheet formatting as necessary.
Note that in step 6, you enter the cell range as UNSTACKED! 
$A$1:$A$51 and not as $A$1:$A$51 because the untallied data 
are located on another (the UNSTACKED) worksheet. (Learn 
more about referring to data on another worksheet, as well as the 
significance of entering the cell range as $A$1:$A$51 and not as 
A1:A51, in Appendix B.)
Steps 1 through 7 construct a frequency distribution for the 
meal costs at city restaurants. To construct a frequency distribution 
for the meal costs at suburban restaurants, repeat steps 1 through 7 
but in step 6 type =FREQUENCY(UNSTACKED!$B$1:$B$51, 
$A$4:$A$11) as the array formula.
To display the distributions for the two groups on one work-
sheet, select the cell range B3:B11 on one of the worksheets. 
Right-click that range and click Copy in the shortcut menu. Open 
to the other worksheet. In that other worksheet, right-click cell C3 
and click Paste Special in the shortcut menu. In the Paste Special 
dialog box, click Values and numbers format and click OK. Ad-
just the worksheet title as necessary. (Learn more about the Paste 
Special command in Appendix B.)
Analysis ToolPak (untallied data)  Use Histogram.
For the example, open to the UNSTACKED worksheet of the 
Restaurants workbook. This worksheet contains the meal cost 
data unstacked in columns A and B and a set of bin numbers ap-
propriate for those data in column D. Then:
	 1.	 Select Data ➔ Data Analysis. In the Data Analysis dialog 
box, select Histogram from the Analysis Tools list and then 
click OK.
In the Histogram dialog box (shown below):
	 2.	 Enter A1:A51 as the Input Range and enter D1:D9 as the Bin 
Range. (If you leave Bin Range blank, the procedure creates 
a set of bins that will not be as well formed as the ones you can 
specify.)
	 3.	 Check Labels and click New Worksheet Ply.
	 4.	 Click OK to create the frequency distribution on a new 
­worksheet.
In the new worksheet:
	 5.	 Select row 1. Right-click this row and click Insert in the 
shortcut menu. Repeat. (This creates two blank rows at the top 
of the worksheet.)
	 6.	 Enter a title in cell A1.
The ToolPak creates a frequency distribution that contains an im-
proper bin labeled More. Correct this error by using these general 
instructions:
	 7.	 Manually add the frequency count of the More row to the 
frequency count of the preceding row. (For the example, the 
More row contains a zero for the frequency, so the frequency 
of the preceding row does not change.)
	 8.	 Select the worksheet row (for this example, row 12) that con-
tains the More row.
	 9.	 Right-click that row and click Delete in the shortcut menu.
Steps 1 through 9 construct a frequency distribution for the 
meal costs at city restaurants. To construct a frequency distribution 
for the meal costs at suburban restaurants, repeat these nine steps 
but in step 6 enter B1:B51 as the Input Range.
The Relative Frequency, Percentage,  
and Cumulative Distributions
Key Technique  Add columns that contain formulas for the 
relative frequency or percentage and cumulative percentage to a 
previously constructed frequency distribution.
Example  Create a distribution that includes the relative fre-
quency or percentage as well as the cumulative percentage infor-
mation found in Tables 2.11 (relative frequency and percentage) 
and 2.14 (cumulative percentage) in Section 2.2 for the restaurant 
meal cost data.
PHStat (untallied data)  Use Frequency Distribution.
For the example, use the PHStat instructions in “The Frequency 
Distribution” to construct a frequency distribution. Note that 
the frequency distribution constructed by PHStat also includes 
columns for the percentages and cumulative percentages. To 
change the column of percentages to a column of relative fre-
quencies, reformat that column. For the example, open to the 
new worksheet that contains the city restaurant frequency  
distribution and:
	 1.	 Select the cell range C4:C11, right-click, and select Format 
Cells from the shortcut menu.
	 2.	 In the Number tab of the Format Cells dialog box, select 
Number as the Category and click OK.
Then repeat these two steps for the new worksheet that contains 
the suburban restaurant frequency distribution.
In-Depth Excel (untallied data)  Use the Distributions 
workbook as a model.
For the example, first construct a frequency distribution  
created using the In-Depth Excel instructions in “The Frequency 

Distribution.” Open to the new worksheet that contains the  
frequency distribution for the city restaurants and:
	 1.	 Enter Percentage in cell C3 and Cumulative Pctage in  
cell D3.
	 2.	 Enter =B4,SUM(+B+4:+B+11) in cell C4 and copy this 
formula down through row 11.
	 3.	 Enter =C4 in cell D4.
	 4.	 Enter =C5 + D4 in cell D5 and copy this formula down 
through row 11.
	 5.	 Select the cell range C4:D11, right-click, and click Format 
Cells in the shortcut menu.
	 6.	 In the Number tab of the Format Cells dialog box, click Per-
centage in the Category list and click OK.
Then open to the worksheet that contains the frequency distribu-
tion for the suburban restaurants and repeat steps 1 through 6.
If you want column C to display relative frequencies instead 
of percentages, enter Rel. Frequencies in cell C3. Select the cell 
range C4:C12, right-click, and click Format Cells in the shortcut 
menu. In the Number tab of the Format Cells dialog box, click 
Number in the Category list and click OK.
Analysis ToolPak  Use Histogram and then modify the work-
sheet created.
For the example, first construct the frequency distributions 
using the Analysis ToolPak instructions in “The Frequency Distri-
bution.” Then use the In-Depth Excel instructions to modify those 
distributions.
EG2.3  Visualizing Categorical 
Variables
Many of the In-Depth Excel instructions in the rest of this Excel 
Guide refer to the following labeled Charts group illustration.
The Bar Chart and the Pie Chart
Key Technique  Use the Excel bar or pie chart feature. If the 
variable to be visualized is untallied, first construct a summary 
table (see the Section EG2.1 “The Summary Table” instructions).
Example  Construct a bar or pie chart from a summary table sim-
ilar to Table 2.3 on page 66.
PHStat  Use One-Way Tables & Charts.
For the example, use the Section EG2.1 “The Summary Table” 
PHStat instructions, but in step 3, check either Bar Chart or Pie 
Chart (or both) in addition to entering a Title, checking Percent-
age Column, and clicking OK.
In-Depth Excel  Use the Summary Table workbook as a 
model.
For the example, open to the OneWayTable worksheet of the 
Summary Table workbook. (The PivotTable in this worksheet 
was constructed using the Section EG2.1 “The Summary Table” 
instructions.) To construct a bar chart:
	 1.	 Select cell range A4:B6. (Begin your selection at cell B6 and 
not at cell A4, as you would normally do.)
	 2.	 In Excel 2013, select Insert, then the Bar icon in the Charts 
group (#2 in the Charts group illustration), and then select the 
first 2-D Bar gallery item (Clustered Bar). In other Excels, 
select Insert ➔ Bar and then select the first 2-D Bar gallery 
item (Clustered Bar).
	 3.	 Right-click the Risk drop-down button in the chart and click 
Hide All Field Buttons on Chart.
	 4.	 (Excel 2013) Select Design ➔ Add Chart Element ➔ Axis 
Titles ➔ Primary Horizontal. 
	
	 (Other Excels) Select Layout ➔ Axis Titles ➔ Primary Hori-
zontal Axis Title ➔ Title Below Axis. Select the words “Axis 
Title” in the chart and enter the title Frequency.
	 5.	 Relocate the chart to a chart sheet and turn off the chart  
legend and gridlines by using the instructions in Appendix 
Section B.6.
Although not the case with the example, sometimes the hori-
zontal axis scale of a bar chart will not begin at 0. If this occurs, 
right-click the horizontal (value) axis in the bar chart and click For-
mat Axis in the shortcut menu. In the Excel 2013 Format Axis task 
pane, click Axis Options. In the Axis Options, enter 0 in the Mini-
mum box and then close the pane. In other Excels, you set this 
value in the Format Axis dialog box. Click Axis Options in the left 
pane, and in the Axis Options right pane, click the first Fixed op-
tion button (for Minimum), enter 0 in its box, and then click Close.
To construct a pie chart, replace steps 2, 4, and 5 with these 
steps:
	 2.	 In Excel 2013, select Insert, then the Pie icon (#4 in the Step 
4 illustration), and then select the first 2-D Pie gallery item 
(Pie). In other Excels, select Insert ➔ Pie and then select the 
first 2-D Pie gallery item (Pie).
	 4.	 (Excel 2013) Select Design ➔ Add Chart Element ➔ Data 
Labels ➔ More Data Label Options. In the Format Data 
Labels task pane, click Label Options. In the Label Options, 
check Category Name and Percentage, clear the other Label 
Contains check boxes, and click Outside End. (To see the La-
bel Options, you may have to first click the chart [fourth] icon 
near the top of the task pane.) Then close the task pane. 
	
	 (Other Excels) Select Layout ➔ Data Labels ➔ More Data 
Label Options. In the Format Data Labels dialog box, click 
Label Options in the left pane. In the Label Options right pane, 
check Category Name and Percentage and clear the other Label 
Contains check boxes. Click Outside End and then click Close.
	 5.	 Relocate the chart to a chart sheet and turn off the chart legend 
and gridlines by using the instructions in Appendix Section B.6.
The Pareto Chart
Key Technique  Use the Excel chart feature with a modified 
summary table.
Example  Construct a Pareto chart of the incomplete ATM trans-
actions equivalent to Figure 2.4 on page 82.
	
Chapter 2 Excel Guide	
117

118	
Chapter 2  Organizing and Visualizing Variables
PHStat  Use One-Way Tables & Charts.
For the example, open to the DATA worksheet of the  
ATM Transactions workbook. Select PHStat ➔ Descriptive 
Statistics ➔ One-Way Tables & Charts. In the procedure’s  
dialog box:
	 1.	 Click Table of Frequencies (because the worksheet contains 
tallied data).
	 2.	 Enter A1:B8 as the Freq. Table Cell Range and check First 
cell contains label.
	 3.	 Enter a Title, check Pareto Chart, and click OK.
In-Depth Excel  Use the Pareto workbook as a model.
For the example, open to the ATMTable worksheet of the ATM 
Transactions workbook. Begin by sorting the modified table by 
decreasing order of frequency:
	 1.	 Select row 11 (the Total row), right-click, and click Hide in 
the shortcut menu. (This prevents the total row from getting 
sorted.)
	 2.	 Select cell B4 (the first frequency), right-click, and select Sort 
➔ Sort Largest to Smallest.
	 3.	 Select rows 10 and 12 (there is no row 11 visible), right-click, 
and click Unhide in the shortcut menu to restore row 11.
Next, add a column for cumulative percentage:
	 4.	 Enter Cumulative Pct. in cell D3. Enter =C4 in cell D4. 
Enter =D4 + C5 in cell D5 and copy this formula down 
through row 10.
	 5.	 Adjust the formatting of column D as necessary.
Next, create the Pareto chart:
	 6.	 Select the cell range A3:A10 and while holding down the Ctrl 
key also select the cell range C3:D10.
	 7.	 In Excel 2013, select Insert, then the Column icon (#1 in the 
illustration on page 117), and select the first 2-D Column 
gallery item (Clustered Column). In other Excels, select  
Insert ➔ Column and select the first 2-D Column gallery 
item (Clustered Column).
	 8.	 Select Format. In the Current Selection group, select Series 
“Cumulative Pct.” from the drop-down list and then click 
Format Selection.
	 9.	 (Excel 2013) In the Format Data Series task pane, click Series 
Options. In the Series Options, click Secondary Axis, and 
then close the task pane. (To see the Series Options, you may 
have to first click the chart [third] icon near the top of the task 
pane.)
	
	 (Other Excels) In the Format Data Series dialog box, click 
Series Options in the left pane, and in the Series Options 
right pane, click Secondary Axis. Click Close.
	10.	 With the cumulative percentage series still selected in the 
Current Selection group, select Design ➔ Change Chart 
Type. In Excel 2013, in the Change Chart Type dialog box, 
click Combo in the All Charts tab. In the Cumulative Pct. 
drop-down list, select the fourth Line gallery item (Line with 
Markers). Then, check Secondary Axis for the Cumulative 
Pct. and click OK. In other Excels, in the Change Chart Type 
dialog box, select the fourth Line gallery item (Line with 
Markers) and click OK.
Next, set the maximum value of the primary and secondary (left 
and right) Y axis scales to 100%. For each Y axis:
	11.	 Right-click on the axis and click Format Axis in the shortcut 
menu.
	12.	 (Excel 2013) In the Format Axis task pane, click Axis Op-
tions. In the Axis Options, enter 1 in the Maximum box. 
Click Tick Marks and select Outside from the Major type 
drop-down list. Then close the Format Axis pane. (To see the 
Axis Options, you may have to first click the chart [fourth] 
icon near the top of the task pane.)
	 	
(Other Excels) In the Format Axis dialog box, click Axis 
Options in the left pane, and in the Axis Options right pane, 
click the Fixed option button for Maximum, enter 1 in its 
box, and click Close.
	13.	 Relocate the chart to a chart sheet, turn off the chart legend 
and gridlines, and add chart and axis titles by using the in-
structions in Appendix Section B.6.
If you use a PivotTable as a summary table, replace steps 1 through 
6 with these steps:
  1.	 Add a percentage column in column C. (See the Section EG2.1 
“The Summary Table” instructions, Steps 10 through 13.)
  2.	 Add a cumulative percentage column in column D. Enter  
Cumulative Pctage in cell D3. Enter =C4 in cell D4. Enter 
=C5 + D4 in cell D5, and copy the formula down through 
all the rows in the PivotTable.
  3.	 Select the total row, right-click, and click Hide in the short-
cut menu. (This prevents the total row from getting sorted.)
  4.	 Right-click the cell that contains the first frequency (typi-
cally this will be cell B4).
  5.	 Right-click and select Sort ➔ Sort Largest to Smallest.
  6.	 Select the cell range of only the percentage and cumulative 
percentage columns (the equivalent of the cell range C3:D10 
in the example).
The Pareto chart constructed from a PivotTable using these mod-
ified steps will not have proper labels for the categories. To add 
the correct labels, right-click over the chart and click Select Data 
in the shortcut menu. In the Select Data Source dialog box, click 
Edit that appears under Horizontal (Category) Axis Labels. In 
the Axis Labels dialog box, drag the mouse to select the cell range 
(A4:A10 in the example) to enter that cell range. Do not type the 
cell range in the Axis label range box as you would otherwise do 
for the reasons explained in Appendix Section B.7. Click OK in 
this dialog box and then click OK in the original dialog box.
The Side-by-Side Chart
Key Technique  Use an Excel bar chart that is based on a con-
tingency table.
Example  Construct a side-by-side chart that displays the fund 
type and risk level, similar to Figure 2.6 on page 83.
PHStat  Use Two-Way Tables & Charts.
For the example, use the Section EG2.1 “The Contingency Table” 
PHStat instructions, but in step 4, check Side-by-Side Bar Chart 
in addition to entering a Title and clicking OK.

In-Depth Excel  Use the Contingency Table workbook as a 
model.
For the example, open to the TwoWayTable worksheet of the 
Contingency Table workbook and:
	 1.	 Select cell A3 (or any other cell inside the PivotTable).
	 2.	 Select Insert ➔ Bar and select the first 2-D Bar gallery item 
(Clustered Bar).
	 3.	 Right-click the Risk drop-down button in the chart and click 
Hide All Field Buttons on Chart.
	 4.	 Relocate the chart to a chart sheet, turn off the gridlines, and 
add chart and axis titles by using the instructions in Appendix 
Section B.6.
When creating a chart from a contingency table that is not a 
PivotTable, select the cell range of the contingency table, includ-
ing row and column headings, but excluding the total row and total 
column, as step 1.
If you need to switch the row and column variables in a side-
by-side chart, right-click the chart and then click Select Data in 
the shortcut menu. In the Select Data Source dialog box, click 
Switch Row/Column and then click OK. (In Excel 2007, if the 
chart is based on a PivotTable, the Switch Row/Column as that 
button will be disabled. In that case, you need to change the Pivot-
Table to change the chart.)
EG2.4  Visualizing Numerical 
Variables
The Stem-and-Leaf Display
Key Technique  Enter leaves as a string of digits that begin with 
the ’ (apostrophe) character.
Example  Construct a stem-and-leaf display of the one-year re-
turn percentage for the value retirement funds, similar to Figure 
2.7 on page 86.
PHStat  Use the Stem-and-Leaf Display.
For the example, open to the UNSTACKED worksheet of the  
Retirement Funds workbook. Select PHStat ➔ Descriptive 
Statistics ➔ Stem-and-Leaf Display. In the procedure’s dialog 
box (shown in the next column):
	 1.	 Enter B1:B90 as the Variable Cell Range and check First 
cell contains label.
	 2.	 Click Set stem unit as and enter 10 in its box.
	 3.	 Enter a Title and click OK.
When creating other displays, use the Set stem unit as option 
sparingly and only if Autocalculate stem unit creates a display 
that has too few or too many stems. (Any stem unit you specify 
must be a power of 10.)
In-Depth Excel  Use the Stem-and-leaf workbook as a model.
Manually construct the stems and leaves on a new worksheet to 
create a stem-and-leaf display. Adjust the column width of the col-
umn that holds the leaves as necessary.
The Histogram
Key Technique  Modify an Excel column chart.
Example  Construct histograms for the one-year return percent-
ages for the growth and value retirement funds, similar to Figure 2.9  
on page 88.
PHStat  Use Histogram & Polygons.
For the example, open to the DATA worksheet of the Retirement 
Funds workbook. Select PHStat ➔ Descriptive Statistics ➔ 
­Histogram & Polygons. In the procedure’s dialog box (shown below):
	 1.	 Enter I1:I317 as the Variable Cell Range, P1:P12 as the 
Bins Cell Range, Q1:Q11 as the Midpoints Cell Range, and 
check First cell in each range contains label.
	 2.	 Click Multiple Groups - Stacked and enter C1:C317 as the 
Grouping Variable Cell Range. (In the DATA worksheet, the 
one-year return percentages for both types of retirement funds 
are stacked, or placed in a single column. The column C val-
ues allow PHStat to separate the returns for growth funds from 
the returns for the value funds.)
	 3.	 Enter a Title, check Histogram, and click OK.
PHStat inserts two new worksheets, each of which contains a fre-
quency distribution and a histogram. To relocate the histograms to 
their own chart sheets, use the instructions in Appendix Section B.6.
As explained in Section 2.2, you cannot define an explicit 
lower boundary for the first bin, so the first bin can never have a 
midpoint. Therefore, the Midpoints Cell Range you enter must 
have one fewer cell than the Bins Cell Range. PHStat associates 
the first midpoint with the second bin and uses--as the label for the 
first bin.
	
Chapter 2 Excel Guide	
119

120	
Chapter 2  Organizing and Visualizing Variables
The example uses the workaround discussed in “Classes and 
Excel Bins” in Section 2.2. When you use this workaround, the 
histogram bar labeled--will always be a zero bar. Appendix Sec-
tion B.8 explains how you can delete this unnecessary bar from 
the histogram, as was done for the examples shown in Section 2.4.
In-Depth Excel  Use the Histogram workbook as a model.
For the example, first construct frequency distributions for the 
growth and value funds. Open to the UNSTACKED worksheet 
of the Retirement Funds workbook. This worksheet contains the 
retirement funds data unstacked in columns A and B and a set of 
bin numbers and midpoints appropriate for those variables in col-
umns D and E. Then:
	 1.	 Right-click the UNSTACKED sheet tab and click Insert in 
the shortcut menu.
	 2.	 In the General tab of the Insert dialog box, click Worksheet 
and then click OK.
In the new worksheet.
	 3.	 Enter a title in cell A1, Bins in cell A3, Frequency in cell B3, 
and Midpoints in cell C3.
	 4.	 Copy the bin number list in the cell range D2:D12 of the  
UNSTACKED worksheet and paste this list into cell A4 of 
the new worksheet.
	 5.	 Enter '--in cell C4. Copy the midpoints list in the cell range 
E2:E11 of the UNSTACKED worksheet and paste this list 
into cell C5 of the new worksheet.
	 6.	 Select the cell range B4:B14 that will hold the array formula.
	 7.	 Type, but do not press the Enter or Tab key, the formula  
=FREQUENCY(UNSTACKED!$A$2:$A$228, $A$4: 
$A$14). Then, while holding down the Ctrl and Shift keys, 
press the Enter key to enter the array formula into the cell 
range B4:B14.
	 8.	 Adjust the worksheet formatting as necessary.
Steps 1 through 8 construct a frequency distribution for the 
growth retirement funds. To construct a frequency distribution for 
the value retirement funds, repeat steps 1 through 8 but in step 7  
type =FREQUENCY(UNSTACKED!$B$1:$B$90, $A$4: 
$A$14)  as the array formula.
Having constructed the two frequency distributions, continue 
by constructing the two histograms. Open to the worksheet that 
contains the frequency distribution for the growth funds and:
	 1.	 Select the cell range B3:B14 (the cell range of the 
­frequencies).
	 2.	 In Excel 2013, select Insert, then the Column icon in the 
Charts group (#3 in the illustration on page 117), and then se-
lect the first 2-D Column gallery item (Clustered Column). 
In other Excels, select Insert ➔ Column and select the first 
2-D Column gallery item (Clustered Column).
	 3.	 Right-click the chart and click Select Data in the shortcut menu.
In the Select Data Source dialog box:
	 4.	 Click Edit under the Horizontal (Categories) Axis Labels 
heading.
	 5.	 In the Axis Labels dialog box, drag the mouse to select the 
cell range C4:C14 (containing the midpoints) to enter that 
cell range. Do not type this cell range in the Axis label range 
box as you would otherwise do for the reasons explained in  
Appendix Section B.7. Click OK in this dialog box and then 
click OK (in the Select Data Source dialog box).
In the chart:
	 6.	 Right-click inside a bar and click Format Data Series in the 
shortcut menu.
	 7.	 (Excel 2013) In the Format Data Series task pane, click Series 
Options. In the Series Options, click Series Options, enter 0 
in the Gap Width box, and then close the task pane. (To see 
the Series Options, you may have to first click the chart [third] 
icon near the top of the task pane.)
	
	 (Other Excels) In the Format Data Series dialog box, click 
Series Options in the left pane, and in the Series Options right 
pane, change the Gap Width slider to No Gap. Click Close.
	 8.	 Relocate the chart to a chart sheet, turn off the chart legend 
and gridlines, add axis titles, and modify the chart title by us-
ing the instructions in Appendix Section B.6.
This example uses the workaround discussed in Section 2.2, 
“Classes and Excel Bins” on page 77. When you use this work-
around, the histogram bar labeled—will always be a zero bar. Ap-
pendix Section B.8 explains how you can delete this unnecessary 
bar from the histogram, as was done for the examples shown in 
Section 2.4.
Analysis ToolPak  Use Histogram.
For the example, open to the UNSTACKED worksheet of the Re-
tirement Funds workbook and:
	 1.	 Select Data ➔ Data Analysis. In the Data Analysis dialog 
box, select Histogram from the Analysis Tools list and then 
click OK.
In the Histogram dialog box:
	 2.	 Enter A1:A228 as the Input Range and enter D1:D12 as the 
Bin Range.
	 3.	 Check Labels, click New Worksheet Ply, and check Chart 
Output.
	 4.	 Click OK to create the frequency distribution and histogram 
on a new worksheet.
In the new worksheet:
	 5.	 Follow steps 5 through 9 of the Analysis ToolPak instructions 
in “The Frequency Distribution” in Section EG2.2.
These steps construct a frequency distribution and histogram 
for the growth funds. To construct a frequency distribution and 
histogram for the value funds, repeat the nine steps but in step 2 
enter B1:B90 as the Input Range. You will need to correct several 
formatting errors that Excel makes to the histograms it constructs. 
For each histogram:
	 1.	 Right-click inside a bar and click Format Data Series in the 
shortcut menu.
	 2.	 (Excel 2013) In the Format Data Series task pane, click Series 
Options. In the Series Options, click Series Options, enter 0 
in the Gap Width box, and then close the task pane. (To see 
the Series Options, you may have to first click the chart [third] 
icon near the top of the task pane.)

	
	 (Other Excels) In the Format Data Series dialog box, click 
Series Options in the left pane, and in the Series Options right 
pane, change the Gap Width slider to No Gap. Click Close.
Histogram bars are labeled by bin numbers. To change the labeling 
to midpoints, open to each of the new worksheets and:
	 3.	 Enter Midpoints in cell C1 and '-- in cell C2. Copy the cell 
range E2:E11 of the UNSTACKED worksheet and paste this 
list into cell C5 of the new worksheet.
	 4.	 Right-click the histogram and click Select Data.
	 5.	 In the Select Data Source dialog box, click Edit under the 
Horizontal (Categories) Axis Labels heading.
	 6.	 In the Axis Labels dialog box, drag the mouse to select the 
cell range C2:C12 to enter that cell range. Do not type this 
cell range in the Axis label range box as you would otherwise 
do for the reasons explained in Appendix Section B.7. Click 
OK in this dialog box and then click OK (in the Select Data 
Source dialog box).
	 7.	 Relocate the chart to a chart sheet, turn off the chart legend 
and modify the chart title by using the instructions in Appen-
dix Section B.6.
This example uses the workaround discussed in Section 2.2, 
“Classes and Excel Bins.” Appendix Section B.8 explains how you 
can delete this unnecessary bar from the histogram, as was done 
for the examples shown in Section 2.4.
The Percentage Polygon and the Cumulative 
Percentage Polygon (Ogive)
Key Technique  Modify an Excel line chart that is based on a 
frequency distribution.
Example  Construct percentage polygons and cumulative per-
centage polygons for the one-year return percentages for the growth 
and value retirement funds, similar to Figure 2.11 on page 89  
and equivalent to Figure 2.12 on page 90.
PHStat  Use Histogram & Polygons.
For the example, use the PHStat instructions for creating a histo-
gram on page 119 but in step 3 of those instructions, also check 
Percentage Polygon and Cumulative Percentage Polygon 
(Ogive) before clicking OK.
In-Depth Excel  Use the Polygons workbook as a model.
For the example, open to the UNSTACKED worksheet of the 
Retirement Funds workbook and follow steps 1 through 8 to 
construct a frequency distribution for the growth retirement funds.  
Repeat steps 1 through 8 but in step 7 type the array formula  
=FREQUENCY(UNSTACKED!$B$1:$B$90, $A$4: $A$14) to 
construct a frequency distribution for the value funds. Open to the 
worksheet that contains the growth funds frequency distribution and:
	 1.	 Select column C. Right-click and click Insert in the shortcut 
menu. Right-click and click Insert in the shortcut menu a sec-
ond time. (The worksheet contains new, blank columns C and D  
and the midpoints column is now column E.)
	 2.	 Enter Percentage in cell C3 and Cumulative Pctage. in cell D3.
	 3.	 Enter =B4/SUM(+B+4:+B+14) in cell C4 and copy this 
formula down through row 14.
	 4.	 Enter = C4 in cell D4.
	 5.	 Enter =C5 + D4 in cell D5 and copy this formula down 
through row 14.
	 6.	 Select the cell range C4:D14, right-click, and click Format 
Cells in the shortcut menu.
	 7.	 In the Number tab of the Format Cells dialog box, click Per-
centage in the Category list and click OK.
Open to the worksheet that contains the value funds frequency dis-
tribution and repeat steps 1 through 7. To construct the percentage 
polygons, open to the worksheet that contains the growth funds 
distribution and:
	 1.	 Select cell range C4:C14.
	 2.	 In Excel 2013, select Insert, then select the Line icon in the 
Charts group (#4 in the illustration on page 117), and then 
select the fourth 2-D Line gallery item (Line with Markers). 
In other Excels, select Insert ➔ Line and select the fourth 
2-D Line gallery item (Line with Markers).
	 3.	 Right-click the chart and click Select Data in the shortcut 
menu.
In the Select Data Source dialog box:
	 4.	 Click Edit under the Legend Entries (Series) heading. In the 
Edit Series dialog box, enter the formula ="Growth Funds" 
as the Series name and click OK.
	 5.	 Click Edit under the Horizontal (Categories) Axis Labels 
heading. In the Axis Labels dialog box, drag the mouse to 
select the cell range E4:E14 to enter that cell range. Do 
not type this cell range in the Axis label range box as you 
would otherwise do for the reasons explained in Appendix 
Section B.7.
	 6.	 Click OK in this dialog box and then click OK (in the Select 
Data Source dialog box).
Back in the chart:
	 7.	 Relocate the chart to a chart sheet, turn off the chart gridlines, 
add axis titles, and modify the chart title by using the instruc-
tions in Appendix Section B.6.
In the new chart sheet:
	 8.	 Right-click the chart and click Select Data in the shortcut 
menu.
	 9.	 In the Select Data Source dialog box, click Add.
In the Edit Series dialog box:
	10.	 Enter the formula ="Value Funds" as the Series name and 
press Tab.
	11.	 With the current value in Series values highlighted, click the 
worksheet tab for the worksheet that contains the value funds 
distribution.
	12.	 Drag the mouse to select the cell range C4:C14 to enter that 
cell range as the Series values. Do not type this cell range in 
the Series values box as you would otherwise do, for the rea-
sons explained in Appendix Section B.7.
	13.	 Click OK. Back in the Select Data Source dialog box, click 
OK.
	
Chapter 2 Excel Guide	
121

122	
Chapter 2  Organizing and Visualizing Variables
To construct the cumulative percentage polygons, open to the 
worksheet that contains the growth funds distribution and repeat 
steps 1 through 13 but replace steps 1, 5, and 12 with these steps:
	 1.	 Select the cell range D4:D14.
	 5.	 Click Edit under the Horizontal (Categories) Axis Labels 
heading. In the Axis Labels dialog box, drag the mouse to se-
lect the cell range A4:A14 to enter that cell range.
	12.	 Drag the mouse to select the cell range D4:D14 to enter that 
cell range as the Series values.
If the Y axis of the cumulative percentage polygon extends 
past 100%, right-click the axis and click Format Axis in the short-
cut menu. In the Excel 2013 Format Axis task pane, click Axis 
Options. In the Axis Options, enter 0 in the Minimum box and 
then close the pane. In other Excels, you set this value in the For-
mat Axis dialog box. Click Axis Options in the left pane, and in 
the Axis Options right pane, click the first Fixed option button (for 
Minimum), enter 0 in its box, and then click Close.
EG2.5  Visualizing Two Numerical 
Variables
The Scatter Plot
Key Technique  Use the Excel scatter chart.
Example  Construct a scatter plot of revenue and value for NBA 
teams, similar to Figure 2.14 on page 94.
PHStat  Use Scatter Plot.
For the example, open to the DATA worksheet of the  
NBAValues workbook. Select PHStat ➔ Descriptive Statistics ➔ 
Scatter Plot. In the procedure’s dialog box (shown below):
	 1.	 Enter D1:D31 as the Y Variable Cell Range.
	 2.	 Enter C1:C31 as the X Variable Cell Range.
	 3.	 Check First cells in each range contains label.
	 4.	 Enter a Title and click OK.
To add a superimposed line like the one shown in Figure 2.14, 
click the chart and use step 3 of the In-Depth Excel instructions.
In-Depth Excel  Use the Scatter Plot workbook as a model.
For the example, open to the DATA worksheet of the  
NBAValues workbook and:
	 1.	 Select the cell range C1:D31.
	 2.	 In Excel 2013, select Insert, then the Scatter (X,Y) icon in 
the Charts group (#5 in the illustration on page 117), and then  
select the first Scatter gallery item (Scatter). In other Excels, 
select Insert ➔ Scatter and select the first Scatter gallery 
item (Scatter with only Markers).
	 3.	 In Excel 2013, select Design ➔ Add Chart Element ➔ 
Trendline ➔ Linear. In other Excels, select Layout ➔ 
Trendline ➔ Linear Trendline.
	 4.	 Relocate the chart to a chart sheet, turn off the chart legend 
and gridlines, add axis titles, and modify the chart title by us-
ing the instructions in Appendix Section B.6.
When constructing Excel scatter charts with other variables, make 
sure that the X variable column precedes (is to the left of) the Y 
variable column. (If the worksheet is arranged Y then X, cut and 
paste so that the Y variable column appears to the right of the X 
variable column.)
The Time-Series Plot
Key Technique  Use the Excel scatter chart.
Example  Construct a time-series plot of movie revenue per year 
from 1995 to 2012, similar to Figure 2.15 on page 95.
In-Depth Excel  Use the Time Series workbook as a model.
For the example, open to the DATA worksheet of the Movie  
Revenues workbook and:
	 1.	 Select the cell range A1:B19.
	 2.	 In Excel 2013, select Insert, then select the Scatter (X, Y) 
icon in the Charts group (#5 in the illustration on page 117), 
and then select the fourth Scatter gallery item (Scatter with 
Straight Lines and Markers). In other Excels, select Insert ➔ 
Scatter and select the fourth Scatter gallery item (Scatter 
with Straight Lines and Markers).
	 3.	 Relocate the chart to a chart sheet, turn off the chart legend 
and gridlines, add axis titles, and modify the chart title by us-
ing the instructions in Appendix Section B.6.
When constructing time-series charts with other variables, make 
sure that the X variable column precedes (is to the left of) the  
Y variable column. (If the worksheet is arranged Y then X, cut 
and paste so that the Y variable column appears to the right of the  
X variable column.)
EG2.6  Organizing Many Categorical 
Variables
Multidimensional Contingency Tables
Key Technique  Use the Excel PivotTable feature.
Example  Construct a PivotTable showing percentage of overall 
total for fund type, risk, and market cap for the retirement funds 
sample, similar to the one shown at the right in Figure 2.16 on 
page 97.
In-Depth Excel  Use the MCT workbook as a model.
For the example, open to the DATA worksheet of the Retirement 
Funds workbook and:
	 1.	 Select Insert ➔ PivotTable.

In the Create PivotTable dialog box:
	 2.	 Click Select a table or range and enter A1:N317 as the  
Table/Range.
	 3.	 Click New Worksheet and then click OK.
Excel inserts a new worksheet and displays the PivotTable Field 
List pane. The worksheet contains a graphical representation of 
a PivotTable that will change as you work inside the PivotTable 
Field List (or PivotTable Fields) task pane. In that pane (partially 
shown in the next column):
	 4.	 Drag Type in the Choose fields to add to report box and 
drop it in the ROWS (or Row Labels) box.
	 5.	 Drag Market Cap in the Choose fields to add to report box 
and drop it in the ROWS (or Row Labels) box.
	 6.	 Drag Risk in the Choose fields to add to report box and drop 
it in the COLUMNS (or Column Labels) box.
	 7.	 Drag Type in the Choose fields to add to report box a second 
time and drop it in the Σ Values box. The dropped label 
changes to Count of Type.
	 8.	 Click (not right-click) the dropped label Count of Type and 
click Value Field Settings in the shortcut menu.
In the Value Field Settings dialog box:
	 9.	 Click the Show Values As tab and select % of Grand Total 
from the Show values as drop-down list (shown below).
	10.	 Click OK.
In the PivotTable:
	11.	 Enter a title in cell A1.
	12.	 Enter a space character in cell A3 to replace the value 
“Count of Type.”
	13.	 Follow steps 8 and 9 of the In-Depth Excel “The Contin-
gency Table” instructions on page 117 to relocate the Low 
column from column D to column B.
If the PivotTable you construct does not contain a row and column 
for the grand totals as the PivotTables in Figure 2.21 contain, follow  
steps 10 through 15 of the In-Depth Excel, “The Contingency  
Table” instructions to include the grand totals.
C h a p t e r  2  M i n i ta b  G u i d e
MG2.1  Organizing Categorical 
Variables
The Summary Table
Use Tally Individual Variables to create a summary table. For  
example, to create a summary table similar to Table 2.3 on page 66,  
open to the Retirement Funds worksheet. Select Stat ➔ 
Tables ➔ Tally Individual Variables. In the procedure’s dialog 
box (shown at right):
	 1.	 Double-click C8  Risk in the variables list to add Risk to the 
Variables box.
	 2.	 Check Counts and Percents.
	 3.	 Click OK.
	
Chapter 2 Minitab Guide	
123

124	
Chapter 2  Organizing and Visualizing Variables
The Contingency Table
Use Cross Tabulation and Chi-Square to create a contingency 
table. For example, to create a contingency table similar to Table 
2.4 on page 67, open to the Retirement Funds worksheet. Select 
Stat ➔ Tables ➔ Cross Tabulation and Chi-Square. In the pro-
cedure’s dialog box (shown below):
	 1.	 Enter Type in the For rows box.
	 2.	 Enter Risk in the For columns box
	 3.	 Check Counts.
	 4.	 Click OK.
To create the other types of contingency tables shown in Tables 2.5 
through 2.7, check Row percents, Column percents, or Total 
percents, respectively, in step 3.
MG2.2  Organizing Numerical 
Variables
Stacked and Unstacked Data
Use Stack or Unstack Columns to rearrange data. For example, to 
unstack the 1YrReturn% variable in column C9 of the Retirement 
Funds worksheet by fund type, open to that worksheet. Select Data 
➔ Unstack Columns. In the procedure’s dialog box (shown below):
	 1.	 Double-click C9  1YrReturn% in the variables list to add 
'1YrReturn%' to the Unstack the data in box and press Tab.
	 2.	 Double-click C3  Type in the variables list to add Type to the 
Using subscripts in box.
	 3.	 Click After last column in use.
	 4.	 Check Name the columns containing the unstacked data.
	 5.	 Check OK.
Minitab inserts two new columns, 1YrReturn%_Growth and 
1YrReturn%_Value, the names of which you can edit.
To stack columns, select Data ➔ Stack ➔ Columns. In the 
Stack Columns dialog box, add the names of columns that contain 
the data to be stacked to the Stack the following columns box and 
then click either New worksheet or Column of current work-
sheet as the place to store the stacked data.
The Ordered Array
Use Sort to create an ordered array. Select Data ➔ Sort and in the 
Sort dialog box (not shown), double-click a column name in the 
variables list to add it to the Sort column(s) box and then press 
Tab. Double-click the same column name in the variables list to 
add it to the first By column box. Click either New worksheet, 
Original column(s), or Column(s) of current worksheet. (If 
you choose the third option, also enter the name of the column in 
which to place the ordered data in the box). Click OK.
The Frequency Distribution
There are no Minitab commands that use classes that you specify 
to create frequency distributions of the type seen in Tables 2.9 
through 2.12. (See also “The Histogram” in Section MG2.4.)
MG2.3  Visualizing Categorical 
Variables
The Bar Chart and the Pie Chart
Use Bar Chart to create a bar chart from a summary table and 
use Pie Chart to create a pie chart from a summary table. For 
example, to create the Figure 2.2 bar chart on page 66, open to the 
Retirement Funds worksheet. Select Graph ➔ Bar Chart. In 
the procedure’s dialog box (shown below):
	 1.	 Select Counts of unique values from the Bars represent 
drop-down list.
	 2.	 In the gallery of choices, click Simple.
	 3.	 Click OK.
In the Bar Chart - Counts of unique values, Simple dialog box 
(shown at top of left column on next page):
	 4.	 Double-click C8  Risk in the variables list to add Risk to 
Categorical variables.
	 5.	 Click OK.

If your data are in the form of a table of frequencies, select Values 
from a table from the Bars represent drop-down list in step 1. 
With this selection, clicking OK in step 3 will display the “Bar 
Chart - Values from a table, One column of values, Simple” dialog 
box. In this dialog box, you enter the columns to be graphed in 
the Graph variables box and, optionally, enter the column in the 
worksheet that holds the categories for the table in the Categorical  
variable box.
Use Pie Chart to create a pie chart from a summary table. 
For example, to create the Figure 2.3 pie chart on page 66, open to 
the Retirement Funds worksheet. Select Graph ➔ Pie Chart. In 
the Pie Chart dialog box (shown below):
	 1.	 Click Chart counts of unique values and then press Tab.
	 2.	 Double-click C8  Risk in the variables list to add Risk to Cat-
egorical variables.
	 3.	 Click Labels.
In the Pie Chart - Labels dialog box (shown at top of next column):
	 4.	 Click the Slice Labels tab.
	 5.	 Check Category name and Percent.
	 6.	 Click OK to return to the original dialog box.
Back in the original Pie Chart dialog box:
	 7.	 Click OK.
The Pareto Chart
Use Pareto Chart to create a Pareto chart. For example, to create 
the Figure 2.4 Pareto chart on page 82, open to the ATM Transac-
tions worksheet. Select Stat ➔ Quality Tools ➔ Pareto Chart. 
In the procedure’s dialog box (shown below):
	 1.	 Double-click C1  Cause in the variables list to add Cause to 
the Defects or attribute data in box.
	 2.	 Double-click C2  Frequency in the variables list to add Fre-
quency to the Frequencies in box.
	 3.	 Click Do not combine.
	 4.	 Click OK.
The Side-by-Side Chart
Use Bar Chart to create a side-by-side chart. For example, to 
create the Figure 2.6 side-by-side chart on page 83, open to the  
Retirement Funds worksheet. Select Graph ➔ Bar Chart.  
In the Bar Charts dialog box:
	 1.	 Select Counts of unique values from the Bars represent 
drop-down list.
	 2.	 In the gallery of choices, click Cluster.
	 3.	 Click OK.
	
Chapter 2 Minitab Guide	
125

126	
Chapter 2  Organizing and Visualizing Variables
In the “Bar Chart - Counts of unique values, Cluster” dialog box 
(shown below):
	 4.	 Double-click C3  Type and C8  Risk in the variables list to 
add Type and Risk to the Categorical variables (2–4, outer-
most first) box.
	 5.	 Click OK.
MG2.4  Visualizing Numerical 
Variables
The Stem-and-Leaf Display
Use Stem-and-Leaf to create a stem-and-leaf display. For  
example, to create the Figure 2.7 stem-and-leaf display on  
page 86, open to the Unstacked1YrReturn Funds worksheet. 
Select Graph ➔ Stem-and-Leaf. In the procedure’s dialog box 
(shown below):
	 1.	 Double-click C2  1YrReturn%_Value in the variables list to 
add '1YrReturn%_Value' in the Graph variables box.
	 2.	 Click OK.
The Histogram
Use Histogram to create a histogram. For example, to create the 
pair of histograms shown in Figure 2.9 on page 88, open to the 
Retirement Funds worksheet. Select Graph ➔ Histogram. 
In the Histograms dialog box (shown at top of next column):
	 1.	 Click Simple and then click OK.
In the Histogram - Simple dialog box (shown below):
	 2.	 Double-click C9  1YrReturn% in the variables list to add 
'1YrReturn%' in the Graph variables box.
	 3.	 Click Multiple Graphs.
In the Histogram - Multiple Graphs dialog box:
	 4.	 In the Multiple Variables tab (not shown), click On separate 
graphs and then click the By Variables tab.
	 5.	 In the By Variables tab (shown below), enter Type in the By 
variables in groups on separate graphs box.
	 6.	 Click OK.

Back in the Histogram - Simple dialog box:
	 7.	 Click OK.
The histograms created use classes that differ from the classes 
used in Figure 2.9 (and in Table 2.10 on page 72) and do not use 
the midpoints shown in Figure 2.9. To better match the histograms 
shown in Figure 2.9, for each histogram:
	 8.	 Right-click the X axis and then click Edit X Scale from the 
shortcut menu.
In the Edit Scale dialog box:
	 9.	 Click the Binning tab (shown below). Click Cutpoint (as the 
Interval Type) and Midpoint/Cutpoint positions and enter 
-15 -10 -5 0 5 10 15 20 25 30 35 in the box (with a space after 
each value).
	10.	 Click the Scale tab (shown below). Click Position of ticks and 
enter -12.5 -7.5 -2.5 2.5 7.5 12.5 17.5 22.5 27.5 32.5 in the box 
(with a space after each value).
	11.	 Click OK.
To create the histogram of the one-year return percentage 
variable for all funds in the retirement fund sample, repeat steps 
1 through 11, but in step 5 delete Type from the By variables in 
groups on separate graphs box.
To modify the histogram bars, double-click over the histogram 
bars and make the appropriate entries and selections in the Edit 
Bars dialog box. To modify an axis, double-click the axis and make 
the appropriate entries and selections in the Edit Scale dialog box.
The Percentage Polygon
Use Histogram to create a percentage polygon. For example, to 
create the pair of percentage polygons shown in Figure 2.11 on 
page 89, open to the Unstacked 1YrReturn worksheet. Select 
Graph ➔ Histogram. In the Histograms dialog box:
	 1.	 Click Simple and then click OK.
In the Histogram - Simple dialog box:
	 2.	 Double-click C1  1YrReturn%_Growth in the variables list 
to add '1YrReturn%_Growth' in the Graph variables box.
	 3.	 Double-click C2  1YrReturn%_Value in the variables list 
to add '1YrReturn%_Value' in the Graph variables box.
	 4.	 Click Scale.
In the Histogram - Scale dialog box:
	 5.	 Click the Y-Scale Type tab. Click Percent, clear Accumu-
late values across bins, and then click OK.
Back again in the Histogram - Simple dialog box:
	 6.	 Click Data View.
In the Histogram - Data View dialog box:
	 7.	 Click the Data Display tab. Check Symbols and clear all of 
the other check boxes.
	 8.	 Click the Smoother tab and then click Lowness and enter 0 
as the Degree of smoothing and 1 as the Number of steps.
	 9.	 Click OK.
Back again in the Histogram - Simple dialog box:
	10.	 Click OK to create the polygons.
The percentage polygons created do not use the classes and mid-
points shown in Figure 2.11. To better match the polygons shown 
in Figure 2.11:
	11.	 Right-click the X axis and then click Edit X Scale from the 
shortcut menu.
In the Edit Scale dialog box:
	12.	 Click the Binning tab. Click Cutpoint as the Interval Type 
and Midpoint/Cutpoint positions and enter -15 -10 -5 0 5 
10 15 20 25 30 35 in the box (with a space after each value).
	13.	 Click the Scale tab. Click Position of ticks and enter -12.5 
-7.5 -2.5 2.5 7.5 12.5 17.5 22.5 27.5 32.5 in the box (with a 
space after each value).
	14.	 Click OK.
The Cumulative Percentage Polygon (Ogive)
Modify the “The Percentage Polygon” instructions to create a cu-
mulative percentage polygon. Replace steps 5 and 12 with the fol-
lowing steps:
	 5.	 Click the Y-Scale Type tab. Click Percent, check Accumu-
late values across bins, and then click OK.
	12.	 Click the Binning tab. Click Midpoint as the Interval Type 
and Midpoint/Cutpoint positions and enter -15 -10 -5 0 5 
10 15 20 25 30 35 in the box (with a space after each value).
	
Chapter 2 Minitab Guide	
127

128	
Chapter 2  Organizing and Visualizing Variables
MG2.5  Visualizing Two Numerical 
Variables
The Scatter Plot
Use Scatterplot to create a scatter plot. For example, to create a 
scatter plot similar to the one shown in Figure 2.14 on page 94, 
open to the NBAValues worksheet. Select Graph ➔ Scatterplot. 
In the Scatterplots dialog box:
	 1.	 Click With Regression and then click OK.
In the Scatterplot - With Regression dialog box (shown below):
	 2.	 Double-click C4  Current Value in the variables list to enter  
'Current Value' in the row 1 Y variables cell.
	 3.	 Enter Revenue in the row 1 X variables cell.
	 4.	 Click OK.
The Time-Series Plot
Use Time Series Plot to create a time-series plot. For example, 
to create the Figure 2.15 time-series plot on page 95, open to the 
Movie Revenues worksheet and select Graph ➔ Time Series 
Plot. In the Time Series Plots dialog box:
	 1.	 Click Simple and then click OK.
In the Time Series Plot - Simple dialog box (shown below):
	 2.	 Double-click C2  Revenues in the variables list to add  
Revenues in the Series box.
	 3.	 Click Time/Scale.
In the Time Series Plot - Time/Scale dialog box (shown below):
	 4.	 Click Stamp and then press Tab.
	 5.	 Double-click C1  Year in the variables list to add Year in the 
Stamp columns (1-3, innermost first) box.
	 6.	 Click OK.
Back in the Time Series Plot - Simple dialog box:
	 7.	 Click OK.
MG2.6  Organizing Many 
Categorical Variables
Multidimensional Contingency Tables
Use Cross Tabulation and Chi-Square to create a multidimen-
sional contingency table. For example, to create a table similar to 
the Figure 2.16 fund type, market cap, and risk table on page 97, 
open to the Retirement Funds worksheet. Select Stat ➔ Tables 
➔ Cross Tabulation and Chi-Square. In the procedure’s dialog 
box:
	 1.	 Double-click C3  Type in the variables list to add Type to the 
For rows box.
	 2.	 Double-click C2  Market Cap in the variables list to add 
'Market Cap' to the For rows box and then press Tab.
	 3.	 Double-click C8  Risk in the variables list to add Risk to the 
For columns box.
	 4.	 Check Counts.
	 5.	 Click OK.
To display the cell values as percentages, as was done in  
Figure 2.1, check Total percents instead of Counts in step 4.

129
Chapter
3
Numerical Descriptive 
Measures
contents
3.1	 Central Tendency
3.2	 Variation and Shape
Visual Explorations: 
Exploring Descriptive Statistics
3.3	 Exploring Numerical Data
3.4	 Numerical Descriptive 
Measures for a Population
3.5	 The Covariance and the 
Coefficient of Correlation
3.6	 Descriptive Statistics: Pitfalls 
and Ethical Issues
Using Statistics: More 
Descriptive Choices, Revisited
Chapter 3 Excel Guide
Chapter 3 Minitab Guide
Objectives
To describe the properties of 
central tendency, variation, and 
shape in numerical data
To construct and interpret a boxplot
To compute descriptive summary 
measures for a population
To compute the covariance and 
the coefficient of correlation
U s i n g  S tat i s t i c s
More Descriptive Choices
As a member of a Choice Is Yours investment service task force, you helped 
organize and visualize the variables found in a sample of 316 retirement funds. 
Now, several weeks later, prospective clients are asking for more information on 
which they can base their investment decisions. In particular, they would like to 
able to compare the results of an individual retirement fund to the results of simi-
lar funds.
For example, while the earlier work your team did shows how the one-year 
return percentages are distributed, prospective clients would like to know how 
the value for a particular mid-cap growth fund compares to the one-year returns 
of all mid-cap growth funds. They also seek to understand how the values for the 
variables collected vary. Are all the values relatively similar? And does any vari-
able have outlier values that are either extremely small or extremely large?
While doing a complete search of the retirement funds data could lead to 
answers to the preceding questions, you wonder if there are easier ways than 
extensive searching to uncover those answers. You also wonder if there are other 
ways of being more descriptive about the sample of funds—providing answers 
to questions not yet raised by prospective clients. If you can help the Choice Is 
Yours investment service provide such answers, prospective clients will be better 
able to evaluate the retirement funds that your firm features.
Baranq/Shutterstock

130	
Chapter 3  Numerical Descriptive Measures
T
he prospective clients in the More Descriptive Choices scenario have begun asking 
questions about numerical variables such as the one-year return percentage. When 
summarizing and describing numerical variables, the organizing and visualizing 
methods discussed in Chapter 2 are only a starting point. You also need to describe such vari-
ables in terms of their central tendency, variation, and shape.
Central tendency is the extent to which the values of a numerical variable group around 
a typical, or central, value. Variation measures the amount of dispersion, or scattering, away 
from a central value that the values of a numerical variable show. The shape of a variable is the 
pattern of the distribution of values from the lowest value to the highest value.
This chapter discusses ways you can compute these numerical descriptive measures as 
you begin to analyze your data within the DCOVA framework. The chapter also talks about the 
covariance and the coefficient of correlation, measures that can help show the strength of the 
association between two numerical variables. Computing the descriptive measures discussed 
in this chapter would be one way to help prospective clients of the Choice Is Yours service find 
the answers they seek.
Most variables show a distinct tendency to group around a central value. When people talk 
about an “average value” or the “middle value” or the “most frequent value,” they are talking 
informally about the mean, median, and mode—three measures of central tendency.
The Mean
The arithmetic mean (typically referred to as the mean) is the most common measure of cen-
tral tendency. The mean can suggest a typical or central value and serves as a “balance point” 
in a set of data, similar to the fulcrum on a seesaw. The mean is the only common measure in 
which all the values play an equal role. You compute the mean by adding together all the val-
ues and then dividing that sum by the number of values in the data set.
The symbol X, called X-bar, is used to represent the mean of a sample. For a sample con-
taining n values, the equation for the mean of a sample is written as
X = sum of the values
number of values
Using the series X1, X2, c, Xn to represent the set of n values and n to represent the number 
of values in the sample, the equation becomes
X = X1 + X2 + g + Xn
n
By using summation notation (discussed fully in Appendix A), you replace the numerator 
X1 + X2 + g + Xn with the term a
n
i = 1
 Xi, which means sum all the Xi values from the first 
X value, X1, to the last X value, Xn, to form Equation (3.1), a formal definition of the sample mean.
3.1  Central Tendency
Sample Mean
The sample mean is the sum of the values in a sample divided by the number of values in 
the sample:
	
X =
a
n
i=1
 Xi
n
	
(3.1)

	
3.1  Central Tendency	
131
Because all the values play an equal role, a mean is greatly affected by any value that is 
very different from the others. When you have such extreme values, you should avoid using the 
mean as a measure of central tendency.
For example, if you knew the typical time it takes you to get ready in the morning, you 
might be able to arrive at your first destination every day in a more timely manner. Using the 
DCOVA framework, you first define the time to get ready as the time from when you get out of 
bed to when you leave your home, rounded to the nearest minute. Then, you collect the times 
for 10 consecutive workdays and organize and store them in  Times .
Using the collected data, you compute the mean to discover the “typical” time it takes for 
you to get ready. For these data:
Day:
1
2
3
4
5
6
7
8
9
10
Time (minutes):
39
29
43
52
39
44
40
31
44
35
the mean time is 39.6 minutes, computed as follows:
  X = sum of the values
number of values
  X =
a
n
i = 1
 Xi
n
  X = 39 + 29 + 43 + 52 + 39 + 44 + 40 + 31 + 44 + 35
10
     = 396
10 = 39.6
Even though no individual day in the sample had a value of 39.6 minutes, allotting this amount 
of time to get ready in the morning would be a reasonable decision to make. The mean is a 
good measure of central tendency in this case because the data set does not contain any excep-
tionally small or large values.
To illustrate how the mean can be greatly affected by any value that is very different from 
the others, imagine that on Day 3, a set of unusual circumstances delayed you getting ready 
by an extra hour, so that the time for that day was 103 minutes. This extreme value causes the 
mean to rise to 45.6 minutes, as follows:
  X = sum of the values
number of values
  X =
a
n
i = 1
 Xi
n
  X = 39 + 29 + 103 + 52 + 39 + 44 + 40 + 31 + 44 + 35
10
  X = 456
10 = 45.6
	
where
 X = sample mean
  n = number of values or sample size
 Xi = ith value of the variable X
 a
n
i=1
 Xi =  summation of all Xi values in the sample

132	
Chapter 3  Numerical Descriptive Measures
The one extreme value has increased the mean by 6 minutes. The extreme value also moved 
the position of the mean relative to all the values. The original mean, 39.6 minutes, had a 
middle, or central, position among the data values: 5 of the times were less than that mean and 
5 were greater than that mean. In contrast, the mean using the extreme value is greater than 9 
of the 10 times, making the new mean a poor measure of central tendency.
Example 3.1
The Mean Calories 
in Cereals
Nutritional data about a sample of seven breakfast cereals (stored in  Cereals ) includes the 
number of calories per serving:
Cereal
Calories
Kellogg’s All Bran
  80
Kellogg’s Corn Flakes
100
Wheaties
100
Nature’s Path Organic Multigrain Flakes
110
Kellogg’s Rice Krispies
130
Post Shredded Wheat Vanilla Almond
190
Kellogg’s Mini Wheats
200
Compute the mean number of calories in these breakfast cereals.
Solution  The mean number of calories is 130, computed as follows:
  X = sum of the values
number of values
 X =
a
n
i = 1
 Xi
n
 = 910
7
= 130
The Median
The median is the middle value in an ordered array of data that has been ranked from smallest 
to largest. Half the values are smaller than or equal to the median, and half the values are larger 
than or equal to the median. The median is not affected by extreme values, so you can use the 
median when extreme values are present.
To compute the median for a set of data, you first rank the values from smallest to largest 
and then use Equation (3.2) to compute the rank of the value that is the median.
Median
	
Median = n + 1
2
 ranked value	
(3.2)
You compute the median by following one of two rules:
 • Rule 1   If the data set contains an odd number of values, the median is the measurement 
associated with the middle-ranked value.
 • Rule 2  If the data set contains an even number of values, the median is the measurement 
associated with the average of the two middle-ranked values.

	
3.1  Central Tendency	
133
To further analyze the sample of 10 times to get ready in the morning, you can compute 
the median. To do so, you rank the daily times as follows:
Ranked values:
29
31
35
39
39
40
43
44
44
52
Ranks:
1
2
3
4
5
6
7
8
9
10
c
Median = 39.5
Because the result of dividing n + 1 by 2 for this sample of 10 is 110 + 12>2 = 5.5, you 
must use Rule 2 and average the measurements associated with the fifth and sixth ranked val-
ues, 39 and 40. Therefore, the median is 39.5. The median of 39.5 means that for half the days, 
the time to get ready is less than or equal to 39.5 minutes, and for half the days, the time to 
get ready is greater than or equal to 39.5 minutes. In this case, the median time to get ready of  
39.5 minutes is very close to the mean time to get ready of 39.6 minutes.
Student Tip
Remember that you  
must rank the values in 
order from the smallest 
to the largest to compute 
the median.
The Mode
The mode is the value that appears most frequently. Like the median and unlike the mean, ex-
treme values do not affect the mode. For a particular variable, there can be several modes or no 
mode at all. For example, for the sample of 10 times to get ready in the morning:
29 31 35 39 39 40 43 44 44 52
there are two modes, 39 minutes and 44 minutes, because each of these values occurs twice. 
However, for this sample of 7 smartphone prices offered by a cellphone provider (stored in 
 Smartphones ):
20 80 150 200 230 280 370
there is no mode. None of the values is “most typical” because each value appears the same 
number of times (once) in the data set.
Example 3.2
Computing the  
Median from an 
Odd-Sized Sample
Nutritional data about a sample of seven breakfast cereals (stored in  Cereals ) includes the 
number of calories per serving (see Example 3.1 on page 132). Compute the median number of 
calories in breakfast cereals.
Solution  Because the result of dividing n + 1 by 2 for this sample of seven is 
17 + 12 >2 = 4, using Rule 1, the median is the measurement associated with the fourth-
ranked value. The number of calories per serving values are ranked from the smallest to the 
largest:
Ranked values:
80
100
100
110
130
190
200
Ranks:
1
2
3
4
5
6
7
c
Median = 110
The median number of calories is 110. Half the breakfast cereals have equal to or less than 110 
calories per serving, and half the breakfast cereals have equal to or more than 110 calories.
Example 3.3
Determining the 
Mode
A systems manager in charge of a company’s network keeps track of the number of server 
failures that occur in a day. Determine the mode for the following data, which represent the 
number of server failures per a day for the past two weeks:
1 3 0 3 26 2 7 4 0 2 3 3 6 3
(continued)

134	
Chapter 3  Numerical Descriptive Measures
The Geometric Mean
When you want to measure the rate of change of a variable over time, you need to use the geo-
metric mean instead of the arithmetic mean. Equation (3.3) defines the geometric mean.
Solution  The ordered array for these data is
0 0 1 2 2 3 3 3 3 3 4 6 7 26
Because 3 occurs five times, more times than any other value, the mode is 3. Thus, the systems 
manager can say that the most common occurrence is having three server failures in a day. For 
this data set, the median is also equal to 3, and the mean is equal to 4.5. The value 26 is an ex-
treme value. For these data, the median and the mode are better measures of central tendency 
than the mean.
Geometric Mean
The geometric mean is the nth root of the product of n values:
	
XG = 1X1 * X2 * g * Xn21>n	
(3.3)
Geometric Mean Rate of Return
	
RG = 311 + R12 * 11 + R22 * g * 11 + Rn241>n - 1	
(3.4)
The geometric mean rate of return measures the average percentage return of an invest-
ment per time period. Equation (3.4) defines the geometric mean rate of return.
where
Ri = rate of return in time period i
To illustrate these measures, consider an investment of $100,000 that declined to a value 
of $50,000 at the end of Year 1 and then rebounded back to its original $100,000 value at the 
end of Year 2. The rate of return for this investment per year for the two-year period is 0 be-
cause the starting and ending value of the investment is unchanged. However, the arithmetic 
mean of the yearly rates of return of this investment is
X = 1-0.502 + 11.002
2
= 0.25 or 25,
because the rate of return for Year 1 is
R1 = a 50,000 - 10,000
100,000
b = -0.50 or -50%
and the rate of return for Year 2 is
R2 = a 10,000 - 50,000
50,000
b = 1.00 or 100%

	
3.2  Variation and Shape	
135
Using Equation (3.4), the geometric mean rate of return per year for the two years is
 RG = 311 + R12 * 11 + R2241>n - 1
 = 311 + 1-0.5022 * 11 + 11.02241>2 - 1
 = 310.502 * 12.0241>2 - 1
 = 31.041>2 - 1
 = 1 - 1 = 0
Using the geometric mean rate of return more accurately reflects the (zero) change in the value 
of the investment per year for the two-year period than does the arithmetic mean.
Example 3.4
Computing the 
Geometric Mean 
Rate of Return
The percentage change in the Russell 2000 Index of the stock prices of 2,000 small companies 
was -5.5% in 2011 and 14.6% in 2012. Compute the geometric rate of return.
Solution  Using Equation (3.4), the geometric mean rate of return in the Russell 2000 
Index for the two years is
 RG = 311 + R12 * 11 + R2241>n - 1
 = 311 + 1-0.05522 * 11 + 10.1462241>2 - 1
 = 310.9452 * 11.146241>2 - 1
 = 11.0829721>2 - 1
 = 1.0407 - 1 = 0.0407
The geometric mean rate of return in the Russell 2000 Index for the two years is 4.07% per year.
3.2  Variation and Shape
In addition to central tendency, every variable can be characterized by its variation and shape. 
Variation measures the spread, or dispersion, of the values. One simple measure of variation 
is the range, the difference between the largest and smallest values. More commonly used in 
statistics are the standard deviation and variance, two measures explained later in this section. 
The shape of a variable represents a pattern of all the values, from the lowest to highest value. 
As you will learn later in this section, many variables have a pattern that looks approximately 
like a bell, with a peak of values somewhere in the middle.
The Range
The range is the difference between the largest and smallest value and is the simplest descrip-
tive measure of variation for a numerical variable.
Range
The range is equal to the largest value minus the smallest value.
	
range = Xlargest - Xsmallest	
(3.5)
To further analyze the sample of 10 times to get ready in the morning, you can compute 
the range. To do so, you rank the data from smallest to largest:
29 31 35 39 39 40 43 44 44 52

136	
Chapter 3  Numerical Descriptive Measures
Using Equation (3.5), the range is 52 - 29 = 23 minutes. The range of 23 minutes indi-
cates that the largest difference between any two days in the time to get ready in the morning 
is 23 minutes.
Example 3.5
Computing  
the Range in the  
Calories in Cereals
Nutritional data about a sample of seven breakfast cereals (stored in  Cereals ) includes the 
number of calories per serving (see Example 3.1 on page 132). Compute the range of the num-
ber of calories for the cereals.
Solution  Ranked from smallest to largest, the calories for the seven cereals are
80 100 100 110 130 190 200
Therefore, using Equation (3.5), the range = 200 - 80 = 120. The largest difference in the 
number of calories between any two cereals is 120.
The range measures the total spread in the set of data. Although the range is a simple 
measure of the total variation of the variable, it does not take into account how the values are 
distributed between the smallest and largest values. In other words, the range does not indicate 
whether the values are evenly distributed, clustered near the middle, or clustered near one or 
both extremes. Thus, using the range as a measure of variation when at least one value is an 
extreme value is misleading.
The Variance and the Standard Deviation
Being a simple measure of variation, the range does not consider how the values distribute or 
cluster between the extremes. Two commonly used measures of variation that account for how 
all the values are distributed are the variance and the standard deviation. These statistics 
measure the “average” scatter around the mean—how larger values fluctuate above it and how 
smaller values fluctuate below it.
A simple measure of variation around the mean might take the difference between each 
value and the mean and then sum these differences. However, if you did that, you would find 
that these differences sum to zero because the mean is the balance point for every numerical 
variable. A measure of variation that differs from one data set to another squares the difference 
between each value and the mean and then sums these squared differences. The sum of these 
squared differences, known as the sum of squares (SS), is then used to compute the sample 
variance 1S22 and the sample standard deviation (S ).
The sample variance (S2 ) is the sum of squares divided by the sample size minus 1. The 
sample standard deviation (S) is the square root of the sample variance. Because this sum of 
squares will always be nonnegative according to the rules of algebra, neither the variance nor 
the standard deviation can ever be negative. For virtually all variables, the variance and stan-
dard deviation will be a positive value. Both of these statistics will be zero only if every value 
in the sample is the same value (i.e., the values show no variation).
For a sample containing n values, X1, X2, X3, c, Xn, the sample variance 1S22 is
S2 = 1X1 - X22 + 1X2 - X22 + g + 1Xn - X22
n - 1
Equations (3.6) and (3.7) define the sample variance and sample standard deviation using 
summation notation. The term a
n
i = 1
1Xi - X22 represents the sum of squares.

	
3.2  Variation and Shape	
137
Note that in both equations, the sum of squares is divided by the sample size minus 1, n - 1. 
The value is used for reasons having to do with statistical inference and the properties of sam-
pling distributions, a topic discussed in Section 7.2 on page 279. For now, observe that the differ-
ence between dividing by n and by n - 1 becomes smaller as the sample size increases.
In practice, you will most likely use the sample standard deviation as the measure of varia-
tion. Unlike the sample variance, a squared quantity, the standard deviation will always be a 
number expressed in the same units as the original sample data. For almost all sets of data, the 
majority of the values in a sample will be within an interval of plus and minus 1 standard devi-
ation above and below the mean. Therefore, knowledge of the mean and the standard deviation 
usually helps define where at least the majority of the data values are clustering.
To hand-compute the sample variance, S2, and the sample standard deviation, S:
	
1.	 Compute the difference between each value and the mean.
	
2.	 Square each difference.
	
3.	 Sum the squared differences.
	
4.	 Divide this total by n - 1 to compute the sample variance.
	
5.	 Take the square root of the sample variance to compute the sample standard deviation.
To further analyze the sample of 10 times to get ready in the morning, Table 3.1 shows the 
first four steps for calculating the variance and standard deviation with a mean 1X2 equal to 39.6. 
(Computing the mean is explained on page 131.) The second column of Table 3.1 shows step 1. 
The third column of Table 3.1 shows step 2. The sum of the squared differences (step 3) is shown at 
the bottom of Table 3.1. This total is then divided by 10 - 1 = 9 to compute the variance (step 4).
Sample Variance
The sample variance is the sum of the squared differences around the mean divided by the 
sample size minus 1:
	
S2 =
a
n
i = 1
1Xi - X22
n - 1
	
(3.6)
	
where
 X = sample mean
 n = sample size
 Xi = ith value of the variable X
 a
n
i = 1
1Xi - X22 = summation of all the squared differences between
the Xi values and X
Sample Standard Deviation
The sample standard deviation is the square root of the sum of the squared differences 
around the mean divided by the sample size minus 1:
	
S = 2S2 = H
a
n
i = 1
1Xi - X22
n - 1
	
(3.7)
Student Tip
Remember, neither the 
variance nor the standard 
deviation can ever be 
negative.

138	
Chapter 3  Numerical Descriptive Measures
You can also compute the variance by substituting values for the terms in Equation (3.6):
 S2 =
a
n
i = 1
1Xi - X22
n - 1
 = 139 - 39.622 + 129 - 39.622 + g + 135 - 39.622
10 - 1
 = 412.4
9
 = 45.82
Because the variance is in squared units (in squared minutes, for these data), to compute the 
standard deviation, you take the square root of the variance. Using Equation (3.7) on page 137,  
the sample standard deviation, S, is
S = 2S2 = H
a
n
i = 1
1Xi - X22
n - 1
= 245.82 = 6.77
This indicates that the getting-ready times in this sample are clustering within 6.77  
minutes around the mean of 39.6 minutes (i.e., clustering between X - 1S = 32.83 and 
X + 1S = 46.37). In fact, 7 out of 10 getting-ready times lie within this interval.
Using the second column of Table 3.1, you can also compute the sum of the differ-
ences between each value and the mean to be zero. For any set of data, this sum will always  
be zero:
a
n
i = 1
1Xi - X2 = 0 for all sets of data
This property is one of the reasons that the mean is used as the most common measure of cen-
tral tendency.
T a b l e  3 . 1
Computing the 
Variance of the 
Getting-Ready Times
Time (X)
Step 1: 1Xi - X2
Step 2: 1Xi - X22
n = 10
X = 39.6
39
-0.60
0.36
29
-10.60
112.36
43
3.40
11.56
52
12.40
153.76
39
-0.60
0.36
44
4.40
19.36
40
0.40
0.16
31
-8.60
73.96
44
4.40
19.36
35
-4.60
21.16
Step 3: Sum
412.40
Step 4: Divide by 1n - 12
45.82

	
3.2  Variation and Shape	
139
Example 3.6
Computing the 
Variance and  
Standard Deviation 
of the Number of 
Calories in Cereals
Nutritional data about a sample of seven breakfast cereals (stored in  Cereals ) includes the 
number of calories per serving (see Example 3.1 on page 132). Compute the variance and stan-
dard deviation of the calories in the cereals.
Solution  Table 3.2 illustrates the computation of the variance and standard deviation for 
the calories in the cereals.
T a b l e  3 . 2
Computing the 
Variance of  
the Calories  
in the Cereals
Calories
Step 1: 1Xi - X2
Step 2: 1Xi - X22
 n = 7
X = 130
  80
-50
  2,500
100
-30
     900
100
-30
     900
110
-20
     400
130
   0
         0
190
   60
  3,600
200
   70
  4,900
Step 3: Sum
13,200
Step 4: Divide by 1n - 12
  2,220
Using Equation (3.6) on page 137:
 S2 =
a
n
i = 1
1Xi - X22
n - 1
 = 180 - 13022 + 1100 - 13022 + g + 1200 - 13022
7 - 1
 = 13,200
6
 = 2,200
Using Equation (3.7) on page 137, the sample standard deviation, S, is
S = 2S2 = H
a
n
i = 1
1Xi - X22
n - 1
= 22,200 = 46.9042
The standard deviation of 46.9042 indicates that the calories in the cereals are cluster-
ing within {46.9042 around the mean of 130 (i.e., clustering between  X - 1S = 83.0958  
and  X + 1S = 176.9042). In fact, 57.1% (four out of seven) of the calories lie within this 
interval.

140	
Chapter 3  Numerical Descriptive Measures
The Coefficient of Variation
The coefficient of variation is equal to the standard deviation divided by the mean, multiplied 
by 100%. Unlike the measures of variation presented previously, the coefficient of variation 
(CV) measures the scatter in the data relative to the mean. The coefficient of variation is a  
relative measure of variation that is always expressed as a percentage rather than in terms of 
the units of the particular data. Equation (3.8) defines the coefficient of variation.
Student Tip
The coefficient of varia-
tion is always expressed 
as a percentage, not in 
the units of the variables.
Coefficient of Variation
The coefficient of variation is equal to the standard deviation divided by the mean, 
­multiplied by 100%.
	
CV = a S
Xb100%	
(3.8)
where
 S = sample standard deviation
 X = sample mean
For the sample of 10 getting-ready times, because X = 39.6 and S = 6.77, the coefficient of 
variation is
CV = a S
Xb100% = a 6.77
39.6b100% = 17.10%
For the getting-ready times, the standard deviation is 17.1% of the size of the mean.
The coefficient of variation is especially useful when comparing two or more sets of data 
that are measured in different units, as Example 3.7 illustrates.
Learn More
The Sharpe ratio, another 
relative measure of variation, 
is often used in financial 
analysis. Read the Short 
Takes for Chapter 3 to learn 
more about this ratio.
Example 3.7
Comparing Two 
Coefficients of 
Variation When the 
Two Variables Have 
Different Units of 
Measurement
Which varies more from cereal to cereal—the number of calories or the amount of sugar  
(in grams)?
Solution  Because calories and the amount of sugar have different units of measurement, 
you need to compare the relative variability in the two measurements.
For calories, using the mean and variance computed in Examples 3.1 and 3.6 on pages 132 
and 111, the coefficient of variation is
CVCalories = a46.9042
130
b100% = 36.08%
For the amount of sugar in grams, the values for the seven cereals are
6 2 4 4 4 11 10
For these data, X = 5.8571 and S = 3.3877. Therefore, the coefficient of variation is
CVSugar = a3.3877
5.8571b100% = 57.84%
You conclude that relative to the mean, the amount of sugar is much more variable than the 
calories.

	
3.2  Variation and Shape	
141
Z Scores
The Z score of a value is the difference between that value and the mean, divided by the stan-
dard deviation. A Z score of 0 indicates that the value is the same as the mean. If a Z score is 
a positive or negative number, it indicates whether the value is above or below the mean and 
by how many standard deviations.
Z scores help identify outliers, the values that seem excessively different from most of 
the rest of the values (see Section 1.3). Values that are very different from the mean will 
have either very small (negative) Z scores or very large (positive) Z scores. As a general rule,  
a Z score that is less than -3.0 or greater than +3.0 indicates an outlier value.
Z Score
The Z score for a value is equal to the difference between the value and the mean, divided 
by the standard deviation:
	
Z = X - X
S
	
(3.9)
To further analyze the sample of 10 times to get ready in the morning, you can compute 
the Z scores. Because the mean is 39.6 minutes, the standard deviation is 6.77 minutes, and the 
time to get ready on the first day is 39.0 minutes, you compute the Z score for Day 1 by using 
Equation (3.9):
 Z = X - X
S
 = 39.0 - 39.6
6.77
 = -0.09
The Z score of -0.09 for the first day indicates that the time to get ready on that day is very 
close to the mean. Table 3.3 presents the Z scores for all 10 days.
T a b l e  3 . 3
Z Scores for the 10 
Getting-Ready Times
Time (X)
Z Score
 X = 39.6
 S = 6.77
39
-0.09
29
-1.57
43
   0.50
52
   1.83
39
-0.09
44
   0.65
40
   0.06
31
-1.27
44
   0.65
35
-0.68
The largest Z score is 1.83 for Day 4, on which the time to get ready was 52 minutes. The 
lowest Z score is -1.57 for Day 2, on which the time to get ready was 29 minutes. Because 
none of the Z scores are less than -3.0 or greater then +3.0, you conclude that the getting-
ready times include no apparent outliers.

142	
Chapter 3  Numerical Descriptive Measures
Shape: Skewness and Kurtosis
The pattern to the distribution of values throughout the entire range of all the values is called 
the shape. The shape of the distribution of data values can be described by two statistics: 
skewness and kurtosis.
Skewness measures the extent to which the data values are not symmetrical around the 
mean. In a perfectly symmetrical distribution, the values below the mean are distributed in 
exactly the same way as the values above the mean, and the skewness is zero. In a skewed 
distribution, there is an imbalance of data values below and above the mean, and the skewness 
is a nonzero value. Figure 3.1 depicts the shape of the distribution of values for three variables, 
with the mean for each variable plotted as a dashed vertical line.
Example 3.8
Computing the  
Z Scores of the  
Number of Calories 
in Cereals
Nutritional data about a sample of seven breakfast cereals (stored in  Cereals ) includes the 
number of calories per serving (see Example 3.1 on page 132). Compute the Z scores of the 
calories in breakfast cereals.
Solution  Table 3.4 presents the Z scores of the calories for the cereals. The largest  
Z score is 1.49, for a cereal with 200 calories. The lowest Z score is -1.07, for a cereal with  
80 calories. There are no apparent outliers in these data because none of the Z scores are less 
than -3.0 or greater than +3.0.
T a b l e  3 . 4
Z Scores of the 
Number of Calories  
in Cereals
Calories
Z Scores
X = 130
S = 46.9042
  80
-1.07
100
-0.64
100
-0.64
110
-0.43
130
   0.00
190
   1.28
200
   1.49
Panel A
Negative, or left-skewed
Panel B
Symmetrical
Panel C
Positive, or right-skewed
F i g u r e  3 . 1
The shapes of three data 
distributions
In Panel A, the distribution of values is left-skewed. In this panel, most of the values are 
in the upper portion of the distribution. A long tail and distortion to the left is caused by some 
extremely small values. Because the skewness statistic for such a distribution will be less than 
zero, the term negative skew is also used to describe this distribution. These extremely small 
values pull the mean downward so that the mean is less than the median.
In Panel B, the distribution of values is symmetrical. The portion of the curve below the 
mean is the mirror image of the portion of the curve above the mean. There is no asymmetry of 
data values below and above the mean, the mean equals the median, and, as noted earlier, the 
skewness is zero.
In Panel C, the distribution of values is right-skewed. In this panel, most of the values are 
in the lower portion of the distribution. A long tail on the right is caused by some extremely 
large values. Because the skewness statistic for such a distribution will be greater than zero, the 
term positive skew is also used to describe this distribution. These extremely large values pull 
the mean upward so that the mean is greater than the median.

	
3.2  Variation and Shape	
143
The observations about the mean and median made when examining Figure 3.1 generally hold 
for most distributions of a continuous numerical variable. Summarized, these observations are:
 • Mean 6 median: negative, or left-skewed distribution
 • Mean = median: symmetrical distribution with zero skewness
 • Mean 7 median: positive, or right-skewed distribution
Kurtosis measures the extent to which values that are very different from the mean affect 
the shape of the distribution of a set of data. Kurtosis affects the peakedness of the curve of 
the distribution—that is, how sharply the curve rises approaching the center of the distribu-
tion. Kurtosis compares the shape of the peak to the shape of the peak of a normal distribution 
(discussed in Chapter 6), which, by definition, has a kurtosis of zero.1 A distribution that has a 
sharper-rising center peak than the peak of a normal distribution has positive kurtosis, a kurtosis  
value that is greater than zero, and is called lepokurtic. A distribution that has a slower- 
rising (flatter) center peak than the peak of a normal distribution has negative kurtosis, a kurto-
sis value that is less than zero, and is called platykurtic. A lepokurtic distribution has a higher 
concentration of values near the mean of the distribution compared to a normal distribution, 
while a platykurtic distribution has a lower concentration compared to a normal distribution.
In affecting the shape of the central peak, the relative concentration of values near the 
mean also affects the ends, or tails, of the curve of a distribution. A lepokurtic distribution has 
fatter tails, many more values in the tails, than a normal distribution has. If decision making 
about a set of data mistakenly assumes a normal distribution, when, in fact, the data forms a 
lepokurtic distribution, then that decision making will underestimate the occurrence of extreme 
values (values that are very different from the mean). Such an observation has been a basis for 
several explanations about the unanticipated reverses and collapses that financial markets have 
experienced in the recent past. (See reference 5 for an example of such an explanation.)
1Several different operational 
definitions exist for kurtosis. The 
definition here, used by both Excel 
and Minitab, is sometimes called  
excess kurtosis to distinguish it 
from other definitions. Read the 
Short Takes for Chapter 3 to learn 
how Excel calculates kurtosis (and 
skewness).
Example 3.9
Descriptive  
Statistics for 
Growth and Value 
Funds
In the More Descriptive Choices scenario, you are interested in comparing the past perfor-
mance of the growth and value funds from a sample of 316 funds. One measure of past perfor-
mance is the one-year return percentage variable. Compute descriptive statistics for the growth 
and value funds.
Solution  Figure 3.2 presents descriptive summary measures for the two types of funds. 
The results include the mean, median, mode, minimum, maximum, range, variance, standard 
deviation, coefficient of variation, skewness, kurtosis, count (the sample size), and standard  
error. The standard error, discussed in Section 7.2, is the standard deviation divided by the 
square root of the sample size.
F i g u r e  3 . 2
Excel and Minitab Descriptive statistics for the one-year return percentages for the growth and value funds
In examining the results, you see that there are some differences in the one-year return 
for the growth and value funds. The growth funds had a mean one-year return of 14.28 and a 
median return of 14.18. This compares to a mean of 14.70 and a median of 15.30 for the value 
(continued)

144	
Chapter 3  Numerical Descriptive Measures
funds. The medians indicate that half of the growth funds had one-year returns of 14.18 or 
better, and half the value funds had one-year returns of 15.30 or better. You conclude that the 
value funds had a slightly higher return than the growth funds.
The growth funds had a higher standard deviation than the value funds (5.0041, as 
compared to 4.4651). Both the growth funds and the value funds showed very little skew-
ness, as the skewness of the growth funds was 0.2039 and the skewness of the value funds 
was -0.2083. The kurtosis of the growth funds was very positive, indicating a distribution 
that was much more peaked than a normal distribution. The kurtosis of the value funds 
was slightly positive indicating a distribution that did not depart markedly from a normal 
distribution.
Example 3.10
Descriptive  
Statistics Using 
Multidimensional 
Contingency Tables
Continuing with the More Descriptive Choices scenario, you wish to explore the effects of 
each combination of type, market cap, and risk on measures of past performance. One measure 
of past performance is the three-year return percentage. Compute the mean three-year return 
percentage for each combination of type, market cap, and risk.
Solution  Compute the mean for each combination by adding the numerical variable 
three-year return percentage to a multidimensional contingency table, first introduced in  
Section 2.6 on page 97 as a way of summarizing many categorical variables. The Add  
Numerical Variable to MCT online topic discusses this technique, using this example to  
explain how to use Excel or Minitab instructions to construct such a table. Shown below are 
the Excel and Minitab tables for this example.
Analyzing each combination of type, market cap, and risk reveals patterns that would not 
be seen if the mean of the three-year return percentage had been computed for only the growth 
and value funds (similar to what is done in Example 3.9). Empty cells (Excel) and starred cells 
(Minitab), such as those for mid-cap growth funds with high risk, represent combinations that 
do not exist in the sample of 316 funds.
   

	
3.2  Variation and Shape	
145
Problems for Sections 3.1 and 3.2
Open the VE-Descriptive Statistics workbook to explore the  
effects of changing data values on measures of central tendency, 
variation, and shape. Change the data values in the cell range 
A2:A11 and then observe the changes to the statistics shown in 
the chart.
Click View the Suggested Activity Page to view a specific 
change you could make to the data values in column A. Click View 
the More About Descriptive Statistics Page to view summary 
definitions of the descriptive statistics shown in the chart. (See  
Appendix C to learn how you can download a copy of this  
workbook.)
V i s u a l  E x p l o r at i o n s   Exploring Descriptive Statistics
Learning the Basics
3.1  For a sample of data where n = 6 given below:
8 5 10 7 3 6
a.	 Calculate the mean, median, and mode.
b.	 Calculate the range, variance, standard deviation, and coeffi-
cient of variation.
c.	 Calculate the Z score. Find the outliers, if any.
d.	 Describe the shape for the given set of data.
3.2  For a sample of data where n = 7 given below:
8 3 10 6 4 13 5
a.	 Calculate the mean, median, and mode.
b.	 Calculate the range, variance, standard deviation, and coeffi-
cient of variation.
c.	 Calculate the Z score. Find the outliers, if any.
d.	 Describe the shape for the given set of data.
3.3  For a sample of data where n = 8 given below:
14 6 3 10 1 8 4 7
a.	 Calculate the mean, median, and mode.
b.	 Calculate the range, variance, standard deviation, and coeffi-
cient of variation.
c.	 Calculate the Z score. Find the outliers if any.
d.	 Describe the shape for the given set of data.
3.4  For a sample of data where n = 4 given below:
-5 8 4 1
a.	 Calculate the mean, median, and mode.
b.	 Calculate the range, variance, standard deviation, and coeffi-
cient of variation.
c.	 Calculate the Z score. Find the outliers if any.
d.	 Describe the shape for the given set of data.
3.5  Assume that your investment with your chosen company 
stocks produced 12% return for the first year and 28% return on the 
second year of operations. Calculate the geometric rate of return 
per year. (Hint: 10% = 0.1, and the rate of return of 28% = 0.28  
for the purpose of all calculations)
3.6  Assume that one of the investments listed in your investment 
portfolio had a 22% return in the first year and a -28% return in 
the second. Calculate the geometric rate of return per year.
Applying the Concepts
3.7  Saveur, a gourmet food, wine, and travel magazine, reported 
the following summary for the household incomes of its two types 
of subscribers—the print reader and the digital reader.
Audience
Median
Saveur reader
$163,108
Saveur.com reader
    84,548
Source: Data extracted from Saveur 2013  
Advertising Media Kit, bit.ly/14tiv4P.
Interpret the median household income for the Saveur readers and 
the Saveur.com readers.

146	
Chapter 3  Numerical Descriptive Measures
3.8  The quality inspection team at a plant for medium size vehicles 
intends to compare the acceptable thickness of two types of brake 
pads. The expected thickness of the brake pads is 12 millimeters. A 
sample size of 6 brake pads of each of the two types was randomly 
selected, and the results showing the thickness of the brake pads 
were sorted in ascending order, as shown in the table below:
Type 1
Type 2
11 11 12 12 13 14
11 12 13 13 14 14 
a.	 Calculate the mean, median, standard deviation and range for 
both types of brake pads.
b.	 Decide whether Type 1 or Type 2 brake pads meet the expecta-
tions set at 12 mm.
c.	 If the last value for the thickness for Type 2 brake pads is set at  
24 mm, calculate the new values for parts (a) and (b) and ex-
plain the effect in the difference.
3.9  The average mileage in miles per gallon (mpg) of 15 different 
brands of snowmobiles with a 2000 cc engine is listed below:
14  15  12  11  16  14  15  13  12  11  13  17  12  15  14
a.	 Calculate the mean, median, and mode.
b.	 Calculate the variance, standard deviation, range, coefficient of 
variation, and Z score.
c.	  Indicate whether the data shows any kind of skewedness. If so, 
comment on the distribution.
If the values indicated above are increased by 3 mpg due to an 
enhanced design of a newly introduced fuel injection system, cal-
culate all the requested values in parts (a) through (c).
SELF 
Test 
3.10  The file  FastFood  contains the amount that a 
sample of 15 customers spent for spent on condiments 
($) while attending a movie at the local theatre.
6.42  5.28  4.82  5.49  7.34  8.41  8.20  7.82
4.95  4.80  5.42  9.41  8.45  7.65  5.25
a.	 Calculate the mean and median.
b.	 Calculate the variance, standard deviation, range, and coeffi-
cient of variation.
c.	 Indicate whether the data shows any kind of skewedness. If so, 
comment on the distribution.
d.	 State your conclusion about the data based on the calculated re-
sults in parts (a) through (c).
3.11  The file  Sedans  contains the overall miles per gallon 
(MPG) of 2013 midsized sedans:
38  26  30  26  25  27  22  27  39  24  24  26  25
23  25  26  31  26  37  22  29  25  33  21  21
Source: Data extracted from “Ratings,” Consumer Reports, April 
2013, pp. 30–31.
a.	 Compute the mean, median, and mode.
b.	 Compute the variance, standard deviation, range, coefficient of 
variation, and Z scores.
c.	 Are the data skewed? If so, how?
d.	 Compare the results of (a) through (c) to those of Problem 3.12 
(a) through (c) that refer to the miles per gallon of small SUVs.
3.12  The file  MHU  contains the overall gallons per kilometer 
(GPK) of a medium-sized mobile home unit.
35  30  37  35  34  35  35  42  40   
37  42  38  35  34  35  34  34
a.	 Calculate the mean, median, and mode. 
b.	 Calculate the variance, standard deviation, range, coefficient of 
variation, and Z score. 
c.	  Indicate whether the data shows any kind of skewedness. If so, 
comment on the distribution.
	If the values indicated above are increased by 3 gallons per 
­kilometer due to the enhanced design of a newly introduced fuel 
combustion system, calculate all the requested value in parts (a) 
through (c).
3.13  The file  AccountingPartners  contains the number of 
­partners in a cohort of rising accounting firms that have been 
tagged as “firms to watch.” The firms have the following numbers 
of partners:
25  18  23  16  16  14  22  17    9  32  22  12  18    9  25    9  28  14
22  18  22  16  12  31  17  12  14  16  30  12  12  10  24  12  11
Source: Data extracted from bit.ly/11asPHm.
a.	 Compute the mean, median, and mode.
b.	 Compute the variance, standard deviation, range, coefficient of 
variation, and Z scores. Are there any outliers? Explain.
c.	 Are the data skewed? If so, how?
d.	 Based on the results of (a) through (c), what conclusions can 
you reach concerning the number of partners in rising account-
ing firms?
3.14  The file  LBF  contains data about the amount of money ($) 
spent by students attending a local book fair.
42.56  32.09    5.37  19.51  32.54  42.69  51.63  30.14
39.47  30.52  38.16  19.35  27.14  53.46  40.12
a.	 Calculate the mean, median and mode.
b.	 Calculate the variance, standard deviation, range, coefficient of 
variation, and Z score. Indicate if there are any outliers. Comment.
c.	 Indicate whether the data shows any kind of skewedness. If so, 
comment on the distribution.
d.	 What conclusions can you draw about the amount of money 
spent according to the calculated values of parts (a) through (c)?
3.15  Is there a difference in the variation of the yields of differ-
ent types of investments? The file  CD Rate  contains the yields for 
one-year certificates of deposit (CDs) and five-year CDs for 23 
banks in the United States, as of March 20, 2013.
Source: Data extracted from www.Bankrate.com, March 20, 2013.
a.	 For one-year and five-year CDs, separately compute the vari-
ance, standard deviation, range, and coefficient of variation.
b.	 Based on the results of (a), do one-year CDs or five-year CDs 
have more variation in the yields offered? Explain.
3.16  The file  HorseRace  contains data about the amount of 
money ($) spent by a group of people at a horse race event.
170  172  168  156  159  143  124  156
a.	 Calculate the mean, median, and mode. 
b.	 Calculate the variance and standard deviation.
c.	 What conclusions can you draw about the amount of money 
spent during the horse race based on the calculated values in (a) 
through (b)?
d.	 Comment on the difference in solutions of (a) through (c) if 168 
is changed to 268.
3.17  A bank manager wants to investigate the wait time for 
customers at an ATM machine within a centrally located mall, 

	
3.2  Variation and Shape	
147
especially between 5:00 and 6:00 pm during weekends. Data col-
lected from a sample of 15 customers during this hour are stored 
in  Bank1 :
3.56  5.20  4.03  5.10  3.40  3.20  4.01  5.35   
4.52  4.68  5.23  4.15  3.45  4.00  5.31
a.	 Calculate the mean and median. 
b.	 Calculate the variance, standard deviation, range, coefficient of 
variation, and Z score. Indicate if there are any outliers. Comment.
c.	 Indicate whether the data shows any kind of skewedness. If so, 
comment on the distribution. 
d.	 A sign posted in front of the ATM machine indicates that the 
expected wait time for customers is less than four minutes. 
Based on this information and the calculated values in (a) 
through (c) does this statement hold true?
3.18  Suppose that another bank branch, located in a residential 
area, is also concerned with the noon-to-1:00 p.m. lunch hour. The 
waiting time, in minutes, collected from a sample of 15 customers 
during this hour, are stored in  Bank2 :
  9.66  5.90  8.02  5.79  8.73  3.82  8.01  8.35
10.49  6.68  5.64  4.08  6.17  9.91  5.47
a.	 Compute the mean and median.
b.	 Compute the variance, standard deviation, range, coefficient of 
variation, and Z scores. Are there any outliers? Explain.
c.	 Are the data skewed? If so, how?
d.	 As a customer walks into the branch office during the lunch 
hour, he asks the branch manager how long he can expect to 
wait. The branch manager replies, “Almost certainly less than 
five minutes.” On the basis of the results of (a) through (c), 
evaluate the accuracy of this statement.
3.19  General Electric (GE) is one of the world’s largest  
companies; it develops, manufactures, and markets a wide range 
of products, including medical diagnostic imaging devices, jet en-
gines, lighting products, and chemicals. In 2011, the stock price 
rose 1.4%, and in 2012, the stock price rose 17.2%.
Source: Data extracted from finance.yahoo.com, June 24, 2012.
a.	 Compute the geometric mean rate of return per year for the 
two-year period 2011–2012. (Hint: Denote an increase of 1.4% 
as R2 = 0.014.)
b.	 If you purchased $1,000 of GE stock at the start of 2011, what 
was its value at the end of 2012?
c.	 Compare the result of (b) to that of Problem 3.20 (b).
SELF 
Test 
3.20  TASER International, Inc., develops, manufac-
tures, and sells nonlethal self-defense devices known 
as Tasers and markets primarily to law enforcement, corrections 
institutions, and the military. TASER’s stock price in 2011 in-
creased by 2.4%, and in 2012, it increased by 74.6%.
Source: Data extracted from finance.yahoo.com, March 29, 2013.
a.	 Compute the geometric mean rate of return per year for the 
two-year period 2011–2012. (Hint: Denote an increase of 2.4% 
as R1 = 0.024.)
b.	 If you purchased $1,000 of TASER stock at the start of 2011, 
what was its value at the end of 2012?
c.	 Compare the result of (b) to that of Problem 3.19 (b).
3.21  The file  Indices  contains data that represent the yearly 
rate of return (in percentage) for the Dow Jones Industrial 
­Average (DJIA), the Standard & Poor’s 500 (S&P 500), and the 
­technology-heavy NASDAQ Composite (NASDAQ) from 2009 
through 2012. These data are:
Year
DJIA
S&P 500
NASDAQ
2012
  7.26
 13.41
15.91
2011
  5.50
0.00
-1.80
2010
11.00
 12.80
16.90
2009
18.80
 23.50
43.90
Source: Data extracted from finance.yahoo.com,  
March 29, 2013.
a.	 Compute the geometric mean rate of return per year for the 
DJIA, S&P 500, and NASDAQ from 2009 through 2012.
b.	 What conclusions can you reach concerning the geometric 
mean rates of return per year of the three market indices?
c.	 Compare the results of (b) to those of Problem 3.22 (b).
3.22  In 2009 through 2012, the value of precious metals fluctu-
ated dramatically. The data in the following table (contained in the 
file  Metals ) represent the yearly rate of return (in percentage) for 
platinum, gold, and silver from 2009 through 2012:
Year
Platinum
Gold
Silver
2012
     8.7
  0.1
  7.1
2011
-21.1
10.2
-9.8
2010
    21.5
29.8
83.7
2009
   55.9
23.9
49.3
Source: Data extracted from finance.yahoo.com, 
March 29, 2013.
a.	 Compute the geometric mean rate of return per year for plati-
num, gold, and silver from 2009 through 2012.
b.	 What conclusions can you reach concerning the geometric 
mean rates of return of the three precious metals?
c.	 Compare the results of (b) to those of Problem 3.21 (b).
3.23  Using the one-year return percentage variable in  
 Retirement Funds :
a.	 Construct a table that computes the mean for each combination 
of type, market cap, and risk.
b.	 Construct a table that computes the standard deviation for each 
combination of type, market cap, and risk.
c.	 What conclusions can you reach concerning differences among 
the types of retirement funds (growth and value), based on mar-
ket cap (small, mid-cap, and large) and the risk (low, average, 
and high)?
3.24  Using the one-year return percentage variable in  
 Retirement Funds :
a.	 Construct a table that computes the mean for each combination 
of type, market cap, and rating.
b.	 Construct a table that computes the standard deviation for each 
combination of type, market cap, and rating.
c.	 What conclusions can you reach concerning differences among 
the types of retirement funds (growth and value), based on mar-
ket cap (small, mid-cap, and large) and the rating (one, two, 
three, four, and five)?

148	
Chapter 3  Numerical Descriptive Measures
3.25  Using the one-year return percentage variable in  
 Retirement Funds :
a.	 Construct a table that computes the mean for each combination 
of market cap, risk, and rating.
b.	 Construct a table that computes the standard deviation for each 
combination of market cap, risk, and rating.
c.	 What conclusions can you reach concerning differences based 
on the market cap (small, mid-cap, and large), risk (low, aver-
age, and high), and rating (one, two, three, four, and five)?
3.26  Using the one-year return percentage variable in  
 Retirement Funds :
a.	 Construct a table that computes the mean for each combination 
of type, risk, and rating.
b.	 Construct a table that computes the standard deviation for each 
combination of type, risk, and rating.
c.	 What conclusions can you reach concerning differences among 
the types of retirement funds (growth and value), based on the 
risk (low, average, and high) and the rating (one, two, three, 
four, and five)?
3.3  Exploring Numerical Data
Sections 3.1 and 3.2 discuss measures of central tendency, variation, and shape. You can also 
visualize the distribution of the values for a numerical variable by computing the quartiles and 
the five-number summary and constructing a boxplot.
Quartiles
Quartiles split the values into four equal parts—the first quartile 1Q12  divides the 
smallest 25.0% of the values from the other 75.0% that are larger. The second quartile 
1Q22  is the median; 50.0% of the values are smaller than or equal to the median, and 
50.0% are larger than or equal to the median. The third quartile 1Q32  divides the small-
est 75.0% of the values from the largest 25.0%. Equations (3.10) and (3.11) define the 
first and third quartiles.
First Quartile, Q1
25.0% of the values are smaller than or equal to Q1, the first quartile, and 75.0% are  
larger than or equal to the first quartile, Q1:
	
Q1 = n + 1
4
 ranked value	
(3.10)
Third Quartile, Q3
75.0% of the values are smaller than or equal to the third quartile, Q3, and 25.0% are  
larger than or equal to the third quartile, Q3:
	
Q3 = 31n + 12
4
 ranked value	
(3.11)
Student Tip
The methods of this sec-
tion are commonly used 
in exploratory data 
analysis.
Student Tip
As is the case when you 
compute the median, 
you must rank the values 
in order from smallest to 
largest before computing 
the quartiles.
Use the following rules to compute the quartiles from a set of ranked values:
 • Rule 1  If the ranked value is a whole number, the quartile is equal to the measure-
ment that corresponds to that ranked value. For example, if the sample size n = 7, the 
first quartile, Q1, is equal to the measurement associated with the 17 + 12>4 = second 
ranked value.

	
3.3  Exploring Numerical Data	
149
 • Rule 2  If the ranked value is a fractional half (2.5, 4.5, etc.), the quartile is equal to 
the measurement that corresponds to the average of the measurements corresponding 
to the two ranked values involved. For example, if the sample size n = 9, the first 
quartile, Q1, is equal to the 19 + 12>4 = 2.5 ranked value, halfway between the sec-
ond ranked value and the third ranked value.
 • Rule 3  If the ranked value is neither a whole number nor a fractional half, you 
round the result to the nearest integer and select the measurement corresponding 
to that ranked value. For example, if the sample size n = 10, the first quartile, Q1, 
is equal to the 110 + 12>4 = 2.75 ranked value. Round 2.75 to 3 and use the third 
ranked value.
To further analyze the sample of 10 times to get ready in the morning, you can compute 
the quartiles. To do so, you rank the data from smallest to largest:
Ranked values:
29
31
35
39
39
40
43
44
44
52
Ranks:
1
2
3
4
5
6
7
8
9
10
The first quartile is the 1n + 12>4 = 110 + 12>4 = 2.75 ranked value. Using Rule 3, 
you round up to the third ranked value. The third ranked value for the getting-ready data is 
35 minutes. You interpret the first quartile of 35 to mean that on 25% of the days, the time to 
get ready is less than or equal to 35 minutes, and on 75% of the days, the time to get ready is 
greater than or equal to 35 minutes.
The third quartile is the 31n + 12>4 = 3110 + 12>4 = 8.25 ranked value. Using Rule 
3 for quartiles, you round this down to the eighth ranked value. The eighth ranked value is  
44 minutes. Thus, on 75% of the days, the time to get ready is less than or equal to 44 minutes, 
and on 25% of the days, the time to get ready is greater than or equal to 44 minutes.
Percentiles  Related to quartiles are percentiles that split a variable into 100 equal parts. 
By this definition, the first quartile is equivalent to the 25th percentile, the second quartile to 
the 50th percentile, and the third quartile to the 75th percentile. Learn more about percentiles 
in the Short Takes for Chapter 3.
Example 3.11
Computing the 
Quartiles
Nutritional data about a sample of seven breakfast cereals (stored in  Cereals ) includes the 
number of calories per serving (see Example 3.1 on page 132). Compute the first quartile 1Q12 
and third quartile 1Q32 of the number of calories for the cereals.
Solution  Ranked from smallest to largest, the numbers of calories for the seven cereals 
are as follows:
Ranked values:	
80  100  100  110  130  190  200
Ranks:	
1	
2	
3	
4	
5	
6	
7
For these data
 Q1 = 1n + 12
4
 ranked value
 = 7 + 1
4
 ranked value = 2nd ranked value
Therefore, using Rule 1, Q1 is the second ranked value. Because the second ranked value is 
100, the first quartile, Q1, is 100.
(continued)

150	
Chapter 3  Numerical Descriptive Measures
The Interquartile Range
The interquartile range (also called the midspread) measures the difference in the center of 
a distribution between the third and first quartiles.
To compute the third quartile, Q3,
 Q3 = 31n + 12
4
 ranked value
 = 317 + 12
4
 ranked value = 6th ranked value
Therefore, using Rule 1, Q3 is the sixth ranked value. Because the sixth ranked value is 190, 
Q3 is 190.
The first quartile of 100 indicates that 25% of the cereals contain 100 calories or fewer per 
serving and 75% contain 100 or more calories. The third quartile of 190 indicates that 75% of 
the cereals contain 190 calories or fewer per serving and 25% contain 190 or more calories.
Interquartile Range
The interquartile range is the difference between the third quartile and the first quartile:
	
Interquartile range = Q3 - Q1	
(3.12)
The interquartile range measures the spread in the middle 50% of the values. Therefore, it 
is not influenced by extreme values. To further analyze the sample of 10 times to get ready in 
the morning, you can compute the interquartile range. You first order the data as follows:
29 31 35 39 39 40 43 44 44 52
You use Equation (3.12) and the earlier results on page 149, Q1 = 35 and Q3 = 44:
Interquartile range = 44 - 35 = 9 minutes
Therefore, the interquartile range in the time to get ready is 9 minutes. The interval 35 to 44 is 
often referred to as the middle fifty.
Example 3.12
Computing the 
­Interquartile Range 
for the Number of 
Calories in Cereals
Nutritional data about a sample of seven breakfast cereals (stored in  Cereals ) includes the 
number of calories per serving (see Example 3.1 on page 132). Compute the interquartile 
range of the number of calories in cereals.
Solution  Ranked from smallest to largest, the numbers of calories for the seven cereals 
are as follows:
80 100 100 110 130 190 200
Using Equation (3.12) and the earlier results from Example 3.11 on page 149, Q1 = 100 and 
Q3 = 190:
Interquartile range = 190 - 100 = 90
Therefore, the interquartile range of the number of calories in cereals is 90 calories.

	
3.3  Exploring Numerical Data	
151
Because the interquartile range does not consider any value smaller than Q1 or larger than Q3, 
it cannot be affected by extreme values. Descriptive statistics such as the median, Q1, Q3, and the 
interquartile range, which are not influenced by extreme values, are called resistant measures.
The Five-Number Summary
The five-number summary for a variable consists of the smallest value 1Xsmallest2, the first 
quartile, the median, the third quartile, and the largest value 1Xlargest2.
Type of Distribution
Comparison
Left-Skewed
Symmetrical
Right-Skewed
The distance from 
Xsmallest to the median 
versus the distance 
from the median to 
Xlargest.
The distance from  
Xsmallest to the median is 
greater than the distance 
from the median to 
Xlargest.
The two distances  
are the same.
The distance from 
Xsmallest to the median 
is less than the 
distance from the 
median to Xlargest.
The distance from 
Xsmallest to Q1 versus 
the distance from Q3 
to Xlargest.
The distance from  
Xsmallest to Q1 is greater 
than the distance from Q3 
to Xlargest.
The two distances  
are the same.
The distance from 
Xsmallest to Q1 is less 
than the distance from 
Q3 to Xlargest.
The distance from Q1 
to the median versus 
the distance from the 
median to Q3.
The distance from Q1  
to the median is greater 
than the distance from  
the median to Q3.
The two distances  
are the same.
The distance from Q1 
to the median is less 
than the distance from 
the median to Q3.
T a b l e  3 . 5
Relationships Among 
the Five-Number 
Summary and the 
Type of Distribution
Five-Number Summary
Xsmallest Q1 Median Q3 Xlargest
To further analyze the sample of 10 times to get ready in the morning, you can compute 
the five-number summary. For these data, the smallest value is 29 minutes, and the largest 
value is 52 minutes (see page 133). Calculations done on pages 133 and 149 show that the 
median = 39.5, Q1 = 35, and Q3 = 44. Therefore, the five-number summary is as follows:
29 35 39.5 44 52
The distance from Xsmallest to the median 139.5 - 29 = 10.52 is slightly less than the 
distance from the median to Xlargest 152 - 39.5 = 12.52. The distance from Xsmallest to  
Q1 135 -29 = 62 is slightly less than the distance from Q3 to Xlargest 152 - 44 = 82. The dis-
tance from Q1 to the median 139.5 - 35 = 4.5) is the same as the distance from the ­median 
to Q3144 - 39.5 = 4.52. Therefore, the getting-ready times are slightly right-skewed.
Example 3.13
Computing the 
Five-Number 
­Summary of the 
Number of Calories 
in Cereals
Nutritional data about a sample of seven breakfast cereals (stored in  Cereals ) includes the 
number of calories per serving (see Example 3.1 on page 132). Compute the five-number sum-
mary of the number of calories in cereals.
Solution  From previous computations for the number of calories in cereals (see pages 
133 and 149–150), you know that the median = 110, Q1 = 100, and Q3 = 190.
The five-number summary provides a way to determine the shape of the distribution for a 
set of data. Table 3.5 explains how relationships among these five statistics help to identify the 
shape of the distribution.
(continued)

152	
Chapter 3  Numerical Descriptive Measures
The Boxplot
The boxplot uses a five-number summary to visualize the shape of the distribution for a vari-
able. Figure 3.3 contains a boxplot for the sample of 10 times to get ready in the morning.
In addition, the smallest value in the data set is 80, and the largest value is 200. Therefore, 
the five-number summary is as follows:
80 100 110 190 200
The three comparisons listed in Table 3.5 are used to evaluate skewness. The distance 
from Xsmallest to the median 1110 - 80 = 302 is less than the distance 1200 - 110 = 902 
from the median to Xlargest. The distance from Xsmallest to Q1 1100 - 80 = 202 is greater 
than the distance from Q3 to Xlargest 1200 - 190 = 102. The distance from Q1 to the median 
1110 - 100 = 102 is less than the distance from the median to Q3 1190 - 110 = 802. Two 
comparisons indicate a right-skewed distribution, whereas the other indicates a left-skewed 
distribution. Therefore, given the small sample size and the conflicting results, the shape is not 
clearly determined.
F i g u r e  3 . 3
Boxplot for the  
getting-ready times
Xsmallest
Xlargest
Q1
Median
Q3
20
25
30
35
40
Time (minutes)
45
50
55
The vertical line drawn within the box represents the median. The vertical line at the left 
side of the box represents the location of Q1, and the vertical line at the right side of the box 
represents the location of Q3. Thus, the box contains the middle 50% of the values. The lower 
25% of the data are represented by a line connecting the left side of the box to the location of 
the smallest value, Xsmallest. Similarly, the upper 25% of the data are represented by a line con-
necting the right side of the box to Xlargest.
The Figure 3.3 boxplot for the getting-ready times shows a slight right-skewness: The 
distance between the median and the highest value is slightly greater than the distance between 
the lowest value and the median, and the right tail is slightly longer than the left tail.
Example 3.14
Boxplots of the 
One-Year Returns 
for the Growth and 
Value Funds
In the More Descriptive Choices scenario, you are interested in comparing the past perfor-
mance of the growth and value funds from a sample of 316 funds. One measure of past perfor-
mance is the one-year return percentage variable. Construct the boxplots for this variable for 
the growth and value funds.
Solution  Figure 3.4 contains the boxplots for the one-year return percentages for the 
growth and value funds. The five-number summary for the growth funds associated with 
these boxplots is Xsmallest =
- 11.28, Q1 = 11.78, median = 14.18, Q3 = 16.64, and 
Xlargest = 33.98. The five-number summary for the value funds associated with these boxplots 
is Xsmallest = 1.67, Q1 = 12.17, median = 15.3, Q3 = 17.23, and Xlargest = 28.27.

	
3.3  Exploring Numerical Data	
153
Figure 3.5 demonstrates the relationship between the boxplot and the density curve for 
four different types of distributions. The area under each density curve is split into quartiles 
corresponding to the five-number summary for the boxplot.
The distributions in Panels A and D of Figure 3.5 are symmetrical. In these distributions, 
the mean and median are equal. In addition, the length of the left tail is equal to the length of 
the right tail, and the median line divides the box in half.
F i g u r e  3 . 4
Excel and Minitab boxplots for the one-year return percentage variable
The lines, or whiskers, that 
extend from the Minitab 
central box each extend 
1.5 times the interquartile 
range from the box. Beyond 
these ranges are the values 
considered to be outliers, 
plotted as asterisks.
The median return, the quartiles, and the maximum returns are slightly higher for the value 
funds than for the growth funds. Both the growth and value funds are somewhat symmetrical, 
but the growth funds have a much larger range. These results are consistent with the statistics 
computed in Figure 3.2 on page 143.
Student Tip
A long tail on the left side 
of the boxplot indicates a 
left-skewed distribution. A 
long tail on the right side 
of the boxplot indicates a 
right-skewed distribution.
F i g u r e  3 . 5
Boxplots and corresponding density curves for four distributions
Panel A
Bell-shaped distribution
Panel B
Left-skewed distribution
Panel C
Right-skewed distribution
Panel D
Rectangular distribution
The distribution in Panel B of Figure 3.5 is left-skewed. The few small values distort 
the mean toward the left tail. For this left-skewed distribution, there is a heavy clustering of 
values at the high end of the scale (i.e., the right side); 75% of all values are found between 
the left edge of the box 1Q12 and the end of the right tail 1Xlargest2. There is a long left tail 
that contains the smallest 25% of the values, demonstrating the lack of symmetry in this 
data set.
The distribution in Panel C of Figure 3.5 is right-skewed. The concentration of values 
is on the low end of the scale (i.e., the left side of the boxplot). Here, 75% of all values are 
found between the beginning of the left tail and the right edge of the box 1Q32. There is a 
long right tail that contains the largest 25% of the values, demonstrating the lack of symmetry 
in this data set.

154	
Chapter 3  Numerical Descriptive Measures
Problems for Section 3.3
Learning the Basics
3.27   The following is a set of data from a sample of n = 7:
11  6  5  10  1  8  4
a.	 Calculate the first quartile 1Q12, the third quartile 1Q32, and the 
interquartile range.
b.	 Find and list the five-number summary.
c.	 Construct a boxplot and use it to describe the shape of the data.
d.	 If each of the data values above is increased by 2, describe the 
effect on the values in part (a), if any.
3.28  The following is a set of data from a sample of n = 6:
9  6  7  11  3  15
a.	 Calculate the first quartile 1Q12, the third quartile 1Q32, and the 
interquartile range.
b.	 Find and list the five-number summary.
c.	 Draw a boxplot and describe its shape.
d.	 If each of the data values above is decreased by 2, describe the 
effect on the values in part (a), if any.
3.29  The following is a set of data from a sample of n = 7:
8  5  8  10  2  13  12
a.	 Calculate the first quartile 1Q12, the third quartile 1Q32, and the 
interquartile range.
b.	 Find and list the five-number summary.
c.	 Construct a boxplot and describe its shape.
d.	 If each of the data values above is reduced by 2, describe the 
effect on the values in part (a), if any.
3.30  The following is a set of data from a sample of n = 5:
7 - 5 - 8 7 9
a.	 Compute the first quartile 1Q12, the third quartile 1Q32, and the 
interquartile range.
b.	 List the five-number summary.
c.	 Construct a boxplot and describe its shape.
d.	 Compare your answer in (c) with that from Problem 3.4 (d) on 
page 145. Discuss.
Applying the Concepts
3.31  The file  AccountingPartners  contains the number of part-
ners in a cohort of rising accounting firms that have been tagged as 
“firms to watch.” The firms have the following numbers of partners:
25  18  23  16  16  14   22  17    9  32  22  12  18    9  25     9  28  14
22  18  22  16  12  31  17  12  14  16  30  12  12  10  24  12  11
Source: Data extracted from bit.ly/11asPHm.
a.	 Compute the first quartile 1Q12, the third quartile 1Q32, and the 
interquartile range.
b.	 List the five-number summary.
c.	 Construct a boxplot and describe its shape.
3.32  The file  MarketPenetration  contains the market penetra-
tion value (i.e., the percentage of the country population that are 
users) for the 15 countries that lead the world in total number of 
Facebook users:
52.56 33.09  5.37 19.41 32.52 41.69 51.61 30.12
39.07 30.62 38.16 49.35 27.13 53.45 40.01
Source: Data extracted from www.socialbakers.com/facebook-statistics/.
a.	 Compute the first quartile 1Q12, the third quartile 1Q32, and the 
interquartile range.
b.	 List the five-number summary.
c.	 Construct a boxplot and describe its shape.
3.33  The following data set represents the average amount of 
money (€) a family spends on grocery shopping in the city of Lon-
don each week. 
278  269  258  264  258  235  450  141 
a.	 Calculate the first quartile 1Q12, the third quartile 1Q32, and the 
interquartile range.
b.	 Find and list the five-number summary.
c.	 Construct a boxplot and describe its shape.
3.34  The file  MHU  contains the overall gallons per kilometer 
(GPK) of a medium-sized mobile home unit.
35  30  37  35  34  35  35  42  40   
37  42  38  35  34  35  34  34
a.	 Calculate the first quartile 1Q12, the third quartile 1Q32, and the 
interquartile range.
b.	 Find and list the five-number summary.
c.	 Construct a boxplot and describe its shape.
3.35  The file  CD Rate  contains the yields for one-year CDs 
and five-year CDs, for 23 banks in the United States, as of March 
20, 2013.
Source: Data extracted from www.Bankrate.com, March 20, 2013.
For each type of account:
a.	 Compute the first quartile 1Q12, the third quartile 1Q32, and the 
interquartile range.
b.	 List the five-number summary.
c.	 Construct a boxplot and describe its shape.
3.36  A bank branch located in a commercial district of a city 
has the business objective of developing an improved process 
for serving customers during the noon-to-1:00 p.m. lunch period.  
The waiting time, in minutes, is defined as the time the customer 
enters the line to when he or she reaches the teller window. Data 
are collected from a sample of 15 customers during this hour. The 
file  Bank1  contains the results, which are listed below:
4.21 5.55 3.02 5.13 4.77 2.34 3.54 3.20
4.50 6.10 0.38 5.12 6.46 6.19 3.79

	
3.4  Numerical Descriptive Measures for a Population	
155
Population Mean
The population mean is the sum of the values in the population divided by the population size, N.
	
m =
a
N
i=1
 Xi
N
	
(3.13)
	
where
 m = population mean
 Xi = ith value of the variable X
 a
N
i=1
 Xi = summation of all Xi values in the population
 N = number of values in the population
Another bank branch, located in a residential area, is also con-
cerned with the noon-to-1:00 p.m. lunch hour. The waiting times, 
in minutes, collected from a sample of 15 customers during this 
hour, are contained in the file  Bank2  and listed here:
 9.66 5.90 8.02 5.79 8.73 3.82 8.01 8.35
10.49 6.68 5.64 4.08 6.17 9.91 5.47
a.	 List the five-number summaries of the waiting times at the two 
bank branches.
b.	 Construct boxplots and describe the shapes of the distributions 
for the two bank branches.
c.	 What similarities and differences are there in the distributions 
of the waiting times at the two bank branches?
3.4  Numerical Descriptive Measures for a Population
Sections 3.1 and 3.2 discuss the statistics that can be computed to describe the properties of 
central tendency and variation for a sample. When you collect data for an entire population 
(see Section 1.3), you compute and analyze population parameters for these properties, includ-
ing the population mean, population variance, and population standard deviation.
To help illustrate these parameters, consider the population of stocks for the 10 compa-
nies in the Dow Jones Industrial Average (DJIA) that form the “Dogs of the Dow,” defined 
as the 10 stocks in the DJIA whose dividend is the highest fraction of their price in the pre-
vious year. (These stocks are used in an alternative investment scheme popularized by Mi-
chael O’Higgins.) Table 3.6 contains the 2012 one-year returns (excluding dividends) for the 
10 “Dow Dog” stocks of 2011. These data, stored in  DowDogs , will be used to illustrate the 
population parameters discussed in this section.
The Population Mean
The population mean is the sum of the values in the population divided by the population 
size, N. This parameter, represented by the Greek lowercase letter mu, m, serves as a measure 
of central tendency. Equation (3.13) defines the population mean.
T a b l e  3 . 6
One-Year Return  
for the “Dogs of the 
Dow”
Stock
One-Year  
Return
Stock
One-Year  
Return
AT&T
11.5
DuPont
-1.8
Verizon
7.9
Johnson & Johnson
6.9
Merck
8.6
Intel
-14.9
Pfizer
15.9
Procter & Gamble
1.8
General Electric
17.2
Kraft Foods Group
11.5
Source: Data extracted from 1Stock1.com.

156	
Chapter 3  Numerical Descriptive Measures
To compute the mean one-year return for the population of “Dow Dog” stocks in Table 
3.6, use Equation (3.13):
m =
a
N
i = 1
 Xi
N
= 11.5 + 7.9 + 8.6 + 15.9 + 17.2 + 1-1.82 + 6.9 + 1-14.92 + 1.8 + 11.5
10
= 64.6
10
= 6.46
Thus, the mean one-year return for the “Dow Dog” stocks is 6.46.
The Population Variance and Standard Deviation
The population variance and the population standard deviation parameters measure variation 
in a population. The population variance is the sum of the squared differences around the 
population mean divided by the population size, N, and the population standard deviation is 
the square root of the population variance. In practice, you will most likely use the population 
standard deviation because, unlike the population variance, the standard deviation will always 
be a number expressed in the same units as the original population data.
The lowercase Greek letter sigma, s, represents the population standard deviation, and 
sigma squared, s2, represents the population variance. Equations (3.14) and (3.15) define 
these parameters. The denominators for the right-side terms in these equations use N and not 
the 1n - 12 term that is found in Equations (3.6) and (3.7) on page 137 that define the sample 
variance and standard deviation.
Population Variance
The population variance is the sum of the squared differences around the population mean 
divided by the population size, N:
	
s2 =
a
N
i=1
 1Xi - m22
N
	
(3.14)
	
where
 m = population mean
 Xi = ith value of the variable X
 a
N
i=1
 1Xi - m22 = summation of all the squared differences between the  
Xi values and m
Population Standard Deviation
	
s = R
a
N
i = 1
 1Xi - m22
N
	
(3.15)

	
3.4  Numerical Descriptive Measures for a Population	
157
To compute the population variance for the data of Table 3.6, you use Equation (3.14):
 s2 =
a
N
i=1
 1Xi - m22
N
=
25.4016 + 2.0736 + 4.5796 + 89.1136 + 115.3476 +
68.2276 + 0.1936 + 456.2496 + 21.7156 + 25.4016
10
 = 808.8304
10
= 80.8304
From Equation (3.15), the population sample standard deviation is
s = 2s2 = H
a
N
i = 1
 1Xi - m22
N
= A
808.3040
10
= 8.9906
Therefore, the typical percentage return differs from the mean of 6.46 by approximately 8.99. This 
large amount of variation suggests that the “Dow Dog” stocks produce results that differ greatly.
The Empirical Rule
In most data sets, a large portion of the values tend to cluster somewhere near the mean. In 
right-skewed data sets, this clustering occurs to the left of the mean—that is, at a value less 
than the mean. In left-skewed data sets, the values tend to cluster to the right of the mean—that 
is, greater than the mean. In symmetrical data sets, where the median and mean are the same, 
the values often tend to cluster around the median and mean, producing a normal distribution 
(discussed in Chapter 6).
The empirical rule states that for population data that form a normal distribution, the fol-
lowing are true:
 • Approximately 68% of the values are within {1 standard deviation from the mean.
 • Approximately 95% of the values are within {2 standard deviations from the mean.
 • Approximately 99.7% of the values are within {3 standard deviations from the mean.
The empirical rule helps you examine variability in a population as well as identify outli-
ers. The empirical rule implies that for normal distributions, only about 1 out of 20 values 
will be beyond 2 standard deviations from the mean in either direction. As a general rule, 
you can consider values not found in the interval m { 2s as potential outliers. The rule 
also implies that only about 3 in 1,000 will be beyond 3 standard deviations from the mean. 
Therefore, ­values not found in the interval m { 3s are almost always ­considered outliers.
Example 3.15
Using the Empirical 
Rule
A population of 2-liter bottles of cola is known to have a mean fill-weight of 2.06 liters and a 
standard deviation of 0.02 liter. The population is known to be bell-shaped. Describe the distri-
bution of fill-weights. Is it very likely that a bottle will contain less than 2 liters of cola?
Solution
 m { s = 2.06 { 0.02 = 12.04, 2.082
 m { 2s = 2.06 { 210.022 = 12.02, 2.102
 m { 3s = 2.06 { 310.022 = 12.00, 2.122
(continued)

158	
Chapter 3  Numerical Descriptive Measures
The Chebyshev Rule
For heavily skewed sets of data and data sets that do not appear to be normally distributed, you 
should use the Chebyshev rule instead of the empirical rule. The Chebyshev rule (see refer-
ence 2) states that for any data set, regardless of shape, the percentage of values that are found 
within distances of k standard deviations from the mean must be at least
a1 - 1
k2b * 100%
You can use this rule for any value of k greater than 1. For example, consider k = 2. The 
Chebyshev rule states that at least 31 - 11>2224 * 100% = 75% of the values must be found 
within {2 standard deviations of the mean.
The Chebyshev rule is very general and applies to any distribution. The rule indicates at 
least what percentage of the values fall within a given distance from the mean. However, if the 
data set is approximately bell-shaped, the empirical rule will more accurately reflect the greater 
concentration of data close to the mean. Table 3.7 compares the Chebyshev and empirical rules.
Using the empirical rule, you can see that approximately 68% of the bottles will contain be-
tween 2.04 and 2.08 liters, approximately 95% will contain between 2.02 and 2.10 liters, and 
approximately 99.7% will contain between 2.00 and 2.12 liters. Therefore, it is highly unlikely 
that a bottle will contain less than 2 liters.
% of Values Found in Intervals Around the Mean
Interval
Chebyshev  
(any distribution)
Empirical Rule  
(normal distribution)
1m - s, m + s2
At least 0%
Approximately 68%
1m - 2s, m + 2s2
At least 75%
Approximately 95%
1m - 3s, m + 3s2
At least 88.89%
Approximately 99.7%
T a b l e  3 . 7
How Data Vary 
Around the Mean
Section EG3.4 describes the 
VE-Variability workbook 
that allows you to use Excel 
to explore the empirical and 
Chebyshev rules.
Example 3.16
Using the  
Chebyshev Rule
As in Example 3.15, a population of 2-liter bottles of cola is known to have a mean fill-weight 
of 2.06 liter and a standard deviation of 0.02 liter. However, the shape of the population is un-
known, and you cannot assume that it is bell-shaped. Describe the distribution of fill-weights. 
Is it very likely that a bottle will contain less than 2 liters of cola?
Solution
 m { s = 2.06 { 0.02 = 12.04, 2.082
 m { 2s = 2.06 { 210.022 = 12.02, 2.102
 m { 3s = 2.06 { 310.022 = 12.00, 2.122
Because the distribution may be skewed, you cannot use the empirical rule. Using the Cheby-
shev rule, you cannot say anything about the percentage of bottles containing between 2.04 
and 2.08 liters. You can state that at least 75% of the bottles will contain between 2.02 and 2.10 
liters and at least 88.89% will contain between 2.00 and 2.12 liters. Therefore, between 0 and 
11.11% of the bottles will contain less than 2 liters.

	
3.5  The Covariance and the Coefficient of Correlation	
159
You can use these two rules to understand how data are distributed around the mean when 
you have sample data. With each rule, you use the value you computed for X in place of m and the 
value you computed for S in place of s. The results you compute using the sample statistics are 
approximations because you used sample statistics 1X, S2 and not population parameters 1m, s2.
Problems for Section 3.4
Learning the Basics
3.37  The following is a set of data for a population with N = 13:
8  6  12  9  4  7  3  2  10  9  7  5  4 
a.	 Calculate the population mean.
b.	 Calculate the standard deviation.
3.38  The following is a set of data for a population with N = 10:
7 5 6 6 6 4 8 6 9 3
a.	 Compute the population mean.
b.	 Compute the population standard deviation.
Applying the Concepts
3.39  The file  Tax  contains the quarterly sales tax receipts (in 
$thousands) submitted to the comptroller of the Village of Fair 
Lake for the period ending March 2013 by all 50 business estab-
lishments in that locale:
10.3
11.1
9.6
9.0
14.5
13.0
6.7
11.0
8.4
10.3
8.0
11.2
7.3
5.3
12.5
8.0
11.8
8.7
10.6
9.5
11.1
10.2
11.1
9.9
9.8
11.6
15.1
12.5
6.5
7.5
10.0
12.9
9.2
10.0
12.8
12.5
9.3
10.4
12.7
10.5
9.3
11.5
10.7
11.6
7.8
10.5
7.6
10.1
8.9
8.6
a.	 Compute the mean, variance, and standard deviation for this 
population.
b.	 What percentage of the 50 businesses has quarterly sales tax 
receipts within {1, 2, or {3 standard deviations of the mean?
c.	 Compare your findings with what would be expected on the ba-
sis of the empirical rule. Are you surprised at the results in (b)?
3.40  Consider a population of 1,024 mutual funds that primarily 
invest in large companies. You have determined that m, the mean 
one-year total percentage return achieved by all the funds, is 8.20 
and that s, the standard deviation, is 2.75.
a.	 According to the empirical rule, what percentage of these funds 
is expected to be within {1 standard deviation of the mean?
b.	 According to the empirical rule, what percentage of these funds 
is expected to be within {2 standard deviations of the mean?
c.	 According to the Chebyshev rule, what percentage of these 
funds is expected to be within {1, {2, or {3 standard devia-
tions of the mean?
d.	 According to the Chebyshev rule, at least 93.75% of these 
funds are expected to have one-year total returns between what 
two amounts?
3.41  The file  CigaretteTax  contains the state cigarette tax (in $) 
for each of the 50 states as of January 1, 2013.
a.	 Compute the population mean and population standard devia-
tion for the state cigarette tax.
b.	 Interpret the parameters in (a).
SELF 
Test 
3.42  The file  Energy  contains the per capita energy  
           consumption, in kilowatt-hours, for each of the 50 
states and the District of Columbia during a recent year.
a.	 Compute the mean, variance, and standard deviation for the 
population.
b.	 What proportion of these states has per capita energy con-
sumption within {1 standard deviation of the mean, within 
{2 standard deviations of the mean, and within {3 standard  
deviations of the mean?
c.	 Compare your findings with what would be expected based on 
the empirical rule. Are you surprised at the results in (b)?
d.	 Repeat (a) through (c) with the District of Columbia removed. 
How have the results changed?
3.43  Thirty companies comprise the DJIA. Just how big are these 
companies? One common method for measuring the size of a 
company is to use its market capitalization, which is computed by 
multiplying the number of stock shares by the price of a share of 
stock. On March 30, 2013, the market capitalization of these com-
panies ranged from Alcoa’s $9.1 billion to ExxonMobil’s $403.7 
billion. The entire population of market capitalization values is 
stored in  DowMarketCap .
Source: Data extracted from money.cnn.com, March 30, 2013.
a.	 Compute the mean and standard deviation of the market 
­capitalization for this population of 30 companies.
b.	 Interpret the parameters computed in (a).
3.5  The Covariance and the Coefficient of Correlation
In Section 2.5, you used scatter plots to visually examine the relationship between two numeri-
cal variables. This section presents two measures of the relationship between two numerical 
variables: the covariance and the coefficient of correlation.

160	
Chapter 3  Numerical Descriptive Measures
Sample Covariance
	
cov1X, Y2 =
a
n
i=1
 1Xi - X2 1Yi - Y2
n - 1
	
(3.16)
T a b l e  3 . 8
Revenues and Values 
for NBA Teams
Team  
Code
Revenue  
($millions)
Value  
($millions)
Team  
Code
Revenue  
($millions)
Value  
($millions)
ATL
99
316
MIA
150
625
BOS
143
730
MIL
87
312
BKN
84
530
MIN
96
364
CHA
93
315
NOH
100
340
CHI
162
800
NYK
243
1,100
CLE
128
434
OKC
127
475
DAL
137
685
ORL
126
470
DEN
110
427
PHI
107
418
DET
125
400
PHX
121
474
GSW
127
555
POR
117
457
HOU
135
568
SAC
96
525
IND
98
383
SAS
135
527
LAC
108
430
TOR
121
405
LAL
197
1,000
UTA
111
432
MEM
96
377
WAS
102
397
Example 3.17
Computing the 
Sample Covariance
In Figure 2.14 on page 94, you constructed a scatter plot that showed the relationship between the 
value and the annual revenue of the 30 NBA professional basketball teams (stored in  NBAValues ). 
Now, you want to measure the association between the annual revenue and value of a team by 
determining the sample covariance.
Solution  Table 3.8 provides the annual revenue and the value of the 30 teams.
Figure 3.6 contains two worksheets that together compute the covariance for these data.
From the result in cell B9 of the covariance worksheet, or by using Equation (3.16) ­directly 
(shown below), you determine that the covariance is 5,767.7:
 cov1X, Y2 = 167,263.3
30 - 1
 = 5,767.7
The Covariance
The covariance measures the strength of the linear relationship between two numerical vari-
ables (X and Y). Equation (3.16) defines the sample covariance, and Example 3.17 illustrates 
its use.

	
3.5  The Covariance and the Coefficient of Correlation	
161
The covariance has a major flaw as a measure of the linear relationship between two nu-
merical variables. Because the covariance can have any value, you cannot use it to determine 
the relative strength of the relationship. In Example 3.17, you cannot tell whether the value 
5,767.7 indicates a strong relationship or a weak relationship between revenue and value. To 
better determine the relative strength of the relationship, you need to compute the coefficient 
of correlation.
The Coefficient of Correlation
The coefficient of correlation measures the relative strength of a linear relationship ­between 
two numerical variables. The values of the coefficient of correlation range from -1 for a 
­perfect negative correlation to +1 for a perfect positive correlation. Perfect in this case means 
that if the points were plotted on a scatter plot, all the points could be connected with a 
straight line.
When dealing with population data for two numerical variables, the Greek letter r 1rho2 is 
used as the symbol for the coefficient of correlation. Figure 3.7 illustrates three different types 
of association between two variables.
F i g u r e  3 . 7
Types of association 
between variables
Y
Panel A
Perfect negative
correlation ( = –1)
X
Y
Panel B
No correlation
( = 0)
X
Y
Panel C
Perfect positive
correlation ( = +1)
X
F i g u r e  3 . 6
Excel data and covariance worksheets for the revenue and value for the 30 NBA teams
In Figure 3.6, the covariance 
worksheet illustration 
includes a list of formulas 
to the right of the cells in 
which they occur, a style 
used throughout the rest of 
this book.

162	
Chapter 3  Numerical Descriptive Measures
In Panel A of Figure 3.7, there is a perfect negative linear relationship between X and Y. 
Thus, the coefficient of correlation, r, equals - 1, and when X increases, Y decreases in a per-
fectly predictable manner. Panel B shows a situation in which there is no relationship between 
X and Y. In this case, the coefficient of correlation, r, equals 0, and as X increases, there is no 
tendency for Y to increase or decrease. Panel C illustrates a perfect positive relationship where 
r equals + 1. In this case, Y increases in a perfectly predictable manner when X increases.
Correlation alone cannot prove that there is a causation effect—that is, that the change 
in the value of one variable caused the change in the other variable. A strong correlation can 
be produced by chance; by the effect of a lurking variable—the third variable not considered 
in the calculation of the correlation; or by a cause-and-effect relationship. You would need to 
perform additional analysis to determine which of these three situations actually produced the 
correlation. Therefore, you can say that causation implies correlation, but correlation alone 
does not imply causation.
Equation (3.17) defines the sample coefficient of correlation (r).
Sample Coefficient of Correlation
	
r = cov1X, Y2
SXSY
	
(3.17)
where
 cov1X, Y2 =
a
n
i = 1
 1Xi - X21Yi - Y)
n - 1
 SX = H
a
n
i=1
 1Xi - X22
n - 1
 SY = H
a
n
i=1
 1Yi - Y22
n - 1
When you have sample data, you can compute the sample coefficient of correlation, r. 
When using sample data, you are unlikely to have a sample coefficient of correlation of exactly 
+1, 0, or -1. Figure 3.8 presents scatter plots along with their respective sample coefficients 
of correlation, r, for six data sets, each of which contains 100 X and Y values.
In Panel A, the coefficient of correlation, r, is -0.9. You can see that for small ­values 
of X, there is a very strong tendency for Y to be large. Likewise, the large values of X tend 
to be paired with small values of Y. The data do not all fall on a straight line, so the asso-
ciation between X and Y cannot be described as perfect. The data in Panel B have a coef-
ficient of correlation equal to -0.6, and the small values of X tend to be paired with large 
values of Y. The linear relationship between X and Y in Panel B is not as strong as that in 
Panel A. Thus, the coefficient of correlation in Panel B is not as negative as that in Panel A.  
In Panel C, the linear relationship between X and Y is very weak, r = -0.3, and there is only 
a slight tendency for the small values of X to be paired with the large values of Y. Panels D 
through F depict data sets that have positive coefficients of correlation because small values of 
X tend to be paired with small values of Y, and large values of X tend to be associated with large 
values of Y. Panel D shows weak positive correlation, with r = 0.3. Panel E shows stronger 
positive correlation, with r = 0.6. Panel F shows very strong positive correlation, with r = 0.9.

	
3.5  The Covariance and the Coefficient of Correlation	
163
F i g u r e  3 . 8
Six scatter plots and their sample coefficients of correlation, r
Panel A
Panel D
Panel B
Panel E
Panel C
Panel F

164	
Chapter 3  Numerical Descriptive Measures
In summary, the coefficient of correlation indicates the linear relationship, or association, 
between two numerical variables. When the coefficient of correlation gets closer to + 1 or - 1, 
the linear relationship between the two variables is stronger. When the coefficient of correla-
tion is near 0, little or no linear relationship exists. The sign of the coefficient of correlation 
indicates whether the data are positively correlated (i.e., the larger values of X are typically 
paired with the larger values of Y) or negatively correlated (i.e., the larger values of X are typi-
cally paired with the smaller values of Y). The existence of a strong correlation does not imply 
a causation effect. It only indicates the tendencies present in the data.
Example 3.18
Computing the 
Sample Coefficient 
of Correlation
In Example 3.17 on page 160, you computed the covariance of the revenue and value for the 
30 NBA teams. Now, you want to measure the relative strength of a linear relationship between 
the revenue and value by determining the sample coefficient of correlation.
Solution  By using Equation (3.17) directly (shown below) or from cell B14 in the coeffi-
cient of correlation worksheet (shown in Figure 3.9), you determine that the sample coefficient 
of correlation is 0.9143:
 r = cov1X, Y2
SXSY
 =
5,767.70
133.21322 1189.93292
 = 0.9143
The value and revenue of the NBA teams are very highly correlated. The teams with the 
lowest revenues have the lowest values. The teams with the highest revenues have the high-
est values. This relationship is very strong, as indicated by the coefficient of correlation, 
r = 0.9143.
In general, you cannot assume that just because two variables are correlated, changes in 
one variable caused changes in the other variable. However, for this example, it makes sense to 
conclude that changes in revenue would tend to cause changes in the value of a team.
Problems for Section 3.5
Learning the Basics
3.44  The following is a set of data from a sample of n = 11 
items:
X
8
6
9
4
7
11
13
5
10
16
19
Y
22
15
25
10
19
31
37
13
27
45
54
a.	 Calculate the covariance.
b.	 Calculate the coefficient of correlation.
c.	 Describe the relation between the two variables X and Y.
F i g u r e  3 . 9
Excel worksheet to 
compute the sample 
coefficient of correlation 
between revenue and 
value
The Figure 3.9 worksheet 
uses the data worksheet 
shown in Figure 3.6  
on page 161.

	
3.6  Descriptive Statistics: Pitfalls and Ethical Issues	
165
Applying the Concepts
3.45  A study of 1,839 college students suggests a link between 
frequency of using Facebook and texting in class and grade point 
average. Students reporting a higher frequency of Facebook and 
texting use in class had lower grade point averages than stu-
dents reporting a lower frequency of Facebook and texting use. 
(Source: Data extracted from “In-Class Multi-Tasking and Aca-
demic Programs.” Computers in Human Computers,  2012, dx.
doi.org/10.1016/j.chb.2012.06.031).
a.	 Does the study suggest that frequency of using Facebook and 
texting in class and grade point average are positively corre-
lated or negatively correlated?
b.	 Do you think that there might be a cause-and-effect relation-
ship between frequency of using Facebook and texting in class 
and grade point average? Explain.
SELF 
Test 
3.46  The file  Cereals  lists the calories and sugar, in  
          grams, in one serving of seven breakfast cereals:
c.	 Which do you think is more valuable in expressing the relation-
ship between first weekend gross, U.S. gross, and worldwide 
gross—the covariance or the coefficient of correlation? ­Explain.
d.	 Based on (a) and (b), what conclusions can you reach about 
the relationship between first weekend gross, U.S. gross, and 
worldwide gross?
3.48  College football is big business, with coaches’ total pay and 
revenues, in millions of dollars. The file  College Football  con-
tains the coaches’ pay and revenues for college football at 105 of 
the 124 schools that are part of the Division I  Football Bowl 
­Subdivision.
Source: Data extracted from “College Football Coaches Continue to 
See Salary Explosion,” USA Today, November 20, 2012.
a.	 Compute the covariance.
b.	 Compute the coefficient of correlation.
c.	 Based on (a) and (b), what conclusions can you reach about the 
relationship between coaches’ total pay and revenues?
3.49  A Pew Research Center survey found that social net-
working is popular in many nations around the world. The file ­ 
 GlobalSocialMedia  contains the level of social media networking 
(measured as the percentage of individuals polled who use social 
networking sites) and the GDP at purchasing power parity (PPP) 
per capita for each of 25 selected countries. (Data extracted from 
Pew Research Center, “Global Digital Communication: Texting, 
Social Networking Popular Worldwide,” updated February 29, 
2012, via the link bit.ly/sNjsmq).
a.	 Compute the covariance.
b.	 Compute the coefficient of correlation.
c.	 Based on (a) and (b), what conclusions can you reach about the 
relationship between the GDP and social media use?
Cereal
Calories
Sugar
Kellogg’s All Bran
  80
  6
Kellogg’s Corn Flakes
100
  2
Wheaties
100
  4
Nature’s Path Organic  
Multigrain Flakes
110
  4
Kellogg’s Rice Krispies
130
  4
Post Shredded Wheat Vanilla 
Almond
190
11
Kellogg’s Mini Wheats
200
10
a.	 Compute the covariance.
b.	 Compute the coefficient of correlation.
c.	 Which do you think is more valuable in expressing the relation-
ship between calories and sugar—the covariance or the coef-
ficient of correlation? Explain.
d.	 Based on (a) and (b), what conclusions can you reach about the 
relationship between calories and sugar?
3.47  Movie companies need to predict the gross receipts of indi-
vidual movies once a movie has debuted. The data, shown in the 
next column and stored in  PotterMovies , are the first weekend 
gross, the U.S. gross, and the worldwide gross (in $millions) of the 
eight Harry Potter movies:
a.	 Compute the covariance between first weekend gross and U.S. 
gross, first weekend gross and worldwide gross, and U.S. gross 
and worldwide gross.
b.	 Compute the coefficient of correlation between first weekend 
gross and U.S. gross, first weekend gross and worldwide gross, 
and U.S. gross and worldwide gross.
Title
First  
Weekend
U.S.  
Gross
Worldwide  
Gross
Sorcerer’s Stone
90.295
317.558
976.458
Chamber of Secrets
88.357
261.988
878.988
Prisoner of Azkaban
93.687
249.539
795.539
Goblet of Fire
102.335
290.013
896.013
Order of the  
Phoenix
77.108
292.005
938.469
Half-Blood Prince
77.836
301.460
934.601
Deathly Hallows –  
Part 1
125.017
295.001
955.417
Deathly Hallows –  
Part 2
169.189
381.011
1,328.111
Source: Data extracted from www.the-numbers.com/interactive/
comp-HarryPotter.php.
3.6  Descriptive Statistics: Pitfalls and Ethical Issues
This chapter describes how a set of numerical data can be characterized by the statistics that 
measure the properties of central tendency, variation, and shape. In business, descriptive statis-
tics such as the ones discussed in this chapter are frequently included in summary reports that 
are prepared periodically.

166	
Chapter 3  Numerical Descriptive Measures
The volume of information available from online, broadcast, or print media has produced 
much skepticism in the minds of many about the objectivity of data. When you are reading in-
formation that contains descriptive statistics, you should keep in mind the quip often attributed 
to the famous nineteenth-century British statesman Benjamin Disraeli: “There are three kinds 
of lies: lies, damned lies, and statistics.”
For example, in examining statistics, you need to compare the mean and the median. Are 
they similar, or are they very different? Or is only the mean provided? The answers to these 
questions will help you determine whether the data are skewed or symmetrical and whether the 
median might be a better measure of central tendency than the mean. In addition, you should 
look to see whether the standard deviation or interquartile range for a very skewed set of data 
has been included in the statistics provided. Without this, it is impossible to determine the 
amount of variation that exists in the data.
Ethical considerations arise when you are deciding what results to include in a report. You 
should document both good and bad results. In addition, when making oral presentations and 
presenting written reports, you need to give results in a fair, objective, and neutral manner. 
Unethical behavior occurs when you selectively fail to report pertinent findings that are detri-
mental to the support of a particular position.
I
n the More Descriptive Choices scenario, you were hired 
by the Choice Is Yours investment company to assist inves-
tors interested in stock mutual funds. A sample of 316 stock 
mutual funds included 227 growth funds and 89 value funds. 
By comparing these two categories, you were able to provide 
investors with valuable insights.
The one-year returns for both the growth funds and the 
value funds were symmetrical, as indicated by the boxplots 
(see Figures 3.4 and 3.5 on page 153). The descriptive statis-
tics (see Figure 3.2 on page 143) allowed you to compare the  
central tendency, variability, and shape of the returns of  
the growth funds and the value funds. The mean indicated that 
the growth funds returned an average of 14.28, and the median 
indicated that half of the growth funds had returns of 14.18 
or more. The value funds’ central tendencies were slightly 
higher than those of the growth funds—they had a mean of 
14.70, and half 
the funds had 
one-year returns 
above 15.30. 
T h e  g r ow t h 
funds showed 
slightly more variability than the value funds, with a standard 
deviation of 5.0041 as compared to 4.4651. The kurtosis of 
growth funds was very positive, indicating a distribution that 
was much more peaked than a normal distribution. Although 
past performance is no assurance of future performance, the 
value funds slightly outperformed the growth funds in 2012. 
(You can examine other variables in  Retirement Funds  to see if 
the value funds outperformed the growth funds for the three-
year period 2010–2012, for the 5-year period 2008–2012 and 
for the 10-year period 2003–2012.)
U s i n g  S tat i s t i c s
More Descriptive Choices, Revisited
S u m m a r y
In this chapter and the previous chapter, you studied de-
scriptive statistics—how you can organize data through 
tables, visualize data through charts, and how you can use 
various statistics to help analyze the data and reach con-
clusions. In Chapter 2, you organized data by construct-
ing summary tables and visualized data by constructing 
bar and pie charts, histograms, and other charts. In this 
chapter, you learned how descriptive statistics such as 
the mean, median, quartiles, range, and standard devia-
tion describe the characteristics of central tendency, vari-
ability, and shape. In addition, you constructed boxplots 
to visualize the distribution of the data. You also learned 
how the coefficient of correlation describes the relation-
ship between two numerical variables. All the methods of 
this chapter are summarized in Table 3.9.
Baranq/Shutterstock

	
Key Equations	
167
You also learned several concepts about variation in data 
that will prove useful in later chapters. These concepts are:
 • The greater the spread or dispersion of the data, the 
larger the range, variance, and standard deviation.
 • The smaller the spread or dispersion of the data, the 
smaller the range, variance, and standard deviation.
	 • If the values are all the same (so that there is no varia-
tion in the data), the range, variance, and standard 
deviation will all equal zero.
 • None of the measures of variation (the range, vari-
ance, and standard deviation) can ever be negative.
In the next chapter, the basic principles of probability 
are presented in order to bridge the gap between the sub-
ject of descriptive statistics and the subject of inferential 
statistics.
T a b l e  3 . 9
Chapter 3 Descriptive 
Statistics Methods
Type of Analysis
Methods
Central tendency
Mean, median, mode (Section 3.1)
Variation and shape
Quartiles, range, interquartile range, variance, 
standard deviation, coefficient of variation, Z 
scores, boxplot (Sections 3.2 through 3.4)
Describing the relationship  
between two numerical variables
Covariance, coefficient of correlation (Section 3.5)
Refere n c e s
	 1.	Booker, J., and L. Ticknor. “A Brief Overview of Kurtosis.” 
www.osti.gov/bridge/purl.cover.jsp?purl=/677174-zdulqk/
webviewable/677174.pdf.
	 2.	Kendall, M. G., A. Stuart, and J. K. Ord. Kendall’s Advanced 
Theory of Statistics, Volume 1: Distribution Theory, 6th ed. 
New York: Oxford University Press, 1994.
	 3.	Microsoft Excel 2013. Redmond, WA: Microsoft Corporation, 
2012.
	 4.	Minitab Release 16. State College, PA: Minitab, Inc., 2010.
	 5.	Taleb, N. The Black Swan, 2nd ed. New York: Random House, 
2010.
K ey Eq u at i o n s
Sample Mean
X =
a
n
i=1
 Xi
n

(3.1)
Median
Median = n + 1
2
 ranked value
(3.2)
Geometric Mean
XG = 1X1 * X2 * g * Xn21>n
(3.3)
Geometric Mean Rate of Return
RG = 311 + R12 * 11 + R22 * g
* 11 + Rn241>n - 1
(3.4)
Range
range = Xlargest - Xsmallest
(3.5)
Sample Variance
S2 =
a
n
i=1
 1Xi - X22
n - 1

(3.6)
Sample Standard Deviation
S = 2S2 = H
a
n
i=1
 1Xi - X22
n - 1

(3.7)
Coefficient of Variation
CV = a S
Xb100%
(3.8)
Z Score
Z = X - X
S

(3.9)

168	
Chapter 3  Numerical Descriptive Measures
K e y  Term s
arithmetic mean (mean)  130
boxplot  152
central tendency  130
Chebyshev rule  158
coefficient of correlation  161
coefficient of variation (CV )  140
covariance  160
dispersion (spread)  135
empirical rule  157
five-number summary  151
geometric mean  134
interquartile range (midspread)  150
kurtosis  143
left-skewed  142
lepokurtic  143
lurking variable  162
mean (arithmetic mean)  130
median  132
midspread (interquartile range)  150
mode  133
outliers  141
percentiles  401
platykurtic  143
population mean  155
population standard deviation  156
population variance  156
Q1: first quartile  148
Q2: second quartile  148
Q3: third quartile  148
quartiles  148
range  135
resistant measure  151
right-skewed  142
sample coefficient of correlation (r)  162
sample covariance  160
sample mean  130
sample standard deviation (S)  136
sample variance 1S22  136
shape  142
skewed  142
skewness  142
spread (dispersion)  135
standard deviation  136
sum of squares (SS)  136
symmetrical  142
variance  136
variation  130
Z score  141
C hec ki n g  Yo u r  U n d e r s ta nding
3.50  What are the properties of a set of numerical data?
3.51  What is meant by the property of central tendency?
3.52  What are the differences among the mean, median, and 
mode, and what are the advantages and disadvantages of each?
3.53  How do you interpret the first quartile, median, and third 
quartile?
3.54  What is meant by the property of variation?
3.55  What does the Z score measure?
3.56  What are the differences among the various measures of 
variation, such as the range, interquartile range, variance, standard 
deviation, and coefficient of variation, and what are the advantages 
and disadvantages of each?
3.57  How does the empirical rule help explain the ways in which 
the values in a set of numerical data cluster and distribute?
3.58  How do the empirical rule and the Chebyshev rule differ?
3.59  What is meant by the property of shape?
3.60  What is the difference between the arithmetic mean and the 
geometric mean?
3.61  What is the difference between skewness and kurtosis?
3.62  How do the covariance and the coefficient of correlation differ?
First Quartile, Q1
Q1 = n + 1
4
 ranked value
(3.10)
Third Quartile, Q3
Q3 = 31n + 12
4
 ranked value
(3.11)
Interquartile Range
Interquartile range = Q3 - Q1
(3.12)
Population Mean
m =
a
N
i=1
 Xi
N

(3.13)
Population Variance
s2 =
a
N
i=1
 1Xi - m22
N

(3.14)
Population Standard Deviation
s = H
a
N
i = 1
 1Xi - m22
N

(3.15)
Sample Covariance
cov1X, Y2 =
a
n
i=1
 1Xi - X2 1Yi - Y2
n - 1

(3.16)
Sample Coefficient of Correlation
r = cov 1X, Y2
SXSY

(3.17)

	
Chapter Review Problems	
169
C h a pte r  R e vi e w P r ob le ms
3.63  The American Society for Quality (ASQ) conducted a sal-
ary survey of all its members. ASQ members work in all areas of 
manufacturing and service-related institutions, with a common 
theme of an interest in quality. For the survey, emails were sent to 
54,337 members, and 6,093 valid responses were received. Man-
ager and quality engineer were the most frequently reported job 
titles among the valid responses. Master Black Belt, a person who 
takes a leadership role as the keeper of the Six Sigma process (see 
Section 19.6) and Green Belt, someone who works on Six Sigma 
projects part time, were among the other job titles cited. Descrip-
tive statistics concerning salaries for these four titles are given in 
the following table:
3.65  One of the major measures of the quality of service provided 
by an organization is the speed with which it responds to customer 
complaints. A large family-held department store selling furniture 
and flooring, including carpet, had undergone a major expansion 
in the past several years. In particular, the flooring department had 
expanded from 2 installation crews to an installation supervisor, 
a measurer, and 15 installation crews. The business objective of 
the company was to reduce the time between when a complaint is 
received and when it is resolved. During a recent year, the com-
pany received 50 complaints concerning carpet installation. The 
data from the 50 complaints, organized in  Furniture , represent the 
number of days between the receipt of a complaint and the resolu-
tion of the complaint:
54
5
35 137
31
27 152
2 123
81
74
27
11
19 126 110 110
29
61
35
94
31
26
5
12
4
165
32
29
28
29
26
25
1
14
13
13
10
5
27
4
52
30
22
36
26
20
23
33
68
a.	 Compute the mean, median, first quartile, and third quartile.
b.	 Compute the range, interquartile range, variance, standard de-
viation, and coefficient of variation.
c.	 Construct a boxplot. Are the data skewed? If so, how?
d.	 On the basis of the results of (a) through (c), if you had to tell the 
president of the company how long a customer should expect to 
wait to have a complaint resolved, what would you say? Explain.
3.66  A manufacturing company produces steel housings for electri-
cal equipment. The main component part of the housing is a steel 
trough that is made of a 14-gauge steel coil. It is produced using a 
250-ton progressive punch press with a wipe-down operation and 
two 90-degree forms placed in the flat steel to make the trough. The 
distance from one side of the form to the other is critical because of 
weatherproofing in outdoor applications. The company requires that 
the width of the trough be between 8.31 inches and 8.61 inches. Data 
are collected from a sample of 49 troughs and stored in  Trough , 
which contains these widths of the troughs, in inches:
8.312 8.343 8.317 8.383 8.348 8.410 8.351 8.373 8.481 8.422
8.476 8.382 8.484 8.403 8.414 8.419 8.385 8.465 8.498 8.447
8.436 8.413 8.489 8.414 8.481 8.415 8.479 8.429 8.458 8.462
8.460 8.444 8.429 8.460 8.412 8.420 8.410 8.405 8.323 8.420
8.396 8.447 8.405 8.439 8.411 8.427 8.420 8.498 8.409
a.	 Compute the mean, median, range, and standard deviation for 
the width. Interpret these measures of central tendency and 
variability.
b.	 List the five-number summary.
c.	 Construct a boxplot and describe its shape.
d.	 What can you conclude about the number of troughs that will 
meet the company’s requirement of troughs being between 8.31 
and 8.61 inches wide?
Job Title
Sample  
Size
Minimum Maximum
Standard  
Deviation Mean
Median
Green  
Belt
26
36,000
135,000
23,399
74,173
70,500
Manager
1,387
38,000
732,000
27,906
91,878
89,000
Quality 
Engineer
766
25,000
185,000
21,933
79,575
78,000
Master  
Black Belt
79
28,000
185,000
27,474
112,946 110,000
Source: Data extracted from M. Hansen “Facing Tight Times,” Quality Progress, 
December 2012, p. 29.
Compare the salaries of Green Belts, managers, quality engineers, 
and Master Black Belts.
3.64  In certain states, savings banks are permitted to sell life insur-
ance. The approval process consists of underwriting, which includes 
a review of the application, a medical information bureau check, 
possible requests for additional medical information and medical 
exams, and a policy compilation stage, in which the policy pages 
are generated and sent to the bank for delivery. The ability to deliver 
approved policies to customers in a timely manner is critical to the 
profitability of this service to the bank. Using the Define, Collect, 
Organize, Visualize, and Analyze steps first discussed in Chapter 2,  
you define the variable of interest as the total processing time  
in days. You collect the data by selecting a random sample of 27 ap-
proved policies during a period of one month. You organize the data 
collected in a worksheet and store them in  Insurance :
73 19 16 64 28 28 31 90 60 56 31 56 22 18
45 48 17 17 17 91 92 63 50 51 69 16 17
a.	 Compute the mean, median, first quartile, and third quartile.
b.	 Compute the range, interquartile range, variance, standard 
­deviation, and coefficient of variation.
c.	 Construct a boxplot. Are the data skewed? If so, how?
d.	 What would you tell a customer who enters the bank to pur-
chase this type of insurance policy and asks how long the 
­approval process takes?

170	
Chapter 3  Numerical Descriptive Measures
3.67  The manufacturing company in Problem 3.66 also produces 
electric insulators. If the insulators break when in use, a short cir-
cuit is likely to occur. To test the strength of the insulators, de-
structive testing is carried out to determine how much force is 
required to break the insulators. Force is measured by observing 
how many pounds must be applied to an insulator before it breaks. 
Data are collected from a sample of 30 insulators. The file  Force  
contains the strengths, as follows:
1,870 1,728 1,656 1,610 1,634 1,784 1,522 1,696 1,592 1,662
1,866 1,764 1,734 1,662 1,734 1,774 1,550 1,756 1,762 1,866
1,820 1,744 1,788 1,688 1,810 1,752 1,680 1,810 1,652 1,736
a.	 Compute the mean, median, range, and standard deviation for 
the force needed to break the insulators.
b.	 Interpret the measures of central tendency and variability in (a).
c.	 Construct a boxplot and describe its shape.
d.	 What can you conclude about the strength of the insulators if 
the company requires a force of at least 1,500 pounds before 
breakage?
3.68  Data were collected on the typical cost of dining at American- 
cuisine restaurants within a 1-mile walking distance of a hotel  
located in a large city. The file  Bundle  contains the typical cost 
(a per transaction cost in $) as well as a Bundle score, a measure 
of overall popularity and customer loyalty, for each of 40 selected 
restaurants. (Data extracted from www.bundle.com via the link 
on-msn.com/MnlBxo.)
a.	 For each variable, compute the mean, median, first quartile, 
and third quartile.
b.	 For each variable, compute the range, interquartile range, vari-
ance, standard deviation, and coefficient of variation.
c.	 For each variable, construct a boxplot. Are the data skewed? If 
so, how?
d.	 Compute the coefficient of correlation between Bundle score 
and typical cost.
e.	 What conclusions can you reach concerning Bundle score and 
typical cost?
3.69  A quality characteristic of interest for a tea-bag-filling 
process is the weight of the tea in the individual bags. If the bags 
are underfilled, two problems arise. First, customers may not be 
able to brew the tea to be as strong as they wish. Second, the 
company may be in violation of the truth-in-labeling laws. For 
this product, the label weight on the package indicates that, on 
average, there are 5.5 grams of tea in a bag. If the mean amount 
of tea in a bag exceeds the label weight, the company is giving 
away product. Getting an exact amount of tea in a bag is prob-
lematic because of variation in the temperature and humidity 
inside the factory, differences in the density of the tea, and the 
extremely fast filling operation of the machine (approximately 
170 bags per minute). The file  Teabags  contains these weights, 
in grams, of a sample of 50 tea bags produced in one hour by a 
single machine:
5.65 5.44 5.42 5.40 5.53 5.34 5.54 5.45 5.52 5.41
5.57 5.40 5.53 5.54 5.55 5.62 5.56 5.46 5.44 5.51
5.47 5.40 5.47 5.61 5.53 5.32 5.67 5.29 5.49 5.55
5.77 5.57 5.42 5.58 5.58 5.50 5.32 5.50 5.53 5.58
5.61 5.45 5.44 5.25 5.56 5.63 5.50 5.57 5.67 5.36
a.	 Compute the mean, median, first quartile, and third quartile.
b.	 Compute the range, interquartile range, variance, standard de-
viation, and coefficient of variation.
c.	 Interpret the measures of central tendency and variation within 
the context of this problem. Why should the company produc-
ing the tea bags be concerned about the central tendency and 
variation?
d.	 Construct a boxplot. Are the data skewed? If so, how?
e.	 Is the company meeting the requirement set forth on the label 
that, on average, there are 5.5 grams of tea in a bag? If you 
were in charge of this process, what changes, if any, would you 
try to make concerning the distribution of weights in the indi-
vidual bags?
3.70  The manufacturer of Boston and Vermont asphalt shingles 
provides its customers with a 20-year warranty on most of its prod-
ucts. To determine whether a shingle will last as long as the warranty 
period, accelerated-life testing is conducted at the manufacturing 
plant. Accelerated-life testing exposes a shingle to the stresses it 
would be subject to in a lifetime of normal use via an experiment 
in a laboratory setting that takes only a few minutes to conduct. In 
this test, a shingle is repeatedly scraped with a brush for a short pe-
riod of time, and the shingle granules removed by the brushing are 
weighed (in grams). Shingles that experience low amounts of gran-
ule loss are expected to last longer in normal use than shingles that 
experience high amounts of granule loss. In this situation, a shingle 
should experience no more than 0.8 gram of granule loss if it is ex-
pected to last the length of the warranty period. The file  Granule  
contains a sample of 170 measurements made on the company’s 
Boston shingles and 140 measurements made on Vermont shingles.
a.	 List the five-number summaries for the Boston shingles and for 
the Vermont shingles.
b.	 Construct side-by-side boxplots for the two brands of shingles 
and describe the shapes of the distributions.
c.	 Comment on the ability of each type of shingle to achieve a 
granule loss of 0.8 gram or less.
d.	 What conclusions can you reach about the cost of a meal at city 
and suburban restaurants?
3.71  The file  Restaurants  contains the cost per meal and the 
ratings of 50 city and 50 suburban restaurants on their food, décor, 
and service (and their summated ratings). Complete the following 
for the urban and suburban restaurants:
Source: Data extracted from Zagat Survey 2013 New York City 
Restaurants and Zagat Survey 2012–2013 Long Island Restaurants.
a.	 Construct the five-number summary of the cost of a meal.
b.	 Construct a boxplot of the cost of a meal. What is the shape of 
the distribution?
c.	 Compute and interpret the correlation coefficient of the sum-
mated rating and the cost of a meal.
d.	 What conclusions can you reach about the cost of a meal at city 
and suburban restaurants?
3.72  The file  Protein  contains calories, protein, and cholesterol 
of popular protein foods (fresh red meats, poultry, and fish).
Source: U.S. Department of Agriculture.
a.	 Compute the correlation coefficient between calories and 
protein.
b.	 Compute the correlation coefficient between calories and  
cholesterol.
c.	 Compute the correlation coefficient between protein and  
cholesterol.
d.	 Based on the results of (a) through (c), what conclusions can 
you reach concerning calories, protein, and cholesterol?

	
Chapter Review Problems	
171
3.73  The file  HotelPrices  contains the prices in British pounds of 
a room at two-star, three-star, and four-star hotels in cities around 
the world in 2012. (Data extracted from bit.ly/Q0qxe4.) Complete 
the following for two-star, three-star, and four-star hotels:
a.	 Compute the mean, median, first quartile, and third quartile.
b.	 Compute the range, interquartile range, variance, standard de-
viation, and coefficient of variation.
c.	 Interpret the measures of central tendency and variation within 
the context of this problem.
d.	 Construct a boxplot. Are the data skewed? If so, how?
e.	 Compute the covariance between the average price at two-star 
and three-star hotels, between two-star and four-star hotels, and 
between three-star and four-star hotels.
f.	 Compute the coefficient of correlation between the average 
price at two-star and three-star hotels, between two-star and 
four-star hotels, and between three-star and four-star hotels.
g.	 Which do you think is more valuable in expressing the relation-
ship between the average price of a room at two-star, three-star, 
and four-star hotels—the covariance or the coefficient of cor-
relation? Explain.
h.	 Based on (f), what conclusions can you reach about the rela-
tionship between the average price of a room at two-star, three-
star, and four-star hotels?
3.74  The file  PropertyTaxes  contains the property taxes per cap-
ita for the 50 states and the District of Columbia.
a.	 Compute the mean, median, first quartile, and third quartile.
b.	 Compute the range, interquartile range, variance, standard de-
viation, and coefficient of variation.
c.	 Construct a boxplot. Are the data skewed? If so, how?
d.	 Based on the results of (a) through (c), what conclusions can 
you reach concerning property taxes per capita for each state 
and the District of Columbia?
3.75  The file  CEO-Compensation  includes the total compensa-
tion (in $millions) of CEOs of 170 large public companies and the 
investment return in 2012.
Source: Data extracted from “CEO Pay Rockets as Economy, Stocks 
Recover,” USA Today, March 27, 2013, p. 1B.
a.	 Compute the mean, median, first quartile, and third quartile.
b.	 Compute the range, interquartile range, variance, standard de-
viation, and coefficient of variation.
c.	 Construct a boxplot. Are the data skewed? If so, how?
d.	 Based on the results of (a) through (c), what conclusions can you 
reach concerning the total compensation (in $millions) of CEOs?
e.	 Compute the correlation coefficient between compensation and 
the investment return in 2012.
f.	 What conclusions can you reach from the results of (e)?
3.76  311 is Chicago’s web and phone portal for government in-
formation and nonemergency services. 311 serves as a compre-
hensive one-stop shop for residents, visitors, and business owners; 
therefore, it is critical that 311 representatives answer calls and 
respond to requests in a timely and accurate fashion. The target 
response time for answering 311 calls is 45 seconds. Agent aban-
donment rate is one of several call center metrics tracked by 311 
officials. This metric tracks the percentage of callers who hang up 
after the target response time of 45 seconds has elapsed. The file  
 311CallCenter  contains the agent abandonment rate for 22 weeks 
of call center operation during the 7:00 a.m.–3:00 p.m. shift.
a.	 Compute the mean, median, first quartile, and third quartile.
b.	 Compute the range, interquartile range, variance, standard de-
viation, and coefficient of variation.
c.	 Construct a boxplot. Are the data skewed? If so, how?
d.	 Compute the correlation coefficient between day and agent 
abandonment rate.
e.	 Based on the results of (a) through (c), what conclusions might 
you reach concerning 311 call center performance operation?
3.77  How much time do Americans living in or near cities spend 
waiting in traffic, and how much does waiting in traffic cost them 
per year? The file  Congestion  includes this cost for 31 cities. 
(Source: Data extracted from “The High Cost of Congestion,” 
Time, ­October 17, 2011, p. 18.) For the time Americans living in 
or near cities spend waiting in traffic and the cost of waiting in 
traffic per year:
a.	 Compute the mean, median, first quartile, and third quartile.
b.	 Compute the range, interquartile range, variance, standard 
­deviation, and coefficient of variation.
c.	 Construct a boxplot. Are the data skewed? If so, how?
d.	 Compute the correlation coefficient between the time spent sit-
ting in traffic and the cost of sitting in traffic.
e.	 Based on the results of (a) through (c), what conclusions might 
you reach concerning the time spent waiting in traffic and the 
cost of waiting in traffic.
3.78  How do the average credit scores of people living in ­various 
American cities differ? The file  Credit Scores  is an ordered ­array 
of the average credit scores of people living in 143 American 
­cities. (Data extracted from usat.ly/17a1fA6)
a.	 Compute the mean, median, first quartile, and third quartile.
b.	 Compute the range, interquartile range, variance, standard 
­deviation, and coefficient of variation.
c.	 Construct a boxplot. Are the data skewed? If so, how?
d.	 Based on the results of (a) through (c), what conclusions might 
you reach concerning the average credit scores of people living 
in various American cities?
3.79  You are planning to study for your statistics examination 
with a group of classmates, one of whom you particularly want to 
impress. This individual has volunteered to use Microsoft Excel to 
generate the needed summary information, tables, and charts for a 
data set that contains several numerical and categorical variables 
assigned by the instructor for study purposes. This person comes 
over to you with the printout and exclaims, “I’ve got it all—the 
means, the medians, the standard deviations, the boxplots, the pie 
charts—for all our variables. The problem is, some of the output 
looks weird—like the boxplots for gender and for major and the 
pie charts for grade point average and for height. Also, I can’t un-
derstand why Professor Szabat said we can’t get the descriptive 
stats for some of the variables; I got them for everything! See, the 
mean for height is 68.23, the mean for grade point average is 2.76, 
the mean for gender is 1.50, the mean for major is 4.33.” What is 
your reply?
Report Writing Exercises
3.80  The file  DomesticBeer  contains the percentage alcohol, 
number of calories per 12 ounces, and number of carbohydrates (in 
grams) per 12 ounces for 152 of the best-selling domestic beers in 
the United States. (Data extracted from bit.ly/17H3Ct, March 20, 
2013.) Write a report that includes a complete descriptive evaluation 
of each of the numerical variables—percentage of alcohol, number 
of calories per 12 ounces, and number of carbohydrates (in grams) 
per 12 ounces. Append to your report all appropriate tables, charts, 
and numerical descriptive measures.

172	
Chapter 3  Numerical Descriptive Measures
C a s e s  f o r  C h a p t e r  3
Managing Ashland MultiComm Services
For what variable in the Chapter 2 “Managing Ashland  
MultiComm Services” case (see page 110) are numerical  
descriptive measures needed?
1.	For the variable you identify, compute the appropriate 
numerical descriptive measures and construct a boxplot.
2.	For the variable you identify, construct a graphical dis-
play. What conclusions can you reach from this other plot 
that cannot be made from the boxplot?
3.	Summarize your findings in a report that can be included 
with the task force’s study.
Digital Case
Apply your knowledge about the proper use of numerical 
descriptive measures in this continuing Digital Case from 
Chapter 2.
Open EndRunGuide.pdf, the EndRun Financial Services 
“Guide to Investing.” Reexamine EndRun’s supporting data 
for the “More Winners Than Losers” and “The Big Eight 
Difference” and then answer the following:
1.	Can descriptive measures be computed for any variables? 
How would such summary statistics support EndRun’s 
claims? How would those summary statistics affect your 
perception of EndRun’s record?
2.	Evaluate the methods EndRun used to summarize the re-
sults presented on the “Customer Survey Results” page. 
Is there anything you would do differently to summarize 
these results?
3.	Note that the last question of the survey has fewer 
responses than the other questions. What factors may 
have limited the number of responses to that question?
CardioGood Fitness
Return to the CardioGood Fitness case first presented on 
page 111. Using the data stored in  CardioGoodFitness :
1.	Compute descriptive statistics to create a customer profile 
for each CardioGood Fitness treadmill product line.
2.	Write a report to be presented to the management of 
­CardioGood Fitness, detailing your findings.
More Descriptive Choices Follow-up
Follow up the Using Statistics Revisited section on page 166  
by computing descriptive statistics to analyze the differences 
in 3-year return percentages, 5-year return percentages, and 
10-year return percentages for the sample of 316 retirement 
funds stored in  Retirement Funds . In your analysis, examine 
differences between the growth and value funds as well as 
the differences among the small, mid-cap, and large market 
cap funds.
Clear Mountain State Student Surveys
1.	 The student news service at Clear Mountain State Uni-
versity (CMSU) has decided to gather data about the un-
dergraduate students who attend CMSU. They create and 
distribute a survey of 14 questions and receive responses 
from 62 undergraduates (stored in  UndergradSurvey ). For 
each numerical variable included in the survey, compute 
all the appropriate descriptive statistics and write a report 
summarizing your conclusions.
2.	The dean of students at CMSU has learned about the 
undergraduate survey and has decided to undertake 
a similar survey for graduate students at CMSU. She 
creates and distributes a survey of 14 questions and re-
ceives responses from 44 graduate students (stored in  
 GradSurvey ). For each numerical variable included in 
the survey, compute all the appropriate descriptive sta-
tistic and write a report summarizing your conclusions.

	
Chapter 3 Excel Guide	
173
EG3.1  Central Tendency
The Mean, Median, and Mode
Key Technique  Use the AVERAGE(variable cell range), 
MEDIAN(variable cell range), and MODE(variable cell range) 
functions to compute these measures.
Example  Compute the mean, median, and mode for the sample 
of getting-ready times introduced in Section 3.1.
PHStat  Use Descriptive Summary.
For the example, open to the DATA worksheet of the Times 
workbook. Select PHStat u  Descriptive Statistics u  
­Descriptive Summary. In the procedure’s dialog box (shown 
below):
	 1.	 Enter A1:A11 as the Raw Data Cell Range and check First 
cell contains label.
	 2.	 Click Single Group Variable.
	 3.	 Enter a Title and click OK.
PHStat inserts a new worksheet that contains various measures of 
central tendency, variation, and shape discussed in Sections 3.1 
and 3.2. This worksheet is similar to the CompleteStatistics work-
sheet of the Descriptive workbook.
In-Depth Excel  Use the CentralTendency worksheet of the 
Descriptive workbook as a model.
For the example, open the Times workbook and insert a new 
worksheet (see Section EG.4 on page 38) and:
	 1.	 Enter a title in cell A1.
	 2.	 Enter Get-Ready Times in cell B3, Mean in cell A4, Median 
in cell A5, and Mode in cell A6.
	 3.	 Enter the formula =AVERAGE1DATA!A:A2 in cell B4, 
the formula =MEDIAN1DATA!A:A2 in cell B5, and the 
­formula =MODE1DATA!A:A2 in cell B6.
For these functions, the variable cell range includes the name of 
the DATA worksheet because the data being summarized appears 
on the separate DATA worksheet.
Analysis ToolPak  Use Descriptive Statistics.
For the example, open to the DATA worksheet of the Times 
workbook and:
	 1.	 Select Data u  Data Analysis.
	 2.	 In the Data Analysis dialog box, select Descriptive Statistics 
from the Analysis Tools list and then click OK.
In the Descriptive Statistics dialog box (shown below):
	 3.	 Enter A1:A11 as the Input Range. Click Columns and check 
Labels in first row.
	 4.	 Click New Worksheet Ply and check Summary statistics, 
Kth Largest, and Kth Smallest.
	 5.	 Click OK.
The ToolPak inserts a new worksheet that contains various 
measures of central tendency, variation, and shape discussed in 
Sections 3.1 and 3.2.
The Geometric Mean
Key Technique  Use the GEOMEAN1 11 + 1R12 2, 11 +
1R22 2, N 11 + 1Rn2 2 2 −1 function to compute the geomet-
ric mean rate of return.
Example  Compute the geometric mean rate of return in the Russell 
2000 Index for the two years as shown in Example 3.4 on page 135.
In-Depth Excel  Enter the formula =GEOMEAN1 11 +
1 −0.0552 2, 11 + 10.1462 2 2 −1 in any cell.
EG3.2  Variation and Shape
The Range
Key Technique  Use the MIN(variable cell range) and 
MAX(variable cell range) functions to help compute the range.
Example  Compute the range for the sample of getting-ready 
times first introduced in Section 3.1.
PHStat  Use Descriptive Summary (see Section EG3.1).
C h a p t e r  3  E x c e l  G u i d e

174	
Chapter 3  Numerical Descriptive Measures
In-Depth Excel  Use the Range worksheet of the Descriptive 
workbook as a model.
For the example, open the worksheet implemented for the example 
in the In-Depth Excel “The Mean, Median, and Mode” instructions.
Enter Minimum in cell A7, Maximum in cell A8, and Range 
in cell A9. Enter the formula =MIN1DATA!A:A2 in cell B7, the 
formula =MAX1DATA!A:A2 in cell B8, and the formula =B8
− B7 in cell B9.
The Variance, Standard Deviation, Coefficient of 
Variation, and Z Scores
Key Technique  Use the VAR.S(variable cell range) and 
STDEV.S(variable cell range) functions to compute the sample 
variation and the sample standard deviation, respectively. Use the 
AVERAGE and STDEV.S functions for the coefficient of varia-
tion. Use the STANDARDIZE(value, mean, standard deviation) 
function to compute Z scores.
Example  Compute the variance, standard deviation, coefficient 
of variation, and Z scores for the sample of getting-ready times 
first introduced in Section 3.1.
PHStat  Use Descriptive Summary (see Section EG3.1).
In-Depth Excel  Use the Variation and ZScores worksheets of 
the Descriptive workbook as models.
For the example, open to the worksheet implemented for the 
earlier examples. Enter Variance in cell A10, Standard De-
viation in cell A11, and Coeff. of Variation in cell A12. 
Enter the formula =VAR.S1DATA!A:A2 in cell B10, the for-
mula =STDEV.S1DATA!A:A2 in cell B11, and the formula 
=B11>AVERAGE1DATA!A:A2 in cell B12. If you previously 
entered the formula for the mean in cell A4 using the Section 
EG3.1 In-Depth Excel instructions, enter the simpler formula 
=B11>B4 in cell B12. Right-click cell B12 and click Format 
Cells in the shortcut menu. In the Number tab of the Format Cells 
dialog box, click Percentage in the Category list, enter 2 as the 
Decimal places, and click OK.
To compute the Z scores, copy the DATA worksheet. In the 
new, copied worksheet, enter Z Score in cell B1. Enter the formula 
= STANDARDIZE(A2, Variation!$B$4, Variation!$B$11) 
in cell B2 and copy the formula down through row 11. If you 
use an Excel version older than Excel 2010, enter Variation_
OLDER!$B$4 and Variation_OLDER!$B$11 as the cell refer-
ences in the formula.
Analysis ToolPak  Use Descriptive Statistics (see Section 
EG3.1). This procedure does not compute Z scores.
Shape: Skewness and Kurtosis
Key Technique  Use the SKEW(variable cell range) and the 
KURT(variable cell range) functions to compute these measures.
Example  Compute the skewness and kurtosis for the sample of 
getting-ready times first introduced in Section 3.1.
PHStat  Use Descriptive Summary (see Section EG3.1).
In-Depth Excel  Use the Shape worksheet of the Descriptive 
workbook as a model.
For the example, open to the worksheet implemented for the ear-
lier examples. Enter Skewness in cell A13 and Kurtosis in cell 
A14. Enter the formula =SKEW1DATA!A:A2 in cell B13 and 
the formula =KURT1DATA!A:A2 in cell B14. Then format 
cells B13 and B14 for four decimal places.
Analysis ToolPak  Use Descriptive Statistics (see Section 
EG3.1).
EG3.3  Exploring Numerical Data
Quartiles
Key Technique  Use the MEDIAN, COUNT, SMALL,  
INT, FLOOR, and CEILING functions in combination with the IF 
decision-making function to compute the quartiles. To apply the 
rules of Section 3.3, avoid using any of the Excel quartile func-
tions to compute the first and third quartiles.
Example  Compute the quartiles for the sample of getting-ready 
times first introduced in Section 3.1.
PHStat  Use Boxplot (discussed later on page 175).
In-Depth Excel  Use the COMPUTE worksheet of the Quar-
tiles workbook as a model.
For the example, the COMPUTE worksheet already computes the 
quartiles for the getting-ready times. To compute the quartiles for 
another set of data, paste the data into column A of the DATA 
worksheet, overwriting the existing getting-ready times.
Open to the COMPUTE_FORMULAS worksheet to exam-
ine the formulas and read the Short Takes for Chapter 3 for an 
extended discussion of the formulas in the worksheet.
The workbook uses the older QUARTILE(variable cell 
range, quartile number) function and not the newer QUARTILE 
.EXC function for reasons explained in Appendix Section F.3. 
Both the older and newer functions use rules that differ from the 
Section 3.3 rules to compute quartiles. To compare the results  
using these newer functions, open to the COMPARE worksheet.
The Interquartile Range
Key Technique  Use a formula to subtract the first quartile from 
the third quartile.
Example  Compute the interquartile range for the sample of get-
ting-ready times first introduced in Section 3.1.
In-Depth Excel  Use the COMPUTE worksheet of the Quar-
tiles workbook (introduced in the previous section) as a model.
For the example, the interquartile range is already computed in 
cell B19 using the formula =B18 −B16.
The Five-Number Summary and the Boxplot
Key Technique  Plot a series of line segments on the same chart 
to construct a boxplot. (Excel chart types do not include boxplots.)
Example  Compute the five-number summary and construct the 
boxplots of the one-year return percentage variable for the growth 
and value funds used in Example 3.14 on page 153.

PHStat  Use Boxplot.
For the example, open to the DATA worksheet of the Retirement 
Funds workbook. Select PHStat u  Descriptive Statistics u  
Boxplot. In the procedure’s dialog box (shown below):
	 1.	 Enter I1:I317 as the Raw Data Cell Range and check First 
cell contains label.
	 2.	 Click Multiple Groups - Stacked and enter C1:C317 as the 
Grouping Variable Cell Range.
	 3.	 Enter a Title, check Five-Number Summary, and click OK.
The boxplot appears on its own chart sheet, separate from the 
worksheet that contains the five-number summary.
In-Depth Excel  Use the worksheets of the Boxplot workbook 
as templates.
For the example, use the PLOT_DATA worksheet which already 
shows the five-number summary and boxplot for the value funds. 
To compute the five-number summary and construct a boxplot for 
the growth funds, copy the growth funds from column A of the 
UNSTACKED worksheet of the Retirement Funds workbook 
and paste into column A of the DATA worksheet of the Boxplot 
workbook.
For other problems, use the PLOT_SUMMARY worksheet 
as the template if the five-number summary has already been de-
termined; otherwise, paste your unsummarized data into column 
A of the DATA worksheet and use the PLOT_DATA worksheet as 
was done for the example.
The worksheets creatively misuse Excel line charting features 
to construct a boxplot. Read the Short Takes for Chapter 3 for an 
explanation of this “misuse.”
EG3.4  Numerical Descriptive 
Measures for a Population
The Population Mean, Population Variance, and 
Population Standard Deviation
Key Technique  Use AVERAGE(variable cell range), 
VAR.P(variable cell range), and STDEV.P(variable cell range) 
to compute these measures.
Example  Compute the population mean, population variance, 
and population standard deviation for the “Dow Dogs” population 
data of Table 3.6 on page 155.
In-Depth Excel  Use the Parameters workbook as a model. 
For the example, the COMPUTE worksheet of the Parameters 
workbook already computes the three population parameters for 
the “Dow Dogs.” If you use an Excel version older than Excel 
2010, use the COMPUTE_OLDER worksheet.
The Empirical Rule and the Chebyshev Rule
Use the COMPUTE worksheet of the VE-Variability work-
book to explore the effects of changing the mean and standard de-
viation on the ranges associated with {1 standard deviation, {2 
standard deviations, and {3 standard deviations from the mean. 
Change the mean in cell B4 and the standard deviation in cell B5 
and then note the updated results in rows 9 through 11.
EG3.5  The Covariance and the 
Coefficient of Correlation
The Covariance
Key Technique  Use the COVARIANCE.S(variable 1 cell 
range, variable 2 cell range) function to compute this measure.
Example  Compute the sample covariance for the NBA team 
revenue and value shown in Figure 3.6 on page 161.
In-Depth Excel  Use the Covariance workbook as a model.
For the example, the revenue and value have already been placed 
in columns A and B of the DATA worksheet and the COMPUTE 
worksheet displays the computed covariance in cell B9. For other 
problems, paste the data for two variables into columns A and B 
of the DATA worksheet, overwriting the revenue and value data.
Read the Short Takes for Chapter 3 for an explanation of 
the formulas found in the DATA and COMPUTE worksheets. If 
you use an Excel version older than Excel 2010, use the COM-
PUTE_OLDER worksheet that computes the covariance without 
using the COVARIANCE.S function that was introduced in Ex-
cel 2010.
The Coefficient of Correlation
Key Technique  Use the CORREL(variable 1 cell range, vari-
able 2 cell range) function to compute this measure.
Example  Compute the coefficient of correlation for the NBA 
team revenue and value data of Example 3.18 on page 164.
In-Depth Excel  Use the Correlation workbook as a model.
For the example, the revenue and value have already been placed 
in columns A and B of the DATA worksheet and the COMPUTE 
worksheet displays the coefficient of correlation in cell B14. For 
other problems, paste the data for two variables into columns 
A and B of the DATA worksheet, overwriting the revenue and 
value data.
The COMPUTE worksheet that uses the COVARIANCE.S 
function to compute the covariance (see the previous section) and 
also uses the DEVSQ, COUNT, and SUMPRODUCT functions 
discussed in Appendix F. Open to the COMPUTE_FORMULAS 
worksheet to examine the use of all these functions.
	
Chapter 3 Excel Guide	
175

176	
Chapter 3  Numerical Descriptive Measures
MG3.1  Central Tendency
The Mean, Median, and Mode
Use Descriptive Statistics to compute the mean, the median, the 
mode, and selected measures of variation and shape. For example, 
to create results similar to Figure 3.2 on page 143 that presents de-
scriptive statistics of the one-year return percentage variable for the 
growth and value funds, open to the Retirement Funds worksheet. 
Select Stat u  Basic Statistics u  Display Descriptive Statis-
tics. In the Display Descriptive Statistics dialog box (shown below):
	 1.	 Double-click C9  1YrReturn% in the variables list to add 
'1YrReturn%' to the Variables box and then press Tab.
	 2.	 Double-click C3  Type in the variables list to add Type to the 
By variables (optional) box.
	 3.	 Click Statistics.
In the Display Descriptive Statistics - Statistics dialog box (shown 
below):
	 4.	 Check Mean, Standard deviation, Variance, Coefficient 
of variation, First quartile, Median, Third quartile, In-
terquartile range, Mode, Minimum, Maximum, Range, 
Skewness, Kurtosis, and N total.
	 5.	 Click OK.
	 6.	 Back in the Display Descriptive Statistics dialog box, click OK.
The Geometric Mean
Use Calculator to compute the geometric mean or the geometric 
mean rate of return. For example, to compute the geometric mean 
rate of return for Example 3.4 on page 135, open to the Invest-
ments worksheet. Select Calc u  Calculator. In the Calculator 
dialog box (shown below):
	 1.	 Enter C2  in the Store result in variable box and then press 
Tab. (C2 is the first empty column on the worksheet and the 
result will be placed in row 1 of column C2.)
	 2.	 Double-click Geometric mean in the Functions scrollable 
list to add GMEAN(number) to the Expression box.
	 3.	 Double-click C1  Rates of Return in the variables list to al-
ter the expression to GMEAN('Rates of Return'). (If you 
prefer, you can directly edit the expression as part of the  
next step.)
	 4.	 Edit the expression so that it reads GMEAN('Rates of 
Return') – 1.
	 5.	 Click OK.
MG3.2  Variation and Shape
The Range, Variance, Standard Deviation,  
and Coefficient of Variation
Use Descriptive Statistics to compute these measures of variation 
and shape. The instructions in Section MG3.1 for computing the 
mean, median, and mode also compute these measures.
Z Scores
Use Standardize to compute Z scores. For example, to compute 
the Table 3.4 Z scores shown on page 142, open to the CEREALS 
worksheet. Select Calc u  Standardize. In the Standardize dia-
log box (shown on page 177):
	 1.	 Double-click C2  Calories in the variables list to add  
Calories to the Input column(s) box and press Tab.
	 2.	 Enter C5 in the Store results in box. (C5 is the first empty 
column on the worksheet and the Z scores will be placed in 
column C5.)
	 3.	 Click Subtract mean and divide by standard deviation.
	 4.	 Click OK.
	 5.	 In the new column C5, enter Z Scores as the name of the column.
C h a p t e r  3  M i n i ta b  G u i d e

	
Chapter 3 Minitab Guide	
177
Shape
Use Descriptive Statistics to compute skewness and kurtosis. The 
instructions in Section MG3.1 for computing the mean, median, 
and mode also compute these measures.
MG3.3  Exploring Numerical Data
Quartiles, the Interquartile Range,  
and the Five-Number Summary
Use Descriptive Statistics to compute these measures. The in-
structions in Section MG3.1 for computing the mean, median, and 
mode also compute these measures.
The Boxplot
Use Boxplot. 
For example, to create the Figure 3.4 boxplots on page 153, open 
to the Retirement Funds worksheet. Select Graph u  Boxplot. 
In the Boxplots dialog box:
	 1.	 Click With Groups in the One Y gallery and then click OK.
In the Boxplot-One Y, With Groups dialog box (shown below):
	 2.	 Double-click C9  1YrReturn% in the variables list to add 
'1YrReturn%' to the Graph variables box and then press Tab.
	 3.	 Double-click C3  Type in the variables list to add Type in the 
Categorical variables box.
	 4.	 Click OK.
In the boxplot created, pausing the mouse pointer over the boxplot 
reveals a number of measures, including the quartiles. For prob-
lems that involve single-group data, click Simple in the One Y 
gallery in step 1.
To rotate the boxplots 90 degrees (as was done in Figure 3.5), 
replace step 4 with these steps 4 through 6:
	 4.	 Click Scale.
	 5.	 In the Axes and Ticks tab of the Boxplot–Scale dialog box, 
check Transpose value and category scales and click OK.
	 6.	 Back in the Boxplot-One Y, With Groups dialog box, click OK.
MG3.4  Numerical Descriptive 
Measures for a Population
The Population Mean, Population Variance,  
and Population Standard Deviation
Minitab does not contain commands that compute these popula-
tion parameters directly.
The Empirical Rule and the Chebyshev Rule
Manually compute the values needed to apply these rules using the 
statistics computed in the Section MG3.1 instructions.
MG3.5  The Covariance and the 
Coefficient of Correlation
The Covariance
Use Covariance. 
For example, to compute the covariance for ­Example 3.17 on  
page 160, open to the NBAValues worksheet. Select Stat u  
Basic Statistics u  Covariance. In the Covariance dialog box 
(shown below):
	 1.	 Double-click C3  Revenue in the variables list to add  
Revenue to the Variables box.
	 2.	 Double-click C4  Current Value in the variables list to add 
'Current Value' to the Variables box.
	 3.	 Click OK.
In the table of numbers produced, the covariance is the number 
that appears in the cell position that is the intersection of the two 
variables (the lower-left cell).

178	
Chapter 3  Numerical Descriptive Measures
The Coefficient of Correlation
Use Correlation.
For example, to compute the coefficient of correlation for Exam-
ple 3.18 on page 164, open to the NBAValues worksheet. Select 
Stat u  Basic Statistics u  Correlation. In the Correlation dia-
log box (shown in the right column):
	 1.	 Double-click C3  Revenue in the variables list to add  
Revenue to the Variables box.
	 2.	 Double-click C4  Current Value in the variables list to add 
'Current Value' to the Variables box.
	 3.	 Click OK.

179
U s i n g  S tat i s t i c s
Possibilities at M&R Electronics World
As the marketing manager for M&R Electronics World, you are analyzing the 
results of an intent-to-purchase study. The heads of 1,000 households were asked 
about their intentions to purchase a large-screen HDTV (one that has a screen 
size of at least 50 inches) sometime during the next 12 months. As a follow-up, 
you plan to survey the same people 12 months later to see whether they pur-
chased a television. For households that did purchase a large-screen HDTV, you 
would like to know whether the television they purchased had a faster refresh 
rate (240 Hz or higher) or a standard refresh rate (60 or 120 Hz), whether they 
also purchased a streaming media box in the past 12 months, and whether they 
were satisfied with their purchase of the large-screen HDTV.
You plan to use the results of this survey to form a new marketing strategy 
that will enhance sales and better target those households likely to purchase 
multiple or more expensive products. What questions can you ask in this survey? 
How can you express the relationships among the various intent-to-purchase re-
sponses of individual households?
contents
4.1	 Basic Probability Concepts
4.2	 Conditional Probability
4.3	 Bayes’ Theorem
Think About This: Divine 
Providence and Spam
4.4	 Counting Rules
4.5	 Ethical Issues and Probability
Using Statistics: Possibilities at 
M&R Electronics World, Revisited
Chapter 4 Excel Guide
Chapter 4 Minitab Guide
Objectives
To understand basic probability 
concepts
To learn about conditional 
probability
To use Bayes’ theorem to revise 
probabilities
To learn various counting rules
Chapter
Basic Probability
4
Shock/Fotolia

180	
Chapter 4  Basic Probability
T
he principles of probability help bridge the worlds of descriptive statistics and in-
ferential statistics. Probability principles are the foundation for the probability dis-
tribution, the concept of mathematical expectation, and the binomial, Poisson, and 
hypergeometric distributions, topics that are discussed in Chapter 5. In this chapter, you will 
learn about probability to answer questions such as the following:
 • What is the probability that a household is planning to purchase a large-screen HDTV in 
the next year?
 • What is the probability that a household will actually purchase a large-screen HDTV?
 • What is the probability that a household is planning to purchase a large-screen HDTV 
and actually purchases the television?
 • Given that the household is planning to purchase a large-screen HDTV, what is the prob-
ability that the purchase is made?
 • Does knowledge of whether a household plans to purchase the television change the 
likelihood of predicting whether the household will purchase the television?
 • What is the probability that a household that purchases a large-screen HDTV will pur-
chase a television with a faster refresh rate?
 • What is the probability that a household that purchases a large-screen HDTV with a 
faster refresh rate will also purchase a streaming media box?
 • What is the probability that a household that purchases a large-screen HDTV will be 
satisfied with the purchase?
With answers to questions such as these, you can begin to form a marketing strategy. You 
can consider whether to target households that have indicated an intent to purchase or to fo-
cus on selling televisions that have faster refresh rates or both. You can also explore whether 
households that purchase large-screen HDTVs with faster refresh rates can be easily persuaded 
to also purchase streaming media boxes.
4.1  Basic Probability Concepts
What is meant by the word probability? A probability is the numerical value representing 
the chance, likelihood, or possibility that a particular event will occur, such as the price of 
a stock increasing, a rainy day, a defective product, or the outcome five dots in a single toss 
of a die. In all these instances, the probability involved is a proportion or fraction whose  
value ranges between 0 and 1, inclusive. An event that has no chance of occurring (the  
impossible event) has a probability of 0. An event that is sure to occur (the certain event) 
has a probability of 1.
There are three types of probability:
 • A priori
 • Empirical
 • Subjective
In the simplest case, where each outcome is equally likely, the chance of occurrence of the 
event is defined in Equation (4.1).
Probability of Occurrence
	
probability of occurrence = X
T	
(4.1)
	
where
X = number of ways in which the event occurs
T = total number of possible outcomes
Student Tip
Remember, a probability 
cannot be negative or 
greater than 1.

	
4.1  Basic Probability Concepts	
181
In a priori probability, the probability of an occurrence is based on prior knowledge of 
the process involved. Consider a standard deck of cards that has 26 red cards and 26 black 
cards. The probability of selecting a black card is 26>52 = 0.50 because there are X = 26 
black cards and T = 52 total cards. What does this probability mean? If each card is replaced 
after it is selected, does it mean that 1 out of the next 2 cards selected will be black? No, be-
cause you cannot say for certain what will happen on the next several selections. However, you 
can say that in the long run, if this selection process is continually repeated, the proportion of 
black cards selected will approach 0.50. Example 4.1 shows another example of computing an 
a priori probability.
Example 4.1
Finding A Priori 
Probabilities
A standard six-sided die has six faces. Each face of the die contains either one, two, three, four, 
five, or six dots. If you roll a die, what is the probability that you will get a face with five dots?
Solution  Each face is equally likely to occur. Because there are six faces, the probability 
of getting a face with five dots is 1/6.
The preceding examples use the a priori probability approach because the number of ways 
the event occurs and the total number of possible outcomes are known from the composition of 
the deck of cards or the faces of the die.
In the empirical probability approach, the probabilities are based on observed data, not 
on prior knowledge of a process. Surveys are often used to generate empirical probabilities. 
Examples of this type of probability are the proportion of individuals in the Using Statistics 
scenario who actually purchase large-screen HDTVs, the proportion of registered voters who 
prefer a certain political candidate, and the proportion of students who have part-time jobs. For 
example, if you take a survey of students, and 60% state that they have part-time jobs, then 
there is a 0.60 probability that an individual student has a part-time job.
The third approach to probability, subjective probability, differs from the other two ap-
proaches because subjective probability differs from person to person. For example, the devel-
opment team for a new product may assign a probability of 0.60 to the chance of success for 
the product, while the president of the company may be less optimistic and assign a probability 
of 0.30. The assignment of subjective probabilities to various outcomes is usually based on a 
combination of an individual’s past experience, personal opinion, and analysis of a particular 
situation. Subjective probability is especially useful in making decisions in situations in which 
you cannot use a priori probability or empirical probability.
Events and Sample Spaces
The basic elements of probability theory are the individual outcomes of a variable under study. 
You need the following definitions to understand probabilities.
Student Tip
Events are represented 
by letters of the alphabet.
Event
Each possible outcome of a variable is referred to as an event.
A simple event is described by a single characteristic.
For example, when you toss a coin, the two possible outcomes are heads and tails. Each of 
these represents a simple event. When you roll a standard six-sided die in which the six faces 
of the die contain either one, two, three, four, five, or six dots, there are six possible simple 
events. An event can be any one of these simple events, a set of them, or a subset of all of them. 
For example, the event of an even number of dots consists of three simple events (i.e., two, 
four, or six dots).

182	
Chapter 4  Basic Probability
Getting two heads when you toss a coin twice is an example of a joint event because it 
consists of heads on the first toss and heads on the second toss.
Joint Event
A joint event is an event that has two or more characteristics.
Complement
The complement of event A (represented by the symbol A′) includes all events that are not 
part of A.
Sample Space
The collection of all the possible events is called the sample space.
The complement of a head is a tail because that is the only event that is not a head. The 
complement of five dots on a die is not getting five dots. Not getting five dots consists of get-
ting one, two, three, four, or six dots.
The sample space for tossing a coin consists of heads and tails. The sample space when 
rolling a die consists of one, two, three, four, five, and six dots. Example 4.2 demonstrates 
events and sample spaces.
Example 4.2
Events and Sample 
Spaces
The Using Statistics scenario on page 179 concerns M&R Electronics World. Table 4.1 pres-
ents the results of the sample of 1,000 households in terms of purchase behavior for large-
screen HDTVs.
T a b l e  4 . 1
Purchase Behavior for 
Large-screen HDTVs
What is the sample space? Give examples of simple events and joint events.
Solution  The sample space consists of the 1,000 respondents. Simple events are “planned 
to purchase,” “did not plan to purchase,” “purchased,” and “did not purchase.” The comple-
ment of the event “planned to purchase” is “did not plan to purchase.” The event “planned to 
purchase and actually purchased” is a joint event because in this joint event, the respondent 
must plan to purchase the television and actually purchase it.
Planned to 
Purchase
Actually Purchased
Yes
No
Total
Yes
200
  50
  250
No
100
650
  750
Total
300
700
1,000
Student Tip
The key word when 
describing a joint event 
is and.

	
4.1  Basic Probability Concepts	
183
Contingency Tables and Venn Diagrams
There are several ways in which you can view a particular sample space. One way involves 
using a contingency table (see Section 2.1) such as the one displayed in Table 4.1. You get the 
values in the cells of the table by subdividing the sample space of 1,000 households according 
to whether someone planned to purchase and actually purchased a large-screen HDTV. For 
example, 200 of the respondents planned to purchase a large-screen HDTV and subsequently 
did purchase the large-screen HDTV.
A second way to present the sample space is by using a Venn diagram. This diagram 
graphically represents the various events as “unions” and “intersections” of circles. Figure 4.1 
presents a typical Venn diagram for a two-variable situation, with each variable having only 
two events (A and A′, B and B′). The circle on the left (the red one) represents all events that 
are part of A.
A
A
B
A
B
B
F i g u r e  4 . 1
Venn diagram for events 
A and B
A
A
B
A′
  B′ = 650
A
B = 350
B
50
200
100
F i g u r e  4 . 2
Venn diagram for the 
M&R Electronics World 
example
The circle on the right (the yellow one) represents all events that are part of B. The area 
contained within circle A and circle B (center area) is the intersection of A and B (written as 
A ¨ B), since it is part of A and also part of B. The total area of the two circles is the union of 
A and B (written as A ∪B) and contains all outcomes that are just part of event A, just part of 
event B, or part of both A and B. The area in the diagram outside of A ∪B contains outcomes 
that are neither part of A nor part of B.
You must define A and B in order to develop a Venn diagram. You can define either event 
as A or B, as long as you are consistent in evaluating the various events. For the large-screen 
HDTV example, you can define the events as follows:
 A = planned to purchase 
 B = actually purchased
 A′ = did not plan to purchase  B′ = did not actually purchase
In drawing the Venn diagram (see Figure 4.2), you must determine the value of the inter-
section of A and B so that the sample space can be divided into its parts. A ¨ B consists of all 
200 households who planned to purchase and actually purchased a large-screen HDTV. The 
remainder of event A (planned to purchase) consists of the 50 households who planned to pur-
chase a large-screen HDTV but did not actually purchase one. The remainder of event B (ac-
tually purchased) consists of the 100 households who did not plan to purchase a large-screen 
HDTV but actually purchased one. The remaining 650 households represent those who neither 
planned to purchase nor actually purchased a large-screen HDTV.
Simple Probability
Now you can answer some of the questions posed in the Using Statistics scenario. Because the 
results are based on data collected in a survey (refer to Table 4.1), you can use the empirical 
probability approach.
As stated previously, the most fundamental rule for probabilities is that they range in value 
from 0 to 1. An impossible event has a probability of 0, and an event that is certain to occur has 
a probability of 1.
Simple probability refers to the probability of occurrence of a simple event, P(A). A 
simple probability in the Using Statistics scenario is the probability of planning to purchase 

184	
Chapter 4  Basic Probability
a large-screen HDTV. How can you determine the probability of selecting a household that 
planned to purchase a large-screen HDTV? Using Equation (4.1) on page 180:
 probability of occurrence = X
T
 P1planned to purchase2 = Number who planned to purchase
Total number of households
 =
250
1,000 = 0.25
Thus, there is a 0.25 (or 25%) chance that a household planned to purchase a large-screen 
HDTV.
Example 4.3 illustrates another application of simple probability.
Example 4.3
Computing the 
Probability That the 
Large-Screen HDTV 
Purchased Had a 
Faster Refresh Rate
In the Using Statistics follow-up survey, additional questions were asked of the 300 households 
that actually purchased large-screen HDTVs. Table 4.2 indicates the consumers’ responses to 
whether the television purchased had a faster refresh rate and whether they also purchased a 
streaming media box in the past 12 months.
Find the probability that if a household that purchased a large-screen HDTV is randomly 
selected, the television purchased had a faster refresh rate.
T a b l e  4 . 2
Purchase Behavior 
Regarding 
Purchasing a Faster 
Refresh Rate 
Television and a 
Streaming Media Box
Solution  Using the following definitions:
 A = purchased a television with a faster refresh rate
 A′ = purchased a television with a standard refresh rate
 B = purchased a streaming media box
 B′ = did not purchase a streaming media box
 P(Faster refresh rate) = Number of faster refresh rate televisions purchased
Total number of televisions
 = 80
300 = 0.267
There is a 26.7% chance that a randomly selected large-screen HDTV purchased has a faster 
refresh rate.
Refresh Rate of 
Television Purchased
Streaming Media Box
Yes
No
Total
Faster
  38
  42
  80
Standard
  70
150
220
Total
108
192
300
Joint Probability
Whereas simple probability refers to the probability of occurrence of simple events, joint 
probability refers to the probability of an occurrence involving two or more events. An ex-
ample of joint probability is the probability that you will get heads on the first toss of a coin 
and heads on the second toss of a coin.

	
4.1  Basic Probability Concepts	
185
In Table 4.1 on page 182, the group of individuals who planned to purchase and actually 
purchased a large-screen HDTV consist only of the outcomes in the single cell “yes—planned 
to purchase and yes—actually purchased.” Because this group consists of 200 households, the 
probability of picking a household that planned to purchase and actually purchased a large-
screen HDTV is
 P1planned to purchase and actually purchased2 = planned to purchase and actually purchased
Total number of respondents
 =
200
1,000 = 0.20
Example 4.4 also demonstrates how to determine joint probability.
Example 4.4
Determining the 
Joint Probability 
That a Household 
Purchased a Large-
Screen HDTV with 
a Faster Refresh 
Rate and Purchased 
a Streaming Media 
Box
In Table 4.2 on page 184, the purchases are cross-classified as having a faster refresh rate or 
having a standard refresh rate and whether the household purchased a streaming media box. 
Find the probability that a randomly selected household that purchased a large-screen HDTV 
also purchased a television that had a faster refresh rate and purchased a streaming media box.
Solution  Using Equation (4.1) on page 180,
P(Television with a faster refresh
rate and streaming media box)  =
Number that purchased a television with a faster
refresh rate and purchased a streaming media box
Total number of large@screen HDTV purchasers
 = 38
300 = 0.127
Therefore, there is a 12.7% chance that a randomly selected household that purchased a large-
screen HDTV purchased a television that had a faster refresh rate and purchased a streaming 
media box.
Marginal Probability
The marginal probability of an event consists of a set of joint probabilities. You can deter-
mine the marginal probability of a particular event by using the concept of joint probability 
just discussed. For example, if B consists of two events, B1 and B2, then P(A), the probability 
of event A, consists of the joint probability of event A occurring with event B1 and the joint 
probability of event A occurring with event B2. You use Equation (4.2) to compute marginal 
probabilities.
Marginal Probability
	
P1A2 = P1A and B12 + P1A and B22 + g + P1A and Bk2	
(4.2)
where B1, B2, g, Bk are k mutually exclusive and collectively exhaustive events, defined 
as follows:
Two events are mutually exclusive if both the events cannot occur simultaneously.
A set of events is collectively exhaustive if one of the events must occur.

186	
Chapter 4  Basic Probability
Heads and tails in a coin toss are mutually exclusive events. The result of a coin toss cannot 
simultaneously be a head and a tail. Heads and tails in a coin toss are also collectively exhaus-
tive events. One of them must occur. If heads does not occur, tails must occur. If tails does not 
occur, heads must occur. Being male and being female are mutually exclusive and collectively 
exhaustive events. No person is both (the two are mutually exclusive), and everyone is one or 
the other (the two are collectively exhaustive).
You can use Equation (4.2) to compute the marginal probability of “planned to purchase” 
a large-screen HDTV:
 P1planned to purchase2 = P1planned to purchase and purchased2
 + P1planned to purchase and did not purchase2
 =
200
1,000 +
50
1,000
 =
250
1,000 = 0.25
You get the same result if you add the number of outcomes that make up the simple event 
“planned to purchase.”
General Addition Rule
How do you find the probability of event “A or B”? You need to consider the occurrence of 
either event A or event B or both A and B. For example, how can you determine the probability 
that a household planned to purchase or actually purchased a large-screen HDTV?
The event “planned to purchase or actually purchased” includes all households that planned 
to purchase and all households that actually purchased a large-screen HDTV. You examine each 
cell of the contingency table (Table 4.1 on page 182) to determine whether it is part of this event. 
From Table 4.1, the cell “planned to purchase and did not actually purchase” is part of the event 
because it includes respondents who planned to purchase. The cell “did not plan to purchase and 
actually purchased” is included because it contains respondents who actually purchased. Finally, 
the cell “planned to purchase and actually purchased” has both characteristics of interest. There-
fore, one way to calculate the probability of “planned to purchase or actually purchased” is
P1planned to purchase or actually purchased2 =  P (planned to purchase and did not actually
 purchase) + P(Did not plan to
 purchase and actually purchased) +
 P(planned to purchase and actually purchased)
 =
50
1,000 +
100
1,000 +
200
1,000
 =
350
1,000 = 0.35
Often, it is easier to determine P(A or B), the probability of the event A or B, by using the general 
addition rule, defined in Equation (4.3).
Student Tip
The key word when using 
the addition rule is or.
General Addition Rule
The probability of A or B is equal to the probability of A plus the probability of B minus 
the probability of A and B.
	
P1A or B2 = P1A2 + P1B2 - P1A and B2	
(4.3)

	
4.1  Basic Probability Concepts	
187
Applying Equation (4.3) to the previous example produces the following result:
P1planned to purchase or actually purchased2 =  P1planned to purchase2
 + P1Actually purchased2 - P(planned to
 purchase and actually purchased)
 =
250
1, 000 +
300
1, 000 -
200
1, 000
 =
350
1, 000 = 0.35
The general addition rule consists of taking the probability of A and adding it to the probabil-
ity of B and then subtracting the probability of the joint event A and B from this total because the 
joint event has already been included in computing both the probability of A and the probability 
of B. Referring to Table 4.1 on page 182, if the outcomes of the event “planned to purchase” are 
added to those of the event “actually purchased,” the joint event “planned to purchase and actually 
purchased” has been included in each of these simple events. Therefore, because this joint event 
has been included twice, you must subtract it to compute the correct result. Example 4.5 illustrates 
another application of the general addition rule.
Example 4.5
Using the General 
Addition Rule for 
the Households 
That Purchased 
Large-Screen 
HDTVs
In Example 4.3 on page 184, the purchases were cross-classified in Table 4.2 as televisions that 
had a faster refresh rate or televisions that had a standard refresh rate and whether the household 
purchased a streaming media box. Find the probability that among households that purchased 
a large-screen HDTV, they purchased a television that had a faster refresh rate or purchased a 
streaming media box.
Solution  Using Equation (4.3),
 P(Television had a faster refresh = P(Television had a faster refresh rate)
 rate or purchased a streaming media box)
+ P(purchased a streaming media box)
 
- P(Television had a faster refresh
 
rate and purchased a streaming media box)
 = 80
300 + 108
300 - 38
300
 = 150
300 = 0.50
Therefore, of households that purchased a large-screen HDTV, there is a 50% chance that a 
randomly selected household purchased a television that had a faster refresh rate or purchased 
a streaming media box.
Problems for Section 4.1
Learning the Basics
4.1  Two coins are tossed.
a.	 Give an example of a simple event.
b.	 Give an example of a joint event.
c.	 What is the complement of a head on the first toss?
d.	 What does the sample space consist of?
4.2  A box contains 14 red pens and 10 green pens. A pen is to be 
selected at random. 
a.	 Give an example of a simple event.
b.	  What is the complement of a green pen?

188	
Chapter 4  Basic Probability
4.3  Consider the following contingency table:
4.8  Do males or females feel more tense or stressed out at work? 
A survey of employed adults conducted online by Harris Interactive 
on behalf of the American Psychological Association revealed the 
following:
D
D′
C
12
28
C′
24
36
What is the probability of event
a.	 D′?
b.	 D and C?
c.	 D′ and C′?
d.	 D′ or C′?
4.4  Consider the following contingency table:
D
D′
C
10
16
C′
12
12
What is the probability of event
a.	 D′?
b.	 D and C?
c.	 D′ and C′?
d.	 D′ or C′?
Applying the Concepts
4.5  For each of the following, indicate whether the type of prob-
ability involved is an example of a priori probability, empirical 
probability, or subjective probability.
a.	 The next child born in the Walton family will be a boy.
b.	 The next Nobel Prize for Literature will be awarded to some-
one from Africa.
c.	 The sum of numbers when rolling two fair dice will be 9.
d.	 The plane flying to deliver medical supplies will be early by 
more than 15 minutes.
4.6  For each of the following, state whether the events created are 
mutually exclusive and whether they are collectively exhaustive.
a.	 Undergraduate business students were asked whether they were 
sophomores or juniors.
b.	 Each respondent was classified by the type of car he or she 
drives: sedan, SUV, American, European, Asian, or none.
c.	 People were asked, “Do you currently live in (i) an apartment 
or (ii) a house?”
d.	 A product was classified as defective or not defective.
4.7  Which of the following events occur with a probability of 
zero? For each, state why or why not. 
a.	 A computer system that has both Apple and Toshiba branding.
b.	 A person who was born in January and has a birthday in 
March. 
c.	 Getting the number 10 when rolling a single fair dice.
d.	 A company that has more than one branch in different  
countries.
Felt Tense or  
Stressed Out at Work
Gender
Yes
No
Male
244
495
Female
282
480
Source: Data extracted from “The 2013 
Work and Well-Being Survey,” American 
Psychological Association and Harris 
Interactive, March 2013, p. 5, bit.ly/11JGcPf.
a.	 Give an example of a simple event.
b.	 Give an example of a joint event.
c.	 What is the complement of “Felt tense or stressed out at work”?
d.	 Why is “Male and felt tense or stressed out at work” a joint event?
4.9  Referring to the contingency table in Problem 4.8, if an em-
ployed adult is selected at random, what is the probability that
a.	 the employed adult felt tense or stressed out at work?
b.	 the employed adult was a male who felt tense or stressed out at 
work?
c.	 the employed adult was a male or felt tense or stressed out at 
work?
d.	 Explain the difference in the results in (b) and (c).
4.10  A poll revealed that the oil spill in the Gulf of Mexico in 
2012 has made people in different countries more aware about en-
vironmental damage as well as about dependency on using oil as a 
fuel source. Two-third of the adult population of America (67%), 
and a little more than half (52%) of the adult population of Eu-
rope are aware of the spill. Assume that the following data was 
gathered. 
USA
Europe
Total
Aware
670
520
1,190
Unaware
330
480
810
Total
1,000
1,000
2,000
a.	 Give an example of a simple event.
b.	 Give an example of a joint event.
c.	 Find the complement of the set of respondents who are from 
Europe.
d.	 Why is the set of respondents who are unaware and are from 
Europe a joint event?
4.11  Referring to the contingency table in Problem 4.10, if a re-
spondent is selected at random, what is the probability that 
a.	 the respondent is from the USA?
b.	 the respondent is unaware of the 2012 oil spill?
c.	 the respondent is unaware of the spill or is from USA?
d.	 Explain the difference in the results in (b) and (c)?

	
4.2  Conditional Probability	
189
SELF 
Test 
4.12  What business and technical skills are critical 
for today’s business intelligence/analytics and informa-
tion management professionals? As part of InformationWeek’s 
2013 U.S. IT Salary Survey, business intelligence/analytics and 
information management professionals, both staff and managers, 
were asked to indicate what business and technical skills are criti-
cal to their job. The list of business and technical skills included 
Analyzing Data. The following table summarizes the responses to 
this skill:
If an American is selected at random, what is the probability that 
he or she
a.	 prefers Pepsi?
b.	 is male and prefers Pepsi?
c.	 is male or prefers Pepsi?
d.	 Explain the difference in the results in (b) and (c).
4.14  A survey of 1,085 adults asked, “Do you enjoy shopping 
for clothing for yourself?” The results (data extracted from “Split  
Decision on Clothes Shopping,” USA Today, January 28, 2011,  
p. 1B) indicated that 51% of the females enjoyed shopping for 
clothing for themselves as compared to 44% of the males. The 
sample sizes of males and females were not provided. Suppose 
that the results indicated that of 542 males, 238 answered yes. Of 
543 females, 276 answered yes. Construct a contingency table to 
evaluate the probabilities. What is the probability that a respon-
dent chosen at random
a.	 enjoys shopping for clothing for himself or herself?
b.	 is a female and enjoys shopping for clothing for herself?
c.	 is a female or is a person who enjoys shopping for clothing?
d.	 is a male or a female?
4.15  Each year, ratings are compiled concerning the performance 
of new cars during the first 90 days of use. Suppose that the cars 
have been categorized according to whether a car needs warranty-
related repair (yes or no) and the country in which the company 
manufacturing a car is based (United States or not United States). 
Based on the data collected, the probability that the new car needs 
a warranty repair is 0.04, the probability that the car was manufac-
tured by a U.S.-based company is 0.60, and the probability that the 
new car needs a warranty repair and was manufactured by a U.S.-
based company is 0.025. Construct a contingency table to evaluate 
the probabilities of a warranty-related repair. What is the probabil-
ity that a new car selected at random
a.	 needs a warranty repair?
b.	 needs a warranty repair and was manufactured by a U.S.-based 
company?
c.	 needs a warranty repair or was manufactured by a U.S.-based 
company?
d.	 needs a warranty repair or was not manufactured by a U.S.-
based company?
Analyzing Data
Professional Position
Staff
Management
Total
Critical
4,374
3,633
  8,007
Not critical
3,436
2,631
  6,067
Total
7,810
6,264
14,074
Source: Data extracted from “IT Salaries Show Slow Growth,” 
InformationWeek Reports, April 2013, p. 40, ubm.io/1ewjKT5.
If a professional is selected at random, what is the probability that 
he or she
a.	 indicates analyzing data as critical to his or her job?
b.	 is a manager?
c.	 indicates analyzing data as critical to his or her job or is a manager?
d.	 Explain the difference in the results in (b) and (c).
4.13  Do Americans prefer Coke or Pepsi? A survey was conducted 
by Public Policy Polling (PPP) in 2013; the results were as follows:
Gender
Preference
Female
Male
Total
Coke
120
  95
215
Pepsi
  95
  80
175
Neither/Unsure
  65
  45
110
Total
280
220
500
Source: Data extracted from “Public Policy Polling” Report 
2013, bit.ly/YKXfzN.
4.2  Conditional Probability
Each example in Section 4.1 involves finding the probability of an event when sampling from 
the entire sample space. How do you determine the probability of an event if you know certain 
information about the events involved?
Computing Conditional Probabilities
Conditional probability refers to the probability of event A, given information about the 
­occurrence of another event, B.

190	
Chapter 4  Basic Probability
Referring to the Using Statistics scenario involving the purchase of large-screen HDTVs, 
suppose you were told that a household planned to purchase a large-screen HDTV. Now, what 
is the probability that the household actually purchased the television?
In this example, the objective is to find P(Actually purchased  Planned to purchase). Here 
you are given the information that the household planned to purchase the large-screen HDTV. 
Therefore, the sample space does not consist of all 1,000 households in the survey. It consists of 
only those households that planned to purchase the large-screen HDTV. Of 250 such households, 
200 actually purchased the large-screen HDTV. Therefore, based on Table 4.1 on page 182, the 
probability that a household actually purchased the large-screen HDTV given that they planned 
to purchase is
 P1Actually purchased planned to purchase2 = planned to purchase and actually purchased
planned to purchase
 = 200
250 = 0.80
You can also use Equation (4.4b) to compute this result:
P1BA2 = P1A and B2
P1A2
where
A = planned to purchase
B = actually purchased
then
 P1Actually purchased planned to purchase2 = 200>1,000
250>1,000
 = 200
250 = 0.80
Example 4.6 further illustrates conditional probability.
Student Tip
The variable that is given 
goes in the denominator 
of Equation (4.4). Since 
you were given planned 
to purchase, planned 
to purchase is in the 
denominator.
Conditional Probability
The probability of A given B is equal to the probability of A and B divided by the probabil-
ity of B.
	
P1AB2 = P1A and B2
P1B2
	
(4.4a)
The probability of B given A is equal to the probability of A and B divided by the 
probability of A.
	
P1BA2 = P1A and B2
P1A2
	
(4.4b)
	
where 
 P(A and B) = joint probability of A and B
 P(A) = marginal probability of A
 P(B) = marginal probability of B

	
4.2  Conditional Probability	
191
Decision Trees
In Table 4.1 on page 182, households are classified according to whether they planned to 
­purchase and whether they actually purchased large-screen HDTVs. A decision tree is an 
­alternative to the contingency table. Figure 4.3 represents the decision tree for this example.
Example 4.6
Finding the Condi-
tional Probability  
of Purchasing  
a Streaming  
Media Box
Table 4.2 on page 184 is a contingency table for whether a household purchased a television 
with a faster refresh rate and whether the household purchased a streaming media box. If a 
household purchased a television with a faster refresh rate, what is the probability that it also 
purchased a streaming media box?
Solution  Because you know that the household purchased a television with a faster refresh 
rate, the sample space is reduced to 80 households. Of these 80 households, 38 also purchased a 
streaming media box. Therefore, the probability that a household purchased a streaming ­media 
box, given that the household purchased a television with a faster refresh rate, is
P(purchased streaming media boxpurchased
television with faster refresh rate)
=
Number purchasing television with
faster refresh rate and streaming media box
Number purchasing television
with faster refresh rate
 = 38
80 = 0.475
If you use Equation (4.4b) on page 190:
A = purchased a television with a faster refresh rate
B = purchased a streaming media box
then
P1BA2 = P1A and B2
P1A2
= 38>300
80>300 = 0.475
Therefore, given that the household purchased a television with a faster refresh rate, there 
is a 47.5% chance that the household also purchased a streaming media box. You can compare 
this conditional probability to the marginal probability of purchasing a streaming media box, 
which is 108>300 = 0.36,  or 36%. These results tell you that households that purchased tele-
visions with a faster refresh rate are more likely to purchase a streaming media box than are 
households that purchased large-screen HDTVs that have a standard refresh rate.
F i g u r e  4 . 3
Decision tree for planned 
to purchase and actually 
purchased
Entire
Set of
Households
Planned to
Purchase
Did Not Plan
to Purchase
Actually Purchased
Actually Purchased
Did Not Actually
Purchase
Did Not Actually
Purchase
P(A′) 5  750 
1,000
P(A and B′)= 50
1,000
P(A′ and B) = 100
1,000
P(A′ and B′) = 650
1,000
P(A) =  250 
1,000
P(A and B) = 200
1,000

192	
Chapter 4  Basic Probability
In Figure 4.3, beginning at the left with the entire set of households, there are two 
“branches” for whether or not the household planned to purchase a large-screen HDTV. Each 
of these branches has two subbranches, corresponding to whether the household actually pur-
chased or did not actually purchase the large-screen HDTV. The probabilities at the end of 
the initial branches represent the marginal probabilities of A and A′. The probabilities at the 
end of each of the four subbranches represent the joint probability for each combination of 
events A and B. You compute the conditional probability by dividing the joint probability by 
the ­appropriate marginal probability.
For example, to compute the probability that the household actually purchased, given that 
the household planned to purchase the large-screen HDTV, you take P(Planned to purchase 
and actually purchased) and divide by P(Planned to purchase). From Figure 4.3,
 P1Actually purchased planned to purchase2 = 200>1,000
250>1,000
 = 200
250 = 0.80
Example 4.7 illustrates how to construct a decision tree.
Example 4.7
Constructing the 
Decision Tree for the 
Households That 
Purchased Large-
Screen HDTVs
Using the cross-classified data in Table 4.2 on page 184, construct the decision tree. Use the 
decision tree to find the probability that a household purchased a streaming media box, given 
that the household purchased a television with a faster refresh rate.
Solution  The decision tree for purchased a streaming media box and a television with a 
faster refresh rate is displayed in Figure 4.4.
F i g u r e  4 . 4
Decision tree for 
purchased a television 
with a faster refresh rate 
and a streaming media 
box
Entire
Set of
Households
Purchased Faster 
Refresh Rate Television 
Did Not Purchase
Faster Refresh Rate
Television
Purchased Streaming
Media Box
Purchased Streaming
Media Box
Did Not Purchase
Streaming Media Box
Did Not Purchase
Streaming Media Box
P(A′) = 220
300
P(A and B′) =  42 
300
P(A′ and B) = 70 
300
P(A′ and B′) =  150 
300
P(A) = 80
300
P(A and B) =  38 
300
Using Equation (4.4b) on page 190 and the following definitions,
A = purchased a television with a faster refresh rate
B = purchased a streaming media box
P1BA2 = P1A and B2
P1A2
= 38>300
80>300 = 0.475

	
4.2  Conditional Probability	
193
Independence
In the example concerning the purchase of large-screen HDTVs, the conditional probability is 
200>250 = 0.80 that the selected household actually purchased the large-screen HDTV, given that 
the household planned to purchase. The simple probability of selecting a household that actually 
purchased is 300>1,000 = 0.30. This result shows that the prior knowledge that the household 
planned to purchase affected the probability that the household actually purchased the television. 
In other words, the outcome of one event is dependent on the outcome of a second event.
When the outcome of one event does not affect the probability of occurrence of another 
event, the events are said to be independent. Independence can be determined by using 
Equation (4.5).
Independence
Two events, A and B, are independent if and only if
	
P1AB2 = P1A2	
(4.5)
	
where 
 P1AB2 = conditional probability of A given B
 P1A2 = marginal probability of A
Example 4.8 demonstrates the use of Equation (4.5).
Example 4.8
Determining  
Independence
In the follow-up survey of the 300 households that actually purchased large-screen HDTVs, 
the households were asked if they were satisfied with their purchases. Table 4.3 cross-classifies 
the responses to the satisfaction question with the responses to whether the television had a 
faster refresh rate.
T a b l e  4 . 3
Satisfaction with 
Purchase of  
Large-Screen  
HDTVs
Television  
Refresh Rate
Satisfied With Purchase?
Yes
No
Total
Faster
  64
16
  80
Standard
176
44
220
Total
240
60
300
Determine whether being satisfied with the purchase and the refresh rate of the television 
purchased are independent.
Solution  For these data,
P1Satisfied Faster refresh rate2 = 64>300
80>300 = 64
80 = 0.80
which is equal to
P1Satisfied2 = 240
300 = 0.80
Thus, being satisfied with the purchase and the refresh rate of the television purchased are 
­independent. Knowledge of one event does not affect the probability of the other event.

194	
Chapter 4  Basic Probability
Multiplication Rules
The general multiplication rule is derived using Equation (4.4a) on page 190:
P1AB2 = P1A and B2
P1B2
and solving for the joint probability P(A and B).
General Multiplication Rule
The probability of A and B is equal to the probability of A given B times the probability of B.
	
P1A and B2 = P1AB2P1B2	
(4.6)
Multiplication Rule for Independent Events
If A and B are independent, the probability of A and B is equal to the probability of A times 
the probability of B.
	
P1A and B2 = P1A2P1B2	
(4.7)
Example 4.9
Using the General 
Multiplication Rule
Consider the 80 households that purchased televisions that had a faster refresh rate. In Table 4.3 
on page 193, you see that 64 households are satisfied with their purchase, and 16 households 
are ­dissatisfied. Suppose 2 households are randomly selected from the 80 households. Find the 
probability that both households are satisfied with their purchase.
Solution  Here you can use the multiplication rule in the following way. If
 A = second household selected is satisfied
 B = first household selected is satisfied
then, using Equation (4.6),
P1A and B2 = P1AB2P1B2
The probability that the first household is satisfied with the purchase is 64>80. However, the 
probability that the second household is also satisfied with the purchase depends on the ­result 
of the first selection. If the first household is not returned to the sample after the ­satisfaction 
level is determined (i.e., sampling without replacement), the number of households remaining 
is 79. If the first household is satisfied, the probability that the second is also satisfied is 63>79 
because 63 satisfied households remain in the sample. Therefore,
P1A and B2 = a 63
79 b a 64
80 b = 0.6380
There is a 63.80% chance that both of the households sampled will be satisfied with their purchase.
Example 4.9 demonstrates the use of the general multiplication rule.
The multiplication rule for independent events is derived by substituting P(A) for 
P1AB2 in Equation (4.6).

	
4.2  Conditional Probability	
195
If this rule holds for two events, A and B, then A and B are independent. Therefore, there are 
two ways to determine independence:
	
1.	Events A and B are independent if, and only if, P1AB2 = P1A2.
	
2.	Events A and B are independent if, and only if, P1A and B2 = P1A2P1B2.
Marginal Probability Using the General Multiplication Rule
In Section 4.1, marginal probability was defined using Equation (4.2) on page 185. You can 
state the equation for marginal probability by using the general multiplication rule. If
P1A2 = P1A and B12 + P1A and B22 + g + P1A and Bk2
then, using the general multiplication rule, Equation (4.8) defines the marginal probability.
Marginal Probability Using the General Multiplication Rule
	
P1A2 = P1AB12P1B12 + P1AB22P1B22 + g + P1ABk2P1Bk2	
(4.8)
where B1, B2, c , Bk are k mutually exclusive and collectively exhaustive events.
To illustrate Equation (4.8), refer to Table 4.1 on page 182. Let
 P1A2 = probability of planned to purchase
 P(B1) = probability of actually purchased
 P(B2) = probability of did not actually purchase
Then, using Equation (4.8), the probability of planned to purchase is
 P1A2 = P1AB12P1B12 + P1AB22P1B22
 = a 200
300b a 300
1,000b + a 50
700b a 700
1,000b
 =
200
1,000 +
50
1,000 =
250
1,000 = 0.25
Problems for Section 4.2
Learning the Basics
4.16  Consider the following contingency table:
4.17  Consider the following contingency table:
B
B′
A
10
20
A′
20
40
What is the probability of
a.	 A B?
b.	 A  B′?
c.	 A′B′?
d.	 Are events A and B independent?
B
B′
A
10
30
A′
25
35
What is the probability of
a.	 A B?
b.	 A′ B′?
c.	 A B′?
d.	 Are events A and B independent?
4.18  If P1A and B2 = 0.4 and P1B2 = 0.8,  find P1A B2.

196	
Chapter 4  Basic Probability
4.19  If P1A2 = 0.7, P1B2 = 0.6, and A and B are independent, 
find P(A and B).
4.20  If P1A2 = 0.3, P1B2 = 0.4,  and P1A and B2 = 0.2,  are A 
and B independent?
Applying the Concepts
4.21  The following data is from an online survey in the USA and 
European countries on whether they consider air travel safe. 
4.23  Do Americans prefer Coke or Pepsi? A survey was con-
ducted by Public Policy Polling (PPP) in 2013; the results were 
as follows:
Air Travel Safety,  
2008 Survey
Safe
Unsafe
USA
69
31
UK
72
28
Source: Data extracted from Harrisinteractive.
com, 2008.
a.	 Given that the respondent feels that air travel is unsafe, find the 
probability that they are from the USA.
b.	 Given that the respondent is from the USA, find the probability 
that they feel air travel is unsafe.
c.	 Explain the difference in the result in (a) and (b).
d.	 Are feeling unsafe in air travel and being a citizen of USA, inde-
pendent events?
4.22  A poll revealed that the oil spill in the Gulf of Mexico in 
2012 has made people in different countries more aware about en-
vironmental damage as well as about dependency on using oil as a 
fuel source. Two-third of the adult population of America (67%), 
and a little more than half (52%) of the adult population of Europe 
are aware of the spill. Assume that the following data was gathered.
USA
Europe
Total
Aware
670
520
1,190
Unaware
   330
   480
810
Total
1,000
1,000
2,000
a.	 Suppose that you know that the respondent is from the USA. 
What is the probability that the respondent is aware of the 
2012 oil spill in the Gulf of Mexico?
b.	 Suppose that you know that the respondent is from Europe. 
What is the probability that the respondent is aware of the 
2012 oil spill in the Gulf of Mexico?
c.	 Are being aware of the 2012 oil spill and hailing from USA or 
Europe, independent events?
Gender
Preference
Female
Male
Total
Coke
120
  95
215
Pepsi
  95
  80
175
Neither/Unsure
  65
  45
110
Total
280
220
500
Source: Data extracted from “Public Policy Polling” Report 
2013, bit.ly/YKXfzN.
a.	 Given that an American is a male, what is the probability that 
he prefers Pepsi?
b.	 Given that an American is a female, what is the probability that 
she prefers Pepsi?
c.	 Is preference independent of gender? Explain.
SELF 
Test 
4.24  What business and technical skills are critical 
for today’s business intelligence/analytics and infor-
mation management professionals? As part of InformationWeek’s 
2013 U.S. IT Salary Survey, business intelligence/analytics and 
information management professionals, both staff and managers, 
were asked to indicate what business and technical skills are criti-
cal to their job. The list of business and technical skills included 
Analyzing Data. The following table summarizes the responses to 
this skill:
Analyzing Data
Professional Position
Staff
Management
Total
Critical
4,374
3,633
  8,007
Not critical
3,436
2,631
  6,067
Total
7,810
6,264
14,074
Source: Data extracted from “IT Salaries Show Slow Growth,” 
InformationWeek Reports, April 2013, p. 40, ubm.io/1ewjKT5.
a.	 Given that a professional is staff, what is the probability that 
the professional indicates analyzing data as critical to his or her 
job?
b.	 Given that a professional is staff, what is the probability that 
the professional does not indicate analyzing data as critical to 
his or her job?
c.	 Given that a professional is a manager, what is the probability 
that the professional indicates analyzing data as critical to his 
or her job?
d.	 Given that a professional is a manager, what is the probability 
that the professional does not indicate analyzing data as critical 
to his or her job?

	
4.3  Bayes’ Theorem	
197
4.25  In a survey  2,085 college students were asked, “Do you en-
joy living on campus?” The result indicated that 61% of the males 
enjoyed living on campus compared to 54% of the females. The 
sample sizes of the males and females were not provided. Suppose 
that the results are as given in the following table:
a.	 If a year is selected at random, what is the probability that the 
S&P 500 finished higher for the year?
b.	 Given that the S&P 500 finished higher after the first five days of 
trading, what is the probability that it finished higher for the year?
c.	 Are the two events “first-week performance” and “annual per-
formance” independent? Explain.
d.	 Look up the performance after the first five days of 2013 and the 
2013 annual performance of the S&P 500 at finance.yahoo.com. 
Comment on the results.
4.28  A standard deck of cards is being used to play a game. There 
are four suits (hearts, diamonds, clubs, and spades), each having 13 
faces (ace, 2, 3, 4, 5, 6, 7, 8, 9, 10, jack, queen, and king), making a 
total of 52 cards. This complete deck is thoroughly mixed, and you 
will receive the first 2 cards from the deck, without replacement 
(the first card is not returned to the deck after it is selected).
a.	 What is the probability that both cards are queens?
b.	 What is the probability that the first card is a 10 and the second 
card is a 5 or 6?
c.	 If you were sampling with replacement (the first card is returned 
to the deck after it is selected), what would be the answer in (a)?
d.	 In the game of blackjack, the face cards (jack, queen, king) 
count as 10 points, and the ace counts as either 1 or 11 points. 
All other cards are counted at their face value. Blackjack is 
achieved if 2 cards total 21 points. What is the probability of 
getting blackjack in this problem?
4.29  A box of nine gloves contains two left-handed gloves and 
seven right-handed gloves.
a.	 If two gloves are randomly selected from the box, without re-
placement (the first glove is not returned to the box after it is 
selected), what is the probability that both gloves selected will 
be right-handed?
b.	 If two gloves are randomly selected from the box, without re-
placement (the first glove is not returned to the box after it is 
selected), what is the probability that there will be one right-
handed glove and one left-handed glove selected?
c.	 If three gloves are selected, with replacement (the gloves are 
returned to the box after they are selected), what is the prob-
ability that all three will be left-handed?
d.	 If you were sampling with replacement (the first glove is ­returned 
to the box after it is selected), what would be the ­answers to (a) 
and (b)?
Gender
Male
Female
Total
Yes
636
563
1,199
No
406
480
   886
Total
1,042
1,043
2,085
a.	 Suppose a female respondent is chosen, what is the probability 
that she does not enjoy living on campus?
b.	 Suppose a respondent enjoys living on campus, what is the prob-
ability that the individual is male?
c.	 Are enjoying living on campus and gender independent?  
Explain.
4.26  Each year, ratings are compiled concerning the performance 
of new cars during the first 90 days of use. Suppose that the cars 
have been categorized according to whether a car needs warranty-
related repair (yes or no) and the country in which the company 
manufacturing a car is based (United States or not United States). 
Based on the data collected, the probability that the new car needs 
a warranty repair is 0.04, the probability that the car is manufac-
tured by a U.S.-based company is 0.60, and the probability that the 
new car needs a warranty repair and was manufactured by a U.S.-
based company is 0.025.
a.	 Suppose you know that a company based in the United States 
manufactured a particular car. What is the probability that the 
car needs a warranty repair?
b.	 Suppose you know that a company based in the United States 
did not manufacture a particular car. What is the probability 
that the car needs a warranty repair?
c.	 Are need for a warranty repair and location of the company 
manufacturing the car independent?
4.27  In 40 of the 62 years from 1950 through 2012 (in 2011 there 
was virtually no change), the S&P 500 finished higher after the 
first five days of trading. In 35 of those 40 years, the S&P 500 
finished higher for the year. Is a good first week a good omen for 
the upcoming year? The following table gives the first-week and 
annual performance over this 62-year period:
First Week
S&P 500’s Annual Performance
Higher
Lower
Higher
35
  5
Lower
11
11
4.3  Bayes’ Theorem
Bayes’ theorem is used to revise previously calculated probabilities based on new informa-
tion. Developed by Thomas Bayes in the eighteenth century (see references 1 and 6), Bayes’ 
theorem is an extension of what you previously learned about conditional probability.
You can apply Bayes’ theorem to the situation in which M&R Electronics World is con-
sidering marketing a new model of televisions. In the past, 40% of the new-model televisions 
have been successful, and 60% have been unsuccessful. Before introducing the new-model 

198	
Chapter 4  Basic Probability
television, the marketing research department conducts an extensive study and ­releases 
a ­report, either ­favorable or unfavorable. In the past, 80% of the successful new-model 
television(s) had ­received favorable ­market research reports, and 30% of the unsuccessful 
new-model television(s) had received favorable reports. For the new model of television un-
der consideration, the marketing research department has issued a favorable report. What is 
the probability that the television will be successful?
Bayes’ theorem is developed from the definition of conditional probability. To find the 
conditional probability of B given A, consider Equation (4.4b) (originally presented on 
page 190 and shown again below):
P1BA2 = P1A and B2
P1A2
= P1AB2P1B2
P1A2
Bayes’ theorem is derived by substituting Equation (4.8) on page 195 for P(A) in the ­denominator of 
Equation (4.4b).
Bayes’ Theorem
	
P(Bi A) =
P(ABi)P(Bi)
P(AB1)P(B1) + P(AB2)P(B2) + g + P(ABk)P(Bk)	
(4.9)
where Bi is the ith event out of k mutually exclusive and collectively exhaustive events.
To use Equation (4.9) for the television-marketing example, let
 event S = successful television 
 event F = favorable report
 event S′ = unsuccessful television  event F′ = unfavorable report
and
 P1S2 = 0.40   P1FS2 = 0.80
 P1S′2 = 0.60   P1FS′2 = 0.30
Then, using Equation (4.9),
 P1S F2 =
P1FS2P1S2
P1FS2P1S2 + P1FS′2P1S′2
 =
10.80210.402
10.80210.402 + 10.30210.602
 =
0.32
0.32 + 0.18 = 0.32
0.50
 = 0.64
The probability of a successful television, given that a favorable report was received, is 
0.64. Thus, the probability of an unsuccessful television, given that a favorable report was re-
ceived, is 1 - 0.64 = 0.36.
Table 4.4 summarizes the computation of the probabilities, and Figure 4.5 presents the 
decision tree. 

	
4.3  Bayes’ Theorem	
199
T a b l e  4 . 4
Bayes’ Theorem 
Computations for the 
Television-Marketing 
Example
Event Si
Prior  
Probability  
P1Si2
Conditional  
Probability  
P1F ∣Si2
Joint  
Probability  
P1F ∣Si2 P1Si2
Revised  
Probability  
P1Si∣F2
 S = successful  
television
0.40
0.80
0.32
 P1SF2 = 0.32>0.50
 = 0.64
S′ = unsuccessful  
television
0.60
0.30
0.18
 0.50
 P1S′ F2 = 0.18>0.50
 = 0.36
F i g u r e  4 . 5
Decision tree for 
marketing a new 
television
P(S) = 0.40 
P(S′) = 0.60 
P(S′ and F′) = P(F′|S′) P(S′)
                     = (0.70) (0.60) = 0.42  
P(S and F′) = P(F′|S) P(S)
                    = (0.20) (0.40) = 0.08  
P(S′ and F) = P(F|S′) P(S′)
                    = (0.30) (0.60) = 0.18  
P(S and F) = P(F|S) P(S)
                   = (0.80) (0.40) = 0.32  
Example 4.10
Using Bayes’ Theo-
rem in a Medical 
Diagnosis Problem
The probability that a person has a certain disease is 0.03. Medical diagnostic tests are avail-
able to determine whether the person actually has the disease. If the disease is actually present, 
the probability that the medical diagnostic test will give a positive result (indicating that the 
disease is present) is 0.90. If the disease is not actually present, the probability of a positive test 
result (indicating that the disease is present) is 0.02. Suppose that the medical diagnostic test 
has given a positive result (indicating that the disease is present). What is the probability that 
the disease is actually present? What is the probability of a positive test result?
Solution  Let
 event D = has disease 
 event T = test is positive
 event D′ = does not have disease  event T′ = test is negative
and
 P1D2 = 0.03  P1TD2 = 0.90
 P1D′2 = 0.97  P1TD′2 = 0.02
Using Equation (4.9) on page 198,
 P1DT2 =
P1TD2P1D2
P1TD2P1D2 + P1TD′2P1D′2
 =
10.90210.032
10.90210.032 + 10.02210.972
 =
0.0270
0.0270 + 0.0194 = 0.0270
0.0464
 = 0.582
Example 4.10 applies Bayes’ theorem to a medical diagnosis problem.
(continued)

200	
Chapter 4  Basic Probability
The probability that the disease is actually present, given that a positive result has occurred 
(indicating that the disease is present), is 0.582. Table 4.5 summarizes the computation of the 
probabilities, and Figure 4.6 presents the decision tree. The denominator in Bayes’ theorem 
represents P(T), the probability of a positive test result, which in this case is 0.0464, or 4.64%.
T a b l e  4 . 5
Bayes’ Theorem 
Computations for the 
Medical Diagnosis 
Problem
Event Di
Prior  
Probability 
 P1Di2
Conditional  
Probability  
P1T ∣Di2
Joint  
Probability  
P1T ∣Di2P1Di2
Revised  
Probability  
P1Di∣T2
D = has disease
0.03
0.90
0.0270
 P1D T2 = 0.0270>0.0464
 = 0.582
D′ = does not  
have disease
0.97
0.02
0.0194
0.0464
 P1D′ T 2 = 0.0194>0.0464
 = 0.418
F i g u r e  4 . 6
Decision tree for a 
medical diagnosis 
problem
P(D) = 0.03
P(D′) = 0.97
P(D and T′) = P(T′|D)P(D)
= (0.10)(0.03) = 0.0030
P(D′ and T) = P(T |D′)P(D′)
= (0.02)(0.97) = 0.0194
P(D′ and T′) = P(T′|D′)P(D′)
= (0.98)(0.97) = 0.9506
P(D and T) = P(T|D)P(D)
= (0.90)(0.03) = 0.0270
T h i n k  A b o u t  T h i s  Divine Providence and Spam
Would you ever guess that the essays Divine Be-
nevolence: Or, An Attempt to Prove That the Princi-
pal End of the Divine Providence and Government 
Is the Happiness of His Creatures and An Essay 
Towards Solving a Problem in the Doctrine of 
Chances were written by the same person? Prob-
ably not, and in doing so, you illustrate a modern-
day application of Bayesian statistics: spam, or 
junk mail filters.
In not guessing correctly, you probably looked 
at the words in the titles of the essays and con-
cluded that they were talking about two different 
things. An implicit rule you used was that word fre-
quencies vary by subject matter. A statistics essay 
would very likely contain the word statistics as well 
as words such as chance, problem, and solving. An 
eighteenth-century essay about theology and reli-
gion would be more likely to contain the uppercase 
forms of Divine and Providence.
Likewise, there are words you would guess 
to be very unlikely to appear in either book, such 
as technical terms from finance, and words that 
are most likely to appear in both—common words 
such as a, and, and the. That words would be 
­either likely or unlikely suggests an application of 
probability theory. Of course, likely and unlikely are 
fuzzy concepts, and we might occasionally mis-
classify an essay if we kept things too simple, such 
as relying solely on the occurrence of the words 
Divine and Providence.
For example, a profile of the late Harris 
­Milstead, better known as Divine, the star of Hair-
spray and other films, visiting Providence (Rhode 
Island), would most certainly not be an essay about 
theology. But if we widened the number of words 
we examined and found such words as movie or 
the name John Waters (Divine’s director in many 
films), we probably would quickly realize the essay 
had something to do with twentieth-century cin-
ema and little to do with theology and religion.
We can use a similar process to try to clas-
sify a new email message in your in-box as either 
spam or a legitimate message (called “ham,” in 
this context). We would first need to add to your 
email program a “spam filter” that has the ability to 
track word frequencies associated with spam and 
ham messages as you identify them on a day-to-
day basis. This would allow the filter to constantly 
update the prior probabilities necessary to use 
Bayes’ theorem. With these probabilities, the filter 
can ask, “What is the probability that an email is 
spam, given the presence of a certain word?”
Applying the terms of Equation (4.9) on 
page 198, such a Bayesian spam filter would mul-
tiply the probability of finding the word in a spam 
email, P1A  B2, by the probability that the email is 
spam, P(B), and then divide by the probability of 
finding the word in an email, the denominator in 
Equation (4.9). Bayesian spam filters also use short-
cuts by focusing on a small set of words that have a 
high probability of being found in a spam message 
as well as on a small set of other words that have a 
low probability of being found in a spam message.
As spammers (people who send junk email) 
learned of such new filters, they tried to outfox 
them. Having learned that Bayesian filters might 
be assigning a high P1A  B2 value to words com-
monly found in spam, such as Viagra, spammers 
thought they could fool the filter by ­misspelling 

	
4.3  Bayes’ Theorem	
201
the word as Vi@gr@ or V1agra. What they over-
looked was that the misspelled variants were 
even more likely to be found in a spam message 
than the original word. Thus, the misspelled vari-
ants made the job of spotting spam easier for the 
Bayesian filters.
Other spammers tried to fool the filters by 
adding “good” words, words that would have a low 
probability of being found in a spam message, or 
“rare” words, words not frequently encountered 
in any message. But these spammers overlooked 
the fact that the conditional probabilities are con-
stantly updated and that words once considered 
“good” would be soon discarded from the good 
list by the filter as their P1A  B2, value increased. 
Likewise, as “rare” words grew more common 
in spam and yet stayed rare in ham, such words 
acted like the misspelled variants that others had 
tried earlier.
Even then, and perhaps after reading about 
Bayesian statistics, spammers thought that they 
could “break” Bayesian filters by inserting random 
words in their messages. Those random words 
would affect the filter by causing it to see many 
words whose P1A  B2, value would be low. The 
Bayesian filter would begin to label many spam 
messages as ham and end up being of no practical 
use. Spammers again overlooked that conditional 
probabilities are constantly updated.
Other spammers decided to eliminate all or 
most of the words in their messages and replace 
them with graphics so that Bayesian filters would 
have very few words with which to form condi-
tional probabilities. But this approach failed, too, as 
Bayesian filters were rewritten to consider things 
other than words in a message. After all, Bayes’ 
theorem concerns events, and “graphics present 
with no text” is as valid an event as “some word, 
X, present in a message.” Other future tricks will 
­ultimately fail for the same reason. (By the way, 
spam filters use non-Bayesian techniques as well, 
which makes spammers’ lives even more difficult.)
Bayesian spam filters are an example of the 
unexpected way that applications of statistics can 
show up in your daily life. You will discover more 
examples as you read the rest of this book. By the 
way, the author of the two essays mentioned earlier 
was Thomas Bayes, who is a lot more famous for 
the second essay than the first essay, a failed at-
tempt to use mathematics and logic to prove the 
existence of God.
Problems for Section 4.3
Learning the Basics
4.30  If P1B2 = 0.05, P1A  B2 = 0.80, P1B′2 = 0.95, and
P1A  B′2 = 0.40, find P1B  A2.
4.31  If P1B2 = 0.20, P1A  B2 = 0.50, P1B′2 = 0.88, and 
P1A  B′2 = 0.38, find the probability P1B  A2.
Applying the Concepts
4.32  In Example 4.10 on page 199, suppose that the probability 
that a medical diagnostic test will give a positive result if the dis-
ease is not present is reduced from 0.02 to 0.01.
a.	 If the medical diagnostic test has given a positive result (indi-
cating that the disease is present), what is the probability that 
the disease is actually present?
b.	 If the medical diagnostic test has given a negative result (indi-
cating that the disease is not present), what is the probability 
that the disease is not present?
4.33  An advertising executive is studying television viewing hab-
its of married men and women during prime-time hours. Based 
on past viewing records, the executive has determined that dur-
ing prime time, husbands are watching television 60% of the time. 
When the husband is watching television, 40% of the time the wife 
is also watching. When the husband is not watching television, 
30% of the time the wife is watching television.
a.	 Find the probability that if the wife is watching television, the 
husband is also watching television.
b.	 Find the probability that the wife is watching television during 
prime time.
SELF 
Test 
4.34  Olive Construction Company is determining 
whether it should submit a bid for a new shopping ­center. 
In the past, Olive’s main competitor, Base ­Construction Company, 
has submitted bids 70% of the time. If Base Construction Company 
does not bid on a job, the probability that Olive Construction Com-
pany will get the job is 0.50. If Base Construction Company bids on 
a job, the probability that Olive Construction Company will get the 
job is 0.25.
a.	 If Olive Construction Company gets the job, what is the prob-
ability that Base Construction Company did not bid?
b.	 What is the probability that Olive Construction Company will 
get the job?
4.35  Laid-off workers who become entrepreneurs because they 
cannot find meaningful employment with another company are 
known as entrepreneurs by necessity. The Wall Street Journal re-
ported that these entrepreneurs by necessity are less likely to grow 
into large businesses than are entrepreneurs by choice. (Source: 
J. Bailey, “Desire—More Than Need—Builds a Business,” The 
Wall Street Journal, May 21, 2001, p. B4.) This article states that 
89% of the entrepreneurs in the United States are entrepreneurs by 
choice and 11% are entrepreneurs by necessity. Only 2% of entre-
preneurs by necessity expect their new business to employ 20 or 
more people within five years, whereas 14% of entrepreneurs by 
choice expect to employ at least 20 people within five years.
a.	 If an entrepreneur is selected at random and that individual ex-
pects that his or her new business will employ 20 or more peo-
ple within five years, what is the probability that this individual 
is an entrepreneur by choice?
b.	 Discuss several possible reasons why entrepreneurs by choice 
are more likely than entrepreneurs by necessity to believe that 
they will grow their businesses.
4.36  The editor of a textbook publishing company is trying to de-
cide whether to publish a proposed business statistics textbook. In-
formation on previous textbooks published indicates that 10% are 
huge successes, 20% are modest successes, 40% break even, and 
30% are losers. However, before a publishing decision is made, the 
book will be reviewed. In the past, 99% of the huge successes re-
ceived favorable reviews, 70% of the moderate successes received 
favorable reviews, 40% of the break-even books received favorable 
reviews, and 20% of the losers received favorable reviews.
a.	 If the proposed textbook receives a favorable review, how 
should the editor revise the probabilities of the various out-
comes to take this information into account?
b.	 What proportion of textbooks receive favorable reviews?

202	
Chapter 4  Basic Probability
4.37  A municipal bond service has three rating categories (A, B, 
and C). Suppose that in the past year, of the municipal bonds issued 
throughout the United States, 70% were rated A, 20% were rated 
B, and 10% were rated C. Of the municipal bonds rated A, 50% 
were issued by cities, 40% by suburbs, and 10% by rural areas. Of 
the municipal bonds rated B, 60% were issued by cities, 20% by 
suburbs, and 20% by rural areas. Of the municipal bonds rated C, 
90% were issued by cities, 5% by suburbs, and 5% by rural areas.
a.	 If a new municipal bond is to be issued by a city, what is the 
probability that it will receive an A rating?
b.	 What proportion of municipal bonds are issued by cities?
c.	 What proportion of municipal bonds are issued by suburbs?
4.4  Counting Rules
In Equation (4.1) on page 180, the probability of occurrence of an outcome was defined as the 
number of ways the outcome occurs, divided by the total number of possible outcomes. Often, 
there are a large number of possible outcomes, and determining the exact number can be dif-
ficult. In such circumstances, rules have been developed for counting the number of possible 
outcomes. This section presents five different counting rules.
Counting Rule 1  Counting rule 1 determines the number of possible outcomes for a set 
of mutually exclusive and collectively exhaustive events.
Counting Rule 1
If any one of k different mutually exclusive and collectively exhaustive events can occur on 
each of n trials, the number of possible outcomes is equal to
	
kn	
(4.10)
For example, using Equation (4.10), the number of different possible outcomes from toss-
ing a two-sided coin five times is 25 = 2 * 2 * 2 * 2 * 2 = 32.
Example 4.11
Rolling a Die Twice
Suppose you roll a die twice. How many different possible outcomes can occur?
Solution  If a six-sided die is rolled twice, using Equation (4.10), the number of different 
outcomes is 62 = 36.
Counting Rule 2  The second counting rule is a more general version of the first counting rule 
and allows the number of possible events to differ from trial to trial.
Counting Rule 2
If there are k1 events on the first trial, k2 events on the second trial, . . . , and kn events on 
the nth trial, then the number of possible outcomes is
	
1k121k22 c1kn2	
(4.11)
For example, a state motor vehicle department would like to know how many license 
plate numbers are available if a license plate number consists of three letters followed 
by three numbers (0 through 9). Using Equation (4.11), if a license plate number con-
sists of three letters followed by three numbers, the total number of possible outcomes is 
126212621262110211021102 = 17,576,000.

	
4.4  Counting Rules	
203
Example 4.12
Determining the 
Number of Different 
Dinners
A restaurant menu has a price-fixed complete dinner that consists of an appetizer, an entrée, a 
beverage, and a dessert. You have a choice of 5 appetizers, 10 entrées, 3 beverages, and 6 des-
serts. Determine the total number of possible dinners.
Solution  Using Equation (4.11), the total number of possible dinners is 1521102132
162 = 900.
Counting Rule 3  The third counting rule involves computing the number of ways that a set 
of items can be arranged in order.
Counting Rule 3
The number of ways that all n items can be arranged in order is
	
n! = 1n21n - 12 c 112	
(4.12)
where n! is called n factorial, and 0! is defined as 1.
Example 4.13
Using Counting 
Rule 3
If a set of six books is to be placed on a shelf, in how many ways can the six books be arranged?
Solution  To begin, you must realize that any of the six books could occupy the first posi-
tion on the shelf. Once the first position is filled, there are five books to choose from in filling 
the second position. You continue this assignment procedure until all the positions are occupied. 
The number of ways that you can arrange six books is
n! = 6! = 162152142132122112 = 720
Counting Rule 4  In many instances you need to know the number of ways in which a 
subset of an entire group of items can be arranged in order. Each possible arrangement is called 
a permutation.
Counting Rule 4: Permutations
The number of ways of arranging x objects selected from n objects in order is
	
nPx =
n!
1n - x2!	
(4.13)
	
where 
 n = total number of objects
 x = number of objects to be arranged
 n! = n factorial = n1n - 12 c112
 P = symbol for permutations1
1On many scientific calculators, 
there is a button labeled nPr that al-
lows you to compute permutations. 
The symbol r is used instead of x.
Student Tip
Both permutations and 
combinations assume 
that you are sampling 
without replacement.

204	
Chapter 4  Basic Probability
Counting Rule 5  In many situations, you are not interested in the order of the outcomes 
but only in the number of ways that x items can be selected from n items, irrespective of or-
der. Each possible selection is called a combination.
Example 4.14
Using Counting 
Rule 4
Modifying Example 4.13, if you have six books, but there is room for only four books on the 
shelf, in how many ways can you arrange these books on the shelf?
Solution  Using Equation (4.13), the number of ordered arrangements of four books se-
lected from six books is equal to
nPx =
n!
1n - x2! =
6!
16 - 42! = 162152142132122112
122112
= 360
2On many scientific calculators, 
there is a button labeled nCr that al-
lows you to compute combinations. 
The symbol r is used instead of x.
Counting Rule 5: Combinations
The number of ways of selecting x objects from n objects, irrespective of order, is equal to
	
nCx =
n!
x!1n - x2!	
(4.14)
	
where
n = total number of objects
x = number of objects to be arranged
n! = n factorial = n1n - 12 c 112
C = symbol for combinations2
If you compare this rule to counting rule 4, you see that it differs only in the inclusion of 
a term x! in the denominator. When permutations were used, all of the arrangements of the x 
objects are distinguishable. With combinations, the x! possible arrangements of objects are  
irrelevant.
Example 4.15
Using Counting 
Rule 5
Modifying Example 4.14, if the order of the books on the shelf is irrelevant, in how many ways 
can you arrange these books on the shelf?
Solution  Using Equation (4.14), the number of combinations of four books selected from 
six books is equal to
nCx =
n!
x!1n - x2! =
6!
4!16 - 42! = 162152142132122112
142132122112122112 = 15
Problems for Section 4.4
Applying the Concepts
SELF 
Test 
4.38  If there are 10 multiple-choice questions on an 
exam, each having three possible answers, how many 
different sequences of answers are there?
4.39  A lock on a bank vault consists of three dials, each with 30 
positions. In order for the vault to open, each of the three dials 
must be in the correct position.
a.	 How many different possible dial combinations are there for 
this lock?

	
4.5  Ethical Issues and Probability	
205
b.	 What is the probability that if you randomly select a position 
on each dial, you will be able to open the bank vault?
c.	 Explain why “dial combinations” are not mathematical combi-
nations expressed by Equation (4.14).
4.40  a.	 If a coin is tossed seven times, how many different out-
comes are possible?
b.	 If a die is tossed seven times, how many different outcomes are 
possible?
c.	 Discuss the differences in your answers to (a) and (b).
4.41  A particular brand of women’s jeans is available in seven 
different sizes, three different colors, and three different styles. 
How many different women’s jeans does the store manager need 
to order to have one pair of each type?
4.42  You would like to make a salad that consists of lettuce, 
tomato, cucumber, and peppers. You go to the supermarket, in-
tending to purchase one variety of each of these ingredients. You 
discover that there are eight varieties of lettuce, four varieties of 
tomatoes, three varieties of cucumbers, and three varieties of pep-
pers for sale at the supermarket. If you buy them all, how many 
different salads can you make?
4.43  A team is being formed that includes four different people. 
There are four different positions on the teams. How many differ-
ent ways are there to assign the four people to the four positions?
4.44  In Major League Baseball, there are five teams in the East-
ern Division of the National League: Atlanta, Florida, New York, 
Philadelphia, and Washington. How many different orders of fin-
ish are there for these five teams? (Assume that there are no ties 
in the standings.) Do you believe that all these orders are equally 
likely? Discuss.
4.45  Referring to Problem 4.44, how many different orders of 
finish are possible for the first four positions?
4.46  A gardener has six rows available in his vegetable garden to 
place tomatoes, eggplant, peppers, cucumbers, beans, and lettuce. 
Each vegetable will be allowed one and only one row. How many 
ways are there to position these vegetables in this garden?
4.47  There are eight members of a team. How many ways are 
there to select a team leader, assistant team leader, and team  
coordinator?
4.48  Four members of a group of 10 people are to be selected to 
a team. How many ways are there to select these four members?
4.49  A student has seven books that she would like to place in 
her backpack. However, there is room for only four books. Re-
gardless of the arrangement, how many ways are there of placing 
four books into the backpack?
4.50  A daily lottery is conducted in which 2 winning numbers 
are selected out of 100 numbers. How many different combina-
tions of winning numbers are possible?
4.51  A reading list for a course contains 20 articles. How many 
ways are there to choose 3 articles from this list?
Ethical issues can arise when any statements related to probability are presented to the public, 
particularly when these statements are part of an advertising campaign for a product or service. 
Unfortunately, many people are not comfortable with numerical concepts (see reference 5) and 
tend to misinterpret the meaning of the probability. In some instances, the misinterpretation is not 
intentional, but in other cases, advertisements may unethically try to mislead potential customers.
One example of a potentially unethical application of probability relates to advertisements 
for state lotteries. When purchasing a lottery ticket, the customer selects a set of numbers (such 
as 6) from a larger list of numbers (such as 54). Although virtually all participants know that 
they are unlikely to win the lottery, they also have very little idea of how unlikely it is for them 
to select all 6 winning numbers from the list of 54 numbers. They have even less of an idea of 
the probability of not selecting any winning numbers.
Given this background, you might consider a recent commercial for a state lottery that 
stated, “We won’t stop until we have made everyone a millionaire” to be deceptive and pos-
sibly unethical. Do you think the state has any intention of ever stopping the lottery, given the 
fact that the state relies on it to bring millions of dollars into its treasury? Is it possible that the 
lottery can make everyone a millionaire? Is it ethical to suggest that the purpose of the lottery 
is to make everyone a millionaire?
Another example of a potentially unethical application of probability relates to an invest-
ment newsletter promising a 90% probability of a 20% annual return on investment. To make 
the claim in the newsletter an ethical one, the investment service needs to (a) explain the basis 
on which this probability estimate rests, (b) provide the probability statement in another for-
mat, such as 9 chances in 10, and (c) explain what happens to the investment in the 10% of the 
cases in which a 20% return is not achieved (e.g., is the entire investment lost?).
These are serious ethical issues. If you were going to write an advertisement for the state 
lottery that ethically describes the probability of winning a certain prize, what would you say? 
If you were going to write an advertisement for the investment newsletter that ethically states 
the probability of a 20% return on an investment, what would you say?
4.5  Ethical Issues and Probability

206	
Chapter 4  Basic Probability
S u m m a r y
This chapter began by developing the basic concepts of 
probability. You learned that probability is a numeric value 
from 0 to 1 that represents the chance, likelihood, or pos-
sibility that a particular event will occur. In addition to sim-
ple probability, you learned about conditional probabilities 
and independent events. Bayes’ theorem was used to revise  
previously calculated probabilities based on new informa-
tion. Throughout the chapter, contingency tables and deci-
sion trees were used to display information. You also learned 
about several counting rules. In the next chapter, important 
discrete probability distributions such as the binomial, Pois-
son, and hypergeometric distributions are developed.
Referen c e s
	 1.	Bellhouse, D. R. “The Reverend Thomas Bayes, FRS: A Biog-
raphy to Celebrate the Tercentenary of His Birth.” Statistical 
Science, 19 (2004), 3–43.
	 2.	Lowd, D., and C. Meek. “Good Word Attacks on Statistical 
Spam Filters.” Presented at the Second Conference on Email 
and Anti-Spam, 2005.
	 3.	Microsoft Excel 2013. Redmond, WA: Microsoft Corp., 2012.
	 4.	Minitab Release 16. State College, PA: Minitab, Inc, 2010.
	 5.	Paulos, J. A. Innumeracy. New York: Hill and Wang, 1988.
	 6.	Silberman, S. “The Quest for Meaning,” Wired 8.02, February 
2000.
	 7.	Zeller, T. “The Fight Against V1@gra (and Other Spam).” The 
New York Times, May 21, 2006, pp. B1, B6.
A
s the marketing manager for M&R Electronics World, 
you analyzed the survey results of an intent-to-purchase 
study. This study asked the heads of 1,000 households about 
their intentions to purchase a large-screen HDTV sometime 
during the next 12 months, and as a follow-up, M&R sur-
veyed the same people 12 months later to see whether such 
a television was purchased. In addition, for households pur-
chasing large-screen HDTVs, the survey asked whether the 
television they purchased had a faster refresh rate, whether  
they also purchased a streaming media box in the past  
12 months, and whether they were satisfied with their  
purchase of the large-screen HDTV.
By analyzing the results of these surveys, you were able 
to uncover many pieces of valuable information that will help 
you plan a marketing strategy to enhance sales and better 
target those households likely to purchase multiple or more 
expensive products. Whereas only 30% of the households 
actually purchased a large-screen HDTV, if a household in-
dicated that it planned to purchase a large-screen HDTV in 
the next 12 months, there was an 80% chance that the house-
hold actually made the purchase. Thus the marketing strategy 
should target those households that have indicated an inten-
tion to purchase.
You determined that for households that purchased a 
television that had a faster refresh rate, there was a 47.5% 
chance that the household also purchased a streaming media 
box. You then compared this conditional probability to the 
marginal probability of purchasing a streaming media box, 
which was 36%. Thus, households that purchased televisions 
that had a faster refresh rate are more likely to purchase a 
streaming media box than are households that purchased 
large-screen HDTVs that have a standard refresh rate.
You were also able to apply Bayes’ theorem to M&R 
Electronics World’s market research reports. The reports inves-
tigate a potential new television model prior to its scheduled 
release. If a favorable report was received, then there was a 
64% chance that the new television model would be successful. 
However, if an unfavorable report was received, there is only 
a 16% chance that the model would be successful. Therefore, 
the marketing strategy of M&R needs to pay close attention to 
whether a report’s conclusion is favorable or unfavorable.
U s i n g  S tat i s t i c s
Possibilities at M&R Electronics 
World, Revisited
Shock/Fotolia

	
Key Terms	
207
K ey Eq u at i o n s
Probability of Occurrence
probability of occurrence = X
T
(4.1)
Marginal Probability
P1A2 = P1A and B12 + P1A and B22
	
+ g + P1A and Bk2
(4.2)
General Addition Rule
P1A or B2 = P1A2 + P1B2 - P1A and B2
(4.3)
Conditional Probability
P1AB2 = P1A and B2
P1B2

(4.4a)
P1BA2 = P1A and B2
P1A2

(4.4b)
Independence
P1AB2 = P1A2
(4.5)
General Multiplication Rule
P1A and B2 = P1AB2P1B2
(4.6)
Multiplication Rule for Independent Events
P1A and B2 = P1A2P1B2
(4.7)
Marginal Probability Using the General  
Multiplication Rule
P1A2 = P1AB12P1B12 + P1AB22P1B22
	
+ g + P1ABk2P1Bk2
(4.8)
Bayes’ Theorem
P1Bi A2 =
P1ABi2P1Bi2
P1AB12P1B12 + P1AB22P1B22 + g +  P1ABk2P1Bk2
	

(4.9)
Counting Rule 1
kn
(4.10)
Counting Rule 2
1k121k22 c 1kn2
(4.11)
Counting Rule 3
n! = 1n21n - 12 c112
(4.12)
Counting Rule 4: Permutations
nPx =
n!
1n - x2!
(4.13)
Counting Rule 5: Combinations
nCx =
n!
x!1n - x2!
(4.14)
K ey Te r ms
a priori probability  181
Bayes’ theorem  197
certain event  180
collectively exhaustive  185
combination  204
complement  182
conditional probability  189
contingency table  183
decision tree  191
empirical probability  181
event  181
general addition rule  186
general multiplication rule  194
impossible event  180
independence  193
joint event  182
joint probability  184
marginal probability  185
multiplication rule for  
independent events  194
mutually exclusive  185
permutation  203
probability  180
sample space  182
simple event  181
simple probability  183
subjective probability  181
Venn diagram  183

208	
Chapter 4  Basic Probability
C hec ki n g  Yo u r  U n d e r s ta nding
4.52  If there are 20 multiple choice questions in an exam, and each 
question has two possible answers, how many different answers are 
possible?
4.53  What is the difference between a simple event and a joint 
event?
4.54  How many different outcomes are possible if a fair die 
is rolled 6 times and how many if a fair coin is tossed 8 times?  
Explain the difference between the results of the two. 
4.55  How many variations can a new type of car have if it is 
manufactured in 5 different exterior colors, 4 different interior col-
ors, and with engines of three different strengths?
4.56  How does conditional probability relate to the concept of 
independence?
4.57  Five different books are to be arranged on a bookshelf. 
There are five different spaces where the books can be placed 
within the same level. In how many different ways can the books 
be arranged?
4.58  How can you use Bayes’ theorem to revise probabilities in 
light of new information?
4.59  In Bayes’ theorem, how does the prior probability differ 
from the revised probability?
C ha pter  R e vi e w P r o b le ms
4.60  A committee with a president, vice president, treasurer, of-
fice manager, and secretary has to be formed by choosing from 
ten people. In how many ways can the random selection be made?
4.61  SHL Americas provides a unique, global perspective of how 
talent is measured in its Global Assessment Trends Report. The re-
port presents the results of an online survey conducted in late 2012 
with HR professionals from companies headquartered through-
out the world. The authors were interested in examining differ-
ences between respondents in emerging economies and those in 
established economies to provide relevant information for readers 
who may be creating assessment programs for organizations with 
global reach; one area of focus was on HR professionals’ response 
to two statements: “My organization views HR as a strategic func-
tion” and “My organization uses talent information to make busi-
ness decisions.” The results are as follows:
Organization Views HR as  
a Strategic Function
Economy
Yes
No
Total
Established
171
  78
249
Emerging
222
121
343
Total
393
199
592
Organization Uses Information  
Talent to Make Business Decisions
Economy
Yes
No
Total
Established
122
127
249
Emerging
130
213
343
Total
252
340
592
What is the probability that a randomly chosen HR professional
a.	 is from an established economy?
b.	 is from an established economy or agrees to the statement “My 
organization uses information talent to make business decisions?”
c.	 does not agree with the statement “My organization views HR 
as a strategic function” and is from an emerging economy?
d.	 does not agree with the statement “My organization views HR 
as a strategic function” or is from an emerging economy?
e.	 Suppose the randomly chosen HR professional does not agree 
with the statement “My organization views HR as a strategic 
function.” What is the probability that the HR professional is 
from an emerging economy?
f.	 Are “My organization views HR as a strategic function” and 
the type of economy independent?
g.	 Is “My organization uses information talent to make business 
decisions” independent of the type of economy?
4.62  The 2012 Restaurant Industry Forecast takes a closer look at 
today’s consumers. Based on a 2011 National Restaurant Associa-
tion survey, consumers are divided into three segments (optimistic, 
cautious, and hunkered-down) based on their financial situation, cur-
rent spending behavior, and economic outlook. Suppose the results, 
based on a sample of 100 males and 100 females, were as follows:
Consumer Segment
Gender
Male
Female
Total
Optimistic
  26
  16
  42
Cautious
  41
  43
  84
Hunkered-down
  33
  41
  74
Total
100
100
200
Source: Data extracted from “The 2012 Restaurant Industry Forecast,” 
National Restaurant Association, 2012, p. 12,
restaurant.org/research/forecast.

	
Chapter Review Problems	
209
If a consumer is selected at random, what is the probability that 
he or she
a.	 is classified as cautious?
b.	 is classified as optimistic or cautious?
c.	 is a male or is classified as hunkered-down?
d.	 is a male and is classified as hunkered-down?
e.	 Given that the consumer selected is a female, what is the prob-
ability that she is classified as optimistic?
4.63  A 2011 joint study by MIT Sloan Management Review and 
IBM Institute for Business Value reports a growing divide between 
those companies that are transforming themselves to take advan-
tage of business analytics and those that have yet to embrace it. 
A survey of business executives, managers, and analysts from 
organizations around the world indicated that 31% of organiza-
tions are “aspirational users” (basic users of analytics), 45% are 
“experienced users” (moderate users of analytics), and 24% are 
“transformed users” (strong and sophisticated users of analytics). 
Furthermore, 62% of the transformed-user organizations indicated 
an intense level of focus on using analytics to better understand and 
connect with customers, as did 49% of the experienced-user or-
ganizations and 34% of the aspirational-user organizations. (Data 
extracted from “Analytics: The Widening Divide, How Companies 
Are Achieving Competitive Advantage Through Analytics,” IBM 
Global Business Services, October, 2011.) If an organization is 
known to have an intense level of focus on using analytics to bet-
ter understand and connect with customers, what is the probability 
that the organization is a transformed-user organization?
4.64  The CMO Council and SAS set out to better understand the 
key challenges, opportunities, and requirements that both chief 
marketing officers (CMOs) and chief information officers (CIOs) 
were facing in their journey to develop a more customer-centric 
enterprise. The following findings are from an online audit of 237 
senior marketers and 210 senior IT executives. (Data extracted 
from “Big Data’s Biggest Role: Aligning the CMO & CIO,” 
March 2013, bit.ly/11z7uKW.)
Big Data Is Critical to Executing  
a Customer-Centric Program
Executive Group
Yes
No
Total
Marketing
  95
142
237
IT
107
103
210
Total
202
245
447
Functional Silos Block Aggregation  
of Customer Data Throughout  
the Organization
Executive Group
Yes
No
Total
Marketing
122
115
237
IT
  95
115
210
Total
217
230
447
a.	 What is the probability that a randomly selected executive 
identifies Big Data as critical to executing a customer-centric 
program?
b.	 Given that a randomly selected executive is a senior marketing 
executive, what is the probability that the executive identifies 
Big Data as critical to executing a customer-centric program?
c.	 Given that a randomly selected executive is a senior IT execu-
tive, what is the probability that the executive identifies Big 
Data as critical to executing a customer-centric program?
d.	 What is the probability that a randomly selected executive 
identifies that functional silos block aggregation of customer 
data throughout the organization?
e.	 Given that a randomly selected executive is a senior market-
ing executive, what is the probability that the executive iden-
tifies that functional silos block aggregation of customer data 
throughout the organization?
f.	 Given that a randomly selected executive is a senior IT execu-
tive, what is the probability that the executive identifies that 
functional silos block aggregation of customer data throughout 
the organization?
g.	 Comment on the results in (a) through (f).
4.65  A 2013 Sage North America survey examined the “finan-
cial literacy” of small business owners. The study found that 23% 
of small business owners indicated concern about income tax 
compliance for their business; 41% of small business owners use 
accounting software, given that the small business owner indi-
cated concern about income tax compliance for his or her busi-
ness. Given that a small business owner did not indicate concern 
about income tax compliance for his or her business, 58% of small 
business owners use accounting software. (Data extracted from 
“Sage Financial Capability Survey: What Small Business Owners 
Don’t Understand Could Be Holding Them Back,” April 17, 2013, 
http://bit.ly/Z3FAqx.)
a.	 Use Bayes’ theorem to find the probability that a small busi-
ness owner uses accounting software, given that the small busi-
ness owner indicated concern about income tax compliance for 
his or her business.
b.	 Compare the result in (a) to the probability that a small busi-
ness owner uses accounting software and comment on whether 
small business owners who are concerned about income tax 
compliance for their business are generally more likely to use 
accounting software than small business owners who are not 
concerned about income tax compliance for their business.

210	
Chapter 4  Basic Probability
C a s e s  f o r  C h a p t e r  4
Digital Case
Apply your knowledge about contingency tables and the 
proper application of simple and joint probabilities in this 
continuing Digital Case from Chapter 3.
Open EndRunGuide.pdf, the EndRun Financial Services 
“Guide to Investing,” and read the information about the 
Guaranteed Investment Package (GIP). Read the claims and 
examine the supporting data. Then answer the following 
questions:
1.  How accurate is the claim of the probability of suc-
cess for EndRun’s GIP? In what ways is the claim  
misleading? How would you calculate and state the prob-
ability of having an annual rate of return not less than 
15%?
2.  Using the table found under the “Show Me the Winning 
Probabilities” subhead, compute the proper probabilities 
for the group of investors. What mistake was made in re-
porting the 7% probability claim?
3.  Are there any probability calculations that would be  
appropriate for rating an investment service? Why or 
why not?
CardioGood Fitness
1.		For each CardioGood Fitness treadmill product line (see 
the  CardioGoodFitness  file), construct two-way contin-
gency tables of gender, education in years, relationship 
status, and self-rated fitness. (There will be a total of six 
tables for each treadmill product.)
2.		For each table you construct, compute all conditional and 
marginal probabilities.
3.		Write a report detailing your findings to be presented to 
the management of CardioGood Fitness.
The Choice Is Yours Follow-Up
1.		Follow up the “Using Statistics: The Choice Is Yours, 
­Revisited” on page 103 by constructing contingency ta-
bles of market cap and type, market cap and risk, market 
cap and rating, type and risk, type and rating, and risk and 
rating for the sample of 316 retirement funds stored in 
  Retirement Funds .
2.		For each table you construct, compute all conditional and 
marginal probabilities.
3.		Write a report summarizing your conclusions.
Clear Mountain State Student Surveys
The Student News Service at Clear Mountain State Univer-
sity (CMSU) has decided to gather data about the under-
graduate students that attend CMSU. CMSU creates and 
distributes a survey of 14 questions and receive responses 
from 62 undergraduates (stored in  UndergradSurvey ).
1.		For these data, construct contingency tables of gender 
and major, gender and graduate school intention, gender 
and employment status, gender and computer preference, 
class and graduate school intention, class and employ-
ment status, major and graduate school intention, major 
and employment status, and major and computer prefer-
ence.
a.	For each of these contingency tables, compute all the 
conditional and marginal probabilities.
b.	Write a report summarizing your conclusions.
2.		The CMSU Dean of Students has learned about the un-
dergraduate survey and has decided to undertake a simi-
lar survey for graduate students at Clear Mountain State. 
She creates and distributes a survey of 14 questions and 
receives responses from 44 graduate students (stored in 
  GradSurvey ). Construct contingency tables of gender and 
graduate major, gender and undergraduate major, gender 
and employment status, gender and computer preference, 
graduate major and undergraduate major, graduate ma-
jor and employment status, and graduate major and com-
puter preference.
a.	For each of these contingency tables, compute all the 
conditional and marginal probabilities.
b.	Write a report summarizing your conclusions.

	
Chapter 4 Excel Guide	
211
EG4.1  Basic Probability Concepts
Simple and Joint Probability and the General 
Addition Rule
Key Technique  Use Excel arithmetic formulas.
Example  Compute simple and joint probabilities for the Table 
4.1 purchase behavior data on page 182.
PHStat2  Use Simple & Joint Probabilities.
For the example, select PHStat ➔ Probability & Prob.  
Distributions ➔ Simple & Joint Probabilities. In the new tem-
plate, similar to the worksheet shown below, fill in the Sample 
Space area with the data.
In-Depth Excel  Use the COMPUTE worksheet of the Prob-
abilities workbook as a template.
The worksheet (shown below) already contains the Table 4.1 pur-
chase behavior data. For other problems, change the sample space 
table entries in the cell ranges C3:D4 and A5:D6.
Read the Short Takes for Chapter 4 for an explanation of 
the formulas found in the COMPUTE worksheet (shown in the 
COMPUTE_FORMULAS worksheet).
EG4.2  Conditional Probability
There is no Excel material for this section.
EG4.3  Bayes’ Theorem
Key Technique  Use Excel arithmetic formulas.
Example  Apply Bayes’ theorem to the television marketing ex-
ample in Section 4.3.
In-Depth Excel  Use the COMPUTE worksheet of the Bayes 
workbook as a template.
The worksheet (shown below) already contains the probabilities 
for the Section 4.3 example. For other problems, change those 
probabilities in the cell range B5:C6.
Open to the COMPUTE_FORMULAS worksheet to exam-
ine the arithmetic formulas that compute the probabilities, which 
are also shown as an inset to the worksheet.
EG4.4  Counting Rules
Counting Rule 1
In-Depth Excel  Use the POWER(k, n) worksheet function in 
a cell formula to compute the number of outcomes given k events 
and n trials. For example, the formula =POWER(6, 2) computes 
the answer for Example 4.11 on page 202.
Counting Rule 2
In-Depth Excel  Use a formula that takes the product of suc-
cessive POWER(k, n) functions to solve problems related to 
counting rule 2. For example, the formula =POWER(26, 3) * 
POWER(10, 3) computes the answer for the state motor vehicle 
department example on page 202.
Counting Rule 3
In-Depth Excel  Use the FACT(n) worksheet function in a cell 
formula to compute how many ways n items can be arranged. For 
example, the formula =FACT(6) computes 6!
Counting Rule 4
In-Depth Excel  Use the PERMUT(n, x) worksheet function 
in a cell formula to compute the number of ways of arranging x 
objects selected from n objects in order. For example, the ­formula 
= PERMUT(6, 4) computes the answer for Example 4.14 on 
page 204.
Counting Rule 5
In-Depth Excel  Use the COMBIN(n, x) worksheet function 
in a cell formula to compute the number of ways of arranging x  
objects selected from n objects, irrespective of order. For example, 
the formula =COMBIN(6, 4) computes the answer for Example 
4.15 on page 204.
C h a p t e r  4  E x c e l  G u i d e

212	
Chapter 4  Basic Probability
MG4.1  Basic Probability Concepts
There is no Minitab material for this section.
MG4.2  Conditional Probability
There is no Minitab material for this section.
MG4.3  Bayes’ Theorem
There is no Minitab material for this section.
MG4.4  Counting Rules
Use Calculator to apply the counting rules. Select Calc ➔ Calcu-
lator. In the Calculator dialog box (shown below):
	 1.	 Enter the column name of an empty column in the Store 
­result in variable box and then press Tab.
	 2.	 Build the appropriate expression (as discussed later in this sec-
tion) in the Expression box. To apply counting rules 3 through 
5, select Arithmetic from the Functions drop-down list to fa-
cilitate the function selection.
	 3.	 Click OK.
If you have previously used the Calculator during your Minitab 
session, you may have to clear the contents of the Expression box 
by selecting the contents and pressing Del before you begin step 2.
Counting Rule 1
Enter an expression that uses the exponential operator **. For ex-
ample, the expression 6 ** 2 computes the answer for Example 
4.11 on page 202.
Counting Rule 2
Enter an expression that uses the exponential operator **. For ex-
ample, the expression 26 ** 3 * 10 ** 3 computes the answer for 
the state motor vehicle department example on page 202.
Counting Rule 3
Enter an expression that uses the FACTORIAL(n) function to 
compute how many ways n items can be arranged. For example, 
the expression FACTORIAL(6) computes 6!
Counting Rule 4
Enter an expression that uses the PERMUTATIONS(n, x) func-
tion to compute the number of ways of arranging x objects selected 
from n objects in order. For example, the expression PERMUTA-
TIONS(6, 4) computes the answer for Example 4.14 on page 204.
Counting Rule 5
Enter an expression that uses the COMBINATIONS(n, x) func-
tion to compute the number of ways of arranging x objects selected 
from n objects, irrespective of order. For example, the expression 
COMBINATIONS(6, 4) computes the answer for Example 4.15 
on page 204.
C h a p t e r  4  M i n i ta b  G u i d e

213
U s i n g  S tat i s t i c s
Events of Interest at Ricknel Home Centers
Like most other large businesses, Ricknel Home Centers, LLC, a regional home 
improvement chain, uses an accounting information system (AIS) to manage its  
accounting and financial data. The Ricknel AIS collects, organizes, stores, ana-
lyzes, and distributes financial information to decision makers both inside and  
outside the firm.
One important function of the Ricknel AIS is to continuously audit account-
ing information, looking for errors or incomplete or improbable information. 
For example, when customers submit orders online, the Ricknel AIS reviews the 
orders for possible mistakes. Any questionable invoices are tagged and included 
in a daily exceptions report. Recent data collected by the company show that the 
likelihood is 0.10 that an order form will be tagged.
As a member of the AIS team, you have been asked by Ricknel manage-
ment to determine the likelihood of finding a certain number of tagged forms in 
a sample of a specific size. For example, what would be the likelihood that none 
of the order forms are tagged in a sample of four forms? That one of the order 
forms is tagged?
How could you determine the solution to this type of probability problem?
contents
5.1	 The Probability Distribution 
for a Discrete Variable
5.2	 Covariance of a Probability 
Distribution and Its 
Application in Finance
5.3	 Binomial Distribution
5.4	 Poisson Distribution
5.5	 Hypergeometric Distribution
5.6	 Using the Poisson Distribution 
to Approximate the Binomial 
Distribution (online)
Using Statistics: Events of 
Interest at Ricknel Home Centers, 
Revisited
Chapter 5 Excel Guide
Chapter 5 Minitab Guide
Objectives
To learn the properties of a 
probability distribution
To compute the expected value 
and variance of a probability 
distribution
To calculate the covariance and 
understand its use in finance
To compute probabilities from 
the binomial, Poisson, and 
hypergeometric distributions
To use the binomial, Poisson, and 
hypergeometric distributions to 
solve business problems
Chapter
Discrete Probability 
Distributions
5
Sebastian Kaulitzki/Shutterstock

214	
Chapter 5  Discrete Probability Distributions
T
his chapter introduces you to the concept and characteristics of probability distribu-
tions. You will learn how the binomial, Poisson, and hypergeometric distributions can 
be applied to help solve business problems. In the Rickel Home Centers scenario, you 
could use a probability distribution as a mathematical model, or small-scale representation, 
that approximates the process. By using such an approximation, you could make inferences 
about the actual order process including the likelihood of finding a certain number of tagged 
forms in a sample.
Recall from Section 1.1 that numerical variables are variables that have values that represent 
quantities, such as the one-year return percentage for a retirement fund or the number of social 
media sites to which you belong. Some numerical variables are discrete, having numerical val-
ues that arise from a counting process, while others are continuous, having numerical values 
that arise from a measuring process (e.g., the one-year return of growth and value funds that 
were the subject of the Using Statistics scenario in Chapters 2 and 3). This chapter deals with 
probability distributions that represent a discrete numerical variable, such as the number of 
social media sites to which you belong.
5.1  The Probability Distribution for a Discrete Variable
Probability Distribution for a Discrete Variable
A probability distribution for a discrete variable is a mutually exclusive list of all the 
possible numerical outcomes along with the probability of occurrence of each outcome.
For example, Table 5.1 gives the distribution of the number of interruptions per day in a 
large computer network. The list in Table 5.1 is collectively exhaustive because all possible 
outcomes are included. Thus, the probabilities sum to 1. Figure 5.1 is a graphical representa-
tion of Table 5.1.
T a b l e  5 . 1
Probability 
Distribution of 
the Number of 
Interruptions per Day
Interruptions per Day
Probability
0
0.35
1
0.25
2
0.20
3
0.10
4
0.05
5
0.05
F i g u r e  5 . 1
Probability distribution 
of the number of 
interruptions per day
0
2
3
4
5
X
P (X)
.3
.2
Interruptions per Days
.1
.4
1
Expected Value of a Discrete Variable
The expected value of a random variable is the mean, m, of its probability distribution. To  
calculate the expected value, you multiply each possible outcome, xi, by its corresponding 
probability, P1X = xi2, and then sum these products.
Student Tip
Remember, expected 
value is just the mean.

	
5.1  The Probability Distribution for a Discrete Variable	
215
For the probability distribution of the number of interruptions per day in a large computer 
network (Table 5.1), the expected value is computed as follows, using Equation (5.1), and is 
also shown in Table 5.2:
 m = E1X2 = a
N
i = 1
xi P1X = xi2
 = 10210.352 + 11210.252 + 12210.202 + 13210.102 + 14210.052 + 15210.052
 = 0 + 0.25 + 0.40 + 0.30 + 0.20 + 0.25
 = 1.40
Expected Value, m, of a Discrete Variable
	
m = E1X2 = a
N
i = 1
xi P1X = xi2	
(5.1)
	
where
xi = the ith value of the discrete variable X
P1X = xi2 = probability of occurrence of the ith value of X
T a b l e  5 . 2
Computing the 
Expected Value 
of the Number of 
Interruptions per Day
Interruptions per Day 1xi2
P1X = xi2
xi P1X = xi2
0
0.35
10210.352 = 0.00
1
0.25
11210.252 = 0.25
2
0.20
12210.202 = 0.40
3
0.10
13210.102 = 0.30
4
0.05
14210.052 = 0.20
5
0.05
15210.052 = 0.25
1.00
m = E1X2 = 1.40
The expected value is 1.40. The expected value of 1.40 interruptions per day is not a pos-
sible result because the actual number of interruptions on a given day must be an integer value. 
The expected value represents the mean number of interruptions on a given day.
Variance and Standard Deviation of a Discrete Variable
You compute the variance of a probability distribution by multiplying each possible squared 
difference 3xi - E1X242 by its corresponding probability, P1X = xi2, and then summing the 
resulting products. Equation (5.2) defines the variance of a discrete variable, and Equation (5.3) 
defines the standard deviation of a discrete variable.
Variance of a Discrete Variable
	
s2 = a
N
i = 1
3xi - E1X242P1X = xi2	
(5.2)
	
where
xi = the ith value of the discrete variable X
P1X = xi2 = probability of occurrence of the ith value of X

216	
Chapter 5  Discrete Probability Distributions
The variance and the standard deviation of the number of interruptions per day are com-
puted as follows and in Table 5.3, using Equations (5.2) and (5.3):
 s2 = a
N
i = 1
3xi - E1X242P1X = xi2
 = 10 - 1.42210.352 + 11 - 1.42210.252 + 12 - 1.42210.202 + 13 - 1.42210.102
+ 14 - 1.42210.052 + 15 - 1.42210.052
 = 0.686 + 0.040 + 0.072 + 0.256 + 0.338 + 0.648
 = 2.04
and
s = 2s2 = 22.04 = 1.4283
Standard Deviation of a Discrete Variable
	
s = 2s2 = B a
N
i = 1
3xi - E1X242P1X = xi2	
(5.3)
T a b l e  5 . 3
Computing the 
Variance and 
Standard Deviation 
of the Number of 
Interruptions per Day
Use the Section EG5.1 
instructions to compute 
the variance and standard 
deviation of a discrete 
variable.
Interruptions 
per Day 1xi2
P1X = xi2
xiP1X = xi2
3xi@E1X242
3xi - E1X242P1X = xi2
0
0.35
0.00
10 - 1.422 = 1.96
11.96210.352 = 0.686
1
0.25
0.25
11 - 1.422 = 0.16
10.16210.252 = 0.040
2
0.20
0.40
12 - 1.422 = 0.36
10.36210.202 = 0.072
3
0.10
0.30
13 - 1.422 = 2.56
12.56210.102 = 0.256
4
0.05
0.20
14 - 1.422 = 6.76
16.76210.052 = 0.338
5
0.05
0.25
15 - 1.422 = 12.96
112.96210.052 = 0.648
1.00
m = E1X2 = 1.40
s2 = 2.04
s = 2s2 = 1.4283
Thus, the mean number of interruptions per day is 1.4, the variance is 2.04, and the stan-
dard deviation is approximately 1.43 interruptions per day.
Problems for Section 5.1
Learning the Basics
5.1  Given the following probability distributions:
a.	 Compute the expected value for each distribution.
b.	 Compute the standard deviation for each distribution.
c.	 Compare the results of distributions A and B.
Distribution A
Distribution B
xi
P1X = xi2
xi
P1X = xi2
0
0.50
0
0.05
1
0.20
1
0.10
2
0.15
2
0.15
3
0.10
3
0.20
4
0.05
4
0.50

	
5.2  Covariance of a Probability Distribution and Its Application in Finance 	
217
Applying the Concepts
SELF 
Test 
5.2  The following table contains the probability distri-
bution for the number of traffic accidents daily in a 
small town:
c.	 Construct the probability distribution representing the different 
outcomes that are possible for a $1 bet on 7.
d.	 Show that the expected long-run profit (or loss) to the player is 
the same, no matter which method of play is used.
5.5  The number of arrivals per minute at an ATM inside a mall 
was recorded over a period of 200 minutes, with the following  
results:
Number of  
Accidents Daily (X)
P1X = xi2
0
0.10
1
0.20
2
0.45
3
0.15
4
0.05
5
0.05
a.	 Compute the mean number of accidents per day.
b.	 Compute the standard deviation.
5.3  A local airline sent out an advertisement via email to its po-
tential customers. It stated that the airline would be holding a lucky 
draw in which the prizes would be (i) fifteen airline tickets or $1,500 
in cash, (ii) five airline tickets or $500 in cash, and (iii) three dollar 
gas coupons redeemable at local gas stations. The fine print on the 
advertisement voucher indicated the probabilities of winning. The 
chance of winning (i) was 1 out of 42,376, (ii) was 1 out of 42,376, 
and (iii) was 42,374 out of 42,376.
a.	 How many emails do you think the airline sent out?
b.	 Using the answer for part (a) and the probabilities listed in the 
fine print of the email, what is the expected value of the prize 
won by a prospective customer?
c.	 Using the answer of for part (a) and the probabilities listed in 
the email, what is the standard deviation of the value of the 
prize won by a prospective customer?
d.	 Do you think this is an effective promotion? Explain.
5.4  In the carnival game Under-or-Over-Seven, a pair of fair 
dice is rolled once, and the resulting sum determines whether the 
player wins or loses his or her bet. For example, the player can bet 
$1 that the sum will be under 7—that is, 2, 3, 4, 5, or 6. For this 
bet, the player wins $1 if the result is under 7 and loses $1 if the 
outcome equals or is greater than 7. Similarly, the player can bet 
$1 that the sum will be over 7—that is, 8, 9, 10, 11, or 12. Here, 
the player wins $1 if the result is over 7 but loses $1 if the result 
is 7 or under. A third method of play is to bet $1 on the outcome 
7. For this bet, the player wins $4 if the result of the roll is 7 and 
loses $1 otherwise.
a.	 Construct the probability distribution representing the different 
outcomes that are possible for a $1 bet on under 7.
b.	 Construct the probability distribution representing the different 
outcomes that are possible for a $1 bet on over 7.
Arrivals
Frequency
0
  3
1
30
2
41
3
48
4
39
5
22
6
10
7
  4
8
  3
a.	 Compute the expected number of arrivals per minute.
b.	 Compute the standard deviation.
5.6  In a semester in college data was collected of the number of 
student absences and was correlated with whether students passed 
the class or not. The data was collated as follows:
Number of Absences
Frequency/
pass
0
150
1
120
2
  67
3
  50
4
  40
5
  10
6
    5
7
    2
8
    1
a.	 Calculate the expected number of absences per semester.
b.	 Calculate the standard deviation.
Section 5.1 defined the expected value, variance, and standard deviation for a single discrete 
variable. In this section, the covariance between two variables is introduced and applied to 
portfolio management, a topic of great interest to financial analysts.
5.2  Covariance of a Probability Distribution  
and Its Application in Finance

218	
Chapter 5  Discrete Probability Distributions
Covariance
The covariance of a probability distribution 1SXY2 measures the strength of the relationship 
between two variables, X and Y. A positive covariance indicates a positive relationship. A neg-
ative covariance indicates a negative relationship. If two variables are independent, their cova-
riance will be zero. Equation (5.4) defines the covariance of discrete random variables X and Y.
Covariance
	
sXY = a
N
i = 1
3xi - E1X243yi - E1Y24P1xi, yi2	
(5.4)
	
where
X = discrete variable X
xi = ith value of X
Y = discrete variable Y
yi = ith value of Y
P1xi, yi2 = probability of occurrence of the ith value of X and the ith value of Y
i = 1, 2, c, N for X and Y
To illustrate the covariance, suppose that you are deciding between two different invest-
ments for the coming year. The first investment is a mutual fund that consists of the stocks 
that comprise the Dow Jones Industrial Average. The second investment is a mutual fund that 
is expected to perform best when economic conditions are weak. Table 5.4 summarizes your 
estimate of the returns (per $1,000 investment) under three economic conditions, each with a 
given probability of occurrence.
T a b l e  5 . 4
Estimated Returns 
for Each Investment 
Under Three 
Economic Conditions
P1xi, yi2
Economic Condition
Investment Return
Dow Jones Fund
Weak-Economy Fund
0.2
Recession
- +300
++200
0.5
Stable economy
  +100
    +50
0.3
Expanding economy
+250
-100
The expected value and standard deviation for each investment and the covariance of the 
two investments are computed as follows:
 Let X = the return of the Dow Jones fund and Y = the return of the weak@economy fund
 E1X2 = mX = 1-300210.22 + 1100210.52 + 1250210.32 = +65
 E1Y2 = mY = 1+200210.22 + 150210.52 + 1-100210.32 = +35
 Var1X2 = s2
X = 1-300 - 652210.22 + 1100 - 652210.52 + 1250 - 652210.32
 = 37,525
 sX = +193.71
 Var1Y2 = s2
Y = 1200 - 352210.22 + 150 - 352210.52 + 1-100 - 352210.32
 = 11,025
 sY = +105.00
 sXY = 1-300 - 6521200 - 35210.22 + 1100 - 652150 - 35210.52
+ 1250 - 6521-100 - 35210.32
 = -12,045 + 262.5 - 7,492.5
 = -19,275
Student Tip
The covariance dis-
cussed in this section 
measures the strength 
of the linear relationship 
between the probability 
distributions of two vari-
ables, while the sample 
covariance discussed 
in Chapter 3 measures 
the strength of the linear 
relationship between two 
numerical variables.

	
5.2  Covariance of a Probability Distribution and Its Application in Finance 	
219
Thus, the Dow Jones fund has a higher expected value (i.e., larger expected return) than the 
weak-economy fund but also has a higher standard deviation (i.e., more risk). The covariance 
of -19,275 between the two investment returns indicates a negative relationship in which the 
return of two investments are varying in the opposite direction. Therefore, when the return on 
one investment is high, typically, the return on the other investment is low.
Expected Value, Variance, and Standard Deviation  
of the Sum of Two Variables
Equations (5.1) through (5.3) define the expected value, variance, and standard deviation of a 
probability distribution, and Equation (5.4) defines the covariance between two variables, X 
and Y. The expected value of the sum of two variables is equal to the sum of the expected 
values. The variance of the sum of two variables is equal to the sum of the variances plus 
twice the covariance. The standard deviation of the sum of two variables is the square root 
of the variance of the sum of two variables.
Expected Value of the Sum of Two Variables
	
E1X + Y2 = E1X2 + E1Y2	
(5.5)
Variance of the Sum of Two Variables
	
Var1X + Y2 = s2
X + Y = s2
X + s2
Y + 2sXY	
(5.6)
Standard Deviation of the Sum of Two Variables
	
sX + Y = 2s2
X + Y	
(5.7)
To illustrate the expected value, variance, and standard deviation of the sum of two vari-
ables, consider the two investments previously discussed. If X = return of the Dow Jones fund 
and Y = return of the weak@economy fund, using Equations (5.5), (5.6), and (5.7),
 E1X + Y2 = E1X2 + E1Y2 = 65 + 35 = +100
 s2
X + Y = s2
X + s2
Y + 2sXY
 = 37,525 + 11,025 + 1221-19,2752
 = 10,000
 sX + Y = +100
The expected value of the sum of the return of the Dow Jones fund and the return of 
the weak-economy fund is $100, with a standard deviation of $100. The standard deviation of 
the sum of the two investments is less than the standard deviation of either single investment 
because there is a large negative covariance between the investments.
Portfolio Expected Return and Portfolio Risk
The covariance and the expected value and standard deviation of the sum of two random vari-
ables can be applied to analyzing portfolios, or groupings of assets made for investment pur-
poses. Investors combine assets into portfolios to reduce their risk (see references 1 and 2). 
Often, the objective is to try to maximize the return while making the risk as small as possible. 
For such portfolios, rather than study the sum of two random variables, the investor weights 
each investment by the proportion of assets assigned to that investment. Equations (5.8) and 
(5.9) define the portfolio expected return and portfolio risk.

220	
Chapter 5  Discrete Probability Distributions
In the previous section, you evaluated the expected return and risk of two different in-
vestments, a Dow Jones fund and a weak-economy fund. You also computed the covari-
ance of the two investments. Now, suppose that you want to form a portfolio of these two 
investments that consists of an equal investment in each of these two funds. To compute 
the portfolio expected return and the portfolio risk, using Equations (5.8) and (5.9), with 
w = 0.50, E1X2 = +65, E1Y2 = +35, s2
X = 37,525, s2
Y = 11,025, and sXY = -19,275,
 E1P2 = 10.521652 + 11 - 0.521352 = +50
 sp = 210.522137,5252 + 11 - 0.522111,0252 + 210.5211 - 0.521-19,2752
 = 22,500 = +50
Thus, the portfolio has an expected return of $50 for each $1,000 invested (a return of 5%) and 
a portfolio risk of $50. The portfolio risk here is smaller than the standard deviation of either 
investment because there is a large negative covariance between the two investments. The fact 
that each investment performs best under different circumstances reduces the overall risk of 
the portfolio.
Collapses in the financial marketplace that have occurred in the recent past have 
caused some investors to consider the effect of outcomes that have only a small chance of 
occurring but that could produce extremely negative results. (Some, including the author 
of reference 6, have labeled these outcomes “black swans.”) Example 5.1 considers such 
an outcome by examining the expected return, the standard deviation of the return, and the 
covariance of two investment strategies—one that invests in a fund that does well when 
there is an extreme recession and the other that invests in a fund that does well under posi-
tive economic conditions.
Portfolio Expected Return
The portfolio expected return for a two-asset investment is equal to the weight assigned 
to asset X multiplied by the expected return of asset X plus the weight assigned to asset Y 
multiplied by the expected return of asset Y.
	
E1P2 = wE1X2 + 11 - w2E1Y2	
(5.8)
	
where
E1P2 = portfolio expected return
w = portion of the portfolio value assigned to asset X
11 - w2 = portion of the portfolio value assigned to asset Y
E1X2 = expected return of asset X
E1Y2 = expected return of asset Y
Portfolio Risk
The portfolio risk for a two-asset investment is equal to the square root of the sum of these 
three products: w2 multiplied by the variance of X, 11 - w22 multiplied by the variance of 
Y, and 2 multiplied by w multiplied by 11 - w2 multiplied by the covariance.
	
sp = 2w2s2
X + 11 - w22s2
Y + 2w11 - w2sXY	
(5.9)

	
5.2  Covariance of a Probability Distribution and Its Application in Finance 	
221
Example 5.1
Computing the Ex-
pected Return, the 
Standard Deviation 
of the Return, and 
the Covariance of 
Two Investment 
Strategies
You plan to invest $1,000 in one of two funds. Table 5.5 shows the annual return (per $1,000) 
of each of these investments under different economic conditions, along with the probability 
that each of these economic conditions will occur.
For the Black Swan fund and the Good Times fund, compute the expected return and standard 
deviation of the return for each fund, and the covariance between the two funds. Would you 
invest in the Black Swan fund or the Good Times fund? Explain.
Solution  Let X = Black Swan fund and Y = Good Times fund.
E1X2 = mX = 1400210.012 + 1-30210.092 + 130210.152 + 150210.352
	
+ 1100210.302 + 1100210.102 = +63.30
E1Y2 = mY = 1-200210.012 + 1-100210.092 + 150210.152 + 190210.352
	
1250210.302 + 1225210.102 = +125.50
Var1X2 = s2
X = 1400 - 63.302210.012 + 1-30 - 63.302210.092 + 130 - 63.302210.152
150 - 63.302210.352 + 1100 - 63.302210.32 + 1100 - 63.302210.12 = 2,684.11
	
sX = +51.81
Var1Y2 = s2
Y = 1-200 - 125.502210.012 + 1-100 - 125.502210.092
	
+ 150 - 125.502210.152 + 190 - 125.502210.352 + 1250 - 125.502210.32
	
+ 1225 - 125.502210.12 = 12,572.25
	
sY = +112.13
sXY = 1400 - 63.3021-200 - 125.50210.012 + 1-30 - 63.3021-100 - 125.50210.092
	
+ 130 - 63.302150 - 125.50210.152 + 150 - 63.302190 - 125.50210.352
	
+ 1100 - 63.3021250 - 125.50210.32 + 1100 - 63.3021225 - 125.50210.12
	
sxy = +3,075.85
Thus, the Good Times fund not only has a much higher expected value (i.e., larger expected 
return) than the Black Swan fund ($125.50 as compared to $63.30 per $1,000) but also has a 
much higher standard deviation ($112.13 vs. $51.81). Deciding which fund to invest in is a 
matter of how much risk you are willing to tolerate. Although the Good Times fund has a much 
higher expected return, many people would be reluctant to invest in a fund where there is a 
chance of a substantial loss.
The covariance of $3,075.85 between the two investments indicates a positive relationship in 
which the two investments are varying in the same direction. Therefore, when the return on one 
investment is high, typically, the return on the other is also high. However, from Table 5.5, you 
can see that the magnitude of the return varies, depending on the economic condition that actu-
ally occurs. Therefore, you might decide to include both funds in your portfolio. The percentage 
allocated to each fund would be based on your tolerance of risk balanced by your desire for 
maximum return (see Problem 5.15).
T a b l e  5 . 5
Estimated Returns of Two Funds
Probability
Economic Condition
Black Swan Fund
Good Times Fund
0.01
Extreme recession
400
-200
0.09
Recession
-30
-100
0.15
Stagnation
30
50
0.35
Slow growth
50
90
0.30
Moderate growth
100
250
0.10
High growth
100
225

222	
Chapter 5  Discrete Probability Distributions
Problems for Section 5.2
Learning the Basics
5.7  The following probability distribution represents the vari-
ables X and Y:
P1x, y2
X
Y
0.35
200
300
0.65
300
200
Compute
a.	 Calculate E1X2 and E1Y2.
b.	 sX and sY. 
c.	 sXY.
d.	 E1X + Y2.
5.8  The following probability distribution represents the variable 
X and Y:
P1x, y2
X
Y
0.15
150
40
0.45
100
30
0.25
-50
30
0.15
250
20
Compute
a.	 Calculate E1X2 and E1Y2.
b.	 sX and sY.
c.	 sXY.
d.	 E1X + Y2.
5.9  If the characteristics of two investments X and Y are given as 
follows:
E1X2 = +60, E1Y2 = +110, s2
X = 8,500,
s2
Y = 13,500, and sXY = 7,000.
a.	 Calculate the expected return for investment if the weight as-
signed to investment X is 0.45.
b.	 Calculate the portfolio risk.
Applying the Concepts
5.10  The process of being served at a bank consists of two inde-
pendent parts—the time waiting in line and the time it takes to be 
served by the teller. Suppose that the time waiting in line has an 
expected value of 4 minutes, with a standard deviation of 1.2 min-
utes, and the time it takes to be served by the teller has an expected 
value of 5.5 minutes, with a standard deviation of 1.5 minutes. 
Compute the
a.	 expected value of the total time it takes to be served at the bank.
b.	 standard deviation of the total time it takes to be served at the 
bank.
5.11  In the portfolio example in this section (see page 220), half 
the portfolio assets are invested in the Dow Jones fund and half in 
a weak-economy fund. Recalculate the portfolio expected return 
and the portfolio risk if
a.	 30% of the portfolio assets are invested in the Dow Jones fund 
and 70% in a weak-economy fund.
b.	 70% of the portfolio assets are invested in the Dow Jones fund 
and 30% in a weak-economy fund.
c.	 Which of the three investment strategies (30%, 50%, or 70% in 
the Dow Jones fund) would you recommend? Why?
5.12  You are trying to develop a strategy for investing 
in two different stocks. The anticipated annual return 
for a $1,000 investment in each stock under four different eco-
nomic conditions has the following probability distribution:
Returns
Probability
Economic Condition
Stock X
Stock Y
0.1
Recession
-100
50
0.3
Slow growth
0
150
0.3
Moderate growth
80
-20
0.3
Fast growth
150
-100
Compute the
a.	 expected return for stock X and for stock Y.
b.	 standard deviation for stock X and for stock Y.
c.	 covariance of stock X and stock Y.
d.	 Would you invest in stock X or stock Y? Explain.
5.13  Suppose that in Problem 5.12 you wanted to create a port-
folio that consists of stock X and stock Y. Compute the portfolio 
expected return and portfolio risk for each of the following per-
centages invested in stock X:
a.	 30%
b.	 50%
c.	 70%
d.	 On the basis of the results of (a) through (c), which portfolio 
would you recommend? Explain.
5.14  In his effort to construct an investment portfolio of two 
stocks Johan allocated $2000 of investment to each of stock X and 
stock Y. The following probability distribution table shows four 
different probability values associated with four different eco-
nomic conditions with the anticipated return on each of the stocks. 
Returns
Probability
Economic Condition
Stock X
Stock Y
0.09
Recession
-150
45
0.25
Slow growth
10
155
0.35
Moderate growth
120
-25
0.31
Fast growth
150
-100
Compute
a.	 the expected return for stock X and stock Y.
b.	 the standard deviation for both stock X and stock Y.
c.	 the covariance for both stock X and stock Y.
d.	 which of the two stocks is a better investment. Explain.
SELF 
Test 

	
5.3  Binomial Distribution	
223
5.15  Suppose that in Example 5.1 on page 221, you wanted to 
create a portfolio that consists of the Black Swan fund and the 
Good Times fund. Compute the portfolio expected return and 
portfolio risk for each of the following percentages invested in the 
Black Swan fund:
a.	 30%
b.	 50%
c.	 70%
d.	 On the basis of the results of (a) through (c), which portfolio 
would you recommend? Explain.
5.16  You plan to invest $1,000 in a corporate bond fund or in a 
common stock fund. The following table presents the annual re-
turn (per $1,000) of each of these investments under various eco-
nomic conditions and the probability that each of those economic 
conditions will occur. Compute the
Probability
Economic 
Condition
Corporate 
Bond Fund
Common 
Stock Fund
0.01
Extreme 
recession
-200
-999
0.09
Recession
-70
-300
0.15
Stagnation
30
-100
0.35
Slow growth
80
100
0.30
Moderate growth
100
150
0.10
High growth
120
350
a.	 expected return for the corporate bond fund and for the com-
mon stock fund.
b.	 standard deviation for the corporate bond fund and for the com-
mon stock fund.
c.	 covariance of the corporate bond fund and the common stock 
fund.
d.	 Would you invest in the corporate bond fund or the common 
stock fund? Explain.
e.	 If you chose to invest in the common stock fund in (d), what do 
you think about the possibility of losing $999 of every $1,000 
invested if there is an extreme recession?
5.17  Suppose that in Problem 5.16 you wanted to create a portfo-
lio that consists of the corporate bond fund and the common stock 
fund. Compute the portfolio expected return and portfolio risk for 
each of the following situations:
a.	 $300 in the corporate bond fund and $700 in the common stock 
fund.
b.	 $500 in each fund.
c.	 $700 in the corporate bond fund and $300 in the common stock 
fund.
d.	 On the basis of the results of (a) through (c), which portfolio 
would you recommend? Explain.
5.3  Binomial Distribution
This is the first of three sections that considers mathematical models. A mathematical model 
is a mathematical expression that represents a variable of interest. When a mathematical model 
exists, you can compute the exact probability of occurrence of any particular value of the vari-
able. For discrete random variables, the mathematical model is a probability distribution 
function.
The binomial distribution is an important mathematical model used in many business sit-
uations. You use the binomial distribution when the discrete variable is the number of events of 
interest in a sample of n observations. The binomial distribution has four important properties.
Student Tip
Do not confuse this use 
of the Greek letter pi, p, 
to represent the probabil-
ity of an event of interest 
with the mathematical 
constant that is the 
ratio of the circumfer-
ence to a diameter of a 
circle—approximately 
3.14159—which is 
also known by the same 
Greek letter.
Properties of the Binomial Distribution
• The sample consists of a fixed number of observations, n.
• Each observation is classified into one of two mutually exclusive and collectively exhaus-
tive categories.
• The probability of an observation being classified as the event of interest, p, is constant 
from observation to observation. Thus, the probability of an observation being classified 
as not being the event of interest, 1 - p, is constant over all observations.
• The value of any observation is independent of the value of any other observation.
Returning to the Ricknel Home Improvement scenario presented on page 213 concerning 
the accounting information system, suppose the event of interest is defined as a tagged order 
form. You want to determine the number of tagged order forms in a given sample of orders.
What results can occur? If the sample contains four orders, there could be none, one, two, 
three, or four tagged order forms. No other value can occur because the number of tagged  

224	
Chapter 5  Discrete Probability Distributions
order forms cannot be more than the sample size, n, and cannot be less than zero. Therefore, 
the range of the binomial random variable is from 0 to n.
Suppose that you observe the following result in a sample of four orders:
First Order
Second Order
Third Order
Fourth Order
Tagged
Tagged
Not tagged
Tagged
What is the probability of having three tagged order forms in a sample of four orders in 
this particular sequence? Because the historical probability of a tagged order is 0.10, the prob-
ability that each order occurs in the sequence is
First Order
Second Order
Third Order
Fourth Order
p = 0.10
p = 0.10
1 - p = 0.90
p = 0.10
Each outcome is independent of the others because the order forms were selected from an 
extremely large or practically infinite population and each order form could only be selected 
once. Therefore, the probability of having this particular sequence is
 pp11 - p2p = p311 - p21
 = 10.102310.9021
 = 10.10210.10210.10210.902
 = 0.0009
This result indicates only the probability of three tagged order forms (events of interest) from a 
sample of four order forms in a specific sequence. To find the number of ways of selecting x objects 
from n objects, irrespective of sequence, you use the rule of combinations1 given in Equation (5.10).
1Refer to Section 4.4 for further 
discussion of counting rules.
Combinations
The number of combinations of selecting x objects2 out of n objects is given by
	
nCx =
n!
x!1n - x2!	
(5.10)
	
where
n! = 1n21n - 12 g  112 is called n factorial. By definition, 0! = 1.
2On many scientific calculators, 
there is a button labeled nCr that al-
lows you to compute the number of 
combinations. On these calculators, 
the symbol r is used instead of x.
With n = 4 and x = 3, there are
nCx =
n!
x!1n - x2! =
4!
3!14 - 32! = 4 *  3 *  2 *  1
13 *  2 *  12112 = 4
such sequences. The four possible sequences are
Sequence 1 = 1tagged, tagged, tagged, not tagged2, with probability
ppp11 - p2 = p311 - p21 = 0.0009
Sequence 2 = 1tagged, tagged, not tagged, tagged2, with probability
pp11 - p2p = p311 - p21 = 0.0009
Sequence 3 = 1tagged, not tagged, tagged, tagged2, with probability
p11 - p2pp = p311 - p21 = 0.0009
Sequence 4 = 1not tagged, tagged, tagged, tagged2, with probability
11 - p2ppp = p311 - p21 = 0.0009

	
5.3  Binomial Distribution	
225
Therefore, the probability of three tagged order forms is equal to
1number of possible sequences2 * 1probability of a particular sequence2
= 142 *  10.00092 = 0.0036
You can make a similar, intuitive derivation for the other possible values of the random 
variable—zero, one, two, and four tagged order forms. However, as n, the sample size, gets 
large, the computations involved in using this intuitive approach become time-consuming. 
Equation (5.11) is the mathematical model that provides a general formula for computing any 
probability from the binomial distribution with the number of events of interest, x, given n  
and p.
Binomial Distribution
	
P1X = x n, p2 =
n!
x!1n - x2! px11 - p2n - x	
(5.11)
	
where
P1X = x n, p2 = probability that X = x events of interest, given n and p
n = number of observations
p = probability of an event of interest
1 - p = probability of not having an event of interest
x = number of events of interest in the sample 1X = 0, 1, 2, c , n2
n!
x!1n - x2! = number of combinations of x events of interest out of n observations
Equation (5.11) restates what was intuitively derived previously. The binomial variable X 
can have any integer value x from 0 through n. In Equation (5.11), the product
px11 - p2n - x
represents the probability of exactly x events of interest from n observations in a particular 
sequence.
The term
n!
x!1n - x2!
is the number of combinations of the x events of interest from the n observations possible. 
Hence, given the number of observations, n, and the probability of an event of interest, p, the 
probability of x events of interest is
 P1X = x n, p2 = 1number of combinations2 *  1probability of a particular combination2
 =
n!
x!1n - x2! px11 - p2n - x
Example 5.2 illustrates the use of Equation (5.11). Examples 5.3 and 5.4 show the compu-
tations for other values of X.

226	
Chapter 5  Discrete Probability Distributions
Example 5.2
Determining 
P1X = 32, Given 
n = 4 and p = 0.1
If the likelihood of a tagged order form is 0.1, what is the probability that there are three 
tagged order forms in the sample of four?
Solution  Using Equation (5.11), the probability of three tagged orders from a sample of 
four is
 P1X = 3 n = 4, p = 0.12 =
4!
3!14 - 32!10.12311 - 0.124 - 3
 =
4!
3!112!10.12310.921
 = 410.1210.1210.1210.92 = 0.0036
Example 5.3
Determining 
P1X Ú 32, Given 
n = 4 and p = 0.1
If the likelihood of a tagged order form is 0.1, what is the probability that there are three or 
more (i.e., at least three) tagged order forms in the sample of four?
Solution  In Example 5.2, you found that the probability of exactly three tagged order 
forms from a sample of four is 0.0036. To compute the probability of at least three tagged 
order forms, you need to add the probability of three tagged order forms to the probability of 
four tagged order forms. The probability of four tagged order forms is
 P1X = 4 n = 4, p = 0.12 =
4!
4!14 - 42!10.12411 - 0.124 - 4
 =
4!
4!102!10.12410.920
 = 110.1210.1210.1210.12112 = 0.0001
Thus, the probability of at least three tagged order forms is
 P1X Ú 32 = P1X = 32 + P1X = 42
 = 0.0036 + 0.0001
 = 0.0037
There is a 0.37% chance that there will be at least three tagged order forms in a sample of 
four.
Student Tip
Another way of saying 
“three or more” is “at 
least three.”
Example 5.4
Determining 
P1X 6 32, Given 
n = 4 and p = 0.1
If the likelihood of a tagged order form is 0.1, what is the probability that there are less than 
three tagged order forms in the sample of four?
Solution  The probability that there are less than three tagged order forms is
P1X 6 32 = P1X = 02 + P1X = 12 + P1X = 22
Using Equation (5.11) on page 225, these probabilities are
 P1X = 0 n = 4, p = 0.12 =
4!
0!14 - 02!10.12011 - 0.124 - 0 = 0.6561

	
5.3  Binomial Distribution	
227
Computing binomial probabilities become tedious as n gets large. Figure 5.2 shows how 
Excel and Minitab can compute binomial probabilities for you. You can also look up binomial 
probabilities in a table of probabilities.
 P1X = 1 n = 4, p = 0.12 =
4!
1!14 - 12! 10.121 11 - 0.124 - 1 = 0.2916
 P1X = 2 n = 4, p = 0.12 =
4!
2!14 - 22!10.12211 - 0.124 - 2 = 0.0486
Therefore, P1X 6 32 = 0.6561 + 0.2916 + 0.0486 = 0.9963. P1X 6 32 could also be cal-
culated from its complement, P1X Ú 32, as follows:
 P1X 6 32 = 1 - P1X Ú 32
 = 1 - 0.0037 = 0.9963
Learn More
The Binomial Table  
online topic contains both 
a  binomial probabilities table 
and a cumulative binomial 
probabilities table and explains 
how to use these tables to 
compute binomial and cumu-
lative binomial probabilities.
F i g u r e  5 . 2
Excel and Minitab results 
for computing binomial 
probabilities with n = 4 
and p = 0.1
The shape of a binomial probability distribution depends on the values of n and p. When-
ever p = 0.5, the binomial distribution is symmetrical, regardless of how large or small the 
value of n. When p ≠0.5, the distribution is skewed. The closer p is to 0.5 and the larger the 
number of observations, n, the less skewed the distribution becomes. For example, the distribu-
tion of the number of tagged order forms is highly right skewed because p = 0.1 and n = 4 
(see Figure 5.3).
F i g u r e  5 . 3
Histogram of the binomial 
probability with n = 4 
and p = 0.1

228	
Chapter 5  Discrete Probability Distributions
Observe from Figure 5.3 that unlike the histogram for continuous variables in Section 2.4, 
the bars for the values are very thin, and there is a large gap between each pair of values. That 
is because the histogram represents a discrete variable. (Theoretically, the bars should have no 
width. They should be vertical lines.)
The mean (or expected value) of the binomial distribution is equal to the product of n and 
p. Instead of using Equation (5.1) on page 215 to compute the mean of the probability distri-
bution, you can use Equation (5.12) to compute the mean for variables that follow the binomial 
distribution.
Mean of the Binomial Distribution
The mean, m, of the binomial distribution is equal to the sample size, n, multiplied by the 
probability of an event of interest, p.
	
m = E1X2 = np	
(5.12)
On the average, over the long run, you theoretically expect  m = E1X2 = np] = 14210.12 =
0.4 tagged order form in a sample of four orders.
The standard deviation of the binomial distribution can be calculated using Equation 
(5.13).
Standard Deviation of the Binomial Distribution
	
s = 2s2 = 2Var1X2 = 2np11 - p2	
(5.13)
The standard deviation of the number of tagged order forms is
s = 2410.1210.92 = 0.60
You get the same result if you use Equation (5.3) on page 216.
Example 5.5 applies the binomial distribution to service at a fast-food restaurant.
Example 5.5
Computing Bino-
mial Probabilities 
for Service at a 
Fast-Food  
Restaurant
Accuracy in taking orders at a drive-through window is important for fast-food chains. Periodi-
cally, QSR Magazine publishes the results of a survey that measures accuracy, defined as the 
percentage of orders that are filled correctly. In a recent year, the percentage of orders filled 
correctly at Wendy’s was approximately 88.9% (bit.ly/QEIeOW). Suppose that you go to 
the drive-through window at Wendy’s and place an order. Two friends of yours independently 
place orders at the drive-through window at the same Wendy’s. What are the probabilities that 
all three, that none of the three, and that at least two of the three orders will be filled correctly? 
What are the mean and standard deviation of the binomial distribution for the number of orders 
filled correctly?
Solution  Because there are three orders and the probability of a correct order is 
0.889, n = 3, and p = 0.889, using Equation (5.11) on page 225,
 P1X = 3 n = 3, p = 0.8892 =
3!
3!13 - 32!10.8892311 - 0.88923 - 3
 =
3!
3!13 - 32!10.8892310.11120
 = 110.889210.889210.8892112 = 0.7026

	
5.3  Binomial Distribution	
229
 P1X = 0 n = 3, p = 0.8892 =
3!
0!13 - 02!10.8892011 - 0.88923 - 0
	
 =
3!
0!13 - 02!10.8892010.11123
	
 = 111210.111210.111210.1112 = 0.0014
 P1X = 2 n = 3, p = 0.8892 =
3!
2!13 - 22!10.8892211 - 0.88923 - 2
	
 =
3!
2!13 - 22!10.8892210.11121
	
 = 310.889210.889210.1112 = 0.2632
 P1X Ú 22 = P1X = 22+P1X = 32
 = 0.2632 + 0.7026
 = 0.9658
Using Equations (5.12) and (5.13),
 m = E1X2 = np = 310.8892 = 2.667
 s = 2s2 = 2Var1X2 = 2np11 - p2
 = 2310.889210.1112
 = 20.2960 = 0.5441
The mean number of orders filled correctly in a sample of three orders is 2.667, and the 
standard deviation is 0.5441. The probability that all three orders are filled correctly is 0.7026, 
or 70.26%. The probability that none of the orders are filled correctly is 0.0014, or 0.14%. The 
probability that at least two orders are filled correctly is 0.9658, or 96.58%.
Problems for Section 5.3
Learning the Basics
5.18  For the following determine P1X2 given:
N
P
X
P1X2
a.
5
0.11
0
b.
12
0.45
9
c.
14
0.55
8
d.
8
0.85
5
5.19  Calculate P1X2 when n  = 8 and p  = 0.45, in parts (a) 
through (d).
a.	 X = 4	
b.  X … 3	
c.  X 6 2	
d.  X 7 1
5.20  Determine the mean and standard of the binomial distribu-
tion for the variable X in each of the following:
a.	 N = 5 and p = 0.10
b.	 N = 5 and p = 0.50
c.	 N = 4 and p = 0.75
d.	 N = 3 and p = 0.45
Applying the Concepts
5.21  The increase or decrease in the price of a stock between  
the beginning and the end of a trading day is assumed to be  
an equally likely random event. What is the probability that a  
stock will show an increase in its closing price on five consecutive 
days?
5.22  A recent YouGov (UK) survey reported that 27% of under-
25-year-olds in the United Kingdom own tablets. (Data extracted 
from “Tablets Spur News Consumption,” bit.ly/12XpmR0). Us-
ing the binomial distribution, what is the probability that in the 
next six under-25-year-olds surveyed,
a.	 four will own a tablet?
b.	 all six will own a tablet?
c.	 at least four will own a tablet?
d.	 What are the mean and standard deviation of the number  
of under-25-year-olds who will own a tablet in a survey  
of six?
e.	 What assumptions do you need to make in (a) through (c)?

230	
Chapter 5  Discrete Probability Distributions
5.23  A student is taking a multiple-choice exam in which each 
question has four choices. Assume that the student has no knowl-
edge of the correct answers to any of the questions. She has de-
cided on a strategy in which she will place four balls (marked 
A, B, C, and D) into a box. She randomly selects one ball for each 
question and replaces the ball in the box. The marking on the ball 
will determine her answer to the question. There are five multiple-
choice questions on the exam. What is the probability that she will 
get
a.	 five questions correct?
b.	 at least four questions correct?
c.	 no questions correct?
d.	 no more than two questions correct?
5.24  A television manufacturing company performs quality assur-
ance tests of its products before shipping them to the market. If the 
company chooses a random sample of 15 sets for the test, and assum-
ing that based on previous data the company has a 6% failure rate for 
the newly manufactured sets, what is the probability that:
a.	 none of the TV sets are defective.
b.	 exactly one of the TV sets is defective.
c.	 two or fewer of the TV sets are defective.
d.	 three or more of the TV sets are defective
5.25  When a customer places an order with Rudy’s On-Line 
Office Supplies, a computerized accounting information system 
(AIS) automatically checks to see if the customer has exceeded 
his or her credit limit. Past records indicate that the probability 
of customers exceeding their credit limit is 0.05. Suppose that, on 
a given day, 20 customers place orders. Assume that the number 
of customers that the AIS detects as having exceeded their credit 
limit is distributed as a binomial random variable.
a.	 What are the mean and standard deviation of the number of 
customers exceeding their credit limits?
b.	 What is the probability that zero customers will exceed their 
credit limits?
c.	 What is the probability that one customer will exceed his or her 
credit limit?
d.	 What is the probability that two or more customers will exceed 
their credit limits?
5.26  In Example 5.5 on page 228, you and two 
friends decided to go to Wendy’s. Now, suppose that in-
stead you go to Burger King, which recently filled approximately 
83% of orders correctly. What is the probability that
a.	 all three orders will be filled correctly?
b.	 none of the three will be filled correctly?
c.	 at least two of the three will be filled correctly?
d.	 What are the mean and standard deviation of the binomial dis-
tribution used in (a) through (c)? Interpret these values.
5.27  In Example 5.5 on page 228, you and two friends decided 
to go to Wendy’s. Now, suppose that instead you go to McDon-
ald’s, which recently filled approximately 90.9% of the orders cor-
rectly. What is the probability that
a.	 all three orders will be filled correctly?
b.	 none of the three will be filled correctly?
c.	 at least two of the three will be filled correctly?
d.	 What are the mean and standard deviation of the binomial dis-
tribution used in (a) through (c)? Interpret these values.
e.	 Compare the result of (a) through (d) with those of Burger King 
in Problem 5.26 and Wendy’s in Example 5.5 on page 228.
5.4  Poisson Distribution
Many studies are based on counts of the occurrences of a particular event in a given interval 
of time or space (often referred to as an area of opportunity). In such an area of opportunity 
there can be more than one occurrence of an event. The Poisson distribution can be used to 
compute probabilities in such situations. Examples of variables that follow the Poisson distri-
bution are the surface defects on a new refrigerator, the number of network failures in a day, 
the number of people arriving at a bank, and the number of fleas on the body of a dog. You can 
use the Poisson distribution to calculate probabilities in situations such as these if the follow-
ing properties hold:
 • You are interested in counting the number of times a particular event occurs in a given 
area of opportunity. The area of opportunity is defined by time, length, surface area, and 
so forth.
 • The probability that an event occurs in a given area of opportunity is the same for all the 
areas of opportunity.
 • The number of events that occur in one area of opportunity is independent of the number 
of events that occur in any other area of opportunity.
 • The probability that two or more events will occur in an area of opportunity approaches 
zero as the area of opportunity becomes smaller.
Consider the number of customers arriving during the lunch hour at a bank located in the 
central business district in a large city. You are interested in the number of customers who 
arrive each minute. Does this situation match the four properties of the Poisson distribution 
given earlier?
SELF 
Test 

	
5.4  Poisson Distribution	
231
First, the event of interest is a customer arriving, and the given area of opportunity is de-
fined as a one-minute interval. Will zero customers arrive, one customer arrive, two customers 
arrive, and so on? Second, it is reasonable to assume that the probability that a customer ar-
rives during a particular one-minute interval is the same as the probability for all the other one-
minute intervals. Third, the arrival of one customer in any one-minute interval has no effect on 
(i.e., is independent of) the arrival of any other customer in any other one-minute interval. Fi-
nally, the probability that two or more customers will arrive in a given time period approaches 
zero as the time interval becomes small. For example, the probability is virtually zero that two 
customers will arrive in a time interval of 0.01 second. Thus, you can use the Poisson distribu-
tion to determine probabilities involving the number of customers arriving at the bank in a one-
minute time interval during the lunch hour.
The Poisson distribution has one characteristic, called l (the Greek lowercase letter 
lambda), which is the mean or expected number of events per unit. The variance of a Poisson 
distribution is also equal to l, and the standard deviation is equal to1l. The number of events, 
X, of the Poisson random variable ranges from 0 to infinity 1∞2.
Equation (5.14) is the mathematical expression for the Poisson distribution for computing 
the probability of X = x events, given that l events are expected.
Poisson Distribution
	
P1X = x l2 = e-llx
x! 	
(5.14)
	
where
P1X = x l2 = probability that X = x events in an area of opportunity given l
l = expected number of events
e = mathematical constant approximated by 2.71828
x = number of events 1x = 0, 1, 2, c2
To illustrate an application of the Poisson distribution, suppose that the mean number of 
customers who arrive per minute at the bank during the noon-to-1 p.m. hour is equal to 3.0. 
What is the probability that in a given minute, exactly two customers will arrive? And what is 
the probability that more than two customers will arrive in a given minute?
Using Equation (5.14) and l = 3, the probability that in a given minute exactly two cus-
tomers will arrive is
P1X = 2 l = 32 = e-3.013.022
2!
=
9
12.7182823122 = 0.2240
To determine the probability that in any given minute more than two customers will arrive,
P1X 7 22 = P1X = 32 + P1X = 42 + g
Because in a probability distribution, all the probabilities must sum to 1, the terms on the right 
side of the equation P1X 7 22 also represent the complement of the probability that X is less 
than or equal to 2 [i.e., 1 - P1X … 22]. Thus,
P1X 7 22 = 1 - P1X … 22 = 1 - 3P1X = 02 + P1X = 12 + P1X = 224

232	
Chapter 5  Discrete Probability Distributions
Now, using Equation (5.14),
 P1X 7 22 = 1 - c e-3.013.020
0!
+ e-3.013.021
1!
+ e-3.013.022
2!
d
 = 1 - 30.0498 + 0.1494 + 0.22404
 = 1 - 0.4232 = 0.5768
Thus, there is a 57.68% chance that more than two customers will arrive in the same minute.
Computing Poisson probabilities can be tedious. Figure 5.4 shows how Excel and Minitab 
can compute Poisson probabilities for you. You can also look up Poisson probabilities in a 
table of probabilities.
F i g u r e  5 . 4
Excel and Minitab results 
for computing Poisson 
probabilities with l = 3
Learn More
The Poisson Table online 
topic contains a table of 
Poisson probabilities and 
explains how to use the 
table to compute Poisson 
probabilities.
Example 5.6
Computing Poisson 
Probabilities
The number of work-related injuries per month in a manufacturing plant is known to follow a 
Poisson distribution, with a mean of 2.5 work-related injuries a month. What is the probability 
that in a given month, no work-related injuries occur? That at least one work-related injury 
­occurs?
Solution  Using Equation (5.14) on page 231 with l = 2.5 (or Excel, Minitab, or a Pois-
son table lookup), the probability that in a given month no work-related injuries occur is
P1X = 0 l = 2.52 = e-2.512.520
0!
=
1
12.7182822.5112 = 0.0821
The probability that there will be no work-related injuries in a given month is 0.0821, or 
8.21%. Thus,
 P1X Ú 12 = 1 - P1X = 02
 = 1 - 0.0821
 = 0.9179
The probability that there will be at least one work-related injury is 0.9179, or 91.79%.

	
5.4  Poisson Distribution	
233
Problems for Section 5.4
Learning the Basics
5.28  Assume that each of the following is a Poisson distribution. 
Calculate the P1X2 for the different values of X.
a.	 If l = 2.6 and X Ú 2
b.	 If l = 7.0 and X Ú 8
c.	 If l = 0.4 and X Ú 1
d.	 If l = 3.8 and X Ú 1
5.29  Assume that each of the following is a Poisson distribution. 
Calculate the P1X2 for the different values of X.
a.	 If l = 3.0 and X Ú 2
b.	 If l = 8 and X Ú 3
c.	 If l = 0.4 and X Ú 1
d.	 If l = 3.8 and X Ú 1
e.	 If l = 4.5 and X Ú 3
5.30  Assume a Poisson distribution with l = 6.0. What is the 
probability that
a.  X = 1
b.  X 6 1
c.  X 7 1
d.  X … 1
Applying the Concepts
5.31  A local internet provider has determined that the daily 
breakdown in internet connection is a Poisson variable. The mean 
number of internet connection breakdowns in a day is 3.4. What is 
the probability that in any given day
a.	 there will be no internet connection breakdown.
b.	 exactly one internet connection breakdown will occur.
c.	 two or more breakdowns in internet connection will occur.
d.	 there will be fewer than three internet connection breakdowns.
5.32  The quality control manager of Marilyn’s Cook-
ies is inspecting a batch of chocolate-chip cookies that 
has just been baked. If the production process is in control, the 
mean number of chocolate-chip parts per cookie is 6.0. What is the 
probability that in any particular cookie being inspected
a.	 fewer than five chocolate-chip parts will be found?
b.	 exactly five chocolate-chip parts will be found?
c.	 five or more chocolate-chip parts will be found?
d.	 either four or five chocolate-chip parts will be found?
5.33  Refer to Problem 5.32. How many cookies in a batch of 100 
should the manager expect to discard if company policy requires 
that all chocolate-chip cookies sold have at least four chocolate-
chip parts?
5.34  The U.S. Department of Transportation maintains statistics 
for mishandled bags per 1,000 airline passengers. In February 
2013, Delta mishandled 2.05 bags per 1,000 passengers. What is 
the probability that in the next 1,000 passengers, Delta will have
a.	 no mishandled bags?
b.	 at least one mishandled bag?
c.	 at least two mishandled bags?
5.35  The U.S. Department of Transportation maintains statistics 
for involuntary denial of boarding. In February 2013, the Ameri-
can Airlines rate of involuntarily denying boarding was 0.74 per 
10,000 passengers. What is the probability that in the next 10,000 
passengers, there will be
a.	 no one involuntarily denied boarding?
b.	 at least one person involuntarily denied boarding?
c.	 at least two persons involuntarily denied boarding?
5.36  The Consumer Financial Protection Bureau’s consumer re-
sponse team hears directly from consumers about the challenges 
they face in the marketplace, brings their concerns to the atten-
tion of financial institutions, and assists in addressing their com-
plaints. The consumer response team accepts complaints related 
to mortgages, bank accounts and services, private student loans, 
other consumer loans, and credit reporting. An analysis of com-
plaints over time indicates that the mean number of credit report-
ing complaints registered by consumers is 2.15 per day. (Source: 
Consumer Response: A Snapshot of Complaints Received, 1.usa.
gov/WZ9N8Q.) Assume that the number of credit reporting com-
plaints registered by consumers is distributed as a Poisson random 
variable. What is the probability that on a given day
a.	 no credit reporting complaints will be registered by consumers?
b.	 exactly one credit reporting complaint will be registered by 
consumers?
c.	 more than one credit reporting complaint will be registered by 
consumers?
d.	 fewer than two credit reporting complaints will be registered by 
consumers?
5.37  J.D. Power and Associates calculates and publishes various 
statistics concerning car quality. The dependability score measures 
problems experienced during the past 12 months by original own-
ers of three-year-old vehicles (those that were introduced for the 
2010 model year). For these models of cars, Ford had 1.27 prob-
lems per car and Toyota had 1.12 problems per car. (Data extracted 
from “2013 U.S. Vehicle Dependability Study,” J.D. Power and 
Associates, February 13, 2013, bit.ly/101aR9l.) Let X be equal to 
the number of problems with a three-year-old Ford.
a.	 What assumptions must be made in order for X to be distributed 
as a Poisson random variable? Are these assumptions reasonable?
Making the assumptions as in (a), if you purchased a Ford in the 
2010 model year, what is the probability that in the past 12 months, 
the car had
b.	 zero problems?
c.	 two or fewer problems?
d.	 Give an operational definition for problem. Why is the opera-
tional definition important in interpreting the initial quality 
score?
5.38  Refer to Problem 5.37. If you purchased a Toyota in the 
2010 model year, what is the probability that in the past 12 months 
the car had
a.	 zero problems?
b.	 two or fewer problems?
c.	 Compare your answers in (a) and (b) to those for the Ford in 
Problem 5.37 (b) and (c).
5.39  Refer to Problem 5.37. Another press release reported in 
2012 that for 2009 model cars, Ford had 1.24 problems per car 
and Toyota had 1.04 problems per car. (Data extracted from “2013 
U.S. Vehicle Dependability Study,” J.D. Power and Associates, 
February 13, 2013, bit.ly/101aR9l.) If you purchased a 2009 Ford, 
what is the probability that in the past 12 months the car had
a.	 zero problems?
b.	 two or fewer problems?
c.	 Compare your answers in (a) and (b) to those for the 2010 
model year Ford in Problem 5.37 (b) and (c).
SELF 
Test 

234	
Chapter 5  Discrete Probability Distributions
5.40  Refer to Problem 5.39. If you purchased a 2009 Toyota, 
what is the probability that in the past 12 months, the car had
a.	 zero problems?
b.	 two or fewer problems?
c.	 Compare your answers in (a) and (b) to those for the 2010 
model year Toyota in Problem 5.38 (a) and (b).
5.41  A toll-free phone number is available from 9 a.m. to 9 p.m. 
for your customers to register complaints about a product pur-
chased from your company. Past history indicates that an average 
of 0.8 calls is received per minute.
a.	 What properties must be true about the situation described here 
in order to use the Poisson distribution to calculate probabili-
ties concerning the number of phone calls received in a one-
minute period?
	
Assuming that this situation matches the properties discussed 
in (a), what is the probability that during a one-minute period
b.	 zero phone calls will be received?
c.	 three or more phone calls will be received?
d.	 What is the maximum number of phone calls that will be re-
ceived in a one-minute period 99.99% of the time?
5.5  Hypergeometric Distribution
Both the binomial distribution and the hypergeometric distribution use the number of events of 
interest in a sample containing n observations. One of the differences in these two probability dis-
tributions is in the way the samples are selected. For the binomial distribution, the sample data are 
selected with replacement from a finite population or without replacement from an infinite popu-
lation. Thus, the probability of an event of interest, p, is constant over all observations, and the 
result of any particular observation is independent of the result of any other observation. For the 
hypergeometric distribution, the sample data are selected without replacement from a finite popu-
lation. Thus, the result of one observation is dependent on the results of the previous observations.
Consider a population of size N. Let E represent the total number of events of interest in 
the population. The hypergeometric distribution is then used to find the probability of x events 
of interest in a sample of size n, selected without replacement. Equation (5.15) represents the 
mathematical expression of the hypergeometric distribution for finding x events of interest, 
given a knowledge of n, N, and E.
Hypergeometric Distribution
	
P1X = x n, N, E2 =
aE
xb aN - E
n - xb
aN
nb
	
(5.15)
	
where
P1X = x n, N, E2 = probability of x events of interest, given knowledge of n, N, and E
n = sample size
N = population size
E = number of events of interest in the population
N - E = number of events that are not of interest in the population
x = number of events of interest in the sample
aE
xb = ECx = number of combinations [see Equation (5.10) on page 224]
x … E
x … n
Because the number of events of interest in the sample, represented by x, cannot be greater 
than the number of events of interest in the population, E, nor can x be greater than the sample 
size, n, the range of the hypergeometric random variable is limited to the sample size or to the 
number of events of interest in the population, whichever is smaller.

	
5.5  Hypergeometric Distribution	
235
Equation (5.16) defines the mean of the hypergeometric distribution, and Equation (5.17) 
defines the standard deviation.
Mean of the Hypergeometric Distribution
	
m = E1X2 = nE
N 	
(5.16)
Standard Deviation of the Hypergeometric Distribution
	
s = B
nE1N - E2
N2
A
N - n
N - 1	
(5.17)
In Equation (5.17), the expression A
N - n
N - 1 is a finite population correction factor that 
results from sampling without replacement from a finite population.
To illustrate the hypergeometric distribution, suppose that you are forming a team of  
8 managers from different departments within your company. Your company has a total of  
30 managers, and 10 of these managers are from the finance department. If you are to randomly 
select members of the team, what is the probability that the team will contain 2 managers from  
the finance department? Here, the population of N = 30 managers within the company is  
finite. In addition, E = 10 are from the finance department. A team of n = 8 members is to 
be selected.
Using Equation (5.15),
 P1x = 2 n = 8, N = 30, E = 102 =
a10
2b a20
6b
a30
8b
 =
a 10!
2!182! b a
1202!
162!1142! b
a
30!
8!1222! b
 = 0.298
Thus, the probability that the team will contain two members from the finance department is 
0.298, or 29.8%.
Computing hypergeometric probabilities can be tedious, especially as N gets large.  
Figure 5.5 shows how Excel and Minitab compute hypergeometric probabilities for the team 
formation example.
F i g u r e  5 . 5
Excel and Minitab 
results for computing 
hypergeometric 
probabilities for 
the team formation 
problem

236	
Chapter 5  Discrete Probability Distributions
Example 5.7 shows an application of the hypergeometric distribution in portfolio selection.
Example 5.7
Computing 
­Hypergeometric 
Probabilities
You are a financial analyst facing the task of selecting mutual funds to purchase for a client’s 
portfolio. You have narrowed the funds to be selected to 10 different funds. In order to diversify 
your client’s portfolio, you will recommend the purchase of 4 different funds. Six of the funds 
are growth funds. What is the probability that of the 4 funds selected, 3 are growth funds?
Solution  Using Equation (5.15) with X = 3, n = 4, N = 10, and E = 6,
 P1X = 3n = 4, N = 10, E = 62 =
a6
3b a4
1b
a10
4b
 =
a
6!
3!132! b a
142!
112!132! b
a 10!
4!162! b
 = 0.3810
The probability that of the 4 funds selected, 3 are growth funds, is 0.3810, or 38.10%.
Problems for Section 5.5
Learning the Basics
5.42  Assume that you have a hyper-geometric distribution. Cal-
culate P1X2 for the following:
n
N
A
P1X2
a.
5
10
5
X = 3
b.
5
  6
4
X = 1
c.
4
12
4
X = 0
d.
8
10
4
X = 3
5.43  Use the information from problem 5.42 and calculate the 
mean and the standard deviation for the following:
n
N
A
P1X2
M
a.
5
10
5
X = 3
2.5
b.
5
12
4
X = 1
1.67
c.
4
12
4
X = 2
1.33
d.
8
10
4
X = 3
3.2
Applying the Concepts
5.44  An auditor for the Internal Revenue Service is se-
lecting a sample of 6 tax returns for an audit. If 2 or more 
of these returns are “improper,” the entire population of 100 tax returns 
will be audited. What is the probability that the entire population will 
be audited if the true number of improper returns in the population is
a.	 25?
b.	 30?
c.	 5?
d.	 10?
e.	 Discuss the differences in your results, depending on the true 
number of improper returns in the population.
5.45  KSDLDS-Pros, an IT project management consulting firm, 
is forming an IT project management team of 5 professionals. In 
the firm of 50 professionals, 8 are considered to be data analytics 
specialists. If the professionals are selected at random, what is the 
probability that the team will include
a.	 no data analytics specialist?
b.	 at least one data analytics specialist?
c.	 no more than two data analytics specialists?
d.	 What is your answer to (a) if the team consists of 7 members?
5.46  A shipment of 40 television sets are being shipped to local 
merchants. Four of the television sets are red. Determine the prob-
ability if 4 televisions are delivered to a particular merchant,
a.	 all the sets are red.
b.	 none of the sets are red.
c.	 at least 1 set is red.
d.	 What would be the answers for (a) through (c) if 6 television 
sets are red?
5.47  As a quality control manager, you are responsible for 
checking the quality level of AC adapters for tablet PCs that 
your company manufactures. You must reject a shipment if you 
find 4 defective units. Suppose a shipment of 40 AC adapters has 
8 defective units and 32 nondefective units. If you sample 12 AC 
adapters, what’s the probability that
a.	 there will be no defective units in the shipment?
b.	 there will be at least 1 defective unit in the shipment?
c.	 there will be 4 defective units in the shipment?
d.	 the shipment will be accepted?
5.48  In Example 5.7 above, a financial analyst was facing the 
task of selecting mutual funds to purchase for a client’s portfolio. 
Suppose that the number of funds had been narrowed to 12 funds 
instead of the 10 funds (still with 6 growth funds) in Example 5.7. 
What is the probability that of the 4 funds selected,
a.	 exactly 1 is a growth fund?
b.	 at least 1 is a growth fund?
c.	 3 are growth fund?
d.	 Compare the result of (c) to the result of Example 5.7.
SELF 
Test 

	
References	
237
S u m m a r y
5.6  Using the Poisson Distribution to Approximate 
the Binomial Distribution
You can use the Poisson distribution to approximate the binomial distribution when n is large 
and p is very small. The approximation gets better as n gets larger and p gets smaller. Read 
more about how to use this approximation in the Section 5.6 online topic.
U s i n g  S tat i s t i c s
Events of Interest at Ricknel Home 
Centers, Revisited
In this chapter, you have studied the probability distribution 
for a discrete variable, the covariance and its application in 
finance, and three important discrete probability distribu-
tions: the binomial, Poisson, and hypergeometric distribu-
tions. In the next chapter, you will study several important 
continuous distributions, including the normal distribution.
To help decide which discrete probability distribution to 
use for a particular situation, you need to ask the following 
questions:
•  Is there a fixed number of observations, n, each of 
which is classified as an event of interest or not an 
event of interest? Is there an area of opportunity? If 
there is a fixed number of observations, n, each of 
which is classified as an event of interest or not an 
event of interest, you use the binomial or hypergeo-
metric distribution. If there is an area of opportunity, 
you use the Poisson distribution.
•  In deciding whether to use the binomial or hypergeo-
metric distribution, is the probability of an event of 
interest constant over all trials? If yes, you can use the 
binomial distribution. If no, you can use the hyper-
geometric distribution.
Refere n c e s
	 1.	Bernstein, P. L. Against the Gods: The Remarkable Story of 
Risk. New York: Wiley, 1996.
	 2.	Emery, D. R., J. D. Finnerty, and J. D. Stowe. Corporate 
­Financial Management, 3rd ed. Upper Saddle River, NJ: 
­Prentice Hall, 2007.
	 3.	Levine, D. M., P. Ramsey, and R. Smidt. Applied Statistics for 
Engineers and Scientists Using Microsoft Excel and Minitab. 
Upper Saddle River, NJ: Prentice Hall, 2001.
	 4.	Microsoft Excel 2013. Redmond, WA: Microsoft Corp., 2012.
	 5.	Minitab Release 16. State College, PA: Minitab, Inc., 2010.
	 6.	Taleb, N. The Black Swan, 2nd ed. New York: Random House, 
2010.
I
n the Ricknel Home Improvement scenario at the be-
ginning of this chapter, you were an accountant for the 
Ricknel Home Improvement Company. The company’s ac-
counting information system automatically reviews order 
forms from online customers for possible mistakes. Any 
questionable invoices are tagged and included in a daily ex-
ceptions report. Knowing that the probability that an order 
will be tagged is 0.10, you were able to use the binomial 
distribution to determine the chance of finding a certain 
number of tagged forms in a sample of size four. There was 
a 65.6% chance that none of the forms would be tagged, a 
29.2% chance that one would be tagged, and a 5.2% chance 
that two or more would be 
tagged. You were also able to 
determine that, on average, 
you would expect 0.4 form to be tagged, and the standard 
deviation of the number of tagged order forms would be 
0.6. Now that you have learned the mechanics of using the  
binomial distribution for a known probability of 0.10 and a 
sample size of four, you will be able to apply the same ap-
proach to any given probability and sample size. Thus, you 
will be able to make inferences about the online ordering 
process and, more importantly, evaluate any changes or pro-
posed changes to the process.
Sebastian Kaulitzki/Shutterstock

238	
Chapter 5  Discrete Probability Distributions
K e y  Eq u at i o n s
Expected Value, M, of a Discrete Variable
m = E1X2 = a
N
i = 1
xi P1X = xi2	
(5.1)
Variance of a Discrete Variable
s2 = a
N
i = 1
3xi - E1X242P1X = xi2	
(5.2)
Standard Deviation of a Discrete Variable
s = 2s2 = xB a
N
i = 1
xi3xi - E1X242P1X = xi2 	
(5.3)
Covariance
sXY = a
N
i = 1
3xi - E1X243yi - E1Y24 P1xi, yi2	
(5.4)
Expected Value of the Sum of Two Variables
E1X + Y2 = E1X2 + E1Y2	
(5.5)
Variance of the Sum of Two Variables
Var1X + Y2 = s2
X + Y = s2
X + s2
Y + 2sXY	
(5.6)
Standard Deviation of the Sum of Two Variables
sX + Y = 2s2
X + Y	
(5.7)
Portfolio Expected Return
E1P2 = wE1X2 + 11 - w2E1Y2	
(5.8)
Portfolio Risk
sp = 2w2s2
X + 11 - w22s2
Y + 2w11 - w2sXY	
(5.9)
Combinations
nCx =
n!
x!1n - x2!	
(5.10)
Binomial Distribution
P1X = x n, p2 =
n!
x!1n - x2! px11 - p2n - x	
(5.11)
Mean of the Binomial Distribution
m = E1X2 = np	
(5.12)
Standard Deviation of the Binomial Distribution
s = 2s2 = 2Var1X2 = 2np11 - p2	
(5.13)
Poisson Distribution
P1X = x l2 = e-llx
x! 	
(5.14)
Hypergeometric Distribution
P1X = x n, N, E2 =
aE
xb aN - E
n - xb
aN
nb
	
(5.15)
Mean of the Hypergeometric Distribution
m = E1X2 = nE
N 	
(5.16)
Standard Deviation of the Hypergeometric Distribution
s = B
nE1N - E2
N2
B
N - n
N - 1	
(5.17)
K e y  Term s
area of opportunity  230
binomial distribution  223
covariance of a probability distribution 
(sXY)  218
expected value  214
expected value of the sum of two 
­variables  219
finite population correction factor  235
hypergeometric distribution  234
mathematical model  223
Poisson distribution  230
portfolios  219
portfolio expected return  219
portfolio risk  219
probability distribution for a discrete 
­variable  214
probability distribution function  223
rule of combinations  224
standard deviation of a discrete 
­variable  215
standard deviation of the sum of two 
­variables  219
variance of a discrete variable  215
variance of the sum of two variables  219

	
Chapter Review Problems	
239
C h ec ki n g  Yo ur  U n de r s ta nding
5.49  What is the meaning of the expected value of a random 
­variable?
5.50  What are the four properties that must be present in order to 
use the binomial distribution?
5.51  What are the four properties that must be present in order to 
use the Poisson distribution?
5.52  When do you use the hypergeometric distribution instead of 
the binomial distribution?
C h a pte r  R e vi e w P r ob le ms
5.53  Darwin Head, a 35-year-old sawmill worker, won $1 mil-
lion and a Chevrolet Malibu Hybrid by scoring 15 goals within 
24 ­seconds at the Vancouver Canucks National Hockey League 
game (B. Ziemer, “Darwin Evolves into an Instant Millionaire,” 
­Vancouver Sun, February 28, 2008, p. 1). Head said he would use 
the money to pay off his mortgage and provide for his children, 
and he had no plans to quit his job. The contest was part of the 
Chevrolet Malibu Million Dollar Shootout, sponsored by General 
Motors Canadian Division. Did GM-Canada risk the $1 ­million? 
No! GM-Canada purchased event insurance from a company spe-
cializing in promotions at sporting events such as a half-court 
basketball shot or a hole-in-one giveaway at the local charity golf 
outing. The event insurance company estimates the probability of 
a contestant winning the contest, and for a modest charge, insures 
the event. The promoters pay the insurance premium but take on 
no added risk as the insurance company will make the large payout 
in the unlikely event that a contestant wins. To see how it works, 
suppose that the insurance company estimates that the probability 
a contestant would win a million-dollar shootout is 0.001 and that 
the insurance company charges $4,000.
a.	 Calculate the expected value of the profit made by the insur-
ance company.
b.	 Many call this kind of situation a win–win opportunity for the 
insurance company and the promoter. Do you agree? Explain.
5.54  Between 1896—when the Dow Jones index was ­created—
and 2012, the index rose in 65% of the years. (Sources: M. ­Hulbert, 
“What the Past Can’t Tell Investors,” The New York Times, January 
3, 2010, p. BU2 and bit.ly/100zwvT.) Based on this information, 
and assuming a binomial distribution, what do you think is the 
probability that the stock market will rise
a.	 next year?
b.	 the year after next?
c.	 in four of the next five years?
d.	 in none of the next five years?
e.	 For this situation, what assumption of the binomial distribution 
might not be valid?
5.55  Smartphone adoption among American teens has increased 
substantially, and mobile access to the Internet is pervasive. One 
in four teenagers are “cell mostly” Internet users—that is, they 
mostly go online using their phone and not using some other de-
vice such as a desktop or laptop computer. (Source: Teens and 
Technology 2013, Pew Research Center, bit.ly/101ciF1.)
If a sample of 10 American teens is selected, what is the prob-
ability that
a.	 4 are “cell mostly” Internet users?
b.	 at least 4 are “cell mostly” Internet users?
c.	 at most 8 are “cell mostly” Internet users?
d.	 If you selected the sample in a particular geographical area and 
found that none of the 10 respondents are “cell mostly” Inter-
net users, what conclusions might you reach about whether 
the percentage of “cell mostly” Internet users in this area was 
25%?
5.56  One theory concerning the Dow Jones Industrial Average is 
that it is likely to increase during U.S. presidential election years. 
From 1964 through 2012, the Dow Jones Industrial Average in-
creased in 10 of the 13 U.S. presidential election years. Assuming 
that this indicator is a random event with no predictive value, you 
would expect that the indicator would be correct 50% of the time.
a.	 What is the probability of the Dow Jones Industrial Average in-
creasing in 10 or more of the 13 U.S. presidential election years 
if the probability of an increase in the Dow Jones Industrial Av-
erage is 0.50?
b.	 What is the probability that the Dow Jones Industrial Average 
will increase in 10 or more of the 13 U.S. presidential election 
years if the probability of an increase in the Dow Jones Indus-
trial Average in any year is 0.75?
5.57  Medical billing errors and fraud are on the rise. According 
to Medical Billing Advocates of America, 8 out of 10 times, the 
medical bills that you get are not right. (Data extracted from “Ser-
vices Diagnose, Treat Medical Billing Errors,” USA Today, June 
20, 2012.) If a sample of 10 medical bills is selected, what is the 
probability that
a.	 0 medical bills will contain errors?
b.	 exactly 5 medical bills will contain errors?
c.	 more than 5 medical bills will contain errors?
d.	 What are the mean and standard deviation of the probability 
distribution?
5.58  Refer to Problem 5.57. Suppose that a quality improvement 
initiative has reduced the percentage of medical bills containing 
errors to 40%. If a sample of 10 medical bills is selected, what is 
the probability that
a.	 0 medical bills will contain errors?
b.	 exactly 5 medical bills will contain errors?
c.	 more than 5 medical bills contain errors?
d.	 What are the mean and standard deviation of the probability 
distribution?
e.	 Compare the results of (a) through (c) to those of Problem 5.57 
(a) through (c).
5.59  Social log-ins involve recommending or sharing an article 
that you read online. According to Janrain, in the first quarter of 
2013, 46% signed in via Facebook compared with 34% for Google. 

240	
Chapter 5  Discrete Probability Distributions
(Source: “Social Login Trends Across the Web for Q1 2013,” bit 
.ly/ZQCRSF.) If a sample of 10 social log-ins is selected, what is 
the probability that
a.	 more than 5 signed in using Facebook?
b.	 more than 5 signed in using Google?
c.	 none signed in using Facebook?
d.	 What assumptions did you have to make to answer (a) through (c)?
5.60  The Consumer Financial Protection Bureau’s consumer re-
sponse team hears directly from consumers about the challenges 
they face in the marketplace, brings their concerns to the attention 
of financial institutions, and assists in addressing their complaints. 
Consumer response accepts complaints related to mortgages, bank 
accounts and services, private student loans, other consumer loans, 
and credit reporting. Of the consumers who registered a bank ac-
count and service complaint, 41% cited “account management” 
as the type of complaint; these complaints are related to open-
ing, closing, or managing the account and address issues, such as 
confusing marketing, denial, fees, statements, and joint accounts. 
(Source: Consumer Response: A Snapshot of Complaints Re-
ceived, 1.usa.gov/WZ9N8Q.) Consider a sample of 20 consum-
ers who registered bank account and service complaints. Use the 
binomial model to answer the following questions:
a.	 What is the expected value, or mean, of the binomial distribution?
b.	 What is the standard deviation of the binomial distribution?
c.	 What is the probability that 10 of the 20 consumers cited 
­“account management” as the type of complaint?
d.	 What is the probability that no more than 5 of the consumers 
cited “account management” as the type of complaint?
e.	 What is the probability that 5 or more of the consumers cited 
“account management” as the type of complaint?
5.61  Refer to Problem 5.60. In the same time period, 27% of the 
consumers registering a bank account and service compliant cited 
“deposit and withdrawal” as the type of complaint; these are issues 
such as transaction holds and unauthorized transactions.
a.	 What is the expected value, or mean, of the binomial distribu-
tion?
b.	 What is the standard deviation of the binomial distribution?
c.	 What is the probability that none of the 20 consumers cited 
“deposit and withdrawal” as the type of complaint?
d.	 What is the probability that no more than 2 of the consumers 
cited “deposit and withdrawal” as the type of complaint?
e.	 What is the probability that 3 or more of the consumers cited 
“deposit and withdrawal” as the type of complaint?
5.62  One theory concerning the S&P 500 Index is that if it in-
creases during the first five trading days of the year, it is likely 
to increase during the entire year. From 1950 through 2012, the 
S&P 500 Index had these early gains in 40 years (in 2011 there 
was virtually no change). In 35 of these 40 years, the S&P 500 
Index increased for the entire year. Assuming that this indicator 
is a random event with no predictive value, you would expect that 
the indicator would be correct 50% of the time. What is the prob-
ability of the S&P 500 Index increasing in 35 or more years if the 
true probability of an increase in the S&P 500 Index is
a.	 0.50?
b.	 0.70?
c.	 0.90?
d.	 Based on the results of (a) through (c), what do you think is the 
probability that the S&P 500 Index will increase if there is an 
early gain in the first five trading days of the year? Explain.
5.63  Spurious correlation refers to the apparent relationship 
between variables that either have no true relationship or are re-
lated to other variables that have not been measured. One widely 
publicized stock market indicator in the United States that is an 
example of spurious correlation is the relationship between the 
winner of the National Football League Super Bowl and the per-
formance of the Dow Jones Industrial Average in that year. The 
“indicator” states that when a team that existed before the National 
Football League merged with the American Football League wins 
the Super Bowl, the Dow Jones Industrial Average will increase in 
that year. (Of course, any correlation between these is spurious as 
one thing has absolutely nothing to do with the other!) Since the 
first Super Bowl was held in 1967 through 2012, the indicator has 
been correct 37 out of 46 times. Assuming that this indicator is a 
random event with no predictive value, you would expect that the 
indicator would be correct 50% of the time.
a.	 What is the probability that the indicator would be correct 37 or 
more times in 46 years?
b.	 What does this tell you about the usefulness of this indicator?
5.64  The National Insurance Crime Bureau says that Miami-
Dade, Broward, and Palm Beach counties account for a substantial 
number of questionable insurance claims referred to investigators. 
(Source: “United Auto Courts Reports,” bit.ly/100DZi9.) Assume 
that the number of questionable insurance claims referred to inves-
tigators by Miami-Dade, Broward, and Palm Beach counties is dis-
tributed as a Poisson random variable with a mean of 10 per day.
a.	 What assumptions need to be made so that the number of ques-
tionable insurance claims referred to investigators by Miami-
Dade, Broward, and Palm Beach counties is distributed as a 
Poisson random variable?
Making the assumptions given in (a), what is the probability that
b.	 5 questionable insurance claims will be referred to investigators 
by Miami-Dade, Broward, and Palm Beach counties in a day?
c.	 10 or fewer questionable insurance claims will be referred to 
investigators by Miami-Dade, Broward, and Palm Beach coun-
ties in a day?
d.	 11 or more questionable insurance claims will be referred to 
investigators by Miami-Dade, Broward, and Palm Beach coun-
ties in a day?
5.65  In the Florida lottery Lotto game, you select six numbers 
from a pool of numbers from 1 to 53 (see flalottery.com). Each 
wager costs $1. You win the jackpot if you match all six numbers 
that you have selected.
Find the probability of
a.	 winning the jackpot.
b.	 matching five numbers.
c.	 matching four numbers.
d.	 matching three numbers.
e.	 matching two numbers.
f.	 matching one number.
g.	 matching none of the numbers.
h.	 If you match zero, one, or two numbers, you do not win any-
thing. What is the probability that you will not win anything?
i.	 The Lotto ticket gives complete game rules and probabilities of 
matching zero through six numbers. The lottery ticket has the 
saying “A Win for Education” on the back of the ticket. Do you 
think Florida’s slogan and the printed complete game rules and 
probabilities of matching zero through six numbers is an ethi-
cal approach to running the lottery game?

	
Cases for Chapter 5	
241
C a s e s  f o r  C h a p t e r  5
Managing Ashland MultiComm Services
The Ashland MultiComm Services (AMS) marketing de-
partment wants to increase subscriptions for its 3-For-All 
telephone, cable, and Internet combined service. AMS mar-
keting has been conducting an aggressive direct-marketing 
campaign that includes postal and electronic mailings and 
telephone solicitations. Feedback from these efforts indi-
cates that including premium channels in this combined 
service is a very important factor for both current and pro-
spective subscribers. After several brainstorming sessions, 
the marketing department has decided to add premium cable 
channels as a no-cost benefit of subscribing to the 3-For-All 
service.
The research director, Mona Fields, is planning to con-
duct a survey among prospective customers to determine 
how many premium channels need to be added to the 3-For-
All service in order to generate a subscription to the service. 
Based on past campaigns and on industry-wide data, she es-
timates the following:
Number of Free  
Premium Channels
Probability of  
Subscriptions
0
0.02
1
0.04
2
0.06
3
0.07
4
0.08
5
0.085
1.	 If a sample of 50 prospective customers is selected and no 
free premium channels are included in the 3-For-All service 
offer, given past results, what is the probability that
a.	fewer than 3 customers will subscribe to the 3-For-All 
service offer?
b.	0 customers or 1 customer will subscribe to the 3-For- 
All service offer?
c.	 more than 4 customers will subscribe to the 3-For-All 
service offer?
d.	Suppose that in the actual survey of 50 prospective 
customers, 4 customers subscribe to the 3-For-All ser-
vice offer. What does this tell you about the previous 
estimate of the proportion of customers who would 
subscribe to the 3-For-All service offer?
2.	Instead of offering no premium free channels as in Prob-
lem 1, suppose that two free premium channels are in-
cluded in the 3-For-All service offer. Given past results, 
what is the probability that
a.	fewer than 3 customers will subscribe to the 3-For-All 
service offer?
b.	0 customers or 1 customer will subscribe to the 3-For-
All service offer?
c.	 more than 4 customers will subscribe to the 3-For-All 
service offer?
d.	Compare the results of (a) through (c) to those of 1.
e.	 Suppose that in the actual survey of 50 prospective 
customers, 6 customers subscribe to the 3-For-All ser-
vice offer. What does this tell you about the previous 
estimate of the proportion of customers who would 
subscribe to the 3-For-All service offer?
f.	 What do the results in (e) tell you about the effect of 
offering free premium channels on the likelihood of 
obtaining subscriptions to the 3-For-All service?
3.	Suppose that additional surveys of 50 prospective cus-
tomers were conducted in which the number of free pre-
mium channels was varied. The results were as ­follows:
Number of Free  
Premium Channels
Number of  
Subscriptions
1
5
3
6
4
6
5
7
How many free premium channels should the research director 
recommend for inclusion in the 3-For-All service? Explain.

242	
Chapter 5  Discrete Probability Distributions
Digital Case
Apply your knowledge about expected value and the covari-
ance in this continuing Digital Case from Chapters 3 and 4.
Open BullsAndBears.pdf, a marketing brochure from En-
dRun Financial Services. Read the claims and examine the 
supporting data. Then answer the following:
1.	Are there any “catches” about the claims the brochure 
makes for the rate of return of Happy Bull and Worried 
Bear funds?
2.	What subjective data influence the rate-of-return analyses 
of these funds? Could EndRun be accused of making false 
and misleading statements? Why or why not?
3.	The expected-return analysis seems to show that the 
Worried Bear fund has a greater expected return than the 
Happy Bull fund. Should a rational investor never invest 
in the Happy Bull fund? Why or why not?

	
Chapter 5 EXCEL Guide	
243
EG5.1  The Probability Distribution  
for a Discrete Variable
Key Technique  Use the SUMPRODUCT(cell range 1, cell 
range 2) function (see Appendix F) to compute the expected value 
and variance.
Example  Compute the expected value, variance, and standard 
deviation for the number of interruptions per day data of Table 5.1 
on page 214.
In-Depth Excel  Use the Discrete Variable workbook as a 
model.
For the example, open to the DATA worksheet of the Dis-
crete Variable workbook. The worksheet already contains the 
­entries needed to compute the expected value, variance, and 
standard deviation (shown in the COMPUTE worksheet) for 
the example.
For other problems, modify the DATA worksheet. Enter the 
probability distribution data into columns A and B and, if neces-
sary, extend columns C through E, first selecting cell range C7:E7 
and then copying that cell range down as many rows as necessary. 
If the probability distribution has fewer than six outcomes, select 
the rows that contain the extra, unwanted outcomes, right-click, 
and then click Delete in the shortcut menu.
Read the short takes for Chapter 5 for an explanation of 
the formulas found in the worksheets.
EG5.2  Covariance of a Probability 
Distribution and its Application 
in Finance
Key Technique  Use the SQRT and SUMPRODUCT func-
tions (see Appendix Section F) to help compute the portfolio anal-
ysis statistics.
Example  Perform the portfolio analysis for the Section 5.2  
investment example.
PHStat  Use Covariance and Portfolio Analysis.
For the example, select PHStat ➔ Decision-Making ➔ Cova-
riance and Portfolio Analysis. In the procedure’s dialog box 
(shown below):
	 1.	 Enter 6 as the Number of Outcomes.
	 2.	 Enter a Title, check Portfolio Management Analysis, and 
click OK.
In the new worksheet (shown below):
	 1.	 Enter the probabilities and outcomes in the table that begins in 
cell B3.
	 2.	 Enter 0.5 as the Weight assigned to X.
In-Depth Excel  Use the COMPUTE worksheet of the Portfo-
lio workbook as a template.
The worksheet (shown below) already contains the data for the ex-
ample. Overwrite the X and P1X2 values and the weight assigned to 
the X value when you enter data for other problems. If a problem has 
more or fewer than three outcomes, first select row 5, right-click, and 
click Insert (or Delete) in the shortcut menu to insert (or delete) rows 
one at a time. If you insert rows, select the cell range B4:J4 and copy 
the contents of this range down through the new table rows.
The worksheet also contains a Calculations Area that contains 
various intermediate calculations. Open the COMPUTE_FOR-
MULAS worksheet to examine all the formulas used in this area.
EG5.3  Binomial Distribution
Key Technique  Use the BINOM.DIST(number of events of 
interest, sample size, probability of an event of interest, FALSE) 
function.
Example  Compute the binomial probabilities for n = 4 and 
p = 0.1, as is done in Figure 5.2 on page 227.
PHStat  Use Binomial.
For the example, select PHStat ➔ Probability & Prob. Distributions 
➔ Binomial. In the procedure’s dialog box (shown on page 244):
	 1.	 Enter 4 as the Sample Size.
	 2.	 Enter 0.1 as the Prob. of an Event of Interest.
	 3.	 Enter 0 as the Outcomes From value and enter 4 as the (Out-
comes) To value.
	 4.	 Enter a Title, check Histogram, and click OK.
C h a p t e r  5  E x c e l  G u i d e

244	
Chapter 5  Discrete Probability Distributions
Check Cumulative Probabilities before clicking OK in step 4 to 
have the procedure include columns for P1…X2, P16X2, P17X2, 
and P1ÚX2 in the binomial probabilities table.
In-Depth Excel  Use the Binomial workbook as a template and 
model.
For the example, open to the COMPUTE worksheet of the Bino-
mial workbook, shown in Figure 5.2 on page 227. The worksheet 
already contains the entries needed for the example. For other 
problems, change the sample size in cell B4 and the probability 
of an event of interest in cell B5. If necessary, extend the binomial 
probabilities table by first selecting cell range A18:B18 and then 
copying that cell range down as many rows as necessary. To con-
struct a histogram of the probability distribution, use the Appendix 
Section B.9 instructions.
Read the Short Takes for Chapter 5 for an explanation of 
the CUMULATIVE worksheet, which computes cumulative prob-
abilities, and the worksheets to use with versions older than Excel 
2010.
EG5.4  Poisson Distribution
Key Technique  Use the POISSON.DIST(number of events of 
interest, the average or expected number of events of interest, 
FALSE) function.
Example  Compute the Poisson probabilities for the customer 
arrival problem in which l = 3, as is done in Figure 5.4 on  
page 232.
PHStat  Use Poisson.
For the example, select PHStat ➔ Probability & Prob. 
­Distributions ➔ Poisson. In this procedure’s dialog box (shown 
below):
	 1.	 Enter 3 as the Mean/Expected No. of Events of Interest.
	 2.	 Enter a Title and click OK.
Check Cumulative Probabilities before clicking OK in step 2 to 
have the procedure include columns for P1…X2, P16X2, P17X2, 
and P1ÚX2 in the Poisson probabilities table. Check Histogram 
to construct a histogram of the Poisson probability distribution.
In-Depth Excel  Use the Poisson workbook as a template.
For the example, open to the COMPUTE worksheet of the 
­Poisson workbook, shown in Figure 5.4 on page 232. The 
­worksheet already contains the entries for the example. For 
other problems, change the mean or expected number of events 
of ­interest in cell E4. To construct a histogram of the probability 
­distribution, use the Appendix Section B.9 instructions.
Read the Short Takes for Chapter 5 for an explanation of the 
CUMULATIVE worksheet, which computes cumulative probabili-
ties, and the worksheets to use with versions older than Excel 2010.
EG5.5  Hypgeometric Distribution
Key Technique  Use the HYPGEOM.DIST(X, sample size, 
number of events of interest in the population, population size, 
FALSE) function.
Example  Compute the hypergeometric probabilities for the 
team formation problem as is done in Figure 5.5 on page 235.
PHStat  Use Hypergeometric.
For the example, select PHStat ➔ Probability & Prob. Distribu-
tions ➔ Hypergeometric. In this procedure’s dialog box (shown 
below):
	 1.	 Enter 8 as the Sample Size.
	 2.	 Enter 10 as the No. of Events of Interest in Pop.
	 3.	 Enter 30 as the Population Size.
	 4.	 Enter a Title and click OK.
Check Histogram to produce a histogram of the probability 
­distribution.
In-Depth Excel  Use the Hypergeometric workbook as a tem-
plate.
For the example, open to the COMPUTE worksheet of the 
­Hypergeometric workbook, shown in Figure 5.5 on page 235. 
The worksheet already contains the entries for the example. For 
other problems, change the sample size in cell B4, the number of 
events of interest in the population in cell B5, and the population 
size in cell B6. To construct a histogram of the probability distri-
bution, use the Appendix Section B.9 instructions.
Read the Short Takes for Chapter 5 an explanation of the 
CUMULATIVE worksheet, which computes cumulative probabili-
ties, and the worksheets to use with versions older than Excel 2010.

	
Chapter 5 Minitab Guide	
245
MG5.1  The Probability Distribution 
for a Discrete Variable
Expected Value of a Discrete Variable
Use Calculator to compute the expected value of a discrete 
­random variable.
For example, to compute the expected value for the number of 
­interruptions per day, open to the Table_5.1 worksheet. Select 
Calc ➔ Calculator. In the Calculator dialog box (shown below):
	 1.	 Enter C3 in the Store result in variable box and then press 
Tab. (C3 is the first empty column on the worksheet.)
	 2.	 Double-click C1  X in the variables list to add X to the Ex-
pression box.
	 3.	 Click * on the simulated keypad to add * to the Expression 
box.
	 4.	 Double-click C2  P(X) in the variables list to form the 
­expression X * 'P(X)' in the Expression box.
	 5.	 Check Assign as a formula.
	 6.	 Click OK.
	 7.	 Enter X*P(X) as the name for column C3.
	 8.	 Reselect Calc ➔ Calculator.
In the Calculator dialog box:
	 9.	 Enter C4 in the Store result in variable box and then press 
Tab. (C4 is the first empty column on the worksheet.)
	10.	 Enter SUM(C3) in the Expression box.
	11.	 If necessary, clear Assign as a formula.
	12.	 Click OK.
MG5.2  Covariance and its  
Application in Finance
There are no Minitab instructions for this section.
MG5.3  Binomial Distribution
Use Binomial to compute binomial probabilities.
For example, to compute these probabilities for the Section 5.3 
tagged orders example on page 224, open to a new, blank work-
sheet and:
	 1.	 Enter X as the name of column C1.
	 2.	 Enter the values 0 through 4 in column C1, starting with  
row 1.
	 3.	 Enter P(X) as the name of column C2.
	 4.	 Select Calc ➔ Probability Distributions ➔ Binomial.
In the Binomial Distribution dialog box (shown below):
	 5.	 Click Probability (to compute the probabilities of exactly X 
events of interest for all values of X).
	 6.	 Enter 4 (the sample size) in the Number of trials box.
	 7.	 Enter 0.1 in the Event probability box.
	 8.	 Click Input column, enter C1 in its box, and press Tab.
	 9.	 Enter C2 in the first Optional storage box.
	10.	 Click OK.
Skip step 9 to create the results shown in Figure 5.2 on page 227.
MG5.4  Poisson Distribution
Use Poisson to compute Poisson probabilities.
For example, to compute these probabilities for the Section 5.4 
bank customer arrivals example on page 231, open to a new, blank 
worksheet and:
	 1.	 Enter X as the name of column C1.
	 2.	 Enter the values 0 through 15 in column C1, starting with row 1.
	 3.	 Enter P(X) as the name of column C2.
	 4.	 Select Calc ➔ Probability Distributions ➔ Poisson.
C h a p t e r  5  M i n i ta b  G u i d e

246	
Chapter 5  Discrete Probability Distributions
In the Poisson Distribution dialog box (shown below):
	 1.	 Click Probability (to compute the probabilities of exactly X 
events of interest for all values of X).
	 2.	 Enter 3 in the Mean box.
	 3.	 Click Input column, enter C1 in its box, and press Tab.
	 4.	 Enter C2 in the first Optional storage box.
	 5.	 Click OK.
Skip step 8 to create the results shown in Figure 5.4 on page 232.
MG5.5  Hypergeometric Distribution
Use Hypergeometric to compute hypergeometric probabilities. 
For example, to compute these probabilities for the Section 5.5 
team-formation example on page 235, open to a new, blank work-
sheet and:
	 1.	 Enter X as the name of column C1.
	 2.	 Enter the values 0 through 8 in column C1, starting with row 1.
	 3.	 Enter P(X) as the name of column C2.
	 4.	 Select Calc ➔ Probability Distributions ➔ Hypergeometric.
In the Hypergeometric Distribution dialog box (shown below):
	 1.	 Click Probability.
	 2.	 Enter 30 in the Population size (N) box.
	 3.	 Enter 10 in the Event count in population (M) box.
	 4.	 Enter 8 in the Sample size (n) box.
	 5.	 Click Input column, enter C1 in its box, and press Tab.
	 6.	 Enter C2 in the first Optional storage box.
	 7.	 Click OK.
Skip step 6 to create the results shown in Figure 5.5 on page 235.

247
U s i n g  S tat i s t i c s
Normal Downloading at MyTVLab
You are a project manager for the MyTVLab website, an online service that 
streams movies and episodes from broadcast and cable TV series and that al-
lows users to upload and share original videos. To attract and retain visitors to 
the website, you need to ensure that users can quickly download the exclusive-
content daily videos.
 To check how fast a video downloads, you open a web browser on a com-
puter at the corporate offices of MyTVLab, load the MyTVLab home page, down-
load the first website-exclusive video, and measure the download time. Download 
time—the amount of time in seconds, that passes from first clicking a download 
link until the video is ready to play—is a function of both the streaming media 
technology used and the number of simultaneous users of the website. Past data 
indicate that the mean download time is 7 seconds and that the standard deviation 
is 2 seconds. Approximately two-thirds of the download times are between 5 and 
9 seconds, and about 95% of the download times are between 3 and 11 seconds. 
In other words, the download times are distributed as a bell-shaped curve, with a 
clustering around the mean of 7 seconds. How could you use this information to 
answer questions about the download times of the first video?
Contents
6.1	 Continuous Probability 
Distributions
6.2	 The Normal Distribution
Visual Explorations: 
Exploring the Normal Distribution
think about this: What is 
Normal?
6.3	 Evaluating Normality
6.4	 The Uniform Distribution
6.5	 The Exponential Distribution
6.6	 The Normal Approximation 
to the Binomial Distribution 
(online)
Using Statistics: Normal 
Downloading at MyTVLab, 
Revisited
Chapter 6 Excel Guide
Chapter 6 minitab guide
Objectives
To compute probabilities from the 
normal distribution
To use the normal distribution to 
solve business problems
To use the normal probability plot 
to determine whether a set of 
data is approximately normally 
distributed
To compute probabilities from the 
uniform distribution
To compute probabilities from the 
exponential distribution
Chapter
The Normal Distribution 
and Other Continuous 
Distributions
6
Cloki/Shutterstock

248	
Chapter 6  The Normal Distribution and Other Continuous Distributions
I
n Chapter 5, accounting managers at Ricknel Home Centers wanted to be able to an-
swer questions about the number of tagged items in a given sample size. As a MyTVLab 
project manager, you face a different task—one that involves a continuous measurement 
­because a download time could be any value and not just a whole number. How can you an-
swer questions, such as the following, about this continuous numerical variable:
 • What proportion of the video downloads take more than 9 seconds?
 • How many seconds elapse before 10% of the downloads are complete?
 • How many seconds elapse before 99% of the downloads are complete?
 • How would enhancing the streaming media technology used affect the answers to these 
questions?
As in Chapter 5, you can use a probability distribution as a model. Reading this chapter 
will help you learn about characteristics of continuous probability distributions and how to use 
the normal distribution to solve business problems.
F i g u r e  6 . 1
Three continuous 
probability distributions
Values of X
Panel A
Normal Distribution
Panel B
Uniform Distribution
Panel C
Exponential Distribution
Values of X
Values of X
6.1  Continuous Probability Distributions
A probability density function is a mathematical expression that defines the distribution of the 
values for a continuous variable. Figure 6.1 graphically displays three probability density functions.
Panel A depicts a normal distribution. The normal distribution is symmetrical and bell-
shaped, implying that most observed values tend to cluster around the mean, which, due to 
the distribution’s symmetrical shape, is equal to the median. Although the values in a normal 
distribution can range from negative infinity to positive infinity, the shape of the distribution 
makes it very unlikely that extremely large or extremely small values will occur.
 Panel B shows a uniform distribution where the values are equally distributed in the range 
between the smallest value and the largest value. Sometimes referred to as the rectangular 
­distribution, the uniform distribution is symmetrical, and therefore the mean equals the median.
 Panel C illustrates an exponential distribution. This distribution is skewed to the right, 
­making the mean larger than the median. The range for an exponential distribution is zero to pos-
itive infinity, but the distribution’s shape makes it unlikely that extremely large values will occur.
6.2  The Normal Distribution
The normal distribution (also known as the Gaussian distribution) is the most common con-
tinuous distribution used in statistics. The normal distribution is vitally important in statistics 
for three main reasons:
 • Numerous continuous variables common in business have distributions that closely 
resemble the normal distribution.
 • The normal distribution can be used to approximate various discrete probability distributions.
 • The normal distribution provides the basis for classical statistical inference because of 
its relationship to the Central Limit Theorem (which is discussed in Section 7.2).
 The normal distribution is represented by the classic bell shape shown in Panel A of 
­Figure 6.1. In the normal distribution, you can calculate the probability that values occur within 
certain ranges or intervals. However, because probability for continuous variables is measured 

	
6.2  The Normal Distribution	
249
as an area under the curve, the probability of a particular value from a continuous distribution 
such as the normal distribution is zero. As an example, time (in seconds) is measured and not 
counted. Therefore, you can determine the probability that the download time for a video on a 
web browser is between 7 and 10 seconds, or the probability that the download time is between 
8 and 9 seconds, or the probability that the download time is between 7.99 and 8.01 seconds. 
However, the probability that the download time is exactly 8 seconds is zero.
The normal distribution has several important theoretical properties:
 • It is symmetrical, and its mean and median are therefore equal.
 • It is bell-shaped in appearance.
 • Its interquartile range is equal to 1.33 standard deviations. Thus, the middle 50% of the 
values are contained within an interval of two-thirds of a standard deviation below 
the mean and two-thirds of a standard deviation above the mean.
 • It has an infinite range 1-∞6 X 6 ∞2.
In practice, many variables have distributions that closely resemble the theoretical prop-
erties of the normal distribution. The data in Table 6.1 represent the amount of soft drink in 
10,000 1-liter bottles filled on a recent day. The continuous variable of interest, the amount 
of soft drink filled, can be approximated by the normal distribution. The measurements of the 
amount of soft drink in the 10,000 bottles cluster in the interval 1.05 to 1.055 liters and distrib-
ute ­symmetrically around that grouping, forming a bell-shaped pattern.
T a b l e  6 . 1
Amount of Fill in 
10,000 Bottles of a 
Soft Drink
Amount of Fill (liters)
Relative Frequency
6 1.025
48>10,000 = 0.0048
1.025 6 1.030
122>10,000 = 0.0122
1.030 6 1.035
325>10,000 = 0.0325
1.035 6 1.040
695>10,000 = 0.0695
1.040 6 1.045
1,198>10,000 = 0.1198
1.045 6 1.050
1,664>10,000 = 0.1664
1.050 6 1.055
1,896>10,000 = 0.1896
1.055 6 1.060
1,664>10,000 = 0.1664
1.060 6 1.065
1,198>10,000 = 0.1198
1.065 6 1.070
695>10,000 = 0.0695
1.070 6 1.075
325>10,000 = 0.0325
1.075 6 1.080
122>10,000 = 0.0122
1.080 or above
48>10,000 =  0.0048 
Total
1.0000
Figure 6.2 shows the relative frequency histogram and polygon for the distribution of the 
amount filled in 10,000 bottles.
F i g u r e  6 . 2
Relative frequency 
histogram and polygon of 
the amount filled in 10,000 
bottles of a soft drink
Source: Data are taken from 
Table 6.1.
.20
1.025
1.035
1.045
1.055
Amount of Fill (liters)
Probability of X
1.065
1.075
1.03
1.04
1.05
1.06
1.07
1.08
.15
.10
.05
0

250	
Chapter 6  The Normal Distribution and Other Continuous Distributions
 For these data, the first three theoretical properties of the normal distribution are approxi-
mately satisfied. However, the fourth one, having an infinite range, is not. The amount filled in 
a bottle cannot possibly be zero or below, nor can a bottle be filled beyond its capacity. From 
Table 6.1, you see that only 48 out of every 10,000 bottles filled are expected to contain 1.08 
liters or more, and an equal number are expected to contain less than 1.025 liters.
 The symbol f1X2 is used to represent a probability density function. The probability 
density function for the normal distribution is given in Equation (6.1).
Normal Probability Density Function
	
f  1X2 =
1
22ps
e-11>2231X - m2>s42
 	
(6.1)
	
where
 e = mathematical constant approximated by 2.71828
 p = mathematical constant approximated by 3.14159
 m = mean
 s = standard deviation
 X = any value of the continuous variable, where - ∞6 X 6 ∞
 Although Equation (6.1) may look complicated, the probabilities of the random ­variable 
X are dependent only on the mean, m, and the standard deviation, s, the two ­parameters of the 
normal distribution, because e and p are mathematical constants. There is a different normal 
distribution for each combination of the mean m and the standard deviation s. Figure 6.3 il-
lustrates this principle. The distributions labeled A and B have the same mean 1m2 but have 
different standard deviations. Distributions A and C have the same standard deviation 1s2 but 
have different means. Distributions B and C have different values for both m and s.
Student Tip
There is a different 
normal distribution for 
each combination of 
the mean, m, and the 
standard deviation, s.
F i g u r e  6 . 3
Three normal 
distributions
A
B
C
Computing Normal Probabilities
To compute normal probabilities, you first convert a normally distributed variable, X, to a 
­standardized normal variable, Z, using the transformation formula, shown in ­Equation 
(6.2). Applying this formula allows you to look up values in a normal ­probability table and 
avoid the tedious and complex computations that Equation (6.1) would ­otherwise require.
Z Transformation Formula
The Z value is equal to the difference between X and the mean, m, divided by the standard 
deviation, s.
	
Z = X - m
s
	
(6.2)

	
6.2  The Normal Distribution	
251
 The transformation formula computes a Z value that expresses the difference of the X value 
from the mean, m, in standard deviation units (see Section 3.2 on page 141) called standardized 
units. While a variable, X, has mean, m, and standard deviation, s, the standardized variable, Z, 
always has mean m = 0 and standard deviation s = 1.
 Then you can determine the probabilities by using Table E.2, the cumulative stan-
dardized normal distribution. For example, recall from the Using Statistics scenario on  
page 247 that past data indicate that the time to download a video is normally distributed, with 
a mean m = 7 seconds and a standard deviation s = 2 seconds. From Figure 6.4, you see that  
every measurement X has a corresponding standardized measurement Z, computed from 
Equation (6.2), the transformation formula.
F i g u r e  6 . 4
Transformation of scales
MyTVLab
Video Download Time
μ – 3σ
μ – 2σ
μ – 1σ
μ
μ + 1σ
μ + 2σ
μ + 3σ
1
3
5
7
9
11
13
X Scale (   = 7,     = 2) 
(   = 0,     = 1) 
–3
–2
–1
0
+1
+2
+3
Z Scale
Therefore, a download time of 9 seconds is equivalent to 1 standardized unit (1 standard 
deviation) above the mean because
Z = 9 - 7
2
=  +1
A download time of 1 second is equivalent to -3 standardized units (3 standard deviations) 
below the mean because
Z = 1 - 7
2
= -3
In Figure 6.4, the standard deviation is the unit of measurement. In other words, a time of 
­9 seconds is 2 seconds (1 standard deviation) higher, or slower, than the mean time of 7 
­seconds. Similarly, a time of 1 second is 6 seconds (3 standard deviations) lower, or faster, 
than the mean time.
 To further illustrate the transformation formula, suppose that another website has a down-
load time for a video that is normally distributed, with a mean m = 4 seconds and a standard 
deviation s = 1 second. Figure 6.5 on page 252 shows this distribution.
Comparing these results with those of the MyTVLab website, you see that a download 
time of 5 seconds is 1 standard deviation above the mean download time because
Z = 5 - 4
1
= +1

252	
Chapter 6  The Normal Distribution and Other Continuous Distributions
A time of 1 second is 3 standard deviations below the mean download time because
Z = 1 - 4
1
= -3
With the Z value computed, you look up the normal probability using a table of values from 
the cumulative standardized normal distribution, such as Table E.2 in Appendix E. Suppose you 
wanted to find the probability that the download time for the MyTVLab website is less than 
9 seconds. Recall from page 251 that transforming X = 9 to standardized Z units, given a mean 
m = 7 seconds and a standard deviation s = 2 seconds, leads to a Z value of +1.00.
With this value, you use Table E.2 to find the cumulative area under the normal curve 
less than (to the left of) Z = +1.00. To read the probability or area under the curve less than 
Z = +1.00, you scan down the Z column in Table E.2 until you locate the Z value of interest 
(in 10ths) in the Z row for 1.0. Next, you read across this row until you intersect the column 
that contains the 100ths place of the Z value. Therefore, in the body of the table, the probability 
for Z = 1.00 corresponds to the intersection of the row Z = 1.0 with the column Z = .00. 
Table 6.2, which reproduces a portion of Table E.2, shows this intersection. The probability 
F i g u r e  6 . 5
A different transformation 
of scales
Video Download Time
for Another Website
1
2
3
4
5
6
7
–3
–2
–1
0
+1
+2
+3
X Scale (   = 4,     = 1) 
(   = 0,     = 1) 
Z Scale
Student Tip
Remember that when 
dealing with a continuous 
distribution such as the 
normal, the word area 
has the same meaning 
as probability.
T a b l e  6 . 2
Finding a Cumulative 
Area Under the 
Normal Curve
Cumulative Probabilities
Z
.00
.01
.02
.03
.04
.05
.06
.07
.08
.09
0.0
.5000
.5040
.5080
.5120
.5160
.5199
.5239
.5279
.5319
.5359
0.1
.5398
.5438
.5478
.5517
.5557
.5596
.5636
.5675
.5714
.5753
0.2
.5793
.5832
.5871
.5910
.5948
.5987
.6026
.6064
.6103
.6141
0.3
.6179
.6217
.6255
.6293
.6331
.6368
.6406
.6443
.6480
.6517
0.4
.6554
.6591
.6628
.6664
.6700
.6736
.6772
.6808
.6844
.6879
0.5
.6915
.6950
.6985
.7019
.7054
.7088
.7123
.7157
.7190
.7224
0.6
.7257
.7291
.7324
.7357
.7389
.7422
.7454
.7486
.7518
.7549
0.7
.7580
.7612
.7642
.7673
.7704
.7734
.7764
.7794
.7823
.7852
0.8
.7881
.7910
.7939
.7967
.7995
.8023
.8051
.8078
.8106
.8133
0.9
.8159
.8186
.8212
.8238
.8264
.8289
.8315
.8340
.8365
.8389
1.0
.8413
.8438
.8461
.8485
.8508
.8531
.8554
.8577
.8599
.8621
Source: Extracted from Table E.2.

	
6.2  The Normal Distribution	
253
 However, for the other website, you see that a time of 5 seconds is 1 standardized unit above 
the mean time of 4 seconds. Thus, the probability that the download time will be less than 5 seconds 
is also 0.8413. Figure 6.7 shows that regardless of the value of the mean, m, and standard deviation, 
s, of a normally distributed variable, Equation (6.2) can transform the X value to a Z value.
F i g u r e  6 . 6
Determining the area 
less than Z from a 
cumulative standardized 
normal distribution
X Scale
Z Scale
Area = 0.8413
1
3
5
7
9
11
13
–3.00
–2.00
–1.00
0
+1.00
+2.00
+3.00
MyTVLab Video
Download Time
Student Tip
You will find it very 
helpful when computing 
probabilities under the 
normal curve if you 
draw a normal curve 
and then enter the 
values for the mean and 
X below the curve and 
shade the desired area 
to be determined under 
the curve.
F i g u r e  6 . 7
Demonstrating a 
transformation of scales 
for corresponding 
cumulative portions 
under two normal curves
MyTVLab Website
3 4 5
7
9
11
+3
+2
+1
0
–1
–2
–3
13
X Scale
Z Scale
Another Website
Now that you have learned to use Table E.2 with Equation (6.2), you can answer many 
questions related to the MyTVLab video download, using the normal distribution.
Example 6.1
Finding P1X 7 92
What is the probability that the video download time for the MyTVLab website will be more 
than 9 seconds?
Solution  The probability that the download time will be less than 9 seconds is 0.8413 (see 
Figure 6.6 above). Thus, the probability that the download time will be more than 9 seconds is the 
complement of less than 9 seconds, 1 - 0.8413 = 0.1587. Figure 6.8 illustrates this result.
listed at the intersection is 0.8413, which means that there is an 84.13% chance that the down-
load time will be less than 9 seconds. Figure 6.6 graphically shows this probability.
F i g u r e  6 . 8
Finding P1X 7 92
X Scale
Z Scale
Area = 0.1587
1
3
5
7
9
11
13
–3.00
–2.00
–1.00
0
+1.00
+2.00
+3.00
0.8413
MyTVLab Video
Download Time

254	
Chapter 6  The Normal Distribution and Other Continuous Distributions
Example 6.2
Finding P1X 6 7
or X 7 92
What is the probability that the video download time for the MyTVLab website will be less 
than 7 seconds or more than 9 seconds?
Solution  To find this probability, you separately calculate the probability of a download 
time less than 7 seconds and the probability of a download time greater than 9 seconds and 
then add these two probabilities together. Figure 6.9 illustrates this result.
Because the mean is 7 seconds, and because the mean is equal to the median in a nor-
mal distribution, 50% of download times are under 7 seconds. From Example 6.1, you know 
that the probability that the download time is greater than 9 seconds is 0.1587. Therefore, 
the probability that a download time is under 7 or over 9 seconds, P1X 6 7 or X 7 92, is 
0.5000 + 0.1587 = 0.6587.
F i g u r e  6 . 9
Finding P1X 6 7
or X 7 92
X Scale
Z Scale
MyTVLab Video
Download Time
Area 0.5000
1
3
5
7
9
11
13
–3.00
–2.00
–1.00
0
+1.00
+2.00
+3.00
Area 0.1587
Area = 0.3413 because
0.8413 – 0.5000 = 0.3413
Example 6.3
Finding  
P15 6 X 6 92
What is the probability that video download time for the MyTVLab website will be between  
5 and 9 seconds—that is, P15 6 X 6 92?
Solution  In Figure 6.10, you can see that the area of interest is located between two 
values, 5 and 9.
F i g u r e  6 . 1 0
Finding P15 6 X 6 92
X Scale
Z Scale
Cumulative area = 0.8413 because
Area shaded dark blue
is 0.8413 – 0.1587 = 0.6826
1
3
5
7
9
11
13
Z =
= +1.00
–3.00
–2.00
–1.00
0
+1.00
+2.00
+3.00
Area = 0.1587 because
Z =
= –1.00
X – 
X – 
In Example 6.1 on page 253, you already found that the area under the normal curve less 
than 9 seconds is 0.8413. To find the area under the normal curve less than 5 seconds,
Z = 5 - 7
2
= -1.00
Using Table E.2, you look up Z = -1.00 and find 0.1587. Therefore, the probability that the 
download time will be between 5 and 9 seconds is 0.8413 - 0.1587 = 0.6826, as displayed 
in Figure 6.10.

	
6.2  The Normal Distribution	
255
The result of Example 6.3 enables you to state that for any normal distribution, 68.26% of 
the values are within {1 standard deviation of the mean. From Figure 6.11, you can see that 
95.44% of the values are within {2 standard deviations of the mean. Thus, 95.44% of the 
download times are between 3 and 11 seconds. From Figure 6.12, you can see that 99.73% of 
the values are within {3 standard deviations above or below the mean.
F i g u r e  6 . 1 1
Finding P13 6 X 6 112
X Scale
Z Scale
Area below is 0.9772 because
1
3
5
7
9
11
13
Z =
= +2.00
–3.00
–2.00
–1.00
0
+1.00
+2.00
+3.00
Area below is 0.0228 because
Z =
= –2.00
X – 
X – 
F i g u r e  6 . 1 2
Finding P11 6 X 6 132
X Scale
Z Scale
Area below is 0.99865 because
1
3
5
7
9
11
13
Z =
= +3.00
–3.00
–2.00
–1.00
0
+1.00
+2.00
+3.00
Area below is 0.00135 because
Z =
= –3.00
X – 
X – 
Thus, 99.73% of the download times are between 1 and 13 seconds. Therefore, it is ­unlikely 
(0.0027, or only 27 in 10,000) that a download time will be so fast or so slow that it will take 
under 1 second or more than 13 seconds. In general, you can use 6s (i.e., 3 standard ­deviations 
below the mean to 3 standard deviations above the mean) as a practical approximation of the 
range for normally distributed data.
 Figures 6.10, 6.11, and 6.12 illustrate that for any normal distribution,
 • Approximately 68.26% of the values fall within {1 standard deviation of the mean
 • Approximately 95.44% of the values fall within {2 standard deviations of the mean
 • Approximately 99.73% of the values fall within {3 standard deviations of the mean
This result is the justification for the empirical rule presented on page 157. The accuracy of the 
empirical rule improves as a data set follows the normal distribution more closely.
Finding X Values
Examples 6.1 through 6.3 require you to use the normal distribution Table E.2 to find an area 
under the normal curve that corresponds to a specific X value. For other situations, you may 
need to do the reverse: Find the X value that corresponds to a specific area. In general, you use 
Equation (6.3) for finding an X value.
Finding an X Value Associated With a Known Probability
The X value is equal to the mean, m, plus the product of the Z value and the standard 
deviation, s.
	
X = m + Zs	
(6.3)

256	
Chapter 6  The Normal Distribution and Other Continuous Distributions
To find a particular value associated with a known probability, follow these steps:
 • Sketch the normal curve and then place the values for the mean and X on the X and Z scales.
 • Find the cumulative area less than X.
 • Shade the area of interest.
 • Using Table E.2, determine the Z value corresponding to the area under the normal curve 
less than X.
 • Using Equation (6.3), solve for X:
X = m + Zs
Examples 6.4 and 6.5 illustrate this technique.
Example 6.4
Finding the X Value 
for a Cumulative 
Probability of 0.10
How much time (in seconds) will elapse before the fastest 10% of the downloads of a MyTVLab 
video are complete?
Solution  Because 10% of the videos are expected to download in under X seconds, the 
area under the normal curve less than this value is 0.1000. Using the body of Table E.2, you 
search for the area or probability of 0.1000. The closest result is 0.1003, as shown in Table 6.3 
(which is extracted from Table E.2).
T a b l e  6 . 3
Finding a Z Value 
Corresponding to a 
Particular Cumulative 
Area (0.10) Under the 
Normal Curve
Cumulative Probabilities
Z
.00
.01
.02
.03
.04
.05
.06
.07
.08
.09
f
f
f
f
f
f
f
f
f
f
f
-1.5
.0668
.0655
.0643
.0630
.0618
.0606
.0594
.0582
.0571
.0559
-1.4
.0808
.0793
.0778
.0764
.0749
.0735
.0721
.0708
.0694
.0681
-1.3
.0968
.0951
.0934
.0918
.0901
.0885
.0869
.0853
.0838
.0823
-1.2
.1151
.1131
.1112
.1093
.1075
.0156
.0138
.1020
.1003
.0985
Source: Extracted from Table E.2.
Working from this area to the margins of the table, you find that the Z value corresponding 
to the particular Z row 1-1.22 and Z column (.08) is -1.28 (see Figure 6.13).
F i g u r e  6 . 1 3
Finding Z to determine X
X Scale
Z Scale
X 
7
–1.28
0
Area is 0.9000
Area is 0.1000
Once you find Z, you use Equation (6.3) on page 255 to determine the X value. 
Substituting m = 7, s = 2, and Z = -1.28,
X = m + Zs
X = 7 + 1-1.282122 = 4.44 seconds
 Thus, 10% of the download times are 4.44 seconds or less.

	
6.2  The Normal Distribution	
257
Example 6.5
Finding the X  
Values That Include 
95% of the  
Download Times
What are the lower and upper values of X, symmetrically distributed around the mean, that 
include 95% of the download times for a video at the MyTVLab website?
Solution  First, you need to find the lower value of X (called XL). Then, you find the upper 
value of X (called XU). Because 95% of the values are between XL and XU, and because XL and 
XU are equally distant from the mean, 2.5% of the values are below XL (see Figure 6.14).
F i g u r e  6 . 1 4
Finding Z to determine XL
X Scale
Z Scale
XL
Area is 0.9750
7
0
–1.96
Area is 0.0250
Although XL is not known, you can find the corresponding Z value because the area under 
the normal curve less than this Z is 0.0250. Using the body of Table 6.4, you search for the 
probability 0.0250.
T a b l e  6 . 4
Finding a Z Value 
Corresponding to 
a Cumulative Area 
of 0.025 Under the 
Normal Curve
Cumulative Area
Z
.00 
.01
.02
.03
.04
.05
.06
.07
.08
.09
f
f
f
f
f
f
f
f
f
f
-2.0
.0228
.0222
.0217
.0212
.0207
.0202
.0197
.0192
.0188
.0183
-1.9
.0287
.0281
.0274
.0268
.0262
.0256
.0250
.0244
.0239
.0233
-1.8
.0359
.0351
.0344
.0336
.0329
.0232
.0314
.0307
.0301
.0294
Source: Extracted from Table E.2.
Working from the body of the table to the margins of the table, you see that the Z value 
corresponding to the particular Z row 1-1.92 and Z column (.06) is -1.96.
 Once you find Z, the final step is to use Equation (6.3) on page 255 as follows:
 X = m + Zs
 = 7 + 1-1.962122
 = 7 - 3.92
 = 3.08 seconds
 You use a similar process to find XU. Because only 2.5% of the video downloads take longer 
than XU seconds, 97.5% of the video downloads take less than XU seconds. From the symmetry 
of the normal distribution, you find that the desired Z value, as shown in Figure 6.15, is +1.96 
(because Z lies to the right of the standardized mean of 0). You can also extract this Z value from 
Table 6.5. You can see that 0.975 is the area under the normal curve less than the Z value of +1.96.
F i g u r e  6 . 1 5
Finding Z to determine XU
X Scale
Z Scale
XU
Area is 0.9750
7
0
+1.96
Area is 0.0250
(continued)

258	
Chapter 6  The Normal Distribution and Other Continuous Distributions
 Instead of looking up cumulative probabilities in a table, you can use Excel or Minitab to 
compute normal probabilities. Figure 6.16 displays an Excel worksheet that computes normal 
probabilities and finds X values for problems similar to Examples 6.1 through 6.5. Figure 6.17 
shows Minitab results for Examples 6.1 and 6.4. (You need to subtract the results in the left 
part of the figure from 1.0 to obtain the answer to Example 6.1)
F i g u r e  6 . 1 6
Excel worksheet for 
computing normal 
probabilities and finding 
X values (shown in two 
parts)
T a b l e  6 . 5
Finding a Z Value 
Corresponding to 
a Cumulative Area 
of 0.975 Under the 
Normal Curve
Cumulative Area
Z
.00
.01
.02
.03
.04
.05
.06
.07
.08
.09
f
f
f
f
f
f
f
f
f
f
f
+1.8
.9641
.9649
.9656
.9664
.9671
.9678
.9686
.9693
.9699
.9706
+1.9
.9713
.9719
.9726
.9732
.9738
.9744
.9750
.9756
.9761
.9767
+2.0
.9772
.9778
.9783
.9788
.9793
.9798
.9803
.9808
.9812
.9817
Source: Extracted from Table E.2.
Using Equation (6.3) on page 255,
 X = m + Zs
 = 7 + 1+1.962122
 = 7 + 3.92
 = 10.92 seconds
Therefore, 95% of the download times are between 3.08 and 10.92 seconds.
F i g u r e  6 . 1 7
Minitab results for 
Examples 6.1 and 6.4

	
6.2  The Normal Distribution	
259
Open the VE-Normal Distribution add-in workbook to ex-
plore the normal distribution. (See Appendix C to learn how you 
can download a copy of this workbook and Appendix Section D.5 
before using this workbook.) When this workbook opens properly, 
it adds a Normal Distribution menu in the Add-ins tab.
To explore the effects of changing the mean and standard devia-
tion on the area under a normal distribution curve workbook, select 
Add-ins ➔ Normal Distribution ➔ Probability Density Func-
tion. The add-in displays a normal curve for the MyTVLab website 
download example and a floating control panel (show at top right). 
Use the control panel spinner buttons to change the values for the 
mean, standard deviation, and X value and then note the effects of 
these changes on the probability of X 6 value and the correspond-
ing shaded area under the curve. To see the normal curve labeled 
with Z values, click Z Values. Click the Reset button to reset the 
control panel values. Click Finish to finish exploring.
To create shaded areas under the curve for problems similar to 
Examples 6.2 and 6.3, select Add-ins ➔ Normal Distribution 
➔ Areas. In the Areas dialog box (shown at bottom right), enter 
values, select an Area Option, and click OK. The add-in creates a 
normal distribution curve with areas that are shaded according to 
the values you entered.
V i s u a l  E x p l o r at i o n s  Exploring the Normal Distribution
T h i n k  A b o u t  T h i s  What Is Normal?
Ironically, the statistician who popularized the use 
of “normal” to describe the distribution discussed 
in Section 6.2 was someone who saw the distribu-
tion as anything but the everyday, anticipated oc-
currence that the adjective normal usually suggests.
Starting with an 1894 paper, Karl Pearson ar-
gued that measurements of phenomena do not natu-
rally, or “normally,” conform to the classic bell shape. 
While this principle underlies much of statistics today, 
Pearson’s point of view was radical to contempo-
raries who saw the world as standardized and nor-
mal. Pearson changed minds by showing that some 
populations are naturally skewed (coining that term in 
passing), and he helped put to rest the notion that the 
normal distribution underlies all phenomena.
Today, people still make the type of mistake 
that Pearson refuted. As a student, you are prob-
ably familiar with discussions about grade inflation, 
a real phenomenon at many schools. But have you 
ever realized that a “proof” of this inflation—that 
there are “too few” low grades because grades are 
skewed toward A’s and B’s—wrongly implies that 
grades should be “normally” distributed? Because 
college students represent small nonrandom sam-
ples, there are plenty of reasons to suspect that the 
distribution of grades would not be “normal.”
Misunderstandings about the normal distri-
bution have occurred both in business and in the 
public sector through the years. These misun-
derstandings have caused a number of business 
blunders and have sparked several public policy 
debates, including the causes of the collapse of 
large financial institutions in 2008. According to 
one theory, the investment banking industry’s ap-
plication of the normal distribution to assess risk 
may have contributed to the global collapse (see 
“A Finer Formula for Assessing Risks,” The New 
York Times, May 11, 2010, p. B2 and reference 8).  
Using the normal distribution led these banks to 
overestimate the probability of having stable mar-
ket conditions and underestimate the chance of 
unusually large market losses. 
According to this theory, the use of other dis-
tributions that have less area in the middle of their 
curves, and, therefore, more in the “tails” that repre-
sent unusual market outcomes, may have led to less 
serious losses.
As you study this chapter, make sure you 
understand the assumptions that must hold for the 
proper use of the “normal” distribution, assumptions 
that were not explicitly verified by the investment 
bankers. And, most importantly, always remember 
that the name normal distribution does not mean 
normal in the everyday sense of the word.

260	
Chapter 6  The Normal Distribution and Other Continuous Distributions
Problems for Section 6.2
Learning the Basics
6.1  Given a standardized normal distribution (with a mean of 0 and 
a standard deviation of 1, as in Table E.2), what is the probability that
a.	 Z is less than 1.20?
b.	 Z is greater than 1.25?
c.	 Z is between 1.25 and 1.70?
d.	 Z is less than 1.20 or greater than 1.70?
6.2  Given a standardized normal distribution (with a mean of 0 and 
a standard deviation of 1, as in Table E.2), what is the probability that
a.	 Z is between -1.23 and 1.64?
b.	 Z is less than -1.27 or greater than 1.74?
c.	 For normal data with values symmetrically distributed around 
the mean, find the Z values that contain 95% of the data.
d.	 Find the value of Z such that the area to the right is 2.5% of the 
total area under the normal curve.
6.3  Given a standardized normal distribution (with a mean of 0 and 
a standard deviation of 1, as in Table E.2), what is the probability that
a.	 Z is less than 1.16.
b.	 Z is greater than -0.21.
c.	 Z is less than -0.21 or greater than 0.21.
d.	 Z is less than -0.21 or greater than 2.06.
6.4  Given a standardized normal distribution (with a mean of 0 
and a standard deviation of 1, as in Table E.2), determine the fol-
lowing probabilities:
a.	 P1Z 6 -0.372
b.	 P1Z 7 2.062
c.	 P1-1.90 6 Z 6 -0.212
d.	 Find the value of Z such that the area to the right of Z is 15.87%
6.5  Given a normal distribution with m = 70 and s = 20, what 
is the probability that
a.	 X 7 110.
b.	 X 6 10.
c.	 X 6 70 or X 7 130.
d.	 Between what two X values (symmetrically distributed around 
the mean) are 70% of the values.
6.6  Given a normal distribution with m = 30 and s = 4, what is 
the probability that
a.	 X 7 38.
b.	 X 6 25.
c.	 Find the X value such that the area to the left of X is 5% of the 
total area under the normal curve.
d.	 Between what two X values (symmetrically distributed around 
the mean) are 40% of the values?
Applying the Concepts
6.7  According to Data360.org, in 2002, the average water con-
sumption per capita in Japan for the year was 93.5 gallons. Assume 
that the water consumption per capita in Japan follows a normal and 
symmetrical distribution with a mean of 93.5 gallons and a standard 
deviation of 12 gallons. Find the following:
a.	 The probability that a person in Japan consumed more than 32 
gallons in 2002.
b.	 The probability that a person in Japan consumed between 15 
and 25 gallons of water in 2002.
c.	 The probability that a person in Japan consumed less than 
15 gallons of water in 2002.
d.	 90% of the people in Japan consumed more than how many 
gallons of water?
SELF 
Test 
6.8  A regional airline carrier determined that the num-
ber of kilometers travelled per airplane per year is nor-
mally distributed with a mean of 700 thousand kilometers and a 
standard deviation of a 100 thousand kilometers.
a.	 What percentage of the planes is expected to travel between 
450 and 700 thousand kilometers in a year?
b.	 What proportion of the planes is expected to travel between 
350 and 600 thousand kilometers in a year?
c.	 Find the distance in kilometers travelled by 70% of the planes.
d.	 Compare the answers of parts (a) through (c) if the standard 
deviation is 80 thousand kilometers.
6.9  Consumers in the UK are expected to spend more on Christ-
mas presents this year with an average gift costing 28.70 pounds. 
(Data extracted from www.mirror.com.uk.) If the amount to be 
spent on Christmas gifts is normally distributed with a standard 
deviation of 12 pounds, find the following:
a.	 The probability that a person in the UK will spend more than 
15 pounds on a Christmas gift.
b.	 The probability that a randomly selected person from the UK 
will spend between 12 and 14 pounds.
c.	 Find the lower and upper limit for spending such that 95% of 
the purchases will fall within these limits.
6.10  The scores on a university entrance exam are normally dis-
tributed with a mean of 72% and a standard deviation of 15.
a.	 Find the probability that a student taking the entrance exam will 
score below 81.
b.	 Find the probability that a student taking the entrance exam will 
score between 65 and 71.
c.	 Find the grade of a student such that the corresponding prob-
ability is higher than 25%.
d.	 If a student scored 85 on this test and his friend scored 65 on 
another entrance test with a mean of 55 and a standard devia-
tion of 2. Which of the two students scored better in reference 
to the group?
6.11  A Nielsen study indicates that mobile subscribers between 
18 and 24 years of age spend a substantial amount of time watching 
video on their devices, reporting a mean of 396 minutes per month. 
(Data extracted from bit.ly/13f4uab.) Assume that the amount of 
time watching video on a mobile device per month is normally dis-
tributed and that the standard deviation is 50 minutes.
a.	 What is the probability that an 18- to 24-year-old mobile sub-
scriber spends less than 321 minutes watching video on his or 
her mobile device per month?
b.	 What is the probability that an 18- to 24-year-old mobile sub-
scriber spends between 320 minutes and 471 minutes watching 
video on his or her mobile device per month?
c.	 What is the probability that an 18- to 24-year-old mobile sub-
scriber spends more than 471 minutes watching video on his or 
her mobile device per month?
d.	 One percent of all 18- to 24-year-old mobile subscribers will 
spend less than how many minutes watching video on his or her 
mobile device per month?

	
6.3  Evaluating Normality	
261
6.12  In 2009, the consumption of milk per capita in China was 
27.5 kilograms. (Data extracted from www.helgilbrary.com.) If 
milk consumption is to be considered a normally distributed data 
with a mean of  27.5 kg and a standard deviation of  6.5 kg. Find 
the following:
a.	 The probability that a person in China consumed more than 
15 kg of milk.
b.	 The probability that a person in China consumed between 10 
and 20 kg of milk.
c.	 The probability that a person in China consumed less than 
10 kg of milk.
d.	 How much milk in kilograms will less than 99% of the people 
in China have consumed?
6.13  The daily exchange rate for currencies fluctuates on a daily 
basis due to many economic conditions affecting the business 
cycle. The exchange rate for a twelve month period in the year 
2004 between the US dollar and the Euro (EUR) shows an ap-
proximately normally distributed behavior with a mean exchange 
rate of 0.804 euros for every dollar and a standard deviation of 
0.0255. Find the following:
a.	 The probability that the exchange rate between the pair of cur-
rencies is between 0.798 and 0.8100.
b.	 The probability that the exchange rate will be larger than 0.845 
euros for every dollar.
c.	 The exchange rate such that 98% of the data falls below it.
d.	 If the standard deviation is changed from the stated value to 
0.03, what will the answers in (a) through (c) be?
6.3  Evaluating Normality
As first stated in Section 6.2, the normal distribution has several important theoretical properties:
 • It is symmetrical; thus, the mean and median are equal.
 • It is bell-shaped; thus, the empirical rule applies.
 • The interquartile range equals 1.33 standard deviations.
 • The range is approximately equal to 6 standard deviations.
As Section 6.2 notes, many continuous variables used in business closely follow a normal dis-
tribution. To determine whether a set of data can be approximated by the normal distribution, 
you either compare the characteristics of the data with the theoretical properties of the normal 
distribution or construct a normal probability plot.
Comparing Data Characteristics to Theoretical Properties
Many continuous variables have characteristics that approximate theoretical properties. How-
ever, other continuous variables are often neither normally distributed nor approximately 
normally distributed. For such variables, the descriptive characteristics of the data are incon-
sistent with the properties of a normal distribution. One approach you can use to determine 
whether a variable follows a normal distribution is to compare the observed characteristics of 
the variable with what would be expected if the variable followed a normal distribution. To do 
so, you can
 • Construct charts and observe their appearance. For small- or moderate-sized data sets, 
create a stem-and-leaf display or a boxplot. For large data sets, in addition, plot a histo-
gram or polygon.
 • Compute descriptive statistics and compare these statistics with the theoretical proper-
ties of the normal distribution. Compare the mean and median. Is the interquartile range 
approximately 1.33 times the standard deviation? Is the range approximately 6 times the 
standard deviation?
 • Evaluate how the values are distributed. Determine whether approximately two-thirds of 
the values lie between the mean and {1 standard deviation. Determine whether approx-
imately four-fifths of the values lie between the mean and {1.28 standard deviations. 
Determine whether approximately 19 out of every 20 values lie between the mean and 
{2 standard deviations.
For example, you can use these techniques to determine whether the one-year returns 
discussed in Chapters 2 and 3 (stored in  Retirement Funds ) follow a normal distribution.
Table 6.6 presents the descriptive statistics and the five-number summary for the one-year 
return percentage variable. Figure 6.18 presents the Excel and Minitab boxplots for the one-
year return percentages.

262	
Chapter 6  The Normal Distribution and Other Continuous Distributions
From Table 6.6, Figure 6.18, and from an ordered array of the returns (not shown here), 
you can make the following statements about the one-year returns:
 • The mean of 14.40 is approximately the same as the median of 14.48. (In a normal 
distribution, the mean and median are equal.)
 • The boxplot is slightly left-skewed. (The normal distribution is symmetrical.)
 • The interquartile range of 5.01 is approximately 1.03 standard deviations. (In a normal 
distribution, the interquartile range is 1.33 standard deviations.)
 • The range of 45.26 is equal to 9.32 standard deviations. (In a normal distribution, the 
range is approximately 6 standard deviations.)
 • 76.27% of the returns are within {1 standard deviation of the mean. (In a normal  
distribution, 68.26% of the values lie within {1 standard deviation of the mean.)
 • 85.13% of the returns are within {1.28 standard deviations of the mean. (In a normal 
distribution, 80% of the values lie within {1.28 standard deviations of the mean.)
 • 94.94% of the returns are within {2 standard deviations of the mean. (In a normal  
distribution, 95.44% of the values lie within {2 standard deviations of the mean.)
 • The skewness statistic is 0.1036 and the kurtosis statistic is 4.2511. (In a normal distribution, 
each of these statistics equals zero.)
T a b l e  6 . 6
Descriptive Statistics 
and Five-Number 
Summary for the 
One-Year Return 
Percentages
Descriptive Statistics for 1YrReturn%
Mean
14.40
Standard deviation
4.86
Median
14.48
Coeff. of variation
33.72%
Mode
14.50
Skewness
0.1036
Minimum
-11.28
Kurtosis
4.2511
Maximum
33.98
Count
316
Range
45.26
Standard error
0.27
Variance
23.57
Five-Number Summary
Minimum
-11.28
First quartile
11.80
Median
14.48
Third quartile
16.81
Maximum
33.98
F i g u r e  6 . 1 8
Excel and Minitab boxplots for the one-year return percentages
Each of the two lines, 
or ­whiskers, that extend 
from the Minitab central 
box extend 1.5 times the 
­interquartile range from the 
box. Beyond these ranges are 
the values considered to be 
outliers, plotted as asterisks.

	
6.3  Evaluating Normality	
263
Based on these statements and the criteria given on page 261, you can conclude that the 
one-year returns are slightly skewed and have somewhat more values within {1 standard 
­deviation of the mean than expected. The range is higher than what would be ­expected in 
a normal distribution, but this is mostly due to the single outlier at -11.28. The skewness 
is very slightly positive, and the kurtosis indicates a distribution that is much more peaked 
than a ­normal distribution. Thus, you can conclude that the data characteristics of the one-year 
­returns differ somewhat from the theoretical properties of a normal distribution.
Constructing the Normal Probability Plot
A normal probability plot is a visual display that helps you evaluate whether the data are 
normally distributed. One common plot is called the quantile–quantile plot. To create this 
plot, you first transform each ordered value to a Z value. For example, if you have a sample of 
n = 19, the Z value for the smallest value corresponds to a cumulative area of
1
n + 1 =
1
19 + 1 = 1
20 = 0.05
The Z value for a cumulative area of 0.05 (from Table E.2) is -1.65. Table 6.7 illustrates the 
entire set of Z values for a sample of n = 19.
In a quantile–quantile plot, the Z values are plotted on the X axis, and the corresponding val-
ues of the variable are plotted on the Y axis. If the data are normally distributed, the values will 
plot along an approximately straight line. Figure 6.19 illustrates the typical shape of the quan-
tile–quantile normal probability plot for a left-skewed distribution (Panel A), a normal distribution 
(Panel B), and a right-skewed distribution (Panel C). If the data are left-skewed, the curve will 
rise more rapidly at first and then level off. If the data are normally distributed, the points will plot 
along an approximately straight line. If the data are right-skewed, the data will rise more slowly at 
first and then rise at a faster rate for higher values of the variable being plotted.
F i g u r e  6 . 1 9
Normal probability 
plots for a left-skewed 
distribution, a normal 
distribution, and a right-
skewed distribution
Left-skewed
Normal
Right-skewed
Panel A
Panel B
Panel C
T a b l e  6 . 7
Ordered Values and 
Corresponding Z 
Values for a Sample of 
n = 19
Ordered  
Value
Z Value
Ordered  
Value
Z Value
Ordered  
Value
Z Value
1
-1.65
  8
-0.25
14
0.52
2
-1.28
  9
-0.13
15
0.67
3
-1.04
10
-0.00
16
0.84
4
-0.84
11
0.13
17
1.04
5
-0.67
12
0.25
18
1.28
6
-0.52
13
0.39
19
1.65
7
-0.39
 
 
 
 

264	
Chapter 6  The Normal Distribution and Other Continuous Distributions
The Minitab normal probability plot has the one-year return percentage variable on the  
X axis and the cumulative percentage for a normal distribution on the Y axis. As with a quantile–
quantile plot, the points will plot along an approximately straight line if the data are normally 
distributed. However, if the data are right-skewed, the curve will rise more rapidly at first and 
then level off. If the data are left-skewed, the data will rise more slowly at first and then rise at a 
faster rate for higher values of the variable being plotted. Observe that although the bulk of the 
points on the normal probability plot approximately follow a straight line, there are several high 
values that depart from a straight line, indicating a distribution that differs somewhat from a nor-
mal distribution.
F i g u r e  6 . 2 0
Excel (quantile–quantile) and Minitab normal probability plots for the one-year returns
Problems for Section 6.3
Learning the Basics
6.14  For a sample of n = 39 elements, find the lower and upper 
values of Z and show that the middle value has a Z value of zero.
6.15  For a sample of n = 6, list the six Z values.
Applying the Concepts
SELF 
Test 
6.16  The file  SUV  contains the overall miles per 
­gallon (MPG) of 2013 small SUVs 1n = 172:
22 23 21 22 25 26 22 22 21
19 22 22 26 23 24 21 22
Source: Data extracted from “Ratings,” Consumer Reports, April 
2013, pp. 34–35.
Decide whether the data appear to be approximately normally 
distributed by
a.	 comparing data characteristics to theoretical properties.
b.	 constructing a normal probability plot.
6.17  As prices of basic commodities have increased over the past 
decade,  the cost of attending a four year university where students 
seek their first college degree have been affected as well. Data for 
the cost of books is listed below:
25  50  75  100  125  150  175  200  225  250
Determine if the data is approximately normally distributed
a.	 Compare the characteristics of the data with the theoretical 
properties of a normally distributed data set.
b.	 Construct a normal probability plot.
6.18  The file  Property Taxes  contains the property taxes per cap-
ita for the 50 states and the District of Columbia. Decide whether 
the data appear to be approximately normally distributed by
a.	 comparing data characteristics to theoretical properties.
b.	 constructing a normal probability plot.
Figure 6.20 shows Excel (quantile–quantile) and Minitab normal probability plots for the 
one-year returns. The Excel quantile–quantile plot shows a single extremely low value followed 
by the bulk of the points that approximately follow a straight line except for a few high values.

	
6.4  The Uniform Distribution	
265
6.19  Thirty companies comprise the DJIA. How big are these 
companies? One common method for measuring the size of a 
company is to use its market capitalization, which is computed 
by multiplying the number of stock shares by the price of a share 
of stock. On March 30, 2013, the market capitalization of these 
­companies ranged from Alcoa’s $9.1 billion to ExxonMobil’s 
$403.7 billion. The entire population of market capitalization 
­values is stored in  DowMarketCap . (Data extracted from money 
.cnn.com, March 30, 2013.) Decide whether the market capital-
ization of companies in the DJIA appears to be approximately 
normally distributed by
a.	 comparing data characteristics to theoretical properties.
b.	 constructing a normal probability plot.
c.	 constructing a histogram.
6.20  One operation of a mill is to cut pieces of steel into parts 
that will later be used as the frame for front seats in an automo-
tive plant. The steel is cut with a diamond saw, and the resulting 
parts must be within {0.005 inch of the length specified by the 
automobile company. The data come from a sample of 100 steel 
parts and are stored in  Steel . The measurement reported is the 
difference, in inches, between the actual length of the steel part, as 
measured by a laser measurement device, and the specified length 
of the steel part. Determine whether the data appear to be approxi-
mately normally distributed by
a.	 comparing data characteristics to theoretical properties.
b.	 constructing a normal probability plot.
6.21  The file  CD Rate  contains the yields for a one-year certifi-
cate of deposit (CD) and a five-year CD for 23 banks in the United 
States, as of March 20, 2013. (Data extracted from www.Bankrate 
.com, March 20, 2013.) For each type of investment, decide whether 
the data appear to be approximately normally distributed by
a.	 comparing data characteristics to theoretical properties.
b.	 constructing a normal probability plot.
6.22  The file  Utility  contains the electricity costs, in dollars, 
during July 2013 for a random sample of 50 one-bedroom apart-
ments in a large city:
96 171 202 178 147 102 153 197 127  82
157 185  90 116 172 111 148 213 130 165
141 149 206 175 123 128 144 168 109 167
95 163 150 154 130 143 187 166 139 149
108 119 183 151 114 135 191 137 129 158
Decide whether the data appear to be approximately normally 
distributed by
a.	 comparing data characteristics to theoretical properties.
b.	 constructing a normal probability plot.
6.4  The Uniform Distribution
In the uniform distribution, the values are evenly distributed in the range between the smallest 
value, a, and the largest value, b. Because of its shape, the uniform distribution is sometimes 
called the rectangular distribution (see Panel B of Figure 6.1 on page 248). Equation (6.4) 
defines the probability density function for the uniform distribution.
Uniform Probability Density Function
	
f 1X2 =
1
b - a if a … X … b and 0 elsewhere	
(6.4)
	
where
a = minimum value of X
b = maximum value of X
Equation (6.5) defines the mean of the uniform distribution, and Equation (6.6) defines the 
variance and standard deviation of the uniform distribution.
Mean of the Uniform Distribution
	
m = a + b
2
	
(6.5)

266	
Chapter 6  The Normal Distribution and Other Continuous Distributions
One of the most common uses of the uniform distribution is in the selection of random 
numbers. When you use simple random sampling (see Section 1.4), you assume that each ran-
dom digit comes from a uniform distribution that has a minimum value of 0 and a maximum 
value of 9.
Figure 6.21 illustrates the uniform distribution with a = 0 and b = 1. The total area in-
side the rectangle is equal to the base (1.0) times the height (1.0). Thus, the resulting area of 
1.0 satisfies the requirement that the area under any probability density function equals 1.0.
Variance and Standard Deviation of the Uniform Distribution
	
s2 = 1b - a22
12
	
(6.6a)
	
s = B
1b - a22
12
	
(6.6b)
F i g u r e  6 . 2 1
Probability density 
function for a uniform 
distribution with a = 0 
and b = 1
X
1.0
.1
0
1.0
.2
.3
.4
.5
.6
.7
.8
.9
1.0
f(X)
In this uniform distribution, what is the probability of getting a random number between 
0.10 and 0.30? The area between 0.10 and 0.30, depicted in Figure 6.22, is equal to the base 
(which is 0.30 - 0.10 = 0.20) times the height (1.0). Therefore,
P10.10 6 X 6 0.302 = 1Base21Height2 = 10.20211.02 = 0.20
F i g u r e  6 . 2 2
Finding  
P10.10 6 X 6 0.302  
for a uniform distribution 
with a = 0 and b = 1
X
.1
0
1.0
.2
.3
.4
.5
.6
.7
.8
.9
1.0
f(X)
From Equations (6.5) and (6.6), the mean and standard deviation of the uniform distribution 
for a = 0 and b = 1 are computed as follows:
 m = a + b
2
 = 0 + 1
2
= 0.5

	
6.4  The Uniform Distribution	
267
and
 s2 = 1b - a22
12
 = 11 - 022
12
 = 1
12 = 0.0833
 s = 10.0833 = 0.2887
Thus, the mean is 0.5, and the standard deviation is 0.2887. 
Example 6.6 provides another application of the uniform distribution.
Example 6.6
Computing Uniform 
Probabilities
In the Normal Downloading at MyTVLab scenario on page 247, the download time of videos 
was assumed to be normally distributed with a mean of 7 seconds. Suppose that the download 
time follows a uniform (instead of a normal) distribution between 4.5 and 9.5 seconds. What is 
the probability that a download time will take more than 9 seconds?
Solution  The download time is uniformly distributed from 4.5 to 9.5 seconds. The 
area between 9 and 9.5 seconds is equal to 0.5 seconds, and the total area in the distribu-
tion is 9.5 - 4.5 = 5 seconds. Therefore, the probability of a download time between 9 and  
9.5 ­seconds is the portion of the area greater than 9, which is equal to 0.5>5.0 = 0.10.  
Because 9.5 is the maximum value in this distribution, the probability of a download time 
above 9 seconds is 0.10. In comparison, if the download time is normally distributed with a 
mean of 7 seconds and a standard deviation of 2 seconds (see Example 6.1 on page 253), the 
probability of a download time above 9 seconds is 0.1587.
Problems for Section 6.4
Learning the Basics
6.23  Suppose you select one value from a uniform distribution 
with a = 0 and b = 10. What is the probability that the value 
will be
a.	 between 5 and 7?
b.	 between 2 and 3?
c.	 What is the mean?
d.	 What is the standard deviation?
Applying the Concepts
SELF 
Test 
6.24  The time it takes for a plane to be cleaned and 
ready for the next flight is uniformly distributed be-
tween 35 and 45 minutes. What is the probability that the cleaning 
time will be
a.	 less than 37minutes?
b.	 between 35 and 40 minutes?
c.	 more than 38 minutes?
d.	 calculate the mean and the standard deviation for the cleaning 
time of an airplane.
6.25  A study of the time spent by visitors to finish the viewing of 
a marine life aquarium is uniformly distributed between 120 and 
200 minutes. What is the probability that the viewing time will be
a.	 between 150 and 190 minutes.
b.	 less than 160 minutes.
c.	 Calculate the mean and the standard deviation for the viewing 
time.
6.26  From past experience it is noted that the time it takes 
to upload final grades for any set of 5 random courses onto the 
university grade portal is uniformly distributed between 35 to 
65 minutes. If the final grades of a set of 5 university courses are 
uploaded into the university grade portal, find the probability that 
the upload time will be
a.	 less than 37 minutes.
b.	 more than 38 minutes.
c.	 between 38 to 62 minutes.
d.	 Calculate the mean and the standard deviation of the upload time.
6.27  The scheduled time for a flight between Kuwait city and the 
city of Dubai in the United Arab Emirates is 75 minutes. Assume 
that the actual flight time is uniformly distributed between 73 and 
85 minutes. Find the probability that the flight time will be
a.	 less than 78 minutes.
b.	 between 75 and 80 minutes.
c.	 greater than 65 minutes.
d.	 Calculate the mean and standard deviation of the flight time 
between the two cities.

268	
Chapter 6  The Normal Distribution and Other Continuous Distributions
6.5  The Exponential Distribution
The exponential distribution is a continuous distribution that is right-skewed and ranges from 
zero to positive infinity (see Panel C of Figure 6.1 on page 248). The exponential distribution is 
widely used in waiting-line (i.e., queuing) theory to model the length of time between arrivals 
in processes such as customers arriving at a bank’s ATM, patients entering a hospital emergency 
room, and hits on a website.
 The exponential distribution is defined by a single parameter, l, the mean number of 
­arrivals per unit of time. The probability density function for the length of time between arrivals 
is given by Equation (6.7).
Exponential Probability Density Function
	
f 1X2 = le-lx for X 7 0	
(6.7)
	
where
 e = mathematical constant approximated by 2.71828
 l = mean number of arrivals per unit
 X = any value of the continuous variable where 0 6 X 6 ∞
 The mean time between arrivals, m, is given by Equation (6.8), and the standard deviation 
of the time between arrivals, s, is given by Equation (6.9).
Mean Time Between Arrivals
	
m = 1
l	
(6.8)
Standard Deviation of the Time Between Arrivals
	
s = 1
l	
(6.9)
The value 1>l is equal to the mean time between arrivals. For example, if the mean number of 
arrivals in a minute is l = 4, then the mean time between arrivals is 1>l = 0.25 minute, or  
15 seconds. Equation (6.10) defines the cumulative probability that the length of time before the 
next arrival is less than or equal to X.
Cumulative Exponential Probability
	
P1arrival time … X2 = 1 - e-lx	
(6.10)
To illustrate the exponential distribution, suppose that customers arrive at a bank’s ATM 
at a rate of 20 per hour. If a customer has just arrived, what is the probability that the next 
customer will arrive within 6 minutes (i.e., 0.1 hour)? For this example, l = 20 and X = 0.1. 
Using Equation (6.10),
 P1arrival time … 0.12 = 1 - e-2010.12
 = 1 - e-2
 = 1 - 0.1353 = 0.8647

	
6.5  The Exponential Distribution	
269
Thus, the probability that a customer will arrive within 6 minutes is 0.8647, or 86.47%. 
­Figure 6.23 shows this probability as computed by Excel and Minitab.
F i g u r e  6 . 2 3
Excel and Minitab 
results for computing 
exponential probability 
that a customer will 
arrive within six minutes
Example 6.7 illustrates the effect on the exponential probability of changing the time 
between arrivals.
Example 6.7
Computing  
Exponential  
Probabilities
In the ATM example, what is the probability that the next customer will arrive within 3 minutes 
(i.e., 0.05 hour)?
Solution  For this example, l = 20 and X = 0.05. Using Equation (6.10),
 P1arrival time … 0.052 = 1 - e-2010.052
 = 1 - e-1
 = 1 - 0.3679 = 0.6321
Thus, the probability that a customer will arrive within 3 minutes is 0.6321, or 63.21%.
Problems for Section 6.5
Learning the Basics
6.28  Given an exponential distribution with l = 10, what is the 
probability that the arrival time is
a.	 less than X = 0.1?
b.	 greater than X = 0.1?
c.	 between X = 0.1 and X = 0.2?
d.	 less than X = 0.1 or greater than X = 0.2?
6.29  Given an exponential distribution with l = 30, what is the 
probability that the arrival time is
a.	 less than X = 0.1?
b.	 greater than X = 0.1?
c.	 between X = 0.1 and X = 0.2?
d.	 less than X = 0.1 or greater than X = 0.2?
6.30  Given an exponential distribution with l = 5, what is the 
probability that the arrival time is
a.	 less than X = 0.3?
b.	 greater than X = 0.3?
c.	 between X = 0.3 and X = 0.5?
d.	 less than X = 0.3 or greater than X = 0.5?
Applying the Concepts
6.31  Autos arrive at a toll plaza located at the entrance to a bridge 
at a rate of 50 per minute during the 5:00-to-6:00 p.m. hour. If an 
auto has just arrived,
a.	 what is the probability that the next auto will arrive within  
3 seconds (0.05 minute)?
b.	 what is the probability that the next auto will arrive within  
1 second (0.0167 minute)?
c.	 What are your answers to (a) and (b) if the rate of arrival of 
autos is 60 per minute?
d.	 What are your answers to (a) and (b) if the rate of arrival of 
autos is 30 per minute?
SELF 
Test 
6.32  Customers arrive at the drive-up window of a 
fast-food restaurant at a rate of 2 per minute during the 
lunch hour.
a.	 What is the probability that the next customer will arrive within 
1 minute?
b.	 What is the probability that the next customer will arrive within 
5 minutes?
c.	 During the dinner time period, the arrival rate is 1 per minute. 
What are your answers to (a) and (b) for this period?

270	
Chapter 6  The Normal Distribution and Other Continuous Distributions
6.33  Telephone calls arrive at the information desk of a large 
computer software company at a rate of 15 per hour.
a.	 What is the probability that the next call will arrive within  
3 minutes (0.05 hour)?
b.	 What is the probability that the next call will arrive within  
15 minutes (0.25 hour)?
c.	 Suppose the company has just introduced an updated version of 
one of its software programs, and telephone calls are now ar-
riving at a rate of 25 per hour. Given this information, what are 
your answers to (a) and (b)?
6.34  Calls arrive at a call center at the rate of 12 per hour. What is 
the probability that the next call arrives in
a.	 less than 3 minutes?
b.	 more than 6 minutes?
c.	 less than 1 minute?
6.35  The time between unplanned shutdowns of a power plant 
has an exponential distribution with a mean of 20 days. Find the 
probability that the time between two unplanned shutdowns is
a.	 less than 14 days.
b.	 more than 21 days.
c.	 less than 7 days.
6.36  Golfers arrive at the starter’s booth of a public golf course at 
a rate of 8 per hour during the Monday-to-Friday midweek period. 
If a golfer has just arrived,
a.	 what is the probability that the next golfer will arrive within  
15 minutes (0.25 hour)?
b.	 what is the probability that the next golfer will arrive within  
3 minutes (0.05 hour)?
c.	 The actual arrival rate on Fridays is 15 per hour. What are your 
answers to (a) and (b) for Fridays?
6.37  Some Internet companies sell a service that will boost a web-
site’s traffic by delivering additional unique visitors. Assume that 
one such company claims it can deliver 1,000 visitors a day. If this 
amount of website traffic is experienced, then the time between visi-
tors has a mean of 1.44 minutes (or 0.6944 per minute). Assume 
that your website gets 1,000 visitors a day and that the time between 
visitors has an exponential distribution. What is the probability that 
the time between two visitors is
a.	 less than 1 minute?
b.	 less than 2 minutes?
c.	 more than 3 minutes?
d.	 Do you think it is reasonable to assume that the time between 
visitors has an exponential distribution?
6.6  The Normal Approximation to the Binomial Distribution
In many circumstances, you can use the normal distribution to approximate the binomial distri-
bution that is discussed in Section 5.3. The Section 6.6 online topic explains this approximation 
and illustrates its use.
I
n the Normal Downloading at MyTVLab scenario, you 
were a project manager for an online social media and 
video website. You sought to ensure that a video could be 
downloaded quickly by visitors to the website. By running 
experiments in the corporate offices, you determined that 
the amount of time, in seconds, that passes from clicking 
a download link until a video is fully displayed is a bell-
shaped distribution with a mean download time of 7 seconds 
and standard deviation of 2 seconds. Using the normal dis-
tribution, you were able to calculate that approximately 84% 
of the download times are 9 seconds or less, and 95% of the 
download times are between 3.08 and 10.92 seconds.
Now that you understand how to compute probabilities 
from the normal distribution, you can evaluate download 
times of a video using different website designs. For example, 
if the standard deviation remained at 2 seconds, lowering the 
mean to 6 seconds would shift the entire distribution lower by 
1 ­second. Thus, approximately 84% of the download times 
would be 8 seconds or less, and 95% of the download times 
would be between 2.08 and 9.92 seconds. Another change 
that could reduce long download times would be reducing 
the variation. For example, consider the case where the mean 
remained at the original 7 seconds but the standard deviation 
was reduced to ­1 ­second. Again, approximately 84% of the 
download times would be 8 seconds or less, and 95% of the 
download times would be between 5.04 and 8.96 seconds.
U s i n g  S tat i s t i c s
Normal Downloading at 
MyTVLab, Revisited
Cloki/Shutterstock

	
Key Equations	
271
S u m m a r y
In this and the previous chapter, you have learned about 
mathematical models called probability distributions 
and how they can be used to solve business problems. In ­ 
Chapter 5, you used discrete probability distributions in sit-
uations where the values come from a counting process such 
as the number of social media sites to which you belong 
or the number of tagged order forms in a report generated 
by an accounting information system. In this chapter, you 
learned about continuous probability distributions where the 
values come from a measuring process such as your height 
or the download time of a video.
Continuous probability distributions come in various 
shapes, but the most common and most important in busi-
ness is the normal distribution. The normal distribution 
is symmetrical; thus, its mean and median are equal. It is 
also bell-shaped, and approximately 68.26% of its values 
are within {1 standard deviation of the mean, approxi-
mately 95.44% of its values are within {2 standard devia-
tions of the mean, and approximately 99.73% of its values 
are within {3 standard deviations of the mean. Although 
many variables in business are closely approximated by the 
normal distribution, do not think that all variables can be 
­approximated by the normal distribution.
In Section 6.3, you learned about various methods for 
evaluating normality in order to determine whether the nor-
mal distribution is a reasonable mathematical model to use 
in specific situations. In Sections 6.4 and 6.5, you studied 
other continuous distributions—in particular, the uniform 
and exponential distributions. Chapter 7 uses the normal 
distribution to develop the subject of statistical inference.
Refere n c e s
	 1.	Gunter, B. “Q-Q Plots.” Quality Progress (February 1994): 
81–86.
	 2.	Levine, D. M., P. Ramsey, and R. Smidt. Applied Statistics for 
Engineers and Scientists Using Microsoft Excel and Minitab. 
Upper Saddle River, NJ: Prentice Hall, 2001.
	 3.	Microsoft Excel 2013. Redmond, WA: Microsoft Corp., 2012.
	 4.	Miller, J. “Earliest Known Uses of Some of the Words of 
Mathematics.” jeff560.tripod.com/mathword.html.
	 5.	Minitab Release 16. State College, PA: Minitab, Inc., 2010.
	 6.	Pearl, R. “Karl Pearson, 1857–1936.” Journal of the American 
Statistical Association, 31 (1936): 653–664.
	 7.	Pearson, E. S. “Some Incidents in the Early History of Biom-
etry and Statistics, 1890–94.” Biometrika 52 (1965): 3–18.
	 8.	Taleb, N. The Black Swan, 2nd ed. New York: Random House, 
2010.
	 9.	Walker, H. “The Contributions of Karl Pearson.” Journal of 
the American Statistical Association 53 (1958): 11–22.
K ey Eq u at i o n s
Normal Probability Density Function
f  1X2 =
1
22ps
e-11>2231X - m2>s42
(6.1)
Z Transformation Formula
Z = X - m
s

(6.2)
Finding an X Value Associated with a  
Known Probability
X = m + Zs
(6.3)
Uniform Probability Density Function
f  1X2 =
1
b - a
(6.4)
Mean of the Uniform Distribution
m = a + b
2

(6.5)
Variance and Standard Deviation of the Uniform Distribution
s2 = 1b - a22
12

(6.6a)
s = B
1b - a22
12

(6.6b)
Exponential Probability Density Function
f  1X2 = le-lx for X 7 0
(6.7)
Mean Time Between Arrivals
m = 1
l
(6.8)
Standard Deviation of the Time Between Arrivals
s = 1
l
(6.9)
Cumulative Exponential Probability
P1arrival time … X2 = 1 - e-lx
(6.10)

272	
Chapter 6  The Normal Distribution and Other Continuous Distributions
K e y  Term s
cumulative standardized normal  
distribution  251
exponential distribution  268
normal distribution  248
normal probability plot  263
probability density function  248
probability density function for the  
normal distribution  250
quantile–quantile plot  263
rectangular distribution  265
standardized normal  
variable  250
transformation formula  250
uniform distribution  265
C hec ki n g  Yo u r  U n d e r s ta nding
6.38  Why is only one normal distribution table such as Table E.2 
needed to find any probability under the normal curve?
6.39  How do you find the area between two values under the nor-
mal curve?
6.40  How do you find the X value that corresponds to a given 
percentile of the normal distribution?
6.41  What are some of the distinguishing properties of a normal 
distribution?
6.42  How does the shape of the normal distribution differ from 
the shapes of the uniform and exponential distributions?
6.43  How can you use the normal probability plot to evaluate 
whether a set of data is normally distributed?
6.44  Under what circumstances can you use the exponential  
distribution?
C ha pter  R e vi e w P r o b le ms
6.45  An industrial sewing machine uses ball bearings that are 
targeted to have a diameter of 0.75 inch. The lower and upper 
specification limits under which the ball bearings can operate are  
0.74 inch and 0.76 inch, respectively. Past experience has indi-
cated that the actual diameter of the ball bearings is approximately 
normally distributed, with a mean of 0.753 inch and a standard de-
viation of 0.004 inch. What is the probability that a ball bearing is
a.	 between the target and the actual mean?
b.	 between the lower specification limit and the target?
c.	 above the upper specification limit?
d.	 below the lower specification limit?
e.	 Of all the ball bearings, 93% of the diameters are greater than 
what value?
6.46  The fill amount in 2-liter soft drink bottles is normally 
distributed, with a mean of 2.0 liters and a standard deviation of  
0.05 liter. If bottles contain less than 95% of the listed net content 
(1.90 liters, in this case), the manufacturer may be subject to pen-
alty by the state office of consumer affairs. Bottles that have a net 
content above 2.10 liters may cause excess spillage upon opening. 
What proportion of the bottles will contain
a.	 between 1.90 and 2.0 liters?
b.	 between 1.90 and 2.10 liters?
c.	 below 1.90 liters or above 2.10 liters?
d.	 At least how much soft drink is contained in 99% of the bottles?
e.	 Ninety-nine percent of the bottles contain an amount that is between 
which two values (symmetrically distributed) around the mean?
6.47  In an effort to reduce the number of bottles that contain less 
than 1.90 liters, the bottler in Problem 6.46 sets the filling machine 
so that the mean is 2.02 liters. Under these circumstances, what 
are your answers in Problem 6.46 (a) through (e)?
6.48  An Ipsos MediaCT study indicates that mobile device own-
ers who used their mobile device while shopping for consumer 
­electronics spent an average of $1,539 on consumer electronics in the 
past six months. (Data extracted from iab.net/showrooming.) As-
sume that the amount spent on consumer electronics in the last six 
months is normally distributed and that the standard deviation is $500. 
a.	 What is the probability that a mobile device owner who used 
his or her mobile device while shopping for consumer electron-
ics spent less than $1,000 on consumer electronics?
b.	 What is the probability that a mobile device owner who used 
his or her mobile device while shopping for consumer electron-
ics spent between $2,500 and $3,000 on consumer electronics?
c.	 Ninety percent of the amounts spent on consumer electronics 
by mobile device owners who used their mobile device while 
shopping for consumer electronics are less than what value?
d.	 Eighty percent of the amounts spent on consumer electronics 
by mobile device owners who used their mobile device while 
shopping for consumer electronics are between what two val-
ues symmetrically distributed around the mean?
6.49  The file  DomesticBeer  contains the percentage alcohol, 
number of calories per 12 ounces, and number of carbohydrates (in 
grams) per 12 ounces for 152 of the best-selling domestic beers 
in the United States. Determine whether each of these variables 
­appears to be approximately normally distributed. Support your 
­decision through the use of appropriate statistics and graphs. (Data 
extracted from www.Beer100.com, March 20, 2013.)
6.50  The evening manager of a restaurant was very concerned 
about the length of time some customers were waiting in line to 
be seated. She also had some concern about the seating times—
that is, the length of time between when a customer is seated and 
the time he or she leaves the restaurant. Over the course of one 
week, 100 customers (no more than 1 per party) were randomly 
selected, and their waiting and seating times (in minutes) were 
recorded in  Wait .

	
Cases for Chapter 6	
273
a.	 Think about your favorite restaurant. Do you think waiting 
times more closely resemble a uniform, an exponential, or a 
normal distribution?
b.	 Again, think about your favorite restaurant. Do you think seat-
ing times more closely resemble a uniform, an exponential, or a 
normal distribution?
c.	 Construct a histogram and a normal probability plot of the 
waiting times. Do you think these waiting times more closely 
resemble a uniform, an exponential, or a normal distribution?
d.	 Construct a histogram and a normal probability plot of the 
seating times. Do you think these seating times more closely 
resemble a uniform, an exponential, or a normal distribution?
6.51  The major stock market indexes had strong results in 2012. 
The mean one-year return for stocks in the S&P 500, a group of 
500 very large companies, was +13.41%. The mean one-year re-
turn for the NASDAQ, a group of 3,200 small and medium-sized 
companies, was +15.91%. Historically, the one-year returns are 
approximately normally distributed, the standard deviation in the 
S&P 500 is approximately 20%, and the standard deviation in the 
NASDAQ is approximately 30%.
a.	 What is the probability that a stock in the S&P 500 gained 
value in 2012?
b.	 What is the probability that a stock in the S&P 500 gained 10% 
or more in 2012?
c.	 What is the probability that a stock in the S&P 500 lost 20% or 
more in 2012?
d.	 What is the probability that a stock in the S&P 500 lost 30% or 
more in 2012?
e.	 Repeat (a) through (d) for a stock in the NASDAQ.
f.	 Write a short summary on your findings. Be sure to include a 
discussion of the risks associated with a large standard deviation.
6.52  The speed at which you can log into a website through a 
smartphone is an important quality characteristic of that website. 
In a recent test, the mean time to log into the JetBlue Airways 
website through a smartphone was 4.237 seconds. (Data extracted 
from N. Trejos, “Travelers Have No Patience for Slow Mobile 
Sites,” USA Today, April 4, 2012, p. 3B.) Suppose that the down-
load time is normally distributed, with a standard deviation of  
1.3 seconds. What is the probability that a download time is
a.	 less than 2 seconds?
b.	 between 1.5 and 2.5 seconds?
c.	 above 1.8 seconds?
d.	 Ninety-nine percent of the download times are slower (higher) 
than how many seconds?
e.	 Ninety-five percent of the download times are between what 
two values, symmetrically distributed around the mean?
f.	 Suppose that the download times are uniformly distributed be-
tween 1 and 9 seconds. What are your answers to (a) through (c)?
6.53  The speed at which you can log into a website through a smart-
phone is an important quality characteristic of that website. In a recent 
test, the mean time to log into the Hertz website through a smartphone 
was 7.524 seconds. (Data extracted from N. Trejos, “Travelers Have 
No Patience for Slow Mobile Sites,” USA Today, April 4, 2012, p. 3B.) 
Suppose that the download time is normally distributed, with a standard 
deviation of 1.7 seconds. What is the probability that a download time is
a.	 less than 2 seconds?
b.	 between 1.5 and 2.5 seconds?
c.	 above 1.8 seconds?
d.	 Ninety-nine percent of the download times are slower (higher) 
than how many seconds?
e.	 Ninety-five percent of the download times are between what 
two values, symmetrically distributed around the mean?
f.	 Suppose that the download times are uniformly distributed be-
tween 1 and 14 seconds. What are your answers to (a) through (d)?
g.	 Compare the results for the JetBlue Airways site computed in 
Problem 6.52 to those of the Hertz website.
6.54  (Class Project) One theory about the daily changes in 
the closing price of stock is that these changes follow a random 
walk—that is, these daily events are independent of each other and 
move upward or downward in a random manner—and can be ap-
proximated by a normal distribution. To test this theory, use either 
a newspaper or the Internet to select one company traded on the 
NYSE, one company traded on the American Stock Exchange, and 
one company traded on the NASDAQ and then do the following:
1.	 Record the daily closing stock price of each of these companies for 
six consecutive weeks (so that you have 30 values per company).
2.	 Compute the daily changes in the closing stock price of each  
of these companies for six consecutive weeks (so that you have 
30 values per company).
Note: The random-walk theory pertains to the daily changes in the 
closing stock price, not the daily closing stock price.
For each of your six data sets, decide whether the data are approxi-
mately normally distributed by
a.	 constructing the stem-and-leaf display, histogram or polygon, 
and boxplot.
b.	 comparing data characteristics to theoretical properties.
c.	 constructing a normal probability plot.
d.	 Discuss the results of (a) through (c). What can you say about 
your three stocks with respect to daily closing prices and daily 
changes in closing prices? Which, if any, of the data sets are 
­approximately normally distributed?
C a s e s  f o r  C h a p t e r  6
Managing Ashland MultiComm Services
The AMS technical services department has embarked on a 
quality improvement effort. Its first project relates to main-
taining the target upload speed for its Internet service sub-
scribers. Upload speeds are measured on a standard scale in 
which the target value is 1.0. Data collected over the past 
year indicate that the upload speed is approximately nor-
mally distributed, with a mean of 1.005 and a standard devi-
ation of 0.10. Each day, one upload speed is measured. The 
upload speed is considered acceptable if the measurement 
on the standard scale is between 0.95 and 1.05.

274	
Chapter 6  The Normal Distribution and Other Continuous Distributions
Digital Case
Apply your knowledge about the normal distribution in this 
Digital Case, which extends the Using Statistics scenario 
from this chapter.
To satisfy concerns of potential customers, the management of 
MyTVLab has undertaken a research project to learn how much 
time it takes users to load a complex video features page. The 
research team has collected data and has made some claims 
based on the assertion that the data follow a normal distribution.
Open MTL_QRTStudy.pdf, which documents the 
work of a quality response team at MyTVLab. Read the 
­internal report that documents the work of the team and 
their conclusions. Then answer the following:
1.	Can the collected data be approximated by the normal 
distribution?
2.	Review and evaluate the conclusions made by the MyTV-
Lab research team. Which conclusions are correct? Which 
ones are incorrect?
3.	If MyTVLab could improve the mean time by five 
­seconds, how would the probabilities change?
CardioGood Fitness
Return to the CardioGood Fitness case (stored in  
 CardioGood Fitness ) first presented on page 111.
1.	For each CardioGood Fitness treadmill product line, 
­determine whether the age, income, usage, and the 
­number of miles the customer expects to walk/run each 
week can be approximated by the normal distribution.
2.	Write a report to be presented to the management of 
­CardioGood Fitness, detailing your findings.
More Descriptive Choices Follow-up
Follow up the More Descriptive Choices Revisited Us-
ing Statistics scenario on page 172 by constructing normal 
probability plots for the 3-year return percentages, 5-year 
return percentages, and 10-year return percentages for the 
sample of 316 retirement funds stored in  Retirement Funds . 
In your analysis, examine differences between the growth 
and value funds as well as the differences among the small, 
mid-cap, and large market cap funds.
Clear Mountain State Student Surveys
1.	 The Student News Service at Clear Mountain State Uni-
versity (CMSU) has decided to gather data about the un-
dergraduate students who attend CMSU. They ­create and 
distribute a survey of 14 questions and receive ­responses 
from 62 undergraduates (stored in  UndergradSurvey ). For 
each numerical variable in the survey, decide whether the 
variable is approximately normally distributed by
a.	 comparing data characteristics to theoretical properties.
b.	constructing a normal probability plot.
c.	 writing a report summarizing your conclusions.
2.	 The dean of students at CMSU has learned about the un-
dergraduate survey and has decided to undertake a similar 
survey for graduate students at CMSU. She creates and 
distributes a survey of 14 questions and receives responses 
from 44 graduate students (stored in  GradSurvey ). For 
each numerical variable in the survey, decide whether the 
variable is approximately normally distributed by
a.	 comparing data characteristics to theoretical properties.
b.	constructing a normal probability plot.
c.	 writing a report summarizing your conclusions.
1.	Assuming that the distribution has not changed from 
what it was in the past year, what is the probability that 
the upload speed is
a.	less than 1.0?
b.	between 0.95 and 1.0?
c.	 between 1.0 and 1.05?
d.	less than 0.95 or greater than 1.05?
2.	The objective of the operations team is to reduce the 
probability that the upload speed is below 1.0. Should 
the team focus on process improvement that increases the 
mean upload speed to 1.05 or on process improvement 
that reduces the standard deviation of the upload speed to 
0.075? Explain.

	
Chapter 6 Excel Guide	
275
EG6.1  Continuous Probability 
Distributions
There are no Excel Guide instructions for this section.
EG6.2  The Normal Distribution
Key Technique  Use the NORM.DIST(X value, mean, stan-
dard deviation, True) function to compute normal probabilities 
and use the NORM.S.INV(percentage) function and the STAN-
DARDIZE function (see Section EG3.2) to compute the Z value.
Example  Compute the normal probabilities for Examples 6.1 
through 6.3 on pages 253 and 254 and the X and Z values for 
­Examples 6.4 and 6.5 on pages 256 and 257.
PHStat  Use Normal.
For the example, select PHStat ➔ Probability & Prob. Distributions 
➔ Normal. In this procedure’s dialog box (shown below):
	 1.	 Enter 7 as the Mean and 2 as the Standard Deviation.
	 2.	 Check Probability for: X 6 = and enter 7 in its box.
	 3.	 Check Probability for: X 7 and enter 9 in its box.
	 4.	 Check Probability for range and enter 5 in the first box and 9 
in the second box.
	 5.	 Check X for Cumulative Percentage and enter 10 in its box.
	 6.	 Check X Values for Percentage and enter 95 in its box.
	 7.	 Enter a Title and click OK.
In-Depth Excel  Use the COMPUTE worksheet of the Nor-
mal workbook as a template.
The worksheet already contains the data for solving the problems 
in Examples 6.1 through 6.5. For other problems, change the val-
ues for the Mean, Standard Deviation, X Value, From X Value, 
To X Value, Cumulative Percentage, and/or Percentage.
Read the Short Takes for Chapter 6 for an explanation of 
the formulas found in the COMPUTE worksheet (shown in the 
­COMPUTE_FORMULAS worksheet). If you use an ­Excel 
­version older than Excel 2010, use the COMPUTE_OLDER 
worksheet.
EG6.3  Evaluating Normality
Comparing Data Characteristics  
to Theoretical Properties
Use the Sections EG3.1 through EG3.3 instructions to compare 
data characteristics to theoretical properties.
Constructing the Normal Probability Plot
Key Technique  Use an Excel Scatter (X, Y) chart with Z values 
computed using the NORM.S.INV function.
Example  Construct the normal probability plot for the one-year 
return percentages for the sample of 316 retirement funds that is 
shown in Figure 6.20 on page 264.
PHStat  Use Normal Probability Plot.
For the example, open to the DATA worksheet of the Retirement 
Funds workbook. Select PHStat ➔ Probability & Prob. Distri-
butions ➔ Normal Probability Plot. In the procedure’s dialog 
box (shown below):
	 1.	 Enter I1:I317 as the Variable Cell Range.
	 2.	 Check First cell contains label.
	 3.	 Enter a Title and click OK.
 In addition to the chart sheet containing the normal probabil-
ity plot, the procedure creates a plot data worksheet identical to the 
PlotData worksheet discussed in the In-Depth Excel instructions.
In-Depth Excel  Use the worksheets of the NPP workbook as 
templates.
The NormalPlot chart sheet displays a normal probability plot us-
ing the rank, the proportion, the Z value, and the variable found in 
the PLOT_DATA worksheet. The PLOT_DATA worksheet already 
contains the one-year return percentages for the example. To construct 
a plot for a different variable, paste the sorted values for that variable 
in column D of the PLOT_DATA worksheet. Adjust the number of 
ranks in column A and the divisor in the formulas in column B to 
compute cumulative percentages to reflect the quantity n + 1 (317 
for the example). (Column C formulas use the NORM.S.INV func-
tion to compute the Z values for those cumulative percentages.)
If you have fewer than 316 values, delete rows from the 
bottom up. If you have more than 316 values, select row 317, 
right-click, click Insert in the shortcut menu, and copy down the 
formulas in columns B and C to the new rows. To create your own 
C h a p t e r  6  E x c e l  G u i d e

276	
Chapter 6  The Normal Distribution and Other Continuous Distributions
normal probability plot for the 1YrReturn% variable, open to the 
PLOT_DATA worksheet and select the cell range C1:D317. Then 
select Insert ➔ Scatter and select the first Scatter gallery item 
(that shows only points and is labeled with Scatter or Scatter 
with only Markers). Relocate the chart to a chart sheet, turn off 
the chart legend and gridlines, add axis titles, and modify the chart 
title by using the instructions in Appendix Section B.6.
If you use an Excel version older than Excel 2010, use the 
PLOT_OLDER worksheet and the NormalPlot_OLDER chart sheet.
EG6.4  The Uniform Distribution
There are no Excel Guide instructions for this section.
EG6.5  The Exponential Distribution
Key Technique  Use the EXPON.DIST(X value, mean, True) 
function.
Example  Compute the exponential probability for the bank 
ATM customer arrival example on page 268.
PHStat  Use Exponential.
For the example, select PHStat ➔ Probability & Prob. Distribu-
tions ➔ Exponential. In the procedure’s dialog box (shown below):
	 1.	 Enter 20 as the Mean per unit (Lambda) and 0.1 as the X Value.
	 2.	 Enter a Title and click OK.
In-Depth Excel  Use the COMPUTE worksheet of the Expo-
nential workbook as a template.
The worksheet already contains the data for the example. For other 
problems, change the Mean and X Value in cells B4 and B5. If 
you use an Excel version older than Excel 2010, use the COM-
PUTE_OLDER worksheet.
C h a p t e r  6  M i n i ta b  G u i d e
MG6.1  Continuous Probability 
Distributions
There are no Minitab Guide instructions for this section.
MG6.2  The Normal Distribution
Use Normal.
For example, to compute the normal probability for Example 6.1 
on page 253, open to a new worksheet. Enter X Value as the name 
of column C1 and enter 9 in the row 1 cell of that column. Select 
Calc ➔ Probability Distributions ➔ Normal. In the Normal Dis-
tribution dialog box (shown below):
	 1.	 Click Cumulative probability.
	 2.	 Enter 7 in the Mean box.
	 3.	 Enter 2 in the Standard deviation box.
	 4.	 Click Input column and enter C1 in its box and press Tab.
	 5.	 Enter C2 in the first Optional storage box.
	 6.	 Click OK.
Minitab places in the row 1 cell of column C2 the probabil-
ity for a download time that is less than 9 seconds with m = 7 
and s = 2. To compute the Example 6.1 probability for a down-
load time that is greater than 9 seconds, select Calc ➔ Calcula-
tor. Enter C3 in the Store result in variable box, enter 1 − C2 
in the Expression box, and click OK. The probability appears in 
row 1 of column C3.
To compute the normal probability for Example 6.4 on page 
256, open to a new worksheet. Enter Cumulative Percentage as 
the name of column C1 and enter 0.1 in the row 1 cell of that col-
umn. Select Calc ➔ Probability Distributions ➔ Normal. In the 
Normal Distribution dialog box:
	 1.	 Click Inverse cumulative probability.
	 2.	 Enter 7 in the Mean box.
	 3.	 Enter 2 in the Standard deviation box.
	 4.	 Click Input column and enter C1 in its box and press Tab.
	 5.	 Enter C2 in the first Optional storage box.
	 6.	 Click OK.
Minitab displays the Example 6.4 Z value corresponding to a  
cumulative area of 0.10. Skip step 5 in either set of instructions to 
create the results shown in Figure 6.17 on page 258.
MG6.3  Evaluating Normality
Comparing Data Characteristics  
to Theoretical Properties
Use instructions in Sections MG3.1 through MG3.3 in the ­ 
Chapter 3 Minitab Guide to compare data characteristics to 
­theoretical properties.

	
Chapter 6 Minitab Guide	
277
Constructing the Normal Probability Plot
Use Probability Plot.
For example, to construct the normal probability plot for the one-
year return percentage for the sample of 316 retirement funds 
shown in Figure 6.20 on page 264, open to the Retirement Funds 
worksheet. Select Graph ➔ Probability Plot and:
	 1.	  In the Probability Plots dialog box, click Single and then click OK.
In the Probability Plot - Single dialog box (shown below):
	 2.	 Double-click C9 1YrReturn% in the variables list to add 
'1YrReturn%' to the Graph variables box.
	 3.	 Click Distribution.
In the Probability Plot - Distribution dialog box (shown below):
	 4.	 Click the Distribution tab and select Normal from the Distri-
bution drop-down list.
	 5.	  Click the Data Display tab. Click Symbols only. If the Show 
confidence interval check box is not disabled (as shown below), 
clear this check box.
	 6.	 Click OK.
	 7.	 Back in the Probability Plot - Single dialog box, click Scale. 
	 8.	 Click the Gridlines tab. Clear all check boxes and then click OK.
	 9.	 Back in the Probability Plot - Single dialog box, click OK.
MG6.4  The Uniform Distribution
There are no Minitab instructions for this section
MG6.5  The Exponential Distribution
Use Exponential. 
For example, to compute the exponential probability for the bank 
ATM customer arrival example on page 268, open to a new work-
sheet. Enter X Value as the name of column C1 and enter 0.1 in 
the row 1 cell of column C1. Select Calc ➔ Probability Distribu-
tions ➔ Exponential. In the Exponential Distribution dialog box 
(shown below):
	 1.	 Click Cumulative probability.
	 2.	 Enter 0.05 in the Scale box. (Minitab defines scale as the 
mean time between arrivals, 1>l = 1>20 = 0.05, not the 
mean number of arrivals, l = 20.)
	 3.	 Leave the Threshold value as 0.0.
	 4.	 Click Input column and enter C1 in its box.
	 5.	 Click OK.

278
U s i n g  S tat i s t i c s
Sampling Oxford Cereals
The automated production line at the Oxford Cereals main plant fills thousands 
of boxes of cereal during each shift. As the plant operations manager, you are re-
sponsible for monitoring the amount of cereal placed in each box. To be consist-
ent with package labeling, boxes should contain a mean of 368 grams of cereal. 
Because of the speed of the process, the cereal weight varies from box to box, 
causing some boxes to be underfilled and others to be overfilled. If the automated 
process fails to work as intended, the mean weight in the boxes could vary too 
much from the label weight of 368 grams to be acceptable.
Because weighing every single box is too time-consuming, costly, and inef-
ficient, you must take a sample of boxes. For each sample you select, you plan to 
weigh the individual boxes and calculate a sample mean. You need to determine 
the probability that such a sample mean could have been randomly selected from 
a population whose mean is 368 grams. Based on your analysis, you will have to 
decide whether to maintain, alter, or shut down the cereal-filling process.
contents
7.1	 Sampling Distributions
7.2	 Sampling Distribution  
of the Mean
Visual Explorations: 
Exploring Sampling Distributions
7.3	 Sampling Distribution of the 
Proportion
7.4	 Sampling from Finite 
Populations (online)
Using Statistics: Sampling 
Oxford Cereals, Revisited
Chapter 7 Excel Guide
Chapter 7 Minitab Guide
Objectives
To learn about the concept of the 
sampling distribution
To compute probabilities related 
to the sample mean and the 
sample proportion
To understand the importance of 
the Central Limit Theorem
Chapter
Sampling  
Distributions
7
Corbis

	
7.2  Sampling Distribution of the Mean	
279
I
n Chapter 6, you used the normal distribution to study the distribution of video down-
load times from the MyTVLab website. In this chapter, you need to make a decision 
about a cereal-filling process, based on the weights of a sample of cereal boxes packaged  
at Oxford Cereals. You will learn about sampling distributions and how to use them to solve 
business problems.
In many applications, you want to make inferences that are based on statistics calculated from 
samples to estimate the values of population parameters. In the next two sections, you will learn 
about how the sample mean (a statistic) is used to estimate the population mean (a parameter) 
and how the sample proportion (a statistic) is used to estimate the population proportion (a pa-
rameter). Your main concern when making a statistical inference is reaching conclusions about 
a population, not about a sample. For example, a political pollster is interested in the sample 
results only as a way of estimating the actual proportion of the votes that each candidate will 
receive from the population of voters. Likewise, as plant operations manager for Oxford Cere-
als, you are only interested in using the mean weight calculated from a sample of cereal boxes 
to estimate the mean weight of a population of boxes.
In practice, you select a single random sample of a predetermined size from the popula-
tion. Hypothetically, to use the sample statistic to estimate the population parameter, you could 
examine every possible sample of a given size that could occur. A sampling distribution is the 
distribution of the results if you actually selected all possible samples. The single result you 
obtain in practice is just one of the results in the sampling distribution.
In Chapter 3, several measures of central tendency, including the mean, median, and mode, 
were discussed. For several reasons, the mean is the most widely used measure of central  
tendency, and the sample mean is often used to estimate the population mean. The sampling 
distribution of the mean is the distribution of all possible sample means if you select all  
possible samples of a given size.
The Unbiased Property of the Sample Mean
The sample mean is unbiased because the mean of all the possible sample means (of a given 
sample size, n) is equal to the population mean, m. A simple example concerning a population 
of four administrative assistants demonstrates this property. Each assistant is asked to apply 
the same set of updates to a human resources database. Table 7.1 presents the number of errors 
made by each of the administrative assistants. This population distribution is shown in Figure 7.1.
7.1  Sampling Distributions
7.2  Sampling Distribution of the Mean
T a b l e  7 . 1
Number of Errors 
Made by Each of 
Four Administrative 
Assistants
Administrative Assistant
Number of Errors
Ann
X1 = 3
Bob
X2 = 2
Carla
X3 = 1
Dave
X4 = 4
F i g u r e  7 . 1
Number of errors made 
by a population of four 
administrative assistants
0
3
2
1
0
1
2
Number of Errors
Frequency
3
4
Learn More
Learn more about the unbi-
ased property of the sample 
mean in the Short Takes for 
Chapter 7.

280	
Chapter 7  Sampling Distributions 
When you have data from a population, you compute the mean by using Equation (7.1), 
and you compute the population standard deviation, s, by using Equation (7.2).
Population Mean
The population mean is the sum of the values in the population divided by the population 
size, N.
	
m =
a
N
i = 1
  Xi
N
	
(7.1)
Population Standard Deviation
	
s = R
a
N
i = 1
  1Xi -  m22
N
	
(7.2)
For the data of Table 7.1,
m = 3 + 2 + 1 + 4
4
= 2.5 errors
and
s = B
13 - 2.522 +  12 - 2.522 +  11 - 2.522 +  14 - 2.522
4
= 1.12 errors
If you select samples of two administrative assistants with replacement from this population, 
there are 16 possible samples 1Nn = 42 = 162. Table 7.2 lists the 16 possible sample out-
comes. If you average all 16 of these sample means, the mean of these values is equal to 2.5, 
which is also the mean of the population, m,
T a b l e  7 . 2
All 16 Samples of 
n = 2 Administrative 
Assistants from a 
Population of N = 4 
Administrative 
Assistants When 
Sampling with 
Replacement
Sample
Administrative Assistants
Sample Outcomes
Sample Mean
1
Ann, Ann
3, 3
X1 = 3
2
Ann, Bob
3, 2
X2 = 2.5
3
Ann, Carla
3, 1
X3 = 2
4
Ann, Dave
3, 4
X4 = 3.5
5
Bob, Ann
2, 3
X5 = 2.5
6
Bob, Bob
2, 2
X6 = 2
7
Bob, Carla
2, 1
X7 = 1.5
8
Bob, Dave
2, 4
X8 = 3
9
Carla, Ann
1, 3
X9 = 2
10
Carla, Bob
1, 2
X10 = 1.5
11
Carla, Carla
1, 1
X11 = 1
12
Carla, Dave
1, 4
X12 = 2.5
13
Dave, Ann
4, 3
X13 = 3.5
14
Dave, Bob
4, 2
X14 = 3
15
Dave, Carla
4, 1
X15 = 2.5
16
Dave, Dave
4, 4
X16 = 4 
mX = 2.5

	
7.2  Sampling Distribution of the Mean	
281
Because the mean of the 16 sample means is equal to the population mean, the sample 
mean is an unbiased estimator of the population mean. Therefore, although you do not know 
how close the sample mean of any particular sample selected is to the population mean, you 
are assured that the mean of all the possible sample means that could have been selected is 
equal to the population mean.
Standard Error of the Mean
Figure 7.2 illustrates the variation in the sample means when selecting all 16 possible samples.
F i g u r e  7 . 2
Sampling distribution of the 
mean, based on all possible 
samples containing two 
administrative assistants
Source: Data are from  
Table 7.2.
0
5
4
3
2
1
0
1
2
Mean Number of Errors
Frequency
3
4
In this small example, although the sample means vary from sample to sample, depending 
on which two administrative assistants are selected, the sample means do not vary as much as 
the individual values in the population. That the sample means are less variable than the indi-
vidual values in the population follows directly from the fact that each sample mean averages 
together all the values in the sample. A population consists of individual outcomes that can 
take on a wide range of values, from extremely small to extremely large. However, if a sample 
contains an extreme value, although this value will have an effect on the sample mean, the 
effect is reduced because the value is averaged with all the other values in the sample. As the 
sample size increases, the effect of a single extreme value becomes smaller because it is aver-
aged with more values.
The value of the standard deviation of all possible sample means, called the standard  
error of the mean, expresses how the sample means vary from sample to sample. As the 
sample size increases, the standard error of the mean decreases by a factor equal to the square 
root of the sample size. Equation (7.3) defines the standard error of the mean when sampling 
with replacement or sampling without replacement from large or infinite populations.
Student Tip
Remember, the  
standard error of the 
mean measures variation 
among the means not 
the individual values.
Standard Error of the Mean
The standard error of the mean, sX, is equal to the standard deviation in the population, s, 
divided by the square root of the sample size, n.
	
sX =
s
2n
	
(7.3)
Example 7.1 computes the standard error of the mean when the sample selected without 
replacement contains less than 5% of the entire population.

282	
Chapter 7  Sampling Distributions 
Sampling from Normally Distributed Populations
Now that the concept of a sampling distribution has been introduced and the standard error of 
the mean has been defined, what distribution will the sample mean, X, follow? If you are sam-
pling from a population that is normally distributed with mean m and standard deviation s, then 
regardless of the sample size, n, the sampling distribution of the mean is normally distributed, 
with mean mX = m, and standard error of the mean sX = s> 2n.
In the simplest case, if you take samples of size n = 1, each possible sample mean is a 
single value from the population because
X =
a
n
i = 1
 Xi
n
= X1
1 = X1
Therefore, if the population is normally distributed, with mean m and standard deviation s, the 
sampling distribution X for samples of n = 1 must also follow the normal distribution, with 
mean mX = m and standard error of the mean sX = s> 11 = s. In addition, as the sample 
size increases, the sampling distribution of the mean still follows a normal distribution, with 
mX = m, but the standard error of the mean decreases so that a larger proportion of sample 
means are closer to the population mean. Figure 7.3 illustrates this reduction in variability. 
Solution  Using Equation (7.3) with n = 25 and s = 15 the standard error of the mean is
sX =
s
2n
=
15
225
= 15
5 = 3
The variation in the sample means for samples of n = 25 is much less than the variation in the 
individual boxes of cereal (i.e., sX = 3, while s = 15).
F i g u r e  7 . 3
Sampling distributions 
of the mean from 
500 samples of sizes 
n = 1, 2, 4, 8, 16, and 32 
selected from a normal 
population
n = 32
n = 16
n = 8
n = 4
0
Z
n = 2
n = 1
Example 7.1
Computing the 
Standard Error of 
the Mean
Returning to the cereal-filling process described in the Using Statistics scenario on page 278, if 
you randomly select a sample of 25 boxes without replacement from the thousands of boxes filled 
during a shift, the sample contains much less than 5% of the population. Given that the standard 
deviation of the cereal-filling process is 15 grams, compute the standard error of the mean.

	
7.2  Sampling Distribution of the Mean	
283
Note that 500 samples of size 1, 2, 4, 8, 16, and 32 were randomly selected from a normally 
distributed population. From the polygons in Figure 7.3, you can see that, although the sam-
pling distribution of the mean is approximately1 normal for each sample size, the sample 
means are distributed more tightly around the population mean as the sample size increases.
To further examine the concept of the sampling distribution of the mean, consider the 
Using Statistics scenario described on page 278. The packaging equipment that is filling  
368-gram boxes of cereal is set so that the amount of cereal in a box is normally distributed, 
with a mean of 368 grams. From past experience, you know the population standard deviation 
for this filling process is 15 grams.
If you randomly select a sample of 25 boxes from the many thousands that are filled in a 
day and the mean weight is computed for this sample, what type of result could you expect? 
For example, do you think that the sample mean could be 368 grams? 200 grams? 365 grams?
The sample acts as a miniature representation of the population, so if the values in the pop-
ulation are normally distributed, the values in the sample should be approximately normally 
distributed. Thus, if the population mean is 368 grams, the sample mean has a good chance of 
being close to 368 grams.
How can you determine the probability that the sample of 25 boxes will have a mean be-
low 365 grams? From the normal distribution (Section 6.2), you know that you can find the 
area below any value X by converting to standardized Z values:
Z = X - m
s
In the examples in Section 6.2, you studied how any single value, X, differs from the popula-
tion mean. Now, in this example, you want to study how a sample mean, X, differs from the 
population mean. Substituting X for X, mX for m, and sX for s in the equation above results in 
Equation (7.4).
Finding Z for the Sampling Distribution of the Mean
The Z value is equal to the difference between the sample mean, X, and the population 
mean, m, divided by the standard error of the mean, sX.
	
Z =
X - mX
sX
=
X - m
s
2n
	
(7.4)
To find the area below 365 grams, from Equation (7.4),
Z = X - mX
sX
= 365 - 368
15
225
= -3
3
= -1.00
The area corresponding to Z = -1.00 in Table E.2 is 0.1587. Therefore, 15.87% of all the 
possible samples of 25 boxes have a sample mean below 365 grams.
The preceding statement is not the same as saying that a certain percentage of individual 
boxes will contain less than 365 grams of cereal. You compute that percentage as follows:
Z = X - m
s
= 365 - 368
15
= -3
15 = -0.20
The area corresponding to Z = -0.20 in Table E.2 is 0.4207. Therefore, 42.07% of the individual 
boxes are expected to contain less than 365 grams. Comparing these results, you see that many 
more individual boxes than sample means are below 365 grams. This result is explained by the 
fact that each sample consists of 25 different values, some small and some large. The averaging 
1Remember that “only” 500 samples 
out of an infinite number of samples 
have been selected, so that the sam-
pling distributions shown are only 
approximations of the population 
distribution.

284	
Chapter 7  Sampling Distributions 
Sometimes you need to find the interval that contains a specific proportion of the sample 
means. To do so, you determine a distance below and above the population mean containing a 
specific area of the normal curve. From Equation (7.4) on page 283,
Z = X - m
s
2n
Solving for X results in Equation (7.5).
Example 7.2
The Effect of  
Sample Size, n, on 
the Computation 
of sX
How is the standard error of the mean affected by increasing the sample size from 25 to 100 
boxes?
Solution  If n = 100 boxes, then using Equation (7.3) on page 281,
sX =
s
2n
=
15
2100
= 15
10 = 1.5
The fourfold increase in the sample size from 25 to 100 reduces the standard error of the mean 
by half—from 3 grams to 1.5 grams. This demonstrates that taking a larger sample results in 
less variability in the sample means from sample to sample.
Example 7.3
The Effect of  
Sample Size, n,  
on the Clustering 
of Means in  
the Sampling  
Distribution
If you select a sample of 100 boxes, what is the probability that the sample mean is below 365 
grams?
Solution  Using Equation (7.4) on page 283,
Z =
X - mX
sX
=
365 - 368
15
2100
=
-3
1.5 = -2.00
From Table E.2, the area less than Z = -2.00 is 0.0228. Therefore, 2.28% of the samples of 
100 boxes have means below 365 grams, as compared with 15.87% for samples of 25 boxes.
Finding X for the Sampling Distribution of the Mean
	
X = m + Z s
2n
	
(7.5)
Example 7.4 illustrates the use of Equation (7.5).
process dilutes the importance of any individual value, particularly when the sample size is large. 
Therefore, the chance that the sample mean of 25 boxes is very different from the population mean 
is less than the chance that a single box is very different from the population mean.
Examples 7.2 and 7.3 show how these results are affected by using different sample sizes.

	
7.2  Sampling Distribution of the Mean	
285
Sampling from Non-normally Distributed Populations— 
The Central Limit Theorem
So far in this section, only the sampling distribution of the mean for a normally distributed 
population has been considered. However, for many analyses, you will either be able to know 
that the population is not normally distributed or conclude that it would be unrealistic to  
assume that the population is normally distributed. An important theorem in statistics, the 
Central Limit Theorem, deals with these situations.
The Central Limit Theorem
As the sample size (the number of values in each sample) gets large enough, the sampling 
distribution of the mean is approximately normally distributed. This is true regardless of 
the shape of the distribution of the individual values in the population.
What sample size is large enough? A great deal of statistical research has gone into this 
issue. As a general rule, statisticians have found that for many population distributions, when 
the sample size is at least 30, the sampling distribution of the mean is approximately normal. 
However, you can apply the Central Limit Theorem for even smaller sample sizes if the popu-
lation distribution is approximately bell-shaped. In the case in which the distribution of a vari-
able is extremely skewed or has more than one mode, you may need sample sizes larger than 
30 to ensure normality in the sampling distribution of the mean.
Figure 7.4 shows the sampling distributions from three different continuous distributions 
(normal, uniform, and exponential) for varying sample sizes (n = 2, 5, and 30) and illustrates 
the application of the Central Limit Theorem to these different populations. In each of the pan-
els, because the sample mean is an unbiased estimator of the population mean, the mean of any 
sampling distribution is always equal to the mean of the population.
Figure 7.4 Panel A shows the sampling distribution of the mean selected from a normal 
population. As mentioned earlier in this section, when the population is normally distributed, 
the sampling distribution of the mean is normally distributed for any sample size. [You can 
measure the variability by using the standard error of the mean, Equation (7.3), on page 281.] 
Observe that for each of the sample sizes illustrated, the shape of the sampling distribution is a 
bell-shaped normal distribution. As the sample size increases from 2 to 30, you can see that the 
variation becomes smaller, resulting in a distribution that has less variation around the mean.
Example 7.4
Determining the  
Interval That  
Includes a Fixed 
Proportion of the 
Sample Means
In the cereal-filling example, find an interval symmetrically distributed around the population 
mean that will include 95% of the sample means, based on samples of 25 boxes.
Solution  If 95% of the sample means are in the interval, then 5% are outside the interval. 
Divide the 5% into two equal parts of 2.5%. The value of Z in Table E.2 corresponding to an 
area of 0.0250 in the lower tail of the normal curve is -1.96, and the value of Z corresponding 
to a cumulative area of 0.9750 (i.e., 0.0250 in the upper tail of the normal curve) is +1.96.
The lower value of X (called XL) and the upper value of X (called XU) are found by using 
Equation (7.5):
 XL = 368 + 1-1.962 15
225
= 368 - 5.88 = 362.12
 XU = 368 + 11.962 15
225
= 368 + 5.88 = 373.88
Therefore, 95% of all sample means, based on samples of 25 boxes, are between 362.12 and 
373.88 grams.

286	
Chapter 7  Sampling Distributions 
Figure 7.4 Panel B depicts the sampling distribution from a population with a uniform (or 
rectangular) distribution (see Section 6.4). When samples of size n = 2 are selected, there is 
a peaking, or central limiting, effect already working, resulting in a sampling distribution that 
looks like a triangle. For n = 5, the sampling distribution is bell-shaped and approximately 
normal. When n = 30, the sampling distribution looks very similar to a normal distribution. 
In general, the larger the sample size, the more closely the sampling distribution will follow a 
normal distribution. As with all other cases, the mean of each sampling distribution is equal to 
the mean of the population, and the variability decreases as the sample size increases.
Figure 7.4 Panel C presents an exponential distribution (see Section 6.5). This population 
is extremely right-skewed. When n = 2, the sampling distribution is still highly right-skewed 
but less so than the distribution of the population. For n = 5, the sampling distribution is 
slightly right-skewed. When n = 30, the sampling distribution looks approximately normal. 
Again, the mean of each sampling distribution is equal to the mean of the population, and the 
variability decreases as the sample size increases.
Using the results from the normal, uniform, and exponential distributions, you can reach 
the following conclusions regarding the Central Limit Theorem:
 • For most distributions, regardless of the shape of the population, the sampling distribu-
tion of the mean is approximately normally distributed if samples of at least size 30 are 
selected.
 • If the distribution of the population is fairly symmetrical, the sampling distribution of 
the mean is approximately normal for samples as small as size 5.
 • If the population is normally distributed, the sampling distribution of the mean is nor-
mally distributed, regardless of the sample size.
F i g u r e  7 . 4
Sampling distribution of 
the mean for different 
populations for samples 
of n = 2, 5 and 30
Population
Values of X
Values of X
Values of X
Values of X
Panel A
Normal Population
Panel B
Uniform Population
Panel C
Exponential Population
Values of X
Values of X
Values of X
Values of X
Values of X
Values of X
Values of X
Values of X
Population
Population
Sampling
Distribution of X
Sampling
Distribution of X
Sampling
Distribution of X
Sampling
Distribution of X
Sampling
Distribution of X
Sampling
Distribution of X
Sampling
Distribution of X
n = 2
n = 5
n = 30
n = 30
n = 30
n = 5
n = 5
Sampling
Distribution of X
n = 2
Sampling
Distribution of X
n = 2

	
7.2  Sampling Distribution of the Mean	
287
Example 7.5
Constructing a 
Sampling  
Distribution for  
a Skewed  
Population
Figure 7.5 shows the distribution of the time it takes to fill orders at a fast-food chain drive-
through lane. Note that the probability distribution table is unlike Table 7.1 (page 279), which 
presents a population in which each value is equally likely to occur.
F i g u r e  7 . 5
Probability distribution of the service time (in minutes) at a fast-food chain drive-through 
lane and a histogram of that probability distribution
Service Time 
(minutes)
Probability
1
0.10
2
0.40
3
0.20
4
0.15
5
0.10
6
0.05
Using Equation 5.1 on page 215, the population mean is computed as 2.9 minutes. Using 
Equation (5.3) on page 216, the population standard deviation is computed as 1.34. Select 100 
samples of n = 2, n = 15, and n = 30. What conclusions can you reach about the sampling 
distribution of the service time (in minutes) at the fast-food chain drive-through lane?
Solution  Table 7.3 represents the mean service time (in minutes) at the fast-food chain 
drive-through lane for 100 different random samples of n = 2. The mean of these 100 sample 
means is 2.825 minutes, and the standard error of the mean is 0.883.
T a b l e  7 . 3
Mean Service Times 
(in minutes) at a Fast-
Food Chain Drive-
Through Lane for 100 
Different Random 
Samples of n = 2
3.5
2.5
3
3.5
4
3
2.5
2
2
2.5
3
3
2.5
2.5
2
2.5
2.5
2
3.5
1.5
2
3
2.5
3
3
2
3.5
3.5
2.5
2
4.5
3.5
4
2
2
4
3.5
2.5
2.5
3.5
3.5
3.5
2
1.5
2.5
2
3.5
3.5
2.5
2.5
2.5
3
3
3.5
2
3.5
2
1.5
5.5
2.5
3.5
3
3
2
1.5
3
2.5
2.5
2.5
2.5
3.5
1.5
6
2
1.5
2.5
3.5
2
3.5
5
2.5
3.5
4.5
3.5
3.5
2
4
2
3
3
4.5
1.5
2.5
2
2.5
2.5
2
2
2
4
The Central Limit Theorem is of crucial importance in using statistical inference to 
reach conclusions about a population. It allows you to make inferences about the population 
mean without having to know the specific shape of the population distribution. ­Example 7.5 
illustrates a sampling distribution for a skewed population.
Table 7.4 represents the mean service time (in minutes) at the fast-food chain drive-through 
lane for 100 different random samples of n = 15. The mean of these 100 sample means is 
2.9313 minutes, and the standard error of the mean is 0.3458.
Table 7.5 represents the mean service time (in minutes) at the fast-food chain drive-through 
lane for 100 different random samples of n = 30. The mean of these 100 sample means is 
2.9527 minutes, and the standard error of the mean is 0.2701.
(continued)

288	
Chapter 7  Sampling Distributions 
T a b l e  7 . 5
Mean Service Times (in minutes) at a Fast-Food Chain Drive-Through Lane for 100 Different Random Samples of n = 30
3.0000
3.3667
3.0000
3.1333
2.8667
2.8333
3.2667
2.9000
2.7000
3.2000
3.2333
2.7667
3.2333
2.8000
3.4000
3.0333
2.8667
3.0000
3.1333
3.4000
2.3000
3.0000
3.0667
2.9667
3.0333
2.4000
2.8667
2.8000
2.5000
2.7000
2.7000
2.9000
2.8333
3.3000
3.1333
2.8667
2.6667
2.6000
3.2333
2.8667
2.7667
2.9333
2.5667
2.5333
3.0333
3.2333
3.0667
2.9667
2.4000
3.3000
2.8000
3.0667
3.2000
2.9667
2.9667
3.2333
3.3667
2.9000
3.0333
3.1333
3.3333
2.8667
2.8333
3.0667
3.3667
3.0667
3.0667
3.2000
3.1667
3.3667
3.0333
3.1667
2.4667
3.0000
2.6333
2.6667
2.9667
3.1333
2.8000
2.8333
2.9333
2.7000
3.0333
2.7333
2.6667
2.6333
3.1333
3.0667
2.5333
3.3333
3.1000
2.5667
2.9000
3.9333
2.9000
2.7000
2.7333
2.8000
2.6667
2.8333
Figure 7.6 Panels A through C show histograms of the mean service time (in minutes) 
at the fast-food chain drive-through lane for the three sets of 100 different random samples 
shown in Tables 7.3 through 7.5. Panel A, the histogram for the mean service time for 100 dif-
ferent random samples of n = 2, shows a skewed distribution, but a distribution that is not as 
skewed as the population distribution of service times shown in Figure 7.5.  
Panel B, the histogram for the mean service time for 100 different random samples of 
n = 15, shows a somewhat symmetrical distribution that contains a concentration of values in 
the center of the distribution. Panel C, the histogram for the mean service time for 100 different  
F i g u r e  7 . 6
Histograms of the mean service time (in minutes) at the fast-food chain drive-through lane of 100 different random 
samples of n = 2 (Panel A, left) and 100 different random samples of n = 15 (Panel B, right)
T a b l e  7 . 4
Mean Service Times (in minutes) at a Fast-Food Chain Drive-Through Lane for 100 Different Random Samples of n = 15
3.5333
2.8667
3.1333
3.6000
2.5333
2.8000
2.8667
3.1333
3.2667
3.3333
3.0000
3.3333
2.7333
2.6000
2.8667
3.0667
2.1333
2.5333
2.8000
3.1333
2.8000
2.7333
2.6000
3.1333
2.8667
3.4667
2.9333
2.8000
2.2000
3.0000
2.9333
2.6000
2.6000
3.1333
3.1333
3.1333
2.5333
3.0667
3.9333
2.8000
3.0000
2.7333
2.6000
2.4667
3.2000
2.4667
3.2000
2.9333
2.8667
3.4667
2.6667
3.0000
3.1333
3.1333
2.7333
2.7333
3.3333
3.4000
3.2000
3.0000
3.2000
3.0000
2.6000
2.9333
3.0667
2.8667
2.2667
2.5333
2.7333
2.2667
2.8000
2.8000
2.6000
3.1333
2.9333
3.0667
3.6667
2.6667
2.8667
2.6667
3.0000
3.4000
2.7333
3.6000
2.6000
2.7333
3.3333
2.6000
2.8667
2.8000
3.7333
2.9333
3.0667
2.6667
2.8667
2.2667
2.7333
2.8667
3.5333
3.2000

	
7.2  Sampling Distribution of the Mean	
289
random samples of n = 30, shows a distribution that appears to be approximately bell-shaped  
with a concentration of values in the center of the distribution. The progression of the  
histograms from a skewed population towards a bell-shaped distribution as the sample size  
increases is consistent with the Central Limit Theorem.
Open the VE-Sampling Distribution add-in workbook to ob-
serve the effects of simulated rolls on the frequency distribution of 
the sum of two dice. (For Excel technical requirements, review Ap-
pendix Section D.4) When this workbook opens properly, it adds a 
Sampling Distribution menu to the Add-ins tab (Apple menu in Excel 
2011).
To observe the effects of simulated throws on the frequency dis-
tribution of the sum of the two dice, select Sampling Distribution 
➔ Two Dice Simulation. In the Sampling Distribution dialog box, 
enter the Number of rolls per tally and click Tally. Click Finish 
when done.
V i s u a l  E x p l o r at i o n s   Exploring Sampling Distributions
Problems for Section 7.2
Learning the Basics
7.1  A quality control officer heads a department that manufac-
tures 1,000,000 units per annum.  Answer the following:
a.	 Is it feasible for him to check all units produced or to check a 
sample of 1,000 units?
b.	 State the possible benefits the officer will avail by testing a 
sample rather than the entire population.
7.2  Comment on the normality of the sampling distribution of 
mean when the 
a.	 sample size is 5
b.	 sample size is 2
c.	 sample size is 30
d.	 sample size is 100
Do so for the following scenarios: population is normally distrib-
uted, fairly symmetrical and not normally distributed.
F i g u r e  7 . 6
(continued)
Panel C: Histogram of 
the mean service time (in 
minutes) at the fast-food 
chain drive-through lane 
of 100 different random 
samples of n = 30

290	
Chapter 7  Sampling Distributions 
Applying the Concepts
7.3  Two researchers were given the task of analyzing the demo-
graphic characteristics of job fair participants. The first researcher 
collected a sample of 100, and the second researcher collected a 
sample of 500.
a.	 Which researcher is expected to get a higher standard deviation 
for the income of the participants?
b.	 Which researcher is expected to get a lower standard error of 
mean for the income of the participants?
7.4  The following data represent the number of days absent per 
year in a population of six employees of a small company:
1 3 6 7 9 10
a.	 Assuming that you sample without replacement, select all pos-
sible samples of n = 2 and construct the sampling distribution 
of the mean. Compute the mean of all the sample means and 
also compute the population mean. Are they equal? What is this 
property called?
b.	 Repeat (a) for all possible samples of n = 3.
c.	 Compare the shape of the sampling distribution of the mean in 
(a) and (b). Which sampling distribution has less variability? 
Why?
d.	 Assuming that you sample with replacement, repeat (a) through 
(c) and compare the results. Which sampling distributions have 
the least variability—those in (a) or (b)? Why?
7.5  Research says that adult humans should drink 2 liters of  
water every day. However, the standard deviation is estimated at 
0.7 liters. An event manager arranged 110 liters of mineral water 
for a one-day conference to be attended by 50 people. 
a.	 Present the distribution of population and the sample on a 
graph.
b.	 Calculate the sample mean and standard error of mean.
c.	 Calculate the probability that the event manager has sufficient 
water during the conference day.
d.	 Calculate the probability that the event manager will have to 
order more water during the conference.
7.6  Assume that in a research report, the average weights of the 
airline passengers has been ascertained as 75kg, which includes the 
baggage per passenger as well. Note that the average weight presents 
non-normal distribution because the passengers are women, men 
and children. Assume that the standard deviation is 20kg.
a.	 Comment on the distribution of the sample mean if a commuter 
plane carries 5 passengers.
b.	 Comment on the distribution of the sample mean if a commuter 
plane carries 35 passengers.
c.	 Calculate the probability that the total weight of plane exceeds 
7,000kg when 100 passengers are travelling.
d.	 Calculate the probability that the total weight of plane is less 
that 7,800kg when 100 passengers are travelling.
7.7  The mean diameter of the rim of Honda tires is 16 inches 
(Source: http://www.goodyear.com/en-US/tires/new-tires). 
Assume that the standard deviation of diameter of the rims is 
0.3inches. For quality control purposes, the diameter of the rims of 
9 tires is measured every hour. The manager applies the rule that 
if the diameter of a rim is greater or equal to 16.25, and lesser or 
equal to 15.75, the manufacturing should be stopped. If the diam-
eter is between 15.75 and 16.25, the manufacturing process is not 
to be disturbed. 
a.	 Calculate the probability of stopping the manufacturing when 
the sample mean is 16 inches.
b.	 Calculate the probability of stopping the manufacturing in case 
the mean is shifted to 16.05 inches.
c.	 Calculate the probability of not disturbing the manufacturing if 
mean shifts to 16.25 inches.
SELF 
Test 
7.8  Today, full-time college students report spending a 
mean of 27 hours per week on academic activities, both 
inside and outside the classroom. (Source: “A Challenge to Col-
lege Students for 2013: Don’t Waste Your 6,570,” Huffington Post, 
January 29, 2013, huff.to/13dNtuT.) Assume the standard devia-
tion of time spent on academic activities is 4 hours. If you select a 
random sample of 16 full-time college students,
a.	 what is the probability that the mean time spent on academic 
activities is at least 26 hours per week?
b.	 there is an 85% chance that the sample mean is less than how 
many hours per week?
c.	 What assumption must you make in order to solve (a) and (b)?
d.	 If you select a random sample of 64 full-time college students, 
there is an 85% chance that the sample mean is less than how 
many hours per week?
7.3  Sampling Distribution of the Proportion
Consider a categorical variable that has only two categories, such as the customer prefers your 
brand or the customer prefers the competitor’s brand. You are interested in the proportion of 
items belonging to one of the categories—for example, the proportion of customers that prefer 
your brand. The population proportion, represented by p, is the proportion of items in the en-
tire population with the characteristic of interest. The sample proportion, represented by p, is 
the proportion of items in the sample with the characteristic of interest. The sample proportion, 
a statistic, is used to estimate the population proportion, a parameter. To calculate the sample 
proportion, you assign one of two possible values, 1 or 0, to represent the presence or absence 
of the characteristic. You then sum all the 1 and 0 values and divide by n, the sample size. For 
example, if, in a sample of five customers, three preferred your brand and two did not, you 
have three 1s and two 0s. Summing the three 1s and two 0s and dividing by the sample size of 
5 results in a sample proportion of 0.60.
Student Tip
Do not confuse this use 
of the Greek letter pi, p, 
to represent the popula-
tion proportion with the 
mathematical constant 
that is the ratio of the 
circumference to a di-
ameter of a circle—ap-
proximately 3.14159—
which is also known by 
the same Greek letter.

	
7.3  Sampling Distribution of the Proportion	
291
The sample proportion, p, will be between 0 and 1. If all items have the characteristic, you 
assign each a score of 1, and p is equal to 1. If half the items have the characteristic, you assign 
half a score of 1 and assign the other half a score of 0, and p is equal to 0.5. If none of the items 
have the characteristic, you assign each a score of 0, and p is equal to 0.
In Section 7.2, you learned that the sample mean, X, is an unbiased estimator of the popula-
tion mean, m. Similarly, the statistic p is an unbiased estimator of the population proportion, p.
By analogy to the sampling distribution of the mean, whose standard error is sX =
s
2n
, the 
standard error of the proportion, sp, is given in Equation (7.7).
Standard Error of the Proportion
	
sp = B
p11 - p2
n
	
(7.7)
The sampling distribution of the proportion follows the binomial distribution, as dis-
cussed in Section 5.3, when sampling with replacement (or without replacement from extremely 
large populations). However, you can use the normal distribution to approximate the binomial 
distribution when np and n11 - p2 are each at least 5. In most cases in which inferences are 
made about the population proportion, the sample size is substantial enough to meet the condi-
tions for using the normal approximation (see reference 1). Therefore, in many instances, you 
can use the normal distribution to estimate the sampling distribution of the proportion.
Substituting p for X, p for m, and B
p11 - p2
n
 for s
2n
 in Equation (7.4) on page 283 
results in Equation (7.8).
Sample Proportion
	
p = X
n = Number of items having the characteristic of interest
Sample size
	
(7.6)
Student Tip
Remember that the 
sample proportion  
cannot be negative and 
also cannot be greater 
than 1.0.
To illustrate the sampling distribution of the proportion, a recent survey (“Can You Stop 
Thinking About Work on Your Vacation?” USA Today Snapshots, October 5, 2011, p. 1A)  
reported that 32% of adults are unable to stop thinking about work while on vacation. Suppose 
that you select a random sample of 200 vacationers who have booked tours from a certain tour 
company, and you want to determine the probability that more than 40% of the vacationers are 
unable to stop thinking about work while on vacation. Because np = 20010.322 = 64 7 5 
and n11 - p2 = 20011 - 0.322 = 136 7 5, the sample size is large enough to assume that 
the sampling distribution of the proportion is approximately normally distributed. Then, using 
Finding Z for the Sampling Distribution of the Proportion
	
Z =
p - p
B
p11 - p2
n
	
(7.8)

292	
Chapter 7  Sampling Distributions 
Problems for Section 7.3
Learning the Basics
7.9  A testing company pre-tests a new drink. Of the 100 people 
who tasted the drink, 65  liked it. 
a.	 Can you assume that the sampling distribution of the propor-
tion is approximately normal?
b.	 Calculate ‘p’ for the sample proportion who liked the drink.
7.10  A random sample of 50 households was selected for a phone 
(landline and cellphone) survey. The key question asked was, “Do 
you or any member of your household own an Apple product 
(iPhone, iPod, iPad, or Mac computer)?” Of the 50 respondents, 
20 said yes and 30 said no.
a.	 Determine the sample proportion, p, of households that own an 
Apple product.
b.	 If the population proportion is 0.45, determine the standard er-
ror of the proportion.
7.11  The following table represents the scores of students in Math-
ematics in the 10th grade. The dean of the school has announced a 
scholarship for students who score more than eighty percent.
75  40  55  43  67  87  64  98  69  56 
45  93  41  89  65  79  76  90  70  71 
a.	 Determine the sample proportion ‘p’ of the students who 
scored more than eighty percent.
b.	 If the population proportion is 0.20, determine the standard  
error of the proportion.
Applying the Concepts
SELF 
Test 
7.12  A political pollster is conducting an analysis of 
sample results in order to make predictions on election 
night. Assuming a two-candidate election, if a specific candidate 
receives at least 55% of the vote in the sample, that candidate will 
be forecast as the winner of the election. If you select a random 
sample of 100 voters, what is the probability that a candidate will 
be forecast as the winner when
a.	 the population percentage of her vote is 50.1%?
b.	 the population percentage of her vote is 60%?
c.	 the population percentage of her vote is 49% (and she will ac-
tually lose the election)?
d.	 If the sample size is increased to 400, what are your answers to 
(a) through (c)? Discuss.
7.13  A tourism magazine surveyed 5,000 people for their ex-
pected travel plans in the following year. The response rate of the 
survey is seventy-five per cent. Forty-five per cent of the respon-
dents of the survey agreed that they already have plans for their 
visits in the following year, while the remaining respondents said 
that they like to plan spontaneously. Assume that while preparing 
a research report for your hospitality class you conduct a similar 
survey and select a sample of 40 people.
a.	 Calculate the probability that the proportion that plans their 
visit in advance lies between 43% and 47%.
b.	 Calculate the probability that proportion that plans their visit in 
advance is less than 60%.
c.	 Determine the confidence interval that the proportion mean is 
60% given the 90% level of confidence.
d.	 Determine the confidence interval that the proportion mean is 
45% given the 90% level of confidence.
7.14  Accenture’s Defining Success global research study found 
that the majority of today’s working women would prefer a bet-
ter work–life balance to an increased salary. One of the most im-
portant contributors to work–life balance identified by the survey 
was “flexibility,” with 80% of women saying that having a flexible 
work schedule is either very important or extremely important to 
their career success. (Source: bit.ly/17IM8gq.) Suppose you se-
lect a sample of 100 working women.
a.	 What is the probability that in the sample fewer than 85% say 
that having a flexible work schedule is either very important or 
extremely important to their career success?
b.	 What is the probability that in the sample between 75% and 
85% say that having a flexible work schedule is either very im-
portant or extremely important to their career success?
the survey percentage of 32% as the population proportion, you can calculate the probability 
that more than 40% of the vacationers are unable to stop thinking about work while on vaca-
tion by using Equation (7.8):
 Z =
p - p
B
p11 - p2
n
 =
0.40 - 0.32
B
10.32210.682
200
=
0.08
B
0.2176
200
=
0.08
0.0330
 = 2.42
Using Table E.2, the area under the normal curve greater than 2.42 is 0.0078. There-
fore, if the population proportion is 0.32, the probability is 0.78% that more than 40% of the  
200 vacationers in the sample will be unable to stop thinking about work while on vacation.

	
7.4  Sampling from Finite Populations	
293
c.	 What is the probability that in the sample more than 82% say 
that having a flexible work schedule is either very important or 
extremely important to their career success?
d.	 If a sample of 400 is taken, how does this change your answers 
to (a) through (c)?
7.15  There are signs of recovery in the US job market. Accord-
ing to a Bloomberg report (Source: http://www.bloomberg. 
com/news/2014-03-07/payrolls-in-u-s-rise-more-than-forecast- 
jobless-rate-climbs.html), more workers have been employed in 
the beginning of 2014. The jobless rate is at 6.7% and the hourly 
wage rate has also witnessed a minor increment of 0.4 percent. 
a.	 Assume you have selected a sample of 100 workers. Assuming 
that the population proportion of the jobless rate is 6.7%, what 
is the probability that the jobless rate remains below 7% for the 
given sample of workers?
b.	 Assume that the sample that you selected above increases from 
100 workers to 500 workers. Considering the population propor-
tion of the jobless rate is 6.7%, calculate the probability that the 
jobless rate remains below 7% for the given sample of workers.
c.	 Assume that the sample that you selected above decreases to 
50 workers. Considering the same population proportion of the 
jobless rate is 6.7%, calculate the probability that the jobless 
rate remains below 7% for the given sample of workers.
d.	 Compare the results that you derived in part a, b and c of this 
question. What conclusion can you draw?
e.	 Assuming the sample size of 100 workers, what is the prob-
ability that the hourly wage increment is 0.5 percent, assuming 
a population proportion of 0.4.
7.16  According to GMI Ratings’ 2013 Women on Boards Report, 
the percentage of women on U.S. boards has increased marginally 
in 2009–2012 and now stands at 14%, well below the values for 
Nordic countries and France. A number of initiatives are underway 
in an effort to increase the representation. For example, a network 
of investors, corporate leaders, and other advocates, known as the 
30% coalition, is seeking to raise the proportion of female direc-
tors to that number (30%) by 2015. This study also reports that 
15% of U.S. companies have three or more female board directors. 
(Data extracted from bit.ly/13oSFem.) If you select a random 
sample of 200 U.S. companies,
a.	 what is the probability that the sample will have between 12% 
and 18% U.S. companies that have three or more female board 
directors?
b.	 the probability is 90% that the sample percentage of U.S. com-
panies having three or more female board directors will be 
contained within what symmetrical limits of the population 
percentage?
c.	 the probability is 95% that the sample percentage of U.S. com-
panies having three or more female board directors will be 
contained within what symmetrical limits of the population 
percentage?
7.17  The Chartered Financial Analyst (CFA) institute reported 
that 51% of its U.S. members indicate that lack of ethical culture 
within financial firms has contributed the most to the lack of trust 
in the financial industry. (Source: Data extracted from Global 
Market Sentiment Survey 2013, cfa.is/YqVCKB.) Suppose that 
you select a sample of 100 CFA members.
a.	 What is the probability that the sample percentage indicating 
that lack of ethical culture within financial firms has contrib-
uted the most to the lack of trust in the financial industry will 
be between 50% and 55%?
b.	 The probability is 90% that the sample percentage will be con-
tained within what symmetrical limits of the population per-
centage?
c.	 The probability is 95% that the sample percentage will be con-
tained within what symmetrical limits of the population per-
centage?
d.	 Suppose you selected a sample of 400 CFA members. How 
does this change your answers in (a) through (c)?
7.18  A Pew Research Center project on the state of news media 
showed that the clearest pattern of news audience growth in 2012 
came on digital platforms. According to Pew Research data, 39% 
of Americans get news online or from a mobile device in a typi-
cal day. (Data extracted from “Key Findings: State of the News  
Media,” Pew Research Center, bit.ly/10kKUTi.)
a.	 Suppose that you take a sample of 100 Americans. If the popu-
lation proportion of Americans who get news online or from a 
mobile device in a typical day is 0.39, what is the probability 
that fewer than 30% in your sample will get news online or 
from a mobile device in a typical day?
b.	 Suppose that you take a sample of 400 Americans. If the popu-
lation proportion of Americans who get news online or from a 
mobile device in a typical day is 0.39, what is the probability 
that fewer than 30% in your sample will get news online or 
from a mobile device in a typical day?
c.	 Discuss the effect of sample size on the sampling distribution 
of the proportion in general and the effect on the probabilities 
in (a) and (b).
7.4  Sampling from Finite Populations
The Central Limit Theorem and the standard errors of the mean and of the proportion are 
based on samples selected with replacement. However, in nearly all survey research, you sam-
ple without replacement from populations that are of a finite size, N. The Section 7.4 online 
topic explains how you use a finite population correction factor to compute the standard er-
ror of the mean and the standard error of the proportion for such samples.

294	
Chapter 7  Sampling Distributions 
S u m m a r y
You studied the sampling distribution of the sample mean 
and the sampling distribution of the sample proportion and 
their relationship to the Central Limit Theorem. You learned 
that the sample mean is an unbiased estimator of the popula-
tion mean, and the sample proportion is an unbiased estima-
tor of the population proportion. In the next five chapters, 
the techniques of confidence intervals and tests of hypoth-
eses commonly used for statistical inference are discussed.
Referen c e s
	 1.	Cochran, W. G. Sampling Techniques, 3rd ed. New York:  
Wiley, 1977.
	 2.	Microsoft Excel 2013. Redmond, WA: Microsoft Corp., 2012.
	 3.	Minitab Release 16. State College, PA: Minitab, Inc., 2010.
K e y  Eq u at i o n s
Population Mean
m =
a
N
i = 1
 Xi
N
	
(7.1)
Population Standard Deviation
s = R
a
N
i = 1
1Xi -  m22
N
	
(7.2)
Standard Error of the Mean
sX =
s
2n
	
(7.3)
Finding Z for the Sampling Distribution of the Mean
Z =
X - mX
sX
=
X - m
s
2n
	
(7.4)
Finding X for the Sampling Distribution of the Mean
X = m + Z s
2n
	
(7.5)
Sample Proportion
p = X
n	
(7.6)
A
s the plant operations manager for Oxford Cereals, 
you were responsible for monitoring the amount of 
cereal placed in each box. To be consistent with package 
labeling, boxes should contain a mean of 368 grams of ce-
real. Thousands of boxes are produced during a shift, and 
weighing every single box was determined to be too time-
consuming, costly, and inefficient. Instead, a sample of 
boxes was selected. Based on your analysis of the sample, 
you had to decide whether to maintain, alter, or shut down 
the process.
Using the concept of the sampling distribution of the 
mean, you were able to determine probabilities that such a 
sample mean could have been randomly selected from a pop-
ulation with a mean of 368 grams. Specifically, if a sample of 
size n = 25 is 
selected from a 
population with 
a mean of 368 
and standard 
deviation of 15,  
you calculated the probability of selecting a sample with a 
mean of 365 grams or less to be 15.87%. If a larger sample 
size is selected, the sample mean should be closer to the popu-
lation mean. This result was illustrated when you calculated 
the probability if the sample size were increased to n = 100. 
Using the larger sample size, you determined the probability  
of selecting a sample with a mean of 365 grams or less to  
be 2.28%.
U s i n g  S tat i s t i c s
Sampling Oxford Cereals, Revisited
Corbis

	
Chapter Review Problems	
295
Standard Error of the Proportion
sp = B
p11 - p2
n
	
(7.7)
Finding Z for the Sampling Distribution of the Proportion
Z =
p - p
B
p11 - p2
n
	
(7.8)
K ey Te r ms
Central Limit Theorem  285
sampling distribution  279
sampling distribution of the mean  279
sampling distribution of the  
proportion  291
standard error of the mean  281
standard error of the proportion  291
unbiased  279
C h ec ki n g  Yo ur  U n de r s ta nding
7.19  Explain the meaning of the terms ‘statistic’ and ‘parameter’. 
Are they interrelated? How?
7.20  Comment on the variability of the mean of more observa-
tions from the population instead of one observation.
7.21  The duration of the meetings in a firm presents skewed dis-
tribution. Using Central Limit Theorem, comment on the distribu-
tion of the mean of a large sample of the meetings.
7.22  Why do you think investigating a sample is more efficient 
for statistical control than investigating an entire population?
7.23  What sample size is large enough to assume that the sam-
pling distribution of the proportion is normally distributed?
C h a pte r  R e vi e w P r ob le ms
7.24  According to a survey, the average number of 9 ounce coffee 
cups consumed daily is 3.1 (Source: http://www.statisticbrain.
com/coffee-drinking-statistics/). Assume that the standard devia-
tion is 0.5 cup. A researcher surveys the daily coffee consumption 
habits in the state of Washington D.C. He draws a sample of 36. 
Calculate the probability that the sample mean will be
a.	 between 3 and 3.01?
b.	 greater than 3.2?
c.	 less than 3?
d.	 Calculate the range of the sample average mean for the middle 
95% area within which the sample means will lie.
e.	 If the researcher is willing to limit the standard error to 1.5% of 
population mean, is the sample size large enough?
f.	 If the researcher is willing to limit the standard error to 3% of 
population mean, then what should be the sample size?
7.25  The annual average return of S&P 500 from 1950-2002 was 
calculated as 9.7% with the standard deviation of 15.5% (Source: 
Hirshey, 2003). A person, who is going to retire in 40 years, wants 
to invest in stocks.
a.	 Assuming that the above annual returns have been impacted by 
extreme values, comment on the normality of the stock return 
over the next 49 years.
b.	 What is the probability that the mean return will be more than 
15%?
c.	 What is the probability that the mean return is less that 5%?
7.26  An orange juice producer buys oranges from a large orange 
grove that has one variety of orange. The amount of juice squeezed 
from these oranges is approximately normally distributed, with a 
mean of 4.70 ounces and a standard deviation of 0.40 ounce. Sup-
pose that you select a sample of 25 oranges.
a.	 What is the probability that the sample mean amount of juice 
will be at least 4.60 ounces?
b.	 The probability is 70% that the sample mean amount of juice 
will be contained between what two values symmetrically dis-
tributed around the population mean?
c.	 The probability is 77% that the sample mean amount of juice 
will be greater than what value?
7.27  In Problem 7.26, suppose that the mean amount of juice 
squeezed is 5.0 ounces.
a.	 What is the probability that the sample mean amount of juice 
will be at least 4.60 ounces?
b.	 The probability is 70% that the sample mean amount of juice 
will be contained between what two values symmetrically dis-
tributed around the population mean?
c.	 The probability is 77% that the sample mean amount of juice 
will be greater than what value?
7.28  The stock market in Mexico reported strong returns in 
2012. The population of stocks earned a mean return of 17.87% 
in 2012. (Data extracted from asxiq.com/blog/stock-markets-in-
the-world-returns-in-2012/.) Assume that the returns for stocks 
on the Mexican stock market were distributed as a normal random 

296	
Chapter 7  Sampling Distributions 
variable, with a mean of 17.87 and a standard deviation of 20. If 
you selected a random sample of 16 stocks from this population, 
what is the probability that the sample would have a mean return
a.	 less than 0 (i.e., a loss)?
b.	 between -10 and 10?
c.	 greater than 10?
7.29  The article mentioned in Problem 7.28 reported that the 
stock market in China had a mean return of 1.54% in 2012. As-
sume that the returns for stocks on the Chinese stock market were 
distributed as a normal random variable, with a mean of 1.54 and 
a standard deviation of 10. If you select an individual stock from 
this population, what is the probability that it would have a return
a.	 less than 0 (i.e., a loss)?
b.	 between -10 and -20?
c.	 greater than -5?
If you selected a random sample of four stocks from this pop-
ulation, what is the probability that the sample would have a mean 
return
d.	 less than 0 (a loss)?
e.	 between -10 and -20?
f.	 greater than -5?
g.	 Compare your results in parts (d) through (f) to those in (a) 
through (c).
7.30  (Class Project) The table of random numbers is an example 
of a uniform distribution because each digit is equally likely to oc-
cur. Starting in the row corresponding to the day of the month in 
which you were born, use a table of random numbers (Table E.1) 
to take one digit at a time.
Select five different samples each of n = 2, n = 5, and 
n = 10. Compute the sample mean of each sample. Develop a 
frequency distribution of the sample means for the results of the 
entire class, based on samples of sizes n = 2, n = 5, and n = 10.
What can be said about the shape of the sampling distribution 
for each of these sample sizes?
7.31  (Class Project) Toss a coin 10 times and record the number 
of heads. If each student performs this experiment five times, a 
frequency distribution of the number of heads can be developed 
from the results of the entire class. Does this distribution seem to 
approximate the normal distribution?
7.32  (Class Project) The number of cars waiting in line at a car 
wash is distributed as follows:
Number of Cars
Probability
0
0.25
1
0.40
2
0.20
3
0.10
4
0.04
5
0.01
You can use a table of random numbers (Table E.1) to select sam-
ples from this distribution by assigning numbers as follows:
	
1.	Start in the row corresponding to the day of the month in 
which you were born.
	
2.	 Select a two-digit random number.
	
3.	If you select a random number from 00 to 24, record a 
length of 0; if from 25 to 64, record a length of 1; if from 65 
to 84, record a length of 2; if from 85 to 94, record a length 
of 3; if from 95 to 98, record a length of 4; if 99, record a 
length of 5.
Select samples of n = 2, n = 5, and n = 10. Compute the 
mean for each sample. For example, if a sample of size 2 results 
in the random numbers 18 and 46, these would correspond to 
lengths 0 and 1, respectively, producing a sample mean of 0.5. If 
each student selects five different samples for each sample size, a 
frequency distribution of the sample means (for each sample size) 
can be developed from the results of the entire class. What conclu-
sions can you reach concerning the sampling distribution of the 
mean as the sample size is increased?
7.33  (Class Project) Using a table of random numbers (Table E.1),  
simulate the selection of different-colored balls from a bowl, as 
follows:
	
1.	Start in the row corresponding to the day of the month in 
which you were born.
	
2.	 Select one-digit numbers.
	
3.	If a random digit between 0 and 6 is selected, consider the ball 
white; if a random digit is a 7, 8, or 9, consider the ball red.
Select samples of n = 10, n = 25, and n = 50 digits. In each 
sample, count the number of white balls and compute the proportion  
of white balls in the sample. If each student in the class selects 
five different samples for each sample size, a frequency distribu-
tion of the proportion of white balls (for each sample size) can be 
developed from the results of the entire class. What conclusions 
can you reach about the sampling distribution of the proportion as 
the sample size is increased?
7.34  (Class Project) Suppose that step 3 of Problem 7.33 uses 
the following rule: “If a random digit between 0 and 8 is selected, 
consider the ball to be white; if a random digit of 9 is selected, 
consider the ball to be red.” Compare and contrast the results in 
this problem and those in Problem 7.33.

	
Cases for Chapter 7	
297
C a s e s  f o r  C h a p t e r  7
Managing Ashland MultiComm Services
Continuing the quality improvement effort first described 
in the Chapter 6 Managing Ashland MultiComm Services 
case, the target upload speed for AMS Internet service sub-
scribers has been monitored. As before, upload speeds are 
measured on a standard scale in which the target value is 
1.0. Data collected over the past year indicate that the up-
load speeds are approximately normally distributed, with a 
mean of 1.005 and a standard deviation of 0.10.
1.	Each day, at 25 random times, the upload speed is mea-
sured. Assuming that the distribution has not changed 
from what it was in the past year, what is the probability 
that the mean upload speed is
a.	less than 1.0?
b.	between 0.95 and 1.0?
c.	 between 1.0 and 1.05?
d.	less than 0.95 or greater than 1.05?
e.	 Suppose that the mean upload speed of today’s sample 
of 25 is 0.952. What conclusion can you reach about 
the upload speed today based on this result? Explain.
2.	Compare the results of AMS Problem 1 (a) through 
(d) to those of AMS Problem 1 in Chapter 6 on page 
274. What conclusions can you reach concerning the  
differences?
Digital Case
Apply your knowledge about sampling distributions in this 
Digital Case, which reconsiders the Oxford Cereals Using 
Statistics scenario.
The advocacy group Consumers Concerned About Cereal 
Cheaters (CCACC) suspects that cereal companies, includ-
ing Oxford Cereals, are cheating consumers by packaging 
cereals at less than labeled weights. Recently, the group 
investigated the package weights of two popular Oxford 
brand cereals. Open CCACC.pdf to examine the group’s 
claims and supporting data, and then answer the following  
questions:
1.	 Are the data collection procedures that the CCACC uses 
to form its conclusions flawed? What procedures could the 
group follow to make its analysis more rigorous?
2.	Assume that the two samples of five cereal boxes (one 
sample for each of two cereal varieties) listed on the 
CCACC website were collected randomly by organiza-
tion members. For each sample,
a.	calculate the sample mean.
b.	assuming that the standard deviation of the process is 
15 grams and the population mean is 368 grams, cal-
culate the percentage of all samples for each process 
that have a sample mean less than the value you calcu-
lated in (a).
c.	 assuming that the standard deviation is 15 grams, cal-
culate the percentage of individual boxes of cereal that 
have a weight less than the value you calculated in (a).
3.	What, if any, conclusions can you form by using your cal-
culations about the filling processes for the two different 
cereals?
4.	A representative from Oxford Cereals has asked that the 
CCACC take down its page discussing shortages in Ox-
ford Cereals boxes. Is this request reasonable? Why or 
why not?
5.	Can the techniques discussed in this chapter be used to 
prove cheating in the manner alleged by the CCACC? 
Why or why not?

298	
Chapter 7  Sampling Distributions 
EG7.1  Sampling Distributions
There are no Excel Guide instructions for this section.
EG7.2  Sampling Distribution  
of the Mean
Key Technique  Use an add-in procedure to create a simulated 
sampling distribution and use the RAND() function to create lists 
of random numbers.
Example  Create a simulated sampling distribution that consists 
of 100 samples of n = 30 from a uniformly distributed popula-
tion.
PHStat  Use Sampling Distributions Simulation.
For the example, select PHStat ➔ Sampling ➔ Sampling  
Distributions Simulation. In the procedure’s dialog box (shown 
below):
	 1.	 Enter 100 as the Number of Samples.
	 2.	 Enter 30 as the Sample Size.
	 3.	 Click Uniform.
	 4.	 Enter a Title and click OK.
The procedure inserts a new worksheet in which the sample 
means, overall mean, and standard error of the mean can be found 
starting in row 34.
In-Depth Excel  Use the SDS worksheet of the SDS workbook 
as a model.
For the example, in a new worksheet, first enter a title in cell 
A1. Then enter the formula =RAND() in cell A2 and then copy 
the formula down 30 rows and across 100 columns (through  
column CV). Then select this cell range (A2:CV31) and use copy 
and paste values as discussed in Appendix Section B.4.
Use the formulas that appear in rows 33 through 37 in the 
SDS_FORMULAS worksheet of the SDS workbook as models 
if you want to compute sample means, the overall mean, and the 
standard error of the mean.
Analysis ToolPak  Use Random Number Generation.
For the example, select Data ➔ Data Analysis. In the Data Analy-
sis dialog box, select Random Number Generation from the 
Analysis Tools list and then click OK.
In the procedure’s dialog box (shown below):
	 1.	 Enter 100 as the Number of Variables.
	 2.	 Enter 30 as the Number of Random Numbers.
	 3.	 Select Uniform from the Distribution drop-down list.
	 4.	 Keep the Parameters values as is.
	 5.	 Click New Worksheet Ply and then click OK.
If, for other problems, you select Discrete in step 3, you 
must be open to a worksheet that contains a cell range of X and 
P(X) values. Enter this cell range as the Value and Probability 
Input Range (not shown when Uniform has been selected) in the  
Parameters section of the dialog box.
Use the formulas that appear in rows 33 through 37 in the 
SDS_FORMULAS worksheet of the SDS workbook as models 
if you want to compute sample means, the overall mean, and the 
standard error of the mean.
EG7.3  Sampling Distribution  
of the Proportion
There are no Excel Guide instructions for this section.
C h a p t e r  7  E x c e l  G u i d e

	
Chapter 7 Minitab Guide	
299
MG7.1  Sampling Distributions
There are no Minitab Guide instructions for this section.
MG7.2  Sampling Distribution  
of the Mean
Use Uniform to create a simulated sampling distribution from a 
uniformly distributed population. For example, to create 100 sam-
ples of n = 30 from a uniformly distributed population, open to a 
new worksheet. Select Calc ➔ Random Data ➔ Uniform. In the 
Uniform Distribution dialog box (shown below):
	 1.	 Enter 100 in the Number of rows of data to generate box.
	 2.	 Enter C1-C30 in the Store in column(s) box (to store the re-
sults in the first 30 columns).
	 3.	 Enter 0.0 in the Lower endpoint box.
	 4.	 Enter 1.0 in the Upper endpoint box.
	 5.	 Click OK.
The 100 samples of n = 30 are entered row-wise in columns 
C1 through C30, an exception to the rule used in this book to enter 
data column-wise. (Row-wise data facilitates the computation of 
means.) While still opened to the worksheet with the 100 samples, 
enter Sample Means as the name of column C31. Select Calc ➔ 
Row Statistics. In the Row Statistics dialog box (shown below):
	 6.	 Click Mean.
	 7.	 Enter C1-C30 in the Input variables box.
	 8.	 Enter C31 in the Store result in box.
	 9.	 Click OK.
	10.	 With the mean for each of the 100 row-wise samples in  
column C31, select Stat ➔ Basic Statistics ➔ Display  
Descriptive Statistics.
	11.	 In the Display Descriptive Statistics dialog box, enter C31 in 
the Variables box and click Statistics.
	12.	 In the Display Descriptive Statistics - Statistics dialog box,  
select Mean and Standard deviation and then click OK.
	13.	 Back in the Display Descriptive Statistics dialog box,  
click OK.
While still open to the worksheet created in steps 1 through 13, se-
lect Graph ➔ Histogram and in the Histograms dialog box, click 
Simple and then click OK. In the Histogram - Simple dialog box:
	 1.	 Enter C31 in the Graph variables box.
	 2.	 Click OK.
Sampling from Normally Distributed 
Populations
Use Normal to create a simulated sampling distribution from a 
normally distributed population. For example, to create 100 sam-
ples of n = 30 from a normally distributed population, open to a 
new worksheet. Select Calc ➔ Random Data ➔ Normal. In the 
Normal Distribution dialog box:
	 1.	 Enter 100 in the Number of rows of data to generate box.
	 2.	 Enter C1-C30 in the Store in column(s) box (to store the 
results in the first 30 columns).
	 3.	 Enter a value for μ in the Mean box.
	 4.	 Enter a value for s in the Standard deviation box.
	 5.	 Click OK.
The 100 samples of n = 30 are entered row-wise in columns C1 
through C30. To compute statistics, select Calc ➔ Row Statistics 
and follow steps 6 through 13 from the set of instructions for a 
uniformly distributed population.
MG7.3  Sampling Distribution  
of the Proportion
There are no Minitab Guide instructions for this section.
C h a p t e r  7  M i n i ta b  G u i d e

300
U s i n g  S tat i s t i c s
Getting Estimates at  
Ricknel Home Centers
As a member of the AIS team at Ricknel Home Centers (see page 213), you 
have already examined the probability of discovering questionable, or “tagged,” 
invoices. Now you have been assigned the task of auditing the accuracy of the 
integrated inventory management and point of sale component of the firm’s retail 
management system.
You could review the contents of each and every inventory and transactional 
record to check the accuracy of this system, but such a detailed review would 
be time-consuming and costly. Could you use statistical inference techniques 
to reach conclusions about the population of all records from a relatively small 
sample collected during an audit? At the end of each month, you could select a 
sample of the sales invoices to estimate population parameters such as
 • The mean dollar amount listed on the sales invoices for the month
 • The proportion of invoices that contain errors that violate the internal con-
trol policy of the warehouse
If you used the sampling technique, how accurate would the results from the 
sample be? How would you use the results you generate? How could you be cer-
tain that the sample size is large enough to give you the information you need?
contents
8.1	 Confidence Interval Estimate 
for the Mean (s Known)
8.2	 Confidence Interval Estimate 
for the Mean (s Unknown)
8.3	 Confidence Interval Estimate 
for the Proportion
8.4	 Determining Sample Size
8.5	 Confidence Interval 
Estimation and Ethical Issues
8.6	 Application of Confidence 
Interval Estimation in Auditing 
(online)
8.7	 Estimation and Sample 
Size Estimation for Finite 
Populations (online)
8.8	 Bootstrapping (online)
Using Statistics: Getting 
Estimates at Ricknel Home 
Centers, Revisited
Chapter 8 Excel Guide
Chapter 8 Minitab Guide
Objectives
To construct and interpret 
confidence interval estimates for 
the mean and the proportion
To determine the sample size 
necessary to develop a 
confidence interval estimate for 
the mean or proportion
Chapter
Confidence Interval 
Estimation
8
Mangostock/Shutterstock

	
8.1  Confidence Interval Estimate for the Mean (s Known)	
301
I
n Section 7.2, you used the Central Limit Theorem and knowledge of the population 
­distribution to determine the percentage of sample means that are within certain dis-
tances of the population mean. For instance, in the cereal-filling example used throughout 
­Chapter 7 (see Example 7.4 on page 285), you can conclude that 95% of all sample means are 
between 362.12 and 373.88 grams. This is an example of deductive reasoning because the con-
clusion is based on taking something that is true in general (for the population) and applying it 
to something specific (the sample means).
Getting the results that Ricknel Home Centers needs requires inductive reasoning. Induc-
tive reasoning lets you use some specifics to make broader generalizations. You cannot guar-
antee that the broader generalizations are absolutely correct, but with a careful choice of the 
specifics and a rigorous methodology, you can get useful conclusions. As a Ricknel accoun-
tant, you need to use inferential statistics, which uses sample results (the “some specifics”) to 
estimate (the making of “broader generalizations”) unknown population parameters such as a 
population mean or a population proportion. Note that statisticians use the word estimate in 
the same sense of the everyday usage: something you are reasonably certain about but cannot 
flatly say is absolutely correct.
You estimate population parameters by using either point estimates or interval estimates. 
A point estimate is the value of a single sample statistic, such as a sample mean. A confi-
dence interval estimate is a range of numbers, called an interval, constructed around the point 
estimate. The confidence interval is constructed such that the probability that the interval in-
cludes the population parameter is known.
Suppose you want to estimate the mean GPA of all the students at your university. The 
mean GPA for all the students is an unknown population mean, denoted by m. You select a 
sample of students and compute the sample mean, denoted by X, to be 2.80. As a point ­estimate 
of the population mean, m, you ask how accurate is the 2.80 value as an estimate of the popula-
tion mean, m? By taking into account the variability from sample to sample (see Section 7.2, 
concerning the sampling distribution of the mean), you can construct a confidence interval 
estimate for the population mean to answer this question.
When you construct a confidence interval estimate, you indicate the confidence of cor-
rectly estimating the value of the population parameter, m. This allows you to say that there 
is a specified confidence that m is somewhere in the range of numbers defined by the interval.
After studying this chapter, you might find that a 95% confidence interval for the mean 
GPA at your university is 2.75 … m … 2.85. You can interpret this interval estimate by stating 
that you are 95% confident that the mean GPA at your university is between 2.75 and 2.85.
In this chapter, you learn to construct a confidence interval for both the population mean 
and population proportion. You also learn how to determine the sample size that is necessary to 
construct a confidence interval of a desired width.
8.1  Confidence Interval Estimate for the Mean (s Known)
In Section 7.2, you used the Central Limit Theorem and knowledge of the population dis-
tribution to determine the percentage of sample means that are within certain distances 
of the population mean. Suppose that in the cereal-filling example you wished to estimate 
the population mean, using the information from a single sample. Thus, rather than taking 
m { 11.9621s> 1n2 to find the upper and lower limits around m, as in Section 7.2, you sub-
stitute the sample mean, X, for the unknown m and use X { 11.9621s> 1n2 as an interval to 
estimate the unknown m. Although in practice you select a single sample of n values and com-
pute the mean, X, in order to understand the full meaning of the interval estimate, you need to 
examine a hypothetical set of all possible samples of n values.
Suppose that a sample of n = 25 cereal boxes has a mean of 362.3 grams and a standard 
deviation of 15 grams. The interval developed to estimate m is 362.3 { 11.9621152>11252, 
or 362.3 { 5.88. The estimate of m is
356.42 … m … 368.18
Student Tip
Remember, the confi-
dence interval is for the 
population mean not the 
sample mean.

302	
Chapter 8  Confidence Interval Estimation
Because the population mean, m (equal to 368), is included within the interval, this sample 
results in a correct statement about m (see Figure 8.1).
368
368.18
362.3
356.42
X1 = 362.3
362.12
373.88
375.38
369.5
363.62
365.88
360
354.12
368
362.12
356.24
379.76
373.88
368
X2 = 369.5
X3 = 360
X4 = 362.12
X5 = 373.88
F i g u r e  8 . 1
Confidence interval 
estimates for five 
different samples of 
n = 25 taken from 
a population where 
m = 368 and s = 15
To continue this hypothetical example, suppose that for a different sample of  
n = 25 boxes, the mean is 369.5. The interval developed from this sample is
369.5 { 11.9621152>11252
or 369.5 { 5.88. The estimate is
363.62 … m … 375.38
Because the population mean, m (equal to 368), is also included within this interval, this state-
ment about m is correct.
Now, before you begin to think that correct statements about m are always made 
by developing a confidence interval estimate, suppose a third hypothetical sample of  
n = 25 boxes is selected and the sample mean is equal to 360 grams. The interval developed 
here is 360 { 11.9621152>11252, or 360 { 5.88. In this case, the estimate of m is
354.12 … m … 365.88
This estimate is not a correct statement because the population mean, m, is not included in the 
interval developed from this sample (see Figure 8.1). Thus, for some samples, the interval esti-
mate for m is correct, but for others it is incorrect. In practice, only one sample is selected, and 
because the population mean is unknown, you cannot determine whether the interval estimate 
is correct. To resolve this, you need to determine the proportion of samples producing intervals 
that result in correct statements about the population mean, m. To do this, consider two other 
hypothetical samples: the case in which X = 362.12 grams and the case in which X = 373.88 
grams. If X = 362.12, the interval is 362.12 { 11.9621152>11252, or 362.12 { 5.88. This 
leads to the following interval:
356.24 … m … 368.00
Because the population mean of 368 is at the upper limit of the interval, the statement is ­correct 
(see Figure 8.1).
When X = 373.88, the interval is 373.88 { 11.9621152>11252, or 373.88 { 5.88. The 
interval estimate for the mean is
368.00 … m … 379.76

	
8.1  Confidence Interval Estimate for the Mean (s Known)	
303
In this case, because the population mean of 368 is included at the lower limit of the inter-
val, the statement is correct.
In Figure 8.1, you see that when the sample mean falls somewhere between 362.12 and 
373.88 grams, the population mean is included somewhere within the interval. In Example 7.4 
on page 285, you found that 95% of the sample means are between 362.12 and 373.88 grams. 
Therefore, 95% of all samples of n = 25 boxes have sample means that will result in intervals 
that include the population mean.
Because, in practice, you select only one sample of size n, and m is unknown, you never 
know for sure whether your specific interval includes the population mean. However, if you 
take all possible samples of n and compute their 95% confidence intervals, 95% of the inter-
vals will include the population mean, and only 5% of them will not. In other words, you have 
95% confidence that the population mean is somewhere in your interval.
Consider once again the first sample discussed in this section. A sample of n = 25 boxes 
had a sample mean of 362.3 grams. The interval constructed to estimate m is
 362.3 { 11.9621152>11252
 362.3 { 5.88
 356.42 … m … 368.18
The interval from 356.42 to 368.18 is referred to as a 95% confidence interval. The follow-
ing contains an interpretation of the interval that most business professionals will under-
stand. (For a technical discussion of different ways to interpret confidence intervals, see 
reference 4.)
“I am 95% confident that the mean amount of cereal in the population of boxes is some-
where between 356.42 and 368.18 grams.”
To help you understand the meaning of the confidence interval, consider the order-filling 
process at a website. Filling orders consists of several steps, including receiving an order, pick-
ing the parts of the order, checking the order, packing, and shipping the order. The file  Order  
contains the time, in minutes, to fill orders for a population of N = 200 orders on a recent day. 
Although in practice the population characteristics are rarely known, for this population of or-
ders, the mean, m, is known to be equal to 69.637 minutes; the standard deviation, s, is known 
to be equal to 10.411 minutes; and the population is normally distributed. To illustrate how the 
sample mean and sample standard deviation can vary from one sample to another, 20 different 
samples of n = 10 were selected from the population of 200 orders, and the sample mean and 
sample standard deviation (and other statistics) were calculated for each sample. Figure 8.2 
shows these results.
F i g u r e  8 . 2
Sample statistics and 
95% confidence intervals 
for 20 samples of n = 10 
randomly selected 
from the population of 
N = 200 orders

304	
Chapter 8  Confidence Interval Estimation
From Figure 8.2, you can see the following:
 • The sample statistics differ from sample to sample. The sample means vary from 61.10 
to 76.26 minutes, the sample standard deviations vary from 6.50 to 14.10 minutes, the 
sample medians vary from 59.70 to 80.60 minutes, and the sample ranges vary from 
21.50 to 41.60 minutes.
 • Some of the sample means are greater than the population mean of 69.637 minutes, and 
some of the sample means are less than the population mean.
 • Some of the sample standard deviations are greater than the population standard devia-
tion of 10.411 minutes, and some of the sample standard deviations are less than the 
population standard deviation.
 • The variation in the sample ranges is much more than the variation in the sample ­standard 
deviations.
The variation of sample statistics from sample to sample is called sampling error. 
­Sampling error is the variation that occurs due to selecting a single sample from the popula-
tion. The size of the sampling error is primarily based on the amount of variation in the popula-
tion and on the sample size. Large samples have less sampling error than small samples, but 
large samples cost more to select.
The last column of Figure 8.2 contains 95% confidence interval estimates of the population 
mean order-filling time, based on the results of those 20 samples of n = 10. Begin by ­examining 
the first sample selected. The sample mean is 74.15 minutes, and the interval estimate for the 
population mean is 67.70 to 80.60 minutes. In a typical study, you would not know for sure 
whether this interval estimate is correct because you rarely know the value of the population 
mean. However, for this example concerning the order-filling times, the population mean is 
known to be 69.637 minutes. If you examine the interval 67.70 to 80.60 minutes, you see that 
the population mean of 69.637 minutes is located between these lower and upper limits. Thus, 
the first sample provides a correct estimate of the population mean in the form of an interval 
estimate. Looking over the other 19 samples, you see that similar results occur for all the other 
samples except for samples 2, 5, and 12. For each of the intervals generated (other than samples 
2, 5, and 12), the population mean of 69.637 minutes is located somewhere within the interval.
For sample 2, the sample mean is 61.10 minutes, and the interval is 54.65 to 67.55 min-
utes; for sample 5, the sample mean is 62.18, and the interval is between 55.73 and 68.63; for  
sample 12, the sample mean is 76.26, and the interval is between 69.81 and 82.71 minutes. The 
population mean of 69.637 minutes is not located within any of these intervals, and the estimate 
of the population mean made using these intervals is incorrect. Although 3 of the 20 intervals did 
not include the population mean, if you had selected all the possible samples of n = 10 from a 
population of N = 200, 95% of the intervals would include the population mean.
In some situations, you might want a higher degree of confidence of including the popula-
tion mean within the interval (such as 99%). In other cases, you might accept less confidence 
(such as 90%) of correctly estimating the population mean. In general, the level of confidence 
is symbolized by 11 - a2 * 100%, where a is the proportion in the tails of the distribution 
that is outside the confidence interval. The proportion in the upper tail of the distribution is 
a>2, and the proportion in the lower tail of the distribution is a>2. You use Equation (8.1) to 
construct a 11 - a2 * 100% confidence interval estimate for the mean with s known.
Confidence Interval for the Mean (s Known)
X { Za>2 s
1n
or
	
X - Za>2
s
1n … m … X + Za>2
s
1n	
(8.1)
where Za>2 is the value corresponding to an upper-tail probability of a>2 from the 
standardized normal distribution (i.e., a cumulative area of 1 - a>2).

	
8.1  Confidence Interval Estimate for the Mean (s Known)	
305
The value of Za>2 needed for constructing a confidence interval is called the critical value for 
the distribution. 95% confidence corresponds to an a value of 0.05. The critical Z value cor-
responding to a cumulative area of 0.975 is 1.96 because there is 0.025 in the upper tail of the 
distribution, and the cumulative area less than Z = 1.96 is 0.975.
There is a different critical value for each level of confidence, 1 - a. A level of confi-
dence of 95% leads to a Z value of 1.96 (see Figure 8.3). 99% confidence corresponds to an  
a value of 0.01. The Z value is approximately 2.58 because the upper-tail area is 0.005 and the 
cumulative area less than Z = 2.58 is 0.995 (see Figure 8.4).
X
0
–1.96
+1.96
.475
.475
Z
.025
.025
F i g u r e  8 . 3
Normal curve for 
determining the Z 
value needed for 95% 
confidence
X
0
–2.58
+2.58
.495
.495
Z
.005
.005
F i g u r e  8 . 4
Normal curve for 
determining the Z 
value needed for 99% 
confidence
Now that various levels of confidence have been considered, why not make the confidence 
level as close to 100% as possible? Before doing so, you need to realize that any increase in 
the level of confidence is achieved only by widening (and making less precise) the confidence 
interval. There is no “free lunch” here. You would have more confidence that the population 
mean is within a broader range of values; however, this might make the interpretation of the 
confidence interval less useful. The trade-off between the width of the confidence interval and 
the level of confidence is discussed in greater depth in the context of determining the sample 
size in Section 8.4. Example 8.1 illustrates the application of the confidence interval estimate.
Example 8.1
Estimating the 
Mean Paper 
Length with 95% 
Confidence
A paper manufacturer has a production process that operates continuously throughout an entire 
production shift. The paper is expected to have a mean length of 11 inches, and the standard 
deviation of the length is 0.02 inch. At periodic intervals, a sample is selected to determine 
whether the mean paper length is still equal to 11 inches or whether something has gone wrong 
in the production process to change the length of the paper produced. You select a random 
sample of 100 sheets, and the mean paper length is 10.998 inches. Construct a 95% confidence 
interval estimate for the population mean paper length.
Solution  Using Equation (8.1) on page 304, with Za>2 = 1.96 for 95% confidence,
 X { Za>2
s
1n = 10.998 { 11.962 0.02
1100
 = 10.998 { 0.0039
 10.9941 … m … 11.0019
Thus, with 95% confidence, you conclude that the population mean is between 10.9941 and 
11.0019 inches. Because the interval includes 11, the value indicating that the production pro-
cess is working properly, you have no reason to believe that anything is wrong with the pro-
duction process.

306	
Chapter 8  Confidence Interval Estimation
Example 8.2 illustrates the effect of using a 99% confidence interval.
Example 8.2
Estimating the 
Mean Paper 
Length with 99% 
Confidence
Construct a 99% confidence interval estimate for the population mean paper length.
Solution  Using Equation (8.1) on page 304, with Za>2 = 2.58 for 99% confidence,
 X { Za>2
s
1n = 10.998 { 12.582 0.02
1100
 = 10.998 { 0.00516
 10.9928 … m … 11.0032
Once again, because 11 is included within this wider interval, you have no reason to believe 
that anything is wrong with the production process.
As discussed in Section 7.2, the sampling distribution of the sample mean, X, is normally 
distributed if the population for your characteristic of interest, X, follows a normal distribution. 
And if the population of X does not follow a normal distribution, the Central Limit Theorem 
almost always ensures that X is approximately normally distributed when n is large. However, 
when dealing with a small sample size and a population that does not follow a normal distribu-
tion, the sampling distribution of X is not normally distributed, and therefore the confidence 
interval discussed in this section is inappropriate. In practice, however, as long as the sample 
size is large enough and the population is not very skewed, you can use the confidence interval 
defined in Equation (8.1) to estimate the population mean when s is known. To assess the as-
sumption of normality, you can evaluate the shape of the sample data by constructing a histo-
gram, stem-and-leaf display, boxplot, or normal probability plot.
Can You Ever Know the Population Standard Deviation?
To solve Equation (8.1), you must know the value for s, the population standard deviation. To 
know s implies that you know all the values in the entire population. (How else would you 
know the value of this population parameter?) If you knew all the values in the entire popu-
lation, you could directly compute the population mean. There would be no need to use the 
inductive reasoning of inferential statistics to estimate the population mean. In other words, 
if you know s, you really do not have a need to use Equation (8.1) to construct a confidence 
interval estimate of the mean (s known).
More significantly, in virtually all real-world business situations, you would never know 
the standard deviation of the population. In business situations, populations are often too 
large to ­examine all the values. So why study the confidence interval estimate of the mean  
(s known) at all? This method serves as an important introduction to the concept of a con-
fidence interval ­because it uses the normal distribution, which has already been thoroughly 
discussed in Chapters 6 and 7. In the next section, you will see that constructing a confidence 
interval estimate when s is not known requires another distribution (the t distribution) not pre-
viously mentioned in this book.
Student Tip
Because understanding 
the confidence interval 
concept is very impor-
tant when reading the 
rest of this book, review 
this section carefully to 
­understand the underly-
ing concept—even 
if you never have a 
practical reason to use 
the confidence interval 
estimate of the mean 
(s known) method.
Problems for Section 8.1
Learning the Basics
8.1  What do you understand by the terms ‘deductive reasoning’ 
and ‘inductive reasoning’? How do you think they are interrelated?
8.2  What do you understand by the terms ‘point estimate’ and 
‘confidence interval estimate’? How do these two estimates de-
termine the population parameter?
8.3  Investigating the sample size of 35, the sample mean and s 
were found to be 15 and 2. Construct a 95% confidence interval 
for p.
8.4  Construct a 99% confidence interval for the data in question 
8.3. Comparing the results of 95% and 99% confidence intervals, 
what conclusions can you draw? Is it feasible to construct a 100% 
confidence interval? Why or why not?

	
8.2  Confidence Interval Estimate for the Mean (s Unknown)	
307
Applying the Concepts
8.5  A market researcher selects a simple random sample of 
n = 100 Twitter users from a population of over 100 million 
Twitter registered users. After analyzing the sample, she states 
that she has 95% confidence that the mean time spent on the site 
per day is between 15 and 57 minutes. Explain the meaning of 
this statement.
8.6  The dean of a management school wants to check the knowl-
edge of the students at his school about day to day international 
affairs. The dean arranged a quiz with 50 participants. Assume that 
the average score is 67 with the s of 15. 
a.	 Construct the 95% confidence interval in which the score of all 
the students in the college will lie.
b.	 Comment on the normality of the distribution of the population.
8.7  Consider the confidence interval estimate discussed in Prob-
lem 8.5. Suppose the population mean time spent on the site is 
36 minutes a day. Is the confidence interval estimate stated in 
Problem 8.5 correct? Explain.
8.8  The quality control officer in a Coke manufacturing unit 
inspects the breaking strength of the glass bottles. The officer 
randomly selects a sample of 500 bottles from the production  
department to check the strength of the glass bottles. Do you think 
that the officer is justified in selecting the sample and he would be 
able to draw conclusions about the strength of all the bottles pro-
duced? Also, if the officer employs the formula given in Equation 
8.1, which statistical measure will he compute first: the mean or the 
standard deviation? What do you think is required in Equation 8.1: 
population standard deviation or the sample standard deviation?
8.9  A bottled water distributor wants to estimate the amount of 
water contained in 1-gallon bottles purchased from a nationally 
known water bottling company. The water bottling company’s 
specifications state that the standard deviation of the amount of 
water is equal to 0.02 gallon. A random sample of 50 bottles is 
selected, and the sample mean amount of water per 1-gallon bottle 
is 0.995 gallon.
a.	 Construct a 99% confidence interval estimate for the popula-
tion mean amount of water included in a 1-gallon bottle.
b.	 On the basis of these results, do you think that the distributor 
has a right to complain to the water bottling company? Why?
c.	 Must you assume that the population amount of water per  
bottle is normally distributed here? Explain.
d.	 Construct a 95% confidence interval estimate. How does this 
change your answer to (b)?
8.10  The operations manager at a compact fluorescent 
light bulb (CFL) factory needs to estimate the mean life 
of a large shipment of CFLs. The manufacturer’s specifications are 
that the standard deviation is 1,000 hours. A random sample of 64 
CFLs indicated a sample mean life of 7,500 hours.
a.	 Construct a 95% confidence interval estimate for the population 
mean life of compact fluorescent light bulbs in this shipment.
b.	 Do you think that the manufacturer has the right to state that 
the compact fluorescent light bulbs have a mean life of 8,000 
hours? Explain.
c.	 Must you assume that the population compact fluorescent light 
bulb life is normally distributed? Explain.
d.	 Suppose that the standard deviation changes to 800 hours. 
What are your answers in (a) and (b)?
SELF 
Test 
8.2  Confidence Interval Estimate for the Mean (s Unknown)
In the previous section, you learned that in most business situations, you do not know s, 
the population standard deviation. This section discusses a method of constructing a confi-
dence interval estimate of m that uses the sample statistic S as an estimate of the population 
parameter s.
Student’s t Distribution
At the start of the twentieth century, William S. Gosset was working at Guinness in Ireland, 
trying to help brew better beer less expensively (see reference 5). As he had only small samples 
to study, he needed to find a way to make inferences about means without having to know s. 
Writing under the pen name “Student,”1 Gosset solved this problem by developing what today 
is known as the Student’s t distribution, or the t distribution.
If the random variable X is normally distributed, then the following statistic:
t = X - m
S
1n
has a t distribution with n - 1 degrees of freedom. This expression has the same form 
as the Z statistic in Equation (7.4) on page 283, except that S is used to estimate the un-
known s.
1Guinness considered all research 
conducted to be proprietary and a 
trade secret. The firm prohibited 
its employees from publishing 
their ­results. Gosset circumvented 
this ban by using the pen name 
“Student” to publish his findings.

308	
Chapter 8  Confidence Interval Estimation
Properties of the t Distribution
The t distribution is very similar in appearance to the standardized normal distribution. Both 
distributions are symmetrical and bell-shaped, with the mean and the median equal to zero. 
However, because S is used to estimate the unknown s, the values of t are more variable than 
those for Z. Therefore, the t distribution has more area in the tails and less in the center than 
does the standardized normal distribution (see Figure 8.5).
Standardized normal distribution 
t distribution
for 5 degrees 
of freedom
F i g u r e  8 . 5
Standardized normal 
distribution and  
t distribution for  
5 degrees of freedom
The degrees of freedom, n - 1, are directly related to the sample size, n. The concept of 
degrees of freedom is discussed further on page 309. As the sample size and degrees of free-
dom increase, S becomes a better estimate of s, and the t distribution gradually approaches the 
standardized normal distribution, until the two are virtually identical. With a sample size of 
about 120 or more, S estimates s closely enough so that there is little difference between the  
t and Z distributions.
As stated earlier, the t distribution assumes that the random variable X is normally distrib-
uted. In practice, however, when the sample size is large enough and the population is not very 
skewed, in most cases you can use the t distribution to estimate the population mean when s is 
unknown. When dealing with a small sample size and a skewed population distribution, the con-
fidence interval estimate may not provide a valid estimate of the population mean. To assess the 
assumption of normality, you can evaluate the shape of the sample data by constructing a histo-
gram, stem-and-leaf display, boxplot, or normal probability plot. However, the ability of any of 
these graphs to help you evaluate normality is limited when you have a small sample size.
You find the critical values of t for the appropriate degrees of freedom from the table of 
the t distribution (see Table E.3). The columns of the table present the most commonly used 
cumulative probabilities and corresponding upper-tail areas. The rows of the table represent 
the degrees of freedom. The critical t values are found in the cells of the table. For example, 
with 99 degrees of freedom, if you want 95% confidence, you find the appropriate value of 
t, as shown in Table 8.1. The 95% confidence level means that 2.5% of the values (an area of 
Cumulative Probabilities
.75
.90
.95
.975
.99
.995
Upper-Tail Areas
Degrees of Freedom
.25
.10
.05
.025
.01
.005
1
1.0000
3.0777
6.3138
12.7062
31.8207
63.6574
2
0.8165
1.8856
2.9200
4.3027
6.9646
9.9248
3
0.7649
1.6377
2.3534
3.1824
4.5407
5.8409
4
0.7407
1.5332
2.1318
2.7764
3.7469
4.6041
5
0.7267
1.4759
2.0150
2.5706
3.3649
4.0322
f
f
f
f
f
f
f
96
0.6771
1.2904
1.6609
1.9850
2.3658
2.6280
97
0.6770
1.2903
1.6607
1.9847
2.3654
2.6275
98
0.6770
1.2902
1.6606
1.9845
2.3650
2.6269
99
0.6770
1.2902
1.6604
1.9842
2.3646
2.6264
100
0.6770
1.2901
1.6602
1.9840
2.3642
2.6259
Source: Extracted from Table E.3.
T a b l e  8 . 1
Determining the 
Critical Value from the 
t Table for an Area 
of 0.025 in Each Tail 
with 99 Degrees of 
Freedom

	
8.2  Confidence Interval Estimate for the Mean (s Unknown)	
309
Note that for a 95% confidence interval, you will always have a cumulative probability 
of 0.975 and an upper-tail area of 0.025. Similarly, for a 99% confidence interval, you 
will have 0.995 and 0.005, and for a 90% confidence interval you will have 0.95 and 
0.05.
The Concept of Degrees of Freedom
In Chapter 3, you learned that the numerator of the sample variance, S2 [see Equation (3.6) on 
page 137], requires the computation of the sum of squares around the sample mean:
a
n
i = 1
 1Xi - X22
In order to compute S2, you first need to know X. Therefore, only n - 1 of the sample 
values are free to vary. This means that you have n - 1 degrees of freedom. For example, 
suppose a sample of five values has a mean of 20. How many values do you need to know 
before you can determine the remainder of the values? The fact that n = 5 and X = 20 
also tells you that
a
n
i = 1
 Xi = 100
because
a
n
i = 1
 Xi
n
= X
Thus, when you know four of the values, the fifth one is not free to vary because the sum must 
be 100. For example, if four of the values are 18, 24, 19, and 16, the fifth value must be 23, so 
that the sum is 100.
0.025) are in each tail of the distribution. Looking in the column for a cumulative probability 
of 0.975 and an upper-tail area of 0.025 in the row corresponding to 99 degrees of freedom 
gives you a critical value for t of 1.9842 (see Figure 8.6). Because t is a symmetrical distri-
bution with a mean of 0, if the upper-tail value is +1.9842, the value for the lower-tail area 
(lower 0.025) is -1.9842. A t value of -1.9842 means that the probability that t is less than 
-1.9842 is 0.025, or 2.5%.
+1.9842
t
.025
Cumulative
area
0.975
F i g u r e  8 . 6
t distribution with 
99 degrees of freedom

310	
Chapter 8  Confidence Interval Estimation
The Confidence Interval Statement
Equation (8.2) defines the 11 - a2 *  100% confidence interval estimate for the mean with 
s unknown.
Confidence Interval for the Mean (s Unknown)
X { ta>2
S
2n
	
or	
X - ta>2
S
1n … m … X +  ta>2
S
1n
(8.2)
where
ta>2 is the critical value corresponding to an upper-tail probability of a>2 (i.e., a 
cumulative area of 1 - a>2) from the t distribution with n - 1 degrees of freedom.
To illustrate the application of the confidence interval estimate for the mean when the 
standard deviation is unknown, recall the Ricknel Home Centers scenario presented on  
page 300. Using the DCOVA steps first discussed on page 30, you define the variable of inter-
est as the dollar amount listed on the sales invoices for the month. Your business objective is to 
estimate the mean dollar amount. Then you collect the data by selecting a sample of 100 sales 
invoices from the population of sales invoices during the month. Once you have collected the 
data, you organize the data in a worksheet. You can construct various graphs (not shown here) 
to better visualize the distribution of the dollar amounts. To analyze the data, you compute the 
sample mean of the 100 sales invoices to be equal to $110.27 and the sample standard devia-
tion to be equal to $28.95. For 95% confidence, the critical value from the t distribution (as 
shown in Table 8.1 on page 308) is 1.9842. Using Equation (8.2),
 X { ta>2
S
1n
 = 110.27 { 11.98422 28.95
1100
 = 110.27 { 5.74
 104.53 … m … 116.01
Figure 8.7 presents this confidence interval estimate of the mean dollar amount as computed 
by Excel and Minitab.
F i g u r e  8 . 7
Excel and Minitab results 
for the confidence 
interval estimate for 
the mean sales invoice 
amount worksheet 
for the Ricknel Home 
Centers example
Thus, with 95% confidence, you conclude that the mean amount of all the sales invoices is 
between $104.53 and $116.01. The 95% confidence level indicates that if you selected all pos-
sible samples of 100 (something that is never done in practice), 95% of the intervals developed 
would include the population mean somewhere within the interval. The validity of this ­confidence 

	
8.2  Confidence Interval Estimate for the Mean (s Unknown)	
311
interval estimate depends on the assumption of normality for the distribution of the amount of the 
sales invoices. With a sample of 100, the normality assumption is not overly restrictive, and the 
use of the t distribution is likely appropriate. Example 8.3 further illustrates how you construct 
the confidence interval for a mean when the population standard deviation is unknown.
Example 8.3
Estimating the 
Mean Processing 
Time of Life 
­Insurance 
­Applications
An insurance company has the business objective of reducing the amount of time it takes to 
approve applications for life insurance. The approval process consists of underwriting, which 
includes a review of the application, a medical information bureau check, possible requests for 
additional medical information and medical exams, and a policy compilation stage in which 
the policy pages are generated and sent for delivery. Using the DCOVA steps first discussed on 
page 30, you define the variable of interest as the total processing time in days. You collect the 
data by selecting a random sample of 27 approved policies during a period of one month. You 
organize the data collected in a worksheet. Table 8.2 lists the total processing time, in days, 
which are stored in  Insurance . To analyze the data, you need to construct a 95% confidence 
interval estimate for the population mean processing time.
T a b l e  8 . 2
Processing Time 
for Life Insurance 
Applications
73
19
16
64
28
28
31
90
60
56
31
56
22
18
45
48
17
17
17
91
92
63
50
51
69
16
17
Solution  To visualize the data, you construct a boxplot of the processing time, as dis-
played in Figure 8.8, and a normal probability plot, as shown in Figure 8.9. To analyze the 
data, you construct the confidence interval estimate shown in Figure 8.10.
F i g u r e  8 . 8
Excel and Minitab boxplots for the processing time for life insurance applications
F i g u r e  8 . 9
Excel and Minitab normal probability plots for the processing time for life insurance applications
(continued)

312	
Chapter 8  Confidence Interval Estimation
The interpretation of the confidence interval when s is unknown is the same as when 
s is known. To illustrate the fact that the confidence interval for the mean varies more 
when s is unknown, return to the example concerning the order-filling times discussed in 
Section 8.1 on pages 275 and 276. Suppose that, in this case, you do not know the popula-
tion standard deviation and instead use the sample standard deviation to construct the con-
fidence interval estimate of the mean. Figure 8.11 shows the results for each of 20 samples 
of n = 10 orders.
In Figure 8.11 on page 313, observe that the standard deviation of the samples varies from 
6.25 (sample 17) to 14.83 (sample 3). Thus, the width of the confidence interval developed var-
ies from 8.94 in sample 17 to 21.22 in sample 3. Because you know that the population mean 
order time m = 69.637 minutes, you can see that the interval for sample 8 169.68 - 85.482 
Figure 8.10 shows that the sample mean is X = 43.89 days and the sample standard de-
viation is S = 25.28 days. Using Equation (8.2) on page 310 to construct the confidence inter-
val, you need to determine the critical value from the t table, using the row for 26 degrees of 
freedom. For 95% confidence, you use the column corresponding to an upper-tail area of 0.025 
and a cumulative probability of 0.975. From Table E.3, you see that ta>2 = 2.0555. Thus, us-
ing X = 43.89, S = 25.28, n = 27, and ta>2 = 2.0555,
 X { ta>2
S
1n
 = 43.89 { 12.0555225.28
127
 = 43.89 { 10.00
 33.89 … m … 53.89
You conclude with 95% confidence that the mean processing time for the popula-
tion of life insurance applications is between 33.89 and 53.89 days. The validity of this 
confidence interval estimate depends on the assumption that the processing time is nor-
mally distributed. From the boxplot displayed in Figure 8.8 and the normal probability 
plot shown in Figure 8.9, the processing time appears right-skewed. Thus, although the 
sample size is close to 30, you would have some concern about the validity of this con-
fidence interval in estimating the population mean processing time. The concern is that a 
95% confidence interval based on a small sample from a skewed distribution will contain 
the population mean less than 95% of the time in repeated sampling. In the case of small 
sample sizes and skewed distributions, you might consider the sample median as an es-
timate of central tendency and construct a confidence interval for the population median 
(see reference 2).
F i g u r e  8 . 1 0
Excel and Minitab 
confidence interval 
estimates for the 
mean processing time 
worksheet for life 
insurance applications

	
8.2  Confidence Interval Estimate for the Mean (s Unknown)	
313
and the ­interval for sample 10 156.41 - 68.692 do not correctly estimate the population mean. 
All the other intervals correctly estimate the population mean. Once again, remember that in 
practice you select only one sample, and you are unable to know for sure whether your one 
sample provides a confidence interval that includes the population mean.
F i g u r e  8 . 1 1
Confidence interval 
estimates of the mean 
for 20 samples of n = 10 
randomly selected 
from the population of 
N = 200 orders with s 
unknown
Problems for Section 8.2
Learning the Basics
8.11  Comment on the largeness of the sample size so that S esti-
mates s closely enough and that the t distribution becomes close 
enough to standardized normal distribution.
8.12  Consider the following sample drawn from a given popula-
tion: 5, 6, 8, 10, 11, 12, 13, 15
a.	 Calculate point estimate of population mean.
b.	 Calculate standard deviation.
c.	 Determine degrees of freedom.
d.	 At 95% confidence level, calculate confidence interval for pop-
ulation mean.
e.	 At 99% confidence level, calculate confidence interval for pop-
ulation mean.
8.13  A case competition committee is required to select one from 
two schools. The members investigated the past grades of 8 ran-
domly selected students. Following are the results of the samples:
School 1: x = 67, s = 15
School 2: x = 75, s = 25
Construct the 99% confidence interval for the population mean. 
Which school should be selected?
8.14  The weights of paint cans of a particular brand present a 
normal distribution. However, a customer doubts that the cans con-
tain lesser quantity than what is written on the can packaging. He 
randomly selects 7 cans and finds the following weight in grams:
3.1, 3.4, 3.0, 3.5, 3.2, 3.3, 1.5
Construct a 95% confidence interval for the population mean, and 
suggest the possible reasons for the difference in intervals.
Applying the Concepts
8.15  A marketing researcher wants to estimate the mean savings  
($) realized by shoppers who showroom. Showrooming is the 
practice of inspecting products in retail stores and then purchas-
ing the products online at a lower price. A random sample of  
100 shoppers who recently purchased a consumer electronics item 
online after making a visit to a retail store yielded a mean savings 
of $58 and a standard deviation of $55.
a.	 Construct a 95% confidence interval estimate for the mean sav-
ings for all showroomers who purchased a consumer electron-
ics item.
b.	 Suppose the owners of a consumer electronics retailer wants  
to ­estimate the total value of lost sales attributed to the next 
1,000 showroomers that enter their retail store. How are the  
results in (a) useful in assisting the consumer electronics retailer 
in their estimation?
8.16  A survey of nonprofit organizations showed 
that online fundraising has increased in the past year. 
Based on a random sample of 55 nonprofits, the mean one-time 
gift donation in the past year was $75, with a standard deviation 
of $9.
a.	 Construct a 95% confidence interval estimate for the popula-
tion mean one-time gift donation.
b.	 Interpret the interval constructed in (a).
8.17  The U.S. Department of Transportation requires tire 
manufacturers to provide tire performance information on the 
sidewall of a tire to better inform prospective customers as they 
make purchasing decisions. One very important measure of 
tire performance is the tread wear index, which indicates the 
tire’s resistance to tread wear compared with a tire graded with 
a base of 100. A tire with a grade of 200 should last twice as 
SELF 
Test 

314	
Chapter 8  Confidence Interval Estimation
long, on average, as a tire graded with a base of 100. A con-
sumer organization wants to estimate the actual tread wear in-
dex of a brand name of tires that claims “graded 200” on the 
sidewall of the tire. A random sample of n = 18 indicates a 
sample mean tread wear index of 195.3 and a sample standard 
deviation of 21.4.
a.	 Assuming that the population of tread wear indexes is normally 
distributed, construct a 95% confidence interval estimate for 
the population mean tread wear index for tires produced by this 
manufacturer under this brand name.
b.	 Do you think that the consumer organization should ­accuse 
the manufacturer of producing tires that do not meet the 
­performance information provided on the sidewall of the tire? 
Explain.
c.	 Explain why an observed tread wear index of 210 for a particu-
lar tire is not unusual, even though it is outside the confidence 
interval developed in (a).
8.18  The file  FastFood  contains the amount that a sample of 
15 customers spent for lunch ($) at a fast-food restaurant:
7.42 6.29 5.83 6.50 8.34 9.51 7.10 6.80 5.90
4.89 6.50 5.52 7.90 8.30 9.60
a.	 Construct a 95% confidence interval estimate for the popula-
tion mean amount spent for lunch ($) at a fast-food restaurant, 
assuming a normal distribution.
b.	 Interpret the interval constructed in (a).
8.19  The file  Sedans  contains the overall miles per gallon 
(MPG) of 2013 midsized sedans:
38 26 30 26 25 27 22 27 39 24 24 26 25
23 25 26 31 26 37 22 29 25 33 21 21
Source: Data extracted from “Ratings,” Consumer Reports, April 
2013, pp. 30–31.
a.	 Construct a 95% confidence interval estimate for the popula-
tion mean MPG of 2013 family sedans, assuming a normal dis-
tribution.
b.	 Interpret the interval constructed in (a).
c.	 Compare the results in (a) to those in Problem 8.20(a).
8.20  The file  SUV  contains the overall MPG of 2013 small 
SUVs:
22 23 21 22 25 26 22 22 21
19 22 22 26 23 24 21 22
Source: Data extracted from “Ratings,” Consumer Reports, April 
2013, pp. 34–35.
a.	 Construct a 95% confidence interval estimate for the 
­population mean MPG of 2013 small SUVs, assuming a nor-
mal distribution.
b.	 Interpret the interval constructed in (a).
c.	 Compare the results in (a) to those in Problem 8.19(a).
8.21  Is there a difference in the yields of different types of in-
vestments? The file  CDRate  contains the yields for a one-year 
certificate of deposit (CD) and a five-year CD for 23 banks in the 
United States as of March 20, 2013. (Data extracted from www 
.Bankrate.com, March 20, 2013.)
a.	 Construct a 95% confidence interval estimate for the mean 
yield of one-year CDs.
b.	 Construct a 95% confidence interval estimate for the mean 
yield of five-year CDs.
c.	 Compare the results of (a) and (b).
8.22  One of the major measures of the quality of service pro-
vided by any organization is the speed with which the organization 
responds to customer complaints. A large family-held department 
store selling furniture and flooring, including carpet, had under-
gone a major expansion in the past several years. In particular, the 
flooring department had expanded from 2 installation crews to an 
installation supervisor, a measurer, and 15 installation crews. The 
store had the business objective of improving its response to com-
plaints. The variable of interest was defined as the number of days 
between when the complaint was made and when it was resolved. 
Data were collected from 50 complaints that were made in the past 
year. The data, stored in  Furniture , are as follows:
54    5    35  137    31  27  152    2  123  81  74  27
11  19  126  110  110  29    61  35    94  31  26    5
12    4  165    32    29  28    29  26    25    1  14  13
13  10      5    27      4  52    30  22    36  26  20  23
33  68
a.	 Construct a 95% confidence interval estimate for the popula-
tion mean number of days between the receipt of a complaint 
and the resolution of the complaint.
b.	 What assumption must you make about the population distribution 
in order to construct the confidence interval estimate in (a)?
c.	 Do you think that the assumption needed in order to construct 
the confidence interval estimate in (a) is valid? Explain.
d.	 What effect might your conclusion in (c) have on the validity of 
the results in (a)?
8.23  A manufacturing company produces electric insulators. You 
define the variable of interest as the strength of the insulators. If 
the insulators break when in use, a short circuit is likely. To test the 
strength of the insulators, you carry out destructive testing to deter-
mine how much force is required to break the insulators. You measure 
force by observing how many pounds are applied to the insulator be-
fore it breaks. You collect the force data for 30 insulators selected for 
the experiment and organize and store these data in  Force :
1,870 1,728 1,656 1,610 1,634 1,784 1,522 1,696
1,592 1,662 1,866 1,764 1,734 1,662 1,734 1,774
1,550 1,756 1,762 1,866 1,820 1,744 1,788 1,688
1,810 1,752 1,680 1,810 1,652 1,736
a.	 Construct a 95% confidence interval estimate for the popula-
tion mean force.
b.	 What assumption must you make about the population dis-
tribution in order to construct the confidence interval esti-
mate in (a)?
c.	 Do you think that the assumption needed in order to construct 
the confidence interval estimate in (a) is valid? Explain.
8.24  The file  MarketPenetration  contains Facebook penetration 
values (the percentage of a country’s population that are Facebook 
users) for 15 countries:
52.56 33.09  5.37 19.41 32.52 41.69 51.61 30.12
39.07 30.62 38.16 49.35 27.13 53.45 40.01
Source: Data extracted from www.socialbakers.com/facebook-
statistics/.

	
8.3  Confidence Interval Estimate for the Proportion	
315
a.	 Construct a 95% confidence interval estimate for the popula-
tion mean Facebook penetration.
b.	 What assumption do you need to make about the population to 
construct the interval in (a)?
c.	 Given the data presented, do you think the assumption needed 
in (a) is valid? Explain.
8.25  One operation of a mill is to cut pieces of steel into parts 
that are used in the frame for front seats in an automobile. The 
steel is cut with a diamond saw, and the resulting parts must be 
cut to be within {0.005 inch of the length specified by the au-
tomobile company. The measurement reported from a sample of 
100 steel parts (stored in  Steel ) is the difference, in inches, be-
tween the actual length of the steel part, as measured by a laser 
measurement device, and the specified length of the steel part. For 
example, the first observation, -0.002, represents a steel part that 
is 0.002 inch shorter than the specified length.
a.	 Construct a 95% confidence interval estimate for the popula-
tion mean difference between the actual length of the steel part 
and the specified length of the steel part.
b.	 What assumption must you make about the population distri-
bution in order to construct the confidence interval estimate in 
(a)?
c.	 Do you think that the assumption needed in order to construct 
the confidence interval estimate in (a) is valid? Explain.
d.	 Compare the conclusions reached in (a) with those of Problem 
2.43 on page 92.
8.3  Confidence Interval Estimate for the Proportion
The concept of a confidence interval also applies to categorical data. With categorical data, 
you want to estimate the proportion of items in a population having a certain characteristic of 
interest. The unknown population proportion is represented by the Greek letter p. The point 
estimate for p is the sample proportion, p = X>n, where n is the sample size and X is the 
number of items in the sample having the characteristic of interest. Equation (8.3) defines the 
confidence interval estimate for the population proportion.
Student Tip
As noted in Chapter 7,  
do not confuse this 
use of the Greek letter 
pi, p, to represent the 
population proportion 
with the mathemati-
cal constant that is the 
ratio of the circumfer-
ence to a ­diameter of a 
circle—approximately 
3.14159—which is 
also known by the same 
Greek letter.
Student Tip
Remember, the sample 
proportion, p, must be 
between 0 and 1.
Confidence Interval Estimate for the Proportion
p { Za>2B
p11 - p2
n
	
or
	
p - Za>2B
p11 - p2
n
… p … p + Za>2B
p11 - p2
n
	
(8.3)
	
where
 p = sample proportion = X
n = Number of items having the characteristic
sample size
 p = population proportion
 Za>2 = critical value from the standardized normal distribution
 n = sample size
Note: To use this equation for the confidence interval, the sample size n must be large 
enough to ensure that both X and n - X are greater than 5.
You can use the confidence interval estimate for the proportion defined in Equation (8.3) 
to estimate the proportion of sales invoices that contain errors (see the Ricknel Home Centers 
scenario on page 300). Using the DCOVA steps, you define the variable of interest as whether 
the invoice contains errors (yes or no). Then, you collect the data from a sample of 100 sales in-
voices. The results, which you organize and store in a worksheet, show that 10 invoices ­contain 
errors. To analyze the data, you compute, for these data, p = X>n = 10>100 = 0.10. Since 
both X = 10 and n - X = 100 - 10 = 90 are 7  5, using Equation (8.3) and Za>2 = 1.96, 
for 95% confidence,

316	
Chapter 8  Confidence Interval Estimation
 p { Za>2B
p11 - p2
n
 = 0.10 { 11.962B
10.10210.902
100
 = 0.10 { 11.96210.032
 = 0.10 { 0.0588
0.0412 … p … 0.1588
Therefore, you have 95% confidence that the population proportion of all sales invoices contain-
ing errors is between 0.0412 and 0.1588. This means that between 4.12% and 15.88% of all the 
sales invoices contain errors. Figure 8.12 shows a confidence interval estimate for this example.
F i g u r e  8 . 1 2
Excel and Minitab 
confidence interval 
estimates for the 
proportion of sales 
invoices that contain 
errors worksheet
Example 8.4 illustrates another application of a confidence interval estimate for the 
­proportion.
Example 8.4
Estimating the 
Proportion of 
­Nonconforming 
Newspapers 
Printed
The operations manager at a large newspaper wants to estimate the proportion of newspapers 
printed that have a nonconforming attribute. Using the DCOVA steps, you define the variable 
of interest as whether the newspaper has excessive ruboff, improper page setup, missing pages, 
or duplicate pages. You collect the data by selecting a random sample of n = 200 newspapers 
from all the newspapers printed during a single day. You organize the results in a worksheet, 
which shows that 35 newspapers contain some type of nonconformance. To analyze the data, 
you need to construct and interpret a 90% confidence interval estimate for the proportion of 
newspapers printed during the day that have a nonconforming attribute.
Solution  Using Equation (8.3),
 p = X
n = 35
200 = 0.175, and with a 90% level of confidence Za>2 = 1.645
p { Za>2B
p11 - p2
n
 = 0.175 { 11.6452B
10.175210.8252
200
 = 0.175 { 11.645210.02692
 = 0.175 { 0.0442
0.1308 … p … 0.2192
You conclude with 90% confidence that the population proportion of all newspapers printed that 
day with nonconformities is between 0.1308 and 0.2192. This means you estimate that between 
13.08% and 21.92% of the newspapers printed on that day have some type of nonconformance.

	
8.3  Confidence Interval Estimate for the Proportion	
317
Equation (8.3) contains a Z statistic because you can use the normal distribution to 
­approximate the binomial distribution when the sample size is sufficiently large. In Example 8.4, 
the confidence interval using Z provides an excellent approximation for the population propor-
tion because both X and n - X are greater than 5. However, if you do not have a sufficiently 
large sample size, you should use the binomial distribution rather than Equation (8.3) (see refer-
ences 1, 3, and 9). The exact confidence intervals for various sample sizes and proportions of 
items of interest have been tabulated by Fisher and Yates (reference 3) and can also be computed 
using Minitab.
Problems for Section 8.3
Learning the Basics
8.26  If n = 200 and X = 50, construct a 95% confidence inter-
val estimate for the population proportion.
8.27  If n = 400 and X = 25, construct a 99% confidence inter-
val estimate for the population proportion.
Applying the Concepts
8.28  A cellphone provider has the business objective 
of wanting to estimate the proportion of subscribers 
who would upgrade to a new cellphone with improved features 
if it were made available at a substantially reduced cost. Data are 
collected from a random sample of 500 subscribers. The results 
indicate that 135 of the subscribers would upgrade to a new cell-
phone at a reduced cost.
a.	 Construct a 99% confidence interval estimate for the popula-
tion proportion of subscribers that would upgrade to a new cell-
phone at a reduced cost.
b.	 How would the manager in charge of promotional programs 
use the results in (a)?
8.29  The results of the National Health and Nutrition Examina-
tion Survey for the year 2011-12 found that 12.9% of the adults of 
age 20 and more in the United States had high total cholesterol. 
(Source: http://www.cdc.gov/nchs/data/databriefs/db132.htm).
a.	 Assume that a researcher surveys a sample size of 100 for 
above 20 years. Construct a 95% confidence interval estimate 
for the population proportion so that the cholesterol level is the 
same as 12.9%.
b.	 Assume that a researcher surveys a sample size of 1,000 for 
more than 20 years. Construct a 95% confidence interval esti-
mate for the population proportion so that the cholesterol level 
is the same as 12.9%.
c.	 Compare the results in (a) and (b).
8.30  Do students start working after they get a bachelor’s or a 
master’s degree? According to the survey of 47,626 college gradu-
ates, it was found that only 19,557 students hold the master’s degree 
and the rest 28,069 hold the bachelor’s degree. However, the report 
also shows that master’s degree holders earn a higher salary than 
the bachelor’s degree holders with little standard deviation (Source: 
http://www.bls.gov/cex/2012/combined/educat.pdf).
a.	 Construct a 95% confidence interval estimate for the popula-
tion proportion that holds the master’s degree among the college 
graduates.
b.	 Construct a 95% confidence interval estimate for the popula-
tion proportion that holds the bachelor’s degree among the col-
lege graduates.
c.	 Construct a 99% confidence interval estimate for the popula-
tion proportion that holds the bachelor’s degree among the col-
lege graduates.
d.	 Construct a 99% confidence interval estimate for the popula-
tion proportion that holds the master’s degree among the col-
lege graduates.
e.	 Summarize the results in (a) through (d) for the impact of 
changes in the confidence intervals.
8.31  The management of a school wants to find out the absentee-
ism rate of the students in all of its 10 branches. They selected a ran-
dom sample of 100 children and found that on an average there was 
an absenteeism rate of 7% in a semester. Construct a 95% confi-
dence interval estimate for the population proportion of the students 
who are absent from the school in the semester. What will be the 
results if the sample size changes to 500. What does this confidence 
interval depict?
8.32  An insurance company proposes two health insurance plans 
for the employees of a company. Plan A is the basic plan and cov-
ers only the medication expenses in case of hospitalization, whereas 
plan B reimburses all the medical expenses of the employees. As a 
human resources manager, you are required to estimate how many 
employees will opt for Plan B. The manager randomly asks 50 em-
ployees and 17 of them prefer Plan B.
a.	 Construct a 95% confidence interval estimate for the popula-
tion proportion of employees who will prefer Plan B. Interpret 
the results of the confidence interval.
b.	 Construct a 99% confidence interval estimate for the popula-
tion proportion of employees who will prefer Plan B. Interpret 
the results of the confidence interval.
c.	 Compare the results in (a) and (b) for different levels of con-
fidence interval estimate for the population proportion of em-
ployees who will prefer plan B.
8.33  In a survey, it was found that consuming fruit juices is not as 
healthy as consuming fruit. It is believed that fruit juices contain ar-
tificial flavors and preservatives (Source: http://www.fda.gov/food/
guidanceregulation/guidancedocumentsregulatoryinformation/
labelingnutrition/ucm064880.htm). Assume that in a survey to as-
sess whether or not people believe fruit juices contain artificial fla-
vor, 53% believed that fruit juices contain artificial flavors.
a.	 In a random sample survey of 200, construct a 95% confidence 
interval for the population proportion who believe that fruit juices 
contain artificial flavors. Do the results indicate that the majority 
of the population believes that they contain artificial flavors?
b.	 Now modify the sample from 200 to 2,500, construct a 95% 
confidence interval for the population proportion that believes 
that fruit juices contain artificial flavors. What do the results 
indicate now?
SELF 
Test 

318	
Chapter 8  Confidence Interval Estimation
8.4  Determining Sample Size
In each confidence interval developed so far in this chapter, the sample size was reported along 
with the results, with little discussion of the width of the resulting confidence interval. In the 
business world, sample sizes are determined prior to data collection to ensure that the confi-
dence interval is narrow enough to be useful in making decisions. Determining the proper sam-
ple size is a complicated procedure, subject to the constraints of budget, time, and the amount 
of acceptable sampling error. In the Ricknel Home Centers scenario, if you want to estimate 
the mean dollar amount of the sales invoices, you must determine in advance how large a sam-
pling error to allow in estimating the population mean. You must also determine, in advance, 
the level of confidence (i.e., 90%, 95%, or 99%) to use in estimating the population parameter.
Sample Size Determination for the Mean
To develop an equation for determining the appropriate sample size needed when constructing 
a confidence interval estimate for the mean, recall Equation (8.1) on page 304:
X { Za>2 s
1n
The amount added to or subtracted from X is equal to half the width of the interval. This quan-
tity represents the amount of imprecision in the estimate that results from sampling error.2 The 
sampling error, e, is defined as
e = Za>2 s
1n
Solving for n gives the sample size needed to construct the appropriate confidence interval 
estimate for the mean. “Appropriate” means that the resulting interval will have an acceptable 
amount of sampling error.
2In this context, some statisticians 
refer to e as the margin of ­error.
Sample Size Determination for the Mean
The sample size, n, is equal to the product of the Za>2 value squared and the standard devia-
tion, s, squared, divided by the square of the sampling error, e.
	
n =
Z2
a>2 s2
e2
	
(8.4)
To compute the sample size, you must know three quantities:
 • The desired confidence level, which determines the value of Za>2, the critical value from 
the standardized normal distribution3
 • The acceptable sampling error, e
 • The standard deviation, s
In some business-to-business relationships that require estimation of important parameters, 
legal contracts specify acceptable levels of sampling error and the confidence level required. 
For companies in the food and drug sectors, government regulations often specify sampling er-
rors and confidence levels. In general, however, it is usually not easy to specify the three quan-
tities needed to determine the sample size. How can you determine the level of confidence and 
sampling error? Typically, these questions are answered only by a subject matter expert (i.e., 
an individual very familiar with the variables under study). Although 95% is the most common 
confidence level used, if more confidence is desired, then 99% might be more appropriate; if 
3You use Z instead of t because, 
to determine the critical value of 
t, you need to know the sample 
size, but you do not know it yet. 
For most studies, the sample size 
needed is large enough that the 
standardized normal distribution is 
a good approximation of the t dis-
tribution.

	
8.4  Determining Sample Size	
319
less confidence is deemed acceptable, then 90% might be used. For the sampling error, you 
should think not of how much sampling error you would like to have (you really do not want 
any error) but of how much you can tolerate when reaching conclusions from the confidence 
interval.
In addition to specifying the confidence level and the sampling error, you need an estimate 
of the standard deviation. Unfortunately, you rarely know the population standard deviation, s. 
In some instances, you can estimate the standard deviation from past data. In other situations, 
you can make an educated guess by taking into account the range and distribution of the vari-
able. For example, if you assume a normal distribution, the range is approximately equal to 6s 
(i.e.,{3s around the mean) so that you estimate s as the range divided by 6. If you cannot 
estimate s in this way, you can conduct a small-scale study and estimate the standard deviation 
from the resulting data.
To explore how to determine the sample size needed for estimating the population mean, 
consider again the audit at Ricknel Home Centers. In Section 8.2, you selected a sample of 
100 sales invoices and constructed a 95% confidence interval estimate for the population mean 
sales invoice amount. How was this sample size determined? Should you have selected a dif-
ferent sample size?
Suppose that, after consulting with company officials, you determine that a sampling er-
ror of no more than { +5 is desired, along with 95% confidence. Past data indicate that the 
standard deviation of the sales amount is approximately $25. Thus, e = +5, s = +25, and 
Za>2 = 1.96 (for 95% confidence). Using Equation (8.4),
 n =
Z2
a>2s2
e2
= 11.962212522
1522
 = 96.04
Because the general rule is to slightly oversatisfy the criteria by rounding the sample size 
up to the next whole integer, you should select a sample of size 97. Thus, the sample of size 
n = 100 used on page 310 is slightly more than what is necessary to satisfy the needs of 
the company, based on the estimated standard deviation, desired confidence level, and sam-
pling error. Because the calculated sample standard deviation is slightly higher than expected, 
$28.95 compared to $25.00, the confidence interval is slightly wider than desired. Figure 8.13 
shows a worksheet for determining the sample size.
F i g u r e  8 . 1 3
Excel worksheet for 
determining the sample 
size for estimating the 
mean sales invoice 
amount for the Ricknel 
Home Centers example
Example 8.5 illustrates another application of determining the sample size needed to  
develop a confidence interval estimate for the mean.
Example 8.5
Determining the 
Sample Size for the 
Mean
Returning to Example 8.3 on page 311, suppose you want to estimate, with 95% confidence, 
the population mean processing time to within {4 days. On the basis of a study conducted 
the previous year, you believe that the standard deviation is 25 days. Determine the sample 
size needed.
(continued)

320	
Chapter 8  Confidence Interval Estimation
Sample Size Determination for the Proportion
So far in this section, you have learned how to determine the sample size needed for estimating 
the population mean. Now suppose that you want to determine the sample size necessary for 
estimating a population proportion.
To determine the sample size needed to estimate a population proportion, p, you use a 
method similar to the method for a population mean. Recall that in developing the sample size 
for a confidence interval for the mean, the sampling error is defined by
e = Za>2 s
1n
When estimating a proportion, you replace s with 2p11 - p2. Thus, the sampling error is
e = Za>2B
p11 - p2
n
Solving for n, you have the sample size necessary to develop a confidence interval estimate for 
a proportion.
Solution  Using Equation (8.4) on page 318 and e = 4, s = 25, and Za>2 = 1.96 for 
95% confidence,
 n =
Z2
a>2 s2
e2
= 11.962212522
1422
 = 150.06
Therefore, you should select a sample of 151 applications because the general rule for deter-
mining sample size is to always round up to the next integer value in order to slightly oversat-
isfy the criteria desired. An actual sampling error slightly larger than 4 will result if the sample 
standard deviation calculated in this sample of 151 is greater than 25 and slightly smaller if the 
sample standard deviation is less than 25.
Sample Size Determination for the Proportion
The sample size n is equal to the product of Za>2 squared, the population proportion, p, and 
1 minus the population proportion, p, divided by the square of the sampling error, e.
	
n =
Z2
a>2p11 - p2
e2
	
(8.5)
To determine the sample size, you must know three quantities:
 • The desired confidence level, which determines the value of Za>2, the critical value from 
the standardized normal distribution
 • The acceptable sampling error (or margin of error), e
 • The population proportion, p
In practice, selecting these quantities requires some planning. Once you determine the 
­desired level of confidence, you can find the appropriate Za>2 value from the standardized 
normal distribution. The sampling error, e, indicates the amount of error that you are will-
ing to tolerate in estimating the population proportion. The third quantity, p, is actually the 

	
8.4  Determining Sample Size	
321
­population parameter that you want to estimate! Thus, how do you state a value for what you 
are trying to determine?
Here you have two alternatives. In many situations, you may have past information or rel-
evant experience that provides an educated estimate of p. If you do not have past information 
or relevant experience, you can try to provide a value for p that would never underestimate the 
sample size needed. Referring to Equation (8.5), you can see that the quantity p11 - p2 ap-
pears in the numerator. Thus, you need to determine the value of p that will make the quantity 
p11 - p2 as large as possible. When p = 0.5, the product p11 - p2 achieves its maximum 
value. To show this result, consider the following values of p, along with the accompanying 
products of p11 - p2 :
 When p = 0.9, then p11 - p2 = 10.9210.12 = 0.09.
 When p = 0.7, then p11 - p2 = 10.7210.32 = 0.21.
 When p = 0.5, then p11 - p2 = 10.5210.52 = 0.25.
 When p = 0.3, then p11 - p2 = 10.3210.72 = 0.21.
 When p = 0.1, then p11 - p2 = 10.1210.92 = 0.09.
Therefore, when you have no prior knowledge or estimate for the population proportion, 
p, you should use p = 0.5 for determining the sample size. Using p = 0.5 produces the 
largest possible sample size and results in the narrowest and most precise confidence interval. 
This increased precision comes at the cost of spending more time and money for an increased 
sample size. Also, note that if you use p = 0.5 and the proportion is different from 0.5, you 
will overestimate the sample size needed, because you will get a confidence interval narrower 
than originally intended.
Returning to the Ricknel Home Centers scenario on page 300, suppose that the auditing 
procedures require you to have 95% confidence in estimating the population proportion of sales 
invoices with errors to within {0.07. The results from past months indicate that the largest 
proportion has been no more than 0.15. Thus, using Equation (8.5) with e = 0.07, p = 0.15, 
and Za>2 = 1.96 for 95% confidence,
 n =
Z2
a>2p11 - p2
e2
 = 11.962210.15210.852
10.0722
 = 99.96
Because the general rule is to round the sample size up to the next whole integer to slightly 
oversatisfy the criteria, a sample size of 100 is needed. Thus, the sample size needed to satisfy 
the requirements of the company, based on the estimated proportion, desired confidence level, 
and sampling error, is equal to the sample size taken on page 315. The actual confidence inter-
val is narrower than required because the sample proportion is 0.10, whereas 0.15 was used for 
p in Equation (8.5). Figure 8.14 shows a worksheet for determining the sample size.
F i g u r e  8 . 1 4
Excel worksheet for 
determining sample 
size for estimating the 
proportion of in-error 
sales invoices for Ricknel 
Home Centers

322	
Chapter 8  Confidence Interval Estimation
Example 8.6 provides another application of determining the sample size for estimating 
the population proportion.
Example 8.6
Determining the 
Sample Size for 
the Population 
­Proportion
You want to have 90% confidence of estimating the proportion of office workers who respond 
to email within an hour to within {0.05. Because you have not previously undertaken such a 
study, there is no information available from past data. Determine the sample size needed.
Solution  Because no information is available from past data, assume that p = 0.50. ­Using 
Equation (8.5) on page 320 and e = 0.05, p = 0.50, and Za>2 = 1.645 for 90% ­confidence,
 n =
Z2
a>2 p11 - p2
e2
 = 11.6452210.50210.502
10.0522
 = 270.6
Therefore, you need a sample of 271 office workers to estimate the population proportion to 
within{0.05 with 90% confidence.
Problems for Section 8.4
Learning the Basics
8.34  Determination of the ‘appropriate’ confidence intervals re-
quires a sufficient sample size. Why is it important to determine 
the sample size? In real life, is the sample size determined before 
or after data collection?
8.35  What are the various components required for determin-
ing the sample size for the mean? Explain in detail the meaning 
and relevance of the concept of acceptable sampling error. How is 
sampling error determined?
8.36  What are the various components required for determining 
the sample size for the proportion? Explain in detail the meaning 
of population parameter. How is population parameter determined?
8.37  Population parameter is an important component for the de-
termination of the sample size for the proportion. Can you also 
determine the sample size when population parameter p is un-
known? How?
Applying the Concepts
8.38  A firm’s advertising policy for the following year will be 
dependent on the market preference for their product. Thus, the 
company is willing to estimate with 95% confidence level and the 
standard error of 0.05.
a.	 Assume that preliminary reports indicate that 25% of users pre-
fer their product, determine the sample size to make an esti-
mate for the preference of the firm’s product.
b.	 Assuming that the firm does not know the population propor-
tion, what sample size should be used by the firm?
8.39  The weekly salaries of trainees immediately after their mas-
ter’s degree have $100 as standard deviation. Specify the sample 
size when the acceptable sampling error is within $20 or less at 
95% confidence level of determining the average weekly salaries 
of the trainees after their master’s degree.
8.40  If a light bulb manufacturing company wants to estimate, 
with 95% confidence, the mean life of compact fluorescent light 
bulbs to within {200 hours and also assumes that the population 
standard deviation is 1,000 hours, how many compact fluorescent 
light bulbs need to be selected?
8.41  If the inspection division of a county weights and measures de-
partment wants to estimate the mean amount of soft-drink fill in 2-liter 
bottles to within {0.01 liter with 95% confidence and also assumes 
that the standard deviation is 0.05 liter, what sample size is needed?
8.42  A private energy supplier wants to estimate the monthly 
electricity utilization bill of a household in metro cities. Based on 
the reports published by the other suppliers, the standard deviation 
was found to be $20. The company wants to estimate the average 
monthly bill with 95% confidence level, assuming the maximum 
acceptable variation of $5.
a.	 Determine the sample size the company should be studying.
b.	 Will the sample size change if confidence level changes to 99%?
8.43  An advertising agency that serves a major radio station 
wants to estimate the mean amount of time that the station’s 
­audience spends listening to the radio daily. From past studies, the 
standard deviation is estimated as 45 minutes.
a.	 What sample size is needed if the agency wants to be 90% con-
fident of being correct to within {5 minutes?
b.	 If 99% confidence is desired, how many listeners need to be 
selected?
8.44  A growing niche in the restaurant business is gourmet-casual 
breakfast, lunch, and brunch. Chains in this group include EggSpec-
tation and Panera Bread. Suppose that the mean per-person check 

	
8.5  Confidence Interval Estimation and Ethical Issues	
323
for breakfast at EggSpectation is approximately $14.50, and the 
mean per-person check for Panera Bread is $8.50.
a.	 Assuming a standard deviation of $2.00, what sample size is 
needed to estimate, with 95% confidence, the mean per-person 
check for EggSpectation to within { +0.25?
b.	 Assuming a standard deviation of $2.50, what sample size is 
needed to estimate, with 95% confidence, the mean per-person 
check for EggSpectation to within { +0.25?
c.	 Assuming a standard deviation of $3.00, what sample size is 
needed to estimate, with 95% confidence, the mean per-person 
check for EggSpectation to within { +0.25?
d.	 Discuss the effect of variation on the sample size needed.
8.45  What advertising medium is most influential in making a 
purchase decision? According to a TVB survey, 37.2% of Ameri-
can adults point to TV. (Data extracted from “TV Seen Most In-
fluential Ad Medium for Purchase Decisions,” MC Marketing 
Charts, June 18, 2012.)
a.	 To conduct a follow-up study that would provide 95% confi-
dence that the point estimate is correct to within {0.04 of the 
population proportion, how large a sample size is required?
b.	 To conduct a follow-up study that would provide 99%  
confidence that the point estimate is correct to within {0.04  
of the population proportion, how many people need to be  
sampled?
c.	 To conduct a follow-up study that would provide 95% confi-
dence that the point estimate is correct to within {0.02 of the 
population proportion, how large a sample size is required?
d.	 To conduct a follow-up study that would provide 99%  
confidence that the point estimate is correct to within {0.02  
of the population proportion, how many people need to be  
sampled?
e.	 Discuss the effects on sample size requirements of changing 
the desired confidence level and the acceptable sampling error.
8.46  A survey of 300 U.S. online shoppers was conducted. In 
response to the question of what would influence the shopper to 
spend more money online in 2012, 18% said free shipping, 13% 
said offering discounts while shopping, and 9% said product re-
views. (Data extracted from “2012 Consumer Shopping Trends 
and Insights,” Steelhouse, Inc., 2012.) Construct a 95% confidence 
interval estimate of the population proportion of online shop-
pers who would be influenced to spend more money online in  
2012 with
a.	 free shipping.
b.	 discounts offered while shopping.
c.	 product reviews.
d.	 You have been asked to update the results of this study. Determine 
the sample size necessary to estimate, with 95% confidence, the 
population proportions in (a) through (c) to within {0.02.
8.47  In a study of 368 San Francisco Bay Area nonprofits, 224 
reported that they are collaborating with other organizations to 
provide services, a necessity as nonprofit agencies are called upon 
to do more with less. (Data extracted from “2012 Nonprofit Pulse 
Survey,” United Way of the Bay Area, 2012, bit.ly/MkGINA.)
a.	 Construct a 95% confidence interval for the proportion of San 
Francisco Bay Area nonprofits that collaborated with other or-
ganizations to provide services.
b.	 Interpret the interval constructed in (a).
c.	 If you wanted to conduct a follow-up study to estimate the 
population proportion of San Francisco Bay Area nonprofits 
that collaborated with other organizations to provide service to 
within{0.01 with 95% confidence, how may Bay Area non-
profits would you survey?
8.48  According to a new study released by Infosys, a global 
leader in consulting, outsourcing, and technology, more than 
three-quarters (77%) of U.S. consumers say that banking on their 
mobile device is convenient. (Data extracted from “Infosys Survey 
Finds Mobile Banking Customers Love Ease and Convenience, 
Yet Reliability and Security Concerns Remain,” PR Newswire, 
2012, bit.ly/Ip9RUF.)
a.	 If you conduct a follow-up study to estimate the population 
proportion of U.S. consumers who say that banking on their 
mobile device is convenient, would you use a p of 0.77 or 0.50 
in the sample size formula?
b.	 Using you answer in part (a), find the sample size necessary 
to estimate, with 95% confidence, the population proportion to 
within {0.03.
8.49  Which store do you think is more expensive—physical or 
online? A recent survey (USA Today, December 10, 2012, p. 1B) 
found that 46% of people aged 20 to 40 thought that physical 
stores were more expensive.
a.	 To conduct a follow-up study that would provide 99% confi-
dence that the point estimate is correct to within {0.03 of the 
population proportion, how many people aged 20 to 40 need to 
be sampled?
b.	 To conduct a follow-up study that would provide 99% confi-
dence that the point estimate is correct to within {0.05 of the 
population proportion, how many people aged 20 to 40 need to 
be sampled?
c.	 Compare the results of (a) and (b).
8.5  Confidence Interval Estimation and Ethical Issues
The selection of samples and the inferences that accompany them raise several ethical ­issues. 
The major ethical issue concerns whether confidence interval estimates accompany point 
­estimates. Failure to include a confidence interval estimate might mislead the user of the 
­results into thinking that the point estimate is all that is needed to predict the population char-
acteristic with certainty. Confidence interval limits (typically set at 95%), the sample size used, 
and an interpretation of the meaning of the confidence interval in terms that a person untrained 
in statistics can understand should always accompany point estimates.
When media outlets publicize the results of a political poll, they often overlook including 
this type of information. Sometimes, the results of a poll include the sampling error, but the 
sampling error is often presented in fine print or as an afterthought to the story being reported. 

324	
Chapter 8  Confidence Interval Estimation
A fully ethical presentation of poll results would give equal prominence to the confidence 
­levels, sample size, sampling error, and confidence limits of the poll.
When you prepare your own point estimates, always state the interval estimate in a promi-
nent place and include a brief explanation of the meaning of the confidence interval. In addi-
tion, make sure you highlight the sample size and sampling error.
8.6  Application of Confidence Interval Estimation  
in Auditing
Auditing is the collection and evaluation of evidence about information related to an economic 
entity in order to determine and report on how well the information corresponds to established 
criteria. Auditing uses probability sampling methods to develop confidence interval estimates. 
The Section 8.6 online topic reviews three common applications of confidence ­interval esti-
mation in auditing.
8.7  Estimation and Sample Size Estimation  
for Finite Populations
To develop confidence interval estimates for population parameters or determine sample sizes 
when estimating population parameters, you use the finite population correction factor when 
samples are selected without replacement from a finite population. The Section 8.7 online 
topic explains how to use the finite population correction factor for these purposes.
8.8  Bootstrapping
The confidence interval estimation procedures discussed in this chapter make assumptions that 
are often not valid, especially for small samples. Bootstrapping, the selection of an initial sam-
ple and repeated sampling from that initial sample, provides an alternative approach that does 
not rely on those assumptions. The Section 8.8 online topic explains this alternative technique.
I
n the Ricknel Home Centers scenario, you were an 
­accountant for a distributor of home improvement supplies 
in the northeastern United States. You were responsible for the 
­accuracy of the integrated inventory management and sales 
information system. You used confidence interval estimation 
techniques to draw conclusions about the population of all re-
cords from a relatively small sample collected during an audit.
At the end of the month, you collected a random sample 
of 100 sales invoices and made the following inferences:
 • With 95% confidence, you concluded that the mean 
amount of all the sales invoices is between $104.53 
and $116.01.
 • With 95% 
confidence, 
you concluded that between 4.12% and 15.88% of all 
the sales invoices contain errors.
These estimates provide an interval of values that you 
believe contain the true population parameters. If these in-
tervals are too wide (i.e., the sampling error is too large) for 
the types of decisions Ricknel Home Centers needs to make, 
you will need to take a larger sample. You can use the sample 
size formulas in Section 8.4 to determine the number of sales 
invoices to sample to ensure that the size of the sampling  
error is acceptable.
U s i n g  S tat i s t i c s
Getting Estimates at  
Ricknel Home Centers, Revisited
Mangostock/Shutterstock

	
Key Equations	
325
S u m m a r y
This chapter discusses confidence intervals for estimating the 
characteristics of a population, along with how you can de-
termine the necessary sample size. You learned how to apply 
these methods to numerical and categorical data. Table 8.3  
provides a list of topics covered in this chapter.
To determine what equation to use for a particular situa-
tion, you need to answer these questions:
 
•  Are you constructing a confidence interval, or are you 
determining sample size?
 
•  Do you have a numerical variable, or do you have a 
categorical variable?
The next four chapters develop a hypothesis-testing ap-
proach to making decisions about population parameters.
Type of Data
Type of Analysis
Numerical
Categorical
Confidence interval for a 
population parameter
Confidence interval estimate 
for the mean (Sections 8.1 
and 8.2)
Confidence interval estimate 
for the proportion (Section 8.3)
Determining sample size
Sample size determination 
for the mean (Section 8.4)
Sample size determination 
for the proportion (Section 
8.4)
Refere n c e s
	 1.	Cochran, W. G. Sampling Techniques, 3rd ed. New York:  
Wiley, 1977.
	 2.	Daniel, W. W. Applied Nonparametric Statistics, 2nd ed.  
Boston: PWS Kent, 1990.
	 3.	Fisher, R. A., and F. Yates. Statistical Tables for Biological, 
Agricultural and Medical Research, 5th ed. Edinburgh: Oliver 
& Boyd, 1957.
	 4.	Hahn, G., and W. Meeker. Statistical Intervals: A Guide for 
Practitioners. New York: John Wiley and Sons, Inc., 1991.
	 5.	Kirk, R. E., ed. Statistical Issues: A Reader for the Behavioral 
Sciences. Belmont, CA: Wadsworth, 1972.
	 6.	Larsen, R. L., and M. L. Marx. An Introduction to Mathemati-
cal Statistics and Its Applications, 4th ed. Upper Saddle River, 
NJ: Prentice Hall, 2006.
	 7.	Microsoft Excel 2013. Redmond, WA: Microsoft Corp., 2012.
	 8.	Minitab Release 16. State College, PA: Minitab, Inc., 2010.
	 9.	Snedecor, G. W., and W. G. Cochran. Statistical Methods,  
7th ed. Ames, IA: Iowa State University Press, 1980.
K ey Eq u at i o n s
Confidence Interval for the Mean (S Known)
X { Za>2
s
2n
or
X - Za>2
s
2n
… m … X + Za>2
s
2n
	
(8.1)
Confidence Interval for the Mean (S Unknown)
X { ta>2
S
2n
or
X - ta>2
S
2n
… m … X + ta>2
S
2n
	
(8.2)
Confidence Interval Estimate for the Proportion
p { Za>2A
p(1 - p)
n
or
p - Za>2A
p11 - p2
n
… p … p + Za>2A
p11 - p2
n
 (8.3)
Sample Size Determination for the Mean
n =
Z2
a>2 s2
e2

(8.4)
Sample Size Determination for the Proportion
n =
Z2
a>2 p11 - p2
e2

(8.5)
T a b l e  8 . 3
Summary of Topics in 
Chapter 8

326	
Chapter 8  Confidence Interval Estimation
K e y  Term s
confidence interval estimate  301
critical value  305
degrees of freedom  307
level of confidence  304
margin of error  318
point estimate  301
sampling error  304
Student’s t distribution  307
C hec ki n g  Yo u r  U n d e r s ta nding
8.50  What are the components for finding a confidence interval? 
What happens to confidence intervals when the confidence level is 
increased?
8.51  Given the sample data, how can we determine confidence 
intervals when the standard deviation of population is unknown?
8.52  How are confidence levels decided in real life? Can 100% 
confidence level be used to determine a population parameter?
8.53  When the population proportion is unknown, can you deter-
mine the sample size? If yes, what is the peculiarity of this sample 
size?
C ha pter  R e vi e w P r o b le ms
8.54  The Pew Internet Project survey of 2,253 American adults 
(data extracted from pewinternet.org/Commentary/2012/Febru-
ary/Pew-Internet-Mobile) found the following:
1,983 have a cellphone
1,307 have a desktop computer
1,374 have a laptop computer
406 have an ebook reader
406 have a tablet computer
a.	 Construct 95% confidence interval estimates for the population 
proportion of the electronic devices adults own.
b.	 What conclusions can you reach concerning what electronic 
devices adults have?
8.55  What do Americans do to conserve energy? The Associ-
ated Press-NORC Center for Public Affairs Research conducted 
a survey of 897 adults who had personally done something to try 
to save energy in the last year (data extracted from “Energy Ef-
ficiency and Independence: How the Public Understands, Learns, 
and Acts,” bit.ly/Maw5hd), and found the following percentages:
Turn off lights: 39%
Turn down heat: 26%
Install more energy-saving appliances: 23%
Drive less/walk more/bicycle more: 18%
Unplug things: 16%
a.	 Construct a 95% confidence interval estimate for the popula-
tion proportion of what adults do to conserve energy.
b.	 What conclusions can you reach concerning what adults do to 
conserve energy?
8.56  A market researcher for a consumer electronics company 
wants to study the media viewing behavior of residents of a par-
ticular area. A random sample of 40 respondents is selected, and 
each respondent is instructed to keep a detailed record of time 
spent engaged viewing content across all screens (traditional TV, 
DVD/Blu-ray, game console, Internet on a computer, video on a 
computer, video on a mobile phone) in a particular week. The re-
sults are as follows:
• Content viewing time per week: X = 41 hours, S = 3.5 
hours.
• 30 respondents have high definition (HD) on at least one 
television set.
a.	 Construct a 95% confidence interval estimate for the mean con-
tent viewing time per week in this area.
b.	 Construct a 95% confidence interval estimate for the population 
proportion of residents who have HD on at least one television set.
Suppose that the market researcher wants to take another survey in 
a different location. Answer these questions:
c.	 What sample size is required to be 95% confident of estimating 
the population mean content viewing time to within {2 hours as-
suming that the population standard deviation is equal to 5 hours?
d.	 How many respondents need to be selected to be 95% confident 
of being within {0.06 of the population proportion who have 
HD on at least one television set if no previous estimate is available?
e.	 Based on (c) and (d), how many respondents should the market 
researcher select if a single survey is being conducted?
8.57  An information technology (IT) consulting firm special-
izing in healthcare solutions wants to study communication defi-
ciencies in the health care industry. A random sample of 70 health 
care clinicians reveals the following:
• Time wasted in a day due to outdated communication tech-
nologies: X = 45 minutes, S = 10 minutes.
• Thirty-six health care clinicians cite inefficiency of pagers 
as the reason for the wasted time.
a.	 Construct a 99% confidence interval estimate for the popula-
tion mean time wasted in a day due to outdated communication 
technologies.
b.	 Construct a 95% confidence interval estimate for the popula-
tion proportion of health care clinicians who cite inefficiency 
of pagers as the reason for the wasted time.

	
Chapter Review Problems	
327
8.58  The human resource (HR) director of a large corporation 
wishes to study absenteeism among its mid-level managers at its 
central office during the year. A random sample of 25 mid-level 
managers reveals the following:
• Absenteeism: X = 6.2 days, S = 7.3 days.
• 13 mid-level managers cite stress as a cause of absence.
a.	 Construct a 95% confidence interval estimate for the mean 
number of absences for mid-level managers during the year.
b.	 Construct a 95% confidence interval estimate for the popula-
tion proportion of mid-level managers who cite stress as a 
cause of absence.
Suppose that the HR director wishes to administer a survey in one 
of its regional offices. Answer these questions:
c.	 What sample size is needed to have 95% confidence in estimat-
ing the population mean absenteeism to within {1.5 days if 
the population standard deviation is estimated to be 8 days?
d.	 How many mid-level managers need to be selected to have 
90% confidence in estimating the population proportion of 
mid-level managers who cite stress as a cause of absence to 
within {0.075 if no previous estimate is available?
e.	 Based on (c) and (d), what sample size is needed if a single 
survey is being conducted?
8.59  A national association devoted to human resource (HR) and 
workplace programs, practices, and training wants to study HR 
department practices and employee turnover of its member or-
ganizations. HR professionals and organization executives focus 
on turnover not only because it has significant cost implications 
but also because it affects overall business performance. A survey 
is designed to estimate the proportion of member organizations 
that have both talent and development programs in place to drive 
human-capital management as well as the member organizations’ 
mean annual employee turnover rate (the ratio of the number of 
employees that left an organization in a given time period to the 
average number of employees in the organization during the given 
time period). A random sample of 100 member organizations re-
veals the following:
• Annual turnover rate: X = 8.1%, S = 1.5%.
• Thirty member organizations have both talent and develop-
ment programs in place to drive human-capital management.
a.	 Construct a 95% confidence interval estimate for the popula-
tion mean annual turnover rate of member organizations.
b.	 Construct a 95% confidence interval estimate for the popula-
tion proportion of member organizations that have both talent 
and development programs in place to drive human-capital 
management.
c.	 What sample size is needed to have 99% confidence of esti-
mating the population mean annual employee turnover rate to 
within {1.5%?
d.	 How many member organizations need to be selected to have 
90% confidence of estimating the population proportion of or-
ganizations that have both talent and development programs in 
place to drive human-capital management to within {.045?
8.60  The financial impact of IT systems downtime is a concern 
of plant operations management today. A survey of manufacturers 
examined the satisfaction level with the reliability and availability 
of their manufacturing IT applications. The variables of focus are 
whether the manufacturer experienced downtime in the past year 
that affected one or more manufacturing IT applications, the num-
ber of downtime incidents that occurred in the past year, and the 
approximate cost of a typical downtime incident. The results from 
a sample of 200 manufacturers are as follows:
• Sixty-two experienced downtime this year that affected one 
or more manufacturing applications.
• Number of downtime incidents: X = 3.5, S = 2.0
• Cost of downtime incidents: X = +18,000, S = +3,000.
a.	 Construct a 90% confidence interval estimate for the popula-
tion proportion of manufacturers who experienced downtime in 
the past year that affected one or more manufacturing IT appli-
cations.
b.	 Construct a 95% confidence interval estimate for the popula-
tion mean number of downtime incidents experienced by man-
ufacturers in the past year.
c.	 Construct a 95% confidence interval estimate for the popula-
tion mean cost of downtime incidents.
8.61  The branch manager of an outlet (Store 1) of a nationwide 
chain of pet supply stores wants to study characteristics of her cus-
tomers. In particular, she decides to focus on two variables: the 
amount of money spent by customers and whether the customers 
own only one dog, only one cat, or more than one dog and/or cat. 
The results from a sample of 70 customers are as follows:
• Amount of money spent: X = +21.34, S = +9.22.
• Thirty-seven customers own only a dog.
• Twenty-six customers own only a cat.
• Seven customers own more than one dog and/or cat.
a.	 Construct a 95% confidence interval estimate for the popula-
tion mean amount spent in the pet supply store.
b.	 Construct a 90% confidence interval estimate for the popula-
tion proportion of customers who own only a cat.
The branch manager of another outlet (Store 2) wishes to conduct 
a similar survey in his store. The manager does not have access to 
the information generated by the manager of Store 1. Answer the 
following questions:
c.	 What sample size is needed to have 95% confidence of  
estimating the population mean amount spent in this store  
to within{ +1.50 if the standard deviation is estimated to  
be $10?
d.	 How many customers need to be selected to have 90% con-
fidence of estimating the population proportion of customers 
who own only a cat to within{0.045?
e.	 Based on your answers to (c) and (d), how large a sample 
should the manager take?
8.62  Scarlett and Heather, the owners of an upscale restaurant 
in Dayton, Ohio, want to study the dining characteristics of their 
customers. They decide to focus on two variables: the amount of 
money spent by customers and whether customers order dessert. 
The results from a sample of 60 customers are as follows:
• Amount spent: X = +38.54, S = +7.26.
• Eighteen customers purchased dessert.
a.	 Construct a 95% confidence interval estimate for the popula-
tion mean amount spent per customer in the restaurant.
b.	 Construct a 90% confidence interval estimate for the popula-
tion proportion of customers who purchase dessert.
Jeanine, the owner of a competing restaurant, wants to conduct a 
similar survey in her restaurant. Jeanine does not have access to 
the information that Scarlett and Heather have obtained from the 
survey they conducted. Answer the following questions:

328	
Chapter 8  Confidence Interval Estimation
c.	 What sample size is needed to have 95% confidence of esti-
mating the population mean amount spent in her restaurant to 
within { +1.50, assuming that the standard deviation is esti-
mated to be $8?
d.	 How many customers need to be selected to have 90% con-
fidence of estimating the population proportion of customers 
who purchase dessert to within {0.04?
e.	 Based on your answers to (c) and (d), how large a sample 
should Jeanine take?
8.63  The manufacturer of Ice Melt claims that its product will 
melt snow and ice at temperatures as low as 0° Fahrenheit. A rep-
resentative for a large chain of hardware stores is interested in test-
ing this claim. The chain purchases a large shipment of 5-pound 
bags for distribution. The representative wants to know, with 95% 
confidence and within {0.05, what proportion of bags of Ice Melt 
perform the job as claimed by the manufacturer.
a.	 How many bags does the representative need to test? What as-
sumption should be made concerning the population propor-
tion? (This is called destructive testing; i.e., the product being 
tested is destroyed by the test and is then unavailable to be 
sold.)
b.	 Suppose that the representative tests 50 bags, and 42 of them 
do the job as claimed. Construct a 95% confidence interval 
estimate for the population proportion that will do the job as 
claimed.
c.	 How can the representative use the results of (b) to determine 
whether to sell the Ice Melt product?
8.64  Claims fraud (illegitimate claims) and buildup (exaggerated 
loss amounts) continue to be major issues of concern among au-
tomobile insurance companies. Fraud is defined as specific ma-
terial misrepresentation of the facts of a loss; buildup is defined 
as the inflation of an otherwise legitimate claim. A recent study 
examined auto injury claims closed with payment under private 
passenger coverages. Detailed data on injury, medical treatment, 
claimed losses, and total payments, as well as claim-handling 
techniques, were collected. In addition, auditors were asked to re-
view the claim files to indicate whether specific elements of fraud 
or buildup appeared in the claim and, in the case of buildup, to 
specify the amount of excess payment. The file  InsuranceClaims  
contains data for 90 randomly selected auto injury claims. The fol-
lowing variables are included: CLAIM—Claim ID; BUILDUP—1 
if buildup indicated, 0 if not; and EXCESSPAYMENT—excess 
payment amount, in dollars.
a.	 Construct a 95% confidence interval for the population propor-
tion of all auto injury files that have exaggerated loss amounts.
b.	 Construct a 95% confidence interval for the population mean 
dollar excess payment amount.
8.65  A quality characteristic of interest for a teabag-filling pro-
cess is the weight of the tea in the individual bags. In this example, 
the label weight on the package indicates that the mean amount is 
5.5 grams of tea in a bag. If the bags are underfilled, two problems 
arise. First, customers may not be able to brew the tea to be as 
strong as they wish. Second, the company may be in violation of 
the truth-in-labeling laws. On the other hand, if the mean amount 
of tea in a bag exceeds the label weight, the company is giving 
away product. Getting an exact amount of tea in a bag is problem-
atic because of variation in the temperature and humidity inside 
the factory, differences in the density of the tea, and the extremely 
fast filling operation of the machine (approximately 170 bags per 
minute). The following data (stored in  Teabags ) are the weights, 
in grams, of a sample of 50 tea bags produced in one hour by a 
single machine:
5.65   5.44   5.42   5.40   5.53   5.34   5.54   5.45   5.52   5.41
5.57   5.40   5.53   5.54   5.55   5.62   5.56   5.46   5.44   5.51
5.47   5.40   5.47   5.61   5.53   5.32   5.67   5.29   5.49   5.55
5.77   5.57   5.42   5.58   5.58   5.50   5.32   5.50   5.53   5.58
5.61   5.45   5.44   5.25   5.56   5.63   5.50   5.57   5.67   5.36
a.	 Construct a 99% confidence interval estimate for the popula-
tion mean weight of the tea bags.
b.	 Is the company meeting the requirement set forth on the label 
that the mean amount of tea in a bag is 5.5 grams?
c.	 Do you think the assumption needed to construct the confi-
dence interval estimate in (a) is valid?
8.66  A manufacturing company produces steel housings for 
electrical equipment. The main component part of the housing is 
a steel trough that is made from a 14-gauge steel coil. It is pro-
duced using a 250-ton progressive punch press with a wipe-down 
operation that puts two 90-degree forms in the flat steel to make 
the trough. The distance from one side of the form to the other is 
critical because of weatherproofing in outdoor applications. The 
widths (in inches), shown below and stored in  Trough , are from a 
sample of 49 troughs:
8.312   8.343   8.317   8.383   8.348   8.410   8.351   8.373   8.481
8.422   8.476   8.382   8.484   8.403   8.414   8.419   8.385   8.465
8.498   8.447   8.436   8.413   8.489   8.414   8.481   8.415   8.479
8.429   8.458   8.462   8.460   8.444   8.429   8.460   8.412   8.420
8.410   8.405   8.323   8.420   8.396   8.447   8.405   8.439   8.411
8.427   8.420   8.498   8.409
a.	 Construct a 95% confidence interval estimate for the mean 
width of the troughs.
b.	 Interpret the interval developed in (a).
c.	 Do you think the assumption needed to construct the confi-
dence interval estimate in (a) in valid?
8.67  The manufacturer of Boston and Vermont asphalt shingles 
knows that product weight is a major factor in a customer’s per-
ception of quality. The last stage of the assembly line packages 
the shingles before they are placed on wooden pallets. Once a pal-
let is full (a pallet for most brands holds 16 squares of shingles), 
it is weighed, and the measurement is recorded. The file  Pallet  
contains the weight (in pounds) from a sample of 368 pallets of 
Boston shingles and 330 pallets of Vermont shingles.
a.	 For the Boston shingles, construct a 95% confidence interval 
estimate for the mean weight.
b.	 For the Vermont shingles, construct a 95% confidence interval 
estimate for the mean weight.
c.	 Do you think the assumption needed to construct the confi-
dence interval estimates in (a) and (b) is valid?
d.	 Based on the results of (a) and (b), what conclusions can you 
reach concerning the mean weight of the Boston and Vermont 
shingles?

	
Cases for Chapter 8	
329
8.68  The manufacturer of Boston and Vermont asphalt shingles 
provides its customers with a 20-year warranty on most of its prod-
ucts. To determine whether a shingle will last the entire warranty 
period, accelerated-life testing is conducted at the manufacturing 
plant. Accelerated-life testing exposes the shingle to the stresses it 
would be subject to in a lifetime of normal use via a laboratory 
experiment that takes only a few minutes to conduct. In this test, a 
shingle is repeatedly scraped with a brush for a short period of time, 
and the shingle granules removed by the brushing are weighed (in 
grams). Shingles that experience low amounts of granule loss are 
expected to last longer in normal use than shingles that experience 
high amounts of granule loss. In this situation, a shingle should ex-
perience no more than 0.8 grams of granule loss if it is expected to 
last the length of the warranty period. The file  Granule  contains a 
sample of 170 measurements made on the company’s Boston shin-
gles and 140 measurements made on Vermont shingles.
a.	 For the Boston shingles, construct a 95% confidence interval 
estimate for the mean granule loss.
b.	 For the Vermont shingles, construct a 95% confidence interval 
estimate for the mean granule loss.
c.	 Do you think the assumption needed to construct the confi-
dence interval estimates in (a) and (b) is valid?
d.	 Based on the results of (a) and (b), what conclusions can you 
reach concerning the mean granule loss of the Boston and Ver-
mont shingles?
Report Writing Exercise
8.69  Referring to the results in Problem 8.66 concerning the 
width of a steel trough, write a report that summarizes your con-
clusions.
C a s e s  f o r  C h a p t e r  8
Managing Ashland MultiComm Services
The marketing department has been considering ways to 
increase the number of new subscriptions to the 3-For-All 
cable/phone/Internet service. Following the suggestion of 
Assistant Manager Lauren Adler, the department staff de-
signed a survey to help determine various characteristics of 
households who subscribe to cable television service from 
Ashland. The survey consists of the following 10 questions:
	 1.	 Does your household subscribe to telephone service 
from Ashland?
(1)  Yes        (2)  No
	 2.	 Does your household subscribe to Internet service from 
Ashland?
(1)  Yes        (2)  No
	 3.	 What type of cable television service do you have?
(1)  Basic       (2)  Enhanced
(If Basic, skip to question 5.)
	 4.	 How often do you watch the cable television stations 
that are only available with enhanced service?
(1)  Every day   (2)  Most days
(3)  Occasionally or never
	 5.	 How often do you watch premium or on-demand ser-
vices that require an extra fee?
(1)  Almost every day    (2)  Several times a week
(3)  Rarely             (4)  Never
	 6.	 Which method did you use to obtain your current AMS 
subscription?
(1)  AMS toll-free phone number
(2)  AMS website
(3)  Direct mail reply card
(4)  Good Tunes & More promotion
(5)  Other
	 7.	 Would you consider subscribing to the 3-For-All cable/
phone/Internet service for a trial period if a discount 
were offered?
(1)  Yes        (2)  No
(If no, skip to question 9.)
	 8.	 If purchased separately, cable, Internet, and phone ser-
vices would currently cost $24.99 per week. How much 
would you be willing to pay per week for the 3-For-All 
cable/phone/Internet service?
	 9.	 Does your household use another provider of telephone 
service?
(1)  Yes        (2)  No
	10.	 AMS may distribute Ashland Gold Cards that would 
provide discounts at selected Ashland-area restaurants 
for subscribers who agree to a two-year subscription 
contract to the 3-For-All service. Would being eligible 
to receive a Gold Card cause you to agree to the two-
year term?
(1)  Yes        (2)  No
Of the 500 households selected that subscribe to cable tele-
vision service from Ashland, 82 households either refused to 
participate, could not be contacted after repeated attempts, 

330	
Chapter 8  Confidence Interval Estimation
or had telephone numbers that were not in service. The sum-
mary results for the 418 households that were contacted are 
as follows:
Household Has AMS Telephone Service
Frequency
Yes
  83
No
335
Household Has AMS Internet Service
Frequency
Yes
262
No
156
Type of Cable Service
Frequency
Basic
164
Enhanced
254
Watches Enhanced Programming
Frequency
Every day
  50
Most days
144
Occasionally or never
  60
Watches Premium or On-Demand Services
Frequency
Almost every day
  14
Several times a week
  35
Almost never
313
Never
  56
Method Used to Obtain Current AMS 
Subscription
Frequency
Toll-free phone number
230
AMS website
106
Direct mail
  46
Good Tunes & More
  10
Other
  26
Would Consider Discounted Trial Offer
Frequency
Yes
  40
No
378
Trial Weekly Rate ($) Willing to Pay (stored in  AMS8 )
23.00 20.00 22.75 20.00 20.00 24.50 17.50 22.25 18.00 21.00
18.25 21.00 18.50 20.75 21.25 22.25 22.75 21.75 19.50 20.75
16.75 19.00 22.25 21.00 16.75 19.00 22.25 21.00 19.50 22.75
23.50 19.50 21.75 22.00 24.00 23.25 19.50 20.75 18.25 21.50
Uses Another Phone Service Provider
Frequency
Yes
354
No
  64
Gold Card Leads to Two-Year Agreement
Frequency
Yes
  38
No
380
Analyze the results of the survey of Ashland households 
that receive AMS cable television service. Write a report  
that discusses the marketing implications of the survey  
results for Ashland MultiComm Services.
Digital Case
Apply your knowledge about confidence interval estimation 
in this Digital Case, which extends the MyTVLab Digital 
Case from Chapter 6.
Among its other features, the MyTVLab website allows 
customers to purchase MyTVLab LifeStyles merchandise 
online. To handle payment processing, the management of 
MyTVLab has contracted with the following firms:
 • PayAFriend (PAF)—This is an online payment system 
with which customers and businesses such as MyTVLab 
register in order to exchange payments in a secure and 
convenient manner, without the need for a credit card.
 • Continental Banking Company (Conbanco)—This 
processing services provider allows MyTVLab custom-
ers to pay for merchandise using nationally recognized 
credit cards issued by a financial institution.
To reduce costs, management is considering eliminat-
ing one of these two payment systems. However, Lorraine 
Hildick of the sales department suspects that customers 
use the two forms of payment in unequal numbers and that 
­customers display different buying behaviors when using 
the two forms of payment. Therefore, she would like to first 
determine the following:
 • The proportion of customers using PAF and the propor-
tion of customers using a credit card to pay for their 
purchases.
 • The mean purchase amount when using PAF and the 
mean purchase amount when using a credit card.
Assist Ms. Hildick by preparing an appropriate analy-
sis. Open PaymentsSample.pdf, read Ms. Hildick’s com-
ments, and use her random sample of 50 transactions as the 
basis for your analysis. Summarize your findings to deter-
mine whether Ms. Hildick’s conjectures about MyTVLab 
LifeStyle customer purchasing behaviors are correct. If you 
want the sampling error to be no more than $3 when esti-
mating the mean purchase amount, is Ms. Hildick’s sample 
large enough to perform a valid analysis?

	
Cases for Chapter 8	
331
Sure Value Convenience Stores
You work in the corporate office for a nationwide conve-
nience store franchise that operates nearly 10,000 stores. 
The per-store daily customer count has been steady, at 900, 
for some time (i.e., the mean number of customers in a store 
in one day is 900). To increase the customer count, the fran-
chise is considering cutting coffee prices. The 12-ounce size 
will now be $0.59 instead of $0.99, and the 16-ounce size 
will be $0.69 instead of $1.19. Even with this reduction in 
price, the franchise will have a 40% gross margin on ­coffee. 
To test the new initiative, the franchise has reduced coffee 
prices in a sample of 34 stores, where customer counts have 
been running almost exactly at the national average of 900. 
After four weeks, the sample stores stabilize at a mean cus-
tomer count of 974 and a standard deviation of 96. This in-
crease seems like a substantial amount to you, but it also 
seems like a pretty small sample. Is there some way to get a 
feel for what the mean per-store count in all the stores will 
be if you cut coffee prices nationwide? Do you think reduc-
ing coffee prices is a good strategy for increasing the mean 
customer count?
CardioGood Fitness
Return to the CardioGood Fitness case first presented on 
page 111. Using the data stored in  CardioGood Fitness :
1.	Construct 95% confidence interval estimates to create a 
customer profile for each CardioGood Fitness treadmill 
product line.
2.	Write a report to be presented to the management of  
CardioGood Fitness detailing your findings.
More Descriptive Choices Follow-Up
Follow up the More Descriptive Choices, Revisited 
­Using Statistics scenario on page 166 by constructing 
95% confidence intervals estimates of the three-year 
­return percentages, five-year return percentages, and 
ten-year return percentages for the sample of growth and 
value funds and for the small, mid-cap, and large market 
cap funds (stored in  Retirement Funds ). In your analysis, 
examine differences between the growth and value funds 
as well as the differences among the small, mid-cap, and 
large market cap funds.
Clear Mountain State Student Surveys
1.	The Student News Service at Clear Mountain State 
University (CMSU) has decided to gather data about 
the undergraduate students that attend CMSU. They 
create and distribute a survey of 14 questions and re-
ceive responses from 62 undergraduates (stored in  
 UndergradSurvey ). For each variable included in the sur-
vey, construct a 95% confidence interval estimate for the 
population characteristic and write a report summarizing 
your conclusions.
2.	The Dean of Students at CMSU has learned about the 
undergraduate survey and has decided to undertake a 
similar survey for graduate students at CMSU. She cre-
ates and distributes a survey of 14 questions and re-
ceives responses from 44 graduate students (stored in 
 GradSurvey  ). For each variable included in the survey, 
construct a 95% confidence interval estimate for the 
population characteristic and write a report summarizing 
your conclusions.

332	
Chapter 8  Confidence Interval Estimation
EG8.1  Confidence Interval Estimate 
for the Mean (S Known)
Key Technique  Use the NORM.S.INV(cumulative percent-
age) to compute the Z value for one-half of the 11 - a2 value and 
use the CONFIDENCE(1 −confidence level, population stan-
dard deviation, sample size) function to compute the half-width of 
a confidence interval.
Example  Compute the confidence interval estimate for the mean 
for the Example 8.1 mean paper length problem on page 305.
PHStat  Use Estimate for the Mean, sigma known.
For the example, select PHStat ➔ Confidence Intervals ➔ 
­Estimate for the Mean, sigma known. In the procedure’s dialog 
box (shown below):
	 1.	 Enter 0.02 as the Population Standard Deviation.
	 2.	 Enter 95 as the Confidence Level percentage.
	 3.	 Click Sample Statistics Known and enter 100 as the Sample 
Size and 10.998 as the Sample Mean.
	 4.	 Enter a Title and click OK.
When using unsummarized data, click Sample Statistics 
­Unknown and enter the Sample Cell Range in step 3.
In-Depth Excel  Use the COMPUTE worksheet of the CIE 
sigma known workbook as a template.
The worksheet already contains the data for the example. For other 
problems, change the Population Standard Deviation, Sample 
Mean, Sample Size, and Confidence Level values in cells B4 
through B7. If you use an Excel version older than Excel 2010, use 
these instructions with the COMPUTE_OLDER worksheet.
EG8.2  Confidence Interval Estimate 
for the Mean (S Unknown)
Key Technique  Use the T.INV.2T(1 −confidence level, 
­degrees of freedom) function to determine the critical value from 
the t distribution.
Example  Compute the Figure 8.7 confidence interval estimate 
for the mean sales invoice amount shown on page 310.
PHStat  Use Estimate for the Mean, sigma unknown.
For the example, select PHStat ➔ Confidence Intervals ➔ 
­Estimate for the Mean, sigma unknown. In the procedure’s dia-
log box (shown below):
	 1.	 Enter 95 as the Confidence Level percentage.
	 2.	 Click Sample Statistics Known and enter 100 as the Sample 
Size, 110.27 as the Sample Mean, and 28.95 as the Sample 
Std. Deviation.
	 3.	 Enter a Title and click OK.
When using unsummarized data, click Sample Statistics 
­Unknown and enter the Sample Cell Range in step 2.
In-Depth Excel  Use the COMPUTE worksheet of the CIE 
sigma unknown workbook as a template.
The worksheet already contains the data for solving the ­example. 
For other problems, change the Sample Standard ­Deviation, 
Sample Mean, Sample Size, and Confidence Level values in 
cells B4 through B7. If you use an Excel version older than Excel 
2010, use these instructions with the COMPUTE_OLDER work-
sheet.
C h a p t e r  8  E x c e l  G u i d e

	
Chapter 8 Excel Guide	
333
EG8.3  Confidence Interval Estimate 
for the Proportion
Key Technique  Use the NORM.S.INV((1 −confidence 
level)/2) function to compute the Z value.
Example  Compute the Figure 8.12 confidence interval estimate 
for the proportion of in-error sales invoices shown on page 316.
PHStat  Use Estimate for the Proportion.
For the example, select PHStat ➔ Confidence Intervals ➔ Es-
timate for the Proportion. In the procedure’s dialog box (shown 
below):
	 1.	 Enter 100 as the Sample Size.
	 2.	 Enter 10 as the Number of Successes.
	 3.	 Enter 95 as the Confidence Level percentage.
	 4.	 Enter a Title and click OK.
In-Depth Excel  Use the COMPUTE worksheet of the CIE 
Proportion workbook as a template.
The worksheet contains the data for the example. Note that 
the formula = SQRT(sample proportion * (1 -  sample 
proportion)/sample size) computes the standard error of the 
­proportion in cell B11.
To compute confidence interval estimates for other prob-
lems, change the Sample Size, Number of Successes, and Con-
fidence Level values in cells B4 through B6. If you use an Excel 
version older than Excel 2010, use these instructions with the  
COMPUTE_OLDER worksheet.
EG8.4  Determining Sample Size
Sample Size Determination for the Mean
Key Technique  Use the NORM.S.INV((1 −confidence  
level)/2) function to compute the Z value and use the 
ROUNDUP(calculated sample size, 0) function to round up the 
computed sample size to the next higher integer.
Example  Determine the sample size for the mean sales invoice 
amount example that is shown in Figure 8.13 on page 319.
PHStat  Use Determination for the Mean.
For the example, select PHStat ➔ Sample Size ➔ Determination 
for the Mean. In the procedure’s dialog box (shown at top right):
	 1.	 Enter 25 as the Population Standard Deviation.
	 2.	 Enter 5 as the Sampling Error.
	 3.	 Enter 95 as the Confidence Level percentage.
	 4.	 Enter a Title and click OK.
In-Depth Excel  Use the COMPUTE worksheet of the Sam-
ple Size Mean workbook as a template.
The worksheet already contains the data for the example. For other 
problems, change the Population Standard Deviation, Sampling 
Error, and Confidence Level values in cells B4 through B6. If 
you use an Excel version older than Excel 2010, use these instruc-
tions with the COMPUTE_OLDER worksheet.
Sample Size Determination for the Proportion
Key Technique  Use the NORM.S.INV and ROUNDUP func-
tions (see previous section) to help determine the sample size 
needed for estimating the proportion.
Example  Determine the sample size for the proportion of in-error 
sales invoices example that is shown in Figure 8.14 on page 321.
PHStat  Use Determination for the Proportion.
For the example, select PHStat ➔ Sample Size ➔ Determination 
for the Proportion. In the procedure’s dialog box (shown below):
	 1.	 Enter 0.15 as the Estimate of True Proportion.
	 2.	 Enter 0.07 as the Sampling Error.
	 3.	 Enter 95 as the Confidence Level percentage.
	 4.	 Enter a Title and click OK.
In-Depth Excel  Use the COMPUTE worksheet of the Sam-
ple Size Proportion workbook as a template.
The worksheet already contains the data for the example. To com-
pute confidence interval estimates for other problems, change the 
Estimate of True Proportion, Sampling Error, and Confidence 
Level in cells B4 through B6. If you use an Excel version older 
than Excel 2010, use these instructions with the COMPUTE_
OLDER worksheet.

334	
Chapter 8  Confidence Interval Estimation
MG8.1  Confidence Interval Estimate 
for the Mean (S Known)
Use 1-Sample Z.
For example, to compute the estimate for the Example 8.1 mean 
paper length problem on page 305, select Stat ➔ Basic Statistics 
➔ 1-Sample Z. In the 1-Sample Z (Test and Confidence Interval) 
dialog box (shown below):
	 1.	 Click Summarized data.
	 2.	 Enter 100 in the Sample size box and 10.998 in the Mean 
box.
	 3.	 Enter 0.02 in the Standard deviation box.
	 4.	 Click Options.
In the 1-Sample Z - Options dialog box (shown below):
	 5.	 Enter 95.0 in the Confidence level box.
	 6.	 Select not equal from the Alternative drop-down list.
	 7.	 Click OK.
	 8.	 Back in the original dialog box, click OK.
When using unsummarized data, click Samples in columns in 
step 1 and, in step 2, enter the name of the column that contains 
the data in the Samples in columns box.
MG8.2  Confidence Interval Estimate 
for the Mean (S Unknown)
Use 1-Sample t.
For example, to compute the Figure 8.7 estimate for the mean 
sales invoice amount on page 310, select Stat ➔ Basic Statistics 
➔ 1-Sample t. In the 1-Sample t (Test and Confidence Interval) 
dialog box (shown below):
	 1.	 Click Summarized data.
	 2.	 Enter 100 in the Sample size box, 110.27 in the Mean box, 
and 28.95 in the Standard deviation box.
	 3.	 Click Options.
In the 1-Sample t - Options dialog box (similar to the 1-Sample 
Z - Options dialog box shown in left column:
	 4.	 Enter 95.0 in the Confidence level box.
	 5.	 Select not equal from the Alternative drop-down list.
	 6.	 Click OK.
	 7.	 Back in the original dialog box, click OK.
When using unsummarized data, click Samples in columns 
in step 1 and, in step 2, enter the name of the column that con-
tains the data. To create a boxplot of the type shown in Figure 8.9 
on page 311, replace step 7 with these steps 7 through 9:
	 7.	 Back in the original dialog box, click Graphs.
	 8.	 In the 1-Sample t - Graphs dialog box, check Boxplot of data 
and then click OK.
	 9.	 Back in the original dialog box, click OK.
MG8.3  Confidence Interval Estimate 
for the Proportion
Use 1 Proportion.
For example, to compute the Figure 8.12 estimate for the propor-
tion of in-error sales invoices on page 316, select Stat ➔ Basic 
Statistics ➔ 1 Proportion. In the 1 Proportion dialog box (shown 
on page 335):
	 1.	 Click Summarized data.
	 2.	 Enter 10 in the Number of events box and 100 in the Number 
of trials box.
	 3.	 Click Options.
C h a p t e r  8  M i n i ta b  G u i d e

	
Chapter 8 Minitab Guide	
335
In the 1 Proportion - Options dialog box (shown below):
	 4.	 Enter 95.0 in the Confidence level box.
	 5.	 Select not equal from the Alternative drop-down list.
	 6.	 Check Use test and interval based on normal distribution.
	 7.	 Click OK (to return to the previous dialog box).
	 8.	 Back in the original dialog box, click OK.
When using unsummarized data, click Samples in columns in 
step 1 and, in step 2, enter the name of the column that contains 
the data.
MG8.4  Determining Sample Size
Minitab version 16 includes Sample Size for Estimation that com-
putes the sample size needed for estimating the mean or the pro-
portion.
To use this new command, select Stat ➔ Power and Sample 
Size ➔ Sample Size for Estimation and in the procedure’s dia-
log box select a parameter from the Parameter drop-down list, 
complete the entries, and click OK. Because this comment is 
not included in Minitab Student 14, the command is not dem-
onstrated or further discussed in this book. (Results using the 
Minitab 16 command will vary slightly from the Excel results 
shown in this chapter.)

336
U s i n g  S tat i s t i c s
Significant Testing at Oxford Cereals
As in Chapter 7, you again find yourself as plant operations manager for ­Oxford 
Cereals. Among other responsibilities, you are responsible for monitoring the 
amount in each cereal box filled. Company specifications require a mean weight 
of 368 grams per box. You must adjust the cereal-filling process when the mean 
fill weight in the population of boxes differs from 368 grams. Adjusting the pro-
cess requires shutting down the cereal production line temporarily, so you do not 
want to make unnecessary adjustments.
What decision-making method can you use to decide if the cereal-filling 
process needs to be adjusted? You decide to begin by selecting a random sam-
ple of 25 cereal boxes and weighing each box. From the weights collected, you 
compute a sample mean. How could that sample mean be used to help decide 
whether adjustment is necessary?
contents
9.1	 Fundamentals of Hypothesis-
Testing Methodology
Can You Ever Know the 
Population Standard Deviation?
9.2	 t Test of Hypothesis for the 
Mean (s Unknown)
9.3	 One-Tail Tests
9.4	 Z Test of Hypothesis for the 
Proportion
9.5	 Potential Hypothesis-Testing 
Pitfalls and Ethical Issues
9.6	 Power of a Test (online)
Using Statistics: Significant 
Testing at Oxford Cereals, 
Revisited
Chapter 9 Excel Guide
Chapter 9 Minitab Guide
Objectives
To learn the basic principles of 
hypothesis testing
To learn to use hypothesis testing 
to test a mean or proportion
To know the assumptions of each 
hypothesis-testing procedure, 
how to evaluate them, and 
the consequences if they are 
seriously violated
To be aware of the pitfalls and 
ethical issues involved in 
hypothesis testing
To learn to avoid the pitfalls 
involved in hypothesis testing
Fundamentals of 
Hypothesis Testing: 
One-Sample Tests
9
Chapter
Peter Close/Shutterstock

	
9.1  Fundamentals of Hypothesis-Testing Methodology	
337
I
n Chapter 7, you learned methods to determine whether the value of a sample mean is 
consistent with a known population mean. In this Oxford Cereals scenario, you seek to 
use a sample mean to validate a claim about the population mean, a somewhat differ-
ent problem. For this type of situation, you use the inferential method known as hypothesis  
testing. Hypothesis testing requires that you state a claim unambiguously. In this scenario, the 
claim is that the population mean is 368 grams. You examine a sample statistic to see if it bet-
ter supports the stated claim, called the null hypothesis, or the mutually exclusive alternative 
hypothesis (for this scenario, that the population mean is not 368 grams).
In this chapter, you will learn several applications of hypothesis testing. You will learn 
how to make inferences about a population parameter by analyzing differences between the 
results observed, the sample statistic, and the results you would expect to get if an underlying 
hypothesis were actually true. For the Oxford Cereals scenario, hypothesis testing allows you 
to infer one of the following:
 • The mean weight of the cereal boxes in the sample is a value consistent with what you 
would expect if the mean of the entire population of cereal boxes were 368 grams.
 • The population mean is not equal to 368 grams because the sample mean is significantly 
different from 368 grams.
9.1  Fundamentals of Hypothesis-Testing Methodology
Hypothesis testing typically begins with a theory, a claim, or an assertion about a particular pa-
rameter of a population. For example, your initial hypothesis in the cereal example is that the 
process is working properly, so the mean fill is 368 grams, and no corrective action is needed.
The Null and Alternative Hypotheses
The hypothesis that the population parameter is equal to the company specification is referred 
to as the null hypothesis. A null hypothesis is often one of status quo and is identified by the 
symbol H0. Here the null hypothesis is that the filling process is working properly, and there-
fore the mean fill is the 368-gram specification provided by Oxford Cereals. This is stated as
H0 : m = 368
Even though information is available only from the sample, the null hypothesis is stated in 
terms of the population parameter because your focus is on the population of all cereal boxes. 
You use the sample statistic to make inferences about the entire filling process. One inference 
may be that the results observed from the sample data indicate that the null hypothesis is false. 
If the null hypothesis is considered false, something else must be true.
Whenever a null hypothesis is specified, an alternative hypothesis is also specified, and it 
must be true if the null hypothesis is false. The alternative hypothesis, H1, is the opposite of 
the null hypothesis, H0. This is stated in the cereal example as
H1 : m ≠368
The alternative hypothesis represents the conclusion reached by rejecting the null hypothesis. 
In many research situations, the alternative hypothesis serves as the hypothesis that is the fo-
cus of the research being conducted. The null hypothesis is rejected when there is sufficient 
evidence from the sample data that the null hypothesis is false. In the cereal example, if the 
weights of the sampled boxes are sufficiently above or below the expected 368-gram mean 
specified by Oxford Cereals, you reject the null hypothesis in favor of the alternative hypothe-
sis that the mean fill is different from 368 grams. You stop production and take whatever action 
is necessary to correct the problem. If the null hypothesis is not rejected, you should continue 
to believe that the process is working correctly and that therefore no corrective action is neces-
sary. In this second circumstance, you have not proven that the process is working correctly. 
Student Tip
Remember, hypothesis 
testing reaches conclu-
sions about parameters, 
not statistics.

338	
Chapter 9  Fundamentals of Hypothesis Testing: One-Sample Tests
Rather, you have failed to prove that it is working incorrectly, and therefore you continue your 
belief (although unproven) in the null hypothesis.
In hypothesis testing, you reject the null hypothesis when the sample evidence suggests 
that it is far more likely that the alternative hypothesis is true. However, failure to reject the 
null hypothesis is not proof that it is true. You can never prove that the null hypothesis is cor-
rect because the decision is based only on the sample information, not on the entire popula-
tion. Therefore, if you fail to reject the null hypothesis, you can only conclude that there is 
insufficient evidence to warrant its rejection. The following key points summarize the null and 
alternative hypotheses:
 • The null hypothesis, H0, represents the current belief in a situation.
 • The alternative hypothesis, H1, is the opposite of the null hypothesis and represents a 
research claim or specific inference you would like to prove.
 • If you reject the null hypothesis, you have statistical proof that the alternative hypothesis 
is correct.
 • If you do not reject the null hypothesis, you have failed to prove the alternative hypoth-
esis. The failure to prove the alternative hypothesis, however, does not mean that you 
have proven the null hypothesis.
 • The null hypothesis, H0, always refers to a specified value of the population parameter 
(such as m), not a sample statistic (such as X).
 • The statement of the null hypothesis always contains an equal sign regarding the speci-
fied value of the population parameter (e.g., H0 : m = 368 grams).
 • The statement of the alternative hypothesis never contains an equal sign regarding the 
specified value of the population parameter (e.g., H1 : m ≠368 grams).
Example 9.1
The Null and 
­Alternative 
­Hypotheses
You are the manager of a fast-food restaurant. You want to determine whether the waiting time 
to place an order has changed in the past month from its previous population mean value of 
4.5 minutes. State the null and alternative hypotheses.
Solution  The null hypothesis is that the population mean has not changed from its previ-
ous value of 4.5 minutes. This is stated as
H0 : m = 4.5
The alternative hypothesis is the opposite of the null hypothesis. Because the null hypothesis is 
that the population mean is 4.5 minutes, the alternative hypothesis is that the population mean 
is not 4.5 minutes. This is stated as
H1 : m ≠4.5
The Critical Value of the Test Statistic
Hypothesis testing uses sample data to determine how likely it is that the null hypothesis is true. 
In the Oxford Cereal Company scenario, the null hypothesis is that the mean amount of cereal 
per box in the entire filling process is 368 grams (the population parameter specified by the com-
pany). You select a sample of boxes from the filling process, weigh each box, and compute the 
sample mean X. This sample statistic is an estimate of the corresponding parameter, the popula-
tion mean, m. Even if the null hypothesis is true, the sample statistic X is likely to differ from the 
value of the parameter (the population mean, m) because of variation due to sampling.
You do expect the sample statistic to be close to the population parameter if the null hypoth-
esis is true. If the sample statistic is close to the population parameter, you have insufficient evi-
dence to reject the null hypothesis. For example, if the sample mean is 367.9 grams, you might 
conclude that the population mean has not changed (i.e., m = 368) because a sample mean of 
367.9 grams is very close to the hypothesized value of 368 grams. Intuitively, you think that it is 
likely that you could get a sample mean of 367.9 grams from a population whose mean is 368.

	
9.1  Fundamentals of Hypothesis-Testing Methodology	
339
However, if there is a large difference between the value of the sample statistic and the 
hypothesized value of the population parameter, you might conclude that the null hypothesis is 
false. For example, if the sample mean is 320 grams, you might conclude that the population 
mean is not 368 grams (i.e., m ≠368) because the sample mean is very far from the hypoth-
esized value of 368 grams. In such a case, you might conclude that it is very unlikely to get a 
sample mean of 320 grams if the population mean is really 368 grams. Therefore, it is more 
logical to conclude that the population mean is not equal to 368 grams. Here you reject the null 
hypothesis.
However, the decision-making process is not always so clear-cut. Determining what is 
“very close” and what is “very different” is arbitrary without clear definitions. Hypothesis- 
testing methodology provides clear definitions for evaluating differences. Furthermore, it enables 
you to quantify the decision-making process by computing the probability of getting a cer-
tain sample result if the null hypothesis is true. You calculate this probability by determining 
the sampling distribution for the sample statistic of interest (e.g., the sample mean) and then 
computing the particular test statistic based on the given sample result. Because the sampling 
distribution for the test statistic often follows a well-known statistical distribution, such as the 
standardized normal distribution or t distribution, you can use these distributions to help deter-
mine whether the null hypothesis is true.
Regions of Rejection and Nonrejection
The sampling distribution of the test statistic is divided into two regions, a region of  
rejection (sometimes called the critical region) and a region of nonrejection (see  
Figure 9.1). If the test statistic falls into the region of nonrejection, you do not reject the 
null hypothesis. In the ­Oxford Cereals scenario, you conclude that there is insufficient 
evidence that the population mean fill is different from 368 grams. If the test statistic falls 
into the rejection region, you ­reject the null hypothesis. In this case, you conclude that the 
population mean is not 368 grams.
Student Tip
Every test statistic fol-
lows a specific sampling 
distribution.
Region of
Rejection
Region of
Nonrejection
Critical
Value
Region of
Rejection
Critical
Value
X
F i g u r e  9 . 1
Regions of rejection 
and nonrejection in 
hypothesis testing
The region of rejection consists of the values of the test statistic that are unlikely to occur 
if the null hypothesis is true. These values are much more likely to occur if the null hypothesis 
is false. Therefore, if a value of the test statistic falls into this rejection region, you reject the 
null hypothesis because that value is unlikely if the null hypothesis is true.
To make a decision concerning the null hypothesis, you first determine the critical value 
of the test statistic. The critical value divides the nonrejection region from the rejection region. 
Determining the critical value depends on the size of the rejection region. The size of the  
rejection region is directly related to the risks involved in using only sample evidence to make 
decisions about a population parameter.
Risks in Decision Making Using Hypothesis Testing
Using hypothesis testing involves the risk of reaching an incorrect conclusion. You might 
wrongly reject a true null hypothesis, H0, or, conversely, you might wrongly not reject a false 
null hypothesis, H0. These types of risk are called Type I and Type II errors.

340	
Chapter 9  Fundamentals of Hypothesis Testing: One-Sample Tests
In the Oxford Cereals scenario, you would make a Type I error if you concluded that the 
population mean fill is not 368 grams when it is 368 grams. This error causes you to needlessly 
adjust the filling process (the “false alarm”) even though the process is working properly. In 
the same scenario, you would make a Type II error if you concluded that the population mean 
fill is 368 grams when it is not 368 grams. In this case, you would allow the process to con-
tinue without adjustment, even though an adjustment is needed (the “missed opportunity”).
Traditionally, you control the Type I error by determining the risk level, a (the lowercase 
Greek letter alpha), that you are willing to have of rejecting the null hypothesis when it is true. 
This risk, or probability, of committing a Type I error is called the level of significance (a). 
­Because you specify the level of significance before you perform the hypothesis test, you directly 
control the risk of committing a Type I error. Traditionally, you select a level of 0.01, 0.05, or 
0.10. The choice of a particular risk level for making a Type I error depends on the cost of mak-
ing a Type I error. After you specify the value for a, you can then determine the critical values 
that divide the rejection and nonrejection regions. You know the size of the ­rejection region be-
cause a is the probability of rejection when the null hypothesis is true. From this, you can then 
determine the critical value or values that divide the rejection and nonrejection regions.
The probability of committing a Type II error is called the b risk. Unlike with a  
Type I error, which you control through the selection of a, the probability of making a  
Type II error depends on the difference between the hypothesized and actual values of the 
population ­parameter. Because large differences are easier to find than small ones, if the dif-
ference between the hypothesized and actual values of the population parameter is large, b is 
small. For example, if the population mean is 330 grams, there is a small chance 1b2 that you 
will conclude that the mean has not changed from 368 grams. However, if the difference be-
tween the hypothesized and actual values of the parameter is small, b is large. For example, if 
the population mean is actually 367 grams, there is a large chance 1b2 that you will conclude 
that the mean is still 368 grams.
Type I and Type II Errors
A Type I error occurs if you reject the null hypothesis, H0, when it is true and should not be 
rejected. A Type I error is a “false alarm.” The probability of a Type I error ­occurring is a.
A Type II error occurs if you do not reject the null hypothesis, H0, when it is false 
and should be rejected. A Type II error represents a “missed opportunity” to take some 
corrective action. The probability of a Type II error occurring is b.
Probability of Type I and Type II Errors
The level of significance 1A2 of a statistical test is the probability of committing a  
Type I error.
The B risk is the probability of committing a Type II error.
The complement of the probability of a Type I error, 11 - a2, is called the confidence 
coefficient. The confidence coefficient is the probability that you will not reject the null 
­hypothesis, H0, when it is true and should not be rejected. In the Oxford Cereals scenario, the 
confidence coefficient measures the probability of concluding that the population mean fill is 
368 grams when it is actually 368 grams.
The complement of the probability of a Type II error, 11 - b2, is called the power of a 
statistical test. The power of a statistical test is the probability that you will reject the null 
­hypothesis when it is false and should be rejected. In the Oxford Cereals scenario, the power  
of the test is the probability that you will correctly conclude that the mean fill amount is not 
368 grams when it actually is not 368 grams.

	
9.1  Fundamentals of Hypothesis-Testing Methodology	
341
Table 9.1 illustrates the results of the two possible decisions (do not reject H0 or reject H0) 
that you can make in any hypothesis test. You can make a correct decision or make one of two 
types of errors.
Complements of Type I and Type II Errors
The confidence coefficient, 11 - a2, is the probability that you will not reject the null 
hypothesis, H0, when it is true and should not be rejected.
The power of a statistical test, 11 - b2, is the probability that you will reject the null 
hypothesis when it is false and should be rejected.
T a b l e  9 . 1
Hypothesis Testing 
and Decision Making
Actual Situation
Statistical Decision
H0 True
H0 False
Do not reject H0
Correct decision  
Confidence = 11 - a2
Type II error  
P1Type II error2 = b
Reject H0
Type I error  
P1Type I error2 = a
Correct decision  
Power = 11 - b2
One way to reduce the probability of making a Type II error is by increasing the sample 
size. Large samples generally permit you to detect even very small differences between the  
hypothesized values and the actual population parameters. For a given level of a, increasing 
the sample size decreases b and therefore increases the power of the statistical test to detect 
that the null hypothesis, H0, is false.
However, there is always a limit to your resources, and this affects the decision of how 
large a sample you can select. For any given sample size, you must consider the trade-offs 
between the two possible types of errors. Because you can directly control the risk of a Type  
I error, you can reduce this risk by selecting a smaller value for a. For example, if the negative 
consequences associated with making a Type I error are substantial, you could select a = 0.01 
instead of 0.05. However, when you decrease a, you increase b, so reducing the risk of a Type 
I error results in an increased risk of a Type II error. However, to reduce b, you could select a 
larger value for a. Therefore, if it is important to try to avoid a Type II error, you can select a 
of 0.05 or 0.10 instead of 0.01.
In the Oxford Cereals scenario, the risk of a Type I error occurring involves conclud-
ing that the mean fill amount has changed from the hypothesized 368 grams when it actually 
has not changed. The risk of a Type II error occurring involves concluding that the mean fill 
amount has not changed from the hypothesized 368 grams when it actually has changed. The 
choice of reasonable values for a and b depends on the costs inherent in each type of error. 
For example, if it is very costly to change the cereal-filling process, you would want to be very 
confident that a change is needed before making any changes. In this case, the risk of a Type I  
error occurring is more important, and you would choose a small a. However, if you want 
to be very certain of detecting changes from a mean of 368 grams, the risk of a Type II error  
occurring is more important, and you would choose a higher level of a.
Now that you have been introduced to hypothesis testing, recall that in the Oxford Cereals 
scenario on page 336, the business problem facing Oxford Cereals is to determine if the mean fill 
weight in the population of boxes in the cereal-filling process differs from 368 grams. To make 
this determination, you select a random sample of 25 boxes, weigh each box, compute the sam-
ple mean, X, and then evaluate the difference between this sample statistic and the hypothesized 
population parameter by comparing the sample mean weight (in grams) to the expected popula-
tion mean of 368 grams specified by the company. The null and alternative hypotheses are:
H0 : m = 368
H1 : m ≠368

342	
Chapter 9  Fundamentals of Hypothesis Testing: One-Sample Tests
Z Test for the Mean (S Known)
When the standard deviation, s, is known (which rarely occurs), you use the Z test for the 
mean if the population is normally distributed. If the population is not normally distributed, 
you can still use the Z test if the sample size is large enough for the Central Limit Theorem 
to take effect (see Section 7.2). Equation (9.1) defines the ZSTAT test statistic for determining  
the difference between the sample mean, X, and the population mean, m, when the standard 
deviation, s, is known.
Z Test for the Mean (s Known)
	
ZSTAT = X - m
s
1n
	
(9.1)
In Equation (9.1), the numerator measures the difference between the observed sample 
mean, X, and the hypothesized mean, m. The denominator is the standard error of the mean, so 
ZSTAT represents the difference between X and m in standard error units.
Hypothesis Testing Using the Critical Value Approach
The critical value approach compares the value of the computed ZSTAT test statistic from  
Equation (9.1) to critical values that divide the normal distribution into regions of rejection  
and nonrejection. The critical values are expressed as standardized Z values that are deter-
mined by the level of significance.
For example, if you use a level of significance of 0.05, the size of the rejection region is 
0.05. Because the null hypothesis contains an equal sign and the alternative hypothesis con-
tains a not equal sign, you have a two-tail test in which the rejection region is divided into 
the two tails of the distribution, with two equal parts of 0.025 in each tail. For this two-tail 
test, a rejection region of 0.025 in each tail of the normal distribution results in a cumula-
tive area of 0.025 below the lower critical value and a cumulative area of 0.975 11 - 0.0252 
below the upper critical value (which leaves an area of 0.025 in the upper tail). According 
to the cumulative standardized normal distribution table (Table E.2), the critical values that 
divide the rejection and nonrejection regions are -1.96 and +1.96. Figure 9.2 illustrates that 
if the mean is actually 368 grams, as H0 claims, the values of the ZSTAT test statistic have 
a standardized normal distribution centered at Z = 0 (which corresponds to an X value of  
368 grams). Values of ZSTAT greater than +1.96 and less than -1.96 indicate that X is 
sufficiently different from the hypothesized m = 368 that it is unlikely that such an X value 
would occur if H0 were true.
Region of
Rejection
Region of
Nonrejection
Critical
Value
Region of
Rejection
Critical
Value
–1.96
0
+1.96
Z
.025
.025
368
.95
X
F i g u r e  9 . 2
Testing a hypothesis 
about the mean  
(s known) at the 0.05 
level of significance
Student Tip
Remember, first you 
determine the level of 
significance. This enables 
you to then determine 
the critical value. A differ-
ent level of significance 
leads to a different critical 
value.
Student Tip
In a two-tail test, there is 
a rejection region in each 
tail of the distribution.

	
9.1  Fundamentals of Hypothesis-Testing Methodology	
343
Therefore, the decision rule is
reject H0 if ZSTAT 7 +1.96
or if ZSTAT 6 - 1.96;
otherwise, do not reject H0.
Suppose that the sample of 25 cereal boxes indicates a sample mean, X, of 372.5 grams, 
and the population standard deviation, s, is 15 grams. Using Equation (9.1) on page 342,
ZSTAT = X - m
s
2n
- 372.5 - 368
15
125
= +1.50
Because ZSTAT = +1.50 is greater than -1.96 and less than +1.96, you do not reject H0 (see 
Figure 9.3).
You continue to believe that the mean fill amount is 368 grams. To take into account the 
possibility of a Type II error, you state the conclusion as “there is insufficient evidence that the 
mean fill is different from 368 grams.”
Student Tip
Remember, the decision 
always concerns H0. 
Either you reject H0 or 
you do not reject H0.
F i g u r e  9 . 3
Testing a hypothesis 
about the mean cereal 
weight (s known) at the 
0.05 level of significance
0
–1.96
+1.96
+1.50
Z
Region of
Rejection
Region of
Nonrejection
Region of
Rejection
.025
.025
.95
Exhibit 9.1 summarizes the critical value approach to hypothesis testing. Steps 1 and 2 are 
part of the Define task, step 5 combines the Collect and Organize tasks, and steps 3, 4, and 6 
involve the Visualize and Analyze tasks of the DCOVA framework first introduced on page 30. 
Examples 9.2 and 9.3 apply the critical value approach to hypothesis testing to Oxford Cereals 
and to a fast-food restaurant.
Exhibit 9.1 
The Critical Value Approach to Hypothesis Testing
Step 1	 State the null hypothesis, H0, and the alternative hypothesis, H1.
Step 2	 Choose the level of significance, a, and the sample size, n. The level of signifi-
cance is based on the relative importance of the risks of committing Type I and 
Type II errors in the problem.
Step 3	 Determine the appropriate test statistic and sampling distribution.
Step 4	 Determine the critical values that divide the rejection and nonrejection regions.
Step 5	 Collect the sample data, organize the results, and compute the value of the test 
statistic.
Step 6	 Make the statistical decision, determine whether the assumptions are valid, and 
state the managerial conclusion in the context of the theory, claim, or assertion 
being tested. If the test statistic falls into the nonrejection region, you do not reject 
the null hypothesis. If the test statistic falls into the rejection region, you reject the 
null hypothesis.

344	
Chapter 9  Fundamentals of Hypothesis Testing: One-Sample Tests
Example 9.2
Applying the 
­Critical Value 
­Approach to 
­Hypothesis Testing 
at Oxford Cereals
State the critical value approach to hypothesis testing at Oxford Cereals.
Solution
Step 1	 State the null and alternative hypotheses. The null hypothesis, H0, is always stated as 
a mathematical expression, using population parameters. In testing whether the mean 
fill is 368 grams, the null hypothesis states that m equals 368. The alternative hypoth-
esis, H1, is also stated as a mathematical expression, using population parameters. 
Therefore, the alternative hypothesis states that m is not equal to 368 grams.
Step 2	 Choose the level of significance and the sample size. You choose the level of signifi-
cance, a, according to the relative importance of the risks of committing Type I and 
Type II errors in the problem. The smaller the value of a, the less risk there is of mak-
ing a Type I error. In this example, making a Type I error means that you conclude that 
the population mean is not 368 grams when it is 368 grams. Thus, you will take cor-
rective action on the filling process even though the process is working properly. Here, 
a = 0.05 is selected. The sample size, n, is 25.
Step 3	 Select the appropriate test statistic. Because s is known from information about the 
filling process, you use the normal distribution and the ZSTAT test statistic.
Step 4	 Determine the rejection region. Critical values for the appropriate test statistic are 
selected so that the rejection region contains a total area of a when H0 is true and the 
nonrejection region contains a total area of 1 - a when H0 is true. Because a = 0.05 
in the cereal example, the critical values of the ZSTAT test statistic are -1.96 and +1.96. 
The rejection region is therefore ZSTAT 6 - 1.96 or ZSTAT 7 +1.96. The nonrejection 
region is -1.96 … ZSTAT … +1.96.
Step 5	 Collect the sample data and compute the value of the test statistic. In the cereal exam-
ple, X = 372.5, and the value of the test statistic is ZSTAT = +1.50.
Step 6	 State the statistical decision and the managerial conclusion. First, determine whether 
the test statistic has fallen into the rejection region or the nonrejection region. 
For the cereal example, ZSTAT = +1.50 is in the region of nonrejection because 
-1.96 … ZSTAT = +1.50 … +1.96. Because the test statistic falls into the nonrejec-
tion region, the statistical decision is to not reject the null hypothesis, H0. The manage-
rial conclusion is that insufficient evidence exists to prove that the mean fill is different 
from 368 grams. No corrective action on the filling process is needed.
Example 9.3
Testing and 
­Rejecting a Null 
­Hypothesis
You are the manager of a fast-food restaurant. The business problem is to determine whether 
the population mean waiting time to place an order has changed in the past month from its 
previous population mean value of 4.5 minutes. From past experience, you can assume that the 
population is normally distributed, with a population standard deviation of 1.2 minutes. You 
select a sample of 25 orders during a one-hour period. The sample mean is 5.1 minutes. Use 
the six-step approach listed in Exhibit 9.1 on page 343 to determine whether there is evidence 
at the 0.05 level of significance that the population mean waiting time to place an order has 
changed in the past month from its previous population mean value of 4.5 minutes.
Solution
Step 1	 The null hypothesis is that the population mean has not changed from its previous 
value of 4.5 minutes:
H0 : m = 4.5
The alternative hypothesis is the opposite of the null hypothesis. Because the null 
hypothesis is that the population mean is 4.5 minutes, the alternative hypothesis is that 
the population mean is not 4.5 minutes:
H1 : m ≠4.5

	
9.1  Fundamentals of Hypothesis-Testing Methodology	
345
Hypothesis Testing Using the p-Value Approach
The p-value is the probability of getting a test statistic equal to or more extreme than the sam-
ple result, given that the null hypothesis, H0, is true. The p-value is also known as the observed 
level of significance. Using the p-value to determine rejection and nonrejection is another  
approach to hypothesis testing.
The decision rules for rejecting H0 in the p-value approach are
 • If the p-value is greater than or equal to a, do not reject the null hypothesis.
 • If the p-value is less than a, reject the null hypothesis.
Step 2	 You have selected a sample of n = 25. The level of significance is 0.05 (i.e., 
a = 0.05).
Step 3	 Because s is assumed to be known, you use the normal distribution and the ZSTAT test 
statistic.
Step 4	 Because a = 0.05, the critical values of the ZSTAT test statistic are -1.96 and +1.96. 
The rejection region is ZSTAT 6 -1.96 or ZSTAT 7 +1.96. The nonrejection region is 
-1.96 … ZSTAT … +1.96
Step 5	 You collect the sample data and compute X = 5.1. Using Equation (9.1) on page 342, 
you compute the test statistic:
ZSTAT = X - m
s
1n
= 5.1 - 4.5
1.2
125
= +2.50
Step 6	 Because ZSTAT = +2.50 7 +1.96, you reject the null hypothesis. You conclude that 
there is evidence that the population mean waiting time to place an order has changed 
from its previous value of 4.5 minutes. The mean waiting time for customers is longer 
now than it was last month. As the manager, you would now want to determine how 
waiting time could be reduced to improve service.
Many people confuse these rules, mistakenly believing that a high p-value is reason for 
rejection. You can avoid this confusion by remembering the following:
If the p@value is low, then H0 must go.
To understand the p-value approach, consider the Oxford Cereals scenario. You tested 
whether the mean fill was equal to 368 grams. The test statistic resulted in a ZSTAT value of 
+1.50 and you did not reject the null hypothesis because +1.50 was less than the upper critical 
value of +1.96 and greater than the lower critical value of -1.96.
To use the p-value approach for the two-tail test, you find the probability that the test 
statistic ZSTAT is equal to or more extreme than 1.50 standard error units from the cen-
ter of a standardized normal distribution. In other words, you need to compute the prob-
ability that the ZSTAT value is greater than +1.50 along with the probability that the ZSTAT 
value is less than-1.50. Table E.2 shows that the probability of a ZSTAT value below -1.50 
is 0.0668. The probability of a value below +1.50 is 0.9332, and the probability of a value 
above +1.50 is 1 - 0.9332 = 0.0668. Therefore, the p-value for this two-tail test is 
0.0668 + 0.0668 = 0.1336 (see Figure 9.4). Thus, the probability of a test statistic equal to or 
more extreme than the sample result is 0.1336. Because 0.1336 is greater than a = 0.05, you 
do not reject the null hypothesis.
Student Tip
A small (or low) p-value 
indicates a small prob-
ability that H0 is true. 
A big or large p-value 
indicates a large prob-
ability that H0 is true.

346	
Chapter 9  Fundamentals of Hypothesis Testing: One-Sample Tests
In this example, the observed sample mean is 372.5 grams, 4.5 grams above the hypoth-
esized value, and the p-value is 0.1336. Thus, if the population mean is 368 grams, there is 
a 13.36% chance that the sample mean differs from 368 grams by at least 4.5 grams (i.e.,  
is Ú  372.5 grams or …  363.5 grams). Therefore, even though 372.5 grams is above the hy-
pothesized value of 368 grams, a result as extreme as or more extreme than 372.5 grams is not 
highly unlikely when the population mean is 368 grams.
Unless you are dealing with a test statistic that follows the normal distribution, you will 
only be able to approximate the p-value from the tables of the distribution. However, Excel and 
Minitab can compute the p-value for any hypothesis test, and this allows you to substitute the 
p-value approach for the critical value approach when you conduct hypothesis testing.
Figure 9.5 displays the Excel and Minitab results for the cereal-filling example discussed 
beginning on page 342. 
–1.50
0
+1.50
Z
.0668
.0668
.8664
F i g u r e  9 . 4
Finding a p-value for a 
two-tail test
F i g u r e  9 . 5
Excel and Minitab results 
for the Z test for the 
mean (s known) for the 
cereal-filling example
Exhibit 9.2 summarizes the p-value approach to hypothesis testing. Example 9.4 applies 
the p-value approach to the fast-food restaurant example.
Exhibit 9.2 
The p-Value Approach to Hypothesis Testing
Step 1	 State the null hypothesis, H0, and the alternative hypothesis, H1.
Step 2	 Choose the level of significance, a, and the sample size, n. The level of signifi-
cance is based on the relative importance of the risks of committing Type I and 
Type II errors in the problem.
Step 3	 Determine the appropriate test statistic and the sampling distribution.
Step 4	 Collect the sample data, compute the value of the test statistic, and compute the 
p-value.
Step 5	 Make the statistical decision and state the managerial conclusion in the context of 
the theory, claim, or assertion being tested. If the p-value is greater than or equal 
to a, do not reject the null hypothesis. If the p-value is less than a, reject the null 
hypothesis.

	
9.1  Fundamentals of Hypothesis-Testing Methodology	
347
Example 9.4
Testing and 
­Rejecting a Null 
­Hypothesis ­Using 
the p-Value 
­Approach
You are the manager of a fast-food restaurant. The business problem is to determine whether 
the population mean waiting time to place an order has changed in the past month from its pre-
vious value of 4.5 minutes. From past experience, you can assume that the population standard 
deviation is 1.2 minutes and the population waiting time is normally distributed. You select a 
sample of 25 orders during a one-hour period. The sample mean is 5.1 minutes. Use the five-
step p-value approach of Exhibit 9.2 to determine whether there is evidence that the population 
mean waiting time to place an order has changed in the past month from its previous popula-
tion mean value of 4.5 minutes.
Solution
Step 1	 The null hypothesis is that the population mean has not changed from its previous 
value of 4.5 minutes:
H0 : m = 4.5
The alternative hypothesis is the opposite of the null hypothesis. Because the null 
hypothesis is that the population mean is 4.5 minutes, the alternative hypothesis is that 
the population mean is not 4.5 minutes:
H1 : m ≠4.5
Step 2	 You have selected a sample of n = 25 and you have chosen a 0.05 level of signifi-
cance (i.e., a = 0.05).
Step 3	 Select the appropriate test statistic. Because s is assumed known, you use the normal 
distribution and the ZSTAT test statistic.
Step 4	 You collect the sample data and compute X = 5.1. Using Equation (9.1) on page 342, 
you compute the test statistic as follows:
ZSTAT = X - m
s
1n
= 5.1 - 4.5
1.2
125
= +2.50
To find the probability of getting a ZSTAT test statistic that is equal to or more extreme 
than 2.50 standard error units from the center of a standardized normal distribution, 
you compute the probability of a ZSTAT value greater than +2.50 along with the prob-
ability of a ZSTAT value less than -2.50. From Table E.2, the probability of a ZSTAT 
value below -2.50 is 0.0062. The probability of a value below +2.50 is 0.9938. 
Therefore, the probability of a value above +2.50 is 1 - 0.9938 = 0.0062. Thus, the 
p-value for this two-tail test is 0.0062 + 0.0062 = 0.0124.
Step 5	 Because the p-value = 0.0124 6 a = 0.05, you reject the null hypothesis. You con-
clude that there is evidence that the population mean waiting time to place an order has 
changed from its previous population mean value of 4.5 minutes. The mean waiting 
time for customers is longer now than it was last month.
A Connection Between Confidence Interval Estimation  
and Hypothesis Testing
This chapter and Chapter 8 discuss confidence interval estimation and hypothesis testing, 
the two major elements of statistical inference. Although confidence interval estimation and 
­hypothesis testing share the same conceptual foundation, they are used for different purposes. 
In Chapter 8, confidence intervals estimated parameters. In this chapter, hypothesis test-
ing makes decisions about specified values of population parameters. Hypothesis tests are 
used when trying to determine whether a parameter is less than, more than, or not equal to 
a ­specified value. Proper interpretation of a confidence interval, however, can also indicate 
whether a parameter is less than, more than, or not equal to a specified value. For example, in 

348	
Chapter 9  Fundamentals of Hypothesis Testing: One-Sample Tests
this section, you tested whether the population mean fill amount was different from 368 grams 
by using Equation (9.1) on page 342:
ZSTAT = X - m
s
1n
Instead of testing the null hypothesis that m = 368 grams, you can reach the same conclu-
sion by constructing a confidence interval estimate of m. If the hypothesized value of m = 368 
is contained within the interval, you do not reject the null hypothesis because 368 would not be 
considered an unusual value. However, if the hypothesized value does not fall into the interval, 
you reject the null hypothesis because m = 368 grams is then considered an unusual value. 
Using Equation (8.1) on page 304 and the following results:
n = 25, X = 372.5 grams, s = 15 grams
for a confidence level of 95% (i.e., a = 0.05),
 X { Za>2 s
1n
 372.5 { 11.962 15
125
 372.5 { 5.88
so that
366.62 … m … 378.38
Because the interval includes the hypothesized value of 368 grams, you do not reject the null 
hypothesis. There is insufficient evidence that the mean fill amount for the entire filling pro-
cess is not 368 grams. You reached the same decision by using a two-tail hypothesis test.
Problems for Section 9.1
Learning the Basics
9.1  If you use a 0.05 level of significance in a two-tail hypothesis 
test, what decision will you make if ZSTAT = -0.76?
9.2  If you use a 0.05 level of significance in a two-tail hypothesis 
test, what decision will you make if ZSTAT = +2.21?
9.3  If you use a 0.10 level of significance in a two-tail hypothesis 
test, what is your decision rule for rejecting a null hypothesis that 
the population mean equals 500 if you use the Z test?
9.4  If you use a 0.01 level of significance in a two-tail hypothesis 
test, what is your decision rule for rejecting H0 : m = 12.5 if you 
use the Z test?
The end of Section 8.1 on page 306 discussed how learning a con-
fidence interval estimation method that required knowing s, the 
population standard deviation, served as an effective introduction to 
the concept of a confidence interval. That section then revealed that 
you would be unlikely to use that procedure for most practical ap-
plications for several reasons.
Likewise, for most practical applications, you are unlikely to use a 
hypothesis-testing method that requires knowing s. If you knew the 
population standard deviation, you would also know the population 
mean and would not need to form a hypothesis about the mean and 
then test that hypothesis. So why study a hypothesis testing of the 
mean, which requires that s is known? Using such a test makes 
it much easier to explain the fundamentals of hypothesis testing. 
With a known population standard deviation, you can use the normal 
distribution and compute p-values using the tables of the normal 
distribution.
Because it is important that you understand the concept of hy-
pothesis testing when reading the rest of this book, review this sec-
tion carefully—even if you anticipate never having a practical reason 
to use the test represented in Equation (9.1).
Can You Ever Know the Population Standard Deviation?

	
9.2  t Test of Hypothesis for the Mean (s Unknown)	
349
9.5  What is your decision in Problem 9.4 if ZSTAT = -2.61?
9.6  What is the p-value if, in a two-tail hypothesis test, 
ZSTAT = +2.00?
9.7  In Problem 9.6, what is your statistical decision if you test the 
null hypothesis at the 0.10 level of significance?
9.8  What is the p-value if, in a two-tail hypothesis test, 
ZSTAT = -1.38?
Applying the Concepts
9.9  In the U.S. legal system, a defendant is presumed innocent un-
til proven guilty. Consider a null hypothesis, H0, that a defendatnt 
is innocent, and an alternative hypothesis, H1, that the defendant 
is guilty. A jury has two possible decisions: Convict the defendant 
(i.e., reject the null hypothesis) or do not convict the defendant (i.e., 
do not reject the null hypothesis). Explain the meaning of the risks 
of committing either a Type I or Type II error in this example.
9.10  Suppose the defendant in Problem 9.9 is presumed guilty 
until proven innocent. How do the null and alternative hypotheses 
differ from those in Problem 9.9? What are the meanings of the 
risks of committing either a Type I or Type II error here?
9.11  Many consumer groups feel that the U.S. Food and Drug 
Administration (FDA) drug approval process is too easy and, as 
a result, too many drugs are approved that are later found to be 
unsafe. On the other hand, a number of industry lobbyists have 
pushed for a more lenient approval process so that pharmaceutical 
companies can get new drugs approved more easily and quickly. 
Consider a null hypothesis that a new, unapproved drug is unsafe 
and an alternative hypothesis that a new, unapproved drug is safe.
a.	 Explain the risks of committing a Type I or Type II error.
b.	 Which type of error are the consumer groups trying to avoid? 
Explain.
c.	 Which type of error are the industry lobbyists trying to avoid? 
Explain.
d.	 How would it be possible to lower the chances of both Type I 
and Type II errors?
9.12  In a nuts snack packet, it is mentioned that the content is 
32% cashews and 68% peanuts. A researcher is trying to find 
out if the actual content of the packet has changed from what 
is stated on the packet. State the null hypothesis to check if the 
packets actually contain 32% cashews, H0, and the alternative 
hypothesis, H1.
9.13  Do marketing majors at your school study more than, less 
than, or about the same as marketing majors at other schools? The 
Washington Post reported the results of the National Survey of  
Student Engagement that found marketing majors studied an aver-
age of 12.1 hours per week. (Data extracted from “Is College Too 
Easy? As Study Time Falls, Debate Rises,” The Washington Post, 
May 21, 2012.) Set up a hypothesis test to try to prove that the mean 
number of hours studied by marketing majors at your school is  
different from the 12.1-hour-per-week benchmark reported by The  
Washington Post.
a.	 State the null and alternative hypothesis.
b.	 What is a Type I error for your test?
c.	 What is a Type II error for your test?
SELF 
Test 
9.14  The quality-control manager at a compact flu-
orescent light bulb (CFL) factory needs to determine 
whether the mean life of a large shipment of CFLs is equal to 
7,500 hours. The population standard deviation is 1,000 hours. 
A random sample of 64 CFLs indicates a sample mean life of 
7,250 hours.
a.	 At the 0.05 level of significance, is there evidence that the 
mean life is different from 7,500 hours?
b.	 Compute the p-value and interpret its meaning.
c.	 Construct a 95% confidence interval estimate of the population 
mean life of the CFLs.
d.	 Compare the results of (a) and (c). What conclusions do you 
reach?
9.15  A survey of the monthly pocket money of 100 students 
shows an average of $100 per month, with a standard deviation of 
$25. At 95% confidence,
a.	 determine confidence intervals.
b.	 test if mean monthly pocket money is $95.
9.16  A bottled water distributor wants to determine whether the 
mean amount of water contained in 1-gallon bottles purchased from 
a nationally known water bottling company is actually 1 gallon.  
You know from the water bottling company specifications that the 
standard deviation of the amount of water per bottle is 0.02 gallon. 
You select a random sample of 50 bottles, and the mean amount of 
water per 1-gallon bottle is 0.995 gallon.
a.	 Is there evidence that the mean amount is different from  
1.0 gallon? (Use a = 0.01.)
b.	 Compute the p-value and interpret its meaning.
c.	 Construct a 99% confidence interval estimate of the population 
mean amount of water per bottle.
d.	 Compare the results of (a) and (c). What conclusions do you reach?
9.17  Suppose that in Problem 9.16, the standard deviation is 
0.012 gallon.
a.	 Repeat (a) through (d) of Problem 9.16, assuming a standard 
deviation of 0.012 gallon.
b.	 Compare the results of (a) to those of Problem 9.16.
9.2  t Test of Hypothesis for the Mean (s Unknown)
In virtually all hypothesis-testing situations concerning the population mean, m, you do not know the 
population standard deviation, s. Instead, you use the sample standard deviation, S. If you ­assume 
that the population is normally distributed, the sampling distribution of the mean follows a t distribu-
tion with n - 1 degrees of freedom, and you use the t test for the mean. If the population is not 
normally distributed, you can still use the t test if the population is not too skewed and the sample 
size is not too small. Equation (9.2) defines the test statistic for determining the difference between 
the sample mean, X, and the population mean, m, when using the sample standard deviation, S.

350	
Chapter 9  Fundamentals of Hypothesis Testing: One-Sample Tests
To illustrate the use of the t test for the mean, return to the Chapter 8 Ricknel Home 
­Centers scenario on page 300. The business objective is to determine whether the mean amount 
per sales invoice is unchanged from the $120 of the past five years. As an accountant for the 
company, you need to determine whether this amount has changed. In other words, the hypoth-
esis test is used to try to determine whether the mean amount per sales invoice is increasing or 
­decreasing.
The Critical Value Approach
To perform this two-tail hypothesis test, you use the six-step method listed in Exhibit 9.1 on 
page 343.
Step 1	 You define the following hypotheses:
H0 : m = 120
H1 : m ≠120
	
	
The alternative hypothesis contains the statement you are trying to prove. If the null 
hypothesis is rejected, then there is statistical evidence that the population mean 
amount per sales invoice is no longer $120. If the statistical conclusion is “do not 
reject H0,” then you will conclude that there is insufficient evidence to prove that the 
mean amount differs from the long-term mean of $120.
Step 2	 You collect the data from a sample of n = 12 sales invoices. You decide to use 
a = 0.05.
Step 3	 Because s is unknown, you use the t distribution and the tSTAT test statistic. You must 
assume that the population of sales invoices is approximately normally distributed in 
order to use the t distribution because the sample size is only 12. This assumption is 
discussed on page 352.
Step 4	 For a given sample size, n, the test statistic tSTAT follows a t distribution with n - 1 
degrees of freedom. The critical values of the t distribution with 12 - 1 = 11 degrees 
of freedom are found in Table E.3, as illustrated in Table 9.2 and Figure 9.6. The 
alternative hypothesis, H1 : m ≠120, has two tails. The area in the rejection region of 
the t distribution’s left (lower) tail is 0.025, and the area in the rejection region of the  
t distribution’s right (upper) tail is also 0.025.
	
	
    From the t table as given in Table E.3, a portion of which is shown in Table 9.2, the 
critical values are {2.2010. The decision rule is
reject H0 if tSTAT 6 - 2.2010
or if tSTAT 7 + 2.2010;
otherwise, do not reject H0.
t Test for the Mean (s Unknown)
	
tSTAT = X - m
S
1n
	
(9.2)
where the tSTAT test statistic follows a t distribution having n - 1 degrees of freedom.
Student Tip
Remember, the null 
hypothesis uses an equal 
sign and the alternative 
hypothesis never uses an 
equal sign.
Student Tip
Since this is a two-tail 
test, the level of sig-
nificance, a = 0.05, 
is divided into two equal 
0.025 parts, in each 
of the two tails of the 
distribution.

Step 5	 You organize and store the data from a random sample of 12 sales invoices in  Invoices :
  108.98    152.22    111.45    110.59    127.46    107.26
93.32      91.97    111.56      75.71    128.58    135.11
	
	
Using Equations (3.1) and (3.7) on pages 130 and 137,
X = +112.85 and S = +20.80
	
	
From Equation (9.2) on page 350,
tSTAT =  X - m
S
2n
= 112.85 - 120
20.80
212
= -1.1908
Region of
Rejection
Region of
Nonrejection
Critical
Value
Region of
Rejection
Critical
Value
–2.2010
0
+2.2010
t
.025
.025
$120
X
.95
F i g u r e  9 . 6
Testing a hypothesis 
about the mean  
(s unknown) at 
the 0.05 level of 
significance with  
11 degrees of 
freedom
Cumulative Probabilities
.75
.90
.95
.975
.99
.995
Upper-Tail Areas
Degrees of Freedom
.25
.10
.05
.025
.01
.005
1
1.0000
3.0777
6.3138
12.7062
31.8207
63.6574
2
0.8165
1.8856
2.9200
4.3027
6.9646
9.9248
3
0.7649
1.6377
2.3534
3.1824
4.5407
5.8409
4
0.7407
1.5332
2.1318
2.7764
3.7469
4.6041
5
0.7267
1.4759
2.0150
2.5706
3.3649
4.0322
6
0.7176
1.4398
1.9432
2.4469
3.1427
3.7074
7
0.7111
1.4149
1.8946
2.3646
2.9980
3.4995
8
0.7064
1.3968
1.8595
2.3060
2.8965
3.3554
9
0.7027
1.3830
1.8331
2.2622
2.8214
3.2498
10
0.6998
1.3722
1.8125
2.2281
2.7638
3.1693
11
0.6974
1.3634
1.7959
2.2010
2.7181
3.1058
Source: Extracted from Table E.3.
T a b l e  9 . 2
Determining the 
Critical Value from the 
t Table for an Area 
of 0.025 in Each Tail, 
with 11 Degrees of 
Freedom
	
9.2  t Test of Hypothesis for the Mean (s Unknown)	
351

352	
Chapter 9  Fundamentals of Hypothesis Testing: One-Sample Tests
The p-Value Approach
To perform this two-tail hypothesis test, you use the five-step method listed in Exhibit 9.2 on 
page 346.
Step 1–3   These steps are the same as in the critical value approach discussed on page 350.
Step 4	
From the Figure 9.7 results, tSTAT = -1.19 and the p-value = 0.2588
Step 5	
Because the p-value of 0.2588 is greater than a = 0.05, you do not reject H0. 
The data provide insufficient evidence to conclude that the mean amount per sales 
invoice differs from $120. The audit suggests that the mean amount per invoice has 
not changed. The p-value indicates that if the null hypothesis is true, the probability 
that a sample of 12 invoices could have a sample mean that differs by $7.15 or more 
from the stated $120 is 0.2588. In other words, if the mean amount per sales invoice 
is truly $120, then there is a 25.88% chance of observing a sample mean below 
$112.85 or above $127.15.
In the preceding example, it is incorrect to state that there is a 25.88% chance that the 
null hypothesis is true. Remember that the p-value is a conditional probability, calculated by  
assuming that the null hypothesis is true. In general, it is proper to state the following:
If the null hypothesis is true, there is a 1p@value2 * 100% chance of observing a test sta-
tistic at least as contradictory to the null hypothesis as the sample result.
Checking the Normality Assumption
You use the t test when the population standard deviation, s, is not known and is estimated us-
ing the sample standard deviation, S. To use the t test, you assume that the data represent a ran-
dom sample from a population that is normally distributed. In practice, as long as the sample 
size is not very small and the population is not very skewed, the t distribution provides a good 
approximation of the sampling distribution of the mean when s is unknown.
There are several ways to evaluate the normality assumption necessary for using the t test. 
You can examine how closely the sample statistics match the normal distribution’s theoretical 
properties. You can also construct a histogram, stem-and-leaf display, boxplot, or normal prob-
ability plot to visualize the distribution of the sales invoice amounts. For details on evaluating 
normality, see Section 6.3 on pages 261–264.
F i g u r e  9 . 7
Excel and Minitab results for the t test of sales invoices
Step 6	 Because -2.2010 6 tSTAT = -1.1908 6 2.2010, you do not reject H0. You have 
insufficient evidence to conclude that the mean amount per sales invoice differs from 
$120. The audit suggests that the mean amount per invoice has not changed.
Figure 9.7 shows the results for this test of hypothesis, as computed by Excel and Minitab.

Figures 9.8 and 9.9 show the descriptive statistics, boxplot, and normal probability plot for 
the sales invoice data.
The mean is very close to the median, and the points on the normal probability appear to 
be increasing approximately in a straight line. The boxplot appears to be approximately sym-
metrical. Thus, you can assume that the population of sales invoices is approximately normally 
distributed. The normality assumption is valid, and therefore the auditor’s results are valid.
The t test is a robust test. A robust test does not lose power if the shape of the popula-
tion departs somewhat from a normal distribution, particularly when the sample size is large 
enough to enable the test statistic t to follow the t distribution. However, you can reach errone-
ous conclusions and can lose statistical power if you use the t test incorrectly. If the sample 
size, n, is small (i.e., less than 30) and you cannot easily make the assumption that the under-
lying population is at least approximately normally distributed, then nonparametric testing 
procedures are more appropriate (see references 2 and 3).
F i g u r e  9 . 8
Excel and Minitab 
descriptive statistics and 
boxplots for the sales 
invoice data
F i g u r e  9 . 9
Excel and Minitab normal probability plots for the sales invoice data
Problems for Section 9.2
Learning the Basics
9.18  If, in a sample of n = 16 selected from a normal popula-
tion, X = 56 and S = 12, what is the value of tSTAT if you are 
testing the null hypothesis H0: m = 50?
9.19  In Problem 9.18, how many degrees of freedom does the  
t test have?
9.20  In Problems 9.18 and 9.19, what are the critical values of t if 
the level of significance, a, is 0.05 and the alternative hypothesis, 
H1, is m ≠50?
9.21  In Problems 9.18, 9.19, and 9.20, what is your statistical de-
cision if the alternative hypothesis, H1, is m ≠50?
	
9.2  t Test of Hypothesis for the Mean (s Unknown)	
353

354	
Chapter 9  Fundamentals of Hypothesis Testing: One-Sample Tests
9.22  If, in a sample of n = 16 selected from a left-skewed popu-
lation, X = 65, and S = 21, would you use the t test to test the 
null hypothesis H0 : m = 60? Discuss.
9.23  If, in a sample of n = 160 selected from a left-skewed pop-
ulation, X = 65, and S = 21, would you use the t test to test the 
null hypothesis H0 : m = 60? Discuss.
Applying the Concepts
SELF 
Test 
9.24  You are the manager of a restaurant for a fast-
food franchise. Last month, the mean waiting time at 
the drive-through window for branches in your geographic region, 
as measured from the time a customer places an order until the 
time the customer receives the order, was 3.7 minutes. You select a 
random sample of 64 orders. The sample mean waiting time is 
3.57 minutes, with a sample standard deviation of 0.8 minute.
a.	 At the 0.05 level of significance, is there evidence that the pop-
ulation mean waiting time is different from 3.7 minutes?
b.	 Because the sample size is 64, do you need to be concerned 
about the shape of the population distribution when conducting 
the t test in (a)? Explain.
9.25  In the modern era, tires last much longer than they used to. 
According to research, the life of tires has increased from 20,000 
miles in 1970 to 80,000 miles in the recent decade. A wholesaler 
wants to estimate the life of a Goodyear tire. He examines a sam-
ple of 50 tires of the Goodyear brand and wants to test whether or 
not the above is true. Assume that the wholesaler tests these sam-
ples for their average life, and finds their average life to be 70,000  
miles, with a standard deviation of 10,000 miles. 
a.	 At 0.05 level of confidence, determine if there is evidence that 
the population mean is different from 70,000 miles.
b.	 Interpret the results for the wholesaler.
9.26  The average price of residential land in San Francisco is 
$420.99 per square feet (Source: http://www.foxbusiness.com/
investing/2012/04/17/ten-most-and-least-affordable-cities-to-
buy-home/). A company willing to invest in property is doubtful 
of the accuracy of the prices. The manager of the company thinks 
that the stated price is on the upper limit and that with the cur-
rent market conditions the price no longer prevails. The manager 
examined the prices of 50 similar properties in that area and found 
an average price of $450 per square feet, with the standard devia-
tion of $100. 
a.	 At 0.05 level of confidence, prove if the average property price 
of $420.99 prevails.
b.	  What conclusion can the manager draw?
9.27  The U.S. Department of Transportation requires tire manu-
facturers to provide performance information on tire sidewalls to 
help prospective buyers make their purchasing decisions. One very 
important piece of information is the tread wear index, which indi-
cates the tire’s resistance to tread wear. A tire with a grade of 200 
should last twice as long, on average, as a tire with a grade of 100.
A consumer organization wants to test the actual tread wear 
index of a brand name of tires that claims “graded 200” on the 
sidewall of the tire. A random sample of n = 18 indicates a sam-
ple mean tread wear index of 195.3 and a sample standard devia-
tion of 21.4.
a.	 Is there evidence that the population mean tread wear index is 
different from 200? (Use a 0.05 level of significance.)
b.	 Determine the p-value and interpret its meaning.
9.28  The file  FastFood  contains the amount that a sample of 
fifteen customers spent for lunch ($) at a fast-food restaurant:
7.42    6.29    5.83    6.50    8.34    9.51    7.10    6.80    5.90
4.89    6.50    5.52    7.90    8.30    9.60
a.	 At the 0.05 level of significance, is there evidence that the 
mean amount spent for lunch is different from $6.50?
b.	 Determine the p-value in (a) and interpret its meaning.
c.	 What assumption must you make about the population distribu-
tion in order to conduct the t test in (a) and (b)?
d.	 Because the sample size is 15, do you need to be concerned 
about the shape of the population distribution when conducting 
the t test in (a)? Explain.
9.29  An insurance company has the business objective of reducing 
the amount of time it takes to approve applications for life insur-
ance. The approval process consists of underwriting, which includes 
a review of the application, a medical information bureau check, 
possible requests for additional medical information and medical 
exams, and a policy compilation stage in which the policy pages are 
generated and sent for delivery. The ability to deliver approved poli-
cies to customers in a timely manner is critical to the profitability of 
this service. During a period of one month, a random sample of 27 
approved policies is selected, and the total processing time, in days, 
is collected. These data, stored in  Insurance , are:
73   19   16   64   28   28   31   90   60   56   31   56   22   18   45   48
17   17   17   91   92   63   50   51   69   16   17
a.	 In the past, the mean processing time was 45 days. At the 0.05 
level of significance, is there evidence that the mean processing 
time has changed from 45 days?
b.	 What assumption about the population distribution is needed in 
order to conduct the t test in (a)?
c.	 Construct a boxplot or a normal probability plot to evaluate the 
assumption made in (b).
d.	 Do you think that the assumption needed in order to conduct 
the t test in (a) is valid? Explain.
9.30  The following data (in  Drink ) represent the amount of soft 
drink filled in a sample of 50 consecutive 2-liter bottles. The re-
sults, listed horizontally in the order of being filled, were:
2.109     2.086     2.066     2.075     2.065     2.057     2.052     2.044
2.036     2.038     2.031     2.029     2.025     2.029     2.023     2.020
2.015     2.014     2.013     2.014     2.012     2.012     2.012     2.010
2.005     2.003     1.999     1.996     1.997     1.992     1.994     1.986
1.984     1.981     1.973     1.975     1.971     1.969     1.966     1.967
1.963     1.957     1.951     1.951     1.947     1.941     1.941     1.938
1.908	
1.894                                   
a.	 At the 0.05 level of significance, is there evidence that the 
mean amount of soft drink filled is different from 2.0 liters?
b.	 Determine the p-value in (a) and interpret its meaning.

c.	 In (a), you assumed that the distribution of the amount of soft 
drink filled was normally distributed. Evaluate this assumption 
by constructing a boxplot or a normal probability plot.
d.	 Do you think that the assumption needed in order to conduct 
the t test in (a) is valid? Explain.
e.	 Examine the values of the 50 bottles in their sequential order, 
as given in the problem. Does there appear to be a pattern to the 
results? If so, what impact might this pattern have on the valid-
ity of the results in (a)?
9.31  One of the major measures of the quality of service pro-
vided by any organization is the speed with which it responds to 
customer complaints. A large family-held department store selling 
furniture and flooring, including carpet, had undergone a major 
expansion in the past several years. In particular, the flooring de-
partment had expanded from 2 installation crews to an installation 
supervisor, a measurer, and 15 installation crews. The store had 
the business objective of improving its response to complaints. 
The variable of interest was defined as the number of days be-
tween when the complaint was made and when it was resolved. 
Data were collected from 50 complaints that were made in the past 
year. These data, stored in  Furniture , are:
54     5     35   137     31   27   152     2   123   81   74   27
11   19   126   110   110   29     61   35     94   31   26     5
12     4   165     32     29   28     29   26     25     1   14   13
13   10       5     27       4   52     30   22     36   26   20   23
33   68                              
a.	 The installation supervisor claims that the mean number of 
days between the receipt of a complaint and the resolution of 
the complaint is 20 days. At the 0.05 level of significance, is 
there evidence that the claim is not true (i.e., the mean number 
of days is different from 20)?
b.	 What assumption about the population distribution is needed in 
order to conduct the t test in (a)?
c.	 Construct a boxplot or a normal probability plot to evaluate the 
assumption made in (b).
d.	 Do you think that the assumption needed in order to conduct 
the t test in (a) is valid? Explain.
9.32  A manufacturing company produces steel housings for 
electrical equipment. The main component part of the housing is 
a steel trough that is made out of a 14-gauge steel coil. It is pro-
duced using a 250-ton progressive punch press with a wipe-down 
operation that puts two 90-degree forms in the flat steel to make 
the trough. The distance from one side of the form to the other is 
critical because of weatherproofing in outdoor applications. The 
company requires that the width of the trough be between 8.31 
inches and 8.61 inches. The file  Trough  contains the widths of the 
troughs, in inches, for a sample of n = 49:
8.312   8.343   8.317   8.383   8.348   8.410   8.351   8.373   8.481   8.422
8.476   8.382   8.484   8.403   8.414   8.419   8.385   8.465   8.498   8.447
8.436   8.413   8.489   8.414   8.481   8.415   8.479   8.429   8.458   8.462
8.460   8.444   8.429   8.460   8.412   8.420   8.410   8.405   8.323   8.420
8.396   8.447   8.405   8.439   8.411   8.427   8.420   8.498   8.409
a.	 At the 0.05 level of significance, is there evidence that the 
mean width of the troughs is different from 8.46 inches?
b.	 What assumption about the population distribution is needed in 
order to conduct the t test in (a)?
c.	 Evaluate the assumption made in (b).
d.	 Do you think that the assumption needed in order to conduct 
the t test in (a) is valid? Explain.
9.33  One operation of a steel mill is to cut pieces of steel into 
parts that are used in the frame for front seats in an automobile. 
The steel is cut with a diamond saw and requires the resulting 
parts must be cut to be within {0.005 inch of the length speci-
fied by the automobile company. The file  Steel  contains a sample 
of 100 steel parts. The measurement reported is the difference, in 
inches, between the actual length of the steel part, as measured by 
a laser measurement device, and the specified length of the steel 
part. For example, a value of -0.002 represents a steel part that is 
0.002 inch shorter than the specified length.
a.	 At the 0.05 level of significance, is there evidence that the 
mean difference is different from 0.0 inches?
b.	 Construct a 95% confidence interval estimate of the population 
mean. Interpret this interval.
c.	 Compare the conclusions reached in (a) and (b).
d.	 Because n = 100, do you have to be concerned about the nor-
mality assumption needed for the t test and t interval?
9.34  In Problem 3.69 on page 170, you were introduced to a 
tea-bag-filling operation. An important quality characteristic of 
interest for this process is the weight of the tea in the individual 
bags. The file  Teabags  contains an ordered array of the weight, in 
grams, of a sample of 50 tea bags produced during an 8-hour shift.
a.	 Is there evidence that the mean amount of tea per bag is differ-
ent from 5.5 grams? (Use a = 0.01.)
b.	 Construct a 99% confidence interval estimate of the population 
mean amount of tea per bag. Interpret this interval.
c.	 Compare the conclusions reached in (a) and (b).
9.35  An article appearing in The Exponent, an independent col-
lege newspaper published by the Purdue Student Publishing Foun-
dation, reported that the average American college student spends 
1 hour (60 minutes) on Facebook daily. (Data extracted from  
bit.ly/QqQHow.) In order to test the validity of this statement, 
you select a sample of 30 Facebook users at your college. The 
results for the time spent on Facebook per day (in minutes) are 
stored in  FacebookTime .
a.	 Is there evidence that the population mean time Facebook time 
is different from 60 minutes? Use the p-value approach and a 
level of significance of 0.05.
b.	 What assumption about the population distribution is needed in 
order to conduct the t test in (a)?
c.	 Make a list of the various ways you could evaluate the assump-
tion noted in (b).
d.	 Evaluate the assumption noted in (b) and determine whether 
the test in (a) is valid.
	
9.2  t Test of Hypothesis for the Mean (s Unknown)	
355

356	
Chapter 9  Fundamentals of Hypothesis Testing: One-Sample Tests
Student Tip
The rejection region 
matches the direction of 
the alternative hypothesis. 
If the alternative hypoth-
esis contains a 6 sign, 
the rejection region is 
in the lower tail. If the 
alternative hypothesis 
contains a 7 sign, the 
rejection region is in the 
upper tail.
9.3  One-Tail Tests
The examples of hypothesis testing in Sections 9.1 and 9.2 are called two-tail tests because 
the rejection region is divided into the two tails of the sampling distribution of the mean. In 
contrast, some hypothesis tests are one-tail tests because they require an alternative hypothesis 
that focuses on a particular direction.
One example of a one-tail hypothesis test would test whether the population mean is less 
than a specified value. One such situation involves the business problem concerning the ser-
vice time at the drive-through window of a fast-food restaurant. According to QSR magazine, 
the speed with which customers are served is of critical importance to the success of the ser-
vice (see bit.ly/WoJpTT). In one past study, an audit of McDonald’s drive-throughs had a 
mean service time of 188.83 seconds, which was slower than the drive-throughs of several 
other fast-food chains. Suppose that McDonald’s began a quality improvement effort to reduce  
the service time by deploying an improved drive-through service process in a sample of  
25 stores. Because McDonald’s would want to institute the new process in all of its stores only 
if the test sample saw a decreased drive-through time, the entire rejection region is located in 
the lower tail of the distribution.
The Critical Value Approach
You wish to determine whether the new drive-through process has a mean that is less than 
188.83 seconds. To perform this one-tail hypothesis test, you use the six-step method listed in 
Exhibit 9.1 on page 343:
Step 1	 You define the null and alternative hypotheses:
H0 : m Ú 188.83
H1 : m 6 188.83
	
	
The alternative hypothesis contains the statement for which you are trying to find evi-
dence. If the conclusion of the test is “reject H0,” there is statistical evidence that 
the mean drive-through time is less than the drive-through time in the old process. 
This would be reason to change the drive-through process for the entire population of 
stores. If the conclusion of the test is “do not reject H0,” then there is insufficient evi-
dence that the mean drive-through time in the new process is significantly less than the 
drive-through time in the old process. If this occurs, there would be insufficient reason 
to institute the new drive-through process in the population of stores.
Step 2	 You collect the data by selecting a sample of n = 25 stores. You decide to use 
a = 0.05.
Step 3	 Because s is unknown, you use the t distribution and the tSTAT test statistic. You need 
to assume that the drive-through time is normally distributed because a sample of only 
25 drive-through times is selected.
Step 4	 The rejection region is entirely contained in the lower tail of the sampling distribution 
of the mean because you want to reject H0 only when the sample mean is significantly 
less than 188.83 seconds. When the entire rejection region is contained in one tail 
of the sampling distribution of the test statistic, the test is called a one-tail test, or 
directional test. If the alternative hypothesis includes the less than sign, the criti-
cal value of t is negative. As shown in Table 9.3 and Figure 9.10, because the entire 
rejection region is in the lower tail of the t distribution and contains an area of 0.05, 
due to the symmetry of the t distribution, the critical value of the t test statistic with 
25 - 1 = 24 degrees of freedom is -1.7109.
	
	
    The decision rule is
reject H0 if tSTAT 6 -1.7109;
otherwise, do not reject H0.

	
9.3  One-Tail Tests	
357
Step 5	 From the sample of 25 stores you selected, you find that the sample mean service time 
at the drive-through equals 170.8 seconds and the sample standard deviation equals 
21.3 seconds. Using n = 25, X = 170.8, S = 21.3, and Equation (9.2) on page 350,
tSTAT = X - m
S
2n
= 170.8 - 188.83
21.3
225
= -4.2324
Step 6	 Because tSTAT = -4.2324 6 -1.7109, you reject the null hypothesis (see  
Figure 9.10). You conclude that the mean service time at the drive-through is less than 
188.83 seconds. There is sufficient evidence to change the drive-through process for 
the entire population of stores.
The p-Value Approach
Use the five steps listed in Exhibit 9.2 on page 346 to illustrate the t test for the drive-through 
time study using the p-value approach:
Step 1–3   These steps are the same as was used in the critical value approach on page 356.
Step 4	
tSTAT = -4.2324 (see step 5 of the critical value approach). Because the alterna-
tive hypothesis indicates a rejection region entirely in the lower tail of the sam-
pling distribution, to compute the p-value, you need to find the probability that the 
tSTAT test statistic will be less than -4.2324. Figure 9.11 on page 358 shows that 
the p-value is 0.0001 (displayed as 0.000 in Minitab).
F i g u r e  9 . 1 0
One-tail test of 
hypothesis for a 
mean (s unknown) 
at the 0.05 level of 
significance
0
–1.7109
t
Region of
Rejection
Region of
Nonrejection
.05
.95
Cumulative Probabilities
.75
.90
.95
.975
.99
.995
Upper-Tail Areas
Degrees of Freedom
.25
.10
.05
.025
.01
.005
1
1.0000
3.0777
6.3138
12.7062
31.8207
63.6574
2
0.8165
1.8856
2.9200
4.3027
6.9646
9.9248
3
0.7649
1.6377
2.3534
3.1824
4.5407
5.8409
f
f
f
f
f
f
f
23
0.6853
1.3195
1.7139
2.0687
2.4999
2.8073
24
0.6848
1.3178
1.7109
2.0639
2.4922
2.7969
25
0.6844
1.3163
1.7081
2.0595
2.4851
2.7874
Source: Extracted from Table E.3.
T a b l e  9 . 3
Determining the 
Critical Value from the 
t Table for an Area of 
0.05 in the Lower Tail, 
with 24 Degrees of 
Freedom

358	
Chapter 9  Fundamentals of Hypothesis Testing: One-Sample Tests
Step 5	 The p-value of 0.0001 is less than a = 0.05 (see Figure 9.12). You reject H0 and 
conclude that the mean service time at the drive-through is less than 188.83 seconds. 
There is sufficient evidence to change the drive-through process for the entire popula-
tion of stores.
F i g u r e  9 . 1 1
Excel and Minitab t test results for the drive-through time study
–4.2324
.9999
t
0.0001
F i g u r e  9 . 1 2
Determining the 
p-value for a one-tail 
test
Example 9.5 illustrates a one-tail test in which the rejection region is in the upper tail.
Example 9.5
A One-Tail Test for 
the Mean
A company that manufactures chocolate bars is particularly concerned that the mean weight 
of a chocolate bar is not greater than 6.03 ounces. A sample of 50 chocolate bars is selected; 
the sample mean is 6.034 ounces, and the sample standard deviation is 0.02 ounce. Using 
the a = 0.01 level of significance, is there evidence that the population mean weight of the 
chocolate bars is greater than 6.03 ounces?
Solution  Using the critical value approach, listed in Exhibit 9.1 on page 343,
Step 1	 First, you define the null and alternative hypotheses:
H0 : m … 6.03
H1 : m 7 6.03
Step 2	 You collect the data from a sample of n = 50. You decide to use a = 0.01.
Step 3	 Because s is unknown, you use the t distribution and the tSTAT test statistic.
Step 4	 The rejection region is entirely contained in the upper tail of the sampling distribution 
of the mean because you want to reject H0 only when the sample mean is significantly 
greater than 6.03 ounces. Because the entire rejection region is in the upper tail of the 
t distribution and contains an area of 0.01, the critical value of the t distribution with 
50 - 1 = 49 degrees of freedom is 2.4049 (see Table E.3).

	
9.3  One-Tail Tests	
359
	
	
    The decision rule is
reject H0 if tSTAT 7 2.4049;
otherwise, do not reject H0.
Step 5	 From your sample of 50 chocolate bars, you find that the sample mean weight 
is 6.034 ounces, and the sample standard deviation is 0.02 ounces. Using 
n = 50, X = 6.034, S = 0.02, and Equation (9.2) on page 350,
tSTAT =  X - m
S
2n
= 6.034 - 6.03
0.02
250
= 1.414
Step 6	 Because tSTAT = 1.414 6 2.4049 or the p-value (from Excel) is 0.0818 7 0.01, you 
do not reject the null hypothesis. There is insufficient evidence to conclude that the 
population mean weight is greater than 6.03 ounces.
To perform one-tail tests of hypotheses, you must properly formulate H0 and H1. A sum-
mary of the null and alternative hypotheses for one-tail tests is as follows:
 • The null hypothesis, H0, represents the status quo or the current belief in a situation.
 • The alternative hypothesis, H1, is the opposite of the null hypothesis and represents a 
research claim or specific inference you would like to prove.
 • If you reject the null hypothesis, you have statistical proof that the alternative hypothesis 
is correct.
 • If you do not reject the null hypothesis, you have failed to prove the alternative hypoth-
esis. The failure to prove the alternative hypothesis, however, does not mean that you 
have proven the null hypothesis.
 • The null hypothesis always refers to a specified value of the population parameter (such 
as m), not to a sample statistic (such as X).
 • The statement of the null hypothesis always contains an equal sign regarding the speci-
fied value of the parameter (e.g., H0 : m Ú 188.83).
 • The statement of the alternative hypothesis never contains an equal sign regarding the 
specified value of the parameter (e.g., H1 : m 6 188.83).
Problems for Section 9.3
Learning the Basics
9.36  In a one-tail hypothesis test where you reject H0 only in the 
upper tail, what is the p-value if ZSTAT = +2.00?
9.37  In Problem 9.36, what is your statistical decision if you test 
the null hypothesis at the 0.05 level of significance?
9.38  In a one-tail hypothesis test where you reject H0 only in the 
lower tail, what is the p-value if ZSTAT = -1.38?
9.39  In Problem 9.38, what is your statistical decision if you test 
the null hypothesis at the 0.01 level of significance?
9.40  In a one-tail hypothesis test where you reject H0 only in the 
lower tail, what is the p-value if ZSTAT = +1.38?
9.41  In Problem 9.40, what is the statistical decision if you test 
the null hypothesis at the 0.01 level of significance?
9.42  In a one-tail hypothesis test where you reject H0 only in 
the upper tail, what is the critical value of the t-test statistic with  
10 degrees of freedom at the 0.01 level of significance?
9.43  In Problem 9.42, what is your statistical decision if 
tSTAT = +2.39?
9.44  In a one-tail hypothesis test where you reject H0 only in the 
lower tail, what is the critical value of the tSTAT test statistic with 
20 degrees of freedom at the 0.01 level of significance?
9.45  In Problem 9.44, what is your statistical decision if 
tSTAT = -1.15?
Applying the Concepts
9.46  The Los Angeles County Metropolitan Transportation Au-
thority has set a bus mechanical reliability goal of 3,900 bus miles. 
Bus mechanical reliability is measured specifically as the number 
of bus miles between mechanical road calls. Suppose a sample of 
100 buses resulted in a sample mean of 3,975 bus miles and a sam-
ple standard deviation of 275 bus miles.
a.	 Is there evidence that the population mean bus miles is more 
than 3,900 bus miles? (Use a 0.05 level of significance.)
b.	 Determine the p-value and interpret its meaning.

360	
Chapter 9  Fundamentals of Hypothesis Testing: One-Sample Tests
9.47  An article on the worldwide parking problem highlighted 
the emerging problems due to the increasing number of cars, and 
that the surface area required just in the United States for car park-
ing is approximately equal to the area of Puerto Rico or 3,459 sq. 
miles (Source: http://www.bbc.com/news/magazine-17271118). 
Assume that the average parking time is 32 minutes. An initiative 
to tackle this problem is to encourage university students to transit 
using bicycles. A professor surveyed 50 students and found that 
the average time taken to park a car is 30 minutes, with a standard 
deviation of 10 minutes. 
a.	 At 0.05 level of significance, can the professor conclude that the 
average time taken to park the cars is more than 32 minutes?
b.	 Explain whether the rejection area lies in the upper tail or lower 
tail. 
SELF 
Test 
9.48  A quality improvement project was conducted 
with the objective of improving the wait time in a 
county health department (CHD) Adult Primary Care Unit 
(APCU). The evaluation plan included waiting room time as one 
key waiting time process measure. Waiting room time was defined 
as the time elapsed between requesting that the patient be seated in 
the waiting room and the time he or she was called to be placed in 
an exam room. Suppose that, initially, a targeted wait time goal of 
25 minutes was set. After implementing an improvement frame-
work and process, the quality improvement team collected data on 
a sample of 355 patients. In this sample, the mean wait time was 
23.05 minutes, with a standard deviation of 16.83 minutes. (Data 
extracted from M. Michael, S. D. Schaffer, P. L. Egan, B. B. Little, 
and P. S. Pritchard, “Improving Wait Times and Patient Satisfac-
tion in Primary Care,” Journal for Healthcare Quality, 2013, 
35(2), pp. 50–60.)
a.	 If you test the null hypothesis at the 0.01 level of significance, 
is there evidence that the population mean wait time is less than 
25 minutes?
b.	 Interpret the meaning of the p-value in this problem.
9.49  You are the manager of a restaurant that delivers pizza to 
college dormitory rooms. You have just changed your delivery 
process in an effort to reduce the mean time between the order and 
completion of delivery from the current 25 minutes. A sample of 
36 orders using the new delivery process yields a sample mean of 
22.4 minutes and a sample standard deviation of 6 minutes.
a.	 Using the six-step critical value approach, at the 0.05 level of 
significance, is there evidence that the population mean deliv-
ery time has been reduced below the previous population mean 
value of 25 minutes?
b.	 At the 0.05 level of significance, use the five-step p-value ap-
proach.
c.	 Interpret the meaning of the p-value in (b).
d.	 Compare your conclusions in (a) and (b).
9.50  A survey of nonprofit organizations showed that online fun-
draising has increased in the past year. Based on a random sample 
of 55 nonprofit organizations, the mean one-time gift donation in 
the past year was $75, with a standard deviation of $9.
a.	 If you test the null hypothesis at the 0.01 level of significance, 
is there evidence that the mean one-time gift donation is greater 
than $70?
b.	 Interpret the meaning of the p-value in this problem.
9.51  The population mean waiting time to check out of a su-
permarket has been 4 minutes. Recently, in an effort to reduce 
the waiting time, the supermarket has experimented with a sys-
tem in which infrared cameras use body heat and in-store soft-
ware to determine how many lanes should be opened. A sample 
of 100 customers was selected, and their mean waiting time to 
check out was 3.25 minutes, with a sample standard deviation of  
2.7 minutes.
a.	 At the 0.05 level of significance, using the critical value ap-
proach to hypothesis testing, is there evidence that the popula-
tion mean waiting time to check out is less than 4 minutes?
b.	 At the 0.05 level of significance, using the p-value approach to 
hypothesis testing, is there evidence that the population mean 
waiting time to check out is less than 4 minutes?
c.	 Interpret the meaning of the p-value in this problem.
d.	 Compare your conclusions in (a) and (b).
9.4  Z Test of Hypothesis for the Proportion
In some situations, you want to test a hypothesis about the proportion of events of interest in 
the population, p, rather than test the population mean. To begin, you select a random sample 
and compute the sample proportion, p = X>n. You then compare the value of this statistic 
to the hypothesized value of the parameter, p, in order to decide whether to reject the null 
­hypothesis.
If the number of events of interest (X) and the number of events that are not of interest 
1n - X2 are each at least five, the sampling distribution of a proportion approximately follows 
a normal distribution, and you can use the Z test for the proportion. Equation (9.3) defines 
this hypothesis test for the difference between the sample proportion, p, and the hypothesized 
population proportion, p.
Student Tip
Do not confuse this use 
of the Greek letter pi, p, 
to represent the popula-
tion proportion with the 
mathematical constant 
that is the ratio of the 
circumference to a  
diameter of a circle— 
approximately 
3.14159—which is 
also known by the same 
Greek letter.
Z Test for the Proportion
	
ZSTAT =
p - p
B
p11 - p2
n
	
(9.3)

	
9.4  Z Test of Hypothesis for the Proportion	
361
Alternatively, by multiplying the numerator and denominator by n, you can write the ZSTAT 
test statistic in terms of the number of events of interest, X, as shown in Equation (9.4).
where
p = sample proportion = X
n = number of events of interest in the sample
sample size
p = hypothesized proportion of events of interest in the population
The ZSTAT test statistic approximately follows a standardized normal distribution 
when X and 1n - X2 are each at least 5.
Z Test for the Proportion in Terms of the Number  
of Events of Interest
	
ZSTAT =
X - np
2np11 - p2
	
(9.4)
The Critical Value Approach
To illustrate the Z test for a proportion, consider a survey that sought to determine whether 
adults could stop thinking about work while on vacation. (Data extracted from “Can You Stop 
Thinking About Work on Your Vacation?” USA Today, October 5, 2011, p. 29A.) Of 1,000 
adults, 320 said that they were unable to stop thinking about work while on vacation. Suppose 
that a survey conducted in the previous year indicated that 30% of adults were unable to stop 
thinking about work while on vacation. Is there evidence that the percentage of adults who 
were unable to stop thinking about work while on vacation has changed from the previous 
year? To investigate this question, the null and alternative hypotheses are follows:
H0 : p = 0.30 (i.e., the proportion of adults who were unable to stop thinking about work 
while on vacation has not changed from the previous year)
H1 : p ≠0.30  (i.e., the proportion of adults who were unable to stop thinking about work 
while on vacation has changed from the previous year)
Because you are interested in determining whether the population proportion of adults who 
were unable to stop thinking about work while on vacation has changed from 0.30 in the previ-
ous year, you use a two-tail test. If you select the a = 0.05 level of significance, the rejection 
and nonrejection regions are set up as in Figure 9.13, and the decision rule is
reject H0 if ZSTAT 6 -1.96 or if ZSTAT 7 +1.96;
otherwise, do not reject H0.
0
–1.96
+1.96
Z
Region of
Rejection
Region of
Nonrejection
Region of
Rejection
.95
F i g u r e  9 . 1 3
Two-tail test of 
hypothesis for the 
proportion at the 0.05 
level of significance

362	
Chapter 9  Fundamentals of Hypothesis Testing: One-Sample Tests
Because 320 of the 1,000 adults stated that they were unable to stop thinking about work while 
on vacation,
p =
320
1,000 = 0.32
Since X = 320 and n - X = 680, each 7 5, using Equation (9.3),
ZSTAT =
p - p
B
p11 - p2
n
=
0.32 - 0.30
B
0.3011 - 0.302
1,000
=
0.02
0.0145 = 1.3801
or, using Equation (9.4),
ZSTAT =
X - np
2np11 - p2
= 320 - 11,000210.302
11,00010.30210.702 =
20
14.4914 = 1.3801
Because ZSTAT = 1.3801 6 1.96, you do not reject H0. There is insufficient evidence that the 
population proportion of all adults who were unable to stop thinking about work on vacation 
has changed from 0.30 in the previous year. Figure 9.14 presents the Excel and Minitab results 
for these data.
F i g u r e  9 . 1 4
Excel and Minitab results 
for the Z test for whether 
the proportion of adults 
who were unable to 
stop thinking about 
work while on vacation 
has changed from the 
previous year
The p-Value Approach
As an alternative to the critical value approach, you can compute the p-value. For this two-tail 
test in which the rejection region is located in the lower tail and the upper tail, you need to 
find the area below a Z value of -1.3801 and above a Z value of +1.3801. Figure 9.14 re-
ports a p-value of 0.1675. Because this value is greater than the selected level of significance 
1a = 0.052, you do not reject the null hypothesis.
Example 9.6 illustrates a one-tail test for a proportion.
Example 9.6
Testing a  
Hypothesis for  
a Proportion
In addition to the business problem of the speed of service at the drive-through, fast-food 
chains want to fill orders correctly. The same audit that reported that McDonald’s had a drive-
through service time of 188.83 seconds also reported that McDonald’s filled 90.9% of its 
drive-through orders correctly (see www.qsrmagazine.com/content/2012-qsr-drive-thru-
study-order=accuracy). Suppose that McDonald’s begins a quality improvement effort to 
­ensure that orders at the drive-through are filled correctly. The business problem is defined 
as determining whether the new process can increase the percentage of orders filled correctly. 

Data are collected from a sample of 400 orders using the new process. The results indicate that 
378 orders were filled correctly. At the 0.01 level of significance, can you conclude that the 
new process has increased the proportion of orders filled correctly?
Solution  The null and alternative hypotheses are 
H0 : p … 0.909 (i.e., the population proportion of orders filled correctly using the new  
­process is less than or equal to 0.909)
H1 : p 7 0.909 (i.e., the population proportion of orders filled correctly using the new 
­process is greater than 0.909)
Since X = 374 and n - X = 26, both 7 5, using Equation (9.3) on page 360,
 p = X
n = 378
400 = 0.945
 ZSTAT =
p - p
B
p11 - p2
n
=
0.945 - 0.909
B
0.90911 - 0.9092
400
= 0.036
0.0144 = 2.5034
The p-value (computed by Excel) for ZSTAT 7 2.5034 is 0.0062.
Using the critical value approach, you reject H0 if ZSTAT 7 2.33. Using the p-value 
­approach, you reject H0 if the p-value 6 0.01. Because ZSTAT = 2.5034 7 2.33 or the 
­p-value = 0.0062 6 0.01, you reject H0. You have evidence that the new process has in-
creased the ­proportion of correct orders above 0.909 or 90.9%.
Problems for Section 9.4
Learning the Basics
9.52  If, in a random sample of 400 items, 88 are defective, what 
is the sample proportion of defective items?
9.53  In Problem 9.52, if the null hypothesis is that 20% of the 
items in the population are defective, what is the value of ZSTAT?
9.54  In Problems 9.52 and 9.53, suppose you are testing the 
null hypothesis H0 : p = 0.20 against the two-tail alternative hy-
pothesis H1 : p ≠0.20 and you choose the level of significance 
a = 0.05. What is your statistical decision?
Applying the Concepts
9.55  The U.S. Department of Education reports that 40% of full-
time college students are employed while attending college. (Data 
extracted from National Center for Education Statistics, The Con-
dition of Education 2012, nces.ed.gov/pubs2012/2012045.pdf.) A 
recent survey of 60 full-time students at a university found that 25 
were employed.
a.	 Use the five-step p-value approach to hypothesis testing and a 
0.05 level of significance to determine whether the proportion 
of full-time students at the university is different from the na-
tional norm of 0.40.
b.	 Assume that the study found that 32 of the 60 full-time students 
were employed and repeat (a). Are the conclusions the same?
9.56  The Center for Disease Control and Prevention believes that 
regular screening between the ages of 50 to 75 years can prevent 
colorectal cancer (Source: http://www.cdc.gov/cancer/colorec-
tal/basic_info/screening/guidelines.htm). If the report was based 
on the assumption that 50% of the population between the age  
group of 50–75 is aware of the test and that the Center is tak-
ing measures to spread the awareness. However, it was found in  
a survey that not much people are aware of this in California. The 
sample size of the survey was 500 where people in the age group 
of 50–75 had been  questioned and it was found that only 235  
people were aware of the test and went for screening once a year. 
a.	 State the null and the alternate hypothesis.
b.	 At 0.05 level of significance, try to determine whether there is 
evidence that the proportion of the people aware of this test is 
greater than 50%.
c.	 Assume that the sample size for the above survey is 100. At 0.05 
level of significance, try to determine whether there is evidence 
that the proportion of the people aware of this test is greater  
than 50%.
d.	 How would the Center interpret the above results?
9.57  One of the issues facing organizations is increasing di-
versity throughout an organization. One of the ways to evaluate 
an organization’s success at increasing diversity is to compare 
the percentage of employees in the organization in a particu-
lar position with a specific background to the percentage in a 
particular position with that specific background in the gen-
eral workforce. Recently, a large academic medical center de-
termined that 9 of 17 employees in a particular position were 
female, whereas 55% of the employees for this position in the 
general workforce were female. At the 0.05 level of signifi-
cance, is there evidence that the proportion of females in this 
position at this medical center is different from what would be 
expected in the general workforce?
	
9.4  Z Test of Hypothesis for the Proportion	
363

364	
Chapter 9  Fundamentals of Hypothesis Testing: One-Sample Tests
SELF 
Test 
9.58  Of 801 surveyed active LinkedIn members, 328 
reported that they are planning to spend at least $1,000 
on consumer electronics in the coming year. (Data extracted from 
bit.ly/RlTfFU.) At the 0.05 level of significance, is there evidence 
that the proportion of all LinkedIn members who plan to spend at 
least $1,000 on consumer electronics in the coming year is differ-
ent from 35%?
9.59  A cellphone provider has the business objective of want-
ing to estimate the proportion of subscribers who would up-
grade to a new cellphone with improved features if it were made 
available at a substantially reduced cost. Data are collected from 
a random sample of 500 subscribers. The results indicate that 
135 of the subscribers would upgrade to a new cellphone at a 
reduced cost.
a.	 At the 0.05 level of significance, is there evidence that more 
than 20% of the customers would upgrade to a new cellphone 
at a reduced cost?
b.	 How would the manager in charge of promotional programs 
concerning residential customers use the results in (a)?
9.60  Actuation Consulting and Enterprise Agility recently con-
ducted a global survey of product teams with the goal of better 
understanding the dynamics of product team performance and 
uncovering the practices that make these teams successful. One 
question posed was “In which of the following ways does your 
organization support aligning members of a core product team?” 
Global respondents were offered five choices. (Data extracted 
from www.actuationconsultingllc.com/blog/?p=285.) The most 
common response (31%) was “shared organizational goals and ob-
jectives linking the team.” Suppose another study is conducted to 
check the validity of this result, with the goal of proving that the 
percentage is less than 31%.
a.	 State the null and research hypotheses.
b.	 A sample of 100 organizations is selected, and results indi-
cate that 28 organizations respond that “shared organizational 
goals and objectives linking the team” is the supported driver 
of alignment. Use either the six-step critical value hypothesis-
testing approach or the five-step p-value approach to determine 
at the 0.05 level of significance whether there is evidence that 
the percentage is less than 31%.
9.5  Potential Hypothesis-Testing Pitfalls and Ethical Issues
To this point, you have studied the fundamental concepts of hypothesis testing. You have 
used hypothesis testing to analyze differences between sample statistics and hypothesized 
population parameters in order to make business decisions concerning the underlying popu-
lation characteristics. You have also learned how to evaluate the risks involved in making 
these decisions.
When planning to carry out a hypothesis test based on a survey, research study, or ­designed 
experiment, you must ask several questions to ensure that you use proper methodology. You 
need to raise and answer questions such as the following in the planning stage:
•  What is the goal of the survey, study, or experiment? How can you translate the 
goal into a null hypothesis and an alternative hypothesis?
•  Is the hypothesis test a two-tail test or one-tail test?
•  Can you select a random sample from the underlying population of interest?
•  What types of data will you collect in the sample? Are the variables numerical 
or ­categorical?
•  At what level of significance should you conduct the hypothesis test?
•  Is the intended sample size large enough to achieve the desired power of the test 
for the level of significance chosen?
•  What statistical test procedure should you use and why?
•  What conclusions and interpretations can you reach from the results of the 
hypothesis test?
Failing to consider these questions early in the planning process can lead to biased or 
­incomplete results. Proper planning can help ensure that the statistical study will provide 
­objective information needed to make good business decisions.
Statistical Significance Versus Practical Significance
You need to make a distinction between the existence of a statistically significant result and 
its practical significance in a field of application. Sometimes, due to a very large sample 
size, you may get a result that is statistically significant but has little practical significance. 
For example, suppose that prior to a national marketing campaign focusing on a series of 
expensive television commercials, you believe that the proportion of people who recognize 

	
9.6  Power of a Test	
365
your brand is 0.30. At the completion of the campaign, a survey of 20,000 people indi-
cates that 6,168 recognized your brand. A one-tail test trying to prove that the proportion is 
now greater than 0.30 results in a p-value of 0.0047, and the correct statistical conclusion is 
that the proportion of consumers recognizing your brand name has now increased. Was the 
campaign successful? The result of the hypothesis test indicates a statistically significant 
increase in brand awareness, but is this increase practically important? The population pro-
portion is now estimated at 6,168>20,000 = 0.3084 = 0.3084 or 30.84%. This increase is 
less than 1% more than the hypothesized value of 30%. Did the large expenses associated 
with the marketing campaign produce a result with a meaningful increase in brand aware-
ness? Because of the minimal real-world impact that an increase of less than 1% has on the 
overall marketing strategy and the huge expenses associated with the marketing campaign, 
you should conclude that the campaign was not successful. On the other hand, if the cam-
paign increased brand awareness from 30% to 50%, you would be inclined to conclude that 
the campaign was successful.
Statistical Insignificance Versus Importance
In contrast to the issue of the practical significance of a statistically significant result is 
the situation in which an important result may not be statistically significant. In a recent 
case (see reference 1), the U.S. Supreme Court ruled that companies cannot rely solely on 
whether the result of a study is significant when determining what they communicate to 
investors. In some situations (see reference 6), the lack of a large enough sample size may 
result in a nonsignificant result when in fact an important difference does exist. A study that 
compared male and female entrepreneurship rates globally and within Massachusetts found 
a significant difference globally but not within Massachusetts, even though the entrepre-
neurship rates for females and for males in the two geographic areas were similar (8.8% for 
males in Massachusetts as compared to 8.4% globally; 5% for females in both geographic 
areas). The difference was due to the fact that the global sample size was 20 times larger 
than the Massachusetts sample size.
Reporting of Findings
In conducting research, you should document both good and bad results. You should not just 
report the results of hypothesis tests that show statistical significance but omit those for which 
there is insufficient evidence in the findings. In instances in which there is insufficient evi-
dence to reject H0, you must make it clear that this does not prove that the null hypothesis is 
true. What the result indicates is that with the sample size used, there is not enough informa-
tion to disprove the null hypothesis.
Ethical Issues
You need to distinguish between poor research methodology and unethical behavior. 
­Ethical considerations arise when the hypothesis-testing process is manipulated. Some of 
the areas where ethical issues can arise include the use of human subjects in experiments, 
the data collection method, the type of test (one-tail or two-tail test), the choice of the 
level of significance, the cleansing and discarding of data, and the failure to report perti-
nent findings.
9.6  Power of a Test
The power of a hypothesis test is the probability that you correctly reject a false null hypoth-
esis. The power of a test is affected by the level of significance, the sample size, and whether 
the test is one-tail or two-tail. The Section 9.6 online topic discusses the concept of the power 
of a test.

366	
Chapter 9  Fundamentals of Hypothesis Testing: One-Sample Tests
A
s the plant operations manager for Oxford Cereals, you 
were responsible for the cereal-filling process. It was 
your responsibility to adjust the process when the mean fill 
weight in the population of boxes deviated from the ­company 
specification of 368 grams. You chose to conduct a hypoth-
esis test.
You determined that the null hypothesis should be 
that the population mean fill was 368 grams. If the mean 
weight of the sampled boxes was sufficiently above or 
below the expected 368-gram mean specified by Oxford  
Cereals, you would reject the null hypothesis in favor of the 
alternative hypothesis that the mean fill was different from  
368 grams. If this happened, you would stop production  
and take whatever action was necessary to correct the prob-
lem. If the null hypothesis was not rejected, you would 
continue to believe in the status quo—that the process 
was working correctly—and therefore take no corrective  
action.
Before proceeding, you considered the risks involved 
with hypothesis tests. If you rejected a true null hypothesis, 
you would make a Type 
I error and conclude that 
the population mean fill 
was not 368 when it ac-
tually was 368 grams. 
This error would result in adjusting the filling process even 
though the process was working properly. If you did not re-
ject a false null hypothesis, you would make a Type II error 
and conclude that the population mean fill was 368 grams 
when it actually was not 368 grams. Here, you would allow 
the process to continue without adjustment even though the 
process was not working properly.
After collecting a random sample of 25 cereal boxes, 
you used either the six-step critical value approach or the 
five-step p-value approach to hypothesis testing. ­Because 
the test statistic fell into the nonrejection region, you did 
not reject the null hypothesis. You concluded that there was 
­insufficient evidence to prove that the mean fill differed from 
368 grams. No corrective action on the filling process was 
needed.
U s i n g  S tat i s t i c s
Significant Testing at Oxford Cereals, Revisited
Shutterstock
S u m m a r y
This chapter presented the foundation of hypothesis testing. 
You learned how to perform tests on the population mean 
and on the population proportion. The chapter developed 
both the critical value approach and the p-value approach to 
hypothesis testing.
In deciding which test to use, you should ask the 
f­ollowing question: Does the test involve a numerical vari-
able or a categorical variable? If the test involves a numerical 
variable, you use the t test for the mean. If the test involves a  
categorical variable, you use the Z test for the proportion.  
Table 9.4 lists the hypothesis tests covered in the chapter.
T a b l e  9 . 4
Summary of Topics in 
Chapter 9
 
Type of Data
Type of Analysis
Numerical
Categorical
Hypothesis test concerning 
a single parameter
Z test of hypothesis for the mean 
(Section 9.1)
t test of hypothesis for the mean 
(Section 9.2)
Z test of hypothesis for the 
proportion (Section 9.4)
Referen c e s
	 1.	Bialik, C. “Making a Stat Less Significant.” The Wall Street 
Journal, April 2, 2011, A5.
	 2.	Bradley, J. V. Distribution-Free Statistical Tests. Upper Saddle 
River, NJ: Prentice Hall, 1968.
	 3.	Daniel, W. Applied Nonparametric Statistics, 2nd ed. Boston: 
Houghton Mifflin, 1990.
	 4.	Microsoft Excel 2013. Redmond, WA: Microsoft Corp., 2012.
	 5.	Minitab Release 16. State College, PA: Minitab, Inc., 2010.
	 6.	Seaman, J., and E. Allen. “Not Significant, But Important?” 
Quality Progress, August 2011, 57–59.

	
Chapter Review Problems	
367
K ey Eq u at i o n s
Z Test for the Mean (S Known)
ZSTAT = X - m
s
1n
	
(9.1)
t Test for the Mean (S Unknown)
tSTAT = X - m
S
1n
	
(9.2)
Z Test for the Proportion
ZSTAT =
p - p
B
p11 - p2
n
	
(9.3)
Z Test for the Proportion in Terms of the Number of 
Events of Interest
ZSTAT =
X - np
2np11 - p2
	
(9.4)
K ey Te r ms
alternative hypothesis 1H12  337
b risk  340
confidence coefficient  341
critical value  339
directional test  356
hypothesis testing  337
level of significance 1a2  340
null hypothesis 1H02  337
one-tail test  356
p-value  345
power of a statistical test  341
region of nonrejection  339
region of rejection  339
robust  353
sample proportion  360
t test for the mean  349
test statistic  339
two-tail test  342
Type I error  340
Type II error  340
Z test for the mean  342
Z test for the proportion  360
C h ec ki n g  Yo u r  U n de r s ta nding
9.61  Explain the terms ‘decision rule’ and ‘critical value’. How 
are the two terms interrelated? What decision rule do you apply 
when using the ZSTAT approach?
9.62  What is the difference between a Type I error and a Type II 
error?
9.63  Under what conditions is a t test used instead of a Z test?
9.64  State in what conditions one-tail hypothesis test can be 
used. Illustrate when the rejection region lies in the upper tail of 
the t distribution.
9.65  When should a t test be used rather than non-parametric 
tests?
9.66  How can a confidence interval estimate for the population 
mean provide conclusions for the corresponding two-tail hypoth-
esis test for the population mean?
9.67  What is the six-step critical value approach to hypothesis 
testing?
9.68  What is the five-step p-value approach to hypothesis testing?
C h a pte r  R e vi e w P r ob le ms
9.69  In hypothesis testing, the common level of significance is 
a = 0.05. Some might argue for a level of significance greater 
than 0.05. Suppose that web designers tested the proportion of 
potential web page visitors with a preference for a new web de-
sign over the existing web design. The null hypothesis was that 
the population proportion of web page visitors preferring the new 
design was 0.50, and the alternative hypothesis was that it was not 
equal to 0.50. The p-value for the test was 0.20.
a.	 State, in statistical terms, the null and alternative hypotheses 
for this example.
b.	 Explain the risks associated with Type I and Type II errors in 
this case.
c.	 What would be the consequences if you rejected the null hy-
pothesis for a p-value of 0.20?
d.	 What might be an argument for raising the value of a?
e.	 What would you do in this situation?
f.	 What is your answer in (e) if the p-value equals 0.12? What if it 
equals 0.06?
9.70  Financial institutions utilize prediction models to predict 
bankruptcy. One such model is the Altman Z-score model, which 
uses multiple corporate income and balance sheet values to mea-
sure the financial health of a company. If the model predicts a 
low Z-score value, the firm is in financial stress and is predicted 
to go bankrupt within the next two years. If the model predicts 
a moderate or high Z-score value, the firm is financially healthy 
and is predicted to be a non-bankrupt firm (see pages.stern.nyu 
.edu/~ealtman/Zscores.pdf). This decision-making procedure 
can be expressed in the hypothesis-testing framework. The null 

368	
Chapter 9  Fundamentals of Hypothesis Testing: One-Sample Tests
­hypothesis is that a firm is predicted to be a non-bankrupt firm. 
The alternative hypothesis is that the firm is predicted to be a 
bankrupt firm.
a.	 Explain the risks associated with committing a Type I error in 
this case.
b.	 Explain the risks associated with committing a Type II error in 
this case.
c.	 Which type of error do you think executives want to avoid?  
Explain.
d.	 How would changes in the model affect the probabilities of 
committing Type I and Type II errors?
9.71  An insurance company is budgeting the premium rates for 
the following year. The company decided the rates of the year be-
fore based on the average amount they paid as insurance claims, 
$1000. However, the manager believes that the actual amount of 
claims that they pay is higher. Thus, they should increase the in-
surance premium. The manager took a sample of 50 claims, and 
found the average mean claim of $1,200, with the standard devia-
tion of $500. 
a.	 At 0.05 level of significance, use the six step critical value ap-
proach to assist the manager in taking the decision if the aver-
age insurance claim of the population is more than $1,000.
b.	 At the 0.05 level of significance, use the five-step p-value ap-
proach to assist the manager in taking the decision if the aver-
age insurance claim of the population is more than $1,000.
c.	 Interpret the meaning of the p-value in (b).
d.	 Compare your conclusions in (a) and (b).
9.72  The owner of a specialty coffee shop wants to study coffee 
purchasing habits of customers at her shop. She selects a random 
sample of 60 customers during a certain week, with the following 
results:
• The amount spent was X = +7.25, S = +1.75
• Thirty-one customers say they “definitely will” recommend 
the specialty coffee shop to family and friends.
a.	 At the 0.05 level of significance, is there evidence that the pop-
ulation mean amount spent was different from $6.50?
b.	 Determine the p-value in (a).
c.	 At the 0.05 level of significance, is there evidence that more 
than 50% of all the customers say they “definitely will” recom-
mend the specialty coffee shop to family and friends?
d.	 What is your answer to (a) if the sample mean equals $6.25?
e.	 What is your answer to (c) if 39 customers say they “definitely 
will” recommend the specialty coffee shop to family and friends?
9.73  An auditor for a government agency was assigned the task 
of evaluating reimbursement for office visits to physicians paid by 
Medicare. The audit was conducted on a sample of 75 of the reim-
bursements, with the following results:
• In 12 of the office visits, there was an incorrect amount of 
reimbursement.
• The amount of reimbursement was X = +93.70, S = +34.55.
a.	 At the 0.05 level of significance, is there evidence that the pop-
ulation mean reimbursement was less than $100?
b.	 At the 0.05 level of significance, is there evidence that the 
proportion of incorrect reimbursements in the population was 
greater than 0.10?
c.	 Discuss the underlying assumptions of the test used in (a).
d.	 What is your answer to (a) if the sample mean equals $90?
e.	 What is your answer to (b) if 15 office visits had incorrect 
­reimbursements?
9.74  A bank branch located in a commercial district of a city 
has the business objective of improving the process for serv-
ing customers during the noon-to-1:00 p.m. lunch period. The 
­waiting time (defined as the time the customer enters the line 
­until he or she reaches the teller window) of a random sample 
of 15 customers is collected, and the results are organized and 
stored in  Bank1 . These data are:
4.21  5.55  3.02  5.13  4.77  2.34  3.54  3.20
4.50  6.10  0.38  5.12  6.46  6.19  3.79
a.	 At the 0.05 level of significance, is there evidence that the pop-
ulation mean waiting time is less than 5 minutes?
b.	 What assumption about the population distribution is needed in 
order to conduct the t test in (a)?
c.	 Construct a boxplot or a normal probability plot to evaluate the 
assumption made in (b).
d.	 Do you think that the assumption needed in order to conduct 
the t test in (a) is valid? Explain.
e.	 As a customer walks into the branch office during the lunch 
hour, she asks the branch manager how long she can expect to 
wait. The branch manager replies, “Almost certainly not longer 
than 5 minutes.” On the basis of the results of (a), evaluate this 
statement.
9.75  A manufacturing company produces electrical insulators. If 
the insulators break when in use, a short circuit is likely to occur. 
To test the strength of the insulators, destructive testing is carried 
out to determine how much force is required to break the insu-
lators. Force is measured by observing the number of pounds of 
force applied to the insulator before it breaks. The following data 
(stored in  Force ) are from 30 insulators subjected to this testing:
1,870  1,728  1,656  1,610  1,634  1,784  1,522  1,696  1,592  1,662
1,866  1,764  1,734  1,662  1,734  1,774  1,550  1,756  1,762  1,866
1,820  1,744  1,788  1,688  1,810  1,752  1,680  1,810  1,652  1,736
a.	 At the 0.05 level of significance, is there evidence that the 
population mean force required to break the insulator is greater 
than 1,500 pounds?
b.	 What assumption about the population distribution is needed in 
order to conduct the t test in (a)?
c.	 Construct a histogram, boxplot, or normal probability plot to 
evaluate the assumption made in (b).
d.	 Do you think that the assumption needed in order to conduct 
the t test in (a) is valid? Explain.
9.76  An important quality characteristic used by the manufac-
turer of Boston and Vermont asphalt shingles is the amount of 
moisture the shingles contain when they are packaged. Customers 
may feel that they have purchased a product lacking in quality if 
they find moisture and wet shingles inside the packaging. In some 
cases, excessive moisture can cause the granules attached to the 
shingles for texture and coloring purposes to fall off the shingles, 
resulting in appearance problems. To monitor the amount of mois-
ture present, the company conducts moisture tests. A shingle is 
weighed and then dried. The shingle is then reweighed, and, based 
on the amount of moisture taken out of the product, the pounds of 
moisture per 100 square feet are calculated. The company would 
like to show that the mean moisture content is less than 0.35 pound 
per 100 square feet. The file  Moisture  includes 36 measurements 

	
Cases for Chapter 9	
369
(in pounds per 100 square feet) for Boston shingles and 31 for 
­Vermont shingles.
a.	 For the Boston shingles, is there evidence at the 0.05 level of 
significance that the population mean moisture content is less 
than 0.35 pound per 100 square feet?
b.	 Interpret the meaning of the p-value in (a).
c.	 For the Vermont shingles, is there evidence at the 0.05 level of 
significance that the population mean moisture content is less 
than 0.35 pound per 100 square feet?
d.	 Interpret the meaning of the p-value in (c).
e.	 What assumption about the population distribution is needed in 
order to conduct the t tests in (a) and (c)?
f.	 Construct histograms, boxplots, or normal probability plots to 
evaluate the assumption made in (a) and (c).
g.	 Do you think that the assumption needed in order to conduct 
the t tests in (a) and (c) is valid? Explain.
9.77  Studies conducted by the manufacturer of Boston and Ver-
mont asphalt shingles have shown product weight to be a ma-
jor factor in the customer’s perception of quality. Moreover, the 
weight represents the amount of raw materials being used and is 
therefore very important to the company from a cost standpoint. 
The last stage of the assembly line packages the shingles before 
the packages are placed on wooden pallets. Once a pallet is full (a 
pallet for most brands holds 16 squares of shingles), it is weighed, 
and the measurement is recorded. The file  Pallet  contains the 
weight (in pounds) from a sample of 368 pallets of Boston shin-
gles and 330 pallets of Vermont shingles.
a.	 For the Boston shingles, is there evidence at the 0.05 level of 
significance that the population mean weight is different from 
3,150 pounds?
b.	 Interpret the meaning of the p-value in (a).
c.	 For the Vermont shingles, is there evidence at the 0.05 level of 
significance that the population mean weight is different from 
3,700 pounds?
d.	 Interpret the meaning of the p-value in (c).
e.	 In (a) through (d), do you have to be concerned with the nor-
mality assumption? Explain.
9.78  The manufacturer of Boston and Vermont asphalt shingles 
provides its customers with a 20-year warranty on most of its 
products. To determine whether a shingle will last through the 
warranty period, accelerated-life testing is conducted at the manu-
facturing plant. Accelerated-life testing exposes the shingle to the 
stresses it would be subject to in a lifetime of normal use in a labo-
ratory setting via an experiment that takes only a few minutes to 
conduct. In this test, a shingle is repeatedly scraped with a brush 
for a short period of time, and the shingle granules removed by 
the brushing are weighed (in grams). Shingles that experience low 
amounts of granule loss are expected to last longer in normal use 
than shingles that experience high amounts of granule loss. The 
file  Granule  contains a sample of 170 measurements made on 
the company’s Boston shingles and 140 measurements made on 
Vermont shingles.
a.	 For the Boston shingles, is there evidence at the 0.05 level of 
significance that the population mean granule loss is different 
from 0.30 grams?
b.	 Interpret the meaning of the p-value in (a).
c.	 For the Vermont shingles, is there evidence at the 0.05 level of 
significance that the population mean granule loss is different 
from 0.30 grams?
d.	 Interpret the meaning of the p-value in (c).
e.	 In (a) through (d), do you have to be concerned with the nor-
mality assumption? Explain.
Report Writing Exercise
9.79  Referring to the results of Problems 9.76 through 9.78 con-
cerning Boston and Vermont shingles, write a report that evaluates 
the moisture level, weight, and granule loss of the two types of 
shingles.
C a s e s  f o r  C h a p t e r  9
Managing Ashland MultiComm Services
Continuing its monitoring of the upload speed first de-
scribed in the Chapter 6 Managing Ashland MultiComm 
Services case on page 273, the technical operations depart-
ment wants to ensure that the mean target upload speed for 
all Internet service subscribers is at least 0.97 on a standard 
scale in which the target value is 1.0. Each day, upload 
speed was measured 50 times, with the following results 
(stored in  AMS9 ).
0.854  1.023  1.005  1.030  1.219  0.977  1.044  0.778  1.122  1.114
1.091  1.086  1.141  0.931  0.723  0.934  1.060  1.047  0.800  0.889
1.012  0.695  0.869  0.734  1.131  0.993  0.762  0.814  1.108  0.805
1.223  1.024  0.884  0.799  0.870  0.898  0.621  0.818  1.113  1.286
1.052  0.678  1.162  0.808  1.012  0.859  0.951  1.112  1.003  0.972
	 1.	 Compute the sample statistics and determine whether 
there is evidence that the population mean upload speed 
is less than 0.97.
	 2.	 Write a memo to management that summarizes your 
conclusions.

370	
Chapter 9  Fundamentals of Hypothesis Testing: One-Sample Tests
Digital Case
Apply your knowledge about hypothesis testing in this Digi-
tal Case, which continues the cereal-fill-packaging dispute 
first discussed in the Digital Case from Chapter 7.
In response to the negative statements made by the Con-
cerned Consumers About Cereal Cheaters (CCACC) in the 
Chapter 7 Digital Case, Oxford Cereals recently conducted 
an experiment concerning cereal packaging. The com-
pany claims that the results of the experiment refute the 
CCACC allegations that Oxford Cereals has been cheat-
ing consumers by packaging cereals at less than labeled  
weights.
Open OxfordCurrentNews.pdf, a portfolio of current 
news releases from Oxford Cereals. Review the relevant 
press releases and supporting documents. Then answer the 
following questions:
1.	Are the results of the experiment valid? Why or why not? 
If you were conducting the experiment, is there anything 
you would change?
2.	Do the results support the claim that Oxford Cereals is not 
cheating its customers?
3.	Is the claim of the Oxford Cereals CEO that many cereal 
boxes contain more than 368 grams surprising? Is it true?
4.	Could there ever be a circumstance in which the results of 
the Oxford Cereals experiment and the CCACC’s results 
are both correct? Explain.
Sure Value Convenience Stores
You work in the corporate office for a nationwide conve-
nience store franchise that operates nearly 10,000 stores. 
The per-store daily customer count (i.e., the mean number 
of customers in a store in one day) has been steady, at 900, 
for some time. To increase the customer count, the chain is 
considering cutting prices for coffee beverages. The small 
size will now be $0.59 instead of $0.99, and the medium 
size will be $0.69 instead of $1.19. Even with this reduction 
in price, the chain will have a 40% gross margin on coffee.
To test the new initiative, the chain has reduced coffee 
prices in a sample of 34 stores, where customer counts have 
been running almost exactly at the national average of 900. Af-
ter four weeks, the sample stores stabilize at a mean customer 
count of 974 and a standard deviation of 96. This increase 
seems like a substantial amount to you, but it also seems like 
a pretty small sample. Is there statistical evidence that reducing 
coffee prices is a good strategy for increasing the mean cus-
tomer count? Be prepared to explain your conclusion.

	
Chapter 9 Excel Guide	
371
EG9.1  Fundamentals of Hypothesis-
Testing Methodology
Key Technique  Use the NORM.S.INV function to compute 
the lower and upper critical values and use NORM.S.DIST (abso-
lute value of the Z test statistic, True) as part of a formula to com-
pute the p-value. Use an IF function (see Appendix Section F.4) to 
determine whether to display a rejection or nonrejection message.
Example  Perform the Figure 9.5 two-tail Z test for the mean for 
the cereal-filling example shown on page 346.
PHStat  Use Z Test for the Mean, sigma known.
For the example, select PHStat ➔ One-Sample Tests ➔ Z Test 
for the Mean, sigma known. In the procedure’s dialog box 
(shown below):
	 1.	 Enter 368 as the Null Hypothesis.
	 2.	 Enter 0.05 as the Level of Significance.
	 3.	 Enter 15 as the Population Standard Deviation.
	 4.	 Click Sample Statistics Known and enter 25 as the Sample 
Size and 372.5 as the Sample Mean.
	 5.	 Click Two-Tail Test.
	 6.	 Enter a Title and click OK.
When using unsummarized data, click Sample Statistics 
­Unknown in step 4 and enter the cell range of the unsummarized 
data as the Sample Cell Range.
In-Depth Excel  Use the COMPUTE worksheet of the Z 
Mean workbook as a template.
The worksheet already contains the data for the example. For other 
problems, change the null hypothesis, level of significance, popu-
lation standard deviation, sample size, and sample mean values in 
cells B4 through B8 as necessary.
Read the Short Takes for Chapter 9 for an explanation of 
the formulas found in the COMPUTE worksheet. If you use an 
Excel version older than Excel 2010, use the COMPUTE_OLDER 
worksheet.
EG9.2  t Test of Hypothesis for the 
Mean (S Unknown)
Key Technique  Use the T.INV.2T(level of significance, de-
grees of freedom) function to compute the lower and upper critical 
values and use T.DIST.2T(absolute value of the t test statistic, de-
grees of freedom) to compute the p-value. Use an IF function (see 
Appendix Section F.4) to determine whether to display a rejection 
or nonrejection message.
Example  Perform the Figure 9.7 two-tail t test for the mean for 
the sales invoices example shown on page 352.
PHStat  Use t Test for the Mean, sigma unknown.
For the example, select PHStat ➔ One-Sample Tests  ➔ t Test 
for the Mean, sigma unknown. In the procedure’s dialog box 
(shown below):
	 1.	 Enter 120 as the Null Hypothesis.
	 2.	 Enter 0.05 as the Level of Significance.
	 3.	 Click Sample Statistics Known and enter 12 as the Sample 
Size, 112.85 as the Sample Mean, and 20.8 as the Sample 
Standard Deviation.
	 4.	 Click Two-Tail Test.
	 5.	 Enter a Title and click OK.
When using unsummarized data, click Sample Statistics 
­Unknown in step 3 and enter the cell range of the unsummarized 
data as the Sample Cell Range.
C h a p t e r  9  E x c e l  G u i d e

372	
Chapter 9  Fundamentals of Hypothesis Testing: One-Sample Tests
In-Depth Excel  Use the COMPUTE worksheet of the
T mean workbook, as a template.
The worksheet already contains the data for the example. For other 
problems, change the values in cells B4 through B8 as necessary.
Read the Short Takes for Chapter 9 for an explanation of 
the formulas found in the COMPUTE worksheet. If you use an 
Excel version older than Excel 2010, use the COMPUTE_OLDER 
worksheet.
EG9.3  One-Tail Tests
Key Technique  Use the functions discussed in Section EG9.1 
and EG9.2 to perform one-tail tests. For the t test of the mean, use 
T.DIST.RT(absolute value of the t test statistic, degrees of free-
dom) to help compute p-values. (See Appendix Section F.4.)
Example  Perform the Figure 9.11 lower-tail t test for the mean 
for the drive-through time study example shown on page 358.
PHStat  Click either Lower-Tail Test or Upper-Tail Test in the 
procedure dialog boxes discussed in Sections EG9.1 and EG9.2 to 
perform a one-tail test.
For the example, select PHStat ➔ One-Sample Tests ➔ t Test 
for the Mean, sigma unknown. In the procedure’s dialog box 
(shown below):
	 1.	 Enter 188.83 as the Null Hypothesis.
	 2.	 Enter 0.05 as the Level of Significance.
	 3.	 Click Sample Statistics Known and enter 25 as the Sample 
Size, 170.8 as the Sample Mean, and 21.3 as the Sample 
Standard Deviation.
	 4.	 Click Lower-Tail Test.
	 5.	 Enter a Title and click OK.
In-Depth Excel  Use the COMPUTE_LOWER worksheet or 
the COMPUTE_UPPER worksheet of the Z Mean workbook 
or the T mean workbook as templates.
For the example, open to the COMPUTE_LOWER worksheet 
of the T mean workbook.
Read the Short Takes for Chapter 9 for an explanation of 
the formulas found in the worksheets. If you use an Excel version 
older than Excel 2010, use the COMPUTE_OLDER worksheet.
EG9.4  Z Test of Hypothesis  
for the Proportion
Key Technique  Use the NORM.S.INV function to compute the 
lower and upper critical values and use NORM.S.DIST(absolute 
value of the Z test statistic, True) as part of a formula to compute 
the p-value. Use an IF function (see Appendix Section F.4) to de-
termine whether to display a rejection or nonrejection message.
Example  Perform the Figure 9.14 two-tail Z test for the propor-
tion of all adults who were unable to stop thinking about work 
while on vacation shown on page 362.
PHStat  Use Z Test for the Proportion.
For the example, select PHStat ➔ One-Sample Tests ➔ Z Test 
for the Proportion. In the procedure’s dialog box (shown below):
	 1.	 Enter 0.3 as the Null Hypothesis.
	 2.	 Enter 0.05 as the Level of Significance.
	 3.	 Enter 320 as the Number of Items of Interest.
	 4.	 Enter 1000 as the Sample Size.
	 5.	 Click Two-Tail Test.
	 6.	 Enter a Title and click OK.
In-Depth Excel  Use the COMPUTE worksheet of the Z Pro-
portion workbook as a template.
The worksheet already contains the data for the example. For other 
problems, change the null hypothesis, level of significance, popu-
lation standard deviation, sample size, and sample mean values in 
cells B4 through B7 as necessary.
Read the Short Takes for Chapter 9 for an explanation  
of the formulas found in the COMPUTE worksheet. Use the 
COMPUTE_LOWER or COMPUTE_UPPER worksheets as 
templates for performing one-tail tests. If you use an Excel version 
older than Excel 2010, use the COMPUTE_OLDER worksheet.

	
Chapter 9 Minitab Guide	
373
MG9.1  Fundamentals of Hypothesis-
Testing Methodology
Use 1-Sample Z to perform the Z test for the mean when s is known.
For example, to perform the two-tail Z test for the Figure 9.5 
­cereal-filling example on page 346, select Stat ➔ Basic Statistics 
➔ 1-Sample Z. In the “1-Sample Z (Test and Confidence Inter-
val)” dialog box (shown below):
	 1.	 Click Summarized data.
	 2.	 Enter 25 in the Sample size box and 372.5 in the Mean box.
	 3.	 Enter 15 in the Standard deviation box.
	 4.	 Check Perform hypothesis test and enter 368 in the Hypoth-
esized mean box.
	 5.	 Click Options.
In the 1-Sample Z - Options dialog box:
	 6.	 Enter 95.0 in the Confidence level box.
	 7.	 Select not equal from the Alternative drop-down list.
	 8.	 Click OK.
	 9.	 Back in the original dialog box, click OK.
When using unsummarized data, open the worksheet that 
contains the data and replace steps 1 and 2 with these steps:
	 1.	 Click Samples in columns.
	 2.	 Enter the name of the column containing the unsummarized 
data in the Samples in column box.
MG9.2  t Test of Hypothesis for the 
Mean (S Unknown)
Use 1-Sample t to perform the t test for the mean when s is  
unknown. 
For example, to perform the t test for the Figure 9.7 sales invoice 
­example on page 352, select Stat ➔ Basic Statistics ➔ 1-Sample t. 
In the 1-Sample t (Test and Confidence Interval) dialog box 
(shown in the next column):
	 1.	 Click Summarized data.
	 2.	 Enter 12 in the Sample size box, 112.85 in the Mean box, 
and 20.8 in the Standard deviation box.
	 3.	 Check Perform hypothesis test and enter 120 in the  
Hypothesized mean box.
	 4.	 Click Options.
In the 1-Sample t - Options dialog box:
	 5.	 Enter 95.0 in the Confidence level box.
	 6.	 Select not equal from the Alternative drop-down list.
	 7.	 Click OK.
	 8.	 Back in the original dialog box, click OK.
When using unsummarized data, open the worksheet that 
contains the data and replace steps 1 and 2 with these steps:
	 1.	 Click Samples in columns.
	 2.	 Enter the name of the column containing the unsummarized 
in the Samples in column box.
To create a boxplot of the unsummarized data, replace step 8 
with the following steps 8 through 10:
	 8.	 Back in the original dialog box, click Graphs.
	 9.	 In the 1-Sample t - Graphs dialog box, check Boxplot of 
data and then click OK.
	10.	 Back in the original dialog box, click OK.
MG9.3  One-Tail Tests
To perform a one-tail test for 1-Sample Z, select less than or 
greater than from the drop-down list in step 7 of the Section 
MG9.1 instructions.
To perform a one-tail test for 1-Sample t, select less than 
or greater than from the drop-down list in step 6 of the Section 
MG9.2 instructions.
C h a p t e r  9  M i n i ta b  G u i d e

374	
Chapter 9  Fundamentals of Hypothesis Testing: One-Sample Tests
MG9.4  Z Test of Hypothesis  
for the Proportion
Use 1 Proportion.
For example, to perform the Figure 9.14 Z test for the propor-
tion of all adults who were unable to stop thinking about work 
while on vacation on page 362, select Stat ➔ Basic Statistics ➔ 
1 Proportion. In the 1 Proportion (Test and Confidence Interval) 
dialog box (shown below):
	 1.	 Click Summarized data.
	 2.	 Enter 320 in the Number of events box and 1000 in the Num-
ber of trials box.
	 3.	 Check Perform hypothesis test and enter 0.3 in the Hypoth-
esized proportion box.
	 4.	 Click Options.
In the 1-Proportion - Options dialog box (shown below):
	 5.	 Enter 95.0 in the Confidence level box.
	 6.	 Select not equal from the Alternative drop-down list.
	 7.	 Check Use test and interval based on normal distribution.
	 8.	 Click OK.
	 9.	 Back in the original dialog box, click OK.
When using unsummarized data, open the worksheet that contains 
the data and replace steps 1 and 2 with these steps:
	 1.	 Click Samples in columns.
	 2.	 Enter the name of the column containing the unsummarized in 
the Samples in column box.
To perform a one-tail test, select less than or greater than 
from the drop-down list in step 6.

375
U s i n g  S tat i s t i c s
For North Fork, Are There Different Means 
to the Ends?
To what extent does the location of products affect sales in a supermarket? As a 
North Fork Beverages sales manager, you are negotiating with the management 
of FoodPlace Supermarkets for the location of displays for the new HandMade 
Real Citrus Cola. FoodPlace Supermarkets has offered you two different end-
aisle display areas to feature your new cola: one near the produce department 
and the other at the front of the aisle that contains other beverage products. These 
ends of aisle, or end-caps, have different costs, and you would like to compare 
the effectiveness of the produce end-cap to the beverage end-cap.
To test the comparative effectiveness of the two end-caps, FoodPlace agrees to 
a pilot study. You will be able to select 20 stores from the supermarket chain that ex-
perience similar storewide sales volumes. You then randomly assign 10 of the  
20 stores to sample 1 and 10 other stores to sample 2. In the sample 1 stores, you 
will place the new cola in the beverage end-cap, while in the sample 2 stores you 
will place the new cola in the produce end-cap. At the end of one week, the sales of 
the new cola will be recorded. How can you determine whether the sales of the new 
cola using beverage end-caps are different from the sales of the new cola using pro-
duce end-caps? How can you decide if the variability in new cola sales from store 
to store is different for the two types of displays? How could you use the answers to 
these questions to improve sales of your new HandMade Real Citrus Cola?
contents
10.1	Comparing the Means 
of Two Independent 
Populations
Do People Really Do This?
10.2	Comparing the Means of 
Two Related Populations
10.3	Comparing the Proportions 
of Two Independent 
Populations
10.4	F Test for the Ratio of Two 
Variances
Using Statistics: For North 
Fork, Are There Different Means 
to the Ends? Revisited
Chapter 10 Excel Guide
Chapter 10 Minitab Guide
Objectives
To compare the means of two 
independent populations
To compare the means of two  
related populations
To compare the proportions of 
two independent populations
To compare the variances of two 
independent populations
Chapter
Two-Sample Tests
10
Fotolia

376	
Chapter 10  Two-Sample Tests
I
n Chapter 9, you learned several hypothesis-testing procedures commonly used to test a 
single sample of data selected from a single population. In this chapter, you learn how 
to extend hypothesis testing to two-sample tests that compare statistics from samples 
selected from two populations. In the North Fork Beverages scenario one such test would be 
“Are the mean weekly sales of the new cola when using the beverage end-cap location (one 
population) different from the mean weekly sales of the new cola when using the produce end-
cap location (a second population)?”
10.1  Comparing the Means of Two  
Independent Populations
In Sections 8.1 and 9.1, you learned that in almost all cases, you would not know the standard 
deviation of the population under study. Likewise, when you take a random sample from each 
of two independent populations, you almost always do not know the standard deviation of ei-
ther population. In addition, when using a two-sample test that compares the means of samples 
selected from two populations, you must establish whether the assumption that the variances in 
the two populations are equal holds. The statistical method used to test whether the means of 
each population are different depends on whether the assumption holds or not.
Pooled-Variance t Test for the Difference  
Between Two Means
If you assume that the random samples are independently selected from two populations and 
that the populations are normally distributed and have equal variances, you can use a pooled- 
variance t test to determine whether there is a significant difference between the means. If the 
populations do not differ greatly from a normal distribution, you can still use the pooled-variance 
t test, especially if the sample sizes are large enough (typically Ú30 for each sample).
Using subscripts to distinguish between the population mean of the first population, m1, 
and the population mean of the second population, m2, the null hypothesis of no difference in 
the means of two independent populations can be stated as
H0: m1 = m2 or m1 - m2 = 0
and the alternative hypothesis, that the means are different, can be stated as
H1: m1 ≠ m2 or m1 - m2 ≠ 0
To test the null hypothesis, you use the pooled-variance t test statistic tSTAT shown in Equation 
(10.1). The pooled-variance t test gets its name from the fact that the test statistic pools, or 
­combines, the two sample variances S2
1 and S2
2 to compute S2
p, the best estimate of the variance 
common to both populations, under the assumption that the two population variances are equal.1
1When the two sample sizes are 
equal (i.e., n1 = n2), the equation 
for the pooled variance can be sim-
plified to
S2
p =
S2
1 + S2
2
2
Student Tip
Whichever population is 
defined as population 1 
in the null and alterna-
tive hypotheses must be 
defined as population 
1 in Equation (10.1). 
Whichever population is 
defined as population 2 
in the null and alterna-
tive hypotheses must be 
defined as population 2 
in Equation (10.1).
Pooled-Variance t Test for the Difference Between Two Means
	
tSTAT = 1X1 -  X22 - 1m1 -  m22
BS2
p a 1
n1
+ 1
n2
b
	
(10.1)
	
where	
S2
p = 1n1 - 12S2
1 + 1n2 - 12S2
2
1n1 - 12 + 1n2 - 12
and	
S2
p = pooled variance
X1 = mean of the sample taken from population 1

	
10.1  Comparing the Means of Two Independent Populations 	
377
For a given level of significance, a, in a two-tail test, you reject the null hypothesis if the 
computed tSTAT test statistic is greater than the upper-tail critical value from the t distribution 
or if the computed tSTAT test statistic is less than the lower-tail critical value from the t distribu-
tion. Figure 10.1 displays the regions of rejection.
 S2
1 = variance of the sample taken from population 1
 n1 = size of the sample taken from population 1
 X2 = mean of the sample taken from population 2
 S2
2 = variance of the sample taken from population 2
 n2 = size of the sample taken from population 2
The tSTAT test statistic follows a t distribution with n1 + n2 - 2 degrees of freedom.
Student Tip
When lower or less than 
is used in an example, 
you have a lower-tail 
test. When upper or 
more than is used in an 
example, you have an 
upper-tail test. When dif-
ferent or the same as is 
used in an example, you 
have a two-tail test.
In a one-tail test in which the rejection region is in the lower tail, you reject the null 
­hypothesis if the computed tSTAT test statistic is less than the lower-tail critical value from 
the t distribution. In a one-tail test in which the rejection region is in the upper tail, you reject  
the null hypothesis if the computed tSTAT test statistic is greater than the upper-tail critical 
value from the t distribution.
To demonstrate the pooled-variance t test, return to the North Fork Beverages scenario on 
page 375. Using the DCOVA problem-solving approach, you define the business objective as 
determining whether there is a difference in the mean weekly sales of the new cola when us-
ing the beverage end-cap location and when using the produce end-cap location. There are two 
populations of interest. The first population is the set of all possible weekly sales of the new cola 
if all the FoodPlace Supermarkets used the beverage end-cap location. The second population is 
the set of all possible weekly sales of the new cola if all the FoodPlace Supermarkets used the 
produce end-cap location. You collect the data from a sample of 10 FoodPlace Supermarkets that 
have been assigned a beverage end-cap location and another sample of 10 FoodPlace Supermar-
kets that have been assigned a produce end-cap location. You organize and store the results in 
 Cola . Table 10.1 contains the new cola sales (in number of cases) for the two samples.
T a b l e  1 0 . 1
Comparing New Cola 
Weekly Sales from 
Two Different End-Cap 
Locations (in number 
of cases)
Display Location
Beverage End-Cap
Produce End-Cap
22
34
52
62
30
52
71
76
54
67
40
64
84
56
59
83
66
90
77
84
The null and alternative hypotheses are
H0: m1 = m2 or m1 - m2 = 0
H1: m1 ≠ m2 or m1 - m2 ≠0
Assuming that the samples are from normal populations having equal variances, 
you can use the pooled-variance t test. The tSTAT test statistic follows a t distribution with 
F i g u r e  1 0 . 1
Regions of rejection 
and nonrejection for the 
pooled-variance t test for 
the difference between 
the means (two-tail test)
0
–ta/2
+ta/2
t
Region of
Rejection
Critical
Value
Region of
Nonrejection
Critical
Value
Region of
Rejection
a/2
a/2
1 – a

378	
Chapter 10  Two-Sample Tests
10 + 10 - 2 = 18 degrees of freedom. Using an a = 0.05 level of significance, you divide 
the rejection region into the two tails for this two-tail test (i.e., two equal parts of 0.025 each). 
Table E.3 shows that the critical values for this two-tail test are +2.1009 and -2.1009. As 
shown in Figure 10.2, the decision rule is
Reject H0 if tSTAT 7 +2.1009
or if tSTAT 6 -2.1009;
otherwise, do not reject H0.
From Figure 10.3, the computed tSTAT test statistic for this test is -3.0446 and the p-value 
is 0.0070.
Using Equation (10.1) on page 376 and the descriptive statistics provided in Figure 10.3,
tSTAT = 1X1 - X22 - 1m1 - m22
BS2
p a 1
n1
+ 1
n2
b
where
 S2
p = 1n1 - 12S2
1 + 1n2 - 12S2
2
1n1 - 12 + 1n2 - 12
 = 9118.726422 + 9112.543322
9 + 9
= 254.0056
F i g u r e  1 0 . 2
Two-tail test of 
hypothesis for the 
difference between the 
means at the 0.05 level 
of significance with 18 
degrees of freedom
–2.1009
+2.1009
0
t
.025
.025
Region of
Rejection
Critical
Value
Region of
Nonrejection
Critical
Value
Region of
Rejection
.95
F i g u r e  1 0 . 3
Excel and Minitab 
pooled-variance  
t test results for  
the two end-cap 
locations data

	
10.1  Comparing the Means of Two Independent Populations 	
379
Therefore,
tSTAT =
150.3 - 72.02 -  0.0
B254.0056 a 1
10 + 1
10 b
=
-21.7
250.801
= -3.0446
You reject the null hypothesis because tSTAT = -3.0446 6 -2.1009 and the p-value is 
0.0070. In other words, the probability that tSTAT 7 3.0446 or tSTAT 6 -3.0446 is equal to 
0.0070. This p-value indicates that if the population means are equal, the probability of ob-
serving a difference this large or larger in the two sample means is only 0.0070. Because the 
p-value is less than a = 0.05, there is sufficient evidence to reject the null hypothesis. You can 
conclude that the mean sales are different for the beverage end-cap and produce end-cap loca-
tions. Because the tSTAT statistic is negative, you can conclude that the mean sales are lower for 
the beverage end-cap location (and, therefore, higher for the produce end-cap location).
In testing for the difference between the means, you assume that the populations are nor-
mally distributed, with equal variances. For situations in which the two populations have equal 
variances, the pooled-variance t test is robust (i.e., not sensitive) to moderate departures from 
the assumption of normality, provided that the sample sizes are large. In such situations, you 
can use the pooled-variance t test without serious effects on its power. However, if you cannot 
assume that both populations are normally distributed, you have two choices. You can use a 
nonparametric procedure, such as the Wilcoxon rank sum test (see Section 12.4), that does not 
depend on the assumption of normality for the two populations, or you can use a normalizing 
transformation (see reference 6) on each of the outcomes and then use the pooled-variance t test.
To check the assumption of normality in each of the two populations, you can construct 
a boxplot of the sales for the two display locations shown in Figure 10.4. For these two small 
samples, there appears to be only moderate departure from normality, so the assumption of 
normality needed for the t test is not seriously violated.
Example 10.1 provides another application of the pooled-variance t test.
F i g u r e  1 0 . 4
Excel and Minitab boxplots for beverage and produce end-cap sales
Example 10.1
Testing for the Dif-
ference in the Mean 
Delivery Times
You and some friends have decided to test the validity of an advertisement by a local pizza res-
taurant, which says it delivers to the dormitories faster than a local branch of a national chain. 
Both the local pizza restaurant and national chain are located across the street from your col-
lege campus. You define the variable of interest as the delivery time, in minutes, from the time 
the pizza is ordered to when it is delivered. You collect the data by ordering 10 pizzas from the 
local pizza restaurant and 10 pizzas from the national chain at different times. You organize 
and store the data in  PizzaTime . Table 10.2 shows the delivery times.
(continued)

380	
Chapter 10  Two-Sample Tests
At the 0.05 level of significance, is there evidence that the mean delivery time for the local 
pizza restaurant is less than the mean delivery time for the national pizza chain?
Solution  Because you want to know whether the mean is lower for the local pizza res-
taurant than for the national pizza chain, you have a one-tail test with the following null and 
alternative hypotheses:
H0: m1 Ú m2 1The mean delivery time for the local pizza restaurant is equal to or 
greater than the mean delivery time for the national pizza chain.2
H1: m1 6 m2 1The mean delivery time for the local pizza restaurant is less than the 
mean delivery time for the national pizza chain.2
Figure 10.5 displays the results for the pooled-variance t test for these data.
To illustrate the computations, using Equation (10.1) on page 376,
tSTAT = 1X1 - X22 - 1m1 - m22
B S2
pa 1
n1
+ 1
n2
b
where
 S2
p = 1n1 - 12S2
1 + 1n2 - 12S2
2
1n1 - 12 + 1n2 - 12
 = 913.095522 + 912.866222
9 + 9
= 8.8986
F i g u r e  1 0 . 5
Excel and Minitab pooled-variance t test results for the pizza delivery time data
T a b l e  1 0 . 2
Delivery Times (in 
minutes) for a Local 
Pizza Restaurant 
and a National Pizza 
Chain
Local
Chain
16.8
18.1
22.0
19.5
11.7
14.1
15.2
17.0
15.6
21.8
18.7
19.5
16.7
13.9
15.6
16.5
17.5
20.8
20.8
24.0

	
10.1  Comparing the Means of Two Independent Populations 	
381
Confidence Interval Estimate for the Difference  
Between Two Means
Instead of, or in addition to, testing for the difference between the means of two independent 
populations, you can use Equation (10.2) to develop a confidence interval estimate of the dif-
ference in the means.
Therefore,
tSTAT = 116.7 - 18.882 - 0.0
B8.8986a 1
10 + 1
10 b
=
-2.18
21.7797
= -1.6341
You do not reject the null hypothesis because tSTAT = -1.6341 7 -1.7341. The p-value 
(as computed in Figure 10.5) is 0.0598. This p-value indicates that the probability that 
tSTAT 6 -1.6341 is equal to 0.0598. In other words, if the population means are equal, the 
probability that the sample mean delivery time for the local pizza restaurant is at least 2.18 
minutes faster than the national chain is 0.0598. Because the p-value is greater than a = 0.05, 
there is insufficient evidence to reject the null hypothesis. Based on these results, there is insuf-
ficient evidence for the local pizza restaurant to make the advertising claim that it has a faster 
delivery time.
Confidence Interval Estimate for the Difference Between  
the Means of Two Independent Populations
1X1 - X22 { ta>2BS2
pa 1
n1
+ 1
n2
b
or
1X1 - X22 - ta>2BS2
pa 1
n1
+ 1
n2
b … m1 - m2 … 1X1 - X22 + ta>2BS2
pa 1
n1
+ 1
n2
b  (10.2)
where ta>2 is the critical value of the t distribution, with n1 + n2 - 2 degrees of freedom, 
for an area of a>2 in the upper tail.
For the sample statistics pertaining to the two end-cap locations reported in Figure 10.3 on 
page 378, using 95% confidence, and Equation (10.2),
X1 = 50.3, n1 = 10, X2 = 72.0, n2 = 10, S2
p = 254.0056, and with 10 + 10 - 2
= 18 degrees of freedom, t0.025 = 2.1009
 150.3 - 72.02 { 12.10092B254.0056 a 1
10 + 1
10 b
 -21.7 { 12.1009217.12752
 -21.7 { 14.97
 -36.67 … m1 - m2 … -6.73
Therefore, you are 95% confident that the difference in mean sales between the beverage and 
produce end-cap locations is between -36.67 cases of cola and -6.73 cases of cola. In other 
words, you can estimate, with 95% confidence, that the produce end-cap location sells, on av-
erage, 6.73 to 36.67 cases more than the beverage end-cap location. From a hypothesis-testing 
perspective, using a two-tail test at the 0.05 level of significance, because the interval does 
not include zero, you reject the null hypothesis of no difference between the means of the two 
populations.

382	
Chapter 10  Two-Sample Tests
t Test for the Difference Between Two Means,  
Assuming Unequal Variances
If you can assume that the two independent populations are normally distributed but cannot 
assume that they have equal variances, you cannot pool the two sample variances into the 
common estimate S2
p and therefore cannot use the pooled-variance t test. Instead, you use the 
separate-variance t test developed by Satterthwaite (see reference 5). Equation (10.3) defines 
the test statistic for the separate-variance t test.
Separate-Variance t Test for the Difference  
Between Two Means
	
tSTAT = 1X1 - X22 - 1m1 - m22
B
S2
1
n1
+ S2
2
n2
	
(10.3)
	
where
 X1 = mean of the sample taken from population 1
 S2
1 = variance of the sample taken from population 1
 n1 = size of the sample taken from population 1
 X2 = mean of the sample taken from population 2
 S2
2 = variance of the sample taken from population 2
 n2 = size of the sample taken from population 2
Computing Degrees of Freedom in the Separate-Variance t Test
	
V =
a S2
1
n1
+ S2
2
n2
b
2
a S2
1
n1
b
2
n1 - 1 +
a S2
2
n2
b
2
n2 - 1
	
(10.4)
The separate-variance t test statistic approximately follows a t distribution with degrees of 
freedom V equal to the integer portion of the following computation shown in Equation (10.4).
For a given level of significance a, you reject the null hypothesis if the computed t test 
statistic is greater than the upper-tail critical value ta>2 from the t distribution with V degrees of 
freedom or if the computed test statistic is less than the lower-tail critical value -ta>2 from the 
t distribution with V degrees of freedom. Thus, the decision rule is
Reject H0 if t 7 ta>2
or if t 6 -ta>2;
otherwise, do not reject H0.
Return to the North Fork Beverages scenario concerning the two end-cap display  
locations. Using Equation (10.4), the separate-variance t test statistic tSTAT is approximated  

	
10.1  Comparing the Means of Two Independent Populations 	
383
by a t distribution with V = 15 degrees of freedom, the integer portion of the following  
computation:
 V =
a S2
1
n1
+ S2
2
n2
b
2
a S2
1
n1
b
2
n1 - 1 +
a S2
2
n2
b
2
n2 - 1
 =
a 350.6778
10
+ 157.3333
10
b
2
a 350.6778
10
b
2
9
+
a 157.3333
10
b
2
9
= 15.72
Using a = 0.05, the upper and lower critical values for this two-tail test found in Table E.3 are 
+2.1315 and -2.1315. As depicted in Figure 10.6, the decision rule is
Reject H0 if tSTAT 7 +2.1315
or if tSTAT 6 -2.1315; 
otherwise, do not reject H0.
Using Equation (10.3) on page 382 and the descriptive statistics provided in Figure 10.3,
 tSTAT = 1X1 - X22 - 1m1 - m22
B
S2
1
n1
+ S2
2
n2
 =
50.3 - 72
B a 350.6778
10
+ 157.3333
10
b
=
-21.7
250.801
= -3.04
Using a 0.05 level of significance, you reject the null hypothesis because t = -3.04
6 -2.1315.
Figure 10.7 on page 384 displays the separate-variance t test results for the end-cap display 
location data. Observe that the test statistic tSTAT = -3.0446 and the p-value is 0.0082 6 0.05. 
Thus, the results for the separate-variance t test are nearly the same as those of the pooled-
variance t test. The assumption of equality of population variances had no appreciable effect 
on the results. Sometimes, however, the results from the pooled-variance and separate-variance 
t tests conflict because the assumption of equal variances is violated. Therefore, it is important 
that you evaluate the assumptions and use those results as a guide in selecting a test procedure. 
F i g u r e  1 0 . 6
Two-tail test of 
hypothesis for the 
difference between the 
means at the 0.05 level  
of significance with  
15 degrees of freedom
–2.1315
+2.1315
0
t
.025
.025
Region of
Rejection
Critical
Value
Region of
Nonrejection
Critical
Value
Region of
Rejection
.95

384	
Chapter 10  Two-Sample Tests
In Section 10.4, the F test for the ratio of two variances is used to determine whether there is 
evidence of a difference in the two population variances. The results of that test can help you 
decide which of the t tests—pooled-variance or separate-variance—is more appropriate.
F i g u r e  1 0 . 7
Excel and Minitab 
separate-variance t test 
results for the sales data 
for the two end-caps
Do People Really Do This?
You may have heard opinions that lead you to 
wonder if decision-makers really use confirma-
tory methods, such as hypothesis testing, in this 
emerging era of big data. The following real case 
study, contributed by a former student of a col-
league of the authors, reveals a role that confir-
matory methods still play in business as well as 
answering another question: “Do businesses really 
monitor their customer service calls for quality as-
surance purposes as they sometime claim?”
In her first full-time job at a financial ser-
vices company, a student was asked to improve a 
training program for new hires at a call center that 
handled customer questions about outstanding 
loans. For feedback and evaluation, she planned 
to randomly select phone calls received by each 
new employee and rate the employee on 10 as-
pects of the call, including whether the employee 
maintained a pleasant tone with the customer. 
When she presented her plan to her boss for ap-
proval, her boss wanted proof that her new train-
ing program would improve customer service. The 
boss, quoting a famous statistician, said “In God 
we trust; all others must bring data.” Faced with 
this request, she called her business statistics pro-
fessor. “Hello, Professor, you’ll never believe why I 
called. I work for a large company, and in the proj-
ect I am currently working on, I have to put some of 
the statistics you taught us to work! Can you help?” 
Together they formulated this test:
•  Randomly assign the 60 most recent hires 
to two training programs. Assign half to the 
preexisting training program and the other 
half to the new training program.
•  At the end of the first month, compare the 
mean score for the 30 employees in the new 
training program against the mean score for 
the 30 employees in the preexisting training 
program.
She listened as her professor explained, 
“What you are trying to show is that the mean 
score from the new training program is higher than 
the mean score from the current program. You can 
make the null hypothesis that the means are equal 
and see if you can reject it in favor of the alterna-
tive that the mean score from the new program is 
higher.”
“Or, as you used to say, ‘if the p-value is low, 
Ho must go!’—yes, I do remember!” she replied. 
Her professor chuckled and added, “If you can 
reject Ho you will have the evidence to present to 
your boss.” She thanked him for his help and got 
back to work, with the newfound confidence that 
she would be able to successfully apply the t  test 
that compares the means of two independent 
­populations.

	
10.1  Comparing the Means of Two Independent Populations 	
385
Problems for Section 10.1
Learning the Basics
10.1  If you have samples of n1 = 12 and n2 = 15, in performing 
the pooled-variance t test, how many degrees of freedom do you 
have?
10.2  Assume that you have a sample of n1 = 8, with the sample 
mean X1 = 42, and a sample standard deviation S1 = 4, and you 
have an independent sample of n2 = 15 from another population 
with a sample mean of X2 = 34 and a sample standard deviation 
S2 = 5.
a.	 What is the value of the pooled-variance tSTAT test statistic for 
testing H0: m1 = m2?
b.	 In finding the critical value, how many degrees of freedom are 
there?
c.	 Using the level of significance a = 0.01, what is the critical 
value for a one-tail test of the hypothesis H0: m1 … m2 against 
the alternative, H1: m1 7 m2?
d.	 What is your statistical decision?
10.3  What assumptions about the two populations are necessary 
in Problem 10.2?
10.4  Referring to Problem 10.2, construct a 95% confidence  
interval estimate of the population mean difference between m1 
and m2.
10.5  Referring to Problem 10.2, if n1 = 5 and n2 = 4, how 
many degrees of freedom do you have?
10.6  Referring to Problem 10.2, if n1 = 5 and n2 = 4, at the 
0.01 level of significance, is there evidence that m1 7 m2?
Applying the Concepts
10.7  When people make estimates, they are influenced by an-
chors to their estimates. A study was conducted in which students 
were asked to estimate the number of calories in a cheeseburger. 
One group was asked to do this after thinking about a calorie-laden 
cheesecake. A second group was asked to do this after thinking 
about an organic fruit salad. The mean number of calories esti-
mated in a cheeseburger was 780 for the group that thought about 
the cheesecake and 1,041 for the group that thought about the or-
ganic fruit salad. (Data extracted from “Drilling Down, Sizing Up 
a Cheeseburger’s Caloric Heft,” The New York Times, October 4, 
2010, p. B2.) Suppose that the study was based on a sample of 
20 people who thought about the cheesecake first and 20 people 
who thought about the organic fruit salad first, and the standard 
deviation of the number of calories in the cheeseburger was 128 
for the people who thought about the cheesecake first and 140 for 
the people who thought about the organic fruit salad first.
a.	 State the null and alternative hypotheses if you want to deter-
mine whether the mean estimated number of calories in the 
cheeseburger is lower for the people who thought about the 
cheesecake first than for the people who thought about the or-
ganic fruit salad first.
b.	 In the context of this study, what is the meaning of the Type I 
­error?
c.	 In the context of this study, what is the meaning of the Type II 
error?
d.	 At the 0.01 level of significance, is there evidence that the 
mean estimated number of calories in the cheeseburger is lower 
for the people who thought about the cheesecake first than for 
the people who thought about the organic fruit salad first?
10.8  Ever improving technologies have led to the development 
of better alloys. Similar research was carried out to explore better 
ways of manufacturing gasoline or diesel-powered engine blocks. 
Researchers developed a manufacturing methodology using com-
pacted graphite cast iron (CGI) and magnesium alloy AMC-SCI 
(Source: http://www.ewp.rpi.edu/hartford/~ernesto/F2010/
EP2/Materials4Students/Lenny/Nguyen2005.pdf). Assume that 
50 tests have been conducted on each of these two alloys in simi-
lar conditions for recording their melting points in Fahrenheit. As-
sume that the average melting point for CGI has been recorded as 
236 with a variance of 25. The average melting point recorded for 
AMC-SCI was 224 with a variance of 15 in all the 50 tests con-
ducted. Answer the following questions, assuming that the popula-
tion variances are equal.
a.	 Construct the confidence intervals at 90% for the population 
mean for CGI.
b.	 Construct the confidence intervals at 95% for the mean differ-
ence between engine blocks using the two alloys.
c.	 At 0.05 level of significance, test the hypothesis that the aver-
age melting point using CGI is greater than the average melting 
point using AMC-SCI. In other words, test the hypothesis of 
the difference in the melting points of the two alloys.
d.	 Calculate p-value and interpret the results in (b) and (c).
10.9  A problem with a phone line that prevents a customer from 
receiving or making calls is upsetting to both the customer and the 
telecommunications company. The file  Phone  contains samples 
of 20 problems reported to two different offices of a telecommuni-
cations company and the time to clear these problems (in minutes) 
from the customers’ lines:
Central Office I Time to Clear Problems (minutes)
1.48   1.75   0.78   2.85   0.52   1.60   4.15   3.97   1.48   3.10
1.02   0.53   0.93   1.60   0.80   1.05   6.32   3.93   5.45   0.97
Central Office II Time to Clear Problems (minutes)
7.55   3.75   0.10   1.10   0.60   0.52   3.30   2.10   0.58   4.02
3.75   0.65   1.92   0.60   1.53   4.23   0.08   1.48   1.65   0.72
a.	 Assuming that the population variances from both offices are 
equal, is there evidence of a difference in the mean waiting 
time between the two offices? (Use a = 0.05.)
b.	 Find the p-value in (a) and interpret its meaning.
c.	 What other assumption is necessary in (a)?
d.	 Assuming that the population variances from both offices are 
equal, construct and interpret a 95% confidence interval esti-
mate of the difference between the population means in the two 
offices.
SELF 
Test 
10.10  Accounting Today identified the top accounting 
firms in 10 geographic regions across the United States. 
Even though all 10 regions reported growth in 2012, the Southeast 

386	
Chapter 10  Two-Sample Tests
and Gulf Coast regions reported the highest combined growths, 
with 7.94% and 9.92%, respectively. A characteristic description 
of the accounting firms in the Southeast and Gulf Coast regions 
included the number of partners in the firm. The file  
 AccountingPartners2  contains the number of partners. (Data ex-
tracted from bit.ly/11asPHm.)
a.	 At the 0.05 level of significance, is there evidence of a differ-
ence between Southeast region accounting firms and Gulf Coast 
accounting firms with respect to the mean number of partners?
b.	 Determine the p-value and interpret its meaning.
c.	 What assumptions do you have to make about the two popula-
tions in order to justify the use of the t test?
10.11  An important feature of digital cameras is battery life—the 
number of shots that can be taken before the battery needs to be 
recharged. The file  Cameras  contains the battery life of 11 sub-
compact cameras and 7 compact cameras. (Data extracted from 
“Cameras,” Consumer Reports, July 2012, pp. 42–44.)
a.	 Assuming that the population variances from both types of 
digital cameras are equal, is there evidence of a difference in 
the mean battery life between the two types of digital cameras 
1a = 0.052?
b.	 Determine the p-value in (a) and interpret its meaning.
c.	 Assuming that the population variances from both types of 
digital cameras are equal, construct and interpret a 95% con-
fidence interval estimate of the difference between the popula-
tion mean battery life of the two types of digital cameras.
10.12  A bank with a branch located in a commercial district of a 
city has the business objective of developing an improved process 
for serving customers during the noon-to-1 p.m. lunch period. 
Management decides to first study the waiting time in the current 
process. The waiting time is defined as the number of minutes 
that elapses from when the customer enters the line until he or 
she reaches the teller window. Data are collected from a random 
sample of 15 customers and stored in  Bank1 . These data are:
4.21   5.55   3.02   5.13   4.77   2.34   3.54   3.20
4.50   6.10   0.38   5.12   6.46   6.19   3.79
Suppose that another branch, located in a residential area, is also 
concerned with improving the process of serving customers in the 
noon-to-1 p.m. lunch period. Data are collected from a random 
sample of 15 customers and stored in  Bank2 . These data are:
  9.66   5.90   8.02   5.79   8.73   3.82   8.01   8.35
10.49   6.68   5.64   4.08   6.17   9.91   5.47
a.	 Assuming that the population variances from both banks are 
equal, is there evidence of a difference in the mean waiting 
time between the two branches? (Use a = 0.05.)
b.	 Determine the p-value in (a) and interpret its meaning.
c.	 In addition to equal variances, what other assumption is neces-
sary in (a)?
d.	 Construct and interpret a 95% confidence interval estimate 
of the difference between the population means in the two 
branches.
10.13  Repeat Problem 10.12 (a), assuming that the population 
variances in the two branches are not equal. Compare these results 
with those of Problem 10.12 (a).
10.14  As a member of the international strategic management 
team in your company, you are assigned the task of exploring  
potential foreign market entry. As part of your initial investiga-
tion, you want to know if there is a difference between developed 
markets and emerging markets with respect to the time required to 
start a business. You select 15 developed countries and 15 emerg-
ing countries. The time required to start a business, defined as 
the number of days needed to complete the procedures to legally 
­operate a business in these countries, is stored in  ForeignMarket .  
(Data extracted from data.worldbank.org.)
a.	 Assuming that the population variances for developed coun-
tries and emerging countries are equal, is there evidence 
of a difference in the mean time required to start a business 
between developed countries and emerging countries? (Use 
a = 0.05.)
b.	 Determine the p-value in (a) and interpret its meaning.
c.	 In addition to equal variances, what other assumption is neces-
sary in (a)?
d.	 Construct a 95% confidence interval estimate of the differ-
ence between the population means of developed countries and 
emerging countries.
10.15  Repeat Problem 10.14 (a), assuming that the population 
variances from developed and emerging countries are not equal. 
Compare these results with those of Problem 10.14 (a).
10.16  An article appearing in The Exponent, an independent 
college newspaper published by the Purdue Student Publishing 
Foundation, reported that the average American college student 
spends 1 hour (60 minutes) on Facebook daily. (Data extracted 
from bit.ly/NQRCJQ.) But you wonder if there is a difference 
between males and females. You select a sample of 60 Facebook 
users (30 males and 30 females) at your college. The time spent 
on Facebook per day (in minutes) for these 60 users is stored in  
 FacebookTime2 .
a.	 Assuming that the variances in the population of times spent on 
Facebook per day are equal, is there evidence of a difference in 
the mean time spent on Facebook per day between males and 
females? (Use a 0.05 level of significance.)
b.	 In addition to equal variances, what other assumption is neces-
sary in (a)?
10.17  Brand valuations are critical to CEOs, financial and 
marketing executives, security analysts, institutional investors, 
and others who depend on well-researched, reliable information 
needed for assessments, and comparisons in decision making. 
Millward Brown Optimor has developed the BrandZ Top 100 
Most Valuable Global Brands for WPP, the world’s largest com-
munications services group. Unlike other studies, the BrandZ Top 
100 Most Valuable Global Brands fuses consumer measures of 
brand equity with financial measures to place a financial value on 
brands. The file  BrandZTechFin  contains the brand values for two 
sectors in the BrandZ Top 100 Most Valuable Global Brands for 
2013: the technology sector and the financial institutions sector. 
(Data extracted from bit.ly/18OL5Mu.)
a.	 Assuming that the population variances are equal, is there evi-
dence of a difference between the technology sector and the 
financial institutions sector with respect to mean brand value? 
(Use a = .05.)
b.	 Repeat (a), assuming that the population variances are not 
equal.
c.	 Compare the results of (a) and (b).

	
10.2  Comparing the Means of Two Related Populations 	
387
10.2  Comparing the Means of Two  
Related Populations
The hypothesis-testing procedures presented in Section 10.1 enable you to examine differ-
ences between the means of two independent populations. In this section, you will learn about 
a procedure for examining the mean difference between two populations when you collect 
sample data from populations that are related—that is, when results of the first population are 
not independent of the results of the second population.
There are two situations that involve related data: when you take repeated measurements 
from the same set of items or individuals or when you match items or individuals according 
to some characteristic. In either situation, you are interested in the difference between the two 
related values rather than the individual values themselves.
When you take repeated measurements on the same items or individuals, you assume 
that the same items or individuals will behave alike if treated alike. Your objective is to show 
that any differences between two measurements of the same items or individuals are due to 
different treatments that have been applied to the items or individuals. For example, when 
performing a taste-testing experiment comparing two beverages, you can use each person in 
the sample as his or her own control so that you can have repeated measurements on the same 
individual.
Another example of repeated measurements involves the pricing of the same goods from 
two different vendors. For example, have you ever wondered whether new textbook prices at 
a local college bookstore are different from the prices offered at a major online retailer? You 
could take two independent samples—that is, select two different sets of textbooks—and then 
use the hypothesis tests discussed in Section 10.1.
However, by random chance, the first sample may have many large-format hardcover text-
books and the second sample may have many small trade paperback books. This would imply 
that the first set of textbooks will always be more expensive than the second set of textbooks, 
regardless of where they are purchased. This observation means that using the Section 10.1 
tests would not be a good choice. The better choice would be to use two related samples—that 
is, to determine the price of the same sample of textbooks at both the local bookstore and the 
online retailer.
The second situation that involves related data between populations is when you have 
matched samples. Here items or individuals are paired together according to some charac-
teristic of interest. For example, in test marketing a product in two different advertising cam-
paigns, a sample of test markets can be matched on the basis of the test market population size 
and/or demographic variables. By accounting for the differences in test market population size 
and/or demographic variables, you are better able to measure the effects of the two different 
advertising campaigns.
Regardless of whether you have matched samples or repeated measurements, the objective 
is to study the difference between two measurements by reducing the effect of the variability 
that is due to the items or individuals themselves. Table 10.3 shows the differences between the 
individual values for two related populations. To read this table, let X11, X12, c, X1n represent 
the n values from the first sample. And let X21, X22, c, X2n represent either the corresponding 
n matched values from a second sample or the corresponding n repeated measurements from 
the initial sample. Then D1, D2, c, Dn will represent the corresponding set of n difference 
scores such that
D1 = X11 - X21, D2 = X12 - X22, c, and Dn = X1n - X2n.
To test for the mean difference between two related populations, you treat the difference 
scores, each Di, as values from a single sample.

388	
Chapter 10  Two-Sample Tests
Paired t Test
If you assume that the difference scores are randomly and independently selected from a 
population that is normally distributed, you can use the paired t test for the mean differ-
ence in related populations to determine whether there is a significant population mean dif-
ference. As with the one-sample t test developed in Section 9.2 [see Equation (9.2) on page 
350], the paired t test statistic follows the t distribution with n - 1 degrees of freedom. 
Although the paired t test assumes that the population is normally distributed, since this test 
is robust, you can use this test as long as the sample size is not very small and the population 
is not highly skewed.
To test the null hypothesis that there is no difference in the means of two related  
populations:
H0: mD = 0 1where mD = m1 - m22
against the alternative that the means are not the same:
H1: mD ≠0
you compute the tSTAT test statistic using Equation (10.5).
Sample
Value
1
2
Difference
1
X11
X21
D1 = X11 - X21
2
X12
X22
D2 = X12 - X22
f
f
f
f
i
X1i
X2i
Di = X1i - X2i
f
f
f
f
n
X1n
X2n
Dn = X1n - X2n
T a b l e  1 0 . 3
Determining the 
Difference Between 
Two Related Samples
Student Tip
Which sample you define 
as group 1 will determine 
whether you will be doing 
a lower-tail test or an 
upper-tail test if you are 
conducting a one-tail test.
Paired t Test for the Mean Difference
	
tSTAT = D - mD
SD
2n
	
(10.5)
	
where
 mD = hypothesized mean difference
 D =
a
n
i = 1
Di
n
 SD = H
a
n
i = 1
1Di - D22
n - 1
The tSTAT test statistic follows a t distribution with n - 1 degrees of freedom.

	
10.2  Comparing the Means of Two Related Populations 	
389
For a two-tail test with a given level of significance, a, you reject the null hypothesis if the 
computed tSTAT test statistic is greater than the upper-tail critical value ta>2 from the t distribu-
tion, or, if the computed tSTAT test statistic is less than the lower-tail critical value -ta>2, from 
the t distribution. The decision rule is
Reject H0 if tSTAT 7 ta>2
or if tSTAT 6 -ta>2;
otherwise, do not reject H0.
You can use the paired t test for the mean difference to investigate a question raised earlier 
in this section: Are new textbook prices at a local college bookstore different from the prices 
offered at a major online retailer?
In this repeated-measurements experiment, you use one set of textbooks. For each textbook, 
you determine the price at the local bookstore and the price at the online retailer. By determin-
ing the two prices for the same textbooks, you can reduce the variability in the prices compared 
with what would occur if you used two independent sets of textbooks. This approach focuses on 
the differences between the prices of the same textbooks offered by the two retailers.
You collect data by conducting an experiment from a sample of n = 16 textbooks used 
primarily in business school courses during a recent semester at a local college. You determine 
the college bookstore price and the online price (which includes shipping costs, if any). You 
organize and store the data in  BookPrices . Table 10.4 shows the results. Notice that each row of 
the table shows the bookstore price and online retailer price for a specific book.
T a b l e  1 0 . 4
Prices of Textbooks at 
the College Bookstore 
and at the Online 
Retailer
Author
Title
Bookstore
Online
Bade
Foundations of Microeconomics 6/e
200.00
121.49
Brigham
Financial Management 13/e
304.00
235.88
Clauretie
Real Estate Finance: Theory and Practice
179.35
107.61
Foner
Give Me Liberty! (Brief) Vol. 2 3/e
72.00
59.99
Garrison
Managerial Accounting
277.15
146.99
Grewal
M: Marketing 3/e
73.75
63.49
Hill
Global Business Today
171.65
138.99
Lafore
Object-Oriented Programming in C+ +
65.00
42.26
Lank
Modern Real Estate Practice 11/e
47.45
65.99
Meyer
Entrepreneurship
106.00
37.83
Mitchell
Public Affairs in the Nation and New York
55.95
102.99
Pindyck
Microeconomics 8/e
224.40
144.99
Robbins
Organizational Behavior 15/e
223.20
179.39
Ross
Fundamentals of Corporate Finance 9/e
250.65
191.49
Schneier
New York Politics: Tale of Two States
34.95
28.66
Wilson
American Government: The Essentials 12/e
172.65
108.49
Your objective is to determine whether there is any difference between the mean textbook 
price at the college bookstore and at the online retailer. In other words, is there evidence that 
the mean price is different between the two textbook sellers? Thus, the null and alternative 
hypotheses are
H0: mD = 0 1There is no difference in the mean price between the college bookstore 
and the online retailer.2
H1: mD ≠0  1There is a difference in the mean price between the college bookstore 
and the online retailer.2

390	
Chapter 10  Two-Sample Tests
Choosing the level of significance a = 0.05 and assuming that the differences are nor-
mally distributed, you use the paired t test [Equation (10.5)]. For a sample of n = 16 text-
books, there are n - 1 = 15 degrees of freedom. Using Table E.3, the decision rule is
Reject H0 if tSTAT 7 2.1314
or if tSTAT 6 -2.1314;
otherwise, do not reject H0.
For the n = 16 differences (see Table 10.4), the sample mean difference is
D =
a
n
i = 1
Di
n
= 681.62
16
= 42.6013
and
SD = H
a
n
i = 1
1Di - D22
n - 1
= 43.797
From Equation (10.5) on page 388,
tSTAT = D - mD
SD
2n
= 42.6013 - 0
43.797
216
= 3.8908
Because tSTAT = 3.8908 7 2.1314, you reject the null hypothesis, H0 (see Figure 10.8). There 
is evidence of a difference in the mean price of textbooks purchased at the college bookstore 
and the online retailer. You can conclude that the mean price is higher at the college bookstore 
than at the online retailer.
F i g u r e  1 0 . 8
Two-tail paired t test 
at the 0.05 level of 
significance with  
15 degrees of freedom
–2.1314
+2.1314
0
t
.025
.025
Region of
Rejection
Critical
Value
Region of
Nonrejection
Critical
Value
Region of
Rejection
.95
Figure 10.9 presents the results for this example, computing both the t test statistic and the 
p-value. Because the p@value = 0.0014 6 a = 0.05, you reject H0. The p-value indicates that 
if the two sources for textbooks have the same population mean price, the probability that one 
source would have a sample mean $42.60 more than the other is 0.0014. Because this prob-
ability is less than a = 0.05, you conclude that there is evidence to reject the null hypothesis.
To evaluate the validity of the assumption of normality, you construct a boxplot of the dif-
ferences, as shown in Figure 10.10.
The Figure 10.10 boxplots show approximate symmetry and look similar to the boxplot 
for the normal distribution displayed in Figure 3.5 on page 153. Thus, the distribution of text-
book price differences does not greatly contradict the underlying assumption of normality. If a 
boxplot, histogram, or normal probability plot reveals that the assumption of underlying nor-
mality in the population is severely violated, then the t test may be inappropriate, especially if 

	
10.2  Comparing the Means of Two Related Populations 	
391
the sample size is small. If you believe that the t test is inappropriate, you can use either a non-
parametric procedure that does not make the assumption of underlying normality (see online 
Section 12.8 or references 1 and 2) or make a data transformation (see reference 6) and then 
recheck the assumptions to determine whether you should use the t test.
F i g u r e  1 0 . 9
Excel and Minitab paired t test results for the textbook price data
F i g u r e  1 0 . 1 0
Excel and Minitab boxplots for the textbook price differences
Example 10.2
Paired t Test of 
Pizza Delivery 
Times
Recall from Example 10.1 on page 379 that a local pizza restaurant situated across the street 
from your college campus advertises that it delivers to the dormitories faster than the local 
branch of a national pizza chain. In order to determine whether this advertisement is valid, you 
and some friends decided to order 10 pizzas from the local pizza restaurant and 10 pizzas from 
the national chain. In fact, each time you ordered a pizza from the local pizza restaurant, at the 
same time, your friends ordered a pizza from the national pizza chain. Thus, you have matched 
samples. For each of the 10 times that pizzas were ordered, you have one measurement from 
the local pizza restaurant and one from the national chain. At the 0.05 level of significance, is 
the mean delivery time for the local pizza restaurant less than the mean delivery time for the 
national pizza chain?
(continued)

392	
Chapter 10  Two-Sample Tests
Solution  Use the paired t test to analyze the Table 10.5 data (stored in  PizzaTime  ).  
Figure 10.11 shows the paired t test results for the pizza delivery data.
T a b l e  1 0 . 5
Delivery Times 
for Local Pizza 
Restaurant and 
National Pizza Chain
The null and alternative hypotheses are
H0: mD Ú 0 1Mean difference in the delivery time between the local pizza 
restaurant and the national pizza chain is greater than or equal to 0.2
H1: mD 6 0 1Mean difference in the delivery time between the local pizza 
restaurant and the national pizza chain is less than 0.2
Choosing the level of significance a = 0.05 and assuming that the differences are normally 
distributed, you use the paired t test [Equation (10.5) on page 388]. For a sample of n = 10 
­delivery times, there are n - 1 = 9 degrees of freedom. Using Table E.3, the decision rule is
Reject H0 if tSTAT 6 -t0.05 = -1.8331;
otherwise, do not reject H0.
To illustrate the computations, for n = 10 differences (see Table 10.5), the sample mean dif-
ference is
D =
a
n
i = 1
Di
n
= -21.8
10
= -2.18
F i g u r e  1 0 . 1 1
Excel and Minitab 
paired t test results for 
the pizza delivery data
Time
Local
Chain
Difference
1
16.8
22.0
-5.2
2
11.7
15.2
-3.5
3
15.6
18.7
-3.1
4
16.7
15.6
1.1
5
17.5
20.8
-3.3
6
18.1
19.5
-1.4
7
14.1
17.0
-2.9
8
21.8
19.5
2.3
9
13.9
16.5
-2.6
10
20.8
24.0
-3.2
-21.8

	
10.2  Comparing the Means of Two Related Populations 	
393
Confidence Interval Estimate for the Mean Difference
Instead of or in addition to testing for the mean difference between two related populations, 
you can use Equation (10.6) to construct a confidence interval estimate for the population 
mean difference.
and the sample standard deviation of the difference is
SD = H
a
n
i = 1
1Di - D22
n - 1
= 2.2641
From Equation (10.5) on page 388,
tSTAT = D - mD
SD
2n
= -2.18 - 0
2.2641
210
= -3.0448
Because tSTAT = -3.0448 is less than -1.8331, you reject the null hypothesis, H0 (the p-value 
is 0.0070 6 0.05). There is evidence that the mean delivery time is lower for the local pizza 
restaurant than for the national pizza chain.
This conclusion differs from the conclusion you reached on page 381 for Example 10.1 
when you used the pooled-variance t test for these data. By pairing the delivery times, you are 
able to focus on the differences between the two pizza delivery services and not the variability 
created by ordering pizzas at different times of day. The paired t test is a more powerful statis-
tical procedure that reduces the variability in the delivery time because you are controlling for 
the time of day the pizza was ordered.
Confidence Interval Estimate for the Mean Difference
D { ta>2
SD
2n
or
	
D - ta>2
SD
2n
 …  mD …  D + ta>2
SD
2n
	
(10.6)
where ta>2 is the critical value of the t distribution, with n - 1 degrees of freedom, for 
an area of a>2 in the upper tail.
Recall the example comparing textbook prices on page 389. Using Equation (10.6), 
D = 42.6013, SD = 43.797, n = 16, and ta>2 = 2.1314 (for 95% confidence and n - 1 = 15 
degrees of freedom),
42.6013 { 12.1314243.797
216
42.6013 { 23.3373
19.264 … mD … 65.9386
Thus, with 95% confidence, you estimate that the population mean difference in textbook 
prices between the college bookstore and the online retailer is between $19.26 and $65.94.  

394	
Chapter 10  Two-Sample Tests
Because the interval estimate does not contain zero, using the 0.05 level of significance and 
a two-tail test, you can conclude that there is evidence of a difference in the mean prices of 
textbooks at the college bookstore and the online retailer. Since both the lower and upper limits 
of the confidence interval are above 0, you can conclude that the mean price is higher at the 
college bookstore than the online retailer.
Problems for Section 10.2
Brand
Expert
A
B
C.C.
24
26
S.E.
27
27
E.G.
19
22
B.L.
24
27
C.M.
22
25
C.N.
26
27
G.N.
27
26
R.M.
25
27
 P.V.
22
23
Learning the Basics
10.18  An experimental design for a paired t test has 20 pairs of 
identical twins. How many degrees of freedom are there in this  
t test?
10.19  A professor wants to test the impact of a training session 
attended by 20 students. He assessed the performance before the 
session based on the given learning objectives as well as after the 
session based on the same learning objectives. Under what condi-
tions do you think the professor can use a paired t-test to test the 
difference in the performance before and after the test?
Applying the Concepts
10.20  Nine experts rated two brands of Colombian 
coffee in a taste-testing experiment. A rating on a 
7-point scale (1 = extremely unpleasing, 7 = extremely pleas-
ing) is given for each of four characteristics: taste, aroma, rich-
ness, and acidity. The ­following data stored in  Coffee  contain the 
ratings accumulated over all four characteristics:
10.21  How do the ratings of TV and phone services compare? 
The file  Telecom  contains the rating of 13 different providers. 
(Data extracted from “Ratings: TV, Phone, and Internet Services,” 
Consumer Reports, May 2012, p. 53.)
a.	 At the 0.05 level of significance, is there evidence of a difference 
in the mean service rating between TV and phone services?
b.	 What assumption is necessary about the population distribution 
in order to perform this test?
c.	 Use a graphical method to evaluate the validity of the assump-
tion in (a).
d.	 Construct and interpret a 95% confidence interval estimate 
of the difference in the mean service rating between TV and 
phone services.
10.22  Super Target versus Walmart: Who has the lowest prices? 
Given Walmart’s slogan “Save Money—Live Better,” you sus-
pect that Walmart does. The prices of 33 foods were compared 
(data extracted from “Supermarket Showdown,” The Palm Beach 
Post, February 13, 2011, pp. 1F, 2F) and the results are stored in  
 TargetWalmart .
a.	 At the 0.05 level of significance, is there evidence that the 
mean price of items is higher at Super Target than at Walmart?
b.	 What assumption is necessary about the population distribution 
in order to perform this test?
c.	 Find the p-value in (a) and interpret its meaning.
10.23  What motivates employees? The Great Place to Work 
Institute evaluated nonfinancial factors both globally and in the 
United States. (Data extracted from L. Petrecca, “Tech Companies 
Top List of ‘Great Workplaces,’” USA Today, October 31, 2011, 
p. 35B.) The results, which indicate the importance rating of each 
factor, are stored in  Motivation .
a.	 At the 0.05 level of significance, is there evidence of a differ-
ence in the mean rating between global and U.S. employees?
b.	 What assumption is necessary about the population distribution 
in order to perform this test?
c.	 Use a graphical method to evaluate the validity of the assump-
tion in (b).
10.24  Multiple myeloma, or blood plasma cancer, is character-
ized by increased blood vessel formulation (angiogenesis) in the 
bone marrow that is a predictive factor in survival. One treatment 
approach used for multiple myeloma is stem cell transplantation 
with the patient’s own stem cells. The data stored in  Myeloma , 
and shown on page 395 represent the bone marrow microvessel 
density for patients who had a complete response to the stem cell 
transplant (as measured by blood and urine tests). The measure-
ments were taken immediately prior to the stem cell transplant and 
at the time the complete response was determined.
a.	 At the 0.05 level of significance, is there evidence of a differ-
ence in the mean ratings between the two brands?
b.	 What assumption is necessary about the population distribution 
in order to perform this test?
c.	 Determine the p-value in (a) and interpret its meaning.
d.	 Construct and interpret a 95% confidence interval estimate of 
the difference in the mean ratings between the two brands.
SELF 
Test 

	
10.3  Comparing the Proportions of Two Independent Populations 	
395
a.	 At the 0.05 level of significance, is there evidence that the 
mean bone marrow microvessel density is higher before the 
stem cell transplant than after the stem cell transplant?
b.	 Interpret the meaning of the p-value in (a).
c.	 Construct and interpret a 95% confidence interval estimate of 
the mean difference in bone marrow microvessel density be-
fore and after the stem cell transplant.
d.	 What assumption is necessary about the population distribution 
in order to perform the test in (a)?
10.25  To assess the effectiveness of a cola video ad, a random 
sample of 38 individuals from a target audience was selected to par-
ticipate in a copy test. Participants viewed two ads, one of which 
was the ad being tested. Participants then answered a series of ques-
tions about how much they liked the ads. An adindex measure was 
created and stored in  Adindex ; the higher the adindex value, the 
more likeable the ad. Compute descriptive statistics and perform a 
paired t test. State your findings and conclusions in a report. (use the 
0.05 level of significance.)
10.26  The file  Concrete1  contains the compressive strength, 
in thousands of pounds per square inch (psi), of 40 samples of 
concrete taken two and seven days after pouring. (Data extracted 
from O. Carrillo-Gamboa and R. F. Gunst, “Measurement- 
Error-Model Collinearities,” Technometrics, 34 (1992):  
454–464.)
a.	 At the 0.01 level of significance, is there evidence that the 
mean strength is lower at two days than at seven days?
b.	 What assumption is necessary about the population distribu-
tion in order to perform this test?
c.	 Find the p-value in (a) and interpret its meaning.
Patient
Before
After
1
158
284
2
189
214
3
202
101
4
353
227
5
416
290
6
426
176
7
441
290
Data extracted from S. V. Rajkumar, R. Fonseca, T. E. 
Witzig, M. A. Gertz, and P. R. Greipp, “Bone Marrow 
Angiogenesis in Patients Achieving Complete Response 
After Stem Cell Transplantation for Multiple Myeloma,” 
Leukemia 13 (1999): 469–472.
10.3  Comparing the Proportions of Two  
Independent Populations
Often, you need to make comparisons and analyze differences between two population propor-
tions. You can perform a test for the difference between two proportions selected from inde-
pendent populations by using two different methods. This section presents a procedure whose 
test statistic, ZSTAT, is approximated by a standardized normal distribution. In Section 12.1, a 
procedure whose test statistic, x2
STAT, is approximated by a chi-square distribution is used. As 
explained in the latter section, the results from these two tests are equivalent.
Z Test for the Difference Between Two Proportions
In evaluating differences between two population proportions, you can use a Z test for the dif-
ference between two proportions. The ZSTAT test statistic is based on the difference between 
two sample proportions 1p1 - p22. This test statistic, given in Equation (10.7), approximately 
follows a standardized normal distribution for large enough sample sizes.
Z Test for the Difference Between Two Proportions
	
ZSTAT = 1p1 - p22 - 1p1 - p22
Bp11 - p2a 1
n1
+ 1
n2
b
	
(10.7)
with
p = X1 + X2
n1 + n2
  p1 = X1
n1 p2 = X2
n2
(continued)

396	
Chapter 10  Two-Sample Tests
The null hypothesis in the Z test for the difference between two proportions states that the 
two population proportions are equal 1p1 = p22. Because the pooled estimate for the popula-
tion proportion is based on the null hypothesis, you combine, or pool, the two sample propor-
tions to compute p, an overall estimate of the common population proportion. This estimate 
is equal to the number of items of interest in the two samples 1X1 + X22 divided by the total 
sample size from the two samples 1n1 + n22.
As shown in the following table, you can use this Z test for the difference between popula-
tion proportions to determine whether there is a difference in the proportion of items of inter-
est in the two populations (two-tail test) or whether one population has a higher proportion of 
items of interest than the other population (one-tail test):
	
where
 p1 = proportion of items of interest in sample 1
 X1 = number of items of interest in sample 1
 n1 = sample size of sample 1
 p1 = proportion of items of interest in population 1
 p2 = proportion of items of interest in sample 2
 X2 = number of items of interest in sample 2
 n2 = sample size of sample 2
 p2 = proportion of items of interest in population 2
 p = pooled estimate of the population proportion of items of interest
The ZSTAT test statistic approximately follows a standardized normal distribution.
Two-Tail Test
One-Tail Test
One-Tail Test
H0: p1 = p2
H0: p1 Ú p2
H0: p1 … p2
H1: p1 ≠p2
H1: p1 6 p2
H1: p1 7 p2
where
p1 = proportion of items of interest in population 1
p2 = proportion of items of interest in population 2
To test the null hypothesis that there is no difference between the proportions of two indepen-
dent populations:
H0: p1 = p2
against the alternative that the two population proportions are not the same:
H1: p1 ≠p2
you use the ZSTAT test statistic, given by Equation (10.7). For a given level of significance, a, 
you reject the null hypothesis if the computed ZSTAT test statistic is greater than the upper-tail 
critical value from the standardized normal distribution or if the computed ZSTAT test statistic is 
less than the lower-tail critical value from the standardized normal distribution.
To illustrate the use of the Z test for the equality of two proportions, suppose that you are 
the manager of T.C. Resort Properties, a collection of five upscale resort hotels located on two 
tropical islands. On one of the islands, T.C. Resort Properties has two hotels, the Beachcomber 
and the Windsurfer. Using the DCOVA problem solving approach, you have defined the business 
objective as improving the return rate of guests at the Beachcomber and the Windsurfer hotels. 
On the survey completed by hotel guests upon or after their departure, one question asked is 
whether the guest is likely to return to the hotel. Responses to this and other questions were col-
lected from 227 guests at the Beachcomber and 262 guests at the Windsurfer. The results for this 
Student Tip
Do not confuse this use 
of the Greek letter pi,  
p, to represent the 
population proportion 
with the mathemati-
cal constant that is the 
ratio of the circumfer-
ence to a diameter of a 
circle—approximately 
3.14159—which is 
also known by the same 
Greek letter.

	
10.3  Comparing the Proportions of Two Independent Populations 	
397
question indicated that 163 of 227 guests at the Beachcomber responded yes, they were likely 
to return to the hotel and 154 of 262 guests at the Windsurfer responded yes, they were likely to 
return to the hotel. At the 0.05 level of significance, is there evidence of a significant difference in 
guest satisfaction (as measured by the likelihood to return to the hotel) between the two hotels?
The null and alternative hypotheses are
H0: p1 = p2 or p1 - p2 = 0
H1: p1 ≠p2 or p1 - p2 ≠0
Using the 0.05 level of significance, the critical values are -1.96 and +1.96 (see Figure 10.12), 
and the decision rule is
Reject H0 if ZSTAT 6 -1.96
or if ZSTAT 7 +1.96;
otherwise, do not reject H0.
Using Equation (10.7) on page 395,
ZSTAT = 1p1 - p22 - 1p1 - p22
Bp11 - p2a 1
n1
+ 1
n2
b
–1.96
+1.96
0
Z
Region of
Rejection
Critical
Value
Region of
Nonrejection
Critical
Value
Region of
Rejection
.025
.025
.95
F i g u r e  1 0 . 1 2
Regions of rejection 
and nonrejection when 
testing a hypothesis for 
the difference between 
two proportions at the 
0.05 level of significance
where
p1 = X1
n1
= 163
227 = 0.7181 p2 = X2
n2
= 154
262 = 0.5878
and
p = X1 + X2
n1 + n2
= 163 + 154
227 + 262 = 317
489 = 0.6483
so that
 ZSTAT =
10.7181 - 0.58782 - 102
B0.648311 - 0.64832a 1
227 +
1
262b
 =
0.1303
210.228210.00822
 =
0.1303
20.00187
 = 0.1303
0.0432 = +3.0088

398	
Chapter 10  Two-Sample Tests
Using the 0.05 level of significance, you reject the null hypothesis because 
ZSTAT = +3.0088 7 +1.96. The p-value is 0.0026 (computed using Table E.2 or from Figure 
10.13) and indicates that if the null hypothesis is true, the probability that a ZSTAT test statistic 
is less than -3.0088 is 0.0013, and, similarly, the probability that a ZSTAT test statistic is greater 
than +3.0088 is 0.0013. Thus, for this two-tail test, the p-value is 0.0013 + 0.0013 = 0.0026. 
Because 0.0026 6 a = 0.05, you reject the null hypothesis. There is evidence to conclude 
that the two hotels are significantly different with respect to guest satisfaction; a greater pro-
portion of guests are willing to return to the Beachcomber than to the Windsurfer.
F i g u r e  1 0 . 1 3
Excel and Minitab Z test results for the difference between two proportions for the hotel guest satisfaction 
problem
Example 10.3
Testing for the 
­Difference  
Between Two 
­Proportions
Are men less likely than women to shop for bargains? A survey reported that when going 
shopping, 24% of men (181 of 756 sampled) and 34% of women (275 of 809 sampled) go for 
bargains. (Data extracted from “Brands More Critical for Dads,” USA Today, July 21, 2011,  
p. 29C.) At the 0.05 level of significance, is the proportion of men who shop for bargains less 
than the proportion of women who shop for bargains?
Solution  Because you want to know whether there is evidence that the proportion of men 
who shop for bargains is less than the proportion of women who shop for bargains, you have a 
one-tail test. The null and alternative hypotheses are
H0: p1 Ú p2 (The proportion of men who shop for bargains is greater than or equal to the 
proportion of women who shop for bargains.)
H1: p1 6 p2 (The proportion of men who shop for bargains is less than the proportion of 
women who shop for bargains.)
Using the 0.05 level of significance, for the one-tail test in the lower tail, the critical value is 
+1.645. The decision rule is
Reject H0 if ZSTAT 6 -1.645;
otherwise, do not reject H0.

	
10.3  Comparing the Proportions of Two Independent Populations 	
399
Using Equation (10.7) on page 395,
ZSTAT = 1p1 - p22 - 1p1 - p22
Bp11 - p2a 1
n1
+ 1
n2
b
where
p1 = X1
n1
= 181
756 = 0.2394 p2 = X2
n2
= 275
809 = 0.3399
and
p = X1 + X2
n1 + n2
= 181 + 275
756 + 809 = 456
1565 = 0.2914
so that
 ZSTAT =
10.2394 - 0.33992 - 102
B0.291411 - 0.29142a 1
756 +
1
809b
 =
-0.1005
210.2065210.002562
 =
-0.1005
20.00053
 = -0.1005
0.0230
= -4.37
Using the 0.05 level of significance, you reject the null hypothesis because ZSTAT = -4.37
6 -1.645. The p-value is 0.0000. Therefore, if the null hypothesis is true, the probability that 
a ZSTAT test statistic is less than -4.37 is 0.0000 (which is less than a = 0.05). You conclude 
that there is evidence that the proportion of men who shop for bargains is less than the propor-
tion of women who shop for bargains.
Confidence Interval Estimate for the Difference  
Between Two Proportions
Instead of or in addition to testing for the difference between the proportions of two indepen-
dent populations, you can construct a confidence interval estimate for the difference between 
the two proportions using Equation (10.8).
Confidence Interval Estimate for the Difference  
Between Two Proportions
1p1 - p22 { Za>2B
p111 - p12
n1
+ p211 - p22
n2
or
1p1 - p22 - Za>2B
p111 - p12
n1
+ p211 - p22
n2
…  1p1 - p22
… 1p1 - p22 + Za>2B
p111 - p12
n1
+ p211 - p22
n2
	
(10.8)

400	
Chapter 10  Two-Sample Tests
To construct a 95% confidence interval estimate for the population difference between the 
proportion of guests who would return to the Beachcomber and who would return to the Wind-
surfer, you use the results on page 397 or from Figure 10.13 on page 398:
p1 = X1
n1
= 163
227 = 0.7181 p2 = X2
n2
= 154
262 = 0.5878
Using Equation (10.8),
10.7181 - 0.58782 { 11.962A
0.718111 - 0.71812
227
+ 0.587811 - 0.58782
262
0.1303 { 11.96)10.04262
0.1303 { 0.0835
0.0468 … 1p1 - p22 … 0.2138
Thus, you have 95% confidence that the difference between the population proportion of guests 
who would return to the Beachcomber and the Windsurfer is between 0.0468 and 0.2138. In 
percentages, the difference is between 4.68% and 21.38%. Guest satisfaction is higher at the 
Beachcomber than at the Windsurfer.
Problems for Section 10.3
Learning the Basics
10.27  Interpret the results about the hypothesis testing of two 
proportions:
a.	 Comment if the null hypothesis should be rejected at 0.05 level 
of significance when the p-value was calculated as 0.047.
b.	 Comment if the null hypothesis should be rejected at 0.01 level 
of significance when the p-value was calculated as 0.023.
10.28  Let n1 = 100, X1 = 45, n2 = 50, and X2 = 25.
a.	 At the 0.01 level of significance, is there evidence of a signifi-
cant difference between the two population proportions?
b.	 Construct a 99% confidence interval estimate for the difference 
between the two population proportions.
Applying the Concepts
10.29  A newspaper article reported that half of the residents of the 
US read news using digital media. The article was substantiated us-
ing a graph depicting the various forms of digital media used to read 
news during 1991–2012. Traditional media such as newspapers dis-
played a negative trend in terms of readability, whereas any media of 
digital news such as online or on mobile displayed positive trends. 
(Source: http://www.poynter.org/latest-news/mediawire/189819/
pew-tv-viewing-habit-grays-as-digital-news-consumption-tops-
print-radio/). Assume that in a survey of 200 people in the age  
group of 18–25 and 200 people in the age group of 26–40 were 
asked for their preference of the medium for reading news. Eighty-
two people in the age group of 18–25 read newspapers, and 104 
people in the age group of 26–40 read newspapers.
a.	 At 0.05 level of significance, can you find any evidence that the 
proportion of people who read newspapers is different in the 
two age groups of 18–25 and 26–40?
b.	  Find the p-value in (a) and interpret its meaning.
a.	 At 0.01 level of significance, construct the confidence interval 
to determine the difference between the proportion of men and 
women who have a high level of commitment towards their  
organization.
b.	  Interpret the results founded in (a).
10.31  A/B testing is a testing method that businesses use to test 
different designs and formats of a web page to determine whether 
a new web page is more effective than a current web page. Web 
designers at TravelTips.com tested a new call to action button on 
its web page. Every visitor to the web page was randomly shown 
Organizational 
Commitment
Respondent Gender
Male
Female
No
31
43
Yes
49
37
10.30  Research revealed that employees of organizations that of-
fer more flexibility and provisions for managing better work life 
balance are more satisfied from their jobs and thus display higher 
commitment to their work. The following data was gathered about 
commitment levels at a survey of 80 men and women (data ex-
tracted from: Scandura, Terri A., and M. Lankau. “Relationships 
of gender, family responsibility and flexible work hours to organi-
zational commitment and job satisfaction.” Journal of Organiza-
tional Behavior, 18.4 (1997): 377-391).

	
10.4  F Test for the Ratio of Two Variances	
401
either the original call to action button (the control) or the new 
call to action button. The metric used to measure success was the 
download rate: the number of people who downloaded the file di-
vided by the number of people who saw that particular call to ac-
tion button. The experiment yielded the following results:
Assume that in a survey of 550 respondents in the age group of  
25–50, 300 believe that mobile phones can be given at the age  
of 14, whereas in 500 respondents in the age group of 50 and 
above, 250 responded that mobile phones should be given at the 
age of 14. 
a.	 At 0.05 level of significance, can you find any evidence that 
the proportion of people who agree that mobile phones can be 
given at the age of 14, is different in the two age groups of  
18–25 and 26–40?
b.	 Find the p-value in (a) and interpret its value.
c.	 Interpret the results in (a) and (b).
10.34  Does gamification motivate customer research manage-
ment (CRM) utilization? Gamification is the use of game me-
chanics to motivate, modify, or reward distinct behaviors. In the 
context of sales effectiveness, it is deployed to encourage both 
sales accomplishments and non-sales activities. A survey of end-
user sales organizations indicates that 31 of 37 gamification-user 
organizations provide mobile access to CRM, whereas 138 of 
275 non-gamification-user organizations provide mobile access 
to CRM. (Data extracted from v1.aberdeen.com/launch/report/
research_briefs/8346-RB-gamification-sales-effectiveness.
asp?lan=US.)
a.	 At the 0.05 level of significance, is there evidence of a differ-
ence between gamification-user sales organizations and non-
gamification-user sales organizations in the proportion that 
provide mobile access to CRM?
b.	 Find the p-value in (a) and interpret its meaning.
10.35  One of the most impressive, innovative advances in online 
fundraising over the past decade is the rise of crowd-funding web-
sites. While features differ from site to site, crowd-funding sites 
are websites that allow you to set up an online fundraising cam-
paign based around a fundraising page, and accept money directly 
from that page using the website’s own credit card processor. 
Kickstarter, one crowd-funding website, reported that 316 of 831 
technology crowd-funding projects were successfully launched 
in the past year and 923 of 2,796 games crowd-funding projects 
were successfully launched in the past year. (Data extracted from  
kickstarter.com/hello?ref=nav.)
a.	 Is there evidence of a significant difference in the proportion of 
technology crowd-funding projects and games crowd-funding 
projects that were successful? (Use a = 0.05.)
b.	 Determine the p-value in (a) and interpret its meaning.
c.	 Construct and interpret a 95% confidence interval estimate for 
the difference between the proportion of technology crowd-
funding projects and games crowd-funding projects that are 
successful.
Variations
Downloads
Visitors
Original call to action button
351
3,642
New call to action button
485
3,556
a.	 What is the proportion (download rate) of visitors who saw the 
original call to action button and downloaded the file?
b.	 What is the proportion (download rate) of visitors who saw the 
new call to action button and downloaded the file?
c.	 At the 0.05 level of significance, is there evidence that the new 
call to action button is more effective than the original?
SELF 
Test 
10.32  The consumer research firm Scarborough ana-
lyzed the 10% of American adults that are either “Super-
banked” or “Unbanked.” Superbanked consumers are defined as U.S. 
adults who live in a household that has multiple asset accounts at 
­financial institutions, as well as some additional investments; 
­Unbanked consumers are U.S. adults who live in a household that 
does not use a bank or credit union. By finding the 5% of ­Americans 
that are Superbanked, Scarborough identifies financially savvy con-
sumers who might be open to diversifying their financial portfolios; 
by identifying the Unbanked, Scarborough provides ­insight into the 
ultimate prospective client for banks and financial institutions. As 
part of its analysis, Scarborough reported that 93% of Superbanked 
consumers use credit cards in the past three months as compared 
to 23% of Unbanked consumers. (Data extracted from bit.ly/
QlABwO.) Suppose that these results were based on 1,000 
­Superbanked consumers and 1,000 Unbanked consumers.
a.	 At the 0.01 level of significance, is there evidence of a signifi-
cant difference between the Superbanked and the Unbanked 
with respect to the proportion that use credit cards?
b.	 Find the p-value in (a) and interpret its meaning.
c.	 Construct and interpret a 99% confidence interval estimate for 
the difference between the Superbanked and the Unbanked 
with respect to the proportion that use credit cards.
10.33  More than half the population of American kids own mo-
bile phones. This led to a discussion about the age limit for ow-
ing phones. For instance, in the USA, elementary school students 
carry mobile phones and in New Zealand, 14 is the age limit. 
10.4  F Test for the Ratio of Two Variances
Often you need to determine whether two independent populations have the same variability. 
By testing variances, you can detect differences in the variability in two independent popula-
tions. One important reason to test for the difference between the variances of two popula-
tions is to determine whether to use the pooled-variance t test (which assumes equal variances) 
or the separate-variance t test (which does not assume equal variances) when comparing the 
means of two independent populations.

402	
Chapter 10  Two-Sample Tests
The test for the difference between the variances of two independent populations is based 
on the ratio of the two sample variances. If you assume that each population is normally dis-
tributed, then the sampling distribution of the ratio S2
1>S2
2 is distributed as an F distribution 
(see Table E.5). The critical values of the F distribution in Table E.5 depend on the degrees 
of freedom in the two samples. The degrees of freedom in the numerator of the ratio are for 
the first sample, and the degrees of freedom in the denominator are for the second sample. The 
first sample taken from the first population is defined as the sample that has the larger sample 
variance. The second sample taken from the second population is the sample with the smaller 
sample variance. Equation (10.9) defines the F test for the ratio of two variances.
F Test Statistic for Testing the Ratio of Two Variances
The FSTAT test statistic is equal to the variance of sample 1 (the larger sample variance) 
divided by the variance of sample 2 (the smaller sample variance).
	
FSTAT = S2
1
S2
2
	
(10.9)
where
S2
1 = variance of sample 1 (the larger sample variance)
S2
2 = variance of sample 2 (the smaller sample variance)
n1 = sample size selected from population 1
n2 = sample size selected from population 2
n1 - 1 = degrees of freedom from sample 1 (i.e., the numerator degrees of freedom)
n2 - 1 = degrees of freedom from sample 2 (i.e., the denominator degrees of freedom)
The FSTAT test statistic follows an F distribution with n1 - 1 and n2 - 1 degrees of 
freedom.
For a given level of significance, a, to test the null hypothesis of equality of population 
variances:
H0: s2
1 = s2
2
against the alternative hypothesis that the two population variances are not equal:
H1: s2
1 ≠s2
2
you reject the null hypothesis if the computed FSTAT test statistic is greater than the upper-tail 
critical value, Fa>2, from the F distribution, with n1 - 1 degrees of freedom in the numerator 
and n2 - 1 degrees of freedom in the denominator. Thus, the decision rule is
Reject H0 if FSTAT 7 Fa>2;
otherwise, do not reject H0.
To illustrate how to use the F test to determine whether the two variances are equal, return 
to the North Fork Beverages scenario on page 425 concerning the sales of the new cola in two 
different end-cap locations. To determine whether to use the pooled-variance t test or the sep-
arate-variance t test in Section 10.1, you can test the equality of the two population variances. 
The null and alternative hypotheses are
 H0: s2
1 = s2
2
H1: s2
1 ≠s2
2
Student Tip
Since the numerator of 
Equation (10.9) contains 
the larger variance, the 
FSTAT statistic is always 
greater than or equal  
to 1.0.

	
10.4  F Test for the Ratio of Two Variances	
403
Because you are defining sample 1 as the group with the larger sample variance, the rejec-
tion region in the upper tail of the F distribution contains a>2. Using the level of significance 
a = 0.05, the rejection region in the upper tail contains 0.025 of the distribution.
Because there are samples of 10 stores for each of the two end-cap locations, there are 
10 - 1 = 9 degrees of freedom in the numerator (the sample with the larger variance) and 
also in the denominator (the sample with the smaller variance). Fa>2, the upper-tail critical 
value of the F distribution, is found directly from Table E.5, a portion of which is presented 
in Table 10.6. Because there are 9 degrees of freedom in the numerator and 9 degrees of free-
dom in the denominator, you find the upper-tail critical value, Fa>2, by looking in the column 
labeled 9 and the row labeled 9. Thus, the upper-tail critical value of this F distribution is 4.03. 
Therefore, the decision rule is
Reject H0 if FSTAT 7 F0.025 = 4.03;
otherwise, do not reject H0.
T a b l e  1 0 . 6
Finding the Upper-
Tail Critical Value of F 
with 9 and 9 Degrees 
of Freedom for an 
Upper-Tail Area of 
0.025
Cumulative Probabilities =0.975  
Upper-Tail Area =0.025  
Numerator df1
Denominator df2
1
2
3
c
7
8
9
1
647.80
799.50
864.20
c
948.20
956.70
963.30
2
38.51
39.00
39.17
c
39.36
39.37
39.39
3
17.44
16.04
15.44
c
14.62
14.54
14.47
f
f
f
f
f
f
f
f
7
8.07
6.54
5.89
c
4.99
4.90
4.82
8
7.57
6.06
5.42
c
4.53
4.43
4.36
9
7.21
5.71
5.08
c
4.20
4.10
4.03
Source: Extracted from Table E.5.
Using Equation (10.9) on page 402 and the cola sales data (see Table 10.1 on page 377),
S2
1 = 118.726422 = 350.6778 S2
2 = 112.543322 = 157.3333
so that
 FSTAT = S2
1
S2
2
 = 350.6778
157.3333 = 2.2289
Because FSTAT = 2.2289 6 4.03, you do not reject H0. Figure 10.14 shows the results for 
this test, including the p-value, 0.2482. Because 0.2482 7 0.05, you conclude that there is no 
evidence of a significant difference in the variability of the sales of the new cola for the two 
end-cap locations.
In testing for a difference between two variances using the F test, you assume that 
each of the two populations is normally distributed. The F test is very sensitive to the nor-
mality assumption. If boxplots or normal probability plots suggest even a mild departure 
from normality for either of the two populations, you should not use the F test. If this hap-
pens, you should use the Levene test (see Section 11.1) or a nonparametric approach (see 
references 1 and 2).
In testing for the equality of variances as part of assessing the validity of the pooled-vari-
ance t test procedure, the F test is a two-tail test with a>2 in the upper tail. However, when you 
are interested in examining the variability in situations other than the pooled-variance t test, 
the F test is often a one-tail test. Example 10.4 illustrates a one-tail test.

404	
Chapter 10  Two-Sample Tests
F i g u r e  1 0 . 1 4
Excel and Minitab F test results for the two end-cap locations data
Example 10.4
A One-Tail Test  
for the Difference 
Between Two  
Variances
Waiting time is a critical issue at fast-food chains, which not only want to minimize the mean 
service time but also want to minimize the variation in the service time from customer to cus-
tomer. One fast-food chain carried out a study to measure the variability in the waiting time 
(defined as the time in minutes from when an order was completed to when it was delivered to 
the customer) at lunch and breakfast at one of the chain’s stores. The results were as follows:
 Lunch: n1 = 25 S2
1 = 4.4
 Breakfast: n2 = 21 S2
2 = 1.9
At the 0.05 level of significance, is there evidence that there is more variability in the ser-
vice time at lunch than at breakfast? Assume that the population service times are normally 
distributed.
Solution  The null and alternative hypotheses are
H0: s2
L … s2
B
H1: s2
L 7 s2
B
The FSTAT test statistic is given by Equation (10.9) on page 402:
FSTAT = S2
1
S2
2
You use Table E.5 to find the upper critical value of the F distribution. With n1 - 1 =
25 - 1 = 24 degrees of freedom in the numerator, n2 - 1 = 21 - 1 = 20 degrees of  
freedom in the denominator, and a = 0.05, the upper-tail critical value, F0.05, is 2.08. The decision 
rule is
Reject H0 if FSTAT 7 2.08;
otherwise, do not reject H0.

	
10.4  F Test for the Ratio of Two Variances	
405
From Equation (10.9) on page 402,
 FSTAT = S2
1
S2
2
 = 4.4
1.9 = 2.3158
Because FSTAT = 2.3158 7 2.08, you reject H0. Using a 0.05 level of significance, you con-
clude that there is evidence that there is more variability in the service time at lunch than at 
breakfast.
Problems for Section 10.4
Learning the Basics
10.36  Determine the upper-tail critical values of F in each of the 
following two-tail tests.
a.	 a = 0.10, n1 = 16, n2 = 21
b.	 a = 0.05, n1 = 16, n2 = 21
c.	 a = 0.01, n1 = 16, n2 = 21
10.37  Determine the upper-tail critical value of F in each of the 
following one-tail tests.
a.	 a = 0.05, n1 = 16, n2 = 21
b.	 a = 0.01, n1 = 16, n2 = 21
10.38  The following information is available for two samples 
­selected from independent normally distributed populations:
 Population A: n1 = 25 S2
1 = 16
 Population B: n2 = 25 S2
2 = 25
a.	 Which sample variance do you place in the numerator of FSTAT?
b.	 What is the value of FSTAT?
10.39  The following information is available for two samples 
­selected from independent normally distributed populations:
 Population A: n1 = 25 S2
1 = 161.9
 Population B: n2 = 25 S2
2 = 133.7
What is the value of FSTAT if you are testing the null hypothesis 
H0: s2
1 = s2
2?
10.40  In Problem 10.39, how many degrees of freedom are there 
in the numerator and denominator of the F test?
10.41  In Problems 10.39 and 10.40, what is the upper-tail critical 
value for F if the level of significance, a, is 0.05 and the alterna-
tive hypothesis is H1: s2
1 ≠s2
2?
10.42  In Problems 10.39 through 10.41, what is your statistical 
decision?
10.43  The following information is available for two samples 
­selected from independent but very right-skewed populations:
 Population A: n1 = 16 S2
1 = 47.3
 Population B: n2 = 13 S2
2 = 36.4
Should you use the F test to test the null hypothesis of equality of 
variances? Discuss.
10.44  In Problem 10.43, assume that two samples are selected 
from independent normally distributed populations.
a.	 At the 0.05 level of significance, is there evidence of a differ-
ence between s2
1 and s2
2?
b.	 Suppose that you want to perform a one-tail test. At the 0.05 
level of significance, what is the upper-tail critical value of F 
to determine whether there is evidence that s2
1 7 s2
2? What is 
your statistical decision?
Applying the Concepts
10.45  A problem with a telephone line that prevents a customer 
from receiving or making calls is upsetting to both the customer 
and the telecommunications company. The file  Phone  contains 
samples of 20 problems reported to two different offices of a tele-
communications company and the time to clear these problems (in 
minutes) from the customers’ lines.
a.	 At the 0.05 level of significance, is there evidence of a differ-
ence in the variability of the time to clear problems between the 
two central offices?
b.	 Determine the p-value in (a) and interpret its meaning.
c.	 What assumption do you need to make in (a) about the two 
populations in order to justify your use of the F test?
d.	 Based on the results of (a) and (b), which t test defined in Sec-
tion 10.1 should you use to compare the mean time to clear 
problems in the two central offices?
SELF 
Test 
10.46  Accounting Today identified the top accounting 
firms in 10 geographic regions across the United States. 
Even though all 10 regions reported growth in 2012, the Southeast 
and Gulf Coast regions reported the highest combined growths, 
with 7.94% and 9.92%, respectively. A characteristic description 
of the accounting firms in the Southeast and Gulf Coast regions 
included the number of partners in the firm. The file  Accounting-
Partners2  contains the number of partners. (Data extracted from 
bit.ly/11asPHm.)
a.	 At the 0.05 level of significance, is there evidence of a differ-
ence in the variability in numbers of partners for Southeast re-
gion accounting firms and Gulf Coast accounting firms?
b.	 Determine the p-value in (a) and interpret its meaning.
c.	 What assumption do you have to make about the two popula-
tions in order to justify the use of the F test?

406	
Chapter 10  Two-Sample Tests
d.	 Based on (a) and (b), which t test defined in Section 10.1 
should you use to test whether there is a significant difference 
in the mean number of partners for Southeast region account-
ing firms and Gulf Coast accounting firms?
10.47  A bank with a branch located in a commercial district 
of a city has the business objective of improving the process for 
serving customers during the noon-to-1 p.m. lunch period. To 
do so, the waiting time (defined as the number of minutes that 
elapses from when the customer enters the line until he or she 
reaches the teller window) needs to be shortened to increase cus-
tomer satisfaction. A random sample of 15 customers is selected 
and the waiting times are collected and stored in  Bank1 . These 
data are:
4.21    5.55    3.02    5.13    4.77    2.34    3.54    3.20
4.50    6.10    0.38    5.12    6.46    6.19    3.79
Suppose that another branch, located in a residential area, is also 
concerned with the noon-to-1 p.m. lunch period. A random sample 
of 15 customers is selected and the waiting times are collected and 
stored in  Bank2 . These data are:
  9.66    5.90    8.02    5.79    8.73    3.82    8.01    8.35
10.49    6.68    5.64    4.08    6.17    9.91    5.47
a.	 Is there evidence of a difference in the variability of the waiting 
time between the two branches? (Use a = 0.05.)
b.	 Determine the p-value in (a) and interpret its meaning.
c.	 What assumption about the population distribution of each 
bank is necessary in (a)? Is the assumption valid for these data?
d.	 Based on the results of (a), is it appropriate to use the pooled-
variance t test to compare the means of the two branches?
10.48  An important feature of digital cameras is battery life, 
the number of shots that can be taken before the battery needs to 
be recharged. The file  Cameras  contains the battery life of 11  
subcompact cameras and 7 compact cameras. (Data extracted from 
“Cameras,” Consumer Reports, July 2012, pp. 42–44.)
a.	 Is there evidence of a difference in the variability of the battery 
life between the two types of digital cameras? (Use a = 0.05.)
b.	 Determine the p-value in (a) and interpret its meaning.
c.	 What assumption about the population distribution of the two 
types of cameras is necessary in (a)? Is the assumption valid for 
these data?
d.	 Based on the results of (a), which t test defined in Section 10.1 
should you use to compare the mean battery life of the two 
types of cameras?
10.49  An article appearing in The Exponent, an independent col-
lege newspaper published by the Purdue Student Publishing Foun-
dation, reported that the average American college student spends 
1 hour (60 minutes) on Facebook daily. (Data extracted from 
bit.ly/NQRCJQ.) You wonder if there is a difference between 
males and females. You select a sample of 60 Facebook users 
(30 males and 30 females) at your college and collect data about the 
time spent on Facebook per day (in minutes) and store these data in 
 FacebookTime2 .
a.	 Using a 0.05 level of significance, is there evidence of a dif-
ference in the variances of time spent on Facebook per day be-
tween males and females?
b.	 On the basis of the results in (a), which t test defined in Sec-
tion 10.1 should you use to compare the means of males and 
females? Discuss.
10.50  Is there a difference in the variation of the yield of five-
year certificates of deposit (CDs) in different cities? The file  
 FiveYearCDRate  contains the yields for a five-year CD for ten 
banks in New York and eight banks in Los Angeles, as of June 3, 
2013. (Data extracted from www.Bankrate.com, June 3, 2013.) 
At the 0.05 level of significance, is there evidence of a difference 
in the variance of the yield of five-year CDs in the two cities? As-
sume that the population yields are normally distributed.
I
n the North Fork Beverages scenario, you were a regional 
sales manager for North Fork Beverages. You compared 
the sales volume of your new HandMade Citrus Cola when 
the product was featured in the beverage aisle end-cap to the 
sales volume when the product was featured in the end-cap 
by the produce department. An experiment was performed 
in which 10 stores used the beverage end-cap location and 
10 stores used the produce end-cap location. Using a t test 
for the difference between two means, you were able to con-
clude that the mean sales using the produce end-cap location 
are higher than the mean sales for the beverage end-cap loca-
tion. A confidence interval allowed you to infer with 95% 
confidence that population mean amount sold at the produce 
end-cap location 
was between 
6.73 and 36.67 
cases more than the beverage end-cap location. You also per-
formed the F test for the difference between two variances 
to see if the store-to-store variability in sales in stores using 
the produce end-cap location differed from the store-to-store 
variability in sales in stores using the beverage end-cap lo-
cation. You concluded that there was no significant differ-
ence in the variability of the sales of cola for the two display 
locations. As a regional sales manager, you decide to lease 
the produce end-cap location in all FoodPlace Supermarkets 
during your next sales promotional period.
U s i n g  S tat i s t i c s
For North Fork, Are There Different  
Means to the Ends? Revisited
Fotolia

	
Summary	
407
S u m m a r y
In this chapter, you were introduced to a variety of tests for 
two samples. For situations in which the samples are inde-
pendent, you learned statistical test procedures for analyz-
ing possible differences between means, proportions, and 
variances. In addition, you learned a test procedure that 
is ­frequently used when analyzing differences between the 
means of two related samples. Remember that you need to 
select the test that is most appropriate for a given set of condi-
tions and to critically investigate the validity of the assump-
tions underlying each of the hypothesis-testing procedures.
Table 10.7 provides a list of topics covered in this chap-
ter. The roadmap in Figure 10.15 illustrates the steps needed 
in determining which two-sample test of hypothesis to use. 
The following are the questions you need to consider:
	1.	What type of variables do you have? If you are dealing 
with categorical variables, use the Z test for the difference 
between two proportions. (This test assumes independent 
samples.)
	2.	If you have a numerical variable, determine whether you 
have independent samples or related samples. If you 
have related samples, and you can assume approximate 
normality, use the paired t test.
	3.	If you have independent samples, is your focus on variabil-
ity or central tendency? If the focus is on variability, and 
you can assume approximate normality, use the F test.
	4.	If your focus is central tendency and you can assume ap-
proximate normality, determine whether you can assume 
that the variances of the two populations are equal. (This 
assumption can be tested using the F test.)
	5.	If you can assume that the two populations have equal 
variances, use the pooled-variance t test. If you cannot 
assume that the two populations have equal variances, 
use the separate-variance t test.
T a b l e  1 0 . 7
Summary of Topics in 
Chapter 10
Types of Data
Type of Analysis
Numerical
Categorical
Comparing two 
populations
t tests for the difference in the 
means of two independent 
populations (Section 10.1)
Z test for the difference 
between two proportions 
(Section 10.3)
Paired t test (Section 10.2) 
F test for the difference between 
two variances (Section 10.4)
F i g u r e  1 0 . 1 5
Roadmap for selecting 
a test of hypothesis for 
two samples
2
   2
 2
 2
No
Yes
Categorical
Numerical
Yes
No
Central
Tendency
Variability
Z test for the
difference
between two
proportions
Type
of
Data
Independent
Samples?
 1 =  2?
Focus
F Test
for  1 =  2
Separate-Variance
t Test
Pooled-Variance
t Test
Two-Sample
Procedures
Paired
t Test
2
2

408	
Chapter 10  Two-Sample Tests
Referen c e s
	 1.	Conover, W. J. Practical Nonparametric Statistics, 3rd ed. 
New York: Wiley, 2000.
	 2.	Daniel, W. Applied Nonparametric Statistics, 2nd ed. Boston: 
Houghton Mifflin, 1990.
	 3.	Microsoft Excel 2013. Redmond, WA: Microsoft Corp., 2012.
	 4.	Minitab Release 16 State College, PA: Minitab, 2010.
	 5.	Satterthwaite, F. E. “An Approximate Distribution of Esti-
mates of Variance Components.” Biometrics Bulletin, 2(1946): 
110–114.
	 6.	Snedecor, G. W., and W. G. Cochran. Statistical Methods, 8th 
ed. Ames, IA: Iowa State University Press, 1989.
K e y  Eq u at i o n s
Pooled-Variance t Test for the Difference Between Two 
Means
tSTAT = 1X1 - X22 - 1m1 - m22
BS2
pa 1
n1
+ 1
n2
b
	
(10.1)
Confidence Interval Estimate for the Difference Between 
the Means of Two Independent Populations
1X1 - X22 { ta>2BS2
pa 1
n1
+ 1
n2
b	
(10.2)
or
1X1 - X22 - ta>2BS2
pa 1
n1
+ 1
n2
b … m1 - m2

… 1X1 - X22 + ta>2BS2
pa 1
n1
+ 1
n2
b
Separate-Variance t Test for the Difference Between Two 
Means
tSTAT = 1X1 - X22 - 1m1 - m22
B
S2
1
n1
+ S2
2
n2
	
(10.3)
Computing Degrees of Freedom in the Separate-Variance 
t Test
V =
a S2
1
n1
+ S2
2
n2
b
2
a S2
1
n1
b
2
n1 - 1 +
a S2
2
n2
b
2
n2 - 1
	
(10.4)
Paired t Test for the Mean Difference
tSTAT = D - mD
SD
2n
	
(10.5)
Confidence Interval Estimate for the Mean Difference
D { ta>2
SD
2n

(10.6)
or
D - ta>2
SD
2n
… mD … D + ta>2
SD
2n
Z Test for the Difference Between Two Proportions
ZSTAT = 1p1 - p22 - 1p1 - p22
Bp11 - p2a 1
n1
+ 1
n2
b

(10.7)
Confidence Interval Estimate for the Difference Between 
Two Proportions
1p1 - p22 {  Za>2B a p111 - p12
n1
+ p211 - p22
n2
b

(10.8)
or
1p1 - p22 - Za>2B
p111 - p12
n1
+ p211 - p22
n2
 …  1p1 - p22

… 1p1 - p22 + Za>2B
p111 - p12
n1
+ p211 - p22
n2
F Test Statistic for Testing the Ratio of Two Variances
FSTAT = S2
1
S2
2

(10.9)

	
Chapter Review Problems	
409
K ey Te r ms
F distribution  402
F test for the ratio of two variances  402
matched samples  387
paired t test for the mean difference  388
pooled-variance t test  376
repeated measurements  387
robust  379
separate-variance t test  382
two-sample tests  376
Z test for the difference between two 
­proportions  395
C h ec ki n g  Yo ur  U n de r s ta nding
10.51  Explain the meaning of the term ‘pooled variance t-test’. 
When should a separate variance t-test be used instead of a pooled 
variance t-test?
10.52  While comparing the mean of two related populations, do 
you think the sample size of both the populations should be the 
same?
10.53  What graphs should be drawn to determine that a popula-
tion is normally distributed?
10.54  Should the sample size of two independent populations be 
same in order to test the difference between their means?
10.55  What is the distinction between repeated measurements 
and matched items?
10.56  A lecturer teaching in a language school wants to compare 
the performance of students in two classes he is teaching: one in 
the morning and the other in the afternoon. Which test should be 
used by the lecturer?
10.57  What are the ways to test the difference between the means 
when the population is not normally distributed?
C h a pte r  R e vi e w P r ob le ms
10.58  The American Society for Quality (ASQ) conducted a sal-
ary survey of all its members. ASQ members work in all areas of 
manufacturing and service-related institutions, with a common 
theme of an interest in quality. Two job titles are black belt and 
green belt. (See Section 19.6 for a description of these titles in 
a Six Sigma quality improvement initiative.) Descriptive statistics 
concerning salaries for these two job titles are given in the follow-
ing table:
 
Job Title
 
Sample Size
 
Mean
Standard 
Deviation
Black belt
121
93,946
21,166
Green belt
26
74,173
23,399
Source: Data extracted from “QP Salary Survey,” Quality Progress, 
December 2012, p. 29.
a.	 Using a 0.05 level of significance, is there a difference in the 
variability of salaries between black belts and green belts?
b.	 Based on the result of (a), which t test defined in Section 10.1 
is appropriate for comparing mean salaries?
c.	 Using a 0.05 level of significance, is the mean salary of black 
belts greater than the mean salary of green belts?
10.59  Do male and female students study the same amount per 
week? In a recent year, 58 sophomore business students were sur-
veyed at a large university that has more than 1,000 sophomore 
business students each year. The file  StudyTime  contains the gen-
der and the number of hours spent studying in a typical week for 
the sampled students.
a.	 At the 0.05 level of significance, is there a difference in the 
­variance of the study time for male students and female 
­students?
b.	 Using the results of (a), which t test is appropriate for compar-
ing the mean study time for male and female students?
c.	 At the 0.05 level of significance, conduct the test selected in 
(b).
d.	 Write a short summary of your findings.
10.60  Do males and females differ in the amount of time they 
talk on the phone and the number of text messages they send? A 
study reported that women spent a mean of 818 minutes per month 
talking as compared to 716 minutes per month for men. (Data ex-
tracted from “Women Talk and Text More,” USA Today, February 1, 
2011, p. 29A.) The sample sizes were not reported. Suppose that 
the sample sizes were 100 each for women and men and that the 
standard deviation for women was 125 minutes per month as com-
pared to 100 minutes per month for men.
a.	 Using a 0.01 level of significance, is there evidence of a differ-
ence in the variances of the amount of time spent talking be-
tween women and men?
b.	 To test for a difference in the mean talking time of women and 
men, is it most appropriate to use the pooled-variance t test or 
the separate-variance t test? Use the most appropriate test to 
determine if there is a difference in the amount of time spent 
talking between women and men.

410	
Chapter 10  Two-Sample Tests
The article also reported that women sent a mean of 716 text 
messages per month compared to 555 per month for men. Suppose 
that the standard deviation for women was 150 text messages per 
month compared to 125 text messages per month for men.
c.	 Using a 0.01 level of significance, is there evidence of a differ-
ence in the variances of the number of text messages sent per 
month by women and men?
d.	 Based on the results of (c), use the most appropriate test to de-
termine, at the 0.01 level of significance, whether there is evi-
dence of a difference in the mean number of text messages sent 
per month by women and men.
10.61  The file  Restaurants  contains the ratings for food, décor, 
service, and the price per person for a sample of 50 restaurants lo-
cated in a city and 50 restaurants located in a suburb. Completely 
analyze the differences between city and suburban restaurants for 
the variables food rating, décor rating, service rating, and cost per 
person, using a = 0.05.
Source: Data extracted from Zagat Survey 2013 New York City 
Restaurants and Zagat Survey 2012–2013 Long Island Restaurants.
10.62  A computer information systems professor is interested in 
studying the amount of time it takes students enrolled in the Intro-
duction to Computers course to write a program in VB.NET. The 
professor hires you to analyze the following results (in minutes), 
stored in  VB , from a random sample of nine students:
10 13 9 15 12 13 11 13 12
a.	 At the 0.05 level of significance, is there evidence that the pop-
ulation mean time is greater than 10 minutes? What will you 
tell the professor?
b.	 Suppose that the professor, when checking her results, realizes 
that the fourth student needed 51 minutes rather than the re-
corded 15 minutes to write the VB.NET program. At the 0.05 
level of significance, reanalyze the question posed in (a), using 
the revised data. What will you tell the professor now?
c.	 The professor is perplexed by these paradoxical results and re-
quests an explanation from you regarding the justification for 
the difference in your findings in (a) and (b). Discuss.
d.	 A few days later, the professor calls to tell you that the dilemma 
is completely resolved. The original number 15 (the fourth data 
value) was correct, and therefore your findings in (a) are being 
used in the article she is writing for a computer journal. Now 
she wants to hire you to compare the results from that group of 
Introduction to Computers students against those from a sam-
ple of 11 computer majors in order to determine whether there 
is evidence that computer majors can write a VB.NET program 
in less time than introductory students. For the computer ma-
jors, the sample mean is 8.5 minutes, and the sample standard 
deviation is 2.0 minutes. At the 0.05 level of significance, com-
pletely analyze these data. What will you tell the professor?
e.	 A few days later, the professor calls again to tell you that a re-
viewer of her article wants her to include the p-value for the 
“correct” result in (a). In addition, the professor inquires about 
an unequal-variances problem, which the reviewer wants her to 
discuss in her article. In your own words, discuss the concept of 
p-value and also describe the unequal-variances problem. Then, 
determine the p-value in (a) and discuss whether the unequal-
variances problem had any meaning in the professor’s study.
Gender
Dessert Ordered
Male
Female
Total
Yes
50
96
146
No
250
234
484
Total
300
330
630
Beef Entrée
Dessert Ordered
Yes
No
Total
Yes
74
68
142
No
123
365
488
Total
197
433
630
10.63  In-store marketing has become an effective tool in super-
markets these days. The prices of branded food products also differ 
across supermarkets due to their supply chain efficiencies, in-store 
promotions, etc. Assume that the prices of 30 different products have 
been recorded in two stores A and B, located within a mile of each 
other. The average price recorded in store A is $3.4 with a standard 
deviation of $1.5, whereas the average price recorded in store B is 
$4.5 with a standard deviation of $1. 
a.	 At 0.05 level of significance, test whether there is any differ-
ence between the means of two populations.
b.	  Construct the confidence interval at 95% for the mean differ-
ence between the two populations.
c.	 Interpret the results in (a) and (b).
10.64  The lengths of life (in hours) of a sample of 40 20-watt 
compact fluorescent light bulbs produced by manufacturer A and 
a sample of 40 20-watt compact fluorescent light bulbs produced 
by manufacturer B are stored in  Bulbs . Completely analyze the 
­differences between the lengths of life of the compact fluorescent 
light bulbs produced by the two manufacturers. (Use a = 0.05.)
10.65  A hotel manager looks to enhance the initial impressions 
that hotel guests have when they check in. Contributing to initial 
impressions is the time it takes to deliver a guest’s luggage to the 
room after check-in. A random sample of 20 deliveries on a par-
ticular day were selected in Wing A of the hotel, and a random 
sample of 20 deliveries were selected in Wing B. The results are 
stored in  Luggage . Analyze the data and determine whether there 
is a difference between the mean delivery times in the two wings 
of the hotel. (Use a = 0.05.)
10.66  The owner of a restaurant that serves Continental-style en-
trées has the business objective of learning more about the patterns 
of patron demand during the Friday-to-Sunday weekend time pe-
riod. She decided to study the demand for dessert during this time 
period. In addition to studying whether a dessert was ordered, she 
will study the gender of the individual and whether a beef entrée 
was ordered. Data were collected from 630 customers and orga-
nized in the following contingency tables:
a.	 At the 0.05 level of significance, is there evidence of a difference 
between males and females in the proportion who order dessert?
b.	 At the 0.05 level of significance, is there evidence of a differ-
ence in the proportion who order dessert based on whether a 
beef entrée has been ordered?

	
Cases for Chapter 10	
411
10.67  The manufacturer of Boston and Vermont asphalt shingles 
knows that product weight is a major factor in the customer’s per-
ception of quality. Moreover, the weight represents the amount of 
raw materials being used and is therefore very important to the 
company from a cost standpoint. The last stage of the assembly 
line packages the shingles before they are placed on wooden pal-
lets. Once a pallet is full (a pallet for most brands holds 16 squares 
of shingles), it is weighed, and the measurement is recorded. The 
file  Pallet  contains the weight (in pounds) from a sample of 368 
pallets of Boston shingles and 330 pallets of Vermont shingles. 
Completely analyze the differences in the weights of the Boston 
and Vermont shingles, using a = 0.05.
10.68  The manufacturer of Boston and Vermont asphalt shingles 
provides its customers with a 20-year warranty on most of its 
products. To determine whether a shingle will last as long as the 
warranty period, the manufacturer conducts accelerated-life test-
ing. Accelerated-life testing exposes the shingle to the stresses it 
would be subject to in a lifetime of normal use in a laboratory set-
ting via an experiment that takes only a few minutes to conduct. 
In this test, a shingle is repeatedly scraped with a brush for a short 
period of time, and the shingle granules removed by the brushing 
are weighed (in grams). Shingles that experience low amounts of 
granule loss are expected to last longer in normal use than shingles 
that experience high amounts of granule loss. In this situation, 
a shingle should experience no more than 0.8 grams of granule 
loss if it is expected to last the length of the warranty period. The 
file  Granule  contains a sample of 170 measurements made on 
the company’s Boston shingles and 140 measurements made on 
­Vermont shingles. Completely analyze the differences in the gran-
ule loss of the Boston and Vermont shingles, using a = 0.05.
10.69  There are a very large number of mutual funds from which 
an investor can choose. Each mutual fund has its own mix of dif-
ferent types of investments. The data in  BestFunds1  present the 
one-year return and the three-year annualized return for the 10 best 
short-term bond funds and the 10 best long-term bond funds, ac-
cording to the U.S. News & World Report. (Data extracted from 
money.usnews.com/mutual-funds/rankings.) Analyze the data 
and determine whether any differences exist between short-term 
and long-term bond funds. (Use the 0.05 level of significance.)
Report Writing Exercise
10.70  Referring to the results of Problems 10.67 and 10.68 con-
cerning the weight and granule loss of Boston and Vermont shin-
gles, write a report that summarizes your conclusions.
C a s e s  f o r  C h a p t e r  1 0
Managing Ashland MultiComm Services
AMS communicates with customers who subscribe to cable 
television services through a special secured email system 
that sends messages about service changes, new features, 
and billing information to in-home digital set-top boxes 
for later display. To enhance customer service, the opera-
tions department established the business objective of re-
ducing the amount of time to fully update each subscriber’s 
set of messages. The department selected two ­candidate 
messaging systems and conducted an experiment in which 
30 randomly chosen cable subscribers were assigned one 
of the two systems (15 assigned to each system). Update 
times were measured, and the results are organized in  
Table AMS10.1 and stored in  AMS10 .
Email Interface 1
Email Interface 2
4.13
3.71
3.75
3.89
3.93
4.22
3.74
4.57
3.36
4.24
3.85
3.90
3.26
4.09
3.73
4.05
4.06
4.07
3.33
3.80
3.96
4.36
3.57
4.38
3.13
3.49
3.68
3.57
3.63
4.74
T a b l e  A M S 1 0 . 1
Update Times (in 
seconds) for Two 
Different Email 
Interfaces

412	
Chapter 10  Two-Sample Tests
1.	Analyze the data in Table AMS10.1 and write a report to 
the computer operations department that indicates your 
findings. Include an appendix in which you discuss the 
reason you selected a particular statistical test to compare 
the two independent groups of callers.
2.	Suppose that instead of the research design described in 
the case, there were only 15 subscribers sampled, and the 
update process for each subscriber email was measured 
for each of the two messaging systems. Suppose that 
the results were organized in Table AMS10.1—making 
each row in the table a pair of values for an individual 
subscriber. Using these suppositions, reanalyze the Table 
AMS10.1 data and write a report for presentation to the 
team that indicates your findings.
Digital Case
Apply your knowledge about hypothesis testing in this Digi-
tal Case, which continues the cereal-fill packaging dispute 
Digital Case from Chapters 7 and 9.
Even after the recent public experiment about cereal box 
weights, Consumers Concerned About Cereal Cheaters 
(CCACC) remains convinced that Oxford Cereals has mis-
led the public. The group has created and circulated More-
Cheating.pdf, a document in which it claims that cereal 
boxes produced at Plant Number 2 in Springville weigh less 
than the claimed mean of 368 grams. Review this document 
and then answer the following questions:
1.	Do the CCACC’s results prove that there is a statistically 
significant difference in the mean weights of cereal boxes 
produced at Plant Numbers 1 and 2?
2.	 Perform the appropriate analysis to test the CCACC’s hypo-
thesis. What conclusions can you reach based on the data?
Sure Value Convenience Stores
You work in the corporate office for a nationwide conve-
nience store franchise that operates nearly 10,000 stores. 
The per-store daily customer count (i.e., the mean number 
of customers in a store in one day) has been steady, at 900, 
for some time. To increase the customer count, the chain is 
considering cutting prices for coffee beverages. The small 
size will now be either $0.59 or $0.79 instead of $0.99. 
Even with this reduction in price, the chain will have a 40% 
gross margin on coffee.
The question to be determined is how much to cut 
prices to increase the daily customer count without reduc-
ing the gross margin on coffee sales too much. The chain 
decides to carry out an experiment in a sample of 30 stores 
where customer counts have been running almost exactly at 
the national average of 900. In 15 of the stores, the price 
of a small coffee will now be $0.59 instead of $0.99, and 
in 15 other stores, the price of a small coffee will now be 
$0.79. After four weeks, the 15 stores that priced the small 
coffee at $0.59 had a mean daily customer count of 964 and 
a standard deviation of 88, and the 15 stores that priced the 
small coffee at $0.79 had a mean daily customer count of 
941 and a standard deviation of 76. Analyze these data (us-
ing the 0.05 level of significance) and answer the following 
questions.
1.	 Does reducing the price of a small coffee to either $0.59 
or $0.79 increase the mean per-store daily customer 
count?
2.	 If reducing the price of a small coffee to either $0.59 or 
$0.79 increases the mean per-store daily customer count, 
is there any difference in the mean per-store daily cus-
tomer count between stores in which a small coffee was 
priced at $0.59 and stores in which a small coffee was 
priced at $0.79?
3.	 What price do you recommend for a small coffee?
CardioGood Fitness
Return to the CardioGood Fitness case first presented on 
page 111. Using the data stored in  CardioGood Fitness :
1.  Determine whether differences exist between males and 
females in their age in years, education in years, an-
nual household income ($), mean number of times the  
customer plans to use the treadmill each week, and mean 
number of miles the customer expects to walk or run 
each week.
2.  Write a report to be presented to the management of 
­CardioGood Fitness detailing your findings.

	
Cases for Chapter 10	
413
More Descriptive Choices Follow-Up
Clear Mountain State Student Surveys
Follow up the Using Statistics scenario “More Descriptive 
Choices, Revisited” on page 166 by determining whether 
there is a difference in the 3-year return percentage, 5-year 
return percentages, and 10-year return percentages of the 
growth and value funds (stored in  Retirement Funds ).
1.	The Student News Service at Clear Mountain State 
­University (CMSU) has decided to gather data about the 
undergraduate students that attend CMSU. It creates and 
distributes a survey of 14 questions and receives responses 
from 62 undergraduates (stored in  UndergradSurvey ).
a.	At the 0.05 level of significance, is there evidence of a 
difference between males and females in grade point 
average, expected starting salary, number of social 
networking sites registered for, age, spending on text-
books and supplies, text messages sent in a week, and 
the wealth needed to feel rich?
b.	At the 0.05 level of significance, is there evidence of a 
difference between students who plan to go to gradu-
ate school and those who do not plan to go to graduate 
school in grade point average, expected starting salary, 
number of social networking sites registered for, age, 
spending on textbooks and supplies, text messages 
sent in a week, and the wealth needed to feel rich?
2.	The dean of students at CMSU has learned about the un-
dergraduate survey and has decided to undertake a simi-
lar survey for graduate students at Clear Mountain State. 
She creates and distributes a survey of 14 questions and 
receives responses from 44 graduate students (stored in 
 GradSurvey ). For these data, at the 0.05 level of signifi-
cance, is there evidence of a difference between males 
and females in age, undergraduate grade point average, 
graduate grade point average, expected salary upon grad-
uation, spending on textbooks and supplies, text mes-
sages sent in a week, and the wealth needed to feel rich?

414	
Chapter 10  Two-Sample Tests
EG10.1  Comparing the Means of Two 
Independent Populations
Pooled-Variance t Test for the Difference 
Between Two Means
Key Technique  Use the T.INV.2T(level of significance, de-
grees of freedom) function to compute the lower and upper critical 
values and use the T.DIST.2T(absolute value of the t test statistic, 
degrees of freedom) to compute the p-value.
Example  Perform the Figure 10.3 pooled-variance t test for the 
two end-cap locations data shown on page 378.
PHStat  Use Pooled-Variance t Test.
For the example, open to the DATA worksheet of the Cola work-
book. Select PHStat ➔ Two-Sample Tests (Unsummarized 
Data) ➔ Pooled-Variance t Test. In the procedure’s dialog box 
(shown below):
	 1.	 Enter 0 as the Hypothesized Difference.
	 2.	 Enter 0.05 as the Level of Significance.
	 3.	 Enter A1:A11 as the Population 1 Sample Cell Range.
	 4.	 Enter B1:B11 as the Population 2 Sample Cell Range.
	 5.	 Check First cells in both ranges contain label.
	 6.	 Click Two-Tail Test.
	 7.	 Enter a Title and click OK.
When using summarized data, select PHStat ➔ Two-Sample 
Tests (Summarized Data) ➔ Pooled-Variance t Test. In that pro-
cedure’s dialog box, enter the hypothesized difference and level of 
significance, as well as the sample size, sample mean, and sample 
standard deviation for each sample.
In-Depth Excel  Use the COMPUTE worksheet of the Pooled-
Variance T workbook as a template.
The worksheet already contains the data and formulas to use the 
unsummarized data for the example. For other problems, use this 
worksheet with either unsummarized or summarized data. For un-
summarized data, paste the data in columns A and B in the DATA-
COPY worksheet and keep the COMPUTE worksheet formulas 
that compute the sample size, sample mean, and sample standard 
deviation in the cell range B7:B13. For summarized data, replace 
the formulas in the cell range B7:B13 with the sample statistics 
and ignore the DATACOPY worksheet.
Use the COMPUTE_LOWER or COMPUTE_UPPER 
worksheets in the same workbook as templates for performing 
one-tail pooled-variance t tests with either unsummarized or sum-
marized data. If you use an Excel version older than Excel 2010, 
use the COMPUTE_OLDER worksheet as a template for both the 
two-tail and one-tail tests.
Analysis ToolPak  Use t-Test: Two-Sample Assuming Equal 
Variances.
For the example, open to the DATA worksheet of the Cola work-
book and:
	 1.	 Select Data ➔ Data Analysis.
	 2.	 In the Data Analysis dialog box, select t-Test: Two-Sample 
Assuming Equal Variances from the Analysis Tools list and 
then click OK.
In the procedure’s dialog box (shown below):
	 3.	 Enter A1:A11 as the Variable 1 Range
	 4.	 Enter B1:B11 as the Variable 2 Range.
	 5.	 Enter 0 as the Hypothesized Mean Difference.
	 6.	 Check Labels and enter 0.05 as Alpha.
	 7.	 Click New Worksheet Ply.
	 8.	 Click OK.
C h a p t e r  1 0  E x c e l  G u i d e

	
Chapter 10 Excel Guide	
415
Results (shown below) appear in a new worksheet that contains 
both two-tail and one-tail test critical values and p-values. Unlike 
the results shown in Figure 10.3, only the positive (upper) critical 
value is listed for the two-tail test.
Confidence Interval Estimate for the Difference 
Between Two Means
PHStat  Modify the PHStat instructions for the pooled-variance 
t test. In step 7, check Confidence Interval Estimate and enter a 
Confidence Level in its box, in addition to entering a Title and 
clicking OK.
In-Depth Excel  Use the In-Depth Excel instructions for the 
pooled-variance t test. The Pooled-Variance T workbook work-
sheets include a confidence interval estimate for the difference be-
tween two means in the cell range D3:E16.
t Test for the Difference Between Two Means, 
Assuming Unequal Variances
Key Technique  Use the T.INV.2T(level of significance, de-
grees of freedom) function to compute the lower and upper critical 
values and use the T.DIST.2T(absolute value of the t test statistic, 
degrees of freedom) to compute the p-value.
Example  Perform the Figure 10.7 separate-variance t test for the 
two end-cap locations data shown on page 384.
PHStat  Use Separate-Variance t Test.
For the example, open to the DATA worksheet of the Cola work-
book. Select PHStat ➔ Two-Sample Tests (Unsummarized 
Data) ➔ Separate-Variance t Test. In the procedure’s dialog box 
(shown in the right column):
	 1.	 Enter 0 as the Hypothesized Difference.
	 2.	 Enter 0.05 as the Level of Significance.
	 3.	 Enter A1:A11 as the Population 1 Sample Cell Range.
	 4.	 Enter B1:B11 as the Population 2 Sample Cell Range.
	 5.	 Check First cells in both ranges contain label.
	 6.	 Click Two-Tail Test.
	 7.	 Enter a Title and click OK.
When using summarized data, select PHStat ➔ Two-Sample 
Tests (Summarized Data) ➔ Separate-Variance t Test. In that 
procedure’s dialog box, enter the hypothesized difference and the 
level of significance, as well as the sample size, sample mean, and 
sample standard deviation for each group.
In-Depth Excel  Use the COMPUTE worksheet of the Sepa-
rate-Variance T workbook as a template.
The worksheet already contains the data and formulas to use the 
unsummarized data for the example. For other problems, use the 
COMPUTE worksheet with either unsummarized or summarized 
data. For unsummarized data, paste the data in columns A and B in 
the DATACOPY worksheet and keep the COMPUTE worksheet 
formulas that compute the sample size, sample mean, and sample 
standard deviation in the cell range B7:B13. For summarized data, 
replace those formulas in the cell range B7:B13 with the sample 
statistics and ignore the DATACOPY worksheet.
Use the COMPUTE_LOWER or COMPUTE_UPPER 
worksheets in the same workbook as templates for performing 
one-tail pooled-variance t tests with either unsummarized or sum-
marized data. If you use an Excel version older than Excel 2010, 
use the COMPUTE_OLDER worksheet as a template for both the 
two-tail and one-tail tests.
Analysis ToolPak  Use t-Test: Two-Sample Assuming Un-
equal Variances.
For the example, open to the DATA worksheet of the Cola work-
book and:
	 1.	 Select Data ➔ Data Analysis.
	 2.	 In the Data Analysis dialog box, select t-Test: Two-Sample 
Assuming Unequal Variances from the Analysis Tools list 
and then click OK.
In the procedure’s dialog box (shown on page 416):
	 3.	 Enter A1:A11 as the Variable 1 Range.
	 4.	 Enter B1:B11 as the Variable 2 Range.
	 5.	 Enter 0 as the Hypothesized Mean Difference.
	 6.	 Check Labels and enter 0.05 as Alpha.
	 7.	 Click New Worksheet Ply.
	 8.	 Click OK.

416	
Chapter 10  Two-Sample Tests
Results (shown below) appear in a new worksheet that contains 
both two-tail and one-tail test critical values and p-values. Unlike 
the results shown in Figure 10.7, only the positive (upper) criti-
cal value is listed for the two-tail test. Because the Analysis Tool-
Pak uses table lookups to approximate the critical values and the 
p-value, the results will differ slightly from the values shown in 
Figure 10.7.
EG10.2  Comparing the Means of Two 
Related Populations
Paired t Test
Key Technique  Use the T.INV.2T(level of significance, de-
grees of freedom) function to compute the lower and upper critical 
values and use the T.DIST.2T(absolute value of the t test statistic, 
degrees of freedom) to compute the p-value.
Example  Perform the Figure 10.9 paired t test for the textbook 
price data shown on page 391.
PHStat  Use Paired t Test.
For the example, open to the DATA worksheet of the Book-
Prices workbook. Select PHStat ➔ Two-Sample Tests (Unsum-
marized Data) ➔ Paired t Test. In the procedure’s dialog box 
(shown in the right column):
	 1.	 Enter 0 as the Hypothesized Mean Difference.
	 2.	 Enter 0.05 as the Level of Significance.
	 3.	 Enter C1:C17 as the Population 1 Sample Cell Range.
	 4.	 Enter D1:D17 as the Population 2 Sample Cell Range.
	 5.	 Check First cells in both ranges contain label.
	 6.	 Click Two-Tail Test.
	 7.	 Enter a Title and click OK.
The procedure creates two worksheets, one of which is similar to 
the PtCalcs worksheet discussed in the following In-Depth Excel 
section. When using summarized data, select PHStat ➔ Two-
Sample Tests (Summarized Data) ➔ Paired t Test. In that pro-
cedure’s dialog box, enter the hypothesized mean difference, the 
level of significance, and the differences cell range.
In-Depth Excel  Use the COMPUTE and PtCalcs worksheets 
of the Paired T workbook as a template.
The COMPUTE and supporting PtCalcs worksheets already con-
tain the textbook price data for the example. The PtCalcs work-
sheet also computes the differences that allow the COMPUTE 
worksheet to compute the SD in cell B11.
For other problems, paste the unsummarized data into columns 
A and B of the PtCalcs worksheet. For sample sizes greater than 16, 
select cell C17 and copy the formula in that cell down through the 
last data row. For sample sizes less than 16, delete the column C for-
mulas for which there are no column A and B values. If you know 
the sample size, D, and SD values, you can ignore the PtCalcs work-
sheet and enter the values in cells B8, B9, and B11 of the COM-
PUTE worksheet, overwriting the formulas that those cells contain.
Use the similar COMPUTE_LOWER and COMPUTE_
UPPER worksheets in the same workbook as templates for per-
forming one-tail tests. If you use an Excel version older than Excel 
2010, use the COMPUTE_OLDER worksheet as a template for 
both the two-tail and one-tail tests.
Analysis ToolPak  Use t-Test: Paired Two Sample for Means.
For the example, open to the DATA worksheet of the BookPrices 
workbook and:
	 1.	 Select Data ➔ Data Analysis.
	 2.	 In the Data Analysis dialog box, select t-Test: Paired Two 
Sample for Means from the Analysis Tools list and then click 
OK.
In the procedure’s dialog box (shown on page 417):
	 3.	 Enter C1:C17 as the Variable 1 Range.
	 4.	 Enter D1:D17 as the Variable 2 Range.
	 5.	 Enter 0 as the Hypothesized Mean Difference.
	 6.	 Check Labels and enter 0.05 as Alpha.
	 7.	 Click New Worksheet Ply.
	 8.	 Click OK.

	
Chapter 10 Excel Guide	
417
Results (shown below) appear in a new worksheet that contains 
both two-tail and one-tail test critical values and p-values. Unlike 
in Figure 10.9, only the positive (upper) critical value is listed for 
the two-tail test.
EG10.3  Comparing the Proportions  
of Two Independent 
Populations
Z Test for the Difference Between  
Two Proportions
Key Technique  Use the NORM.S.INV (percentage) function 
to compute the critical values and use the NORM.S.DIST (ab-
solute value of the Z test statistic, True) function to compute the 
p-value.
Example  Perform the Figure 10.13 Z test for the hotel guest sat-
isfaction survey shown on page 398.
PHStat  Use Z Test for Differences in Two Proportions.
For the example, select PHStat ➔ Two-Sample Tests (Summa-
rized Data) ➔ Z Test for Differences in Two Proportions. In the 
procedure’s dialog box (shown in the right column):
	 1.	 Enter 0 as the Hypothesized Difference.
	 2.	 Enter 0.05 as the Level of Significance.
	 3.	 For the Population 1 Sample, enter 163 as the Number of 
Items of Interest and 227 as the Sample Size.
	 4.	 For the Population 2 Sample, enter 154 as the Number of 
Items of Interest and 262 as the Sample Size.
	 5.	 Click Two-Tail Test.
	 6.	 Enter a Title and click OK.
In-Depth Excel  Use the COMPUTE worksheet of the Z Two 
Proportions workbook as a template.
The worksheet already contains data for the hotel guest satisfac-
tion survey. For other problems, change the hypothesized differ-
ence, the level of significance, and the number of items of interest 
and sample size for each group in the cell range B4:B11.
Use the similar COMPUTE_LOWER and COMPUTE_
UPPER worksheets in the same workbook as templates for 
performing one-tail Z tests for the difference between two propor-
tions. If you use an Excel version older than Excel 2010, use the 
COMPUTE_OLDER worksheet as a template for both the two-tail 
and one-tail tests.
Confidence Interval Estimate for the Difference 
Between Two Proportions
PHStat  Modify the PHStat instructions for the Z test for the 
difference between two proportions. In step 6, also check Confi-
dence Interval Estimate and enter a Confidence Level in its box, 
in addition to entering a Title and clicking OK.
In-Depth Excel  Use the In-Depth Excel instructions for the Z 
test for the difference between two proportions. The Z Two Pro-
portions workbook worksheets include a confidence interval es-
timate for the difference between two means in the cell range 
D3:E16.
EG10.4  f Test for the Ratio of Two 
Variances
Key Technique  Use the F.INV.RT(level of significance / 2, 
population 1 sample degrees of freedom, population 2 sample 
degrees of freedom) function to compute the upper critical value 
and use the F.DIST.RT(F test statistic, population 1 sample de-
grees of freedom, population 2 sample degrees of freedom) func-
tion to compute the p-values.
Example  Perform the Figure 10.14 F test for the ratio of two 
variances for the two end-cap locations data shown on page 404.

418	
Chapter 10  Two-Sample Tests
PHStat  Use F Test for Differences in Two Variances.
For the example, open to the DATA worksheet of the Cola work-
book. Select PHStat ➔ Two-Sample Tests (Unsummarized 
Data) ➔ F Test for Differences in Two Variances. In the proce-
dure’s dialog box (shown below):
	 1.	 Enter 0.05 as the Level of Significance.
	 2.	 Enter A1:A11 as the Population 1 Sample Cell Range.
	 3.	 Enter B1:B11 as the Population 2 Sample Cell Range.
	 4.	 Check First cells in both ranges contain label.
	 5.	 Click Two-Tail Test.
	 6.	 Enter a Title and click OK.
When using summarized data, select PHStat ➔ Two-Sample 
Tests (Summarized Data) ➔ F Test for Differences in Two 
Variances. In that procedure’s dialog box, enter the level of signif-
icance and the sample size and sample variance for each sample.
In-Depth Excel  Use the COMPUTE worksheet of the F Two 
Variances workbook as a template.
The worksheet already contains the data and formulas for using 
the unsummarized data for the example. For unsummarized data, 
paste the data in columns A and B in the DATACOPY worksheet 
and keep the COMPUTE worksheet formulas that compute the 
sample size and sample variance for the two samples in cell range 
B4:B10. For summarized data, replace the COMPUTE worksheet 
formulas in cell ranges B4:B10 with the sample statistics and ig-
nore the DATACOPY worksheet.
Use the similar COMPUTE_UPPER worksheet in the same 
workbook as a template for performing the upper-tail test. If you 
use an Excel version older than Excel 2010, use the COMPUTE_
OLDER worksheet as a template for both the two-tail and upper-
tail tests.
Analysis ToolPak  Use F-Test Two-Sample for Variances.
For the example, open to the DATA worksheet of the Cola work-
book and:
	 1.	 Select Data ➔ Data Analysis.
	 2.	 In the Data Analysis dialog box, select F-Test Two-Sample 
for Variances from the Analysis Tools list and then click OK.
In the procedure’s dialog box (shown below):
	 3.	 Enter A1:A11 as the Variable 1 Range and enter B1:B11 as 
the Variable 2 Range.
	 4.	 Check Labels and enter 0.05 as Alpha.
	 5.	 Click New Worksheet Ply.
	 6.	 Click OK.
Results (shown below) appear in a new worksheet and include 
only the one-tail test p-value (0.1241), which must be doubled for 
the two-tail test shown in Figure 10.14 on page 404.

	
Chapter 10 Minitab Guide	
419
MG10.1  Comparing the Means of Two 
Independent Populations
Pooled-Variance t Test for the Difference 
Between Two Means
Use 2-Sample t.
For example, to perform the Figure 10.3 pooled-variance t test for 
the two end-cap locations shown on page 378, open to the Cola 
worksheet. Select Stat ➔ Basic Statistics ➔ 2-Sample t. In the 
2-Sample t (Test and Confidence Interval) dialog box (shown  
below):
	 1.	 Click Samples in different columns and press Tab.
	 2.	 Double-click C1  Beverage in the variables list to add Bever-
age to the First box.
	 3.	 Double-click C2  Produce in the variables list to add Produce 
to the Second box.
	 4.	 Check Assume equal variances.
	 5.	 Click Options.
In the 2-Sample t-Options dialog box (not shown):
	 6.	 Enter 95.0 in the Confidence level box.
	 7.	 Select not equal from the Alternative drop-down list.
	 8.	 Click OK.
	 9.	 Back in the original dialog box, click OK.
For stacked data, use these replacement steps 1 through 3:
	 1.	 Click Samples in one column.
	 2.	 Enter the name of the column that contains the measurement 
in the Samples box.
	 3.	 Enter the name of the column that contains the sample names 
in the Subscripts box.
To create a boxplot for the analysis, replace step 9 with the follow-
ing steps 9 through 11:
	 9.	 Back in the original dialog box, click Graphs.
	10.	 In the 2-Sample t-Graphs dialog box (not shown), check 
­Boxplots of data and then click OK.
	11.	 Back in the original dialog box, click OK.
For a one-tail test, select less than or greater than in step 7.
Confidence Interval Estimate for the Difference 
Between Two Means
Use the instructions for the pooled-variance t test, which computes 
a confidence interval estimate as part of the analysis.
t Test for the Difference Between Two Means, 
Assuming Unequal Variances
Use the instructions for the pooled-variance t test with this  
replacement step 4:
	 4.	 Clear Assume equal variances.
MG10.2  Comparing the Means of Two 
Related Populations
Paired t Test
Use Paired t.
For example, to perform the Figure 10.9 paired t test for the 
­textbook price data on page 391, open to the BookPrices work-
sheet. Select Stat ➔ Basic Statistics ➔ Paired t. In the Paired t 
(Test and Confidence Interval) dialog box (shown below):
	 1.	 Click Samples in columns and press Tab.
	 2.	 Double-click C3  Bookstore in the variables list to enter 
Bookstore in the First sample box.
	 3.	 Double-click C4  Online in the variables list to enter Online 
in the Second sample box.
	 4.	 Click Options.
C h a p t e r  1 0  M i n i ta b  G u i d e

420	
Chapter 10  Two-Sample Tests
In the Paired t-Options dialog box (not shown):
	 5.	 Enter 95.0 in the Confidence level box.
	 6.	 Select not equal from the Alternative drop-down list.
	 7.	 Click OK.
	 8.	 Back in the original dialog box, click OK.
To create a boxplot, replace step 8 with the following steps 8 
through 10:
	 8.	 Back in the original dialog box, click Graphs.
	 9.	 In the Paired t-Graphs dialog box (not shown), check Box-
plots of data and then click OK.
	10.	 Back in the original dialog box, click OK.
For a one-tail test, select less than or greater than in step 6.
Confidence Interval Estimate for the  
Mean Difference
Use the instructions for the paired t test, which computes a confi-
dence interval estimate as part of the analysis.
MG10.3  Comparing the Proportions 
of Two Independent 
Populations
Z Test for the Difference Between  
Two Proportions
Use 2 Proportions.
For example, to perform the Figure 10.13 Z test for the hotel guest 
satisfaction survey on page 398, select Stat ➔ Basic Statistics ➔ 
2 Proportions. In the 2 Proportions (Test and Confidence Inter-
val) dialog box (shown below):
	 1.	Click Summarized data.
	 2.	In the First row, enter 163 in the Events box and 227 in the 
Trials box.
	 3.	In the Second row, enter 154 in the Events box and 262 in 
the Trials box.
	 4.	Click Options.
In the 2 Proportions - Options dialog box (shown in the right column):
	 5.	 Enter 95.0 in the Confidence level box.
	 6.	 Enter 0.0 in the Test difference box.
	 7.	 Select not equal from the Alternative drop-down list.
	 8.	 Check Use pooled estimate of p for test.
	 9.	 Click OK.
	10.	 Back in the 2 Proportions (Test and Confidence Interval) 
­dialog box, click OK.
Confidence Interval Estimate for the Difference 
Between Two Proportions
Use the instructions for the Z test for the difference between two 
proportions, which computes a confidence interval estimate as part 
of the analysis.
MG10.4  f Test for the Ratio of Two 
Variances
Use 2 Variances.
For example, to perform the Figure 10.14 F test for the two end-
cap locations on page 404, open to the COLA worksheet. Select 
Stat ➔ Basic Statistics ➔ 2 Variances. In the 2 Variances (Test 
and Confidence Interval) dialog box (shown below):
	 1.	 Select Samples in different columns from the Data drop-
down list and press Tab.
	 2.	 Double-click C1  Beverage in the variables list to add  
Beverage to the First box.
	 3.	 Double-click C2  Produce in the variables list to add  
Produce to the Second box.
	 4.	 Click Graphs.
In the 2 Variances - Graphs dialog box (not shown):
	 5.	Clear all check boxes.
	 6.	Click OK.
	 7.	Back in the 2 Variances (Test and Confidence Interval)  
dialog box, click OK.

	
Chapter 10 Minitab Guide	
421
For summarized data, select Sample standard deviations or 
Sample variances in step 1 and enter the sample size and the sam-
ple statistics for the two variables in lieu of steps 2 and 3.
For stacked data, use these replacement steps 1 through 3:
	 1.	 Select Samples in one column from the Data drop-down list.
	 2.	 Enter the name of the column that contains the measurement 
in the Samples box.
	 3.	 Enter the name of the column that contains the sample names 
in the Subscripts box.
If you use an older version of Minitab, you will see a 2 Vari-
ances dialog box instead of the 2 Variances (Test and Confidence 
Interval) dialog box. This older dialog box is similar, and you click 
either Samples in different columns or Summarized data and then 
make entries similar to the ones listed in this section. The results cre-
ated will differ slightly from the results shown in Figure 10.14.

422
U s i n g  S tat i s t i c s
The Means to Find Differences  
at Arlington’s
The senior management of Arlington’s, a general merchandiser that competes 
with discount and wholesale club retailers, has just completed a new strategic 
plan. Among other things, that plan identifies the boosting of mobile electronics 
sales as an important goal for the future. As a member of the sales team, you are 
eager to begin implementing this goal, especially as a key regional competitor, 
Whitney Wireless, has floundered as it awaits integration with the electronics  
retailer Good Tunes & More (see the Chapter 1 Using Statistics scenario).
Your team has already explored a number of possibilities but has decided to 
explore how different in-store locations might affect the sales of mobile electron-
ics merchandise. Your team wants to design an experiment in which mobile elec-
tronics in selected stores will be sold in one of these three new in-store locations 
rather than in the current in-aisle location: at the front of the store near weekly 
specials, in an end-of-aisle (end-cap) special kiosk display, or adjacent to the 
­Expert Counter that is staffed with specially trained salespeople.
You select 20 Arlington’s stores that have similar annual sales and divide the 
stores into four groups of five stores each. To each group, you assign a different in-
store sales location for mobile electronics (either the current in-aisle, “front,” “kiosk,” 
or “expert” locations). How would you determine if varying the locations had an effect 
on mobile electronics sales? If you also wanted to explore the effects of permitting 
mobile ­payment methods to buy this type of merchandise, could you design an experi-
ment that examined this second factor as it examined the effects of in-store location?
contents
11.1  The Completely Randomized 
Design: One-Way ANOVA
11.2  The Randomized Block 
Design
11.3  The Factorial Design:  
Two-Way ANOVA
11.4  Fixed Effects, Random 
Effects, and Mixed Effects 
Models (online)
Using Statistics: The Means 
to Find Differences at Arlington’s 
Revisited
Chapter 11 Excel Guide
Chapter 11 Minitab Guide
Objectives
To introduce the basic concepts of 
experimental design
To learn to use the one-way 
analysis of variance to test for 
differences among the means of 
several groups
To learn when and how to use a 
randomized block design
To learn to use the two-way 
analysis of variance and interpret 
the interaction effect
To learn to perform multiple 
comparisons in a one-way 
analysis of variance, a 
randomized block design, and a 
two-way analysis of variance
Analysis of Variance
11
Chapter
Pavel L Photo and Video/Shutterstock

	
11.1  The Completely Randomized Design: One-Way ANOVA 	
423
C
omparing possible differences has been the subject of the statistical methods dis-
cussed in the previous two chapters. In the one-sample tests of Chapter 9, the com-
parison is to a standard, such as a certain mean weight for a cereal box being filled by 
a production line. In Chapter 10, the comparison is between samples taken from two popula-
tions. Analysis of variance, known by the acronym ANOVA, allows statistical comparison 
among samples taken from many populations.
In ANOVA, the comparison is typically the result of an experiment. For example, the man-
agement of a general merchandiser might be brainstorming ways of improving sales of mobile 
electronics items. At Arlington’s, the management decided to try selling those items in four dif-
ferent in-store locations and then observe what the sales would be in each of those locations. The 
basis for an ANOVA experiment is called the factor, which in the Arlington’s scenario is in-store 
location. Used in this way, the statistical use of factor complements everyday business use of the 
word, such as in the question “How much of a factor is in-store location in determining mobile 
electronics sales?” that the Arlington’s sales management team may have asked.
The actual different locations (in-aisle, “front,” “kiosk,” and “expert”) are the levels of 
the factor. Levels of a factor are analogous to the categories of a categorical variable, but you 
call in-store location a factor and not a categorical variable because the variable under study is 
mobile electronics sales. Levels provide the basis of comparison by dividing the variable un-
der study into groups. In the Arlington’s scenario, the groups are the stores selling the mobile 
electronics in-aisle, the stores selling that merchandise at the “front,” the stores selling that 
merchandise in “kiosks,” and stores selling the merchandise adjacent to the “experts.”
When performing ANOVA analysis, among the types of experiments that can be con-
ducted are:
 • Completely randomized design: An experiment with only one factor.
 • Randomized block design: An experiment in which the members of each group have 
been placed in blocks either by being matched or subjected to repeated measurements as 
was done with the two populations of a paired t test (discussed in Section 10.2).
 • Factorial design: An experiment in which more than one factor is considered. This chap-
ter discusses two-way ANOVA that involves two factors as an example of this type of 
design. (Arlington’s considering the effects of allowing mobile payments while also 
experimenting with in-store location would be an example of a factorial design.)
Determining the type of design and the factor or factors the design uses becomes an additional 
step in the Define task of the DCOVA framework when performing ANOVA analysis.
While ANOVA literally does analyze variation, the purpose of ANOVA is to reach conclu-
sions about possible differences among the means of each group, analogous to the hypothesis 
tests of the previous chapter. Every ANOVA design uses samples that represent each group 
and subdivides the total variation observed across all samples (all groups) toward the goal of 
analyzing possible differences among the means of each group. How this subdivision, called 
partitioning, works is a function of the design being used, but total variation, represented by 
the quantity sum of squares total (SST), will always be the starting point. As with other sta-
tistical methods, ANOVA requires making assumptions about the populations that the groups 
represent. While these assumptions are discussed on page 433 as part of Section 11.1, the as-
sumptions apply for all of the ANOVA methods discussed in this chapter.
Student Tip
ANOVA is also related 
to regression, a topic dis-
cussed later in this book. 
Because of ANOVA’s 
special relationship with 
both hypothesis testing 
and regression, under-
standing the foundational 
concepts of ANOVA will 
prove very helpful in un-
derstanding other types 
of analysis presented in 
Chapters 13 through 17.
The completely randomized design is the ANOVA method that analyzes a single factor. You 
execute this design using the statistical method one-way ANOVA. One-way ANOVA is a two-
part process. You first determine if there is a significant difference among the group means. If 
you reject the null hypothesis that there is no difference among the means, you continue with a 
second method that seeks to identify the groups whose means are significantly different from 
the other group means.
11.1  The Completely Randomized Design:  
One-Way ANOVA

424	
Chapter 11  Analysis of Variance
Analyzing Variation in One-Way ANOVA
In one-way ANOVA, to analyze variation towards the goal of determining possible differences 
among the group means, you partition the total variation into variation that is due to differences 
among the groups and variation that is due to differences within the groups (see Figure 11.1). 
The within-group variation (SSW) measures random variation. The among-group variation 
(SSA) measures differences from group to group. The symbol n represents the number of val-
ues in all groups and the symbol c represents the number of groups.
Assuming that the c groups represent populations whose values are randomly and inde-
pendently selected, follow a normal distribution, and have equal variances, the null hypothesis 
of no differences in the population means:
H0: m1 = m2 = . . . = mc
is tested against the alternative that not all the c population means are equal:
H1: Not all mj are equal 1where j = 1,  2, c ,  c2.
To perform an ANOVA test of equality of population means, you subdivide the total varia-
tion in the values into two parts—that which is due to variation among the groups and that 
which is due to variation within the groups. The total variation is represented by the sum of 
squares total (SST). Because the population means of the c groups are assumed to be equal 
under the null hypothesis, you compute the total variation among all the values by summing 
the squared differences between each individual value and the grand mean, X. The grand 
mean is the mean of all the values in all the groups combined. Equation (11.1) shows the com-
putation of the total variation.
F i g u r e  1 1 . 1
Partitioning the total 
variation in a completely 
randomized design
Partitioning the Total Variation
SST = SSA + SSW
Among-Group
Variation (SSA)
df = c – 1
Within-Group
Variation (SSW )
df = n – c
Total Variation 
(SST )
df = n – 1
If using Excel, always 
organize multiple-sample 
data as unstacked data, one 
column per group. (Some 
Minitab procedures work best 
with stacked data.) For more 
information about unstacked 
(and stacked) data, see  
page 77.
Student Tip
Another way of stating 
the alternative hypothesis, 
H1, is that at least one 
population mean is differ-
ent from the others.
Total Variation in One-Way Anova
	
SST = a
c
j = 1 a
nj
i = 1
1Xij - X22	
(11.1)
	
where
 X =
a
c
j = 1 a
nj
i = 1
Xij
n
= grand mean
 Xij = ith value in group j
 nj = number of values in group j
 n = total number of values in all groups combined 
1that is,  n = n1 + n2 + g +  nc2
 c = number of groups

	
11.1  The Completely Randomized Design: One-Way ANOVA 	
425
You compute the among-group variation, usually called the sum of squares among 
groups (SSA), by summing the squared differences between the sample mean of each group, 
Xj, and the grand mean, X, weighted by the sample size, nj, in each group. Equation (11.2) 
shows the computation of the among-group variation.
Student Tip
Remember that a sum of 
squares (SS) cannot be 
negative.
Among-Group Variation in One-Way Anova
	
SSA = a
c
j = 1
nj1Xj - X22	
(11.2)
	
where
 c = number of groups
 nj = number of values in group j
 Xj = sample mean of group j
 X = grand mean
The within-group variation, usually called the sum of squares within groups (SSW), 
measures the difference between each value and the mean of its own group and sums the 
squares of these differences over all groups. Equation (11.3) shows the computation of the 
within-group variation.
Within-Group Variation in One-Way Anova
	
SSW = a
c
j = 1 a
nj
i = 1
1Xij - Xj22	
(11.3)
	
where
 Xij = ith value in group j
 Xj = sample mean of group j
Because you are comparing c groups, there are c - 1 degrees of freedom associated 
with the sum of squares among groups. Because each of the c groups contributes nj - 1 
degrees of freedom, there are n - c degrees of freedom associated with the sum of squares 
within groups. In addition, there are n - 1 degrees of freedom associated with the sum of 
squares total because you are comparing each value, Xij, to the grand mean, X, based on all 
n values.
If you divide each of these sums of squares by its respective degrees of freedom, you have 
three variances, which in ANOVA are known as mean squares: MSA (mean square among), 
MSW (mean square within), and MST (mean square total).

426	
Chapter 11  Analysis of Variance
F Test for Differences Among More Than Two Means
To determine if there is a significant difference among the group means, you use the F test for 
differences among more than two means. If the null hypothesis is true and there are no differ-
ences among the c group means, MSA, MSW, and MST, will provide estimates of the overall 
variance in the population. Thus, to test the null hypothesis:
H0: m1 = m2 = g = mc
against the alternative:
H1: Not all mj are equal (where j = 1, 2, c ,  c)
you compute the one-way ANOVA FSTAT test statistic as the ratio of MSA to MSW, as in  
Equation (11.5).
Mean Squares in One-Way Anova
	
MSA =
SSA
c - 1	
(11.4a)
	
MSW = SSW
n - c	
(11.4b)
	
MST =
SST
n - 1	
(11.4c)
Student Tip
Remember, mean square 
is just another term for 
variance that is used in 
the analysis of variance. 
Also, since the mean 
square is equal to the 
sum of squares divided 
by the degrees of free-
dom, a mean square can 
never be negative.
Student Tip
The test statistic com-
pares mean squares 
(the variances) because 
one-way ANOVA reaches 
conclusions about pos-
sible differences among 
the means of c groups 
by examining variances.
One-Way Anova FSTAT Test Statistic
	
FSTAT = MSA
MSW	
(11.5)
The FSTAT test statistic follows an F distribution, with c - 1 degrees of freedom in the 
numerator and n - c degrees of freedom in the denominator.
For a given level of significance, a, you reject the null hypothesis if the FSTAT test statistic 
computed in Equation (11.5) is greater than the upper-tail critical value, Fa, from the F dis-
tribution with c - 1 degrees of freedom in the numerator and n - c in the denominator (see 
Table E.5). Thus, as shown in Figure 11.2, the decision rule is
Reject H0 if FSTAT 7 Fa;
otherwise,  do not reject H0.
(1 – α)
0
F
Fα
α
Region of
Rejection
Region of
Nonrejection
Critical
Value
F i g u r e  1 1 . 2
Regions of rejection and 
nonrejection when using 
ANOVA

	
11.1  The Completely Randomized Design: One-Way ANOVA 	
427
If the null hypothesis is true, the computed FSTAT test statistic is expected to be approxi-
mately equal to 1 because both the numerator and denominator mean square terms are estimat-
ing the overall variance in the population. If H0 is false (and there are differences in the group 
means), the computed FSTAT test statistic is expected to be larger than 1 because the numerator, 
MSA, is estimating the differences among groups in addition to the overall variability in the 
values, while the denominator, MSW, is measuring only the overall variability in the values. 
Therefore, you reject the null hypothesis at a selected level of significance, a, only if the com-
puted FSTAT test statistic is greater than Fa, the upper-tail critical value of the F distribution 
having c - 1 and n - c degrees of freedom.
Table 11.1 presents the ANOVA summary table that is typically used to summarize the 
results of a one-way ANOVA. The table includes entries for the sources of variation (among 
groups, within groups, and total), the degrees of freedom, the sums of squares, the mean 
squares (the variances), and the computed FSTAT test statistic. The table may also include the 
p-value, the probability of having an FSTAT value as large as or larger than the one computed, 
given that the null hypothesis is true. The p-value allows you to reach conclusions about the 
null hypothesis without needing to refer to a table of critical values of the F distribution. If the 
p-value is less than the chosen level of significance, a, you reject the null hypothesis.
T a b l e  1 1 . 1
ANOVA Summary Table
Source
Degrees of 
Freedom
Sum of  
Squares
Mean Square 
(Variance)
F
Among groups
c - 1
SSA
MSA =
SSA
c - 1
FSTAT = MSA
MSW
Within groups
n - c
SSW
MSW =
SSW
n - c
Total
n - 1
SST
To illustrate the one-way ANOVA F test, return to the Arlington’s scenario (see page 422). 
You define the business problem as whether significant differences exist in the mobile elec-
tronics sales for the four different in-store locations, the four groups for the ANOVA analysis.
To test the comparative effectiveness of the four in-store locations, you conduct a 60-day 
experiment at 20 same-sized stores that have similar storewide net sales. You randomly as-
sign five stores to use the current in-aisle location, five stores to use the front of the store near 
weekly specials, five stores to use the end-cap special kiosk display, and five stores to use the 
location adjacent to the Expert Counter. At the end of the experiment, you organize the mobile 
electronics sales data by group and store the data in unstacked format in  Mobile Electronics . 
Figure 11.3 presents that unstacked data, along with the sample mean and the sample standard 
deviation for each group.
F i g u r e  1 1 . 3
Mobile electronic 
sales ($000), sample 
means, and sample 
standard deviations for 
four different in-store 
locations
Figure 11.3 shows differences among the sample means for the mobile electronics sales 
for the four in-store locations. For the original in-aisle location, mean sales were $29.982 thou-
sands, whereas mean sales at the three new locations varied from $30.334 thousands (“expert” 
location) to $30.912 thousands (“kiosk” location) to $31.994 thousands (“front” location).

428	
Chapter 11  Analysis of Variance
Differences in the mobile electronic sales for the four in-store locations can also be pre-
sented visually. In Figure 11.4, the Minitab cell means plot displays the four sample means 
and connects the sample means with a straight line. In the same figure, the Excel scatter plot 
presents the mobile electronics sales at each store in each group, permitting you to observe 
differences within each location as well as among the four locations. (In this example, because 
the difference within each group is slight, the points for each group overlap and blur together.)
Having observed that the four sample means appear to be different, you use the F test for 
differences among more than two means to determine if these sample means are sufficiently 
different to conclude that the population means are not all equal. The null hypothesis states 
that there is no difference in the mean sales among the four in-store locations:
H0: m1 = m2 = m3 = m4
The alternative hypothesis states that at least one of the in-store location mean sales differs 
from the other means:
H1: Not all the means are equal.
To construct the ANOVA summary table, you first compute the sample means in each 
group (see Figure 11.3 on page 427). Then you compute the grand mean by summing all 20 
values and dividing by the total number of values:
X =
a
c
j = 1
 a
nj
j = 1
Xij
n
= 616.12
20
 = 30.806
Then, using Equations (11.1) through (11.3) on pages 424–425, you compute the sum of 
squares:
SSA = a
c
j = 1
nj 1Xj - X22 = 152129.982 - 30.80622 + 152131.994 - 30.80622
+ 152130.912 - 30.80622 + 152130.334 - 30.80622
= 11.6217
Student Tip
If the sample sizes in 
each group were larger, 
you could construct 
stem-and-leaf displays, 
boxplots, and normal 
probability plots as addi-
tional ways of visualizing 
the sales data.
F i g u r e  1 1 . 4
Excel scatter plot and Minitab main effects plot of mobile electronics sales for four in-store locations
In the Excel scatter plot, the locations in-aisle, front, kiosk, and expert  
were relabeled 1, 2, 3, and 4 in order to use the scatter plot chart type.

	
11.1  The Completely Randomized Design: One-Way ANOVA 	
429
SSW = a
c
j = 1
 a
nj
i = 1
1Xij - Xj22
= 130.06 - 29.98222 + g +  129.74 - 29.98222 +  132.22 - 31.99422 + g
+ 132.29 - 31.99422 + 130.78 - 30.91222 + g + 131.13 - 30.91222 
+ 130.33 - 30.33422 + g + 130.55 - 30.33422
= 0.7026
SST = a
c
j = 1
  a
nj
i = 1
1Xij - X22
= 130.06 - 30.80622 + 129.96 - 30.80622 + g + 130.55 - 30.80622
= 12.3243
You compute the mean squares by dividing the sum of squares by the corresponding degrees of 
freedom [see Equation (11.4) on page 426]. Because c = 4 and n = 20,
 MSA =
SSA
c - 1 = 11.6217
4 - 1
= 3.8739
 MSW = SSW
n - c = 0.7026
20 - 4 = 0.0439
so that using Equation (11.5) on page 426,
FSTAT = MSA
MSW = 3.8739
0.0439 = 88.2186
Because you are trying to determine whether MSA is greater than MSW, you only reject H0 
if FSTAT is greater than the upper critical value of F. For a selected level of significance, a, you 
find the upper-tail critical value, Fa, from the F distribution using Table E.5. A portion of Table 
E.5 is presented in Table 11.2. In the in-store location sales experiment, there are 3 degrees 
of freedom in the numerator and 16 degrees of freedom in the denominator. Fa, the upper-tail 
critical value at the 0.05 level of significance, is 3.24.
T a b l e  1 1 . 2
Finding the Critical 
Value of F with 3 
and 16 Degrees of 
Freedom at the 0.05 
Level of Significance
Cumulative Probabilities = 0.95 
Upper-Tail Area = 0.05 
Numerator df1
Denominator df2
1
2
3
4
5
6
7
8
9
f
f
f
f
f
f
f
f
f
f
11
4.84
3.98
3.59
3.36
3.20
3.09
3.01
2.95
2.90
12
4.75
3.89
3.49
3.26
3.11
3.00
2.91
2.85
2.80
13
4.67
3.81
3.41
3.18
3.03
2.92
2.83
2.77
2.71
14
4.60
3.74
3.34
3.11
2.96
2.85
2.76
2.70
2.65
15
4.54
3.68
3.29
3.06
2.90
2.79
2.71
2.64
2.59
16
4.49
3.63
3.24
3.01
2.85
2.74
2.66
2.59
2.54
Source: Extracted from Table E.5.
Because FSTAT = 88.2186 is greater than Fa = 3.24, you reject the null hypothesis (see 
Figure 11.5). You conclude that there is a significant difference in the mean sales for the four 
in-store locations.

430	
Chapter 11  Analysis of Variance
Figure 11.6 shows the ANOVA results for the in-store location sales experiment, including 
the p-value. In Figure 11.6, what Table 11.1 (see page 427) labels Among Groups is labeled 
Between Groups in the Excel worksheet. Minitab labels Among Groups as Factor and Within 
Groups as Error.
F i g u r e  1 1 . 5
Regions of rejection 
and nonrejection for the 
one-way ANOVA at the 
0.05 level of significance, 
with 3 and 16 degrees of 
freedom
0
3.24
F
.05
.95
Region of
Rejection
Region of
Nonrejection
Critical
Value
The p-value, or probability of getting a computed FSTAT statistic of 88.2186 or larger when 
the null hypothesis is true, is 0.0000. Because this p-value is less than the specified a of 0.05, 
you reject the null hypothesis. The p-value of 0.0000 indicates that there is a 0.00% chance 
of observing differences this large or larger if the population means for the four in-store loca-
tions are all equal. After performing the one-way ANOVA and finding a significant difference 
among the in-store locations, you still do not know which in-store locations differ. All you 
know is that there is sufficient evidence to state that the population means are not all the same. 
In other words, one or more population means are significantly different. To determine which 
in-store locations differ, you can use a multiple comparisons procedure such as the Tukey-
Kramer procedure.
Multiple Comparisons: The Tukey-Kramer Procedure
In the Arlington’s scenario on page 422, you used the one-way ANOVA F test to determine  
that there was a difference among the suppliers. The next step is to construct multiple  
comparisons to test the null hypothesis that the differences in the means of all pairs of in-store 
locations are equal to 0.
Although many procedures are available (see references 5, 6, and 10), this text uses the 
Tukey-Kramer multiple comparisons procedure for one-way ANOVA to determine which 
of the c means are significantly different. This procedure enables you to simultaneously make 
comparisons between all pairs of groups. The procedure consists of the following four steps:
1.  Compute the absolute mean differences, Xj - Xj′  (where j refers to group j, j′ refers 
to group j′, and j ≠j′), among all pairs of sample means [c(c - 1)>2 pairs].
Student Tip
You have an a level of 
risk in the entire set of 
comparisons not just a 
single comparison.
F i g u r e  1 1 . 6
Excel and Minitab ANOVA results for the in-store location sales experiment
The formulas in the Excel 
results worksheet are not 
shown in Figure 11.6 but are 
discussed in Section EG11.1 
and the Short Takes for 
Chapter 11.

	
11.1  The Completely Randomized Design: One-Way ANOVA 	
431
2.  Compute the critical range for the Tukey-Kramer procedure, using Equation (11.6). 
If the sample sizes differ, compute a critical range for each pairwise comparison of 
sample means.
Critical Range for the Tukey-Kramer Procedure
	
Critical range = QaB
MSW
2
a 1
nj
+ 1
nj′
b
(11.6)
where
nj = the sample size in group j
nj′ = the sample size in group j′
Qa = the upper-tail critical value from a Studentized range distribution having c degrees 
of freedom in the numerator and n - c degrees of freedom in the denominator.
Student Tip
Table E.7 contains the 
critical values for the 
Studentized range  
distribution.
3.  Compare each of the c1c - 12>2 pairs of means against its corresponding critical 
range. Declare a specific pair significantly different if the absolute difference in the 
sample means, Xj - Xj′, is greater than the critical range.
4.  Interpret the results.
In the mobile electronics sales example, there are four in-store locations. Thus, there are 
414 - 12>2 = 6 pairwise comparisons. To apply the Tukey-Kramer multiple comparisons 
procedure, you first compute the absolute mean differences for all six pairwise comparisons:
	
1.  X1 - X2 = 29.982 - 31.994 = 2.012
	
2.  X1 - X3 = 29.982 - 30.912 = 0.930
	
3.  X1 - X4 = 29.982 - 30.334 = 0.352
	
4.  X2 - X3 = 31.994 - 30.912 = 1.082
	
5.  X2 - X4 = 31.994 - 30.334 = 1.660
	
6.  X3 - X4 = 30.912 - 30.334 = 0.578
You then compute only one critical range because the sample sizes in the four groups are 
equal. (Had the sample sizes in some of the groups been different, you would compute several 
critical ranges.) From the ANOVA summary table (Figure 11.6 on page 430), MSW = 0.0439 
and nj = nj′ = 5. From Table E.7, for a = 0.05,  c = 4, and n - c = 20 - 4 = 16,  Qa, 
the upper-tail critical value of the test statistic, is 4.05 (see Table 11.3).
T a b l e  1 1 . 3
Finding the Studentized 
Range, Qa, Statistic for 
a = 0.05, with 4 and  
16 Degrees of Freedom
Cumulative Probabilities = 0.95 
Upper@Tail Area = 0.05 
Numerator df1
Denominator df2
2
3
4
5
6
7
8
9
f
f
f
f
f
f
f
f
f
11
3.11
3.82
4.26
4.57
4.82
5.03
5.20
5.35
12
3.08
3.77
4.20
4.51
4.75
4.95
5.12
5.27
13
3.06
3.73
4.15
4.45
4.69
4.88
5.05
5.19
14
3.03
3.70
4.11
4.41
4.64
4.83
4.99
5.13
15
3.01
3.67
4.08
4.37
4.60
4.78
4.94
5.08
16
3.00
3.65
4.05
4.33
4.56
4.74
4.90
5.03
Source: Extracted from Table E.7.

432	
Chapter 11  Analysis of Variance
From Equation (11.6),
Critical range = 4.05B a 0.0439
2
b a 1
5 + 1
5b = 0.3795
Because the absolute mean difference for five pairs (1, 2, 4, 5, and 6) is greater than 0.3795, 
you can conclude that there is a significant difference between the mobile electronic sales 
means of those pairs. Because the absolute mean difference for pair 3 (in-aisle and expert loca-
tions) is 0.352, which is less than 0.3795, you conclude that there is no evidence of a difference 
in the means of those two locations. These results allow you to estimate that the population 
mean sales for mobile electronics items will be higher at the front location than any other loca-
tion and that the population mean sales for mobile electronics items at kiosk locations will be 
higher when compared to either the in-aisle or expert locations. As a member of the Arling-
ton’s sales team, you would conclude that further study and experimentation with the in-store 
location of mobile electronics items is appropriate.
Figure 11.7 presents the Excel and Minitab results for the Tukey-Kramer procedure for the 
mobile electronics sales in-store location experiment. Note that by using a = 0.05, you are 
able to make all six of the comparisons with an overall error rate of only 5%.
The Figure 11.7 Excel results follow the steps used on pages 430–431 for evaluating the 
comparisons. The Minitab results show the comparisons in terms of interval estimates. Each 
interval is computed. Any interval that does not include 0 is considered significant. Thus all the 
comparisons are significant except for the comparison of in-aisle to expert store location. The 
interval for that comparison includes 0 since the lower limit is -0.0275 and the upper limit is 
0.7315.
The Analysis of Means (ANOM)
The analysis of means (ANOM) provides an alternative approach that allows you to determine 
which, if any, of the c groups has a mean significantly different from the overall mean of all 
the group means combined. The ANOM online topic explains this alternative approach and 
illustrates its use.
F i g u r e  1 1 . 7
Excel and Minitab Tukey-Kramer procedure results for the in-store location sales experiment
The formulas in the Excel 
results worksheet are not 
shown in Figure 11.6 but are 
discussed in Section EG11.1 
and the Short Takes for 
Chapter 11.

	
11.1  The Completely Randomized Design: One-Way ANOVA 	
433
ANOVA Assumptions
In Chapters 9 and 10, you learned about the assumptions required in order to use each hypothesis- 
testing procedure and the consequences of departures from these assumptions. To use the one-
way ANOVA F test, you must make the following assumptions about the populations:
 • Randomness and independence
 • Normality
 • Homogeneity of variance
The first assumption, randomness and independence, is critically important. The va-
lidity of any experiment depends on random sampling and/or the randomization process. To 
avoid biases in the outcomes, you need to select random samples from the c groups or use the 
randomization process to randomly assign the items to the c levels of the factor. Selecting a 
random sample or randomly assigning the levels ensures that a value from one group is inde-
pendent of any other value in the experiment. Departures from this assumption can seriously 
affect inferences from the ANOVA. These problems are discussed more thoroughly in refer-
ences 5 and 10.
The second assumption, normality, states that the sample values in each group are from 
a normally distributed population. Just as in the case of the t test, the one-way ANOVA F test 
is fairly robust against departures from the normal distribution. As long as the distributions are 
not extremely different from a normal distribution, the level of significance of the ANOVA F 
test is usually not greatly affected, particularly for large samples. You can assess the normality 
of each of the c samples by constructing a normal probability plot or a boxplot.
The third assumption, homogeneity of variance, states that the variances of the c groups 
are equal (i.e., s2
1 = s2
2 = g = s2
c). If you have equal sample sizes in each group, infer-
ences based on the F distribution are not seriously affected by unequal variances. However, if 
you have unequal sample sizes, unequal variances can have a serious effect on inferences from 
the ANOVA procedure. Thus, when possible, you should have equal sample sizes in all groups. 
You can use the Levene test for homogeneity of variance to test whether the variances of the c 
groups are equal.
When only the normality assumption is violated, you can use the Kruskal-Wallis rank test, 
a nonparametric procedure discussed in Section 12.5. When only the homogeneity-of-variance 
assumption is violated, you can use procedures similar to those used in the separate-variance 
t test of Section 10.1(see references 1 and 2). When both the normality and homogeneity-of-
variance assumptions have been violated, you need to use an appropriate data transformation 
that both normalizes the data and reduces the differences in variances (see reference 6) or use a 
more general nonparametric procedure (see references 2 and 3).
Levene Test for Homogeneity of Variance
Although the one-way ANOVA F test is relatively robust with respect to the assumption of 
equal group variances, large differences in the group variances can seriously affect the level 
of significance and the power of the F test. One powerful yet simple procedure for testing the 
equality of the variances is the modified Levene test (see references 1 and 7). To test for the 
homogeneity of variance, you use the following null hypothesis:
H0: s2
1 = s2
2 = g = s2
c
against the alternative hypothesis:
H1: Not all s2
j  are equal 1j = 1, 2, 3, c , c2
To test the null hypothesis of equal variances, you first compute the absolute value of the 
difference between each value and the median of the group. Then you perform a one-way 
ANOVA on these absolute differences. Most statisticians suggest using a level of significance 
of a = 0.05 when performing the ANOVA. To illustrate the modified Levene test, return to the 
Figure 11.3 data and summary statistics on page 427 for the Arlington’s scenario concerning 
Student Tip
To use the one-way 
ANOVA F test, the vari-
able to be analyzed must 
either be interval or ratio 
scaled.
Student Tip
Remember when per-
forming the Levene test 
that you are conducting 
a one-way ANOVA on 
the absolute differences 
from the median in each 
group, not on the actual 
values themselves.

434	
Chapter 11  Analysis of Variance
Using the absolute differences given in Table 11.4, you perform a one-way ANOVA (see 
Figure 11.8).
In-Aisle  
(Median = 29.96)
Front  
(Median = 32.13)
Kiosk  
(Median = 30.91)
Expert  
(Median = 30.29)
30.06 - 29.96 = 0.10
32.22 - 32.13 = 0.09
30.78 - 30.91 = 0.13
30.33 - 30.29 = 0.04
29.96 - 29.96 = 0.00
31.47 - 32.13 = 0.66
30.91 - 30.91 = 0.00
30.29 - 30.29 = 0.00
30.19 - 29.96 = 0.23
32.13 - 32.13 = 0.00
30.79 - 30.91 = 0.12
30.25 - 30.29 = 0.04
29.96 - 29.96 = 0.00
31.86 - 32.13 = 0.27
30.95 - 30.91 = 0.04
30.25 - 30.29 = 0.04
29.74 - 29.96 = 0.22
32.29 - 32.13 = 0.16
31.13 - 30.91 = 0.22
30.55 - 30.29 = 0.26
T a b l e  1 1 . 4
Absolute 
Differences from 
the Median 
Sales for Four 
Locations
F i g u r e  1 1 . 8
Excel and Minitab 
Levene test results for 
the absolute differences 
for the in-store location 
sales experiment
From the Figure 11.8 results, observe that FSTAT = 1.0556. (The Excel worksheet labels 
this value F and Minitab labels the value Test statstic.) Because FSTAT = 1.0556 6 3.2389 (or 
the p@value = 0.3953 7 0.05), you do not reject H0. There is insufficient evidence of a sig-
nificant difference among the four variances. In other words, it is reasonable to assume that the 
four in-store locations have an equal amount of variability in sales. Therefore, the homogeneity- 
of-variance assumption for the ANOVA procedure is justified.
Example 11.1 illustrates another example of the one-way ANOVA.
Example 11.1
ANOVA of the 
Speed of Drive-
Through Service at 
Fast-Food Chains
For fast-food restaurants, the drive-through window is an important revenue source. The chain 
that offers the fastest service is likely to attract additional customers. Each year QSR Maga-
zine, www.qsrmagazine.com, publishes its results of a survey of drive-through service times 
(from menu board to departure) at fast-food chains. In a recent year, the mean time was 129.75 
seconds for Wendy’s, 149.69 seconds for Taco Bell, 201.33 seconds for Burger King, 188.83 
seconds for McDonald’s, and 190.06 seconds for Chick-fil-A. Suppose the study was based on 
20 customers for each fast-food chain. At the 0.05 level of significance, is there evidence of a 
difference in the mean drive-through service times of the five chains?
Table 11.5 contains the ANOVA table for this problem.
T a b l e  1 1 . 5
ANOVA Summary 
Table of Drive-
Through Service 
Times at Fast-Food 
Chains
Source
Degrees of 
Freedom
Sum of 
Squares
Mean 
Squares
F
p-value
Among chains
  4
75,048.74
18,762.185
143.66
0.0000
Within chains
95
12,407.00
   130.60
the in-store location sales experiment. Table 11.4 summarizes the absolute differences from 
the median of each location.

	
11.1  The Completely Randomized Design: One-Way ANOVA 	
435
Solution
 H0: m1 = m2 = m3 = m4 = m5 where 1 = Wendy>s,  2 = Taco Bell, 3 = Burger King,
 4 = McDonald>s, 5 = Chick@fil@A
 H1: Not all mj are equal       where j = 1, 2, 3, 4, 5
Decision rule: If the p-value 6 0.05, reject H0. Because the p-value is 0.0000, which is 
less than a = 0.05, reject H0. You have sufficient evidence to conclude that the mean drive-
through times of the five chains are not all equal.
To determine which of the means are significantly different from one another, use the 
Tukey-Kramer procedure [Equation (11.6) on page 431] to establish the critical range:
Critical value of Q with 5 and 95 degrees of freedom ≈3.92
 Critical range = QaB a MSW
2
b a 1
nj
+ 1
nj′
b = 13.922B a 130.6
2
b a 1
20 + 1
20b
 = 10.02
Any observed difference greater than 10.02 is considered significant. The mean drive-
through service times are different between Wendy’s (mean of 129.75 seconds) and Taco Bell, 
Burger King, McDonald’s, and Chick-fil-A and also between Taco Bell (mean of 149.69) and 
Burger King, McDonald’s, and Chick-fil-A. In addition, the mean drive-through service time 
is different between Burger King and McDonald’s, and between Burger King and Chick-fil-A.  
Thus, with 95% confidence, you can conclude that the estimated population mean drive-
through service time is faster for Wendy’s than for Taco Bell. In addition, the population mean 
service time for Wendy’s and for Taco Bell is faster than those of Burger King, McDonald’s, 
and Chick-fil-A. Also, the population mean drive-through service time for Burger King is 
slower than for McDonald’s and for Chick-Fil-A.
Problems for Section 11.1
Learning the Basics
11.1  An experiment has a single factor with five groups and 
seven values in each group.
a.	 How many degrees of freedom are there in determining the 
among-group variation?
b.	 How many degrees of freedom are there in determining the 
within-group variation?
c.	 How many degrees of freedom are there in determining the to-
tal variation?
11.2  You are working with the same experiment as in Problem 11.1.
a.	 If SSA = 60 and SST = 210, what is SSW?
b.	 What is MSA?
c.	 What is MSW?
d.	 What is the value of FSTAT?
11.3  You are working with the same experiment as in Problems 
11.1 and 11.2.
a.	 Construct the ANOVA summary table and fill in all values in 
the table.
b.	 At the 0.05 level of significance, what is the upper-tail critical 
value from the F distribution?
c.	 State the decision rule for testing the null hypothesis that all 
five groups have equal population means.
d.	 What is your statistical decision?
11.4  State the assumptions about the population to use a one-way 
ANOVA F test. What alternative tests can be used if
a.	 only the assumption of normality is violated.
b.	 only the assumption of homogeneity-of-variance is violated.
c.	 both the assumptions of normality and homogeneity-of-variance 
have been violated.
11.5  Consider an experiment with four groups, with eight values 
in each. For the ANOVA summary table below, fill in all the miss-
ing results:
Source
Degrees of 
Freedom
Sum of 
Squares
Mean 
Square 
(Variance)
F
Among  
  groups
c - 1 = ?
SSA = ?
MSA = 80
FSTAT = ?
Within  
  groups
n - c = ?
SSW = 560 MSW = ?
Total
n - 1 = ?
SST = ?
11.6  You are working with the same experiment as in  
Problem 11.5.
a.	 At the 0.05 level of significance, state the decision rule for testing 
the null hypothesis that all four groups have equal population means.

436	
Chapter 11  Analysis of Variance
b.	 What is your statistical decision?
c.	 At the 0.05 level of significance, what is the upper-tail critical 
value from the Studentized range distribution?
d.	 To perform the Tukey-Kramer procedure, what is the critical 
range?
Applying the Concepts
11.7  Accounting Today identified the top accounting firms in 
10 geographic regions across the United States. Even though all 
10 ­regions reported growth in 2012, the Capital, Great Lakes, 
­Mid-­Atlantic, and New England regions reported relatively similar 
combined growths, of 7.2%, 7.72%, 5.00%, and 5.03%, respec-
tively. A characteristic description of the accounting firms in the 
Capital, Great Lakes, Mid-Atlantic, and New England regions in-
cluded the number of partners in the firm. 
The file  AccountingPartners4  contains the number of part-
ners. (Data extracted from bit.ly/11XZNPr.)
a.	 At the 0.05 level of significance, is there evidence of a differ-
ence among the Capital, Great Lakes, Mid-Atlantic, and New 
England region accounting firms with respect to the mean 
number of partners?
b.	 If the results in (a) indicate that it is appropriate to do so, use 
the Tukey-Kramer procedure to determine which regions differ 
in the mean number of partners. Discuss your findings.
11.8  The more costly and time consuming it is to 
export and ­import, the more difficult it is for local 
companies to be competitive and to reach international markets. 
As part of an initial investigation exploring foreign market entry, 
10 countries were selected from each of four global regions. The 
cost associated with importing a standardized cargo of goods by 
sea transport in these ­countries (in US$ per container) is stored in  
 ForeignMarket2 . (Data extracted from doingbusiness.org/data.)
a.	 At the 0.05 level of significance, is there evidence of a ­difference 
in the mean cost of importing across the four global regions?
b.	 If appropriate, determine which global regions differ in mean 
cost of importing.
c.	 At the 0.05 level of significance, is there evidence of a ­difference 
in the variation in cost of importing among the four global  
regions?
d.	 Which global region(s) should you consider for foreign market 
entry? Explain.
11.9  A hospital conducted a study of the waiting time in its 
­emergency room. The hospital has a main campus and three 
­satellite locations. Management had a business objective of reduc-
ing waiting time for emergency room cases that did not require 
immediate attention. To study this, a random sample of 15 emer-
gency room cases that did not require immediate attention at each 
location were selected on a particular day, and the waiting times 
(measured from check-in to when the patient was called into the 
clinic area) were collected and stored in  ERWaiting .
a.	 At the 0.05 level of significance, is there evidence of a differ-
ence in the mean waiting times in the four locations?
b.	 If appropriate, determine which locations differ in mean 
­waiting time.
c.	 At the 0.05 level of significance, is there evidence of a ­difference 
in the variation in waiting time among the four locations?
11.10  A manufacturer of pens has hired an advertising agency 
to develop an advertising campaign for the upcoming holiday sea-
son. To prepare for this project, the research director decides to 
initiate a study of the effect of advertising on product perception. 
An experiment is designed to compare five different advertise-
ments. Advertisement A greatly undersells the pen’s characteris-
tics. Advertisement B slightly undersells the pen’s characteristics. 
Advertisement C slightly oversells the pen’s characteristics. Ad-
vertisement D greatly oversells the pen’s characteristics. Adver-
tisement E attempts to correctly state the pen’s characteristics. A 
sample of 30 adult respondents, taken from a larger focus group, 
is randomly assigned to the five advertisements (so that there 
are 6 respondents to each advertisement). After reading the ad-
vertisement and developing a sense of “product expectation,” all 
respondents unknowingly receive the same pen to evaluate. The 
respondents are permitted to test the pen and the plausibility of 
the advertising copy. The respondents are then asked to rate the 
pen from 1 to 7 (lowest to highest) on the product characteristic 
scales of appearance, durability, and writing performance. The 
combined scores of three ratings (appearance, durability, and 
writing performance) for the 30 respondents, stored in  Pen , are 
as follows:
A
B
C
D
E
15
16
  8
  5
12
18
17
  7
  6
19
17
21
10
13
18
19
16
15
11
12
19
19
14
  9
17
20
17
14
10
14
a.	 At the 0.05 level of significance, is there evidence of a 
­difference in the mean rating of the pens following exposure to 
five advertisements?
b.	 If appropriate, determine which advertisements differ in mean 
ratings.
c.	 At the 0.05 level of significance, is there evidence of a ­difference 
in the variation in ratings among the five advertisements?
d.	 Which advertisement(s) should you use, and which advertise- 
ment(s) should you avoid? Explain.
11.11  QSR has been reporting on the largest quick-serve and 
fast-casual brands in the United States for nearly 15 years. The file 
 QSR  contains the food segment (burger, chicken, pizza, or sand-
wich) and U.S. mean sales per unit ($ thousands) for each of 38 
quick-service brands. (Data extracted from bit.ly/16GJlEn.)
a.	 At the 0.05 level of significance, is there evidence of a differ-
ence in the mean U.S. mean sales per unit ($ thousands) among 
the food segments?
b.	 At the 0.05 level of significance, is there a difference in the 
variation in U.S. average sales per unit ($ thousands) among 
the food segments?
c.	 What effect does your result in (b) have on the validity of the 
results in (a)?
SELF 
Test 

	
11.1  The Completely Randomized Design: One-Way ANOVA 	
437
 
Kidney
 
Shrimp
Chicken 
Liver
 
Salmon
 
Beef
2.37
2.26
2.29
1.79
2.09
2.62
2.69
2.23
2.33
1.87
2.31
2.25
2.41
1.96
1.67
2.47
2.45
2.68
2.05
1.64
2.59
2.34
2.25
2.26
2.16
2.62
2.37
2.17
2.24
1.75
2.34
2.22
2.37
1.96
1.18
2.47
2.56
2.26
1.58
1.92
2.45
2.36
2.45
2.18
1.32
2.32
2.59
2.57
1.93
1.94
11.14  A sporting goods manufacturing company wanted to 
­compare the distance traveled by golf balls produced using four 
different designs. Ten balls were manufactured with each design 
and were brought to the local golf course for the club professional 
to test. The order in which the balls were hit with the same club 
from the first tee was randomized so that the pro did not know 
which type of ball was being hit. All 40 balls were hit in a short 
period of time, during which the environmental conditions were 
essentially the same. The results (distance traveled in yards) for 
the four ­designs are stored in  Golfball  and shown in the following 
table:
Design 1
Design 2
Design 3
Design 4
206.32
217.08
226.77
230.55
207.94
221.43
224.79
227.95
206.19
218.04
229.75
231.84
204.45
224.13
228.51
224.87
209.65
211.82
221.44
229.49
203.81
213.90
223.85
231.10
206.75
221.28
223.97
221.53
205.68
229.43
234.30
235.45
204.49
213.54
219.50
228.35
210.86
214.51
233.00
225.09
a.	 At the 0.05 level of significance, is there evidence of a differ-
ence in the mean distances traveled by the golf balls with dif-
ferent designs?
b.	 If the results in (a) indicate that it is appropriate to do so, use 
the Tukey-Kramer procedure to determine which designs differ 
in mean distances.
c.	 What assumptions are necessary in (a)?
d.	 At the 0.05 level of significance, is there evidence of a differ-
ence in the variation of the distances traveled by the golf balls 
with different designs?
e.	 What golf ball design should the manufacturing manager 
choose? Explain.
11.12  Researchers conducted a study to determine whether grad-
uates with an academic background in the discipline of leadership 
studies were better equipped with essential soft skills required 
to be successful in contemporary organizations than students 
with no leadership education and/or students with a certificate in 
leadership. The Teams Skills Questionnaire was used to capture  
students’ self-reported ratings of their soft skills. The researchers 
found the following:
Source
Degrees of 
Freedom
Sum of 
Squares
Mean 
Squares
F
Among groups
  2
  1.879
Within groups
297
31.865
Total
299
33.744
Group
N
Mean
No coursework in leadership
109
3.290
Certificate in leadership
  90
3.362
Degree in leadership
102
3.471
Source: Data Extracted from C. Brungardt, “The Intersection Between 
Soft Skill Development and Leadership Education,” Journal of  
Leadership Education, 10 (Winter 2011): 1–22.
a.	 Complete the ANOVA summary table.
b.	 At the 0.05 level of significance, is there evidence of a differ-
ence in the mean soft-skill score reported by different groups?
c.	 If the results in (b) indicate that it is appropriate, use the Tukey-
Kramer procedure to determine which groups differ in mean 
soft-skill score. Discuss your findings.
11.13  A pet food company has a business objective of expanding 
its product line beyond its current kidney- and shrimp-based cat 
foods. The company developed two new products, one based on 
chicken liver and the other based on salmon. The company con-
ducted an experiment to compare the two new products with its 
two existing ones, as well as a generic beef-based product sold at 
a supermarket chain.
For the experiment, a sample of 50 cats from the population 
at a ­local animal shelter was selected. Ten cats were randomly 
­assigned to each of the five products being tested. Each of the 
cats was then presented with 3 ounces of the selected food in a 
dish at feeding time. The researchers defined the variable to be 
measured as the number of ounces of food that the cat consumed 
within a ­10-minute time interval that began when the filled dish 
was ­presented. The results for this experiment are summarized in 
the table at top right and stored in  CatFood .
a.	 At the 0.05 level of significance, is there evidence of a dif-
ference in the mean amount of food eaten among the various 
products?
b.	 If appropriate, determine which products appear to differ 
­significantly in the mean amount of food eaten.
c.	 At the 0.05 level of significance, is there evidence of a differ-
ence in the variation in the amount of food eaten among the 
various products?
d.	 What should the pet food company conclude? Fully describe 
the pet food company’s options with respect to the products.

438	
Chapter 11  Analysis of Variance
Section 11.1 discussed how to use the one-way ANOVA F test to evaluate differences among 
the means of more than two independent groups. The randomized block design evaluates 
­differences among more than two groups in which the members of each group have been 
placed in blocks either by being matched or subjected to repeated measurements in the same 
way as in the paired t test ­(discussed in Section 10.2). Blocking removes variability due to in-
dividual differences so that the differences among the groups are more evident.
Although blocks are used in a randomized block design, the focus of the analysis is on 
the differences among the different groups. As is the case in completely randomized designs, 
groups are often different levels pertaining to a factor of interest. A randomized block design 
is often more statistically powerful than a completely randomized design (see references 5, 6, 
and 10). For example, if the factor of interest is advertising medium, three groups could be the 
following different levels: television, radio, and newspaper. Using different cities as blocks 
removes the variability of the different cities from the random error so as to better detect differ-
ences among the three advertising mediums.
Testing for Factor and Block Effects
Recall from Figure 11.1 on page 424 that, in the completely randomized design, the total 
­variation (SST) is subdivided into variation due to differences among the c groups (SSA) and 
variation due to variation within the c groups (SSW). Within-group variation is considered 
­random variation, and among-group variation is due to differences from group to group.
To remove the effects of the blocking from the random variation component in the 
­randomized block design, the within-group variation (SSW) is subdivided into variation 
due to differences among the blocks (SSBL) and random variation (SSE). Therefore, as 
presented in Figure 11.9, in a randomized block design, the total variation is the sum of 
three components: among-group variation (SSA), among-block variation (SSBL), and ran-
dom variation (SSE).
11.2  The Randomized Block Design
F i g u r e  1 1 . 9
Partitioning the total 
variation in a randomized 
block model
Partitioning the Total Variation
SST = SSA + SSBL + SSE
Among-Group Variation (SSA)
d.f. = c – 1
Among-Block Variation (SSBL)
d.f. = r – 1
Random Variation (SSE)
d.f. = (r – 1)(c – 1)
Total Variation (SST)
d.f. = n – 1
The following definitions are needed to develop the ANOVA procedure for the random-
ized block design:
 r = the number of blocks
 c = the number of groups
 n = the total number of values 1where n = rc2
 Xij = the value in the ith block for the jth group
 Xi. = the mean of all the values in block i
 X.j = the mean of all the values for group j
 a
c
j = 1 a
r
i = 1
Xij = the grand total

	
11.2  The Randomized Block Design	
439
The total variation, also called sum of squares total (SST), is a measure of the variation 
among all the values. You compute SST by summing the squared differences between each in-
dividual value and the grand mean, X, that is based on all n values. Equation (11.7) shows the 
computation for total variation.
Total Variation in the Randomized Block Design
	
SST = a
c
j = 1 a
r
i = 1
1Xij - X22	
(11.7)
	
where
X =
a
c
j = 1 a
r
i = 1
Xij
rc
 (i.e., the grand mean)
You compute the among-group variation, also called the sum of squares among groups 
(SSA), by summing the squared differences between the sample mean of each group, X.j, and 
the grand mean, X, weighted by the number of blocks, r. Equation (11.8) shows the computa-
tion for the among-group variation.
Among-Group Variation in the Randomized Block Design
	
SSA =  r a
c
j = 1
1X.j - X22	
(11.8)
	
where
X.j =
a
r
i = 1
Xij
r
You compute the among-block variation, also called the sum of squares among blocks 
(SSBL), by summing the squared differences between the mean of each block, Xi., and the 
grand mean, X, weighted by the number of groups, c. Equation (11.9) shows the computation 
for the among-block variation.
Among-Block Variation in the Randomized Block Design
	
SSBL =  c a
r
i = 1
1Xi . - X22	
(11.9)
	
where
Xi. =
a
c
j = 1
Xij
c
You compute the random variation, also called the sum of squares error (SSE), by sum-
ming the squared differences among all the values after the effect of the groups and blocks 
have been accounted for. Equation (11.10) shows the computation for random variation.
Random Variation in the Randomized Block Design
	
SSE = a
c
j = 1 a
r
i = 1
1Xij - X.j - Xi. + X22	
(11.10)

440	
Chapter 11  Analysis of Variance
Because you are comparing c groups, there are c - 1 degrees of freedom associated with 
the sum of squares among groups (SSA). Similarly, because there are r blocks, there are r - 1 
degrees of freedom associated with the sum of squares among blocks (SSBL). Moreover, there 
are n - 1 degrees of freedom associated with the sum of squares total (SST) because you are 
comparing each value, Xij, to the grand mean, X, based on all n values. Therefore, because the 
degrees of freedom for each of the sources of variation must add to the degrees of freedom 
for the total variation, you compute the degrees of freedom for the sum of squares error (SSE) 
component by subtraction and algebraic manipulation. Thus, the degrees of freedom associ-
ated with the sum of squares error is 1r - 121c - 12.
If you divide each of the component sums of squares by its associated degrees of free-
dom, you have the three variances, or mean square terms (MSA, MSBL, and MSE). Equations 
(11.11a–c) give the mean square terms needed for the ANOVA table.
The Mean Squares in the Randomized Block Design
	
MSA =
SSA
c - 1	
(11.11a)
	
MSBL = SSBL
r - 1	
(11.11b)
	
MSE =
SSE
1r - 121c - 12	
(11.11c)
The first step in analyzing a randomized block design is to test for a factor effect—that 
is, to test for any differences among the c group means. If the assumptions of the analysis of 
­variance are valid, the null hypothesis of no differences in the c group means:
H0: m1 = m2 = g = mc
is tested against the alternative that not all the c group means are equal:
H1: Not all m.j are equal 1where j = 1, 2, . . . , c2
by computing the FSTAT test statistic given in Equation (11.12).
FSTAT Statistic FOR FACTOR EFFECT
	
FSTAT = MSA
MSE	
(11.12)
The FSTAT test statistic follows an F distribution with c - 1 degrees of freedom for the 
MSA term and 1r - 121c - 12 degrees of freedom for the MSE term. For a given level of sig-
nificance a, you reject the null hypothesis if the computed FSTAT test statistic is greater than the 
upper-tail critical value, Fa, from the F distribution with c - 1 and 1r - 121c - 12 degrees 
of freedom (see Table E.5). The decision rule is:
Reject H0 if FSTAT 7 Fa;
otherwise, do not reject H0.
To examine whether the randomized block design was advantageous to use, some statisticians 
suggest that you perform the F test for block effects. The null hypothesis of no block effects:
H0: m1. = m2. = g = mr.

	
11.2  The Randomized Block Design	
441
is tested against the alternative:
H1: Not all mi are equal 1where i = 1, 2, c , r2
using the FSTAT test statistic for block effect given in Equation (11.13).
FSTAT Statistic for Block Effects
	
FSTAT = MSBL
MSE 	
(11.13)
You reject the null hypothesis at the a level of significance if the computed FSTAT test 
statistic is greater than the upper-tail critical value Fa from the F distribution with r - 1 and 
1r - 121c - 12 degrees of freedom (see Table E.5). That is, the decision rule is
Reject H0 if FSTAT 7 Fa;
otherwise, do not reject H0.
The results of the analysis-of-variance procedure are usually displayed in an ANOVA 
summary table, as shown in Table 11.6.
T a b l e  1 1 . 6
Analysis-of-Variance 
Table for the 
Randomized Block 
Design
 
Source
Degrees of 
Freedom
Sum of 
Squares
Mean Square  
(Variance)
 
F
Among groups 1A2
c - 1
SSA
MSA =
SSA
c - 1
FSTAT = MSA
MSE
Among blocks 1BL2
r - 1
SSBL
MSBL = SSBL
r - 1
FSTAT = MSBL
MSE
Error Total
1r - 121c - 12
rc - 1
SSE
SST
MSE =
SSE
1r - 121c - 12
To illustrate the randomized block design, suppose that a quick-service chain wants to 
evaluate the service at four of its restaurants. The customer service director for the chain hires 
six evaluators with varied experiences in food-service evaluations to act as raters. To reduce 
the effect of the variability between evaluators, you use a randomized block design, with 
­evaluators serving as the blocks. The four restaurants are the groups of interest.
The six evaluators rate the service at each of the four restaurants in a random order. A 
­rating scale from 0 (low) to 100 (high) is used. Table 11.7 summarizes the results (stored in  
 QSRChain ), along with the group totals, group means, block totals, block means, grand total, 
and grand mean.
T a b l e  1 1 . 7
Service Ratings for  
Four Restaurants of a 
Quick-Service Chain
Restaurant
Evaluators
Henry St
Surf Ave
Granby
Blvd N
Totals
Means
1
70
61
82
74
287
71.75
2
77
75
88
76
316
79.00
3
76
67
90
80
313
78.25
4
80
63
96
76
315
78.75
5
84
66
92
84
326
81.50
6
78
68
98
86
330
82.50
Totals
465
400
546
476
1,887
Means
77.50
66.67
91.00
79.33
78.625

442	
Chapter 11  Analysis of Variance
In addition, from Table 11.7,
r = 6    c = 4    n = rc = 24
and
X =
a
c
j = 1 a
r
i = 1
Xij
rc
= 1,887
24
= 78.625
Figure 11.10 shows the results for this randomized block design. (In the Excel ANOVA table, 
Rows are the evaluators and Columns are the restaurants.)
F i g u r e  1 1 . 1 0
Excel and Minitab randomized block design results for the quick-service chain study
Using the 0.05 level of significance to test for differences among the restaurants, you 
reject the null hypothesis 1H0: m1 = m2 = m3 = m42 if the computed FSTAT test statistic is 
greater than 3.29, the upper-tail critical value from the F distribution with 3 and 15 degrees of 
freedom in the numerator and denominator, respectively (see Figure 11.11).
F i g u r e  1 1 . 1 1
Regions of rejection 
and nonrejection for 
the quick service chain 
study at the 0.05 level of 
significance with 3 and 
15 degrees of freedom
Region of
Rejection
Region of
Nonrejection
Critical
Value
0
3.29
F
.05
.95
Because FSTAT = 39.7581 7 Fa = 3.29, or because the p-value = 0.0000 6 0.05, you 
reject H0 and conclude that there is evidence of a difference in the mean ratings among the 
different restaurants. The extremely small p-value indicates that if the means from the four 
restaurants are equal, the probability is 0.0000 that you will get differences as large or larger 
among the sample means, as observed in this study. Thus, there is little degree of belief in 

	
11.2  The Randomized Block Design	
443
the null hypothesis. You conclude that the alternative hypothesis is correct: The mean ratings 
among the four restaurants are different.
As a check on the effectiveness of blocking, you can test for a difference among the evalu-
ators. The decision rule, using the 0.05 level of significance, is to reject the null hypothesis 
1H0: m1 = m2 =  . . . = m62 if the computed FSTAT test statistic is greater than 2.90, the upper-
tail critical value from the F distribution with 5 and 15 degrees of freedom (see Figure 11.12). 
Because FSTAT = 3.7818 7 Fa = 2.90 or because the p-value = 0.0205 6 0.05, you reject 
H0 and conclude that there is evidence of a difference among the evaluators. Thus, you con-
clude that the blocking has been advantageous in reducing the random error.
The assumptions of the one-way analysis of variance (randomness and independence, nor-
mality, and homogeneity of variance) also apply to the randomized block design. If the nor-
mality assumption is violated, you can use the Friedman rank test (see Online Section 12.9). 
In addition, you need to assume that there is no interacting effect between the groups and the 
blocks. In other words, you need to assume that any differences between the groups (the res-
taurants) are consistent across the entire set of blocks (the evaluators). The concept of interac-
tion is discussed further in Section 11.3.
Did the blocking result in an increase in precision in comparing the different groups? To an-
swer this question, you use Equation (11.14) to calculate the estimated relative efficiency (RE) 
of the randomized block design as compared with the completely randomized design.
F i g u r e  1 1 . 1 2
Regions of rejection 
and nonrejection for 
the quick-service chain 
study at the 0.05 level of 
significance with 5 and  
15 degrees of freedom
0
2.90
F
.05
.95
Region of
Rejection
Region of
Nonrejection
Critical
Value
Estimated Relative Efficiency
	
RE = 1r - 12MSBL + r1c - 12MSE
1rc - 12MSE
	
(11.14)
Using Figure 11.10,
RE = 152156.6752 + 162132114.9862
1232114.9862
= 1.60
This value for relative efficiency means that it would take 1.6 times as many observations in 
a one-way ANOVA design as compared to the randomized block design in order to have the 
same precision in comparing the restaurants.
Multiple Comparisons: The Tukey Procedure
As in the case of the completely randomized design, once you reject the null hypothesis of 
no differences between the groups, you need to determine which groups are significantly dif-
ferent from the others. For the randomized block design, you can use a procedure developed 
by Tukey (see reference 10). Equation (11.15) gives the critical range for the Tukey multiple 
comparisons procedure for randomized block designs.

444	
Chapter 11  Analysis of Variance
To perform the multiple comparisons, you do the following:
1.  Compute the absolute mean differences, X.j - X.j′, (where j ≠j′), among all 
c1c - 12>2 pairs of sample means.
2.  Compute the critical range for the Tukey procedure using Equation (11.15).
3.  Compare each of the c1c - 12>2 pairs against the critical range. If the absolute dif-
ference in a specific pair of sample means, say X.j - X.j′, is greater than the critical 
range, then group j and group j′ is significantly different.
4.  Interpret the results.
To apply the Tukey procedure, return to the quick-service chain study. Because there 
are four restaurants, there are 414 - 12>2 = 6 possible pairwise comparisons. From Figure 
11.10, the absolute mean differences are
	
1.   X.1 - X.2 = 77.50 - 66.67 = 10.83
	
2.   X.1 - X.3 = 77.50 - 91.00 = 13.50
	
3.   X.1 - X.4 = 77.50 - 79.33 =  1.83
	
4.   X.2 - X.3 = 66.67 - 91.00 = 24.33
	
5.   X.2 - X.4 = 66.67 - 79.33 = 12.66
	
6.   X.3 - X.4 = 91.00 - 79.33 = 11.67
Locate MSE = 14.986 and r = 6 in Figure 11.10 to determine the critical range. From 
Table E.7 [for a = .05, c = 4, and 1r - 121c - 12 = 15], Qa, the upper-tail critical value of 
the test statistic with 4 and 15 degrees of freedom, is 4.08. Using Equation (11.15),
Critical range = 4.08A
14.986
6
= 6.448
All pairwise comparisons except X.1 - X.4 are greater than the critical range. Therefore, you 
conclude with 95% confidence that the population mean rating is different between all pairs 
of restaurant branches except for Henry St. and Blvd. N. In addition, Granby has the highest 
mean ratings (i.e., most preferred) and Surf Ave has the lowest (i.e., least preferred).
The Critical Range for the Randomized Block Design
	
Critical range = QaA
MSE
r
	
(11.15)
where Qa is the upper-tail critical value from a Studentized range distribution having c de-
grees of freedom in the numerator and 1r - 121c - 12 degrees of freedom in the denomi-
nator. Values for the Studentized range distribution are found in Table E.7.
Problems for Section 11.2
Learning the Basics
11.15  Given a randomized block experiment with five groups 
and seven blocks, answer the following:
a.	 How many degrees of freedom are there in determining the 
among-group variation?
b.	 How many degrees of freedom are there in determining the 
among-block variation?
c.	 How many degrees of freedom are there in determining the ran-
dom variation?
d.	 How many degrees of freedom are there in determining the to-
tal variation?
11.16  From Problem 11.15,
a.	 if SSA = 60, SSBL = 75, and SST = 210, what is SSE?
b.	 what are MSA, MSBL, and MSE?
c.	 what is the value of the FSTAT test statistic for the factor effect?
d.	 what is the value of the FSTAT test statistic for the block effect?
11.17  From Problems 11.15 and 11.16,
a.	 construct the ANOVA summary table and fill in all values in 
the body of the table.
b.	 at the 0.05 level of significance, is there evidence of a differ-
ence in the group means?
c.	 at the 0.05 level of significance, is there evidence of a differ-
ence due to blocks?

	
11.2  The Randomized Block Design	
445
11.18  From Problems 11.15, 11.16, and 11.17,
a.	 to perform the Tukey procedure, how many degrees of freedom 
are there in the numerator, and how many degrees of freedom 
are there in the denominator of the Studentized range distribu-
tion?
b.	 at the 0.05 level of significance, what is the upper-tail critical 
value from the Studentized range distribution?
c.	 to perform the Tukey procedure, what is the critical range?
11.19  Given a randomized block experiment with three groups 
and seven blocks,
a.	 how many degrees of freedom are there in determining the 
among-group variation?
b.	 how many degrees of freedom are there in determining the 
among-block variation?
c.	 how many degrees of freedom are there in determining the ran-
dom variation?
d.	 how many degrees of freedom are there in determining the total 
variation?
11.20  From Problem 11.19, if SSA = 36 and the randomized 
block FSTAT statistic is 6.0,
a.	 what are MSE and SSE?
b.	 what is SSBL if the FSTAT test statistic for block effect is 4.0?
c.	 what is SST?
d.	 at the 0.01 level of significance, is there evidence of an effect 
due to groups, and is there evidence of an effect due to blocks?
11.21  Given a randomized block experiment with four groups 
and eight blocks, in the following ANOVA summary table, fill in 
all the missing results.
Source
Degrees of 
Freedom
Sum of 
Squares
Mean 
Square 
(Variance)
F
Among  
  groups
c - 1 = ?
SSA = ?
MSA = 80
FSTAT = ?
Among  
  blocks
r - 1 = ?
SSBL = 540
MSBL = ?
FSTAT = 5.0
Error
1r - 12  
  1c - 12 = ?
SSE = ?
MSE = ?
Total
rc - 1 = ?
SST = ?
11.22  From Problem 11.21,
a.	 at the 0.05 level of significance, is there evidence of a differ-
ence among the four group means?
b.	 at the 0.05 level of significance, is there evidence of an effect 
due to blocks?
Applying the Concepts
11.23  Nine experts rated four brands of Colombian cof-
fee in a taste-testing experiment. A rating on a 7-point scale 
11 = extremely unpleasing, 7 = extremely pleasing2 is given for 
each of four characteristics: taste, aroma, richness, and acidity. 
The following data (stored in  Coffee ) display the summated rat-
ings, accumulated over all four characteristics.
Brand
Expert
A
B
C
D
C.C.
24
26
25
22
S.E.
27
27
26
24
E.G.
19
22
20
16
B.L.
24
27
25
23
C.M.
22
25
22
21
C.N.
26
27
24
24
G.N.
27
26
22
23
R.M.
25
27
24
21
P.V.
22
23
20
19
At the 0.05 level of significance, completely analyze the data to 
determine whether there is evidence of a difference in the sum-
mated ratings of the four brands of Colombian coffee and, if so, 
which of the brands are rated highest (i.e., best). What can you 
conclude?
11.24  How do the ratings for TV, phone, and Internet services 
compare? The data in  Telecom2  represent the mean ratings in 13 
different cities.
Source: Data extracted from “Ratings: TV, Phone, and Internet 
Services,” Consumer Reports, May 2013, pp. 24–25.
a.	 At the 0.05 level of significance, determine whether there is 
evidence of a difference in the mean rating between TV, phone, 
and Internet services.
b.	 If appropriate, use the Tukey procedure to determine which ser-
vices’ mean ratings differ. Again, use a 0.05 level of significance.
11.25  An article discussed the price of a market basket of items 
at Publix and Winn-Dixie supermarkets, and at Walmart and Su-
per Target stores, located in South Florida. The file  Supermarket-
Prices  contains the prices listed in that article (data extracted from 
“S. Salisbury, Supermarket Showdown,” The Palm Beach Post, 
February 11, 2011, pp. 1F, 7F)
a.	 At the 0.05 level of significance, determine whether there is 
evidence of a difference in the mean prices for these items at 
the four supermarkets.
b.	 What assumptions are necessary to perform this test?
c.	 If appropriate, use the Tukey procedure to determine which  
supermarket mean prices differ. 1Use a = 0.05.2
d.	 Was there a significant block effect in this experiment? Explain.
11.26  How different are the rates of return of money market ac-
counts and certificates of deposit that vary in their term length? 
The data in  MMCDRate  contain the money market, 1-year CD, 
two-year CD, and five-year CD rates for banks in a suburban area 
(data extracted from “Consumer Money Rates,” Newsday, June 
13, 2013, p. A47).
a.	 At the 0.05 level of significance, determine whether there is ev-
idence of a difference in the mean rates for these investments.
b.	 What assumptions are necessary to perform this test?
c.	 If appropriate, use the Tukey procedure to determine which  
investments differ. 1Use a = 0.05.2
d.	 Was there a significant block effect in their mean rates in this 
experiment? Explain.

446	
Chapter 11  Analysis of Variance
11.27  Philips Semiconductors is a leading European manufac-
turer of integrated circuits. Integrated circuits are produced on 
silicon wafers, which are ground to target thickness early in the 
production process. The wafers are positioned in different loca-
tions on a grinder and kept in place using vacuum decompression. 
One of the goals of process improvement is to reduce the variabil-
ity in the thickness of the wafers in different positions and in dif-
ferent batches. Data were collected from a sample of 30 batches. 
In each batch, the thickness of the wafers on positions 1 and 2 
(outer circle), 18 and 19 (middle circle), and 28 (inner circle) was 
measured and stored in  Circuits . At the 0.01 level of significance, 
completely analyze the data to determine whether there is evi-
dence of a difference in the mean thickness of the wafers for the 
five positions and, if so, which of the positions are different. What 
can you conclude?
Source: Data extracted from K. C. B. Roes and R. J. M. M. Does, 
“Shewhart-Type Charts in Nonstandard Situations,” Technometrics, 
37 (1995),  pp. 15–24.
11.28  The data in  Concrete2  represent the compressive strength 
in thousands of pounds per square inch of 40 samples of concrete 
taken 2, 7, and 28 days after pouring.
Source: Data extracted from O. Carrillo-Gamboa and R. F. Gunst, 
“Measurement-Error-Model Collinearities,” Technometrics, 34 
(1992), pp. 454–464.
a.	 At the 0.05 level of significance, is there evidence of a differ-
ence in the mean compressive strength after 2, 7, and 28 days?
b.	 If appropriate, use the Tukey procedure to determine the days 
that differ in mean compressive strength. 1Use a = 0.05.2
c.	 Determine the relative efficiency of the randomized block de-
sign as compared with the completely randomized (one-way 
ANOVA) design.
d.	 Construct boxplots of the compressive strength for the different 
time periods.
e.	 Based on the results of (a), (b), and (d), is there a pattern in the 
compressive strength over the three time periods?
11.3  The Factorial Design: Two-Way ANOVA
In Section 11.1, you learned about the completely randomized design and in Section 11.2, 
you learned about the randomized block design. In this section, the single-factor completely 
randomized design is extended to the two-factor factorial design, in which two factors are 
simultaneously evaluated. Each factor is evaluated at two or more levels. For example, in the 
Arlington’s scenario on page 422, the company faces the business problem of simultaneously 
evaluating four locations and the effectiveness of providing mobile payment to determine 
which location should be used and whether mobile payment should be made available. Al-
though this section uses only two factors, you can extend factorial designs to three or more 
factors (see references 4, 5, 6, 7, and 10).
To analyze data from a two-factor factorial design, you use two-way ANOVA. The  
following definitions are needed to develop the two-way ANOVA procedure:
 r = number of levels of factor A
 c = number of levels of factor B
 n′ = number of values 1replicates2 for each cell (combination of a particular level
 of factor A and a particular level of factor B)
n = number of values in the entire experiment 1where n = rcn′2
 Xijk = value of the kth observation for level i of factor A and level j of factor B
 X =
a
r
i = 1 a
c
j = 1 a
n′
k = 1
Xijk
rcn′
= grand mean
 Xi.. =
a
c
j = 1 a
n′
k = 1
Xijk
cn′
 =  mean of the ith level of factor A (where i = 1, 2, c , r)
 X.j. =
a
r
i = 1 a
n′
k = 1
Xijk
rn′
 =  mean of the jth level of factor B (where i = 1, 2, c , c)
 Xij. =
a
n′
k = 1
Xijk
n′
 =  mean of the cell ij, the combination of the ith level of factor A
and the jth level of factor B

	
11.3  The Factorial Design: Two-Way ANOVA	
447
Because of the complexity of these computations, you should only use computerized 
methods when performing this analysis. However, to help explain the two-way ANOVA, the 
decomposition of the total variation is illustrated. In this discussion, only cases in which there 
are an equal number of values (also called replicates) (sample sizes n′) for each combination 
of the levels of factor A with those of factor B are considered. (See references 1 and 6 for a 
discussion of two-factor factorial designs with unequal sample sizes.)
Factor and Interaction Effects
There is an interaction between factors A and B if the effect of factor A is different for various 
levels of factor B. Thus, when dividing the total variation into different sources of variation, 
you need to account for a possible interaction effect, as well as for factor A, factor B, and ran-
dom error. To accomplish this, the total variation (SST) is subdivided into sum of squares due 
to factor A (or SSA), sum of squares due to factor B (or SSB), sum of squares due to the interac-
tion effect of A and B (or SSAB), and sum of squares due to random variation (or SSE). This 
decomposition of the total variation (SST) is displayed in Figure 11.13.
F i g u r e  1 1 . 1 3
Partitioning the total 
variation in a two-factor 
factorial design
Partitioning the Total Variation
SST = SSA + SSB  + SSAB + SSE
Factor A Variation (SSA)
df = r – 1 
Factor B Variation (SSB)
df = c – 1 
Interaction (SSAB)
df = (r – 1)(c – 1)
Random Variation (SSE)
df = rc(n′ – 1) 
Total Variation (SST)
df = n – 1 
The sum of squares total (SST) represents the total variation among all the values around 
the grand mean. Equation (11.16) shows the computation for total variation.
Total Variation in Two-Way Anova
	
SST = a
r
i = 1 a
c
j = 1 a
n′
k = 1
1Xijk - X22	
(11.16)
The sum of squares due to factor A (SSA) represents the differences among the various 
levels of factor A and the grand mean. Equation (11.17) shows the computation for factor A 
variation.
Factor A Variation
	
SSA = cn′ a
r
i = 1
1Xi.. - X22	
(11.17)

448	
Chapter 11  Analysis of Variance
The sum of squares due to factor B (SSB) represents the differences among the various 
levels of factor B and the grand mean. Equation (11.18) shows the computation for factor B 
variation.
Factor B Variation
	
SSB = rn′ a
c
j = 1
1X.j. - X22	
(11.18)
The sum of squares due to interaction (SSAB) represents the interacting effect of spe-
cific combinations of factor A and factor B. Equation (11.19) shows the computation for inter-
action variation.
Interaction Variation
	
SSAB = n′ a
r
i = 1 a
c
j = 1
1Xij. - Xi.. - X.j. + X22	
(11.19)
The sum of squares error (SSE) represents random variation—that is, the differences 
among the values within each cell and the corresponding cell mean. Equation (11.20) shows 
the computation for random variation.
Random Variation in Two-Way Anova
	
SSE = a
r
i = 1 a
c
j = 1 a
n′
k = 1
1Xijk - Xij.22	
(11.20)
Because there are r levels of factor A, there are r - 1 degrees of freedom associated with 
SSA. Similarly, because there are c levels of factor B, there are c - 1 degrees of freedom as-
sociated with SSB. Because there are n′ replicates in each of the rc cells, there are rc1n′ - 12 
degrees of freedom associated with the SSE term. Carrying this further, there are n - 1 de-
grees of freedom associated with the sum of squares total (SST) because you are comparing 
each value, Xijk, to the grand mean, X, based on all n values. Therefore, because the degrees 
of freedom for each of the sources of variation must add to the degrees of freedom for the 
total variation (SST), you can calculate the degrees of freedom for the interaction component 
(SSAB) by subtraction. The degrees of freedom for interaction are 1r - 121c - 12.
If you divide each sum of squares by its associated degrees of freedom, you have the four 
variances or mean square terms (MSA, MSB, MSAB, and MSE). Equations (11.21a–d) give the 
mean square terms needed for the two-way ANOVA table.
Student Tip
Remember, mean square 
is another term for  
variance.
Mean Squares in Two-Way Anova
	
MSA =
SSA
r - 1	
(11.21a)
	
MSB =
SSB
c - 1	
(11.21b)

	
11.3  The Factorial Design: Two-Way ANOVA	
449
Testing for Factor and Interaction Effects
There are three different tests to perform in a two-way ANOVA:
 • A test of the hypothesis of no difference due to factor A
 • A test of the hypothesis of no difference due to factor B
 • A test of the hypothesis of no interaction of factors A and B
To test the hypothesis of no difference due to factor A:
H0: m1.. = m2.. = g = mr..
against the alternative:
H1: Not all mi.. are equal
you use the FSTAT test statistic in Equation (11.22).
	
MSAB =
SSAB
1r - 121c - 12	
(11.21c)
MSE =
SSE
rc1n′ - 12	
(11.21d)
F Test for Factor A Effect
	
FSTAT = MSA
MSE	
(11.22)
You reject the null hypothesis at the a level of significance if
FSTAT = MSA
MSE 7 Fa
where Fa is the upper-tail critical value from an F distribution with r - 1 and rc1n′ - 12  
degrees of freedom.
To test the hypothesis of no difference due to factor B:
H0: m.1. = m.2. = g = m.c.
against the alternative:
H1: Not all m.j. are equal
you use the FSTAT test statistic in Equation (11.23).
F Test for Factor B Effect
	
FSTAT = MSB
MSE	
(11.23)
You reject the null hypothesis at the a level of significance if
FSTAT = MSB
MSE 7 Fa

450	
Chapter 11  Analysis of Variance
where Fa is the upper-tail critical value from an F distribution with c - 1 and rc1n′ - 12 
degrees of freedom.
To test the hypothesis of no interaction of factors A and B:
H0: The interaction of A and B is equal to zero
against the alternative:
H1: The interaction of A and B is not equal to zero
you use the FSTAT test statistic in Equation (11.24).
Student Tip
In each of these F tests, 
the denominator of the 
FSTAT statistic is MSE.
F Test for Interaction Effect
	
FSTAT = MSAB
MSE 	
(11.24)
You reject the null hypothesis at the a level of significance if
FSTAT = MSAB
MSE
7 Fa
where Fa is the upper-tail critical value from an F distribution with 1r - 121c - 12 and 
rc1n′ - 12 degrees of freedom.
Table 11.8 presents the entire two-way ANOVA table.
T a b l e  1 1 . 8
Analysis of Variance 
Table for the Two-
Factor Factorial 
Design
Source
Degrees of 
Freedom
Sum of Squares
Mean Square  
(Variance)
F
A
r - 1
SSA
MSA =
SSA
r - 1
FSTAT = MSA
MSE
B
c - 1
SSB
MSB =
SSB
c - 1
FSTAT = MSB
MSE
AB
1r - 121c - 12
SSAB
MSAB =
SSAB
1r - 121c - 12
FSTAT = MSAB
MSE
Error
rc1n′ - 12
SSE
MSE =
SSE
rc1n′ - 12
Total
n - 1
SST
To illustrate two-way ANOVA, return to the Arlington’s scenario on page 422. As a mem-
ber of the sales team, you first explored how different in-store locations might affect the sales 
of mobile electronics merchandise using one-way ANOVA. Now, to explore the effects of per-
mitting mobile payment methods to buy mobile electronics merchandise, you can design an 
experiment that examines this second (B) factor as it studies the effects of in-store location 
(factor A) using two-way ANOVA. Two-way ANOVA will allow you to determine if there is a 
significant difference in mobile electronics sales among the four in-store locations and whether 
permitting mobile payment methods makes a difference.
To test the effects of the two factors, you conduct a 60-day experiment at 40 same-sized 
stores that have similar storewide net sales. You randomly assign ten stores to use the current 
in-aisle location, ten stores to use the front of the store near weekly specials, ten stores to 
use the end-cap special kiosk display, and ten stores to use the location adjacent to the Ex-
pert Counter. In five stores in each of the four groups, you permit mobile payment methods 

	
11.3  The Factorial Design: Two-Way ANOVA	
451
(for the other five in each group, mobile payment methods are not permitted). At the end of 
the experiment, you organize the mobile electronics sales data by group and store the data in  
 Mobile Electronics2 . Table 11.9 presents the data of the experiment.
T a b l e  1 1 . 9
Mobile Electronics 
Sales at Four In-Store 
Locations with Mobile 
Payments Permitted 
and Not Permitted
Mobile  
Payments
In-Store Location
In-Aisle
Front
Kiosk
Expert
No
30.06
32.22
30.78
30.33
No
29.96
31.47
30.91
30.29
No
30.19
32.13
30.79
30.25
No
29.96
31.86
30.95
30.25
No
29.74
32.29
31.13
30.55
Yes
30.66
32.81
31.34
31.03
Yes
29.99
32.65
31.80
31.77
Yes
30.73
32.81
32.00
30.97
Yes
30.72
32.42
31.07
31.43
Yes
30.73
33.12
31.69
30.72
Figure 11.14 presents the results for this example. In the Excel results, the A, B, and  
Error sources of variation in Table 11.8 above are labeled Sample, Columns, and Within,  
respectively. In the Minitab results, the names of the factors (Mobile Payments and In-Store  
Location) are used to label the A and B sources of variation.
F i g u r e  1 1 . 1 4
Excel and Minitab two-way ANOVA results for the in-store location sales and mobile payment experiment
To interpret the results, you start by testing whether there is an interaction effect between 
factor A (mobile payments) and factor B (in-store locations). If the interaction effect is signifi-
cant, further analysis will focus on this interaction. If the interaction effect is not significant, 
you can focus on the main effects—the potential effect of permitting mobile payment (factor A)  
and the potential differences in in-store locations (factor B).

452	
Chapter 11  Analysis of Variance
Using the 0.05 level of significance, to determine whether there is evidence of an inter-
action effect, you reject the null hypothesis of no interaction between mobile payments and 
in-store locations if the computed FSTAT statistic is greater than 2.9011, the upper-tail critical 
value from the F distribution, with 3 and 32 degrees of freedom (see Figures 11.14 and 11.15).1
Because FSTAT = 0.4100 6 2.9011 or the p@value = 0.7469 7 0.05, you do not reject 
H0. You conclude that there is insufficient evidence of an interaction effect between mobile  
payment and in-store location. You can now focus on the main effects.
1Table E.5 does not provide the 
upper-tail critical values from the 
F distribution with 32 degrees of 
freedom in the denominator. When 
the desired degrees of freedom 
are not provided in the table, use 
the p-value computed by Excel or 
Minitab.
F i g u r e  1 1 . 1 5
Regions of rejection and 
nonrejection at the 0.05 
level of significance,  
with 3 and 32 degrees  
of freedom
Region of
Rejection
Region of
Nonrejection
Critical
Value
0
2.9011
F
.05
.95
Using the 0.05 level of significance and testing whether there is an effect due to mobile 
payment options (yes or no) (factor A), you reject the null hypothesis if the computed FSTAT test 
statistic is greater than 4.1491, the upper-tail critical value from the F distribution with 1 and 
32 degrees of freedom (see Figures 11.14 and 11.16). Because FSTAT = 62.7258 7 4.1491 or 
the p@value = 0.0000 6 0.05, you reject H0. You conclude that there is evidence of a differ-
ence in the mean sales when mobile payment methods are permitted as compared to when they 
are not. Because the mean sales when mobile payment methods are permitted is 31.523 and is 
30.806 when they are not, you can conclude that permitting mobile payment methods has led 
to an increase in mean sales.
F i g u r e  1 1 . 1 6
Regions of rejection and 
nonrejection at the 0.05 
level of significance,  
with 1 and 32 degrees  
of freedom
0
4.1491
F
.05
.95
Region of
Rejection
Region of
Nonrejection
Critical
Value
Using the 0.05 level of significance and testing for a difference among the in-store loca-
tions (factor B), you reject the null hypothesis of no difference if the computed FSTAT test sta-
tistic is greater than 2.9011, the upper-tail critical value from the F distribution with 3 degrees 
of freedom in the numerator and 32 degrees of freedom in the denominator (see Figures 11.14 
and 11.15). Because FSTAT = 98.9631 7 2.9011 or the p@value = 0.0000 6 0.05, you reject 
H0. You conclude that there is evidence of a difference in the mean sales among the four in-
store locations.
Multiple Comparisons: The Tukey Procedure
If one or both of the factor effects are significant and there is no significant interaction effect, 
when there are more than two levels of a factor, you can determine the particular levels that 
are significantly different by using the Tukey multiple comparisons procedure for two-way 
ANOVA (see references 6 and 10). Equation (11.25) gives the critical range for factor A.

	
11.3  The Factorial Design: Two-Way ANOVA	
453
Equation (11.26) gives the critical range for factor B.
Critical Range for Factor A
	
Critical range = QaA
MSE
cn′ 	
(11.25)
where Qa is the upper-tail critical value from a Studentized range distribution having r and 
rc1n′ - 12 degrees of freedom. (Values for the Studentized range distribution are found in 
Table E.7.)
Critical Range for Factor B
	
Critical range = QaA
MSE
rn′ 	
(11.26)
where Qa is the upper-tail critical value from a Studentized range distribution having c and 
rc1n′ - 12 degrees of freedom. (Values for the Studentized range distribution are found in 
Table E.7.)
To use the Tukey procedure, return to the mobile electronics sales data of Table 11.7 on 
page 451. In the ANOVA summary table in Figure 11.14 on page 451, the interaction effect is 
not significant. Because there are only two categories for mobile payment (yes and no), there 
are no multiple comparisons to be constructed. Using a = 0.05, there is evidence of a signifi-
cant difference among the four in-store locations that comprise factor B. Thus, you can use the 
Tukey multiple comparisons procedure to determine which of the four in-store locations differ.
Because there are four in-store locations, there are 414 - 12>2 = 6 pairwise compari-
sons. Using the calculations presented in Figure 11.14, the absolute mean differences are as 
follows:
	
1.  X.1. - X.2.  = 30.274 - 32.378 = 2.104
	
2.  X.1. - X.3.  = 30.274 - 31.246 = 0.972
	
3.  X.1. - X.4.  = 30.274 - 30.759 = 0.485
	
4.  X.2. - X.3.  = 32.378 - 31.246 = 1.132
	
5.  X.2. - X.4.  = 32.378 - 30.759 = 1.619
	
6.  X.3. - X.4.  = 31.246 - 30.759 = 0.487
To determine the critical range, refer to Figure 11.14 to find MSE = 0.0821,  r = 2, 
c = 4, and n′ = 5. From Table E.7 [for a = 0.05,  c = 4, and rc1n′ - 12 = 32], Qa, the 
upper-tail critical value of the Studentized range distribution with 4 and 32 degrees of freedom 
is approximately 3.84. Using Equation (11.17),
Critical range = 3.84A
0.0821
10
= 0.3482
Because each of the six comparisons is greater than the critical range of 0.3482, you can 
conclude that the population mean sales is different for the four in-store locations. The front 
location is estimated to have higher mean sales than the other three in-store locations. The 
kiosk location is estimated to have higher mean sales than the in-aisle and expert locations. 
The expert location is estimated to have higher mean sales than the in-aisle location. Note 
that by using a = 0.05, you are able to make all six comparisons with an overall error rate of  
only 5%.

454	
Chapter 11  Analysis of Variance
Visualizing Interaction Effects: The Cell Means Plot
You can get a better understanding of the interaction effect by plotting the cell means, the 
means of all possible factor-level combinations. Figure 11.17 presents a cell means plot that 
uses the cell means for the mobile payments permitted/in-store location combinations shown 
in Figure 11.14 on page 451. From the plot of the mean sales for each combination of mobile 
payments permitted and in-store location, observe that the two lines (representing the two lev-
els of mobile payments, yes and no,) are roughly parallel. This indicates that the difference 
between the mean sales for stores that permit mobile payment methods and those that do not 
is virtually the same for the four in-store locations. In other words, there is no interaction be-
tween these two factors, as was indicated by the F test.
Interpreting Interaction Effects
How do you interpret an interaction? When there is an interaction, some levels of factor A 
respond better with certain levels of factor B. For example, with respect to mobile electronics 
sales, suppose that some in-store locations were better when mobile payment methods were 
permitted and other in-store locations were better when mobile payment methods were not 
permitted. If this were true, the lines of Figure 11.17 would not be nearly as parallel, and the 
interaction effect might be statistically significant. In such a situation, the difference between 
whether mobile payment methods were permitted is no longer the same for all in-store loca-
tions. Such an outcome would also complicate the interpretation of the main effects because 
differences in one factor (whether mobile payment methods were permitted) would not be 
consistent across the other factor (the in-store locations).
Example 11.2 illustrates a situation with a significant interaction effect.
Example 11.2
Interpreting Sig-
nificant Interaction 
Effects
A nationwide company specializing in preparing students for college and graduate school en-
trance exams, such as the SAT, ACT, GRE, and LSAT, had the business objective of improving 
its ACT preparatory course. Two factors of interest to the company are the length of the course 
(a condensed 10-day period or a regular 30-day period) and the type of course (traditional 
classroom or online distance learning). The company collected data by randomly assigning 10 
clients to each of the four cells that represent a combination of length of the course and type of 
course. The results are organized in the file  ACT  and presented in Table 11.10.
What are the effects of the type of course and the length of the course on ACT scores?
F i g u r e  1 1 . 1 7
Excel and Minitab cell means plots for mobile electronic sales based on mobile payments permitted and 
in-store location

	
11.3  The Factorial Design: Two-Way ANOVA	
455
T a b l e  1 1 . 1 0
ACT Scores for 
Different Types and 
Lengths of Courses
Length of Course
Type of Course
Condensed
Regular
Traditional
26
18
34
28
Traditional
27
24
24
21
Traditional
25
19
35
23
Traditional
21
20
31
29
Traditional
21
18
28
26
Online
27
21
24
21
Online
29
32
16
19
Online
30
20
22
19
Online
24
28
20
24
Online
30
29
23
25
F i g u r e  1 1 . 1 8
Excel and Minitab cell means plot of ACT scores
Solution  The cell means plot presented in Figure 11.18 shows a strong interaction be-
tween the type of course and the length of the course. The nonparallel lines indicate that the 
effect of condensing the course depends on whether the course is taught in the traditional 
classroom or by online distance learning. The online mean score is higher when the course is 
condensed to a 10-day period, whereas the traditional mean score is higher when the course 
takes place over the regular 30-day period.
To verify the visual analysis provided by interpreting the cell means plot, you begin by 
testing whether there is a statistically significant interaction between factor A (length of course) 
and factor B (type of course). Using a 0.05 level of significance, you reject the null hypothesis 
because FSTAT = 24.2569 7 4.1132 or the p-value equals 0.0000 6 0.05 (see Figure 11.19 
on next page). Thus, the hypothesis test confirms the interaction evident in the cell means plot.
The existence of this significant interaction effect complicates the interpretation of the 
hypothesis tests concerning the two main effects. You cannot directly conclude that there 
is no effect with respect to length of course and type of course, even though both have 
p@values 7 0.05.
Given that the interaction is significant, you can reanalyze the data with the two factors 
collapsed into four groups of a single factor rather than a two-way ANOVA with two lev-
els of each of the two factors. You can reorganize the data as follows: Group 1 is traditional  
(continued)

456	
Chapter 11  Analysis of Variance
F i g u r e  1 1 . 1 9
Excel and Minitab two-way ANOVA results for the ACT scores
condensed, Group 2 is traditional regular, Group 3 is online condensed, and Group 4 is online 
regular. Figure 11.20 shows the results for these data, stored in  ACT-OneWay .
From Figure 11.20, because FSTAT = 8.2239 7 2.8663 or p@value = 0.0003 6 0.05, 
there is evidence of a significant difference in the four groups (traditional condensed, tradi-
tional regular, online condensed, and online regular). Traditional condensed is different from 
traditional regular and from online condensed. Traditional regular is also different from online 
regular, and online condensed is also different from online regular. Thus, whether condensing 
a course is a good idea depends on whether the course is offered in a traditional classroom or 
as an online distance learning course. To ensure the highest mean ACT scores, the company 
should use the traditional approach for courses that are given over a 30-day period but use the 
online approach for courses that are condensed into a 10-day period.
F i g u r e  1 1 . 2 0
Excel (below) and Minitab (page 457) one-way ANOVA and Tukey-Kramer results for the ACT scores

	
11.3  The Factorial Design: Two-Way ANOVA	
457
Problems for Section 11.3
Learning the Basics
11.29  Consider a two-factor factorial design with three levels for 
factor A, three levels for factor B, and four replicates in each of the 
nine cells.
a.	 How many degrees of freedom are there in determining the fac-
tor A variation and the factor B variation?
b.	 How many degrees of freedom are there in determining the in-
teraction variation?
c.	 How many degrees of freedom are there in determining the ran-
dom variation?
d.	 How many degrees of freedom are there in determining the to-
tal variation?
11.30  Assume that you are working with the results from 
Problem 11.29, and SSA = 120, SSB = 110, SSE = 270, and 
SST = 540.
a.	 What is SSAB?
b.	 What are MSA and MSB?
c.	 What is MSAB?
d.	 What is MSE?
11.31  Assume that you are working with the results from 
Problems 11.29 and 11.30.
a.	 What is the value of the FSTAT test statistic for the interac-
tion effect?
b.	 What is the value of the FSTAT test statistic for the factor A  
effect?
c.	 What is the value of the FSTAT test statistic for the factor B  
effect?
d.	 Form the ANOVA summary table and fill in all values in the 
body of the table.
11.32  Given the results from Problems 11.29 through 11.31,
a.	 at the 0.05 level of significance, is there an effect due to  
factor A?
b.	 at the 0.05 level of significance, is there an effect due to  
factor B?
c.	 at the 0.05 level of significance, is there an interaction effect?
11.33  Given a two-way ANOVA with two levels for factor A, five 
levels for factor B, and four replicates in each of the 10 cells, 
with SSA = 18, SSB = 64, SSE = 60, and SST = 150,
a.	 form the ANOVA summary table and fill in all values in the 
body of the table.
b.	 at the 0.05 level of significance, is there an effect due to  
factor A?
c.	 at the 0.05 level of significance, is there an effect due to  
factor B?
d.	 at the 0.05 level of significance, is there an interaction effect?
11.34  Given a two-factor factorial experiment and the ANOVA 
summary table that follows, fill in all the missing results:
11.35  Given the results from Problem 11.34,
a.	 at the 0.05 level of significance, is there an effect due to  
factor A?
b.	 at the 0.05 level of significance, is there an effect due to  
factor B?
c.	 at the 0.05 level of significance, is there an interaction effect?
Source
Degrees of  
Freedom
Sum of 
Squares
Mean 
Square 
(Variance)
F
A
r - 1 = 2
SSA = ?
MSA = 80
FSTAT = ?
B
c - 1 = ?
SSB = 220
MSB = ?
FSTAT = 11.0
AB
1r - 121c - 12 = 8
SSAB = ?
MSAB = 10
FSTAT = ?
Error
rc1n′-12 = 30
SSE = ?
MSE = ?
Total
n - 1 = ?
SST = ?

458	
Chapter 11  Analysis of Variance
Applying the Concepts
11.36  An experiment was conducted to study the extrusion process 
of biodegradable packaging foam. Two of the factors considered 
for their effect on the unit density (mg/ml) were the die tempera-
ture (145° C vs. 155° C) and the die diameter (3 mm vs. 4 mm).  
The results are stored in  PackagingFoam1 . (Data extracted 
from W. Y. Koh, K. M. Eskridge, and M. A. Hanna, “Supersatu-
rated Split-Plot Designs,” Journal of Quality Technology, 45,  
January 2013, pp. 61–72.)
At the 0.05 level of significance,
a.	 is there an interaction between die temperature and die  
diameter?
b.	 is there an effect due to die temperature?
c.	 is there an effect due to die diameter?
d.	 Plot the mean unit density for each die temperature for each die  
diameter.
e.	 What can you conclude about the effect of die temperature and 
die diameter on mean unit density?
11.37  Referring to Problem 11.36, the effect of die temperature 
and die diameter on the foam diameter was also measured and the 
results stored in  PackagaingFoam2 .
At the 0.05 level of significance,
a.	 is there an interaction between die temperature and die 
­diameter?
b.	 is there an effect due to die temperature?
c.	 is there an effect due to die diameter?
d.	 Plot the mean foam diameter for each die temperature and die 
diameter.
e.	 What conclusions can you reach concerning the importance of 
each of these two factors on the foam diameter?
SELF 
Test 
11.38  A student team in a business statistics course per-
formed a factorial experiment to investigate the time re-
quired for pain-relief tablets to dissolve in a glass of water. The two 
factors of interest were brand name (Equate, Kroger, or Alka-Seltzer) 
and water temperature (hot or cold). The experiment consisted of 
four replicates for each of the six factor combinations. The following 
data, stored in  PainRelief , show the time a tablet took to dissolve (in 
seconds) for the 24 tablets used in the experiment:
Pain-Relief Tablet Brand
Water
Equate
Kroger
Alka-Seltzer
Cold
85.87
75.98
100.11
Cold
78.69
87.66
99.65
Cold
76.42
85.71
100.83
Cold
74.43
86.31
94.16
Hot
21.53
24.10
23.80
Hot
26.26
25.83
21.29
Hot
24.95
26.32
20.82
Hot
21.52
22.91
23.21
At the 0.05 level of significance,
a.	 is there an interaction between brand of pain reliever and water 
temperature?
b.	 is there an effect due to brand?
c.	 is there an effect due to water temperature?
d.	 Plot the mean dissolving time for each brand for each water 
temperature.
e.	 Discuss the results of (a) through (d).
11.39  A metallurgy company wanted to investigate the effect of 
the percentage of ammonium and the stir rate on the density of the 
powder produced. The results (stored in  Density ) are as follows:
Stir Rate
Ammonium (%)
100
150
  2
10.95
7.54
  2
14.68
6.66
  2
17.68
8.03
  2
15.18
8.84
30
12.65
12.46
30
15.12
14.96
30
17.48
14.96
30
15.96
12.62
Source: Extracted from L. Johnson and K. McNeilly, “Results 
May Not Vary,” Quality Progress, May 2011, pp. 41–48.
At the 0.05 level of significance,
a.	 is there an interaction between the percentage of ammonium 
and the stir rate?
b.	 is there an effect due to the percentage of ammonium?
c.	 is there an effect due to the stir rate?
d.	 Plot the mean density for each percentage of ammonium for 
each stir rate.
e.	 Discuss the results of (a) through (d).
11.40  An experiment was conducted to try to resolve a problem 
of brake discs overheating at high speed on construction equip-
ment. Five different brake discs were measured by two different 
temperature gauges. The temperature of each brake disc and gauge 
combination was measured at eight different times and the results 
stored in  Brakes .
Source: Data extracted from M. Awad, T. P. Erdmann, V. Shansal, and 
B. Barth, “A Measurement System Analysis Approach for Hard-to-
Repeat Events,” Quality Engineering 21 (2009): 300–305.
At the 0.05 level of significance,
a.	 is there an interaction between the brake discs and the gauges?
b.	 is there an effect due to brake discs?
c.	 is there an effect due to the gauges?
d.	 Plot the mean temperature for each brake disc for each gauge.
e.	 Discuss the results of (a) through (d).

	
Summary	
459
11.4  Fixed Effects, Random Effects,  
and Mixed Effects Models
Sections 11.1 through 11.3 do not consider the distinction between how the levels of a factor 
were selected. The equation for the F test depends on whether the levels of a factor were spe-
cifically selected or randomly selected from a population. The Section 11.4 online topic pres-
ents the appropriate F tests to use when the levels of a factor are either specifically selected or 
randomly selected from a population of levels.
S u m m a r y
In this chapter, various statistical procedures were used to 
analyze the effect of one or two factors of interest. The as-
sumptions required for using these procedures were dis-
cussed in detail. Remember that you need to critically 
investigate the validity of the assumptions underlying the 
hypothesis-testing procedures. Table 11.11 summarizes the 
topics covered in this chapter.
I
n the Arlington’s scenario, you needed to determine 
whether there were differences in mobile electronics sales 
among four in-store locations as well as determine whether 
permitting mobile payments had an effect on those sales.
Using the one-way ANOVA, you determined that there 
was a difference in the mean sales for the four in-store loca-
tions. You then were able to conclude that the mean sales for 
the front location was higher than the current in-aisle or ex-
perimental kiosk or expert locations, that the kiosk location 
mean sales were higher than the in-aisle or expert locations, 
and that there was no evidence of a difference between the 
mean sales for the in-aisle and expert locations. Using the 
two-way ANOVA, you determined that there was no inter-
action between in-store location and permitting mobile pay-
ment methods and that mean sales were higher when mobile 
payment methods were permitted than when such methods 
were not. In addi-
tion, you concluded 
that the population 
mean sales is different for the four in-store locations and 
reached these other conclusions:
 • The front location is estimated to have higher mean 
sales than the other three locations.
 • The kiosk location is estimated to have higher mean 
sales than the current in-aisle location or expert location.
 • The expert location is estimated to have higher mean 
sales than the current in-aisle location.
Your next step as a member of the sales team might be to 
further investigate the differences among the sales locations 
as well as examine other factors that could influence mobile 
electronics sale.
U s i n g  S tat i s t i c s
The Means to Find Differences at  
Arlington’s Revisited
Pavel L Photo and Video/Shutterstock
T a b l e  1 1 . 1 1
Summary of Topics in 
Chapter 11
Type of Analysis (numerical data only)
Number of Factors
Comparing more than two groups
One-way analysis of variance (Section 11.1)
Randomized block design (Section 11.2)
Two-way analysis of variance (Section 11.3)

460	
Chapter 11  Analysis of Variance
Referen c e s
	 1.	Berenson, M. L., D. M. Levine, and M. Goldstein. Inter-
mediate Statistical Methods and Applications: A Computer  
Package Approach. Upper Saddle River, NJ: Prentice Hall, 
1983.
	 2.	Conover, W. J. Practical Nonparametric Statistics, 3rd ed. 
New York: Wiley, 2000.
	 3.	Daniel, W. W. Applied Nonparametric Statistics, 2nd ed.  
Boston: PWS Kent, 1990.
	 4.	Gitlow, H. S., and D. M. Levine. Six Sigma for Green Belts 
and Champions: Foundations, DMAIC, Tools, Cases, and Cer-
tification. Upper Saddle River, NJ: Financial Times/Prentice 
Hall, 2005.
	 5.	Hicks, C. R., and K. V. Turner. Fundamental Concepts in the 
Design of Experiments, 5th ed. New York: Oxford University 
Press, 1999.
	 6.	Kutner, M. H., J. Neter, C. Nachtsheim, and W. Li. Applied 
Linear Statistical Models, 5th ed. New York: McGraw-Hill-
Irwin, 2005.
	 7.	Levine, D. M. Statistics for Six Sigma Green Belts. Upper  
Saddle River, NJ: Financial Times/Prentice Hall, 2006.
	 8.	Microsoft Excel 2013. Redmond, WA: Microsoft Corp., 2012.
	 9.	Minitab Release 16. State College, PA: Minitab, Inc., 2010.
	10.	Montgomery, D. M. Design and Analysis of Experiments,  
6th ed. New York: Wiley, 2005.
K e y  Eq u at i o n s
Total Variation in One-Way ANOVA
SST = a
c
j = 1 a
nj
i = 1
1Xij - X22
(11.1)
Among-Group Variation in One-Way ANOVA
SSA = a
c
j = 1
nj1Xj - X22
(11.2)
Within-Group Variation in One-Way ANOVA
SSW = a
c
j = 1 a
nj
i = 1
1Xij - Xj22
(11.3)
Mean Squares in One-Way ANOVA
MSA =
SSA
c - 1
(11.4a)
MSW = SSW
n - c
(11.4b)
MST =
SST
n - 1
(11.4c)
One-Way ANOVA FSTAT Test Statistic
FSTAT = MSA
MSW
(11.5)
Critical Range for the Tukey-Kramer Procedure
Critical range = QaB
MSW
2
a 1
nj
+ 1
nj′
b
(11.6)
Total Variation in the Randomized Block Design
SST = a
c
j = 1 a
r
i = 1
(Xij - X)2
(11.7)
Among-Group Variation in the Randomized Block Design
SSA = r a
c
j = 1
(X.j - X)2
(11.8)
Among-Block Variation in the Randomized Block Design
SSBL = c a
r
i = 1
(Xi. - X)2
(11.9)
Random Variation in the Randomized Block Design
SSE = a
c
j = 1 a
r
i = 1
(Xij - X.j - Xi. + X)2
(11.10)
Mean Squares in the Randomized Block Design
MSA =
SSA
c - 1
(11.11a)
MSBL = SSBL
r - 1
(11.11b)
MSE =
SSE
(r - 1)(c - 1)
(11.11c)
FSTAT Statistic for Factor Effect
FSTAT = MSA
MSE
(11.12)

	
Key Terms	
461
FSTAT Statistic for Block Effects
FSTAT = MSBL
MSE 
(11.13)
Estimated Relative Efficiency
RE = (r - 1)MSBL + r(c - 1)MSE
(rc - 1)MSE

(11.14)
Critical Range for the Randomized Block Design
Critical range = QaA
MSE
r

(11.15)
Total Variation in Two-Way ANOVA
SST = a
r
i = 1 a
c
j = 1 a
n′
k = 1
1Xijk - X22
(11.16)
Factor A Variation
SSA = cn′ a
r
i = 1
1Xi.. - X22
(11.17)
Factor B Variation
SSB = rn′ a
c
j = 1
1X.j. - X22
(11.18)
Interaction Variation
SSAB = n′ a
r
i = 1 a
c
j = 1
1Xij. - Xi.. - X.j. + X22
(11.19)
Random Variation in Two-Way ANOVA
SSE = a
r
i = 1 a
c
j = 1 a
n′
k = 1
1Xijk - Xij.22
(11.20)
Mean Squares in Two-Way ANOVA
MSA =
SSA
r - 1
(11.21a)
MSB =
SSB
c - 1
(11.21b)
MSAB =
SSAB
1r - 121c - 12
(11.21c)
MSE =
SSE
rc1n′ - 12
(11.21d)
F Test for Factor A Effect
FSTAT = MSA
MSE
(11.22)
F Test for Factor B Effect
FSTAT = MSB
MSE
(11.23)
F Test for Interaction Effect
FSTAT = MSAB
MSE 
(11.24)
Critical Range for Factor A
Critical range = QaA
MSE
cn′ 
(11.25)
Critical Range for Factor B
Critical range = QaA
MSE
rn′ 
(11.26)
K ey Te r ms
among-block variation  439
among-group variation  424
analysis of variance (ANOVA)  423
ANOVA summary table  427
blocks  438
cell means  454
completely randomized design  423
critical range  431
estimated relative efficiency  443
F distribution  426
factor  423
grand mean, X  424
groups  423
homogeneity of variance  433
interaction  447
levels  423
Levene test  433
main effect  451
mean squares  425
multiple comparisons  430
normality  433
one-way ANOVA  423
randomized block design  438
randomness and independence  433
replicates  447
Studentized range distribution  431
sum of squares among blocks (SSBL)  439

462	
Chapter 11  Analysis of Variance
C ha pter  R e vi e w P r o b le ms
11.52  You are the production manager at a parachute manu-
facturing company. Parachutes are woven in your factory using 
a synthetic fiber purchased from one of four different suppliers.  
The strength of these fibers is an important characteristic that en-
sures quality parachutes. You need to decide whether the synthetic 
fibers from each of your four suppliers result in parachutes of 
equal strength. Furthermore, to produce parachutes your factory 
uses two types of looms, the Jetta and the Turk. You need to de-
termine if the parachutes woven on each type of loom are equally 
strong. You also want to know if any differences in the strength of 
the parachute can be attributed to the four suppliers are dependent 
on the type of loom used. You conduct an experiment in which 
five different parachutes from each supplier are manufactured on 
each of the two different looms and collect and store the data in  
 ParachuteTwoWay .
At the 0.05 level of significance,
a.	 is there an interaction between supplier and loom?
b.	 is there an effect due to loom?
c.	 is there an effect due to supplier?
d.	 Plot the mean strength for each supplier for each loom.
e.	 If appropriate, use the Tukey procedure to determine differ-
ences between suppliers.
f.	 Repeat the analysis, using the  Parachuteoneway  file with sup-
pliers as the only factor. Compare your results to those of (b) 
and (e).
11.53  Medical wires are used in the manufacture of cardiovas-
cular devices. A study was conducted to determine the effect of 
several factors on the ratio of the load on a test specimen (YS) 
to the ultimate tensile strength (UTS). The file  MedicalWires1  
contains the study results, which examined factors including the  
machine (W95 vs. W96) and the reduction angle (narrow vs. 
wide). (Data extracted from B. Nepal, S. Mohanty, and L. Kay, 
“Quality Improvement of Medical Wire Manufacturing Process,” 
Quality Engineering 25, 2013, pp. 151–163.)
At the 0.05 level of significance,
a.	 is there an interaction between machine type and reduction  
angle?
b.	 is there an effect due to machine type?
c.	 is there an effect due to reduction angle?
d.	 Plot the mean  ratio of the load on a test specimen (YS) to the 
ultimate tensile strength (UTS) for each machine type for each 
reduction angle.
e.	 What can you conclude about the effects of machine type and 
reduction angle on the ratio of the load on a test specimen (YS) 
to the ultimate tensile strength (UTS)? Explain.
f.	 Repeat the analysis, using reduction angle as the only factor 
(see the  MedicalWires2  file. Compare your results to those of 
(c) and (e).
sum of squares among groups (SSA)  425
sum of squares due to factor A (SSA)  447
sum of squares due to factor B (SSB)  448
sum of squares due to interaction 
(SSAB)  448
sum of squares error (SSE)  439
sum of squares total (SST)  423
sum of squares within groups (SSW)  425
total variation  424
Tukey multiple comparison procedure  
for randomized block designs  443
Tukey multiple comparisons procedure  
for two-way ANOVA  452
Tukey-Kramer multiple comparisons 
­procedure for one-way ANOVA  430
two-factor factorial design  446
two-way ANOVA  446
within-group variation  424
C hec ki n g  Yo u r  U n d e r s ta nding
11.41  In a one-way ANOVA, what is the difference between the 
among-groups variance MSA and the within-groups variance MSW?
11.42  What is the difference between the completely random-
ized one-way ANOVA design and the randomized block design?
11.43  What are the distinguishing features of the completely 
randomized design, the randomized block design, and two-factor 
factorial designs?
11.44  What are the assumptions of ANOVA?
11.45  Under what conditions should you use the one-way 
ANOVA F test to examine possible differences among the means 
of c independent populations?
11.46  What is the purpose of using a Tukey-Kramer multiple 
comparisons procedure and a Tukey multiple comparisons 
procedure?
11.47  Justify why using randomized block design is often advan-
tageous in reducing random error.
11.48  What is the difference between the one-way ANOVA F 
test and the Levene test?
11.49  Do all the assumptions of one-way ANOVA apply to ran-
domized block design? Is any additional assumption applicable to 
randomized block design?
11.50  In a completely randomized design, describe the process 
of partitioning the total variation.
11.51  Which graphs can be used to visualize the distribution of 
each c group when sample sizes in each of the groups is large?

	
Chapter Review Problems	
463
11.54  An operations manager wants to examine the effect of 
air-jet pressure (in pounds per square inch [psi]) on the breaking 
strength of yarn. Three different levels of air-jet pressure are to 
be considered: 30 psi, 40 psi, and 50 psi. A random sample of 18 
yarns are selected from the same batch, and the yarns are randomly 
assigned, 6 each, to the 3 levels of air-jet pressure. The breaking 
strength scores are stored in  Yarn .
a.	 Is there evidence of a significant difference in the variances 
of the breaking strengths for the three air-jet pressures? (Use 
a = 0.05.)
b.	 At the 0.05 level of significance, is there evidence of a difference 
among mean breaking strengths for the three air-jet pressures?
c.	 If appropriate, use the Tukey-Kramer procedure to determine 
which air-jet pressures significantly differ with respect to mean 
breaking strength. (Use a = 0.05.)
d.	 What should the operations manager conclude?
11.55  Suppose that, when setting up the experiment in Prob-
lem 11.54, the operations manager is able to study the effect of 
side-to-side aspect in addition to air-jet pressure. Thus, instead of 
the one-factor completely randomized design in Problem 11.54, 
a two-factor factorial design was used, with the first factor, side-
to-side aspect, having two levels (nozzle and opposite) and the 
second factor, air-jet pressure, having three levels (30 psi, 40 psi, 
and 50 psi). A sample of 18 yarns is randomly assigned, 3 to each 
of the 6 side-to-side aspect and pressure level combinations. The 
breaking-strength scores, stored in  Yarn , are as follows:
Desired Time
Type of Breakfast
Early Time 
Period
Late Time  
Period
Continental
1.2
-2.5
Continental
2.1
3.0
Continental
3.3
-0.2
Continental
4.4
1.2
Continental
3.4
1.2
Continental
5.3
0.7
Continental
2.2
-1.3
Continental
1.0
0.2
Continental
5.4
-0.5
Continental
1.4
3.8
American
4.4
6.0
American
1.1
2.3
American
4.8
4.2
American
7.1
3.8
American
6.7
5.5
American
5.6
1.8
American
9.5
5.1
American
4.1
4.2
American
7.9
4.9
American
9.4
4.0
Air-Jet Pressure
Side-to-Side Aspect
30 psi
40 psi
50 psi
Nozzle
25.5
24.8
23.2
Nozzle
24.9
23.7
23.7
Nozzle
26.1
24.4
22.7
Opposite
24.7
23.6
22.6
Opposite
24.2
23.3
22.8
Opposite
23.6
21.4
24.9
bed in each room. If the customer wishes to receive a room service 
breakfast, he or she places the order form on the doorknob before 
11 p.m. The current system requires customers to select a 15-minute 
interval for desired delivery time (6:30–6:45 a.m., 6:45–7:00 a.m., 
etc.). The new system is designed to allow the customer to request 
a specific delivery time. The hotel wants to measure the difference 
(in minutes) between the actual delivery time and the requested de-
livery time of room service orders for breakfast. (A negative time 
means that the order was delivered before the requested time. A 
positive time means that the order was delivered after the requested 
time.) The factors included were the menu choice (American or 
Continental) and the desired time period in which the order was 
to be delivered (Early Time Period [6:30–8:00 a.m.] or Late Time 
Period [8:00–9:30 a.m.]). Ten orders for each combination of menu 
choice and desired time period were studied on a particular day. 
The data, stored in  Breakfast , are as follows:
At the 0.05 level of significance,
a.	 is there an interaction between side-to-side aspect and air-jet 
pressure?
b.	 is there an effect due to side-to-side aspect?
c.	 is there an effect due to air-jet pressure?
d.	 Plot the mean yarn breaking strength for each level of side-to-
side aspect for each level of air-jet pressure.
e.	 If appropriate, use the Tukey procedure to study differences 
among the air-jet pressures.
f.	 On the basis of the results of (a) through (e), what conclusions 
can you reach concerning yarn breaking strength? Discuss.
g.	 Compare your results in (a) through (f) with those from the 
completely randomized design in Problem 11.38. Discuss fully.
11.56  A hotel wanted to develop a new system for delivering room 
service breakfasts. In the current system, an order form is left on the 
At the 0.05 level of significance,
a.	 is there an interaction between type of breakfast and desired 
time?
b.	 is there an effect due to type of breakfast?
c.	 is there an effect due to desired time?
d.	 Plot the mean delivery time difference for each desired time for 
each type of breakfast.
e.	 On the basis of the results of (a) through (d), what conclusions 
can you reach concerning delivery time difference? Discuss.

464	
Chapter 11  Analysis of Variance
11.57  Refer to the room service experiment in Problem 11.56. 
Now suppose that the results are as shown below and stored in  
 Breakfast2 . Repeat (a) through (e), using these data, and compare 
the results to those of (a) through (e) of Problem 11.56.
Desired Time
Type of Breakfast
Early
Late
Continental
1.2
-0.5
Continental
2.1
5.0
Continental
3.3
1.8
Continental
4.4
3.2
Continental
3.4
3.2
Continental
5.3
2.7
Continental
2.2
0.7
Continental
1.0
2.2
Continental
5.4
1.5
Continental
1.4
5.8
American
4.4
6.0
American
1.1
2.3
American
4.8
4.2
American
7.1
3.8
American
6.7
5.5
American
5.6
1.8
American
9.5
5.1
American
4.1
4.2
American
7.9
4.9
American
9.4
4.0
coded weight. For example, a can containing 2.90 ounces was 
given a coded weight of -0.10. Results were stored in  CatFood2 .
Analyze these data and write a report for presentation to the 
team. Indicate the importance of the piece size and the fill height 
on the weight of the canned cat food. Be sure to include a recom-
mendation for the level of each factor that will come closest to 
meeting the target weight and the limitations of this experiment, 
along with recommendations for future experiments that might be 
undertaken.
11.59  Brand valuations are critical to CEOs, financial and 
marketing executives, security analysts, institutional investors, 
and others who depend on well-researched, reliable information 
needed for assessments and comparisons in decision making. 
Millward Brown Optimor has developed the BrandZ Top 100 
Most Valuable Global Brands for WPP, the world’s largest com-
munications services group. Unlike other studies, the BrandZ 
Top 100 Most Valuable Global Brands fuses consumer mea-
sures of brand equity with financial measures to place a finan-
cial value on brands. The file  BrandZTechFinTele  contains the 
brand values for three sectors in the BrandZ Top 100 Most Valu-
able Global Brands for 2013: the technology sector, the financial 
institutions sector, and the telecom sector. (Data extracted from  
bit.ly/18OL5Mu.)
a.	 At the 0.01 level of significance, is there evidence of a differ-
ence in mean brand value among the sectors?
b.	 What assumptions are necessary in order to complete (a)? 
Comment on the validity of these assumptions.
c.	 If appropriate, use the Tukey procedure to determine the sec-
tors that differ in mean rating. (Use a = 0.01.)
11.60  An investor can choose from a very large number of mu-
tual funds. Each mutual fund has its own mix of different types 
of investments. The data in  BestFunds2  present the one-year re-
turn and the three-year annualized return for the 10 best short-term 
bond, long-term bond, and world bond funds, according to the 
U.S. News & World Report. (Data extracted from money.usnews 
.com/mutual-funds/rankings.) Analyze the data and determine 
whether any differences exist in the one-year return and the three-
year annualized return between short-term, long-term, and world 
bond funds. (Use the 0.05 level of significance.)
11.61  An investor can choose from a very large number of mu-
tual funds. Each mutual fund has its own mix of different types 
of investments. The data in  BestFunds3  present the one-year re-
turn and the three-year annualized return for the 10 best small cap 
growth, mid-cap growth, and large cap growth funds, according 
to the U.S. News & World Report. (Data extracted from money 
.usnews.com/mutual-funds/rankings.) Analyze the data and de-
termine whether any differences exist in the one-year return and 
the three-year annualized return between small cap growth, mid-
cap growth, and large cap growth funds. (Use the 0.05 level of 
significance.)
11.58  A pet food company has the business objective of hav-
ing the weight of a can of cat food come as close to the speci-
fied weight as possible. Realizing that the size of the pieces of 
meat contained in a can and the can fill height could impact the 
weight of a can, a team studying the weight of canned cat food 
wondered whether the current larger chunk size produced higher 
can weight and more variability. The team decided to study the ef-
fect on weight of a cutting size that was finer than the current size. 
In addition, the team slightly lowered the target for the sensing 
mechanism that determines the fill height in order to determine the 
effect of the fill height on can weight.
Twenty cans were filled for each of the four combinations 
of piece size (fine and current) and fill height (low and current). 
The contents of each can were weighed, and the amount above or 
below the label weight of 3 ounces was recorded as the variable  

	
Cases for Chapter 11	
465
C a s e s  f o r  C h a p t e r  1 1
Managing Ashland MultiComm Services
Phase 1
The computer operations department had a business objec-
tive of reducing the amount of time to fully update each sub-
scriber’s set of messages in a special secured email system. 
An experiment was conducted in which 24 subscribers were 
selected and three different messaging systems were used. 
Eight subscribers were assigned to each system, and the up-
date times were measured. The results, stored in  AMS11-1 , 
are presented in Table AMS11.1.
T a b l e  A M S 1 1 . 1
Update Times (in seconds) for Three Different 
Systems
System 1
System 2
System 3
38.8
41.8
32.9
42.1
36.4
36.1
45.2
39.1
39.2
34.8
28.7
29.3
48.3
36.4
41.9
37.8
36.1
31.7
41.1
35.8
35.2
43.6
33.7
38.1
1.	Analyze the data in Table AMS11.1 and write a report to 
the computer operations department that indicates your 
findings. Include an appendix in which you discuss the 
reason you selected a particular statistical test to compare 
the three email interfaces.
DO NOT CONTINUE UNTIL THE PHASE 1 EXERCISE 
HAS BEEN COMPLETED.
2.	 Completely analyze these data and write a report to the team 
that indicates the importance of each of the two factors and/
or the interaction between them on the update time. Include 
recommendations for future experiments to perform.
T a b l e  A M S 1 1 . 2
Update Times (in seconds), Based on Messaging 
System and Media Used
Interface
Media
System 1
System 2
System 3
Cable
4.56
4.17
3.53
4.90
4.28
3.77
4.18
4.00
4.10
3.56
3.96
2.87
4.34
3.60
3.18
Fiber
4.41
3.79
4.33
4.08
4.11
4.00
4.69
3.58
4.31
5.18
4.53
3.96
4.85
4.02
3.32
PHASE 2
After analyzing the data in Table AMS11.1, the computer 
operations department team decided to also study the effect 
of the connection media used (cable or fiber).
The team designed a study in which a total of 30 sub-
scribers were chosen. The subscribers were randomly as-
signed to one of the three messaging systems so that there 
were five subscribers in each of the six combinations of the 
two factors—messaging system and media used. Measure-
ments were taken on the updated time. Table AMS11.2 sum-
marizes the results that are stored in  AMS11-2 .
Digital Case
Apply your knowledge about ANOVA in this Digital Case, 
which continues the cereal-fill packaging dispute Digital 
Case from Chapters 7, 9, and 10.
After reviewing CCACC’s latest document (see the Digi-
tal Case for Chapter 10 on page 412), Oxford Cereals has  
released SecondAnalysis.pdf, a press kit that Oxford Cere-
als has assembled to refute the claim that it is guilty of using 
selective data. Review the Oxford Cereals press kit and then 
answer the following questions.
1.	Does Oxford Cereals have a legitimate argument? Why 
or why not?
2.	Assuming that the samples Oxford Cereals has posted 
were randomly selected, perform the appropriate analysis 
to resolve the ongoing weight dispute.
3.	What conclusions can you reach from your results? If you 
were called as an expert witness, would you support the 
claims of the CCACC or the claims of Oxford Cereals? 
Explain.

466	
Chapter 11  Analysis of Variance
Sure Value Convenience Stores
You work in the corporate office for a nationwide conve-
nience store franchise that operates nearly 10,000 stores. 
The per-store daily customer count (i.e., the mean number 
of customers in a store in one day) has been steady, at 900, 
for some time. To increase the customer count, the chain is 
considering cutting prices for coffee beverages. The ques-
tion to be determined is how much to cut prices to increase 
the daily customer count without reducing the gross mar-
gin on coffee sales too much. You decide to carry out an  
experiment in a sample of 24 stores where customer counts 
have been running almost exactly at the national average of 
900. In 6 of the stores, the price of a small coffee will now  
be $0.59, in 6 stores the price of a small coffee will now be 
$0.69, in 6 stores, the price of a small coffee will now be 
$0.79, and in 6 stores, the price of a small coffee will now 
be $0.89. After four weeks of selling the coffee at the new 
price, the daily customer counts in the stores were recorded 
and stored in  CoffeeSales .
1.  Analyze the data and determine whether there is evi-
dence of a difference in the daily customer count, based 
on the price of a small coffee.
2.  If appropriate, determine which mean prices differ in 
daily customer counts.
3.  What price do you recommend for a small coffee?
CardioGood Fitness
Return to the CardioGood Fitness case (stored in  Cardio-
Good Fitness ) first presented on page 111.
1.  Determine whether differences exist between custom-
ers based on the product purchased (TM195, TM498, 
TM798) in their age in years, education in years, an-
nual household income ($), mean number of times the  
customer plans to use the treadmill each week, and mean 
number of miles the customer expects to walk or run 
each week.
2.  Write a report to be presented to the management of  
CardioGood Fitness detailing your findings.
More Descriptive Choices Follow-Up
Follow up the Using Statistics scenario “More Descriptive 
Choices, Revisited” on page 166 by determining whether 
there is a difference between the small, mid-cap, and large 
market cap funds in the three-year return percentages, five-
year return percentages, and ten-year return percentages 
(stored in  Retirement Funds ).
Clear Mountain State Student Surveys
1.  The Student News Service at Clear Mountain State  
University (CMSU) has decided to gather data about 
the undergraduate students who attend CMSU. They 
create and distribute a survey of 14 questions and  
receive responses from 62 undergraduates (stored in  
 UndergradSurvey ).
a.	At the 0.05 level of significance, is there evidence 
of a difference based on academic major in expected 
starting salary, number of social networking sites reg-
istered for, age, spending on textbooks and supplies, 
text messages sent in a week, and the wealth needed 
to feel rich?
b.	At the 0.05 level of significance, is there evidence of a 
difference based on graduate school intention in grade 
point average, expected starting salary, number of so-
cial networking sites registered for, age, spending on 
textbooks and supplies, text messages sent in a week, 
and the wealth needed to feel rich?

	
Cases for Chapter 11	
467
2.  The dean of students at CMSU has learned about the un-
dergraduate survey and has decided to undertake a simi-
lar survey for graduate students at Clear Mountain State. 
She creates and distributes a survey of 14 questions and 
receives responses from 44 graduate students (stored  
in  GradSurvey ). For these data, at the 0.05 level of 
­significance,
a.	is there evidence of a difference based on undergradu-
ate major in age, undergraduate grade point average, 
graduate grade point average, expected salary upon 
graduation, spending on textbooks and supplies, text 
messages sent in a week, and the wealth needed to 
feel rich?
b.	is there evidence of a difference based on gradu-
ate major in age, undergraduate grade point average, 
graduate grade point average, expected salary upon 
graduation, spending on textbooks and supplies, text 
messages sent in a week, and the wealth needed to 
feel rich?
c.	 is there evidence of a difference based on employ-
ment status in age, undergraduate grade point average, 
graduate grade point average, expected salary upon 
graduation, spending on textbooks and supplies, text 
messages sent in a week, and the wealth needed to 
feel rich?

468	
Chapter 11  Analysis of Variance
EG11.1  The Completely Randomized 
Design: One-Way ANOVA
Analyzing Variation in One-Way ANOVA
Key Technique  Use the Section EG2.5 instructions to construct 
scatter plots using stacked data. If necessary, change the levels of 
the factor to consecutive integers beginning with 1, as was done 
for the in-store location sales experiment data in Figure 11.4 on 
page 428.
F Test for Differences Among More Than Two 
Means
Key Technique  Use the DEVSQ (cell range of data of all 
groups) function to compute SST and uses an expression in the 
form SST – DEVSQ (group 1 data cell range) – DEVSQ (group 
2 data cell range) … – DEVSQ (group n data cell range) to com-
pute SSA.
Example  Perform the Figure 11.6 one-way ANOVA for the in-
store location sales experiment shown on page 430.
PHStat  Use One-Way ANOVA.
For the example, open to the DATA worksheet of the Mobile 
Electronics workbook. Select PHStat ➔ Multiple-Sample 
Tests ➔ One-Way ANOVA. In the procedure’s dialog box (shown  
below):
	 1.	 Enter 0.05 as the Level of Significance.
	 2.	 Enter A1:D6 as the Group Data Cell Range.
	 3.	 Check First cells contain label.
	 4.	 Enter a Title, clear the Tukey-Kramer Procedure check box, 
and click OK.
In addition to the worksheet shown in Figure 11.6, this procedure 
creates an ASFData worksheet to hold the data used for the test. 
See the following In-Depth Excel section for a complete descrip-
tion of this worksheet.
In-Depth Excel  Use the COMPUTE worksheet of the One-
Way ANOVA workbook as a template.
The COMPUTE worksheet, and the supporting ASFData work-
sheet, already contains the data for the example. Modifying the 
One-Way ANOVA workbook for use with other problems is 
more difficult than modifications discussed in the previous Excel 
Guides. To modify the workbook:
	 1.	 Paste the data for the problem into the ASFData worksheet, 
overwriting the in-store locations sales experiment data.
In the COMPUTE worksheet (see Figure 11.6):
	 2.	 Edit the SST formula = DEVSQ(ASFData!A1:D6) in cell 
B16 to use the cell range of the new data just pasted into the 
ASFData worksheet.
	 3.	 Edit the cell B13 SSA formula so there are as many 
DEVSQ(group column cell range) terms as there are groups.
	 4.	 Change the level of significance in cell G17, if necessary.
	 5.	 If the problem contains three groups, select row 8, right-click, 
and select Delete from the shortcut menu.
	
	 If the problem contains more than four groups, select row 8, 
right-click, and click Insert from the shortcut menu. Repeat 
this step as many times as necessary.
	 6.	 If you inserted new rows, enter (not copy) the formulas for 
those rows, using the formulas in row 7 as models.
	 7.	 Adjust table formatting as necessary.
Read the Short Takes for Chapter 11 for an explanation of 
the formulas found in the COMPUTE worksheet (shown in the 
COMPUTE_FORMULAS worksheet). If you use an Excel ver-
sion older than Excel 2010, use the COMPUTE_OLDER work-
sheet.
Analysis ToolPak  Use Anova: Single Factor.
For the example, open to the DATA worksheet of the Mobile 
Electronics workbook and:
	 1.	 Select Data ➔ Data Analysis.
	 2.	 In the Data Analysis dialog box, select Anova: Single Factor 
from the Analysis Tools list and then click OK.
In the procedure’s dialog box (shown below):
	 3.	 Enter A1:D6 as the Input Range.
	 4.	 Click Columns, check Labels in First Row, and enter 0.05 as 
Alpha.
	 5.	 Click New Worksheet Ply.
	 6.	 Click OK.
C h a p t e r  1 1  E x c e l  G u i d e

	
Chapter 11 Excel Guide	
469
The Analysis ToolPak creates a worksheet that does not use  
formulas but is similar in layout to the Figure 11.6 worksheet on 
page 430.
Multiple Comparisons: The Tukey-Kramer 
Procedure
Key Technique  Use formulas to compute the absolute mean 
differences and use the IF function to compare pairs of means.
Example  Perform the Figure 11.7 Tukey-Kramer procedure for 
the in-store location sales experiment shown on page 430.
PHStat  Use the PHStat instructions for the one-way ANOVA 
F test to perform the Tukey-Kramer procedure, checking 
Tukey-Kramer Procedure instead in step 4. The procedure 
creates a worksheet identical to the one shown in Figure 11.7 
on page 432 and discussed in the following In-Depth Excel sec-
tion. To complete the worksheet, enter the Studentized range 
Q statistic (use Table E.7) for the level of significance and the 
numerator and denominator degrees of freedom that are given 
in the worksheet.
In-Depth Excel  To perform the Tukey-Kramer procedure, first 
use the In-Depth Excel instructions for the one-way ANOVA F test 
and then use the appropriate “TK” worksheet in the One-Way 
ANOVA workbook.
For the example, open to the TK4 worksheet that already has 
the value of the Q statistic (4.05) entered in cell B15.
The TK worksheets can be used for problems using three 
(TK3), four (TK4), five (TK5), six (TK6), or seven (TK7) groups. 
Use Table E.7 to look up the proper value of the Studentized 
range Q statistic for the level of significance and the numerator  
and denominator degrees of freedom for the problem. When you 
use either the TK5, TK6, and TK7 worksheets, you must also enter  
the name, sample mean, and sample size for the fifth and, if appli-
cable, sixth and seventh groups.
Read the Short Takes for Chapter 11 for an explanation of 
the formulas found in the COMPUTE worksheet (shown in the 
COMPUTE_FORMULAS worksheet). If you use an Excel  
version older than Excel 2010, use the TK4_OLDER worksheet.
Analysis ToolPak  Modify the previous In-Depth Excel instruc-
tions to perform the Tukey-Kramer procedure in conjunction with 
using the Anova: Single Factor procedure. Transfer selected val-
ues from the Analysis ToolPak results worksheet to one of the TK 
worksheets in the One-Way ANOVA workbook. For example, to 
perform the Figure 11.7 Tukey-Kramer procedure for the in-store 
location sales experiment on page 432:
	 1.	 Use the Anova: Single Factor procedure, as described earlier 
in this section, to create a worksheet that contains ANOVA re-
sults for the in-store locations experiment.
	 2.	 Record the name, sample size (in the Count column), and sam-
ple mean (in the Average column) of each group. Also record 
the MSW value, found in the cell that is the intersection of the 
MS column and Within Groups row, and the denominator  
degrees of freedom, found in the cell that is the intersection of 
the df column and Within Groups row.
	 3.	 Open to the TK4 worksheet of the One-Way ANOVA  
workbook.
In the TK4 worksheet:
	 4.	 Overwrite the formulas in cell range A5:C8 by entering the 
name, sample mean, and sample size of each group into that 
range.
	 5.	 Enter 0.05 as the Level of significance in cell B11.
	 6.	 Enter 4 as the Numerator d.f. (equal to the number of groups) 
in cell B12.
	 7.	 Enter 16 as the Denominator d.f in cell B13.
	 8.	 Enter 0.0439 as the MSW in cell B14.
	 9.	 Enter 4.05 as the Q Statistic in cell B15. (Look up the Studen-
tized range Q statistic using Table E.7.)
Levene Test for Homogeneity of Variance
Key Technique  Use the techniques for performing a one-way 
ANOVA.
Example  Perform the Figure 11.8 Levene test for the in-store 
location sales experiment shown on page 434.
PHStat  Use Levene Test.
For the example, open to the DATA worksheet of the Mobile 
Electronics workbook. Select PHStat ➔ Multiple-Sample Tests 
➔ Levene Test. In the procedure’s dialog box (shown below):
	 1.	 Enter 0.05 as the Level of Significance.
	 2.	 Enter A1:D6 as the Sample Data Cell Range.
	 3.	 Check First cells contain label.
	 4.	 Enter a Title and click OK.
The procedure creates a worksheet that performs the Table 11.4 
absolute differences computations (see page 434) as well as the 
Figure 11.8 worksheet. See the following In-Depth Excel section 
for a description of these worksheets.
In-Depth Excel  Use the COMPUTE worksheet of the Levene 
workbook as a template.
The COMPUTE worksheet and the supporting AbsDiffs and 
DATA worksheets already contain the data for the example.
For other problems in which the absolute differences are al-
ready known, paste the absolute differences into the AbsDiffs 

470	
Chapter 11  Analysis of Variance
worksheet. Otherwise, paste the problem data into the DATA 
worksheet, add formulas to compute the median for each group, 
and adjust the AbsDiffs worksheet as necessary. For example, 
for the in-store location sales experiment, the following steps 1 
through 7 were done with the workbook open to the DATA work-
sheet:
	 1.	 Enter the label Medians in cell A7, the first empty cell in  
column A.
	 2.	 Enter the formula =MEDIAN(A2:A6) in cell A8. (Cell 
range A2:A6 contains the data for the first group, Supplier 1.)
	 3.	 Copy the cell A8 formula across through column D.
	 4.	 Open to the AbsDiffs worksheet.
In the AbsDiffs worksheet:
	 5.	 Enter row 1 column headings AbsDiff1, AbsDiff2, AbsDiff3, 
and AbsDiff4 in columns A through D.
	 6.	 Enter the formula =ABS(DATA!A2 – DATA!A8) in cell A2. 
Copy this formula down through row 6.
	 7.	 Copy the formulas now in cell range A2:A6 across through 
column D. Absolute differences now appear in the cell range 
A2:D6.
If you use an Excel version older than Excel 2010, use the  
COMPUTE_OLDER worksheet.
Analysis ToolPak  Use Anova: Single Factor with absolute 
difference data to perform the Levene test. If the absolute differ-
ences have not already been computed, use steps 1 through 7 of 
the preceding In-Depth Excel instructions to compute them.
EG11.2  The Randomized Block  
Design
Key Technique  Use the F.INV.RT, F.DIST.RT, and DEVSQ 
functions to help compute the ANOVA summary table statistics 
for a randomized block design. Enter F.INV.RT(level of signifi-
cance, degrees of freedom for source, Error degrees of freedom) 
to compute the F critical value for the among-groups (A) and 
among-blocks (BL) sources of variation. Enter F.DIST.RT(F test 
statistic for source, degrees of freedom for rows, Error degrees 
of freedom within groups) to calculate the p-value for the two 
sources of variation. Use the DEVSQ function to compute SSA, 
SSBL, SSE, and SST.
Example  Perform the Figure 11.10 randomized block design for 
the quick-service restaurant chain study on page 442.
PHStat  Use Randomized Block Design.
For the example, open to the DATA worksheet of the QSR 
Chain workbook. Select PHStat ➔ Multiple-Sample Tests ➔ 
Randomized Block Design. In the procedure’s dialog box (shown 
at top right):
	 1.	 Enter 0.05 as the Level of Significance.
	 2.	 Enter A1:E7 as the Sample Data Cell Range.
	 3.	 Check First cells contain label.
	 4.	 Enter a Title and click OK.
This procedure requires that the labels that identify factor A appear 
stacked in column A, followed by columns for factor B.
In-Depth Excel  Use the COMPUTE worksheet of the Ran-
domized Block workbook as a model.
For the example, the worksheet already uses the contents of the 
DATA worksheet to perform the test for the example. In the work-
sheet ANOVA summary table, the source labeled Among groups 
(A) in Table 11.6 on page 441 is labeled Columns, and the source 
Among blocks (BL) is labeled Rows.
For problems with the same number of levels for each fac-
tor as the example, paste the data for the problem into the DATA 
worksheet, overwriting the restaurant rating data. Because of 
the complexity of the COMPUTE worksheet, consider using ei-
ther PHStat or the Analysis ToolPak for other problems that have 
a different mix of factors and levels. If that is not possible, use 
the instructions in the Short Takes for Chapter 11 to modify the 
Randomized Block workbook.
Read the Short Takes for Chapter 11 for an explanation of 
the formulas found in the COMPUTE worksheet (shown in the 
COMPUTE_FORMULAS worksheet). If you are using an older 
Excel version, use the COMPUTE_OLDER worksheet instead of 
the COMPUTE worksheet.
Analysis ToolPak  Use the Anova: Two-Factor Without  
Replication.
For the example, open to the DATA worksheet of the QSRChain 
workbook and:
	 1.	 Select Data ➔ Data Analysis.
	 2.	 In the Data Analysis dialog box, select Anova: Two-Factor 
Without Replication from the Analysis Tools list and then 
click OK.
In the procedure’s dialog box (shown below):
	 3.	 Enter A1:E7 as the Input Range.
	 4.	 Check Labels and enter 0.05 as Alpha.
	 5.	 Click New Worksheet Ply.
	 6.	 Click OK to create the worksheet.

	
Chapter 11 Excel Guide	
471
This procedure requires that the labels that identify blocks appear 
stacked in column A and that group names appear in row 1, start-
ing with cell B1.
The Analysis ToolPak creates a worksheet that is visually 
similar to Figure 11.10 but contains only values and does not in-
clude any cell formulas. The ToolPak worksheet also does not con-
tain the level of significance in row 24.
EG11.3  The Factorial Design:  
Two-Way ANOVA
Key Technique  Use the DEVSQ function to compute SSA, 
SSB, SSAB, SSE, and SST.
Example  Perform the Figure 11.14 two-way ANOVA for the 
in-store location sales and mobile payment experiment shown on 
page 451.
PHStat  Use Two-Way ANOVA with replication.
For the example, open to the DATA worksheet of the Mobile 
Electronics2 workbook. Select PHStat ➔ Multiple-Sample 
Tests ➔ Two-Way ANOVA. In the procedure’s dialog box (shown 
below):
	 1.	 Enter 0.05 as the Level of Significance.
	 2.	 Enter A1:E11 as the Sample Data Cell Range.
	 3.	 Check First cells contain label.
	 4.	 Enter a Title and click OK.
This procedure requires that the labels that identify factor A appear 
stacked in column A, followed by columns for factor B.
In-Depth Excel  Use the COMPUTE worksheet of the Two-
Way ANOVA workbook as a model.
For the example, the worksheet already uses the contents of the 
DATA worksheet to perform the test for the example.
Because of the complexity of the COMPUTE worksheet, 
consider using either PHStat or the Analysis ToolPak for other 
problems, especially ones that have a different mix of factors and 
levels. If that is not possible, use the instructions in the Short 
Takes for Chapter 11 to modify the Two-Way workbook. For 
problems in which r = 2 and c = 4, paste the data for the prob-
lem into the ATFData worksheet, overwriting the in-store loca-
tion and mobile payments data and then adjust the factor level 
headings in the COMPUTE worksheet.
Read the Short Takes for Chapter 11 for an explanation of 
the formulas found in the COMPUTE worksheet (shown in the 
COMPUTE_FORMULAS worksheet). If you use an Excel  
version older than Excel 2010, use the COMPUTE_OLDER 
worksheet instead of the COMPUTE worksheet.
Analysis ToolPak  Use Anova: Two-Factor With Replication.
For the example, open to the DATA worksheet of the  Mobile 
Electronics2  workbook and:
	 1.	 Select Data ➔ Data Analysis.
	 2.	 In the Data Analysis dialog box, select Anova: Two-Factor 
With Replication from the Analysis Tools list and then  
click OK.
In the procedure’s dialog box (shown below):
	 3.	 Enter A1:E11 as the Input Range.
	 4.	 Enter 5 as the Rows per sample.
	 5.	 Enter 0.05 as Alpha.
	 6.	 Click New Worksheet Ply.
	 7.	 Click OK.
This procedure requires that the labels that identify factor A  
appear stacked in column A, followed by columns for factor B. 
The Analysis ToolPak creates a worksheet that does not use  
formulas but is similar in layout to the Figure 11.14 worksheet.
Visualizing Interaction Effects:  
The Cell Means Plot
Key Technique  Use the SUMPRODUCT(cell range 1, cell 
range 2) function to compute the expected value and variance.
Example  Construct the Figure 11.17 cell means plot for mobile 
electronics sales based on mobile payments permitted and in-store 
location on page 454.
PHStat  Modify the PHStat instructions for the two-way 
ANOVA. In step 4, check Cell Means Plot before clicking OK.
In-Depth Excel  Create a cell means plot from a two-way 
ANOVA COMPUTE worksheet.
For the example, open to the COMPUTE worksheet of the Two-
Way ANOVA workbook and:
	 1.	 Insert a new worksheet.
	 2.	 Copy cell range B3:E3 of the COMPUTE worksheet (the fac-
tor B level names) to cell B1 of the new worksheet, using the 
Paste Special Values option.

472	
Chapter 11  Analysis of Variance
	 3.	 Copy the cell range B7:E7 of the COMPUTE worksheet (the 
AVERAGE row for the factor A No level) and paste to cell B2 
of the new worksheet, using the Paste Special Values option.
	 4.	 Copy the cell range B13:E13 of the COMPUTE worksheet 
(the AVERAGE row for the factor A Yes level) and paste to 
cell B3 of a new worksheet, using the Paste Special Values 
option.
	 5.	 Enter No in cell B3 and Yes in cell A3 of the new worksheet as 
labels for the factor A levels.
	 6.	 Select the cell range A1:E3.
	 7.	 Select Insert ➔ Line and select the fourth 2-D Line gallery 
choice (Line with Markers).
	 8.	 Relocate the chart to a chart sheet, add axis titles, and modify 
the chart title by using the instructions in Appendix Section B.6.
For other problems, insert a new worksheet and first copy and 
paste the factor B level names to row 1 of the new worksheet and 
then copy and use Paste Special to transfer the values in the Aver-
age rows data for each factor B level to the new worksheet. (See 
Appendix B to learn more about the Paste Special command.)
Analysis ToolPak  Use the In-Depth Excel instructions.
MG11.1  The Completely Randomized 
Design: One-Way ANOVA
Analyzing Variation in One-Way ANOVA
Use Main Effects Plot (requires stacked data).
For example, to construct the Figure 11.4 main effects plot for 
the in-store location sales experiment on page 428, open to the 
Mobile Electronics Stacked worksheet. Select Stat ➔ ANOVA 
➔ Main Effects Plot. In the Main Effects Plot dialog box (shown 
below):
	 1.	 Double-click C2  Sales in the variables list to add Sales to the 
Responses box and press Tab.
	 2.	 Double-click C1  Location in the variables list to add Loca-
tion to the Factors box.
	 3.	 Click OK.
In step 2, if the column entered in the Factors box contains a text 
variable, as it does in the example, Minitab will sort the factor 
levels alphabetically. To present levels in a different order, as was 
done in Figure 11.4, right-click one of the factor levels in the chart 
and click Edit X Scale from the shortcut menu. In the Edit Scale 
dialog box, click Specified, type the factor levels in the desired 
order separated by spaces, and click OK.
F Test for Differences Among More  
Than Two Means
Use One-Way (Unstacked) or One-Way (for stacked data.)
For example, to perform the Figure 11.6 one-way ANOVA for  
the in-store location sales experiment on page 430, open to the  
 Mobile Electronics  worksheet. Select Stat ➔ ANOVA ➔ One-
Way (Unstacked). In the One-Way Analysis of Variance dialog 
box (shown below):
	 1.	 Enter C1-C4 in the Responses (in separate columns) box.
	 2.	 Enter 95.0 in the Confidence level box.
	 3.	 Click Comparisons.
In the One-Way Multiple Comparisons dialog box (shown below):
	 4.	 Clear all check boxes.
	 5.	 Click OK.
C h a p t e r  1 1  M i n i ta b  G u i d e

	
Chapter 11 Minitab Guide	
473
	 6.	 Back in the original dialog box, click Graphs.
	In the One-Way Analysis of Variance - Graphs dialog box (not 
shown):
	 7.	 Check Boxplots of data.
	 8.	 Click OK.
	 9.	 Back in the original dialog box, click OK.
When using stacked data, select Stat ➔ ANOVA ➔ One-
Way and in step 1 enter the name of the column that contains the 
variable of interest in the Response box and enter the name of the 
column that contains the factor names in the Factor box.
Multiple Comparisons: The Tukey-Kramer 
Procedure
Use the previous set of instructions to perform the Tukey-Kramer 
procedure, replacing step 4 with:
	 4.	 Check Tukey’s, family error rate and enter 5 in its box.  
(A family error rate of 5 produces comparisons with an overall 
confidence level of 95%.)
Levene Test for Homogeneity of Variance
Use Test for Equal Variances (requires stacked data). 
For example, to perform the Figure 11.8 Levene test for the in-
store location sales experiment on page 434, open to the Mobile 
Electronics Stacked worksheet, which contains the data of the 
Parachute worksheet in stacked order. Select Stat ➔ ANOVA ➔ 
Test for Equal Variances. In the Test for Equal Variances dialog 
box (shown below):
	 1.	 Double-click C2  Sales in the variables list to add Sales to the 
Response box
	 2.	 Double-click C1  Location in the variables list to add Loca-
tion to the Factor box.
	 3.	 Enter 95.0 in the Confidence level box.
	 4.	 Click OK.
The Levene test results shown in Figure 11.8 on page 434  
appear last in the results this procedure creates.
MG11.2  The Randomized Block 
Design
Use Two-Way (requires stacked data).
For example, to perform the Figure 11.10 randomized block de-
sign for the quick-service restaurant chain study shown on page 
442, open to the QSRChain worksheet. Select Stat ➔ ANOVA 
➔ Two-Way. In the Two-Way Analysis of Variance dialog box 
(shown below):
	 1.	 Double-click C3  Rating in the variables list to add Rating to 
the Response box.
	 2.	 Double-click C1  Evaluator in the variables list to add Evalu-
ator to the Row factor box.
	 3.	 Double-click C2  Restaurant in the variables list to add Res-
taurant to the Column factor box.
	 4.	 Check Display means for both the row and column factors.
	 5.	 Enter 95.0 in the Confidence level box.
	 6.	 Check Fit Additive Model.
	 7.	 Click OK.
MG11.3  The Factorial Design:  
Two-Way ANOVA
Use Two-Way (requires stacked data). 
For example, to perform the Figure 11.14 two-way ANOVA for 
the in-store location sales and mobile payment experiment on page 
451, open to the Mobile Electronics2 worksheet. Select Stat ➔ 
ANOVA ➔ Two-Way. In the Two-Way Analysis of Variance dia-
log box (shown on page 474):
	 1.	 Double-click C3  Sales in the variables list to add Sales to the 
Response box.
	 2.	 Double-click C1  Mobile Payments in the variables list to 
add Mobile Payments to the Row factor box.
	 3.	 Double-click C2  Location in the variables list to add  
Location to the Column factor box.
	 4.	 Check Display Means for both the row and column factors.
	 5.	 Enter 95.0 in the Confidence level box.
	 6.	 Click OK.

474	
Chapter 11  Analysis of Variance
Visualizing Interaction Effects:  
The Cell Means Plot
Use Interactions Plot. This procedure requires stacked data.
For example, to construct the Figure 11.17 cell means plot  
for mobile electronic sales based on mobile payments permit-
ted and in-store location shown on page 454, open to the Mobile 
Electronics2 worksheet. Select Stat ➔ ANOVA ➔ Interactions 
Plot. In the Interactions Plot dialog box (shown below):
	 1.	 Double-click C3  Sales in the variables list to add Sales to the 
Responses box and press Tab.
	 2.	 Enter C2-C3 to the Factors box.
	 3.	 Clear Display full interaction plot matrix.
	 4.	 Click OK.

475
U s i n g  S tat i s t i c s
Avoiding Guesswork About  
Resort Guests
You are the manager of T.C. Resort Properties, a collection of five upscale ho-
tels located on two tropical islands. Guests who are satisfied with the quality of 
services during their stay are more likely to return on a future vacation and to 
recommend the hotel to friends and relatives. You have defined the business ob-
jective as improving the percentage of guests who choose to return to the hotels 
later. To assess the quality of services being provided by your hotels, your staff 
encourages guests to complete a satisfaction survey when they check out or via 
email after they check out.
You need to analyze the data from these surveys to determine the overall 
satisfaction with the services provided, the likelihood that the guests will return 
to the hotel, and the reasons some guests indicate that they will not return. For 
example, on one island, T.C. Resort Properties operates the Beachcomber and 
Windsurfer hotels. Is the perceived quality at the Beachcomber Hotel the same as 
at the Windsurfer Hotel? If there is a difference, how can you use this informa-
tion to improve the overall quality of service at T.C. Resort Properties? Further-
more, if guests indicate that they are not planning to return, what are the most 
common reasons cited for this decision? Are the reasons cited unique to a certain 
hotel or common to all hotels operated by T.C. Resort Properties?
contents
12.1  Chi-Square Test for the 
Difference Between Two 
Proportions
12.2  Chi-Square Test for 
Differences Among More 
Than Two Proportions
12.3  Chi-Square Test of 
Independence
12.4  Wilcoxon Rank Sum Test: A 
Nonparametric Method for Two 
Independent Populations
12.5  Kruskal-Wallis Rank Test: A 
Nonparametric Method for 
the One-Way ANOVA
12.6  McNemar Test for the Difference 
Between Two Proportions 
(Related Samples) (online)
12.7  Chi-Square Test for the Variance 
or Standard Deviation (online)
12.8  Wilcoxon Signed Ranks Test: 
A Nonparametric Test for Two 
Related Populations (online)
12.9  Friedman Rank Test: A 
Nonparametric Test for The 
Randomized Block Design (online)
Using Statistics: Avoiding 
Guesswork About Resort Guests, 
Revisited
Chapter 12 Excel Guide
Chapter 12 Minitab Guide
Objectives
To learn when to use the chi-square 
test for contingency tables
To learn to use the Marascuilo 
procedure for determining pairwise 
differences when evaluating more 
than two proportions
To learn to use nonparametric tests
Chapter
Chi-Square and 
Nonparametric Tests
12
Maturos1812>Shutterstock

476	
Chapter 12  Chi-Square and Nonparametric Tests
I
n the preceding three chapters, you used hypothesis-testing procedures to analyze  
both numerical and categorical data. Chapter 9 presented some one-sample tests,  
Chapter 10 developed several two-sample tests, and Chapter 11 discussed analysis of 
variance (ANOVA). This chapter extends hypothesis testing to analyze differences between 
population proportions based on two or more samples and to test the hypothesis of inde-
pendence in the joint responses to two categorical variables. The chapter concludes with  
nonparametric tests as alternatives to several Chapter 10 and 11 hypothesis tests.
In Section 10.3, you studied the Z test for the difference between two proportions. In this sec-
tion, the differences between two proportions are examined from a different perspective. The 
hypothesis-testing procedure uses a test statistic, whose sampling distribution is approximated 
by a chi-square 1x22 distribution. The results of this x2 test are equivalent to those of the Z test 
described in Section 10.3.
If you are interested in comparing the counts of categorical responses between two inde-
pendent groups, you can develop a two-way contingency table to display the frequency of 
occurrence of items of interest and items not of interest for each group. (Contingency tables 
were first discussed in Section 2.1, and in Chapter 4, contingency tables were used to define 
and study probability.)
To illustrate a contingency table, return to the Using Statistics scenario concerning T.C.  
Resort Properties. On one of the islands, T.C. Resort Properties has two hotels (the Beachcomber 
and the Windsurfer). You collect data from customer satisfaction surveys and focus on the re-
sponses to the single question “Are you likely to choose this hotel again?” You organize the 
results of the survey and determine that 163 of 227 guests at the Beachcomber responded yes to 
“Are you likely to choose this hotel again?” and 154 of 262 guests at the Windsurfer responded 
yes to “Are you likely to choose this hotel again?” You want to analyze the results to determine 
whether, at the 0.05 level of significance, there is evidence of a significant difference in guest 
satisfaction (as measured by likelihood to return to the hotel) between the two hotels.
The contingency table displayed in Table 12.1, which has two rows and two columns, is 
called a 2 * 2 contingency table. The cells in the table indicate the frequency for each row-
and-column combination.
12.1  Chi-Square Test for the Difference Between  
Two Proportions
where
X1 = number of items of interest in group 1
X2 = number of items of interest in group 2
n1 - X1 = number of items that are not of interest in group 1
n2 - X2 = number of items that are not of interest in group 2
X = X1 + X2, the total number of items of interest
n - X = 1n1 - X12 + 1n2 - X22, the total number of items that are not of interest
T a b l e  1 2 . 1
Layout of a 2 * 2 
Contingency Table
Column Variable
Row Variable
Group 1
Group 2
Totals
Items of interest
X1
X2
X
Items not of interest
n1 - X1
n2 - X2
n - X
Totals
n1
n2
n

	
12.1  Chi-Square Test for the Difference Between Two Proportions 	
477
n1 = sample size in group 1
n2 = sample size in group 2
n = n1 + n2 = total sample size
Table 12.2 is the contingency table for the hotel guest satisfaction study. The contingency 
table has two rows, indicating whether the guests would return to the hotel or would not return 
to the hotel, and two columns, one for each hotel. The cells in the table indicate the frequency 
of each row-and-column combination. The row totals indicate the number of guests who would 
return to the hotel and the number of guests who would not return to the hotel. The column 
totals are the sample sizes for each hotel location.
To test whether the population proportion of guests who would return to the Beachcomber, 
p1, is equal to the population proportion of guests who would return to the Windsurfer, p2, you 
can use the chi-square 1X22 test for the difference between two proportions. To test the 
null hypothesis that there is no difference between the two population proportions:
H0: p1 = p2
against the alternative that the two population proportions are not the same:
H1: p1 ≠p2
you use the x2
STAT test statistic, shown in Equation (12.1) whose sampling distribution follows 
the chi-square distribution.
T a b l e  1 2 . 2
2 * 2 Contingency 
Table for the Hotel 
Guest Satisfaction 
Survey
Hotel
Choose Hotel Again?
Beachcomber
Windsurfer
Total
Yes
163
154
317
No
64
108
172
Total
227
262
489
Student Tip
Do not confuse this 
use of the Greek letter 
pi, p, to represent the 
population proportion 
with the mathematical 
constant that is the ratio 
of the circumference to a 
diameter of a circle— 
approximately 3.14159—
which is also known by 
the same Greek letter.
Student Tip
You are computing 
the squared difference 
between fo and fe. There-
fore, unlike the ZSTAT and 
tSTAT test statistics, the 
x2
STAT test statistic can 
never be negative.
x2 Test for the Difference Between Two Proportions
The x2
STAT test statistic is equal to the squared difference between the observed and expect-
ed frequencies, divided by the expected frequency in each cell of the table, summed over 
all cells of the table.
	
x2
STAT =
a
all cells
1fo - fe22
fe
	
(12.1)
where
fo = observed frequency in a particular cell of a contingency table
fe = expected frequency in a particular cell if the null hypothesis is true
The x2
STAT test statistic approximately follows a chi-square distribution with 1 degree of 
freedom.1
1In general, the degrees of freedom 
in a contingency table are equal to 
(number of rows -1) multiplied by 
(number of columns -1).
To compute the expected frequency, fe, in any cell, you need to understand that if the null 
hypothesis is true, the proportion of items of interest in the two populations will be equal. In 
such situations, the sample proportions you compute from each of the two groups would differ 
from each other only by chance. Each would provide an estimate of the common population 

478	
Chapter 12  Chi-Square and Nonparametric Tests
parameter, p. A statistic that combines these two separate estimates together into one overall 
estimate of the population parameter provides more information than either of the two separate 
estimates could provide by itself. This statistic, given by the symbol p, represents the estimated 
overall proportion of items of interest for the two groups combined (i.e., the total number of 
items of interest divided by the total sample size). The complement of p, 1 - p, represents the 
estimated overall proportion of items that are not of interest in the two groups. Using the nota-
tion presented in Table 12.1 on page 476, Equation (12.2) defines p.
Student Tip
Remember, the sample 
proportion, p, must be 
between 0 and 1.
Computing the Estimated Overall Proportion for Two Groups
	
p = X1 + X2
n1 + n2
= X
n	
(12.2)
To compute the expected frequency, fe, for cells that involve items of interest (i.e., the cells 
in the first row in the contingency table), you multiply the sample size (or column total) for a 
group by p. To compute the expected frequency, fe, for cells that involve items that are not of 
interest (i.e., the cells in the second row in the contingency table), you multiply the sample size 
(or column total) for a group by 1 - p.
The sampling distribution of the x2
STAT test statistic shown in Equation (12.1) on page 477 
approximately follows a chi-square 1X22 distribution (see Table E.4) with 1 degree of free-
dom. Using a level of significance a, you reject the null hypothesis if the computed x2
STAT test 
statistic is greater than x2
a, the upper-tail critical value from the x2 distribution with 1 degree of 
freedom. Thus, the decision rule is
Reject H0 if x2
STAT 7 x2
a;
otherwise, do not reject H0.
Figure 12.1 illustrates the decision rule.
Student Tip
Remember that the 
rejection region for this 
test is only in the upper 
tail of the chi-square 
distribution.
F i g u r e  1 2 . 1
Regions of rejection and 
nonrejection when using 
the chi-square test for 
the difference between 
two proportions, with 
level of significance a
0
α
Region of
Nonrejection
Critical
Value
Region of
Rejection
(1 – α)
2
χ
If the null hypothesis is true, the computed x2
STAT test statistic should be close to zero 
because the squared difference between what is actually observed in each cell, fo, and what 
is theoretically expected, fe, should be very small. If H0 is false, then there are differences in 
the population proportions, and the computed x2
STAT test statistic is expected to be large. How-
ever, what is a large difference in a cell is relative. Because you are dividing by the expected 
frequencies, the same actual difference between fo and fe from a cell with a small number of 
expected frequencies contributes more to the x2
STAT test statistic than a cell with a large number 
of expected frequencies.
To illustrate the use of the chi-square test for the difference between two proportions, re-
turn to the Using Statistics scenario concerning T.C. Resort Properties on page 475 and the 
corresponding contingency table displayed in Table 12.2 on page 477. The null hypothesis 

	
12.1  Chi-Square Test for the Difference Between Two Proportions 	
479
1H0: p1 = p22 states that there is no difference between the proportion of guests who are 
likely to choose either of these hotels again. To begin,
p = X1 + X2
n1 + n2
= 163 + 154
227 + 262 = 317
489 = 0.6483
p is the estimate of the common parameter p, the population proportion of guests who are likely to 
choose either of these hotels again if the null hypothesis is true. The estimated proportion of guests 
who are not likely to choose these hotels again is the complement of p, 1 - 0.6483 = 0.3517. 
Multiplying these two proportions by the sample size for the Beachcomber Hotel gives the number 
of guests expected to choose the Beachcomber again and the number not expected to choose this 
hotel again. In a similar manner, multiplying the two proportions by the Windsurfer Hotel’s sample 
size yields the corresponding expected frequencies for that group.
Example 12.1
Computing the  
Expected  
Frequencies
Compute the expected frequencies for each of the four cells of Table 12.2 on page 477.
Solution
Yes—Beachcomber: p = 0.6483 and n1 = 227, so fe = 147.16
Yes—Windsurfer: p = 0.6483 and n2 = 262, so fe = 169.84
No—Beachcomber: 1 - p = 0.3517 and n1 = 227, so fe = 79.84
No—Windsurfer: 1 - p = 0.3517 and n2 = 262, so fe = 92.16
Table 12.3 presents these expected frequencies next to the corresponding observed frequencies.
T a b l e  1 2 . 3
Comparing the 
Observed 1fo2 
and Expected 1fe2 
Frequencies
Hotel
Beachcomber
Windsurfer
Choose Hotel Again?
Observed
Expected
Observed
Expected
Total
Yes
163
147.16
154
169.84
317
No
64
79.84
108
92.16
172
Total
227
227.00
262
262.00
489
To test the null hypothesis that the population proportions are equal:
H0: p1 = p2
against the alternative that the population proportions are not equal:
H1: p1 ≠p2
you use the observed and expected frequencies from Table 12.3 to compute the x2
STAT test  
statistic given by Equation (12.1) on page 477. Table 12.4 presents these calculations.
T a b l e  1 2 . 4
Computing the x2
STAT 
Test Statistic for the 
Hotel Guest Satisfaction 
Survey
fo
fe
1 fo −fe2
1 fo −fe22
1 fo −fe22>fe
163
147.16
   15.84
250.91
1.71
154
169.84
-15.84
250.91
1.48
  64
  79.84
-15.84
250.91
3.14
108
  92.16
   15.84
250.91
2.72
9.05
The chi-square 1x22 distribution is a right-skewed distribution whose shape depends solely 
on the number of degrees of freedom. You find the critical value for the x2 test from Table E.4, 
a portion of which is presented in Table 12.5.

480	
Chapter 12  Chi-Square and Nonparametric Tests
The values in Table 12.5 refer to selected upper-tail areas of the x2 distribution. A 2 * 2 
contingency table has 1 degree of freedom because there are two rows and two columns. [The 
degrees of freedom are equal to the (number of rows -1)(number of columns -1).] Using 
a = 0.05, with 1 degree of freedom, the critical value of x2 from Table 12.5 is 3.841. You 
reject H0 if the computed x2
STAT test statistic is greater than 3.841 (see Figure 12.2). Because 
x2
STAT = 9.05 7 3.841, you reject H0. You conclude that the proportion of guests who would 
return to the Beachcomber is different from the proportion of guests who would return to the 
Windsurfer.
T a b l e  1 2 . 5
Finding the Critical 
Value from the Chi-
Square Distribution 
with 1 Degree of 
Freedom, Using 
the 0.05 Level of 
Significance
Cumulative Probabilities
.005
.01
c
.95
.975
.99
.995
Upper-Tail Area
Degrees of Freedom
.995
.99
c
.05
.025
.01
.005
1
c
  3.841
  5.024
  6.635
  7.879
2
0.010
0.020
c
  5.991
  7.378
  9.210
10.597
3
0.072
0.115
c
  7.815
  9.348
11.345
12.838
4
0.207
0.297
c
  9.488
11.143
13.277
14.860
5
0.412
0.554
c
11.071
12.833
15.086
16.750
Region of
Nonrejection
Critical
Value
Region of
Rejection
0
3.841
.05
.95
2
χ
F i g u r e  1 2 . 2
Regions of rejection 
and nonrejection when 
finding the x2 critical 
value with 1 degree of 
freedom, at the 0.05 
level of significance
Figure 12.3 shows the Excel and Minitab results for the Table 12.2 guest satisfaction con-
tingency table on page 477.
F i g u r e  1 2 . 3
Excel and Minitab results 
of the chi-square test 
for the two-hotel guest 
satisfaction survey

	
12.1  Chi-Square Test for the Difference Between Two Proportions 	
481
These results include the expected frequencies, x2
STAT, degrees of freedom, and p-value. 
The computed x2
STAT test statistic is 9.0526, which is greater than the critical value of 3.8415 
(or the p@value = 0.0026 6 0.05), so you reject the null hypothesis that there is no difference 
in guest satisfaction between the two hotels. The p-value, equal to 0.0026, is the probability 
of observing sample proportions as different as or more different from the actual difference 
between the Beachcomber and Windsurfer 10.718 - 0.588 = 0.132 observed in the sample 
data, if the population proportions for the Beachcomber and Windsurfer hotels are equal. Thus, 
there is strong evidence to conclude that the two hotels are significantly different with respect 
to guest satisfaction, as measured by whether a guest is likely to return to the hotel again. From 
Table 12.3 on page 479 you can see that a greater proportion of guests are likely to return to the 
Beachcomber than to the Windsurfer.
For the x2 test to give accurate results for a 2 * 2 table, you must assume that each  
expected frequency is at least 5. If this assumption is not satisfied, you can use alternative  
procedures, such as Fisher’s exact test (see references 1, 2, and 4).
In the hotel guest satisfaction survey, both the Z test based on the standardized normal 
distribution (see Section 10.3) and the x2 test based on the chi-square distribution lead to the 
same conclusion. You can explain this result by the interrelationship between the standardized 
normal distribution and a chi-square distribution with 1 degree of freedom. For such situations, 
the x2
STAT test statistic is the square of the ZSTAT test statistic. 
For example, in the guest satisfaction study, the computed ZSTAT test statistic is +3.0088, 
and the computed x2
STAT test statistic is 9.0526. Except for rounding differences, this 9.0526 
value is the square of +3.0088 [i.e., 1+3.008822 ≅9.0526]. Also, if you compare the critical 
values of the test statistics from the two distributions, at the 0.05 level of significance, the x2 
value of 3.841 with 1 degree of freedom is the square of the Z value of {1.96. Furthermore, 
the p-values for both tests are equal. Therefore, when testing the null hypothesis of equality of 
proportions:
H0: p1 = p2
against the alternative that the population proportions are not equal:
H1: p1 ≠p2
the Z test and the x2 test are equivalent. If you are interested in determining whether there is 
evidence of a directional difference, such as p1 7 p2, you must use the Z test, with the entire 
rejection region located in one tail of the standardized normal distribution.
In Section 12.2, the x2 test is extended to make comparisons and evaluate differences 
between the proportions among more than two groups. However, you cannot use the Z test if 
there are more than two groups.
Problems for Section 12.1
Learning the Basics
12.1  Determine the critical value of x2 with 1 degree of freedom 
in each of the following circumstances:
a.	 a = 0.01
b.	 a = 0.005
c.	 a = 0.10
12.2  Determine the critical value of x2 with 1 degree of freedom 
in each of the following circumstances:
a.	 a = 0.05
b.	 a = 0.025
c.	 a = 0.01
12.3  The following table shows (fictitious) data for vaccination 
and recorded cases of flu infections:
Vaccination
Flu Infection
No Flu Infection
Total
Yes
50
762
812
No
190
2246
2436
Total
240
3008
3248
a.	 At 0.05 significance level, Compute x2
STAT.
b.	 Is vaccination helpful in controlling flu?

482	
Chapter 12  Chi-Square and Nonparametric Tests
12.4  The following data is collected for the use of fertilizers by 
owners and tenants:
Owners
Tenants
Total
Fertilizers
400
200
600
No Fertilizers
250
150
400
Total
650
350
1000
a.	 At 0.05 significance level, compute x2
STAT.
b.	 Do owners use more fertilizers?
Applying the Concepts
12.5  A study has been conducted on the rate of depression and 
their relation to demographic features such as age, race, gender, 
etc. The survey was administered to 155 patients and it was found 
women are more likely to be depressed compared to men (Data 
extracted from: Gottlieb SS, Khatta M, Friedmann E, et al. The in-
fluence of age, gender, and race on the prevalence of depression in 
heart failure patients. J Am Coll Cardiol. 2004; 43(9):1542-1549. 
doi:10.1016/j.jacc.2003.10.064.). The following data related to the 
depression and the gender has been imported from the study:
Depressed
Not Depressed
Total
Men
54
68
122
Women
21
12
33
Total
75
80
155
a.	 Set up the null and alternative hypotheses to determine whether 
there is a difference in the depression levels of men and women.
b.	 At 0.05 significance level, compute x2
STAT. Is there any evi-
dence of a significant difference between the proportion of men 
and women and their depression levels?
c.	 Determine the p-value in (a) and interpret its meaning.
12.6  A research has been conducted to find if dogs resemble 
their owners. The finding of the research was that owners tend to 
­select dogs that in some way, resemble them and the resemblance 
increases with the duration of ownership. Assume that this finding 
is specific to a particular breed of dogs and the following data has 
been collected:
 
Resemble 
owner
Do not  
Resemble Owner
Total
Specific Breed
20
11
31
Other Dogs
12
17
29
Total
32
28
60
a.	 Set up the null and alternative hypotheses to determine whether 
there is a difference in resemblance of the dogs of a specific 
breed with their owners.
b.	 At the 0.05 level of significance, calculate chi-square statis-
tic to test the hypothesis that the specific breed dogs resemble 
their owners.
c.	 Determine the p-value in (b) and interpret its meaning.
12.7  Do males or females feel more tense or stressed out at 
work? A survey of employed adults conducted online by Harris 
Interactive on behalf of the American Psychological Association 
revealed the following:
Felt Tense or Stressed  
Out at Work
Gender
Yes
No
Male
244
495
Female
282
480
Source: Data extracted from “The 2013 Work and Well-Being 
Survey,” American Psychological Association and Harris 
Interactive, March 2013, p. 5, bit.ly/11JGcPf.
a.	 At the 0.05 level of significance, is there evidence of a differ-
ence between males and females in the proportion who feel 
stressed out at work?
b.	 Determine the p-value in (a) and interpret its meaning.
SELF 
Test 
12.8  Consumer research firm Scarborough analyzed the 
10% of American adults who are either “Superbanked” or 
“Unbanked.” Superbanked consumers are defined as U.S. adults who 
live in a household that has multiple asset accounts at financial institu-
tions, as well as some additional investments; Unbanked consumers 
are U.S. adults who live in a household that does not use a bank or 
credit union. By finding the 5% of Americans who are Superbanked, 
Scarborough identifies financially savvy consumers who might be 
open to diversifying their financial portfolios; by identifying the Un-
banked, Scarborough provides insight into the ultimate prospective 
client for banks and financial institutions. As part of its analysis, Scar-
borough reported that 93% of Superbanked consumers use credit 
cards as compared to 23% of Unbanked consumers. (Data extracted 
from bit.ly/Syi9kN.) Suppose that these results were based on 
1,000 Superbanked consumers and 1,000 Unbanked consumers.
a.	 At the 0.01 level of significance, is there evidence of a signifi-
cant difference between the Superbanked and the Unbanked 
with respect to the proportion that use credit cards?
b.	 Determine the p-value in (a) and interpret is meaning.
c.	 Compare the results of (a) and (b) to those of Problem 10.32 on 
page 401.
12.9  A research report states that an adolescent’s overall health im-
pacts their physical and psychological well being. The report looks 
at the direct relationship between nutritional intake and academic 
achievement. Milk intake was looked at specifically, as it is rich in 
calcium and contributes to bone growth. However, it was found that 
teens consume twice as much soft drink as milk. Assume the follow-
ing data is collected:
Variations
Drink 
milk
Do not drink 
milk
Good PE Grades
67
33
Not good PE grades
22
48

	
12.2  Chi-Square Test for Differences Among More Than Two Proportions 	
483
a.	 Set up the null and alternative hypotheses to determine whether 
there is a difference in the consumption of milk and the grades 
in physical education.
b.	 At the 0.05 level of significance, calculate chi-square statistic 
to test the hypothesis if there is a difference in the consumption 
of milk and grades in physical education.
c.	 Determine the p-value in (b) and interpret its meaning.
12.10  Does gamification motivate customer research manage-
ment (CRM) utilization? Gamification is the use of game me-
chanics to motivate, modify, or reward distinct behaviors. In the 
context of sales effectiveness, it is deployed to encourage both 
sales accomplishments and nonsales activities. A survey of end-
user sales organizations indicates that 31 of 37 gamification-user 
organizations provide mobile access to CRM, whereas 138 of 275 
non-gamification-user organizations provide mobile access to 
CRM. (Data extracted from bit.ly/12Omho7.)
a.	 Construct a 2 * 2 contingency table.
b.	 At the 0.05 level of significance, is there evidence of a differ-
ence between gamification-user sales organizations and non-
gamification-user sales organizations in the proportion that 
provide mobile access to CRM?
c.	 Find the p-value in (a) and interpret its meaning.
d.	 Compare the results of (a) and (b) to those of Problem 10.34 on 
page 401.
12.2  Chi-Square Test for Differences Among More  
Than Two Proportions
In this section, the x2 test is extended to compare more than two independent populations. The 
letter c is used to represent the number of independent populations under consideration. Thus, 
the contingency table now has two rows and c columns. To test the null hypothesis that there 
are no differences among the c population proportions:
H0: p1 = p2 = g = pc
against the alternative that not all the c population proportions are equal:
H1: Not all pj are equal 1where j = 1, 2, c, c2
you use Equation (12.1) on page 477:
x2
STAT =
a
all cells
1fo - fe22
fe
where
fo = observed frequency in a particular cell of a 2 * c contingency table
fe = expected frequency in a particular cell if the null hypothesis is true
If the null hypothesis is true and the proportions are equal across all c populations, the  
c sample proportions should differ only by chance. In such a situation, a statistic that combines 
these c separate estimates into one overall estimate of the population proportion, p, provides 
more information than any one of the c separate estimates alone. To expand on Equation (12.2) 
on page 478, the statistic p in Equation (12.3) represents the estimated overall proportion for 
all c groups combined.
Computing the Estimated Overall Proportion for c Groups
	
p = X1 + X2 + g + Xc
n1 + n2 + g + nc
= X
n	
(12.3)
To compute the expected frequency, fe, for each cell in the first row in the contingency 
table, multiply each sample size (or column total) by p. To compute the expected frequency, fe, 
for each cell in the second row in the contingency table, multiply each sample size (or column 
total) by 11 - p2. The sampling distribution of the test statistic shown in Equation (12.1) on 
page 477 approximately follows a chi-square distribution, with degrees of freedom equal to the 
number of rows in the contingency table minus 1, multiplied by the number of columns in the 
table minus 1. For a 2 : c contingency table, there are c - 1 degrees of freedom:
Degrees of freedom = 12 - 121c - 12 = c - 1

484	
Chapter 12  Chi-Square and Nonparametric Tests
Using the level of significance a, you reject the null hypothesis if the computed x2
STAT 
test statistic is greater than x2
a, the upper-tail critical value from a chi-square distribution with 
c - 1 degrees of freedom. Therefore, the decision rule is
Reject H0 if x2
STAT 7 x2
a;
otherwise, do not reject H0.
Figure 12.4 illustrates this decision rule.
F i g u r e  1 2 . 4
Regions of rejection 
and nonrejection when 
testing for differences 
among c proportions 
using the x2 test
0
α
Region of
Nonrejection
Critical
Value
Region of
Rejection
2
χ
(1 – α)
To illustrate the x2 test for equality of proportions when there are more than two groups, 
return to the Using Statistics scenario on page 475 concerning T.C. Resort Properties. Once 
again, you define the business objective as improving the quality of service, but this time, you 
are comparing three hotels located on a different island. Data are collected from customer sat-
isfaction surveys at these three hotels. You organize the responses into the contingency table 
shown in Table 12.6.
Because the null hypothesis states that there are no differences among the three hotels in 
the proportion of guests who would likely return again, you use Equation (12.3) to calculate an 
estimate of p, the population proportion of guests who would likely return again:
 p = X1 + X2 + g + Xc
n1 + n2 + g + nc
= X
n
 = 1128 + 199 + 1862
1216 + 232 + 2522 = 513
700
 = 0.733
The estimated overall proportion of guests who would not be likely to return again is the 
complement, 11 - p2, or 0.267. Multiplying these two proportions by the sample size for each 
hotel yields the expected number of guests who would and would not likely return.
T a b l e  1 2 . 6
2 * 3 Contingency 
Table for Guest 
Satisfaction Survey
Hotel
Choose Hotel Again?
Golden Palm
Palm Royale
Palm Princess
Total
Yes
128
199
186
513
No
  88
  33
  66
187
Total
216
232
252
700

	
12.2  Chi-Square Test for Differences Among More Than Two Proportions 	
485
Table 12.7 presents these expected frequencies.
Example 12.2
Computing the  
Expected  
Frequencies
Compute the expected frequencies for each of the six cells in Table 12.6.
Solution
Yes—Golden Palm: p = 0.733 and n1 = 216, so fe = 158.30
Yes—Palm Royale: p = 0.733 and n2 = 232, so fe = 170.02
Yes—Palm Princess: p = 0.733 and n3 = 252, so fe = 184.68
No—Golden Palm: 1 - p = 0.267 and n1 = 216, so fe = 57.70
No—Palm Royale: 1 - p = 0.267 and n2 = 232, so fe = 61.98
No—Palm Princess: 1 - p = 0.267 and n3 = 252, so fe = 67.32
T a b l e  1 2 . 7
Contingency Table of 
Expected Frequencies 
from a Guest  
Satis­faction Survey  
of Three Hotels
Hotel
Choose Hotel Again?
Golden Palm
Palm Royale
Palm Princess
Total
Yes
158.30
170.02
184.68
513
No
  57.70
  61.98
  67.32
187
Total
216.00
232.00
252.00
700
To test the null hypothesis that the proportions are equal:
H0: p1 = p2 = p3
against the alternative that not all three proportions are equal:
H1: Not all pj are equal 1where j = 1, 2, 32
you use the observed frequencies from Table 12.6 and the expected frequencies from  
Table 12.7 to compute the x2
STAT test statistic [given by Equation (12.1) on page 477].  
Table 12.8 presents the calculations.
T a b l e  1 2 . 8
Computing the x2
STAT 
Test Statistic for the 
Three-Hotel Guest 
Satisfaction Survey
fo
fe
1fo - fe2
1fo - fe22
1fo - fe22>fe
128
158.30
-30.30
918.09
5.80
199
170.02
28.98
839.84
4.94
186
184.68
1.32
1.74
0.01
88
57.70
30.30
918.09
15.91
33
61.98
-28.98
839.84
13.55
66
67.32
-1.32
1.74
0.02
40.23
You use Table E.4 to find the critical value of the x2 test statistic. In the guest satisfaction 
survey, because there are three hotels, there are 12 - 1213 - 12 = 2 degrees of freedom.  
Using a = 0.05, the x2 critical value with 2 degrees of freedom is 5.991 (see Figure 12.5).
0
5.991
Region of
Nonrejection
Critical
Value
Region of
Rejection
.05
.95
χ2
F i g u r e  1 2 . 5
Regions of rejection and 
nonrejection when testing 
for differences in three 
proportions at the  
0.05 level of significance, 
with 2 degrees of 
freedom

486	
Chapter 12  Chi-Square and Nonparametric Tests
Because the computed x2
STAT test statistic is 40.23, which is greater than this critical value, 
you reject the null hypothesis. Figure 12.6 shows the Excel and Minitab results for this prob-
lem. These results also report the p-value. Because the p-value is 0.0000, less than a = 0.05, 
you reject the null hypothesis. Further, this p-value indicates that there is virtually no chance 
that there will be differences this large or larger among the three sample proportions, if the 
population proportions for the three hotels are equal. Thus, there is sufficient evidence to con-
clude that the hotel properties are different with respect to the proportion of guests who are 
likely to return.
For the x2 test to give accurate results when dealing with 2 * c contingency tables, all 
expected frequencies must be large. The definition of “large” has led to research among stat-
isticians. Some statisticians (see reference 5) have found that the test gives accurate results as 
long as all expected frequencies are at least 0.5. Other statisticians, more conservative in their 
approach, believe that no more than 20% of the cells should contain expected frequencies less 
than 5, and no cells should have expected frequencies less than 1 (see reference 3). As a rea-
sonable compromise between these points of view, to ensure the validity of the test, you should 
make sure that each expected frequency is at least 1. To do this, you may need to collapse two 
or more low-expected-frequency categories into one category in the contingency table before 
performing the test. If combining categories is undesirable, you can use one of the available 
alternative procedures (see references 1, 2, and 7).
The Marascuilo Procedure
Rejecting the null hypothesis in a x2 test of equality of proportions in a 2 * c table allows 
you to only reach the conclusion that not all c population proportions are equal. To determine 
which proportions differ, you use a multiple-comparisons procedure such as the Marascuilo 
procedure.
F i g u r e  1 2 . 6
Excel and Minitab chi-square test results for the three-hotel guest satisfaction survey

	
12.2  Chi-Square Test for Differences Among More Than Two Proportions 	
487
Then, you compare each of the c1c - 12>2 pairs of sample proportions against its corre-
sponding critical range. You declare a specific pair significantly different if the absolute differ-
ence in the sample proportions, pj - pj′, is greater than its critical range.
To apply the Marascuilo procedure, return to the three-hotel guest satisfaction survey.  
Using the x2 test, you concluded that there was evidence of a significant difference among the 
population proportions. From Table 12.6 on page 484, the three sample proportions are
 p1 = X1
n1
= 128
216 = 0.5926
 p2 = X2
n2
= 199
232 = 0.8578
 p3 = X3
n3
= 186
252 = 0.7381
Next, you compute the absolute differences in sample proportions and their corresponding 
critical ranges. Because there are three hotels, there are 13213 - 12>2 = 3 pairwise compari-
sons. Using Table E.4 and an overall level of significance of 0.05, the upper-tail critical value 
for a chi-square distribution having 1c - 12 = 2 degrees of freedom is 5.991. Thus,
2x2
a = 25.991 = 2.4477
The following displays the absolute differences and the critical ranges for each comparison.
Absolute Difference in Proportions
Critical Range
pj - pj′ 
2.4477B
pj11 - pj2
nj
+
pj′11 - pj′2
nj′
p1 - p2 = 0.5926 - 0.8578 = 0.2652
2.4477B
10.5926210.40742
216
+
10.8578210.14222
232
= 0.0992
p1 - p3 = 0.5926 - 0.7381 = 0.1455
2.4477B
10.5926210.40742
216
+
10.7381210.26192
252
= 0.1063
p2 - p3 = 0.8578 - 0.7381 = 0.1197
2.4477B
10.8578210.14222
232
+
10.7381210.26192
252
= 0.0880
Student Tip
You have an a level of 
risk in the entire set of 
comparisons not just a 
single comparison.
Critical Range for the Marascuilo Procedure
	
Critical range = 2x2
aB
pj11 - pj2
nj
+
pj′11 - pj′2
nj′
	
(12.4)
	
where
pj = proportion of items of interest in group j
pj′ = proportion of items of interest in group j′
nj = sample size in group j
nj′ = sample size in group j′
The Marascuilo procedure enables you to make comparisons between all pairs of groups. 
First, you compute the sample proportions. Then, you use Equation (12.4) to compute the criti-
cal ranges for the Marascuilo procedure. You compute a different critical range for each pair-
wise comparison of sample proportions.

488	
Chapter 12  Chi-Square and Nonparametric Tests
Figure 12.7 shows Excel results for this example.
As the final step, you compare the absolute differences to the critical ranges. If the abso-
lute difference is greater than the critical range, the proportions are significantly different. At 
the 0.05 level of significance, you can conclude that guest satisfaction is higher at the Palm 
Royale 1p2 = 0.8582 than at either the Golden Palm 1p1 = 0.5932 or the Palm Princess 
1p3 = 0.7382 and that guest satisfaction is also higher at the Palm Princess than at the Golden 
Palm. These results clearly suggest that you should investigate possible reasons for these dif-
ferences. In particular, you should try to determine why satisfaction is significantly lower at 
the Golden Palm than at the other two hotels.
The Analysis of Proportions (ANOP)
In many situations, you need to examine differences among several proportions. The analysis 
of proportions (ANOP) method provides a confidence interval approach that allows you to de-
termine which, if any, of the c groups has a proportion significantly different from the overall 
mean of all the group proportions combined. The ANOP online topic discusses this method 
and illustrates its use.
F i g u r e  1 2 . 7
Excel Marascuilo 
procedure results for 
the three-hotel guest 
satisfaction survey
Problems for Section 12.2
Learning the Basics
12.11  Calculate the degrees of freedom and the corresponding 
critical value at 0.05 level of significance for the
a.	 2 * 3 contingency table.
b.	 4 * 4 contingency table.
c.	 2 * 5 contingency table.
12.12  Use the following contingency table:
A
B
C
Total
1
10
30
  50
  90
2
40
45
  50
135
Total
50
75
100
225
a.	 Compute the expected frequency for each cell.
b.	 Compute x2
STAT. Is it significant at a = 0.05?
12.13  Use the following contingency table:
A
B
C
Total
1
20
30
25
  75
2
30
20
25
  75
Total
50
50
50
150
a.	 Compute the expected frequency for each cell.
b.	 Compute x2
STAT. Is it significant at a = 0.05?
Applying the Concepts
12.14  Do workers prefer to buy lunch rather than pack their own 
lunch? A survey of employed Americans found that 75% of the 
18- to 24-year-olds, 77% of the 25- to 34-year-olds, 72% of the 
35- to 44-year-olds, 58% of the 45- to 54-year-olds, 57% of the 
54- to 64-year-olds, and 55% of the 65+ -year-olds buy lunch 
throughout the work week. (Data extracted from bit.ly/z99CeN.) 
Suppose the survey was based on 200 employed Americans in 

	
12.3  Chi-Square Test of Independence	
489
each of six age groups: 18 to 24, 25 to 34, 35 to 44, 45 to 54, 55 to 
64, and 65+.
a.	 At the 0.05 level of significance, is there evidence of a differ-
ence among the age groups in the preference for buying lunch?
b.	 Determine the p-value in (a) and interpret its meaning.
c.	 If appropriate, use the Marascuilo procedure and a = 0.05 to 
determine which age groups differ.
12.15  What are travelers’ technologies of choice? Tablets ac-
count for a growing share of the multiuse devices travelers are 
using. An observational study of passengers on air, bus, and train 
travel found that 8.4% of airline passengers, 5.9% of Amtrak pas-
sengers, 4.9% of commuter train passengers, and 3.7% of curb-
side bus passengers were observed using a tablet at some point 
during their travel. (Data extracted from afterhours.e-strategy 
.com/passenger-tablet-use-by-transportation-mode-c.) Sup-
pose these results were based on 500 passengers in each of the 
four transportation modes: airline, Amtrak train, commuter train, 
and curbside bus.
a.	 At the 0.05 level of significance, is there evidence of a differ-
ence among the transportation modes with respect to use of 
tablets?
b.	 Compute the p-value and interpret its meaning.
c.	 If appropriate, use the Marascuilo procedure and a = 0.05 to 
determine which transportation modes differ.
SELF 
Test 
12.16  Social media users use a variety of devices to 
access social networking; mobile phones are increas-
ingly popular. However, is there a difference in the various age 
groups in the proportion of social media users who use their 
­mobile phone to access social networking? A study showed the 
following results for the different age groups:
Use Mobile Phone to Access 
Social Networking?
Age
18–34
35–64
65+
Yes
59%
36%
13%
No
41%
64%
87%
Source: Data extracted from “State of the Media: U.S. Digital Consumer 
Report Q3–Q4 2011,” The Nielsen Company, 2012, p. 9.
Assume that 200 social media users for each age group were  
surveyed.
a.	 At the 0.05 level of significance, is there evidence of a differ-
ence among the age groups with respect to use of mobile phone 
for accessing social networking?
b.	 Determine the p-value in (a) and interpret its meaning.
c.	 If appropriate, use the Marascuilo procedure and a = 0.05 to 
determine which age groups differ with respect to use of a mo-
bile phone for accessing social networking.
d.	 Discuss the implications of (a) and (c). How can marketers use 
this information to improve their sales return on investment 
(ROI)?
12.17  Repeat (a) and (b) of Problem 12.16, assuming that only 
10 social media users for each age group were surveyed. Dis-
cuss the implications of sample size on the x2 test for differences 
among more than two populations.
12.18  Who uses a cellphone while watching TV? The Pew  
Research Center’s Internet and American Life Project measured 
the prevalence of multiscreen viewing experiences by asking 
American adults who are cellphone owners whether they had used 
their phone to engage in several activities while watching TV. The 
study reported that 171 of 316 (54%) of urban American cell-
phone owners sampled, 516 of 993 (52%) of suburban American  
cellphone owners sampled, and 251 of 557 (45%) of rural American  
cellphone owners used their phone to engage in several  
activities while watching TV. (Data extracted from “The Rise of the  
Connected Viewer,” Pew Research Center’s Internet & American 
Life Project, July 17, 2012.)
a.	 Is there evidence of a significant difference among the urban, 
suburban, and rural American cellphone owners with respect to 
the proportion who use their phone to engage in several activi-
ties while watching TV? (Use a = 0.05).
b.	 Determine the p-value and interpret its meaning.
c.	 If appropriate, use the Marascuilo procedure and a = 0.05 to 
determine which groups differ.
12.19  In an article, Jon Stine, director of the Internet business 
solutions group at Cisco Systems Inc., New York said that sixty-
five percent of all U.S. shoppers use the Internet to research prod-
ucts and services, yet 90 percent of all non-grocery transactions 
in 2012 occurred in the store. (Source: http://www.luxurydaily.
com/54pc-of-u-s-consumers-crave-in-store-digital-mobile-
touch-points-cisco/). Assume that a researcher wants to see what 
products consumers prefer to buy online or in-store and finds that 
50 out of the 200 who bought electronics, 150 out of the 250 who 
bought grocery, and 130 out of the 300 who bought apparels, use 
an online medium of purchase.
a.	 Is there evidence of a significant difference among the purchase 
of electronics, grocery and apparels and the mode of purchase 
using online medium? (Use a = 0.05).
b.	 Determine the p-value and interpret its meaning.
12.3  Chi-Square Test of Independence
In Sections 12.1 and 12.2, you used the x2 test to evaluate potential differences among popula-
tion proportions. For a contingency table that has r rows and c columns, you can generalize the 
x2 test as a test of independence for two categorical variables.
For a test of independence, the null and alternative hypotheses follow:
H0: The two categorical variables are independent 1i.e., there is no relationship between them2.
H1: The two categorical variables are dependent 1i.e., there is a relationship between them2.

490	
Chapter 12  Chi-Square and Nonparametric Tests
Once again, you use Equation (12.1) on page 477 to compute the test statistic:
x2
STAT =
a
all cells
1fo - fe22
fe
You reject the null hypothesis at the a level of significance if the computed value of the x2
STAT 
test statistic is greater than x2
a, the upper-tail critical value from a chi-square distribution with 
1r - 121c - 12 degrees of freedom (see Figure 12.8).
0
α
Region of
Nonrejection
Critical
Value
Region of
Rejection
χ2
(1 – α)
F i g u r e  1 2 . 8
Regions of rejection and 
nonrejection when testing 
for independence in an 
r * c contingency table, 
using the x2 test
Thus, the decision rule is
Reject H0 if x2
STAT 7 x2
a;
otherwise, do not reject H0.
The chi-square 1X22 test of independence is similar to the x2 test for equality of propor-
tions. The test statistics and the decision rules are the same, but the null and alternative hypothe-
ses and conclusions are different. For example, in the guest satisfaction survey of Sections 12.1  
and 12.2, there is evidence of a significant difference between the hotels with respect to the 
proportion of guests who would return. From a different viewpoint, you could conclude that 
there is a significant relationship between the hotels and the likelihood that a guest would  
return. However, the two types of tests differ in how the samples are selected.
In a test for equality of proportions, there is one factor of interest, with two or more lev-
els. These levels represent samples selected from independent populations. The categorical 
responses in each group or level are classified into two categories, such as an item of interest 
and not an item of interest. The objective is to make comparisons and evaluate differences 
between the proportions of the items of interest among the various levels. However, in a test 
for independence, there are two factors of interest, each of which has two or more levels. You 
select one sample and tally the joint responses to the two categorical variables into the cells of 
a contingency table.
To illustrate the x2 test for independence, suppose that, in the three-hotel guest satisfaction 
survey, respondents who stated that they were not likely to return also indicated the primary rea-
son for their unwillingness to return. Table 12.9 presents the resulting 4 * 3 contingency table.
Student Tip
Remember that inde-
pendence means no 
relationship, so you 
do not reject the null 
hypothesis. Dependence 
means there is a relation-
ship, so you reject the 
null hypothesis.
T a b l e  1 2 . 9
Contingency Table of 
Primary Reason for 
Not Returning and 
Hotel
Primary Reason for  
not Returning
Hotel
Golden Palm
Palm Royale
Palm Princess
Total
Price
23
  7
37
67
Location
39
13
  8
60
Room accommodation
13
  5
13
31
Other
13
  8
  8
29
Total
88
33
66
187

	
12.3  Chi-Square Test of Independence	
491
In Table 12.9, observe that of the primary reasons for not planning to return to the hotel, 
67 were due to price, 60 were due to location, 31 were due to room accommodation, and 29 
were due to other reasons. In Table 12.6 on page 484, there were 88 guests at the Golden Palm, 
33 guests at the Palm Royale, and 66 guests at the Palm Princess who were not planning to 
return. The observed frequencies in the cells of the 4 * 3 contingency table represent the joint 
tallies of the sampled guests with respect to primary reason for not returning and the hotel 
where they stayed. The null and alternative hypotheses are
H0: There is no relationship between the primary reason for not returning and the hotel.
H1: There is a relationship between the primary reason for not returning and the hotel.
To test this null hypothesis of independence against the alternative that there is a relationship 
between the two categorical variables, you use Equation (12.1) on page 477 to compute the test 
statistic:
x2
STAT =
a
all cells
1fo - fe22
fe
where
fo = observed frequency in a particular cell of the r * c contingency table
fe = expected frequency in a particular cell if the null hypothesis of independence is true
To compute the expected frequency, fe, in any cell, you use the multiplication rule for 
independent events discussed on page 194 [see Equation (4.7)]. For example, under the null 
hypothesis of independence, the probability of responses expected in the upper-left-corner cell 
representing primary reason of price for the Golden Palm is the product of the two separate 
probabilities P1Price2 and P1Golden Palm2. Here, the proportion of reasons that are due to 
price, P1Price2, is 67>187 = 0.3583, and the proportion of all responses from the Golden 
Palm, P1Golden Palm2, is 88>187 = 0.4706. If the null hypothesis is true, then the primary 
reason for not returning and the hotel are independent:
 P1Price and Golden Palm2 = P1Price2 * P1Golden Palm2
 = 10.35832 * 10.47062
 = 0.1686
The expected frequency is the product of the overall sample size, n, and this probability, 
187 * 0.1686 = 31.53. The fe values for the remaining cells are shown in Table 12.10.
T a b l e  1 2 . 1 0
Contingency Table of 
Expected Frequencies 
of Primary Reason for 
Not Returning with 
Hotel
Primary Reason for  
not Returning
Hotel
Golden Palm
Palm Royale
Palm Princess
Total
Price
31.53
11.82
23.65
  67
Location
28.24
10.59
21.18
  60
Room accommodation
14.59
  5.47
10.94
  31
Other
13.65
  5.12
10.24
  29
Total
88.00
33.00
66.00
187
You can also compute the expected frequency by taking the product of the row total and 
column total for a cell and dividing this product by the overall sample size, as Equation (12.5) 
shows.

492	
Chapter 12  Chi-Square and Nonparametric Tests
This alternate method results in simpler computations. For example, using Equation (12.5) 
for the upper-left-corner cell (price for the Golden Palm),
fe = Row total * Column total
n
= 16721882
187
= 31.53
and for the lower-right-corner cell (other reason for the Palm Princess),
fe = Row total * Column total
n
= 12921662
187
= 10.24
To perform the test of independence, you use the x2
STAT test statistic shown in Equation (12.1)  
on page 477. The sampling distribution of the x2
STAT test statistic approximately follows a chi-
square distribution, with degrees of freedom equal to the number of rows in the contingency 
table minus 1, multiplied by the number of columns in the table minus 1:
 Degrees of freedom = 1r - 121c - 12
 = 14 - 1213 - 12 = 6
Table 12.11 presents the computations for the x2
STAT test statistic.
Computing the Expected Frequency
The expected frequency in a cell is the product of its row total and column total, divided by 
the overall sample size.
	
fe = Row total * Column total
n
	
(12.5)
	
where
Row total = sum of the frequencies in the row
Column total = sum of the frequencies in the column
n = overall sample size
T a b l e  1 2 . 1 1
Computing the x2
STAT 
Test Statistic for the 
Test of Independence
Cell
fo
fe
1fo - fe2
1fo - fe22
1fo - fe22>fe
Price/Golden Palm
23
31.53
-8.53
72.76
2.31
Price/Palm Royale
7
11.82
-4.82
23.23
1.97
Price/Palm Princess
37
23.65
13.35
178.22
7.54
Location/Golden Palm
39
28.24
10.76
115.78
4.10
Location/Palm Royale
13
10.59
2.41
5.81
0.55
Location/Palm Princess
8
21.18
-13.18
173.71
8.20
Room/Golden Palm
13
14.59
-1.59
2.53
0.17
Room/Palm Royale
5
5.47
-0.47
0.22
0.04
Room/Palm Princess
13
10.94
2.06
4.24
0.39
Other/Golden Palm
13
13.65
-0.65
0.42
0.03
Other/Palm Royale
8
5.12
2.88
8.29
1.62
Other/Palm Princess
8
10.24
-2.24
5.02
0.49
27.41

	
12.3  Chi-Square Test of Independence	
493
Using the a = 0.05 level of significance, the upper-tail critical value from the 
chi-square distribution with 6 degrees of freedom is 12.592 (see Table E.4). Because 
x2
STAT = 27.41 7 12.592, you reject the null hypothesis of independence (see Figure 12.9).
Figure 12.10 shows the Excel and Minitab results for this test, which are identical when 
rounded to three decimal places. Because x2
STAT = 27.410 7 12.592, you reject the null  
hypothesis of independence. Using the p-value approach, you reject the null hypothesis of  
independence because the p@value = 0.000 6 0.05. The p-value indicates that there is  
virtually no chance of having a relationship this strong or stronger between the hotel and the 
primary reasons for not returning in a sample, if the primary reasons for not returning are 
independent of the specific hotels in the entire population. Thus, there is strong evidence of a 
relationship between the primary reason for not returning and the hotel.
F i g u r e  1 2 . 9
Regions of rejection 
and nonrejection when 
testing for independence 
in the three hotel guest 
satisfaction survey 
example at the 0.05 level 
of significance, with  
6 degrees of freedom
0
Region of
Nonrejection
Critical
Value
Region of
Rejection
χ2
.05
12.592
F i g u r e  1 2 . 1 0
Excel and Minitab chi-square test results for the Table 12.9 primary reason for not returning to hotel data

494	
Chapter 12  Chi-Square and Nonparametric Tests
Examination of the observed and expected frequencies (see Table 12.11 above on page 493) 
reveals that price is underrepresented as a reason for not returning to the Golden Palm (i.e., fo = 23 
and fe = 31.53) but is overrepresented at the Palm Princess. Guests are more satisfied with the 
price at the Golden Palm than at the Palm Princess. Location is overrepresented as a reason for not 
returning to the Golden Palm but greatly underrepresented at the Palm Princess. Thus, guests are 
much more satisfied with the location of the Palm Princess than with that of the Golden Palm.
To ensure accurate results, all expected frequencies need to be large in order to use the x2 
test when dealing with r * c contingency tables. As in the case of 2 * c contingency tables 
in Section 12.2, all expected frequencies should be at least 1. For contingency tables in which 
one or more expected frequencies are less than 1, you can use the chi-square test after collaps-
ing two or more low-frequency rows into one row (or collapsing two or more low-frequency 
columns into one column). Merging rows or columns usually results in expected frequencies 
sufficiently large to ensure the accuracy of the x2 test.
Problems for Section 12.3
Learning the Basics
12.20  If a contingency table has three rows and four columns, 
how many degrees of freedom are there for the x2 test of indepen-
dence?
12.21  When performing a x2 test of independence in a contin-
gency table with r rows and c columns, determine the upper-tail 
critical value of the test statistic in each of the following circum-
stances:
a.	 a = 0.05, r = 4 rows, c = 5 columns
b.	 a = 0.01, r = 4 rows, c = 5 columns
c.	 a = 0.01, r = 4 rows, c = 6 columns
d.	 a = 0.01, r = 3 rows, c = 6 columns
e.	 a = 0.01, r = 6 rows, c = 3 columns
Applying the Concepts
12.22  Research has proved that there is a relationship between 
the mode of commuting and stress level. In a study it was found 
that train and bus commuters project more stress followed by 
car commuters, and walkers project little or no stress (Source:  
http://www4.dcu.ie/dcubs/crmld/working_papers/Psychology-
of-Commuting1.pdf). Assume that the following data has been 
collected in a survey related to the mode of commuting options 
and the respective stress levels:
Stress Level
High
Moderate
Low
Total
Train
9
5
4
18
Bus
17
8
7
32
Car
18
7
10
35
Walk
5
10
15
30
Total
49
30
36
115
At 0.05 level of significance, is there evidence of a rela-
tionship between a mode of commuting and the stress level of 
­commuters?
12.23  Is there a generation gap in the type of music that people 
listen to? The following table represents the type of favorite music 
for a sample of 1,000 respondents classified according to their age 
group:
Favorite Type
Age
16–29
30–49
50–64
65 and over
Total
Rock
71
62
51
27
211
Rap or hip-hop
40
21
7
3
71
Rhythm and  
  blues
48
46
46
40
180
Country
43
53
59
79
234
Classical
22
28
33
46
129
Jazz
18
26
36
43
123
Salsa
8
14
18
12
52
Total
250
250
250
250
1000
At the 0.05 level of significance, is there evidence of a rela-
tionship between favorite type of music and age group?
SELF 
Test 
12.24  How often do Facebook users post? A study  
by the Pew Research Center (data extracted from  
bit.ly/1axbobZ) revealed the following results:
Frequency
Age Group
18–22
23–35
36–49
50–65
65+
Total
Several times  
  a day
20
22
  9
  2
  1
  54
About once  
  a day
28
37
14
  4
  1
  84
3–5 days per  
  week
32
46
31
  7
  2
118
1–2 days per  
  week
32
69
35
17
  7
160
Every few  
  weeks
22
66
47
28
  6
169
Less often
16
59
56
61
18
210
Never
  6
15
42
66
23
152
Total
156
314
234
185
58
947
At the 0.01 level of significance, is there evidence of a significant 
relationship between frequency of posting on Facebook and age?

	
12.4  Wilcoxon Rank Sum Test: A Nonparametric Method for Two Independent Populations	
495
12.25  While analyzing the employee turnover rate a human re-
source manager produced the following table from the collected 
data:
EMPLOYEE TYPES
years of employment
Up to  
2 years
2–5  
years
5 years and 
above
Directors
4
10
6
Managers
5
9
7
Executives
7
12
10
Supervisors
8
18
28
Workers
5
28
38
At 0.05 level of significance, is there evidence of a significant 
relationship between the employees as per organizational hierar-
chy and their years of employment? If so, explain the relationship.
12.26  The 2012 Restaurant Industry Forecast takes a closer 
look at today’s consumers. Based on a 2011 National Restaurant  
Association survey, American adults are categorized into one of 
three consumer segments (optimistic, cautious, and hunkered 
down) based on their financial situation, current spending behav-
ior, and economic outlook, as well as the geographic region where 
they reside. Suppose the results, based on a sample 1,000 American  
adults, are as follows:
Consumer 
Segment
Geographic Region
Midwest
Northeast
South
West
Total
Optimistic
  67
  23
  60
  63
  213
Cautious
101
  57
127
133
  418
Hunkered  
  down
  83
  46
115
125
  369
Total
251
126
302
321
1000
Source: Data extracted from “The 2012 Restaurant Industry Forecast,” 
National Restaurant Association, 2012, p. 12.
At the 0.05 level of significance, is there evidence of a sig-
nificant relationship between consumer segment and geographic 
region?
12.4  Wilcoxon Rank Sum Test: A Nonparametric Method 
for Two Independent Populations
In Section 10.1, you used the t test for the difference between the means of two independent 
populations. If sample sizes are small and you cannot assume that the data in each sample are 
from normally distributed populations, you have two choices:
 • Use a nonparametric method that does not depend on the assumption of normality for 
the two populations.
 • Use the pooled-variance t test, following a normalizing transformation on the data (see 
reference 10).
Nonparametric methods require few or no assumptions about the populations from 
which data are obtained (see reference 4). One such method is the Wilcoxon rank sum test for 
testing whether there is a difference between two medians. The Wilcoxon rank sum test is al-
most as powerful as the pooled-variance and separate-variance t tests discussed in Section 10.1 
under conditions appropriate to these tests and is likely to be more powerful when the assump-
tions of those t tests are not met. In addition, you can use the Wilcoxon rank sum test when you 
have only ordinal data, as often happens in consumer behavior and marketing research.
To perform the Wilcoxon rank sum test, you replace the values in the two samples of sizes 
n1 and n2 with their combined ranks (unless the data contained the ranks initially). You begin 
by defining n = n1 + n2 as the total sample size. Next, you assign the ranks so that rank 1 is 
given to the smallest of the n combined values, rank 2 is given to the second smallest, and so 
on, until rank n is given to the largest. If several values are tied, you assign each value the aver-
age of the ranks that otherwise would have been assigned had there been no ties.
Whenever the two sample sizes are unequal, n1 represents the smaller sample and n2 the 
larger sample. The Wilcoxon rank sum test statistic, T1, is defined as the sum of the ranks as-
signed to the n1 values in the smaller sample. (For equal-sized samples, either sample may be 
used for determining T1.) For any integer value n, the sum of the first n consecutive integers is 
n1n + 12>2. Therefore, T1 plus T2, the sum of the ranks assigned to the n2 items in the second 
sample, must equal n1n + 12>2. You can use Equation (12.6) to check the accuracy of your 
rankings.
Student Tip
Remember that you 
combine the two groups 
before you rank the 
values.

496	
Chapter 12  Chi-Square and Nonparametric Tests
When n1 and n2 are each … 10, you use Table E.6 to find the critical values of the test 
statistic T1. For a two-tail test shown in Figure 12.11 Panel A, you reject the null hypothesis 
if the computed value of T1 is greater than or equal to the upper critical value, or if T1 is less 
than or equal to the lower critical value. For one-tail tests having the alternative hypothesis 
H1: M1 6 M2 that the median of population 11M12 is less than the median of population 2 
1M22, you reject the null hypothesis if the observed value of T1 is less than or equal to the 
lower critical value (shown in Figure 12.11 Panel B). For one-tail tests having the alternative 
hypothesis H1: M1 7 M2, you reject the null hypothesis if the observed value of T1 equals or is 
greater than the upper critical value (shown in Figure 12.11 Panel C).
Checking the Rankings
	
T1 + T2 = n1n + 12
2
	
(12.6)
Region of Rejection
Region of Nonrejection
0
–Z
+Z
0
–Z
0
+Z
Panel A
Panel B
Panel C
H1: M1 < M2
H1: M1 ≠ M2
μT1
μT1
μT1
T1L
T1L
H1: M1 > M2  
T1U
T1U
F i g u r e  1 2 . 1 1
Regions of rejection and 
nonrejection using the 
Wilcoxon rank sum test
For large sample sizes, the test statistic T1 is approximately normally distributed, with the 
mean, mT1, equal to
mT1 = n11n + 12
2
and the standard deviation, sT1, equal to
sT1 = B
n1n21n + 12
12
Therefore, Equation (12.7) defines the standardized Z test statistic for the Wilcoxon rank sum test.
Student Tip
Remember that the 
group that is defined as 
group 1 when computing 
the test statistic T1 must 
also be defined as group 
1 in the null and alterna-
tive hypotheses.
Large-Sample Wilcoxon Rank Sum Test
	
ZSTAT =
T1 - n11n + 12
2
B
n1n21n + 12
12
	
(12.7)
where the test statistic ZSTAT approximately follows a standardized normal distribution.

	
12.4  Wilcoxon Rank Sum Test: A Nonparametric Method for Two Independent Populations	
497
You use Equation (12.7) when the sample sizes are outside the range of Table E.6. Based 
on a, the level of significance selected, you reject the null hypothesis if the ZSTAT test statistic 
falls in the rejection region.
To study an application of the Wilcoxon rank sum test, recall the Chapter 10 Using  
Statistics scenario concerning cola sales for two different end-cap display locations (stored 
in  Cola ). If you cannot assume that the populations are normally distributed, you can use the 
Wilcoxon rank sum test to evaluate possible differences in the median sales for the two display 
locations.2 The cola sales data and the combined ranks are shown in Table 12.12.
2To test for differences in the me-
dian sales between the two loca-
tions, you must assume that the 
distributions of sales in both popu-
lations are identical except for dif-
ferences in central tendency (i.e., 
the medians).
T a b l e  1 2 . 1 2
Forming the Combined 
Rankings
Because you have not stated in advance which display location is likely to have a higher 
median, you use a two-tail test with the following null and alternative hypotheses:
H0: M1 = M2 1the median sales are equal2
H1: M1 ≠M2 1the median sales are not equal2
Next, you compute T1, the sum of the ranks assigned to the smaller sample. When the sam-
ple sizes are equal, as in this example, you can define either sample as the group from which to 
compute T1. Choosing the beverage end-cap display as the first group,
T1 = 1 + 3 + 5.5 + 10 + 2 + 4 + 11 + 18.5 + 8 + 9 = 72
As a check on the ranking procedure, you compute T2 from
T2 = 5.5 + 14 + 15 + 7 + 13 + 17 + 12 + 20 + 16 + 18.5 = 138
and then use Equation (12.6) on page 496 to show that the sum of the first n = 20 integers in 
the combined ranking is equal to T1 + T2:
 T1 + T2 = n1n + 12
2
 72 + 138 = 201212
2
= 210
 210 = 210
Next, you use Table E.6 to determine the lower- and upper-tail critical values for the test 
statistic T1. From Table 12.13, a portion of Table E.6, observe that for a level of significance of 
0.05, the critical values are 78 and 132. The decision rule is
Reject H0 if T1 … 78 or if T1 Ú 132;
otherwise,  do not reject H0.
Beverage End-cap 
1n1 = 102
Combined 
Ranking
Produce End-cap 
1n2 = 102
Combined 
Ranking
22
  1.0
52
  5.5
34
  3.0
71
14.0
52
  5.5
76
15.0
62
10.0
54
  7.0
30
  2.0
67
13.0
40
  4.0
83
17.0
64
11.0
66
12.0
84
18.5
90
20.0
56
  8.0
77
16.0
59
  9.0
84
18.5
Source: Data are taken from Table 10.1 on page 377.

498	
Chapter 12  Chi-Square and Nonparametric Tests
Because the test statistic T1 = 72 6 78, you reject H0. There is evidence of a significant 
difference in the median sales for the two display locations. Because the sum of the ranks is 
lower for the beverage end-cap display, you conclude that median sales are lower for the bever-
age end-cap display.
Figure 12.12 shows the Wilcoxon rank sum test results (Excel) and the equivalent Mann-
Whitney test results (Minitab) for the cola sales data. The p-values differ because Minitab 
adjusts for ties and computes an exact probability while Excel uses the normal approximation. 
From these results, you reject the null hypothesis because the p-value is 0.0126 (0.0139 in 
Minitab results), which is less than a = 0.05. This p-value indicates that if the medians of the 
two populations are equal, the chance of finding a difference at least this large in the samples is 
only 0.0126 (0.0139 in Minitab).
T a b l e  1 2 . 1 3
Finding the Lower- 
and Upper-Tail 
Critical Values for the 
Wilcoxon Rank Sum 
Test Statistic, T1, Where 
n1 = 10,  n2 = 10, and 
a = 0.05
A
n1
n2
One-
tail
Two-
tail
4
5
6
7
8
9
10
(Lower, Upper)
.05
.10
16,40
24,51
33,63
43,76
54,90
66,105
9
.025
.05
14,42
22,53
31,65
40,79
51,93
62,109
.01
.02
13,43
20,55
28,68
37,82
47,97
59,112
.005
.01
11,45
18,57
26,70
35,84
45,99
56,115
.05
.10
17,43
26,54
35,67
45,81
56,96
69,111
82,128
10
.025
.05
15,45
23,57
32,70
42,84
53,99
65,115
78,132
.01
.02
13,47
21,59
29,73
39,87
49,103
61,119
74,136
.005
.01
12,48
19,61
27,75
37,89
47,105
58,122
71,139
Source: Extracted from Table E.6.
Table E.6 shows the lower and upper critical values of the Wilcoxon rank sum test statistic, 
T1, but only for situations in which both n1 and n2 are less than or equal to 10. If either one or 
both of the sample sizes are greater than 10, you must use the large-sample Z approximation 
formula [Equation (12.7) on page 496]. However, you can also use this approximation formula 
F i g u r e  1 2 . 1 2
Wilcoxon rank sum test and Mann-Whitney test (Minitab) results for cola sales for two different end-cap locations

	
12.4  Wilcoxon Rank Sum Test: A Nonparametric Method for Two Independent Populations	
499
for small sample sizes. To demonstrate the large-sample Z approximation formula, consider 
the cola sales data. Using Equation (12.7),
 ZSTAT =
T1 - n11n + 12
2
B
n1n21n + 12
12
 =
72 - 11021212
2
B
110211021212
12
 = 72 - 105
13.2288
= -2.4946
Because ZSTAT = -2.4946 6 -1.96, the critical value of Z at the 0.05 level of signifi-
cance 1or p@value = 0.0126 6 0.052, you reject H0.
Problems for Section 12.4
Learning the Basics
12.27  Using Table E.6, determine the lower- and upper-tail criti-
cal values for the Wilcoxon rank sum test statistic, T1, in each of 
the following two-tail tests:
a.	 a = 0.10, n1 = 6, n2 = 8
b.	 a = 0.05, n1 = 6, n2 = 8
c.	 a = 0.01, n1 = 6, n2 = 8
d.	 Given the results in (a) through (c), what do you conclude re-
garding the width of the region of nonrejection as the selected 
level of significance, a, gets smaller?
12.28  Using Table E.6, determine the lower-tail critical value for 
the Wilcoxon rank sum test statistic, T1, in each of the following 
one-tail tests:
a.	 a = 0.05, n1 = 6, n2 = 8
b.	 a = 0.025, n1 = 6, n2 = 8
c.	 a = 0.01, n1 = 6, n2 = 8
d.	 a = 0.005, n1 = 6, n2 = 8
12.29  The following information is available for two samples se-
lected from independent populations:
 Sample 1: n1 = 7 Assigned ranks: 4 1 8 2 5 10 11
 Sample 2: n2 = 9 Assigned ranks: 7 16 12 9 3 14 13 6 15
What is the value of T1 if you are testing the null hypothesis 
H0: M1 = M2?
12.30  In Problem 12.29, what are the lower- and upper-tail 
critical values for the test statistic T1 from Table E.6 if you use 
a 0.05 level of significance and the alternative hypothesis is 
H1: M1 ≠M2?
12.31  In Problems 12.29 and 12.30, what is your statistical  
decision?
12.32  The following information is available for two samples  
selected from independent and similarly shaped right-skewed  
populations:
 Sample 1: n1 = 5 1.1  2.3  2.9  3.6  14.7
 Sample 2: n2 = 6  2.8  4.4  4.4  5.2  6.0  18.5
a.	 Replace the observed values with the corresponding ranks 
(where 1 = smallest value; n = n1 + n2 = 11 = largest 
value) in the combined samples.
b.	 What is the value of the test statistic T1?
c.	 Compute the value of T2, the sum of the ranks in the larger  
sample.
d.	 To check the accuracy of your rankings, use Equation (12.6) on 
page 496 to demonstrate that T1 + T2 =
n1n + 12
2
12.33  From Problem 12.32, at the 0.05 level of significance, de-
termine the lower-tail critical value for the Wilcoxon rank sum test 
statistic, T1, if you want to test the null hypothesis, H0: M1 Ú M2, 
against the one-tail alternative, H1: M1 6 M2.
12.34  In Problems 12.32 and 12.33, what is your statistical  
decision?
Applying the Concepts
12.35  A vice president for marketing recruits 20 college gradu-
ates for management training. Each of the 20 individuals is ran-
domly assigned to one of two groups (10 in each group). A 
“traditional” method of training (T) is used in one group, and an 
“experimental” method (E) is used in the other. After the graduates 
spend six months on the job, the vice president ranks them on the 
basis of their performance, from 1 (worst) to 20 (best), with the 
following results (stored in the file  TestRank ):
T: 1 2 3 5 9 10 12 13 14 15
E: 4 6 7 8 11 16 17 18 19 20
Is there evidence of a difference in the median performance be-
tween the two methods? (Use a = 0.05.)

500	
Chapter 12  Chi-Square and Nonparametric Tests
12.36  Wine experts Gaiter and Brecher use a six-category scale 
when rating wines: Yech, OK, Good, Very Good, Delicious, and 
Delicious! Suppose Gaiter and Brecher tested wines from a ran-
dom sample of eight inexpensive California Cabernets and a ran-
dom sample of eight inexpensive Washington Cabernets, where 
inexpensive means wines with a U.S. suggested retail price of less 
than $20, and assigned the following ratings:
California—Good, Delicious, Yech, OK, OK, Very Good,  
Yech, OK
Washington—Very Good, OK, Delicious!, Very Good, 
Delicious, Good, Delicious, Delicious!
The ratings were then ranked and the ratings and the rankings 
stored in  Cabernet . (Data extracted from D. Gaiter and J. Brecher, 
“A Good U.S. Cabernet Is Hard to Find,” The Wall Street Journal, 
May 19, 2006, p. W7.)
a.	 Are the data collected by rating wines using this scale nominal, 
ordinal, interval, or ratio?
b.	 Why is the two-sample t test defined in Section 10.1 inappro-
priate to test the mean rating of California Cabernets versus 
Washington Cabernets?
c.	 Is there evidence of a significant difference in the median rat-
ing of California Cabernets and Washington Cabernets? (Use 
a = 0.05.)
12.37  A problem with a telephone line that prevents a customer 
from receiving or making calls is upsetting to both the customer 
and the telephone company. The file  Phone  contains samples of 
20 problems reported to two different offices of a telecommunica-
tions company and the time to clear these problems (in minutes) 
from the customers’ lines:
Central Office I Time to Clear Problems (Minutes)
1.48 1.75 0.78 2.85 0.52 1.60 4.15 3.97 1.48 3.10
1.02 0.53 0.93 1.60 0.80 1.05 6.32 3.93 5.45 0.97
Central Office II Time to Clear Problems (Minutes)
7.55 3.75 0.10 1.10 0.60 0.52 3.30 2.10 0.58 4.02
3.75 0.65 1.92 0.60 1.53 4.23 0.08 1.48 1.65 0.72
a.	 Is there evidence of a difference in the median time to clear 
problems between offices? (Use a = 0.05.)
b.	 What assumptions must you make in (a)?
c.	 Compare the results of (a) with those of Problem 10.9(a) on 
page 385.
SELF 
Test 
12.38  The management of a hotel has the business ob-
jective of increasing the return rate for hotel guests. 
One aspect of first impressions by guests relates to the time it 
takes to deliver a guest’s luggage to the room after check-in to the 
hotel. A random sample of 20 deliveries on a particular day were 
selected in Wing A of the hotel, and a random sample of  
20 deliveries were selected in Wing B. Delivery times were col-
lected and stored in  Luggage .
a.	 Is there evidence of a difference in the median delivery times in 
the two wings of the hotel? (Use a = 0.05.)
b.	 Compare the results of (a) with those of Problem 10.65 on  
page 410.
12.39  The lengths of life (in hours) of a sample of 40 100-watt 
compact fluorescent light bulbs produced by Manufacturer A and 
a sample of 40 20-watt compact fluorescent light bulbs produced 
by Manufacturer B are stored in  Bulbs .
a.	 Using a 0.05 level of significance, is there evidence of a differ-
ence in the median life of bulbs produced by the two manufac-
turers?
b.	 What assumptions must you make in (a)?
c.	 Compare the results of (a) with those of Problem 10.64 on 
page 410. Discuss.
12.40  Brand valuations are critical to CEOs, financial and mar-
keting executives, security analysts, institutional investors, and 
others who depend on well-researched, reliable information for as-
sessments and comparisons in decision making. Millward Brown, 
Inc., has annually compiled its BrandZ Top 100 Most Valuable 
Global Brands since 1996. Unlike other studies, the BrandZ rank-
ings combines consumer measures of brand equity with finan-
cial measures to establish a brand value for each brands. The file 
 BrandZTechFin  contains the brand values for two sectors in the 
BrandZ Top 100 Most Valuable Global Brands for 2013: the tech-
nology sector and the financial institutions sector. (Data extracted 
from “BrandZ Top 1000 Most Valuable Global Brands 2011,” 
Millward Brown, Inc., retrieved from bit.ly/18OL5Mu.)
a.	 Using a 0.05 level of significance, is there evidence of a differ-
ence in the median brand value between the two sectors?
b.	 What assumptions must you make in (a)?
c.	 Compare the results of (a) with those of Problem 10.17 on 
page 386. Discuss.
12.41  A bank with a branch located in a commercial district of a 
city has developed an improved process for serving customers during 
the noon-to-1 p.m. lunch period. The bank has the business objective 
of reducing the waiting time (defined as the number of minutes that 
elapse from when the customer enters the line until he or she reaches 
the teller window) to increase customer satisfaction. A random sam-
ple of 15 customers is selected and waiting times are collected and 
stored in  Bank1 . These waiting times (in minutes) are:
4.21 5.55 3.02 5.13 4.77 2.34 3.54 3.20
4.50 6.10 0.38 5.12 6.46 6.19 3.79
Another branch, located in a residential area, is also concerned 
with the noon-to-1 p.m. lunch period. A random sample of 15 cus-
tomers is selected and waiting times are collected and stored in 
 Bank2 . These waiting times (in minutes) are:
 9.66 5.90 8.02 5.79 8.73 3.82 8.01 8.35
10.49 6.68 5.64 4.08 6.17 9.91 5.47
a.	 Is there evidence of a difference in the median waiting time 
between the two branches? (Use a = 0.05.)
b.	 What assumptions must you make in (a)?
c.	 Compare the results (a) with those of Problem 10.12 (a) on 
page 386. Discuss.
12.42  An important feature of digital cameras is battery life, the 
number of shots that can be taken before the battery needs to be 
recharged. The file  Cameras  contains the battery life of 11 sub-
compact cameras and 7 compact cameras. (Data extracted from 
“Cameras,” Consumer Reports, July 2012, pp. 42–44.)
a.	 Is there evidence of a difference in the median battery life between 
subcompact cameras and compact cameras? (Use a = 0.05.)
b.	 What assumptions must you make in (a)?
c.	 Compare the results of (a) with those of Problem 10.11 (a) on 
page 386. Discuss.

	
12.5  Kruskal-Wallis Rank Test: A Nonparametric Method for the One-Way ANOVA	
501
12.5  Kruskal-Wallis Rank Test: A Nonparametric Method 
for the One-Way ANOVA
If the normality assumption of the one-way ANOVA F test is violated, you can use the  
Kruskal-Wallis rank test. The Kruskal-Wallis rank test for differences among c medians 
(where c 7 2) is an extension of the Wilcoxon rank sum test for two independent populations, 
discussed in Section 12.4. The Kruskal-Wallis test has the same power relative to the one-way 
ANOVA F test that the Wilcoxon rank sum test has relative to the t test.
You use the Kruskal-Wallis rank test to test whether c independent groups have equal me-
dians. The null hypothesis is
H0: M1 = M2 = g =  Mc
and the alternative hypothesis is
H1: Not all Mj are equal 1where j = 1, 2, c, c2.
To use the Kruskal-Wallis rank test, you first replace the values in the c samples with their 
combined ranks (if necessary). Rank 1 is given to the smallest of the combined values and rank 
n to the largest of the combined values (where n = n1 + n2 + g + nc). If any values are 
tied, you assign each of them the mean of the ranks they would have otherwise been assigned 
if ties had not been present in the data.
The Kruskal-Wallis test is an alternative to the one-way ANOVA F test. Instead of com-
paring each of the c group means against the grand mean, the Kruskal-Wallis test compares 
the mean rank in each of the c groups against the overall mean rank, based on all n combined 
values. Equation (12.8) defines the Kruskal-Wallis test statistic, H.
Student Tip
Remember that you 
combine the groups be-
fore you rank the values.
Kruskal-Wallis Rank Test for Differences Among c Medians
	
H = c
12
n1n + 12 a
c
j = 1
T 2
j
nj
d - 31n + 12	
(12.8)
	
where
n = total number of values over the combined samples
nj = number of values in the jth sample 1j = 1, 2, c , c2
Tj = sum of the ranks assigned to the jth sample
T 2
j = square of the sum of the ranks assigned to the jth sample
c = number of groups
If there is a significant difference among the c groups, the mean rank differs considerably 
from group to group. In the process of squaring these differences, the test statistic H becomes 
large. If there are no differences present, the test statistic H is small because the mean of the 
ranks assigned in each group should be very similar from group to group.
As the sample sizes in each group get large (i.e., at least 5), the sampling distribution of 
the test statistic, H, approximately follows the chi-square distribution with c - 1 degrees of 
freedom. Thus, you reject the null hypothesis if the computed value of H is greater than the 
upper-tail critical value (see Figure 12.13). Therefore, the decision rule is
Reject H0 if H 7 x2
a;
otherwise, do not reject H0.

502	
Chapter 12  Chi-Square and Nonparametric Tests
To illustrate the Kruskal-Wallis rank test for differences among c medians, return to the 
Arlington’s scenario on page 422 that concerns the in-store sales location experiment. If you 
cannot assume that the mobile electronics sales is normally distributed in all c groups, you can 
use the Kruskal-Wallis rank test.
The null hypothesis is that the median mobile electronics sales from each of the four in-
store locations are equal. The alternative hypothesis is that at least one of these medians differs 
from the others:
H0: M1 = M2 = M3 = M4
H1: Not all Mj are equal 1where j = 1, 2, 3, 42.
Table 12.14 presents the data (stored in  Second Experiment ), along with the corresponding 
ranks of a second in-store location sales experiment at Arlington’s.
F i g u r e  1 2 . 1 3
Determining the 
rejection region for the 
Kruskal-Wallis test
Region of
Rejection
Region of
Nonrejection
Critical
Value
0
α
1 – α
χ2
χ2α
T a b l e  1 2 . 1 4
Mobile Electronics 
Sales and Rank for Four 
In-Store Locations for 
Second Experiment
In-Aisle
Front
Kiosk
Expert
Sales
Rank
Sales
Rank
Sales
Rank
Sales
Rank
29.60
2
33.21
20
31.23
14
29.83
6.5
29.59
1
31.80
17
31.14
13
29.69
4
30.11
9
30.54
11
30.04
8
29.61
3
29.83
6.5
31.39
15
30.87
12
29.72
5
30.32
10
33.05
18
33.08
19
31.41
16
In assigning ranks to the sales, the lowest sales, the second in-aisle sales in Table 12.4, is 
assigned the rank of 1 and the highest sales, the first front sales, is assigned the rank of 20. 
Because the fourth in-aisle sales and the first expert sales are tied for ranks 6 and 7, each is  
assigned the rank 6.5.
After all the ranks are assigned, you compute the sum of the ranks for each group:
Rank sums: T1 = 28.5  T2 = 81  T3 = 66  T4 = 34.5
As a check on the rankings, recall from Equation (12.6) on page 496 that for any integer n, 
the sum of the first n consecutive integers is n1n + 12>2. Therefore,
 T1 + T2 + T3 + T4 = n1n + 12
2
 28.5 + 81 + 66 + 34.5 = 12021212
2
 210 = 210

	
12.5  Kruskal-Wallis Rank Test: A Nonparametric Method for the One-Way ANOVA	
503
To test the null hypothesis of equal population medians, you calculate the test statistic H 
using Equation (12.8) on page 501:
 H = c
12
n1n + 12 a
c
j = 1
T 2
j
nj
d - 31n + 12
 = e
12
12021212 c 128.522
5
+ 18122
5
+ 16622
5
+ 134.522
5
d f - 31212
 = a 12
420b12,583.92 - 63 = 10.8257
The test statistic H approximately follows a chi-square distribution with c - l degrees of  
freedom. Using a 0.05 level of significance, x2
a, the upper-tail critical value of the chi-square 
distribution with c - l = 3 degrees of freedom, is 7.815 (see Table 12.15). Because the com-
puted value of the test statistic H = 10.8257 is greater than the critical value of 7.815, you 
reject the null hypothesis and conclude that the median mobile electronics sales is not the same 
for all the in-store locations.
Figure 12.14 show the Excel and Minitab Kruskal-Wallis rank test results, which are iden-
tical when rounded to three decimal places. From these results, you reject the null hypothesis 
because the p@value = 0.0127 6 0.05. At this point, you could simultaneously compare all 
pairs of suppliers to determine which ones differ (see reference 2).
F i g u r e  1 2 . 1 4
Excel and Minitab Kruskal-Wallis rank test results for the differences among the median mobile electronics sales for 
four in-store locations
T a b l e  1 2 . 1 5
Finding x2
a, the Upper-
Tail Critical Value for 
the Kruskal-Wallis Rank 
Test, at the 0.05 Level 
of Significance with  
3 Degrees of Freedom
Cumulative Area
.005
.01
.025
.05
.10
.25
.75
.90
.95
.975
Upper-Tail Area
Degrees  
of Freedom
.995
.99
.975
.95
.90
.75
.25
.10
.05
.025
1
—
—
0.001
0.004
0.016
0.102
1.323
2.706
3.841
5.024
2
0.010
0.020
0.051
0.103
0.211
0.575
2.773
4.605
5.991
7.378
3
0.072
0.115
0.216
0.352
0.584
1.213
4.108
6.251
7.815
9.348
4
0.207
0.297
0.484
0.711
1.064
1.923
5.385
7.779
9.488
11.143
5
0.412
0.554
0.831
1.145
1.610
2.675
6.626
9.236
11.071
12.833
Source: Extracted from Table E.4.

504	
Chapter 12  Chi-Square and Nonparametric Tests
Assumptions
To use the Kruskal-Wallis rank test, the following assumptions must be met:
 • The c samples are randomly and independently selected from their respective popula-
tions.
 • The underlying variable is continuous.
 • The data provide at least a set of ranks, both within and among the c samples.
 • The c populations have the same variability.
 • The c populations have the same shape.
The Kruskal-Wallis procedure makes less stringent assumptions than does the F test.  
If you ignore the last two assumptions (variability and shape), you can still use the Kruskal-
Wallis rank test to determine whether at least one of the populations differs from the other 
populations in some characteristic—such as central tendency, variation, or shape.
To use the F test, you must assume that the c samples are from normal populations that 
have equal variances. When the more stringent assumptions of the F test hold, you should use 
the F test instead of the Kruskal-Wallis test because it has slightly more power to detect sig-
nificant differences among groups. However, if the assumptions of the F test do not hold, you 
should use the Kruskal-Wallis test.
Problems for Section 12.5
Learning the Basics
12.43  What is the upper-tail critical value from the chi-square 
distribution if you use the Kruskal-Wallis rank test for comparing 
the medians in six populations at the 0.01 level of significance?
12.44  For this problem, use the results of Problem 12.43.
a.	 State the decision rule for testing the null hypothesis that all six 
groups have equal population medians.
b.	 What is your statistical decision if the computed value of the 
test statistic H is 13.77?
Applying the Concepts
12.45  A pet food company has the business objective of expand-
ing its product line beyond its current kidney- and shrimp-based 
cat foods. The company developed two new products—one based 
on chicken livers and the other based on salmon. The company 
conducted an experiment to compare the two new products with 
its two existing ones, as well as a generic beef-based product sold 
in a supermarket chain.
For the experiment, a sample of 50 cats from the population 
at a local animal shelter was selected. Ten cats were randomly as-
signed to each of the five products being tested. Each of the cats 
was then presented with 3 ounces of the selected food in a dish at 
feeding time. The researchers defined the variable to be measured 
as the number of ounces of food that the cat consumed within a 
10-minute time interval that began when the filled dish was pre-
sented. The results for this experiment are summarized in the table 
at right and stored in  CatFood .
a.	 At the 0.05 level of significance, is there evidence of a signifi-
cant difference in the median amount of food eaten among the 
various products?
b.	 Compare the results of (a) with those of Problem 11.13 (a) on 
page 437.
c.	 Which test is more appropriate for these data: the Kruskal- 
Wallis rank test or the one-way ANOVA F test? Explain.
Kidney
Shrimp
Chicken Liver
Salmon
Beef
2.37
2.26
2.29
1.79
2.09
2.62
2.69
2.23
2.33
1.87
2.31
2.25
2.41
1.96
1.67
2.47
2.45
2.68
2.05
1.64
2.59
2.34
2.25
2.26
2.16
2.62
2.37
2.17
2.24
1.75
2.34
2.22
2.37
1.96
1.18
2.47
2.56
2.26
1.58
1.92
2.45
2.36
2.45
2.18
1.32
2.32
2.59
2.57
1.93
1.94
SELF 
Test 
12.46  A hospital conducted a study of the waiting 
time in its emergency room. The hospital has a main 
campus, along with three satellite locations. Management had a 
business objective of reducing waiting time for emergency room 
cases that did not require immediate attention. To study this, a 
random sample of 15 emergency room cases at each location 
were selected on a particular day, and the waiting time (recorded 
from check-in to when the patient was called into the clinic 
a r e a )  wa s  m e a s u r e d .  T h e  r e s u l t s  a r e  s t o r e d  i n 
 ERWaiting .
a.	 At the 0.05 level of significance, is there evidence of a differ-
ence in the median waiting times in the four locations?
b.	 Compare the results of (a) with those of Problem 11.9 (a) on 
page 436.

	
12.6  McNemar Test for the Difference Between Two Proportions (Related Samples) 	
505
12.47  QSR magazine has been reporting on the largest quick-
serve and fast-casual brands in the United States for nearly  
15 years. The file  QSR  contains the food segment (burger, 
chicken, pizza, or sandwich) and U.S. mean sales per unit ($thou-
sands) for each of 38 quick-service brands. (Data extracted from 
bit.ly/Oj6EcY.)
a.	 At the 0.05 level of significance, is there evidence of a differ-
ence in the median U.S. average sales per unit ($thousands) 
among the food segments?
b.	 Compare the results of (a) with those of Problem 11.11 (a) on 
page 436.
12.48  An advertising agency has been hired by a manufacturer of 
pens to develop an advertising campaign for the upcoming holiday 
season. To prepare for this project, the research director decides to 
initiate a study of the effect of advertising on product perception. 
An experiment is designed to compare five different advertise-
ments. Advertisement A greatly undersells the pen’s characteris-
tics. Advertisement B slightly undersells the pen’s characteristics. 
Advertisement C slightly oversells the pen’s characteristics. Ad-
vertisement D greatly oversells the pen’s characteristics. Adver-
tisement E attempts to correctly state the pen’s characteristics.
A sample of 30 adult respondents, taken from a larger focus 
group, is randomly assigned to the five advertisements (so that 
there are six respondents to each). After reading the advertisement 
and developing a sense of product expectation, all respondents 
unknowingly receive the same pen to evaluate. The respondents 
are permitted to test the pen and the plausibility of the advertising 
copy. The respondents are then asked to rate the pen from 1 to 7 
on the product characteristic scales of appearance, durability, and 
writing performance. The combined scores of three ratings (ap-
pearance, durability, and writing performance) for the 30 respon-
dents are stored in  Pen . These data are:
A
B
C
D
E
15
16
8
5
12
18
17
7
6
19
17
21
10
13
18
19
16
15
11
12
19
19
14
9
17
20
17
14
10
14
a.	 At the 0.05 level of significance, is there evidence of a differ-
ence in the median ratings of the five advertisements?
b.	 Compare the results of (a) with those of Problem 11.10 (a) on 
page 436.
c.	 Which test is more appropriate for these data: the Kruskal- 
Wallis rank test or the one-way ANOVA F test? Explain.
12.49  A sporting goods manufacturing company wanted to com-
pare the distance traveled by golf balls produced using each of 
four different designs. Ten balls of each design were manufactured 
and brought to the local golf course for the club professional to 
test. The order in which the balls were hit with the same club from 
the first tee was randomized so that the pro did not know which 
type of ball was being hit. All 40 balls were hit in a short period of 
time, during which the environmental conditions were essentially 
the same. The results (distance traveled in yards) for the four de-
signs are stored in  Golfball .
a.	 At the 0.05 level of significance, is there evidence of a differ-
ence in the median distances traveled by the golf balls with dif-
ferent designs?
b.	 Compare the results of (a) with those of Problem 11.14 (a) on 
page 437.
12.50  The more costly and time consuming it is to export and 
import, the more difficult it is for local companies to be competi-
tive and to reach international markets. As part of an initial inves-
tigation exploring foreign market entry, 10 countries were selected 
from each of four global regions. The cost associated with import-
ing a standardized cargo of goods by sea transport in these coun-
tries (in US$ per container) is stored in  ForeignMarket2 . (Data 
extracted from doingbusiness.org/data).
a.	 At the 0.05 level of significance, is there evidence of a dif-
ference in the median cost across the four global regions as-
sociated with importing a standardized cargo of goods by sea 
transport?
b.	 Compare the results in (a) to those in Problem 11.8 (a) on  
page 436.
12.6  McNemar Test for the Difference Between  
Two Proportions (Related Samples)
Tests such as chi-square test for the difference between two proportions discussed in  
Section 12.1 require independent samples from each population. However, sometimes when 
you are testing differences between the proportion of items of interest, the data are collected 
from repeated measurements or matched samples.
To test whether there is evidence of a difference between the proportions when the data 
have been collected from two related samples, you can use the McNemar test. The Section 12.6  
online topic discusses this test and illustrates its use.

506	
Chapter 12  Chi-Square and Nonparametric Tests
12.7  Chi-Square Test for the Variance  
or Standard Deviation
When analyzing numerical data, sometimes you need to test a hypothesis about the population 
variance or standard deviation. Assuming that the data are normally distributed, you use the x2 
test for the variance or standard deviation to test whether the population variance or standard 
deviation is equal to a specified value. The Section 12.7 online topic discusses this test and 
illustrates its use.
12.8  Wilcoxon Signed Ranks Test: A Nonparametric Test 
for Two Related Populations
In Section 10.2, you used the paired t test to compare the means of two populations when you 
had repeated measures or matched samples. The paired t test assumes that the data are mea-
sured on an interval or a ratio scale and are normally distributed. If you cannot make these as-
sumptions, you can use the nonparametric Wilcoxon signed ranks test to test for the median 
difference. The Section 12.8 online topic discusses this test and illustrates its use.
12.9  Friedman Rank Test: A Nonparametric Test  
for the Randomized Block Design
When analyzing a randomized block design, sometimes the data consists only of ranks within 
each block. Other times, you cannot assume that the data from each of the c groups are from 
normally distributed populations. In these situations, you can use the Friedman rank test. The 
Section 12.9 online topic discusses this test and illustrates its use.
I
n the Using Statistics scenario, you were the manager of 
T.C. Resort Properties, a collection of five upscale hotels 
located on two tropical islands. To assess the quality of ser-
vices being provided by your hotels, guests are encouraged 
to complete a satisfaction survey when they check out or 
via email after they check out. You analyzed the data from 
these surveys to determine the overall satisfaction with the 
services provided, the likelihood that the guests will return 
to the hotel, and the reasons given by some guests for not 
wanting to return.
On one island, T.C. Resort Properties operates the 
Beachcomber and Windsurfer hotels. You performed a chi-
square test for the difference in two proportions and con-
cluded that a greater proportion of guests are willing to 
return to the Beachcomber Hotel than to the Windsurfer. On 
the other island, T.C. Resort Properties operates the Golden 
Palm, Palm Royale, and Palm Princess hotels. To see if 
guest satisfaction was the same among the three hotels, you  
performed a chi-square test for the differences among more 
than two proportions. The test confirmed that the three pro-
portions are not equal, and guests seem to be most likely to 
return to the Palm Royale and least likely to return to the 
Golden Palm.
In addition, you investigated whether the reasons given 
for not returning to the Golden Palm, Palm Royale, and 
Palm Princess were unique to a certain hotel or common to 
all three hotels. By performing a chi-square test of indepen-
dence, you determined that the reasons given for wanting 
to return or not depended on the hotel where the guests had 
been staying. By examining the observed and expected fre-
quencies, you concluded that guests were more satisfied with 
the price at the Golden Palm and were much more satisfied 
with the location of the Palm Princess. Guest satisfaction 
with room accommodations was not significantly different 
among the three hotels.
U s i n g  S tat i s t i c s
Avoiding Guesswork About 
Resort Guests, Revisited
Maturos1812>Shutterstock

	
References	
507
S u m m a r y
Figure 12.15 presents a roadmap for this chapter. First, you 
used hypothesis testing for analyzing categorical data from 
two independent samples and from more than two inde-
pendent samples. In addition, the rules of probability from  
Section 4.2 were extended to the hypothesis of indepen-
dence in the joint responses to two categorical variables. 
You also studied two nonparametric tests. You used the  
Wilcoxon rank sum test when the assumptions of the t test 
for two independent samples were violated and the Kruskal-
Wallis test when the assumptions of the one-way ANOVA F 
test were violated.
Tests for 
Proportions
χ2  Tests of 
Independence
2
3 or More
1
r × c
Tables
2 × c
Tables
2 × 2
Tables
Contingency
Tables
Wilcoxon
Rank Sum Test
Kruskal-Wallis
Test
Nonparametric
Tests
Categorical
Data Procedures
Z  Test for a
Proportion
(see Section 9.4)
Number of
Samples
χ2 Test for
p1 = p2 
Z  Test for
p1 = p2
(see Section 10.3)
χ2 Test for
p1 = p2 = ... = pc
Marascuilo
Procedure
F i g u r e  1 2 . 1 5
Roadmap of Chapter 12
Refere n c e s
	 1.	Conover, W. J. Practical Nonparametric Statistics, 3rd ed. 
New York: Wiley, 2000.
	 2.	Daniel, W. W. Applied Nonparametric Statistics, 2nd ed.  
Boston: PWS Kent, 1990.
	 3.	Dixon, W. J., and F. J. Massey, Jr. Introduction to Statistical 
Analysis, 4th ed. New York: McGraw-Hill, 1983.
	 4.	Hollander, M., and D. A. Wolfe. Nonparametric Statistical 
Methods, 2nd ed. New York: Wiley, 1999.
	 5.	Lewontin, R. C., and J. Felsenstein. “Robustness of Homo-
geneity Tests in 2 * n Tables,” Biometrics, 21(March 1965): 
19–33.
	 6.	Marascuilo, L. A. “Large-Sample Multiple Comparisons,” 
Psychological Bulletin, 65(1966): 280–290.
	 7.	Marascuilo, L. A., and M. McSweeney. Nonparametric and 
Distribution-Free Methods for the Social Sciences. Monterey, 
CA: Brooks/Cole, 1977.
	 8.	Microsoft Excel 2013. Redmond, WA: Microsoft Corp., 2012.
	 9.	Minitab Release 16. State College, PA: Minitab Inc., 2010.
	10.	Winer, B. J., D. R. Brown, and K. M. Michels. Statistical  
Principles in Experimental Design, 3rd ed. New York:  
McGraw-Hill, 1989.

508	
Chapter 12  Chi-Square and Nonparametric Tests
K e y  Eq u at i o n s
X2 Test for the Difference Between Two Proportions
x2
STAT =
a
all cells
1fo - fe22
fe
	
(12.1)
Computing the Estimated Overall Proportion for  
Two Groups
p = X1 + X2
n1 + n2
= X
n	
(12.2)
Computing the Estimated Overall Proportion for c Groups
p = X1 + X2 + g + Xc
n1 + n2 + g + nc
= X
n	
(12.3)
Critical Range for the Marascuilo Procedure
Critical range = 2x2
aB
pj11 - pj2
nj
+  
pj′11 - pj′2
nj′
 (12.4)
Computing the Expected Frequency
fe = Row total * Column total
n
	
(12.5)
Checking the Rankings
T1 + T2 = n1n + 12
2
	
(12.6)
Large-Sample Wilcoxon Rank Sum Test
ZSTAT =
T1 - n11n + 12
2
B
n1 n21n + 12
12
	
(12.7)
Kruskal-Wallis Rank Test for Differences Among c 
Medians
 H = c
12
n1n + 12 a
c
j = 1
T 2
j
nj
d - 31n + 12	
(12.8)
K e y  Term s
chi-square 1x22 distribution  478
chi-square 1x22 test for the difference be-
tween two proportions  477
chi-square 1x22 test of independence  490
expected frequency 1fe2  477
Kruskal-Wallis rank test  501
Marascuilo procedure  487
nonparametric methods  495
observed frequency 1fo2  477
2 * c contingency table  483
2 * 2 contingency table  476
two-way contingency table  476
Wilcoxon rank sum test  495
C hec ki n g  Yo u r  U n d e r s ta nding
12.51  What is the difference between Chi-Square test for differ-
ences among more than two proportions and Chi-Square Test of 
Independence?
12.52  State the purpose of using multiple-comparisons procedure 
such as the Marascuilo procedure. Explain briefly the computation 
for Marascuilo procedure to determine the difference in proportion.
12.53  Under what conditions should you use the x2 test of inde-
pendence?
12.54  Under what conditions should you use the Wilcoxon rank 
sum test instead of the t test for the difference between the means?
12.55  Under what conditions should you use the Kruskal-Wallis 
rank test instead of the one-way ANOVA?
C ha pter  R e vi e w P r o b le ms
12.56  Undergraduate students at Miami University in Oxford, 
Ohio, were surveyed in order to evaluate the effect of gender and 
price on purchasing a pizza from Pizza Hut. Students were told to 
suppose that they were planning to have a large two-topping pizza 
delivered to their residence that evening. The students had to de-
cide between ordering from Pizza Hut at a reduced price of $8.49 
(the regular price for a large two-topping pizza from the Oxford 
Pizza Hut at the time was $11.49) and ordering a pizza from a  

	
Chapter Review Problems	
509
different pizzeria. The results from this question are summarized 
in the following contingency table:
Pizzeria
Gender
Pizza Hut
Other
Total
Female
4
13
17
Male
6
12
18
Total
10
25
35
a.	 Using a 0.05 level of significance, is there evidence of a differ-
ence between males and females in their pizzeria selection?
b.	 What is your answer to (a) if nine of the male students selected 
Pizza Hut and nine selected another pizzeria?
A subsequent survey evaluated purchase decisions at other prices. 
These results are summarized in the following contingency table:
Price
Pizzeria
$8.49
$11.49
$14.49
Total
Pizza Hut
10
  5
  2
17
Other
25
23
27
75
Total
35
28
29
92
c.	 Using a 0.05 level of significance and using the data in the  
second contingency table, is there evidence of a difference in 
pizzeria selection based on price?
d.	 Determine the p-value in (c) and interpret its meaning.
12.57  What social media tools do marketers commonly use? 
The “2012 Social Media Marketing Industry Report” by Social 
Media Examiner (socialmediaexaminer.com) surveyed the per-
centage of marketers who commonly use an indicated social me-
dia tool. Surveyed were both B2B marketers, marketers that focus 
primarily on attracting businesses, and B2C marketers, market-
ers that primarily target consumers. Suppose the survey was 
based on 500 B2B marketers and 500 B2C marketers and yielded 
the results in the following table. (Data extracted from bit.ly 
/QmMxPa.)
Business Focus
Social Media Tool
B2B
B2C
Facebook
87%
96%
Twitter
84%
80%
LinkedIn
87%
59%
YouTube or other video
56%
59%
For each social media tool, at the 0.05 level of significance, de-
termine whether there is a difference between B2B marketers and 
B2C marketers in the proportion who used each social media tool.
12.58  A company is considering an organizational change  
involving the use of self-managed work teams. To assess the  
attitudes of employees of the company toward this change, a  
sample of 400 employees is selected and asked whether they favor 
the institution of self-managed work teams in the organization. Three  
responses are permitted: favor, neutral, or oppose. The results of 
the survey, cross-classified by type of job and attitude toward self-
managed work teams, are summarized as follows:
Self-Managed  
Work Teams
Type of Job
Favor
Neutral
Oppose
Total
Hourly worker
108
46
71
225
Supervisor
18
12
30
60
Middle management
35
14
26
75
Upper management
24
7
9
40
Total
185
79
136
400
a.	 At the 0.05 level of significance, is there evidence of a rela-
tionship between attitude toward self-managed work teams and 
type of job?
	
	
The survey also asked respondents about their attitudes to-
ward instituting a policy whereby an employee could take one 
additional vacation day per month without pay. The results, 
cross-classified by type of job, are as follows:
Vacation Time Without Pay
Type of Job
Favor
Neutral
Oppose
Total
Hourly worker
135
23
  67
225
Supervisor
  39
  7
  14
  60
Middle management
  47
  6
  22
  75
Upper management
  26
  6
    8
  40
Total
247
42
111
400
b.	 At the 0.05 level of significance, is there evidence of a rela-
tionship between attitude toward vacation time without pay and 
type of job?
12.59  A company that produces and markets continuing educa-
tion programs on DVDs for the educational testing industry has 
traditionally mailed advertising to prospective customers. A mar-
ket research study was undertaken to compare two approaches: 
mailing a sample DVD upon request that contained highlights of 
the full DVD and sending an email containing a link to a website 
from which sample material could be downloaded. Of those who 
responded to either the mailing or the email, the results were as 
follows in terms of purchase of the complete DVD:
Type of Media Used
Purchased
Mailing
Email
Total
Yes
26
11
37
No
227
247
474
Total
253
258
511

510	
Chapter 12  Chi-Square and Nonparametric Tests
a.	 At the 0.05 level of significance, is there evidence of a differ-
ence in the proportion of DVDs purchased on the basis of the 
type of media used?
b.	 On the basis of the results of (a), which type of media should 
the company use in the future? Explain the rationale for your 
decision.
The company also wanted to determine which of three sales  
approaches should be used to generate sales among those who  
either requested the sample DVD by mail or downloaded the  
sample DVD but did not purchase the full DVD: (1) targeted 
email, (2) a DVD that contained additional features, or (3) a tele-
phone call to prospective customers. The 474 respondents who did 
not initially purchase the full DVD were randomly assigned to one 
of the three sales approaches. The results, in terms of purchases of 
the fullprogram DVD, are as follows:
Sales Approach
Action
Targeted 
Email
More 
Complete 
DVD
Telephone 
Call
Total
Purchase
    5
  17
  18
  40
Don’t purchase
153
141
140
434
Total
158
158
158
474
c.	 At the 0.05 level of significance, is there evidence of a differ-
ence in the proportion of DVDs purchased on the basis of the 
sales strategy used?
d.	 On the basis of the results of (c), which sales approach do you 
think the company should use in the future? Explain the ratio-
nale for your decision.
C a s e s  f o r  C h a p t e r  1 2
Managing Ashland MultiComm Services
Phase 1
Reviewing the results of its research, the marketing depart-
ment team concluded that a segment of Ashland households 
might be interested in a discounted trial subscription to the 
AMS 3-For-All cable/phone/Internet service. The team de-
cided to test various discounts before determining the type 
of discount to offer during the trial period. It decided to 
conduct an experiment using three types of discounts plus a 
plan that offered no discount during the trial period:
1.	No discount for the 3-For-All cable/phone/Internet ser-
vice. Subscribers would pay $24.99 per week for the 
3-For-All cable/phone/Internet service during the 90-day 
trial period.
2.	Moderate discount for the 3-For-All cable/phone/ 
Internet service. Subscribers would pay $19.99 per week 
for the 3-For-All cable/phone/Internet service during the 
90-day trial period.
3.	Substantial discount for the 3-For-All cable/phone/ 
Internet service. Subscribers would pay $14.99 per week 
for the 3-For-All cable/phone/Internet service during the 
90-day trial period.
4.	Discount restaurant card. Subscribers would be given a 
special card providing a discount of 15% at selected res-
taurants in Ashland during the trial period.
Each participant in the experiment was randomly as-
signed to a discount plan. A random sample of 100 sub-
scribers to each plan during the trial period was tracked to 
determine how many would continue to subscribe to the 
3-For-All service after the trial period. Table AMS12.1 sum-
marizes the results.
T a b l e  A M S 1 2 . 1
Number of Subscribers Who Continue Subscriptions 
After Trial Period with Four Discount Plans
Continue 
Subscriptions 
After Trial 
Period
Discount Plans
No 
Discount
Moderate 
Discount
Substantial 
Discount
Restaurant 
Card
Total
Yes
24
30
38
51
143
No
76
70
62
49
257
Total
100
100
100
100
400
1.	Analyze the results of the experiment. Write a report to 
the team that includes your recommendation for which 
discount plan to use. Be prepared to discuss the limita-
tions and assumptions of the experiment.
Phase 2
The marketing department team discussed the results of the 
survey presented in Chapter 8, on pages 329–330. The team 
realized that the evaluation of individual questions was pro-
viding only limited information. In order to further understand 
the market for the 3-For-All cable/phone/Internet service, the 
data were organized in the following contingency tables:
Has AMS Telephone 
Service
Has AMS  
Internet Service
Yes
No
Total
Yes
  55
  28
  83
No
207
128
335
Total
262
156
418

	
Cases for Chapter 12	
511
Discount Trial
Type of Service
Yes
No
Total
Basic
  8
156
164
Enhanced
32
222
254
Total
40
378
418
Watches Premium or  
on-Demand Services
Type of 
Service
Almost 
Every Day
Several 
Times a 
Week
Almost 
Never
Never
Total
Basic
2
5
127
30
164
Enhanced
12
30
186
26
254
Total
14
35
313
56
418
Watches Premium or  
on-Demand Services
Discount
Almost 
Every 
Day
Several 
Times a 
Week
Almost 
Never
Never
Total
Yes
4
5
27
4
40
No
10
30
286
52
378
Total
14
35
313
56
418
Method for Current Subscription
Discount
Toll-Free 
Phone
AMS 
Website
Direct Mail  
Reply Card
Good Tunes  
& More
Other Total
Yes
11
21
5
1
2
40
No
219
85
41
9
24
378
Total
230
106
46
10
26
418
 
Method for Current Subscription
 
Gold 
Card
Toll-Free 
Phone
AMS 
Website
Direct Mail 
Reply Card
Good Tunes  
& More
Other
Total
Yes
  10
  20
  5
  1
  2
  38
No
220
  86
41
  9
24
380
Total
230
106
46
10
26
418
	 2.	 Analyze the results of the contingency tables. Write a 
report for the marketing department team, discussing 
the marketing implications of the results for Ashland 
MultiComm Services.
Digital Case
Apply your knowledge of testing for the difference between 
two proportions in this Digital Case, which extends the T.C. 
Resort Properties Using Statistics scenario of this chapter.
As T.C. Resort Properties seeks to improve its customer 
service, the company faces new competition from SunLow 
Resorts. SunLow has recently opened resort hotels on the 
islands where T.C. Resort Properties has its five hotels.  
SunLow is currently advertising that a random survey of 
300 customers revealed that about 60% of the customers 
preferred its “Concierge Class” travel reward program over 
the T.C. Resorts “TCRewards Plus” program.
Open and review ConciergeClass.pdf, an electronic 
brochure that describes the Concierge Class program and 
compares it to the T.C. Resorts program. Then answer the 
following questions:
1.	Are the claims made by SunLow valid?
2.	What analyses of the survey data would lead to a more 
favorable impression about T.C. Resort Properties?
3.	Perform one of the analyses identified in your answer to 
step 2.
4.	Review the data about the T.C. Resort Properties custom-
ers presented in this chapter. Are there any other ques-
tions that you might include in a future survey of travel 
reward programs? Explain.
Sure Value Convenience Stores
You work in the corporate office for a nationwide conve-
nience store franchise that operates nearly 10,000 stores. 
The per-store daily customer count (i.e., the mean num-
ber of customers in a store in one day) has been steady,  
at 900, for some time. To increase the customer count, the 
chain is considering cutting prices for coffee beverages. 
Management needs to determine how much prices can be 
cut in order to increase the daily customer count without  

512	
Chapter 12  Chi-Square and Nonparametric Tests
reducing the gross margin on coffee sales too much. You 
decide to carry out an experiment in a sample of 24 stores 
where customer counts have been running almost exactly at 
the national average of 900. In 6 of the stores, a small coffee 
will be $0.59, in another 6 stores the price will be $0.69, in 
a third group of 6 stores, the price will be $0.79, and in a 
fourth group of 6 stores, the price will now be $0.89. After 
four weeks, the daily customer count in the stores is stored 
in  CoffeeSales .
At the 0.05 level of significance, is there evidence of a 
difference in the median daily customer count based on the 
price of a small coffee? What price should the stores sell the 
coffee for?
CardioGood Fitness
Return to the CardioGood Fitness case first presented on 
page 111. The data for this case are stored in  CardioGood 
 Fitness .
1.	Determine whether differences exist in the median age in 
years, education in years, annual household income ($), 
number of times the customer plans to use the treadmill 
each week, and the number of miles the customer expects 
to walk or run each week based on the product purchased 
(TM195, TM498, TM798).
2.	Determine whether differences exist in the relation-
ship status (single or partnered), and the self-rated fit-
ness based on the product purchased (TM195, TM498, 
TM798).
3.	Write a report to be presented to the management of Car-
dioGood Fitness, detailing your findings.
More Descriptive Choices Follow-Up
Follow up the “Using Statistics: More Descriptive Choices, 
Revisited” on page 166 by using the data that are stored in 
 Retirement Funds  to:
1.	Determine whether there is a difference between the 
growth and value funds in the median three-year return 
percentages, five-year return percentages, and ten-year 
return percentages.
2.	Determine whether there is a difference between the 
small, mid-cap, and large market cap funds in the  
median three-year return percentages, five-year return 
percentages, and ten-year return percentages.
3.	Determine whether there is a difference in risk based on 
market cap, a difference in rating based on market cap, a 
difference in risk based on type of fund, and a difference 
in rating based on type of fund.
4.	Write a report summarizing your findings.
Clear Mountain State Student Surveys
1.	The Student News Service at Clear Mountain State  
University (CMSU) has decided to gather data about 
the undergraduate students that attend CMSU. It cre-
ates and distributes a survey of 14 questions and receives 
responses from 62 undergraduates, which it stores in  
 UndergradSurvey .
a.  Construct contingency tables using gender, major, 
plans to go to graduate school, and employment status. 
(You need to construct six tables, taking two variables 
at a time.) Analyze the data at the 0.05 level of signifi-
cance to determine whether any significant relation-
ships exist among these variables.
b.	At the 0.05 level of significance, is there evidence of a 
difference between males and females in median grade 
point average, expected starting salary, number of 
social networking sites registered for, age, spending on 
textbooks and supplies, text messages sent in a week, 
and the wealth needed to feel rich?
c.	At the 0.05 level of significance, is there evidence of a 
difference between students who plan to go to gradu-
ate school and those who do not plan to go to gradu-
ate school in median grade point average, expected 
­starting salary, number of social networking sites reg-
istered for, age, spending on textbooks and supplies, 

	
Cases for Chapter 12	
513
text messages sent in a week, and the wealth needed to 
feel rich?
d.	At the 0.05 level of significance, is there evidence 
of a difference based on academic major, in median 
expected starting salary, number of social networking 
sites registered for, age, spending on textbooks and 
supplies, text messages sent in a week, and the wealth 
needed to feel rich?
e.	At the 0.05 level of significance, is there evidence 
of a difference based on graduate school intention in 
median grade point average, expected starting salary, 
number of social networking sites registered for, age, 
spending on textbooks and supplies, text messages 
sent in a week, and the wealth needed to feel rich?
2.	The dean of students at CMSU has learned about the 
undergraduate survey and has decided to undertake a 
similar survey for graduate students at CMSU. She cre-
ates and distributes a survey of 14 questions and re-
ceives responses from 44 graduate students, which 
she stores them in  GradSurvey . For these data, at the  
0.05 level of significance:
a.	Construct contingency tables using gender, undergrad-
uate major, graduate major, and employment status. 
(You need to construct six tables, taking two variables 
at a time.) Analyze the data to determine whether any 
significant relationships exist among these variables.
b.	Is there evidence of a difference between males and 
females in the median age, undergraduate grade point 
average, graduate grade point average, expected salary 
upon graduation, spending on textbooks and supplies, 
text messages sent in a week, and the wealth needed to 
feel rich?
c.	Is there evidence of a difference based on undergradu-
ate major in the median age, undergraduate grade point 
average, graduate grade point average, expected salary 
upon graduation, spending on textbooks and supplies, 
text messages sent in a week, and the wealth needed to 
feel rich?
d.	Is there evidence of a difference based on graduate 
major in the median age, undergraduate grade point 
average, graduate grade point average, expected salary 
upon graduation, spending on textbooks and supplies, 
text messages sent in a week, and the wealth needed to 
feel rich?
e.	Is there evidence of a difference based on employ-
ment status in the median age, undergraduate grade 
point average, graduate grade point average, expected 
salary upon graduation, spending on textbooks and 
supplies, text messages sent in a week, and the wealth 
needed to feel rich?

514	
Chapter 12  Chi-Square and Nonparametric Tests
EG12.1  Chi-Square Test for the 
Difference Between Two 
Proportions
Key Technique  Use the CHISQ.INV.RT(level of significance, 
degrees of freedom) function to compute the critical value and 
use the CHISQ.DIST.RT(chi-square test statistic, degrees of  
freedom) function to compute the p-value.
Example  Perform this chi-square test for the two-hotel guest 
satisfaction data shown in Figure 12.3 on page 480.
PHStat  Use Chi-Square Test for Differences in Two  
Proportions.
For the example, select PHStat ➔ Two-Sample Tests  
(Summarized Data) ➔ Chi-Square Test for Differences in Two 
Proportions. In the procedure’s dialog box, enter 0.05 as the Level 
of Significance, enter a Title, and click OK. In the new worksheet:
	 1.	 Read the yellow note about entering values and then press the 
Delete key to delete the note.
	 2.	 Enter Hotel in cell B4 and Choose Again? in cell A5.
	 3.	 Enter Beachcomber in cell B5 and Windsurfer in cell C5.
	 4.	 Enter Yes in cell A6 and No in cell A7.
	 5.	 Enter 163, 64, 154, and 108 in cells B6, B7, C6, and C7,  
respectively.
In-Depth Excel  Use the COMPUTE worksheet of the  
Chi-Square workbook as a template.
The worksheet already contains the Table 12.2 two-hotel guest 
satisfaction data. For other problems, change the Observed  
Frequencies cell counts and row and column labels in rows 4 
through 7.
Read the Short Takes for Chapter 12 for an explanation of 
the formulas found in the COMPUTE worksheet (shown in the 
COMPUTE_FORMULAS worksheet). If you are using an older 
Excel version, use the COMPUTE_OLDER worksheet.
EG12.2  Chi-Square Test for 
Differences Among More 
Than Two Proportions
Key Technique  Use the CHISQ.INV.RT and CHISQ.DIST.
RT functions to compute the critical value and the p-value,  
respectively.
Example  Perform this chi-square test for the three-hotel guest 
satisfaction data shown in Figure 12.6 on page 486.
PHStat  Use Chi-Square Test.
For the example, select PHStat ➔ Multiple-Sample Tests ➔ 
Chi-Square Test. In the procedure’s dialog box (shown in right 
column):
	 1.	 Enter 0.05 as the Level of Significance.
	 2.	 Enter 2 as the Number of Rows.
	 3.	 Enter 3 as the Number of Columns.
	 4.	 Enter a Title and click OK.
In the new worksheet:
	 5.	 Read the yellow note instructions about entering values and 
then press the Delete key to delete the note.
	 6.	 Enter the Table 12.6 data (see page 484), including row and 
column labels, in rows 4 through 7. The #DIV/0! error mes-
sages will disappear when you finish entering all the table 
data.
In-Depth Excel  Use the ChiSquare2x3 worksheet of the  
Chi-Square Worksheets workbook as a model.
The worksheet already contains the Table 12.6 guest satisfaction 
data (see page 484). For other 2 * 3 problems, change the Ob-
served Frequencies cell counts and row and column labels in 
rows 4 through 7. For 2 * 4 problems, use the ChiSquare2x4 
worksheet and change the Observed Frequencies cell counts 
and row and column labels in that worksheet. For 2 * 5 prob-
lems, use the ChiSquare2x5 worksheet and change the  
Observed Frequencies cell counts and row and column labels in 
that worksheet.
The formulas that are found in the ChiSquare2x3 workbook 
(shown in the ChiSquare2x3_FORMULAS worksheet) are 
similar to the formulas found in the COMPUTE worksheet of the 
Chi-Square workbook (see the previous section). If you use an  
Excel version older than Excel 2010, use the ChiSquare2x3_
OLDER worksheet.
The Marascuilo Procedure
Key Technique  Use formulas to compute the absolute differ-
ences and the critical range.
Example  Perform the Marascuilo Procedure for the guest sat-
isfaction survey that is shown in Figure 12.7 on page 488.
PHStat  Modify the PHStat instructions of the previous section. 
In step 4, check Marascuilo Procedure in addition to entering a 
Title and clicking OK.
C h a p t e r  1 2  E x c e l  G u i d e

	
Chapter 12 Excel Guide	
515
In-Depth Excel  Use the Marascuilo2x3 of the Chi-Square 
Worksheets workbook as a template.
The worksheet requires no entries or changes to use. For 2 * 4 
problems, use the Marascuilo2x4 worksheet and for 2 * 5 prob-
lems, use the Marascuilo2x5 worksheet.
Read the Short Takes for Chapter 12 for an explanation of 
the formulas found in the Marascuilo2x3 worksheet (shown in the 
Marascuilo2x3_FORMULAS worksheet). If you use an Excel 
version older than Excel 2010, use the Marascuilo2x3_OLDER 
worksheet.
EG12.3  Chi-Square Test of 
Independence
Key Technique  Use the CHISQ.INV.RT and CHISQ.DIST.
RT functions to compute the critical value and the p-value,  
respectively.
Example  Perform this chi-square test for the primary reason  
for not returning to hotel data that is shown in Figure 12.10 on 
page 493.
PHStat  Use Chi-Square Test.
For the example, select PHStat ➔ Multiple-Sample Tests 
➔ Chi-Square Test. In the procedure’s dialog box (shown  
below):
	 1.	 Enter 0.05 as the Level of Significance.
	 2.	 Enter 4 as the Number of Rows.
	 3.	 Enter 3 as the Number of Columns.
	 4.	 Enter a Title and click OK.
In the new worksheet:
	 5.	 Read the yellow note about entering values and then press the 
Delete key to delete the note.
	 6.	 Enter the Table 12.9 data on page 490, including row and col-
umn labels, in rows 4 through 9. The #DIV/0! error messages 
will disappear when you finish entering all of the table data.
In-Depth Excel  Use the ChiSquare4x3 worksheet of the Chi-
Square Worksheets workbook as a model. 
The worksheet already contains the Table 12.9 primary reason 
for not returning to hotel data (see page 490). For other 4 * 3 
problems, change the Observed Frequencies cell counts and 
row and column labels in rows 4 through 9. For 3 * 4 prob-
lems, use the ChiSquare3x4 worksheet. For 4 * 3 problems, 
use the ChiSquare4x3 worksheet. For 7 * 3 problems, use 
the ChiSquare7x3 worksheet. For 8 * 3 problems, use the 
ChiSquare8x3 worksheet. For each of these other worksheets, 
enter the contingency table data for the problem in the Observed 
Frequencies area.
Read the Short Takes for Chapter 12 to the Calculations  
area in columns G though I (not shown in Figure 12.10). The 
formulas found in the COMPUTE worksheet (shown in the  
COMPUTE_FORMULAS worksheet) are similar to those in 
the other chi-square worksheets discussed in this Excel Guide.
If you use an Excel version older than Excel 2010, use the 
ChiSquare4x3_OLDER worksheet.
EG12.4  Wilcoxon Rank Sum Test:  
a Nonparametric Method for 
Two Independent Populations
Key Technique  Use the NORM.S.INV(level of significance) 
function to compute the upper and lower critical values and use 
NORM.S.DIST(absolute value of the Z test statistic) as part of 
a formula to compute the p-value. For unsummarized data, use 
the COUNTIF and SUMIF functions (see Appendix Section F.4) 
to compute the sample sizes and the sum of ranks for a sample,  
respectively.
Example  Perform the Figure 12.12 Wilcoxon rank sum test for 
the cola sales for the two different end-cap locations.
PHStat  Use Wilcoxon Rank Sum Test.
For the example, open to the DATA worksheet of the Cola work-
book. Select PHStat ➔ Two-Sample Tests (Unsummarized 
Data) ➔ Wilcoxon Rank Sum Test. In the procedure’s dialog 
box (shown below):
	 1.	 Enter 0.05 as the Level of Significance.
	 2.	 Enter A1:A11 as the Population 1 Sample Cell Range.
	 3.	 Enter B1:B11 as the Population 2 Sample Cell Range.
	 4.	 Check First cells in both ranges contain label.
	 5.	 Click Two-Tail Test.
	 6.	 Enter a Title and click OK.
The procedure creates a SortedRanks worksheet that contains the 
sorted ranks in addition to the worksheet shown in Figure 12.12. 
Both of these worksheets are discussed in the following In-Depth 
Excel instructions.

516	
Chapter 12  Chi-Square and Nonparametric Tests
In-Depth Excel  Use the COMPUTE worksheet of the  
Wilcoxon workbook as a template.
The worksheet already contains data and formulas to use the un-
summarized data for the example. For other problems that use un-
summarized data, first open to the SortedRanks worksheet and 
enter the sorted values for both groups in stacked format. Use col-
umn A for the sample names and column B for the sorted values. 
Assign a rank for each value and enter the ranks in column C of 
the same worksheet. Then open to the COMPUTE worksheet (or 
the similar COMPUTE_ALL worksheet, if performing a one-tail 
test) and edit the formulas in cells B7, B8, B10, and B11.
For problems with summarized data, overwrite the formulas 
that compute the Sample Size and Sum of Ranks in the cell range 
B7:B11, with the values for these statistics.
Open to the COMPUTE_ALL_ FORMULAS worksheet 
to view all formulas in the COMPUTE_ALL worksheet. If you 
use an Excel version older than Excel 2010, use the COMPUTE_
ALL_OLDER worksheet for all tests.
EG12.5  Kruskal-Wallis Rank Test:  
a Nonparametric Method  
for the One-Way ANOVA
Key Technique  Use the CHISQ.INV.RT(level of significance, 
number of groups - 1) function to compute the critical value and 
use the CHISQ.DIST.RT(H test statistic, number of groups - 1) 
function to compute the p-value. For unsummarized data, use the 
COUNTIF and SUMIF functions (see Appendix Section F.4) 
to compute the sample sizes and the sum of ranks for a sample,  
respectively.
Example  Perform the Figure 12.14 Kruskal-Wallis rank test for 
differences among the four median mobile electronics sales for 
four in-store locations that is shown on page 503.
PHStat  Use Kruskal-Wallis Rank Test.
For the example, open to the DATA worksheet of the Second  
Experiment workbook. Select PHStat ➔ Multiple-Sample 
Tests ➔ Kruskal-Wallis Rank Test. In the procedure’s dialog 
box (shown in right column):
	 1.	 Enter 0.05 as the Level of Significance.
	 2.	 Enter A1:D6 as the Sample Data Cell Range.
	 3.	 Check First cells contain label.
	 4.	 Enter a Title and click OK.
The procedure creates a SortedRanks worksheet that contains 
sorted ranks in addition to the worksheet shown in Figure 12.14 on 
page 503. Both of these worksheets are discussed in the following 
In-Depth Excel instructions.
In-Depth Excel  Use the KruskalWallis4 worksheet of the 
Kruskal-Wallis Worksheets workbook as a template.
The worksheet already contains the data and formulas to use the 
unsummarized data for the example. For other problems with four 
groups and unsummarized data, first open to the SortedRanks 
worksheet and enter the sorted values for both groups in stacked 
format. Use column A for the sample names and column B for the 
sorted values. Assign ranks for each value and enter the ranks in 
column C of the same worksheet. Also paste your unsummarized 
stacked data in columns, starting with column E. (The row 1 cells, 
starting with cell E1, are used to identify each group.) Then open 
to the KruskalWallis4 worksheet and edit the formulas in columns 
E and F.
For other problems with four groups and summarized data, 
open to the KruskalWallis4 worksheet and overwrite the formulas 
that display the group names and compute the Sample Size and 
Sum of Ranks in columns D, E, and F with the values for these 
statistics. For other problems with three groups, use the similar 
KruskalWallis3 worksheet.
Open to the KruskalWallis4_FORMULAS worksheet to 
view all formulas in the KruskalWallis4 worksheet. If you use 
an Excel version older than Excel 2010, use the KruskalWallis4_
OLDER or KruskalWallis3_OLDER worksheets.

	
Chapter 12 Minitab Guide	
517
MG12.1  Chi-Square Test for the 
Difference Between Two 
Proportions
Use Chi-Square Test (Two-Way Table in Worksheet) (requires 
summarized data). 
For example, to perform the Figure 12.3 test for the two-hotel 
guest satisfaction data on page 480, open to the Two-Hotel  
Survey worksheet. Select Stat ➔ Tables ➔ Chi-Square Test 
(Two-Way Table in Worksheet). In the Chi-Square Test (Table in 
Worksheet) dialog box (shown below):
	 1.	 Double-click C2  Beachcomber in the variables list to add 
Beachcomber to the Columns containing the table box.
	 2.	 Double-click C3  Windsurfer in the variables list to add 
Windsurfer to the Columns containing the table box.
	 3.	 Click OK.
Minitab can also perform a chi-square test for the differ-
ence between two proportions using unsummarized data. Use the  
Section MG2.1 instructions for using Cross Tabulation and  
Chi-Square to create contingency tables (see page 124), replacing 
step 4 with these steps 4 though 7:
	 4.	 Click Chi-Square.
In the Cross Tabulation - Chi-Square dialog box:
	 5.	 Select Chi-Square analysis, Expected cell counts, and Each 
cell’s contribution to the Chi-Square statistic.
	 6.	 Click OK.
	 7.	 Back in the original dialog box, click OK.
MG12.2  Chi-Square Test for 
Differences Among More 
Than Two Proportions
Use Chi-Square Test (Two-Way Table in Worksheet) (requires 
summarized data). 
Use modified Section MG2.1 instructions on the page 124 for 
­using Cross Tabulation and Chi-Square to perform the chi-
square test with unsummarized data. See Section MG12.1 for de-
tailed ­instructions.
To perform the Figure 12.6 test for the three-hotel guest 
satisfaction data on page 486, open to the Three-Hotel Survey  
worksheet, select Stat ➔ Tables ➔ Chi-Square Test (Two-Way 
Table in Worksheet). In the Chi-Square Test (Table in Worksheet) 
dialog box, enter the names of columns 2 through 4 in the Col-
umns containing the table box and click OK.
The Marascuilo Procedure
There are no Minitab Guide instructions for this procedure.
MG12.3  Chi-Square Test of 
Independence
Use the Section MG2.1 instructions for either Chi-Square Test 
(Two-Way Table in Worksheet) for summarized data or the 
(modified) Cross Tabulation and Chi-Square for unsummarized 
data to perform this test.
MG12.4  Wilcoxon Rank Sum Test: 
a Nonparametric METHOD for 
Two Independent Populations
Use Mann-Whitney to perform a test equivalent to the Wilcoxon 
rank sum test. 
For example, to perform the Figure 12.12 test for the cola sales 
for the two different end-cap locations on page 498, open to the 
Cola worksheet. Select Stat ➔ Nonparametrics ➔ Mann- 
Whitney. In the Mann-Whitney dialog box (shown below):
	 1.	 Double-click C1  Beverage in the variables list to add  
Beverage in the First Sample box.
	 2.	 Double-click C2  Produce in the variables list to add Produce 
in the Second Sample box.
	 3.	 Enter 95.0 in the Confidence level box.
	 4.	 Select not equal in the Alternative drop-down list.
	 5.	 Click OK.
C h a p t e r  1 2  M i n i ta b  G u i d e

518	
Chapter 12  Chi-Square and Nonparametric Tests
MG12.5  Kruskal-Wallis Rank Test: 
a Nonparametric METHOD 
for the One-Way ANOVA
Use Kruskal-Wallis. 
For example, to perform the Figure 12.14 Kruskal-Wallis rank test 
for differences among the four median mobile electronics sales 
for four in-store locations on page 502, open to the Second Ex-
periment Stacked worksheet. Select Stat ➔ Nonparametrics 
➔ Kruskal-Wallis. In the Kruskal-Wallis dialog box (shown at 
right):
	 1.	 Double-click C2  Sales in the variables list to add Sales in the 
Response box.
	 2.	 Double-click C1  Location in the variables list to add Loca-
tion in the Factor box.
	 3.	 Click OK.

519
U s i n g  S tat i s t i c s
Knowing Customers at Sunflowers Apparel
Having survived recent economic slowdowns that have diminished their competi-
tors, Sunflowers Apparel, a chain of upscale fashion stores for women, is in the 
midst of a companywide review that includes researching the factors that make 
their stores successful. Until recently, Sunflowers managers did not use data 
analysis to help select where to open stores, relying instead on subjective factors, 
such as the availability of an inexpensive lease or the perception that a particular 
location seemed ideal for one of their stores.
As the new director of planning, you have already consulted with marketing data 
firms that specialize in identifying and classifying groups of consumers. Based on 
such preliminary analyses, you have already tentatively discovered that the profile of 
Sunflowers shoppers may not only be the upper middle class long suspected of being 
the chain’s clientele but may also include younger, aspirational families with young 
children, and, surprisingly, urban hipsters that set trends and are mostly single.
You seek to develop a systematic approach that will lead to making better 
decisions during the site-selection process. As a starting point, you have asked 
one marketing data firm to collect and organize data for the number of people in 
the identified groups of interest who live within a fixed radius of each store. You 
believe that the greater numbers of profiled customers contribute to store sales, 
and you want to explore the possible use of this relationship in the decision- 
making process. How can you use statistics so that you can forecast the annual 
sales of a proposed store based on the number of profiled customers that reside 
within a fixed radius of a Sunflowers store?
contents
13.1  Types of Regression Models
13.2	 Determining the Simple 
Linear Regression Equation
Visual Explorations: Exploring 
Simple Linear Regression Coefficients
13.3	 Measures of Variation
13.4	 Assumptions of Regression
13.5	 Residual Analysis
13.6	 Measuring Autocorrelation: 
The Durbin-Watson Statistic
13.7	 Inferences About the Slope 
and Correlation Coefficient
13.8	 Estimation of Mean Values 
and Prediction of Individual 
Values
13.9	 Potential Pitfalls in Regression
Six Steps for Avoiding the Potential 
Pitfalls
Using Statistics: Knowing 
Customers at Sunflowers Apparel, 
Revisited
Chapter 13 Excel Guide
Chapter 13 Minitab Guide
Objectives
To learn to use regression analysis 
to predict the value of a 
dependent variable based on the 
value of an independent variable
Understand the meaning of the 
regression coefficients b0 and b1
To learn to evaluate the assumptions 
of regression analysis and what to 
do if the assumptions are violated
To make inferences about the slope 
and correlation coefficient
To estimate mean values and 
predict individual values
Simple Linear 
Regression
13
Chapter
Fotolia

520	
Chapter 13  Simple Linear Regression
I
n this chapter and the next two chapters, you learn regression analysis techniques that 
help uncover relationships between variables. Regression analysis leads to selection of a 
model that expresses how one or more independent variables can be used to predict the 
value of another variable, called the dependent variable. Regression models identify the type 
of mathematical relationship that exists between a dependent variable and an independent vari-
able, thereby enabling you to quantify the effect that a change in the independent variable has 
on the dependent variable. Models also help you identify unusual values that may be outliers 
(see page 606).
This chapter discusses simple linear regression models that use a single numerical 
independent variable, X, to predict the numerical dependent variable, Y. (Chapters 14 and 
15 discuss multiple regression models that use several independent variables to predict the 
dependent variable.) In the Sunflowers scenario, your initial belief reflects a possible sim-
ple linear regression model in which the number of profiled customers would be the single  
numerical independent variable, X, being used to predict the annual sales of the store, the 
dependent variable, Y.
Using a scatter plot (also known as scatter diagram) to visualize the X and Y variables, a tech-
nique introduced in Section 2.5 on page 93, can help suggest a starting point for regression 
analysis. The scatter plots in Figure 13.1 illustrates six possible relationships between an X and 
Y variable.
13.1  Types of Regression Models
Y
Panel A
Positive linear relationship
X
Y
Panel B
Negative linear relationship
X
Y
Panel F
No relationship between X and Y
X
Y
Panel C
Positive curvilinear relationship
X
Y
Panel D
U-shaped curvilinear relationship
X
Y
Panel E
Negative curvilinear relationship
X
F i g u r e  1 3 . 1
Six types of relationships 
found in scatter plots

	
13.1  Types of Regression Models	
521
Equation (13.1) expresses this relationship mathematically by defining the simple linear 
regression model.
Y
0
ΔX = “change in X”
ΔY = “change in Y”
0
X
β0
F i g u r e  1 3 . 2
A straight-line relationship
Simple Linear Regression Model
	
Yi = b0 + b1Xi + ei	
(13.1)
where
b0 = Y intercept for the population
b1 = slope for the population
ei = random error in Y for observation i
Yi = dependent variable (sometimes referred to as the response variable) for observation i
Xi = independent variable (sometimes referred to as the predictor, or explanatory 
variable) for observation i
In Panel A, values of Y are generally increasing linearly as X increases. This panel is simi-
lar to Figure 13.3 on page 522, which illustrates the positive relationship between the num-
ber of profiled customers of the store and the store’s annual sales for the Sunflowers Apparel 
women’s clothing store chain.
Panel B is an example of a negative linear relationship. As X increases, the values of Y are 
generally decreasing. An example of this type of relationship might be the price of a particular 
product and the amount of sales. As the price charged for the product increases, the amount of 
sales may tend to decrease.
Panel C shows a positive curvilinear relationship between X and Y. The values of Y increase as 
X increases, but this increase tapers off beyond certain values of X. An example of a positive curvi-
linear relationship might be the age and maintenance cost of a machine. As a machine gets older, 
the maintenance cost may rise rapidly at first but then level off beyond a certain number of years.
Panel D shows a U-shaped relationship between X and Y. As X increases, at first Y gener-
ally decreases; but as X continues to increase, Y not only stops decreasing but actually increases 
above its minimum value. An example of this type of relationship might be entrepreneurial 
activity and levels of economic development as measured by GDP per capita. Entrepreneurial 
activity occurs more in the least and most developed countries.
Panel E illustrates an exponential relationship between X and Y. In this case, Y decreases 
very rapidly as X first increases, but then it decreases much less rapidly as X increases further. 
An example of an exponential relationship could be the value of an automobile and its age. 
The value drops drastically from its original price in the first year, but it decreases much less 
rapidly in subsequent years.
Finally, Panel F shows a set of data in which there is very little or no relationship between 
X and Y. High and low values of Y appear at each value of X.
Simple Linear Regression Models
Although scatter plots provide preliminary analysis, more sophisticated statistical procedures 
determine the most appropriate model for a set of variables. Simple linear regression models 
represent the simplest relationship of a straight-line or linear relationship. Figure 13.2 illus-
trates this relationship.

522	
Chapter 13  Simple Linear Regression
The Yi = b0 + b1Xi portion of the simple linear regression model expressed in  
Equation (13.1) is a straight line. The slope of the line, b1, represents the expected change in Y 
per unit change in X. It represents the mean amount that Y changes (either positively or nega-
tively) for a one-unit change in X. The Y intercept, b0, represents the mean value of Y when X 
equals 0. The last component of the model, ei, represents the random error in Y for each obser-
vation, i. In other words, ei is the vertical distance of the actual value of Yi above or below the 
expected value of Yi on the line.
13.2  Determining the Simple Linear Regression Equation
In the Sunflowers Apparel scenario on page 519, the business objective of the director of plan-
ning is to forecast annual sales for all new stores, based on the number of profiled custom-
ers who live no more than 30 minutes from a Sunflowers store. To examine the relationship 
between the number of profiled customers (in millions) who live within a fixed radius from 
a Sunflowers store and its annual sales ($millions), data were collected from a sample of  
14 stores. Table 13.1 shows the organized data, which are stored in  SiteSelection .
T a b l e  1 3 . 1
Number of Profiled 
Customers (in millions) 
and Annual Sales (in 
$millions) for a Sample 
of 14 Sunflowers 
Apparel Stores
Store
Profiled 
Customers 
(millions)
Annual Sales 
($millions)
1
3.7
5.7
2
3.6
5.9
3
2.8
6.7
4
5.6
9.5
5
3.3
5.4
6
2.2
3.5
7
3.3
6.2
Store
Profiled 
Customers 
(millions)
Annual Sales 
($millions)
  8
3.1
4.7
  9
3.2
6.1
10
3.5
4.9
11
5.2
10.7
12
4.6
7.6
13
5.8
11.8
14
3.0
4.1
F i g u r e  1 3 . 3
Scatter plot for the 
Sunflowers Apparel data
The Least-Squares Method
In the preceding section, a statistical model is hypothesized to represent the relationship be-
tween two variables—number of profiled customers and sales—in the entire population of 
Sunflowers Apparel stores. However, as shown in Table 13.1, the data are collected from a 
random sample of stores. If certain assumptions are valid (see Section 13.4), you can use the 
sample Y intercept, b0, and the sample slope, b1, as estimates of the respective population  
parameters, b0 and b1. Equation (13.2) uses these estimates to form the simple linear regres-
sion equation. This straight line is often referred to as the prediction line.
Figure 13.3 displays the scatter plot for the data in Table 13.1. Observe the increasing rela-
tionship between profiled customers 1X2 and annual sales 1Y2. As the number of profiled cus-
tomers increases, annual sales increase approximately as a straight line (superimposed on scatter 
plot). Thus, you can assume that a straight line provides a useful mathematical model of this 
relationship. Now you need to determine the specific straight line that is the best fit to these data.

	
13.2  Determining the Simple Linear Regression Equation	
523
Student Tip
In mathematics, the sym-
bol b is often used for 
the Y intercept instead of 
b0 and the symbol m is 
often used for the slope 
instead of b1.
Simple Linear Regression Equation: The Prediction Line
The predicted value of Y equals the Y intercept plus the slope multiplied by the value of X.
	
Yni = b0 + b1Xi	
(13.2)
	
where
Yni = predicted value of Y for observation i
Xi = value of X for observation i
b0 = sample Y intercept
b1 = sample slope
Equation (13.2) requires you to determine two regression coefficients—b0 (the sample Y in-
tercept) and b1 (the sample slope). The most common approach to finding b0 and b1 is using 
the least-squares method. This method minimizes the sum of the squared differences between 
the actual values 1Yi2 and the predicted values 1Yni2, using the simple linear regression equation 
[i.e., the prediction line; see Equation (13.2)]. This sum of squared differences is equal to
a
n
i = 1
1Yi - Yni22
Because Yni = b0 + b1Xi,
a
n
i = 1
1Yi - Yni22 = a
n
i = 1
3Yi - 1b0 + b1Xi242
Because this equation has two unknowns, b0 and b1, the sum of squared differences depends 
on the sample Y intercept, b0, and the sample slope, b1. The least-squares method deter-
mines the values of b0 and b1 that minimize the sum of squared differences around the predic-
tion line. Any values for b0 and b1 other than those determined by the least-squares method  
result in a greater sum of squared differences between the actual values 1Yi2 and the predicted  
values 1Yni2.
Figure 13.4 presents the worksheet for the simple linear regression model for the  
Table 13.1 Sunflowers Apparel data. In this figure, Excel labels b0 as Intercept and Minitab 
labels b0 as Constant. They each label b1 as Profiled Customers.
Student Tip
Although the solutions 
to Examples 13.3 and 
13.4 (on pages 526–
527 and 532–533, 
respectively) present the 
formulas for computing 
these values (and oth-
ers), you should always 
consider using software 
to compute the values of 
the terms discussed in 
this chapter.
F i g u r e  1 3 . 4
Excel and Minitab simple linear regression models for the Sunflowers Apparel data

524	
Chapter 13  Simple Linear Regression
In Figure 13.4, observe that b0 = -1.2088 and b1 = 2.0742. Using Equation (13.2) on 
page 523, the prediction line for these data is
Yni = -1.2088 + 2.0742Xi
The slope, b1, is +2.0742. This means that for each increase of 1 unit in X, the predicted 
mean value of Y is estimated to increase by 2.0742 units. In other words, for each increase of  
1.0 million profiled customers within 30 minutes of the store, the predicted mean annual sales 
are estimated to increase by $2.0742 million. Thus, the slope represents the portion of the  
annual sales that are estimated to vary according to the number of profiled customers.
The Y intercept, b0, is -1.2088. The Y intercept represents the predicted value of Y when 
X equals 0. Because the number of profiled customers of the store cannot be 0, this Y intercept 
has little or no practical interpretation. Also, the Y intercept for this example is outside the 
range of the observed values of the X variable, and therefore interpretations of the value of b0 
should be made cautiously. Figure 13.5 displays the actual values and the prediction line.
Student Tip
Remember that a  
positive slope means  
that as X increases, Y is  
predicted to increase.  
A negative slope means 
that as X increases, Y is 
predicted to decrease.
F i g u r e  1 3 . 5
Scatter plot and 
prediction line for 
Sunflowers Apparel data
Example 13.1 illustrates a situation in which there is a direct interpretation for the Y  
intercept, b0.
Return to the Sunflowers Apparel scenario on page 519. Example 13.2 illustrates how you 
use the prediction line to predict the annual sales.
Example 13.1
Interpreting the Y 
Intercept, b0, and 
the Slope, b1
A statistics professor wants to use the number of hours a student studies for a statistics final 
exam 1X2 to predict the final exam score 1Y2. A regression model is fit based on data collected 
from a class during the previous semester, with the following results:
Yni = 35.0 + 3Xi
What is the interpretation of the Y intercept, b0, and the slope, b1?
Solution  The Y intercept b0 = 35.0 indicates that when the student does not study for the 
final exam, the predicted mean final exam score is 35.0. The slope b1 = 3 indicates that for 
each increase of one hour in studying time, the predicted change in the mean final exam score 
is +3.0. In other words, the final exam score is predicted to increase by a mean of 3 points for 
each one-hour increase in studying time.

	
13.2  Determining the Simple Linear Regression Equation	
525
Predictions in Regression Analysis:  
Interpolation Versus Extrapolation
When using a regression model for prediction purposes, you should consider only the  
relevant range of the independent variable in making predictions. This relevant range includes 
all values from the smallest to the largest X used in developing the regression model. Hence, 
when predicting Y for a given value of X, you can interpolate within this relevant range of the X 
values, but you should not extrapolate beyond the range of X values. When you use the number 
of profiled customers to predict annual sales, the number of profiled customers (in millions) 
varies from 2.2 to 5.8 (see Table 13.1 on page 522). Therefore, you should predict annual sales 
only for stores that have between 2.2 and 5.8 million profiled customers. Any prediction of 
annual sales for stores outside this range assumes that the observed relationship between sales 
and the number of profiled customers for stores that have between 2.2 and 5.8 million profiled 
customers is the same as for stores outside this range. For example, you cannot extrapolate 
the linear relationship beyond 5.8 million profiled customers in Example 13.2. It would be 
improper to use the prediction line to forecast the sales for a new store that has 8 million pro-
filed customers because the relationship between sales and the number of profiled customers 
may have a point of diminishing returns. If that is true, as the number of profiled customers 
increases beyond 5.8 million, the effect on sales may become smaller and smaller.
Computing the Y Intercept, b0, and the Slope, b1
For small data sets, you can use a hand calculator to compute the least-squares regression coef-
ficients. Equations (13.3) and (13.4) give the values of b0 and b1, which minimize
a
n
i = 1
1Yi - Yni22 = a
n
i = 1
3Yi - 1b0 + b1Xi242
Example 13.2
Predicting Annual 
Sales Based on 
Number of Profiled 
Customers
Use the prediction line to predict the annual sales for a store with 4 million profiled customers.
Solution  You can determine the predicted value of annual sales by substituting X = 4 
(millions of profiled customers) into the simple linear regression equation:
 Yni =  -1.2088 + 2.0742Xi
 Yni =  -1.2088 + 2.0742142 = 7.0879 or +7,087,900
Thus, a store with 4 million profiled customers has predicted mean annual sales of $7,087,900.
Computational Formula for the Slope, b1
	
b1 = SSXY
SSX 	
(13.3)
	
where
 SSXY = a
n
i = 1
1Xi - X21Yi - Y2 = a
n
i = 1
XiYi -
a a
n
i = 1
Xib a a
n
i = 1
Yib
n
 SSX = a
n
i = 1
1Xi - X22 = a
n
i = 1
X2
i -
a a
n
i = 1
Xib
2
n
(continued)

526	
Chapter 13  Simple Linear Regression
Computational Formula for The Y Intercept, b0
	
b0 = Y - b1X	
(13.4)
	
where
 Y =
a
n
i = 1
Yi
n
 X =
a
n
i = 1
Xi
n
Example 13.3
Computing the Y 
Intercept, b0, and 
the Slope, b1
Compute the Y intercept, b0, and the slope, b1, for the Sunflowers Apparel data.
Solution  In Equations (13.3) and (13.4), five quantities need to be computed to determine 
b1 and b0. These are n, the sample size; a
n
i = 1
Xi, the sum of the X values; a
n
i = 1
Yi, the sum of the  
Y values; a
n
i = 1
X2
i , the sum of the squared X values; and a
n
i = 1
XiYi, the sum of the product of X and Y. 
For the Sunflowers Apparel data, the number of profiled customers 1X2 is used to predict the 
annual sales 1Y2 in a store. Table 13.2 presents the computations of the sums needed for the 
site selection problem. The table also includes a
n
i = 1
Y 2
i , the sum of the squared Y values that will 
be used to compute SST in Section 13.3.
T a b l e  1 3 . 2
Computations for the 
Sunflowers Apparel 
Data
Store
Profiled Customers  
(X)
Annual Sales  
(Y)
X2
Y 2
XY
1
3.7
5.7
13.69
32.49
21.09
2
3.6
5.9
12.96
34.81
21.24
3
2.8
6.7
7.84
44.89
18.76
4
5.6
9.5
31.36
90.25
53.20
5
3.3
5.4
10.89
29.16
17.82
6
2.2
3.5
4.84
12.25
7.70
7
3.3
6.2
10.89
38.44
20.46
8
3.1
4.7
9.61
22.09
14.57
9
3.2
6.1
10.24
37.21
19.52
10
3.5
4.9
12.25
24.01
17.15
11
5.2
10.7
27.04
114.49
55.64
12
4.6
7.6
21.16
57.76
34.96
13
5.8
11.8
33.64
139.24
68.44
14
3.0
4.1
9.00
16.81
12.30
Totals
52.9
92.8
215.41
693.90
382.85

	
13.2  Determining the Simple Linear Regression Equation	
527
Using Equations (13.3) and (13.4), you can compute b0 and b1:
 SSXY = a
n
i = 1
1Xi - X21Yi - Y2 = a
n
i = 1
XiYi -
a a
n
i = 1
Xib a a
n
i = 1
Yib
n
 = 382.85 - 152.92192.82
14
 = 382.85 - 350.65142
 = 32.19858
 SSX = a
n
i = 1
1Xi - X22 = a
n
i = 1
X2
i -
a a
n
i = 1
Xib
2
n
 = 215.41 - 152.922
14
 = 215.41 - 199.88642
 = 15.52358
With these values, compute b1:
 b1 = SSXY
SSX
 = 32.19858
15.52358
= 2.07417
and:
Y =
a
n
i = 1
Yi
n
= 92.8
14
= 6.62857
 X =
a
n
i = 1
Xi
n
= 52.9
14
= 3.77857
With these values, compute b0:
 b0 = Y - b1X
 = 6.62857 - 2.0741713.778572
 = -1.2088265
Student Tip
Coefficients computed 
manually with the as-
sistance of handheld 
calculators may dif-
fer slightly because of 
rounding errors caused 
by the limited number of 
decimal places that your 
calculator might use.

528	
Chapter 13  Simple Linear Regression
Problems for Section 13.2
Learning the Basics
13.1  The following regression line predicts the final exam grades 
based on interim examination grades:
Yni = 0.51 + 3.15Xi
a.	 Explain, in words, the dependent and independent variable.
b.	 Explain what the value of 0.51 represents.
c.	 Determine the change in Y, if the value of X changes from  
70 to 80.
13.2  If the values of X in Problem 13.1 range from 2 to 25, should 
you use this model to predict the mean value of Y when X equals
a.	 3?
b.	 -3?
c.	 0?
d.	 24?
13.3  The following regression line predicts the final exam grades 
based on interim examination grades:
a.	 Explain, in words, the dependent and independent variable.
b.	 Explain what the value of 0.51 represents.
c.	 Determine the change in Y, if the value of X changes from 70 to 80.
Applying the Concepts
SELF 
Test 
13.4  The production of wine is a multibillion-dollar 
worldwide industry. In an attempt to develop a model 
of wine quality as judged by wine experts, data was collected from 
red wine variants of Portuguese “Vinho Verde” wine. (Data 
­extracted from P. Cortez, Cerdeira, A., Almeida, F., Matos, T., 
and  Reis, J., “Modeling Wine Preferences by Data  
Mining from Physiochemical Properties,” Decision Support Sys-
tems, 47, 2009, pp. 547–553 and bit.ly/9xKlEa.) A sample of 50 
wines is stored in  VinhoVerde . Develop a simple linear regression 
model to predict wine quality, measured on a scale from 0 (very 
bad) to 10 (excellent), based on alcohol content (%).
a.	 Construct a scatter plot.
For these data, b0 = -0.3529 and b1 = 0.5624.
b.	 Interpret the meaning of the slope, b1, in this problem.
c.	 Predict the mean wine quality for wines with a 10% alcohol 
content.
d.	 What conclusion can you reach based on the results of (a)–(c)?
Open the VE-Simple Linear Regression add-in workbook 
to explore the coefficients. (See Appendix C to learn how you can 
download a copy of this workbook and Appendix Section D.5 be-
fore using this workbook.) When this workbook opens properly, it 
adds a Simple Linear Regression menu in either the Add-ins tab  
(Microsoft Windows) or the Apple menu bar (OS X).
To explore the effects of changing the simple linear regression 
coefficients, select Simple Linear Regression ➔ Explore Coef-
ficients. In the Explore Coefficients floating control panel (shown 
inset below), click the spinner buttons for b1 slope (the slope of the 
prediction line) and b0 intercept (the Y intercept of the prediction 
line) to change the prediction line. Using the visual feedback of the 
chart, try to create a prediction line that is as close as possible to 
the prediction line defined by the least-squares estimates. In other 
words, try to make the Difference from Target SSE value as small 
as possible. (See page 531 for an explanation of SSE.)
At any time, click Reset to reset the b1 and b0 values or 
Solution to reveal the prediction line defined by the least-squares 
method. Click Finish when you are finished with this exercise.
Using Your Own Regression Data
Select Simple Linear Regression using your worksheet data 
from the Simple Linear Regression menu to explore the simple 
linear regression coefficients using data you supply from a work-
sheet. In the procedure’s dialog box, enter the cell range of your  
Y variable as the Y Variable Cell Range and the cell range of your 
X variable as the X Variable Cell Range. Click First cells in both 
ranges contain a label, enter a Title, and click OK. After the scat-
ter plot appears onscreen, continue with the Explore Coefficients 
floating control panel as described in the left column.
V i s u a l  E x p l o r at i o n s   Exploring Simple Linear Regression Coefficients

	
13.2  Determining the Simple Linear Regression Equation	
529
13.5  Zagat’s publishes restaurant ratings for various locations 
in the United States. The file  Restaurants  contains the Zagat  
rating for food, décor, service, and the cost per person for a sample  
of 100 restaurants located in New York City and in a suburb of 
New York City. Develop a regression model to predict the cost per 
person, based on a variable that represents the sum of the ratings 
for food, décor, and service.
Sources: Extracted from Zagat Survey 2013, New York City 
Restaurants; and Zagat Survey 2012–2013, Long Island Restaurants.
a.	 Construct a scatter plot.
	
For these data, b0 = -46.7718 and b1 = 1.4963.
b.	 Assuming a linear relationship, use the least-squares method to 
compute the regression coefficients b0 and b1.
c.	 Interpret the meaning of the Y intercept, b0, and the slope, b1, in 
this problem.
d.	 Predict the mean cost per person for a restaurant with a sum-
mated rating of 50.
e.	 What should you tell the owner of a group of restaurants in this 
geographical area about the relationship between the summated 
rating and the cost of a meal?
13.6  The owner of a moving company typically has his most ex-
perienced manager predict the total number of labor hours that will 
be required to complete an upcoming move. This approach has 
proved useful in the past, but the owner has the business objective 
of developing a more accurate method of predicting labor hours. In 
a preliminary effort to provide a more accurate method, the owner 
has decided to use the number of cubic feet moved as the indepen-
dent variable and has collected data for 36 moves in which the ori-
gin and destination were within the borough of Manhattan in New 
York City and in which the travel time was an insignificant portion 
of the hours worked. The data are stored in Moving .
a.	 Construct a scatter plot.
b.	 Assuming a linear relationship, use the least-squares method to 
determine the regression coefficients b0 and b1.
c.	 Interpret the meaning of the slope, b1, in this problem.
d.	 Predict the mean labor hours for moving 500 cubic feet.
e.	 What should you tell the owner of the moving company about 
the relationship between cubic feet moved and labor hours?
13.7  Starbucks Coffee Co. uses a data-based approach to im-
proving the quality and customer satisfaction of its products. 
When survey data indicated that Starbucks needed to improve its 
package-sealing process, an experiment was conducted to deter-
mine the factors in the bag-sealing equipment that might be af-
fecting the ease of opening the bag without tearing the inner liner 
of the bag. (Data extracted from L. Johnson and S. Burrows, “For  
Starbucks, It’s in the Bag,” Quality Progress, March 2011,  
pp. 17–23.) One factor that could affect the rating of the ability of 
the bag to resist tears was the plate gap on the bag-sealing equip-
ment. Data were collected on 19 bags in which the plate gap was 
varied. The results are stored in  Starbucks .
a.	 Construct a scatter plot.
b.	 Assuming a linear relationship, use the least-squares method to 
determine the regression coefficients b0 and b1.
c.	 Interpret the meaning of the slope, b1, in this problem.
d.	 Predict the mean tear rating when the plate gap is equal to 0.
e.	 What should you tell management of Starbucks about the rela-
tionship between the plate gap and the tear rating?
13.8  The value of a sports franchise is directly related to  
the amount of revenue that a franchise can generate. The file 
 BBRevenue2013  represents the value in 2013 (in $millions) 
and the annual revenue (in $millions) for the 30 Major League  
Baseball franchises. (Data extracted from www.forbes.com/ 
mlb-valuations/list.) Suppose you want to develop a simple linear 
regression model to predict franchise value based on annual rev-
enue generated.
a.	 Construct a scatter plot.
b.	 Use the least-squares method to determine the regression coef-
ficients b0 and b1.
c.	 Interpret the meaning of b0 and b1 in this problem.
d.	 Predict the mean value of a baseball franchise that generates  
$250 million of annual revenue.
e.	 What would you tell a group considering an investment in a 
major league baseball team about the relationship between rev-
enue and the value of a team?
13.9  An agent for a residential real estate company in a sub-
urb located outside of Washington, DC, has the business objec-
tive of developing more accurate estimates of the monthly rental 
cost for apartments. Toward that goal, the agent would like to 
use the size of an apartment, as defined by square footage to 
predict the monthly rental cost. The agent selects a sample of 
48 one-bedroom apartments and collects and stores the data in 
 RentSilverSpring .
a.	 Construct a scatter plot.
b.	 Use the least-squares method to determine the regression coef-
ficients b0 and b1.
c.	 Interpret the meaning of b0 and b1 in this problem.
d.	 Predict the mean monthly rent for an apartment that has 800 
square feet.
e.	 Why would it not be appropriate to use the model to predict the 
monthly rent for apartments that have 1,500 square feet?
f.	 Your friends Jim and Jennifer are considering signing a lease 
for an a one-bedroom apartment in this residential neighbor-
hood. They are trying to decide between two apartments, one 
with 800 square feet for a monthly rent of $1,130 and the other 
with 830 square feet for a monthly rent of $1,410. Based on (a) 
through (d), which apartment do you think is a better deal?
13.10  A company that holds the DVD distribution rights to mov-
ies previously released only in theaters has the business objective 
of developing estimates of the sales revenue of DVDs. Toward this 
goal, a company analyst plans to use box office gross to predict 
DVD sales revenue. For 43 movies, the analyst collects the box 
office gross (in $millions) in the year that they were released and 
the DVD revenue (in $millions) in the following year and stores 
these data in  Movie . (Data extracted from bit.ly/14uuPNB and bit 
.ly/HMh6Kx.)
For these data,
a.	 Construct a scatter plot.
b.	 Assuming a linear relationship, use the least-squares method to 
determine the regression coefficients b0 and b1.
c.	 Interpret the meaning of the slope, b1, in this problem.
d.	 Predict the mean sales revenue for a movie DVD that had a box 
office gross of $100 million.
e.	 What conclusions can you reach about predicting DVD rev-
enue from movie gross?

530	
Chapter 13  Simple Linear Regression
Computing the Sum of Squares
The regression sum of squares 1SSR2 is based on the difference between Yni (the predicted value 
of Y from the prediction line) and Y (the mean value of Y). The error sum of squares 1SSE2 
represents the part of the variation in Y that is not explained by the regression. It is based on the 
difference between Yi and Yni. The total sum of squares (SST) is equal to the regression sum of 
squares 1SSR2 plus the error sum of squares 1SSE2. Equations (13.5), (13.6), (13.7), and (13.8) 
define these measures of variation and the total sum of squares (SST).
13.3  Measures of Variation
When using the least-squares method to determine the regression coefficients for a set of data, 
you need to compute three measures of variation. The first measure, the total sum of squares 
(SST), is a measure of variation of the Yi values around their mean, Y. The total variation, 
or total sum of squares, is subdivided into explained variation and unexplained variation. 
The explained variation, or regression sum of squares (SSR), represents variation that is  
explained by the relationship between X and Y, and the unexplained variation, or error sum of 
squares (SSE), represents variation due to factors other than the relationship between X and Y. 
Figure 13.6 shows the different measures of variation for a single Yi value.
(Error sum
of squares)
  
(Regression sum
of squares)
Y
Xi
Yi
 Yi = b0 + b1Xi
(Total sum of squares)
Y
X
0
F i g u r e  1 3 . 6
Measures of variation
Measures of Variation in Regression
The total sum of squares (SST) is equal to the regression sum of squares 1SSR2 plus the 
error sum of squares 1SSE2.
	
SST = SSR + SSE	
(13.5)
Total Sum of Squares 1SST2
The total sum of squares 1SST2 is equal to the sum of the squared differences between  
each observed value of Y and the mean value of Y.
SST = total sum of squares
	
= a
n
i = 1
1Yi - Y22	
(13.6)

	
13.3  Measures of Variation	
531
Figure 13.7 shows the sum of squares portion of the Figure 13.4 results for the Sunflowers 
Apparel data. The total variation, SST, is equal to 78.7686. This amount is subdivided into the 
sum of squares explained by the regression 1SSR2, equal to 66.7854, and the sum of squares 
unexplained by the regression 1SSE2, equal to 11.9832. From Equation (13.5) on page 530:
 SST = SSR + SSE
 78.7686 = 66.7854 + 11.9832
Regression Sum of Squares (SSR)
The regression sum of squares 1SSR2 is equal to the sum of the squared differences be-
tween each predicted value of Y and the mean value of Y.
SSR = explained variation or regression sum of squares
	
= a
n
i = 1
1Yni - Y22	
(13.7)
Error Sum of Squares (SSE)
The error sum of squares 1SSE2 is equal to the sum of the squared differences between 
each observed value of Y and the predicted value of Y.
SSE = Unexplained variation or error sum of squares
	
= a
n
i = 1
1Yi - Yni22	
(13.8)
F i g u r e  1 3 . 7
Excel and Minitab sum of squares portion for the Sunflowers Apparel data
The Coefficient of Determination
By themselves, SSR, SSE, and SST provide little information. However, the ratio of the re-
gression sum of squares (SSR) to the total sum of squares 1SST2 measures the proportion of 
variation in Y that is explained by the linear relationship of the independent variable X with the 
dependent variable Y in the regression model. This ratio, called the coefficient of determina-
tion, r 2, is defined in Equation (13.9).
Coefficient of Determination
The coefficient of determination is equal to the regression sum of squares (i.e., explained 
variation) divided by the total sum of squares (i.e., total variation).
	
r 2 = regression sum of squares
total sum of squares
= SSR
SST	
(13.9)

532	
Chapter 13  Simple Linear Regression
The coefficient of determination measures the proportion of variation in Y that is  
explained by the variation in the independent variable X in the regression model.
For the Sunflowers Apparel data, with SSR = 66.7854, SSE = 11.9832, and SST = 
78.7686,
r 2 = 66.7854
78.7686 = 0.8479
Therefore, 84.79% of the variation in annual sales is explained by the variability in the number 
of profiled customers. This large r 2 indicates a strong linear relationship between these two 
variables because the regression model has explained 84.79% of the variability in predicting 
annual sales. Only 15.21% of the sample variability in annual sales is due to factors other than 
what is accounted for by the linear regression model that uses the number of profiled customers.
Figure 13.8 presents the regression statistics table portion of the Figure 13.4 results for the 
Sunflowers Apparel data. This table contains the coefficient of determination.
Student Tip
r2 must be a value 
­between 0 and 1. It  
cannot be negative.
F i g u r e  1 3 . 8
Excel and Minitab 
regression statistics for 
the Sunflowers Apparel 
data
Example 13.4
Computing the 
­Coefficient of 
­Determination
Compute the coefficient of determination, r 2, for the Sunflowers Apparel data.
Solution  You can compute SST, SSR, and SSE, which are defined in Equations (13.6), 
(13.7), and (13.8) on pages 530 and 531, by using Equations (13.10), (13.11), and (13.12).
Computational Formula for SST
	
SST = a
n
i = 1
1Yi - Y22 = a
n
i = 1
Y 2
i -
a a
n
i = 1
Yib
2
n
	
(13.10)
Computational Formula for SSR
	
SSR = a
n
i = 1
1Yni - Y22
	
= b0a
n
i = 1
Yi + b1a
n
i = 1
XiYi -
a a
n
i = 1
Yib
2
n
	
(13.11)
Computational Formula for SSE
	
SSE = a
n
i = 1
1Yi - Yni22 = a
n
i = 1
Y 2
 i - b0a
n
i = 1
Yi - b1a
n
i = 1
XiYi	
(13.12)
Using the summary results from Table 13.2 on page 526,
 SST = a
n
i = 1
1Yi - Y22 = a
n
i = 1
Y 2
i -
a a
n
i = 1
Yib
2
n
 = 693.9 - 192.822
14

	
13.3  Measures of Variation	
533
Standard Error of the Estimate
Although the least-squares method produces the line that fits the data with the minimum 
amount of prediction error, unless all the observed data points fall on a straight line, the pre-
diction line is not a perfect predictor. Just as all data values cannot be expected to be exactly 
equal to their mean, neither can all the values in a regression analysis be expected to be located 
exactly on the prediction line. Figure 13.5 on page 524 illustrates the variability around the 
prediction line for the Sunflowers Apparel data. Notice that many of the observed values of Y 
fall near the prediction line, but none of the values are exactly on the line.
The standard error of the estimate measures the variability of the observed Y values 
from the predicted Y values in the same way that the standard deviation in Chapter 3 measures 
the variability of each value around the sample mean. In other words, the standard error of the 
estimate is the standard deviation around the prediction line, whereas the standard deviation 
in Chapter 3 is the standard deviation around the sample mean. Equation (13.13) defines the 
standard error of the estimate, represented by the symbol SYX.
 = 693.9 - 615.13142
 = 78.76858
 SSR = a
n
i = 1
1Yni - Y22
 = b0a
n
i = 1
Yi + b1a
n
i = 1
XiYi -
a a
n
i = 1
Yib
2
n
 = 1-1.20882652192.82 + 12.0741721382.852 - 192.822
14
 = 66.7854
 SSE = a
n
i = 1
1Yi - Yni22
 = a
n
i = 1
Y 2
i - b0a
n
i = 1
Yi - b1a
n
i = 1
XiYi
 = 693.9 - 1-1.20882652192.82 - 12.0741721382.852
 = 11.9832
Therefore,
r 2 = 66.7854
78.7686 = 0.8479
Student Tip
Coefficients computed  
manually with the 
­assistance of handheld 
calculators may differ 
slightly.
Standard Error of the Estimate
	
SYX = A
SSE
n - 2 = H
a
n
i = 1
1Yi - Yni22
n - 2
	
(13.13)
	
where
Yi = actual value of Y for a given Xi
Yni = predicted value of Y for a given Xi
SSE = error sum of squares

534	
Chapter 13  Simple Linear Regression
From Equation (13.8) and Figure 13.4 or Figure 13.7 on pages 523 or 531, SSE = 11.9832. 
Thus,
SYX = A
11.9832
14 - 2 = 0.9993
This standard error of the estimate, equal to 0.9993 millions of dollars (i.e., $999,300), is 
labeled Standard Error in the Figure 13.8 Excel results and S in the Minitab results. The 
standard error of the estimate represents a measure of the variation around the prediction 
line. It is measured in the same units as the dependent variable Y. The interpretation of the 
standard error of the estimate is similar to that of the standard deviation. Just as the standard 
deviation measures variability around the mean, the standard error of the estimate measures 
variability around the prediction line. For Sunflowers Apparel, the typical difference between 
actual ­annual sales at a store and the predicted annual sales using the regression equation is 
approximately $999,300.
Problems for Section 13.3
Learning the Basics
13.11  How do you interpret a coefficient of determination, r2, 
equal to 0.80?
13.12  If SSR = 36 and SSE = 4, determine SST and then com-
pute the coefficient of determination, r2, and interpret its meaning.
13.13  Assume that the results of a report state that the coefficient 
of determination varies from 0.75 to 0.89. Interpret the results.
13.14  The r2 extracted from two research reports predicting sales 
are 0.45 and 0.95. Which report will project sales better?
13.15  Can the value of r2 be negative? Why or why not?
Applying the Concepts
SELF 
Test 
13.16  In Problem 13.4 on page 528, the percentage  
of alcohol was used to predict wine quality (stored in 
 VinhoVerde  ). For those data, SSR = 21.8677 and SST = 64.0000.
a.	 Determine the coefficient of determination, r2, and interpret its 
meaning.
b.	 Determine the standard error of the estimate.
c.	 How useful do you think this regression model is for predicting 
sales?
13.17  In Problem 13.5 on page 529, you used the summated rat-
ing to predict the cost of a restaurant meal (stored in  Restaurants ). 
For those data, SSR = 9,740.0629 and SST = 17,844.75.
a.	 Determine the coefficient of determination, r2, and interpret its 
meaning.
b.	 Determine the standard error of the estimate.
c.	 How useful do you think this regression model is for predicting 
the cost of a restaurant meal?
13.18  In Problem 13.6 on page 529, an owner of a moving com-
pany wanted to predict labor hours, based on the cubic feet moved 
(stored in  Moving ). Using the results of that problem,
a.	 determine the coefficient of determination, r2, and interpret its 
meaning.
b.	 determine the standard error of the estimate.
c.	 How useful do you think this regression model is for predicting 
labor hours?
13.19  In Problem 13.7 on page 529, you used the plate gap on 
the bag-sealing equipment to predict the tear rating of a bag of cof-
fee (stored in  Starbucks ). Using the results of that problem,
a.	 determine the coefficient of determination, r2, and interpret its 
meaning.
b.	 determine the standard error of the estimate.
c.	 How useful do you think this regression model is for predicting the 
tear rating based on the plate gap in the bag-sealing equipment?
13.20  In Problem 13.8 on page 529, you used annual  
revenues to predict the value of a baseball franchise (stored in 
 BBRevenue2013 ). Using the results of that problem,
a.	 determine the coefficient of determination, r2, and interpret its 
meaning.
b.	 determine the standard error of the estimate.
c.	 How useful do you think this regression model is for predicting 
the value of a baseball franchise?
13.21  In Problem 13.9 on page 529, an agent for a real  
estate company wanted to predict the monthly rent for one- 
bedroom apartments, based on the size of the apartment (stored  
in  Rent-SilverSpring ). Using the results of that problem,
a.	 determine the coefficient of determination, r2, and interpret its 
meaning.
b.	 determine the standard error of the estimate.
c.	 How useful do you think this regression model is for predicting 
the monthly rent?
d.	 Can you think of other variables that might explain the varia-
tion in monthly rent?
13.22  In Problem 13.10 on page 529, you used box office gross 
to predict DVD revenue (stored in  Movie ). Using the results of 
that problem,
a.	 determine the coefficient of determination, r2, and interpret its 
meaning.
b.	 determine the standard error of the estimate.
c.	 How useful do you think this regression model is for predicting 
DVD revenue?
d.	 Can you think of other variables that might explain the varia-
tion in DVD revenue?

	
13.5  Residual Analysis	
535
13.4  Assumptions of Regression
When hypothesis testing and the analysis of variance were discussed in Chapters 9 through 12, 
the importance of the assumptions to the validity of any conclusions reached was emphasized. 
The assumptions necessary for regression are similar to those of the analysis of variance be-
cause both are part of the general category of linear models (reference 4).
The four assumptions of regression (known by the acronym LINE) are as follows:
 • Linearity
 • Independence of errors
 • Normality of error
 • Equal variance
The first assumption, linearity, states that the relationship between variables is linear.  
Relationships between variables that are not linear are discussed in Chapter 15.
The second assumption, independence of errors, requires that the errors 1ei2 be indepen-
dent of one another. This assumption is particularly important when data are collected over a 
period of time. In such situations, the errors in a specific time period are sometimes correlated 
with those of the previous time period.
The third assumption, normality, requires that the errors 1ei2 be normally distributed at 
each value of X. Like the t test and the ANOVA F test, regression analysis is fairly robust 
against departures from the normality assumption. As long as the distribution of the errors at 
each level of X is not extremely different from a normal distribution, inferences about b0 and 
b1 are not seriously affected.
The fourth assumption, equal variance, or homoscedasticity, requires that the variance of 
the errors 1ei2 be constant for all values of X. In other words, the variability of Y values is the same 
when X is a low value as when X is a high value. The equal-variance assumption is important when 
making inferences about b0 and b1. If there are serious departures from this assumption, you can 
use either data transformations or weighted least-squares methods (see reference 4).
13.5  Residual Analysis
Sections 13.2 and 13.3 developed a regression model using the least-squares method for the 
Sunflowers Apparel data. Is this the correct model for these data? Are the assumptions pre-
sented in Section 13.4 valid? Residual analysis visually evaluates these assumptions and helps 
you determine whether the regression model that has been selected is appropriate.
The residual, or estimated error value, ei, is the difference between the observed 1Yi2 and 
predicted 1Yni2 values of the dependent variable for a given value of Xi. A residual appears on 
a scatter plot as the vertical distance between an observed value of Y and the prediction line. 
Equation (13.14) defines the residual.
Residual
The residual is equal to the difference between the observed value of Y and the predicted 
value of Y.
	
ei = Yi - Yni	
(13.14)
Evaluating the Assumptions
Recall from Section 13.4 that the four assumptions of regression (known by the acronym 
LINE) are linearity, independence, normality, and equal variance.

536	
Chapter 13  Simple Linear Regression
To assess linearity, you plot the residuals against the independent variable (number of 
profiled customers, in millions) in Figure 13.11. Although there is widespread scatter in the 
residual plot, there is no clear pattern or relationship between the residuals and Xi. The residuals 
appear to be evenly spread above and below 0 for different values of X. You can conclude that 
the linear model is appropriate for the Sunflowers Apparel data.
Student Tip
When there is no appar-
ent pattern in the residual 
plot, the plot will look like 
a random scattering of 
points.
Linearity  To evaluate linearity, you plot the residuals on the vertical axis against the cor-
responding Xi values of the independent variable on the horizontal axis. If the linear model is 
appropriate for the data, you will not see any apparent pattern in the plot. However, if the linear 
model is not appropriate, in the residual plot, there will be a relationship between the Xi values 
and the residuals, ei.
You can see such a pattern in the residuals in Figure 13.9. Panel A shows a situation in 
which, although there is an increasing trend in Y as X increases, the relationship seems cur-
vilinear because the upward trend decreases for increasing values of X. This effect is even 
more apparent in Panel B, where there is a clear relationship between Xi and ei. By remov-
ing the linear trend of X with Y, the residual plot has exposed the lack of fit in the simple 
linear model more clearly than the scatter plot in Panel A. For these data, a quadratic or 
curvilinear model (see Section 15.1) is a better fit and should be used instead of the simple 
linear model.
Y
Panel A
Panel B
X
0
e
X
F i g u r e  1 3 . 9
Studying the 
appropriateness of the 
simple linear regression 
model
To determine whether the simple linear regression model for the Sunflowers Apparel data 
is appropriate, you need to determine the residuals. Figure 13.10 displays the predicted annual 
sales values and residuals for the Sunflowers Apparel data.
F i g u r e  1 3 . 1 0
Table of residuals for the 
Sunflowers Apparel data

	
13.5  Residual Analysis	
537
F i g u r e  1 3 . 1 1
Plot of residuals against 
the profiled customers 
of a store for the 
Sunflowers Apparel data
Independence  You can evaluate the assumption of independence of the errors by plotting the 
residuals in the order or sequence in which the data were collected. If the values of Y are part of 
a time series (see Section 2.5), a residual may sometimes be related to the residual that precedes 
it. If this relationship exists between consecutive residuals (which violates the assumption of in-
dependence), the plot of the residuals versus the time in which the data were collected will often 
show a cyclical pattern. Because the Sunflowers Apparel data were collected during the same 
time period, you do not need to evaluate the independence assumption for these data.
Normality  You can evaluate the assumption of normality in the errors by constructing a 
histogram (see Section 2.4), using a stem-and-leaf display (see Section 2.4), a boxplot (see  
Section 3.3), or a normal probability plot (see Section 6.3). To evaluate the normality assump-
tion for the Sunflowers Apparel data, Table 13.3 organizes the residuals into a frequency distri-
bution and Figure 13.12 is a normal probability plot.
T a b l e  1 3 . 3
Frequency Distribution 
of 14 Residual Values 
for the Sunflowers 
Apparel Data
Residuals
Frequency
-1.25 but less than -0.75
4
-0.75 but less than -0.25
3
-0.25 but less than +0.25
2
+0.25 but less than +0.75
2
+0.75 but less than +1.25
2
+1.25 but less than +1.75
0
+1.75 but less than +2.25
1
14
Although the small sample size makes it difficult to evaluate normality, from the normal 
probability plot of the residuals in Figure 13.12, the data do not appear to depart substantially 
from a normal distribution. The robustness of regression analysis with modest departures from 
normality enables you to conclude that you should not be overly concerned about departures 
from this normality assumption in the Sunflowers Apparel data.
F i g u r e  1 3 . 1 2
Excel and Minitab normal probability plots of the residuals for the Sunflowers Apparel data

538	
Chapter 13  Simple Linear Regression
Equal Variance  You can evaluate the assumption of equal variance from a plot of the re-
siduals with Xi. You examine the plot to see if there is approximately the same amount of varia-
tion in the residuals at each value of X. For the Sunflowers Apparel data of Figure 13.11 on  
page 636, there do not appear to be major differences in the variability of the residuals for dif-
ferent Xi values. Thus, you can conclude that there is no apparent violation in the assumption 
of equal variance at each level of X.
To examine a case in which the equal-variance assumption is violated, observe Figure 13.13, 
which is a plot of the residuals with Xi for a hypothetical set of data. This plot is fan shaped 
because the variability of the residuals increases dramatically as X increases. Because this plot 
shows unequal variances of the residuals at different levels of X, the equal-variance assumption 
is invalid.
Learn More 
You can also test the as-
sumption of equal variance 
by performing the White 
test (see reference 7). Learn 
more about this test in the 
White Test online topic.
F i g u r e  1 3 . 1 3
Violation of equal 
variance
Residuals
0
X
Problems for Section 13.5
Learning the Basics
13.23  The following results provide the X values, residuals, and 
a residual plot from a regression analysis:
Is there any evidence of a pattern in the residuals? Explain.
13.24  The following results show the X values, residuals, and a 
residual plot from a regression analysis:
Is there any evidence of a pattern in the residuals? Explain.

	
13.6  Measuring Autocorrelation: The Durbin-Watson Statistic 	
539
Applying the Concepts
13.25  In Problem 13.5 on page 529, you used the summated 
rating to predict the cost of a restaurant meal. Perform a residual 
analysis for these data (stored in  Restaurants ). Evaluate whether 
the assumptions of regression have been seriously violated.
SELF 
Test 
13.26  In Problem 13.4 on page 528, you used the per-
centage of alcohol to predict wine quality. Perform a 
residual analysis for these data (stored in  VinhoVerde ). Evaluate 
whether the assumptions of regression have been seriously 
­violated.
13.27  In Problem 13.7 on page 529, you used the plate gap on the 
bag-sealing equipment to predict the tear rating of a bag of coffee. 
Perform a residual analysis for these data (stored in  Starbucks ). 
Based on these results, evaluate whether the assumptions of  
regression have been seriously violated.
13.28  In Problem 13.6 on page 529, the owner of a moving com-
pany wanted to predict labor hours based on the cubic feet moved. 
Perform a residual analysis for these data (stored in  Moving ). 
13.6  Measuring Autocorrelation:  
The Durbin-Watson Statistic
One of the basic assumptions of the regression model is the independence of the errors. This 
assumption is sometimes violated when data are collected over sequential time periods be-
cause a residual at any one time period sometimes is similar to residuals at adjacent time peri-
ods. This pattern in the residuals is called autocorrelation. When a set of data has substantial 
autocorrelation, the validity of a regression model is in serious doubt.
Residual Plots to Detect Autocorrelation
As mentioned in Section 13.5, one way to detect autocorrelation is to plot the residuals in 
time order. If a positive autocorrelation effect exists, there will be clusters of residuals with 
the same sign, and you will readily detect an apparent pattern. If negative autocorrelation ex-
ists, residuals will tend to jump back and forth from positive to negative to positive, and so on. 
Because negative autocorrelation is very rarely seen in regression analysis, the example in this 
section illustrates positive autocorrelation.
To illustrate positive autocorrelation, consider the case of a package delivery store man-
ager who wants to be able to predict weekly sales. In approaching this problem, the manager 
has decided to develop a regression model to use the number of customers making purchases 
as an independent variable. She collects data for a period of 15 weeks and then organizes and 
stores these data in  FifteenWeeks ). Table 13.4 presents these data.
T a b l e  1 3 . 4
Customers and Sales 
for a Period of 15 
Consecutive Weeks
Week
Customers
Sales ($thousands)
1
794
9.33
2
799
8.26
3
837
7.48
4
855
9.08
5
845
9.83
6
844
10.09
7
863
11.01
8
875
11.49
Week
Customers
Sales ($thousands)
  9
880
12.07
10
905
12.55
11
886
11.92
12
843
10.27
13
904
11.80
14
950
12.15
15
841
9.64
Based on these results, evaluate whether the assumptions of re-
gression have been seriously violated.
13.29  In Problem 13.9 on page 529, an agent for a real estate 
company wanted to predict the monthly rent for one-bedroom 
apartments, based on the size of the apartments. Perform a resid-
ual analysis for these data (stored in  RentSilverSpring ). Based on 
these results, evaluate whether the assumptions of regression have 
been seriously violated.
13.30  In Problem 13.8 on page 529, you used annual revenues to 
predict the value of a baseball franchise. Perform a residual analy-
sis for these data (stored in  BBRevenue2013 ). Based on these 
results, evaluate whether the assumptions of regression have been 
seriously violated.
13.31  In Problem 13.10 on page 529, you used box office gross 
to predict DVD revenue. Perform a residual analysis for these data 
(stored in  Movie ). Based on these results, evaluate whether the 
assumptions of regression have been seriously violated.

540	
Chapter 13  Simple Linear Regression
Because the data are collected over a period of 15 consecutive weeks at the same store, 
you need to determine whether there is autocorrelation. First, you can develop the simple lin-
ear regression model you can use to predict sales based on the number of customers assuming 
there is no autocorrelation in the residuals. Figure 13.14 presents Excel and Minitab results for 
these data.
F i g u r e  1 3 . 1 4
Excel and Minitab regression results for the Table 13.4 package delivery store data
From Figure 13.14, observe that r 2 is 0.6574, indicating that 65.74% of the variation in 
sales is explained by variation in the number of customers. In addition, the Y intercept, b0, is 
-16.0322 and the slope, b1, is 0.0308. However, before using this model for prediction, you 
must perform a residual analysis. Because the data have been collected over a consecutive 
period of 15 weeks, in addition to checking the linearity, normality, and equal-variance as-
sumptions, you must investigate the independence-of-errors assumption. To do this, you plot 
the residuals versus time in Figure 13.15 in order to examine whether a pattern in the residu-
als exists. In Figure 13.15, you can see that the residuals tend to fluctuate up and down in a 
cyclical pattern. This cyclical pattern provides strong cause for concern about the existence 
of autocorrelation in the residuals and, therefore, a violation of the independence-of-errors 
assumption.
F i g u r e  1 3 . 1 5
Residual plot for the 
Table 13.4 package 
delivery store data
The Durbin-Watson Statistic
The Durbin-Watson statistic is used to measure autocorrelation. This statistic measures the  
correlation between each residual and the residual for the previous time period. Equation (13.15)  
defines the Durbin-Watson statistic.

	
13.6  Measuring Autocorrelation: The Durbin-Watson Statistic 	
541
In Equation (13.15), the numerator, a
n
i = 2
1ei - ei - 122, represents the squared difference 
between two successive residuals, summed from the second value to the nth value and the 
denominator, a
n
i = 1
e2
i , represents the sum of the squared residuals. This means that the value of 
the Durbin-Watson statistic, D, will approach 0 if successive residuals are positively autocor-
related. If the residuals are not correlated, the value of D will be close to 2. (If the residuals 
are negatively autocorrelated, D will be greater than 2 and could even approach its maximum 
value of 4.) For the package delivery store data, the Durbin-Watson statistic, D, is 0.8830. (See 
the Figure 13.16 Excel results or the Figure 13.14 Minitab results.)
Durbin-Watson Statistic
	
D =
a
n
i = 2
1ei - ei - 122
a
n
i = 1
e2
i
	
(13.15)
	
where
ei = residual at the time period i
F i g u r e  1 3 . 1 6
Excel Durbin-Watson 
statistic worksheet for the 
package delivery store 
data
Minitab reports the Durbin-
Watson statistic as part of  
the regression results (see 
Figure 13.14 on page 540).
You need to determine when the autocorrelation is large enough to conclude that there 
is significant positive autocorrelation. To do so, you compare D to the critical values of the 
Durbin-Watson statistic found in Table E.8, a portion of which is presented in Table 13.5. The 
critical values depend on a, the significance level chosen, n, the sample size, and k, the number 
of independent variables in the model (in simple linear regression, k = 1).
T a b l e  1 3 . 5
Finding Critical Values 
of the Durbin-Watson 
Statistic
A = .05
k = 1
k = 2
k = 3
k = 4
k = 5
n
dL
dU
dL
dU
dL
dU
dL
dU
dL
dU
15
1.08
1.36
.95
1.54
.82
1.75
.69
1.97
.56
2.21
16
1.10
1.37
.98
1.54
.86
1.73
.74
1.93
.62
2.15
17
1.13
1.38
1.02
1.54
.90
1.71
.78
1.90
.67
2.10
18
1.16
1.39
1.05
1.53
.93
1.69
.82
1.87
.71
2.06
In Table 13.5, two values are shown for each combination of a (level of significance), 
n (sample size), and k (number of independent variables in the model). The first value, dL, 
represents the lower critical value. If D is below dL, you conclude that there is evidence of 

542	
Chapter 13  Simple Linear Regression
positive autocorrelation among the residuals. If this occurs, the least-squares method used in 
this chapter is inappropriate, and you should use alternative methods (see reference 4). The 
second value, dU, represents the upper critical value of D, above which you would conclude 
that there is no evidence of positive autocorrelation among the residuals. If D is between dL 
and dU, you are unable to arrive at a definite conclusion.
For the package delivery store data, with one independent variable 1k = 12 and 15 values 
1n = 152, dL = 1.08 and dU = 1.36. Because D = 0.8830 6 1.08, you conclude that there 
is positive autocorrelation among the residuals. The least-squares regression analysis of the 
data shown in Figure 13.14 on page 540 is inappropriate because of the presence of significant  
positive autocorrelation among the residuals. In other words, the independence-of-errors  
assumption is invalid. You need to use alternative approaches, discussed in reference 4.
Problems for Section 13.6
Learning the Basics
13.32  The residuals for 10 consecutive time periods are as  
follows:
Applying the Concepts
13.34  In Problem 13.7 on page 529 concerning the bag sealing 
equipment at Starbucks, you used the plate gap to predict the tear 
rating.
a.	 Is it necessary to compute the Durbin-Watson statistic in this 
case? Explain.
b.	 Under what circumstances is it necessary to compute the 
Durbin-Watson statistic before proceeding with the least-
squares method of regression analysis?
13.35  What is the relationship between the price of crude  
oil and the price you pay at the pump for gasoline? The file  
 Oil & Gasoline  contains the price ($) for a barrel of crude oil 
(Cushing, Oklahoma, spot price) and a gallon of gasoline (U.S. 
average conventional spot price) for 181 weeks, ending June 14, 
2013. (Data extracted from Energy Information Administration, 
U.S. Department of Energy, www.eia.doe.gov.)
a.	 Construct a scatter plot with the price of oil on the horizontal 
axis and the price of gasoline on the vertical axis.
b.	 Use the least-squares method to develop a simple linear regres-
sion equation to predict the price of a gallon of gasoline using 
the price of a barrel of crude oil as the independent variable.
c.	 Interpret the meaning of the slope, b1, in this problem.
d.	 Plot the residuals versus the time period.
e.	 Compute the Durbin-Watson statistic.
f.	 At the 0.05 level of significance, is there evidence of positive 
autocorrelation among the residuals?
g.	 Based on the results of (d) through (f), is there reason to ques-
tion the validity of the model?
h.	 What conclusions can you reach concerning the relationship 
between the price of a barrel of crude oil and the price of a gal-
lon of gasoline?
SELF 
Test 
13.36  A mail-order catalog business that sells per-
sonal computer supplies, software, and hardware main-
tains a centralized warehouse for the distribution of products 
ordered. Management is currently examining the process of distri-
bution from the warehouse and has the business objective of deter-
mining the factors that affect warehouse distribution costs. 
Currently, a handling fee is added to the order, regardless of the 
amount of the order. Data that indicate the warehouse distribution 
a.	 Plot the residuals over time. What conclusion can you reach 
about the pattern of the residuals over time?
b.	 Based on (a), what conclusion can you reach about the autocor-
relation of the residuals?
13.33  The residuals for 15 consecutive time periods are as  
follows:
Time Period
Residual
1
-5
2
-4
3
-3
4
-2
5
-1
Time Period
Residual
6
+1
7
+2
8
+3
9
+4
10
+5
Time Period
Residual
1
+4
2
-6
3
-1
4
-5
5
+2
6
+5
7
-2
8
+7
Time Period
Residual
  9
+6
10
-3
11
+1
12
+3
13
   0
14
-4
15
-7
a.	 Plot the residuals over time. What conclusion can you reach 
about the pattern of the residuals over time?
b.	 Compute the Durbin-Watson statistic. At the 0.05 level of sig-
nificance, is there evidence of positive autocorrelation among 
the residuals?
c.	 Based on (a) and (b), what conclusion can you reach about the 
autocorrelation of the residuals?

	
13.7  Inferences About the Slope and Correlation Coefficient	
543
costs and the number of orders received have been collected over 
the past 24 months and are stored in  Warecost . The results are:
Months
Distribution Cost 
($thousands)
Number of Orders
1
52.95
4,015
2
71.66
3,806
3
85.58
5,309
4
63.69
4,262
5
72.81
4,296
6
68.44
4,097
7
52.46
3,213
8
70.77
4,809
9
82.03
5,237
10
74.39
4,732
11
70.84
4,413
12
54.08
2,921
13
62.98
3,977
14
72.30
4,428
15
58.99
3,964
16
79.38
4,582
17
94.44
5,582
18
59.74
3,450
19
90.50
5,079
20
93.24
5,735
21
69.33
4,269
22
53.71
3,708
23
89.18
5,387
24
66.80
4,161
a.	 Assuming a linear relationship, use the least-squares method to 
find the regression coefficients b0 and b1.
b.	 Predict the monthly warehouse distribution costs when the 
number of orders is 4,500.
c.	 Plot the residuals versus the time period.
d.	 Compute the Durbin-Watson statistic. At the 0.05 level of sig-
nificance, is there evidence of positive autocorrelation among 
the residuals?
e.	 Based on the results of (c) and (d), is there reason to question 
the validity of the model?
f.	 What conclusions can you reach concerning the factors that af-
fect distribution costs?
13.37  A freshly brewed shot of espresso has three distinct com-
ponents: the heart, body, and crema. The separation of these three 
components typically lasts only 10 to 20 seconds. To use the 
espresso shot in making a latte, a cappuccino, or another drink, the 
shot must be poured into the beverage during the separation of the 
heart, body, and crema. If the shot is used after the separation oc-
curs, the drink becomes excessively bitter and acidic, ruining the 
final drink. Thus, a longer separation time allows the drink-maker 
more time to pour the shot and ensure that the beverage will meet 
expectations. An employee at a coffee shop hypothesized that the 
harder the espresso grounds were tamped down into the portafil-
ter before brewing, the longer the separation time would be. An 
experiment using 24 observations was conducted to test this rela-
tionship. The independent variable Tamp measures the distance, in 
inches, between the espresso grounds and the top of the portafilter 
(i.e., the harder the tamp, the greater the distance). The dependent 
variable Time is the number of seconds the heart, body, and crema 
are separated (i.e., the amount of time after the shot is poured be-
fore it must be used for the customer’s beverage). The data are 
stored in  Espresso .
a.	 Use the least-squares method to develop a simple regression 
equation with Time as the dependent variable and Tamp as the 
independent variable.
b.	 Predict the separation time for a tamp distance of 0.50 inch.
c.	 Plot the residuals versus the time order of experimentation. Are 
there any noticeable patterns?
d.	 Compute the Durbin-Watson statistic. At the 0.05 level of sig-
nificance, is there evidence of positive autocorrelation among 
the residuals?
e.	 Based on the results of (c) and (d), is there reason to question 
the validity of the model?
f.	 What conclusions can you reach concerning the effect of tamp-
ing on the time of separation?
13.38  The owners of a chain of ice cream stores have the busi-
ness objective of improving the forecast of daily sales so that staff-
ing shortages can be minimized during the summer season. As a 
starting point, the owners decide to develop a simple linear regres-
sion model to predict daily sales based on atmospheric tempera-
ture. They select a sample of 21 consecutive days and store the 
results in  IceCream . (Hint: Determine which are the independent 
and dependent variables.)
a.	 Assuming a linear relationship, use the least-squares method to 
compute the regression coefficients b0 and b1.
b.	 Predict the sales for a day in which the temperature is 83°F.
c.	 Plot the residuals versus the time period.
d.	 Compute the Durbin-Watson statistic. At the 0.05 level of sig-
nificance, is there evidence of positive autocorrelation among 
the residuals?
e.	 Based on the results of (c) and (d), is there reason to question 
the validity of the model?
f.	 What conclusions can you reach concerning the relationship 
between sales and atmospheric temperature?
13.7  Inferences About the Slope and Correlation 
Coefficient
In Sections 13.1 through 13.3, regression was used solely for descriptive purposes. You learned 
how to determine the regression coefficients using the least-squares method and how to predict 
Y for a given value of X. In addition, you learned how to compute and interpret the standard 
error of the estimate and the coefficient of determination.

544	
Chapter 13  Simple Linear Regression
When residual analysis, as discussed in Section 13.5, indicates that the assumptions of a 
least-squares regression model are not seriously violated and that the straight-line model is ap-
propriate, you can make inferences about the linear relationship between the variables in the 
population.
t Test for the Slope
To determine the existence of a significant linear relationship between the X and Y variables, 
you test whether b1 (the population slope) is equal to 0. The null and alternative hypotheses are 
as follows:
 H0: b1 = 0 3there is no linear relationship 1the slope is zero2.4
 H1: b1 ≠0 3there is a linear relationship 1the slope is not zero2.4
If you reject the null hypothesis, you conclude that there is evidence of a linear relationship. 
Equation (13.16) defines the test statistic for the slope, which is based on the sampling distri-
bution of the slope.
Testing a Hypothesis for a Population Slope, b1, Using the t Test
The tSTAT test statistic equals the difference between the sample slope and hypothesized 
value of the population slope divided by Sb1, the standard error of the slope.
	
tSTAT = b1 - b1
Sb1
	
(13.16)
	
where
 Sb1 =
SYX
2SSX
 SSX = a
n
i = 1
1Xi - X22
The tSTAT test statistic follows a t distribution with n - 2 degrees of freedom.
Return to the Sunflowers Apparel scenario on page 519. To test whether there is a significant 
linear relationship between the size of the store and the annual sales at the 0.05 level of signifi-
cance, refer to the t test results shown in Figure 13.17.
F i g u r e  1 3 . 1 7
Excel and Minitab t test for the slope results for the Sunflowers Apparel data
From Figure 13.4 or Figure 13.17,
b1 = +2.0742 n = 14 Sb1 = 0.2536
and
 tSTAT = b1 - b1
Sb1
 = 2.0742 - 0
0.2536
= 8.178

	
13.7  Inferences About the Slope and Correlation Coefficient	
545
Using the 0.05 level of significance, the critical value of t with n - 2 = 12 degrees of free-
dom is 2.1788. Because tSTAT = 8.178 7 2.1788 or because the p-value is 0.0000, which 
is less than a = 0.05, you reject H0 (see Figure 13.18). Hence, you can conclude that there 
is a significant linear relationship between mean annual sales and the number of profiled  
customers.
–2.1788
+2.1788
0
t
Region of
Nonrejection
Region of
Rejection
Critical
Value
Critical
Value
Region of
Rejection
F i g u r e  1 3 . 1 8
Testing a hypothesis 
about the population 
slope at the 0.05 level 
of significance, with 12 
degrees of freedom
F Test for the Slope
As an alternative to the t test, in simple linear regression, you can use an F test to determine 
whether the slope is statistically significant. In Section 10.4, you used the F distribution to test 
the ratio of two variances. Equation (13.17) defines the F test for the slope as the ratio of the 
variance that is due to the regression 1MSR2 divided by the error variance 1MSE = S2
YX2.
Testing a Hypothesis for a Population Slope, b1, Using the F Test
The FSTAT test statistic is equal to the regression mean square 1MSR2 divided by the mean 
square error 1MSE2.
	
FSTAT = MSR
MSE	
(13.17)
	
where
 MSR = SSR
1
= SSR
 MSE =
SSE
n - 2
The FSTAT test statistic follows an F distribution with 1 and n - 2 degrees of freedom.
Using a level of significance a, the decision rule is
reject H0 if FSTAT 7 Fa;
otherwise, do not reject H0.

546	
Chapter 13  Simple Linear Regression
Table 13.6 organizes the complete set of results into an analysis of variance (ANOVA)  
table.
T a b l e  1 3 . 6
ANOVA Table 
for Testing the 
Significance of a 
Regression Coefficient
Source
df
Sum of Squares
Mean Square (variance)
F
Regression
1
SSR
MSR = SSR
1
= SSR
FSTAT = MSR
MSE
Error
n - 2
SSE
MSE =
SSE
n - 2
Total
n - 1
SST
Figure 13.19, a completed ANOVA table for the Sunflowers sales data (extracted from  
Figure 13.4), shows that the computed FSTAT test statistic is 66.8792 and the p-value  
is 0.0000.
F i g u r e  1 3 . 1 9
Excel and Minitab F test results for the Sunflowers Apparel data
In simple linear regression, 
t2 = F.
Using a level of significance of 0.05, from Table E.5, the critical value of the F distribution, 
with 1 and 12 degrees of freedom, is 4.75 (see Figure 13.20). Because FSTAT = 66.8792 7 4.75 
or because the p-value = 0.0000 6 0.05, you reject H0 and conclude that there is a significant 
linear relationship between the number of profiled customers and annual sales. Because the  
F test in Equation (13.17) on page 545 is equivalent to the t test in Equation (13.16) on page 544, 
you reach the same conclusion.
0
F
4.75
Region of
Rejection
Critical
Value
Region of
Nonrejection
F i g u r e  1 3 . 2 0
Regions of rejection 
and nonrejection 
when testing for the 
significance of the slope 
at the 0.05 level of 
significance, with 1 and 
12 degrees of freedom

	
13.7  Inferences About the Slope and Correlation Coefficient	
547
From the Figure 13.17 results on page 544,
b1 = 2.0742 n = 14 Sb1 = 0.2536
To construct a 95% confidence interval estimate, a>2 = 0.025, and from Table E.3, 
ta>2 = 2.1788. Thus,
 b1 { ta>2Sb1 = 2.0742 { 12.1788210.25362
 = 2.0742 { 0.5526
 1.5216 …  b1 …  2.6268
Therefore, you have 95% confidence that the estimated population slope is between 1.5216 
and 2.6268. The confidence interval indicates that for each increase of one million profiled 
customers, predicted annual sales are estimated to increase by at least $1,521,600 but no more 
than $2,626,800. Because both of these values are above 0, you have evidence of a significant 
linear relationship between annual sales and the number of profiled customers. Had the inter-
val included 0, you would have concluded that there is no evidence of a significant relationship 
between the variables.
t Test for the Correlation Coefficient
In Section 3.5 on page 161, the strength of the relationship between two numerical variables 
was measured using the correlation coefficient, r. The values of the coefficient of correlation 
range from -1 for a perfect negative correlation to +1 for a perfect positive correlation. You 
can use the correlation coefficient to determine whether there is a statistically significant linear 
relationship between X and Y. To do so, you hypothesize that the population correlation coef-
ficient, r, is 0. Thus, the null and alternative hypotheses are
 H0: r = 0  1no correlation2
 H1: r ≠0  1correlation2
Equation (13.19) defines the test statistic for determining the existence of a significant  
correlation.
Confidence Interval Estimate for the Slope
As an alternative to testing for the existence of a linear relationship between the variables, you 
can construct a confidence interval estimate of b1 using Equation (13.18).
Confidence Interval Estimate of the Slope, b1
The confidence interval estimate for the population slope can be constructed by taking the 
sample slope, b1, and adding and subtracting the critical t value multiplied by the standard 
error of the slope.
b1 { ta>2Sb1
	
b1 - ta>2Sb1 … b1 … b1 + ta>2Sb1	
(13.18)
where
ta>2 = critical value corresponding to an upper-tail probability of a>2 from the t 
distribution with n - 2 degrees of freedom (i.e., a cumulative area of 1 - a>2)

548	
Chapter 13  Simple Linear Regression
In the Sunflowers Apparel problem, r 2 = 0.8479 and b1 = +2.0742 (see Figure 13.4 on  
page 523). Because b1 7 0, the correlation coefficient for annual sales and store size is the 
positive square root of r 2—that is, r = + 20.8479 = +0.9208. You use Equation (13.19a) to 
test the null hypothesis that there is no correlation between these two variables. This results in 
the following tSTAT statistic:
 tSTAT =
r - 0
B
1 - r 2
n - 2
 =
0.9208 - 0
B
1 - 10.920822
14 - 2
= 8.178
Using the 0.05 level of significance, because tSTAT = 8.178 7 2.1788, you reject the null 
hypothesis. You conclude that there is a significant association between annual sales and the 
number of profiled customers. This tSTAT test statistic is equivalent to the tSTAT test statistic 
found when testing whether the population slope, b1, is equal to zero.
Testing for the Existence of Correlation
	
tSTAT =
r - r
A
1 - r 2
n - 2
	
(13.19a)
	
where
 r = + 2r 2 if b1 7 0
 r = - 2r 2 if b1 6 0
The tSTAT test statistic follows a t distribution with n - 2 degrees of freedom. r is 
calculated as in Equation (3.17) on page 162:
	
r = cov1X, Y2
SXSY
	
(13.19b)
	
where
 cov1X, Y2 =
a
n
i = 1
1Xi - X21Yi - Y2
n - 1
 SX = H
a
n
i = 1
1Xi - X22
n - 1
 SY = H
a
n
i = 1
1Yi - Y22
n - 1
Problems for Section 13.7
Learning the Basics
13.39  You are testing the null hypothesis that there is no linear 
relationship between two variables, X and Y. From your sample of 
n = 10, you determine that r = 0.80.
a.	 What is the value of the t test statistic tSTAT?
b.	 At the a = 0.05 level of significance, what are the critical  
values?
c.	 Based on your answers to (a) and (b), what statistical decision 
should you make?
13.40  You are testing the null hypothesis that there is no linear 
relationship between two variables, X and Y. From your sample of 
n = 18, you determine that b1 = +4.5 and Sb1 = 1.5.
a.	 What is the value of tSTAT?
b.	 At the a = 0.05 level of significance, what are the critical values?

	
13.7  Inferences About the Slope and Correlation Coefficient	
549
c.	 Based on your answers to (a) and (b), what statistical decision 
should you make?
d.	 Construct a 95% confidence interval estimate of the population 
slope, b1.
13.41  You are testing the null hypothesis that there is no linear 
relationship between two variables, X and Y. From your sample of 
n = 20, you determine that SSR = 60 and SSE = 40.
a.	 What is the value of FSTAT?
b.	 At the a = 0.05 level of significance, what is the critical value?
c.	 Based on your answers to (a) and (b), what statistical decision 
should you make?
d.	 Compute the correlation coefficient by first computing r 2 and 
assuming that b1 is negative.
e.	 At the 0.05 level of significance, is there a significant correla-
tion between X and Y?
Applying the Concepts
SELF 
Test 
13.42  In Problem 13.4 on page 529, you used the per-
centage of alcohol to predict wine quality. The data are 
stored in  VinhoVerde . From the results of that problem,  
b1 = 0.5624 and Sb1 = 0.1127.
a.	 At the 0.05 level of significance, is there evidence of a linear re-
lationship between the percentage of alcohol and wine quality?
b.	 Construct a 95% confidence interval estimate of the population 
slope, b1.
13.43  In Problem 13.5 on page 528, you used the summated rat-
ing of a restaurant to predict the cost of a meal. The data are stored 
in  Restaurants . Using the results of that problem, b1 = 1.4963 
and Sb1 = 0.1379.
a.	 At the 0.05 level of significance, is there evidence of a linear 
relationship between the summated rating of a restaurant and 
the cost of a meal?
b.	 Construct a 95% confidence interval estimate of the population 
slope, b1.
13.44  In Problem 13.6 on page 529, the owner of a moving com-
pany wanted to predict labor hours, based on the number of cubic 
feet moved. The data are stored in  Moving . Use the results of that 
problem.
a.	 At the 0.05 level of significance, is there evidence of a linear 
relationship between the number of cubic feet moved and labor 
hours?
b.	 Construct a 95% confidence interval estimate of the population 
slope, b1.
13.45  In Problem 13.7 on page 529, you used the plate gap in the 
bag-sealing equipment to predict the tear rating of a bag of coffee. 
The data are stored in  Starbucks . Use the results of that problem.
a.	 At the 0.05 level of significance, is there evidence of a linear 
relationship between the plate gap of the bag-sealing machine 
and the tear rating of a bag of coffee?
b.	 Construct a 95% confidence interval estimate of the population 
slope, b1.
13.46  In Problem 13.8 on page 529, you used annual revenues 
to predict the value of a baseball franchise. The data are stored in 
 BBRevenue2013 . Use the results of that problem.
a.	 At the 0.05 level of significance, is there evidence of a linear 
relationship between annual revenue and franchise value?
b.	 Construct a 95% confidence interval estimate of the population 
slope, b1.
13.47  In Problem 13.9 on page 529, an agent for a real estate 
company wanted to predict the monthly rent for one-bedroom 
apartments, based on the size of the apartment. The data are stored 
in  RentSilverSpring . Use the results of that problem.
a.	 At the 0.05 level of significance, is there evidence of a linear 
relationship between the size of the apartment and the monthly 
rent?
b.	 Construct a 95% confidence interval estimate of the population 
slope, b1.
13.48  In Problem 13.10 on page 529, you used box office gross 
to predict DVD revenue. The data are stored in  Movie . Use the 
results of that problem.
a.	 At the 0.05 level of significance, is there evidence of a linear 
relationship between box office gross and DVD revenue?
b.	 Construct a 95% confidence interval estimate of the population 
slope, b1.
13.49  The volatility of a stock is often measured by its beta value. 
You can estimate the beta value of a stock by developing a simple 
linear regression model, using the percentage weekly change in 
the stock as the dependent variable and the percentage weekly 
change in a market index as the independent variable. The S&P 
500 Index is a common index to use. For example, if you wanted 
to estimate the beta value for Disney, you could use the following 
model, which is sometimes referred to as a market model:
1% weekly change in Disney2 = b0
+ b11% weekly change in S & p 500 index2 + e
The least-squares regression estimate of the slope b1 is the esti-
mate of the beta value for Disney. A stock with a beta value of 
1.0 tends to move the same as the overall market. A stock with a 
beta value of 1.5 tends to move 50% more than the overall market, 
and a stock with a beta value of 0.6 tends to move only 60% as 
much as the overall market. Stocks with negative beta values tend 
to move in the opposite direction of the overall market. The fol-
lowing table gives some beta values for some widely held stocks 
as of August 12, 2012:
Company
Ticker Symbol
Beta
Procter & Gamble
PG
0.32
Dr. Pepper Snapple Group
DPS
-0.02
Disney
DIS
1.07
Apple
AAPL
0.69
eBay
EBAY
0.79
Marriott
MAR
1.32
Source: Data extracted from finance.yahoo.com, June 26, 2013.
a.	 For each of the six companies, interpret the beta value.
b.	 How can investors use the beta value as a guide for investing?
13.50  Index funds are mutual funds that try to mimic the move-
ment of leading indexes, such as the S&P 500 or the Russell 2000. 
The beta values (as described in Problem 13.49) for these funds 
are therefore approximately 1.0, and the estimated market models 
for these funds are approximately
1% weekly change in index fund2 = 0.0 + 1.0
1% weekly change in the index2

550	
Chapter 13  Simple Linear Regression
Leveraged index funds are designed to magnify the movement of 
major indexes. Direxion Funds is a leading provider of leveraged 
index and other alternative-class mutual fund products for invest-
ment advisors and sophisticated investors. Two of the company’s 
funds are shown in the following table:
Name
Ticker 
Symbol
Description
Daily Small Cap  
  Bull 3x Fund
TNA
300% of the Russell  
  2000 Index
Monthly S & P  
  Bear 2x Fund
DSSSX
200% of the S&P 500  
  Index
Source: Data extracted from www.direxionfunds.com.
The estimated market models for these funds are approximately
1% weekly change in tNA2 = 0.0 + 3.0 
1% weekly change in the russell 20002
1% weekly change in DSSSX2 = 0.0 + 2.0
1% weekly change in the S & p 500 Index2
Thus, if the Russell 2000 Index gains 10% over a period of time, 
the leveraged mutual fund TNA gains approximately 30%. On the 
downside, if the same index loses 20%, TNA loses approximately 
60%.
a.	 The objective of the Direxion Funds Midcap Bull 3x fund, 
MDU, is 300% of the performance of the S & P Midcap 400 
Index. What is its approximate market model?
b.	 If the S & P Midcap 400 Index gains 10% in a year, what return 
do you expect MDU to have?
c.	 If the S & P Midcap 400 Index loses 20% in a year, what return 
do you expect MDU to have?
d.	 What type of investors should be attracted to leveraged index 
funds? What type of investors should stay away from these 
funds?
13.51  The file  Cereals  contains the calories and sugar, in grams, 
in one serving of seven breakfast cereals:
Cereal
Calories
Sugar
Kellogg’s All Bran
  80
  6
Kellogg’s Corn Flakes
100
  2
Wheaties
100
  4
Nature’s Path Organic Multigrain Flakes
110
  4
Kellogg’s Rice Krispies
130
  4
Post Shredded Wheat Vanilla Almond
190
11
Kellogg’s Mini Wheats
200
10
a.	 Compute and interpret the coefficient of correlation, r.
b.	 At the 0.05 level of significance, is there a significant linear 
relationship between calories and sugar?
13.52  Movie companies need to predict the gross receipts of an 
individual movie once the movie has debuted. The following re-
sults (stored in  PotterMovies ) are the first weekend gross, the 
U.S. gross, and the worldwide gross (in $millions) of the eight 
Harry Potter movies that debuted from 2001 to 2011:
Title
First 
Weekend
U.S. 
Gross
Worldwide 
Gross
Sorcerer’s Stone
90.295
317.558
976.458
Chamber of Secrets
88.357
261.988
878.988
Prisoner of Azkaban
93.687
249.539
795.539
Goblet of Fire
102.335
290.013
896.013
Order of the Phoenix
77.108
292.005
938.469
Half-Blood Prince
77.836
301.460
934.601
Deathly Hallows:  
  Part I
125.017
295.001
955.417
Deathly Hallows:  
  Part II
169.189
381.001
1,328.11
Source: Data extracted from www.the-numbers.com/interactive/
comp-Harry-Potter.php.
a.	 Compute the coefficient of correlation between first weekend 
gross and U.S. gross, first weekend gross and worldwide gross, 
and U.S. gross and worldwide gross.
b.	 At the 0.05 level of significance, is there a significant linear 
relationship between first weekend gross and U.S. gross, first 
weekend gross and worldwide gross, and U.S. gross and world-
wide gross?
13.53  College football is big business, with coaches’ salaries, 
revenues, and expenses in millions of dollars. The file  College 
Football  contains the coaches’ pay and revenue for college foot-
ball at 105 of the 124 schools that are part of the Division I Foot-
ball Bowl Subdivision. (Data extracted from “College Football 
Coaches Continue to See Salary Explosion,” USA Today, Novem-
ber 20, 2012, p. 8C.)
a.	 Compute and interpret the coefficient of correlation, r.
b.	 At the 0.05 level of significance, is there a significant linear 
relationship between a coach’s pay and revenue?
13.54  A survey by the Pew Research Center found that social 
networking is popular in many nations around the world. The file 
 GlobalSocialMedia  contains the level of social media networking 
(measured as the percent of individuals polled who use social net-
working sites) and the GDP per capita based on purchasing power 
parity (PPP) for each of 25 selected countries. (Data extracted 
from “Global Digital Communication: Texting, Social Networking 
Popular Worldwide,” The Pew Research Center, updated February 
29, 2012, p. 5.)
a.	 Compute and interpret the coefficient of correlation, r.
b.	 At the 0.05 level of significance, is there a significant linear 
relationship between GDP and social media usage?
c.	 What conclusions can you reach about the relationship between 
GDP and social media usage?

	
13.8  Estimation of Mean Values and Prediction of Individual Values 	
551
The width of the confidence interval in Equation (13.20) depends on several factors. In-
creased variation around the prediction line, as measured by the standard error of the estimate, 
results in a wider interval. As you would expect, increased sample size reduces the width of the 
interval. In addition, the width of the interval varies at different values of X. When you predict 
Y for values of X close to X, the interval is narrower than for predictions for X values farther 
away from X.
In the Sunflowers Apparel example, suppose you want to construct a 95% confidence in-
terval estimate of the mean annual sales for the entire population of stores that have four mil-
lion profiled customers 1X = 42. Using the simple linear regression equation,
 Yni =  -1.2088 +  2.0742Xi
 = -1.2088 +  2.0742142 = 7.0879 1millions of dollars2
13.8  Estimation of Mean Values and Prediction  
of Individual Values
In Chapter 8, you studied the concept of the confidence interval estimate of the population 
mean. In Example 13.2 on page 525, you used the prediction line to predict the mean value 
of Y for a given X. The mean annual sales for stores that had four million profiled customers 
within a fixed radius was predicted to be 7.0879 millions of dollars ($7,087,900). This esti-
mate, however, is a point estimate of the population mean. This section presents methods to 
develop a confidence interval estimate for the mean response for a given X and for developing 
a prediction interval for an individual response, Y, for a given value of X.
The Confidence Interval Estimate for the Mean Response
Equation (13.20) defines the confidence interval estimate for the mean response for a  
given X.
Confidence Interval Estimate for the Mean of Y
Yni { ta>2SYX2hi
	
Yni - ta>2SYX2hi … mYX = Xi … Yni + ta>2SYX2hi	
(13.20)
	
where
 hi = 1
n + 1Xi - X 22
SSX
 Yni = predicted value of Y; Yni = b0 + b1Xi
 SYX = standard error of the estimate
 n = sample size
 Xi = given value of X
 mYX = Xi = mean value of Y when X = Xi
 SSX = a
n
i = 1
1Xi - X22
ta>2 = critical value corresponding to an upper-tail probability of a>2 from the  
t distribution with n - 2 degrees of freedom (i.e., a cumulative area of 1 - a>2)

552	
Chapter 13  Simple Linear Regression
Also, given the following:
 X = 3.7786  SYX = 0.9993
 SSX = a
n
i = 1
1Xi - X22 = 15.5236
From Table E.3, ta>2 = 2.1788. Thus,
Yni { ta>2SYX2hi
where
hi = 1
n + 1Xi - X 22
SSX
so that
Yni { ta>2SYXB
1
n + 1Xi - X22
SSX
 = 7.0879 { 12.1788210.99932B
1
14 + 14 - 3.778622
15.5236
 = 7.0879 { 0.5946
so
6.4932 … mY X = 4 … 7.6825
Therefore, the 95% confidence interval estimate is that the population mean annual sales are 
between $6,493,200 and $7,682,500 for stores with four million profiled customers.
The Prediction Interval for an Individual Response
In addition to constructing a confidence interval for the mean value of Y, you can also con-
struct a prediction interval for an individual value of Y. Although the form of this interval is 
similar to that of the confidence interval estimate of Equation (13.20), the prediction interval is 
predicting an individual value, not estimating a mean. Equation (13.21) defines the prediction 
interval for an individual response, Y, at a given value, Xi, denoted by YX = Xi.
Prediction Interval for an Individual Response, Y
	
Yni { ta>2SYX21 + hi	
(13.21)
Yni - ta>2SYX21 + hi … YX = Xi … Yni + ta>2SYX21 + hi
where
YX = Xi = future value of Y when X = Xi
ta>2 = critical value corresponding to an upper-tail probability of a>2 from the  
t distribution with n - 2 degrees of freedom (i.e., a cumulative area of 1 - a>2)
In addition, hi, Yni, SYX, n, and Xi are defined as in Equation (13.20) on page 551.
To construct a 95% prediction interval of the annual sales for an individual store that has 
four million profiled customers 1X = 42, you first compute Yni. Using the prediction line:
 Yni = -1.2088 + 2.0742Xi
 = -1.2088 + 2.0742142
 = 7.0879 1millions of dollars2

	
13.8  Estimation of Mean Values and Prediction of Individual Values 	
553
Also, given the following:
 X = 3.7786  SYX = 0.9993
 SSX = a
n
i = 1
1Xi - X22 = 15.5236
From Table E.3, ta>2 = 2.1788. Thus,
Yni { ta>2SYX21 + hi
where
hi = 1
n +
1Xi - X 22
a
n
i = 1
1Xi - X 22
so that
 Yni { ta>2SYXB1 + 1
n + 1Xi - X 22
SSX
 
 = 7.0879 { 12.1788210.99932B1 + 1
14 + 14 - 3.778622
15.5236
 = 7.0879 { 2.2570
so
4.8308 … YX = 4 … 9.3449
Therefore, with 95% confidence, you predict that the annual sales for an individual store with 
four million profiled customers is between $4,830,800 and $9,344,900.
Figure 13.21 presents Excel and Minitab results for the confidence interval estimate and the 
prediction interval for the Sunflowers Apparel data. If you compare the results of the confidence 
interval estimate and the prediction interval, you see that the width of the prediction interval for an 
individual store is much wider than the confidence interval estimate for the mean. Remember that 
there is much more variation in predicting an individual value than in estimating a mean value.
F i g u r e  1 3 . 2 1
Excel and Minitab confidence interval estimate and prediction interval worksheets for the Sunflowers Apparel data

554	
Chapter 13  Simple Linear Regression
Problems for Section 13.8
Learning the Basics
13.55  Based on a sample of n = 20, the least-squares method 
was used to develop the following prediction line: Yni = 5 + 3Xi. 
In addition,
SYX = 1.0 X = 2  a
n
i = 1
1Xi - X 22 = 20
a.	 Construct a 95% confidence interval estimate of the population 
mean response for X = 2.
b.	 Construct a 95% prediction interval of an individual response 
for X = 2.
13.56  Based on a sample of n = 20, the least-squares method 
was used to develop the following prediction line: Yni = 5 + 3Xi. 
In addition,
SYX = 1.0  X = 2 a
n
i = 1
1Xi - X22 = 20
a.	 Construct a 95% confidence interval estimate of the population 
mean response for X = 4.
b.	 Construct a 95% prediction interval of an individual response 
for X = 4.
c.	 Compare the results of (a) and (b) with those of Problem 13.55 
(a) and (b). Which intervals are wider? Why?
Applying the Concepts
13.57  In Problem 13.5 on page 528, you used the summated rat-
ing of a restaurant to predict the cost of a meal. The data are stored 
in  Restaurants . For these data, SYX = 9.094 and hi = 0.046319 
when X = 50.
a.	 Construct a 95% confidence interval estimate of the mean cost 
of a meal for restaurants that have a summated rating of 50.
b.	 Construct a 95% prediction interval of the cost of a meal for an 
individual restaurant that has a summated rating of 50.
c.	 Explain the difference in the results in (a) and (b).
SELF 
Test 
13.58  In Problem 13.4 on page 528, you used the per-
centage of alcohol to predict wine quality. The data are 
stored in  VinhoVerde . For these data, SYX = 0.9369 and 
hi = 0.024934 when X = 10.
a.	 Construct a 95% confidence interval estimate of the mean wine 
quality rating for all wines that have 10% alcohol.
b.	 Construct a 95% prediction interval of the wine quality rating 
of an individual wine that has 10% alcohol.
c.	 Explain the difference in the results in (a) and (b).
13.59  In Problem 13.7 on page 529, you used the plate gap on 
the bag-sealing equipment to predict the tear rating of a bag of cof-
fee. The data are stored in  Starbucks .
a.	 Construct a 95% confidence interval estimate of the mean tear 
rating for all bags of coffee when the plate gap is 0.
b.	 Construct a 95% prediction interval of the tear rating for an in-
dividual bag of coffee when the plate gap is 0.
c.	 Why is the interval in (a) narrower than the interval in (b)?
13.60  In Problem 13.6 on page 529, the owner of a moving com-
pany wanted to predict labor hours based on the number of cubic 
feet moved. The data are stored in  Moving .
a.	 Construct a 95% confidence interval estimate of the mean labor 
hours for all moves of 500 cubic feet.
b.	 Construct a 95% prediction interval of the labor hours of an 
individual move that has 500 cubic feet.
c.	 Why is the interval in (a) narrower than the interval in (b)?
13.61  In Problem 13.9 on page 529, an agent for a real estate 
company wanted to predict the monthly rent for one-bedroom 
apartments, based on the size of an apartment. The data are stored 
in  RentSilverSpring .
a.	 Construct a 95% confidence interval estimate of the mean 
monthly rental for all one-bedroom apartments that are 800 
square feet in size.
b.	 Construct a 95% prediction interval of the monthly rental for 
an individual one-bedroom apartment that is 800 square feet in 
size.
c.	 Explain the difference in the results in (a) and (b).
13.62  In Problem 13.8 on page 529, you predicted the value of a 
baseball franchise, based on current revenue. The data are stored 
in  BBRevenue2013 .
a.	 Construct a 95% confidence interval estimate of the mean value 
of all baseball franchises that generate $250 million of annual 
revenue.
b.	 Construct a 95% prediction interval of the value of an individual 
baseball franchise that generates $250 million of annual revenue.
c.	 Explain the difference in the results in (a) and (b).
13.63  In Problem 13.10 on page 529, you used box office gross 
to predict DVD revenue. The data are stored in  Movie . The com-
pany is about to release a movie on DVD that had a box office 
gross of $100 million.
a.	 What is the predicted DVD revenue?
b.	 Which interval is more useful here, the confidence interval es-
timate of the mean or the prediction interval for an individual 
response? Explain.
c.	 Construct and interpret the interval you selected in (b).

	
13.9  Potential Pitfalls in Regression	
555
Anscombe (reference 1) showed that all four data sets given in Table 13.7 have the following  
identical results:
 Yni =  3.0 + 0.5Xi
 SYX =  1.237
 Sb1 =  0.118
 r 2 =  0.667
T a b l e  1 3 . 7
Four Sets of Artificial 
Data
Data Set A
Data Set B
Data Set C
Data Set D
Xi
Yi
Xi
Yi
Xi
Yi
Xi
Yi
10
8.04
10
9.14
10
7.46
8
6.58
14
9.96
14
8.10
14
8.84
8
5.76
5
5.68
5
4.74
5
5.73
8
7.71
8
6.95
8
8.14
8
6.77
8
8.84
9
8.81
9
8.77
9
7.11
8
8.47
12
10.84
12
9.13
12
8.15
8
7.04
4
4.26
4
3.10
4
5.39
8
5.25
7
4.82
7
7.26
7
6.42
19
12.50
11
8.33
11
9.26
11
7.81
8
5.56
13
7.58
13
8.74
13
12.74
8
7.91
6
7.24
6
6.13
6
6.08
8
6.89
Source: Data extracted from F. J. Anscombe, “Graphs in Statistical Analysis,” The American 
Statistician, 27 (1973), 17–21.
13.9  Potential Pitfalls in Regression
When using regression analysis, some of the potential pitfalls are:
 • Lacking awareness of the assumptions of least-squares regression
 • Not knowing how to evaluate the assumptions of least-squares regression
 • Not knowing what the alternatives are to least-squares regression if a particular assump-
tion is violated
 • Using a regression model without knowledge of the subject matter
 • Extrapolating outside the relevant range
 • Concluding that a significant relationship identified in an observational study is due to a 
cause-and-effect relationship
The widespread availability of spreadsheet and statistical applications has made regression 
analysis much more feasible today than it once was. However, many users who have access to 
such applications do not understand how to use regression analysis properly. Someone who 
is not familiar with either the assumptions of regression or how to evaluate the assumptions 
cannot be expected to know what the alternatives to least-squares regression are if a particular 
assumption is violated.
The data in Table 13.7 (stored in  Anscombe ) illustrate the importance of using scatter plots 
and residual analysis to go beyond the basic number crunching of computing the Y intercept, 
the slope, and r 2.

556	
Chapter 13  Simple Linear Regression
 SSR = explained variation = a
n
i = 1
1Yni - Y22 = 27.51
 SSE = Unexplained variation = a
n
i = 1
1Yi - Yni22 = 13.76
 SST = total variation = a
n
i = 1
1Yi - Y22 = 41.27
If you stopped the analysis at this point, you would fail to observe the important differences 
among the four data sets that scatter plots and residual plots can reveal.
From the scatter plots and the residual plots of Figure 13.22, you see how different the 
data sets are. Each has a different relationship between X and Y. The only data set that seems 
to approximately follow a straight line is data set A. The residual plot for data set A does not 
show any obvious patterns or outlying residuals. This is certainly not true for data sets B, C, 
and D. The scatter plot for data set B shows that a curvilinear regression model is more ap-
propriate. This conclusion is reinforced by the residual plot for data set B. The scatter plot and 
the residual plot for data set C clearly show an outlying observation. In this case, one approach 
used is to remove the outlier and reestimate the regression model (see Section 14.8). The scat-
ter plot for data set D represents a situation in which the model is heavily dependent on the 
outcome of a single data point (X8 = 19 and Y8 = 12.50). Any regression model with this 
characteristic should be used with caution. 
5
10
Y
10
Data Set B
15
5
10
Y
5
10
Data Set A
15
20
5
20
5
10
5
10
Data Set C
15
20
5
10
Y
5
10
15
20
5
10
Y
5
10
Data Set D
15
20
X
X
X
X
–2
–1
0
+1
+2
5
20
X
15
10
Residual
Data Set A
–2
–1
0
+1
+4
5
20
X
15
10
+2
+3
Residual
Data Set C
–2
–1
0
+1
+4
5
20
X
15
10
+2
+3
Residual
Data Set D
–2
–1
0
+1
+2
5
20
X
15
10
Residual
Data Set B
F i g u r e  1 3 . 2 2
Scatter plots and residual plots for the data sets A, B, C, and D
Scatter plots
Residual plots

	
Summary	
557
S u m m a r y
As you can see from the chapter roadmap in Figure 13.23, 
this chapter develops the simple linear regression model 
and discusses the assumptions and how to evaluate them. 
Once you are assured that the model is appropriate, you can 
predict values by using the prediction line and test for the 
significance of the slope. In Chapters 14 and 15, regression 
analysis is extended to situations in which more than one 
independent variable is used to predict the value of a depen-
dent variable.
Constructing scatter plots and residual plots are important tasks in 
the following six-step strategy to avoid the potential pitfalls in re-
gression. Consider using this strategy every time you undertake a 
regression analysis.
Step 1  Construct a scatter plot to observe the possible relation-
ship between X and Y.
Step 2  Perform a residual analysis to check the assumptions of 
regression (linearity, independence, normality, equal variance):
a.  Plot the residuals versus the independent variable to determine 
whether the linear model is appropriate and to check for equal 
variance.
b.  Construct a histogram, stem-and-leaf display, boxplot, or nor-
mal probability plot of the residuals to check for normality.
c.  Plot the residuals versus time to check for independence. (This 
step is necessary only if the data are collected over time.)
Step 3  If there are violations of the assumptions, use alternative 
methods to least-squares regression or alternative least-squares 
models (see reference 4).
Step 4  If there are no violations of the assumptions, carry out tests 
for the significance of the regression coefficients and develop confi-
dence and prediction intervals.
Step 5  Refrain from making predictions and forecasts outside the 
relevant range of the independent variable.
Step 6  Remember that the relationships identified in observa-
tional studies may or may not be due to cause-and-effect relation-
ships. And while causation implies correlation, correlation does not 
imply causation.
Six Steps for Avoiding the Potential Pitfalls
I
n the Knowing Customers at Sunflowers Apparel scenario, 
you were the director of planning for a chain of upscale 
clothing stores for women. Until now, Sunflowers managers 
selected sites based on factors such as the availability of a 
good lease or a subjective opinion that a location seemed like 
a good place for a store. To make more objective decisions, 
you used the more systematic DCOVA approach to identify 
and classify groups of consumers and developed a regres-
sion model to analyze the relationship between the number 
of profiled customers that live within a fixed radius of a  
Sunflowers store and the annual sales of the store. The 
model indicated 
that about 84.8% 
of the variation in 
sales was explained by the number of profiled customers that 
live within a fixed radius of a Sunflowers store. Furthermore, 
for each increase of one million profiled customers, mean 
annual sales were estimated to increase by $2.0742 million. 
You can now use your model to help make better decisions 
when selecting new sites for stores as well as to forecast 
sales for existing stores.
U s i n g  S tat i s t i c s
Knowing Customers at Sunflowers Apparel, 
Revisited
Fotolia 

558	
Chapter 13  Simple Linear Regression
Referen c e s
	 1.	Anscombe, F. J. “Graphs in Statistical Analysis.” The  
American Statistician, 27(1973): 17–21.
	 2.	Hoaglin, D. C., and R. Welsch. “The Hat Matrix in Regression 
and ANOVA.” The American Statistician, 32(1978): 17–22.
	 3.	Hocking, R. R. “Developments in Linear Regression Method-
ology: 1959–1982.” Technometrics, 25(1983): 219–250.
	 4.	 Kutner, M. H., C. J. Nachtsheim, J. Neter, and W. Li. Applied 
Linear Statistical Models, 5th ed. New York: McGraw-Hill/Irwin, 
2005.
	 5.	Microsoft Excel 2013. Redmond, WA: Microsoft Corp., 
2012.
	 6.	Minitab Release 16. State College, PA: Minitab Inc., 2010.
	 7.	White, H. “A Heteroscedasticity-Consistent Covariance 
Matrix Estimator and a Direct Test for Heteroscedasticity.” 
Econometrica, 48(1980): 817–838.
F i g u r e  1 3 . 2 3
Roadmap for simple linear regression
Yes
Yes
Yes
Yes
No
Regression
Correlation
No
No
No
Use Alternative to
Least-Squares Regression
Estimate
β1
Use Model for
Prediction and Estimation
Model
Signiﬁcant
?
Testing H0:
β1 = 0
(See Assumptions)
Model
Appropriate
?
Residual Analysis
Is
Autocorrelation
Present
?
Compute
Durbin-Watson
Statistic
Plot Residuals
over Time
Data
Collected
in Sequential
Order
?
Primary
Focus
Prediction Line
Scatter Plot
Least-Squares
Regression Analysis
Simple Linear Regression
and Correlation
Coefﬁcient
of Correlation, r
Testing H0:
ρ = 0
 Estimate
µYlX=Xi
 Predict
YX=Xi

	
Key Equations	
559
K ey Eq u at i o n s
Simple Linear Regression Model
Yi = b0 + b1Xi + ei	
(13.1)
Simple Linear Regression Equation:  
The Prediction Line
Yni = b0 + b1Xi	
(13.2)
Computational Formula for the Slope, b1
b1 = SSXY
SSX 	
(13.3)
Computational Formula for the Y Intercept, b0
b0 = Y - b1X	
(13.4)
Measures of Variation in Regression
SST = SSR + SSE	
(13.5)
Total Sum of Squares (SST)
SST = total sum of squares = a
n
i = 1
1Yi - Y22	
(13.6)
Regression Sum of Squares (SSR)
SSR = explained variation or regression sum of squares
	
= a
n
i =  1
1Yni - Y22	
(13.7)
Error Sum of Squares (SSE)
SSE = Unexplained variation or error sum of squares
	
= a
n
i = 1
1Yi - Yni22	
(13.8)
Coefficient of Determination
r 2 = regression sum of squares
total sum of squares
= SSR
SST	
(13.9)
Computational Formula for SST
SST = a
n
i = 1
1Yi - Y22 = a
n
i = 1
Y 2
i -
a a
n
i = 1
Yib
2
n
	
(13.10)
Computational Formula for SSR
SSR = a
n
i = 1
1Yni - Y22
	
= b0a
n
i = 1
Yi + b1a
n
i = 1
XiYi -
a a
n
i = 1
Yib
2
n
	
(13.11)
Computational Formula for SSE
SSE = a
n
i = 1
1Yi - Yni22 = a
n
i = 1
Y 2
i - b0a
n
i = 1
Yi - b1a
n
i = 1
XiYi
	
(13.12)
Standard Error of the Estimate
SYX = A
SSE
n - 2 = H
a
n
i = 1
1Yi - Yni22
n - 2
	
(13.13)
Residual
ei = Yi - Yni	
(13.14)
Durbin-Watson Statistic
D =
a
n
i = 2
1ei - ei - 122
a
n
i = 1
e2
i
	
(13.15)
Testing a Hypothesis for a Population Slope, B1, Using the 
t Test
tSTAT = b1 - b1
Sb1
	
(13.16)
Testing a Hypothesis for a Population Slope, B1, Using the 
F Test
FSTAT = MSR
MSE	
(13.17)
Confidence Interval Estimate of the Slope, B1
b1 { ta>2Sb1
b1 - ta>2Sb1 … b1 … b1 + ta>2Sb1	
(13.18)
Testing for the Existence of Correlation
tSTAT =
r - r
A
1 - r 2
n - 2
	
(13.19a)
r = cov1X, Y2
SXSY
	
(13.19b)
Confidence Interval Estimate for the Mean of Y
Yni {  ta>2SYX2hi
Yni -  ta>2SYX2hi … mYX = Xi … Yni + ta>2SYX2hi	 (13.20)
Prediction Interval for an Individual Response, Y
Yni {  ta>2SYX21 + hi
	Yni -  ta>2SYX21 + hi … YX = Xi … Yni + ta>2SYX21 + hi 
	
(13.21)

560	
Chapter 13  Simple Linear Regression
K e y  Term s
assumptions of regression  535
autocorrelation  539
coefficient of determination  532
confidence interval estimate for the mean 
response  551
correlation coefficient  547
dependent variable  520
Durbin-Watson statistic  540
equal variance  535
error sum of squares (SSE)  530
explained variation  530
explanatory variable  521
homoscedasticity  535
independence of errors  535
independent variable  520
least-squares method  523
linearity  535
linear relationship  521
model  520
normality  535
prediction interval for an individual  
response, Y  552
prediction line  522
regression analysis  520
regression coefficient  523
regression sum of squares (SSR)  530
relevant range  525
residual  535
residual analysis  535
response variable  521
simple linear regression  520
simple linear regression equation  522
slope  522
standard error of the estimate  533
total sum of squares (SST)  530
total variation  530
unexplained variation  530
Y intercept  522
C hec ki n g  Yo u r  U n d e r s ta nding
13.64  What do you understand by the term standard error of  
estimate?
13.65  Define the terms explained variation and unexplained varia-
tion. How are they interrelated? Illustrate.
13.66  Explain how simple linear regression equations help in 
making business decisions.
13.67  Explain the meaning of ‘regression sum of squares’ and 
‘error sum of squares’.
13.68  What does r2 determine about the statistical significance of 
a relationship between a dependent and an independent variable?
13.69  Why and how should you carry out a residual analysis as 
part of a regression model?
13.70  Explain homoscedasticity. Can you use linear regression if 
this assumption is violated?
13.71  Explain autocorrelation. How is it measured?
13.72  What is the difference between a confidence interval estimate 
of the mean response, mYX = Xi, and a prediction interval of YX = Xi?
C ha pter  R e vi e w P r o b le ms
13.73  Can you use Twitter activity to forecast box office receipts 
on the opening weekend? The following data (stored in  Twitter-
Movies ) indicate the Twitter activity (“want to see”) and the re-
ceipts ($) per theater on the weekend a movie opened for seven 
movies:
Movie
Twitter 
Activity
Receipts ($)
The Devil Inside
219,509
14,763
The Dictator
6,405
5,796
Paranormal Activity 3
165,128
15,829
The Hunger Games
579,288
36,871
Bridesmaids
6,564
8,995
Red Tails
11,104
7,477
Act of Valor
9,152
8,054
Source: R. Dodes, “Twitter Goes to the Movies,” The Wall Street 
Journal, August 3, 2012, pp. D1–D12.
a.	 Use the least-squares method to compute the regression coef-
ficients b0 and b1.
b.	 Interpret the meaning of b0 and b1 in this problem.
c.	 Predict the mean receipts for a movie that has a Twitter activity 
of 100,000.
d.	 Should you use the model to predict the receipts for a movie 
that has a Twitter activity of 1,000,000? Why or why not?
e.	 Determine the coefficient of determination, r 2, and explain its 
meaning in this problem.
f.	 Perform a residual analysis. Is there any evidence of a pattern 
in the residuals? Explain.
g.	 At the 0.05 level of significance, is there evidence of a linear 
relationship between Twitter activity and receipts?
h.	 Construct a 95% confidence interval estimate of the mean re-
ceipts for a movie that has a Twitter activity of 100,000 and a 
95% prediction interval of the receipts for a single movie that 
has a Twitter activity of 100,000.
i.	 Based on the results of (a)–(h), do you think that Twitter activ-
ity is a useful predictor of receipts on the first weekend a movie 
opens? What issues about these data might make you hesitant 
to use Twitter activity to predict receipts?
13.74  Management of a soft-drink bottling company has the 
business objective of developing a method for allocating delivery 
costs to customers. Although one cost clearly relates to travel time 
within a particular route, another variable cost reflects the time re-
quired to unload the cases of soft drink at the delivery point. To be-
gin, management decided to develop a regression model to predict 
delivery time based on the number of cases delivered. A sample 

	
Chapter Review Problems	
561
 
 
Customer
 
Number  
of Cases
Delivery  
Time  
(minutes)
1
  52
32.1
2
  64
34.8
3
  73
36.2
4
  85
37.8
5
  95
37.8
6
103
39.7
7
116
38.5
8
121
41.9
9
143
44.2
10
157
47.1
 
 
Customer
 
Number  
of Cases
Delivery  
Time  
(minutes)
11
161
43.0
12
184
49.4
13
202
57.2
14
218
56.8
15
243
60.6
16
254
61.2
17
267
58.2
18
275
63.1
19
287
65.6
20
298
67.3
a.	 Use the least-squares method to compute the regression coef-
ficients b0 and b1.
b.	 Interpret the meaning of b0 and b1 in this problem.
c.	 Predict the mean delivery time for 150 cases of soft drink.
d.	 Should you use the model to predict the delivery time for a cus-
tomer who is receiving 500 cases of soft drink? Why or why not?
e.	 Determine the coefficient of determination, r 2, and explain its 
meaning in this problem.
f.	 Perform a residual analysis. Is there any evidence of a pattern 
in the residuals? Explain.
g.	 At the 0.05 level of significance, is there evidence of a linear 
relationship between delivery time and the number of cases 
­delivered?
h.	 Construct a 95% confidence interval estimate of the mean 
­delivery time for 150 cases of soft drink and a 95% prediction 
interval of the delivery time for a single delivery of 150 cases 
of soft drink.
i.	 What conclusions can you reach from (a)–(h) about the rela-
tionship between the number of cases and delivery time?
13.75  Measuring the height of a California redwood tree is very 
difficult because these trees grow to heights of over 300 feet. Peo-
ple familiar with these trees understand that the height of a Cali-
fornia redwood tree is related to other characteristics of the tree, 
including the diameter of the tree at the breast height of a person. 
The data in  Redwood  represent the height (in feet) and diameter 
(in inches) at the breast height of a person for a sample of 21 Cali-
fornia redwood trees.
a.	 Assuming a linear relationship, use the least-squares method 
to compute the regression coefficients b0 and b1. State the re-
gression equation that predicts the height of a tree based on the 
tree’s diameter at breast height of a person.
b.	 Interpret the meaning of the slope in this equation.
c.	 Predict the mean height for a tree that has a breast height diam-
eter of 25 inches.
d.	 Interpret the meaning of the coefficient of determination in this 
problem.
e.	 Perform a residual analysis on the results and determine the ad-
equacy of the model.
f.	 Determine whether there is a significant relationship between 
the height of redwood trees and the breast height diameter at 
the 0.05 level of significance.
g.	 Construct a 95% confidence interval estimate of the population 
slope between the height of the redwood trees and breast height 
diameter.
h.	 What conclusions can you reach about the relationship of the 
diameter of the tree and its height?
13.76  You want to develop a model to predict the assessed value 
of homes based on their size. A sample of 30 single-family houses 
listed for sale in Silver Spring, Maryland, a suburb of Washington, 
DC, is selected to study the relationship between assessed value 
(in $thousands) and size (in thousands of square feet), and the 
data is collected and stored in  SilverSpring . (Hint: First determine 
which are the independent and dependent variables.)
a.	 Construct a scatter plot and, assuming a linear relationship, use 
the least-squares method to compute the regression coefficients 
b0 and b1.
b.	 Interpret the meaning of the Y intercept, b0, and the slope, b1, 
in this problem.
c.	 Use the prediction line developed in (a) to predict the mean as-
sessed value for a house whose size is 2,000 square feet.
d.	 Determine the coefficient of determination, r 2, and interpret its 
meaning in this problem.
e.	 Perform a residual analysis on your results and evaluate the re-
gression assumptions.
f.	 At the 0.05 level of significance, is there evidence of a linear 
relationship between assessed value and size?
g.	 Construct a 95% confidence interval estimate of the population 
slope.
h.	 What conclusions can you reach about the relationship between 
the size of the house and its assessed value?
13.77  You want to develop a model to predict the taxes of houses, 
based on assessed value. A sample of 30 single-family houses 
listed for sale in Silver Spring, Maryland, a suburb of Washing-
ton, DC, is selected. The taxes (in $) and the assessed value of the 
houses (in $thousands) are recorded and stored in  SilverSpring . 
(Hint: First determine which are the independent and dependent 
variables.)
a.	 Construct a scatter plot and, assuming a linear relationship, use 
the least-squares method to compute the regression coefficients 
b0 and b1.
b.	 Interpret the meaning of the Y intercept, b0, and the slope, b1, 
in this problem.
c.	 Use the prediction line developed in (a) to predict the mean 
taxes for a house whose assessed value is $400,000.
d.	 Determine the coefficient of determination, r 2, and interpret its 
meaning in this problem.
e.	 Perform a residual analysis on your results and evaluate the re-
gression assumptions.
f.	 At the 0.05 level of significance, is there evidence of a linear 
relationship between taxes and assessed value?
g.	 What conclusions can you reach concerning the relationship 
between taxes and assessed value?
13.78  The director of graduate studies at a large college of busi-
ness has the objective of predicting the grade point average (GPA) 
of students in an MBA program. The director begins by using the 
of 20 deliveries within a territory was selected. The delivery times 
and the number of cases delivered were organized in the following 
table and stored in  Delivery :

562	
Chapter 13  Simple Linear Regression
Graduate Management Admission Test (GMAT) score. A sample 
of 20 students who have completed two years in the program is 
selected and stored in  GPIGMAT .
a.	 Construct a scatter plot and, assuming a linear relationship, use 
the least-squares method to compute the regression coefficients 
b0 and b1.
b.	 Interpret the meaning of the Y intercept, b0, and the slope, b1, 
in this problem.
c.	 Use the prediction line developed in (a) to predict the mean 
GPA for a student with a GMAT score of 600.
d.	 Determine the coefficient of determination, r 2, and interpret its 
meaning in this problem.
e.	 Perform a residual analysis on your results and evaluate the re-
gression assumptions.
f.	 At the 0.05 level of significance, is there evidence of a linear 
relationship between GMAT score and GPA?
g.	 Construct a 95% confidence interval estimate of the mean GPA 
of students with a GMAT score of 600 and a 95% prediction 
interval of the GPA for a particular student with a GMAT score 
of 600.
h.	 Construct a 95% confidence interval estimate of the population 
slope.
i.	 What conclusions can you reach concerning the relationship 
between GMAT score and GPA?
13.79  An accountant for a large department store has the busi-
ness objective of developing a model to predict the amount of time 
it takes to process invoices. Data are collected from the past 32 
working days, and the number of invoices processed and comple-
tion time (in hours) are stored in  Invoice . (Hint: First determine 
which are the independent and dependent variables.)
a.	 Assuming a linear relationship, use the least-squares method to 
compute the regression coefficients b0 and b1.
b.	 Interpret the meaning of the Y intercept, b0, and the slope, b1, in 
this problem.
c.	 Use the prediction line developed in (a) to predict the mean 
amount of time it would take to process 150 invoices.
d.	 Determine the coefficient of determination, r 2, and interpret its 
meaning.
e.	 Plot the residuals against the number of invoices processed and 
also against time.
f.	 Based on the plots in (e), does the model seem appropriate?
g.	 Based on the results in (e) and (f), what conclusions can you 
reach about the validity of the prediction made in (c)?
h.	 What conclusions can you reach about the relationship between 
the number of invoices and the completion time?
13.80  On January 28, 1986, the space shuttle Challenger exploded, 
and seven astronauts were killed. Prior to the launch, the predicted 
atmospheric temperature was for freezing weather at the launch site. 
Engineers for Morton Thiokol (the manufacturer of the rocket mo-
tor) prepared charts to make the case that the launch should not take 
place due to the cold weather. These arguments were rejected, and the 
launch tragically took place. Upon investigation after the tragedy, ex-
perts agreed that the disaster occurred because of leaky rubber O-rings 
that did not seal properly due to the cold temperature. Data indicating 
the atmospheric temperature at the time of 23 previous launches and 
the O-ring damage index are stored in  O-Ring .
Note: Data from flight 4 is omitted due to unknown O-ring condition.
Sources: Data extracted from Report of the Presidential Commission 
on the Space Shuttle Challenger Accident, Washington, DC, 1986, 
Vol. II (H1–H3) and Vol. IV (664); and Post-Challenger Evaluation 
of Space Shuttle Risk Assessment and Management, Washington, DC, 
1988, pp. 135–136.
a.	 Construct a scatter plot for the seven flights in which there was 
O-ring damage (O-ring damage index ≠0). What conclu-
sions, if any, can you reach about the relationship between at-
mospheric temperature and O-ring damage?
b.	 Construct a scatter plot for all 23 flights.
c.	 Explain any differences in the interpretation of the relationship 
between atmospheric temperature and O-ring damage in (a) 
and (b).
d.	 Based on the scatter plot in (b), provide reasons why a pre-
diction should not be made for an atmospheric temperature 
of 31°F, the temperature on the morning of the launch of the 
Challenger.
e.	 Although the assumption of a linear relationship may not be 
valid for the set of 23 flights, fit a simple linear regression 
model to predict O-ring damage, based on atmospheric tem-
perature.
f.	 Include the prediction line found in (e) on the scatter plot de-
veloped in (b).
g.	 Based on the results in (f), do you think a linear model is ap-
propriate for these data? Explain.
h.	 Perform a residual analysis. What conclusions do you reach?
13.81  A baseball analyst would like to study various team sta-
tistics for the 2012 baseball season to determine which variables 
might be useful in predicting the number of wins achieved by 
teams during the season. He begins by using a team’s earned run 
average (ERA), a measure of pitching performance, to predict the 
number of wins. He collects the team ERA and team wins for each 
of the 30 Major League Baseball teams and stores these data in 
 BB2012 . (Hint: First determine which are the independent and 
dependent variables.)
a.	 Assuming a linear relationship, use the least-squares method to 
compute the regression coefficients b0 and b1.
b.	 Interpret the meaning of the Y intercept, b0, and the slope, b1, in 
this problem.
c.	 Use the prediction line developed in (a) to predict the mean 
number of wins for a team with an ERA of 4.50.
d.	 Compute the coefficient of determination, r 2, and interpret its 
meaning.
e.	 Perform a residual analysis on your results and determine the 
adequacy of the fit of the model.
f.	 At the 0.05 level of significance, is there evidence of a linear 
relationship between the number of wins and the ERA?
g.	 Construct a 95% confidence interval estimate of the mean num-
ber of wins expected for teams with an ERA of 4.50.
h.	 Construct a 95% prediction interval of the number of wins for 
an individual team that has an ERA of 4.50.
i.	 Construct a 95% confidence interval estimate of the population 
slope.
j.	 The 30 teams constitute a population. In order to use statistical 
inference, as in (f) through (i), the data must be assumed to rep-
resent a random sample. What “population” would this sample 
be drawing conclusions about?
k.	 What other independent variables might you consider for inclu-
sion in the model?
l.	 What conclusions can you reach concerning the relationship 
between ERA and wins?

	
Chapter Review Problems	
563
13.82  Can you use the annual revenues generated by National 
Basketball Association (NBA) franchises to predict franchise val-
ues? Figure 2.14 on page 94 shows a scatter plot of revenue with 
franchise value, and Figure 3.9 on page 164, shows the correlation 
coefficient. Now, you want to develop a simple linear regression 
model to predict franchise values based on revenues. (Franchise 
values and revenues are stored in  NBAValues .)
a.	 Assuming a linear relationship, use the least-squares method to 
compute the regression coefficients b0 and b1.
b.	 Interpret the meaning of the Y intercept, b0, and the slope, b1, in 
this problem.
c.	 Predict the mean value of an NBA franchise that generates 
$150 million of annual revenue.
d.	 Compute the coefficient of determination, r 2, and interpret its 
meaning.
e.	 Perform a residual analysis on your results and evaluate the re-
gression assumptions.
f.	 At the 0.05 level of significance, is there evidence of a linear 
relationship between the annual revenues generated and the 
value of an NBA franchise?
g.	 Construct a 95% confidence interval estimate of the mean value 
of all NBA franchises that generate $150 million of annual rev-
enue.
h.	 Construct a 95% prediction interval of the value of an individ-
ual NBA franchise that generates $150 million of annual rev-
enue.
i.	 Compare the results of (a) through (h) to those of baseball fran-
chises in Problems 13.8, 13.20, 13.30, 13.46, and 13.62 and 
European soccer teams in Problem 13.83.
13.83  In Problem 13.82 you used annual revenue to develop 
a model to predict the franchise value of National Basketball 
Association (NBA) teams. Can you also use the annual rev-
enues generated by European soccer teams to predict franchise  
values? (European soccer team values and revenues are stored in 
 SoccerValues2013 .)
a.	 Repeat Problem 13.82 (a) through (h) for the European soccer 
teams.
b.	 Compare the results of (a) to those of baseball franchises in 
Problems 13.8, 13.20, 13.30, 13.46, and 13.62 and NBA fran-
chises in Problem 13.82.
13.84  During the fall harvest season in the United States, pump-
kins are sold in large quantities at farm stands. Often, instead of 
weighing the pumpkins prior to sale, the farm stand operator will 
just place the pumpkin in the appropriate circular cutout on the 
counter. When asked why this was done, one farmer replied, “I can 
tell the weight of the pumpkin from its circumference.” To deter-
mine whether this was really true, the circumference and weight of 
each pumpkin from a sample of 23 pumpkins were determined and 
the results stored in  Pumpkin .
a.	 Assuming a linear relationship, use the least-squares method to 
compute the regression coefficients b0 and b1.
b.	 Interpret the meaning of the slope, b1, in this problem.
c.	 Predict the mean weight for a pumpkin that is 60 centimeters in 
circumference.
d.	 Do you think it is a good idea for the farmer to sell pumpkins 
by circumference instead of weight? Explain.
e.	 Determine the coefficient of determination, r 2, and interpret its 
meaning.
f.	 Perform a residual analysis for these data and evaluate the re-
gression assumptions.
g.	 At the 0.05 level of significance, is there evidence of a lin-
ear relationship between the circumference and weight of a  
pumpkin?
h.	 Construct a 95% confidence interval estimate of the population 
slope, b1.
13.85  Refer to the discussion of beta values and market mod-
els in Problem 13.49 on page 549. The S&P 500 Index tracks the 
overall movement of the stock market by considering the stock 
prices of 500 large corporations. The file  StockPrices2012  con-
tains 2012 weekly data for the S&P 500 and three companies. The 
following variables are included:
WEEK—Week ending on date given
S&P—Weekly closing value for the S&P 500 Index
GE—Weekly closing stock price for General Electric
DISCA—Weekly closing stock price for Discovery ­ 
  Communications
GOOG—Weekly closing stock price for Google
Source: Data extracted from finance.yahoo.com, June 26, 2013.
a.	 Estimate the market model for GE. (Hint: Use the percentage 
change in the S&P 500 Index as the independent variable and 
the percentage change in GE’s stock price as the dependent 
variable.)
b.	 Interpret the beta value for GE.
c.	 Repeat (a) and (b) for Discovery Communications.
d.	 Repeat (a) and (b) for Google.
e.	 Write a brief summary of your findings.
13.86  The file  CEO-Compensation  includes the total compensa-
tion (in $millions) for CEOs of 170 large public companies and 
the investment return in 2012. (Data extracted from “CEO Pay 
Rockets as Economy, Stocks Recover,” USA Today, March 27, 
2013, p. 1B.)
a.	 Compute the correlation coefficient between compensation and 
the investment return in 2012.
b.	 At the 0.05 level of significance, is the correlation between 
compensation and the investment return in 2012 statistically 
significant?
c.	 Write a short summary of your findings in (a) and (b). Do the 
results surprise you?
Report Writing Exercise
13.87  In Problems 13.8, 13.20, 13.30, 13.46, 13.62, 13.82, and 
13.83, you developed regression models to predict franchise value 
of major league baseball, NBA basketball, and soccer teams. Now, 
write a report based on the models you developed. Append to your 
report all appropriate charts and statistical information.

564	
Chapter 13  Simple Linear Regression
C a s e s  f o r  C h a p t e r  1 3
Managing Ashland MultiComm Services
To ensure that as many trial subscriptions to the 3-For-All 
service as possible are converted to regular subscriptions, 
the marketing department works closely with the customer 
support department to accomplish a smooth initial process 
for the trial subscription customers. To assist in this effort, 
the marketing department needs to accurately forecast the 
monthly total of new regular subscriptions.
A team consisting of managers from the marketing and 
customer support departments was convened to develop a 
better method of forecasting new subscriptions. Previously, 
after examining new subscription data for the prior three 
months, a group of three managers would develop a sub-
jective forecast of the number of new subscriptions. Livia 
Salvador, who was recently hired by the company to provide 
expertise in quantitative forecasting methods, suggested that 
the department look for factors that might help in predicting 
new subscriptions.
Members of the team found that the forecasts in the 
past year had been particularly inaccurate because in some 
months, much more time was spent on telemarketing than in 
other months. Livia collected data (stored in  AMS13 ) for the 
number of new subscriptions and hours spent on telemarket-
ing for each month for the past two years.
1.	What criticism can you make concerning the method of 
forecasting that involved taking the new subscriptions 
data for the prior three months as the basis for future pro-
jections?
2.	What factors other than number of telemarketing hours 
spent might be useful in predicting the number of new 
subscriptions? Explain.
3.	a.  Analyze the data and develop a regression model to 
predict the number of new subscriptions for a month, 
based on the number of hours spent on telemarketing 
for new subscriptions.
b.	If you expect to spend 1,200 hours on telemarketing 
per month, estimate the number of new subscriptions 
for the month. Indicate the assumptions on which this 
prediction is based. Do you think these assumptions 
are valid? Explain.
c.	 What would be the danger of predicting the number of 
new subscriptions for a month in which 2,000 hours 
were spent on telemarketing?
Digital Case
Apply your knowledge of simple linear regression in this 
Digital Case, which extends the Sunflowers Apparel Using 
Statistics scenario from this chapter.
Leasing agents from the Triangle Mall Management  
Corporation have suggested that Sunflowers consider sev-
eral locations in some of Triangle’s newly renovated life-
style malls that cater to shoppers with higher-than-mean 
disposable income. Although the locations are smaller than 
the typical Sunflowers location, the leasing agents argue 
that higher-than-mean disposable income in the surrounding 
community is a better predictor than profiled customers of 
higher sales. The leasing agents maintain that sample data 
from 14 Sunflowers stores prove that this is true.
Open Triangle_Sunflower.pdf and review the leasing 
agents’ proposal and supporting documents. Then answer 
the following questions:
1.	Should mean disposable income be used to predict sales 
based on the sample of 14 Sunflowers stores?
2.	Should the management of Sunflowers accept the claims 
of Triangle’s leasing agents? Why or why not?
3.	Is it possible that the mean disposable income of the sur-
rounding area is not an important factor in leasing new 
locations? Explain.
4.	 Are there any other factors not mentioned by the leasing 
agents that might be relevant to the store leasing decision?
Brynne Packaging
Brynne Packaging is a large packaging company, offering 
its customers the highest standards in innovative packaging 
solutions and reliable service. About 25% of the employ-
ees at Brynne Packaging are machine operators. The hu-
man resources department has suggested that the company 
consider using the Wesman Personnel Classification Test 
(WPCT), a measure of reasoning ability, to screen appli-
cants for the machine operator job. In order to assess the 
WPCT as a predictor of future job performance, 25 recent 
applicants were tested using the WPCT; all were hired,  

	
Cases for Chapter 13	
565
regardless of their WPCT score. At a later time, supervi-
sors were asked to rate the quality of the job performance of 
these 25 employees, using a 1-to-10 rating scale (where 1 = 
very low and 10 = very high). Factors considered in the rat-
ings included the employee’s output, defect rate, ability to 
implement continuous quality procedures, and contributions 
to team problem solving efforts. The file  BrynnePackaging  
contains the WPCT scores (WPCT) and job performance 
ratings (Ratings) for the 25 employees.
1.	Assess the significance and importance of WPCT score 
as a predictor of job performance. Defend your answer.
2.	 Predict the mean job performance rating for all employees 
with a WPCT score of 6. Give a point prediction as well 
as a 95% confidence interval. Do you have any concerns 
using the regression model for predicting mean job perfor-
mance rating given the WPCT score of 6?
3.	Evaluate whether the assumptions of regression have 
been seriously violated.

566	
Chapter 13  Simple Linear Regression
EG13.1  Types of Regression Models
There are no Excel Guide instructions for this section.
EG13.2  Determining the Simple Linear 
Regression Equation
Key Technique  Use the LINEST(cell range of Y variable, cell 
range of X variable, True, True) array function to compute the 
b1 and b0 coefficients, the b1 and b0 standard errors, r 2 and the 
standard error of the estimate, the F test statistic and error df, and 
SSR and SSE.
Example  Perform the Figure 13.4 analysis of the Sunflowers 
Apparel data on page 523.
PHStat  Use Simple Linear Regression.
For the example, open to the DATA worksheet of the SiteSelec-
tion workbook. Select PHStat ➔ Regression ➔ Simple Linear 
Regression. In the procedure’s dialog box (shown below):
	 1.	 Enter C1:C15 as the Y Variable Cell Range.
	 2.	 Enter B1:B15 as the X Variable Cell Range.
	 3.	 Check First cells in both ranges contain label.
	 4.	 Enter 95 as the Confidence level for regression coefficients.
	 5.	 Check Regression Statistics Table and ANOVA and Coef-
ficients Table.
	 6.	 Enter a Title and click OK.
The procedure creates a worksheet that contains a copy of 
your data as well as the worksheet shown in Figure 13.4. For more 
information about these worksheets, read the following In-Depth 
Excel section.
To create a scatter plot that contains a prediction line and re-
gression equation similar to Figure 13.5 on page 524, modify step 
6 by checking Scatter Plot before clicking OK.
In-Depth Excel  Use the COMPUTE worksheet of the Simple 
Linear Regression workbook as a template. (Use the Simple 
Linear Regression 2007 workbook if you use an Excel version 
that is older than Excel 2010.) For the example, the worksheet 
uses the regression data already in the SLRDATA worksheet to 
perform the regression analysis.
Figure 13.4 does not show the Calculations area in columns 
K through M. This area contains an array formula in the cell range 
L2:M6 that contains the expression LINEST(cell range of Y vari-
able, cell range of X variable, True, True) to compute the b1 and 
b0 coefficients in cells L2 and M2, the b1 and b0 standard errors in 
cells L3 and M3, r 2 and the standard error of the estimate in cells 
L4 and M4, the F test statistic and error df in cells L5 and M5, 
and SSR and SSE in cells L6 and M6. In cell L9, the expression 
T.INV.2T(1 – confidence level, Error degrees of freedom) com-
putes the critical value for the t test. Open the COMPUTE_FOR-
MULAS worksheet to examine all the formulas in the worksheet, 
some of which are discussed in later sections in this Excel Guide.
To perform simple linear regression for other data, paste the 
regression data into the SLRDATA worksheet. Paste the values 
for the X variable into column A and the values for the Y variable 
into column B. Then, open to the COMPUTE worksheet. Enter the 
confidence level in cell L8 and edit the array formula in the cell 
range L2:M6. To edit the array formula, first select L2:M6, next 
make changes to the array formula, and then, while holding down 
the Control and Shift keys (or the Command key on a Mac), 
press the Enter key.
To create a scatter plot that contains a prediction line and re-
gression equation similar to Figure 13.5 on page 524, first use the 
Section EG2.5 In-Depth Excel scatter plot instructions with the 
Table 13.1 Sunflowers Apparel data to create a scatter plot. Then 
select the chart and:
	 1.	 Select Design ➔ Add Chart Element ➔ Trendline ➔ More 
Trendline Options.
In the Format Trendline pane (shown on page 567):
	 2.	 Click Linear, check both Display Equation on chart and 
Display R-squared value on chart.
C h a p t e r  1 3  E x c e l  G u i d e

	
Chapter 13 Excel Guide	
567
If you use an Excel version that is older than Excel 2010, 
­after selecting the chart:
	 1.	 Select Layout ➔ Trendline ➔ More Trendline Options.
In the Format Trendline dialog box (similar to the Format Trend-
line pane):
	 2.	 Click Trendline Options in the left pane. In the Trendline Op-
tions right pane, click Linear, check Display Equation on 
chart, check Display R-squared value on chart, and then 
click Close.
For scatter plots of other data, if the X axis does not appear 
at the bottom of the plot, right-click the Y axis and click Format 
Axis from the shortcut menu. In the Format Axis dialog box, click 
Axis Options in the left pane. In the Axis Options pane on the 
right, click Axis value and in its box enter the value shown in the 
dimmed Minimum box at the top of the pane. Then click Close.
Analysis ToolPak  Use Regression.
For the example, open to the DATA worksheet of the SiteSelection  
workbook and:
	 1.	 Select Data ➔ Data Analysis.
	 2.	 In the Data Analysis dialog box, select Regression from the 
Analysis Tools list and then click OK.
In the Regression dialog box (shown in next column):
	 3.	 Enter C1:C15 as the Input Y Range and enter B1:B15 as the 
­Input X Range.
	 4.	 Check Labels and check Confidence Level and enter 95 in its 
box.
	 5.	 Click New Worksheet Ply and then click OK.
EG13.3  Measures of Variation
The measures of variation are computed as part of creating the 
simple linear regression worksheet using the Section EG13.2 in-
structions.
If you use either Section EG13.2 PHStat or In-Depth Excel 
instructions, formulas used to compute these measures are in the 
COMPUTE worksheet that is created. Formulas in cells B5, B7, 
B13, C12, C13, D12, and E12 copy values computed by the array 
formula in cell range L2:M6. In cell F12, the F.DIST.RT(F test 
statistic, regression degrees of freedom, error degrees of free-
dom), function computes the p-value for the F test for the slope, 
discussed in Section 13.7. (The similar FDIST function is used in 
the COMPUTE worksheet of the Simple Linear Regression 2007 
workbook.)
EG13.4  Assumptions of Regression
There are no Excel Guide instructions for this section.
EG13.5  Residual Analysis
Key Technique  Use arithmetic formulas to compute the residu-
als. To evaluate assumptions, use the Section EG2.5 scatter plot 
instructions for constructing residual plots and the Section EG6.3 
instructions for constructing normal probability plots.
Example  Compute the residuals for the Table 13.1 Sunflowers 
Apparel data on page 522.
PHStat  Use the Section EG13.2 PHStat instructions. Modify 
step 5 by checking Residuals Table and Residual Plot in addi-
tion to checking Regression Statistics Table and ANOVA and 
Coefficients Table. To construct a normal probability plot, follow 
the Section EG6.3 PHStat instructions using the cell range of the 
­residuals as the Variable Cell Range in step 1.
In-Depth Excel  Use the RESIDUALS worksheet of the Sim-
ple Linear Regression workbook as a template.
This worksheet already computes the residuals for the example. 
Column C formulas compute the predicted Y values (labeled 
Predicted Annual Sales in Figure 13.10 on page 536) by first 
multiplying the X values by the b1 coefficient in cell B18 of the 
COMPUTE worksheet and then adding the b0 coefficient (in 

568	
Chapter 13  Simple Linear Regression
cell B17 of COMPUTE). Column E formulas compute residuals 
by subtracting the predicted Y values from the Y values (labeled  
Annual Sales in Figure 13.10).
For other problems, modify this worksheet by pasting the X 
values into column B and the Y values into column D. Then, for 
sample sizes smaller than 14, delete the extra rows. For sample 
sizes greater than 14, copy the column C and E formulas down 
through the row containing the last pair and X and Y values and 
add the new observation numbers in column A.
To construct a residual plot similar to Figure 13.11 on  
page 537, use the original X variable and the residuals (plotted 
as the Y variable) as the chart data and follow the Section EG2.5 
scatter plot instructions. To construct a normal probability plot,  
follow the Section EG6.3 In-Depth Excel instructions, using the 
cell range of the residuals as the Variable Cell Range.
Analysis ToolPak  Use the Section EG13.2 Analysis ToolPak 
instructions.
Modify step 5 by checking Residuals and Residual Plots before 
clicking New Worksheet Ply and then OK. To construct a resid-
ual plot or normal probability plot, use the In-Depth Excel instruc-
tions.
EG13.6  Measuring Autocorrelation: 
the Durbin-Watson Statistic
Key Technique  Use the SUMXMY2(cell range of the sec-
ond through last residual, cell range of the first through the 
second-to-last residual) function to compute the sum of squared 
difference of the residuals, the numerator in Equation (13.15) on  
page 541, and use the SUMSQ(cell range of the residuals) func-
tion to compute the sum of squared residuals, the denominator in 
Equation (13.15).
Example  Compute the Durbin-Watson statistic for the Sunflow-
ers Apparel data on page 522.
PHStat  Use the PHStat instructions at the beginning of Section 
EG13.2. Modify step 6 by checking the Durbin-Watson Statistic 
output option before clicking OK.
In-Depth Excel  Use the DURBIN_WATSON worksheet 
of the Simple Linear Regression workbook as a template. The 
worksheet uses the SUMXMY2 function in cell B3 and the 
SUMSQ function in cell B4.
The DURBIN_WATSON worksheet of the Package Deliv-
ery workbook computes the statistic for the Figure 13.16 pack-
age delivery store example on page 541. (This workbook also uses 
the COMPUTE and RESIDUALS worksheet templates from the 
Simple Linear Regression workbook.)
To compute the Durbin-Watson statistic for other problems, 
first create the simple linear regression model and the residuals 
for the problem, using the Sections EG13.2 and EG13.5 In-Depth 
Excel instructions. Then open the DURBIN_WATSON worksheet 
and edit the formulas in cell B3 and B4 to point to the proper cell 
ranges of the new residuals.
EG13.7  Inferences About the Slope 
and Correlation Coefficient
The t test for the slope and F test for the slope are included in the 
worksheet created by using the Section EG13.2 instructions. The 
t test computations in the worksheets created by using the PHStat 
and In-Depth Excel instructions are discussed in Section EG13.2. 
The F test computations are discussed in Section EG13.3.
EG13.8  Estimation of Mean Values 
and Prediction of Individual 
Values
Key Technique  Use the TREND(Y variable cell range, X 
variable cell range, X value) function to compute the predicted Y 
value for the X value and use the DEVSQ(X variable cell range) 
function to compute the SSX value.
Example  Compute the Figure 13.21 confidence interval esti-
mate and prediction interval for the Sunflowers Apparel data that 
is shown on page 522.
PHStat  Use the Section EG13.2 PHStat instructions but replace 
step 6 with these steps 6 and 7:
	 1.	 Check Confidence Int. Est. & Prediction Int. for X=and 
enter 4 in its box. Enter 95 as the percentage for Confidence 
level for intervals.
	 2.	 Enter a Title and click OK.
The additional worksheet created is discussed in the following In-
Depth Excel instructions.
In-Depth Excel  Use the CIEandPI worksheet of the Simple 
Linear Regression workbook, as a template.
The worksheet already contains the data and formulas for 
the example. The worksheet uses the T.INV.2T (1 – confidence 
level, degrees of freedom) function to compute the t critical 
value in cell B10 and the TREND function to compute the pre-
dicted Y value for the X value in cell B15. In cell B12, the function 
DEVSQ(SLRData!A:A) computes the SSX value that is used, in 
turn, to help compute the h statistic in cell B14.
To compute a confidence interval estimate and prediction in-
terval for other problems:
	 1.	 Paste the regression data into the SLRData worksheet. Use 
column A for the X variable data and column B for the Y vari-
able data.
	 2.	 Open to the CIEandPI worksheet.
In the CIEandPI worksheet:
	 3.	 Change values for the X Value and Confidence Level, as is 
necessary.
	 4.	 Edit the cell ranges used in the cell B15 formula that uses the 
TREND function to refer to the new cell ranges for the Y and 
X variables.

	
Chapter 13 Minitab Guide	
569
MG13.1  Types of Regression Models
There are no Minitab Guide instructions for this section.
MG13.2  Determining the Simple 
Linear Regression Equation
Use Regression to perform a simple linear regression analysis. 
For example, to perform the Figure 13.4 analysis of the Sunflowers 
Apparel data on page 523, open to the SiteSelection worksheet. 
Select Stat ➔ Regression ➔ Regression. In the Regression dialog 
box (shown below):
	  1.	 Double-click C3  Annual Sales in the variables list to add 
'Annual Sales' to the Response box.
	  2.	 Double-click C2  Profiled Customers in the variables list to 
add 'Profiled Customers' to the Predictors box.
	  3.	 Click Graphs.
In the Regression - Graphs dialog box (shown below):
	  4.	 Click Regular (in Residuals for Plots) and Individual Plots 
(in Residual Plots).
	  5.	 Check Histogram of residuals, Normal plot of residuals, 
Residuals versus fits, and Residuals versus order and then 
press Tab.
	  6.	 Double-click C2  Profiled Customers in the variables list to 
add 'Profiled Customers' in the Residuals versus the vari-
ables box.
	  7.	 Click OK.
	  8.	 Back in the Regression dialog box, click Results.
In the Regression - Results dialog box (not shown):
	  9.	 Click Regression equation, table of coefficients, s,  
R-squared, and basic analysis of variance and then  
click OK.
	10.	 Back in the Regression dialog box, click Options.
In the Regression - Options dialog box (shown below):
	11.	 Check Fit Intercept.
	12.	 Clear all the Display and Lack of Fit Test check boxes.
	13.	 Enter 4 in the Prediction intervals for new observations box.
	14.	 Enter 95 in the Confidence level box.
	15.	 Click OK.
	16.	 Back in the Regression dialog box, click OK.
To create a scatter plot that contains a prediction line and 
regression equation similar to Figure 13.5 on page 524, use the 
Section MG2.6 scatter plot instructions with the Table 13.1  
Sunflowers Apparel data.
MG13.3  Measures of Variation
The measures of variation are computed in the Analysis of  
Variance table that is part of the simple linear regression results 
created using the Section MG13.2 instructions.
MG13.4  Assumptions
There are no Minitab Guide instructions for this section.
MG13.5  Residual Analysis
Selections in step 5 of the Section MG13.2 instructions create the 
residual plots and normal probability plots necessary for residual 
analysis. To create the list of residual values similar to the last col-
umn in Figure 13.10 on page 536, replace step 15 of the Section 
MG13.2 instructions with these steps 15 through 17:
	15.	 Click Storage.
	16.	 In the Regression - Storage dialog box, check Residuals and 
then click OK.
	17.	 Back in the Regression dialog box, click OK.
C h a p t e r  1 3  M i n i ta b  G u i d e

570	
Chapter 13  Simple Linear Regression
MG13.6  Measuring Autocorrelation: 
the Durbin-Watson Statistic
To compute the Durbin-Watson statistic, use the Section MG13.2 
instructions but check Durbin-Watson statistic (in the Regres-
sion - Options dialog box) as part of step 12.
MG13.7  Inferences About the Slope 
and Correlation Coefficient
The t test for the slope and F test for the slope are included in the 
results created by using the Section MG13.2 instructions.
MG13.8  Estimation of Mean Values 
and Prediction of Individual 
Values
The confidence interval estimate and prediction interval are  
included in the results created by using the Section MG13.2  
instructions.

571
U s i n g  S tat i s t i c s
The Multiple Effects of OmniPower Bars
You are a marketing manager for OmniFoods, with oversight for nutrition bars and 
similar snack items. You seek to revive the sales of OmniPower, the company’s primary 
product in this category. Originally marketed as a high-energy bar to runners, mountain 
climbers, and other athletes, OmniPower reached its greatest sales in an earlier time, 
when high-energy bars were most popular with consumers. Now, you seek to remarket 
the product as a nutrition bar to benefit from the booming market for such bars.
Because the marketplace already contains several successful nutrition bars, you 
need to develop an effective marketing strategy. In particular, you need to determine 
the effect that price and in-store promotional expenses (special in-store coupons, 
signs, and displays as well as the cost of free samples) will have on sales of  
OmniPower. Before marketing the bar nationwide, you plan to conduct a test-market 
study of OmniPower sales, using a sample of 34 stores in a supermarket chain. 
How can you extend the linear regression methods discussed in Chapter 13 to  
incorporate the ­effects of price and promotion into the same model? How can you 
use this model to improve the success of the nationwide introduction of OmniPower?
contents
14.1  Developing a Multiple 
Regression Model
14.2  r2, Adjusted r2, and the 
Overall F Test
14.3  Residual Analysis for the 
Multiple Regression Model
14.4  Inferences Concerning 
the Population Regression 
Coefficients
14.5  Testing Portions of the 
Multiple Regression Model
14.6  Using Dummy Variables 
and Interaction Terms in 
Regression Models
14.7  Logistic Regression
14.8  Influence Analysis
Using Statistics: The Multiple 
Effects of OmniPower Bars, 
Revisited
Chapter 14 Excel Guide
Chapter 14 Minitab Guide
Objectives
To learn to develop a multiple 
regression model
To learn to interpret the regression 
coefficients
To learn to determine which 
independent variables to include 
in the regression model
To learn to determine which ­ 
independent variables are 
most important in predicting a 
dependent variable
To learn to use categorical 
independent variables in a 
regression model
To use logistic regression to predict 
a categorical dependent variable
To learn to identify individual 
observations that may be 
unduly influencing the multiple 
regression model
Introduction to  
Multiple Regression
14
Chapter
Ariwasabi/Shutterstock

572	
Chapter 14  Introduction to Multiple Regression 
C
hapter 13 discusses simple linear regression models that use one numerical indepen-
dent variable, X, to predict the value of a numerical dependent variable, Y. Often you 
can make better predictions by using more than one independent variable. This chap-
ter introduces you to multiple regression models that use two or more independent variables 
to predict the value of a dependent variable.
14.1  Developing a Multiple Regression Model
In the OmniPower Bars scenario, your business objective, to determine the effect that price 
and in-store promotional expenses will have on sales, calls for examining a multiple regression 
model in which the price of an OmniPower bar in cents 1X12 and the monthly budget for in-
store promotional expenditures in dollars 1X22 are the independent variables and the number of 
OmniPower bars sold in a month (Y) is the dependent variable.
To develop this model, you collect data from a sample of 34 stores in a supermarket chain 
selected for a test-market study of OmniPower. You choose stores in a way to ensure that 
they all have approximately the same monthly sales volume. You organize and store the data 
­collected in  OmniPower . Table 14.1 presents these data.
T a b l e  1 4 . 1
Monthly OmniPower 
Sales, Price, and 
Promotional 
Expenditures
Store
Sales
Price
Promotion
Store
Sales
Price
Promotion
1
4,141
59
200
18
2,730
79
400
2
3,842
59
200
19
2,618
79
400
3
3,056
59
200
20
4,421
79
400
4
3,519
59
200
21
4,113
79
600
5
4,226
59
400
22
3,746
79
600
6
4,630
59
400
23
3,532
79
600
7
3,507
59
400
24
3,825
79
600
8
3,754
59
400
25
1,096
99
200
9
5,000
59
600
26
761
99
200
10
5,120
59
600
27
2,088
99
200
11
4,011
59
600
28
820
99
200
12
5,015
59
600
29
2,114
99
400
13
1,916
79
200
30
1,882
99
400
14
675
79
200
31
2,159
99
400
15
3,636
79
200
32
1,602
99
400
16
3,224
79
200
33
3,354
99
600
17
2,295
79
400
34
2,927
99
600
When there are two independent variables in the multiple regression model, using a 
three-dimensional (3D) scatter plot can help suggest a starting point for analysis. Figure 14.1 
on page 573 presents a 3D scatter plot of the OmniPower data. In this figure, points are plot-
ted at a height equal to their sales and have drop lines down to their corresponding price 
and promotion expense values. Rotating 3D plots can sometimes reveal patterns. One rotated 
view (Figure 14.1 right) suggests a negative linear relationship between sales and price (sales 
decrease as price increases) and a positive linear relationship between sales and promotional 
expenses (sales increase as those expenses increase). These relationships are not easily seen 
in the original orientation of the scatter plot.

	
14.1  Developing a Multiple Regression Model	
573
Interpreting the Regression Coefficients
When there are several independent variables, you can extend the simple linear regression 
model of Equation (13.1) on page 521 by assuming a linear relationship between each inde-
pendent variable and the dependent variable. For example, with k independent variables, the 
multiple regression model is expressed in Equation (14.1).
Excel does not include the 
capability to construct 3D 
scatter plots
F i g u r e  1 4 . 1
Original (left) and rotated (right) Minitab 3D scatter plot of the monthly OmniPower sales, price, and promotional expenses
Multiple Regression Model with k Independent Variables
	
Yi = b0 + b1X1i + b2X2i + b3X3i + g + bkXki + ei	
(14.1)
	
where
b0 = Y intercept
b1 = slope of Y with variable X1, holding variables X2, X3, c, Xk constant
b2 = slope of Y with variable X2, holding variables X1, X3, c, Xk constant
b3 = slope of Y with variable X3, holding variables X1, X2, c, Xk constant
f
bk = slope of Y with variable Xk holding variables X1, X2, X3, c, Xk - 1 constant
ei = random error in Y for observation i
Equation (14.2) defines the multiple regression model with two independent variables.
Multiple Regression Model with Two Independent Variables
	
Yi = b0 + b1X1i + b2X2i + ei	
(14.2)
	
where
b0 = intercept
b1 = slope of Y with variable X1, holding variable X2 constant
b2 = slope of Y with variable X2, holding variable X1 constant
ei = random error in Y for observation i

574	
Chapter 14  Introduction to Multiple Regression 
Compare the multiple regression model to the simple linear regression model [Equation (13.1) 
on page 521]:
Yi =  b0 +  b1Xi + ei
In the simple linear regression model, the slope, b1, represents the change in the mean of Y per 
unit change in X and does not take into account any other variables. In the multiple regression 
model with two independent variables [Equation (14.2)], the slope, b1, represents the change 
in the mean of Y per unit change in X1, taking into account the effect of X2.
As in the case of simple linear regression, you use the least-squares method to compute the 
sample regression coefficients b0, b1, and b2 as estimates of the population parameters b0, b1, 
and b2. Equation (14.3) defines the regression equation for a multiple regression model with 
two independent variables.
Student Tip
Because multiple 
regression computations 
are more complex than 
computations for simple 
linear regression, always 
use a computerized 
method to obtain multiple 
regression results.
Multiple Regression Equation with Two Independent Variables
	
Yni = b0 + b1X1i + b2X2i	
(14.3)
Figure 14.2 shows Excel and Minitab results for the OmniPower sales data multiple ­regression 
model. In these results, the b0 coefficient is labeled Intercept by Excel and Constant by Minitab.
From Figure 14.2, the computed values of the three regression coefficients are
b0 = 5,837.5208  b1 = -53.2173  b2 = 3.6131
Therefore, the multiple regression equation is
Yni = 5,837.5208 - 53.2173X1i + 3.6131X2i
F i g u r e  1 4 . 2
Excel and Minitab results for the OmniPower sales multiple regression model

	
14.1  Developing a Multiple Regression Model	
575
The sample Y intercept 1b0 = 5,837.52082 estimates the number of OmniPower bars sold 
in a month if the price is $0.00 and the total amount spent on promotional expenditures is also 
$0.00. Because these values of price and promotion are outside the range of price and promo-
tion used in the test-market study, and because they make no sense in the context of the prob-
lem, the value of b0 has little or no practical interpretation.
The slope of price with OmniPower sales 1b1 = -53.21732 indicates that, for a given 
amount of monthly promotional expenditures, the predicted mean sales of OmniPower are es-
timated to decrease by 53.2173 bars per month for each 1-cent increase in the price. The slope 
of monthly promotional expenditures with OmniPower sales 1b2 = 3.61312 indicates that, for 
a given price, the predicted mean sales of OmniPower are estimated to increase by 3.6131 bars 
for each additional $1 spent on promotions. These estimates allow you to better understand 
the likely effect that price and promotion decisions will have in the marketplace. For example, 
a 10-cent decrease in price is predicted to increase mean sales by 532.173 bars, with a fixed 
amount of monthly promotional expenditures. A $100 increase in promotional expenditures is 
predicted to increase mean sales by 361.31 bars for a given price.
Regression coefficients in multiple regression are called net regression coefficients, and 
they estimate the predicted mean change in Y per unit change in a particular X, holding con-
stant the effect of the other X variables. For example, in the study of OmniPower bar sales, 
for a store with a given amount of promotional expenditures, the mean sales are predicted to 
decrease by 53.2173 bars per month for each 1-cent increase in the price of an OmniPower 
bar. Another way to interpret this “net effect” is to think of two stores with an equal amount 
of promotional expenditures. If the first store charges 1 cent more than the other store, the net 
effect of this difference is that the first store is predicted to sell a mean of 53.2173 fewer bars 
per month than the second store. To interpret the net effect of promotional expenditures, you 
can consider two stores that are charging the same price. If the first store spends $1 more on 
promotional expenditures, the net effect of this difference is that the first store is predicted to 
sell a mean of 3.6131 more bars per month than the second store.
Predicting the Dependent Variable Y
You can use the multiple regression equation to predict values of the dependent variable. For 
example, what are the predicted mean sales for a store charging 79 cents during a month in 
which promotional expenditures are $400? Using the multiple regression equation,
Yni = 5,837.5208 - 53.2173X1i + 3.6131X2i
with X1i = 79 and X2i = 400,
 Yni = 5,837.5208 - 53.21731792 + 3.613114002
 = 3,078.57
Thus, you predict that stores charging 79 cents and spending $400 in promotional expenditures 
will sell a mean of 3,078.57 OmniPower bars per month.
After you have developed the regression equation, done a residual analysis (see Section 14.3), 
and determined the significance of the overall fitted model (see Section 14.2), you can construct 
a confidence interval estimate of the mean value and a prediction interval for an individual value. 
Figure 14.3 presents Excel and Minitab results that compute a confidence interval estimate and a 
prediction interval for the OmniPower sales data.
Student Tip
Remember that in multiple 
regression, the regression 
coefficients are conditional 
on holding constant 
the other independent 
variables. The slope of b1 
holds constant the effect 
of variable X2. The slope 
of b2 holds constant the 
effect of variable X1.
Student Tip
You should only predict 
within the range of 
the values of all the 
independent variables.
where
Yni = predicted monthly sales of OmniPower bars for store i
X1i = price of OmniPower bar (in cents) for store i
X2i = monthly in-store promotional expenditures (in $) for store i

576	
Chapter 14  Introduction to Multiple Regression 
The 95% confidence interval estimate of the mean OmniPower sales for all stores ­charging 
79 cents and spending $400 in promotional expenditures is 2,854.07 to 3,303.08 bars. The 
­prediction interval for an individual store is 1,758.01 to 4,399.14 bars.
Problems for Section 14.1
Learning the Basics
14.1  For the following regression equation: Production = 1.5 
+ 0.7 Experience + 0.5 Aptitude Test
a.	 	Specify the dependent and independent variables.
b.	 	Rewrite the regression equation using Yni and Xi.
14.2  For the following regression equation: Demand = 1.5 + 0.7 
Income + 0.5 Need
a.	 	Specify the dependent and independent variables.
b.	 	What will be the demand when income is 0 and need is 1?
Applying the Concepts
14.3  A shoe manufacturer is considering developing a new brand 
of running shoes. The business problem facing the marketing analyst 
is to determine which variables should be used to predict durability 
(i.e., the effect of long-term impact). Two independent variables un-
der consideration are X1 (FOREIMP), a measurement of the forefoot 
shock-absorbing capability, and X2 (MIDSOLE), a measurement of 
the change in impact properties over time. The dependent variable Y 
is LTIMP, a measure of the shoe’s durability after a repeated impact 
test. Data are collected from a random sample of 15 types of cur-
rently manufactured running shoes, with the following results:
Variable
Coefficients
Standard  
Error
t Statistic
p-Value
Intercept
-0.02686
0.06905
-0.39
0.7034
FOREIMP
0.79116
0.06295
12.57
0.0000
MIDSOLE
0.60484
0.07174
8.43
0.0000
a.	 	State the multiple regression equation.
b.	 	Interpret the meaning of the slopes, b1 and b2, in this problem.
c.	 What conclusions can you reach concerning durability?
SELF 
Test 
14.4  A mail-order catalog business selling personal 
computer supplies, software, and hardware maintains a 
centralized warehouse. Management is currently examining the 
process of distribution from the warehouse. The business problem 
facing management relates to the factors that affect warehouse dis-
tribution costs. Currently, a handling fee is added to each order, 
regardless of the amount of the order. Data collected over the past 
24 months (stored in  WareCost ) indicate the warehouse distribu-
tion costs (in $thousands), the sales (in $thousands), and the num-
ber of orders received.
a.	 	State the multiple regression equation.
b.	 	Interpret the meaning of the slopes, b1 and b2, in this problem.
c.	 	Explain why the regression coefficient, b0, has no practical 
meaning in the context of this problem.
d.	 	Predict the mean monthly warehouse distribution cost when 
sales are $400,000 and the number of orders is 4,500.
e.	 	Construct a 95% confidence interval estimate for the mean 
monthly warehouse distribution cost when sales are $400,000 
and the number of orders is 4,500.
F i g u r e  1 4 . 3
Excel confidence interval 
estimate and prediction 
interval worksheet for the 
OmniPower sales data

	
14.1  Developing a Multiple Regression Model	
577
f.	 	Construct a 95% prediction interval for the monthly warehouse 
distribution cost for a particular month when sales are $400,000 
and the number of orders is 4,500.
g.	 	Explain why the interval in (e) is narrower than the interval in (f).
h.	 What conclusions can you reach concerning warehouse distri-
bution cost?
SELF 
Test 
14.5  The production of wine is a multibillion-dollar 
worldwide industry. In an attempt to develop a model of 
wine quality as judged by wine experts, data was collected from 
red wine variants of Portuguese “Vinho Verde” wine. A sample of 
50 wines is stored in  VinhoVerde . (Data extracted from P. Cortez, 
Cerdeira, A., Almeida, F., Matos, T., and Reis, J., “Modeling Wine 
Preferences by Data Mining from Physiochemical Properties,”  
Decision Support Systems, 47, 2009, pp. 547–553 and bit.ly/ 
9xKlEa.) Develop a multiple linear regression model to predict 
wine quality, measured on a scale from 0 (very bad) to 10 (excel-
lent) based on alcohol content (%) and the amount of chlorides.
a.	 	State the multiple regression equation.
b.	 	Interpret the meaning of the slopes, b1 and b2, in this problem.
c.	 	Explain why the regression coefficient, b0, has no practical 
meaning in the context of this problem.
d.	 	Predict the mean wine quality rating for wines that have 10% 
alcohol and chlorides of 0.08.
e.	 	Construct a 95% confidence interval estimate for the mean 
wine quality rating for wines that have 10% alcohol and chlo-
rides of 0.08.
f.	 	Construct a 95% prediction interval for the wine quality rating for 
an individual wine that has 10% alcohol and chlorides of 0.08.
g.	 What conclusions can you reach concerning this regression model?
14.6  The business problem facing a consumer products company is 
to measure the effectiveness of different types of advertising media 
in the promotion of its products. Specifically, the company is inter-
ested in the effectiveness of radio advertising and newspaper adver-
tising (including the cost of discount coupons). During a one-month 
test period, data were collected from a sample of 22 cities with ap-
proximately equal populations. Each city is allocated a specific ex-
penditure level for radio advertising and for newspaper advertising. 
The sales of the product (in $thousands) and also the levels of media 
expenditure (in $thousands) during the test month are recorded, with 
the following results shown below and stored in  Advertise :
City
Sales 
($thousands)
Radio  
Advertising 
($thousands)
Newspaper  
Advertising 
($thousands)
1
973
0
40
2
1,119
0
40
3
875
25
25
4
625
25
25
5
910
30
30
6
971
30
30
7
931
35
35
8
1,177
35
35
9
882
40
25
10
982
40
25
11
1,628
45
45
12
1,577
45
45
13
1,044
50
0
14
914
50
0
15
1,329
55
25
a.	 	State the multiple regression equation.
b.	 	Interpret the meaning of the slopes, b1 and b2, in this problem.
c.	 	Interpret the meaning of the regression coefficient, b0.
d.	 	Which type of advertising is more effective? Explain.
14.7  The business problem facing the director of broadcasting op-
erations for a television station was the issue of standby hours (i.e., 
hours in which unionized graphic artists at the station are paid but 
are not actually involved in any activity) and what factors were re-
lated to standby hours. The study included the following variables:
Standby hours (Y)—Total number of standby hours in a week
Total staff present 1X12—Weekly total of people-days
Remote hours 1X22—Total number of hours worked by ­employees  
  at locations away from the central plant
Data were collected for 26 weeks; these data are organized and 
stored in  Standby .
a.	 	State the multiple regression equation.
b.	 	Interpret the meaning of the slopes, b1 and b2, in this problem.
c.	 	Explain why the regression coefficient, b0, has no practical 
meaning in the context of this problem.
d.	 	Predict the mean standby hours for a week in which the total staff 
present have 310 people-days and the remote hours total 400.
e.	 	Construct a 95% confidence interval estimate for the mean 
standby hours for weeks in which the total staff present have 
310 people-days and remote hours total 400.
f.	 	Construct a 95% prediction interval for the standby hours for 
a single week in which the total staff present have 310 people-
days and the remote hours total 400.
g.	 What conclusions can you reach concerning standby hours?
14.8  Nassau County is located approximately 25 miles east of 
New York City. The data organized and stored in  GlenCove  in-
clude the fair market value (in $thousands), land area of the prop-
erty in acres, and age, in years, for a sample of 30 single-family 
homes located in Glen Cove, a small city in Nassau County. Develop 
a multiple linear regression model to predict the fair market value 
based on land area of the property and age, in years.
a.	 	State the multiple regression equation.
b.	 	Interpret the meaning of the slopes, b1 and b2, in this problem.
c.	 	Explain why the regression coefficient, b0, has no practical 
meaning in the context of this problem.
d.	 	Predict the mean fair market value for a house that has a land 
area of 0.25 acre and is 55 years old.
e.	 	Construct a 95% confidence interval estimate for the mean fair 
market value for houses that have a land area of 0.25 acre and 
are 55 years old.
f.	 	Construct a 95% prediction interval estimate for the fair market 
value for an individual house that has a land area of 0.25 acre 
and is 55 years old.
City
Sales 
($thousands)
Radio  
Advertising 
($thousands)
Newspaper  
Advertising 
($thousands)
16
1,330
55
25
17
1,405
60
30
18
1,436
60
30
19
1,521
65
35
20
1,741
65
35
21
1,866
70
40
22
1,717
70
40

578	
Chapter 14  Introduction to Multiple Regression 
Coefficient of Multiple Determination
The coefficient of multiple determination is equal to the regression sum of squares (SSR) 
divided by the total sum of squares (SST).
	
r 2 = regression sum of squares
total sum of squares
= SSR
SST	
(14.4)
In the OmniPower example, from Figure 14.2 on page 574, SSR = 39,472,730.77 and 
SST = 52,093,677.44. Thus,
r 2 = SSR
SST = 39,472,730.77
52,093,677.44 = 0.7577
The coefficient of multiple determination 1r 2 = 0.75772 indicates that 75.77% of the varia-
tion in sales is explained by the variation in the price and in the promotional expenditures.  
The coefficient of multiple determination also appears in the Figure 14.2 results on page 574, 
labeled R Square in the Excel results and R-Sq in the Minitab results.
Adjusted r2
When considering multiple regression models, some statisticians suggest that you should use 
the adjusted r 2 to take into account both the number of independent variables in the model 
and the sample size. Reporting the adjusted r 2 is extremely important when you are comparing 
two or more regression models that predict the same dependent variable but have a different 
number of independent variables. Equation (14.5) defines the adjusted r 2.
Student Tip
Remember that r 2 in 
multiple regression 
represents the proportion 
of the variation in the 
dependent variable Y that 
is explained by all the 
independent X variables 
included in the model.
Adjusted r2
	
r 2
adj = 1 - c11 - r 22
n - 1
n - k - 1 d 	
(14.5)
where k is the number of independent variables in the regression equation.
14.2  r2, Adjusted r2, and the Overall F Test
This section discusses three methods you can use to evaluate the overall multiple regression 
model: the coefficient of multiple determination, r2, the adjusted r2, and the overall F test.
Coefficient of Multiple Determination
Recall from Section 13.3 that the coefficient of determination, r 2, measures the proportion of 
the variation in Y that is explained by the independent variable X in the simple linear regression 
model. In multiple regression, the coefficient of multiple determination represents the pro-
portion of the variation in Y that is explained by all the independent variables. Equation (14.4) 
defines the coefficient of multiple determination for a multiple regression model with two or 
more independent variables.

	
14.2  r2, Adjusted r2, and the Overall F Test	
579
Test for the Significance of the Overall Multiple  
Regression Model
You use the overall F test to determine whether there is a significant relationship between the 
dependent variable and the entire set of independent variables (the overall multiple regression 
model). Because there is more than one independent variable, you use the following null and 
alternative hypotheses:
H0: b1 = b2 = g = bk = 0 1there is no linear relationship between the dependent 
variable and the independent variables.2
H1: At least one bj ≠0, j = 1, 2, c, k 1there is a linear relationship between the
dependent variable and at least one of the
independent variables.2
Equation (14.6) defines the overall F test statistic. Table 14.2 presents the ANOVA 
­summary table.
Thus, for the OmniPower data, because r 2 = 0.7577, n = 34, and k = 2,
 r 2
adj = 1 - c11 - 0.75772 
34 - 1
34 - 2 - 1 d
 = 1 - c10.2423233
31 d
 = 1 - 0.2579
 = 0.7421
Therefore, 74.21% of the variation in sales is explained by the multiple regression model—­
adjusted for the number of independent variables and sample size. The adjusted r 2 also ­appears 
in the Figure 14.2 results on page 574, labeled Adjusted R Square in the Excel ­results and R 
Sq(adj) in the Minitab results.
T a b l e  1 4 . 2
ANOVA Summary 
Table for the Overall 
F Test
Source
Degrees of  
Freedom
Sum of  
Squares
Mean Squares  
(Variance)
F
Regression
k
SSR
MSR = SSR
k
FSTAT = MSR
MSE
Error
n - k - 1
SSE
MSE =
SSE
n - k - 1
Total
n - 1
SST
Overall F Test
The FSTAT test statistic is equal to the regression mean square (MSR) divided by the mean 
square error (MSE).
	
FSTAT = MSR
MSE	
(14.6)
	
where
k = number of independent variables in the regression model
The FSTAT  test statistic follows an F distribution with k and n - k - 1 degrees of freedom.
Student Tip
Remember that you are 
testing whether at least 
one independent variable 
has a linear relationship 
with the dependent 
variable. If you reject H0, 
you are not concluding 
that all the independent 
variables have a linear 
relationship with the 
dependent variable, 
only that at least one 
independent variable 
does.

580	
Chapter 14  Introduction to Multiple Regression 
0
3.32
.95
.05
Region of
Nonrejection
F
Critical
Value
Region of
Rejection
F i g u r e  1 4 . 4
Testing for the 
significance of a set of 
regression coefficients 
at the 0.05 level of 
significance, with 2 and 
31 degrees of freedom
Problems for Section 14.2
Learning the Basics
14.9  The following ANOVA summary table is for a multiple re-
gression model with two independent variables:
Source
Degrees of  
Freedom
Sum of  
Squares
Mean  
Squares
F
Regression
  2
  60
Error
18
120
Total
20
180
Source
Degrees of 
Freedom
Sum of 
Squares
Mean 
Squares
F
Regression
  2
  30
Error
10
120
Total
12
150
a.	 	Determine the regression mean square (MSR) and the mean 
square error (MSE).
b.	 	Compute the overall FSTAT test statistic.
c.	 	Determine whether there is a significant relationship between Y 
and the two independent variables at the 0.05 level of significance.
d.	 	Compute the coefficient of multiple determination, r2, and in-
terpret its meaning.
e.	 	Compute the adjusted r2.
14.10  The following ANOVA summary table is for a multiple re-
gression model with two independent variables:
a.	 	Determine the regression mean square (MSR) and the mean 
square error (MSE).
b.	 	Compute the overall FSTAT test statistic.
c.	 	Determine whether there is a significant relationship between Y 
and the two independent variables at the 0.05 level of significance.
d.	 	Compute the coefficient of multiple determination, r2, and in-
terpret its meaning.
e.	 	Compute the adjusted r2.
Applying the Concepts
14.11  A financial analyst engaged in business valuation obtained 
financial data on 71 drug companies (Industry Group SIC 3 code: 
283). The file  BusinessValuation  contains the following variables:
COMPANY—Drug Company name
PB fye—Price-to-book-value ratio (fiscal year ending)
ROE—Return on equity
SGROWTH—Growth (GS5)
a.	 	Develop a regression model to predict price-to-book-value  
ratio based on return on equity.
b.	 	Develop a regression model to predict price-to-book-value ratio  
based on growth.
c.	 	Develop a regression model to predict price-to-book-value  
ratio based on return on equity and growth.
d.	 	Compute and interpret the adjusted r2 for each of the three 
models.
e.	 	Which of these three models do you think is the best predictor 
of price-to-book-value ratio?
The decision rule is
reject H0 at the a level of significance if FSTAT 7 Fa;
otherwise, do not reject H0.
Using a 0.05 level of significance, the critical value of the F distribution with 2 and 31 
degrees of freedom found in Table E.5 is approximately 3.32 (see Figure 14.4 on page 580). 
From Figure 14.2 on page 574, the FSTAT test statistic given in the ANOVA summary table 
is 48.4771. Because 48.4771 7 3.32, or because the p@value = 0.000 6 0.05, you reject H0 
and conclude that at least one of the independent variables (price and/or promotional expen-
ditures) is related to sales.

	
14.3  Residual Analysis for the Multiple Regression Model	
581
Source
Degrees of 
Freedom
Sum of  
Squares
Mean  
Squares
F
p-Value
Regression
  2
12.61020
6.30510
97.69
0.0001
Error
12
  0.77453
0.06454
Total
14
13.38473
14.12  In Problem 14.3 on page 576, you predicted the durability 
of a brand of running shoe, based on the forefoot shock-absorbing 
capability and the change in impact properties over time. The re-
gression analysis resulted in the following ANOVA summary table:
b.	 	interpret the meaning of the p-value.
c.	 	compute the coefficient of multiple determination, r2, and in-
terpret its meaning.
d.	 	compute the adjusted r2.
14.15  In Problem 14.7 on page 577, you used the total staff pres-
ent and remote hours to predict standby hours (stored in  Standby ). 
Using the results from that problem,
a.	 	determine whether there is a significant relationship between 
standby hours and the two independent variables (total staff 
present and remote hours) at the 0.05 level of significance.
b.	 	interpret the meaning of the p-value.
c.	 	compute the coefficient of multiple determination, r2, and in-
terpret its meaning.
d.	 	compute the adjusted r2.
14.16  In Problem 14.6 on page 577, you used radio advertising 
and newspaper advertising to predict sales (stored in  Advertise ). 
Using the results from that problem,
a.	 	determine whether there is a significant relationship between 
sales and the two independent variables (radio advertising and 
newspaper advertising) at the 0.05 level of significance.
b.	 	interpret the meaning of the p-value.
c.	 	compute the coefficient of multiple determination, r2, and 
­interpret its meaning.
d.	 	compute the adjusted r2.
14.17  In Problem 14.8 on page 577, you used the land area of 
a property and the age of a house to predict the fair market value 
(stored in  GlenCove ). Using the results from that problem,
a.	 	determine whether there is a significant relationship between 
fair market value and the two independent variables (land 
area of a property and age of a house) at the 0.05 level of  
significance.
b.	 	interpret the meaning of the p-value.
c.	 	compute the coefficient of multiple determination, r2, and in-
terpret its meaning.
d.	 	compute the adjusted r2.
14.3  Residual Analysis for the Multiple Regression Model
In Section 13.5, you used residual analysis to evaluate the fit of the simple linear regression 
model. For the multiple regression model with two independent variables, you need to con-
struct and analyze the following residual plots:
 • Residuals versus Yni
 • Residuals versus X1i
 • Residuals versus X2i
 • Residuals versus time
The first residual plot examines the pattern of residuals versus the predicted values of Y. 
If the residuals show a pattern for the predicted values of Y, there is evidence of a possible 
­curvilinear effect (see Section 15.1) in at least one independent variable, a possible violation of 
the assumption of equal variance (see Figure 13.13 on page 538), and/or the need to transform 
the Y variable.
The second and third residual plots involve the independent variables. Patterns in the plot 
of the residuals versus an independent variable may indicate the existence of a curvilinear  
Student Tip
As is the case with simple 
linear regression, a residual 
plot that does not contain 
any apparent patterns 
will look like a random 
scattering of points.
a.	 	Determine whether there is a significant relationship between 
durability and the two independent variables at the 0.05 level of 
significance.
b.	 	Interpret the meaning of the p-value.
c.	 	Compute the coefficient of multiple determination, r2, and in-
terpret its meaning.
d.	 	Compute the adjusted r2.
14.13  In Problem 14.5 on page 577, you used the percent-
age of alcohol and chlorides to predict wine quality (stored in  
 VinhoVerde ). Use the results from that problem to do the  
following:
a.	 	Determine whether there is a significant relationship between 
wine quality and the two independent variables (percentage of 
alcohol and chlorides) at the 0.05 level of significance.
b.	 	Interpret the meaning of the p-value.
c.	 	Compute the coefficient of multiple determination, r2, and in-
terpret its meaning.
d.	 	Compute the adjusted r2.
SELF 
Test 
14.14  In Problem 14.4 on page 576, you used sales 
and number of orders to predict distribution costs at a 
mail-order catalog business (stored in  WareCost ). Using the re-
sults from that problem,
a.	 	determine whether there is a significant relationship between 
distribution costs and the two independent variables (sales and 
number of orders) at the 0.05 level of significance.

582	
Chapter 14  Introduction to Multiple Regression 
F i g u r e  1 4 . 5
Residual plots for the OmniPower sales data: residuals versus predicted Y, residuals versus price, and residuals versus 
promotional expenditures
Problems for Section 14.3
Applying the Concepts
14.18  In Problem 14.4 on page 576, you used sales and number of 
orders to predict distribution costs at a mail-order catalog ­business 
(stored in  WareCost ).
a.	 	Plot the residuals versus Yni.
b.	 	Plot the residuals versus X1i.
c.	 	Plot the residuals versus X2i.
d.	 	Plot the residuals versus time.
e.	 	In the residual plots created in (a) through (d), is there any evi-
dence of a violation of the regression assumptions? Explain.
f.	 	Determine the Durbin-Watson statistic.
g.	 	At the 0.05 level of significance, is there evidence of positive 
autocorrelation in the residuals?
14.19  In Problem 14.5 on page 577, you used the percentage of al-
cohol and chlorides to predict wine quality (stored in  VinhoVerde ).
a.	 	Plot the residuals versus Yni
b.	 	Plot the residuals versus X1i.
c.	 	Plot the residuals versus X2i.
d.	 	In the residual plots created in (a) through (c), is there any evi-
dence of a violation of the regression assumptions? Explain.
e.	 	Should you compute the Durbin-Watson statistic for these 
data? Explain.
14.20  In Problem 14.6 on page 577, you used radio advertising 
and newspaper advertising to predict sales (stored in  Advertise ).
a.	 	Perform a residual analysis on your results.
b.	 	If appropriate, perform the Durbin-Watson test, using a = 0.05.
c.	 	Are the regression assumptions valid for these data?
14.21  In Problem 14.7 on page 577, you used the total staff 
present and remote hours to predict standby hours (stored in  
 Standby ).
a.	 	Perform a residual analysis on your results.
b.	 	If appropriate, perform the Durbin-Watson test, using a = 0.05.
c.	 	Are the regression assumptions valid for these data?
14.22  In Problem 14.8 on page 577, you used the land area of 
a property and the age of a house to predict the fair market value 
(stored in  GlenCove ).
a.	 	Perform a residual analysis on your results.
b.	 	If appropriate, perform the Durbin-Watson test, using a = 0.05.
c.	 	Are the regression assumptions valid for these data?
effect and, therefore, the need to add a curvilinear independent variable to the multiple regres-
sion model (see Section 15.1).
The fourth plot is used to investigate patterns in the residuals in order to validate the inde-
pendence assumption when the data are collected in time order. Associated with this residual 
plot, as in Section 13.6, you can compute the Durbin-Watson statistic to determine the exis-
tence of positive autocorrelation among the residuals.
Figure 14.5 presents the residual plots for the OmniPower sales example. There is very 
little or no pattern in the relationship between the residuals and the predicted value of Y, the 
value of X1 (price), or the value of X2 (promotional expenditures). Thus, you can conclude that 
the multiple regression model is appropriate for predicting sales. There is no need to plot the 
residuals versus time because the data were not collected in time order.

	
14.4  Inferences Concerning the Population Regression Coefficients 	
583
14.4  Inferences Concerning the Population  
Regression Coefficients
In Section 13.7, you tested the slope in a simple linear regression model to determine the sig-
nificance of the relationship between X and Y. In addition, you constructed a confidence interval 
estimate of the population slope. This section extends those procedures to multiple regression.
Tests of Hypothesis
In a simple linear regression model, to test a hypothesis concerning the population slope, b1, 
you used Equation (13.16) on page 544:
tSTAT = b1 - b1
Sb1
Equation (14.7) generalizes this equation for multiple regression.
Testing for the Slope in Multiple Regression
	
tSTAT =
bj - bj
Sbj
	
(14.7)
	
where
bj = slope of variable j with Y, holding constant the effects of all other  
independent variables
Sbj = standard error of the regression coefficient bj
k = number of independent variables in the regression equation
bj = hypothesized value of the population slope for variable j, holding  
constant the effects of all other independent variables
tSTAT = test statistic for a t distribution with n - k - 1 degrees of freedom
To determine whether variable X2 (amount of promotional expenditures) has a signifi-
cant effect on sales, taking into account the price of OmniPower bars, the null and alternative 
­hypotheses are
 H0: b2 = 0
 H1: b2 ≠0
From Equation (14.7) and Figure 14.2 on page 574,
 tSTAT = b2 - b2
Sb2
 = 3.6131 - 0
0.6852
= 5.2728
If you select a level of significance of 0.05, the critical values of t for 31 degrees of freedom 
from Table E.3 are -2.0395 and +2.0395 (see Figure 14.6).

584	
Chapter 14  Introduction to Multiple Regression 
Confidence Interval Estimate for the Slope
	
bj { ta>2Sbj	
(14.8)
	
where
ta>2 = the critical value corresponding to an upper-tail probability of a>2 from the 
t distribution with n - k - 1 degrees of freedom (i.e., a cumulative area of 
1 - a>2)
k = the number of independent variables
From Figure 14.2 on page 574, observe that the computed tSTAT test statistic is 5.2728. Be-
cause tSTAT = 5.2728 7 2.0395 or because the p-value is 0.0000, you reject H0 and conclude 
that there is a significant relationship between the variable X2 (promotional expenditures) and 
sales, taking into account the price, X1. The extremely small p-value allows you to strongly reject 
the null hypothesis that there is no linear relationship between sales and promotional expenditures. 
Example 14.1 presents the test for the significance of b1, the slope of sales with price.
F i g u r e  1 4 . 6
Testing for significance of 
a regression coefficient 
at the 0.05 level of 
significance, with 31 
degrees of freedom
–2.0395
+2.0395
0
t
Region of
Rejection
Critical
Value
Region of
Nonrejection
Critical
Value
Region of
Rejection
As shown with these two independent variables, the test of significance for a specific 
­regression coefficient in multiple regression is a test for the significance of adding that variable 
into a regression model, given that the other variable is included. In other words, the t test for 
the regression coefficient is actually a test for the contribution of each independent variable.
Confidence Interval Estimation
Instead of testing the significance of a population slope, you may want to estimate the value of 
a population slope. Equation (14.8) defines the confidence interval estimate for a population 
slope in multiple regression.
To construct a 95% confidence interval estimate of the population slope, b1 (the effect of 
price, X1, on sales, Y, holding constant the effect of promotional expenditures, X2), the critical 
Example 14.1
Testing for the 
Significance of the 
Slope of Sales with 
Price
At the 0.05 level of significance, is there evidence that the slope of sales with price is different 
from zero?
Solution  From Figure 14.2 on page 574, tSTAT = -7.7664 6 -2.0395 (the critical 
value for a = 0.05) or the p-value = 0.0000 6 0.05. Thus, there is a significant relationship 
­between price, X1, and sales, taking into account the promotional expenditures, X2.

	
14.4  Inferences Concerning the Population Regression Coefficients 	
585
value of t at the 95% confidence level with 31 degrees of freedom is 2.0395 (see Table E.3). 
Then, using Equation (14.8) and Figure 14.2 on page 574,
b1 { ta>2Sb1
-53.2173 { 12.0395216.85222
-53.2173 { 13.9752
-67.1925 … b1 … -39.2421
Taking into account the effect of promotional expenditures, the estimated effect of a 1-cent 
increase in price is to reduce mean sales by approximately 39.2 to 67.2 bars. You have 95% 
confidence that this interval correctly estimates the relationship between these variables. From a 
hypothesis-testing viewpoint, because this confidence interval does not include 0, you conclude 
that the regression coefficient, b1, has a significant effect.
Example 14.2 constructs and interprets a confidence interval estimate for the slope of sales 
with promotional expenditures.
Example 14.2
Constructing a 
Confidence ­Interval 
Estimate for the 
Slope of Sales 
with Promotional 
­Expenditures
Construct a 95% confidence interval estimate of the population slope of sales with promotional 
expenditures.
Solution  The critical value of t at the 95% confidence level, with 31 degrees of freedom, 
is 2.0395 (see Table E.3). Using Equation (14.8) and Figure 14.2 on page 574,
b2 { ta>2Sb2
3.6131 { 12.0395210.68522
3.6131 { 1.3975
2.2156 … b2 … 5.0106
Thus, taking into account the effect of price, the estimated effect of each additional dollar of 
promotional expenditures is to increase mean sales by approximately 2.22 to 5.01 bars. You have 
95% confidence that this interval correctly estimates the relationship between these variables. 
From a hypothesis-testing viewpoint, because this confidence interval does not include 0, you 
can conclude that the regression coefficient, b2, has a significant effect.
a.	 	Which variable has the largest slope, in units of a t statistic?
b.	 	Construct a 95% confidence interval estimate of the population 
slope, b1.
c.	 	At the 0.05 level of significance, determine whether each 
independent variable makes a significant contribution to the 
regression model. On the basis of these results, indicate the 
independent variables to include in this model.
Applying the Concepts
14.25  In Problem 14.3 on page 576, you predicted the dura-
bility of a brand of running shoe, based on the forefoot shock-
absorbing capability (FOREIMP) and the change in impact 
properties over time (MIDSOLE) for a sample of 15 pairs of 
shoes. Use the following results:
Problems for Section 14.4
Learning the Basics
14.23  Use the following information from a multiple regression 
analysis:
n = 25  b1 = 5  b2 = 10  Sb1 = 2  Sb2 = 8
a.	 	Which variable has the largest slope, in units of a t statistic?
b.	 	Construct a 95% confidence interval estimate of the population 
slope, b1.
c.	 	At the 0.05 level of significance, determine whether each in-
dependent variable makes a significant contribution to the 
regression model. On the basis of these results, indicate the in-
dependent variables to include in this model.
14.24  Use the following information from a multiple regression 
analysis:
n = 20  b1 = 4  b2 = 3  Sb1 = 1.2  Sb2 = 0.8

586	
Chapter 14  Introduction to Multiple Regression 
a.	 	Construct a 95% confidence interval estimate of the population 
slope between durability and forefoot shock-absorbing capability.
b.	 	At the 0.05 level of significance, determine whether each in-
dependent variable makes a significant contribution to the 
regression model. On the basis of these results, indicate the in-
dependent variables to include in this model.
SELF 
Test 
14.26  In Problem 14.4 on page 576, you used sales 
and number of orders to predict distribution costs at a 
mail-order catalog business (stored in  WareCost ). Using the re-
sults from that problem,
a.	 	construct a 95% confidence interval estimate of the population 
slope between distribution cost and sales.
b.	 	at the 0.05 level of significance, determine whether each in-
dependent variable makes a significant contribution to the 
regression model. On the basis of these results, indicate the in-
dependent variables to include in this model.
14.27  In Problem 14.5 on page 576, you used the percent-
age of alcohol and chlorides to predict wine quality (stored in  
 VinhoVerde ). Using the results from that problem,
a.	 	construct a 95% confidence interval estimate of the population 
slope between wine quality and the percentage of alcohol.
b.	 	at the 0.05 level of significance, determine whether each in-
dependent variable makes a significant contribution to the 
regression model. On the basis of these results, indicate the in-
dependent variables to include in this model.
14.28  In Problem 14.6 on page 577, you used radio advertising 
and newspaper advertising to predict sales (stored in  Advertise ). 
Using the results from that problem,
a.	 	construct a 95% confidence interval estimate of the population 
slope between sales and radio advertising.
b.	 	at the 0.05 level of significance, determine whether each in-
dependent variable makes a significant contribution to the 
regression model. On the basis of these results, indicate the in-
dependent variables to include in this model.
14.29  In Problem 14.7 on page 577, you used the total number 
of staff present and remote hours to predict standby hours (stored 
in  Standby ). Using the results from that problem,
a.	 	construct a 95% confidence interval estimate of the popula-
tion slope between standby hours and total number of staff 
present.
b.	 	at the 0.05 level of significance, determine whether each in-
dependent variable makes a significant contribution to the 
regression model. On the basis of these results, indicate the in-
dependent variables to include in this model.
14.30  In Problem 14.8 on page 577, you used land area of a 
property and age of a house to predict the fair market value (stored 
in  GlenCove ). Using the results from that problem,
a.	 	construct a 95% confidence interval estimate of the population 
slope between fair market value and land area of a property.
b.	 	at the 0.05 level of significance, determine whether each in-
dependent variable makes a significant contribution to the 
regression model. On the basis of these results, indicate the in-
dependent variables to include in this model.
Variable
Coefficient
Standard  
Error
t Statistic
p-Value
Intercept
-0.02686
0.06905
-0.39
0.7034
FOREIMP
   0.79116
0.06295
 12.57
0.0000
MIDSOLE
   0.60484
0.07174
   8.43
0.0000
14.5  Testing Portions of the Multiple Regression Model
In developing a multiple regression model, you want to use only those independent ­variables 
that significantly reduce the error in predicting the value of a dependent variable. If an 
­independent variable does not improve the prediction, you can delete it from the multiple 
­regression model and use a model with fewer independent variables.
The partial F test is an alternative method to the t test discussed in Section 14.4 for determin-
ing the contribution of an independent variable. Using this method, you determine the contribution 
to the regression sum of squares made by each independent variable after all the other independent 
variables have been included in the model. The new independent variable is included only if it 
significantly improves the model.
To conduct partial F tests for the OmniPower sales example, you need to evaluate the 
contribution of promotional expenditures 1X22 after price 1X12 has been included in the model 
and also evaluate the contribution of price 1X12 after promotional expenditures 1X22 have been 
included in the model.
In general, if there are several independent variables, you determine the contribution of 
each independent variable by taking into account the regression sum of squares of a model that 
includes all independent variables except the one of interest, j. This regression sum of squares 
is denoted SSR (all Xs except j). Equation (14.9) determines the contribution of variable j, 
­assuming that all other variables are already included.

	
14.5  Testing Portions of the Multiple Regression Model	
587
If there are two independent variables, you use Equations (14.10a) and (14.10b) to deter-
mine the contribution of each variable.
Determining the Contribution of an Independent Variable  
to the Regression Model
	
SSR1Xj All Xs except j2 = SSR 1All Xs2 - SSR 1All Xs except j2	
(14.9)
Contribution of Variable X1, Given That X2 has Been Included
	
SSR1X1X22 = SSR1X1 and X22 - SSR1X22	
(14.10a)
Contribution of Variable X2, Given That X1 has Been Included
	
SSR1X2X12 = SSR1X1 and X22 - SSR1X12	
(14.10b)
The term SSR1X22 represents the sum of squares due to regression for a model that in-
cludes only the independent variable X2 (promotional expenditures). Similarly, SSR1X12 
­represents the sum of squares due to regression for a model that includes only the independent 
variable X1 (price). Figures 14.7 and 14.8 present results for these two models.
F i g u r e  1 4 . 7
Excel and Minitab results for the simple linear regression model of sales with promotional expenditures, SSR(X2)
F i g u r e  1 4 . 8
Excel and Minitab results for the simple linear regression model of sales with price, SSR(X1)

588	
Chapter 14  Introduction to Multiple Regression 
From Figure 14.7, SSR1X22 = 14,915,814.10 and from Figure 14.2 on page 574, SSR  
(X1 and X2) = 39,472,730.77. Then, using Equation (14.10a),
 SSR1X1X22 = SSR1X1 and X22 - SSR1X22
 = 39,472,730.77 - 14,915,814.10
 = 24,556,916.67
To determine whether X1 significantly improves the model after X2 has been included, you  
divide the regression sum of squares into two component parts, as shown in Table 14.3.
T a b l e  1 4 . 3
ANOVA Table Dividing 
the Regression Sum of 
Squares into Components 
to Determine the 
Contribution of  
Variable X1
Source
Degrees of  
Freedom
Sum of  
Squares
Mean Square  
(Variance)
F
Regression
2
39,472,730.77
19,736,365.39
e
X2
X1X2
f
e1
1f
e14,915,814.10
24,556,916.67f
24,556,916.67
60.32
Error
31
12,620,946.67
   407,127.31
Total
33
52,093,677.44
The null and alternative hypotheses to test for the contribution of X1 to the model are:
H0: Variable X1 does not significantly improve the model after variable X2 has been included.
H1: Variable X1 significantly improves the model after variable X2 has been included.
Equation (14.11) defines the partial F test statistic for testing the contribution of an inde-
pendent variable.
Partial F Test Statistic
	
FSTAT =
SSR1Xj All Xs except j2
MSE
	
(14.11)
The partial F test statistic follows an F distribution with 1 and n - k - 1 degrees of freedom.
From Table 14.3,
FSTAT = 24,556,916.67
407,127.31
= 60.32
The partial FSTAT test statistic has 1 and n - k - 1 = 34 - 2 - 1 = 31 degrees of freedom. 
Using a level of significance of 0.05, the critical value from Table E.5 is approximately 4.17 (see 
Figure 14.9).
F i g u r e  1 4 . 9
Testing for the 
contribution of a 
regression coefficient 
to a multiple regression 
model at the 0.05 level 
of significance, with 
1 and 31 degrees of 
freedom
0
4.17
F
Region of
Rejection
Region of
Nonrejection
Critical
Value

	
14.5  Testing Portions of the Multiple Regression Model	
589
Because the computed partial FSTAT test statistic (60.32) is greater than this critical F value 
(4.17), you reject H0. You conclude that the addition of variable X1 (price) significantly im-
proves a regression model that already contains variable X2 (promotional expenditures).
To evaluate the contribution of variable X2 (promotional expenditures) to a model in  
which variable X1 (price) has been included, you need to use Equation (14.10b). First, from 
Figure 14.8 on page 587, observe that SSR1X12 = 28,153,486.15. Second, from Table 14.3, 
observe that SSR(X1 and X2) = 39,472,730.77. Then, using Equation (14.10b) on page 587,
SSR1X2X12 = 39,472,730.77 - 28,153,486.15 = 11,319,244.62
To determine whether X2 significantly improves a model after X1 has been included, you can 
divide the regression sum of squares into two component parts, as shown in Table 14.4.
The null and alternative hypotheses to test for the contribution of X2 to the model are:
H0: Variable X2 does not significantly improve the model after variable X1 has been included.
H1: Variable X2 significantly improves the model after variable X1 has been included.
Using Equation (14.11) and Table 14.4,
FSTAT = 11,319,244.62
407,127.31
= 27.80
In Figure 14.9, you can see that, using a 0.05 level of significance, the critical value of F, 
with 1 and 31 degrees of freedom, is approximately 4.17. Because the computed partial FSTAT 
test statistic (27.80) is greater than this critical value (4.17), you reject H0. You can conclude 
that the addition of variable X2 (promotional expenditures) significantly improves the multiple 
regression model already containing X1 (price).
Thus, by testing for the contribution of each independent variable after the other indepen-
dent variable has been included in the model, you determine that each of the two independent 
variables significantly improves the model. Therefore, the multiple regression model should 
include both price, X1, and promotional expenditures, X2.
The partial F test statistic developed in this section and the t test statistic of Equation (14.7)  
on page 583 are both used to determine the contribution of an independent variable to a multi-
ple regression model. The hypothesis tests associated with these two statistics always result in 
the same decision (i.e., the p-values are identical). The tSTAT test statistics for the OmniPower 
regression model are -7.7664 and +5.2728, and the corresponding FSTAT test statistics are 
60.32 and 27.80. Equation (14.12) states this relationship between t and F.1
1This relationship holds only when 
the FSTAT statistic has 1 degree of 
freedom in the numerator.
Relationship Between a t Statistic and an F Statistic
	
t2
STAT = FSTAT	
(14.12)
Source
Degrees of  
Freedom
Sum of  
Squares
Mean Square  
(Variance)
F
Regression
2
39,472,730.77
19,736,365.39
e
X1
X2X1
f
e1
1f
e28,153,486.15
11,319,244.62f
11,319,244.62
27.80
Error
31
12,620,946.67
407,127.31
Total
33
52,093,677.44
T a b l e  1 4 . 4
ANOVA Table Dividing 
the Regression 
Sum of Squares 
into Components 
to Determine the 
Contribution of 
Variable X2

590	
Chapter 14  Introduction to Multiple Regression 
Coefficients of Partial Determination
Recall from Section 14.2 that the coefficient of multiple determination, r 2, measures the pro-
portion of the variation in Y that is explained by variation in the independent variables. The 
coefficients of partial determination (r 2
Y1.2 and r 2
Y2.1) measure the proportion of the variation 
in the dependent variable that is explained by each independent variable while controlling for, 
or holding constant, the other independent variable. Equation (14.13) defines the coefficients 
of partial determination for a multiple regression model with two independent variables.
Student Tip
The coefficients of partial 
determination measure 
the proportion of the 
variation in the dependent 
variable explained by 
a specific independent 
variable, holding the 
other independent 
variables constant. They 
are different from the 
coefficient of multiple 
determination that 
measures the proportion 
of the variation in the 
dependent variable 
explained by the entire 
set of independent 
variables included in the 
model.
Coefficients of Partial Determination for a Multiple 
Regression Model Containing Two Independent Variables
	
r 2
Y1.2 =
SSR1X1X22
SST - SSR1X1 and X22 + SSR1X1X22	
(14.13a)
and
	
r 2
Y2.1 =
SSR1X2X12
SST - SSR1X1 and X22 + SSR1X2X12	
(14.13b)
	
where
SSR1X1X22 = sum of squares of the contribution of variable X1 to the regression 
model, given that variable X2 has been included in the model
SST = total sum of squares for Y
SSR1X1 and X22 = regression sum of squares when variables X1 and X2 are both 
included in the multiple regression model
SSR1X2X12 = sum of squares of the contribution of variable X2 to the regression 
model, given that variable X1 has been included in the model
For the OmniPower sales example,
 r 2
Y1.2 =
24,556,916.67
52,093,677.44 - 39,472,730.77 + 24,556,916.67
 = 0.6605
 r 2
Y2.1 =
11,319,244.62
52,093,677.44 - 39,472,730.77 + 11,319,244.62
 = 0.4728
The coefficient of partial determination, r 2
Y1.2, of variable Y with X1 while holding X2 constant 
is 0.6605. Thus, for a given (constant) amount of promotional expenditures, 66.05% of the 
variation in OmniPower sales is explained by the variation in the price. The coefficient of par-
tial determination, r 2
Y2.1, of variable Y with X2 while holding X1 constant is 0.4728. Thus, for 
a given (constant) price, 47.28% of the variation in sales of OmniPower bars is explained by 
variation in the amount of promotional expenditures.
Equation (14.14) defines the coefficient of partial determination for the jth variable in a 
multiple regression model containing several (k) independent variables.
Coefficient of Partial Determination for a Multiple Regression 
Model Containing K Independent Variables
	
r 2
Yj.1All variables except j2 =
SSR1Xj All Xs except j2
SST - SSR1All Xs2 + SSR1Xj All Xs except j2	
(14.14)

	
14.6  Using Dummy Variables and Interaction Terms in Regression Models 	
591
Problems for Section 14.5
Learning the Basics
14.31  The following is the ANOVA summary table for a multiple 
regression model with two independent variables:
If SSR1X12 = 45 and SSR1X22 = 25,
a.	 	determine whether there is a significant relationship between Y 
and each independent variable at the 0.05 level of significance.
b.	 	compute the coefficients of partial determination, r 2
Y1.2 and 
r 2
Y2.1, and interpret their meaning.
14.32  The following is the ANOVA summary table for a multiple 
regression model with two independent variables:
If SSR1X12 = 20 and SSR1X22 = 15,
a.	 	determine whether there is a significant relationship between Y 
and each independent variable at the 0.05 level of significance.
b.	 	compute the coefficients of partial determination, r 2
Y1.2 and 
r 2
Y2.1, and interpret their meaning.
Applying the Concepts
14.33  In Problem 14.5 on page 577, you used alcohol percentage 
and chlorides to predict wine quality (stored in  VinhoVerde ). Us-
ing the results from that problem,
a.	 	at the 0.05 level of significance, determine whether each in-
dependent variable makes a significant contribution to the re-
gression model. On the basis of these results, indicate the most 
appropriate regression model for this set of data.
b.	 	compute the coefficients of partial determination, r 2
Y1.2 and 
r 2
Y2.1, and interpret their meaning.
14.34  In Problem 14.4 on page 576, you used sales 
and number of orders to predict distribution costs at  
a mail-order catalog business (stored in  WareCost ). Using the 
results from that problem,
a.	 	at the 0.05 level of significance, determine whether each in-
dependent variable makes a significant contribution to the re-
gression model. On the basis of these results, indicate the most 
appropriate regression model for this set of data.
b.	 	compute the coefficients of partial determination, r 2
Y1.2 and 
r 2
Y2.1, and interpret their meaning.
14.35  In Problem 14.7 on page 577, you used the total staff pres-
ent and remote hours to predict standby hours (stored in  Standby ). 
Using the results from that problem,
a.	 	at the 0.05 level of significance, determine whether each in-
dependent variable makes a significant contribution to the re-
gression model. On the basis of these results, indicate the most 
appropriate regression model for this set of data.
b.	 	compute the coefficients of partial determination, r 2
Y1.2 and 
r 2
Y2.1, and interpret their meaning.
14.36  In Problem 14.6 on page 577, you used radio advertising 
and newspaper advertising to predict sales (stored in  Advertise ). 
Using the results from that problem,
a.	 	at the 0.05 level of significance, determine whether each in-
dependent variable makes a significant contribution to the re-
gression model. On the basis of these results, indicate the most 
appropriate regression model for this set of data.
b.	 	compute the coefficients of partial determination, r 2
Y1.2 and 
r 2
Y2.1, and interpret their meaning.
14.37  In Problem 14.8 on page 577, you used land area of a 
property and age of a house to predict the fair market value (stored 
in  GlenCove ). Using the results from that problem,
a.	 	at the 0.05 level of significance, determine whether each in-
dependent variable makes a significant contribution to the re-
gression model. On the basis of these results, indicate the most 
appropriate regression model for this set of data.
b.	 	compute the coefficients of partial determination, r 2
Y1.2 and 
r 2
Y2.1, and interpret their meaning.
Source
Degrees of   
  Freedom
Sum of 
Squares
Mean 
Squares
F
Regression
2
60
Error
18
120
Total
20
180
Source
Degrees of 
Freedom
Sum of 
Squares
Mean 
Squares
F
Regression
2
30
Error
10
120
Total
12
150
SELF 
Test 
14.6  Using Dummy Variables and Interaction Terms  
in Regression Models
The multiple regression models discussed in Sections 14.1 through 14.5 assumed that each 
independent variable is a numerical variable. For example, in Section 14.1, you used price and 
promotional expenditures, two numerical independent variables, to predict the monthly sales of 
OmniPower nutrition bars. However, for some models, you need to include the effect of a cate-
gorical independent variable. For example, to predict the monthly sales of the OmniPower bars, 
you might include the categorical variable end-cap location in the model to explore the possible 
effect on sales caused by displaying the OmniPower bars in the two different end-cap display 
locations, produce or beverage, used in the North Fork Beverages scenario in Chapter 10.

592	
Chapter 14  Introduction to Multiple Regression 
Dummy Variables
You use a dummy variable to include a categorical independent variable in a regression 
model. A dummy variable Xd recodes the categories of a categorical variable using the numeric 
values 0 and 1. In the special case of a categorical independent variable that has only two cate-
gories, you define one dummy variable, Xd, and use the values 0 and 1 to represent the two cat-
egories. For example, for the categorical variable end-cap location discussed in the Chapter 10  
Using Statistics scenario, the dummy variable, Xd, would have these values:
 Xd = 0 if the observation is in first category 1produce end@cap2
 Xd = 1 if the observation is in second category 1beverage end@cap2
To illustrate using dummy variables in regression, consider the business problem that in-
volves developing a model for predicting the assessed value ($thousands) of houses in Silver 
Spring, Maryland, based on house size (in thousands of square feet) and whether the house has 
a fireplace. To include the categorical variable for the presence of a fireplace, the dummy vari-
able X2 is defined as
 X2 = 0 if the house does not have a fireplace
 X2 = 1 if the house has a fireplace
Assuming that the slope of assessed value with the size of the house is the same for houses 
that have and do not have a fireplace, the multiple regression model is
Yi = b0 + b1X1i + b2X2i + ei
where
Yi = assessed value, in thousands of dollars, for house i
b0 = Y intercept
X1i = house size, in thousands of square feet, for house i
b1 = slope of assessed value with house size, holding constant the presence or 
absence of a fireplace
X2i = dummy variable that represents the absence or presence of a fireplace for house i
b2 = net effect of the presence of a fireplace on assessed value, holding constant the 
house size
ei = random error in Y for house i
Figure 14.10 presents the regression results for this model, using a sample of 30 Silver Spring 
houses listed for sale that was extracted from trulia.com and stored in  SilverSpring . In these results, 
the dummy variable X2 is labeled as FireplaceCoded (Excel) or Fireplace Coded (Minitab).
F i g u r e  1 4 . 1 0
Excel and Minitab results for the regression model that includes size of house and presence of fireplace

	
14.6  Using Dummy Variables and Interaction Terms in Regression Models 	
593
From Figure 14.10, the regression equation is
Yni = 269.4185 + 49.8215X1i + 12.1623X2i
For houses without a fireplace, you substitute X2 = 0 into the regression equation:
 Yni = 269.4185 + 49.8215X1i + 12.1623X2i
 = 269.4185 + 49.8215X1i + 12.1623102
 = 269.4185 + 49.8215X1i
For houses with a fireplace, you substitute X2 = 1 into the regression equation:
 Yni = 269.4185 + 49.8215X1i + 12.1623X2i
 = 269.4185 + 49.8215X1i + 12.1623112
 = 281.5807 + 49.8215X1i
In this model, the regression coefficients are interpreted as follows:
	 • Holding constant whether a house has a fireplace, for each increase of 1.0 thousand 
square feet in house size, the predicted mean assessed value is estimated to increase by 
49.8215 thousand dollars (i.e., $49,821.50).
 • Holding constant the house size, the presence of a fireplace is estimated to increase the 
predicted mean assessed value of the house by 12.1623 thousand dollars (i.e., $12,162.30).
In Figure 14.10, the tSTAT test statistic for the slope of house size with assessed value is 3.5253, 
and the p-value is 0.015; the tSTAT test statistic for presence of a fireplace is 0.4499, and the p-value 
is 0.6564. Thus, using the 0.05 level of significance, since 0.0015 6 0.05, the size of the house 
makes a significant contribution to the model. However, since 0.6564 7 0.05, the presence of a 
fireplace does not make a significant contribution to the model. In addition, from Figure 14.10, 
observe that the coefficient of multiple determination indicates that 33.23% of the variation in as-
sessed value is explained by variation in house size and whether the house has a fireplace. Thus, the 
variable fireplace does not make a significant contribution and should not be included in the model.
In some situations, the categorical independent variable has more than two categories. When 
this occurs, two or more dummy variables are needed. Example 14.3 illustrates such a situation.
Student Tip
Remember that an 
independent variable 
does not always make a 
significant contribution to 
a regression model.
Example 14.3
Modeling a Three-
Level Categorical 
Variable
Define a multiple regression model using sales as the dependent variable and package design 
and price as independent variables. Package design is a three-level categorical variable with 
designs A, B, or C.
Solution  To model the three-level categorical variable package design, two dummy variables, 
X1 and X2, are needed:
 X1i = 1 if package design A is used in observation i; 0 otherwise
 X2i = 1 if package design B is used in observation i; 0 otherwise
Thus, if observation i uses package design A, then X1i = 1 and X2i = 0; if observation i uses 
package design B, then X1i = 0 and X2i = 1; and if observation i uses package design C, then 
X1i = X2i = 0. Thus, package design C becomes the baseline category to which the effect of 
package design A and package design B is compared. A third independent variable is used for price:
X3i = price for observation i
Thus, the regression model for this example is
Yi = b0 + b1X1i + b2X2i + b3X3i + ei
(continued)

594	
Chapter 14  Introduction to Multiple Regression 
where
Yi = sales for observation i
b0 = Y intercept
b1 = difference between the predicted sales of design A and the predicted sales of design 
C, holding price constant
b2 = difference between the predicted sales of design B and the predicted sales of design 
C, holding price constant
b3 = slope of sales with price, holding the package design constant
ei = random error in Y for observation i
Interactions
In the regression models discussed so far, the effect an independent variable has on the depen-
dent variable has been assumed to be independent of the other independent variables in the 
model. An interaction occurs if the effect of an independent variable on the dependent variable 
changes according to the value of a second independent variable. For example, it is possible 
that advertising will have a large effect on the sales of a product when the price of a product is 
low. However, if the price of the product is too high, increases in advertising will not dramati-
cally change sales. In this case, price and advertising are said to interact. In other words, you 
cannot make general statements about the effect of advertising on sales. The effect that adver-
tising has on sales is dependent on the price. You use an interaction term (sometimes referred 
to as a cross-product term) to model an interaction effect in a regression model.
To illustrate the concept of interaction and use of an interaction term, return to the example 
concerning the assessed values of homes discussed on pages 592–593. In the regression model, 
you assumed that the effect that house size has on the assessed value is independent of whether the 
house has a fireplace. In other words, you assumed that the slope of assessed value with house size 
is the same for all houses, regardless of whether the house contains a fireplace. If these two slopes 
are different, an interaction exists between the house size and the presence or absence of a fireplace.
To evaluate whether an interaction exists, you first define an interaction term that is the 
product of the independent variable X1 (house size) and the dummy variable X2 (Fireplace-
Coded). You then test whether this interaction variable makes a significant contribution to the 
regression model. If the interaction is significant, you cannot use the original model for predic-
tion. For these data you define the following:
X3 = X1 * X2
Figure 14.11 presents regression results for the model that includes the house size, X1, the pres-
ence of a fireplace, X2, and the interaction of X1 and X2 (defined as X3 and labeled Size*Fireplace).
F i g u r e  1 4 . 1 1
Excel and Minitab results for the regression model that includes house size, presence of fireplace, and interaction of 
house size and fireplace

	
14.6  Using Dummy Variables and Interaction Terms in Regression Models 	
595
To test for the existence of an interaction, you use the null hypothesis:
H0: b3 = 0
versus the alternative hypothesis:
H1: b3 ≠0.
In Figure 14.11, the tSTAT test statistic for the interaction of size and fireplace is -0.7474. 
Because tSTAT = -0.7474 7 -2.201 or the p-value = 0.4615 7 0.05, you do not reject 
the null hypothesis. Therefore, the interaction does not make a significant contribution to the 
model, given that house size and presence of a fireplace are already included. You can con-
clude that the slope of assessed value with size is the same for houses with fireplaces and 
houses without fireplaces.
Regression models can have several numerical independent variables along with a dummy 
variable. Example 14.4 illustrates a regression model in which there are two numerical inde-
pendent variables and a categorical independent variable.
Student Tip
It is possible that the 
interaction between two 
independent variables 
will be significant even 
though one of the 
independent variables is 
not significant.
Example 14.4
Studying a Regres-
sion Model That 
Contains a Dummy 
Variable
The business problem facing a real estate developer involves predicting heating oil consump-
tion in single-family houses. The independent variables considered are atmospheric tempera-
ture 1°F2, X1, and the amount of attic insulation (inches), X2. Data are collected from a sample 
of 15 single-family houses. Of the 15 houses selected, houses 1, 4, 6, 7, 8, 10, and 12 are 
ranch-style houses. The data are organized and stored in  HeatingOil . Develop and analyze an 
appropriate regression model, using these three independent variables X1, X2, and X3 (where 
X3 is the dummy variable for ranch-style houses).
Solution  Define X3, a dummy variable for ranch-style house, as follows:
 X3 = 0 if the style is not ranch
 X3 = 1 if the style is ranch
Assuming that the slope between heating oil consumption and atmospheric temperature, X1, 
and between heating oil consumption and the amount of attic insulation, X2, is the same for 
both styles of houses, the regression model is
Yi = b0 + b1X1i + b2X2i + b3X3i + ei
where
Yi = monthly heating oil consumption, in gallons, for house i
b0 = Y intercept
b1 = slope of heating oil consumption with atmospheric temperature, holding constant the 
effect of attic insulation and the style of the house
b2 = slope of heating oil consumption with attic insulation, holding constant the effect of 
atmospheric temperature and the style of the house
b3 = incremental effect of the presence of a ranch-style house, holding constant the effect 
of atmospheric temperature and attic insulation
ei = random error in Y for house i
Figure 14.12 on page 596 presents results for this regression model.
(continued)

596	
Chapter 14  Introduction to Multiple Regression 
F i g u r e  1 4 . 1 2
Excel and Minitab results for the regression model that includes temperature, insulation, and style for the 
heating oil data
From the results in Figure 14.12, the regression equation is
Yni = 592.5401 - 5.5251X1i - 21.3761X2i - 38.9727X3i
For houses that are not ranch style, because X3 = 0, the regression equation reduces to
Yni = 592.5401 - 5.5251X1i - 21.3761X2i
For houses that are ranch style, because X3 = 1, the regression equation reduces to
Yni = 553.5674 - 5.5251X1i - 21.3761X2i
In this model, the regression coefficients are interpreted as follows:
 • Holding constant the attic insulation and the house style, for each additional 1°F increase 
in atmospheric temperature, you estimate that the predicted heating oil consumption 
decreases by 5.5251 gallons.
 • Holding constant the atmospheric temperature and the house style, for each additional 
1-inch increase in attic insulation, you estimate that the predicted heating oil consump-
tion decreases by 21.3761 gallons.
 • b3 measures the effect on oil consumption of having a ranch-style house 1X3 = 12 com-
pared with having a house that is not ranch style 1X3 = 02. Thus, with atmospheric 
temperature and attic insulation held constant, you estimate that the predicted heating oil 
consumption is 38.9727 gallons less for a ranch-style house than for a house that is not 
ranch style.
The three tSTAT test statistics representing the slopes for temperature, insulation, and ranch 
style are -27.0267, -14.7623, and -4.6627. Each of the corresponding p-values is extremely 
small (less than 0.001). Thus, each of the three variables makes a significant contribution to the 
model. In addition, the coefficient of multiple determination indicates that 98.84% of the varia-
tion in oil usage is explained by variation in temperature, insulation, and whether the house is 
ranch style.
Before you can use the model in Example 14.4, you need to determine whether the inde-
pendent variables interact with each other. In Example 14.5, three interaction terms are added 
to the model.

	
14.6  Using Dummy Variables and Interaction Terms in Regression Models 	
597
Example 14.5
Evaluating a  
Regression Model 
with Several  
Interactions
For the data of Example 14.4, determine whether adding the interaction terms makes a signifi-
cant contribution to the regression model.
Solution  To evaluate possible interactions between the independent variables, three inter-
action terms are constructed as follows: X4 = X1 * X2, X5 = X1 * X3, and X6 = X2 * X3. 
The regression model is now
Yi = b0 + b1X1i + b2X2i + b3X3i + b4X4i + b5X5i + b6X6i + ei
where X1 is temperature, X2 is insulation, X3 is the dummy variable ranch style, X4 is the in-
teraction between temperature and insulation, X5 is the interaction between temperature and 
ranch style, and X6 is the interaction between insulation and ranch style. Figure 14.13 presents 
the results for this regression model.
F i g u r e  1 4 . 1 3
Excel and Minitab results for the regression model that includes temperature, X1; insulation, X2; the dummy 
variable ranch-style, X3; the interaction of temperature and insulation, X4; the interaction of temperature and 
ranch-style, X5; and the interaction of insulation and ranch-style, X6
To test whether the three interactions significantly improve the regression model, you use 
the partial F test. The null and alternative hypotheses are
 H0 : b4 = b5 = b6 = 0 1there are no interactions among X1, X2, and X3.2
 H1: b4 ≠0 and>or b5 ≠0 and>or b6 ≠0 1X1 interacts with X2,
and>or X1 interacts with X3, and>or X2 interacts with X3.2
From Figure 14.13,
SSR1X1, X2, X3, X4, X5, X62 = 234,510.5818 with 6 degrees of freedom
and from Figure 14.12 on page 596, SSR 1X1, X2, X32 = 233,406.9094 with 3 degrees of free-
dom. Thus,
SSR1X1, X2, X3, X4, X5, X62 - SSR1X1, X2, X32 = 234,510.5818 - 233,406.9094 = 1,103.6724
The difference in degrees of freedom is 6 - 3 = 3.
(continued)

598	
Chapter 14  Introduction to Multiple Regression 
To use the partial F test for the simultaneous contribution of three variables to a model, 
you use an extension of Equation (14.11) on page 588.2 The partial FSTAT test statistic is
FSTAT = 3SSR1X1, X2, X3, X4, X5, X62 - SSR1X1, X2, X324>3
MSE1X1, X2, X3, X4, X5, X62
= 1,103.6724>3
203.0809
= 1.8115
You compare the computed FSTAT test statistic to the critical F value for 3 and 8 degrees 
of freedom. Using a level of significance of 0.05, the critical F value from Table E.5 is 4.07. 
Because FSTAT = 1.8115 6 4.07, you conclude that the interactions do not make a significant 
contribution to the model, given that the model already includes temperature, X1; insulation, 
X2; and whether the house is ranch style, X3. Therefore, the multiple regression model using 
X1, X2, and X3 but no interaction terms is the better model. If you rejected this null hypothesis, 
you would then test the contribution of each interaction separately in order to determine which 
interaction terms to include in the model.
2In general, if a model has several 
independent variables and you 
want to test whether additional 
independent variables contribute 
to the model, the numerator of the 
F test is SSR (for all independent 
variables) minus SSR (for the ini-
tial set of variables) divided by 
the number of independent vari-
ables whose contribution is being 
tested.
Problems for Section 14.6
Learning the Basics
14.38  Answer the following questions based on the regression 
equation: 
Wagei = 200 + 40 experiencei + 70 femalei + 20 educationi
a.	 	Which one do you think can be a dummy variable? Why?
b.	 	What is the predicted wage of a female employee if she has 5 
years of experience and education?
c.	 	Assume that all female employees have a master’s degree and 
none of the male employees have a formal education degree. In 
this scenario, do you think that there are any chances of interac-
tion? State the reason.
Applying the Concepts
14.39  The chair of the accounting department plans to develop a 
regression model to predict the grade point average in accounting 
for those students who are graduating and have completed the ac-
counting major, based on a student’s SAT score and whether the 
student received a grade of B or higher in the introductory statis-
tics course (0 = no and 1 = yes).
a.	 	Explain the steps involved in developing a regression model for 
these data. Be sure to indicate the particular models you need 
to evaluate and compare.
b.	 	Suppose the regression coefficient for the variable whether the 
student received a grade of B or higher in the introductory sta-
tistics course is +0.30. How do you interpret this result?
14.40  A real estate association in a suburban community would 
like to study the relationship between the size of a single-family 
house (as measured by the number of rooms) and the selling price 
of the house (in $thousands). Two different neighborhoods are in-
cluded in the study, one on the east side of the community 1=02 
and the other on the west side 1=12. A random sample of 20 
houses was selected, with the results stored in  Neighbor . For (a) 
through (k), do not include an interaction term.
a.	 	State the multiple regression equation that predicts the selling 
price, based on the number of rooms and the neighborhood.
b.	 	Interpret the regression coefficients in (a).
c.	 	Predict the mean selling price for a house with nine rooms that 
is located in an east-side neighborhood. Construct a 95% con-
fidence interval estimate and a 95% prediction interval.
d.	 	Perform a residual analysis on the results and determine 
whether the regression assumptions are valid.
e.	 	Is there a significant relationship between selling price and the 
two independent variables (rooms and neighborhood) at the 
0.05 level of significance?
f.	 	At the 0.05 level of significance, determine whether each inde-
pendent variable makes a contribution to the regression model. 
Indicate the most appropriate regression model for this set of 
data.
g.	 	Construct and interpret a 95% confidence interval estimate of 
the population slope for the relationship between selling price 
and number of rooms.
h.	 	Construct and interpret a 95% confidence interval estimate of 
the population slope for the relationship between selling price 
and neighborhood.
i.	
	Compute and interpret the adjusted r 2.
j.	 	Compute the coefficients of partial determination and interpret 
their meaning.
k.	 	What assumption do you need to make about the slope of sell-
ing price with number of rooms?
l.	
	Add an interaction term to the model and, at the 0.05 level of 
significance, determine whether it makes a significant contri-
bution to the model.
m.	 	On the basis of the results of (f) and (l), which model is most 
appropriate? Explain.
n.	 What conclusions can the real estate association reach about 
the effect of the number of rooms and neighborhood on the 
selling price of homes?
14.41  In Problem 14.5 on page 577, you developed a multiple 
regression model to predict wine quality for red wines. Now, you 
wish to determine whether there is an effect on wine quality due to 
whether the wine is white (0) or red (1). These data are organized 
and stored in  VinhoVerde-Redandwhite . Develop a multiple re-
gression model to predict wine quality based on the percentage of 
alcohol and the type of wine.

	
14.6  Using Dummy Variables and Interaction Terms in Regression Models 	
599
For (a) through (m), do not include an interaction term.
a.	 	State the multiple regression equation that predicts wine qual-
ity based on the percentage of alcohol and the type of wine.
b.	 	Interpret the regression coefficients in (a).
c.	 	Predict the mean quality for a red wine that has 10% alcohol. 
Construct a 95% confidence interval estimate and a 95% pre-
diction interval.
d.	 	Perform a residual analysis on the results and determine 
whether the regression assumptions are valid.
e.	 	Is there a significant relationship between wine quality and the 
two independent variables (percentage of alcohol and the type 
of wine) at the 0.05 level of significance?
f.	 	At the 0.05 level of significance, determine whether each inde-
pendent variable makes a contribution to the regression model. 
Indicate the most appropriate regression model for this set of 
data.
g.	 	Construct and interpret 95% confidence interval estimates of 
the population slope for the relationship between wine quality 
and the percentage of alcohol and between wine quality and 
the type of wine.
h.	 	Compare the slope in (b) with the slope for the simple linear 
regression model of Problem 13.4 on page 528. Explain the 
difference in the results.
i.	
	Compute and interpret the meaning of the coefficient of mul-
tiple determination, r 2.
j.	 	Compute and interpret the adjusted r 2.
k.	 	Compare r 2 with the r 2 value computed in Problem 13.16 (a) 
on page 534.
l.	
	Compute the coefficients of partial determination and interpret 
their meaning.
m.	 	What assumption about the slope of type of wine with wine 
quality do you need to make in this problem?
n.	 	Add an interaction term to the model and, at the 0.05 level of 
significance, determine whether it makes a significant contri-
bution to the model.
o.	 	On the basis of the results of (f) and (n), which model is most 
appropriate? Explain.
p.	 What conclusions can you reach concerning the effect of alco-
hol percentage and type of wine on wine quality?
14.42  In mining engineering, holes are often drilled through 
rock, using drill bits. As a drill hole gets deeper, additional rods are 
added to the drill bit to enable additional drilling to take place. It 
is expected that drilling time increases with depth. This increased 
drilling time could be caused by several factors, including the 
mass of the drill rods that are strung together. The business prob-
lem relates to whether drilling is faster using dry drilling holes or 
wet drilling holes. Using dry drilling holes involves forcing com-
pressed air down the drill rods to flush the cuttings and drive the 
hammer. Using wet drilling holes involves forcing water rather 
than air down the hole. Data have been collected from a sample of 
50 drill holes that contains measurements of the time to drill each 
additional 5 feet (in minutes), the depth (in feet), and whether the 
hole was a dry drilling hole or a wet drilling hole. The data are 
organized and stored in  Drill . (Data extracted from R. Penner and 
D. G. Watts, “Mining Information,” The American Statistician, 
45, 1991, pp. 4–9.) Develop a model to predict additional drilling 
time, based on depth and type of drilling hole (dry or wet). For (a) 
through (k) do not include an interaction term.
a.	 	State the multiple regression equation.
b.	 	Interpret the regression coefficients in (a).
c.	 	Predict the mean additional drilling time for a dry drilling hole 
at a depth of 100 feet. Construct a 95% confidence interval 
estimate and a 95% prediction interval.
d.	 	Perform a residual analysis on the results and determine 
whether the regression assumptions are valid.
e.	 	Is there a significant relationship between additional drilling 
time and the two independent variables (depth and type of 
drilling hole) at the 0.05 level of significance?
f.	 	At the 0.05 level of significance, determine whether each inde- 
pendent variable makes a contribution to the regression 
model. Indicate the most appropriate regression model for this 
set of data.
g.	 	Construct a 95% confidence interval estimate of the popula-
tion slope for the relationship between additional drilling time 
and depth.
h.	 	Construct a 95% confidence interval estimate of the popula-
tion slope for the relationship between additional drilling time 
and the type of hole drilled.
i.	
	Compute and interpret the adjusted r 2.
j.	 	Compute the coefficients of partial determination and interpret 
their meaning.
k.	 	What assumption do you need to make about the slope of ad-
ditional drilling time with depth?
l.	
	Add an interaction term to the model and, at the 0.05 level of 
significance, determine whether it makes a significant contri-
bution to the model.
m.	 	On the basis of the results of (f) and (l), which model is most 
appropriate? Explain.
n.	 What conclusions can you reach concerning the effect of depth 
and type of drilling hole on drilling time?
14.43  The owner of a moving company typically has his most 
experienced manager predict the total number of labor hours that 
will be required to complete an upcoming move. This approach 
has proved useful in the past, but the owner has the business ob-
jective of developing a more accurate method of predicting labor 
hours. In a preliminary effort to provide a more accurate method, 
the owner has decided to use the number of cubic feet moved and 
whether there is an elevator in the apartment building as the inde-
pendent variables and has collected data for 36 moves in which the 
origin and destination were within the borough of Manhattan in 
New York City and the travel time was an insignificant portion of 
the hours worked. The data are organized and stored in  Moving . 
For (a) through (k), do not include an interaction term.
a.	 	State the multiple regression equation for predicting labor 
hours, using the number of cubic feet moved and whether 
there is an elevator.
b.	 	Interpret the regression coefficients in (a).
c.	 	Predict the mean labor hours for moving 500 cubic feet in an 
apartment building that has an elevator and construct a 95% 
confidence interval estimate and a 95% prediction interval.
d.	 	Perform a residual analysis on the results and determine 
whether the regression assumptions are valid.
e.	 	Is there a significant relationship between labor hours and 
the two independent variables (cubic feet moved and whether 
there is an elevator in the apartment building) at the 0.05 level 
of significance?
f.	 	At the 0.05 level of significance, determine whether each inde- 
pendent variable makes a contribution to the regression 
model. Indicate the most appropriate regression model for this 
set of data.

600	
Chapter 14  Introduction to Multiple Regression 
g.	 	Construct a 95% confidence interval estimate of the popula-
tion slope for the relationship between labor hours and cubic 
feet moved.
h.	 	Construct a 95% confidence interval estimate for the relation-
ship between labor hours and the presence of an elevator.
i.	
	Compute and interpret the adjusted r 2.
j.	 	Compute the coefficients of partial determination and interpret 
their meaning.
k.	 	What assumption do you need to make about the slope of 
labor hours with cubic feet moved?
l.	
	Add an interaction term to the model, and at the 0.05 level of 
significance, determine whether it makes a significant contri-
bution to the model.
m.	 	On the basis of the results of (f) and (l), which model is most 
appropriate? Explain.
n.	 What conclusions can you reach concerning the effect of the 
number of cubic feet moved and whether there is an elevator 
on labor hours?
14.44  In Problem 14.4 on page 576, you used sales 
and orders to predict distribution cost (stored in 
 WareCost ). Develop a regression model to predict distribution 
cost that includes sales, orders, and the interaction of sales and 
orders.
a.	 	At the 0.05 level of significance, is there evidence that the in-
teraction term makes a significant contribution to the model?
b.	 	Which regression model is more appropriate, the one used in 
(a) or the one used in Problem 14.4? Explain.
14.45  Zagat’s publishes restaurant ratings for various locations 
in the United States. The file  Restaurants  contains the Zagat rat-
ing for food, décor, service, and cost per person for a sample of 50 
restaurants located in a city and 50 restaurants located in a suburb. 
(Data extracted from Zagat Survey 2013, New York City Restau-
rants; and Zagat Survey 2012–2013, Long Island Restaurants.) 
Develop a regression model to predict the cost per person, based 
on a variable that represents the sum of the ratings for food, décor, 
and service and a dummy variable concerning location (city versus 
suburban). For (a) through (m), do not include an interaction term.
a.	 	State the multiple regression equation.
b.	 	Interpret the regression coefficients in (a).
c.	 	Predict the mean cost for a restaurant with a summated rating 
of 60 that is located in a city and construct a 95% confidence 
interval estimate and a 95% prediction interval.
d.	 	Perform a residual analysis on the results and determine 
whether the regression assumptions are satisfied.
e.	 	Is there a significant relationship between price and the two 
independent variables (summated rating and location) at the 
0.05 level of significance?
f.	 	At the 0.05 level of significance, determine whether each inde-
pendent variable makes a contribution to the regression model. 
Indicate the most appropriate regression model for this set of 
data.
g.	 	Construct a 95% confidence interval estimate of the popula-
tion slope for the relationship between cost and summated  
rating.
h.	 	Compare the slope in (b) with the slope for the simple linear 
regression model of Problem 13.5 on page 528. Explain the 
difference in the results.
i.	
	Compute and interpret the meaning of the coefficient of mul-
tiple determination.
j.	 	Compute and interpret the adjusted r 2.
k.	 	Compare r 2 with the r 2 value computed in Problem 13.17 (b) 
on page 534.
l.	
	Compute the coefficients of partial determination and interpret 
their meaning.
m.	 	What assumption about the slope of cost with summated rat-
ing do you need to make in this problem?
n.	 	Add an interaction term to the model and, at the 0.05 level of 
significance, determine whether it makes a significant contri-
bution to the model.
o.	 	On the basis of the results of (f) and (n), which model is most 
appropriate? Explain.
p.	 What conclusions can you reach about the effect of the sum-
mated rating and the location of the restaurant on the cost of a 
meal?
14.46  In Problem 14.6 on page 577, you used radio advertising 
and newspaper advertising to predict sales (stored in  Advertise ). 
Develop a regression model to predict sales that includes radio 
advertising, newspaper advertising, and the interaction of radio 
advertising and newspaper advertising.
a.	 	At the 0.05 level of significance, is there evidence that the in-
teraction term makes a significant contribution to the model?
b.	 	Which regression model is more appropriate, the one used in 
this problem or the one used in Problem 14.6? Explain.
14.47  In Problem 14.5 on page 577, the percentage of alcohol 
and chlorides were used to predict the quality of red wines (stored 
in  VinhoVerde ). Develop a regression model that includes the 
percentage of alcohol, the chlorides, and the interaction of the per-
centage of alcohol and the chlorides to predict wine quality.
a.	 	At the 0.05 level of significance, is there evidence that the in-
teraction term makes a significant contribution to the model?
b.	 	Which regression model is more appropriate, the one used in 
this problem or the one used in Problem 14.5? Explain.
14.48  In Problem 14.7 on page 577, you used total staff present 
and remote hours to predict standby hours (stored in  Standby ). 
Develop a regression model to predict standby hours that includes 
total staff present, remote hours, and the interaction of total staff 
present and remote hours.
a.	 	At the 0.05 level of significance, is there evidence that the in-
teraction term makes a significant contribution to the model?
b.	 	Which regression model is more appropriate, the one used in 
this problem or the one used in Problem 14.7? Explain.
14.49  The director of a training program for a large insurance 
company has the business objective of determining which training 
method is best for training underwriters. The three methods to be 
evaluated are classroom, online, and courseware app. The 30 train-
ees are divided into three randomly assigned groups of 10. Before 
the start of the training, each trainee is given a proficiency exam 
that measures mathematics and computer skills. At the end of the 
training, all students take the same end-of-training exam. The re-
sults are organized and stored in  Underwriting .
Develop a multiple regression model to predict the score on 
the end-of-training exam, based on the score on the proficiency 
exam and the method of training used. For (a) through (k), do not 
include an interaction term.
a.	 	State the multiple regression equation.
b.	 	Interpret the regression coefficients in (a).
SELF 
Test 

	
14.7  Logistic Regression	
601
c.	 	Predict the mean end-of-training exam score for a student with 
a proficiency exam score of 100 who had courseware app-
based training.
d.	 	Perform a residual analysis on your results and determine 
whether the regression assumptions are valid.
e.	 	Is there a significant relationship between the end-of-training 
exam score and the independent variables (proficiency score 
and training method) at the 0.05 level of significance?
f.	
	At the 0.05 level of significance, determine whether each inde-
pendent variable makes a contribution to the regression model. 
Indicate the most appropriate regression model for this set of data.
g.	 	Construct and interpret a 95% confidence interval estimate of 
the population slope for the relationship between the end-of-
training exam score and the proficiency exam score.
h.	 	Construct and interpret 95% confidence interval estimates of 
the population slope for the relationship between the end-of-
training exam score and type of training method.
i.	
	Compute and interpret the adjusted r 2.
j.	 	Compute the coefficients of partial determination and interpret 
their meaning.
k.	 	What assumption about the slope of proficiency score with 
end-of-training exam score do you need to make in this  
problem?
l.	
	Add interaction terms to the model and, at the 0.05 level of 
significance, determine whether any interaction terms make a 
significant contribution to the model.
m.	 	On the basis of the results of (f) and (l), which model is most 
appropriate? Explain.
14.7  Logistic Regression
The discussion of the simple linear regression model in Chapter 13 and the multiple regres-
sion models in Sections 14.1 through 14.6 only considered numerical dependent variables. 
However, in many applications, the dependent variable is a categorical variable that takes on 
one of only two possible values, such as a customer purchases a product or a customer does not 
purchase a product. Using a categorical dependent variable violates the normality assumption 
of the least-squares method and can also result in predicted Y values that are impossible.
An alternative approach to least-squares regression originally applied to survival data in 
the health sciences (see reference 5), logistic regression, enables you to use regression models 
to predict the probability of a particular categorical response for a given set of independent 
variables. The logistic regression model uses the odds ratio, which represents the probability  
of an event of interest compared with the probability of not having an event of interest.  
Equation (14.15) defines the odds ratio.
Odds Ratio
	
Odds ratio =
probability of an event of interest
1 - probability of an event of interest	
(14.15)
Using Equation (14.15), if the probability of an event of interest is 0.50, the odds ratio is
Odds ratio =
0.50
1 - 0.50 = 1.0, or 1 to 1
If the probability of an event of interest is 0.75, the odds ratio is
Odds ratio =
0.75
1 - 0.75 = 3.0, or 3 to 1
The logistic regression model is based on the natural logarithm of the odds ratio, ln(odds ratio).
Equation (14.16) on page 602 defines the logistic regression model for k independent  
variables.

602	
Chapter 14  Introduction to Multiple Regression 
In Sections 13.2 and 14.1, the method of least squares was used to develop a regression 
equation. In logistic regression, a mathematical method called maximum likelihood estimation 
is typically used to develop a regression equation to predict the natural logarithm of this odds 
ratio. Equation (14.17) defines the logistic regression equation.
Logistic Regression Model
	
ln1Odds ratio2 = b0 + b1X1i + b2X2i + g + bkXki + ei	
(14.16)
	
where
 k = number of independent variables in the model
 ei = random error in observation i
Student Tip
ln is the symbol used 
for natural logarithms, 
also known as base 
e logarithms. ln(x ) 
is the logarithm of x 
having base e, where 
e ≅2.718282.
Logistic Regression Equation
ln1estimated odds ratio2 = b0 + b1X1i + b2X2i + g + bkXki	
(14.17)
Once you have determined the logistic regression equation, you use Equation (14.18) to 
compute the estimated odds ratio.
Estimated Odds Ratio
estimated odds ratio = eln1estimated odds ratio2	
(14.18)
Once you have computed the estimated odds ratio, you use Equation (14.19) to compute 
the estimated probability of an event of interest.
Estimated Probability of an Event of Interest
estimated probability of an event of interest =
estimated odds ratio
1 + estimated odds ratio	
(14.19)
To illustrate the use of logistic regression, consider the case of the sales and marketing 
manager for the credit card division of a major financial company. The manager wants to con-
duct a campaign to persuade existing holders of the bank’s standard credit card to upgrade, for 
a nominal annual fee, to the bank’s platinum card. The manager wonders, “Which of the exist-
ing standard credit cardholders should we target for this campaign?”
The manager has access to the results from a sample of 30 cardholders who were tar-
geted during a pilot campaign last year. These results have been organized as three variables  
and stored in  CardStudy . The three variables are cardholder upgraded to a premium card, 
Y (0 = no, 1 = yes); and two independent variables: prior year’s credit card purchases (in 
$thousands), X1; and cardholder ordered additional credit cards for other authorized users, X2 
(0 = no, 1 = yes). Figure 14.14 presents the Excel and Minitab results for the logistic regres-
sion model using these data.

	
14.7  Logistic Regression	
603
In this model, the regression coefficients are interpreted as follows:
 • The regression constant, b0, is –6.9394. This means that for a credit cardholder who did 
not charge any purchases last year and who does not have additional cards, the estimated 
natural logarithm of the odds ratio of purchasing the premium card is –6.9394.
 • The regression coefficient, b1, is 0.1395. This means that holding constant the effect of 
whether the credit cardholder has additional cards for members of the household, for 
each increase of $1,000 in annual credit card spending using the company’s card, the 
estimated natural logarithm of the odds ratio of purchasing the premium card increases 
by 0.1395. Therefore, cardholders who charged more in the previous year are more 
likely to upgrade to a premium card.
 • The regression coefficient, b2, is 2.7743. This means that holding constant the annual 
credit card spending, the estimated natural logarithm of the odds ratio of purchasing 
the premium card increases by 2.7743 for a credit cardholder who has additional cards 
for members of the household compared with one who does not have additional cards. 
Therefore, cardholders possessing additional cards for other members of the household 
are much more likely to upgrade to a premium card.
The regression coefficients suggest that the credit card company should develop a market-
ing campaign that targets cardholders who tend to charge large amounts to their cards, and to 
households that possess more than one card.
As is the case with least-squares regression models, a main purpose of performing logistic 
regression analysis is to provide predictions of a dependent variable. For example, consider a 
cardholder who charged $36,000 last year and possesses additional cards for members of the 
household. What is the probability the cardholder will upgrade to the premium card during the 
marketing campaign? Using X1 = 36, X2 = 1, Equation (14.17) on page 602, and the results 
displayed in Figure 14.14 above,
F i g u r e  1 4 . 1 4
Excel and Minitab 
logistic regression results 
for the credit card pilot 
study data
 ln1estimated odds of purchasing versus not purchasing2 = -6.9394 + 10.139521362 + 12.77432112
 = 0.8569
Then, using Equation (14.18) on page 602,
estimated odds ratio = e0.8569 = 2.3558
Therefore, the odds are 2.3558 to 1 that a credit cardholder who spent $36,000 last year and has 
additional cards will purchase the premium card during the campaign. Using Equation (14.19)  
on page 602, you can convert this odds ratio to a probability:
 estimated probability of purchasing premium card =
2.3558
1 + 2.3558
 = 0.702
For other problems, Excel 
and Minitab results may vary 
slightly due to the limitations 
of the Excel methods used  
in the Section EG14.7  
instructions.

604	
Chapter 14  Introduction to Multiple Regression 
Thus, the estimated probability is 0.702 that a credit cardholder who spent $36,000 last year 
and has additional cards will purchase the premium card during the campaign. In other words, 
you predict 70.2% of such individuals will purchase the premium card.
Now that you have used the logistic regression model for prediction, you need to deter-
mine whether or not the model is a good-fitting model. The deviance statistic is frequently 
used to determine whether the current model provides a good fit to the data. This statistic mea-
sures the fit of the current model compared with a model that has as many parameters as there 
are data points (what is called a saturated model). The deviance statistic follows a chi-square 
distribution with n - k - 1 degrees of freedom. The null and alternative hypotheses are
 H0: the model is a good@fitting model.
 H1: the model is not a good@fitting model.
When using the deviance statistic for logistic regression, the null hypothesis represents a good-
fitting model, which is the opposite of the null hypothesis when using the overall F test for the 
multiple regression model (see Section 14.2). Using the a level of significance, the decision 
rule is
 reject H0 if deviance 7 x2
a;
 otherwise, do not reject Ho.
The critical value for a x2 statistic with n - k - 1 = 30 - 2 - 1 = 27 degrees of freedom 
is 40.113 (see Table E.4). From Figure 14.14 on page 603, the deviance = 20.0769 6 40.113. 
Thus, you do not reject H0, and you conclude that there is insufficient evidence that the model 
is not a good-fitting one.
Now that you have concluded that the model is a good-fitting one, you need to evaluate 
whether each of the independent variables makes a significant contribution to the model in the 
presence of the others. As is the case with linear regression in Sections 13.7 and 14.4, the test sta-
tistic is based on the ratio of the regression coefficient to the standard error of the regression co-
efficient. In logistic regression, this ratio is defined by the Wald statistic, which approximately 
follows the normal distribution. From Figure 14.14, the Wald statistic (labeled Z) is 2.049 for X1 
and 2.3261 for X2. Each of these is greater than the critical value of +1.96 of the normal distribu-
tion at the 0.05 level of significance (the p-values are 0.0405 and 0.02). You can conclude that 
each of the two independent variables makes a contribution to the model in the presence of the 
other. Therefore, you should include both these independent variables in the model.
Problems for Section 14.7
Learning the Basics
14.50  Interpret the meaning of a slope coefficient equal to 2.2 in 
logistic regression.
14.51  Given an estimated odds ratio of 2.5, compute the esti-
mated probability of an event of interest.
14.52  Given an estimated odds ratio of 0.75, compute the esti-
mated probability of an event of interest.
14.53  Consider the following logistic regression equation:
ln1estimated odds ratio2 = 0.1 + 0.5X1i + 0.2X2i
a.	 	Interpret the meaning of the logistic regression coefficients.
b.	 	If X1 = 2 and X2 = 1.5, compute the estimated odds ratio and 
interpret its meaning.
c.	 	On the basis of the results of (b), compute the estimated prob-
ability of an event of interest.
Applying the Concepts
14.54  Refer to Figure 14.14 on page 603.
a.	 	Predict the probability that a cardholder who charged $36,000 
last year and does not have any additional credit cards for 
members of the household will purchase the platinum card dur-
ing the marketing campaign.
b.	 	Compare the results in (a) with those for a person with addi-
tional credit cards.
c.	 	Predict the probability that a cardholder who charged $18,000 
and does not have any additional credit cards for other autho-
rized users will purchase the platinum card during the market-
ing campaign.
d.	 	Compare the results of (a) and (c) and indicate what implica-
tions these results might have for the strategy for the marketing 
campaign.
Student Tip
Unlike other hypothesis 
tests, rejecting the null 
hypothesis here means 
that the model is not a 
good fit.

	
14.7  Logistic Regression	
605
14.55  A study was conducted to determine the factors in-
volved in the rate of participation of discharged cardiac patients 
in a rehabilitation program. Data were collected from 516 treated  
patients (data extracted from F. Van Der Meulen, T. Vermaat, and 
P. Williams, “Case Study: An Application of Logistic Regression 
in a Six Sigma Project in Health Care,” Quality Engineering, 2011, 
pp. 113–124). Among the variables used to predict participation 
(0 = no, 1 = yes) were the distance traveled to rehabilitation in 
kilometers, whether the person had a car (0 = no, 1 = yes), and 
the age of the person in years. The summarized data are:
a.	 	State the logistic regression model.
b.	 	Using the model in (a), predict the probability that a patient 
will participate in rehabilitation if he or she travels 20 km to 
rehabilitation, has a car, and is 65 years old.
c.	 	Using the model in (a), predict the probability that a patient 
will participate in rehabilitation if he or she travels 20 km to 
rehabilitation, does not have a car, and is 65 years old.
d.	 	Compare the results of (b) and (c).
e.	 	At the 0.05 level of significance, is there evidence that the dis-
tance traveled, whether the patient has a car, and the age of the 
patient each make a significant contribution to the model?
f.	 	What conclusions can you reach about the likelihood of a pa-
tient participating in the rehabilitation program?
14.56  Referring to Problem 14.41 on page 598, you have decided 
to analyze whether there are differences in fixed acidity, chlorides, 
and pH between white wines and red wines (0 = white 1 = red). 
Using the data stored in  RedandWhite ,
a.	 Develop a logistic regression model to predict whether the 
wine is red based on the fixed acidity, chlorides, and pH.
b.	 Explain the meaning of the regression coefficients in the model 
developed in (a).
c.	 Predict the probability that a wine is red if it has a fixed acidity 
of 7.0, chlorides of 0.04, and pH of 3.5.
d.	 At the 0.05 level of significance, is there evidence that the 
logistic regression model developed in (a) is a good fitting 
model?
e.	 At the 0.05 level of significance, is there evidence that fixed 
acidity, chlorides, and pH each make a significant contribution 
to the model?
f.	 What conclusions concerning the probability of a wine se-
lected being red can you reach?
14.57  Undergraduate students at Miami University in Oxford, 
Ohio, were surveyed in order to evaluate the effect of price on the 
purchase of a pizza from Pizza Hut. The students were asked to 
suppose that they were going to have a large two-topping pizza 
delivered to their residence. Then they were asked to select from 
either Pizza Hut or another pizzeria of their choice. The price they 
would have to pay to get a Pizza Hut pizza differed from survey to 
survey. For example, some surveys used the price $11.49. Other 
prices investigated were $8.49, $9.49, $10.49, $12.49, $13.49, and 
$14.49. The dependent variable for this study is whether or not a 
student will select Pizza Hut. Possible independent variables are 
the price of a Pizza Hut pizza and the gender of the student. The 
file  PizzaHut  contains responses from 220 students and includes 
these three variables:
Gender: 1 = male, 0 = female
Price: 8.49, 9.49, 10.49, 11.49, 12.49, 13.49, or 14.49
Purchase: 1 = the student selected Pizza Hut, 0 = the
student selected another pizzeria
a.	 	Develop a logistic regression model to predict the probability 
that a student selects Pizza Hut based on the price of the pizza. 
Is price an important indicator of purchase selection?
b.	 	Develop a logistic regression model to predict the probability 
that a student selects Pizza Hut based on the price of the pizza 
and the gender of the student. Is price an important indicator 
of purchase selection? Is gender an important indicator of pur-
chase selection?
c.	 	Compare the results from (a) and (b). Which model would you 
choose? Discuss.
d.	 	Using the model selected in (c), predict the probability that a 
student will select Pizza Hut if the price is $8.99.
e.	 	Using the model selected in (c), predict the probability that a 
student will select Pizza Hut if the price is $11.49.
f.	 	Using the model selected in (c), predict the probability that a 
student will select Pizza Hut if the price is $13.99.
14.58  An automotive insurance company wants to predict which 
filed stolen vehicle claims are fraudulent, based on the mean num-
ber of claims submitted per year by the policy holder and whether 
the policy is a new policy, that is, is one year old or less (coded as 
1 = yes, 0 = no). Data from a random sample of 98 automotive 
insurance claims, organized and stored in  InsuranceFraud , show 
that 49 are fraudulent (coded as 1) and 49 are not (coded as 0). 
(Data extracted from A. Gepp et al., “A Comparative Analysis of  
Decision Trees vis-à-vis Other Computational Data Mining  
Techniques in Automotive Insurance Fraud Detection,” Journal of 
Data Science, 10 (2012), pp. 537–561.)
a.	 Develop a logistic regression model to predict the probability 
of a fraudulent claim, based on the number of claims submitted 
per year by the policy holder and whether the policy is new.
b.	 Explain the meaning of the regression coefficients in the model 
in (a).
c.	 Predict the probability of a fraudulent claim given that the 
policy holder has submitted a mean of one claim per year and 
holds a new policy.
d.	 At the 0.05 level of significance, is there evidence that a logistic 
regression model that uses the mean number of claims submit-
ted per year by the policy holder and whether the policy is new 
to predict the probability of a fraudulent claim is a good fitting 
model?
e.	 At the 0.05 level of significance, is there evidence that the mean 
number of claims submitted per year by the policy holder and 
whether the policy is new each makes a significant contribution 
to the logistic model?
f.	 Develop a logistic regression model that includes only the 
number of claims submitted per year by the policy holder to 
predict the probability of a fraudulent claim.
g.	 Develop a logistic regression model that includes only whether 
the policy is new to predict a fraudulent claim.
h.	 Compare the models in (a), (f), and (g). Evaluate the differ-
ences among the models.
Estimate
Standard 
Error
Z Value
p-value
Intercept
5.7765
0.8619
6.702
0.0000
Distance
-0.0675
0.0111
-6.113
0.0000
Car
1.9369
0.2720
7.121
0.0000
Age
-0.0599
0.0119
-5.037
0.0000

606	
Chapter 14  Introduction to Multiple Regression 
14.59  A marketing manager wants to predict customers with 
the risk of churning (switching their service contracts to another 
company) based on the number of calls the customer makes to the 
company call center and the number of visits the customer makes 
to the local service center. Data from a random sample of 30 cus-
tomers, organized and stored in  Churn  show that 15 have churned 
(coded as 1) and 15 have not (coded as 0)
a.	 Develop a logistic regression model to predict the probability 
of churn, based on the number of calls the customer makes to 
the company call center and the number of visits the customer 
makes to the local service center.
b.	 Explain the meaning of the regression coefficients in the model 
in (a).
c.	 Predict the probability of churn for a customer who called the 
company call center 10 times and visited the local service cen-
ter once.
d.	 At the 0.05 level of significance, is there evidence that a logis-
tic regression model that uses the number of calls the customer 
makes to the company call center and the number of visits the cus-
tomer makes to the local service center is a good fitting model?
e.	 At the 0.05 level of significance, is there evidence that the num-
ber of calls the customer makes to the company call center and 
the number of visits the customer makes to the local service cen-
ter each make a significant contribution to the logistic model?
f.	 Develop a logistic regression model that includes only the 
number of calls the customer makes to the company call center 
to predict the probability of churn.
g.	 Develop a logistic regression model that includes only the 
number of visits the customer makes to the local service center 
to predict churn.
h.	 Compare the models in (a), (f), and (g). Evaluate the differ-
ences among the models.
14.8  Influence Analysis
In Sections 13.5 and 14.3, you used residual analysis to evaluate the regression assumptions. 
This section introduces several methods that measure the influence of individual values:
 • The hat matrix elements, hi
 • The Studentized deleted residuals, ti
 • Cook’s distance statistic, Di
Figure 14.15 presents these statistics for the OmniPower sales data, as computed by 
Minitab.
F i g u r e  1 4 . 1 5
Minitab worksheet 
containing computed 
values for the 
Studentized deleted 
residuals (labeled 
TRES1), the hat matrix 
elements, and Cook’s 
distance statistics for the 
OmniPower sales data

	
14.8  Influence Analysis	
607
The Hat Matrix Elements, hi
In Section 13.8, hi was defined for the simple linear regression model when constructing the 
confidence interval estimate of the mean response. For multiple regression models, the equa-
tion for calculating the hat matrix diagonal elements, hi, requires the use of matrix algebra 
and is beyond the scope of this text (see references 1, 2, and 6).
The hat matrix diagonal element for observation i, denoted hi, reflects the possible influ-
ence of Xi on the regression equation. If potentially influential observations are present, you 
may need to delete them from the model. In a regression model containing k independent vari-
ables, Hoaglin and Welsch (see reference 6) suggest the following decision rule:
If hi 7 21k + 12>n,
then Xi is an influential observation and is a candidate for removal from the model.
For the OmniPower sales data, because n = 34 and k = 2, you flag any hi value greater 
than 2(2 + 1)>34 = 0.1765. Referring to Figure 14.15, you see that none of the hi values are 
greater than 0.1429. Therefore, none of the observations are candidates for removal from the 
analysis.
The Studentized Deleted Residuals, ti 
Recall from Section 13.5 that a residual is the difference between the observed value of Y and 
the predicted value of Y [see Equation (13.14) on page 535]. Studentized residuals are the re-
siduals divided by the standard error of the estimate Syx and adjusted for the distance from X. 
The Studentized deleted residual, expressed as a t statistic in Equation (14.20), measures the 
difference of each Yi, from the value predicted by a model that includes all observations except 
observation i.
Studentized Deleted Residual
	
ti = eiA
n - k - 1
SSE(1 - hi) - e2
i
	
(14.20)
	
where
 ei = residual for observation i
 k = number of independent variables
 SSE = error sum of squares of the regression model fitted
 hi = hat matrix diagonal element for observation i
Hoaglin and Welsch (see reference 6) suggest that if ti 7 ta>2 or ti 6 ta>2 (using a level of 
significance of 0.10), the observed and predicted values are so different that observation i is 
highly influential on the regression equation and is a candidate for removal.
For the OmniPower sales data, n = 34 and k = 2. Thus, you flag any ti whose absolute 
value is greater than 1.6973 (see Table E.3). In Figure 14.15, t14 = -3.08402, t15 = 2.20612, 
and t20 = 2.27527 are highlighted. Thus, the 14th, 15th, and 20th observations may each have 
an adverse effect on the model. These observations were not previously flagged according to 
the hi criterion. Since hi and ti measure different aspects of influence, neither criterion is suffi-
cient by itself. When hi is small, ti may be large. When hi is large, ti may be moderate or small 
because the observed Yi, is consistent with the rest of the data.
Cook’s Distance Statistic, Di
Cook’s distance statistic, Di, based on both hi and the Studentized residual, is a third criterion 
for identifying influential observations. To decide whether an observation flagged by either the 
hi or ti criterion is unduly affecting the model, Cook and Weisberg (see reference 4) developed 
Cook’s Di statistic.

608	
Chapter 14  Introduction to Multiple Regression 
Cook’s Di Statistic
	
Di =
e2
i
k MSE
 c
hi
(1 - hi)2 d 	
(14.21)
	
where
 ei = residual for observation i
 k = number of independent variables
 MSE = mean square error of the regression model fitted
 hi = hat matrix diagonal element for observation i
Cook and Weisberg suggest that if Di 7 Fa (the critical value of the F distribution having 
k + 1 degrees of freedom in the numerator and n - k - 1 degrees of freedom in the denomi-
nator at a 0.50 level of significance), the observation is highly influential on the regression 
equation and is a candidate for removal.
Table 14.5 shows critical values for Cook’s Di statistic.
T a b l e  1 4 . 5
Selected Critical Values of F for Cook’s Di Statistic
A = 0.50
Numerator df = k + 1
Denominator 
df = n −k −1
2
3
4
5
6
7
8
9
10
12
15
20
10
.743
.845
.899
.932
.954
.971
.983
.992
1.00
1.01
1.02
1.03 
11
.739
.840
.893
.926
.948
.964
.977
.986
.994
1.01
1.02
1.03 
12
.735
.835
.888
.921
.943
.959
.972
.981
.989
1.00
1.01
1.02 
15
.726
.826
.878
.911
.933
.949
.960
.970
.977
.989
1.00
1.01 
20
.718
.816
.868
.900
.922
.938
.950
.959
.966
.977
.989
1.00 
24
.714
.812
.863
.895
.917
.932
.944
.953
.961
.972
.983
.994 
30
.709
.807
.858
.890
.912
.927
.939
.948
.955
.966
.978
.989 
40
.705
.802
.854
.885
.907
.922
.934
.943
.950
.961
.972
.983 
60
.701
.798
.849
.880
.901
.917
.928
.937
.945
.956
.967
.978 
120
.697
.793
.844
.875
.896
.912
.923
.932
.939
.950
.961
.972 
q
.693
.789
.839
.870
.891
.907
.918
.927
.934
.945
.956
.967 
For the OmniPower sales data, since n = 34 and k = 2, there are 3 degrees of freedom in 
the numerator and 31 degrees of freedom in the denominator. Thus, any Di 7 0.807 is flagged. 
Referring to Figure 14.15, you see that none of the Di values exceed 0.187, and therefore no 
observations are identified as influential using Cook’s Di statistic.
Comparison of Statistics
For the OmniPower sales data, the three statistics do not lead to a consistent set of conclu-
sions about the influence of each observation on the multiple regression model. According to 
both the hi and the Di criteria, none of the observations is a candidate for removal. Under such 
circumstances, most statisticians would conclude that there is insufficient evidence for the re-
moval of any observation from the analysis.
Individual statisticians may show a preference for a particular statistic to evaluate the in-
fluence of each observation on the multiple regression model, including statistics not discussed 
in this section (see references 1 and 3). Currently, no consensus exists as to which statistic is 
the best one to use.

	
Summary	
609
Problems for Section 14.8
Applying the Concepts
14.60  In Problem 14.4 on page 576, you used sales and number 
of orders to predict distribution costs at a mail-order catalog busi-
ness (stored in  Warecost ). Perform an influence analysis on your 
results and determine whether any observations should be deleted 
from the analysis. If necessary, reanalyze the regression model af-
ter deleting these observations and compare your results.
14.61  In Problem 14.5 on page 577, you used the percentage 
of alcohol and the chlorides to predict wine quality (stored in  
 VinhoVerde ). Perform an influence analysis on your results and 
determine whether any observations should be deleted from the 
analysis. If necessary, reanalyze the regression model after delet-
ing these observations and compare your results.
14.62  In Problem 14.6 on page 577, you used the amount of ra-
dio advertising and newspaper advertising to predict sales (stored 
in  Advertise ). Perform an influence analysis on your results and 
determine whether any observations should be deleted from the 
analysis. If necessary, reanalyze the regression model after delet-
ing these observations and compare your results.
14.63  In Problem 14.7 on page 577, you used the total staff present 
and remote hours to predict standby hours (stored in  Standby ).  
Perform an influence analysis on your results and determine whether 
any observations should be deleted from the analysis. If necessary, 
reanalyze the regression model after deleting these observations and 
compare your results.
14.64  In Problem 14.8 on page 577, you used the land area of 
the property and age in years to predict fair market value (stored 
in  GlenCove ). Perform an influence analysis on your results and 
determine whether any observations should be deleted from the 
analysis. If necessary, reanalyze the regression model after delet-
ing these observations and compare your results.
S u m m a r y
Figure 14.16 presents a roadmap of this chapter. In this 
chapter, you learned how to develop and fit multiple 
­regression models that use two or more independent vari-
ables to predict the value of a dependent variable. You also 
learned how to include categorical independent variables 
and interaction terms in regression models and learned the  
logistic regression model that is used to predict a categorical 
dependent variable.
I
n the Using Statistics scenario, you were a marketing man-
ager for OmniFoods, responsible for nutrition bars and 
similar snack items. You needed to determine the effect that 
price and in-store promotions would have on sales of Om-
niPower nutrition bars in order to develop an effective mar-
keting strategy. A sample of 34 stores in a supermarket chain 
was selected for a test-market study. The stores charged be-
tween 59 and 99 cents per bar and were given an in-store 
promotion budget between $200 and $600.
At the end of the one-month test-market study, you 
performed a multiple regression analysis on the data. Two 
independent variables were considered: the price of an Om-
niPower bar and the monthly budget for in-store promotional 
expenditures. The dependent variable was the number of 
OmniPower bars sold in a month. The coefficient of determi-
nation indicated that 75.8% of the variation in sales was ex-
plained by knowing the price charged and the amount spent 
on in-store promotions. The model indicated that the pre-
dicted sales of OmniPower are estimated to decrease by 532 
bars per month for 
each 10-cent increase 
in the price, and the 
predicted sales are estimated to increase by 361 bars for each 
additional $100 spent on promotions.
After studying the relative effects of price and pro-
motion, OmniFoods needs to set price and promotion 
standards for a nationwide introduction (obviously, lower 
prices and higher promotion budgets lead to more sales, but 
they do so at a lower profit margin). You determined that 
if stores spend $400 a month for in-store promotions and 
charge 79 cents, the 95% confidence interval estimate of 
the mean monthly sales is 2,854 to 3,303 bars. OmniFoods 
can multiply the lower and upper bounds of this confidence 
interval by the number of stores included in the nationwide 
introduction to estimate total monthly sales. For example, 
if 1,000 stores are in the nationwide introduction, then 
total monthly sales should be between 2.854 million and  
3.308 million bars.
U s i n g  S tat i s t i c s
The Multiple Effects of  
OmniPower Bars, Revisited
Ariwasabi/Shutterstock

610	
Chapter 14  Introduction to Multiple Regression 
F i g u r e  1 4 . 1 6
Roadmap for multiple regression
No
No
No
No
Yes
Yes
Yes
Yes
Predict
Y
Estimate
βj
Is the
Dependent
Variable
Numerical
?
Multiple
Regression
Determine Whether
Interaction Term(s)
Are Signiﬁcant
 Fit the
Selected Model
Does the
Model Contain
Dummy Variables
and/or Interaction
Terms
?
Residual
Analysis
Test Portions
of Model
Logistic
Regression
Use Model for
Prediction and
Estimation
Are the
Assumptions
of Regression
Satisﬁed
?
 Test Overall
Model Signiﬁcance
H0: β1 = β2 = ... = βk = 0
Is Overall
Model
Signiﬁcant
?
Estimate
μ

	
Key Equations	
611
Refere n c e s
	 1.	Andrews, D. F., and D. Pregibon. “Finding the Outliers that 
Matter.” Journal of the Royal Statistical Society 40 (Ser. B., 
1978): 85–93.
	 2.	Atkinson, A. C. “Robust and Diagnostic Regression Analysis.” 
Communications in Statistics 11 (1982): 2559–2572.
	 3.	Belsley, D. A., E. Kuh, and R. Welsch. Regression Diagnos-
tics: Identifying Influential Data and Sources of Collinearity. 
New York: Wiley, 1980.
	 4.	Cook, R. D., and S. Weisberg. Residuals and Influence in 
­Regression. New York: Chapman and Hall, 1982.
	 5.	Hosmer, D. W., and S. Lemeshow. Applied Logistic  
Regression, 2nd ed. New York: Wiley, 2001.
	 6.	Hoaglin, D. C., and R. Welsch. “The Hat Matrix in Regression 
and ANOVA,” The American Statistician, 32, (1978), 17–22.
	 7.	Kutner, M., C. Nachtsheim, J. Neter, and W. Li. Applied 
­Linear Statistical Models, 5th ed. New York: McGraw-Hill/
Irwin, 2005.
	 8.	Microsoft Excel 2013. Redmond, WA: Microsoft Corp., 2012.
	 9.	Minitab Release 16. State College, PA: Minitab, Inc., 2010.
K ey Eq u at i o n s
Multiple Regression Model with k Independent Variables
Yi = b0 + b1X1i + b2X2i + b3X3i + g + bkXki + ei 
	
(14.1)
Multiple Regression Model with Two Independent 
Variables
Yi = b0 + b1X1i + b2X2i + ei	
(14.2)
Multiple Regression Equation with Two Independent 
Variables
Yni = b0 + b1X1i + b2X2i	
(14.3)
Coefficient of Multiple Determination
r 2 = regression sum of squares
total sum of squares
= SSR
SST	
(14.4)
Adjusted r 2
r 2
adj = 1 - c11 - r 22
n - 1
n - k - 1 d 	
(14.5)
Overall F Test
FSTAT = MSR
MSE	
(14.6)
Testing for the Slope in Multiple Regression
tSTAT =
bj - bj
Sbj
	
(14.7)
Confidence Interval Estimate for the Slope
bj { ta>2Sbj	
(14.8)
Determining the Contribution of an Independent  
Variable to the Regression Model
SSR1Xj All Xs except j2 = SSR 1All Xs2 -
  SSR 1All Xs except j2	
(14.9)
Contribution of Variable X1, Given That X2 Has Been 
Included
SSR1X1X22 = SSR1X1 and X22 - SSR1X22	
(14.10a)
Contribution of Variable X2, Given That X1 Has Been 
Included
SSR1X2X12 = SSR1X1 and X22 - SSR1X12	
(14.10b)
Partial F Test Statistic
FSTAT =
SSR1Xj All Xs except j2
MSE
	
(14.11)
Relationship Between a t Statistic and an F Statistic
t2
STAT = FSTAT	
(14.12)
Coefficients of Partial Determination for a Multiple 
Regression Model Containing Two Independent Variables
r 2
Y1.2 =
SSR1X1X22
SST - SSR1X1 and X22 + SSR1X1X22	 (14.13a)
and
r 2
Y2.1 =
SSR1X2X12
SST - SSR1X1 and X22 + SSR1X2X12	 (14.13b)
Coefficient of Partial Determination for a Multiple 
Regression Model Containing k Independent Variables
r 2
Yj.1All variables except j2 =
SSR1Xj All Xs except j2
SST - SSR1All Xs2 + SSR1Xj All Xs except j2
	
(14.14)
Odds Ratio
Odds ratio =
probability of an event of interest
1 - probability of an event of interest
	
(14.15)
Logistic Regression Model
ln1Odds ratio2 = b0 + b1X1i + b2X2i + g + bkXki + ei
	
(14.16)
Logistic Regression Equation
ln1estimated odds ratio2 = b0 + b1X1i + b2X2i + g + bkXki
	
(14.17)

612	
Chapter 14  Introduction to Multiple Regression 
Estimated Odds Ratio
estimated odds ratio = eln1estimated odds ratio2	
(14.18)
Estimated Probability of an Event of Interest
estimated probability of an event of interest
=
estimated odds ratio
1 + estimated odds ratio	
(14.19)
Studentized Deleted Residual
ti = eiA
n - k - 1
SSE(1 - hi) - e2
i
	
(14.20)
Cook’s Di Statistic
Di =
e2
i
k MSEc
hi
(1 - hi)2 d 	
(14.21)
K e y  Term s
adjusted r 2  578
coefficient of multiple determination  577
coefficient of partial determination  590
Cook’s distance statistic Di  607
cross-product term  594
deviance statistic  604
dummy variable  592
hat matrix diagonal elements hi  607
interaction  594
interaction term  594
logistic regression  601
multiple regression model  572
net regression coefficient  575
odds ratio  601
overall F test  579
partial F test  586
Studentized deleted residual  607
Wald statistic  604
C hec ki n g  Yo u r  U n d e r s ta nding
14.65  What is the difference between ordinary least square 
­regression and logistic regression?
14.66  What do you understand by the terms maximum likelihood 
estimation, deviance statistic and Wald statistic?
14.67  How do multiple regression equations help in forecasting?
14.68  Explain r2 and adjusted r2? Why is it important to report 
adjusted r2 as a part of multiple regression model?
14.69  Discuss how to test the significance of each of the inde-
pendent variables and the entire multiple regression model.
14.70  How can you decide on the number of independent vari-
ables and that adding one more would improve the predictability 
of the dependent variable?
14.71  Explain the purpose of calculating F test statistic and t test 
statistic. Are they interrelated?
14.72  When a dummy variable is included in a regression model 
that has one numerical independent variable, what assumption do 
you need to make concerning the slope between the dependent 
variable, Y, and the numerical independent variable, X?
14.73  When do you use logistic regression?
14.74  What is the difference between the hat matrix diagonal el-
ements hi and the Studentized deleted residuals?
C ha pter  R e vi e w P r o b le ms
14.75  Increasing customer satisfaction typically results in in-
creased purchase behavior. For many products, there is more than 
one measure of customer satisfaction. In many, purchase behavior 
can increase dramatically with an increase in just one of the customer 
satisfaction measures. Gunst and Barry (“One Way to Moderate  
Ceiling Effects,” Quality Progress, October 2003, pp. 83–85) con-
sider a product with two satisfaction measures, X1 and X2, that 
range from the lowest level of satisfaction, 1, to the highest level 
of satisfaction, 7. The dependent variable, Y, is a measure of pur-
chase behavior, with the highest value generating the most sales. 
Consider the regression equation:
Yni = -3.888 + 1.449X1i + 1.462X2i - 0.190X1iX2i
Suppose that X1 is the perceived quality of the product and X2 
is the perceived value of the product. (Note: If the customer thinks 
the product is overpriced, he or she perceives it to be of low value 
and vice versa.)
a.	 What is the predicted purchase behavior when X1 = 2 and 
X2 = 2?
b.	 What is the predicted purchase behavior when X1 = 2 and 
X2 = 7?
c.	 What is the predicted purchase behavior when X1 = 7 and 
X2 = 2?
d.	 What is the predicted purchase behavior when X1 = 7 and 
X2 = 7?
e.	 What is the regression equation when X2 = 2? What is the 
slope for X1 now?
f.	 What is the regression equation when X2 = 7? What is the 
slope for X1 now?
g.	 What is the regression equation when X1 = 2? What is the 
slope for X2 now?
h.	 What is the regression equation when X1 = 7? What is the 
slope for X2 now?
i.	 Discuss the implications of (a) through (h) in the context of in-
creasing sales for this product with two customer satisfaction 
measures.

	
Chapter Review Problems	
613
14.76  The owner of a moving company typically has his most 
experienced manager predict the total number of labor hours that 
will be required to complete an upcoming move. This approach 
has proved useful in the past, but the owner has the business ob-
jective of developing a more accurate method of predicting labor 
hours. In a preliminary effort to provide a more accurate method, 
the owner has decided to use the number of cubic feet moved and 
the number of pieces of large furniture as the independent vari-
ables and has collected data for 36 moves in which the origin and 
destination were within the borough of Manhattan in New York 
City and the travel time was an insignificant portion of the hours 
worked. The data are organized and stored in  Moving .
a.	 State the multiple regression equation.
b.	 Interpret the meaning of the slopes in this equation.
c.	 Predict the mean labor hours for moving 500 cubic feet with 
two large pieces of furniture.
d.	 Perform a residual analysis on your results and determine 
whether the regression assumptions are valid.
e.	 Determine whether there is a significant relationship between 
labor hours and the two independent variables (the number of 
cubic feet moved and the number of pieces of large furniture) at 
the 0.05 level of significance.
f.	 Determine the p-value in (e) and interpret its meaning.
g.	 Interpret the meaning of the coefficient of multiple determina-
tion in this problem.
h.	 Determine the adjusted r 2.
i.	 At the 0.05 level of significance, determine whether each inde-
pendent variable makes a significant contribution to the regres-
sion model. Indicate the most appropriate regression model for 
this set of data.
j.	 Determine the p-values in (i) and interpret their meaning.
k.	 Construct a 95% confidence interval estimate of the population 
slope between labor hours and the number of cubic feet moved. 
How does the interpretation of the slope here differ from that in 
Problem 13.44 on page 549?
l.	 Compute and interpret the coefficients of partial determination.
m.	What conclusions can you reach concerning labor hours?
14.77  Professional basketball has truly become a sport that gen-
erates interest among fans around the world. More and more play-
ers come from outside the United States to play in the National 
Basketball Association (NBA). You want to develop a regression 
model to predict the number of wins achieved by each NBA team, 
based on field goal (shots made) percentage for the team and for 
the opponent. The data are stored in  NBA2012 .
a.	 State the multiple regression equation.
b.	 Interpret the meaning of the slopes in this equation.
c.	 Predict the mean number of wins for a team that has a field 
goal percentage of 45% and an opponent field goal percentage 
of 44%.
d.	 Perform a residual analysis on your results and determine 
whether the regression assumptions are valid.
e.	 Is there a significant relationship between number of wins and 
the two independent variables (field goal percentage for the 
team and for the opponent) at the 0.05 level of significance?
f.	 Determine the p-value in (e) and interpret its meaning.
g.	 Interpret the meaning of the coefficient of multiple determina-
tion in this problem.
h.	 Determine the adjusted r 2.
i.	 At the 0.05 level of significance, determine whether each 
­independent variable makes a significant contribution to the 
­regression model. Indicate the most appropriate regression 
model for this set of data.
j.	 Determine the p-values in (i) and interpret their meaning.
k.	 Compute and interpret the coefficients of partial determination.
l.	 Perform an influence analysis on your results and determine 
whether any observations should be deleted from the model. If 
necessary, reanalyze the regression model after deleting these 
observations and compare your results to those of the original 
model.
m.	What conclusions can you reach concerning field goal percent-
age (team and opponent) in predicting the number of wins?
14.78  A sample of 30 houses recently listed for sale in Silver 
Spring, Maryland, was selected with the objective of developing a 
model to predict the assessed value (in $thousands), using the size 
of the house (in thousands of square feet) and age (in years). The 
results are stored in  Silver Spring .
a.	 Fit a multiple regression model.
b.	 Interpret the meaning of the slopes in this model.
c.	 Predict the mean assessed value for a house that has 2,000 
square feet and is 55 years old.
d.	 Perform a residual analysis on your results and determine 
whether the regression assumptions are valid.
e.	 Determine whether there is a significant relationship between 
assessed value and the two independent variables (house size 
and age) at the 0.05 level of significance.
f.	 Determine the p-value in (e) and interpret its meaning.
g.	 Interpret the meaning of the coefficient of multiple determina-
tion in this problem.
h.	 Determine the adjusted r 2.
i.	 At the 0.05 level of significance, determine whether each inde-
pendent variable makes a significant contribution to the regres-
sion model. Indicate the most appropriate regression model for 
this set of data.
j.	 Determine the p-values in (i) and interpret their meaning.
k.	 Construct a 95% confidence interval estimate of the popula-
tion slope between assessed value and the size of the house. 
How does the interpretation of the slope here differ from that in 
Problem 13.76 on page 561?
l.	 Compute and interpret the coefficients of partial determination.
m.	What conclusions can you reach about the assessed value?
14.79  Measuring the height of a California redwood tree is very 
difficult because these trees grow to heights over 300 feet. People 
familiar with these trees understand that the height of a California 
redwood tree is related to other characteristics of the tree, includ-
ing the diameter of the tree at the breast height of a person (in 
inches) and the thickness of the bark of the tree (in inches). The 
file  Redwood  contains the height, diameter at breast height of a 
person, and bark thickness for a sample of 21 California redwood 
trees.
a.	 State the multiple regression equation that predicts the height 
of a tree, based on the tree’s diameter at breast height and the 
thickness of the bark.
b.	 Interpret the meaning of the slopes in this equation.
c.	 Predict the mean height for a tree that has a breast height diam-
eter of 25 inches and a bark thickness of 2 inches.
d.	 Interpret the meaning of the coefficient of multiple determina-
tion in this problem.
e.	 Perform a residual analysis on the results and determine 
whether the regression assumptions are valid.

614	
Chapter 14  Introduction to Multiple Regression 
f.	 Determine whether there is a significant relationship between 
the height of redwood trees and the two independent variables 
(breast-height diameter and bark thickness) at the 0.05 level of 
significance.
g.	 Construct a 95% confidence interval estimate of the population 
slope between the height of redwood trees and breast-height di-
ameter and between the height of redwood trees and the bark 
thickness.
h.	 At the 0.05 level of significance, determine whether each in-
dependent variable makes a significant contribution to the re-
gression model. Indicate the independent variables to include 
in this model.
i.	 Construct a 95% confidence interval estimate of the mean 
height for trees that have a breast-height diameter of 25 inches 
and a bark thickness of 2 inches, along with a prediction inter-
val for an individual tree.
j.	 Compute and interpret the coefficients of partial determination.
k.	 Perform an influence analysis on your results and determine 
whether any observations should be deleted from the model. If 
necessary, reanalyze the regression model after deleting these 
observations and compare your results to those of the original 
model.
l.	 What conclusions can you reach concerning the effect of the 
diameter of the tree and the thickness of the bark on the height 
of the tree?
14.80  A sample of 30 houses recently listed for sale in Silver 
Spring, Maryland, was selected with the objective of developing 
a model to predict the taxes (in $) based on the assessed value of 
houses (in $thousands) and the age of the houses (in years) (stored in 
 SilverSpring ):
a.	 State the multiple regression equation.
b.	 Interpret the meaning of the slopes in this equation.
c.	 Predict the mean taxes for a house that has an assessed value of 
$400,000 and is 50 years old.
d.	 Perform a residual analysis on the results and determine 
whether the regression assumptions are valid.
e.	 Determine whether there is a significant relationship between 
taxes and the two independent variables (assessed value and 
age) at the 0.05 level of significance.
f.	 Determine the p-value in (e) and interpret its meaning.
g.	 Interpret the meaning of the coefficient of multiple determina-
tion in this problem.
h.	 Determine the adjusted r 2.
i.	 At the 0.05 level of significance, determine whether each inde-
pendent variable makes a significant contribution to the regres-
sion model. Indicate the most appropriate regression model for 
this set of data.
j.	 Determine the p-values in (i) and interpret their meaning.
k.	 Construct a 95% confidence interval estimate of the population 
slope between taxes and assessed value. How does the inter-
pretation of the slope here differ from that of Problem 13.77 on 
page 561?
l.	 Compute and interpret the coefficients of partial determination.
m.	The real estate assessor’s office has been publicly quoted as 
saying that the age of a house has no bearing on its taxes. Based 
on your answers to (a) through (l), do you agree with this state-
ment? Explain.
14.81  A baseball analytics specialist wants to determine which 
variables are important in predicting a team’s wins in a given 
season. He has collected data related to wins, earned run average 
(ERA), and runs scored for the 2012 season (stored in  BB2012 ). 
Develop a model to predict the number of wins based on ERA and 
runs scored.
a.	 State the multiple regression equation.
b.	 Interpret the meaning of the slopes in this equation.
c.	 Predict the mean number of wins for a team that has an ERA of 
4.50 and has scored 750 runs.
d.	 Perform a residual analysis on the results and determine 
whether the regression assumptions are valid.
e.	 Is there a significant relationship between the number of wins 
and the two independent variables (ERA and runs scored) at 
the 0.05 level of significance?
f.	 Determine the p-value in (e) and interpret its meaning.
g.	 Interpret the meaning of the coefficient of multiple determina-
tion in this problem.
h.	 Determine the adjusted r 2.
i.	 At the 0.05 level of significance, determine whether each inde-
pendent variable makes a significant contribution to the regres-
sion model. Indicate the most appropriate regression model for 
this set of data.
j.	 Determine the p-values in (i) and interpret their meaning.
k.	 Construct a 95% confidence interval estimate of the population 
slope between wins and ERA.
l.	 Compute and interpret the coefficients of partial determination.
m.	Which is more important in predicting wins—pitching, as mea-
sured by ERA, or offense, as measured by runs scored? Explain.
n.	 Perform an influence analysis on your results and determine 
whether any observations should be deleted from the model. If 
necessary, reanalyze the regression model after deleting these 
observations and compare your results to those of the original 
model.
14.82  Referring to Problem 14.81, suppose that in addition to us-
ing ERA to predict the number of wins, the analytics specialist 
wants to include the league (0 = American, 1 = National) as an 
independent variable. Develop a model to predict wins based on 
ERA and league. For (a) through (k), do not include an interaction 
term.
a.	 State the multiple regression equation.
b.	 Interpret the slopes in (a).
c.	 Predict the mean number of wins for a team with an ERA of 
4.50 in the American League. Construct a 95% confidence 
­interval estimate for all teams and a 95% prediction interval for 
an individual team.
d.	 Perform a residual analysis on the results and determine 
whether the regression assumptions are valid.
e.	 Is there a significant relationship between wins and the two in-
dependent variables (ERA and league) at the 0.05 level of sig-
nificance?
f.	 At the 0.05 level of significance, determine whether each inde-
pendent variable makes a contribution to the regression model. 
Indicate the most appropriate regression model for this set of 
data.
g.	 Construct a 95% confidence interval estimate of the population 
slope for the relationship between wins and ERA.
h.	 Construct a 95% confidence interval estimate of the population 
slope for the relationship between wins and league.
i.	 Compute and interpret the adjusted r 2.
j.	 Compute and interpret the coefficients of partial determination.
k.	 What assumption do you have to make about the slope of wins 
with ERA?

	
Cases for Chapter 14	
615
l.	 Add an interaction term to the model and, at the 0.05 level of 
significance, determine whether it makes a significant contribu-
tion to the model.
m.	On the basis of the results of (f) and (l), which model is most 
appropriate? Explain.
14.83  You are a real estate broker who wants to compare prop-
erty values in Glen Cove and Roslyn (which are located approxi-
mately 8 miles apart). In order to do so, you will analyze the data 
in  GCRoslyn , a file that includes samples of houses from Glen 
Cove and Roslyn. Making sure to include the dummy variable 
for location (Glen Cove or Roslyn), develop a regression model 
to predict fair market value, based on the land area of a property, 
the age of a house, and location. Be sure to determine whether any 
interaction terms need to be included in the model.
14.84  A recent article discussed a metal deposition process in 
which a piece of metal is placed in an acid bath and an alloy is 
layered on top of it. The business objective of engineers work-
ing on the process was to reduce variation in the thickness of 
the alloy layer. To begin, the temperature and the pressure in the 
tank holding the acid bath are to be studied as independent vari-
ables. Data are collected from 50 samples. The results are orga-
nized and stored in  Thickness . (Data extracted from J. Conklin,  
“It’s a Marathon, Not a Sprint,” Quality Progress, June 2009,  
pp. 46–49.)
Develop a multiple regression model that uses temperature 
and the pressure in the tank holding the acid bath to predict the 
thickness of the alloy layer. Be sure to perform a thorough residual 
analysis. The article suggests that there is a significant interac-
tion between the pressure and the temperature in the tank. Do you 
agree?
14.85  Starbucks Coffee Co. uses a data-based approach to im-
proving the quality and customer satisfaction of its products. When 
survey data indicated that Starbucks needed to improve its package 
sealing process, an experiment was conducted to determine the fac-
tors in the bag-sealing equipment that might be affecting the ease 
of opening the bag without tearing the inner liner of the bag. (Data 
extracted from L. Johnson and S. Burrows, “For Starbucks, It’s in 
the Bag,” Quality Progress, March 2011, pp. 17–23.) Among the 
factors that could affect the rating of the ability of the bag to resist 
tears were the viscosity, pressure, and plate gap on the bag-sealing 
equipment. Data were collected on 19 bags in which the plate gap 
was varied. The results are stored in  Starbucks . Develop a mul-
tiple regression model that uses the viscosity, pressure, and plate 
gap on the bag-sealing equipment to predict the tear rating of the 
bag. Be sure to perform a thorough residual analysis. Do you think 
that you need to use all three independent variables in the model? 
Explain.
14.86  An experiment was conducted to study the extrusion process 
of biodegradable packaging foam (data extracted from W. Y. Koh,  
K. M. Eskridge, and M. A. Hanna, “Supersaturated Split-Plot  
Designs,” Journal of Quality Technology, 45, January 2013, pp. 
61–72). Among the factors considered for their effect on the unit 
density (mg/ml) were the die temperature (145°C versus 155°C) 
and the die diameter (3 mm versus 4 mm. The results were stored  
in  PackagingFoam3 . Develop a multiple regression model that uses 
die temperature and die diameter to predict the unit density (mg/
ml). Be sure to perform a thorough residual and influence analysis. 
Do you think that you need to use both independent variables in the 
model? Explain.
14.87  Referring to Problem 14.86, instead of predicting the unit 
density, you now wish to predict the foam diameter from results 
stored in  PackagingFoam4 . Develop a multiple regression model 
that uses die temperature and die diameter to predict the foam di-
ameter (mg/ml). Be sure to perform a thorough residual and influ-
ence analysis. Do you think that you need to use both independent 
variables in the model? Explain.
Managing Ashland MultiComm Services
In its continuing study of the 3-For-All subscription solicita-
tion process, a marketing department team wants to test the 
effects of two types of structured sales presentations (personal 
formal and personal informal) and the number of hours spent 
on telemarketing on the number of new subscriptions. The 
staff has recorded these data for the past 24 weeks in  AMS14 .
Analyze these data and develop a multiple regres-
sion model to predict the number of new subscriptions 
for a week, based on the number of hours spent on 
­telemarketing and the sales presentation type. Write a 
report, giving detailed findings concerning the regres-
sion model used.
C a s e s  f o r  C h a p t e r  1 4

616	
Chapter 14  Introduction to Multiple Regression 
Digital Case
Apply your knowledge of multiple regression models in this 
Digital Case, which extends the OmniFoods Using Statistics 
scenario from this chapter.
To ensure a successful test marketing of its OmniPower 
energy bars, the OmniFoods marketing department has 
contracted with In-Store Placements Group (ISPG), a mer-
chandising consulting firm. ISPG will work with the gro-
cery store chain that is conducting the test-market study. 
Using the same 34-store sample used in the test-market 
study, ISPG claims that the choice of shelf location and the 
presence of in-store OmniPower coupon dispensers both in-
crease sales of the energy bars.
Open Omni_ISPGMemo.pdf to review the ISPG claims 
and supporting data. Then answer the following questions:
1.	Are the supporting data consistent with ISPG’s claims? 
Perform an appropriate statistical analysis to confirm (or 
discredit) the stated relationship between sales and the 
two independent variables of product shelf location and 
the presence of in-store OmniPower coupon dispensers.
2.	If you were advising OmniFoods, would you recommend 
using a specific shelf location and in-store coupon dis-
pensers to sell OmniPower bars?
3.	What additional data would you advise collecting in 
order to determine the effectiveness of the sales promo-
tion techniques used by ISPG?

	
Chapter 14 EXCEL Guide	
617
EG14.1  Developing a Multiple 
Regression Model
Interpreting the Regression Coefficients
Key Technique  Use the LINEST(cell range of Y variable, cell 
range of X variables, True, True) function to compute the regres-
sion coefficients and other values related to a multiple regression 
analysis.
Example  Develop the Figure 14.2 multiple regression model for 
the OmniPower sales data shown on page 574.
PHStat  Use Multiple Regression.
For the example, open to the DATA worksheet of the  
OmniPower workbook. Select PHStat ➔ Regression ➔  
Multiple ­Regression, and in the procedure’s dialog box (shown 
below):
	 1.	 Enter A1:A35 as the Y Variable Cell Range.
	 2.	 Enter B1:C35 as the X Variables Cell Range.
	 3.	 Check First cells in both ranges contain label.
	 4.	 Enter 95 as the Confidence level for regression coefficients.
	 5.	 Check Regression Statistics Table and ANOVA and Coef-
ficients Table.
	 6.	 Enter a Title and click OK.
The procedure creates a worksheet that contains a copy of your 
data in addition to the Figure 14.2 worksheet. For more informa-
tion about these worksheets, read the following In-Depth Excel 
section.
In-Depth Excel  Use the COMPUTE worksheet of the Mul-
tiple Regression workbook as a template.
For the example, the COMPUTE worksheet uses the OmniPower 
sales data already in the MRData worksheet to perform the re-
gression analysis. To perform multiple regression analyses for 
other data, paste the regression data into the MRData worksheet.
Figure 14.2 does not show the Calculations area in col-
umns K through N. In the cell range L2:N6, an array formula 
uses the LINEST function to compute intercepts, standard 
­error values, and other regression statistics. The Calculations 
area also contains the user-supplied confidence level and for-
mulas to compute the critical value of the t statistic and half-
widths.
To perform a multiple regression analysis with other data, 
paste the regression data into the MRData worksheet. Paste the 
values for the Y variable into column A and the values for the  
X variables into consecutive columns, starting with column 
B. Then, open to the COMPUTE worksheet. Enter the confi-
dence level in cell L8 and edit the five-row-by-three-column 
array formula that starts with cell L2 (the cell range L2:N6). 
If you have more than two independent variables, select the 
wider range that adds a column for each independent variable 
in excess of two. For example, with three independent vari-
ables, select the cell range L2:O6.Then, edit the array formula 
to reflect the data you pasted into the MRData worksheet. 
Your cell ranges should start with row 2 so as to exclude the 
row 1 variable names (an exception to the usual practice in 
this book). Remember to press the Enter key while holding 
down the Control and Shift keys (or the Command key on 
a Mac) to enter the array formula as discussed in Appendix 
Section B.3.
Read the Short Takes for Chapter 14 for an explanation of 
the formulas found in the COMPUTE worksheet (shown in the 
COMPUTE_FORMULAS worksheet). If you use an Excel ver-
sion that is older than Excel 2010, use the same-name worksheets 
in the Multiple Regression 2007 workbook.
Analysis ToolPak  Use Regression.
For the example, open to the DATA worksheet of the Omni­
Power workbook and:
	 1.	 Select Data ➔ Data Analysis.
	 2.	 In the Data Analysis dialog box, select Regression from the 
Analysis Tools list and then click OK.
In the Regression dialog box (shown on page 618):
	 3.	 Enter A1:A35 as the Input Y Range and enter B1:C35 as the 
Input X Range.
	 4.	 Check Labels and check Confidence Level and enter 95 in 
its box.
	 5.	 Click New Worksheet Ply.
	 6.	 Click OK.
C h a p t e r  1 4  E x c e l  G u i d e

618	
Chapter 14  Introduction to Multiple Regression 
Predicting the Dependent Variable Y
Key Technique  Use the MMULT array function and the 
T.INV.2T function to help compute intermediate values that de-
termine the confidence interval estimate and prediction interval.
Example  Compute the Figure 14.3 confidence interval estimate 
and prediction interval for the OmniPower sales data shown on 
page 576.
PHStat  Use the PHStat “Interpreting the Regression Coeffi-
cients” instructions but replace step 6 with the following steps 6 
through 8:
	 6.	 Check Confidence Interval Estimate & Prediction Interval 
and enter 95 as the percentage for Confidence level for inter-
vals.
	 7.	 Enter a Title and click OK.
	 8.	 In the new worksheet, enter 79 in cell B6 and enter 400 in cell B7.
These steps create a new worksheet that is discussed in the follow-
ing In-Depth Excel instructions.
In-Depth Excel  Use the CIEandPI worksheet of the Multiple 
Regression workbook as a template.
The worksheet already contains the data and formulas for 
the example. The worksheet uses the MMULT function (see  
Appendix Section F.4) in several array formulas that perform  
matrix operations.
Modifying this worksheet for other models with more than 
two independent variables requires knowledge that is beyond the 
scope of this book. For other models with two independent vari-
ables, first paste the data for those variables into columns B and 
C of the MRArray worksheet and adjust the number of entries in 
column A (all of which are 1). Then, adjust the COMPUTE work-
sheet to reflect the new regression data, using the In-Depth Excel 
“Interpreting the Regression Coefficients” instructions. Finally, 
open to the CIEandPI worksheet and edit the array formula in cell 
range B9:D11 and the labels in cells A6 and A7.
Read the Short Takes for Chapter 14 for an explanation  
of the formulas found in the CIEandPI worksheet (shown in the 
CIEandPI_FORMULAS worksheet). If you use an Excel ver-
sion that is older than Excel 2010, use the CIEandPI worksheet in 
the Multiple Regression 2007 workbook.
EG14.2  r2, Adjusted r2, and the Overall 
F Test
The coefficient of multiple determination, r 2, the adjusted r 2, and 
the overall F test are all computed as part of creating the multiple 
regression results worksheet using the Section EG14.1 instruc-
tions. If you use either the PHStat or In-Depth Excel instructions, 
formulas are used to compute these results in the COMPUTE 
worksheet. Formulas in cells B5, B7, B13, C12, C13, D12, and 
E12 copy values computed by the array formula in cell range 
L2:N6. In cell F12, the expression F.DIST.RT(F test statistic, 1, 
error degrees of freedom) computes the p-value for the overall  
F test.
EG14.3  Residual Analysis for the 
Multiple Regression Model
Key Technique  Use arithmetic formulas and some results from 
the multiple regression COMPUTE worksheet to compute residuals.
Example  Perform the residual analysis for the OmniPower sales 
data discussed in Section 14.3, starting on page 581.
PHStat  Use the Section EG14.1 “Interpreting the Regression 
Coefficients” PHStat instructions. Modify step 5 by checking Re-
siduals Table and Residual Plots in addition to checking Regres-
sion Statistics Table and ANOVA and Coefficients Table.
In-Depth Excel  Use the RESIDUALS worksheet of the Mul-
tiple Regression workbook as a template. Then construct residual 
plots for the residuals and the predicted value of Y and for the re-
siduals and each of the independent variables.
For the example, the RESIDUALS worksheet uses the  
OmniPower sales data already in the MRData worksheet to com-
pute the residuals. To compute residuals for other data, first use 
the Section EG14.1 “Interpreting the Regression Coefficients”  
In-Depth Excel instructions to modify the MRData and COMPUTE 
worksheets. Then, open to the RESIDUALS worksheet and:
	 1.	 If the number of independent variables is greater than 2, ­select 
column D, right-click, and click Insert from the shortcut 
menu. Repeat this step as many times as necessary to create 
the additional columns to hold all the X variables.
	 2.	 Paste the data for the X variables into columns, starting with 
column B.
	 3.	 Paste Y values in column E (or in the second-to-last column if 
there are more than two X variables).
	 4.	 For sample sizes smaller than 34, delete the extra rows. For 
sample sizes greater than 34, copy the predicted Y and residu-
als formulas down through the row containing the last pair 
of X and Y values. Also, add the new observation numbers in 
column A.
To construct the residual plots, open to the RESIDUALS 
worksheet and select pairs of columns and then use the Section 
EG2.5 In-Depth Excel “The Scatter Plot” instructions. (If you for-
got to select the columns, Excel will construct a meaningless plot 
of all of the data in the RESIDUALS worksheet.) For example, to 
construct the residual plot for the residuals and the predicted value 
of Y, select columns D and F. (See Appendix Section B.7 for help 
in selecting a non-contiguous cell range.)

	
Chapter 14 EXCEL Guide	
619
Read the Short Takes for Chapter 14 for an explanation of 
the formulas found in the RESIDUALS worksheet (shown in the 
RESIDUALS_FORMULAS worksheet).
Analysis ToolPak  Use the Section EG14.1 Analysis ToolPak 
instructions. Modify step 5 by checking Residuals and ­Residual 
Plots before clicking New Worksheet Ply and then OK. The 
­Residuals Plots option constructs residual plots only for each 
independent variable. To construct a plot of the residuals and the 
predicted value of Y, select the predicted and residuals cells (in the 
RESIDUAL OUTPUT area of the regression results worksheet) 
and then apply the Section EG2.5 In-Depth Excel “The Scatter 
Plot” instructions.
EG14.4  Inferences Concerning 
the Population Regression 
Coefficients
The regression results worksheets created by using the  
Section EG14.1 instructions include the information needed  
to make the inferences discussed in Section 14.4.
EG14.5  Testing Portions of the 
Multiple Regression Model
Key Technique  Adapt the Section EG14.1 “Interpreting the 
Regression Coefficients” instructions and the Section EG13.2 in-
structions to develop the regression analyses needed.
Example  Test portions of the multiple regression model for the 
OmniPower sales data as discussed in Section 14.5, starting on 
page 586.
PHStat  Use the Section EG14.1 PHStat “Interpreting the Re-
gression Coefficients” instructions but modify step 6 by checking 
Coefficients of Partial Determination before you click OK.
In-Depth Excel  Use one of the CPD worksheets of the Mul-
tiple Regression workbook as a template.
For the example, the CPD_2 worksheet already contains the data 
to compute the coefficients of partial determination. For other 
problems, you use a two-step process to compute the coefficients 
of partial determination. You first use the Section EG14.1 and the 
Section EG13.2 In-Depth Excel instructions to create all possible 
regression results worksheets in a copy of the Multiple Regres-
sion workbook. For example, if you have two independent vari-
ables, you perform three regression analyses: Y with X1 and X2, 
Y with X1, and Y with X2, to create three regression results work-
sheets. Then, you open to the CPD worksheet for the number of 
independent variables (CPD_2, CPD_3, and CPD_4 worksheets 
are included) and follow the italicized instructions to copy and 
Paste Special Values (see Appendix Section B.4) from the regres-
sion results worksheets.
EG14.6  Using Dummy Variables 
and Interaction Terms in 
Regression Models
Dummy Variables
Key Technique  Use Find and Replace to create a dummy 
variable from a two-level categorical variable. Before using Find 
and Replace, copy and paste the categorical values to another col-
umn in order to preserve the original values.
Example  From the two-level categorical variable Fireplace, cre-
ate the dummy variable named FireplaceCoded that is used in the 
Figure 14.10 regression model on page 592.
In-Depth Excel  For the example, open to the DATA worksheet 
of the SilverSpring workbook and:
	 1.	 Copy and paste the Fireplace values in column I to column J 
(the first empty column). Enter FireplaceCoded in cell J1.
	 2.	 Select column J.
	 3.	 Press Ctrl+H (the keyboard shortcut for Find and Replace).
In the Find and Replace dialog box:
	 4.	 Enter Yes in the Find what box and enter 1 in the Replace 
with box.
	 5.	 Click Replace All. If a message box to confirm the replace-
ment appears, click OK to continue.
	 6.	 Enter No in the Find what box and enter 0 in the Replace 
with box.
	 7.	 Click Replace All. If a message box to confirm the replace-
ment appears, click OK to continue.
	 8.	 Click Close.
Categorical variables that have more than two levels require the use of 
formulas in multiple columns. For example, to create the dummy vari-
ables for Example 14.3 on page 593, two columns are needed. Assume 
that the three-level categorical variable mentioned in the example is 
in Column D of the opened worksheet. A first new column that con-
tains formulas in the form = IF1column D cell = first level, 1, 02 
and a second new column that contains formulas in the form 
=IF1column D cell = second level, 1, 02 would properly create 
the two dummy variables that the example requires.
Interactions
To create an interaction term, add a column of formulas that mul-
tiply one independent variable by another. For example, if the 
first independent variable appeared in column B and the second 
independent variable appeared in column C, enter the formula 
=B2 * C2 in the row 2 cell of an empty new column and then copy 
the formula down through all rows of data to create the ­interaction.
EG14.7  Logistic Regression
Key Technique  Use an automated process that incorporates the 
use of the Solver add-in to develop a logistic regression analysis 
model.
Example  Develop the Figure 14.14 logistic regression model for 
the credit card pilot study data shown on page 603.
PHStat  Use Logistic Regression.
For the example, open to the DATA worksheet of the CardStudy 
workbook. Select PHStat ➔ Regression ➔ Logistic Regression, 
and in the procedure’s dialog box (shown on page 620):
	 1.	 Enter A1:A31 as the Y Variable Cell Range.
	 2.	 Enter B1:C31 as the X Variables Cell Range.

620	
Chapter 14  Introduction to Multiple Regression 
	 3.	 Check First cells in both ranges contain label.
	 4.	 Enter a Title and click OK.
If the Solver add-in is not installed (see Appendix Section D.6), 
PHStat will display an error message instead of the Logistic  
­Regression dialog box. The COMPUTE worksheet created 
­contains a number of columns not shown in Figure 14.14 that con-
tain supporting data.
In-Depth Excel  Use the Logistic Regression add-in work-
book. This workbook requires that the Solver add-in be installed 
(see Appendix Section D.6).
For the example, first open to the DATA worksheet of the 
CardStudy workbook. Then open the Logistic Regression 
add-in workbook. When this workbook opens properly, it adds 
a Logistic Add-in menu in either the Add-ins tab (Microsoft 
Windows) or the Apple menu bar (OS X). Select Logistic Add-
in ➔ Logistic Regression from either the Add-ins tab or the 
Apple menu bar. In the Logistic Regression dialog box (shown 
below):
	 1.	 Enter A1:A31 as the Y Variable Cell Range.
	 2.	 Enter B1:C31 as the X Variables Cell Range.
	 3.	 Check First cells in both ranges contain label.
	 4.	 Enter a Title and click OK.
If the Solver add-in is not installed, you will see an error message 
in lieu of the Logistic Regression dialog box. This add-in work-
book requires data workbooks to be in the .xlsx format and not the 
older .xls format.
EG14.8  Influence Analysis
There are no Excel Guide instructions for this section.
MG14.1  Developing a Multiple 
Regression Model
Use 3D Scatterplot to create a three-dimensional plot for the 
special case of a regression model that contains two independent 
variables. For example, to create the Figure 14.1 plot on page 573 
for the OmniPower sales data, open the OmniPower worksheet. 
­Select Graph ➔ 3D Scatterplot. In the 3D Scatterplots dialog 
box, click Simple and then click OK. In the 3D Scatterplot - Sim-
ple dialog box (shown in right column):
	 1.	 Double-click C1  Sales in the variables list to add Sales to the 
Z variable box.
	 2.	 Double-click C2  Price in the variables list to add Price to the 
Y variable box.
	 3.	 Double-click C3  Promotional Expenses in the variables list 
to add 'Promotional Expenses' to the X variable box.
	 4.	 Click Data View.
In the 3D Scatterplot - Data View dialog box:
	 5.	 Check Symbols and Project lines.
	 6.	 Click OK.
	 7.	 Back in the 3D Scatterplot - Simple dialog box, click OK.
Rotate the scatter plot using the icons to rotate the X, Y, and Z 
axes in the 3D Graph Tools toolbar. Select Tools ➔ Toolbars ➔ 3D 
Graph Tools if this toolbar is not visible in the Minitab window. 
C h a p t e r  1 4  M i n i ta b  G u i d e

	
Chapter 14 Minitab Guide	
621
The right scatter plot in Figure 14.1 was rotated clockwise about 
90 degrees around the Z axis and was slightly rotated about the two 
other axes.
Interpreting the Regression coefficients
Use Regression to perform a multiple regression analysis. For 
example, to perform the Figure 14.2 analysis of the OmniPower 
sales data on page 574, open to the OmniPower worksheet. Se-
lect Stat ➔ Regression ➔ Regression. In the Regression dialog 
box (shown below):
	  1.	 Double-click C1  Sales in the variables list to add Sales to 
the Response box.
	  2.	 Double-click C2  Price in the variables list to add Price to 
the Predictors box.
	  3.	 Double-click C3  Promotional Expenses in the variables 
list to add 'Promotional Expenses' to the Predictors box.
	  4.	 Click Graphs.
In the Regression - Graphs dialog box (shown below):
	  5.	 Click Regular and Individual Plots.
	  6.	 Check Histogram of residuals and Residuals versus fits 
and clear the other check boxes.
	  7.	 Click anywhere inside the Residuals versus the variables box.
	  8.	 Double-click C2  Price in the variables list to add Price in 
the Residuals versus the variables box.
	  9.	 Double-click C3  Promotional Expenses in the variables 
list to add 'Promotional Expenses' in the Residuals versus 
the variables box.
	10.	 Click OK.
	11.	 Back in the Regression dialog box, click Results.
In the Regression - Results dialog box (not shown):
	12.	 Click In addition, the full table of fits and residuals and 
then click OK.
	13.	 Back in the Regression dialog box, click Options.
In the Regression - Options dialog box (shown below):
	14.	 Check Fit Intercept.
	15.	 Clear all the Display and Lack of Fit Test check boxes.
	16.	 Enter 79 and 400 in the Prediction intervals for new obser-
vations box.
	17.	 Enter 95 in the Confidence level box.
	18.	 Click OK.
	19.	 Back in the Regression dialog box, click OK.
The results in the Session Window will include additional items 
that are not shown in Figure 14.2.
Predicting the Dependent Variable Y
The regression results created by using the Section MG14.1 in-
structions include the confidence interval estimation and predic-
tion interval. Figure 14.3 on page 576 shows these items for the 
OmniPower sales data.
MG14.2  r2, Adjusted r2, and the 
Overall F Test
The coefficient of multiple determination, r 2, the adjusted 
r 2, and the overall F test are all computed as part of creat-
ing the multiple regression results using the Section MG14.1 
­instructions.
MG14.3  Residual Analysis for the 
Multiple Regression Model
The regression results created by using the MG14.1 instructions 
include a residual analysis.
MG14.4  Inferences Concerning 
the Population Regression 
Coefficients
The regression results created by using the MG14.1 instructions 
include the information needed to make the inferences discussed 
in Section 14.4.

622	
Chapter 14  Introduction to Multiple Regression 
MG14.5  Testing Portions of the 
Multiple Regression Model
You compute the coefficients of partial determination by using 
a two-step process. You first use the Section MG14.1 instruc-
tions to create all possible regression results in the same project 
file. For example, if you have two independent variables, you 
perform three regression analyses—Y with X1 and X2, Y with 
X1, and Y with X2—to create three sets of regression results. 
With those results you can then compute the partial F test and 
the coefficients of partial determination using the instructions 
in Section 14.5.
MG14.6  Using Dummy Variables 
and Interaction Terms in 
Regression Models
Dummy Variables
Use Text to Numeric to create a dummy variable. For example, to 
create from the categorical variable Fireplace the dummy variable 
named Fireplace Coded that is used in the Figure 14.10 regression 
model on page 592, open to the SilverSpring worksheet. Select 
Data ➔ Code ➔ Text to Numeric. In the Code - Text to Numeric 
dialog box (shown below):
	 1.	 Double-click C9  Fireplace in the variables list to add Fire-
place to the Code data from columns box and press Tab.
	 2.	 Enter C10 in the Store coded data in columns box and press 
Tab. (Column C10 is the first empty column in the work-
sheet.)
	 3.	 In the first row, enter Yes in the Original Values (eg, red 
"light blue") box and enter 1 in the New box.
	 4.	 In the second row, enter No in the Original Values (eg, red 
"light blue") box and enter 0 in the New box.
	 5.	 Click OK.
	 6.	 Enter Fireplace Coded as the name of column C10.
Interactions
Use Calculator to add a new column that contains the product 
of multiplying one independent variable by another to create an 
interaction term. For example, to create an interaction term of size 
and the dummy variable FireplaceCoded that is used in the Fig-
ure 14.11 regression model on page 594, open to the SilverSpring 
worksheet. Use the “Dummy Variables” instructions in the pre-
ceding part to create the Fireplace Coded column in the work-
sheet. Select Calc ➔ Calculator. In the Calculator dialog box 
(shown below):
	 1.	 Enter C11 in the Store result in variable box and press Tab.
	 2.	 Enter Size * 'Fireplace Coded' in the Expression box.
	 3.	 Click OK.
	 4.	 Enter Size*Fireplace as the name for column C11.
MG14.7  Logistic Regression
Use Binary Logistic Regression to perform a logistic regression. 
For example, to perform the Figure 14.14 logistic regression anal-
ysis shown on page 603, open to CardStudy worksheet. Select 
Stat ➔ Regression ➔ Binary Logistic Regression. In the Binary 
Logistic Regression dialog box (shown on page 623):
	 1.	 Click Response in response/frequency format and press 
Tab.
	 2.	 Double-click C1  Upgraded in the variables list to add 
­Upgraded in the Response box.
	 3.	 Click inside the Model box.
	 4.	 Double-click C2  Purchases in the variables list to add 
­Purchases to the Model box.
	 5.	 Double-click C3  Extra Cards in the variables list to add 
­'Extra Cards' to the Model box and press Tab.
	 6.	 Double-click C3  Extra Cards in the variables list to add 
'Extra Cards' to the Factors box (because Extra Cards is a 
­categorical variable).

	
Chapter 14 Minitab Guide	
623
	 7.	 Click OK.
MG14.8  Influence Analysis
Use Regression. Use the Section MG14.1 “Interpreting the  
Regression Coefficients” instructions, replacing step 19 of those  
instructions with the steps 19 through 22 listed below.
For example, to perform the Figure 14.15 analysis of the  
OmniPower sales data shown on page 606, replace step 19 with 
these steps 19 through 22:
	19.	 Back in the Regression dialog box, click Storage.
In the Regression - Storage dialog box:
	20.	 Check Deleted t residuals, Hi (leverages), and Cook’s 
­distance.
	21.	 Click OK.
	22.	 Back in the Regression dialog box, click OK.

624
U s i n g  S tat i s t i c s
Valuing Parsimony at WSTA-TV
Your job as the broadcast operations manager at local station WSTA-TV has 
proven more challenging of late, as you adjust to initiatives that have been an-
nounced by the station’s new owners. To meet the new business objective of re-
ducing expenses by 8% during the next fiscal year, you seek to investigate ways 
to reduce unnecessary labor expenses associated with the staff of graphic artists 
employed by the station. Currently, these graphic artists receive hourly pay for a 
significant number of standby hours, hours for which they are present at the sta-
tion but not assigned any specific task to do.
You believe that an appropriate model will help you to predict the number of 
future standby hours, identify the root causes of excessive numbers of standby hours, 
and allow you to reduce the total number of future standby hours. You plan to first 
collect weekly data for the number of standby hours and these four variables: the 
number of graphic artists present, the number of remote hours, the number of Dubner 
hours, and the total labor hours. Then, you seek to build a multiple regression model 
that will help determine which variables most heavily affect standby hours.
How do you build the model that has the most appropriate mix of independ-
ent variables? Are there statistical techniques that can help you identify a “best” 
model without having to consider all possible models?
contents
15.1  The Quadratic Regression 
Model
15.2  Using Transformations in 
Regression Models
15.3  Collinearity
15.4  Model Building
Steps for Successful Model 
Building
15.5  Pitfalls in Multiple Regression 
and Ethical Issues
Using Statistics: Valuing 
Parsimony at WSTA-TV, Revisited
Chapter 15 Excel Guide
Chapter 15 Minitab Guide
Objectives
To use quadratic terms in a 
regression model
To use transformed variables in a 
regression model
To measure the correlation among 
independent variables
To build a regression model using 
either the stepwise or best-
subsets approach
To avoid the pitfalls involved in 
developing a multiple regression 
model
Multiple Regression 
Model Building
15
Chapter
Alamy

	
15.1  The Quadratic Regression Model	
625
C
hapter 14 discussed multiple regression models with two independent variables. This 
chapter considers regression models that contain more than two independent vari-
ables. The chapter discusses model-building concepts that will help to develop the 
best model when confronted with a set of data that has many independent variables, such as 
the data to be collected at WSTA-TV. These concepts include quadratic independent variables, 
transformations of the dependent or independent variables, stepwise regression, and best-sub-
sets regression.
15.1  The Quadratic Regression Model
The simple regression model discussed in Chapter 13 and the multiple regression model dis-
cussed in Chapter 14 assume that the relationship between Y and each independent variable 
is linear. However, in Section 13.1, several different types of nonlinear relationships between 
variables were introduced. One of the most common nonlinear relationships is a quadratic, or 
curvilinear, relationship between two variables in which Y increases (or decreases) at a chang-
ing rate for various values of X (see Figure 13.1, Panels C through E, on page 520). You can 
use the quadratic regression model defined in Equation (15.1) to analyze this type of relation-
ship between X and Y.
Quadratic Regression Model
	
Yi = b0 + b1X1i + b2X2
1i + ei	
(15.1)
	
where
b0 = Y intercept
b1 = coefficient of the linear effect on Y
b2 = coefficient of the quadratic effect on Y
ei = random error in Y for observation i
This quadratic regression model is similar to the multiple regression model with two inde-
pendent variables [see Equation (14.2) on page 573] except that the second independent vari-
able, the quadratic term, is the square of the first independent variable. Once again, you use 
the least-squares method to compute sample regression coefficients (b0, b1, and b2) as estimates 
of the population parameters (b0, b1, and b2). Equation (15.2) defines the regression equation 
for the quadratic model with an independent variable 1X12 and a dependent variable (Y).
Student Tip
A quadratic regression 
model is a curvilinear 
model that has an X term 
and an X squared term. 
Other curvilinear models 
can have additional X 
terms that might involve 
X cubed, X raised to the 
fourth power, and so on.
Quadratic Regression Equation
	
Yni = b0 + b1X1i + b2X2
1i	
(15.2)
In Equation (15.2), the first regression coefficient, b0, represents the Y intercept; the sec-
ond regression coefficient, b1, represents the linear effect; and the third regression coefficient, 
b2, represents the quadratic effect.
Finding the Regression Coefficients and Predicting Y
To illustrate the quadratic regression model, consider a study that examined the business prob-
lem facing a concrete supplier of how adding fly ash affects the strength of concrete. (Fly 
ash is an inexpensive industrial waste by-product that can be used as a substitute for Portland  
cement, a more expensive ingredient of concrete.) Batches of concrete were prepared in which 

626	
Chapter 15  Multiple Regression Model Building
the percentage of fly ash ranged from 0% to 60%. Data were collected from a sample of  
18 batches and organized and stored in  FlyAsh . Table 15.1 summarizes the results.
By creating the scatter plot in Figure 15.1 to visualize these data, you will be better able to 
select the proper model for expressing the relationship between fly ash percentage and strength.
F i g u r e  1 5 . 1
Scatter plot of fly ash 
percentage (X) and 
strength (Y)
Figure 15.1 indicates an initial increase in the strength of the concrete as the percentage 
of fly ash increases. The strength appears to level off and then drop after achieving maximum 
strength at about 40% fly ash. Strength for 50% fly ash is slightly below strength at 40%, but 
strength at 60% fly ash is substantially below strength at 50%. Therefore, you should fit a qua-
dratic model, not a linear model, to estimate strength based on fly ash percentage.
Figure 15.2 shows Excel and Minitab results for these data. 
From Figure 15.2,
b0 = 4,486.3611 b1 = 63.0052 b2 = -0.8765
F i g u r e  1 5 . 2
Excel and Minitab regression results for the concrete strength data
T a b l e  1 5 . 1
Fly Ash Percentage and 
Strength of 18 Batches 
of 28-Day-Old Concrete
Fly Ash %
Strength (psi)
Fly Ash %
Strength (psi)
  0
4,779
40
5,995
  0
4,706
40
5,628
  0
4,350
40
5,897
20
5,189
50
5,746
20
5,140
50
5,719
20
4,976
50
5,782
30
5,110
60
4,895
30
5,685
60
5,030
30
5,618
60
4,648

	
15.1  The Quadratic Regression Model	
627
Therefore, the quadratic regression equation is
Yni = 4,486.3611 + 63.0052X1i - 0.8765X2
1i
where
Yni = predicted strength for sample i
X1i = percentage of fly ash for sample i
Figure 15.3 is a scatter plot of this quadratic regression equation that shows the fit of the 
quadratic regression model to the original data.
F i g u r e  1 5 . 3
Scatter plot showing the 
quadratic relationship 
between fly ash 
percentage and strength 
for the concrete data
From the quadratic regression equation and Figure 15.3, the Y intercept 1b0 = 4,486.36112 
is the predicted strength when the percentage of fly ash is 0. To interpret the coefficients b1 and 
b2, observe that after an initial increase, strength decreases as fly ash percentage increases. 
This nonlinear relationship is further demonstrated by predicting the strength for fly ash per-
centages of 20, 40, and 60. Using the quadratic regression equation,
Yni = 4,486.3611 + 63.0052X1i - 0.8765X2
1i
for X1i = 20,
Yni = 4,486.3611 + 63.00521202 - 0.876512022 = 5,395.865
for X1i = 40,
Yni = 4,486.3611 + 63.00521402 - 0.876514022 = 5,604.169
and for X1i = 60,
Yni = 4,486.3611 + 63.00521602 - 0.876516022 = 5,111.273
Thus, the predicted concrete strength for 40% fly ash is 208.304 psi above the predicted 
strength for 20% fly ash, but the predicted strength for 60% fly ash is 492.896 psi below the 
predicted strength for 40% fly ash. The concrete supplier should consider using a fly ash per-
centage of 40% and not using fly ash percentages of 20% or 60% because those percentages 
lead to reduced concrete strength.
Testing for the Significance of the Quadratic Model
After you calculate the quadratic regression equation, you can test whether there is a signifi-
cant overall relationship between strength, Y, and fly ash percentage, X1. The null and alterna-
tive hypotheses are as follows:
 H0: b1 = b2 = 0 1There is no overall relationship between X1 and Y.2
 H1: b1 and>or b2 ≠0 1There is an overall relationship between X1 and Y.2
Student Tip
Remember that you are 
testing whether at least 
one independent variable 
has a linear relation-
ship with the dependent 
variable. If you reject H0, 
you are not concluding 
that all the independent 
variables have a linear 
relationship with the 
dependent variable, only 
that at least one inde-
pendent variable does.

628	
Chapter 15  Multiple Regression Model Building
Equation (14.6) on page 579 defines the overall FSTAT test statistic used for this test:
FSTAT = MSR
MSE
From the Figure 15.2 results on page 626,
FSTAT = MSR
MSE = 1,347,736.745
97,414.4674
= 13.8351
If you choose a level of significance of 0.05, from Table E.5, the critical value of the F distri-
bution, with 2 and 18 - 2 - 1 = 15 degrees of freedom, is 3.68 (see Figure 15.4). Because 
FSTAT = 13.8351 7 3.68, or because the p-value = 0.0004 6 0.05, you reject the null hy-
pothesis 1H02 and conclude that there is a significant overall relationship between strength and 
fly ash percentage.
0
F
3.68
Region of
Nonrejection
Region of
Rejection
Critical
Value
F i g u r e  1 5 . 4
Testing for the existence 
of the overall relationship 
at the 0.05 level of 
significance, with 2 and 
15 degrees of freedom
Testing the Quadratic Effect
In using a regression model to examine a relationship between two variables, you want to find 
not only the most accurate model but also the simplest model that expresses that relationship. 
Therefore, you need to examine whether there is a significant difference between the quadratic 
model:
Yi = b0 + b1X1i + b2X2
1i + ei
and the linear model:
Yi = b0 + b1X1i + ei
In Section 14.4, you used the t test to determine whether each independent variable makes a 
significant contribution to the regression model. To test the significance of the contribution of 
the quadratic effect, you use the following null and alternative hypotheses:
 H0: Including the quadratic effect does not significantly improve the model 1b2 = 02.
 H1: Including the quadratic effect significantly improves the model 1b2 ≠02.
The standard error of each regression coefficient and its corresponding tSTAT test statistic are 
part of the regression results (see Figure 15.2 on page 626). Equation (14.7) on page 583  
defines the tSTAT test statistic:
 tSTAT = b2 - b2
Sb2
 = -0.8765 - 0
0.1966
= -4.4578
If you select the 0.05 level of significance, then from Table E.3, the critical values for the t dis-
tribution with 15 degrees of freedom are -2.1315 and +2.1315 (see Figure 15.5).

	
15.1  The Quadratic Regression Model	
629
Because tSTAT = -4.4578 6 -2.1315 or because the p-value = 0.0005 6 0.05, you re-
ject H0 and conclude that the quadratic model is significantly better than the linear model for 
representing the relationship between strength and fly ash percentage.
Example 15.1 provides an additional illustration of a possible quadratic effect.
F i g u r e  1 5 . 5
Testing for the 
contribution of the 
quadratic effect to a 
regression model at the 
0.05 level of significance, 
with 15 degrees of 
freedom
–2.1315
+2.1315
0
Region of
Rejection
Region of
Nonrejection
Critical
Value
Region of
Rejection
Critical
Value
t
Example 15.1
Studying the  
Quadratic Effect  
in a Multiple  
Regression Model
A real estate developer studying the business problem of estimating the consumption of heat-
ing oil by single-family houses has decided to examine the effect of atmospheric tempera-
ture and the amount of attic insulation on heating oil consumption. Data are collected from a 
random sample of 15 single-family houses. The data are organized and stored in  HeatingOil . 
Figure 15.6 shows the regression results for a multiple regression model using the two inde-
pendent variables: atmospheric temperature and attic insulation.
F i g u r e  1 5 . 6
Excel and Minitab results for the multiple linear regression model predicting monthly consumption of heating oil
The residual plot for attic insulation (not shown here) contained some evidence of a qua-
dratic effect. Thus, the real estate developer reanalyzed the data by adding a quadratic term 
for attic insulation to the multiple regression model. At the 0.05 level of significance, is there 
evidence of a significant quadratic effect for attic insulation?
(continued)

630	
Chapter 15  Multiple Regression Model Building
The Coefficient of Multiple Determination
In the multiple regression model, the coefficient of multiple determination, r 2 (see  
Section 14.2), represents the proportion of variation in Y that is explained by variation in the in-
dependent variables. Consider the quadratic regression model you used to predict the strength 
of concrete using fly ash and fly ash squared. You compute r 2 by using Equation (14.4) on 
page 578:
r 2 = SSR
SST
From Figure 15.2 on page 626,
SSR = 2,695,473.4897   SST = 4,156,690.5
Thus,
r 2 = SSR
SST = 2,695,473.4897
4,156,690.5
= 0.6485
Solution  Figure 15.7 shows the results for this regression model.
F i g u r e  1 5 . 7
Excel and Minitab results for the multiple regression model with a quadratic term for attic insulation
The multiple regression equation is
Yni = 624.5864 - 5.3626X1i - 44.5868X2i + 1.8667X2
2i
To test for the significance of the quadratic effect:
H0: Including the quadratic effect of insulation does not significantly improve the 
    model 1b3 = 02.
H1: Including the quadratic effect of insulation significantly improves the model 1b3 ≠02.
From Figure 15.7 and Table E.3 with 15 - 3 - 1 = 11 degrees of freedom, -2.2010
6 tSTAT = 1.6611 6 2.2010 (or the p@value = 0.1249 7 0.05). Therefore, you do not re-
ject the null hypothesis. You conclude that there is insufficient evidence that the quadratic  
effect for attic insulation is different from zero. In the interest of keeping the model as simple 
as possible, you should use the multiple regression equation shown in Figure 15.6:
Yni = 562.1510 - 5.4366X1i - 20.0123X2i
Student Tip
Remember that r 2 in 
multiple regression 
represents the propor-
tion of the variation in the 
dependent variable Y that 
is explained by all the 
independent X variables 
included in the model. So, 
in this case of quadratic 
regression, r 2 represents 
the proportion of the 
variation in the dependent 
variable Y  that is ex-
plained by the linear term 
and the quadratic term.

	
15.1  The Quadratic Regression Model	
631
This coefficient of multiple determination indicates that 64.85% of the variation in strength 
is explained by the quadratic relationship between strength and the percentage of fly ash. You 
should also compute r 2
adj to account for the number of independent variables and the sample 
size. In the quadratic regression model, k = 2 because there are two independent variables, X1 
and X2
1. Thus, using Equation (14.5) on page 578,
 r 2
adj = 1 - c11 - r 22
1n - 12
1n - k - 12 d
 = 1 - c11 - 0.6485217
15 d
 = 1 - 0.3984
 = 0.6016
Problems for Section 15.1
Learning the Basics
15.1  Consider an event of a boy participating in a shot put com-
petition. In a recent competition he won the competition by throw-
ing a shot put to a height of 23 meters, covering a distance of 
50 meters. Experts believe that if he reduces the height, then he 
can throw even farther. Thus, they wanted to establish the relation-
ship between the height and the length covered by the ball. The 
following data has been collected:
Distance (meters)
7
21
34
48
61
68
Height (meters)
9
16
25
27
25
22
a.	 Plot the given figures on a graph and comment if linear regres-
sion equation is the best fit.
b.	 Formulate a quadratic multiple regression equation for the 
given data.
c.	 Predict the height if the distance covered by the ball is  
30 ­meters.
Applying the Concepts
15.2  The law of diminishing marginal utility specifies the satisfac-
tion derived from the consumption of additional units of a product. 
According to it, utility increases first and subsequently increases at 
a diminishing rate and finally decreases. A similar theory has been 
applied to estimate people’s feelings towards the height of individu-
als. The author proved that people’s level of satisfaction increases if 
their height increases at a certain level. Thereafter, it starts decreasing 
(data extracted from: http://ftp.iza.org/dp3344.pdf). The following 
equation has been derived by the author by considering height as an 
independent variable and subjective individual’s feelings as a depen-
dent variable.
Subjective feeling of height = -54.28 + 0.67 Actual height -  
  0.0015 actual height squared
a.	 Assume the value of height starting from 100 to 400, with 20 
consecutive intervals. Construct a table for predicting the sub-
jective feeling of people.
b.	 Plot the data obtained on a graph. Assuming the same dependent 
and independent variable, which variable would you present as 
legend entries?
c.	 Based on the graph, relate it to the definition of law of marginal 
utility which is introduced at the beginning of this question.
d.	 Based on the table calculated in (a), calculate the coefficient of 
multiple determination. What conclusions can you draw about 
the relationship between the height of individuals and the rela-
tive subjective feelings?
15.3  A national chain of consumer electronics stores had the busi-
ness objective of determining the effectiveness of newspaper adver-
tising. To promote sales, the chain relies heavily on local newspaper 
advertising to support its modest exposure in nationwide television 
commercials. A sample of 20 cities with similar populations and 
monthly sales totals were assigned different newspaper advertising 
budgets for one month. The following table, stored in  Advertising , 
summarizes the sales (in $millions) and the newspaper advertising 
budgets (in $thousands) observed during the study:
Sales  
($millions)
Newspaper  
Advertising  
($thousands)
Sales  
($millions)
Newspaper  
Advertising  
($thousands)
6.14
  5
6.84
15
6.04
  5
6.66
15
6.21
  5
6.95
20
6.32
  5
6.65
20
6.42
10
6.83
20
6.56
10
6.81
20
6.67
10
7.03
25
6.35
10
6.88
25
6.76
15
6.84
25
6.79
15
6.99
25
a.	 Construct a scatter plot for newspaper advertising and sales.
b.	 Fit a quadratic regression model and state the quadratic regression 
equation.
c.	 Predict the mean monthly sales for a city with newspaper  
advertising of $20,000.
d.	 Perform a residual analysis on the results and determine 
whether the regression assumptions are valid.
e.	 At the 0.05 level of significance, is there a significant quadratic 
relationship between monthly sales and newspaper advertising?

632	
Chapter 15  Multiple Regression Model Building
f.	 At the 0.05 level of significance, determine whether the quadratic 
model is a better fit than the linear model.
g.	 Interpret the meaning of the coefficient of multiple determination.
h.	 Compute the adjusted r2.
i.	 What conclusions can you reach concerning the relationship 
between newspaper advertising and sales?
15.4  Is the number of calories in a beer related to the number of 
carbohydrates and/or the percentage of alcohol in the beer? Data 
concerning 152 of the best-selling domestic beers in the United 
States are stored in  DomesticBeer . The values for three variables 
are included: the number of calories per 12 ounces, the alco-
hol percentage, and the number of carbohydrates (in grams) per  
12 ounces. (Data extracted from www.beer100.com/beercalories 
.htm, March 21, 2013.)
a.	 Perform a multiple linear regression analysis, using calories as 
the dependent variable and percentage alcohol and number of 
carbohydrates as the independent variables.
b.	 Add quadratic terms for alcohol percentage and the number of 
carbohydrates.
c.	 Which model is better, the one in (a) or (b)?
d.	 What conclusions can you reach concerning the relationship 
between the number of calories in a beer and the alcohol per-
centage and number of carbohydrates?
15.5  In the production of printed circuit boards, errors in the 
alignment of electrical connections are a source of scrap. The data 
in the file  RegistrationError-HighCost  contains the registration  
error and the temperature used in the production of circuit boards 
in an experiment in which higher cost material was used. (Data  
extracted from C. Nachtsheim and B. Jones, “A Powerful Analytical  
Tool,” Six Sigma Forum Magazine, August 2003, pp. 30–33.)
a.	 Construct a scatter plot for temperature and registration error.
b.	 Fit a quadratic regression model to predict registration error 
and state the quadratic regression equation.
c.	 Perform a residual analysis on the results and determine 
whether the regression model is valid.
d.	 At the 0.05 level of significance, is there a significant quadratic 
relationship between temperature and registration error?
e.	 At the 0.05 level of significance, determine whether the qua-
dratic model is a better fit than the linear model.
f.	 Interpret the meaning of the coefficient of multiple determination.
g.	 Compute the adjusted r2.
h.	 What conclusions can you reach concerning the relationship 
between registration error and temperature?
SELF 
Test 
15.6  A production manager wishes to examine the re-
lationship between unit production (number of units 
produced) and associated costs (total cost). The file  CostEstimation   
contains data for 10 months of production.
a.	 Construct a scatter plot for unit production and total cost.
b.	 Fit a quadratic regression model to predict total cost and state 
the quadratic regression equation.
c.	 Predict the mean total cost when production is 145 units.
d.	 Perform a residual analysis on the results and determine 
whether the regression model is valid.
e.	 At the 0.05 level of significance, is there a significant quadratic 
relationship between monthly unit production and total cost?
f.	 What is the p-value in (e)? Interpret its meaning.
g.	 At the 0.05 level of significance, determine whether the qua-
dratic model is a better fit than the linear model.
h.	 What is the p-value in (g)? Interpret its meaning.
i.	 Interpret the meaning of the coefficient of multiple determination.
j.	 Compute the adjusted r2.
k.	 What conclusions can you reach concerning the relationship 
between cost and unit production?
15.7  Researchers wanted to investigate the relationship between 
employment and accommodation capacity in the European travel 
and tourism industry. The file  EuroTourism  contains a sample of 
27 European countries. Variables included are the number of jobs 
generated in the travel and tourism industry in 2012 and the num-
ber of establishments that provide overnight accommodation for 
tourists. (Data extracted from www.marketline.com.)
a.	 Construct a scatter plot of the number of jobs generated in the 
travel and tourism industry in 2012 (Y) and the number of estab-
lishments that provide overnight accommodation for tourists (X).
b.	 Fit a quadratic regression model to predict the number of jobs 
generated and state the quadratic regression equation.
c.	 Predict the mean number of jobs generated in the travel and 
tourism industry for a country with 3,000 establishments that 
provide overnight accommodation for tourists.
d.	 Perform a residual analysis on the results and determine 
whether the regression model is valid.
e.	 At the 0.05 level of significance, is there a significant quadratic 
relationship between the number of jobs generated in the travel 
and tourism industry in 2012 and the number of establishments 
that provide overnight accommodation for tourists?
f.	 What is the p-value in (e)? Interpret its meaning.
g.	 At the 0.05 level of significance, determine whether the qua-
dratic model is a better fit than the linear model.
h.	 What is the p-value in (g)? Interpret its meaning.
i.	 Interpret the meaning of the coefficient of multiple determination.
j.	 Compute the adjusted r2.
k.	 What conclusions can you reach concerning the relationship 
between the number of jobs generated in the travel and tourism 
industry in 2012 and the number of establishments that provide 
overnight accommodation for tourists?
15.2  Using Transformations in Regression Models
This section introduces regression models in which the independent variable, the dependent 
variable, or both are transformed in order to either overcome violations of the assumptions of 
regression or to make a model whose form is not linear into a linear model. Among the many 
transformations available (see reference 1) are the square-root transformation and transformations 
involving the common logarithm (base 10) and the natural logarithm (base e).1
1For more information on loga-
rithms, see Appendix Section A.3.

	
15.2  Using Transformations in Regression Models	
633
The Square-Root Transformation
The square-root transformation is often used to overcome violations of the equal-­variance 
assumption as well as to transform a model whose form is not linear into a linear model. 
Equation (15.3) shows a regression model that uses a square-root transformation of the 
­independent variable.
Student Tip
The log (log) of a number 
is the power to which 
10 needs to be raised 
to equal that number. 
The natural log (ln) of a 
number is the power to 
which e, Euler’s number, 
needs to be raised to 
equal that number.
Regression Model with a Square-Root Transformation
	
Yi = b0 + b12X1i + ei	
(15.3)
Example 15.2 illustrates the use of a square-root transformation.
X
Y
X
Y
1
55.0
3
105.7
1
65.0
4
120.1
2
87.9
4
112.3
2
79.8
5
121.5
3
98.5
5
129.9
Example 15.2
Using the Square-
Root Transformation
Given the following values for X and Y, use a square-root transformation for the X variable:
Construct a scatter plot for X and Y and for the square root of X and Y.
Solution  Figure 15.8 displays both scatter plots.
F i g u r e  1 5 . 8
Example 15.2 scatter plots of X and Y and the square root of X and Y
You can see that the square-root transformation has transformed a nonlinear relationship 
into a linear relationship.
The Log Transformation
The logarithmic transformation is often used to overcome violations of the equal-variance 
assumption. You can also use the logarithmic transformation to change a nonlinear model into 
a linear model. Equation (15.4) shows a multiplicative model.

634	
Chapter 15  Multiple Regression Model Building
By taking base 10 logarithms of both the dependent and independent variables, you can 
transform Equation (15.4) to the model shown in Equation (15.5).
Original Multiplicative Model
	
Yi = b0Xb1
1i Xb2
2i ei	
(15.4)
Transformed Multiplicative Model
	
 log Yi = log1b0Xb1
1i Xb2
2i ei2
	
(15.5)
	
 = log b0 + log1Xb1
1i 2 + log1Xb2
2i 2 + log ei 	
	
 = log b0 + b1log X1i + b2 log X2i + log ei	
Original Exponential Model
	
Yi = eb0+b1X1i+b2X2i ei	
(15.6)
Transformed Exponential Model
	
 ln Yi = ln1eb0 + b1X1i + b2X2i ei2
	
(15.7)
	
 = ln1eb0 + b1X1i + b2X2i2 + ln ei	
	
 = b0 + b1X1i + b2X2i + ln ei	
Thus, Equation (15.5) is linear in the logarithms. Similarly, you can transform the expo-
nential model shown in Equation (15.6) to a linear form by taking the natural logarithm of both 
sides of the equation. Equation (15.7) is the transformed model.
Example 15.3 illustrates the use of a natural log transformation.
X
Y
X
Y
1
0.7
3
  4.8
1
0.5
4
12.9
2
1.6
4
11.5
2
1.8
5
32.1
3
4.2
5
33.9
Example 15.3
Using the Natural 
Log Transformation
Given the following values for X and Y, use a natural logarithm transformation for the Y variable:
Construct a scatter plot for X and Y and for X and the natural logarithm of Y.
Solution  Figure 15.9 displays both scatter plots. The plots show that the natural logarithm 
transformation has changed a nonlinear relationship into a linear relationship.

	
15.2  Using Transformations in Regression Models	
635
F i g u r e  1 5 . 9
Example 15.3 scatter plots of X and Y and X and the natural logarithm of Y
c.	 At the 0.05 level of significance, is there a significant relation-
ship between the natural logarithm of calories and the percent-
age of alcohol and the number of carbohydrates?
d.	 Interpret the meaning of the coefficient of determination, r2, in 
this problem.
e.	 Compute the adjusted r2.
f.	 Compare your results with those in Problems 15.4 and 15.10. 
Which model is best? Why?
15.12  Using the data of Problem 15.6 on page 632, stored in  
 CostEstimation , perform a natural logarithm transformation of 
the dependent variable (total cost). Using the transformed depen-
dent variable and the unit production as the independent variable, 
perform a regression analysis.
a.	 State the regression equation.
b.	 Predict the mean total cost when production is 145 units.
c.	 Perform a residual analysis of the results and determine 
whether the regression assumptions are valid.
d.	 At the 0.05 level of significance, is there a significant relationship 
between the natural logarithm of total cost and unit production?
e.	 Interpret the meaning of the coefficient of determination, r2, in 
this problem.
f.	 Compute the adjusted r2.
g.	 Compare your results with those in Problem 15.6. Which model 
is better? Why?
15.13  Using the data of Problem 15.6 on page 632, stored in  
 CostEstimation , perform a square-root transformation of the inde-
pendent variable (unit production). Using total cost as the depen-
dent variable and the transformed independent variable, perform a 
regression analysis.
a.	 State the regression equation.
b.	 Predict the mean total cost when production is 145 units.
c.	 Perform a residual analysis of the results and determine 
whether the regression model is valid.
d.	 At the 0.05 level of significance, is there a significant relation-
ship between total cost and the square root of unit production?
e.	 Interpret the meaning of the coefficient of determination, r2, in 
this problem.
f.	 Compute the adjusted r2.
g.	 Compare your results with those of Problems 15.6 and 15.12. 
Which model is best? Why?
Problems for Section 15.2
Learning the Basics
15.8  Consider the following regression equation:
log Yni = log 3.07 + 0.9 log X1i + 1.41 log X2i
a.	 Predict the value of Y when X1 = 8.5 and X2 = 5.2.
b.	 Interpret the meaning of the regression coefficients b0, b1, and b2.
15.9  Consider the following regression equation:
ln Yni = 4.62 + 0.5X1i + 0.7X2i
a.	 Predict the value of Y when X1 = 8.5 and X2 = 5.2.
b.	 Interpret the meaning of the regression coefficients b0, b1, and b2.
Applying the Concepts
SELF 
Test 
15.10  Using the data of Problem 15.4 on page 632, 
stored in  DomesticBeer , perform a square-root trans-
formation on each of the independent variables (percentage alco-
hol and number of carbohydrates). Using calories as the dependent 
variable and the transformed independent variables, perform a 
multiple regression analysis.
a.	 State the regression equation.
b.	 Perform a residual analysis of the results and determine 
whether the regression model is valid.
c.	 At the 0.05 level of significance, is there a significant relation-
ship between calories and the square root of the percentage of 
alcohol and the square root of the number of carbohydrates?
d.	 Interpret the meaning of the coefficient of determination, r2, in 
this problem.
e.	 Compute the adjusted r2.
f.	 Compare your results with those in Problem 15.4. Which model 
is better? Why?
15.11  Using the data of Problem 15.4 on page 632, stored in ­ 
 DomesticBeer , perform a natural logarithmic transformation of the 
dependent variable (calories). Using the transformed dependent vari-
able and the percentage of alcohol and the number of carbohydrates 
as the independent variables, perform a multiple regression analysis.
a.	 State the regression equation.
b.	 Perform a residual analysis of the results and determine 
whether the regression assumptions are valid.

636	
Chapter 15  Multiple Regression Model Building
15.3  Collinearity
One important problem in developing multiple regression models involves the possible  
collinearity of the independent variables. This condition refers to situations in which two or 
more of the independent variables are highly correlated with each other. In such situations, 
collinear variables do not provide unique information, and it becomes difficult to separate the 
effects of such variables on the dependent variable. When collinearity exists, the values of 
the regression coefficients for the correlated variables may fluctuate drastically, depending on 
which independent variables are included in the model.
One method of measuring collinearity is to determine the variance inflationary factor 
(VIF) for each independent variable. Equation (15.8) defines VIFj, the variance inflationary 
factor for variable j.
Variance Inflationary Factor
	
VIFj =
1
1 - R2
j
	
(15.8)
where R2
j  is the coefficient of multiple determination for a regression model, using variable 
Xj as the dependent variable and all other X variables as independent variables.
If there are only two independent variables, R2
1 is the coefficient of determination between 
X1 and X2. It is identical to R2
2, which is the coefficient of determination between X2 and X1. If 
there are three independent variables, then R2
1 is the coefficient of multiple determination of X1 
with X2 and X3; R2
2 is the coefficient of multiple determination of X2 with X1 and X3; and R2
3 is 
the coefficient of multiple determination of X3 with X1 and X2.
If a set of independent variables is uncorrelated, each VIFj is equal to 1. If the set is highly 
correlated, then a VIFj might even exceed 10. Marquardt (see reference 2) suggests that if VIFj is 
greater than 10, there is too much correlation between the variable Xj and the other independent 
variables. However, other statisticians suggest a more conservative criterion. Snee (see reference 5)  
recommends using alternatives to least-squares regression if the maximum VIFj exceeds 5.
You need to proceed with extreme caution when using a multiple regression model that has 
one or more large VIF values. You can use the model to predict values of the dependent vari-
able only in the case where the values of the independent variables used in the prediction are 
in the relevant range of the values in the data set. However, you cannot extrapolate to values 
of the independent variables not observed in the sample data. And because the independent 
variables contain overlapping information, you should always avoid interpreting the regression 
coefficient estimates separately because there is no way to accurately estimate the individual 
effects of the independent variables. One solution to the problem is to delete the variable with 
the largest VIF value. The reduced model (i.e., the model with the independent variable with 
the largest VIF value deleted) is often free of collinearity problems. If you determine that all the 
independent variables are needed in the model, you can use methods discussed in reference 1.
In the OmniPower sales data (see Section 14.1), the correlation between the two indepen-
dent variables, price and promotional expenditure, is -0.0968. Because there are only two 
independent variables in the model, from Equation (15.8):
 VIF1 = VIF2 =
1
1 - 1-0.096822
 = 1.009
Thus, you can conclude that you should not be concerned with collinearity for the OmniPower 
sales data.

	
15.4  Model Building	
637
In models containing quadratic and interaction terms, collinearity is usually present. The linear 
and quadratic terms of an independent variable are usually highly correlated with each other, and 
an interaction term is often correlated with one or both of the independent variables making up the 
interaction. Thus, you cannot interpret individual regression coefficients separately. You need to in-
terpret the linear and quadratic regression coefficients together in order to understand the nonlinear 
relationship. Likewise, you need to interpret an interaction regression coefficient in conjunction 
with the two regression coefficients associated with the variables comprising the interaction. In 
summary, large VIFs in quadratic or interaction models do not necessarily mean that the model is 
not a good one. They do, however, require you to carefully interpret the regression coefficients.
Problems for Section 15.3
Learning the Basics
15.14  If the coefficient of determination between two indepen-
dent variables is 0.20, what is the VIF?
15.15  What is variance inflationary factor (VIF)? Assuming  
VIFj = 7, what conclusion can you draw regarding the applicability 
of the regression model?
Applying the Concepts
SELF 
Test 
15.16  Refer to Problem 14.4 on page 576. Perform  
a multiple regression analysis using the data in 
 WareCost  and determine the VIF for each independent variable in 
the model. Is there reason to suspect the existence of collinearity?
15.17  Refer to Problem 14.5 on page 577. Perform a multiple 
regression analysis using the data in  VinhoVerde  and determine 
the VIF for each independent variable in the model. Is there reason 
to suspect the existence of collinearity?
15.18  Refer to Problem 14.6 on page 577. Perform a multiple 
regression analysis using the data in  Advertise  and determine the 
VIF for each independent variable in the model. Is there reason to 
suspect the existence of collinearity?
15.19  Refer to Problem 14.7 on page 577. Perform a multiple 
regression analysis using the data in  Standby  and determine the 
VIF for each independent variable in the model. Is there reason to 
suspect the existence of collinearity?
15.20  Refer to Problem 14.8 on page 577. Perform a multiple 
regression analysis using the data in  GlenCove  and determine the 
VIF for each independent variable in the model. Is there reason to 
suspect the existence of collinearity?
15.4  Model Building
This chapter and Chapter 14 have introduced you to many different topics in regression analy-
sis, including quadratic terms, dummy variables, and interaction terms. In this section, you 
learn a structured approach to building the most appropriate regression model. As you will see, 
successful model building incorporates many of the topics you have studied so far.
To begin, refer to the WSTA-TV scenario introduced on page 624, in which four indepen-
dent variables (total staff present, remote hours, Dubner hours, and total labor hours) are con-
sidered in the business problem that involves developing a regression model to predict standby 
hours of unionized graphic artists. Data are collected over a period of 26 weeks and organized 
and stored in  Standby . Table 15.2 summarizes these data.
T a b l e  1 5 . 2
Predicting Standby 
Hours Based on Total 
Staff Present, Remote 
Hours, Dubner Hours, 
and Total Labor Hours
Week
Standby  
Hours (Y)
Total Staff  
Present 1X12
Remote  
Hours 1X22
Dubner  
Hours 1X32
Total Labor  
Hours 1X42
  1
245
338
414
323
2,001
  2
177
333
598
340
2,030
  3
271
358
656
340
2,226
  4
211
372
631
352
2,154
  5
196
339
528
380
2,078
  6
135
289
409
339
2,080
  7
195
334
382
331
2,073
  8
118
293
399
311
1,758
  9
116
325
343
328
1,624
10
147
311
338
353
1,889
11
154
304
353
518
1,988
12
146
312
289
440
2,049
13
115
283
388
276
1,796
(continued)

638	
Chapter 15  Multiple Regression Model Building
To develop a model to predict the dependent variable, standby hours in the WSTA-TV sce-
nario, you need to be guided by a general problem-solving strategy, or heuristic. One heuristic 
appropriate for building regression models uses the principle of parsimony.
Parsimony guides you to select the regression model with the fewest independent vari-
ables that can predict the dependent variable adequately. Regression models with fewer inde-
pendent variables are easier to interpret, particularly because they are less likely to be affected 
by collinearity problems (described in Section 15.3).
Developing an appropriate model when many independent variables are under consider-
ation involves complexities that are not present with a model that has only two independent 
variables. The evaluation of all possible regression models is more computationally complex. 
And, although you can quantitatively evaluate competing models, there may not be a uniquely 
best model but several equally appropriate models.
To begin analyzing the standby-hours data, you compute the variance inflationary factors 
[see Equation (15.8) on page 636] to measure the amount of collinearity among the indepen-
dent variables. The values for the four VIFs for this model appear in Figure 15.10, along with 
the results for the model that uses the four independent variables.
Week
Standby  
Hours (Y)
Total Staff  
Present 1X12
Remote  
Hours 1X22
Dubner  
Hours 1X32
Total Labor  
Hours 1X42
14
161
307
402
207
1,720
15
274
322
151
287
2,056
16
245
335
228
290
1,890
17
201
350
271
355
2,187
18
183
339
440
300
2,032
19
237
327
475
284
1,856
20
175
328
347
337
2,068
21
152
319
449
279
1,813
22
188
325
336
244
1,808
23
188
322
267
253
1,834
24
197
317
235
272
1,973
25
261
315
164
223
1,839
26
232
331
270
272
1,935
T a b l e  1 5 . 2
(continued)
F i g u r e  1 5 . 1 0
Excel and Minitab regression results for predicting standby hours based on four independent variables (Excel results 
contain additional worksheets for Durbin-Watson statistic, inset, and VIF)

	
15.4  Model Building	
639
Observe that all the VIF values in Figure 15.10 are relatively small, ranging from a high of 
1.9993 for the total labor hours to a low of 1.2333 for remote hours. Thus, on the basis of the 
criteria developed by Snee that all VIF values should be less than 5.0 (see reference 5), there is 
little evidence of collinearity among the set of independent variables.
The Stepwise Regression Approach to Model Building
You continue your analysis of the standby-hours data by attempting to determine whether a 
subset of all independent variables yields an adequate and appropriate model. The first ap-
proach described here is stepwise regression, which attempts to find the “best” regression 
model without examining all possible models.
The first step of stepwise regression is to find the best model that uses one independent 
variable. The next step is to find the best of the remaining independent variables to add to 
the model selected in the first step. An important feature of the stepwise approach is that an 
independent variable that has entered into the model at an early stage may subsequently be re-
moved after other independent variables are considered. Thus, in stepwise regression, variables 
are either added to or deleted from the regression model at each step of the model-building 
process. The t test for the slope (see Section 14.4) or the partial FSTAT test statistic (see Sec-
tion 14.5) is used to determine whether variables are added or deleted. The stepwise procedure 
terminates with the selection of a best-fitting model when no additional variables can be added 
to or deleted from the last model evaluated. Figure 15.11 below shows the Excel (PHStat) and 
Minitab stepwise regression results for the standby-hours data.
F i g u r e  1 5 . 1 1
Excel (PHStat) and Minitab stepwise regression results for the standby-hours data
For this example, a significance level of 0.05 is used to enter a variable into the model or to 
delete a variable from the model. The first variable entered into the model is total staff, the vari-
able that correlates most highly with the dependent variable standby hours. Because the p-value 
of 0.0011 is less than 0.05, total staff is included in the regression model.
The next step involves selecting a second independent variable for the model. The second 
variable chosen is one that makes the largest contribution to the model, given that the first 
variable has been selected. For this model, the second variable is remote hours. Because the 
p-value of 0.0269 for remote hours is less than 0.05, the remote hours variable is included in 
the regression model.

640	
Chapter 15  Multiple Regression Model Building
After the remote hours variable is entered into the model, the stepwise procedure determines 
whether total staff is still an important contributing variable or whether it can be eliminated from 
the model. Because the p-value of 0.0001 for total staff is less than 0.05, total staff remains in the 
regression model.
The next step involves selecting a third independent variable for the model. Because 
none of the other variables meets the 0.05 criterion for entry into the model, the stepwise 
procedure terminates with a model that includes total staff present and the number of re-
mote hours.
This stepwise regression approach to model building was originally developed more than 
four decades ago, when regression computations on computers were time-consuming and 
costly. Although stepwise regression limited the evaluation of alternative models, the method 
was deemed a good trade-off between evaluation and cost.
Given the ability of today’s computers to perform regression computations at very low 
cost and high speed, stepwise regression has been superseded to some extent by the best-­
subsets approach, discussed next, which evaluates a larger set of alternative models. Stepwise 
regression is not obsolete, however. Today, many businesses use stepwise regression as part of 
data mining, techniques that allow the extraction of useful knowledge from large repositories 
of data (see Section 17.2).
The Best-Subsets Approach to Model Building
The best-subsets approach evaluates all possible regression models for a given set of indepen-
dent variables. Figure 15.12 presents best-subsets regression results of all possible regression 
models for the standby-hours data.
F i g u r e  1 5 . 1 2
Excel (using PHStat) and 
Minitab best-subsets 
regression results for the 
standby-hours data
A criterion often used in model building is the adjusted r2, which adjusts the r2 of each 
model to account for the number of independent variables in the model as well as for the 
sample size (see Section 14.2). Because model building requires you to compare models with 
different numbers of independent variables, the adjusted r2 is more appropriate than r2. Refer-
ring to Figure 15.12, you see that the adjusted r2 reaches a maximum value of 0.5513 when all 
four independent variables plus the intercept term (for a total of five estimated parameters) are 
included in the model.
A second criterion often used in the evaluation of competing models is the Cp statistic  
developed by Mallows (see reference 1). The Cp statistic, defined in Equation (15.9),  
measures the differences between a fitted regression model and a true model, along with 
random error.

	
15.4  Model Building	
641
Using Equation (15.9) to compute Cp for the model containing total staff and remote hours,
n = 26  k = 2  T = 4 + 1 = 5  R2
k = 0.4899  R2
T = 0.6231
so that
 Cp = 11 - 0.48992126 - 52
1 - 0.6231
- 326 - 212 + 124
 = 8.4193
When a regression model with k independent variables contains only random differences 
from a true model, the mean value of Cp is k + 1, the number of parameters. Thus, in evaluat-
ing many alternative regression models, the goal is to find models whose Cp is close to or less 
than k + 1. In Figure 15.12, you see that only the model with all four independent variables 
considered contains a Cp value close to or below k + 1. Therefore, using the Cp criterion, you 
should choose that model.
Although it is not the case here, the Cp statistic often provides several alternative models for 
you to evaluate in greater depth. Moreover, the best model or models using the Cp criterion might 
differ from the model selected using the adjusted r2 and/or the model selected using the stepwise 
procedure. (Note here that the model selected using stepwise regression has a Cp value of 8.4193, 
which is substantially above the suggested criterion of k + 1 = 3 for that model.) Remember 
that there may not be a uniquely best model, but there may be several equally appropriate mod-
els. Final model selection often involves using subjective criteria, such as parsimony, interpret-
ability, and departure from model assumptions (as evaluated by residual analysis).
When you have finished selecting the independent variables to include in the model, you 
need to perform a residual analysis to evaluate the regression assumptions, and because the 
data were collected in time order, you also need to compute the Durbin-Watson statistic to de-
termine whether there is autocorrelation in the residuals (see Section 13.6). From Figure 15.10 
on page 638, you see that the Durbin-Watson statistic, D, is 2.2197. Because D is greater than 
2.0, there is no indication of positive correlation in the residuals. Figure 15.13 on page 642 
presents the plots used in the residual analysis.
None of the residual plots versus the total staff, the remote hours, the Dubner hours, and 
the total labor hours reveal apparent patterns. In addition, a histogram of the residuals (shown 
in Figure 15.14 on page 642) indicates only moderate departure from normality, and a plot of 
the residuals versus the predicted values of Y (also shown in Figure 15.14) does not show evi-
dence of unequal variance.
Cp Statistic
	
Cp = 11 - R2
k21n - T2
1 - R2
T
- 3n - 21k + 124	
(15.9)
	
where
k = number of independent variables included in a regression model
T = total number of parameters (including the intercept) to be estimated in the  
full regression model
R2
k = coefficient of multiple determination for a regression model that has k 
independent variables
R2
T = coefficient of multiple determination for a full regression model that  
contains all T estimated parameters

642	
Chapter 15  Multiple Regression Model Building
Because the residual analysis appears to confirm the aptness of the model, you can now 
use various influence measures discussed in Section 14.8 to determine whether any of the val-
ues unduly influence the regression equation. Figure 15.15 on page 643 presents the hi, ti, and 
Cook’s Di statistics for the fitted model and highlights certain observations for further analysis.
Using the decision rule suggested by Hoaglin and Welsch, you flag any observation that 
has a hi value greater than 21k + 12>n = 214 + 12>26 = 0.3846. From Figure 15.15, you 
see that observations 6 1h6 = 0.40492, 9 1h9 = 0.45372, and 11 1h11 = 0.52172 have hi val-
ues that are greater than 0.3846 and therefore are candidates for deletion from the analysis.
You next apply the Studentized deleted residual measure ti to the influence analysis. 
Using the decision rule suggested by Hoaglin and Welsch, any observation with ti value 
greater than 1.7247 or less than -1.7247 should be flagged. From Figure 15.15, you see that 
t3 = 1.741, t11 = 2.0673, t17 = -2.0072, and t19 = 2.1029, which suggests that observa-
tions 3, 11, 17, and 19 may have an adverse effect on the model. You note that observation 11 
is also a candidate for deletion according to the hi criterion.
F i g u r e  1 5 . 1 3
Residual plots for the standby-hours data
F i g u r e  1 5 . 1 4
Histogram of the residuals and scatter plot of the residuals versus the predicted values of Y
Student Tip
You can use Table E.3 
to find the critical t 
value with n - k - 2
= 26 - 4 - 2 = 20 
degrees of freedom.

	
15.4  Model Building	
643
You then apply a third criterion, Cook’s Di statistic, based on hi and the standardized 
­residual. For the model, in which k = 4 and n = 26, using the decision rule suggested by 
Cook and Weisberg, you flag any Di 7 0.899, the critical value for the F statistic having  
5 and 21 degrees of freedom at the 0.50 level of significance (see Table 14.15 on page 608). 
From Figure 15.15, none of the Di values exceed 0.899, including the Di for observation 11 
(0.807). You note that according to Cook’s Di statistic, no observations are candidates for deletion.
With this influence analysis, you conclude that you have no clear basis for removing any 
of the observations from the analysis. Therefore, using the Figure 15.10 regression model 
­results, you state the regression equation as
Yni = -330.8318 + 1.2456X1i - 0.1184X2i - 0.2971X3i + 0.1305X4i
Example 15.4 presents a situation in which there are several alternative models in which 
the Cp statistic is close to or less than k + 1.
F i g u r e  1 5 . 1 5
Minitab influence 
statistics for the standby 
hours data
T a b l e  1 5 . 3
Partial Results 
from Best-Subsets 
Regression
Number of Variables
r 2
Adjusted r 2
Cp
Variables Included
1
0.121
0.119
113.9
X4
1
0.093
0.090
130.4
X1
1
0.083
0.080
136.2
X3
2
0.214
0.210
  62.1
X3, X4
2
0.191
0.186
  75.6
X1, X3
2
0.181
0.177
  81.0
X1, X4
3
0.285
0.280
  22.6
X1, X3, X4
3
0.268
0.263
  32.4
X3, X4, X5
3
0.240
0.234
  49.0
X2, X3, X4
4
0.308
0.301
  11.3
X1, X2, X3, X4
4
0.304
0.297
  14.0
X1, X3, X4, X6
4
0.296
0.289
  18.3
X1, X3, X4, X5
5
0.317
0.308
  8.2
X1, X2, X3, X4, X5
5
0.315
0.306
  9.6
X1, X2, X3, X4, X6
5
0.313
0.304
  10.7
X1, X3, X4, X5, X6
6
0.323
0.313
  6.8
X1, X2, X3, X4, X5, X6
6
0.319
0.309
  9.0
X1, X2, X3, X4, X5, X7
6
0.317
0.306
  10.4
X1, X2, X3, X4, X6, X7
7
0.324
0.312
  8.0
X1, X2, X3, X4, X5, X6, X7
Example 15.4
Choosing Among 
Alternative Regres-
sion Models
Table 15.3 shows results from a best-subsets regression analysis of a regression model with seven 
independent variables. Determine which regression model you would choose as the best model.
(continued)

644	
Chapter 15  Multiple Regression Model Building
Model Validation
The final step in the model-building process is to validate the selected regression model. This 
step involves checking the model against data that were not part of the sample analyzed. The 
following are several ways of validating a regression model:
 • Collect new data and compare the results.
 • Compare the results of the regression model to previous results.
 • If the data set is large, split the data into two parts and cross-validate the results.
Perhaps the best way of validating a regression model is by collecting new data. If the re-
sults with new data are consistent with the selected regression model, you have strong reason 
to believe that the fitted regression model is applicable in a wide set of circumstances.
If it is not possible to collect new data, you can use one of the two other approaches. In 
one approach, you compare your regression coefficients and predictions to previous results. If 
the data set is large, you can use cross-validation. First, you split the data into two parts. Then 
you use the first part of the data to develop the regression model. You then use the second part 
of the data to evaluate the predictive ability of the regression model.
Steps for successful model building are summarized below and in Figure 15.16.
Solution  From Table 15.3, you need to determine which models have Cp values that are 
less than or close to k + 1. Two models meet this criterion. The model with six independent 
variables 1X1, X2, X3, X4, X5, X62 has a Cp value of 6.8, which is less than k + 1 = 6 + 1 = 7, 
and the full model with seven independent variables 1X1, X2, X3, X4, X5, X6, X72 has a Cp value 
of 8.0. One way you can choose among the two models is to select the model with the largest 
adjusted r 2—that is, the model with six independent variables. Another way to select a final 
model is to determine whether the models contain a subset of variables that are common. Then 
you test whether the contribution of the additional variables is significant. In this case, be-
cause the models differ only by the inclusion of variable X7 in the full model, you test whether 
variable X7 makes a significant contribution to the regression model, given that the variables 
X1, X2, X3, X4, X5, and X6 are already included in the model. If the contribution is statistically 
significant, then you should include variable X7 in the regression model. If variable X7 does 
not make a statistically significant contribution, you should not include it in the model.
The following steps summarize the steps for successful model build-
ing discussed in Section 15.4.
Step 1  Choose the independent variables to be considered for the 
multiple regression model.
Step 2  Develop a regression model that includes all of the inde-
pendent variables.
Step 3  Compute the VIF for each independent variable.
If none of the independent variables has a VIF 7 5, continue  
  with step 4.
If only one of the independent variables has a VIF 7 5, eliminate  
  that independent variable and continue with step 4.
If more than one of the independent variables has a VIF 7 5,  
  eliminate the independent variable that has the highest VIF  
  from the regression model and repeat step 2 with the  
  remaining independent variables.
Step 4  Perform a best-subsets regression with the remaining in-
dependent variables and compute the Cp statistic and/or the adjusted 
r 2 for each model.
Step 5  List all models that have Cp close to or less than k + 1 
and/or a high adjusted r 2.
Step 6  From the models listed in step 4, choose a best model.
Step 7  Perform a complete analysis of the model chosen, includ-
ing a residual analysis and influence analysis.
Step 8  Review the results of the residual analysis and influence 
analysis. If necessary, add quadratic and/or interaction terms, transform 
variables, and delete individual observations, as necessary, and repeat 
steps 2 through 6. If no changes are necessary, continue with step 8.
Step 9  Validate the regression model. If validated, use the regres-
sion model for prediction and inference.
Steps for Successful Model Building

	
15.4  Model Building	
645
Problems for Section 15.4
Learning the Basics
15.21  You are considering four independent variables for inclu-
sion in a regression model. You select a sample of n = 30, with 
the following results:
	 1.	The model that includes independent variables A and B has a 
Cp value equal to 4.6.
	 2.	The model that includes independent variables A and C has a 
Cp value equal to 2.4.
	 3.	The model that includes independent variables A, B, and C 
has a Cp value equal to 2.7.
a.	 Which models meet the criterion for further consideration? 
Explain.
b.	 How would you compare the model that contains inde-
pendent variables A, B, and C to the model that contains 
independent variables A and B? Explain.
15.22  You are considering six independent variables for inclu-
sion in a regression model. You select a sample of n = 40, with 
the following results:
k = 2 T = 6 + 1 = 7 R2
k = 0.274 R2
T = 0.653
a.	 Compute the Cp value for this two-independent-variable model.
b.	 Based on your answer to (a), does this model meet the criterion 
for further consideration as the best model? Explain.
Does Any
X Variable
Have a
VIF . 5?
Run Regression Model
with All Independent
Variables to Find VIFs
List All Models That
Have Cp Close to or
Less Than k 1 1
Choose a “Best” Model
Among These Models
Perform a Residual Analysis
and an Inﬂuence Analysis
on Selected Model
Run Best-Subsets
Regression to Obtain
Best Models with k
Terms for a Given Number
of Independent Variables
Based on Analyses of Model,
Add Quadratic or Interaction
Terms, Transform Variables,
or Delete Observations,
as necessary
Validate and Use Selected
Model for Prediction and
Conﬁdence Interval
Estimation
Choose Independent
Variables to Be
Considered
Yes
No
No
Yes
Eliminate the X Variable
With the Largest VIF
Eliminate the X Variable
With the VIF . 5
Does
More
Than One X
Variable Have
VIF . 5
?
F i g u r e  1 5 . 1 6
Roadmap for model building

646	
Chapter 15  Multiple Regression Model Building
Applying the Concepts
15.23  The file  FTMBA  contains a sample of 2012 top-ranked 
full-time MBA programs. Variables included are mean starting sal-
ary upon graduation ($), percent of students with job offers within 
three months of graduation, program cost ($), and total number of 
students per program. (Data extracted from buswk.co/25d1ZC.) 
Develop the most appropriate multiple regression model to pre-
dict the mean starting salary upon graduation. Be sure to include 
a thorough residual and influence analysis. In addition, provide a 
detailed explanation of the results, including a comparison of the 
most appropriate multiple regression model to the best simple lin-
ear regression model.
SELF 
Test 
15.24  You need to develop a model to predict the 
­assessed value of houses in Silver Spring, Maryland, 
based on the size of the house, whether it has a fireplace, the num-
ber of bedrooms, and the number of bathrooms. A sample of 
30 houses is selected and the results are stored in  SilverSpring . 
­Develop the most appropriate multiple regression model to predict 
assessed value. Be sure to perform a thorough residual and influ-
ence analysis. In addition, provide a detailed explanation of the 
results.
15.25  Accounting Today identified top public accounting firms 
in ten geographic regions across the U.S. The file  Accounting 
Partners6  contains data for public accounting firms in the South-
east, Gulf Coast, and Capital Regions. The variables are: revenue 
($M), number of partners in the firm, number of professionals in 
the firm, proportion of business dedicated to management advi-
sory services (MAS%), whether the firm is located in the South-
east Region 10 = no, 1 = yes2, and whether the firm is located in 
the Gulf Coast Region 10 = no, 1 = yes2. (Data extracted from 
bit.ly/176TPfR.)
Develop the most appropriate multiple regression model to pre-
dict firm revenue. Be sure to perform a thorough residual and influ-
ence analysis. In addition, provide a detailed explanation of the results.
15.5  Pitfalls in Multiple Regression and Ethical Issues
Pitfalls in Multiple Regression
Model building is an art as well as a science. Different individuals may not always agree on the 
best multiple regression model. To construct a good regression model, you should use the pro-
cess described on page 644. In doing so, you must avoid certain pitfalls that can interfere with 
the development of a useful model. Section 13.9 discussed pitfalls in simple linear regression 
and strategies for avoiding them. Now that you have studied a variety of multiple regression 
models, you need to take some additional precautions. To avoid pitfalls in multiple regression, 
you also need to
 • Interpret the regression coefficient for a particular independent variable from a perspec-
tive in which the values of all other independent variables are held constant.
 • Evaluate residual plots for each independent variable.
 • Evaluate interaction and quadratic terms.
 • Compute the VIF for each independent variable before determining which independent 
variables to include in the model.
 • Examine several alternative models, using best-subsets regression.
 • Use influence anlaysis to determine whether to remove any observations from the  
analysis.
 • Use logistic regression instead of least squares regression when the dependent variable 
is categorical.
 • Validate the model before implementing it.
Ethical Issues
Ethical issues arise when a user who wants to make predictions manipulates the development 
process of the multiple regression model. The key here is intent. In addition to the situations 
discussed in Section 13.9, unethical behavior occurs when someone uses multiple regression 
analysis and willfully fails to remove from consideration independent variables that exhibit a 
high collinearity with other independent variables or willfully fails to use methods other than 
least-squares regression when the assumptions necessary for least-squares regression are seri-
ously violated.

	
Key Equations	
647
S u m m a r y
This chapter discussed quadratic regression models, 
transformations, collinearity, and model building. The  
Figure 15.17 roadmap on page 648 summarizes and interre-
lates these topics. (Chapter 15 references appear on page 649.)
K ey Eq u at i o n s
Quadratic Regression Model
Yi = b0 + b1X1i + b2X2
1i + ei	
(15.1)
Quadratic Regression Equation
Yni = b0 + b1X1i + b2X2
1i	
(15.2)
Regression Model with a Square-Root  
Transformation
Yi = b0 + b12X1i + ei	
(15.3)
Original Multiplicative Model
Yi = b0Xb1
1i Xb2
2i ei	
(15.4)
Transformed Multiplicative Model
 log Yi = log1b0Xb1
1i Xb2
2i ei2
 = log b0 + log1Xb1
1i 2 + log1Xb2
2i 2 + log ei
 = log b0 + b1 log X1i + b2 log X2i + log ei	 (15.5)
Original Exponential Model
Yi = eb0 + b1X1i + b2X2i ei	
(15.6)
Transformed Exponential Model
 ln Yi = ln1eb0 + b1X1i + b2X2iei2
 = ln1eb0 + b1X1i + b2X2i2 + ln ei
 = b0 + b1X1i + b2X2i + ln ei	
(15.7)
Variance Inflationary Factor
VIFj =
1
1 - R2
j
	
(15.8)
Cp Statistic
Cp = 11 - R2
k21n - T2
1 - R2
T
- 3n - 21k + 124	
(15.9)
I
n the Using Statistics scenario, you were the WSTA-TV 
broadcast operations manager, who sought to reduce labor 
expenses. You needed to determine which variables have an 
effect on standby hours, the time during which graphic art-
ists employed by the station are idle but getting paid. You 
collected data concerning standby hours and the total num-
ber of staff present, remote hours, Dubner hours, and total 
labor hours over a period of 26 weeks.
You performed a multiple regression analysis on the 
data. The coefficient of multiple determination indicated 
that 62.31% of the variation in standby hours can be ex-
plained by variation in the number of graphic artists present 
and the number of remote hours, Dubner hours, and total la-
bor hours. The model indicated that standby hours are esti-
mated to increase by 1.2456 hours for each additional staff 
member present, holding constant the other independent  
variables; to 
d e c r e a s e  b y 
0.1184 hour for each additional remote hour, holding 
constant the other independent variables; to decrease by 
0.2971 hour for each additional Dubner hour, holding con-
stant the other independent variables; and to increase by 
0.1305 hour for each additional labor hour, holding con-
stant the other independent variables. Each of the four 
independent variables had a significant effect on standby 
hours, holding constant the other independent variables. 
This regression model enables you to predict standby hours 
based on the total number of graphic artists present, remote 
hours, Dubner hours, and total labor hours. Any predictions 
­developed by the model can then be carefully monitored, 
new data can be collected, and other variables may pos-
sibly be considered.
U s i n g  S tat i s t i c s
Valuing Parsimony at WSTA-TV, Revisited
Alamy

648	
Chapter 15  Multiple Regression Model Building
Any Observations
to be Removed?
Excessive
Collinearity?
Yes
Yes
Yes
Yes
No
No
No
No
Yes
No
Is
Model
Signiﬁcant
?
Is br
Signiﬁcant
?
Assumptions
Validated?
Transformations
Dummy Variables
and
Interaction Terms
Model Building
Methods
Quadratic
Terms
Best-
Subsets
Regression
Stepwise
Regression
Adjusted
r2
Cp
Estimate
br
Estimate
m and
Predict Y
Fitting the
Best Model
Determining
and Interpreting
Regression Coefﬁcients
Multiple Regression
Techniques
Inﬂuence Analysis
Residual
Analysis
Collinearity
Analysis
Test Portions
of Model
Test Overall Model
Signiﬁcance
H0: b12b22…2bk20
Validate and Use
Model for Prediction
and Estimation
F i g u r e  1 5 . 1 7
Roadmap for multiple regression

	
Chapter Review Problems	
649
Refere n c e s
	 1.	Kutner, M., C. Nachtsheim, J. Neter, and W. Li. Applied Linear 
Statistical Models, 5th ed. New York: McGraw-Hill/Irwin, 2005.
	 2.	Marquardt, D. W. W. “Comment on Smith and Campbell 
(1980).” Journal of the American Statistical Association, 75 
(1980): 87–91.
	 3.	Microsoft Excel 2013. Redmond, WA: Microsoft Corp., 
2012.
	 4.	Minitab Release 16. State College, PA: Minitab, Inc., 2010.
	 5.	Snee, R. D. “Some Aspects of Nonorthogonal Data Analysis, 
Part I. Developing Prediction Equations.” Journal of Quality 
Technology 5 (1973): 67–79.
K ey Te r ms
best-subsets approach  640
Cp statistic  640
collinearity  636
cross-validation  644
logarithmic transformation  633
parsimony  638
quadratic regression model  625
quadratic term  625
square-root transformation  633
stepwise regression  639
variance inflationary factor (VIF)  636
C h ec ki n g  Yo ur  U n de r s ta nding
15.26  Define collinearity. How does it make the interpretation of 
multiple regression model difficult?
15.27  What are the steps involved in formulating an overall  
regression model building?
15.28  How does the correlation between independent variables 
impact multiple regression model building?
C h a pte r  R e vi e w P r ob le ms
15.29  What is model-validation? How do you think it is impor-
tant for real life prediction using regression model?
15.30  In the production of printed circuit boards, errors in the 
alignment of electrical connections are a source of scrap. The file 
 RegistrationError  contains the registration error, the temperature, 
the pressure, and the cost of the material (low versus high) used in the 
production of circuit boards. (Data extracted from C. Nachtsheim  
and B. Jones, “A Powerful Analytical Tool,” Six Sigma Forum 
Magazine, August 2003, pp. 30–33.) Develop the most appropriate 
multiple regression model to predict registration error.
15.31  Hemlock Farms is a community located in the Pocono 
Mountains area of eastern Pennsylvania. The file  HemlockFarms  
contains information on homes that were recently for sale. The 
variables included were
List Price—Asking price of the house
Hot Tub—Whether the house has a hot tub, with 0 = No and 
1 = Yes
Lake View—Whether the house has a lake view, with 0 = No 
and 1 = Yes
Bathrooms—Number of bathrooms
Bedrooms—Number of bedrooms
Loft/Den—Whether the house has a loft or den, with 0 = No 
and 1 = Yes
Finished basement—Whether the house has a finished basement, 
with 0 = No and 1 = Yes
Acres—Number of acres for the property
Develop the most appropriate multiple regression model to predict 
the asking price. Be sure to perform a thorough residual and influ-
ence analysis. In addition, provide a detailed explanation of your 
results.
15.32  Nassau County is located approximately 25 miles east of 
New York City. The file  GlenCove  contains a sample of 30 sin-
gle-family homes located in Glen Cove. Variables included are the 
fair market value, land area of the property (acres), interior size of 
the house (square feet), age (years), number of rooms, number of 
bathrooms, and number of cars that can be parked in the garage.
a.	 Develop the most appropriate multiple regression model to pre-
dict fair market value.
b.	 Compare the results in (a) with those of Problems 15.33 (a) and 
15.34 (a).

650	
Chapter 15  Multiple Regression Model Building
15.33  Data similar to those in Problem 15.32 are available for 
homes located in Roslyn (approximately 8 miles from Glen Cove) 
and are stored in  Roslyn .
a.	 Perform an analysis similar to that of Problem 15.32.
b.	 Compare the results in (a) with those of Problems 15.32 (a) and 
15.34 (a).
15.34  Data similar to Problem 15.32 are available for homes lo-
cated in Freeport (located approximately 20 miles from Roslyn) 
and are stored in  Freeport .
a.	 Perform an analysis similar to that of Problem 15.32.
b.	 Compare the results in (a) with those of Problems 15.32 (a) and 
15.33 (a).
15.35  You are a real estate broker who wants to compare prop-
erty values in Glen Cove and Roslyn (which are located approxi-
mately 8 miles apart). Use the data in  GCRoslyn . Make sure to 
include the dummy variable for location (Glen Cove or Roslyn) in 
the regression model.
a.	 Develop the most appropriate multiple regression model to pre-
dict fair market value.
b.	 What conclusions can you reach concerning the differences in 
fair market value between Glen Cove and Roslyn?
15.36  You are a real estate broker who wants to compare prop-
erty values in Glen Cove, Freeport, and Roslyn. Use the data in  
 GCFreeRoslyn .
a.	 Develop the most appropriate multiple regression model to pre-
dict fair market value.
b.	 What conclusions can you reach concerning the differences in 
fair market value between Glen Cove, Freeport, and Roslyn?
15.37  Financial analysts engage in business valuation to deter-
mine a company’s value. A standard approach uses the multiple 
of earnings method: You multiply a company’s profits by a certain 
value (average or median) to arrive at a final value. More recently, 
regression analysis has been demonstrated to consistently deliver 
more accurate predictions. A valuator has been given the assign-
ment of valuing a drug company. He obtained financial data on 72 
drug companies (Industry Group Standard Industrial Classification  
[SIC] 3 code 283), which included pharmaceutical preparation 
firms (SIC 4 code 2834), in vitro and in vivo diagnostic sub-
stances firms (SIC 4 code 2835), and biological products firms 
(SIC 4 2836). The file  BusinessValuation2  contains the following  
variables:
COMPANY—Drug company name
TS—Ticker symbol
SIC 3—Standard Industrial Classification 3 code (industry 
group identifier)
SIC 4—Standard Industrial Classification 4 code (industry 
identifier)
PB fye—Price-to-book value ratio (fiscal year ending)
PE fye—Price-to-earnings ratio (fiscal year ending)
NL Assets—Natural log of assets (as a measure of size)
ROE—Return on equity
SGROWTH—Growth (GS5)
DEBT/EBITDA—Ratio of debt to earnings before interest, 
taxes, depreciation, and amortization
D2834—Dummy variable indicator of SIC 4 code 2834 (1 if 
2834, 0 if not)
D2835—Dummy variable indicator of SIC 4 code 2835 (1 if 
2835, 0 if not)
Develop the most appropriate multiple regression model to pre-
dict the price-to-book value ratio. Be sure to perform a thorough 
residual and influence analysis. In addition, provide a detailed ex-
planation of your results.
15.38  A recent article (J. Conklin, “It’s a Marathon, Not a 
Sprint,” Quality Progress, June 2009, pp. 46–49) discussed a metal 
deposition process in which a piece of metal is placed in an acid 
bath and an alloy is layered on top of it. The key quality character-
istic is the thickness of the alloy layer. The file  Thickness  contains 
the following variables:
Thickness—Thickness of the alloy layer
Catalyst—Catalyst concentration in the acid bath
pH—pH level of the acid bath
Pressure—Pressure in the tank holding the acid bath
Temp—Temperature in the tank holding the acid bath
Voltage—Voltage applied to the tank holding the acid bath
Develop the most appropriate multiple regression model to predict 
the thickness of the alloy layer. Be sure to perform a thorough re-
sidual and influence analysis. The article suggests that there is a 
significant interaction between the pressure and the temperature in 
the tank. Do you agree?
15.39  A molding machine that contains different cavities is 
used in producing plastic parts. The product characteristics of 
interest are the product length (in.) and weight (g). The mold 
cavities were filled with raw material powder and then vibrated 
during the experiment. The factors that were varied were the vi-
bration time (seconds), the vibration pressure (psi), the vibration 
amplitude (%), the raw material density (g/mL), and the quantity 
of raw material (scoops). The experiment was conducted in two  
different cavities on the molding machine. The data are stored  
in  Molding . (Data extracted from M. Lopez and M. McShane-
Vaughn, “Maximizing Product, Minimizing Costs,” Six Sigma Forum  
Magazine, February 2008, pp. 18–23.)
a.	 Develop the most appropriate multiple regression model to pre-
dict the product length in cavity 1. Be sure to perform a thor-
ough residual and influence analysis. In addition, provide a 
detailed explanation of your results.
b.	 Repeat (a) for cavity 2.
c.	 Compare the results for length in the two cavities.
d.	 Develop the most appropriate multiple regression model to 
predict the product weight in cavity 1. Be sure to perform a 
thorough residual and influence analysis. In addition, provide a 
detailed explanation of your results.
e.	 Repeat (d) for cavity 2.
f.	 Compare the results for weight in the two cavities.
Report Writing Exercise
15.40  In Problems 15.32–15.36 you developed multiple regres-
sion models to predict the fair market value of houses in Glen Cove, 
Roslyn, and Freeport. Now write a report based on the models you 
developed. Append all appropriate charts and statistical information 
to your report.

	
Cases for Chapter 15	
651
Variable
Comments
SOLIDS
Percentage of solids in the filter cake.
PH
Acidity. This measure of acidity indicates 
bacterial action in the clarifier and is 
controlled by the amount of downtime in 
the system. As bacterial action progresses, 
organic acids are produced that can be 
measured using pH.
LOWER
Pressure of the vacuum line below the fluid 
line on the rotating drum.
UPPER
Pressure of the vacuum line above the fluid 
line on the rotating drum.
THICK
Filter cake thickness, measured on the  
drum.
VARIDRIV
Setting used to control the drum speed. May 
differ from DRUMSPD due to mechanical 
inefficiencies.
DRUMSPD
Speed at which the drum is rotating when 
collecting the filter cake. Measured with a 
stopwatch.
C a s e s  f o r  C h a p t e r  1 5
The Mountain States Potato Company
Mountain States Potato Company sells a by-product of its 
potato-processing operation, called a filter cake, to area 
feedlots as cattle feed. The business problem faced by the 
feedlot owners is that the cattle are not gaining weight as 
quickly as they once were. The feedlot owners believe that 
the root cause of the problem is that the percentage of solids 
in the filter cake is too low.
Historically, the percentage of solids in the filter cakes 
ran slightly above 12%. Lately, however, the solids are run-
ning in the 11% range. What is actually affecting the solids 
is a mystery, but something has to be done quickly. Indi-
viduals involved in the process were asked to identify vari-
ables that might affect the percentage of solids. This review 
turned up the six variables (in addition to the percentage of 
solids) listed in the right column. Data collected by monitor-
ing the process several times daily for 20 days are stored in 
 Potato .
1.		Thoroughly analyze the data and develop a regression 
model to predict the percentage of solids.
2.		Write an executive summary concerning your findings 
to the president of the Mountain States Potato Company. 
Include specific recommendations on how to get the per-
centage of solids back above 12%.
Sure Value Convenience Stores
You work in the corporate office for a nationwide conve-
nience store franchise that operates nearly 10,000 stores. The 
per-store daily customer count (i.e., the mean number of cus-
tomers in a store in one day) has been steady, at 900, for some 
time. To increase the customer count, the chain is considering 
cutting prices for coffee beverages. The question to be deter-
mined is how much prices should be cut to increase the daily 
customer count without reducing the gross margin on coffee 
sales too much. You decide to carry out an experiment in a 
sample of 24 stores where customer counts have been run-
ning almost exactly at the national average of 900. In six of 
the stores, the price of a small coffee will now be $0.59, in 
six stores the price of a small coffee will now be $0.69, in six 
stores, the price of a small coffee will now be $0.79, and in 
six stores, the price of a small coffee will now be $0.89. After 
four weeks at the new prices, the daily customer count in the 
stores is determined and is stored in  CoffeeSales2 .
a.	 Construct a scatter plot for price and sales.
b.	 Fit a quadratic regression model and state the quadratic 
regression equation.
c.	 Predict the mean weekly sales for a small coffee priced at  
79 cents.
d.	 Perform a residual analysis on the results and determine 
whether the regression model is valid.
e.	 At the 0.05 level of significance, is there a significant 
quadratic relationship between weekly sales and price?
f.	 At the 0.05 level of significance, determine whether the 
quadratic model is a better fit than the linear model.
g.	 Interpret the meaning of the coefficient of multiple 
determination.
h.	 Compute the adjusted r2.
i.	 What price do you recommend the small coffee should 
be sold for?

652	
Chapter 15  Multiple Regression Model Building
Digital Case
Apply your knowledge of multiple regression model building 
in this Digital Case, which extends the Chapter 14 Omni­
Power Bars Using Statistics scenario.
Still concerned about ensuring a successful test marketing of 
its OmniPower bars, the marketing department of OmniFoods 
has contacted Connect2Coupons (C2C), another merchan-
dising consultancy. C2C suggests that earlier analysis done 
by In-Store Placements Group (ISPG) was faulty because it 
did not use the correct type of data. C2C claims that its Inter-
net-based viral marketing will have an even greater effect on  
OmniPower energy bar sales, as new data from the same 
34-store sample will show. In response, ISPG says its earlier 
claims are valid and has reported to the OmniFoods marketing 
department that it can discern no simple relationship between 
C2C’s viral marketing and increased OmniPower sales.
Open OmniPowerForum15.pdf to review all the claims 
made in a private online forum and chat hosted on the Omni-
Foods corporate website. Then answer the following:
1.	Which of the claims are true? False? True but mislead-
ing? Support your answer by performing an appropriate 
statistical analysis.
2.	If the grocery store chain allowed OmniFoods to use an 
unlimited number of sales techniques, which techniques 
should it use? Explain.
3.	 If the grocery store chain allowed OmniFoods to use only 
one sales technique, which technique should it use? Explain.
The Craybill Instrumentation Company Case
The Craybill Instrumentation Company produces highly 
technical industrial instrumentation devices. The human re-
sources (HR) director has the business objective of improv-
ing recruiting decisions concerning sales managers. The 
company has 45 sales regions, each headed by a sales man-
ager. Many of the sales managers have degrees in electrical 
engineering, and due to the technical nature of the product 
line, several company officials believe that only applicants 
with degrees in electrical engineering should be considered.
At the time of their application, candidates are asked 
to take the Strong-Campbell Interest Inventory Test and 
the Wonderlic Personnel Test. Due to the time and money 
involved with the testing, some discussion has taken place 
about dropping one or both of the tests. To start, the HR di-
rector gathered information on each of the 45 current sales 
managers, including years of selling experience, electri-
cal engineering background, and the scores from both the 
Wonderlic and Strong-Campbell tests. The HR director has 
decided to use regression modeling to predict a dependent 
variable of “sales index” score, which is the ratio of the re-
gions’ actual sales divided by the target sales. The target 
values are constructed each year by upper management, in 
consultation with the sales managers, and are based on past 
performance and market potential within each region. The 
file  Managers  contains information on the 45 current sales 
managers. The following variables are included:
Sales—Ratio of yearly sales divided by the target sales 
value for that region; the target values were mutually 
agreed-upon “realistic expectations”
Wonder—Score from the Wonderlic Personnel Test; the 
higher the score, the higher the applicant’s perceived 
ability to manage
SC—Score on the Strong-Campbell Interest Inventory 
Test; the higher the score, the higher the applicant’s 
perceived interest in sales
Experience—Number of years of selling experience 
prior to becoming a sales manager
Engineer—Dummy variable that equals 1 if the sales 
manager has a degree in electrical engineering and 0 
otherwise
a.	 Develop the most appropriate regression model to pre-
dict sales.
b.	 Do you think that the company should continue admin-
istering both the Wonderlic and Strong-Campbell tests? 
Explain.
c.	 Do the data support the argument that electrical engineers 
outperform the other sales managers? Would you support 
the idea to hire only electrical engineers? Explain.
d.	 How important is prior selling experience in this case? 
Explain.
e.	 Discuss in detail how the HR director should incorporate the 
regression model you developed into the recruiting process.
More Descriptive Choices Follow-Up
Follow-up the Using Statistics scenario “More Descriptive 
Choices, Revisited” on page 166, by developing regression 
models to predict the one-year return, the three-year return, 
the five-year return, and the ten-year return based on the as-
sets, turnover ratio, expense ratio, beta, standard deviation,  
type of fund (growth versus value), and risk (stored in  
­ Retirement Funds ). (For this analysis, combine low and aver-
age risk into the new category “not high.”) Be sure to per-
form a thorough residual and influence analysis. Provide a 
summary report that explains your results in detail.

	
Chapter 15 EXCEL Guide	
653
EG15.1  The Quadratic  
Regression Model
Key Technique  Use the exponential operator 1^2 in a column of 
formulas to create a quadratic term.
Example  Create the quadratic term for the Section 15.1 fly ash 
percentage analysis and construct the Figure 15.3 scatter plot that 
shows the quadratic relationship between fly ash percentage and 
strength shown on page 627.
To create the quadratic term, open to the DATA worksheet 
of the FlyAsh workbook. That worksheet contains the indepen-
dent variable FlyAsh% in column A and the dependent variable 
Strength in column B. Select column B (Strength), right-click, 
and click Insert from the shortcut menu to add a new column B. 
(Strength becomes column C.) Enter the label FlyAsh%^2 in cell 
B1 and then enter the formula =A2^2 in cell B2. Copy this for-
mula down the column through all the data rows.
Perform a regression analysis using this new variable by 
adapting either the Section EG14.1 PHStat or In-Depth Excel in-
structions. Then adapt the appropriate Section EG2.5 instructions 
to construct a scatter plot. Select that chart. Then select Design ➔ 
Add Chart Element ➔ Trendline ➔ More Trendline Options 
and in the Format Trendline pane (shown below) click Polynomial  
and check Display Equation on chart in the Format Trendline pane. 
In Excel versions older than Excel 2013, select Layout ➔ 
Trendline ➔ More Trendline Options and in the Format Trendline 
dialog box (similar to the Format Trendline Pane), click Trendline 
Options in the left pane and in the Trendline Options right pane, 
click Polynomial, check Display Equation on chart, and click OK.
While the quadratic term FlyAsh%^2 could be created in 
any column, placing independent variables in contiguous columns 
is a best practice and mandatory if you use the Analysis ToolPak 
Regression procedure.
EG15.2  Using Transformations in 
Regression Models
The Square-Root Transformation
To the worksheet that contains your regression data, add a new 
column of formulas that computes the square root of one of the 
independent variables to create a square-root transformation. For 
example, to create a square root transformation in a blank col-
umn D for an independent variable in a column C, enter the for-
mula =SQRT1C22 in cell D2 of that worksheet and copy the 
formula down through all data rows. If the rightmost column in 
the worksheet contains the dependent variable, first select that 
column, right-click, and click Insert from the shortcut menu and 
place the transformation in that new column.
The Log Transformation
To the worksheet that contains your regression data, add a new 
column of formulas that computes the common (base 10) log-
arithm or natural logarithm (base e) of one of the independent 
variables to create a log transformation. For example, to create 
a common logarithm transformation in a blank column D for a 
variable in a column C, enter the formula =LOG1C22 in cell 
D2 of that worksheet and copy the formula down through all data 
rows. To create a natural logarithm transformation in a blank col-
umn D for a variable in column C, enter the formula =LN1C22 
in cell D2 of that worksheet and copy the formula down through 
all data rows.
If the dependent variable appears in a column to the immedi-
ate right of the independent variable being transformed, first select 
the dependent variable column, right-click, and click Insert from 
the shortcut menu and then place the transformation of the inde-
pendent variable in that new column.
EG15.3  Collinearity
PHStat  To compute the variance inflationary factor (VIF), use 
the “Interpreting the Regression Coefficients” PHStat instructions 
in Section EG14.1 on page 617, but modify step 6 by checking 
Variance Inflationary Factor (VIF) before you click OK. The 
VIF will appear in cell B9 of the regression results worksheet, im-
mediately following the Regression Statistics area.
In-Depth Excel  To compute the variance inflationary factor, 
first use the “Interpreting the Regression Coefficients” In-Depth 
Excel instructions in Section EG14.1 on page 617 to create re-
gression results worksheets for every combination of indepen-
dent variables in which one serves as the dependent variable. 
Then, in each of the regression results worksheets, enter the label 
VIF in cell A9 and enter the formula =1>11 - B52 in cell B9 to 
compute the VIF.
C h a p t e r  1 5  E x c e l  G u i d e

654	
Chapter 15  Multiple Regression Model Building
EG15.4  Model Building
The Stepwise Regression Approach  
to Model Building
Key Technique  Use PHStat to perform a stepwise analysis.
Example  Perform the Figure 15.11 stepwise analysis for the 
standby-hours data that is shown on page 639.
PHStat  Use Stepwise Regression.
For the example, open to the DATA worksheet of the Standby 
workbook. Select PHStat ➔ Regression ➔ Stepwise Regres-
sion. In the procedure’s dialog box (shown below):
	 1.	 Enter A1:A27 as the Y Variable Cell Range.
	 2.	 Enter B1:E27 as the X Variables Cell Range.
	 3.	 Check First cells in both ranges contain label.
	 4.	 Enter 95 as the Confidence level for regression coefficients.
	 5.	 Click p values as the Stepwise Criteria.
	 6.	 Click General Stepwise and keep the pair of .05 values as the 
p value to enter and the p value to remove.
	 7.	 Enter a Title and click OK.
This procedure may take more than a few seconds to construct 
its results. The procedure finishes when the statement “Stepwise 
ends” is added to the stepwise regression results worksheet (shown 
in row 29 in Figure 15.11 on page 639).
The Best-Subsets Approach to Model Building
Key Technique  Use PHStat to perform a stepwise analysis.
Example  Perform the Figure 15.12 best subsets analysis for the 
standby-hours data that is shown on page 640.
PHStat  Use Best Subsets.
For the example, open to the DATA worksheet of the Standby 
workbook. Select PHStat ➔ Regression ➔ Best Subsets. In the 
procedure’s dialog box (shown below):
	 1.	 Enter A1:A27 as the Y Variable Cell Range.
	 2.	 Enter B1:E27 as the X Variables Cell Range.
	 3.	 Check First cells in each range contains label.
	 4.	 Enter 95 as the Confidence level for regression coefficients.
	 5.	 Enter a Title and click OK.
This procedure constructs many regression results worksheets 
(that may be seen as a flickering in the Excel window) as it evalu-
ates each subset of independent variables.
MG15.1  The Quadratic Regression 
Model
Use Calculator to compute the square of one of the independent 
variables to create a quadratic term.
For example, to create a quadratic term for the Section 15.1 fly 
ash analysis, open to the FlyAsh worksheet. Select Calc ➔ Cal-
culator. In the Calculator dialog box (shown on page 655):
	 1.	 Enter C3 in the Store result in variable box and press Tab.
	 2.	 Double-click C1  Fly Ash% in the variables list to add 'Fly 
Ash%' to the Expression box.
	 3.	 Click ** and then 2 on the simulated calculator keypad to add 
**2 to the Expression box.
	 4.	 Click OK.
	 5.	 Enter Fly Ash%^2 as the name for column C3.
C h a p t e r  1 5  M i n i ta b  G u i d e

	
Chapter 15 Minitab Guide	
655
To perform a regression analysis using this new variable, use the 
Section MG14.1 instructions on page 620.
MG15.2  Using Transformations in 
Regression Models
Use Calculator to transform a variable. Open to the worksheet 
that contains your regression data. Select Calc ➔ Calculator. In 
the Calculator dialog box:
	 1.	 Enter the name of the empty column that will contain the 
transformed values in the Store result in variable box and 
press Tab.
	 2.	 Select All functions from the Functions drop-down list.
	 3.	 In the list of functions, select one of these choices: Square 
root, Log base 10, or Natural log (log base e). Selecting 
these choices enters SQRT(number), LOGTEN(number), 
or LN(number), respectively, in the Expression box.
	 4.	 Double-click the name of the variable to be transformed in the 
variables list to replace number with the variable name in the 
Expression box.
	 5.	 Click OK.
	 6.	 Enter a column name for the transformed values.
To perform a regression analysis using this new variable, see  
Section MG14.1 on page 620.
MG15.3  Collinearity
To compute the variance inflationary factor, modify the Section 
MG14.1 “Interpreting the Regression Coefficients” instructions 
on page 620. In step 15, check Variance inflation factors while 
clearing the other Display and Lack of Fit Test check boxes in the 
Regression - Options dialog box.
MG15.4  Model Building
The Stepwise Regression Approach  
to Model Building
Use Stepwise.
For example, to create the Figure 15.11 stepwise analysis of the 
standby-hours data on page 639, open to the Standby worksheet. 
Select Stat ➔ Regression ➔ Stepwise. In the Stepwise Regres-
sion dialog box (shown below):
	 1.	 Double-click C1  Standby in the variables list to add Standby 
in the Response box.
	 2.	 Enter C2-C5 in the Predictors box. (Entering C2-C5 is a 
shortcut way of referring to the four variables in columns 2 
through 5. This shortcut avoids having to double-click the 
name of each of these variables in order to add them to the 
Predictors box.)
	 3.	 Click Methods.
In the Stepwise-Methods dialog box (shown below):
	 4.	 Click Use alpha values.
	 5.	 Click Stepwise.
	 6.	 Enter 0.05 in the Alpha to enter box and 0.05 in the Alpha to 
remove box.
	 7.	 Click OK.
	 8.	 Back in the Stepwise Regression dialog box, click OK.

656	
Chapter 15  Multiple Regression Model Building
The Best-Subsets Approach to Model Building
Use Best Subsets.
For example, to create the Figure 15.12 stepwise analysis of the 
standby-hours data on page 640, open to the Standby worksheet. 
Select Stat ➔ Regression ➔ Best Subsets. In the Best Subsets 
Regression dialog box (shown below):
	 1.	 Double-click C1  Standby in the variables list to add Standby 
in the Response box.
	 2.	 Enter C2-C5 in the Free Predictors box. (Entering C2-C5 is 
a shortcut way of referring to the four variables in columns 2 
through 5 as explained in the previous set of instructions.)
	 3.	 Click Options.
In the Best Subsets Regression - Options dialog box (shown  
below):
	 4.	 Enter 1 in the Minimum box and keep the Maximum box 
empty.
	 5.	 Enter 3 in the Models of each size to print box.
	 6.	 Check Fit intercept
	 7.	 Click OK.
	 8.	 Back in the Best Subsets Regression dialog box, click OK.

657
U s i n g  S tat i s t i c s
Principled Forecasting
You are a financial analyst for The Principled, a large financial services com-
pany. You need to better evaluate investment opportunities for your clients. To 
assist in the forecasting, you have collected a time series for yearly movie at-
tendance and the revenues of two large well-known companies, The Coca-Cola 
Company, and Wal-Mart Stores, Inc. Each time series has unique characteristics 
due to differences in business activities and growth patterns. You understand that 
you can choose among several different types of forecasting models. How do 
you decide which type of forecasting is best? How do you use the information 
gained from the forecasting models to evaluate investment opportunities for your 
clients?
contents
16.1  The Importance of Business 
Forecasting
16.2  Component Factors of  
Time-Series Models
16.3  Smoothing an Annual Time 
Series
16.4  Least-Squares Trend Fitting 
and Forecasting
16.5  Autoregressive Modeling for 
Trend Fitting and Forecasting
16.6  Choosing an Appropriate 
Forecasting Model
16.7  Time-Series Forecasting of 
Seasonal Data
16.8  Index Numbers (online)
Think About This: Let The 
Model User Beware
Using Statistics: Principled 
Forecasting, Revisited
Chapter 16 Excel Guide
Chapter 16 Minitab Guide
Objectives
To learn to construct different 
time-series forecasting models—
moving averages, exponential 
smoothing, the linear trend, the 
quadratic trend, the exponential 
trend—and the autoregressive 
models and least-squares 
models for seasonal data
To Learn to choose the most 
appropriate time-series 
forecasting model
Time-Series Forecasting
16
Chapter

658	
Chapter 16  Time-Series Forecasting
I
n Chapters 13 through 15, you used regression analysis as a tool for model building and 
prediction. In this chapter, regression analysis and other statistical methodologies are ap-
plied to time-series data. A time series is a set of numerical data collected over time. Due 
to differences in the features of data for various investments described in the Using Statistics 
scenario, you need to consider several different approaches for forecasting time-series data.
This chapter begins with an introduction to the importance of business forecasting (see 
Section 16.1) and a description of the components of time-series models (see Section 16.2). 
The coverage of forecasting models begins with annual time-series data. Section 16.3 presents 
moving averages and exponential smoothing methods for smoothing a series. This is followed 
by least-squares trend fitting and forecasting in Section 16.4 and autoregressive modeling  
in Section 16.5. Section 16.6 discusses how to choose among alternative forecasting models. 
Section 16.7 develops models for monthly and quarterly time series.
Because economic and business conditions vary over time, managers must find ways to keep 
abreast of the effects that such changes will have on their organizations. One technique that can 
aid in planning for the future needs is forecasting. Forecasting involves monitoring changes 
that occur over time and predicting the future. For example, marketing executives at a retailer 
might forecast product demand, sales revenues, consumer preferences, and inventory, among 
other things, in order to make decisions regarding product promotions and strategic planning. 
Government officials forecast unemployment, inflation, industrial production, and revenues 
from income taxes in order to formulate economic policies. And the administrators of a col-
lege need to forecast student enrollment in order to plan for the construction of dormitories and 
academic facilities and plan for student and faculty recruitment.
Two common approaches to forecasting are qualitative and quantitative forecasting. 
Qualitative forecasting methods are especially important when historical data are unavail-
able. Qualitative forecasting methods are considered to be highly subjective and judgmental. 
Quantitative forecasting methods make use of historical data. The goal of these methods is 
to use past data to predict future values.
Quantitative forecasting methods are subdivided into two types: time series and causal. 
Time-series forecasting methods involve forecasting future values based entirely on the past 
and present values of a variable. For example, the daily closing prices of a particular stock 
on the New York Stock Exchange constitute a time series. Other examples of economic or 
business time series are the consumer price index (CPI), the quarterly gross domestic product 
(GDP), and the annual sales revenues of a particular company.
Causal forecasting methods involve the determination of factors that relate to the 
variable you are trying to forecast. These include multiple regression analysis with lagged  
variables, econometric modeling, leading indicator analysis, and other economic barometers 
that are beyond the scope of this text (see references 2–4). The emphasis in this chapter is on 
time-series forecasting methods.
16.1  The Importance of Business Forecasting
Time-series forecasting assumes that the factors that have influenced activities in the past and 
present will continue to do so in approximately the same way in the future. Time-series fore-
casting seeks to identify and isolate these component factors in order to make predictions. 
Typically, the following four factors are examined in time-series models:
 • Trend
 • Cyclical effect
 • Irregular or random effect
 • Seasonal effect
16.2  Component Factors of Time-Series Models

	
16.3  Smoothing an Annual Time Series	
659
A trend is an overall long-term upward or downward movement in a time series. The cyclical 
effect involves the up-and-down swings or movements through the series. Cyclical movements 
vary in length, usually lasting from 2 to 10 years. They differ in intensity and are often correlated 
with a business cycle. In some time periods, the values are higher than would be predicted by a 
trend line (i.e., they are at or near the peak of a cycle). In other time periods, the values are lower 
than would be predicted by a trend line (i.e., they are at or near the bottom of a cycle). Any data 
that do not follow the trend modified by the cyclical component are considered part of the irregular  
effect, or random effect. When you have monthly or quarterly data, an additional component, the  
seasonal effect, is considered, along with the trend, cyclical, and irregular effects.
Your first step in a time-series analysis is to visualize the data and observe whether any 
patterns exist over time. You must determine whether there is a long-term upward or down-
ward movement in the series (i.e., a trend). If there is no obvious long-term upward or down-
ward trend, then you can use moving averages or exponential smoothing to smooth the series 
(see Section 16.3). If a trend is present, you can consider several time-series forecasting meth-
ods. (See Sections 16.4 and 16.5 for forecasting annual data and Section 16.7 for forecasting 
monthly or quarterly time series.)
One of the investments considered in The Principled scenario is the entertainment industry.  
Table 16.1 presents the yearly U.S. and Canada movie attendance (in billions) from 2002 through 
2012 (stored in  Movie Attendance ) and Figure 16.1 presents the time-series plot of these data.
16.3  Smoothing an Annual Time Series
F i g u r e  1 6 . 1
Time-series plot of movie 
attendance from 2002 
through 2012
Figure 16.1 seems to show a slight downward trend in movie attendance between 2002 to 
2012. However, when you examine annual data such as in Figure 16.1, your visual impression 
of the long-term trend in the series is sometimes obscured by the amount of variation from 
year to year. Often, you cannot judge whether any long-term upward or downward trend ex-
ists in the series. To get a better overall impression of the pattern of movement in the data over 
time, you can use the methods of moving averages or exponential smoothing.
T a b l e  1 6 . 1
Movie Attendance from 
2002 Through 2012
Year
Attendance 
(billions)
Year
Attendance 
(billions)
Year
Attendance 
(billions)
2002
1.57
2006
1.40
2010
1.34
2003
1.52
2007
1.40
2011
1.28
2004
1.50
2008
1.34
2012
1.36
2005
1.38
2009
1.42
Source: Data extracted from Theatrical Market Statistics, 2011 and 2012 editions, Motion Picture 
Association of America, Inc.

660	
Chapter 16  Time-Series Forecasting
Moving Averages
Moving averages for a chosen period of length L consist of a series of means, each computed 
over time for a sequence of L observed values. Moving averages, represented by the symbol 
MA1L2, can be greatly affected by the value chosen for L, which should be an integer value that 
corresponds to, or is a multiple of, the estimated average length of a cycle in the time series.
To illustrate, suppose you want to compute five-year moving averages from a series that 
has n = 11 years. Because L = 5, the five-year moving averages consist of a series of means 
computed by averaging consecutive sequences of five values. You compute the first five-year 
moving average by summing the values for the first five years in the series and dividing by 5:
MA152 = Y1 + Y2 + Y3 + Y4 + Y5
5
You compute the second five-year moving average by summing the values of years 2 
through 6 in the series and then dividing by 5:
MA152 = Y2 + Y3 + Y4 + Y5 + Y6
5
You continue this process until you have computed the last of these five-year moving aver-
ages by summing the values of the last 5 years in the series (i.e., years 7 through 11) and then 
dividing by 5:
MA152 = Y7 + Y8 + Y9 + Y10 + Y11
5
When you have annual time-series data, L should be an odd number of years. By follow-
ing this rule, you are unable to compute any moving averages for the first 1L - 12>2 years or 
the last 1L - 12>2 years of the series. Thus, for a five-year moving average, you cannot make 
computations for the first two years or the last two years of the series.
When plotting moving averages, you plot each of the computed values against the middle 
year of the sequence of years used to compute it. If n = 11 and L = 5, the first moving aver-
age is centered on the third year, the second moving average is centered on the fourth year, and 
the last moving average is centered on the ninth year. Example 16.1 illustrates the computation 
of five-year moving averages.
Student Tip
Remember that you 
cannot compute moving 
averages at the begin-
ning and at the end of 
the series.
Example 16.1
Computing  
Five-Year Moving 
Averages
The following data represent total revenues (in $millions) for a fast-food store over the 11-year 
period 2003 to 2013:
4.0 5.0 7.0 6.0 8.0 9.0 5.0 7.0 7.5 5.5 6.5
Compute the five-year moving averages for this annual time series and plot the revenue and 
moving averages.
Solution  To compute the five-year moving averages, you first compute the total for the 
five years and then divide this total by 5. The first of the five-year moving averages is
MA152 = Y1 + Y2 + Y3 + Y4 + Y5
5
= 4.0 + 5.0 + 7.0 + 6.0 + 8.0
5
= 30.0
5
= 6.0
The moving average is centered on the middle value—the third year of this time series. 
To compute the second of the five-year moving averages, you compute the total of the second 
through sixth years and divide this total by 5:
MA152 = Y2 + Y3 + Y4 + Y5 + Y6
5
= 5.0 + 7.0 + 6.0 + 8.0 + 9.0
5
= 35.0
5
= 7.0

	
16.3  Smoothing an Annual Time Series	
661
This moving average is centered on the new middle value—the fourth year of the time 
series. The remaining moving averages are
 MA152 = Y3 + Y4 + Y5 + Y6 + Y7
5
= 7.0 + 6.0 + 8.0 + 9.0 + 5.0
5
= 35.0
5
= 7.0
 MA152 = Y4 + Y5 + Y6 + Y7 + Y8
5
= 6.0 + 8.0 + 9.0 + 5.0 + 7.0
5
= 35.0
5
= 7.0
 MA152 = Y5 + Y6 + Y7 + Y8 + Y9
5
= 8.0 + 9.0 + 5.0 + 7.0 + 7.5
5
= 36.5
5
= 7.3
 MA152 = Y6 + Y7 + Y8 + Y9 + Y10
5
= 9.0 + 5.0 + 7.0 + 7.5 + 5.5
5
= 34.0
5
= 6.8
 MA152 = Y7 + Y8 + Y9 + Y10 + Y11
5
= 5.0 + 7.0 + 7.5 + 5.5 + 6.5
5
= 31.5
5
= 6.3
These moving averages are centered on their respective middle values—the fifth, sixth, seventh, 
eighth, and ninth years in the time series. When you use the five-year moving averages, you are 
unable to compute a moving average for the first two or last two values in the time series.
Figure 16.2 plots the revenues and the five-year moving average for the fast-food store. 
Observe that the moving average exhibits much less variation than the revenues since they 
have smoothed the data.
F i g u r e  1 6 . 2
Fast-food store revenue 
and five-year moving 
average
In practice, you can avoid the tedious computations by using Excel or Minitab to compute 
moving averages. Figure 16.3 presents the annual movie attendance (in billions) from 2002 
through 2012, the computations for three- and five-year moving averages, and a plot of the 
original data and the moving averages.
F i g u r e  1 6 . 3
Excel three- and five-year 
moving averages and plot 
for the movie attendance 
data

662	
Chapter 16  Time-Series Forecasting
In Figure 16.3, there is no three-year moving average for the first year and the last year, 
and there is no five-year moving average for the first two years and last two years. Both the  
three-year and five-year moving averages have smoothed out the variation that exists in  
the movie attendance. Both the three-year and five-year moving average show a slight down-
ward trend in movie attendance. The five-year moving average smooths the series more than 
the three-year moving average because the period is longer. However, the longer the period, 
the smaller the number of moving averages you can compute. Therefore, selecting moving 
averages that are longer than five or seven years is usually undesirable because too many 
moving average values are missing at the beginning and end of the series.
The selection of L, the length of the period used for constructing the averages, is highly 
subjective. If cyclical fluctuations are present in the data, you should choose an integer value 
of L that corresponds to (or is a multiple of) the estimated length of a cycle in the series. 
For annual time-series data that has no obvious cyclical fluctuations, you should choose three 
years, five years, or seven years as the value of L, depending on the amount of smoothing  
desired and the amount of data available.
Exponential Smoothing
Exponential smoothing consists of a series of exponentially weighted moving averages. 
The weights assigned to the values change so that the most recent (the last) value receives  
the highest weight, the previous value receives the second-highest weight, and so on, with the 
first value receiving the lowest weight. Throughout the series, each exponentially smoothed 
value depends on all previous values, which is an advantage of exponential smoothing over the 
method of moving averages. Exponential smoothing also allows you to compute short-term 
(one period into the future) forecasts when the presence and type of long-term trend in a time 
series is difficult to determine.
The equation developed for exponentially smoothing a series in any time period, i, is based 
on only three terms—the current value in the time series, Yi; the previously computed expo-
nentially smoothed value, Ei - 1; and an assigned weight or smoothing coefficient, W. You use 
Equation (16.1) to exponentially smooth a time series.
Computing an Exponentially Smoothed Value in time Period i
E1 = Y1
	
Ei = WYi + 11 - W2Ei - 1 i = 2, 3, 4, c 	
(16.1)
where
Ei = value of the exponentially smoothed series being computed in time 
period i
Ei - 1 = value of the exponentially smoothed series already computed in time 
period i - 1
Yi = observed value of the time series in period i
W = subjectively assigned weight or smoothing coefficient 
1where 0 6 W 6 12; although W can approach 1.0, in virtually all 
business applications, W … 0.5
Choosing the weight or smoothing coefficient (i.e., W) that you assign to the time series 
is critical. Unfortunately, this selection is somewhat subjective. If your goal is to smooth 
a series by eliminating unwanted cyclical and irregular variations in order to see the overall 

	
16.3  Smoothing an Annual Time Series	
663
­long-term tendency of the series, you should select a small value for W (close to 0). If your goal 
is forecasting future short-term directions, you should choose a large value for W (close to 0.5).  
Figure 16.4 presents the exponentially smoothed values (with smoothing coefficients 
W = 0.50 and W = 0.25), the movie attendance from 2002 to 2012, and a plot of the original 
data and the two exponentially smoothed time series. Observe that exponential smoothing has 
smoothed out some of the variation in the movie attendance.
F i g u r e  1 6 . 4
Exponentially smoothed 
series (W = 0.50 and 
W = 0.25) worksheet 
and plot for the movie 
attendance data
To illustrate these exponential smoothing computations for a smoothing coefficient 
of W = 0.25, you begin with the initial value Y2002 = 1.57 as the first smoothed value 
1E2002 = 1.572. Then, using the value of the time series for 2003 1Y2003 = 1.522, you smooth 
the series for 2003 by computing
 E2003 = WY2003 + 11 - W2E2002
 = 10.25211.522 + 10.75211.572 = 1.5575
To smooth the series for 2004:
 E2004 = WY2004 + 11 - W2E2003
 = 10.25211.52 + 10.75211.55752 = 1.5431
To smooth the series for 2005:
 E2005 = WY2005 + 11 - W2E2004
 = 10.25211.382 + 10.75211.54312 = 1.5023
You continue this process until you have computed the exponentially smoothed values for all 
11 years in the series, as shown in Figure 16.4.
In general, you compute the current smoothed value as follows:
Current smoothed value = 1W21Current value2 + 11 - W21Previous smoothed value2
Remember that the smoothed value for the first year is the observed value in the first year.
To use exponential smoothing for forecasting, you use the smoothed value in the current 
time period as the forecast of the value in the following period 1Yni + 12.
Forecasting Time Period i + 1
	
Yni + 1 = Ei	
(16.2)

664	
Chapter 16  Time-Series Forecasting
To forecast the movie attendance in 2013, using a smoothing coefficient of W = 0.25, you 
use the smoothed value for 2012 as its estimate.
Yn2012 + 1 = E2012
Yn2013 = E2012
Yn2013 = 1.3701
The exponentially smoothed forecast for 2013 is 1.3701 billion.
Problems for Section 16.3
Learning the Basics
16.1  If you are using exponential smoothing for forecasting an 
annual time series of revenues, what is your forecast for next year 
if the smoothed value for this year is $32.4 million?
16.2  What is exponential smoothing? Explain how the weights 
are assigned to a time series data.  Comment on the relative impor-
tance of the time series data if,
a.	 W = 0
b.	 W = 1
c.	 W = 0.5
16.3  You are using exponential smoothing on an annual time 
series concerning total revenues (in $millions). You decide to 
use a smoothing coefficient of W = 0.20, and the exponentially 
smoothed value for 2013 is E2013 = 10.202112.12 + 10.80219.42.
a.	 What is the smoothed value of this series in 2013?
b.	 What is the smoothed value of this series in 2014 if the value of 
the series in that year is $11.5 million?
Applying the Concepts
SELF 
Test 
16.4  The data below (stored in  Drive-ThruSpeed ) repre-
sent the average time (in seconds) it took to be served at 
the drive-through at McDonald’s from 1998 to 2012:
Year
Drive-Through 
Speed (seconds)
Year
Drive-Through 
Speed (seconds)
1998
177.59
2006
163.90
1999
167.02
2007
167.10
2000
169.88
2008
158.77
2001
170.85
2009
174.22
2002
162.72
2010
179.27
2003
156.92
2011
184.20
2004
152.52
2012
188.83
2005
167.90
Source: Data extracted from bit.ly/QEIeOW.
a.	 Plot the time series.
b.	 Fit a three-year moving average to the data and plot the  
results.
c.	 Using a smoothing coefficient of W = 0.50, exponentially 
smooth the series and plot the results.
d.	 What is your exponentially smoothed forecast for 2013?
e.	 Repeat (c) and (d), using W = 0.25.
f.	 Compare the results of (d) and (e).
g.	 What conclusions can you reach about the drive-through speed 
at McDonald’s?
16.5  The following data, stored in  Spills  provide the number of 
oil spills in the Gulf of Mexico from 1996 to 2012:
Year
Number of Spills
Year
Number of Spills
1996
4
2005
49
1997
3
2006
14
1998
9
2007
4
1999
5
2008
33
2000
7
2009
11
2001
9
2010
5
2002
12
2011
3
2003
12
2012
8
2004
22
Source: Data extracted from www.bsee.gov/Inspection-and-Enforcement 
/Accidents-and-Incidents/Spills.
a.	 Plot the time series.
b.	 Fit a three-year moving average to the data and plot the results.
c.	 Using a smoothing coefficient of W = 0.50, exponentially 
smooth the series and plot the results.
d.	 What is your exponentially smoothed forecast for 2013?
e.	 Repeat (c) and (d), using W = 0.25.
f.	 Compare the results of (d) and (e).
g.	 What conclusions can you reach concerning the number of oil 
spills in the Gulf of Mexico?
16.6  How have stocks performed in the past? The following table 
presents the data stored in  Stock Performance , which show the 
performance of a broad measure of stock performance (by per-
centage) for each decade from the 1830s through the 2000s:
Decade
Performance (%)
Decade
Performance (%)
1830s
2.8
1920s
13.3
1840s
12.8
1930s
-2.2
1850s
6.6
1940s
9.6
1860s
12.5
1950s
18.2
1870s
7.5
1960s
8.3
1880s
6.0
1970s
6.6
1890s
5.5
1980s
16.6
1900s
10.9
1990s
17.6
1910s
2.2
 2000s*
-0.5
*Through December 15, 2009.
Source: T. Lauricella, “Investors Hope the ’10s Beat the ’00s,” The Wall 
Street Journal, December 21, 2009, pp. C1, C2.

	
16.4  Least-Squares Trend Fitting and Forecasting	
665
a.	 Plot the time series.
b.	 Fit a three-period moving average to the data and plot the re-
sults.
c.	 Using a smoothing coefficient of W = 0.50, exponentially 
smooth the series and plot the results.
d.	 What is your exponentially smoothed forecast for the 2010s?
e.	 Repeat (c) and (d), using W = 0.25.
f.	 Compare the results of (d) and (e).
g.	 What conclusions can you reach concerning how stocks have 
performed in the past?
16.7  The following data (stored in  CoffeePricesPortugal  ) rep-
resent the retail price of coffee 1in :>kg2 in Portugal from 2004 
to 2011:
Year
Retail Price  
1@>kg2
2004
8.60
2005
8.53
2006
8.32
2007
8.23
2008
8.58
2009
8.45
2010
8.31
2011
8.60
Source: International Coffee  
Organization, www.ico.org.
a.	 Plot the data.
b.	 Fit a three-year moving average to the data and plot the results.
c.	 Using a smoothing coefficient of W = 0.50, exponentially 
smooth the series and plot the results.
d.	 What is your exponentially smoothed forecast for 2012?
e.	 Repeat (c) and (d), using a smoothing coefficient of W = 0.25.
f.	 Compare the results of (d) and (e).
g.	 What conclusions can you reach about the retail price of coffee 
in Portugal?
16.8  The file  Audits  contains the number of audits of corpo­
rations with assets of more than $250 million conducted by the 
­Internal Revenue Service. (Data extracted from www.irs.gov.)
a.	 Plot the data.
b.	 Fit a three-year moving average to the data and plot the results.
c.	 Using a smoothing coefficient of W = 0.50, exponentially 
smooth the series and plot the results.
d.	 What is your exponentially smoothed forecast for 2013?
e.	 Repeat (c) and (d), using a smoothing coefficient of W = 0.25.
f.	 Compare the results of (d) and (e).
g.	 What conclusions can you reach concerning the number of  
audits of corporations with assets of more than $250 million 
conducted by the Internal Revenue Service?
16.4  Least-Squares Trend Fitting and Forecasting
Trend is the component factor of a time series most often used to make intermediate and long-
range forecasts. To get a visual impression of the overall long-term movements in a time series, 
you construct a time-series plot. If a straight-line trend adequately fits the data, you can use a 
linear trend model [see Equation (16.3) and Section 13.2]. If the time-series data indicate some 
long-run downward or upward quadratic movement, you can use a quadratic trend model [see 
Equation (16.4) and Section 15.1]. When the time-series data increase at a rate such that the 
percentage difference from value to value is constant, you can use an exponential trend model 
[see Equation (16.5)].
The Linear Trend Model
The linear trend model:
Yi =  b0 + b1Xi + ei
is the simplest forecasting model. Equation (16.3) defines the linear trend forecasting equation.
Linear Trend Forecasting Equation
	
Yni = b0 + b1Xi	
(16.3)

666	
Chapter 16  Time-Series Forecasting
Recall that in linear regression analysis, you use the method of least squares to compute 
the sample slope, b1, and the sample Y intercept, b0. You then substitute the values for X into 
Equation (16.3) to predict Y.
When using the least-squares method for fitting trends in a time series, you can simplify 
the interpretation of the coefficients by assigning coded values to the X (time) variable. You as-
sign consecutively numbered integers, starting with 0, as the coded values for the time periods. 
For example, in time-series data that have been recorded annually for 17 years, you assign the 
coded value 0 to the first year, the coded value 1 to the second year, the coded value 2 to the 
third year, and so on, concluding by assigning 16 to the seventeenth year.
In The Principled scenario on page 657, one of the companies of interest is The Coca-Cola 
Company. Founded in 1886 and headquartered in Atlanta, Georgia, Coca-Cola manufactures, 
distributes, and markets more than 500 beverage brands in over 200 countries worldwide. 
Brands include Coca-Cola, Diet Coke, Fanta, and Sprite, four of the world’s top five nonal-
coholic sparkling beverage products. According to The Coca-Cola Company’s website, rev-
enues in 2012 topped $48 billion. Table 16.2 lists The Coca-Cola Company’s gross revenues  
(in $billions) from 1996 to 2012 (stored in  Coca-Cola ).
Figure 16.5 presents the regression results for the simple linear regression model that uses 
the consecutive coded values 0 through 16 as the X (coded year) variable. These results pro-
duce the following linear trend forecasting equation:
Yni = 13.2745 + 1.6326Xi
where X1 = 0 represents 1996.
F i g u r e  1 6 . 5
Excel and Minitab 
regression results for 
the linear trend model 
to forecast revenues  
(in $billions) for The 
Coca-Cola Company
T a b l e  1 6 . 2
Revenues for The 
Coca-Cola Company 
(1996–2012)
Year
Revenues ($billions)
Year
Revenues ($billions)
1996
18.5
2005
23.1
1997
18.9
2006
24.1
1998
18.8
2007
28.9
1999
19.8
2008
31.9
2000
20.5
2009
31.0
2001
20.1
2010
35.1
2002
19.6
2011
46.5
2003
21.0
2012
48.0
2004
21.9
Source: Data extracted from Mergent’s Handbook of Common Stocks, 2006; and www.thecoca-
colacompany.com/investors/annual_other_reports.html.

	
16.4  Least-Squares Trend Fitting and Forecasting	
667
You interpret the regression coefficients as follows:
 • The Y intercept, b0 = 13.2745, is the predicted revenues (in $billions) at The Coca-Cola 
Company during the origin, or base, year, 1996.
 • The slope, b1 = 1.6326, indicates that revenues are predicted to increase by  
$1.6326 billion per year.
To project the trend in the revenues at Coca-Cola to 2013, you substitute X18 = 17, the 
code for 2013 into the linear trend forecasting equation:
Yni = 13.2745 + 1.63261172 = 41.0287 billions of dollars
The trend line is plotted in Figure 16.6, along with the observed values of the time series. 
There is a strong upward linear trend, and r 2 is 0.7712, indicating that more than 77% of the 
variation in revenues is explained by the linear trend of the time series. However, you can ob-
serve that the revenue for the most recent year, 2012, is substantially above the trend line, that 
the early years are also slightly above the trend line, but the middle years are below the trend 
line. To investigate whether a different trend model might provide a better fit, a quadratic trend 
model and an exponential trend model can be fitted.
F i g u r e  1 6 . 6
Plot of the linear trend 
forecasting equation for 
The Coca-Cola Company 
revenue data
The Quadratic Trend Model
A quadratic trend model:
Yi = b0 + b1Xi + b2X2
i + ei
is a nonlinear model that contains a linear term and a curvilinear term in addition to a Y inter-
cept. Using the least-squares method for a quadratic model described in Section 15.1, you can 
develop a quadratic trend forecasting equation, as presented in Equation (16.4).
Quadratic Trend Forecasting Equation
	
Yni = b0 + b1Xi + b2X2
i 	
(16.4)
where
b0 = estimated Y intercept
b1 = estimated linear effect on Y
b2 = estimated quadratic effect on Y

668	
Chapter 16  Time-Series Forecasting
Figure 16.7 presents the regression results for the quadratic trend model used to forecast 
revenues at The Coca-Cola Company.
In Figure 16.7,
Yni = 20.6249 - 1.3075Xi + 0.1838X2
i
where the year coded 0 is 1996.
To compute a forecast using the quadratic trend equation, you substitute the appropriate 
coded X value into this equation. For example, to forecast the trend in revenues for 2013 (i.e., 
X = 17),
Yni = 20.6249 - 1.30751172 + 0.183811722 = 51.5156
Figure 16.8 plots the quadratic trend forecasting equation along with the time series for 
the actual data. This quadratic trend model provides a better fit (adjusted r 2 = 0.9506) to the 
time series than does the linear trend model. The tSTAT test statistic for the contribution of the 
quadratic term to the model is 7.756 (p-value = 0.0000).
F i g u r e  1 6 . 8
Plot of the quadratic 
trend forecasting 
equation for The  
Coca-Cola Company 
revenue data
F i g u r e  1 6 . 7
Excel and Minitab regression results for the quadratic trend model to forecast revenues (in $billions) for The Coca-Cola 
Company

	
16.4  Least-Squares Trend Fitting and Forecasting	
669
The Exponential Trend Model
When a time series increases at a rate such that the percentage difference from value to value is 
constant, an exponential trend is present. Equation (16.5) defines the exponential trend model.
Exponential Trend Model
	
Yi = b0bXi
1 ei	
(16.5)
where
b0 = Y intercept
1b1 - 12 * 100% = annual compound growth rate (in %)
The model in Equation (16.5) is not in the form of a linear regression model. To transform 
this nonlinear model to a linear model, you use a base 10 logarithm transformation.1 Taking 
the logarithm of each side of Equation (16.5) results in Equation (16.6).
1Alternatively, you can use base e 
logarithms. For more information  
on logarithms, see Section A.3 in 
Appendix A.
Transformed Exponential Trend Model
	
 log1Yi2 = log1b0bXi
1 ei2
(16.6)
 = log1b02 + log1bXi
1 2 + log1ei2
 = log1b02 + Xilog1b12 + log1ei2
Equation (16.6) is a linear model you can estimate using the least-squares method, 
with log1Yi2 as the dependent variable and Xi as the independent variable. This results in  
Equation (16.7).
Exponential Trend Forecasting Equation
	
log1Yni2 = b0 + b1Xi	
(16.7a)
where
b0 = estimate of log1b02 and thus 10b0 = bn0
b1 = estimate of log1b12 and thus 10b1 = bn1
therefore,
	
Yni = bn0bnXi
1 	
(16.7b)
where
1bn1 - 12 * 100% is the estimated annual compound growth rate 1in %2
Student Tip
Log is the symbol used 
for base 10 logarithms. 
The log of a number is 
the power that 10 needs 
to be raised to equal that 
number.
Figure 16.9 shows the Excel and Minitab regression results for an exponential trend model 
of revenues at The Coca-Cola Company.
Using Equation (16.7a) and the results from Figure 16.9,
log1Yni2 = 1.2005 + 0.0248Xi
where the year coded 0 is 1996.

670	
Chapter 16  Time-Series Forecasting
You compute the values for bn0 and bn1 by taking the antilog of the regression coefficients 
(b0 and b1):
 bn0 = antilog1b02 = antilog11.20052 = 101.2005 = 15.8672
 bn1 = antilog1b12 = antilog10.02482 = 100.0248 = 1.0588
Thus, using Equation (16.7b), the exponential trend forecasting equation is
Yni = 115.8672211.05882Xi
where the year coded 0 is 1996.
The Y intercept, bn0 = 15.8672 billions of dollars, is the revenue forecast for the base year 
1996. The value 1bn1 - 12 * 100% = 5.88% is the annual compound growth rate in revenues 
at The Coca-Cola Company.
For forecasting purposes, you substitute the appropriate coded X values into either 
­Equation (16.7a) or Equation (16.7b). For example, to forecast revenues for 2013 (i.e., X = 17) 
using Equation (16.7a),
 log1Yni2 = 1.2005 + 0.02481172 = 1.6221
 Yni = antilog11.62212 = 101.6221 = 41.889 billions of dollars
Figure 16.10 plots the exponential trend forecasting equation, along with the time-series 
data. The adjusted r 2 for the exponential trend model (0.8427) is greater than the adjusted r 2 
for the linear trend model (0.7559) but less than for the quadratic model (0.9506).
F i g u r e  1 6 . 9
Excel and Minitab 
regression results for 
the exponential trend 
model to forecast 
revenues (in $billions) 
for The Coca-Cola 
Company
F i g u r e  1 6 . 1 0
Plot of the exponential 
trend forecasting 
equation for The  
Coca-Cola Company 
revenue data

	
16.4  Least-Squares Trend Fitting and Forecasting	
671
Model Selection Using First, Second,  
and Percentage Differences
You have used the linear, quadratic, and exponential models to forecast revenues for The Coca-
Cola Company. How can you determine which of these models is the most appropriate model? 
In addition to visually inspecting time-series plots and comparing adjusted r 2 values, you can 
compute and examine first, second, and percentage differences. The identifying features of 
linear, quadratic, and exponential trend models are as follows:
 • If a linear trend model provides a perfect fit to a time series, then the first differences are 
constant. Thus,
1Y2 - Y12 = 1Y3 - Y22 = c = 1Yn - Yn - 12
 • If a quadratic trend model provides a perfect fit to a time series, then the second differ-
ences are constant. Thus,
31Y3 - Y22 - 1Y2 - Y124 = 31Y4 - Y32 - 1Y3 - Y224 = c = 31Yn - Yn - 12 - 1Yn - 1 - Yn - 224
 • If an exponential trend model provides a perfect fit to a time series, then the percentage 
differences between consecutive values are constant. Thus,
Y2 - Y1
Y1
* 100% = Y3 - Y2
Y2
* 100% = g = Yn - Yn - 1
Yn - 1
* 100%
Although you should not expect a perfectly fitting model for any particular set of time-series 
data, you can consider the first differences, second differences, and percentage differences as 
guides in choosing an appropriate model. Examples 16.2, 16.3, and 16.4 illustrate linear, qua-
dratic, and exponential trend models that have perfect (or nearly perfect) fits to their respective 
data sets.
Example 16.2
A Linear Trend 
Model with a  
Perfect Fit
The following time series represents the number of customers per year (in thousands) at a 
branch of a fast-food chain:
Year
2004
2005
2006
2007
2008
2009
2010
2011
2012
2013
Customers Y
200
205
210
215
220
225
230
235
240
245
Using first differences, show that the linear trend model provides a perfect fit to these data.
Solution  The following table shows the solution:
Year
2004
2005
2006
2007
2008
2009
2010
2011
2012
2013
Customers Y
200
205
210
215
220
225
230
235
240
245
First differences
5.0
5.0
5.0
5.0
5.0
5.0
5.0
5.0
5.0
The differences between consecutive values in the series are the same throughout. Thus, the 
number of customers at the branch of the fast-food chain shows a linear growth pattern.

672	
Chapter 16  Time-Series Forecasting
Example 16.3
A Quadratic Trend 
Model with a  
Perfect Fit
The following time series represents the number of customers per year (in thousands) at  
another branch of a fast-food chain:
Year
2004
2005
2006
2007
2008
2009
2010
2011
2012
2013
Customers Y
200
201
203.5
207.5
213
220
228.5
238.5
250
263
Using second differences, show that the quadratic trend model provides a perfect fit to  
these data.
Solution  The following table shows the solution:
Year
2004
2005
2006
2007
2008
2009
2010
2011
2012
2013
Customers Y
200
201
203.5
207.5
213
220
228.5
238.5
250
263
First differences
1.0
2.5
4.0
5.5
7.0
8.5
10.0
11.5
13.0
Second differences
1.5
1.5
1.5
1.5
1.5
1.5
1.5
1.5
The second differences between consecutive pairs of values in the series are the same through-
out. Thus, branch of the fast-food chain shows a quadratic growth pattern. Its rate of growth is 
accelerating over time.
Example 16.4
An Exponential 
Trend Model  
with an Almost  
Perfect Fit
The following time series represents the number of customers per year (in thousands) for an-
other branch of the fast-food chain:
Year
2004
2005
2006
2007
2008
2009
2010
2011
2012
2013
Customers Y
200
206
212.18
218.55
225.11
231.86
238.82
245.98
253.36
260.96
Using percentage differences, show that the exponential trend model provides almost a perfect 
fit to these data.
Solution  The following table shows the solution:
Year
2004
2005
2006
2007
2008
2009
2010
2011
2012
2013
Customers Y
200
206
212.18
218.55
225.11
231.86
238.82
245.98
253.36
260.96
Percentage  
  differences
3.0
3.0
3.0
3.0
3.0
3.0
3.0
3.0
3.0
The percentage differences between consecutive values in the series are approximately the 
same throughout. Thus, this branch of the fast-food chain shows an exponential growth  
pattern. Its rate of growth is approximately 3% per year.

	
16.4  Least-Squares Trend Fitting and Forecasting	
673
Figure 16.11 shows a worksheet that compares the first, second, and percentage differ-
ences for the revenues data at The Coca-Cola Company. Neither the first differences, second 
differences, nor percentage differences are constant across the series. Therefore, other models 
(including those considered in Section 16.5) may be more appropriate.
F i g u r e  1 6 . 1 1
Excel worksheet that 
compares first, second, 
and percentage 
differences in revenues  
(in $billions) for The  
Coca-Cola Company
Problems for Section 16.4
Learning the Basics
16.9  Which graph would you require to get a visual expression 
of time series data? Explain which trend model should be used in 
case the time series data rejects the following types of graphs:
a.	 straight line trend.
b.	 long term downward or upward quadratic movement.
c.	 increase at a rate such that the percentage difference from value 
to value is constant.
d.	 Among the models you get as an answer in (a), (b) and (c), how 
would you determine which model to choose?
16.10  Assume the following linear trend forecasting model for 
the number of passengers travelling by air from 1975 to 1986:
Yni = 8.2 + 73.3Xi
a.	 Considering that the data has been collected from 1975 to 
1986, explain how the value of X has been coded to derive the 
above equation.
b.	 Construct a table presenting the values of X and Y and plot it on 
the graph.
c.	 Predict the number of airline travelers in the year 1987.
16.11  Assume that the data that has been used to formulate linear 
trend model in question 16.10 has also been used to formulate the 
following quadratic model:
Yni = 120 + 25.3 Xi + 3.69 Xi
2
a.	 Construct a table presenting the values of X and Y. Plot it on a 
graph.
b.	 Make a graph, assuming the actual passengers travelled were 
181, 184, 230, 235, 302, 409, 458, 639, 647, 792, 801, 942
c.	 Comment whether the model in 16.10 or 16.11 is a better fit.
Applying the Concepts
SELF 
Test 
16.12  Bed Bath & Beyond is a nationwide chain of 
retail stores that sell a wide assortment of merchandise, 
including domestics merchandise and home furnishings, as well as 
food, giftware, and health and beauty care items. The number of 
stores open at the end of the fiscal year from 1997 to 2013 is stored 
in  Bed & Bath  and shown in right column.
Year
Stores Opened
Year
Stores Opened
1997
108
2006
809
1998
141
2007
888
1999
186
2008
971
2000
241
2009
1,037
2001
311
2010
1,100
2002
396
2011
1,139
2003
519
2012
1,173
2004
629
2013
1,471
2005
721
Source: Data extracted from Bed Bath & Beyond Annual Report, 2013.
a.	 Plot the data.
b.	 Compute a linear trend forecasting equation and plot the  
results.
c.	 Compute a quadratic trend forecasting equation and plot the  
results.
d.	 Compute an exponential trend forecasting equation and plot the 
results.
e.	 Using the forecasting equations in (b) through (d), what are your 
annual forecasts of the number of stores open for 2014 and 2015?
f.	 How can you explain the differences in the three forecasts in 
(e)? What forecast do you think you should use? Why?

674	
Chapter 16  Time-Series Forecasting
16.13  Gross domestic product (GDP) is a major indicator of a 
nation’s overall economic activity. It consists of personal con-
sumption expenditures, gross domestic investment, net exports of 
goods and services, and government consumption expenditures. The 
file  GDP  contains the GDP (in billions of current dollars) for the 
United States from 1980 to 2012. (Data extracted from Bureau of 
Economic Analysis, U.S. Department of Commerce, www.bea.gov.)
a.	 Plot the data.
b.	 Compute a linear trend forecasting equation and plot the trend line.
c.	 What are your forecasts for 2013 and 2014?
d.	 What conclusions can you reach concerning the trend in GDP?
16.14  The data in  FedReceipt  represent federal receipts from 
1978 through 2012, in billions of current dollars, from individual 
and corporate income tax, social insurance, excise tax, estate and 
gift tax, customs duties, and federal reserve deposits. (Data ex-
tracted from “Historical Federal Receipt and Outlay Summary,” 
Tax Policy Center, bit.ly/7dGCmz)
a.	 Plot the series of data.
b.	 Compute a linear trend forecasting equation and plot the trend line.
c.	 What are your forecasts of the federal receipts for 2013 and 2014?
d.	 What conclusions can you reach concerning the trend in federal 
receipts?
16.15  The file  ComputerSales  contains the U.S. total computer 
and software sales (in $millions) from 1992 through 2012.
a.	 Plot the data.
b.	 Compute a linear trend forecasting equation and plot the trend line.
c.	 Compute a quadratic trend forecasting equation and plot the results.
d.	 Compute an exponential trend forecasting equation and plot the 
results.
e.	 Which model is the most appropriate?
f.	 Using the most appropriate model, forecast U.S. total computer 
and software sales, in millions, for 2013.
16.16  The data shown in the following table and stored in  
 Solar Power  represent the yearly amount of solar power gener-
ated by utilities (in millions of kWh) in the United States from 
2002 through 2012:
Year
Solar Power 
Generated  
(millions of kWh)
Year
Solar Power 
Generated 
(millions of kWh)
2002
555
2008
864
2003
534
2009
891
2004
575
2010
1,212
2005
550
2011
1,814
2006
508
2012
4,432
2007
612
Source: Data extracted from en.wikipedia.org/wiki/Solar_power_in__
the_United_States.
a.	 Plot the data.
b.	 Compute a linear trend forecasting equation and plot the trend 
line.
c.	 Compute a quadratic trend forecasting equation and plot the  
results.
d.	 Compute an exponential trend forecasting equation and plot the 
results.
e.	 Using the models in (b) through (d), what are your annual trend 
forecasts of the yearly amount of solar power generated by utili-
ties (in millions of kWh) in the United States in 2013 and 2014?
16.17  The file  CarProduction  contains the number of passenger 
cars produced in the U.S. from 1999 to 2012. (Data extracted from 
www.statista.com.)
a.	 Plot the data.
b.	 Compute a linear trend forecasting equation and plot the trend line.
c.	 Compute a quadratic trend forecasting equation and plot the results.
d.	 Compute an exponential trend forecasting equation and plot the 
results.
e.	 Which model is the most appropriate?
f.	 Using the most appropriate model, forecast the U.S. car pro-
duction for 2013.
16.18  The average salary of Major League Baseball players on open-
ing day from 2000 to 2013 is stored in  BBSalaries  and shown below.
Year
Salary ($millions)
Year
Salary ($millions)
2000
1.99
2007
2.92
2001
2.29
2008
3.13
2002
2.38
2009
3.26
2003
2.58
2010
3.27
2004
2.49
2011
3.32
2005
2.63
2012
3.38
2006
2.83
2013
4.25
Source: Data extracted from “Baseball Salaries,” USA Today, April 6, 2009, 
p. 6C; and mlb.com.
a.	 Plot the data.
b.	 Compute a linear trend forecasting equation and plot the trend line.
c.	 Compute a quadratic trend forecasting equation and plot the results.
d.	 Compute an exponential trend forecasting equation and plot the 
results.
e.	 Which model is the most appropriate?
f.	 Using the most appropriate model, forecast the average salary 
for 2014.
16.19  The file  Silver  contains the following prices in London for 
an ounce of silver (in US$) on the last day of the year from 1999 
to 2012:
Year
Price (US$/ounce)
Year
Price (US$/ounce)
1999
5.330
2006
12.900
2000
4.570
2007
14.760
2001
4.520
2008
10.790
2002
4.670
2009
16.990
2003
5.965
2010
30.630
2004
6.815
2011
28.180
2005
8.830
2012
29.950
Source: Data extracted from bit.ly/1afifi.
a.	 Plot the data.
b.	 Compute a linear trend forecasting equation and plot the trend line.
c.	 Compute a quadratic trend forecasting equation and plot the results.
d.	 Compute an exponential trend forecasting equation and plot the 
results.
e.	 Which model is the most appropriate?
f.	 Using the most appropriate model, forecast the price of silver at 
the end of 2013.
16.20  The data in  CPI-U  reflect the annual values of the consumer 
price index (CPI) in the United States over the 48-year period 1965 
through 2012, using 1982 through 1984 as the base period. This 
index measures the average change in prices over time in a fixed 

	
16.5  Autoregressive Modeling for Trend Fitting and Forecasting 	
675
“market basket” of goods and services purchased by all urban con-
sumers, including urban wage earners (i.e., clerical, professional, 
managerial, and technical workers; self-employed individuals; and 
short-term workers), unemployed individuals, and retirees. (Data 
extracted from Bureau of Labor Statistics, U.S. ­Department of 
­Labor, www.bls.gov.)
a.	 Plot the data.
b.	 Describe the movement in this time series over the 48-year period.
c.	 Compute a linear trend forecasting equation and plot the trend line.
d.	 Compute a quadratic trend forecasting equation and plot the results.
e.	 Compute an exponential trend forecasting equation and plot the 
results.
f.	 Which model is the most appropriate?
g.	 Using the most appropriate model, forecast the CPI for 2013 
and 2014.
16.21  Although you should not expect a perfectly fitting model 
for any time-series data, you can consider the first differences, sec-
ond differences, and percentage differences for a given series as 
guides in choosing an appropriate model.
For this problem, use each of the time series presented in the table 
in the left column and stored in  Tsmodel1 :
a.	 Determine the most appropriate model.
b.	 Compute the forecasting equation.
c.	 Forecast the value for 2013.
16.22  A time-series plot often helps you determine the appro-
priate model to use. For this problem, use each of the time series 
presented in the following table and stored in  TsModel2 :
Year
Series I
Series II
Series III
2001
  10.0
30.0
  60.0
2002
  15.1
33.1
  67.9
2003
  24.0
36.4
  76.1
2004
  36.7
39.9
  84.0
2005
  53.8
43.9
  92.2
2006
  74.8
48.2
100.0
2007
100.0
53.2
108.0
2008
129.2
58.2
115.8
2009
162.4
64.5
124.1
2010
199.0
70.7
132.0
2011
239.3
77.1
140.0
2012
283.5
83.9
147.8
Year
Series I
Series II
2001
100.0
100.0
2002
115.2
115.2
2003
130.1
131.7
2004
144.9
150.8
2005
160.0
174.1
2006
175.0
200.0
2007
189.8
230.8
2008
204.9
266.1
2009
219.8
305.5
2010
235.0
351.8
2011
249.8
403.0
2012
264.9
469.2
a.	 Plot the observed data (Y) over time (X) and plot the logarithm 
of the observed data (log Y) over time (X) to determine whether 
a linear trend model or an exponential trend model is more ap-
propriate. (Hint: If the plot of log Y versus X appears to be lin-
ear, an exponential trend model provides an appropriate fit.)
b.	 Compute the appropriate forecasting equation.
c.	 Forecast the value for 2013.
16.5  Autoregressive Modeling for Trend Fitting  
and Forecasting
Frequently, the values of a time series at particular points in time are highly correlated with 
the values that precede and succeed them. This type of correlation is called autocorrelation. 
When the autocorrelation exists between values that are in consecutive periods in a time series, 
the time series displays first-order autocorrelation. When the autocorrelation exists between 
values that are two periods apart, the time series displays second-order autocorrelation. For 
the general case in which the autocorrelation exists between values that are p periods apart, the 
time series displays pth-order autocorrelation.
Autoregressive modeling is a technique used to forecast time series that display autocorre-
lation.2 This type of modeling uses a set of lagged predictor variables to overcome the problems 
that autocorrelation causes with other models. A lagged predictor variable takes its value from 
the value of predictor variable for another time period. For the general case of pth-order autocor-
relation, you create a set of p lagged predictor variables such that the first lagged predictor vari-
able takes its value from the value of a predictor variable that is one time period away, the lag; 
that the second lagged predictor variable takes its value from the value of a predictor variable that 
is two time periods away; and so on until the last, or pth, lagged predictor variable that takes its 
value from the value of a predictor variable that is p time periods away.
Equation (16.8) defines the pth-order autoregressive model. In the equation, 
A0, A1, c, Ap represent the parameters and a0, a1, c, ap represent the corresponding regres-
sion coefficients. This is similar to the multiple regression model, Equation (14.1) on page 573,  
2The exponential smoothing model 
described in Section 16.3 and the 
­autoregressive models described 
in this section are special cases of 
­autoregressive integrated moving 
average (ARIMA) models developed 
by Box and Jenkins (see reference 2).

676	
Chapter 16  Time-Series Forecasting
in which b0, b1, c, bk, represent the regression parameters and b0, b1, c, bk represent the 
corresponding regression coefficients.
Student Tip
d is the Greek letter delta
pth-Order Autoregressive Models
	
Yi = A0 + A1Yi - 1 + A2Yi - 2 + g + ApYi - p + di	
(16.8)
where
Yi = observed value of the series at time i
Yi - 1 = observed value of the series at time i - 1
Yi - 2 = observed value of the series at time i - 2
Yi - p = observed value of the series at time i - p
p = number of autoregression parameters (not including a Y intercept) to 
be estimated from least-squares regression analysis
A0, A1, A2, c, Ap = autoregression parameters to be estimated from least-squares 
regression analysis
di = a nonautocorrelated random error component (with mean = 0 and 
constant variance)
Equations (16.9) and (16.10) define two specific autoregressive models. Equation (16.9) 
defines the first-order autoregressive model and is similar in form to the simple linear re-
gression model, Equation (13.1) on page 521. Equation (16.10) defines the second-order 
autoregressive model and is similar to the multiple regression model with two independent 
variables, Equation (14.2) on page 573.
First-Order Autoregressive Model
	
Yi = A0 + A1Yi - 1 + di	
(16.9)
Second-Order Autoregressive Model
	
Yi = A0 + A1Yi - 1 + A2Yi - 2 + di	
(16.10)
Selecting an Appropriate Autoregressive Model
Selecting an appropriate autoregressive model can be complicated. You must weigh the ad-
vantages of using a simpler model against the concern of not taking into account important 
autocorrelation in the data. You also must be concerned with selecting a higher-order model 
that requires estimates of numerous parameters, some of which may be unnecessary, especially 
if n, the number of values in the series, is small. The reason for this concern is that when com-
puting an estimate of Ap, you lose p out of the n data values when comparing each data value 
with the data value p periods earlier. Examples 16.5 and 16.6 illustrate this loss of data values.
Example 16.5
Comparison 
Schema for a  
First-Order  
Autoregressive 
Model
Consider the following series of n = 7 consecutive annual values:
Year
1
2
3
4
5
6
7
Series
31
34
37
35
36
43
40
Show the comparisons needed for a first-order autoregressive model.

	
16.5  Autoregressive Modeling for Trend Fitting and Forecasting 	
677
Determining the Appropriateness of a Selected Model
After selecting a model and using the least-squares method to compute the regression coef-
ficients, you need to determine the appropriateness of the model. Either you can select a par-
ticular pth-order autoregressive model based on previous experiences with similar data or start 
with a model that contains several autoregressive parameters and then eliminate the higher-
order parameters that do not significantly contribute to the model. In this latter approach, you 
use a t test for the significance of Ap, the highest-order autoregressive parameter in the current 
model under consideration. The null and alternative hypotheses are:
 H0: Ap = 0
 H1: Ap ≠0
Equation (16.11) defines the test statistic.
Solution 
Year i
First-Order Autoregressive Model 1Lag1: Yi versus Yi - 12
1
31 4 c
2
34 4 31
3
37 4 34
4
35 4 37
5
36 4 35
6
43 4 36
7
40 4 43
Because Y1 is the first value and there is no value prior to it, Y1 is not used in the regression 
analysis. Therefore, the first-order autoregressive model would be based on six pairs of values.
Example 16.6
Comparison 
Schema for a 
­Second-Order 
­Autoregressive 
Model
Consider the following series of n = 7 consecutive annual values:
Year
1
2
3
4
5
6
7
Series
31
34
37
35
36
43
40
Year i
Second-Order Autoregressive Model 1Lag2: Yi vs. Yi - 1 and Yi vs. Yi - 22
1
31 4 c  and 31 4 c
2
34 4 31 and 34 4 c
3
37 4 34 and 37 4 31
4
35 4 37 and 35 4 34
5
36 4 35 and 36 4 37
6
43 4 36 and 43 4 35
7
40 4 43 and 40 4 36
Show the comparisons needed for a second-order autoregressive model.
Solution 
Because no value is recorded prior to Y1, the first two comparisons, each of which requires a 
value prior to Y1, cannot be used when performing regression analysis. Therefore, the second-
order autoregressive model would be based on five pairs of values.

678	
Chapter 16  Time-Series Forecasting
For a given level of significance, a, you reject the null hypothesis if the tSTAT test statistic 
is greater than the upper-tail critical value from the t distribution or if the tSTAT test statistic is 
less than the lower-tail critical value from the t distribution. Thus, the decision rule is
reject H0 if tSTAT 6 -ta>2 or if tSTAT 7 ta>2;
otherwise, do not reject H0.
Figure 16.12 illustrates the decision rule and regions of rejection and nonrejection.
t Test for Significance of the Highest-Order  
Autoregressive Parameter, Ap
	
tSTAT =
ap - Ap
Sap
	
(16.11)
where
Ap = hypothesized value of the highest-order parameter, Ap, in the autoregressive 
model
ap = regression coefficient that estimates the highest-order parameter, Ap, in the 
autoregressive model
Sap = standard deviation of ap
The tSTAT test statistic follows a t distribution with n - 2p - 1 degrees of freedom.
In addition to the degrees of freedom lost for each of the p population parameters 
you are estimating, p additional degrees of freedom are lost because there are p fewer 
comparisons to be made from the original n values in the time series.
F i g u r e  1 6 . 1 2
Rejection regions 
for a two-tail test 
for the significance 
of the highest-order 
autoregressive 
parameter Ap
Region of
Rejection
Region of
Nonrejection
Critical
Value
Region of
Rejection
Critical
Value
0
t
+t
–t 
Ap = 0
a/2
a/2
a/2
a/2
1 – a
If you do not reject the null hypothesis that Ap = 0, you conclude that the selected model 
contains too many estimated autoregressive parameters. You then discard the highest-order 
term and develop an autoregressive model of order p - 1, using the least-squares method. 
You then repeat the test of the hypothesis that the new highest-order parameter is 0. This test-
ing and modeling continues until you reject H0. When this occurs, you can conclude that the 
remaining highest-order parameter is significant, and you can use that model for forecasting 
purposes.

	
16.5  Autoregressive Modeling for Trend Fitting and Forecasting 	
679
Equation (16.12) defines the fitted pth-order autoregressive equation.
Fitted pth-Order Autoregressive Equation
	
Yni = a0 + a1Yi - 1 + a2Yi - 2 + g + apYi - p	
(16.12)
where
Yni = fitted values of the series at time i
Yi - 1 = observed value of the series at time i - 1
Yi - 2 = observed value of the series at time i - 2
Yi - p = observed value of the series at time i - p
p = number of autoregression parameters (not including a Y intercept) 
to be estimated from least-squares regression analysis
a0, a1, a2, c , ap = regression coefficients
pth-Order Autoregressive Forecasting Equation
	
Ynn + j = a0 + a1Ynn + j - 1 + a2Ynn + j - 2 + g + apYnn + j - p	
(16.13)
where
a0, a1, a2, c, ap = regression coefficients that estimate the parameters
p = number of autoregression parameters (not including a Y intercept) 
to be estimated from least-squares regression analysis
j = number of years into the future
Ynn + j - p = forecast of Yn + j - p from the current year for j - p 7 0
Ynn + j - p = observed value for Yn + j - p for j - p …  0
Thus, to make forecasts j years into the future, using a third-order autoregressive model, 
you need only the most recent p = 3 values (Yn, Yn - 1, and Yn - 2) and the regression estimates 
a0, a1, a2, and a3.
To forecast one year ahead, Equation (16.13) becomes
Ynn + 1 = a0 +  a1Yn + a2Yn - 1 + a3Yn - 2
To forecast two years ahead, Equation (16.13) becomes
Ynn + 2 = a0 + a1Ynn + 1 + a2Yn + a3Yn - 1
To forecast three years ahead, Equation (16.13) becomes
Ynn + 3 = a0 +  a1Ynn + 2 + a2Ynn + 1 + a3Yn
and so on.
Autoregressive modeling is a powerful forecasting technique for time series that have 
autocorrelation. To summarize, you construct an autoregressive model by following these 
steps:
	
1.	 Choose a value for p, the highest-order parameter in the autoregressive model to be 
evaluated, realizing that the t test for significance is based on n - 2p - 1 degrees of 
freedom.
	
2.	 Create a set of p lagged predictor variables. (See Figure 16.13 for an example.)
You use Equation (16.13) to forecast j years into the future from the current nth time period.

680	
Chapter 16  Time-Series Forecasting
F i g u r e  1 6 . 1 3
Minitab worksheet 
data for developing 
first-order, second-
order, and third-order 
autoregressive models 
of the revenues for The 
Coca-Cola Company 
(1996–2012)
	
3.	 Perform a least-squares analysis of the multiple regression model containing all  
p lagged predictor variables using Excel or Minitab.
	
4.	 Test for the significance of Ap, the highest-order autoregressive parameter in the model.
	
5.	 If you do not reject the null hypothesis, discard the pth variable and repeat steps 3  
and 4. The test for the significance of the new highest-order parameter is based on a 
t distribution whose degrees of freedom are revised to correspond with the revised 
number of predictors.
If you reject the null hypothesis, select the autoregressive model with all p predictors 
for fitting [see Equation (16.12)] and forecasting [see Equation (16.13)].
To demonstrate the autoregressive modeling approach, return to the time series 
concerning the revenues for The Coca-Cola Company over the 17-year period 1996 
through 2012. Figure 16.13 displays a worksheet that organizes the data for the first-
order, second-order, and third-order autoregressive models. 
Student Tip
Remember that in an au-
toregressive model, the 
independent variable(s) 
are equal to the depen-
dent variable lagged by 
a certain number of time 
periods.
F i g u r e  1 6 . 1 4
Excel and Minitab 
regression results 
for a third-order 
autoregressive 
model for The Coca-
Cola Company 
revenues
The worksheet contains the lagged predictor variables Lag1, Lag2, and Lag3 in columns 
C3, C4, and C5. Use all three lagged predictors to fit the third-order autoregressive model. Use 
only Lag1 and Lag2 to fit the second-order autoregressive model, and use only Lag1 to fit the 
first-order autoregressive models. Thus, out of n = 17 values, p = 1, 2, or 3 values out of 
n = 17 are lost in the comparisons needed for developing the first-order, second-order, and 
third-order autoregressive models.
Selecting an autoregressive model that best fits the annual time series begins with the 
third-order autoregressive model shown in Figure 16.14.

	
16.5  Autoregressive Modeling for Trend Fitting and Forecasting 	
681
From Figure 16.14, the fitted third-order autoregressive equation is
Yni = -12.0725 + 0.7974Yi - 1 - 0.9298Yi - 2 + 1.8322Yi - 3
where the first year in the series is 1999.
Next, you test for the significance of A3, the highest-order parameter. The highest-order 
­regression coefficient, a3, for the fitted third-order autoregressive model is 1.8322, with a 
­standard error of 0.3185.
To test the null hypothesis:
H0: A3 = 0
against the alternative hypothesis:
H1: A3 ≠0
using Equation (16.11) on page 678 and the worksheet results given in Figure 16.14,
tSTAT = a3 - A3
Sa3
= 1.8322 - 0
0.3185
= 5.7529
Using a 0.05 level of significance, the two-tail t test with 14 - 3 - 1 = 10 degrees of free-
dom has critical values of {2.2281. Because tSTAT = 5.7529 7 2.2281 or because the 
p@value = 0.0002 6 0.05, you reject H0. You conclude that the third-order parameter of the 
autoregressive model is significant and should remain in the model.
The model-building approach has led to the selection of the third-order autoregres-
sive model as the most appropriate for the given data. Using the estimates a0 = -12.0725, 
a1 = 0.7974, a2 = -0.9298, and a3 = 1.8322, as well as the most recent data value 
Y16 = 48.0, the forecasts of revenues from Equation (16.13) on page 679 at The Coca-Cola 
Company for 2013 and 2014 are
Ynn + j = -12.0725 + 0.7974 Ynn + j - 1 - 0.9298Ynn + j - 2 + 1.8322Ynn + j - 3
Therefore, for 2013, one year ( j = 1) ahead:
 Yn17 = -12.0725 + 0.7974 148.02 - 0.9298146.52 + 1.8322135.12
 = 47.2722 billions of dollars
and for 2014, two years ( j = 2) ahead:
 Yn18 = -12.0725 + 0.7974 147.77222 - 0.9298148.02 + 1.8322146.52
 = 66.19 billions of dollars
Figure 16.15 displays the actual and predicted Y values from the third-order autoregressive 
model.
Student Tip
In many cases, the 
third-order autoregres-
sive coefficient will not 
be significant. For such 
cases, you need to elimi-
nate the third-order term 
and fit a second-order 
autoregressive model. If 
the second-order autore-
gressive coefficient is not 
significant, you eliminate 
the second-order term 
and fit a first-order 
autoregressive model.
F i g u r e  1 6 . 1 5
Plot of actual and 
predicted revenues 
from a third-order 
autoregressive model for 
The Coca-Cola Company

682	
Chapter 16  Time-Series Forecasting
Problems for Section 16.5
Learning the Basics
16.23  You are given an annual time series with 40 consecutive 
values and asked to fit a fifth-order autoregressive model.
a.	 How many comparisons are lost in developing the autoregres-
sive model?
b.	 How many parameters do you need to estimate?
c.	 Which of the original 40 values do you need for forecasting?
d.	 State the fifth-order autoregressive model.
e.	 Write an equation to indicate how you would forecast j years 
into the future.
16.24  A third-order autoregressive model is fitted to an annual 
time series with 17 values and has the following estimated param-
eters and standard errors:
 a0 = 4.50 a1 = 1.80 a2 = 0.80 a3 = 0.24
 Sa1 = 0.50 Sa2 = 0.30 Sa3 = 0.10
At the 0.05 level of significance, test the appropriateness of the 
fitted model.
16.25  Refer to Problem 16.24. The three most recent values are
Y15 = 23 Y16 = 28 Y17 = 34
Forecast the values for the next year and the following year.
16.26  Assume for the mean loan interest rate, following autore-
gressive model has been derived:
Interest Rate = -2.0 + 1.8 (Interest Rate)i-1 - 0.5 (Interest Rate)i-2
a.	 Which order autoregressive model is it?
b.	 Predict the forecasted interest rate for the year 1996, assuming the 
interest rate in 1995 was 7 and in 1994 was 6.4.
Applying the Concepts
16.27  Using the data for Problem 16.15 on page 674 that rep-
resent U.S. total computer and software sales (in $millions) from 
1992 through 2012 (stored in  ComputerSales ),
a.	 fit a third-order autoregressive model to the total sales and test 
for the significance of the third-order autoregressive parameter. 
(Use a = 0.05.)
b.	 if necessary, fit a second-order autoregressive model to the total 
sales and test for the significance of the second-order autore-
gressive parameter. (Use a = 0.05.)
c.	 if necessary, fit a first-order autoregressive model to the total 
sales and test for the significance of the first-order autoregres-
sive parameter. (Use a = 0.05.)
d.	 if appropriate, forecast the total sales in 2013.
SELF 
Test 
16.28  Using the data for Problem 16.12 on page 673 
concerning the number of stores open for Bed Bath & 
Beyond from 1997 through 2013 (stored in  Bed & Bath ),
a.	 fit a third-order autoregressive model to the number of stores 
and test for the significance of the third-order autoregressive 
parameter. (Use a = 0.05.)
b.	 if necessary, fit a second-order autoregressive model to the 
number of stores and test for the significance of the second-
order autoregressive parameter. (Use a = 0.05.)
c.	 if necessary, fit a first-order autoregressive model to the num-
ber of stores and test for the significance of the first-order  
autoregressive parameter. (Use a = 0.05.)
d.	 if appropriate, forecast the number of stores open in 2014 and 
2015.
16.29  Using the data for Problem 16.17 on page 674 concerning 
the number of passenger cars produced in the United States from 
1999 to 2012 (stored in  CarProduction ),
a.	 fit a third-order autoregressive model to the number of pas-
senger cars produced in the United States and test for the sig-
nificance of the third-order autoregressive parameter. (Use 
a = 0.05.)
b.	 if necessary, fit a second-order autoregressive model to the 
number of passenger cars produced in the United States and 
test for the significance of the second-order autoregressive  
parameter. (Use a = 0.05.)
c.	 if necessary, fit a first-order autoregressive model to the num-
ber of passenger cars produced in the United States and test 
for the significance of the first-order autoregressive parameter. 
(Use a = 0.05.)
d.	 forecast the U.S. car production for 2013.
16.30  Using the average baseball salary from 2000 through 2013 
data for Problem 16.18 on page 674 (stored in  BBSalaries ),
a.	 fit a third-order autoregressive model to the average baseball 
salary and test for the significance of the third-order autore-
gressive parameter. (Use a = 0.05.)
b.	 if necessary, fit a second-order autoregressive model to the  
average baseball salary and test for the significance of the  
second-order autoregressive parameter. (Use a = 0.05.)
c.	 if necessary, fit a first-order autoregressive model to the aver-
age baseball salary and test for the significance of the first- 
order autoregressive parameter. (Use a = 0.05.)
d.	 forecast the average baseball salary for 2014.
16.31  Using the yearly amount of solar power generated by utili-
ties (in millions of kWh) in the United States from 2002 through 
2012 data for Problem 16.16 on page 674 (stored in  SolarPower ),
a.	 fit a third-order autoregressive model to the amount of solar 
power installed and test for the significance of the third-order 
autoregressive parameter. (Use a = 0.05.)
b.	 if necessary, fit a second-order autoregressive model to the 
amount of solar power installed and test for the significance of 
the second-order autoregressive parameter. (Use a = 0.05.)
c.	 if necessary, fit a first-order autoregressive model to the amount 
of solar power installed and test for the significance of the first-
order autoregressive parameter. (Use a = 0.05.)
d.	 forecast the yearly amount of solar power generated by utilities 
(in millions of kWh) in the United States in 2013 and 2014.

	
16.6  Choosing an Appropriate Forecasting Model	
683
16.6  Choosing an Appropriate Forecasting Model
In Sections 16.4 and 16.5, you studied six time-series methods for forecasting: the linear trend 
model, the quadratic trend model, and the exponential trend model in Section 16.4; and the 
first-order, second-order, and pth-order autoregressive models in Section 16.5. Is there a best 
model? Among these models, which one should you select for forecasting? The following 
guidelines are provided for determining the adequacy of a particular forecasting model. These 
guidelines are based on a judgment of how well the model fits the data and assume that you 
can use past data to predict future values of the time series:
 • Perform a residual analysis.
 • Measure the magnitude of the residuals through squared differences.
 • Measure the magnitude of the residuals through absolute differences.
 • Use the principle of parsimony.
A discussion of these guidelines follows.
Performing a Residual Analysis
Recall from Sections 13.5 and 14.3 that residuals are the differences between observed and 
predicted values. After fitting a particular model to a time series, you plot the residuals over 
the n time periods. As shown in Figure 16.16 Panel A, if the particular model fits adequately, 
the residuals represent the irregular component of the time series. Therefore, they should be 
randomly distributed throughout the series. However, as illustrated in the three remaining 
panels of Figure 16.16, if the particular model does not fit adequately, the residuals may show 
a systematic pattern, such as a failure to account for trend (Panel B), a failure to account for 
cyclical variation (Panel C), or, with monthly or quarterly data, a failure to account for sea-
sonal variation (Panel D).
F i g u r e  1 6 . 1 6
Residual analysis for 
studying patterns of 
errors in regression 
models
0
0
1
2
3
4
5
Time (years)
Panel A
Randomly distributed forecast errors
6
7
8
9 10
0
0
1
2
3
4
5
Time (years)
Panel C
Cyclical effects not accounted for
6
7
8
9 10
ei = Yi – Yi
ei = Yi – Yi
ei = Yi – Yi
ei = Yi – Yi
0
0
1
2
3
4
5
Time (years)
Panel B
Trend not accounted for
6
7
8
9 10
0
0
1
2
3
4
5
Time (years)
Panel D
Seasonal effects not accounted for  
6
7
8
9 10
Measuring the Magnitude of the Residuals Through 
Squared or Absolute Differences
If, after performing a residual analysis, you still believe that two or more models appear to fit 
the data adequately, you can use additional methods for model selection. Numerous measures 
based on the residuals are available (see references 1 and 4).
In regression analysis (see Section 13.3), you have already used the standard error of the 
estimate SYX as a measure of variation around the predicted values. For a particular model, this 
measure is based on the sum of squared differences between the actual and predicted ­values 
in a time series. If a model fits the time-series data perfectly, then the standard error of the 

684	
Chapter 16  Time-Series Forecasting
estimate is zero. If a model fits the time-series data poorly, then SYX is large. Thus, when com-
paring the adequacy of two or more forecasting models, you can select the model with the 
smallest SYX as most appropriate.
However, a major drawback to using SYX when comparing forecasting models is that 
whenever there is a large difference between even a single Yi and Yni, the value of SYX becomes 
overly inflated because the differences between Yi and Yni are squared. For this reason, many 
statisticians prefer the mean absolute deviation (MAD). Equation (16.14) defines the MAD as 
the mean of the absolute differences between the actual and predicted values in a time series.
Mean Absolute Deviation
	
MAD =
a
n
i=1
Yi - Yni 
n
	
(16.14)
If a model fits the time-series data perfectly, the MAD is zero. If a model fits the time-
series data poorly, the MAD is large. When comparing two or more forecasting models, you 
can select the one with the smallest MAD as the most appropriate model.
Using the Principle of Parsimony
If, after performing a residual analysis and comparing the SYX and MAD measures, you still 
believe that two or more models appear to adequately fit the data, you can use the principle of 
parsimony for model selection. As first explained in Section 15.4, parsimony guides you to 
select the regression model with the fewest independent variables that can predict the depen-
dent variable adequately. In general, the principle of parsimony guides you to select the least 
complex regression model. Among the six forecasting models studied in this chapter, most 
statisticians consider the least-squares linear and quadratic models and the first-order autore-
gressive model as simpler than the second- and pth-order autoregressive models and the least-
squares exponential model.
A Comparison of Four Forecasting Methods
To illustrate the model selection process, you can compare four of the forecasting models used 
in Sections 16.4 and 16.5: the linear model, the quadratic model, the exponential model, and 
the third-order autoregressive model. Figure 16.17 shows the residual plots for the four models 
for The Coca-Cola Company revenues. In reaching conclusions from these residual plots, you 
must use caution because there are only 17 values for the linear model, the quadratic model, 
and the exponential model and only 14 values for the third-order autoregressive model.
In Figure 16.17, observe that the residuals in the linear model, quadratic model, and exponen-
tial model are positive for the early years, negative for the intermediate years, and positive again for 
the latest years. For the autoregressive model, the residuals do not exhibit any systematic pattern.
To summarize, on the basis of the residual analysis of all four forecasting models, it ­appears 
that the third-order autoregressive model is the most appropriate, and the linear, quadratic, and 
exponential models are not appropriate. For further verification, you can compare the magni-
tude of the residuals in the four models. Figure 16.18 shows the actual values 1Yi2 along with 
the predicted values Yni, the residuals 1ei2, the error sum of squares (SSE), the standard error of 
the estimate 1SYX2, and the mean absolute deviation (MAD) for each of the four models.
For this time series, SYX and MAD provide fairly similar results. A comparison of the SYX 
and MAD clearly indicates that the linear model provides the poorest fit followed by the expo-
nential model and then the quadratic model. The third-order autoregressive model provides the 
best fit. Thus, you should choose the third-order autoregressive model as the best model.

	
16.6  Choosing an Appropriate Forecasting Model	
685
After you select a particular forecasting model, you need to continually monitor your fore-
casts. If large errors between forecasted and actual values occur, the underlying structure of the 
time series may have changed. Remember that the forecasting methods presented in this chapter 
assume that the patterns inherent in the past will continue into the future. Large forecasting errors 
are an indication that this assumption may no longer be true.
F i g u r e  1 6 . 1 7
Residual plots for four forecasting models
F i g u r e  1 6 . 1 8
Comparison of four 
forecasting models using 
SSE, SYX, and MAD
Problems for Section 16.6
Learning the Basics
16.32  Answer the following questions regarding choosing an ap-
propriate model:
a.	 What is the first step in choosing which model fits the data 
best?
b.	 If you cannot decide on using the above step, what shall be the 
next test you could undertake?
c.	 What is the principle of parsimony? Under what circumstances 
can you use it?
16.33  Refer to Problem 16.32. Suppose the first residual is 12.0 
(instead of 2.0) and the last residual is -11.0 (instead of -1.0).
a.	 Compute SYX and interpret your findings
b.	 Compute the MAD and interpret your findings.

686	
Chapter 16  Time-Series Forecasting
Applying the Concepts
16.34  Using the yearly amount of solar power generated by utili-
ties (in millions of kWh) in the United States from 2002 through 
2012 data for Problem 16.16 on page 674 and Problem 16.31 on 
page 682 (stored in  SolarPower ),
a.	 perform a residual analysis.
b.	 compute the standard error of the estimate 1SYX2.
c.	 compute the MAD.
d.	 On the basis of (a) through (c), and the principle of parsimony, 
which forecasting model would you select? Discuss.
16.35  Using the U.S. total computer and software sales data 
for Problem 16.15 on page 674 and Problem 16.27 on page 682 
(stored in  ComputerSales ),
a.	 perform a residual analysis for each model.
b.	 compute the standard error of the estimate 1SYX2 for each model.
c.	 compute the MAD for each model.
d.	 On the basis of (a) through (c) and the principle of parsimony, 
which forecasting model would you select? Discuss.
SELF 
Test 
16.36  Using the number of stores open for Bed  
Bath & Beyond from 1997 through 2013 data for  
Problem 16.12 on page 673 and Problem 16.28 on page 682 
(stored in  Bed & Bath ),
a.	 perform a residual analysis for each model.
b.	 compute the standard error of the estimate 1SYX2 for each model.
c.	 compute the MAD for each model.
d.	 On the basis of (a) through (c) and the principle of parsimony, 
which forecasting model would you select? Discuss.
16.37  Using the number of passenger cars produced in the 
U.S. from 1999 to 2012 data for Problem 16.17 on page 674 and 
­Problem 16.29 on page 682 (stored in  CarProduction ),
a.	 perform a residual analysis for each model.
b.	 compute the standard error of the estimate 1SYX2 for each 
model.
c.	 compute the MAD for each model.
d.	 On the basis of (a) through (c) and the principle of parsimony, 
which forecasting model would you select? Discuss.
16.38  Using the average baseball salary from 2000 through 
2013 data for Problem 16.18 on page 674 and Problem 16.30 on 
page 682 (stored in  BBSalaries ),
a.	 perform a residual analysis for each model.
b.	 compute the standard error of the estimate 1SYX2 for each 
model.
c.	 compute the MAD for each model.
d.	 On the basis of (a) through (c) and the principle of parsimony, 
which forecasting model would you select? Discuss.
16.39  Refer to the results for Problem 16.13 on page 674 that 
used the file  GDP ,
a.	 perform a residual analysis.
b.	 compute the standard error of the estimate 1SYX2.
c.	 compute the MAD.
d.	 On the basis of (a) through (c), are you satisfied with your 
­linear trend forecasts in Problem 16.13? Discuss.
16.7  Time-Series Forecasting of Seasonal Data
So far, this chapter has focused on forecasting annual data. However, many time series are col-
lected quarterly or monthly, and others are collected weekly, daily, and even hourly. When a 
time series is collected quarterly or monthly, you must consider the impact of seasonal effects. 
In this section, regression model building is used to forecast monthly or quarterly data.
One of the companies of interest in the Using Statistics scenario is Wal-Mart Stores, Inc. In 
2013, according to the company’s website, Wal-Mart Stores, Inc., operated more than 10,000 
retail units in 27 countries and had revenues that exceeded $400 billion. Walmart revenues are 
highly seasonal, and therefore you need to analyze quarterly revenues. The fiscal year for the 
company ends January 31. Thus, the fourth quarter of 2013 includes November and December 
2012 as well as January 2013. Table 16.3 lists the quarterly revenues (in $billions) from 2008 
to 2013 that are stored in  Walmart . Figure 16.19 displays the time series.
T a b l e  1 6 . 3
Quarterly Revenues  
(in $billions) for  
Wal-Mart Stores, Inc., 
2008–2013
Year
Quarter
2008
2009
2010
2011
2012
2013
1
  86.4
  95.3
  93.5
  99.1
103.4
113.4
2
  93.0
102.7
100.9
103.0
108.6
113.5
3
  91.9
  98.6
  99.4
101.2
109.5
113.2
4
107.3
107.9
113.7
115.6
122.3
127.1
Source: Data extracted from Wal-Mart Stores, Inc., walmartstores.com.

	
16.7  Time-Series Forecasting of Seasonal Data	
687
Least-Squares Forecasting with Monthly or Quarterly Data
To develop a least-squares regression model that includes a seasonal component, the least-
squares exponential trend fitting method used in Section 16.4 is combined with dummy vari-
ables (see Section 14.6) to model the seasonal component.
Equation (16.15) defines the exponential trend model for quarterly data.
F i g u r e  1 6 . 1 9
Plot of quarterly 
revenues ($billions) for 
Wal-Mart Stores, Inc., 
2008–2013
Exponential Model with Quarterly Data
	
Yi = b0bXi
1 bQ1
2 bQ2
3 bQ3
4 ei	
(16.15)
where
Xi = coded quarterly value, i = 0, 1, 2, c
Q1 = 1 if first quarter, 0 if not first quarter
Q2 = 1 if second quarter, 0 if not second quarter
Q3 = 1 if third quarter, 0 if not third quarter
b0 = Y intercept
1b1 - 12 * 100, = quarterly compound growth rate (in %)
b2 = multiplier for first quarter relative to fourth quarter
b3 = multiplier for second quarter relative to fourth quarter
b4 = multiplier for third quarter relative to fourth quarter
ei = value of the irregular component for time period i
The model in Equation (16.15) is not in the form of a linear regression model. To trans-
form this nonlinear model to a linear model, you use a base 10 logarithmic transformation.3 
Taking the logarithm of each side of Equation (16.15) results in Equation (16.16).
3Alternatively, you can use base e 
logarithms. For more information 
on logarithms, see Section A.3 in 
Appendix A.
Transformed Exponential Model with Quarterly Data
 log1Yi2 = log1b0bXi
1 bQ1
2 bQ2
3 bQ3
4 ei2
(16.16)
 = log1b02 + log1bXi
1 2 + log1bQ1
2 2 + log1bQ2
3 2 + log1bQ3
4 2 + log1ei2
 = log1b02 + Xi log1b12 + Q1 log1b22 + Q2 log1b32 + Q3 log1b42 + log1ei2
Equation (16.16) is a linear model that you can estimate using least-squares regression. 
Performing the regression analysis using log1Yi2 as the dependent variable and Xi, Q1, Q2, and 
Q3 as the independent variables results in Equation (16.17).

688	
Chapter 16  Time-Series Forecasting
Equation (16.18) is used for monthly data.
Exponential Growth with Quarterly Data Forecasting Equation
	
log1Yni2 = b0 + b1Xi + b2Q1 + b3Q2 + b4Q3	
(16.17)
where
b0 = estimate of log1b02 and thus 10b0 = bn0
b1 = estimate of log1b12 and thus 10b1 = bn1
b2 = estimate of log1b22 and thus 10b2 = bn2
b3 = estimate of log1b32 and thus 10b3 = bn3
b4 = estimate of log1b42 and thus 10b4 = bn4
Exponential Model with Monthly Data
	
Yi = b0bXi
1 bM1
2 bM2
3 bM3
4 bM4
5 bM5
6 bM6
7 bM7
8 bM8
9 bM9
10 bM10
11 bM11
12 ei	
(16.18)
where
Xi = coded monthly value, i = 0, 1, 2, c
M1 = 1 if January, 0 if not January
M2 = 1 if February, 0 if not February
M3 = 1 if March, 0 if not March
f
M11 = 1 if November, 0 if not November
b0 = Y intercept
1b1 - 12 * 100, = monthly compound growth rate (in %)
b2 = multiplier for January relative to December
b3 = multiplier for February relative to December
b4 = multiplier for March relative to December
f
b12 = multiplier for November relative to December
ei = value of the irregular component for time period i
Transformed Exponential Model with Monthly Data
	
 log1Yi2 = log 1b0bXi
1 bM1
2 bM2
3 bM3
4 bM4
5 bM5
6 bM6
7 bM7
8 bM8
9 bM9
10 bM10
11 bM11
12 ei2	
(16.19)
 = log1b02 + Xi log1b12 + M1 log1b22 + M2 log1b32
 + M3 log1b42 + M4 log1b52 + M5 log1b62 + M6 log1b72
 + M7 log1b82 + M8 log1b92 + M9 log1b102 + M10 log1b112
 + M11 log1b122 + log1ei2
The model in Equation (16.18) is not in the form of a linear regression model. To trans-
form this nonlinear model into a linear model, you can use a base 10 logarithm transformation. 
Taking the logarithm of each side of Equation (16.18) results in Equation (16.19).

	
16.7  Time-Series Forecasting of Seasonal Data	
689
Equation (16.19) is a linear model that you can estimate using the least-squares method. 
Performing the regression analysis using log1Yi2 as the dependent variable and Xi, M1, M2, c, 
and M11 as the independent variables results in Equation (16.20).
Exponential Growth with Monthly Data Forecasting Equation
 log1Yni2 = b0 + b1Xi + b2M1 + b3M2 + b4M3 + b5M4 + b6M5 + b7M6
	
 + b8M7 + b9M8 + b10M9 + b11M10 + b12M11
	 (16.20)
where
b0 = estimate of log1b02 and thus 10b0 = bn0
b1 = estimate of log1b12 and thus 10b1 = bn1
b2 = estimate of log1b22 and thus 10b2 = bn2
b3 = estimate of log1b32 and thus 10b3 = bn3
f
b12 = estimate of log1b122 and thus 10b12 = bn12
Q1, Q2, and Q3 are the three dummy variables needed to represent the four quarter periods 
in a quarterly time series. M1, M2, M3, c, M11 are the 11 dummy variables needed to repre-
sent the 12 months in a monthly time series. In building the model, you use log1Yi2 instead of 
Yi values and then find the regression coefficients by taking the antilog of the regression coef-
ficients developed from Equations (16.17) and (16.20).
Although at first glance these regression models look imposing, when fitting or forecasting 
for any one time period, the values of all or all but one of the dummy variables in the model are 
equal to zero, and the equations simplify dramatically. In establishing the dummy variables for 
quarterly time-series data, the fourth quarter is the base period and has a coded value of zero 
for each dummy variable. With a quarterly time series, Equation (16.17) reduces as follows:
For any first quarter: log1Yni2 = b0 + b1Xi + b2
For any second quarter: log1Yni2 = b0 + b1Xi + b3
For any third quarter: log1Yni2 = b0 + b1Xi + b4
For any fourth quarter: log1Yni2 = b0 + b1Xi
When establishing the dummy variables for each month, December serves as the base pe-
riod and has a coded value of 0 for each dummy variable. For exa.mple, with a monthly time 
series, Equation (16.20) reduces as follows:
For any January: log1Yni2 = b0 + b1Xi + b2
For any February: log1Yni2 = b0 +  b1Xi + b3
f
For any November: log1Yni2 = b0 + b1Xi + b12
For any December: log1Yni2 = b0 + b1Xi
To demonstrate the process of model building and least-squares forecasting with a quar-
terly time series, return to the Wal-Mart Stores, Inc., revenue data (in billions of dollars) origi-
nally displayed in Table 16.3 on page 686. The data are from the first quarter of 2008 through 
the last quarter of 2013. Figure 16.20 shows the Excel and Minitab regression results for the 
quarterly exponential trend model.
From Figure 16.20, the model fits the data very well. The coefficient of determination 
r2 = 0.9461, the adjusted r2 = 0.9348, and the overall F test results in an FSTAT test statistic 
of 83.391 (p-value = 0.000). At the 0.05 level of significance, each regression coefficient is 

690	
Chapter 16  Time-Series Forecasting
The interpretations for bn0, bn1, bn2, bn3, and bn4 are as follows:
 • The Y intercept, bn0 = 101.6951 (in $billions), is the unadjusted forecast for quarterly 
revenues in the first quarter of 2008, the initial quarter in the time series. Unadjusted 
means that the seasonal component is not incorporated in the forecast.
 • The value 1bn1 - 12 * 100, = 0.0097, or 0.97,, is the estimated quarterly compound 
growth rate in revenues, after adjusting for the seasonal component.
 • bn2 = 0.8756 is the seasonal multiplier for the first quarter relative to the fourth quarter; 
it indicates that there is 1 - 0.8756 = 12.44, less revenue for the first quarter than for 
the fourth quarter.
 • bn3 = 0.9137 is the seasonal multiplier for the second quarter relative to the fourth quarter; 
it indicates that there is 1 - 0.9137 = 8.63, less revenue for the second quarter than for 
the fourth quarter.
 • bn4 = 0.8929 is the seasonal multiplier for the third quarter relative to the fourth quarter; 
it indicates that there is 1 - 0.8929 = 10.71, less revenue for the third quarter than for 
the fourth quarter. Thus, the fourth quarter, which includes the holiday shopping season, 
has the strongest sales.
Using the regression coefficients b0, b1, b2, b3, and b4, and Equation (16.17) on page 688, 
you can make forecasts for selected quarters. As an example, to predict revenues for the fourth 
quarter of 2013 1Xi = 232,
 log1Yni2 = b0 + b1Xi
 = 2.0073 + 10.004221232
 = 2.1039
Thus,
log1Yni2 = 102.1039 = 127.0282
F i g u r e  1 6 . 2 0
Excel and Minitab regression results for the quarterly revenue data for Wal-Mart Stores, Inc.
Regression Coefficient
bi = log Bni
Bni = antilog 1bi2 = 10bi
b0: Y intercept
   2.0073
101.6951
b1: coded quarter
   0.0042
  1.0097
b2: first quarter
-0.0577
  0.8756
b3: second quarter
-0.0392
  0.9137
b4: third quarter
-0.0492
  0.8929
highly statistically significant and contributes to the model. The following summary includes 
the antilogs of all the regression coefficients:

	
16.7  Time-Series Forecasting of Seasonal Data	
691
The predicted revenue for the fourth quarter of fiscal 2013 is $127.0282 billion. To make a 
forecast for a future time period, such as the first quarter of fiscal 2014 1Xi = 24, Q1 = 12,
 log1Yni2 = b0 + b1Xi + b2Q1
 = 2.0073 + 10.004221242 + 1-0.05772112
 = 2.0504
Thus,
Yni = 102.0504 = 112.3052
The predicted revenue for the first quarter of fiscal 2014 is $112.3052 billion.
Problems for Section 16.7
Learning the Basics
16.40  In forecasting a monthly time series over a five-year pe-
riod from January 2009 to December 2013, the exponential trend 
forecasting equation for January is
log Yni = 2.0 + 0.01Xi + 0.10 1January2
Take the antilog of the appropriate coefficient from this equation 
and interpret the
a.	 Y intercept, bn0.
b.	 monthly compound growth rate.
c.	 January multiplier.
16.41  In forecasting daily time-series data, how many dummy vari-
ables are needed to account for the seasonal component day of the week?
16.42  In forecasting a quarterly time series over the five-year pe-
riod from the first quarter of 2009 through the fourth quarter of 
2013, the exponential trend forecasting equation is given by
log Yni = 3.0 + 0.10Xi - 0.25Q1 + 0.20Q2 + 0.15Q3
where quarter zero is the first quarter of 2009. Take the antilog of 
the appropriate coefficient from this equation and interpret the
a.	 Y intercept, bn0.
b.	 quarterly compound growth rate.
c.	 second-quarter multiplier.
16.43  Refer to the exponential model given in Problem 16.42.
a.	 What is the fitted value of the series in the fourth quarter of 2011?
b.	 What is the fitted value of the series in the first quarter of 2011?
c.	 What is the forecast in the fourth quarter of 2013?
d.	 What is the forecast in the first quarter of 2014?
Applying the Concepts
SELF 
Test 
16.44  The data in  Toys R Us  are quarterly revenues 
(in $millions) for Toys R Us from 1996-Q1 through 
2013-Q1. (Data extracted from Standard & Poor’s Stock Reports, 
November 1995, November 1998, and April 2002, and Toys R Us, 
Inc., www.toysrus.com.)
a.	 Do you think that the revenues for Toys R Us are subject to 
seasonal variation? Explain.
b.	 Plot the data. Does this chart support your answer in (a)?
c.	 Develop an exponential trend forecasting equation with quar-
terly components.
d.	 Interpret the quarterly compound growth rate.
e.	 Interpret the quarterly multipliers.
f.	 What are the forecasts for 2013-Q2, 2013-Q3, 2013-Q4, and all 
four quarters of 2014?
16.45  Are gasoline prices higher during the height of the sum-
mer vacation season than at other times? The file  GasPrices  con-
tains the mean monthly prices (in $/gallon) for unleaded gasoline 
in the United States from January 2006 to May 2013. (Data ex-
tracted from U.S. Energy Information Administration, www.eia 
.gov/totalenergy/data/monthly/pdf/sec9_6.pdf.)
a.	 Construct a time-series plot.
b.	 Develop an exponential trend forecasting equation with 
monthly components.
c.	 Interpret the monthly compound growth rate.
d.	 Interpret the monthly multipliers.
e.	 Write a short summary of your findings.
16.46  The data in  Travel  show the average traffic on Google 
recorded at the beginning of each month from January 2004 to 
­August 2012 for searches from the United States concerning travel 
(scaled to the average traffic for the entire time period based on a 
fixed point at the beginning of the time period). (Data retrieved 
from Google Trends, www.google.com/trends, August 13, 2012.)
a.	 Plot the time-series data.
b.	 Develop an exponential trend forecasting equation with 
monthly components.
c.	 What is the fitted value in August 2012?
d.	 What are the forecasts for the last four months of 2012?
e.	 Interpret the monthly compound growth rate.
f.	 Interpret the July multiplier.
16.47  The file  CallCenter  contains the monthly call volume for 
an existing product. (Data extracted from S. Madadevan and J. 
Overstreet, “Use of Warranty and Reliability Data to Inform Call 
Center Staffing,” Quality Engineering 24 (2012): 386–399.)
a.	 Construct the time-series plot.
b.	 Describe the monthly pattern in the data.
c.	 In general, would you say that the overall call volume is in-
creasing or decreasing? Explain.
d.	 Develop an exponential trend forecasting equation with 
monthly components.
e.	 Interpret the monthly compound growth rate.
f.	 Interpret the January multiplier.
g.	 What is the predicted call volume for month 60?
h.	 What is the predicted call volume for month 61?
i.	 How can this type of time-series forecasting benefit the call center?

692	
Chapter 16  Time-Series Forecasting
16.48  The file  Silver-Q  contains the price in London for an 
ounce of silver (in US$) at the end of each quarter from 2004 
through 2012. (Data extracted from bit.ly/1afifi.)
a.	 Plot the data.
b.	 Develop an exponential trend forecasting equation with quar-
terly components.
c.	 Interpret the quarterly compound growth rate.
d.	 Interpret the first quarter multiplier.
e.	 What is the fitted value for the last quarter of 2012?
f.	 What are the forecasts for all four quarters of 2013?
g.	 Are the forecasts in (f) accurate? Explain.
16.49  The file  Gold  contains the price in London for an ounce 
of gold (in US$) at the end of each quarter from 2004 through 
2012. (Data extracted from bit.ly/1afifi.)
a.	 Plot the data.
b.	 Develop an exponential trend forecasting equation with quar-
terly components.
c.	 Interpret the quarterly compound growth rate.
d.	 Interpret the first quarter multiplier.
e.	 What is the fitted value for the last quarter of 2012?
f.	 What are the forecasts for all four quarters of 2013?
g.	 Are the forecasts in (f) accurate? Explain.
16.8  Index Numbers
An index number measures the value of an item (or group of items) at a particular point in  
time as a percentage of the value of an item (or group of items) at another point in time. The 
Section 16.8 online topic discusses this concept and illustrates its application.
T h i n k  A b o u t  T h i s  Let the Model User Beware
When using a model, you must always review the 
assumptions built into the model and think about 
how novel or changing circumstances may render 
the model less useful.
Implicit in the time-series models developed 
in this chapter is that past data can be used to help 
predict the future. While using past data in this way 
is a legitimate application of time-series models, 
every so often, a crisis in financial markets illus-
trates that using models that rely on the past to 
predict the future is not without risk.
For example, during August 2007, many 
hedge funds suffered unprecedented losses. 
­Apparently, many hedge fund managers used mod-
els that based their investment strategy on trading 
patterns over long time periods. These models did 
not—and could not—reflect trading patterns con-
trary to historical patterns (G. Morgenson, “A Week 
When Risk Came Home to Roost,” The New York 
Times, August 12, 2007, pp. B1, B7). When fund 
managers in early August 2007 needed to sell 
stocks due to losses in their fixed income portfo-
lios, stocks that were previously stronger became 
weaker, and weaker ones became stronger—the 
reverse of what the models expected. Making mat-
ters worse, many fund managers were using simi-
lar models and rigidly made investment decisions 
solely based on what those models said. These 
similar actions multiplied the effect of the selling 
pressure, an effect that the models had not con-
sidered and that therefore could not be seen in the 
models’ results.
This example illustrates that using models 
does not absolve you of the responsibility of be-
ing a thoughtful decision maker. Go ahead and 
use models—when appropriately used, they will 
enhance your decision making. But always remem-
ber that no model can completely remove the risk 
involved in making a business decision.
I
n the Using Statistics scenario, you were the financial ana-
lyst for The Principled, a large financial services company. 
You needed to forecast movie attendance, revenues for Coca-
Cola, and for Walmart to better evaluate investment opportu-
nities for your clients.
For movie attendance, you used moving averages  
and exponential smoothing methods to develop forecasts. 
You predicted that the movie attendance in 2013 would be 
1.37 billion.
For The Coca-Cola Company, you used least-squares lin-
ear, quadratic, and exponential models and autoregressive mod-
els to develop forecasts. You evaluated these alternative models 
and determined that the third-order autoregressive model gave 
the best forecast, according to several criteria. You predicted 
that the rev-
enue of The 
C o c a - C o l a 
Company would be $47.2722 billion in 2013 and $66.19 
­billion in 2014.
For Wal-Mart Stores, Inc., you used a least-squares 
regression model with seasonal components to develop 
forecasts. You predicted that Wal-Mart Stores would have 
revenues of $112.3052 billion in the first quarter of fiscal 
2014.
Given these forecasts, you now need to determine 
whether your clients should invest, and if so, how much they 
should invest in the movie industry or in The Coca-Cola 
Company or in Wal-Mart Stores, Inc.
U s i n g  S tat i s t i c s
Principled Forecasting, Revisited

	
References	
693
S u m m a r y
In this chapter, you studied smoothing techniques, least-
squares trend fitting, autoregressive models, and forecasting of 
seasonal data. Figure 16.21 provides a summary chart for the 
time-series methods discussed in this chapter.
When using time-series forecasting, you need to plot 
the time series and answer the following question: Is there 
a trend in the data? If there is a trend, then you can use the 
autoregressive model or the linear, quadratic, or exponential 
trend models. If there is no obvious trend in the time-series 
plot, then you should use moving averages or exponential 
smoothing to smooth out the effect of random effects and 
possible cyclical effects. After smoothing the data, if a trend 
is still not present, then you can use exponential smoothing 
to forecast short-term future values. If smoothing the data 
reveals a trend, then you can use the autoregressive model, 
or the linear, quadratic, or exponential trend models.
F i g u r e  1 6 . 2 1
Summary chart of 
time-series forecasting 
methods
Yes
No
Yes
No
Exponential
Smoothing
Moving
Averages
Annual
Data?
Least-Squares Forecasting with
Transformed Exponential Model
Forecasting
Models
Linear
Trend
Quadratic
Trend
Autoregressive
Models
Exponential
Trend
Model
Selection
Time-Series
Forecasting
Trend
?
Refere n c e s
	 1.	Bowerman, B. L., R. T. O’Connell, and A. Koehler. 
­Forecasting, Time Series, and Regression, 4th ed. Belmont, CA:  
Duxbury Press, 2005.
	 2.	Box, G. E. P., G. M. Jenkins, and G. C. Reinsel. Time Series 
Analysis: Forecasting and Control, 3rd ed. Upper Saddle 
River, NJ: Prentice Hall, 1994.
	 3.	Frees, E. W. Data Analysis Using Regression Models: The 
­Business Perspective. Upper Saddle River, NJ: Prentice Hall, 
1996.
	 4.	Hanke, J. E., D. W. Wichern, and A. G. Reitsch. Business 
­Forecasting, 7th ed. Upper Saddle River, NJ: Prentice Hall, 
2001.
	 5.	Microsoft Excel 2013. Redmond, WA: Microsoft Corp., 2012.
	 6.	Minitab Release 16. State College, PA: Minitab Inc., 2010.

694	
Chapter 16  Time-Series Forecasting
K e y  Eq u at i o n s
Computing an Exponentially Smoothed Value in Time 
Period i
E1 = Y1
Ei = WYi + 11 - W2Ei - 1  i = 2, 3, 4, c 	
(16.1)
Forecasting Time Period i + 1
Yni+1 = Ei	
(16.2)
Linear Trend Forecasting Equation
Yni = b0 + b1Xi	
(16.3)
Quadratic Trend Forecasting Equation
Yni = b0 + b1Xi + b2X2
i 	
(16.4)
Exponential Trend Model
Yi = b0bXi
1 ei	
(16.5)
Transformed Exponential Trend Model
 log1Yi2 = log1b0bXi
1 ei2
= log1b02 + log1bXi
1 2 + log1ei2
= log1b02 + Xilog1b12 + log1ei2	
(16.6)
Exponential Trend Forecasting Equation
log1Yni2 = b0 + b1Xi	
(16.7a)
Yni = bn0 bnXi
1 	
(16.7b)
pth-Order Autoregressive Models
Yi = A0 + A1Yi - 1 + A2Yi - 2 + g + ApYi - p + di	(16.8)
First-Order Autoregressive Model
Yi = A0 + A1Yi-1 + di	
(16.9)
Second-Order Autoregressive Model
Yi = A0 + A1Yi-1 + A2Yi-2 +  di	
(16.10)
t Test for Significance of the Highest-Order Autoregressive 
Parameter, Ap
tSTAT =
ap - Ap
Sap
	
(16.11)
Fitted pth-Order Autoregressive Equation
Yni = a0 + a1Yi - 1 + a2Yi - 2 + g + apYi - p	
(16.12)
pth-Order Autoregressive Forecasting Equation
Ynn + j = a0 + a1Ynn + j - 1 + a2Ynn + j - 2 + g+ apYnn + j - p  

(16.13)
Mean Absolute Deviation
MAD =
a
n
i = 1
Yi - Yni 
n

(16.14)
Exponential Model with Quarterly Data
Yi = b0bXi
1 bQ1
2 bQ2
3 bQ3
4 ei
(16.15)
Transformed Exponential Model with Quarterly Data
log1Yi2 = log1b0bXi
1 bQ1
2 bQ2
3 bQ3
4 ei2
(16.16)
= log1b02 + log1bXi
1 2 + log1bQ1
2 2 + log1bQ2
3 2
+ log1bQ3
4 2 + log1ei2
= log1b02 + Xilog1b12 + Q1 log1b22
+ Q2 log1b32 + Q3 log1b42 + log1ei2
Exponential Growth with Quarterly Data Forecasting 
Equation
log1Yni2 = b0 + b1Xi + b2Q1 + b3Q2 + b4Q3
(16.17)
Exponential Model with Monthly Data
Yi = b0bXi
1 bM1
2 bM2
3 bM3
4 bM4
5 bM5
6 bM6
7 bM7
8 bM8
9 bM9
10 bM10
11 bM11
12 ei 

(16.18)
Transformed Exponential Model with Monthly Data
log1Yi2 = log1b0bXi
1 bM1
2 bM2
3 bM3
4 bM4
5 bM5
6 bM6
7 bM7
8 bM8
9 bM9
10 bM10
11 bM11
12 ei2
= log1b02 + Xi log1b12 + M1 log1b22 + M2 log1b32
+ M3 log1b42 + M4 log1b52 + M5 log1b62 + M6 log1b72
+ M7 log1b82 + M8 log1b92 + M9 log1b102
+ M10 log1b112 + M11 log1b122 + log1ei2
(16.19)
Exponential Growth with Monthly Data  
Forecasting Equation
log1Yni2 = b0 + b1Xi + b2M1 + b3M2 + b4M3 + b5M4 + b6M5
+ b7M6 + b8M7 + b9M8 + b10M9 + b11M10 + b12M11
(16.20)
K e y  Term s
autoregressive modeling  675
causal forecasting methods  658
cyclical effect  659
exponential smoothing  662
exponential trend model  668
first-order autocorrelation  675
first-order autoregressive model  676
forecasting  658
irregular effect  659
lagged predictor variable  675
linear trend model  665
mean absolute deviation  
(MAD)  684
moving averages  660
parsimony  684
pth-order autocorrelation  675
pth-order autoregressive model  675
quadratic trend model  667
qualitative forecasting method  658
quantitative forecasting method  658
random effect  659
seasonal effect  659
second-order autocorrelation  675
second-order autoregressive model  676
time series  658
time-series forecasting methods  658
trend  659

	
Chapter Review Problems	
695
C h ec ki n g  Yo ur  U n de r s ta nding
16.50  What are the components of a time-series model? When 
does an annual time series require smoothing? Explain the moving 
average method of smoothing time series data and forecasting.
16.51  Answer the following:
a.	 How does standard error of estimate help in selecting an appro-
priate time series model?
b.	 What is the main demerit of standard errors of estimate that lead to 
the introduction of mean absolute deviation?
16.52  What is the importance of business forecasting?
16.53  Explain the difference between qualitative and quantitative 
forecasting methods.
16.54  What is the difference between time series and causal 
forecasting methods?
16.55  What are the components of a time-series model? How 
can you determine which pattern exists over time and which model 
you should use?
16.56  How does the exponential smoothing method of time se-
ries differ from the moving average method?
16.57  How are first, second and percentage differences used to 
determine a model?
16.58  What is pth-order autocorrelation? How does it help in for-
mulating an autoregressive model of trend fitting and forecasting?
C h a pte r  R e vi e w P r ob le ms
16.59  The data in the following table, stored in  Polio  , represent 
the annual incidence rates (per 100,000 persons) of reported acute 
poliomyelitis recorded over five-year periods from 1915 to 1955:
c.	 compute an exponential trend forecasting equation for monthly 
data.
d.	 interpret the monthly compound growth rate.
e.	 interpret the monthly multipliers. Do the multipliers support 
your answers in (a) and (b)?
f.	 compare the results for the wellhead prices and the residential prices.
16.62  The data in the following table, stored in  McDonalds , 
represent the gross revenues (in billions of current dollars) of 
­McDonald’s Corporation from 1975 through 2012:
Year 1915 1920 1925 1930 1935 1940 1945 1950 1955
Rate
3.1
2.2
5.3
7.5
8.5
7.4
10.3
22.1
17.6
Source: Data extracted from B. Wattenberg, ed., The Statistical History of 
the United States: From Colonial Times to the Present, ser. B303.
a.	 Plot the data.
b.	 Compute the linear trend forecasting equation and plot the 
trend line.
c.	 What are your forecasts for 1960, 1965, and 1970?
d.	 Using a library or the Internet, find the actually reported inci-
dence rates of acute poliomyelitis for 1960, 1965, and 1970. 
Record your results.
e.	 Why are the forecasts you made in (c) not useful? Discuss.
16.60  The U.S. Department of Labor gathers and publishes sta-
tistics concerning the labor market. The file  Workforce  contains 
data on the size of the U.S. civilian noninstitutional population 
of people 16 years and over (in thousands) and the U.S. civilian 
noninstitutional workforce of people 16 years and over (in thou-
sands) for 1984–2012. The workforce variable reports the number 
of people in the population who have a job or are actively looking 
for a job. (Data extracted from Bureau of Labor Statistics, U.S. 
Department of Labor, www.bls.gov.)
a.	 Plot the time series for the U.S. civilian noninstitutional popu-
lation of people 16 years and older.
b.	 Compute the linear trend forecasting equation.
c.	 Forecast the U.S. civilian noninstitutional population of people 
16 years and older for 2013 and 2014.
d.	 Repeat (a) through (c) for the U.S. civilian noninstitutional 
workforce of people 16 years and older.
16.61  The monthly wellhead and residential prices for natural gas 
(dollars per thousand cubic feet) in the United States from January 
2008 through December 2012 are stored in  Natural Gas2 . (Data 
­extracted from Energy Information Administration, U.S. ­Department  
of Energy, www.eia.gov, Natural Gas Monthly, May 31, 2013.)
For the wellhead price and the residential price,
a.	 do you think the price for natural gas has a seasonal component?
b.	 plot the time series. Does this chart support your answer in (a)?
a.	 Plot the data.
b.	 Compute the linear trend forecasting equation.
c.	 Compute the quadratic trend forecasting equation.
d.	 Compute the exponential trend forecasting equation.
e.	 Determine the best-fitting autoregressive model, using a = 0.05.
f.	 Perform a residual analysis for each of the models in (b) 
through (e).
g.	 Compute the standard error of the estimate 1SYX2 and the MAD 
for each corresponding model in (f).
h.	 On the basis of your results in (f) and (g), along with a consid-
eration of the principle of parsimony, which model would you 
select for purposes of forecasting? Discuss.
i.	 Using the selected model in (h), forecast gross revenues for 2013.
Year
Revenues 
($billions)
Year
Revenues 
($billions)
Year
Revenues 
($billions)
1975
1.0
1988
  5.6
2001
14.8
1976
1.2
1989
  6.1
2002
15.2
1977
1.4
1990
  6.8
2003
16.8
1978
1.7
1991
  6.7
2004
18.6
1979
1.9
1992
  7.1
2005
19.8
1980
2.2
1993
  7.4
2006
20.9
1981
2.5
1994
  8.3
2007
22.8
1982
2.8
1995
  9.8
2008
23.5
1983
3.1
1996
10.7
2009
22.7
1984
3.4
1997
11.4
2010
24.1
1985
3.8
1998
12.4
2011
27.0
1986
4.2
1999
13.3
2012
27.6
1987
4.9
2000
14.2
Source: Data extracted from Moody’s Handbook of Common Stocks, 
1980, 1989, and 1999; Mergent’s Handbook of Common Stocks, Spring 
2002; and “Investors: About McDonalds,” www.mcdonalds.com.

696	
Chapter 16  Time-Series Forecasting
16.63  Teachers’ Retirement System of the City of New York of-
fers several types of investments for its members. Among the 
choices are investments with fixed and variable rates of return. 
There are several categories of variable-return investments. The 
Diversified Equity Fund consists of investments that are primarily 
made in stocks, and the Stable-Value Fund consists of investments 
in corporate bonds and other types of lower-risk instruments. The 
data in  TRSNYC  represent the value of a unit of each type of vari-
able-return investment at the beginning of each year from 1984 to 
2013. (Data extracted from “Historical Data-Unit Values, Teachers’ 
Retirement System of the City of New York,” bit.ly/SESJF5.)
For each of the two time series,
a.	 plot the data.
b.	 compute the linear trend forecasting equation.
c.	 compute the quadratic trend forecasting equation.
d.	 compute the exponential trend forecasting equation.
e.	 determine the best-fitting autoregressive model, using a = 0.05.
f.	 Perform a residual analysis for each of the models in (b) 
through (e).
g.	 Compute the standard error of the estimate 1SYX2 and the MAD 
for each corresponding model in (f).
h.	 On the basis of your results in (f) and (g), along with a consid-
eration of the principle of parsimony, which model would you 
select for purposes of forecasting? Discuss.
i.	 Using the selected model in (h), forecast the unit values for 2014.
j.	 Based on the results of (a) through (i), what investment strategy 
would you recommend for a member of the Teachers’ Retire-
ment System of the City of New York? Explain.
Report Writing Exercise
16.64  As a consultant to an investment company trading in 
various currencies, you have been assigned the task of studying 
long-term trends in the exchange rates of the Canadian dollar, the 
Japanese yen, and the English pound. Data from 1980 to 2012 are 
stored in  Currency , where the Canadian dollar, the Japanese yen, 
and the English pound are expressed in units per U.S. dollar.
Develop a forecasting model for the exchange rate of each of 
these three currencies and provide forecasts for 2013 and 2014 for 
each currency. Write an executive summary for a presentation to be 
given to the investment company. Append to this executive summary 
a discussion regarding possible limitations that may exist in these 
models.
C a s e s  f o r  C h a p t e r  1 6
Managing Ashland MultiComm Services
As part of the continuing strategic initiative to increase sub-
scribers to the 3-For-All cable/phone/Internet services, the 
marketing department is closely monitoring the number of 
subscribers. To help do so, forecasts are to be developed for 
the number of subscribers in the future. To accomplish this 
task, the number of subscribers for the most recent 24-month 
period has been determined and is stored in  AMS16 .
1.	Analyze these data and develop a model to forecast the 
number of subscribers. Present your findings in a report 
that includes the assumptions of the model and its limita-
tions. Forecast the number of subscribers for the next four 
months.
2.	Would you be willing to use the model developed to fore-
cast the number of subscribers one year into the future? 
Explain.
3.	Compare the trend in the number of subscribers to the 
number of new subscribers per month stored in  AMS13  . 
What explanation can you provide for any differences?
Digital Case
Apply your knowledge about time-series forecasting in this 
Digital Case.
The Ashland Herald competes for readers in the Tri-Cities 
area with the newer Oxford Glen Journal (OGJ). Recently, 
the circulation staff at the OGJ claimed that their news-
paper’s circulation and subscription base is growing faster 
than that of the Herald and that local advertisers would 
do better if they transferred their advertisements from 
the Herald to the OGJ. The circulation department of the  
Herald has complained to the Ashland Chamber of 
­Commerce about OGJ’s claims and has asked the cham-
ber to investigate, a request that was welcomed by OGJ’s 
circulation staff.
Open ACC_Mediation216.pdf to review the circula-
tion dispute information collected by the Ashland Chamber 
of Commerce. Then answer the following:
1.	Which newspaper would you say has the right to claim 
the fastest-growing circulation and subscription base? 
Support your answer by performing and summarizing an 
appropriate statistical analysis.
2.	What is the single most positive fact about the Herald’s 
circulation and subscription base? What is the single most 
positive fact about the OGJ’s circulation and subscription 
base? Explain your answers.
3.	 What additional data would be helpful in investigating the 
circulation claims made by the staffs of each newspaper?

	
Chapter 16 Excel Guide	
697
EG16.1  The Importance of Business 
Forecasting
There are no Excel Guide instructions for this section.
EG16.2  Component Factors of  
Time-Series Models
There are no Excel Guide instructions for this section.
EG16.3  Smoothing an Annual  
Time Series
Moving Averages
Key Technique  Use the AVERAGE(cell range that contains a 
sequence of L observed values) function to compute moving aver-
ages and use the special worksheet value #N/A (not available) for 
time periods in which no moving average can be computed.
Example  Compute the three- and five-year moving averages  
for the movie attendance data that is shown in Figure 16.3 on  
page 662.
In-Depth Excel  Use the COMPUTE worksheet of the  
Moving Averages workbook as a template.
The worksheet already contains the data and formulas for the ex-
ample. For other problems, paste the time-series data into columns 
A and B and adjust the moving average entries in columns C and 
D. (Open to the COMPUTE_FORMULAS worksheet to exam-
ine all formulas the worksheet uses.)
To construct a moving average plot for other problems, open 
to the adjusted COMPUTE worksheet and:
	 1.	 Select the cell range of the time-series data and the moving 
averages. (For the example, this cell range is A1:D12.)
	 2.	 Select Insert ➔ Scatter (X, Y) or Bubble Chart (in older 
Excels Scatter) and select the Scatter gallery choice named 
Scatter with Straight Lines and Markers.
	 3.	 Relocate the chart to a chart sheet, turn off the gridlines, add 
axis titles, and modify the chart title by using the instructions 
in Appendix Section B.6.
Exponential Smoothing
Key Technique  Use arithmetic formulas to compute exponen-
tially smoothed values.
Example  Compute the exponentially smoothed series 
1W = 0.50 and W = 0.252 for the movie attendance data that is 
shown in Figure 16.4 on page 663.
In-Depth Excel  Use the COMPUTE worksheet of the 
­Exponential Smoothing workbook, as a template.
The worksheet already contains the data and formulas for 
the example. In this worksheet, cells C2 and D2 contain the 
formula =B2 that copies the initial value of the time se-
ries. The exponential smoothing begins in row 3, with cell 
C3 formula =0.5 * B3 + 0.5 * C2, and cell D3 formula 
=0.25 * B3 + 0.75 * D2. Note that in these formulas, the expres-
sion 1 - W in Equation (16.1) on page 662 has been simplified to 
the values 0.5 and 0.75, respectively. (Open to the COMPUTE_
FORMULAS worksheet to examine all of the exponential 
smoothing formulas the worksheet uses.)
For other problems, paste the time-series data into columns A 
and B and adjust the exponentially smoothed entries in columns C 
and D. For problems with fewer than 11 time periods, delete the 
excess rows. For problems with more than 11 time periods, se-
lect row 12, right-click, and click Insert in the shortcut menu. Re-
peat as many times as there are new rows. Then select cell range 
C11:D11 and copy the contents of this range down through the 
new table rows.
To construct a plot of exponentially smoothed values for other 
problems, open to the adjusted COMPUTE worksheet and:
	 1.	 Select the cell range of the time-series data and the exponen-
tially smoothed values. (For the example, this cell range is 
A1:D12.)
	 2.	 Select Insert ➔ Scatter (X, Y) or Bubble Chart (in older 
Excels Scatter) and select the Scatter gallery choice named 
Scatter with Straight Lines and Markers.
	 3.	 Relocate the chart to a chart sheet, turn off the gridlines, add 
axis titles, and modify the chart title by using the instructions 
in Appendix Section B.6.
Analysis ToolPak  Use Exponential Smoothing.
For the example, open to the DATA worksheet of the Movie  
Attendance workbook and:
	 1.	 Select Data ➔ Data Analysis.
	 2.	 In the Data Analysis dialog box, select Exponential Smoothing 
from the Analysis Tools list and then click OK.
In the Exponential Smoothing dialog box (shown below):
	 1.	 Enter B1:B12 as the Input Range.
	 2.	 Enter 0.5 as the Damping factor. (The damping factor is 
equal to 1 - W.)
	 3.	 Check Labels, enter C1 as the Output Range, and click OK.
C h a p t e r  1 6  E x c e l  G u i d e

698	
Chapter 16  Time-Series Forecasting
In the new column C:
	 1.	 Copy the last formula in cell C11 to cell C12.
	 2.	 Enter the column heading ES1W = .502 in cell C1, replac-
ing the #N/A value.
To create the exponentially smoothed values that use a 
smoothing coefficient of W = 0.25, repeat steps 3 through 7 with 
these modifications: Enter 0.75 as the Damping factor in step 4, 
enter D1 as the Output Range in step 5, and enter ES1W = .252 
as the column heading in step 7.
EG16.4  Least-Squares Trend Fitting 
and Forecasting
The Linear Trend Model
Modify the Section EG13.2 instructions (see page 666) to create 
a linear trend model. Use the cell range of the coded variable as 
the X variable cell range (called the X Variable Cell Range in 
the PHStat instructions, called the cell range of X variable in the 
In-Depth Excel instructions, and called the Input X Range in the 
Analysis ToolPak instructions). If you need to create coded values, 
enter them manually in a column. (If you have many coded values, 
you can use Home ➔ Fill (in the Editing group) ➔ Series and in 
the Series dialog box, click Columns and Linear, and select ap-
propriate values for Step value and Stop value.
The Quadratic Trend Model
Modify the Section EG15.1 instructions (see page 653) to create a 
quadratic trend model. Use the cell range of the coded variable and 
the squared coded variable as the X variables cell range (called the 
X Variables Cell Range in the PHStat instructions and the Input 
X Range in the Analysis ToolPak instructions). Use the Sections 
EG15.2 and EG15.1 instructions to create the squared coded vari-
able and to plot the quadratic trend.
The Exponential Trend Model
Key Technique  Use the POWER(10, predicted log(Y)) func-
tion to compute the predicted Y values from the predicted log(Y) 
results.
To create an exponential trend model, first convert the  
values of the dependent variable Y to log(Y) values using the 
­Section EG15.2 instructions on page 653. Then perform a simple 
linear regression analysis with residual analysis using the log(Y) 
values. Modify the Section EG13.5 “Residual Analysis” instruc-
tions using the cell range of the log(Y) values as the Y variable cell 
range and the cell range of the coded variable as the X variable 
cell range. (Note that the residual analysis instructions incorporate 
the Section EG13.2 “Determining the Simple Linear Regression 
Equation” instructions.)
Note the Y and X variable cell ranges are called the Y Vari-
able Cell Range and X Variable Cell Range in the PHStat 
instructions, the cell range of Y variable and cell range of X vari-
able in the In-Depth Excel instructions, and the Input Y Range 
and Input X Range in the Analysis ToolPak instructions.
If you use the PHStat or In-Depth Excel instructions, residuals  
will appear in a residuals worksheet. If you use the Analysis 
ToolPak instructions, residuals will appear in the RESIDUAL 
OUTPUT area of the regression results worksheet. Because you 
use log(Y) values for the regression, the predicted Y and residuals 
listed are log values that need to be converted. [The Analysis Tool-
Pak incorrectly labels the new column for the logs of the residuals 
as Residuals, and not as LOG(Residuals), as you might expect.]
Convert the predicted log(Y) results to predicted Y results us-
ing the POWER function. Use an empty column in the residuals 
worksheet (PHStat or In-Depth Excel) or empty column ranges 
to the right of RESIDUALS OUTPUT area (Analysis ToolPak) to 
first add a column of formulas that use the POWER function to 
compute the predicted Y values. Then, add a second column that 
contains the original Y values. (Copy the original Y values to this 
column.) Finally, add a third new column that contains formulas in 
the form = 1revenue cell −predicted revenue cell2  to compute 
the actual residuals.
Use columns G through I of the RESIDUALS worksheet 
of the Exponential Trend workbook as a model. (Use the 
­Exponential Trend 2007 workbook if you use an Excel version 
that is older than Excel 2010.) The worksheet already contains the 
values and formulas needed to create the Figure 16.10 plot that 
fits an exponential trend forecasting equation for The Coca-Cola 
Company revenues (see page 670).
To construct an exponential trend plot, first select the cell 
range of the time-series data and then use the Section EG2.5 in-
structions to construct a scatter plot. (For The Coca-Cola Com-
pany revenue example, use the cell range is B1:B18 in the Data 
worksheet of the Coca-Cola workbook.) Select the chart and
	 1.	 Select Design ➔ Add Chart Element ➔ Trendline ➔ More 
Trendline Options.
	 2.	 In the Format Trendline pane, click Exponential.
If you use an Excel version older than Excel 2013, select 
Layout ➔ Trendline ➔ More Trendline Options. In the Format 
Trendline dialog box, click Trendline Options in the left pane and 
in the Trendline Options right pane, click Exponential and click OK.
Model Selection Using First, Second,  
and Percentage Differences
Use arithmetic formulas to compute the first, second, and per-
centage differences. Use division formulas to compute the per-
centage differences and use subtraction formulas to compute the 
first and second differences. Use the COMPUTE worksheet of 
the Differences workbook, shown in Figure 16.11 on page 673, 
as a model for developing a differences worksheet. (Open to the 
COMPUTE_FORMULAS worksheet to see all formulas used.)
EG16.5  Autoregressive Modeling 
for Trend Fitting and 
Forecasting
Creating Lagged Predictor Variables
Create lagged predictor variables by creating a column of formu-
las that refer to a previous row’s (previous time period’s) Y value. 
Enter the special worksheet value #N/A (not available) for the 
cells in the column to which lagged values do not apply.
Use the COMPUTE worksheet of the Lagged Predictors 
workbook, similar to the Figure 16.13 Minitab worksheet on  
page 680 as a model for developing lagged predictor variables for 

	
Chapter 16 Excel Guide	
699
the first-order, second-order, and third-order autoregressive mod-
els. (Open to the COMPUTE_FORMULAS worksheet to see all 
formulas used.)
When specifying cell ranges for a lagged predictor variable, 
you include only rows that contain lagged values. Contrary to the 
usual practice in this book, you do not include rows that contain 
#N/A, nor do you include the row 1 column heading.
Autoregressive Modeling
Modify the Section EG14.1 instructions (see page 617) to create 
a third-order or second-order autoregressive model. Use the cell 
range of the first-order, second-order, and third-order lagged pre-
dictor variables as the X variables cell range for the third-order 
model. Use the cell range of the first-order and second-order 
lagged predictor variables as the X variables cell range for the 
second-order model (The X variables cell range is the X Variables 
Cell Range in the PHStat instructions and the Input X Range in 
the Analysis ToolPak instructions.) If using the PHStat instruc-
tions, modify step 3 to clear not check First cells in both ranges 
contain label. If using the In-Depth Excel instructions, use the 
COMPUTE3 worksheet in lieu of the COMPUTE worksheet for 
the third-order model. If using the Analysis ToolPak instructions, 
do not check Labels in step 4.
Modify the Section EG13.2 instructions (see page 566) to 
create a first-order autoregressive model. Use the cell range of the 
first-order lagged predictor variable as the X variable cell range 
(called the X Variable Cell Range in the PHStat instructions, the 
cell range of X variable in the In-Depth Excel instructions, and 
the Input X Range in the Analysis ToolPak instructions). If us-
ing the PHStat instructions, modify step 3 to clear not check First 
cells in both ranges contain label. If using the Analysis ToolPak 
instructions, do not check Labels in step 4.
EG16.6  Choosing an Appropriate 
Forecasting Model
Performing a Residual Analysis
To create residual plots for the linear trend model or the first-order 
autoregressive model, use the instructions in Section EG13.5 on 
page 567. To create residual plots for the quadratic trend model or 
second-order autoregressive model, use the instructions in Section 
EG14.3 on page 618. To create residual plots for the exponential 
trend model, use the instructions in Section EG16.4 on page 698. 
To create residual plots for the third-order autoregressive model, 
use the instructions in Section EG14.3 on page 618 but use the 
RESIDUALS3 worksheet instead of the RESIDUALS worksheet 
if using the In-Depth Excel instructions.
Measuring the Magnitude of the Residuals 
Through Squared or Absolute Differences
To compute the mean absolute deviation (MAD), first per-
form a residual analysis. Then add a formula in the form =
SUMPRODUCT(ABS(cell range of residual values)) /
COUNT(cell range of the residual values). In the cell range of 
the residual values do not include the column heading as is the 
standard practice in this book. (See Appendix Section F.4 to learn 
more about the application of SUMPRODUCT function in this 
formula.)
The RESIDUALS_FORMULAS worksheet of the 
­Exponential Trend workbook shows an example of this formula 
in cell I19 for The Coca-Cola Company revenues example.
A Comparison of Four Forecasting Methods
Construct a model comparison worksheet similar to the one shown 
in Figure 16.18 on page 685 by using Paste Special values (see 
Appendix Section B.4) to transfer results from regression results 
worksheets. For the SSE values (row 20 in Figure 16.8), copy the 
regression results worksheet cell C13, the SS value for Residual 
in the ANOVA table. For the SYX values (row), copy the regres-
sion results worksheet cell B7, labeled Standard Error, for all but 
the exponential trend model. For the MAD values, add formulas as 
discussed in the previous section.
For the SYX value for the exponential trend model, enter a for-
mula in the form =SQRT(exponential SSE cell / (COUNT(cell 
range of exponential residuals) - 2)). For the Figure 16.18 work-
sheet, this formula is =SQRT(H20 /(COUNT(H3:H19) - 2)). 
Use the COMPARE worksheet of the Forecasting Comparison 
workbook as a model. Open to the COMPARE_FORMULAS 
worksheet to examine all formulas. This worksheet also shows an 
alternative way that uses a formula to display the SSE values.
EG16.7  Time-Series Forecasting of 
Seasonal Data
Least-Squares Forecasting with Monthly  
or Quarterly Data
To develop a least-squares regression model for monthly or 
quarterly data, add columns of formulas that use the IF func-
tion (see Appendix Section F.4) to create dummy variables for 
the quarterly or monthly data. Enter all formulas in the form 
= IF1comparison, 1, 02.
Shown below are the first five rows of columns F through K 
of a data worksheet that contains dummy variables. In the first il-
lustration, columns F, G, and H contain the quarterly dummy vari-
ables Q1, Q2, and Q3 that are based on column B coded quarter 
values (not shown). In the second illustration, columns J and K 
contain the two monthly variables M1 and M6 that are based on 
column C month values (also not shown).

700	
Chapter 16  Time-Series Forecasting
MG16.1  The Importance of Business 
Forecasting
There are no Minitab Guide instructions for this section.
MG16.2  Component Factors of  
Time-Series Models
There are no Minitab Guide instructions for this section.
MG16.3  Smoothing an Annual  
Time Series
Moving Averages
Use Moving Average.
For example, to compute the Figure 16.3 moving averages shown 
on page 662, open to the Movie Attendance worksheet. Select 
Stat ➔ Time Series ➔ Moving Average. In the Moving Average 
dialog box (shown below):
	 1.	 Double-click C2  Attendance in the variables list to add  
Attendance to the Variable box.
	 2.	 Enter 3 in the MA length box.
	 3.	 Check Center the moving averages.
	 4.	 Click Storage.
	 5.	 In the Moving Average – Storage dialog box (not shown), 
check Moving Averages and then click OK.
	 6.	 Back in the Moving Average dialog box, click Graphs.
	 7.	 In the Moving Average – Graphs dialog box (not shown), 
click Plot smoothed vs. actual and clear all check boxes, 
and then click OK.
	 8.	 Back in the Moving Average dialog box, click Results.
	 9.	 In the Moving Average - Results dialog box (not shown), 
click Summary table and results table and then click OK.
	10.	 Back in the Moving Average dialog box, click OK.
	11.	 Enter MA 3-Yr as the name for column C3 (replacing 
AVER1).
To add the five-year moving averages, repeat steps 1 through 10, 
entering 5 in the MA length box in step 2. Then enter MA 5-Yr as 
the name for column C4 (replacing AVER1).
Exponential Smoothing
Use Single Exp Smoothing to compute exponential smoothed 
values. For example, to compute the Figure 16.4 exponen-
tial smoothed values shown on page 663, open to the Movie 
­Attendance worksheet. Select Stat ➔ Time Series ➔ Single 
Exp Smoothing. In the Single Exponential Smoothing dialog box 
(shown below):
	 1.	 Double-click C2  Attendance in the variables list to add  
Attendance to the Variable box.
	 2.	 Click Use and enter 0.50 in its box (for a W value of 0.50).
	 3.	 Click Options.
	 4.	 In the Single Exponential Smoothing - Options dialog box, 
enter 1 in the Use average of first K observations box and 
then click OK.
	 5.	 Back in the Moving Average dialog box, click Storage.
	 6.	 In the Single Exponential Smoothing – Storage dialog box 
(not shown), check Smoothed data and then click OK.
	 7.	 Back in the Moving Average dialog box, click Graphs.
	 8.	 In the Moving Average – Graphs dialog box (not shown), 
click Plot smoothed vs. actual and clear all check boxes, 
and then click OK.
	 9.	 Back in the Single Exponential Smoothing dialog box, click 
Results.
	10.	 In the Single Exponential Smoothing - Results dialog box, 
click Summary table and results table and then click OK.
	11.	 Back in the Single Exponential Smoothing dialog box, click 
OK.
	12.	 Enter ES(W = 0.50) as the name for column C3 (replacing 
SMOO1).
C h a p t e r  1 6  M i n i ta b  G u i d e

	
Chapter 16 Minitab Guide	
701
For a W value of 0.25,  repeat steps 1 through 11, entering 0.25 
in step 2. Then enter ES(W = 0.25) as the name for column C4 
(replacing SMOO1).
MG16.4  Least-Squares Trend Fitting 
and Forecasting
In Chapters 13 through 15, you used Minitab for the simple linear 
regression model and for a variety of multiple regression models. 
In this chapter, time-series models were developed assuming ei-
ther a linear, quadratic, or exponential trend. For the linear trend 
model, see Section MG13.2 on page 569. For the quadratic and the 
exponential trend models, see Sections MG15.1 and MG15.2 on 
pages 654–655.
MG16.5  Autoregressive Modeling 
for Trend Fitting and 
Forecasting
Creating Lagged Predictor Variables
Use Lag to create lagged predictor variables for autoregressive 
models. For example, to create the Figure 16.13 lagged vari-
ables worksheet on page 662, open to the Coca-Cola worksheet.  
Select Stat ➔ Time Series ➔ Lag. In the Lag dialog box (shown  
below):
	 1.	 Double-click C3  Revenues in the variables list to add 
­Revenues to the Series box.
	 2.	 Enter C3 in the Store lags in box and press Tab.
	 3.	 Enter 1 in the Lag box (for a one-period lag).
	 4.	 Click OK.
	 5.	 In the worksheet, enter Lag1 as the name for column C3.
	 6.	 Again select Stat ➔ Time Series ➔ Lag. In the Lag dialog 
box, enter C4 in the Store lags in box, press Tab, and enter 2 
in the Lag box (for a 2-period lag). Click OK.
	 7.	 In the worksheet, enter Lag2 as the name for column C4.
	 8.	 Reselect Stat ➔ Time Series ➔ Lag. In the Lag dialog box, 
enter C5 in the Store lags in box, press Tab, and enter 3 in the 
Lag box (for a 3-period lag). Click OK.
	 9.	 In the worksheet, enter Lag3 as the name for column C5.
Autoregressive Modeling
Modify the Section MG14.1 “Interpreting the Regression 
­Coefficients” instructions (see page 620) to create a third-order or 
second-order autoregressive model. Add the names of the columns 
containing the first-order, second-order, and third-order lagged 
predictor variables to the Predictors box for the third-order 
model. Add the names of the columns containing the first-order, 
and second-order lagged predictor variables to the Predictors box 
for the second-order model.
Modify the Section MG13.2 instructions (see page 569) to 
create a first-order autoregressive model. In step 2, add the name 
of the column containing the first-order lagged predictor variable 
to the Predictors box.
MG16.6  Choosing an Appropriate 
Forecasting Model
A Comparison of Four Forecasting Methods
When you compare the four forecasting models, you use residual 
analysis to examine the models. Use the instructions in Section 
MG13.5 on page 569 to create residual plots for the linear trend 
model or first-order autoregressive models. Use the instructions in 
Section MG14.1 on page 620 to create residual plots for the qua-
dratic and the exponential trend models.
MG16.7  Time-Series Forecasting  
of Seasonal Data
Least-Squares Forecasting with Monthly  
or Quarterly Data
Use Calculator to create dummy variables for the quarterly or 
monthly data. For example, to create the dummy quarterly vari-
able Q1 for the Table 16.3 Wal-Mart Stores quarterly revenues on  
page 686, open to the WalMart worksheet. Select Calc ➔  
Calculator. In the Calculator dialog box:
	 1.	 Enter C5 in the Store result in variable box.
	 2.	 Enter IF(Quarter = 1,1,0) in the Expression box.
	 3.	 Click OK.
	 4.	 Enter Q1 as the name for column C5.
In step 2, use the expression IF(Quarter = 2,1,0) to create the 
dummy quarterly variable Q2 or use IF(Quarter = 3,1,0) to cre-
ate the quarterly variable Q3.
For monthly variables, first convert the values to text, if nec- 
essary, using Change Data Type (see Section MG1.1 on page 62). 
Then select Calc ➔ Calculator and enter expressions such as 
IF(Month="January",1,0) in the Expression box.

702
U s i n g  S tat i s t i c s
Finding the Right Lines at WaldoLands*
The managers of WaldoLands, the theme park that licenses the characters from 
the Waldowood stories, seek to stabilize and grow their business. Last tourist sea-
son, their park was plagued by a number of major ride breakdowns, long lines at 
popular attractions and key food service areas, and a general inability to respond 
to the park’s day-to-day operating status.
Last year’s problems led to numerous unfavorable reviews in key social 
­media travel websites and the managers are concerned that possible patrons may 
decide to visit competing parks run by Universal Parks & Resorts and Six Flags 
Entertainment. For this year, the managers have added the LineJumper service 
that allows patrons to “jump” to the head of a line and are offering the premium-
priced No-Stress-Express experience that offers special guided tours and behind-
the-scenes access. The managers also hope the new multimillion-dollar Rabbit 
Creek Racers and a greatly expanded MirrorGate Experience, based on a popular 
sci-fi franchise, will boost attendance, even as the managers fret about the techni-
cal complexity of these rides.
As part of the general management team, you puzzle over how to manage 
the park to maximize both revenues and customer satisfaction. You have also 
been asked to ensure that guests at the adjacent Waldowood Resort & Convention 
Center have opportunities for an enhanced theme park experience. How can you 
achieve these goals while managing an enterprise that is the size of a small city?
contents
17.1  Descriptive Analytics
17.2  Predictive Analytics
17.3  Classification and Regression 
Trees
17.4  Neural Networks
17.5  Cluster Analysis
17.6  Multidimensional Scaling
Using Statistics: Finding 
the Right Lines at WaldoLands, 
Revisited
Chapter 17 Software Guide
Objectives
To develop dashboard elements 
such as sparklines, gauges, 
bullet graphs, and treemaps for 
descriptive analytics.
To learn how to use classification 
and regression trees for 
predictive analytics
To learn how to use neural nets for 
predictive analytics
To learn how to use cluster 
analysis for predictive analytics
To learn how to use 
multidimensional scaling for 
predictive analytics
Business Analytics
17
Chapter
*The contents, descriptions, and characters of WaldoLands and Waldowood are copyright © 2014, 2011 
Waldowood Productions, and used with permission.
Arinahabich/Fotolia

	
17.1  Descriptive Analytics	
703
F
or many chapters now, you have been reading about methods that make inferences 
about population data. Many of these techniques were developed 100 or more years 
ago and were first made practical by the use of sample data. Other techniques awaited 
computerization to make them practical and widely used.
Today, business statistics continues to evolve. In this book, you have already read how to 
use visualizations as a means of preliminary analysis. Such a technique was not practical until 
recently when scatter plots and the like could be constructed in the “blink of an eye” on cheap, 
everyday display devices. (Such plots once required expensive, large, special-purpose plot-
ters and took minutes to construct.) As business people gain the ability to retrieve and process 
larger amounts of data in smaller amounts of time, sometimes approaching near real time,1 
some have asked at what point does the need for using samples to expedite analysis disappear? 
Might there not be a day when business decision makers could just analyze all of the data con-
tinuously as it flows into the business in near real time?
While that day of continuous data analysis has not yet arrived in most cases, these ques-
tions taken together have created the demand for methods known collectively as business 
analytics. Analytics represents an evolution of preexisting statistical methods combined with 
advances in information systems and techniques from management science. Analytics is natu-
rally interdisciplinary and this nature underscores how statistics is an important part of your 
business education, one of the themes of the Getting Started chapter.
Descriptive analytics, predictive analytics, and prescriptive analytics form the three broad 
categories of analytic methods. Descriptive analytics explores business activities that have 
occurred or are occurring in the present moment. Predictive analytics identifies what is likely 
to occur in the (near) future and finds relationships in data that may not be readily apparent us-
ing descriptive analytics. Prescriptive analytics investigates what should occur and prescribes 
the best course of action for the future. Predictive and prescriptive analytics make practical the 
use of big data (see page 32) to support decision making, although many of these techniques 
also work with smaller sets of data, as examples in this chapter demonstrate. This chapter be-
gins with descriptive analytics but focuses on predictive analytics. The chapter does not cover 
prescriptive analytics methods.
1Near real time in this sense means 
in a time period short enough that 
the results produced can immedi-
ately affect operational manage-
ment decision making.
In Chapters 2 and 3, you learned descriptive methods that organized and visualized data that 
had been previously collected. What if you could combine, collect, organize, and visualize 
tasks of the DCOVA framework into a single task that could be done in near real-time? That 
would change the historical nature of descriptive methods—your summaries represent the sta-
tus of a business activity in the past—into a tool that could be used for day-to-day, if not 
minute-by-minute, business monitoring in the present. Giving decision makers this ability is 
one of the goals of descriptive analytics.
Being able to do real-time monitoring can be useful for a business that handles a perish-
able inventory. Perishable inventory is inventory that will disappear after a particular event 
takes place such as an airplane taking off for its destination or the end of a concert. Empty 
seats on the airplane or at the concert cannot be sold at a later time. Perishable inventory also 
occurs with less tangible inventory such as spaces reserved for advertisements on a commer-
cial web page—such spaces cannot be sold after the page has been viewed. In the past, the 
problem of perishable inventory was handled by models that predicted consumer behavior 
based on historical patterns. A concert promoter set prices based on the best estimation of 
ticket-buying behavior. Today, by constantly monitoring sales, the promoter can use a dynamic 
pricing model in which the price of tickets could fluctuate in near real time based on whether 
sales are exceeding or failing to meet predicted demand.
Real-time monitoring can also be useful for a business that manages flows of people or ob-
jects that can be adjusted in near real time, especially when there are more than one flow and 
the flows are interrelated. For example, overseers of a large sports stadium could benefit from 
monitoring the flows of cars in parking facilities as well as the flow of fans into the stadium and 
17.1  Descriptive Analytics

704	
Chapter 17  Business Analytics
redirect stadium personnel to assist at points of congestion. In the WaldoLands scenario, manag-
ers could monitor flows of patrons through the ticket booths and into the theme park while also 
keeping an eye on the length of waiting lines and the use of the LineJumper service. This would 
allow the managers to adjust ride lengths or dispatch live performers to entertain patrons in line 
and to try to redirect patrons to areas of the park that are currently under capacity.
Dashboards
Over several decades, people talked about developing executive information systems that would 
put information at the “fingertips” of decision makers. Many of these efforts have spurred the 
development of dashboards that use descriptive analytics methods to present up-to-the-minute 
operational status about a business.
An analytic dashboard provides this information in a visual form that is intended to be easy 
to comprehend and review. Dashboards can contain the summary tables and charts discussed in 
Chapter 2, as well as newer or more novel forms of information presentation, that can summarize 
big data as well as smaller sets of data. The dashboard in Figure 17.1 displays key WaldoLands 
operational statistics that are updated on a near-real-time basis. Clicking one of the categories 
would lead to other displays that contain additional information about theme park operations.
While many assembly-line 
processes such as the cereal-
filling line in the Oxford 
Cereals case (see Chapter 7)  
are flows, such business 
processes may not be good 
candidates for real-time 
adjustments for reasons 
explained in the online 
Chapter 19.
F i g u r e  1 7 . 1
A WaldoLands dashboard
Sparklines are one of the descriptive analytic methods that dashboards can contain. 
­Sparklines summarize time-series data as small, compact graphs designed to appear as part of 
a table (or a written passage). In Figure 17.2, sparklines display the wait times for WaldoLands 
attractions at half-hour intervals for the current day, helping to provide context for the current 
wait times that are indicated by the dot markers. For example, the sparkline for the Rabbit 
Springs Racers ride shows that the current wait time is one of the longest wait times for the day.
F i g u r e  1 7 . 2
WaldoLands wait times 
table with sparklines
Student Tip
To be used properly, a 
set of sparklines must 
share the same X and Y 
axes, a requirement that 
some programs, includ-
ing Microsoft Excel, do 
not enforce.
Learn More
Learn how this dashboard 
could be enhanced in the 
Short Takes for Chapter 17.

	
17.1  Descriptive Analytics	
705
Analogous to automotive dashboards, analytic dashboards can provide warnings when 
predefined conditions are met or exceeded. Figure 17.3 contains a set of gauges and a bullet 
graph that both display the wait-line status for WaldoLands attractions. These displays com-
bine a single numerical measure (wait time) with one of five categorical values that rates the 
wait time subjectively, from excellent (less than 25 minutes) to poor (more than 85 minutes). 
While gauges have been a popular choice in business, most information design specialists 
prefer bullet graphs because those graphs foster the direct comparison of each measurement 
(wait time in Figure 17.3). Gauges can also consume a lot of visual space in a dashboard. For 
­example, in Figure 17.3, note the amount of the space the gauges consume to show the status 
of the six most popular rides. The corresponding bullet graph can display the status of 14 rides 
and present the wait times in a way that facilitates comparisons. For these reasons, some con-
sider gauges little more than examples of chartjunk (see reference 4), even as many decision 
makers request them due to their visual appeal.2
2This tension between what deci-
sion makers might find visually ap-
pealing and what statisticians and 
information specialists have found 
most useful reflects the relative 
newness of these descriptive meth-
ods. Over time, this tension may 
ease and an acceptable standard for 
representing such information may 
emerge.
F i g u r e  1 7 . 3
Gauges and bullet graph of wait times for WaldoLands attractions
Dashboards may also contain treemaps that help visualize two variables, one of which 
must be categorical. Treemaps are especially useful when categories can be grouped to form a 
multilevel hierarchy or tree. Figure 17.4 displays a pair of treemaps that visualize the number 
of social media comments made today about WaldoLands attractions (the size of each rect-
angle) and gives an assessment of the average comment’s favorability (the color), from very 
positive (dark blue) to very negative (dark red). The left treemap shows each ride grouped by 
the “land” of WaldoLands (StrausLand, the BWLand, or FamilyLand) where the attraction 
is found. The right treemap shows the data for the six most popular WaldoLands attractions, 
­illustrating that treemaps can be used with nonhierarchical information as well.
F i g u r e  1 7 . 4
Treemaps of number and favorability of social media comments about WaldoLands attractions

706	
Chapter 17  Business Analytics
When combined with the Figure 17.3 gauges or bullet graph, the treemap at right in 
­Figure 17.4 would allow managers to preliminarily conclude that the negativity of comments 
seems to be tied to current wait lines and that rides with the shortest wait lines may generate 
the fewest social media comments. These relationships could then be further investigated and, 
if the former one was confirmed, managers could, in the future, respond to excessive wait 
lines by shortening the ride length to handle more customers, by sending live performers to 
entertain those waiting in line, or by instructing park staff to divert incoming park patrons to 
other rides.
Note that gauges, bullet graphs, and treemaps use color to represent the value of a second 
variable, thereby increasing the data density of the displays, one of the principles of good in-
formation design (see reference 11). However, when using these displays, particularly bullet 
graphs, and treemaps, avoid using color spectrums that run from red to green, the two colors 
most subject to confusion due to color vision deficiencies. (This is less of a problem with 
gauges, as colors subject to confusion will have unique positions on the gauge dial.)
There is a tradeoff using treemaps when the second variable is numerical and has been 
recoded as a color or shade on a color spectrum (as was done in Figure 17.4). You must 
­always consider whether the positive effects of making a preliminary analysis through ­visual 
means offsets the loss of the preciseness of the numerical values. While this tradeoff is 
­always favorable when summarizing sets of “big data,” the advantage narrows as the set of 
data shrinks. (Thus could be used as an argument against constructing the treemap of the six 
most popular rides shown in Figure 17.4.)
Data Discovery
Data discovery methods allow decision makers to interactively organize or visualize data and 
perform preliminary analyses. These methods can be used to take a closer look at historical or 
status data, to quickly review data for unusual values or outliers, or to construct visualizations 
for management presentations. In these ways, data discovery realizes the earlier promise of 
executive information systems to give decision makers the tools of data exploration and pre-
sentation.
In its simplest version, data discovery involves drill-down, the revealing of the data 
that underlies a higher-level summary. For example, clicking the merchandise entry in the 
­Figure 17.1 WaldoLands dashboard would reveal more detailed information such as the table 
of sales by “lands” shown in the left table in Figure 17.5. In turn, this summary can be drilled 
down to reveal sales by each store in the theme park (see right table in Figure 17.5). At this 
level of detail, sales at Peri’s Playtime are significantly lower than the other stores, perhaps 
suggesting that this store be closed, relocated, or have its merchandise mix reconsidered.
Student Tip
Treemaps are especially 
effective at displaying 
nested geographical 
data, such as market 
preferences or voting 
patterns by state and 
county, as they eliminate 
the distortion that placing 
such data on political  
(geographic) maps 
can create because 
of ­differences in the 
sizes of the political 
units. For ­example, a 
treemap would elimi-
nate the ­distortion that 
always makes sparsely 
­populated Montana much 
more prominent than  
the densely populated 
New Jersey on a U.S 
political map.
F i g u r e  1 7 . 5
WaldoLands merchandise 
sales summarized on two 
different levels

	
17.1  Descriptive Analytics	
707
Another level of drill-down (not shown) would reveal the sales of each item or SKU 
(stock-keeping unit) sold in each store. By reorganizing that list by item, WaldoLands manag-
ers could discover which items are selling the best and may be subject to stockouts.
Drill-down can also reveal the data for variables not initially displayed in a summary table 
or chart. For example, Figure 2.16 on page 97 displays a multidimensional contingency table 
for the sample of 316 retirement funds used in earlier chapters. This Excel PivotTable also 
demonstrates one level of drill-down, drilling down the type of fund by market cap. Further 
drill-down is possible by clicking one of the cells. For example, clicking the cell that contains 
2.85%, the percentage of funds that are small market cap value funds with low risk, creates 
the Figure 17.6 worksheet that contains the details for the 9 retirement funds that the 2.85% 
represents.
F i g u r e  1 7 . 6
Results of drilling down to the details about small market cap value funds with low risk
Data discovery also allows decision makers to add or remove variables or statistics to 
­uncover new patterns in the data. Replacing the count of funds with the mean of the ten-year 
return percentage to the retirement funds PivotTable reveals that many of the highest percent-
age gains over the ten-year period come from small market cap funds, particularly those funds 
with low risk and that small market cap value funds with low risk had the highest mean ten-
year percentage return (see Figure 17.7). In addition, mid-cap funds had a higher return than 
large cap funds for both growth and value funds and also for low and average risk funds.
F i g u r e  1 7 . 7
Mean 10-year return 
percentages for the 
sample of 316 retirement 
funds, by fund type, 
market cap, and risk
Some data discovery methods are primarily visual. Perhaps the simplest method is the 
direct manipulation of a visual, as was done with the three-dimensional scatter plot shown in 
Figure 14.1 on page 573. With map-type data, zooming, the visual equivalent of drill-down is 
also possible. For example, the websites such as GasBuddy.com and GasPriceWatch.com 
display fuel prices at gas stations as a series of maps that can be zoomed down to see specific 
locations.
Data discovery can also provide a means of preliminary analysis. For example, Figure 17.8 
displays several Excel slicers, panels of clickable buttons that appear superimposed over a work-
sheet. Each slicer panel corresponds to one of the variables that is under study, and each button in 
a variable’s slicer panel represents a unique value of the variable that is found in the data.

708	
Chapter 17  Business Analytics
Slicers allow you to work with more than three or four variables at the same time in a way 
that avoids creating overly complex multidimensional contingency tables that would be hard 
to read. By clicking specific buttons in slicer panels, you can filter data to ask questions of the 
data you have collected. For example, with the Figure 17.8 slicer panels you could ask ques-
tions such as
	
1.	 What are the attributes of the fund with the highest expense ratio?
	
2.	 What is the type and market cap of the five-star fund with the lowest expense ratio?
Figure 17.9 shows slicer displays that answer these two questions. Note that Excel has 
disabled, or dimmed, the buttons that represent values that the current filtering excludes. This 
allows you to visually discover the answers to questions. For example, the answer to Question 
1 is a large market cap growth fund that is rated three stars. For Question 2, the answer is a 
large market cap value fund.
F i g u r e  1 7 . 8
Excel slicers for a 
PivotTable based on the 
Chapter 2 sample of 316 
retirement funds
Slicers are not included 
in OS X Excel 2011 and 
Microsoft Windows versions 
of Excel that are older than 
Excel 2010
F i g u r e  1 7 . 9
Slicer displays for 
Questions 1 (left) and 2 
(right)
Because data discovery provides a means of preliminary analysis, data discovery is often 
part of many predictive analytics methods. Later in this chapter, you will recognize data dis-
covery techniques that are embedded in cluster analysis and multidimensional scaling.
Problems for Section 17.1
17.1  The Edmunds.com NHTSA Complaints Activity Report is 
the result of the examination of the frequency, trends and com-
position of consumer vehicle complaint submissions at the auto-
maker, brand, and category levels (data extracted from edmu.in 
/Ybmpuz). The table at the right, stored in  Automaker1 , con-
tains complaints received by six automakers for January 2013. 
When the number of complaints are below 300, the complaint 
rating is considered to be low, when the number of complaints 
is between 300 and 500, the complaint rating is considered to be 
medium, and when the number of complaints is above 500, the 
complaint rating is considered to be high.
Automaker
Number of Complaints
American Honda
169
Chrysler LLC
439
Ford Motor Company
440
General Motors
551
Nissan Motors Corporation
467
Toyota Motor Sales
332
a.	 Construct a gauge for each automaker.
b.	 Construct a bullet graph for the automakers.
c.	 Which display is more effective at comparing the number of 
complaints for each automaker?

	
17.1  Descriptive Analytics	
709
17.2  There are a very large number of mutual funds from which 
an investor can choose. Each mutual fund has its own mix of dif-
ferent types of investments. The file  BestFunds1  contains the 
one-year return percentage and the three-year annualized return 
percentage for the ten best short-term bond and long-term bond 
funds according to the U.S. News & World Report score. (Data 
­extracted from money.usnews.com/mutual-funds/rankings.)
a.	 Construct bullet graphs of the one-year returns and the three-year 
returns. For the purposes of comparison, consider a return below 
5% as low-performing, a return between 5 and 10% as medium-
performing, and a return above 10% as high-performing.
b.	 Why would you not want to construct a gauge for each bond 
fund?
c.	 What conclusions can you reach about the one-year and three-
year return percentages for the short-term bond and long-term 
bond funds?
17.3  A financial analyst was interested in comparing the price-
to-book ratio (P/B) of drug companies. The analyst collected P/B 
ratios for 71 drug companies (Industry Group SIC 3 code: 283) 
and stored them as part of the file  BusinessValuation .
a.	 Visually evaluate the P/B ratios by constructing a bullet graph. 
For the purposes of comparison, consider a P/B ratio that is 2 
or less as excellent, a P/B ratio that is between 2 and 5 as ac-
ceptable, and a P/B ratio that is above 5 as unacceptable.
b.	 Why would using gauges be a poor choice for this analysis?
c.	 Are the three groupings of P/B ratios helpful in analyzing the 
data? What constitutes an acceptable P/B ratio varies by in-
dustry and is partially based on subjective analysis. For the 
purposes of information presentation, would you redefine or 
subdivide the current acceptable category?
17.4  The file  BBCost2012  contains the total cost (in $) for four 
tickets, two beers, four soft drinks, four hot dogs, two game pro-
grams, two baseball caps, and parking for one vehicle at each of 
the 30 Major League Baseball (MLB) parks during the 2012 sea-
son. (Data extracted from fancostexperience.com.)
a.	 Visually evaluate the total cost at each MLB park by construct-
ing a bullet graph. For the purposes of comparison, consider a 
total cost (in $) below $180 as inexpensive, between $180 and 
$240 as typical, and above $240 as expensive.
b.	 Compare the bullet graph constructed in (a) with the stem-and-
leaf display constructed in Problem 2.36 on page 91 for the 
same data.
c.	 Which display best visualizes the distribution of costs? Why?
d.	 Name something that the bullet graph reveals about the data 
that the stem-and-leaf display does not. How could that be used 
as the basis for future analysis of total costs at MLB parks?
17.5  Referring to the data in Problem 2.56 on page 96 
­concerning movie attendance between 2002 and 2012 (stored in  
 Movie Attendance2 ):
a.	 Construct a sparkline graph for the movie attendance between 
2002 and 2012.
b.	 What conclusions can you reach about the movie attendance 
between 2002 and 2012?
c.	 Compare the sparkline graph with the time-series plot con-
structed in Problem 2.56 on page 96 for the same data. When 
would using the sparkline graph be the better choice to visualize  
these data? When would using the time-series plot be the better 
choice?
d.	 Might you ever use both a sparkline graph and a time-series 
plot in the same analysis report? Explain your reasoning.
17.6  The file  StockIndices  contains the data that represent the 
total rate of return (in percentage) for the Dow Jones Industrial 
Average (DJIA), the Standard & Poor’s 500 (S&P 500), and the 
technology-heavy NASDAQ Composite (NASDAQ) from 2006 
through 2012. Source: Data extracted from finance.yahoo.com, 
March 29, 2013.
a.	 Construct sparklines for the rate of return per year for the DJIA, 
S&P 500, and NASDAQ from 2006 through 2012.
b.	 What conclusions can you reach concerning the rates of return 
per year of the three market indices?
c.	 Compare the results of (b) to those of Problem 17.7 (b).
17.7  From 2006 to 2012, the value of precious metals fluctu-
ated dramatically. The file  Metal Indices  contains the total rate 
of return (in percentage) for platinum, gold, and silver from 
2006 through 2012. (Data extracted from finance.yahoo.com, 
March 29, 2013.)
a.	 Construct sparklines for rate of return per year for platinum, 
gold, and silver from 2006 through 2012.
b.	 What conclusions can you reach concerning the rates of return 
of the three precious metals?
c.	 Compare the results of (b) to those of Problem 17.6 (b).
17.8  Drive-through service time is an important quality attribute 
for fast food chains. The data in  ServiceTime  are the mean service 
times for Burger King, Chick-Fil-A, McDonald’s, and Wendy’s in 
12 recent years. (Data extracted from bit.ly/qhvP3Zb.)
a.	 Construct sparklines of the mean service times for Burger King, 
Chick-Fil-A, McDonald’s, and Wendy’s in 12 recent years.
b.	 What conclusions can you reach concerning the mean service 
times for Burger King, Chick-Fil-A, McDonald’s, and Wendy’s 
in 12 recent years?
17.9  Sales of automobiles in the United States fluctuate from 
month to month and year to year. The data in the file  AutoSales 
represent the sales for various manufacturers in July 2013 and 
the change from July 2012 sales in percentages. (Data extracted 
from www.nytimes.com/interactive/2013/08/01/business/How 
-the-Auto-Industry-Fared-in-July.htm.)
a.	 Construct a treemap of the sales of autos and the change in 
sales from July 2012.
b.	 What conclusions can you reach concerning the sales of autos 
and the change in sales from July 2012?
17.10  The value of a National Basketball Association (NBA) 
franchise has increased dramatically over the past few years. The 
value of a franchise varies based on the size of the city in which 
the team is located, the amount of revenue it receives, and the suc-
cess of the team. The file  NBAValues  contains the value of each 
team and the change in value in the past year. (Data extracted from 
www.forbes.com/nba-valuations.)
a.	 Construct a treemap that visualizes the values of the NBA 
teams (size) and the one year changes in value (color).
b.	 What conclusions can you reach concerning the value of NBA 
teams and the one year change in value?

710	
Chapter 17  Business Analytics
17.11  The annual ranking of the FT Global 500 2013 provides 
a snapshot of the world’s largest companies. The companies are 
ranked by market capitalization—the greater the stock market 
value of a company, the higher the ranking. The market capital-
izations (in $billions) and the 52-week change in market capi-
talizations (in percentages) for companies in the Automobile & 
Parts, Financial Services, Health Care Equipment & Services, and 
­Software & Computer Services sectors are stored in  FTGlobal500 . 
(Data extracted from ft.com/intl/indepth/ft500.)
a.	 Construct a treemap that presents the each company’s market 
capitalization (size) and the 52-week change in market capital-
ization (color) grouped by sector and country.
b.	 Which sector seems to have the best gains in the market capi-
talizations of its companies? Which sectors seem to have the 
worst gains (or greatest losses)?
c.	 Construct a treemap that presents each company’s market capi-
talization (size) and the 52-week change in market capitaliza-
tion (color) grouped by country.
d.	 What comparison can be more easily made with the  
treemap constructed in (c) compared to the treemap constructed 
in (a)?
17.12  Your task as a member of the International Strategic 
­Management Team at your company is to investigate the poten-
tial for entry into a foreign market. As part of your initial inves-
tigation, you must provide an assessment of the economies of 
countries in the Americas and the Asia and Pacific regions. The 
file  DoingBusiness  contains the 2012 GDPs per capita for these 
countries as well as the number of Internet users in 2011 (per 100 
people) and the number of mobile cellular subscriptions in 2011 
(per 100 people). (Data extracted from data.worldbank.org.)
a.	 Construct a treemap of the GDPs per capita (size) and their 
number of Internet users in 2011 (per 100 people) (color) for 
each country grouped by region.
b.	 Construct a treemap of the GDPs per capita (size) and their 
number of mobile cellular subscriptions in 2011 (per 100 peo-
ple) (color) for each country grouped by region.
c.	 What patterns to these data do the two treemaps suggest?  
Are the patterns in the two treemaps similar or different?  
Explain.
17.13  Using the sample of retirement funds stored in  
 Retirement Funds :
a.	 Construct a table that tallies type, market cap, and risk.
b.	 Drill down to examine the large cap growth funds with high 
risk. How many funds are there? What conclusions can you 
reach about these funds?
17.14  Using the sample of retirement funds stored in  
 Retirement Funds :
a.	 Construct a table that tallies type, market cap, and rating.
b.	 Drill down to examine the large cap growth funds with a rating 
of three. How many funds are there? What conclusions can you 
reach about these funds?
17.15  Using the sample of retirement funds stored in  
 Retirement Funds :
a.	 Construct a table that tallies market cap, risk, and rating.
b.	 Drill down to examine the large cap funds that are high risk 
with a rating of three. How many funds are there? What conclu-
sions can you reach about these funds?
17.16  Using the sample of retirement funds stored in  
 Retirement Funds :
a.	 Construct a table that tallies type, risk, and rating.
b.	 Drill down to examine the growth funds with high risk with a 
rating of three. How many funds are there? What conclusions 
can you reach about these funds?
17.17  Using the sample of retirement funds stored in  
 Retirement Funds , what are the attributes of the fund with the 
highest five-year return?
17.18  Using the sample of retirement funds stored in  
 Retirement Funds , what five-year returns are associated with 
small market cap funds that have a rating of five stars?
17.19  Using the sample of retirement funds stored in  
 Retirement Funds , which fund(s) in the sample have the lowest 
five-year return?
17.20  Using the sample of retirement funds stored in  
 Retirement Funds , what is the type and market cap of the five-star 
fund with the highest five-year return?
17.2  Predictive Analytics
Predictive analytics are methods that determine what is likely to occur in the future. These 
methods fall into one of four categories:
 • Prediction: Assigning a value to a target based on a model
 • Classification: Assigning items in a collection to target categories or classes
 • Clustering: Finding natural groupings in data
 • Association: Finding items that tend to co-occur and specifying the rules that govern 
their co-occurrence
Earlier chapters have already discussed some prediction and classification methods including 
regression and correlation and logistic regression. This chapter discusses classification and 
regression trees, neural networks, cluster analysis, and multidimensional scaling.
While predictive analytics has been used with samples from a population, the grow-
ing use of big data has led to more widespread use. Combining predictive methods with 
big data requires data mining. Data mining are the techniques that allow the extraction of  

	
17.3  Classification and Regression Trees	
711
useful knowledge from huge repositories of data. That extraction is rarely simple and typically 
requires statistical methods and computer science algorithms as well as the database-type 
operations associated with simple searches and retrievals of data. Because of this common 
association of predictive analytics and data mining, some people intertwine the meanings of 
the two terms or use them jointly in the phrase “predictive analytics and data mining.” Even 
though predictive analytics methods may be most useful to a business decision maker when 
used with data mining, these methods can exist independently of data mining and can be used 
with smaller sets of data, as the examples used in this chapter illustrate.
Table 17.1 identifies and classifies the specific predictive analytics methods that are dis-
cussed in the remainder of this chapter.
T a b l e  1 7 . 1
Chapter 17 Predictive 
Analytics Methods
Method for
Method
Prediction
Classification
Clustering
Association
Classification and Regression Trees
•
•
Neural Networks
•
•
•
Cluster Analysis
•
Multidimensional Scaling (MDS)
•
•
Section 4.2 introduced decision trees, a visual alternative to contingency tables. Classification 
and regression trees are decision trees that split data into groups based on the values of indepen-
dent or explanatory (X) variables. This splitting determines which values of a specific independent 
­variable are useful in predicting the dependent (Y) variable.
Using a categorical dependent Y variable results in a classification tree. Using a numeri-
cal dependent Y variable results in a regression tree. To construct the tree, the classification 
and regression tree method chooses the independent variable that provides the best split of the 
data at each node in the tree, starting with the root. Successfully completing a classification or 
regression tree requires:
 • Rules for splitting the data at each node based on one independent variable.
 • Rules for deciding when a branch cannot be split any more.
 • A prediction for the target variable at each node.
When using classification and regression trees, the variables included in the model are 
not determined prior to the analysis but are selected based on the algorithm used by the clas-
sification and regression tree process. For example, the statistical software JMP uses the Gini 
impurity, the product of the probability that each item is chosen multiplied by the probability 
that the item has been misclassified, as a basis to determine how to split items at each node 
(see reference 6).
Classification and regression trees are not affected by the distribution of the variables that 
make up the data. Typically, trees are developed through several layers of branches until either 
no further gain in the fit occurs or the splitting has continued as far as possible. Splitting can 
be followed by pruning back the tree as is necessary. As with regression modeling, in order to 
validate any classification and regression tree analysis, where possible, you should split the 
data into a training sample that is used to develop models for analysis and a validation sample 
that is used to determine the validity of the models developed in the training sample.
To illustrate a classification tree analysis, return to the Section 14.7 card study example on 
page 602 in which a logistic regression model was used to predict the proportion of credit card 
holders who would upgrade to a premium card. Using JMP, you can create the classification 
tree results shown in Figure 17.10.
17.3  Classification and Regression Trees

712	
Chapter 17  Business Analytics
In the tree diagram portion of Figure 17.10, the root represents all the data. The Yes 
rate, 0.4333, reflects the 13 out of 30 cardholders who have upgraded to a premium card. 
(The No rate, 0.5667, reflects the 17 cardholders who have not upgraded.) The first split of 
the data is based on whether the cardholder has additional cards. In the Extra Cards(Yes) 
node that represents those cardholders who have extra cards, the Yes rate, 0.7857, repre-
sents the 11 of these 14 have upgraded. In the Extra Cards(No) node that represents those 
cardholders who do not have extra cards, the Yes rate, 0.125, represents the 2 out of 16 who 
have upgraded.
At the next level, the two nodes are split based on the amount of annual purchases. For the 
Extra Cards (Yes) node, the split is between those who charge more than $49,738.80 per year 
and those who charge less than $49,738.80 per year. For the Extra Cards(No) node, the split 
is between those who charge more than $35,389.90 per year and those who charge less than 
$35, 389.90 per year. For those cardholders who have extra cards and use the card to purchase 
more than $49,738.80 per year (the left node on the bottom level), the rate of 1.0 reflects that 
all five have upgraded. For those cardholders who have extra cards and use the card to pur-
chase less than $49,738.80 per year (second from left), the rate 0.6667 reflects that 6 of the 9 
have upgraded. For those cardholders who do not have extra cards and use the card to purchase 
more than $35,389.90 per year (second from right), the rate 0.4000 reflects that 2 of the 5 have 
upgraded. For those cardholders who do not have extra cards and use the card to purchase less 
than $35,389.90 per year (right node on the bottom level), the rate of 0 reflects that none of the 
11 have upgraded.
These results show that customers who order extra cards are more likely to upgrade a pre-
mium card and that those customers in this group who charge extensively are most likely to up-
grade. (Least likely to upgrade to a premium card are customers who have only a single charge 
card and have charged less than $35,389.90.) Therefore, managers at the credit card company 
F i g u r e  1 7 . 1 0
Classification tree results  
for predicting the 
proportion of credit card 
holders who would upgrade 
to a premium card

	
17.3  Classification and Regression Trees	
713
might want to make a special appeal to customers who charge extensively and have ordered 
extra cards while creating a broader marketing effort aimed at all those who have ­ordered extra 
cards.
The r 2 of 0.516, shown in the summary box below the plot, means that 51.6% of the varia-
tion in whether a cardholder upgrades can be explained by the variation in whether the card-
holder has additional cards and the amount the cardholder charges per year. The Akaike 
information criterion (AIC) and the AICc, the “corrected” version of the AIC, measure the 
relative quality of the model.
Akaike Information Criterion (AIC)
	
AIC = 2k - 2 ln(L)	
(17.1)
where
k = the number of parameters in the model
L is the maximum value of the likelihood function for the model
Akaike Information Criterion corrected (AICc)
AICc corrects AIC for the sample size.
	
AICc = AIC + 2k(k + 1)
n - k - 1	
(17.2)
where
n = sample size
LogWorth
	
LogWorth = -log10( p@value)	
(17.3)
where the adjusted p-value is based on the number of ways that splits can occur.
Using the AICc provides a way of comparing alternative models in terms of information 
loss. At each branch, JMP reports the LogWorth and G2 statistics. The LogWorth statistic is 
the basis for splitting at each node of the classification tree. LogWorth is calculated as:
The G2 statistic is the likelihood ratio chi-square statistic. It is a measure of the probability 
that can be attributed to the response that has occurred. In Figure 17.10, the Prob column is the 
predicted probability for that node of the tree.
Regression Tree Example
To illustrate a regression tree analysis, return to the Chapter 14 OmniPower scenario.  
As discussed on page 571, you are a marketing manager to determine the effect that price and 
in-store promotional expenses will have on sales of OmniPower bars. 
Figure 17.11 on page 714 presents the regression tree constructed by JMP for predicting 
the sales of OmniPower bars.
Student Tip
Remember that a 
­regression tree results 
from using a numerical 
dependent variable Y.

714	
Chapter 17  Business Analytics
Observe from Figure 17.11 that the tree root represents all the data. The r 2 value of 0.541 means 
that 54.1% of the variation in sales can be explained by variation in the price and promotional 
expenses.
The first split in the tree is based on the price being less than 99 cents or greater than or 
equal to 99 cents. For the 10 stores in which the price was greater than or equal to 99 cents, the 
mean sales were $1,880. For the 24 stores in which the price was less than 99 cents, the mean 
sales were $3,606.3333.
The next split is on the portion of the tree that contained the 24 stores in which the price 
was less than 99 cents. For the 12 stores in which the price was greater than or equal to 
79 cents, the mean sales were $3,060.9167. For the 12 stores in which the price was less than 
79 cents, the mean sales were $4,151.75.
You can conclude that sales were much higher at stores in which the price was below 
99 cents. (Because the prices were set as either 59, 79, or 99 cents, this result means that sales 
were higher at stores where the bars were sold at 59 or 79 cents.) Going one step further, you 
can conclude that sales were higher at stores where the price of OmniPower bars was below 
79 cents (in other words, 59 cents). Given these results, the OmniFoods marketing manager 
could then determine how pricing the bars at 59 cents would impact their profitability.
F i g u r e  1 7 . 1 1
Regression tree results 
for predicting the sales of 
OmniPower bars
Problems for Section 17.3
17.21  A hotel has designed a new system for room service deliv-
ery of breakfast that allows the customer to select a specific deliv-
ery time. The file  Satisfaction  contains the difference between the 
actual and requested delivery times (a negative time means that the 
breakfast was delivered before the requested time) recorded for 30 
deliveries on a particular day along with whether the customer had 
previously stayed at the hotel.
a.	 Using all the data as the training sample, develop a classifica-
tion tree model to predict the probability that the customer will 
be satisfied based on the delivery time difference and whether 
the customer had previously stayed at the hotel.
b.	 What conclusions can you reach about the probability that the 
customer will be satisfied?

	
17.3  Classification and Regression Trees	
715
17.22  A marketing manager wants to predict customers with risk 
of churning (switching their service contracts to another company) 
based on the number of calls the customer makes to the company 
call center and the number of visits the customer makes to the lo-
cal service center. Data from a random sample of 30 customers are 
organized and stored in  Churn .
a.	 Using all the data as the training sample, develop a classifica-
tion tree model to predict the probability of churning, based on 
the number of calls the customer makes to the company call 
center and the number of visits the customer makes to the local 
service center.
b.	 What conclusions can you reach about the probability of  
churning?
17.23  An automotive insurance company wants to predict which 
filed stolen vehicle claims are fraudulent, based on the number of 
claims submitted per year by the policy holder and whether the 
policy is a new policy, that is, is one year old or less (coded as 
1 = yes, 0 = no). Data from a random sample of 98 automo-
tive insurance claims are organized and stored in  InsuranceFraud . 
(Data extracted from Gelp et al., “A Comparative Analysis of 
­Decision Trees vis-à-vis Other Computational Data Mining 
­Techniques in Automotive Insurance Fraud Detection,” Journal of 
Data Science, 10 (2012), pp. 537–561.)
a.	 Using all the data as the training sample, develop a classifica-
tion tree model to predict the probability of a fraudulent claim, 
based on the number of claims submitted per year by the policy 
holder and whether the policy is new.
b.	 What conclusions can you reach about the probability of a 
fraudulent claim?
c.	 Using half the data as the training sample and the other half of 
the data as the validation sample, develop a classification tree 
model to predict the probability of a fraudulent claim, based on 
the number of claims submitted per year by the policy holder 
and whether the policy is new.
d.	 What differences exist in the results of (a) and (c)? What con-
clusions can you reach about the models fit from the training 
samples in (a) and (c)?
17.24  Undergraduate students at Miami University in Oxford, 
Ohio, were surveyed in order to evaluate the effect of price on the 
purchase of a pizza from Pizza Hut. The students were asked to 
suppose that they were going to have a large two-topping pizza 
delivered to their residence. Then they were asked to select from 
either Pizza Hut or another pizzeria of their choice. The price they 
would have to pay to get a Pizza Hut pizza differed from survey to 
survey. For example, some surveys used the price $11.49. Other 
prices investigated were $8.49, $9.49, $10.49, $12.49, $13.49, 
and $14.49. The dependent variable for this study is whether or 
not a student will select Pizza Hut. The independent variables 
are the price of a Pizza Hut pizza and the gender of the student 
(1 = male, 0 = female). The results of these surveys are stored 
in  PizzaHut .
a.	 Using half the data as the training sample and the other half of 
the data as the validation sample, develop a classification tree 
model to predict the probability the student will select Pizza 
Hut based on the price of a Pizza Hut pizza and the gender of 
the student.
b.	 What conclusions can you reach about the probability the stu-
dent will select Pizza Hut?
17.25  The business problem facing a consumer products com-
pany is to measure the effectiveness of different types of adver-
tising media in the promotion of its products. Specifically, the 
company is interested in the effectiveness of radio advertising 
and newspaper advertising (including the cost of discount cou-
pons). During a one-month test period, data were collected from 
a sample of 22 cities with approximately equal populations. Each 
city is allocated a specific expenditure level for radio advertising 
and for newspaper advertising. The sales of the product (in thou-
sands of dollars) and also the levels of media expenditure (in thou-
sands of dollars) during the test month are recorded and stored in  
 Advertise .
a.	 Using all the data as the training sample, develop a regression 
tree model to predict the sales of the product.
b.	 What conclusions can you reach about the sales of the product?
17.26  Starbucks Coffee Co. uses a data-based approach for im-
proving the quality and customer satisfaction of its products. When 
survey data indicated that Starbucks needed to improve its pack-
age sealing process, an experiment was conducted (data extracted 
from L. Johnson and S. Burrows, “For Starbucks, It’s in the Bag,” 
Quality Progress, March 2011, pp. 17–23) to determine the factors 
in the bag-sealing equipment that might be affecting the ease of 
opening the bag without tearing the inner liner of the bag. Among 
the factors that could affect the rating of the ability of the bag to 
resist tears were the viscosity, pressure, and plate gap on the bag-
sealing equipment. Data were collected on 19 bags in which the 
plate gap was varied. The results are stored in  Starbucks .
a.	 Using all the data as the training sample, develop a regression 
tree model to predict the rating of the ability of the bag to resist 
tears.
b.	 What conclusions can you reach about the rating of the ability 
of the bag to resist tears?
17.27  In mining engineering, holes are often drilled through 
rock using drill bits. As a drill hole gets deeper, additional rods are 
added to the drill bit to enable additional drilling to take place. It 
is expected that drilling time increases with depth. This increased 
drilling time could be caused by several factors, including the 
mass of the drill rods that are strung together. The business prob-
lem relates to whether drilling is faster using dry drilling holes or 
wet drilling holes. Using dry drilling holes involves forcing com-
pressed air down the drill rods to flush the cuttings and drive the 
hammer. Using wet drilling holes involves forcing water rather 
than air down the hole. Data have been collected from a sample of 
50 drill holes that contains measurements of the time to drill each 
additional 5 feet (in minutes), the depth (in feet), and whether the 
hole was a dry drilling hole or a wet drilling hole. The data are 
organized and stored in  Drill .
a.	 Using half the data as the training sample and the other half 
of the data as the validation sample, develop a regression tree 
model to predict the drilling time.
b.	 What conclusions can you reach about the drilling time?
17.28  The owner of a moving company typically has his most 
experienced manager predict the total number of labor hours that 
will be required to complete an upcoming move. This approach 
has proved useful in the past, but the owner has the business ob-
jective of developing a more accurate method of predicting labor 
hours. In a preliminary effort to provide a more accurate method, 

716	
Chapter 17  Business Analytics
the owner has decided to use the number of cubic feet moved, the 
number of large pieces of furniture, and whether there is an eleva-
tor in the apartment building as the independent variables and has 
collected data for 36 moves in which the origin and destination 
were within the borough of Manhattan in New York City and the 
travel time was an insignificant portion of the hours worked. The 
data are organized and stored in  Moving .
a.	 Using all the data as the training sample, develop a regression 
tree model to predict the labor hours.
b.	 What conclusions can you reach about the labor hours?
17.4  Neural Networks
Neural networks are powerful, flexible data mining techniques that construct models from 
patterns and relationships uncovered in data. Unlike the inferential methods in earlier chapters, 
in which you must supply a model to be tested, neural networks “learn” from the data to con-
struct that model for you.3 Neural networks are very flexible and can be applied to prediction, 
classification, and clustering problems and, as nonparametric methods, do not make a priori 
assumptions about the distribution of the data. Because they do not require a supplied model 
as a starting point, neural networks are particularly valuable in analyzing big data in which the 
process of model transformations discussed in Chapters 14 and 15 would be too unwieldy and 
time consuming to perform.
Multilayer Perceptrons
All neural networks contain complex computations that begin with inputs and end with out-
puts. Neural networks used for prediction and classification are typically multilayer percep-
trons (MLPs) that contain an input layer, a hidden layer, and an output layer, as shown in 
Figure 17.12.
3Because neural networks were 
inspired by the architecture of the 
human brain, some describe such 
networks as learning from the data 
to form the best model. This “learn-
ing” in neural networks, while 
complex, is more analogous to the 
computation and evaluation of in-
ferential methods discussed earlier 
in this book than to the many types 
of human learning. Therefore this 
book uses “uncovers” where other 
sources might use “learn.”
F i g u r e  1 7 . 1 2
Structure of a multilayer 
perceptron
Input Layer
Input 1
Input 2
Input 3
Input 4
Hidden Layer
Output Layer
Output
To construct models, MLPs use error back propagation. To begin, the input layer nodes 
(the circles in Figure 17.12) send the various inputs to the nodes of the hidden layer. The inputs 
have associated weights. Processing elements4 that comprise the hidden layer combine the 
weighted inputs and apply a nonlinear function, such as the S-shaped hyperbolic tangent func-
tion, to the combination. The hyperbolic tangent function is a function that varies between 
-1 and +1.
4Processing elements are meant to 
simulate the neuron in the brain 
and therefore are often referred to 
as ­artificial neurons or nodes. For 
more on the relationship between 
biological and artificial neural net-
works, see reference 10.

	
17.4  Neural Networks	
717
The results of the processing element computations in the hidden layer are sent to the 
output layer. The output layer combines the results it receives from the hidden layer and com-
pares it to the target Y value. The output layer then sends back to the hidden layer nodes (the 
start of the back propagation) its estimate of the difference between the predicted results and 
the target value (the error rate). Computations in the hidden layer then backwardly influence 
the weighting done near the input layer, and the process continues forward a second time to 
the output layer. This forward-and-backward computation among the three layers continues 
until the output layer detects that the error rate has been minimized or is at an acceptable level. 
(This minimization of errors is analogous to the minimization of prediction error of regression  
models.) At this point, the model is established.
To establish a neural network, you use some of your data as the training data and some 
of it as the validation data. Neural networks use the training data to uncover a model that by 
some criteria best describes the patterns and relationships in the data. The model is then ap-
plied to the validation data to see if the model can make the correct prediction or classification.
Models that neural networks construct can be difficult to interpret (see reference 5). Neu-
ral networks also can suffer from poor quality of data, insufficient data, or overfitted models, 
models that only work well with the data used to construct that model. Determining the num-
ber of hidden nodes (processing elements) can be inexact process. One source suggests that 
when using a neural network for classification to start with one hidden node per class (see 
reference 7).
To illustrate an MLP, recall the Section 14.7 card study example on page 602 that sought 
to identify credit cardholders who would be likely to upgrade to a premium card. Figure 17.13 
shows the MLP results computed by JMP for both the training and validation samples.
Hyperbolic Tangent Function
	
e2x - 1
e2x + 1	
(17.4)
where x is a linear combination of the X variables.
Learn More
Learn more about the 
­mathematical ­computations 
used in MLP neural networks 
in the Short Takes for 
Chapter 17.
F i g u r e  1 7 . 1 3
Multilayer Perceptron 
results for classifying 
credit card holders who 
would upgrade to a 
premium card
The table of parameter 
estimates of the model 
contains the final estimated 
weights that resulted 
from the backpropagation 
training process. Even 
though these weights 
cannot be interpreted in the 
same manner as regression 
coefficients, weights are 
crucial in that they store 
the patterns that were 
uncovered (“learned”) in 
analyzing the data.

718	
Chapter 17  Business Analytics
The validation data results can be used as a representation of the model’s predictive power on 
future observations. The validation data misclassification rate, 0.0909, means that 9% of the card-
holders in the validation set were inaccurately classified using the trained MLP neural network. 
The Confusion Matrix report shows a contingency table of the actual and predicted values for the 
Upgraded variable. The Confusion Rates report is equal to the Confusion Matrix report, with the 
numbers divided by the row totals. Of the 11 cardholders in the validation set, 5 actually upgraded 
to the premium card and 6 did not. Of the 5 cardholders who actually did upgrade to the premium 
card, 5 or 1.0000 were correctly classified by the MLP neural network; of the 6 cardholders who 
actually did not upgrade to the premium card, 5 or 0.8333 were correctly classified by the MLP NN.
To illustrate a neural network analysis with a continuous numerical dependent variable, 
return to the Chapter 14 OmniPower scenario on page 571. Figure 17.14 presents the neural 
network results for predicting the sales of OmniPower bars. The r 2 statistic for the validation 
data is 0.9172. This indicates that the model is doing a good job in predicting the target in the 
validation data.
F i g u r e  1 7 . 1 4
Multilayer perceptron 
results for predicting the 
sales of OmniPower bars
Problems for Section 17.4
17.29  A hotel has designed a new system for room service 
­delivery of breakfast that allows the customer to select a spe-
cific delivery time. The file  Satisfaction  contains the difference 
­between the actual and requested delivery times recorded (a 
negative time means that the breakfast was delivered before the 
requested time) for 30 deliveries on a particular day along with 
whether the customer had previously stayed at the hotel.
a.	 Develop a neural net model to predict the probability that the 
customer will be satisfied, based on the delivery time difference 
and whether the customer had previously stayed at the hotel.
b.	 What conclusions can you reach about the probability that the 
customer will be satisfied?
17.30  A marketing manager wants to predict customers with risk 
of churning (switching their service contracts to another company) 
based on the number of calls the customer makes to the company 
call center and the number of visits the customer makes to the lo-
cal service center. Data from a random sample of 30 customers are 
organized and stored in  Churn .
a.	 Develop a neural net model to predict the probability of churn-
ing, based on the number of calls the customer makes to the 
company call center and the number of visits the customer 
makes to the local service center.
b.	 What conclusions can you reach about the probability of  
churning?
17.31  An automotive insurance company wants to predict which 
filed stolen vehicle claims are fraudulent, based on the number of 
claims submitted per year by the policy holder and whether the 
policy is a new policy, that is, is one year old or less (coded as 
1 = yes, 0 = no). Data from a random sample of 98 automotive 
insurance claims are organized and stored in  InsuranceFraud . 
(Data extracted from Gelp et al., “A Comparative Analysis of 
­Decision Trees vis-à-vis Other Computational Data Mining 
­Techniques in Automotive Insurance Fraud Detection,” Journal of 
Data Science, 10 (2012), pp. 537–561.)
a.	 Develop a neural net model to predict the probability of a 
fraudulent claim, based on the number of claims submitted per 
year by the policy holder and whether the policy is new.
b.	 What conclusions can you reach about the probability of a 
fraudulent claim?

	
17.5  Cluster Analysis	
719
17.32  Undergraduate students at Miami University in Oxford, 
Ohio, were surveyed in order to evaluate the effect of price on the 
purchase of a pizza from Pizza Hut. The students were asked to sup-
pose that they were going to have a large two-topping pizza delivered 
to their residence. Then they were asked to select from either Pizza 
Hut or another pizzeria of their choice. The price they would have 
to pay to get a Pizza Hut pizza differed from survey to survey. For 
example, some surveys used the price $11.49. Other prices investi-
gated were $8.49, $9.49, $10.49, $12.49, $13.49, and $14.49. The 
dependent variable for this study is whether or not a student will se-
lect Pizza Hut. The independent variables are the price of a Pizza Hut 
pizza and the gender of the student (1 = male, 0 = female). The 
results of these surveys are stored in  PizzaHut .
a.	 Develop a neural net model to predict the probability the stu-
dent will select Pizza Hut based on the price of a Pizza Hut 
pizza and the gender of the student.
b.	 What conclusions can you reach about the probability the stu-
dent will select Pizza Hut?
17.33  The business problem facing a consumer products company 
is to measure the effectiveness of different types of advertising me-
dia in the promotion of its products. Specifically, the company is 
interested in the effectiveness of radio advertising and newspaper 
advertising (including the cost of discount coupons). During a one-
month test period, data were collected from a sample of 22 cities 
with approximately equal populations. Each city is allocated a spe-
cific expenditure level for radio advertising and for newspaper ad-
vertising. The sales of the product (in thousands of dollars) and also 
the levels of media expenditure (in thousands of dollars) during the 
test month are recorded and stored in  Advertise .
a.	 Develop a neural net model to predict the sales of the product.
b.	 What conclusions can you reach about the sales of the product?
17.34  Starbucks Coffee Co. uses a data-based approach for im-
proving the quality and customer satisfaction of its products. When 
survey data indicated that Starbucks needed to improve its pack-
age sealing process, an experiment was conducted (data extracted 
from L. Johnson and S. Burrows, “For Starbucks, It’s in the Bag,” 
Quality Progress, March 2011, pp. 17–23) to determine the factors 
in the bag-sealing equipment that might be affecting the ease of 
opening the bag without tearing the inner liner of the bag. Among 
the factors that could affect the rating of the ability of the bag to 
resist tears were the viscosity, pressure, and plate gap on the bag-
sealing equipment. Data was collected on 19 bags in which the 
plate gap was varied. The results are stored in  Starbucks .
a.	 Develop a neural net model to predict the rating of the ability 
of the bag to resist tears.
b.	 What conclusions can you reach about the rating of the ability 
of the bag to resist tears?
17.35  In mining engineering, holes are often drilled through 
rock, using drill bits. As a drill hole gets deeper, additional rods 
are added to the drill bit to enable additional drilling to take place. 
It is expected that drilling time increases with depth. This in-
creased drilling time could be caused by several factors, including 
the mass of the drill rods that are strung together. The business 
problem relates to whether drilling is faster using dry drilling 
holes or wet drilling holes. Using dry drilling holes involves forc-
ing compressed air down the drill rods to flush the cuttings and 
drive the hammer. Using wet drilling holes involves forcing water 
rather than air down the hole. Data have been collected from a 
sample of 50 drill holes and contain measurements of the time to 
drill each additional 5 feet (in minutes), the depth (in feet), and 
whether the hole was a dry drilling hole or a wet drilling hole. The 
data are organized and stored in  Drill .
a.	 Develop a neural net model to predict the drilling time.
b.	 What conclusions can you reach about the drilling time?
17.36  The owner of a moving company typically has his most 
experienced manager predict the total number of labor hours that 
will be required to complete an upcoming move. This approach 
has proved useful in the past, but the owner has the business ob-
jective of developing a more accurate method of predicting labor 
hours. In a preliminary effort to provide a more accurate method, 
the owner has decided to use the number of cubic feet moved, the 
number of large pieces of furniture, and whether there is an eleva-
tor in the apartment building as the independent variables and has 
collected data for 36 moves in which the origin and destination 
were within the borough of Manhattan in New York City and the 
travel time was an insignificant portion of the hours worked. The 
data are organized and stored in  Moving .
a.	 Develop a neural net model to predict the labor hours.
b.	 What conclusions can you reach about the labor hours?
17.5  Cluster Analysis
Cluster analysis seeks to classify data into a sequence of groupings such that objects in each 
group are more alike other objects in their group than they are to objects found in other groups. 
Cluster analysis can be performed in several different ways; two of the most common tech-
niques are hierarchical clustering and k-means clustering.
In hierarchical clustering, the analysis starts with each object in its own cluster. Then, 
the two objects that are determined to be the closest to each other are merged into a single 
cluster. The merging of the two closest objects repeats until there remains only one cluster 
that includes all objects. In k-means clustering, the number of clusters (k) is set at the start of 
the process. Objects are then assigned to clusters in an iterative process that seeks to make the 
means of the k clusters as different as possible. During the iterative process, unlike hierarchical 
clustering, in which clusters once formed are never changed later in the process, objects may 
be reassigned to a different cluster later in the process.
To perform a cluster analysis, you must determine how to measure the distance between 
objects and how to measure the distance between clusters. The most common measure of 
Student Tip
Different clustering 
techniques can produce 
different results

720	
Chapter 17  Business Analytics
distance between objects used in cluster analysis is Euclidean distance that measures the 
distance between objects as the square root of the sum of the squared differences between 
objects over all r dimensions.
Euclidean Distance (cluster analysis)
	
dij = A a
r
k = 1
(Xik - Xjk)2	
(17.5)
where
dij = distance between object i and object j
Xik = value of object i in dimension k
Xjk = value of object j in dimension k
r = number of dimensions 
Among the measures of distance between clusters are complete linkage, single linkage, 
average linkage, and Ward’s minimum variance method. Complete linkage bases the distance 
between clusters on the maximum distance between objects in one cluster and another cluster. 
Single linkage bases the distance between clusters on the minimum distance between objects 
in one cluster and another cluster. Average linkage bases the distance between clusters on the 
mean distance between objects in one cluster and another cluster. Ward’s minimum variance 
method bases the distance between clusters on the sum of squares over all variables between 
objects in one cluster and another cluster.
To illustrate cluster analysis, suppose that you wanted to examine the similarities and dis-
similarities among various sports. You collect data from a survey on the perceptions of four 
attributes of nine sports (basketball, skiing, baseball, Ping-Pong, hockey, track and field, bowl-
ing, tennis, and U.S. football) and store the data in  Sports . You define the following seven-
point rating scales for the four variables that correspond to the four attributes:
 • Movement speed: 1 = fast paced to 7 = slow paced
 • Rules: 1 = complicated rules to 7 = simple rules
 • Team orientation: 1 = team sport to 7 = individual sport
 • Amount of contact: 1 = noncontact to 7 = contact
Figure 17.15 presents the results of a JMP complete linkage cluster analysis based on the 
mean score of each sport on each rating scale.
F i g u r e  1 7 . 1 5
JMP cluster analysis 
results for the different 
sports
Student Tip
Different distance 
­measures can produce 
different clustering 
results.

	
17.6  Multidimensional Scaling	
721
From examining either the tree diagram (which JMP calls a dendrogram) from left to right 
or the clustering history, observe that the first two sports that cluster together are track and 
field and bowling followed by basketball and hockey. Then skiing and tennis join together fol-
lowed by Ping-Pong merging with track and field and bowling. This process continues until all 
the sports are merged into one cluster.
When there are three clusters remaining, the sports in the three clusters are {basketball, 
hockey, U.S. football, and baseball}, {bowling, Ping-Pong, and track and field}, and {tennis 
and skiing}. The first cluster of {basketball, hockey, U.S. football, and baseball} appears to 
represent team sports. The second cluster of {bowling, Ping-Pong, and track and field} are 
slow moving individually contested sports. The third cluster {tennis and skiing} represents fast 
moving individually contested sports.
Problems for Section 17.5
17.37  Movie companies need to predict the gross receipts of in-
dividual movies once the movie has debuted. The following re-
sults, stored in  PotterMovies , are the first weekend gross, the 
U.S. gross, and the worldwide gross (in $millions) of the Harry 
Potter movies.
a.	 Perform a cluster analysis using the complete linkage method 
on the Harry Potter movies based on the first weekend gross, 
the U.S. gross, and the worldwide gross (in $millions).
b.	 What conclusions can you reach about which Harry Potter 
movies are most similar?
17.38  The file  Cereals  contains the calories, carbohydrates, and 
sugar, in grams, in one serving of seven breakfast cereals.
a.	 Perform a cluster analysis using the complete linkage method 
on the cereals based on the calories, carbohydrates, and sugar 
in grams.
b.	 What conclusions can you reach about which cereals are most 
similar?
17.39  The file  Protein  contains calorie and cholesterol informa-
tion for popular protein foods (fresh red meats, poultry, and fish) 
compiled by the U.S. Department of Agriculture.
a.	 Perform a cluster analysis using the complete linkage method on 
the protein foods based on the calories and cholesterol, in grams.
b.	 What conclusions can you reach about which protein foods are 
most similar?
c.	 Perform a cluster analysis using Ward’s method on the protein 
foods based on the calories and cholesterol in grams.
d.	 What conclusions can you reach about which protein foods are 
most similar?
e.	 Compare the results of (a) and (c). Are there any differences in 
your conclusions? Explain
17.40  A Pew Research Center survey found that social net-
working is popular in many nations around the world. The file  
 GlobalSocialMedia  contains the level of social media network-
ing (measured as the percent of individuals polled who use social 
networking sites) and the GDP at purchasing power parity (PPP) 
per capita for each of 25 selected countries. (Data extracted from 
“Global Digital Communication: Texting, Social Networking 
­Popular Worldwide,” Pew Research Center, bit.ly/sNjsmq.)
a.	 Perform a cluster analysis using the complete linkage method 
on the nations based on the level of social media networking 
(measured as the percent of individuals polled who use social 
networking sites) and the GDP at purchasing power parity 
(PPP) per capita.
b.	 What conclusions can you reach about which nations are most 
similar?
17.6  Multidimensional Scaling
Multidimensional scaling (MDS) visualizes objects in a two or more dimensional space, or 
map, with the goal of discovering patterns of similarities or dissimilarities among the objects. 
One challenge of MDS is to interpret the significance of the dimensions of the map and un-
derstand the reasons behind the distance between individual objects or apparent groups of ob-
jects. There are two main types of multidimensional scaling: metric multidimensional scaling 
that ­assumes that the distance between objects is ratio scaled and nonmetric multidimensional 
­scaling that assumes that the distance between objects is ordinal scaled.
To perform an MDS analysis, you must determine how to measure the distance between 
objects (the basis for placing objects in the map) and the number of map dimensions to be 
­interpreted. The most common measure of distance between objects used is the Euclidean 
­distance that measures the distance between objects as the square root of the sum of the 
squared differences between objects over all r dimensions.

722	
Chapter 17  Business Analytics
You obtain MDS results in a varying number of dimensions, usually from one dimension 
to five. The goal of MDS analysis is minimize the number of dimensions used to interpret the 
results while maximizing the goodness of fit of the results to the original data. The goodness of 
fit is measured by the stress statistic as defined in Equation (17.7).
Euclidean Distance (MDS)
	
dij = A a
r
k = 1
(Xik - Xjk)2	
(17.6)
where
dij = distance between object i and object j
Xik = value of object i in dimension k
Xjk = value of object j in dimension k
r = number of dimensions 
Stress Statistic
	
Stress =
f
a
m
i, j = 1
(dij - dnij)2
a
m
i, j = 1
(dij - d)2
	
(17.7)
where
dij = distance between objects i and j
dnij = the fitted regression value estimated by the multidimensional scaling  
algorithm from the original data for objects i and j
d = mean distance between objects
m = n(n - 1)>2
n = number of objects
While the smaller the stress statistic, the better the fit, there is no fixed rule about what consti-
tutes an acceptable value for the stress statistic. Because, as a general rule, the stress statistic 
decreases as the number of dimensions increases, using many dimensions can cause the stress 
statistic to approach 0 (a perfect fit), but at the cost of creating a map that could be as complex 
as the original data itself! Usually, a good rule is to increase dimensions as long as the stress 
statistic decreases substantially. In many cases, the decrease in the stress statistic begins to 
level off after the second or third dimension is considered. When this occurs, you can limit 
yourself to trying to interpret two or three dimensions. Attempting to interpret more than three 
dimensions in many cases can be extremely challenging.
To illustrate MDS analysis, suppose that you wanted to examine the similarities and dis-
similarities among various sports. You collect data from a survey on the perceptions of four 
attributes of nine sports (basketball, skiing, baseball, Ping-Pong, hockey, track and field, bowl-
ing, tennis, and U.S. football) and store the data in  Sports . You define the following seven-
point rating scales for the four variables that correspond to the four attributes:
 • Movement speed: 1 = fast paced to 7 = slow paced
 • Rules: 1 = complicated rules to 7 = simple rules
 • Team orientation: 1 = team sport to 7 = individual sport
 • Amount of contact: 1 = noncontact to 7 = contact
Figure 17.16 presents the results of the MDS analysis based on the mean score of each  
attribute for each sport.

	
17.6  Multidimensional Scaling	
723
The JMP results reveal a stress statistic of 0.3166 in one dimension, 0.1376 in two dimen-
sions, and 0.0788 in three dimensions. Because there is a large difference in the stress statistic 
between one and two dimensions but only a small difference in the stress statistic between two 
and three dimensions, you would choose to begin by interpreting the two-dimensional results 
shown in Figure 17.17.
F i g u r e  1 7 . 1 6
Multidimensional scaling 
stress results for nine 
sports
F i g u r e  1 7 . 1 7
Two-dimensional MDS 
map for perceptions 
about nine sports
To interpret a two-dimensional map, you look for points that appear close to each other as 
well as points that appear distant from each other. Although not the case with Figure 17.17, you 
may need to rotate the map in order to better interpret the dimensions. From Figure 17.17, observe 
that U.S. football, hockey, and basketball are close to each other. Tennis, Ping-Pong, and bowling 
are somewhat close to each other, and track and field and skiing are separate from the others.
To best interpret the dimensions separating the sports, observe that if you rotate the map 
clockwise 45 degrees, one axis appears to separate the team sports (U.S. football, hockey, 
basketball, and baseball) from the nonteam sports. The other axis appears to separate the fast-
paced contact sports (U.S. football, hockey, and basketball) from the slow-paced noncontact 
sports such as Ping-Pong, bowling, and tennis.
After interpreting the two-dimensional map, you can check to see if interpreting a three- 
dimensional map yields a better result. Interpreting a three-dimensional map is inherently harder 
as there are many more ways to examine and rotate the cube-like map. Figure 17.18 shows the 
original and rotated three-dimensional map. The rotated map seems to show team sports gather-
ing near the “ceiling,” while individual sports gather near the “floor.” As this does not enhance 
the interpretation, you would use the simpler two-dimensional map for your final analysis.

724	
Chapter 17  Business Analytics
F i g u r e  1 7 . 1 8
Original and rotated three-dimensional MDS maps for perceptions about nine sports
Problems for Section 17.6
17.41  Movie companies need to predict the gross receipts of indi-
vidual movies once the movie has debuted. The following results, 
stored in  PotterMovies , are the first weekend gross, the U.S. gross, 
and the worldwide gross (in $millions) of the Harry Potter movies.
a.	 Perform a multidimensional scaling analysis on the Harry 
­Potter movies based on the first weekend gross, the U.S. gross, 
and the worldwide gross (in $millions).
b.	 What conclusions can you reach about which Harry Potter 
movies are most similar?
17.42  The file  Cereals  contains the calories, carbohydrates, and 
sugar, in grams, in one serving of seven breakfast cereals.
a.	 Perform a multidimensional scaling analysis on the cereals 
based on the calories, carbohydrates, and sugar in grams.
b.	 What conclusions can you reach about which cereals are most 
similar?
17.43  The file  Protein  contains calorie and cholesterol informa-
tion for popular protein foods (fresh red meats, poultry, and fish) 
compiled by the U.S. Department of Agriculture.
a.	 Perform a multidimensional scaling analysis on the protein 
foods based on the calories and cholesterol, in grams.
b.	 What conclusions can you reach about which protein foods are 
most similar?
17.44  A Pew Research Center survey found that social net-
working is popular in many nations around the world. The file 
 GlobalSocialMedia  contains the level of social media network-
ing (measured as the percent of individuals polled who use social 
networking sites) and the GDP at purchasing power parity (PPP) 
per capita for each of 21 selected countries. (Data extracted from 
“Global Digital Communication: Texting, Social Networking 
Popular Worldwide,” Pew Research Center, bit.ly/sNjsmq.
a.	 Perform a multidimensional scaling analysis on the nations 
based on the level of social media networking (measured as the 
percent of individuals polled who use social networking sites) 
and the GDP at purchasing power parity (PPP) per capita.
b.	 What conclusions can you reach about which nations are most 
similar?
B
usiness analytics has various uses at WaldoLands. Descrip-
tive analytics help report the current status of the theme 
park and handle the logistics of an enterprise as complex as 
a small city. Predictive analytics can be used to anticipate de-
mands for attractions, food, or other services, as well as dis-
cover patterns about park patrons that could be used to enhance 
advertising efforts or generate additional revenues. Working 
with “big data” that tracks spending patterns and links theme 
park data to credit file 
­information, other predic-
tive methods could identify which groups of hotel guests are 
the theme park’s “best” customers and might be candidates for 
future targeted marketing efforts. Prescriptive analytics, not 
discussed in this chapter, could be used to help plan the stra-
tegic investments in the park and to anticipate the effects of a 
changing economic climate or new competition.
U s i n g  S tat i s t i c s
Finding the Right Lines at WaldoLands, 
Revisited

	
Key Terms	
725
Refere n c e s
	 1.	Breiman, L., J. Friedman, C. J. Stone, and R. A. Olshen. 
­Classification and Regression Trees. London: Chapman and 
Hall, 1984.
	 2.	Cox, T. F., and M. A. Cox. Multidimensional Scaling, Second 
ed. Boca Raton, FL: CRC Press, 2010.
	 3.	 Everitt, B. S., S. Landau, and M. Leese. Cluster Analysis, Fifth ed. 
New York: John Wiley, 2011.
	 4.	Few, S. Information Dashboard Design: Displaying Data 
for At-a-Glance Monitoring, Second ed. Burlingame, CA: 
­Analytics Press, 2013.
	 5.	Hakimpoor, H., K. Arshad, H. Tat, N. Khani, and M. 
­Rahmandoust. “Artificial Neural Network Application in 
Management.” World Applied Sciences Journal, 2011, 14(7): 
1008–1019.
	 6.	JMP Version 10. Cary, NC: SAS Institute. 2012.
	 7.	Lindoff, G., and M. Berry. Data Mining Techniques: For 
­Marketing, Sales, and Customer Relationship Management. 
Hoboken, NJ: Wiley Publishing, Inc., 2011.
	 8.	Loh, W. Y. “Fifty Years of Classification and Regression 
Trees.” International Statistical Review, 2013.
	 9.	Microsoft Excel 2013. Redmond, WA: Microsoft Corp., 2012.
	10.	Tableau Public Version 8. Seattle, WA: Tableau Software, 
2013.
	11.	Tufte, E. Beautiful Evidence. Cheshire, CT: Graphics Press, 
2006.
	12.	Turban, E., R. Sharda, J. Aronson, and D. King. “Networks 
for Data Mining, Online Chapter 6,” Business Intelligence: A 
Managerial Approach. Upper Saddle River, NJ: Prentice Hall, 
2008.
K ey Eq u at i o n s
Akaike Information Criterion (AIC)
AIC = 2k - 2 ln(L)	
(17.1)
Akaike Information Criterion corrected (AICc)
AICc = AIC + 2k(k + 1)
n - k - 1	
(17.2)
LogWorth
LogWorth = -log10( p@value)	
(17.3)
Hyperbolic Tangent Function
e2x - 1
e2x + 1	
(17.4)
Euclidean Distance (Cluster Analysis)
dij = A a
r
k = 1
(Xik - Xjk)2	
(17.5)
Euclidean Distance (MDS)
dij = A a
r
k = 1
(Xik - Xjk)2	
(17.6)
Stress Statistic
Stress =
f
a
m
i,  j = 1
(dij - dnij)2
a
m
i,  j = 1
(dij - d)2
	
(17.7)
K ey Te r ms
Akaike information criterion (AIC)  720
average linkage  720
bullet graph  705
business analytics  703
classification and regression trees  711
cluster analysis  719
complete linkage  720
dashboard  704
data mining  710
data discovery  706
descriptive analytics  703
drill-down  706
Euclidean distance  720
gauges  705
Gini impurity  711
hierarchical clustering  719
hyperbolic tangent function  716
k-means clustering  719
multidimensional scaling (MDS)  721
multilayer perceptrons (MLPs)  716
neutral networks  716
predictive analytics  703
prescriptive analytics  703
processing elements  716
single linkage  720
slicers  707
sparklines  704
stress statistic  722
training data  717
treemap  705
Ward’s minimum variance method  720

726	
Chapter 17  Business Analytics
C hec ki n g  Yo u r  U n d e r s ta nding
17.45  What is business analytics and what are its three 
­categories? For what type of business is real-time monitoring 
more apparent?
17.46  How does sparklines add value to dashboards?
17.47  What kind of data can be effectively presented using 
treemaps?
17.48  How are computations carried in the three layers of multi-
layer perceptrons?
17.49  Do gauge graphs and bullet graphs provide similar infor-
mation? If yes, which one should statisticians choose?
17.50  What is the difference between drill-down and slicers 
method of data discovery?
C ha pter  R e vi e w P r o b le ms
17.51  The file  DomesticBeer2  contains the number of calo-
ries per 12 ounces and number of carbohydrates (in grams) per 
12 ounces for a sample of 15 of the best-selling domestic beers in 
the United States. (Data extracted from www.beer100.com/beer-
calories.htm.)
a.	 Visually evaluate the number of calories per 12 ounces for each 
beer by constructing a bullet graph. For the purposes of com-
parison, consider calories below 100 as low, between 100 and 
160 as medium, and above 160 as high.
b.	 Visually evaluate the number of carbohydrates (in grams) per 
12 ounces for each beer by constructing a bullet graph. For 
the purposes of comparison, consider carbohydrates below 
10 grams as low, between 10 and 14 grams as medium, and 
above 14 grams as high.
c.	 What preliminary conclusions can you reach about the number 
of calories and carbohydrates in the beers?
d.	 Why would constructing sets of gauges for the calories and car-
bohydrates be a less effective means of visualizing these data?
17.52  The file  Currency2  contains the value of the Canadian 
dollar, English pound, and Euro for one U.S. dollar from 2002 to 
2012.
a.	 Construct sparklines for the value of the U.S. dollar in terms of 
the Canadian dollar, English pound, and Euro.
b.	 What conclusions can you reach about the value of the U.S. 
dollar in terms of the Canadian dollar, English pound, and Euro 
from 2002 to 2012.
17.53  The production of wine is a multibillion-dollar world-
wide industry. In an attempt to develop a model of wine quality 
as judged by wine experts, data were collected from red and white 
wine variants of Portuguese “Vinho Verde” wine. (Data extracted 
from P. Cortez, Cerdeira, A., Almeida, F., Matos, T., and Reis, J., 
“Modeling Wine Preferences by Data Mining from Physiochemi-
cal Properties,” Decision Support Systems, 47, 2009, pp. 547–553 
and bit.ly/9xKlEa.) The population of 6,497 wines is stored  
in  VinhoVerde Population .
a.	 Using half the data as the training sample and the other half of 
the data as the validation sample, develop a classification tree 
model to predict the probability that the wine is red. (Consider 
the entire set of variables in your analysis.)
b.	 What conclusions can you reach about the probability that the 
wine is red.
c.	 Repeat (a) using a neural net model.
d.	 Compare the results of (a) and (c).
17.54  Referring to the data in Problem 17.53,
a.	 Using half the data as the training sample and the other half 
of the data as the validation sample, develop a regression tree 
model to predict wine quality. (Consider the entire set of vari-
ables in your analysis.)
b.	 What conclusions can you reach about wine quality?
c.	 Repeat (a) using a neural net model.
d.	 Compare the results of (a) and (c).
17.55  The file  FTMBA  contains a sample of 2012 top-ranked 
full-time MBA programs. Variables included are mean starting 
salary upon graduation ($), percentage of students with job of-
fers within three months of graduation, program cost ($), and ­total 
number of students per program. (Data extracted from buswk 
.co/25d1ZC.)
a.	 Using all the data as the training sample, develop a regression 
tree model to predict the mean starting salary upon graduation
b.	 What conclusions can you reach about the mean starting salary 
upon graduation?
c.	 Using half the data as the training sample and the other half 
of the data as the validation sample, develop a regression tree 
model to predict the mean starting salary upon graduation.
d.	 What differences exist in the results of (a) and (c)?
e.	 Repeat (c) using a neural net model.
f.	 Compare the results of (c) and (e).
17.56  A specialist in baseball analytics is interested in determin-
ing which variables are important in predicting a team’s wins in a 
given baseball season. He has collected data in  BB2012  that in-
cludes the number of wins, ERA, saves, runs scored, hits allowed, 
walks allowed, and errors for the 2012 season.
a.	 Using half the data as the training sample and the other half 
of the data as the validation sample, develop a regression tree 
model to predict the number of wins.
b.	 What conclusions can you reach about the number of wins?
c.	 Repeat (a) using a neural net model.
d.	 Compare the results of (a) and (c).
17.57  Nassau County is located approximately 25 miles east 
of New York City. Data in  GlenCove  are from a sample of 30 
single-family homes located in Glen Cove. Variables included 
are the fair market value, land area of the property (acres), in-
terior size of the house (square feet), age (years), number of 
rooms, number of bathrooms, and number of cars that can be 
parked in the garage.
a.	 Using all the data as the training sample, develop a regression 
tree model to predict the fair market value.

	
Case for Chapter 17	
727
b.	 What conclusions can you reach about the fair market value?
c.	 Using half the data as the training sample and the other half 
of the data as the validation sample, develop a regression tree 
model to predict the fair market value.
d.	 What differences exist in the results of (a) and (c)?
e.	 Repeat (c) using a neural net model.
f.	 Compare the results of (c) and (e).
17.58  A market research study has been conducted by a travel 
website that specializes in restaurants with the business objective 
of determining which types of foods are perceived to be similar 
and which are perceived to be different. The following 10 types of 
foods were studied:
Japanese
Mandarin (Chinese)
Cantonese (Chinese)
American
Szechuan (Chinese)
Spanish
French
Italian
Mexican
Greek
The mean values of each food on the scales of
Bland (1) Spicy (7)
Light (1) Heavy (7)
Low calories (1) High calories (7)
are stored in  Foods .
a.	 Perform a cluster analysis on the types of foods.
b.	 Perform a multidimensional scaling analysis on the types of foods.
d.	 What conclusions can you reach about which types of foods are 
most similar?
17.59  A specialist in baseball analytics seeks to study which 
baseball teams were most similar in 2012. He has collected data 
in  BB2012  related to ERA, saves, runs scored, hits allowed, walks 
allowed, and errors for the 2012 season.
a.	 Perform a cluster analysis on the baseball teams.
b.	 Perform a multidimensional scaling analysis on the baseball 
teams.
c.	 What conclusions can you reach about which baseball teams 
were similar in 2012?
C a s e  f o r  C h a p t e r  1 7
The Mountain States Potato Company
On page 651, you studied the Mountain States Potato 
­Company which needed to determine why the percentage of 
solids in the filter cake that it sells was below its historical 
value. Construct a regression tree model for the percentage 
of solids in the filter cake and include the results in the re-
port that is to be submitted to the president of the company.

728	
Chapter 17  Business Analytics
Introduction
Chapter 17 discusses a number of statistical methods not included 
in or weakly supported by Microsoft Excel and Minitab and uses 
JMP, a separate statistical application, for its predictive analytics 
examples. For these reasons, this Software Guide presents instruc-
tions for using JMP as well as Tableau Public 8, a browser-based  
(“cloud-based”) application that specializes in interactive data vi-
sualization, in addition to Microsoft Excel. (There are no Minitab 
instructions for Chapter 17 methods, although some of the Chapter 
2 and Chapter 3 Minitab instructions can be adapted for descriptive 
analytics.) Use Table SG17.1 as a guide for choosing which pro-
gram to use for which method. As explained in the Short Takes 
for Chapter 17, you can download a free 30-day, full-featured trial 
version of JMP from the SAS Institute. You can also download for 
free upon registration Tableau Public 8, a “cloud-based” Internet 
application, from Tableau Software and the Gauge and Treemap 
Apps for Office for Excel 2013 or Office 365 for Microsoft Win-
dows from the Microsoft Office Store.
T a b l e  S G 1 7 . 1
Software Guide Instructions for Chapter 17 Methods
Method
Excel
Tableau
JMP
Sparklines
(1)
Gauges
Gauge App
Bullet Graph
•
•
Treemaps
Treemap App
•
•
Drill-down
•
•
Slicers
(2)
•
Classification and  
  Regression Trees
•
Neural Networks
•
Cluster Analysis
•
Multidimensional  
  Scaling
(3)
Notes:
(1)  Also available in Excel 2010.
(2)  Not available in OS X Excel 2011 or Office 365 for Mac.
(3)  Requires additional downloads of a JMP add-in  
(free upon registration) and R, the free, open-source 
statistical package. (See the Short Takes for Chapter 17 
for further details.)
This book does not include a complete orientation to either 
JMP or Tableau Public 8 (as was done for Excel and Minitab in 
the opening chapters). If you are a computing novice you may 
find using JMP or Tableau Public initially challenging. While both 
programs include extensive tutorials, mastery of basic computing 
skills is assumed and, in the case of JMP, the tutorials cover basic 
statistical methods and not the advanced analytics methods used in 
this chapter.
SG17.1  Descriptive Analytics
Sparklines
In-Depth Excel  Use Sparklines.
For example, to create the Figure 17.2 sparklines display, open to the 
DATA worksheet of the WL_WaitHistory workbook. In this work-
sheet, ride names are in column A and the historical wait times data by 
half-hours are in Columns C through W. Select cell range C3:W16 and:
	 1.	 Select Insert ➔ Line (in the Sparklines group).
	 2.	 In the Create Sparkines dialog box, enter B3:B16 as the Location 
Range and click OK.
	 3.	 Select cell B3 (or any other column B cell that contains a spar-
kline). Then select Design (in the Sparkline Tools) ➔ Axis.
	 4.	 In the Axis drop-down gallery, click Same for all Sparklines 
in the Vertical Axis Minimum Value Options. Click Axis a 
second time and click Same for all Sparklines in the Vertical 
Axis Maximum Value Options.
	 5.	 Check Last Point (in the Show group of the Design tab). 
In the Data worksheet, the row heights of rows 3 through 16 
have already been increased to 20 units. You can increase the row 
height further or widen column B by selecting the rows (or column), 
right-clicking and selecting Row Height (or Column Width) from 
the shortcut menu, entering a new value, and clicking OK.
Gauges
In-Depth Excel  Use the Gauge App (requires being signed in 
to the Microsoft Office Store).
For example, to construct a dashboard of six gauges, equivalent to 
the one shown in Figure 17.3 on page 705, open to the TopSixDATA 
worksheet of the WL_WaitData workbook and:
	 1.	 Select Insert ➔ Apps for Office and click Gauge in the Apps 
for Office gallery. If selecting Gauge from the Apps for Office 
dialog box (and not from the Recently Used Apps list), also 
click Insert.
	 2.	 When the RE:VISION Gauge panel appears, click Insert 
Gauges.
	 3.	 In the Select Data dialog box, enter C2:C7 and click OK.
	 4.	 Drag and resize the panel of six gauges until they can be easily 
read.
By default, gauges constructed by the Gauge App have a 
minimum value of 0 and a maximum of 100 and include three 
gauge zones: green, defined as values between 0 and 25; yel-
low, defined as values between 40 and 60, and red, defined as 
values between 75 and 100. These default values create a gauge 
that effectively has five zones: green, white (values from 25 to 
40), yellow, white again (values from 60 to 75), and red. By 
adjusting the scope of the yellow range, you can create gauges 
that contain three or four zones. The Figure 17.3 set of gauges 
­contains five zones that ­differ in scope from the default ranges. 
C h a p t e r  1 7  S o f t w a r e  G u i d e

	
Chapter 17 Software Guide	
729
To adjust the zones, click the circular gear icon at the top of 
panel. In the Settings panel:
	 5.	 Enter 45 as the Yellow Range Min Value.
	 6.	 Enter 65 as the Yellow Range Max Value.
	 7.	 Enter 85 as the Red Range Min Value.
	 8.	 Click the back icon (left-facing arrow icon) at the top of the 
Settings panel.
If you use an Excel version older than Excel 2011 (or a newer 
version not signed into the Microsoft Office Store), open to the 
Gauge worksheet of the GaugeBullet workbook to view a non-
modifiable version of the set of gauges that steps 1 through 8 
construct. Also note that various third-party vendors offer Excel 
add-ins that can add gauges to current and older versions of Excel. 
For example, the Figure 17.3 set of gauges are adapted and en-
hanced versions of gauges that the free version of the BeGraphic 
add-in (begraphic.com) can create.
Gauges can also be simulated in any Excel version by overlay-
ing a pie chart on a donut chart (a chart type not discussed in this 
book). In the pie chart, all slices of the pie chart are set to invisible 
except for the slice that represents the needle, and the background 
of the pie chart is also set to be invisible. In the donut chart, the 
lower half of the donut is made invisible to form an 180-degree arc 
that serves as the legend for the zones of the gauge. The pie is then 
moved over and scaled to the donut chart to complete the gauge. 
Open to the COMPUTE worksheet of the RadialGauge work-
book to see an illustration of this method and to experiment with the 
user-changeable values in the tinted cells in columns B and C.
Bullet Graph
In-Depth Excel  Use the BulletGraph worksheet of the 
GaugeBullet workbook as a model for simulating a bullet graph.
To construct a simulated bullet graph in Excel, you create a 
bar chart of the variable being graphed with a transparent back-
ground and overlay this chart on a bar chart that displays the col-
ored zones. For example, to construct a chart similar to the bullet 
graph shown in Figure 17.3 on page 705, open to the waitDATA 
worksheet of the WL_WaitData workbook and:
	 1.	 Select cell range B1:C15.
	 2.	 Select Insert, then the bar chart icon, and select the first 2-D 
Bar galley item (Clustered Bar).
	 3.	 In the newly constructed bar chart, turn off the gridlines (and, 
in Excel 2010, legend), by using the instructions in Appendix 
Section B.6.
	 4.	 Right-click in the whitespace to the right of the chart title 
(upper right corner of chart in Excel 2010) and click Format 
Chart Area in the shortcut menu.
	 5.	 In the Fill part of the Format Chart Area pane (dialog box in 
Excel 2010), click No fill. (In Excel 2010, also click Close.) 
The background of the chart becomes transparent. In Excel 
2010, also right-click the plot area, click Format Plot Area, 
and in the Fill part of the Format Plot Area dialog box, click 
No fill and then click Close.
Next, construct the bar chart that will serve as the colored zones 
for the bullet graph.
	 6.	 In the cell range D2:D6, enter the values 25, 20, 20, 20, and 
15, to define the five zones of the Figure 17.3 bullet graph. 
Then select this edited cell range D2:D6.
	  7.	 Select Insert, then the bar chart icon, and select the second 
2-D Bar galley item (Stacked Bar).
	  8.	 In the newly constructed bar chart, turn off the gridlines (and, 
in Excel 2010, legend) by using the instructions in ­Appendix 
Section B.6.
	  9.	 Right-click in the whitespace to the right of the chart title 
(upper right corner of chart in Excel 2010) and click Select 
Data in the shortcut menu.
	10.	 In the Select Data Source dialog box, click Switch Row/
Column and then click OK. A chart of five simple bars be-
comes a chart of one stacked bar with five parts.
	11.	 Right-click the one stacked bar and click Format Data 
­Series in the shortcut menu. In Excel 2013, click the icon 
that looks like a small histogram; otherwise click Series 
Options in the left pane. In the Series Options part of the 
Format Data Series pane (dialog box in Excel 2010), change 
Gap Width to 0%. (In Excel 2010, also click Close.)
	12.	 Change the coloring of the stacked bars. In Excel 2013, ­select 
Design ➔ Change Colors and in the gallery click one of the 
color spectrums. In Excel 2010, select Design and click one 
of the items in the Chart Styles group. Be sure to choose a 
set of colors that does not include the color used for the bars 
in the bar chart you constructed using steps 1 through 5.
	13.	 Right-click the horizontal chart axis and click Format Axis 
in the shortcut menu.
	14.	 In the Axis Options of the Format Axis pane (dialog box in 
Excel 2010), enter 100 as the Maximum. In Excel 2010, 
first click Fixed in the Maximum line first, then enter 100, 
and then click Close.
	15.	 Adjust the size of the chart, as necessary, by clicking a cor-
ner of the bar chart frame and then dragging that corner to 
resize the chart.
	16.	 Right-click the chart border and select Send to Back ➔ 
Send to Back in the shortcut menu.
	17.	 Drag the bar chart with the transparent background over the 
stacked bar chart and adjust so that the zeroes on the hori-
zontal axis of both charts coincide. Then adjust the width of 
that bar chart so that all other horizontal axis numbers that 
the two charts share coincide.
For other problems, you need to identify the maximum value and 
enter the proper set of values in new column in order to correct the 
correct stacked bar chart that serves to display the zones for the 
bullet graph.
Tableau Public  Modify the bar chart created automatically 
from the data.
For example, to construct the Figure 17.3 bullet graph of wait 
times, select File ➔ New and:
	  1.	 Select Data ➔ Connect to Data. In the Connect to Data 
panel, click Microsoft Excel.
	  2.	 In the Open dialog box, navigate to the location of the  
WL_WaitData Excel workbook, select that file, and then 
click Open.
	  3.	 In the Excel Workbook Connection dialog box, click wait-
DATA in the Step 2 list box and click OK.

730	
Chapter 17  Business Analytics
Tableau Public displays a Data pane similar to one shown below 
next to an empty worksheet.
From the Data pane:
	  4.	 Drag Ride in the Dimensions group and drop it in the Rows 
box of the new worksheet area.
	  5.	 Drag Wait in the Measures group and drop it in the Col-
umns box. (Wait changes to SUM(Wait).)
Tableau constructs a simple bar chart of wait times. To change this 
chart into a bullet graph:
	  6.	 Right-click the Wait axis, and in the shortcut menu click 
Add Reference Line.
In the Add Reference Line dialog box (shown below):
	  7.	 Click Distribution.
	  8.	 Click Per Cell as the Scope.
	  9.	 Click the Value entry to reveal the Value entries (shown as 
inset below).
	10.	 In the Value entries, click Percentages, enter 25,45,65,85 as 
the Percentages, select Constant from the drop-down list, 
and enter 100 as the Percent of value. (Although the drop-
down list appears after the Percent of entry, select Constant 
before entering the value 100.)
	11.	 In the Label drop-down list, select None.
	12.	 In the Fill drop-down list, select Stoplight.
	13.	 Check Fill Above, Fill Below, and Reverse.
	14.	 Click OK.
You can adjust the formatting of axis labels or the chart title by 
right-clicking over the element and clicking Format in the short-
cut menu and making adjustments in the Format pane. The Format 
pane appears over the same space used to display the Data pane. 
To go back to the Data pane, close the Format pane.
Treemap
In-Depth Excel  Use the Treemap app (requires being signed 
in to the Microsoft Office Store).
For example, to construct a treemap of WaldoLands social media 
comments grouped by “Land,” shown at left in Figure 17.4, on 
page 705, open to the DATA worksheet of the WL_SocialData 
workbook and:
	 1.	 Select Insert ➔ Apps for Office and click Treemap in the 
Apps for Office gallery. If selecting Treemap from the Apps 
for Office dialog box (and not from the Recently Used Apps 
list), also click Insert.
In the Treemap panel:
	 2.	 Click Name list and in the Select Data dialog box enter 
A2:B15 and click OK. A treemap begins to take shape in the 
Treemap panel.
	 3.	 Click Size and in the Select Data dialog box enter C2:C15 and 
click OK.
	 4.	 Click Color (under Size), and in the Select Data dialog box 
enter D2:D15 and click OK.
If the treemap displayed does not use the red-to-blue spectrum, 
click the spectrum icon (under Title) and click the red-to-blue 
(third) spectrum.
To construct the treemap shown at right in Figure 17.4, re-
peat steps 1 through 4, entering B2:B7 in step 2, C2:C7 in step 
3, and D2:D7 in step 4. If you use an Excel version older than 
Excel 2011 (or a newer version not signed into the Microsoft 
­Office Store), open to the Nested and Simple worksheets of 
the Treemap workbook to view nonmodifiable versions of the 
Figure 17.4 treemaps.
JMP  Use Treemap.
For example, to construct a treemap of WaldoLands social media 
comments grouped by “Land,” similar to the one shown at left in 
Figure 17.4 on page 705, open the WL_SocialData Excel work-
book. Select File ➔ Open. In the Open Data File dialog box, nav-
igate to the location of the file, select the file, and then click Open. 
In the DATA - JMP window:
	 1.	 Select Graph ➔ Tree Map.
In the Tree Map - JMP dialog box (shown below):

	
Chapter 17 Software Guide	
731
	 2.	 Drag Land to the Categories box.
	 3.	 Drag Ride to the Categories box.
	 4.	 Drag Comments to the Sizes box.
	 5.	 Drag Rating to the Coloring box.
	 6.	 Click OK.
Adjust the size of the treemap as necessary to clearly display the 
labels. By default, JMP colors the treemap using blue for the unfa-
vorable ratings and red for the favorable ratings, the inverse of the 
colorings in Figure 17.4. To change the color spectrum, click the  
drop-down button that is part of the chart title, click Color Theme, 
and then click one of the submenu choices. Note that there is no 
predefined spectrum that uses red for the lowest values.
To construct a treemap similar to the one shown at right in 
Figure 17.4, in the TopSix DATA - JMP window, repeat steps 1 
through 6, skipping step 2.
Tableau Public  Use the treemap feature.
For example, to construct a treemap of WaldoLands social media 
comments grouped by “Land,” similar to the one shown at left in 
Figure 17.4 on page 705, select File ➔ New and:
	  1.	 Select Data ➔ Connect to Data. In the Connect to Data 
panel, click Microsoft Excel.
	  2.	 In the Open dialog box, navigate to the location of the WL_
SocialData Excel workbook, select that file, and then click 
Open.
	  3.	 In the Excel Workbook Connection dialog box, click DATA 
in the Step 2 list box and click OK.
Tableau Public displays a Data pane next to an empty  
worksheet. From the Data pane:
	  4.	 Drag Comments in the Measures group and drop it on the 
Size icon in the Marks area.
	  5.	 Drag Rating in the Measures group and drop it on the Color 
icon in the Marks area.
	  6.	 Drag Land in the Dimensions group and drop it on the 
­Label icon in the Marks area of the new worksheet area.
	  7.	 Drag Ride in the Dimensions group and drop it on the Label 
icon in the Marks area.
Tableau Public constructs a treemap and updates the Marks area 
(called the “Marks card” by Tableau) and adds a SUM(rating) area 
similar to these areas:
To change the red-to-green spectrum to colors less likely to be 
confused:
	  8.	 Double-Click the SUM(Rating) color spectrum.
In the Edit Colors [(Rating)] dialog box:
	  9.	 Select Red-Blue Diverging from the Palette drop-down list.
	10.	 Click OK.
Change the dimensions of the treemap to allow all labels to be 
displayed. To change a dimension, move the mouse pointer over 
an edge and then drag the edge to adjust. To adjust the formatting 
of the labels, select Format ➔ Font, and in the Font pane change 
the font attributes for Pane in the default group.
Tableau Public allows the interactive collapse of a level of 
data. For example, to collapse the rides into their lands, right-
click Ride in the Marks area, and in the shortcut menu click 
­Attribute. The treemap changes to a three area map, one for 
each of WaldoLands’ three lands. To restore the original map, 
right-click Ride in the Marks area, and in the shortcut menu 
click Dimension.
There are two ways to construct the treemap shown at 
right in Figure 17.4. You can repeat steps 1 through 10, click-
ing ­Top6DATA in step 3 and skipping step 4, or you can use 
the filter function to alter the treemap that is produced by the 
original steps 1 through 10. To use the filter function, first­ 
­follow steps 1 through 10 above. Right-click Ride in the 
Marks area, and in the shortcut menu click Filter. In the Gen-
eral tab of the Filter [Ride] dialog box, clear the check boxes 
for the rides that are not part of the top six attractions and 
then click OK.
Data Discovery
In-Depth Excel  Use PivotTables and Slicer.
For drill-down, construct a PivotTable, adapting the Section 
EG2.6 “Multidimensional Contingency Tables” as necessary. 
(While those instructions discuss using only categorical variables, 
the same set of instructions can be used for a mix of categorical 
and numerical variables.) Then click on the “ + ” buttons that 
precede the row categories to expand the table one-level deeper. 
­Figure 17.5 on page 706 illustrates this operation.
To reveal the data for variables not initially included in the 
initial PivotTable, as illustrated by Figure 17.6 on page 707, in-
clude the cell range of those variables when first defining the 
­PivotTable and then later click on a cell that contains the value of 
interest as also discussed on page 123.
To construct the Figure 17.6 slicer dashboard, first construct 
a PivotTable using the In-Depth Excel “The Contingency Table” 
instructions on page 114. Click cell A3 in the PivotTable and:
	 1.	 Select Insert ➔ Slicer.
In the Insert Slicers dialog box (shown on page 732):
	 2.	 Check Market Cap, Type, Expense Ratio, and Star Rating.
	 3.	 Click OK.

732	
Chapter 17  Business Analytics
	 4.	 In the worksheet, drag the slicers to reposition them. If neces-
sary, resize slicer panels as you would resize a window.
Click the value buttons in the slicers to explore the data. For ex-
ample, to create the display shown at left in Figure 17.9 on page 
708 that answers the question about the attributes of the fund with 
the highest expense ratio, click 6.97 in the Expense Ratio slicer.
When you click a value button, the icon at the top right of the 
slicer changes to include a red X (as can be seen in both Expense Ratio 
slicers in Figure 17.9). Click this icon to reset the slicer. When you click 
a value button, value buttons in other slicers may become dimmed (as 
have buttons such as Value, Mid-Cap, Small, Five, Four, One, and Two 
in Figure 17.9). Dimmed value buttons represent values that are not 
found in the currently “sliced” data, and if you click a dimmed value 
button, the PivotTable will be empty and show no values.
JMP  Use Graph Builder.
Summary charts can be used as the basis for both drill-down and 
slicer-like operations in JMP. For example, to create a drill-down sim-
ilar to the example that Figure 17.6 illustrates, open the Retirement 
Funds Excel workbook. Select File ➔ Open. In the Open Data File 
dialog box, navigate to the location of the file, select the file, and then 
click Open. In the DATA - JMP window, select Analyze ➔ Distribu-
tion and in the Distribution - JMP dialog box (shown below):
	 1.	 Drag Type to the Y, Columns box.
	 2.	 Drag Risk to the By box.
	 3.	 Click OK.
JMP constructs a panel of three histograms that show the distri-
bution of growth and value funds for each type of risk (low, average, 
and high). Double-click a histogram bar to display all of the variables 
for funds that have the bar’s combination of type and risk values.
To create a slicer-like display, in the DATA - JMP window, 
again select Analyze ➔ Distribution and in the Distribution - JMP  
dialog box:
	 1.	 Drag Type to the Y, Columns box.
	 2.	 Drag Market Cap to the Y, Columns box.
	 3.	 Drag Star Rating to the Y, Columns box
	 4.	 Drag Expense Ratio to the Y, Columns box
	 5.	 Click OK.
JMP constructs a panel that contains four histograms, one for 
each variable. (The display for the numerical variable Expense ­Ratio 
also includes a box-and-whisker plot.) Click a specific bar to display 
the proportion of the bars in the other histograms related to that bar’s 
value. For example, when you click the Value bar in the Type histo-
gram, you can see that among value funds large is the most frequent 
market cap and that three stars is the most frequent star rating.
SG17.2  Predictive Analytics
There are no software guide instructions for this section.
SG17.3  Classification and  
Regression Trees
Classification Tree
JMP  Use Partition.
For example, to perform the Figure 17.10 classification tree analy-
sis for predicting the proportion of credit card holders who would 
upgrade to a premium card, open the CardStudy Excel workbook. 
Select File ➔ Open. In the Open Data File dialog box, navigate to 
the location of the file, select the file, and then click Open. Because 
the Upgraded and Extra Cards variables have been coded with val-
ues 0 and 1, JMP mistakes these categorical variables as numerical 
variables (and would perform an incorrect analysis on the data).
To change the variable type of Extra Cards to categorical, 
right-click the Extra Cards column and click Column Info in the 
shortcut menu. In the Extra Cards - JMP dialog box (shown be-
low), select Character from the Data Type drop-down list and 
click OK. (The Modeling Type changes from Continuous to Nom-
inal.) To change Upgraded to a categorical variable, right-click the 
Upgraded column, click Column Info, and select Character as 
the Data Type and click OK.
Verify these operations by examining the icons that appear in 
the Columns panel to the left of the JMP worksheet. If these icons, 
which JMP calls modeling type icons, appear as 
shown at right, the Upgraded and Extra Cards 
variables have been properly identified as cat-
egorical variables (with a nominal scale).

	
Chapter 17 Software Guide	
733
While at this point, you could begin your analysis, the results 
will use the values 0 and 1, whose meanings may be misinter-
preted, for the two categorical variables. Better would be results 
that use the easily understood values No and Yes. To recode Extra 
Cards in this way, select the Extra Cards column and:
	 1.	 Select Cols ➔ Recode.
	 2.	 In the Recode - JMP dialog box (shown below), enter No as 
the New Value for 0, enter Yes as the New Value for 1, and 
click OK.
To recode Upgraded, select the Upgraded column and repeat the 
previous steps 1 and 2.
With the data properly prepared, in the Data - JMP window, 
select Analyze ➔ Modeling ➔ Partition. In the Partition dialog 
box (shown below):
	 3.	 Drag Upgraded to the Y, Response box.
	 4.	 Drag Purchases to the X, Factor box.
	 5.	 Drag Extra Cards to the X, Factor box.
	 6.	 Click OK.
In the new DATA - Partition of Upgraded - JMP dialog box:
	 7.	 Click the drop-down button to the left of the diagram title and 
click Split Best. Repeat this step until clicking Split Best no 
longer has any effect on the tree diagram.
	 8.	 If the contents of the diagram do not match Figure 17.10, 
click the drop-down button to the left and then select Display 
­Options. To match Figure 17.10, all choices on the Display 
Options submenu should be checked, except the last two 
choices, Show Split Candidates and Sort Split Candidates. 
If necessary, click unchecked choices, one at a time, until all 
but the last choice are checked.
	 9.	 Click Color Points. The Color Points button disappears and 
the “No” points in the plot appear in red and the “Yes” points 
appear in blue.
At any point, click Prune to remove the last split operation. To 
enhance the display of the points in the plot, right-click a point, 
then click Marker Size from the shortcut menu and click one of 
the size choices.
Regression Tree
JMP  Use Partition.
For example, to perform the Figure 17.11 regression tree analysis 
for predicting the sales of OmniPower bars, open the OmniPower 
Excel workbook. Select File ➔ Open. In the Open Data File dia-
log box, navigate to the location of the file, select the file, and then 
click Open. Because JMP properly identifies Sales, Price, and 
Promotion as numerical variables, there is no need to change vari-
able types as is done in the preceding classification tree example. 
In the DATA - JMP window, select Analyze ➔ Modeling ➔ Par-
tition. In the Partition dialog box:
	 1.	 Drag Sales to the Y, Response box.
	 2.	 Drag Price to the X, Factor box.
	 3.	 Drag Promotion to the X, Factor box.
	 4.	 Click OK.
In the new DATA - Partition of Upgraded - JMP dialog box:
	 5.	 Click the drop-down button to the left of the title Partition for 
Sales and click Split Best. Repeat this step until clicking Split 
Best no longer has any effect on the tree diagram.
At any point, click Prune to remove the last split operation. To 
enhance the display of the points in the plot, right-click a point, 
then click Marker Size from the shortcut menu and click one of 
the size choices.
SG17.4  Neural Networks
JMP  Use Neural.
For example, to perform the Figure 17.13 MLP analysis for clas-
sifying credit card holders who would be likely to upgrade to a 
premium card, open the CardStudy Excel workbook. Select File 
➔ Open. In the Open Data File dialog box, navigate to the loca-
tion of the file, select the file, and then click Open. As discussed 
in Section SG17.3, because the Upgraded and Extra Cards vari-
ables have been coded with values 0 and 1, JMP mistakes these 
categorical variables for numerical variables (and would perform 
an incorrect analysis on the data).
First use the instructions in Section SG17.3 to change the data 
type (to Character) for these two variables. Then use the instruc-
tions in that section for recoding the values of the two variables as 
No and Yes. With the data properly prepared, select Analyze ➔ 
Modeling ➔ Neural. In the Neural JMP dialog box (similar to the 
Partition - JMP dialog box shown in Section SG17.3):
	 1.	 Drag Upgraded to the Y, Response box.
	 2.	 Drag Purchases to the X, Factor box.
	 3.	 Drag Extra Cards to the X, Factor box.
	 4.	 Click OK.
In the next dialog box:
	 5.	 Leave Holdback as the Validation Method and 0.3333 as the 
Holdback Proportion.
	 6.	 Enter 2 as the number of Hidden Nodes.
	 7.	 Click Go.

734	
Chapter 17  Business Analytics
In the DATA - Neural of Upgraded - JMP window:
	 8.	 Click the triangle icons to the left of the two Confusion 
­Matrix titles and the two Confusion Rates titles to display 
these tables.
	 9.	 Click the drop-down button to the left of the title Model 
NTanH(2) and click Show Estimates.
Because the initial weights for the model are chosen at random, 
the actual results will almost certainly differ from the Figure 17.13 
results. If the validation data misclassification rate is too high, you 
may generate additional models. To generate an additional model, 
click the triangle icon to the left of the title Model Launch and 
then click Go. To eliminate a model generated, right-click on the 
model’s Model NTanH(2) title and click Remove Fit in the short-
cut menu.
To perform the Figure 17.14 MLP analysis for predicting the 
sales of OmniPower bars, open the OmniPower Excel workbook. 
Select File ➔ Open. In the Open Data File dialog box, navigate 
to the location of the file, select the file, and then click Open. 
Because JMP properly identifies Sales, Price, and Promotion as 
numerical variables, there is no need to change variable types 
as is done in the preceding example. In the DATA - JMP win-
dow, select Analyze ➔ Modeling ➔ Neural. In the Neural JMP  
dialog box:
	 1.	 Drag Sales to the Y, Response box.
	 2.	 Drag Price to the X, Factor box.
	 3.	 Drag Promotion to the X, Factor box.
	 4.	 Click OK.
In the next dialog box:
	 5.	 Leave Holdback as the Validation Method and 0.3333 as the 
Holdback Proportion.
	 6.	 Enter 3 as the number of Hidden Nodes.
	 7.	 Click Go.
In the DATA - Neural of Sales - JMP window:
	 8.	 Click the drop-down button to the left of the title Model 
NTanH(3) and click Show Estimates.
As noted in the first example, because the initial weights for the 
model are chosen at random, the actual results will almost cer-
tainly differ from the Figure 17.14 results. If necessary, you can 
generate additional models by clicking Go in Model Launch as 
explained in the previous example.
SG17.5  Cluster Analysis
JMP 10  Use Cluster.
For example, to perform the Figure 17.15 cluster analysis for the 
different sports, open the Sports Excel workbook. Select File ➔ 
Open. In the Open Data File dialog box, navigate to the location of 
the file, select the file, and then click Open. (JMP properly identi-
fies all five variables that comprise the file.) In the DATA - JMP 
window, select Analyze ➔ Multivariate Methods ➔ Cluster. In 
the Cluster dialog box (shown in the right column):
	 1.	 Drag Movement Speed to the Y, Columns box.
	 2.	 Drag Rules to the Y, Columns box.
	 3.	 Drag Team Oriented to the Y, Columns box.
	 4.	 Drag Amount of Contact to the Y, Columns box.
	 5.	 Drag Sport to the Label box.
	 6.	 In the Options drop-down list, click Hierarchical and click 
Complete (in the Method group).
	 7.	 Click OK.
In the DATA - Hierarchical Cluster - JMP dialog box, click the tri-
angle icon to the left of the title Clustering History to reveal how 
the clustering was done.
SG17.6  Multidimensional Scaling
JMP  Use the Multidimensional Scaling add-in and the R statisti-
cal package
For example, to perform the Figure 17.16 multidimensional scal-
ing analysis based on the mean score of each attribute for each 
sport, open the Sports Excel workbook. Select File ➔ Open. In 
the Open Data File dialog box, navigate to the location of the file, 
select the file, and then click Open. (JMP properly identifies all 
five variables that comprise the file.) In the DATA - JMP window, 
click Add-Ins ➔ Multidimensional Scaling. In the General tab 
of the Multidimensional Scaling - JMP dialog box:
	 1.	 Click R project.
	 2.	 Drag Movement Speed to the Y, Columns box.
	 3.	 Drag Rules to the Y, Columns box.
	 4.	 Drag Team Oriented to the Y, Columns box.
	 5.	 Drag Sport to the Label Column box.
	 6.	 Click Run.
The add-in displays an overlay plot in a new NMDS Fit Output 
- JMP window. To view the results for a particular number of 
dimensions, click the plot point for that number of dimensions. 
Then click Display MDS Results for Selected Dimension. To 
view the table of stress values, click the drop-down list button 
to the left of the title Overlay Plot and click Script, then Data 
Table Window.
The add-in also works with SAS. To use SAS, click SAS 
Project in step 1.

735
Chapter
A Roadmap for 
Analyzing Data
18
contents
18.1  Analyzing Numerical 
Variables
Describing the Characteristics of  
a Numerical Variable
Reaching Conclusions About 
the Population Mean and/or 
Standard Deviation
Determining Whether the Mean 
and/or Standard Deviation 
Differs Depending on the Group
Determining Which Factors Affect  
the Value of a Variable
Predicting the Value of a Variable 
Based on the Values of Other 
Variables
Determining Whether the Values  
of a Variable Are Stable over Time
18.2  Analyzing Categorical Variables
Describing the Proportion of Items 
of Interest in Each Category
Reaching Conclusions About the 
Proportion of Items of Interest
Determining Whether the 
Proportion of Items of Interest 
Differs Depending on the Group
Predicting the Proportion of Items 
of Interest Based on the Values 
of Other Variables
Determining Whether the 
Proportion of Items of Interest Is 
Stable over Time
Using Statistics: Mounting 
Future Analyses, Revisited
U s i n g  S tat i s t i c s
Mounting Future Analyses
Learning business statistics is a lot like climbing a mountain. At first, it may 
seem intimidating, or even overwhelming, but over time you learn techniques 
that help make the task much more manageable. In Section GS.1, you learned 
how the DCOVA framework can make the big task of applying statistics to 
­business problems more manageable. After learning methods in early chapters to 
Define, Collect, and Organize data, you have spent most of your time studying 
ways to Visualize and Analyze data.
Determining what methods to use to analyze data may have seemed 
­straightforward when doing homework problems from a particular chapter, but what 
approach will you take when you find yourself in new situations, needing to analyze 
data for ­another course or to 
help solve a problem in a real 
business setting? ­After all, when 
you solved a problem from a 
chapter on multiple regression, 
you “knew” that ­multiple regres-
sion methods would be part of 
your analysis. In new situations, 
you might wonder whether you 
should use multiple regression—
or whether using simple linear 
regression would be better—or 
whether any type of regression 
would be ­appropriate. You also 
might wonder if you should use 
a combination of methods from 
several ­different chapters to help 
solve the problems you face.
The question for you be-
comes: How can you apply the 
statistical methods you have 
learned to new situations that 
require you to analyze data?
Courtesy of Sharyn Rosenberg
Objectives
Identify the questions to ask when 
choosing which statistical methods 
to use to conduct data analysis
Generate rules for applying statistics 
in future studies and analyses

736	
Chapter 18  A Roadmap for Analyzing Data
R
eviewing Table 18.1, which contains a summary of the contents of this book, arranged 
by data analysis task, would be a good starting point for answering the question posed 
in the Using Statistics scenario.
T a b l e  1 8 . 1
Commonly Used 
Data Analysis Tasks 
Discussed in This Book
Describing a Group or Several Groups
For Numerical Variables:
Ordered array, stem-and-leaf display, frequency distribution, relative frequency distribution,  
  percentage distribution, cumulative percentage distribution, histogram, polygon, cumulative  
  percentage polygon (Sections 2.2 and 2.4)
Boxplot (Section 3.3)
Normal probability plot (Section 6.3)
Bullet graph, gauge, treemap (Section 17.1)
Mean, median, mode, quartiles, geometric mean, range, interquartile range, standard deviation,  
  variance, coefficient of variation, skewness, kurtosis (Sections 3.1, 3.2, and 3.3)
Index numbers (online Section 16.8)
For Categorical Variables:
Summary table, bar chart, pie chart, Pareto chart (Sections 2.1 and 2.3)
Contingency tables and multidimensional tables (Sections 2.1 and 2.6)
Making Inferences About One Group
For Numerical Variables:
Confidence interval estimate of the mean (Sections 8.1 and 8.2)
t test for the mean (Section 9.2)
Chi-square test for a variance or standard deviation (online Section 12.7)
For Categorical Variables:
Confidence interval estimate of the proportion (Section 8.3)
Z test for the proportion (Section 9.4)
Comparing Two Groups
For Numerical Variables:
Tests for the difference in the means of two independent populations (Section 10.1)
Wilcoxon rank sum test (Section 12.4)
Paired t test (Section 10.2)
F test for the difference between two variances (Section 10.4)
Wilcoxon signed rank test (online Section 12.8)
For Categorical Variables:
Z test for the difference between two proportions (Section 10.3)
Chi-square test for the difference between two proportions (Section 12.1)
McNemar test for two related samples (online Section 12.6)
Comparing More Than Two Groups
For Numerical Variables:
One-way analysis of variance (Section 11.1)
Kruskal-Wallis rank test (Section 12.5)
Randomized block design (Section 11.2)
Two-way analysis of variance (Section 11.3)
Friedman rank test (online Section 12.9)
For Categorical Variables:
Chi-square test for differences among more than two proportions (Section 12.2)
Analyzing the Relationship Between Two Variables
For Numerical Variables:
Scatter plot, time-series plot (Section 2.5)
Covariance, coefficient of correlation, t test of correlation (Sections 3.5 and 13.7)
Simple linear regression (Chapter 13)
Time-series forecasting (Chapter 16)
For Categorical Variables:
Contingency table, side-by-side bar chart (Sections 2.1 and 2.3)
Chi-square test of independence (Section 12.3)

	
18.1  Analyzing Numerical Variables	
737
Analyzing the Relationship Between Two or More Variables
For Numerical Dependent Variables:
Multiple regression (Chapters 14 and 15)
Regression tree (Section 17.3)
Neural network (Section 17.4)
For Categorical Dependent Variables:
Logistic regression (Section 14.7)
Classification tree (Section 17.3)
Neural network (Section 17.4)
CLASSIFYING OBJECTS INTO GROUPS
For a Set of Variables:
Cluster analysis (Section 17.5)
Multidimensional scaling (Section 17.6)
Analyzing Process Data
For Numerical Variables:
X and R control charts (online Section 19.5)
For Categorical Variables:
p chart (online Section 19.2)
For Counts of Nonconformities:
c chart (online Section 19.4)
In the DCOVA approach, the first thing you do is to define the variables that you want to study in 
order to solve a business problem or meet a business objective. To do this, you must identify the type 
of business problem (whether you are describing a group or making inferences about a group, among 
other choices) and then determine the type of variable—numerical or categorical—you are analyzing.
In Table 18.1, the all-uppercase first-level headings identify types of business problems, and the 
second-level headings always include the two types of variables. The entries in Table 18.1 identify the 
specific statistical methods appropriate for a particular type of business problem and type of variable.
Choosing appropriate statistical methods for your data is the single most important task you 
face and is at the heart of “doing statistics.” But this selection process is also the single most difficult 
thing you do when applying statistics! How, then, can you ensure that you have made an appropriate 
choice? By asking a series of questions, you can guide yourself to the appropriate choice of methods.
The rest of this chapter presents questions that will help guide you in making this choice. Two 
lists of questions, one for numerical variables and the other for categorical variables, are presented 
in the next two sections. Having two lists makes the decision you face more manageable while also 
reinforcing the importance of identifying the type of variable that you seek to analyze.
Student Tip
Recall that numerical 
variables have values 
that represent ­quantities, 
while categorical vari-
ables have values that 
can only be placed into 
categories, such as yes 
and no.
exhibit 18.1  Questions to Ask When Analyzing Numerical Variables
• Do you want to describe the characteristics of the variable (possibly broken down into 
several groups)?
• Do you want to reach conclusions about the mean and/or standard deviation of the 
variable in a population?
• Do you want to determine whether the mean and/or standard deviation of the variable 
differs depending on the group?
• Do you want to determine which factors affect the value of a variable?
• Do you want to predict the value of the variable based on the values of other variables?
• Do you want to determine whether the values of the variable are stable over time?
18.1  Analyzing Numerical Variables
Exhibit 18.1 presents the list of questions to ask if you plan to analyze a numerical variable. 
Each question is independent of the others, and you can ask as many or as few questions as is 
appropriate for your analysis. How to go about answering these questions follows Exhibit 18.1.

738	
Chapter 18  A Roadmap for Analyzing Data
Describing the Characteristics of a Numerical Variable
You develop tables and charts and compute descriptive statistics to describe characteristics 
such as central tendency, variation, and shape. Specifically, you can create a stem-and-leaf 
display, percentage distribution, histogram, polygon, boxplot, normal probability plot, bullet 
graph, gauge, and treemap (see Sections 2.2, 2.4, 3.3, 6.3, and 17.1), and you can compute 
statistics such as the mean, median, mode, quartiles, range, interquartile range, standard devia-
tion, variance, coefficient of variation, skewness, and kurtosis (see Sections 3.1, 3.2, and 3.3).
Reaching Conclusions About the Population Mean  
and/or Standard Deviation
You have several different choices, and you can use any combination of these choices. To 
estimate the mean value of the variable in a population, you construct a confidence interval 
estimate of the mean (see Section 8.2). To determine whether the population mean is equal to a 
specific value, you conduct a t test of hypothesis for the mean (see Section 9.2). To determine 
whether the population standard deviation or variance is equal to a specific value, you conduct 
a x2 test of hypothesis for the standard deviation or variance (see online Section 12.7).
Determining Whether the Mean and/or Standard Deviation 
Differs Depending on the Group
When examining differences between groups, you first need to establish which categorical vari-
able to use to divide your data into groups. You then need to know whether this grouping variable 
divides your data in two groups (such as male and female groups for a gender variable) or whether 
the variable divides your data into more than two groups (such as the four in-store locations for 
mobile electronics discussed in Section 11.1). Finally, you must ask whether your data set contains 
independent groups or whether your data set contains matched or repeated measurements.
If the Grouping Variable Defines Two Independent Groups and You Are 
­Interested in Central Tendency  Which hypothesis tests you use depends on the as-
sumptions you make about your data.
If you assume that your numerical variable is normally distributed and that the variances 
are equal, you conduct a pooled t test for the difference between the means (see Section 10.1). 
If you cannot assume that the variances are equal, you conduct a separate-variance t test for the 
difference between the means (see Section 10.1). To test whether the variances are equal, as-
suming that the populations are normally distributed, you can conduct an F test for the differ-
ences between the variances. In either case, if you believe that your numerical variables are 
not normally distributed, you can perform a Wilcoxon rank sum test (see Section 12.4) and 
compare the results of this test to those of the t test.
To evaluate the assumption of normality that the pooled t test and separate-variance t test 
include, you can construct boxplots and normal probability plots for each group.
If the Grouping Variable Defines Two Groups of Matched Samples or Repe-
ated Measurements and You Are Interested in Central Tendency  If you can 
assume that the paired differences are normally distributed, you conduct a paired t test (see 
Section 10.2). If you cannot assume that the paired differences are normally distributed, you 
conduct a Wilcoxon signed rank test (see online Section 12.8)
If the Grouping Variable Defines Two Independent Groups and You Are 
­Interested in Variability  If you can assume that your numerical variable is normally 
distributed, you conduct an F test for the difference between two variances (see Section 10.4).
If the Grouping Variable Defines More Than Two Independent Groups and 
You Are Interested in Central Tendency  If you can assume that the values of the 
numerical variable are normally distributed, you conduct a one-way analysis of variance (see 
Section 11.1); otherwise, you conduct a Kruskal-Wallis rank test (see Section 12.5).

	
18.2  Analyzing Categorical Variables	
739
If the Grouping Variable Defines More Than Two Groups of Matched Samples or 
Repeated Measurements and You Are Interested in Central Tendency  Suppose 
that you have a design where the rows represent the blocks and the columns represent the levels 
of a factor. If you can assume that the values of the numerical variable are normally distributed, 
you conduct a randomized block design F test (see Section 11.2). If you cannot assume that 
the paired differences are normally distributed, you conduct a Friedman rank test (see online 
Section 12.9)
Determining Which Factors Affect the Value of a Variable
If there are two factors to be examined to determine their effect on the values of a variable, you 
develop a two-factor factorial design (see Section 11.3).
Predicting the Value of a Variable Based  
on the Values of Other Variables
When predicting the values of a numerical dependent variable, you conduct least-squares regres-
sion analysis. The least-squares regression model you develop depends on the number of inde-
pendent variables in your model. If there is only one independent variable being used to predict 
the numerical dependent variable of interest, you develop a simple linear regression model (see 
Chapter 13); otherwise, you develop a multiple regression model (see Chapters 14 and 15) or a 
regression tree (see Section 17.3) or a neural network (see Section 17.4).
If you have values over a period of time and you want to forecast the variable for future 
time periods, you can use moving averages, exponential smoothing, least-squares forecasting, 
and autoregressive modeling (see Chapter 16).
Determining Whether the Values of a Variable  
Are Stable Over Time
If you are studying a process and have collected data on the values of a numerical variable over 
a time period, you construct R and X charts (see online Section 19.5). If you have collected 
data in which the values are counts of the number of nonconformities, you construct a c chart 
(see online Section 19.4).
18.2  Analyzing Categorical Variables
Exhibit 18.2 presents the list of questions to ask if you plan to analyze a categorical vari-
able. Each question is independent of the others, and you can ask as many or as few ques-
tions as is appropriate for your analysis. How to go about answering these questions follows  
Exhibit 18.2.
exhibit 18.2
Questions to Ask When Analyzing Categorical Variables
• Do you want to describe the proportion of items of interest in each category (possibly 
broken down into several groups)?
• Do you want to reach conclusions about the proportion of items of interest in a population?
• Do you want to determine whether the proportion of items of interest differs depending 
on the group?
• Do you want to predict the proportion of items of interest based on the values of other 
variables?
• Do you want to determine whether the proportion of items of interest is stable over time?

740	
Chapter 18  A Roadmap for Analyzing Data
Describing the Proportion of Items  
of Interest in Each Category
You create summary tables and use these charts: bar chart, pie chart, Pareto chart, or side-by-
side bar chart (see Sections 2.1 and 2.3).
Reaching Conclusions About the Proportion  
of Items of Interest
You have two different choices. You can estimate the proportion of items of interest in a popu-
lation by constructing a confidence interval estimate of the proportion (see Section 8.3). Or, 
you can determine whether the population proportion is equal to a specific value by conducting 
a Z test of hypothesis for the proportion (see Section 9.4).
Determining Whether the Proportion of Items  
of Interest Differs Depending on the Group
When examining this difference, you first need to establish the number of categories associ-
ated with your categorical variable and the number of groups in your analysis. If your data 
contain two groups, you must also ask if your data contain independent groups or if your data 
contain matched samples or repeated measurements.
For Two Categories and Two Independent Groups  You conduct either the Z test 
for the difference between two proportions (see Section 10.3) or the x2 test for the difference 
between two proportions (see Section 12.1).
For Two Categories and Two Groups of Matched or Repeated Measurements  
You conduct the McNemar test (see online Section 12.6).
For Two Categories and More Than Two Independent Groups  You conduct a 
x2 test for the difference among several proportions (see Section 12.2).
For More Than Two Categories and More Than Two Groups  You develop con-
tingency tables and use multidimensional contingency tables to drill down to examine relation-
ships among two or more categorical variables (Sections 2.1, 2.6, and 17.1). When you have 
two categorical variables, you conduct a x2 test of independence (see Section 12.3).
Predicting the Proportion of Items of Interest Based  
on the Values of Other Variables
You develop a logistic regression model (see Section 14.7) or you use classification trees (see 
Section 17.3) or neural networks (see Section 17.4).
Determining Whether the Proportion  
of Items of Interest Is Stable Over Time
If you are studying a process and have collected data over a time period, you can create the 
appropriate control chart. If you have collected the proportion of items of interest over a time 
period, you develop a p chart (see online Section 19.2).

	
Chapter Review Problems	
741
C h a pte r  R e vi e w P r ob le ms
T
his chapter summarizes all the methods discussed in the 
first 17 chapters of this book. The data analysis methods 
discussed in the book are organized in Table 18.1 accord-
ing to whether each method is used for describing a group 
or several groups, for making inferences about one group or 
comparing two or more groups, or for analyzing relation-
ships between two or more variables. Then, sets of questions 
are listed in Exhibits 18.1 and 
18.2 to assist you in determin-
ing what method to use to ana-
lyze your data.
U s i n g  S tat i s t i c s
Mounting Future Analyses, Revisited
Courtesy of Sharyn Rosenberg
Digital Case
Whereas other Digital Cases asked you to apply your knowl-
edge about the proper use of statistics, this case helps you 
remember how to properly apply that knowledge.
Guadalupe Cooper and Gilbert Chandler had worked very 
hard all semester long in their business statistics course. 
They now faced a final project in which they had to estab-
lish a plan to analyze a set of data that had been assigned to 
them by their instructor. As they looked through the online 
materials for their statistics textbook, they found DataAnal-
ysisGuide.pdf in the ­Digital Case materials. “Gee, this is 
like the material in Chapter 18, but in interactive form!” one 
of them noted. They both then knew what questions they 
needed to ask in order to get started on their final semester 
task.
18.1  In many manufacturing processes, the term work-in-process 
(often abbreviated WIP) is used. At the LSS Publishing book man-
ufacturing plants, WIP represents the time it takes for sheets from 
a press to be folded, gathered, sewn, tipped on end sheets, and 
bound together to form a book, and the book placed in a packing 
carton. The operational definition of the variable of interest, pro-
cessing time, is the number of days (measured in hundredths) from 
when the sheets come off the press to when the book is placed in a 
packing carton. The company has the business objective of deter-
mining whether there are differences in the WIP between plants. 
Data have been collected from samples of 20 books at each of two 
production plants. The data, stored in   WIP  , are as follows:
Plant A
  5.62
5.29 16.25 10.92 11.46 21.62 8.45
8.58
5.41 11.42
11.62
7.29   7.50   7.96   4.42 10.50 7.58
9.29
7.54   8.92
Plant B
9.54 11.46 16.62 12.62 25.75 15.41 14.29 13.13 13.71 10.04
5.75 12.46   9.17 13.21   6.00   2.33 14.25   5.37   6.25   9.71
Completely analyze the data.
18.2  Many factors determine the attendance at Major League 
Baseball games. These factors can include when the game is 
played, the weather, the opponent, whether the team is having a 
good season, and whether a marketing promotion is held. Popular 
promotions during a recent season included the traditional hat days 
and poster days and the newer craze, bobble-heads of star players. 
(Data extracted from T. C. Boyd and T. C. Krehbiel, “An Analy-
sis of the Effects of Specific Promotion Types on Attendance at  
Major League Baseball Games,” Mid-American Journal of Business, 
2006, 21, pp. 21–32.) The file   Baseball   includes the following 
variables for a recent Major League Baseball season:
TEAM—Kansas City Royals, Philadelphia Phillies,
Chicago Cubs, or Cincinnati Reds
ATTENDANCE—Paid attendance for the game
TEMP—High temperature for the day
WIN%—Team’s winning percentage at the time of the game
OPWIN%—Opponent team’s winning percentage at the time of 
the game
WEEKEND—1 if game played on Friday, Saturday, or Sunday; 0 
otherwise
PROMOTION—1 if a promotion was held; 0 if no promotion was 
held
You want to predict attendance and determine the factors that in-
fluence attendance. Completely analyze the data for the Kansas 
City Royals.
18.3  Repeat Problem 18.2 for the Philadelphia Phillies.
18.4  Repeat Problem 18.2 for the Chicago Cubs.
18.5  Repeat Problem 18.2 for the Cincinnati Reds.
18.6  The file  EuroTourism2  contains a sample of 27 European 
countries. Variables included are the number of jobs generated in the 
travel and tourism industry in 2012, the spending on business travel 

742	
Chapter 18  A Roadmap for Analyzing Data
within the country by residents and international visitors in 2012, 
the total number of international visitors who visited the country 
in 2012, and the number of establishments that provide overnight 
­accommodation for tourists. (Data extracted from www.marketline 
.com.) You want to be able to predict the number of jobs generated 
in the travel and tourism industry. Completely analyze the data.
18.7  The file  Philly  contains a sample of 25 neighborhoods in 
Philadelphia. Variables included are neighborhood population, me-
dian sales price of homes in 2012, mean number of days homes 
were on the market in 2012, number of homes sold in 2012, ­median 
neighborhood household income, percentage of residents in the 
neighborhood with a bachelor’s degree or higher, and whether the 
neighborhood is considered “hot” (coded as 1 = yes, 0 = no). 
(Data extracted from bit.ly/13M7KuP.) You want to be able to pre-
dict median sales price of homes. Completely analyze the data.
18.8  Professional basketball has truly become a sport that gener-
ates interest among fans around the world. More and more play-
ers come from outside the United States to play in the National  
Basketball Association (NBA). Many factors could impact the 
number of wins achieved by each NBA team. In addition to the 
number of wins, the file  NBA2012  contains team statistics for 
points per game (for team, opponent, and the difference between 
team and opponent), field goal (shots made) percentage (for team, 
opponent, and the difference between team and opponent), turn-
overs (losing the ball before a shot is taken) per game (for team, 
opponent, and the difference between team and opponent), and the 
rebound percentage. You want to be able to predict to predict the 
number of wins. Completely analyze the data.
18.9  The data in  UsedCars  represent characteristics of cars that 
are currently part of an inventory of a used car dealership. The 
variables included are car, year, age, price ($), mileage, power 
(hp), and fuel (mpg).
You want to describe each of these variables, and you would 
like to predict the price of the used cars. Analyze the data.
18.10  A study was conducted to determine whether any gender 
bias existed in an academic science environment. Faculty from 
several universities were asked to rate candidates for the position 
of undergraduate laboratory manager based on their application. 
The gender of the applicant was given in the applicant’s materials. 
The raters were from either biology, chemistry, or physics depart-
ments. Each rater was to give a competence rating to the appli-
cant’s materials on a seven point scale with 1 being the lowest and 
7 being the highest. In addition, the rater supplied a starting salary 
that should be offered to the applicant. These data (which have 
been altered from an actual study to preserve the anonymity of the 
respondents) are stored in  Candidate Assessment .
Analyze the data. Do you think that there is any gender bias 
in the evaluations? Support your point of view with specific refer-
ences to your data analysis.
18.11  Zagat’s publishes restaurant ratings for various locations 
in the United States. The file  Restaurants2  contains the Zagat rat-
ing for food, décor, service, cost per person, and popularity index 
(popularity points the restaurant received divided by the number of 
people who voted for that restaurant) for various types of restau-
rants in New York City.
You want to study differences in the cost of a meal for the 
different types of cuisines and also want to be able to predict the 
cost of a meal. Completely analyze the data. (Data extracted from 
Zagat Survey 2012 New York City Restaurants).
18.12  The data in the file  BankMarketing  are from a direct 
marketing campaign conducted by a Portuguese banking institu-
tion (Data extracted from S. Moro, R. Laureano and P. Cortez,  
“Using Data Mining for Bank Direct Marketing: An Application 
of the CRISP-DM Methodology,” in P. Novais et al. (Eds.), Pro-
ceedings of the European Simulation and Modeling Conference—
ESM’2011, pp. 117–121.) The variables included were age, type 
of job, marital status, education, whether credit is in default, aver-
age yearly balance in account in Euros, whether there is a housing 
loan, whether there is a personal loan, last contact duration in sec-
onds, number of contacts performed during this campaign, and has 
the client purchased a term deposit.
Analyze the data and assess the likelihood that the client will 
purchase a term deposit.
18.13  A mining company operates a large heap-leach gold mine 
in the western United States. The gold mined at this location con-
sists of ore that is very low grade, having about 0.0032 ounce of 
gold in 1 ton of ore. The process of heap-leaching involves the 
mining, crushing, stacking, and leaching millions of tons of gold 
ore per year. In the process, ore is placed in a large heap on an 
impermeable pad. A weak chemical solution is sprinkled over the 
heap and is collected at the bottom after percolating through the 
ore. As the solution percolates through the ore, the gold is dis-
solved and is later recovered from the solution. This technology, 
which has been used for more than 30 years, has made the opera-
tion profitable. Due to the large amount of ore that is handled, the 
company is continually exploring ways to improve the process. As 
part of an expansion several years ago, the stacking process was 
automated with the construction of a computer controlled stacker. 
This stacker was designed to load 35,000 tons of ore per day at a 
cost that was less than the previous process that used manually 
operated trucks and bulldozers. However, since its installation, 
the stacker has not been able to achieve these results consistently. 
Data for a recent 35-day period that indicate the amount stacked 
(tons) and the downtime (minutes) are stored in the file  Mining . 
Other data that indicate the causes for the downtime are stored  
in  Mining2 .
Analyze the data, making sure to present conclusions about 
the daily amount stacked and the causes of the downtime. In ad-
dition, be sure to develop a model to predict the amount stacked 
based on downtime.
18.14  A survey was conducted on the characteristics of house-
holds in the United States. The data (which have been altered from 
an actual study to preserve the anonymity of the respondents) are 
stored in  Households . The variables are gender, age, Hispanic 
origin, type of dwelling, age of dwelling in years, years living at 
dwelling, number of bedrooms, number of vehicles kept at dwell-
ing, fuel type at dwelling, monthly cost of fuel at dwelling ($), 
U.S. citizenship, college degree, marital status, work for pay in 
previous week, mode of transportation to work, commuting time 
in minutes, hours worked per week, type of organization, annual 
earned income ($), and total annual income ($).
Analyze these data and prepare a report describing your 
­conclusions.
18.15  The file  HybridSales  contains the number of domestic and 
imported hybrid vehicles sold in the United States from 1999 to 
2012. (Data extracted from bit.ly/17hJk9H.) You want to be able 
to predict the number of domestic and imported hybrid vehicles 
sold in the United States in 2013 and 2014. Completely analyze 
the data.

Appendices
A.	
Basic Math Concepts and 
Symbols
A.1	
Rules for Arithmetic Operations
A.2	
Rules for Algebra: Exponents and 
Square Roots
A.3	
Rules for Logarithms
A.4	
Summation Notation
A.5	
Statistical Symbols
A.6	
Greek Alphabet
B.	
Required Excel Skills
B.1	
Worksheet Entries and References
B.2	
Absolute and Relative Cell References
B.3	
Entering Formulas into Worksheets
B.4	
Pasting with Paste Special
B.5	
Basic Worksheet Formatting
B.6	
Chart Formatting
B.7	
Selecting Cell Ranges for Charts
B.8	
Deleting the “Extra” Bar from  
a Histogram
B.9	
Creating Histograms for Discrete 
Probability Distributions
C.	
Online Resources
C.1	
About the Online Resources  
for This Book
C.2	
Accessing the MyStatLab  
Course Online
C.3	
Details of Downloadable Files
C.4	
PHStat
D.	 CONFIGURING MICROSOFT EXCEL
D.1	
Getting Microsoft Excel Ready  
for Use (ALL)
D.2	
Getting PHStat Ready for Use (ALL)
D.3	
Configuring Excel Security for Add-In 
Usage (WIN)
D.4	
Opening PHStat (ALL)
D.5	
Using a Visual Explorations Add-in 
Workbook (ALL)
D.6	
Checking for the Presence of the 
Analysis ToolPak or Solver Add-Ins 
(ALL)
E.	
Tables
E.1	
Table of Random Numbers
E.2	
The Cumulative Standardized Normal 
Distribution
E.3	
Critical Values of t
E.4	
Critical Values of x2
E.5	
Critical Values of F
E.6	
Lower and Upper Critical Values, T1, of 
the Wilcoxon Rank Sum Test
E.7	
Critical Values of the Studentized 
Range, Q
E.8	
Critical Values, dL and dU, of the 
Durbin-Watson Statistic, D
E.9	
Control Chart Factors
E.10	 The Standardized Normal Distribution
F.	
USEFUL EXCEL KNOWLEDGE
F.1	
Useful Keyboard Shortcuts
F.2	
Verifying Formulas and Worksheets
F.3	
New Function Names
F.4	
Understanding the Nonstatistical 
Functions
G.	 SOFTWARE FAQS
G.1	
PHStat FAQs
G.2	
Microsoft Excel FAQs
G.3	
FAQs for New Microsoft Excel 2013 Users
G.4	
Minitab FAQs
Self-Test Solutions and Answers  
to Selected Even-Numbered  
Problems
743

A p p e n d i x  A    Basic Math Concepts and Symbols
744
A.1  Rules for Arithmetic Operations
Rule
Example
  1. a + b = c and b + a = c
2 + 1 = 3 and 1 + 2 = 3
  2. a + 1b + c2 = 1a + b2 + c
5 + 17 + 42 = 15 + 72 + 4 = 16
  3. a - b = c but b - a ≠c
9 - 7 = 2 but 7 - 9 ≠2
  4. 1a21b2 = 1b21a2
172162 = 162172 = 42
  5. 1a21b + c2 = ab + ac
12213 + 52 = 122132 + 122152 = 16
  6. a , b ≠b , a
12 , 3 ≠3 , 12
  7. a + b
c
= a
c + b
c
7 + 3
2
= 7
2 + 3
2 = 5
  8. 
a
b + c ≠a
b + a
c
3
4 + 5 ≠3
4 + 3
5
  9. 1
a + 1
b = b + a
ab
1
3 + 1
5 = 5 + 3
132152 = 8
15
10. a a
bb a c
d b = a ac
bd b
a 2
3b a 6
7b = a 122162
132172 b = 12
21
11. a
b , c
d = ad
bc
5
8 , 3
7 = a 152172
182132 b = 35
24
A.2  Rules for Algebra: Exponents and Square Roots
Rule
Example
1. 1Xa21Xb2 = Xa + b
14221432 = 45
2. 1Xa2b = Xab
12223 = 26
3. 1Xa>Xb2 = Xa - b
35
33 = 32
4. Xa
Xa = X0 = 1
34
34 = 30 = 1
5. 1XY = 1X1Y
11252142 = 12514 = 10
6. A
X
Y = 2X
2Y
A
16
100 =
216
2100
= 0.40

	
Appendix A  Basic Math Concepts and Symbols	
745
Base 10
Log is the symbol used for base-10 logarithms:
A.3  Rules for Logarithms
Rule
Example
1. log110a2 = a
log11002 = log11022 = 2
2. If log1a2 = b, then a = 10b
If log1a2 = 2, then a = 102 = 100
3. log1ab2 = log1a2 + log1b2
log11002 = log3110211024 = log1102 + log1102 
         = 1 + 1 = 2
4. log1ab2 = 1b2 log1a2
log11,0002 = log11032 = 132 log1102 = 132112 = 3
5. log1a>b2 = log1a2 - log1b2
log11002 = log11,000>102 = log11,0002 - log1102
         = 3 - 1 = 2
Example  
Take the base-10 logarithm of each side of the following equation:
Y = b0b1
Xe
Solution:  Apply rules 3 and 4:
 log1Y2 = log1b0bX
1e2
 = log1b02 + log1bX
12 + log1e2
 = log1b02 + X log1b12 + log1e2
Base e
ln is the symbol used for base e logarithms, commonly referred to as natural logarithms. e is 
Euler’s number, and e ≅2.718282:
Rule
Example
1. ln1ea2 = a
ln17.3890562 = ln1e22 = 2
2. If ln1a2 = b, then a = eb
If ln1a2 = 2, then a = e2 = 7.389056
3. ln1ab2 = ln1a2 + ln1b2 
ln11002 = ln31102 11024 
= ln1102 + ln1102 = 2.302585 + 2.302585 = 4.605170
4. ln1ab2 = 1b2 ln1a2
ln11,0002 = ln11032 = 3 ln1102 = 312.3025852 = 6.907755
5. ln1a>b2 = ln1a2 - ln1b2 ln11002 = ln11,000>102 = ln11,0002 - ln1102 
       = 6.907755 - 2.302585 = 4.605170

746	
Appendices
The symbol Σ, the Greek capital letter sigma, represents “taking the sum of.” Consider a set of 
n values for variable X. The expression a
n
i = 1
Xi means to take the sum of the n values for variable 
X. Thus:
a
n
i = 1
Xi = X1 + X2 + X3 + g + Xn
The following problem illustrates the use of the symbol Σ. Consider five values of a variable 
X:  X1 = 2, X2 = 0, X3 = -1, X4 = 5, and X5 = 7. Thus:
a
5
i = 1
Xi = X1 + X2 + X3 + X4 + X5 = 2 + 0 + 1-12 + 5 + 7 = 13
In statistics, the squared values of a variable are often summed. Thus:
a
n
i = 1
X2
i = X2
1 + X2
2 + X2
3 + g+ X2
n
and, in the example above:
 a
5
i = 1
X2
i = X2
1 + X2
2 + X2
3 + X2
4 + X2
5
 = 22 + 02 + 1-122 + 52 + 72
 = 4 + 0 + 1 + 25 + 49
 = 79
a
n
i = 1
X2
i , the summation of the squares, is not the same as a a
n
i = 1
Xib
2
, the square of the sum:
a
n
i = 1
X2
i ≠a a
n
i = 1
Xib
2
In the example given above, the summation of squares is equal to 79. This is not equal to the 
square of the sum, which is 132 = 169.
Another frequently used operation involves the summation of the product. Consider two 
variables, X and Y, each having n values. Then:
a
n
i = 1
XiYi = X1Y1 + X2Y2 + X3Y3 + g+ XnYn
Example  
Take the base e logarithm of each side of the following equation:
Y = b0bX
1e
Solution:  Apply rules 3 and 4:
 ln1Y2 = ln1b0bX
1e2
 = ln1b02 + ln1bX
12 + ln1e2
 = ln1b02 + X ln1b12 + ln1e2
A.4  Summation Notation

	
Appendix A  Basic Math Concepts and Symbols	
747
Continuing with the previous example, suppose there is a second variable, Y, whose five 
values are Y1 = 1, Y2 = 3, Y3 = -2, Y4 = 4, and Y5 = 3. Then,
 a
n
i = 1
XiYi = X1Y1 + X2Y2 + X3Y3 + X4Y4 + X5Y5
 = 122112 + 102132 + 1-121-22 + 152142 + 172132
 = 2 + 0 + 2 + 20 + 21
 = 45
In computing a
n
i = 1
XiYi, you need to realize that the first value of X is multiplied by the first 
value of Y, the second value of X is multiplied by the second value of Y, and so on. These 
products are then summed in order to compute the desired result. However, the summation of 
products is not equal to the product of the individual sums:
a
n
i = 1
XiYi ≠a a
n
i = 1
Xib a a
n
i = 1
Yib
In this example,
a
5
i = 1
Xi = 13
and
a
5
i = 1
Yi = 1 + 3 + 1-22 + 4 + 3 = 9
so that
a a
5
i = 1
Xib a a
5
i = 1
Yib = 1132192 = 117
However,
a
5
i = 1
XiYi = 45
The following table summarizes these results:
Value
Xi
Yi
XiYi
1
2
1
2
2
0
3
0
3
-1
-2
2
4
5
4
20
5
7
3
21
a
5
i = 1
Xi = 13
a
5
i = 1
Yi = 9
a
5
i = 1
XiYi = 45
Rule 1  The summation of the values of two variables is equal to the sum of the values of 
each summed variable:
a
n
i = 1
1Xi + Yi2 = a
n
i = 1
Xi + a
n
i = 1
Yi

748	
Appendices
Thus,
 a
5
i = 1
1Xi + Yi2 = 12 + 12 + 10 + 32 + 1-1 + (-222 + 15 + 42 + 17 + 32
 = 3 + 3 + 1-32 + 9 + 10
 = 22
 a
5
i = 1
Xi + a
5
i = 1
Yi = 13 + 9 = 22
Rule 2  The summation of a difference between the values of two variables is equal to the 
difference between the summed values of the variables:
a
n
i = 1
1Xi - Yi2 = a
n
i = 1
Xi - a
n
i = 1
Yi
Thus,
 a
5
i = 1
1Xi - Yi2 = 12 - 12 + 10 - 32 + 1-1 - (-222 + 15 - 42 + 17 - 32
 = 1 + 1-32 + 1 + 1 + 4
 = 4
 a
5
i = 1
Xi - a
5
i = 1
Yi = 13 - 9 = 4
Rule 3  The sum of a constant times a variable is equal to that constant times the sum of the 
values of the variable:
a
n
i = 1
cXi = c a
n
i = 1
Xi
where c is a constant. Thus, if c = 2,
 a
5
i = 1
cXi = a
5
i = 1
2Xi = 122122 + 122102 + 1221-12 + 122152 + 122172
 = 4 + 0 + 1-22 + 10 + 14
 = 26
 c a
5
i = 1
Xi = 2 a
5
i = 1
Xi = 1221132 = 26
Rule 4  A constant summed n times will be equal to n times the value of the constant.
a
n
i = 1
c = nc
where c is a constant. Thus, if the constant c = 2 is summed 5 times,
 a
5
i = 1
c = 2 + 2 + 2 + 2 + 2 = 10
 nc = 152122 = 10
Example  
Suppose there are six values for the variables X and Y, such that X1 = 2, X2 = 1, X3 = 5,
X4 = -3, X5 = 1, X6 = -2 and Y1 = 4, Y2 = 0, Y3 = -1, Y4 = 2, Y5 = 7, and Y6 = -3. 
Compute each of the following:
(a)	 a
6
i = 1
Xi
(b)	 a
6
i = 1
Yi

	
Appendix A  Basic Math Concepts and Symbols	
749
References
	 1.	Bashaw, W. L., Mathematics for Statistics (New York: Wiley, 1969).
	 2.	Lanzer, P., Basic Math: Fractions, Decimals, Percents (Hicksville, NY: Video Aided  
Instruction, 2006).
	 3.	Levine, D. and A. Brandwein, The MBA Primer: Business Statistics, 3rd ed. (Cincinnati, 
OH: Cengage Publishing, 2011).
	 4.	Levine, D., Statistics (Hicksville, NY: Video Aided Instruction, 2006).
	 5.	Shane, H., Algebra 1 (Hicksville, NY: Video Aided Instruction, 2006).
A.5  Statistical Symbols
+  add
*  multiply
-  subtract
,  divide
=  equal to
≠ not equal to
≅ approximately equal to
6  less than
7  greater than
…  less than or equal to
Ú  greater than or equal to
Greek Letter
Letter Name
English Equivalent
Greek Letter
Letter Name
English Equivalent
A
a
Alpha
a
N
n
Nu
n
B
b
Beta
b
Ξ
j
Xi
x
Γ
g
Gamma
g
Ο
o
Omicron
oi
∆
d
Delta
d
Π
p
Pi
p
E
e
Epsilon
ei
P
r
Rho
r
Z
z
Zeta
z
Σ
s
Sigma
s
H
h
Eta
e
T
t
Tau
t
ϴ
u
Theta
th
Y
y
Upsilon
u
I
i
Iota
i
Φ
f
Phi
ph
K
k
Kappa
k
Χ
x
Chi
ch
Λ
l
Lambda
l
Ψ
c
Psi
ps
M
m
Mu
m
Ω
v
Omega
o
A.6  Greek Alphabet
	(c)	 a
6
i = 1
X2
i
(d)	 a
6
i = 1
Y 2
i
(e)		 a
6
i = 1
XiYi
(f)		 a
6
i = 1
1Xi + Yi2
(g)	 a
6
i = 1
1Xi - Yi2
(h)	 a
6
i = 1
1Xi - 3Yi + 2X2
i 2
(i)		 a
6
i = 1
1cXi2, where c = -1
(j)		 a
6
i = 1
1Xi - 3Yi + c2, where c = +3
Answers
(a) 4  (b) 9  (c) 44  (d) 79  (e) 10  (f) (13)  (g) -5  (h) 65  (i) -4  (j) -5

A p p e n d i x  B     Required Excel Skills
This appendix reviews the Excel skills and operations you 
need to know in order to make effective use of Microsoft 
Excel. As stated in Section EG.1 on page 36, if you plan 
to use the In-Depth Excel instructions, you will need to be 
familiar with the contents of Sections B.1 through B.4 as a 
minimum. Mastery of the skills and operations in this appen-
dix is less necessary if you plan to use PHStat (or the Analysis 
ToolPak), but knowing them will prove useful if you need to 
customize the worksheets that PHStat creates or plan to cre-
ate your summary presentations from those results.
If you find the level of this appendix too challenging or 
are unfamiliar with the skills listed in Table EG.A on page 36,  
you should first download and review “Basic Computing 
Skills” that is available online (see Appendix C for details).
B.1  Worksheet Entries  
and References
As discussed in Section GS.4, Microsoft Excel uses work-
sheets to both store data and present the results of analy-
ses. In worksheet cells, you enter the data for variables, text 
that serves to label data or title a worksheet, or formulas. 
Formulas are instructions that perform a calculation or 
some other computing task such as logical decision making. 
Formulas are typically found in worksheets that you use to 
present intermediate calculations or the results of an analy-
sis. In some cases, formulas create or prepare new data to be 
analyzed.
Formulas typically use values found in other cells to 
compute a result that is displayed in the cell that stores the 
formula. This means that when you see that a particular 
worksheet cell is displaying the value, say, 5, you cannot 
determine from casual inspection if the worksheet creator 
typed the number 5 into the cell or if the creator typed a 
formula that results in the display of the value 5. This trait 
of worksheets means you should always carefully review 
the contents of each worksheet you use. In this book, each 
worksheet with formulas that you might use is accompanied 
by a “formulas” worksheet that presents the worksheet in a 
mode that allows you to see all the formulas that have been 
entered in the worksheet.
Cell References
Most formulas use values that have been entered into other 
cells. To refer to those cells, Excel uses an addressing, or 
referencing, system that is based on the tabular nature of a 
worksheet. Columns are designated with letters and rows are 
designated with numbers such that the cell in the first row 
and first column is called A1, the cell in the third row and 
first column is called A3, and the cell in the third column 
and first row is C1. To refer to a cell in a formula, you use a 
cell reference in the form WorksheetName!ColumnRow. For 
example, Data!A2 refers to the cell in the Data worksheet 
that is in column A and row 2.
You can also use only the ColumnRow portion of a full 
address—for example, A2—as a shorthand way of referring 
to a cell that is on the same worksheet as the one into which 
you are entering a formula. (Excel calls the worksheet into 
which you are making entries the current worksheet.) If 
the worksheet name contains spaces or special characters, 
such as CITY DATA or Figure_1.2, you must enclose the 
sheet name in a pair of single quotes, as in 'CITY DATA'!A2 
or 'Figure_1.2'!A2.
To refer to a group of cells, such as the cells of a col-
umn that store the data for a particular variable, you use a 
cell range. A cell range names the upper-left cell and the 
lower-right cell of the group, using the form Worksheet 
­Name!UpperLeftCell:LowerRightCell. For example, the cell 
range DATA!A1:A11 identifies the first 11 cells in the first 
column of the DATA worksheet. Cell ranges can extend over 
multiple columns; the cell range DATA!A1:D11 would refer 
to the first 11 cells in the first 4 columns of the worksheet. 
Cell ranges in the form Column:Column (or Row:Row) that 
refer to all cells in a column (or row) are also allowed. In 
this book, you will occasionally see cell ranges such as 
B:B that refer to all the cells in a column B for situations 
in which the number of cell entries in column B would be 
unknown to the worksheet creator.
As with single cell references, you can skip the 
­WorksheetName! part of the reference if you are entering 
a cell range on the current worksheet. And if a worksheet 
name contains spaces or special characters, the worksheet 
name must be enclosed in a pair of single quotes. Note, that 
in some Excel dialog boxes, you must include the worksheet 
name as part of the cell reference in order to get the proper 
results. (Such cases are noted in the instructions in this book 
when they arise.)
Although not used in this book, cell references can 
include a workbook name in the form '[WorkbookName]
WorksheetName'!ColumnRow or '[WorkbookName] 
­WorksheetName'! UpperLeftCell:LowerRightCell. You 
might discover such references if you inadvertently copy 
certain types of worksheets or chart sheets from one work-
book to another.
750

	
Appendix B  Required Excel Skills	
751
Recalculation
When you use formulas that refer to other cells, the result 
displayed by the formulas automatically changes as the val-
ues in the cells to which the formula refers change. This pro-
cess, called recalculation, was the original novel feature of 
worksheet programs and first led to these programs being 
widely used in accounting.
Recalculation forms the basis for constructing work-
sheet templates and models. Templates are worksheets in 
which you only need to enter values to get results. Tem-
plates can be reused over and over again, by entering differ-
ent sets of values. Many of the worksheets illustrated in this 
book are templates. For those worksheets, you need only to 
enter new values, typically into cells that are tinted a light 
turquoise color, to get the results you need. Other work-
sheets illustrated are models, which are similar to templates 
but require the editing of certain formulas as new values are 
entered into a worksheet. In this book, worksheet models 
have been designed to simplify such editing tasks and to 
provide the most generalized solution.
Worksheets that use formulas capable of recalculation 
are sometimes called “live” worksheets to distinguish them 
from worksheets that contain only text and numeric entries 
(“dead” worksheets). A novel feature of the PHStat add-in 
that you can use with this book is that just about every work-
sheet the add-in constructs for you is a “live” worksheet. 
This means that, as first noted in Section EG.1 on page 36, 
you get the same results, the same worksheets, whether you 
use PHStat or the In-Depth Excel instructions in the Excel 
Guides. This is dissimilar to many other add-ins that pro-
duce results in the form of dead worksheets that cannot be 
reused in any way.
B.2  Absolute and Relative 
Cell References
Many worksheets contain columns (or rows) of similar-
looking formulas. For example, column C in a worksheet 
might contain formulas that sum the contents of the column 
A and column B rows. The formula for cell C2 would be 
=A2 + B2, the formula for cell C3 would be =A3 + B3, 
for cell C4, =A4 + B4, and so on, down column C. To 
avoid the drudgery of typing many similar formulas, you can 
copy a formula and paste it into all the cells in a selected cell 
range. For example, to copy a formula that has been entered 
in cell C2 down the column through row 12:
	 1.	Right-click cell C2 and press Ctrl+C to copy the for-
mula. A movie marquee–like highlight appears around 
cell C2.
	 2.	Select the cell range C3:C12.
	 3.	With the cell range highlighted, press Ctrl+V to paste 
the formula into the cells of the cell range.
When you perform this copy-and-paste operation, Excel ad-
justs these relative cell references in formulas so that copy-
ing the formula =A2 + B2 from cell C2 to cell C3 results 
in the formula =A3 + B3 being pasted into cell C3, the 
formula =A4 + B4 being pasted into cell C4, and so on.
There are circumstances in which you do not want 
­Excel to adjust all or part of a formula. For example, if you 
were copying the cell C2 formula = 1A2 + B22 ,B15, and 
cell B15 contained the divisor to be used in all formulas, 
you would not want to see pasted into cell C3 the formula 
= 1A3 + B32 ,B16. To prevent Excel from adjusting a cell 
reference, you use absolute cell references by inserting 
dollar signs ($) before the column and row references of a 
relative cell reference. For example, the absolute cell refer-
ence $B$15 in the copied cell C2 formula = 1A2 + B22 ,
$B$15 will cause Excel to paste the formula = 1A2 + B22 ,
$B$15 into cell C3.
For ease of reading, formulas shown in the worksheet 
illustrations in this book show relative cell references, even 
in cases where using absolute cell references would assist in 
the physical entry of the formulas. Do not confuse the use of 
the dollar sign symbol with the worksheet formatting opera-
tion that displays numbers as dollar currency amounts. (See 
Section B.5 to learn how to format cells to display numeric 
values as dollar currency amounts.)
B.3  Entering Formulas  
into Worksheets
To enter a formula into a cell, first select the cell and then 
begin the entry by typing the equal sign 1=2. What follows 
the equal sign can be a combination of mathematical and 
data-processing operations and cell references that is ter-
minated by pressing Enter. For simple formulas, you use 
the symbols +, -, *, ,, and n for the operations addition, 
subtraction, multiplication, division, and exponentiation (a 
number raised to a power), respectively. For example, the 
formula =A2 + B2 adds the contents of cells A2 and B2 
displays the sum as the value in the cell containing the for-
mula. To revise a formula, either retype the formula or edit 
it in the formula bar.
Because formulas display their results and not them-
selves when entered in a cell, you should always review and 
verify any formula you enter before you use its worksheet to 
get results. One way to view all the formulas in a worksheet 
is to press Ctrl+` (grave accent). After your formula review, 
you can press Ctrl+` a second time to restore the normal 
display of values.
Functions
You can use worksheet functions in formulas to simplify 
certain arithmetic formulas or to gain access to advanced 
processing or statistical functions. For example, instead 

752	
Appendices
of typing =A2 + A3 + A4 + A5 + A6, you could use 
the SUM function to enter the equivalent, and shorter, for-
mula =SUM1A2:A62. Functions are entered by typing 
their names followed by a pair of parentheses. For almost 
all functions, you need to make at least one entry inside the 
pair of parentheses. For functions that require two or more 
entries, you separate entries with commas, as in the function 
QUARTILE (variable cell range, quartile number) func-
tion that is discussed in Section EG3.3.
To use a worksheet function in a formula, either type 
the function as shown in the instructions in this book or 
select a function from one of the galleries in the Function 
­Library group of the Formulas tab. For example, to enter the 
formula =QUARTILE(A2:A20, 2) in cell C2, you could 
either type these 20 characters directly into the cell or se-
lect cell C2 and then select Formulas ➔ More Functions 
➔ Statistical and click QUARTILE from the drop-down 
list and then enter A2:A20 and 2 in the Function Arguments 
dialog box and click OK. (For some functions, the selec-
tion process is much shorter and, in Excel versions older 
than Excel 2007, you select Formulas ➔ Insert Function 
and then make the necessary entries and selections in one or 
more dialog boxes that follow.)
Entering Array Formulas
An array formula is a formula that you enter just once but 
that applies to all of the cells in a selected cell range (the 
“array”). To enter an array formula, first select the cell range 
and then type the formula, and then, while holding down the 
Ctrl and Shift keys, press Enter to enter the array formula 
into all of the cells of the cell range. (In OS X Excel, you 
can also press Command+Enter to enter an array formula.)
To edit an array formula, you must first select the entire 
cell range that contains the array formula, then edit the for-
mula and then press Enter while holding down Ctrl+Shift 
(or press Command+Enter). When you select a cell that 
contains an array formula, Excel adds a pair of curly braces 
56 to the display of the formula in the formula bar. These 
curly braces disappear when you start to edit the formula. 
Including a pair of curly braces around a formula when doc-
umenting a worksheet is a convention to indicate that a par-
ticular formula is an array formula, but at no time will you 
ever type the curly braces when you enter an array formula.
B.4  Pasting with Paste 
Special
While the keyboard shortcuts Ctrl+C and Ctrl+V to copy 
and paste cell contents will often suffice, pasting data from 
one worksheet to another can sometimes cause unexpected 
side effects. When the two worksheets are in different work-
books, a simple paste creates an external link to the original 
workbook. This can lead to errors later if the first workbook is 
unavailable when the second one is being used. Even pasting  
between worksheets in the same workbook can lead to prob-
lems if what is being pasted is a cell range of formulas.
To avoid such side effects, use Paste Special in such 
situations. To use this operation, copy the source cell range 
using Ctrl+C and then right-click the cell (or cell range) 
that is the target of the paste and click Paste Special from 
the shortcut menu.
In the Paste Special dialog box (shown below), click 
Values and then click OK. For the first case, Paste ­Special 
Values pastes the current values of the cells in the first 
workbook and not formulas that use cell references to the 
first workbook.
Paste Special can paste other types of information, 
including cell formatting information. In some copying 
contexts, placing the mouse pointer over Paste Special in 
the shortcut menu will reveal a gallery of shortcuts to the 
choices presented in the Paste Special dialog box. For a full 
discussion of these additional features of Paste Special, see 
the Microsoft Excel help system.
If you use PHStat and have data for a procedure in the 
form of formulas, copy your data and then use Paste ­Special 
to paste columns of equivalent values. (Click Values in  
the Paste Special dialog box to create the values.) Then use 
the columns of values as the cell range of the data for the 
procedure. PHStat will not work properly if the data for a 
procedure are in the form of formulas.
B.5  Basic Worksheet  
Cell Formatting
You can change many aspects of how Excel displays the 
contents of worksheet cells through formatting operations. 
You format cells either by making entries in the Format 
Cells dialog box or by clicking shortcut buttons in the Home 
tab at the top of the Excel window. If you are new to Excel, 
you may find that using the Format Cells dialog box method 
to be easier, at least initially. Then, over time, you may want 
to switch to the Home tab shortcuts discussed later in this 
section.
To use the Format Cells dialog box, right-click a cell 
(or cell range) and click Format Cells in the shortcut menu. 

	
Appendix B  Required Excel Skills	
753
Excel displays the Number tab of the dialog box (partially 
shown below).
Clicking a Category changes the panel to the right of 
the list. For example, clicking Number displays a panel 
(partially shown below) in which you can set the number of 
decimal places. (Many cells in the worksheets used in this 
book have been set to display four decimal places.)
You can also change the numeric formatting of cells 
by clicking the various buttons of the Number group in the 
Home tab (shown below).
When you click the Alignment tab of the Format 
Cells dialog box (shown below), you display a panel in 
which you can control such things such as whether cell 
contents get displayed centered or top- or bottom-an-
chored in a cell and whether cell contents are horizon-
tally centered or left or right justified.
These choices in this panel are duplicated in the Align-
ment group of the Home tab (shown below).
In the Ribbon interface, many buttons, such as Merge & 
Center, have an associated drop-down list that you display 
by clicking the drop-down arrow at the right. For Merge & 
Center, this drop-down displays a gallery of similar choices 
(shown below).
While the Font tab of the Format Cells dialog box 
allows you to change the text attributes used to display 
cell contents, consider using the equivalent choices in the 
Font group of the Home tab (shown below). Using this 
group is a more convenient way of making choices such 
as changing the typeface or point size or styling text to be 
bold or italic.
To change the background color of a cell, click the fill 
icon in the Font group. Clicking this icon changes the back-
ground color to the color that appears behind the bucket 
(yellow in the illustration below). Clicking the drop-down 
button to the right of the fill icon displays a gallery of colors 
(shown below) from which you can select a color or click 
More Colors for even more choices. (The letter A icon and 
its drop-down button offer similar choices for the color of 
the text being displayed.)
To adjust the width of a column to an optimal size, ­select 
the column and then select Format ➔ Autofit ­Column 
Width (shown on page 754) in the Cells group. Excel will 

754	
Appendices
adjust the width of the column to accommodate the display 
of the values in all of the cells of the column.
B.6  Chart Formatting
Microsoft Excel does not always use best practices when con-
structing charts. Many of the In-Depth Excel instructions that 
involve charts refer you to this section so that you can correct 
the formatting of a chart that was just constructed. To apply 
any of the following corrections, you must first select the chart 
that is to be corrected. (If Chart Tools or PivotChart Tools 
appears above the Ribbon tabs, you have selected a chart.)
If, when you open to a chart sheet, the chart is either 
too large to be fully seen or too small and surrounded by a 
frame mat that is too large, click Zoom Out or Zoom In, lo-
cated in the lower-right portion of the Excel window frame, 
to adjust the chart display.
In the following, instructions preceded with (2013) ap-
ply only to Excel 2013. Unlike other Excel versions, in Excel 
2013 some of the selections, such as the gridlines selections, 
are toggles that turn on (or off) a chart element.
Changes You Most Commonly Make
To relocate a chart to its own chart sheet:
	 1.	Click the chart background and click Move Chart from 
the shortcut menu.
	 2.	In the Move Chart dialog box, click New Sheet, enter a 
name for the new chart sheet, and click OK.
To turn off the improper horizontal gridlines:
(2013) Design ➔ Add Chart Element ➔  
Gridlines ➔ Primary Major Horizontal
Layout ➔ Gridlines ➔ Primary Horizontal ­ 
Gridlines ➔ None
To turn off the improper vertical gridlines:
(2013) Design ➔ Add Chart Element ➔  
Gridlines ➔ Primary Major Horizontal
Layout ➔ Gridlines ➔ Primary Vertical  
Gridlines ➔ None
To turn off the chart legend:
(2013) Design ➔ Add Chart Element ➔ Legend ➔ 
None
Layout ➔ Legend ➔ None
If you use Excel 2007, you will also need to apply these 
changes:
Layout ➔ Data Labels ➔ None
Layout ➔ Data Table ➔ None
These two apply only to Excel 2007.
Chart and Axis Titles
To add a chart title to a chart missing a title:
	 1.	In Excel 2013, select Design ➔ Add Chart Element E3 
Chart Title ➔ Above Chart. Otherwise, click on the 
chart and then select Layout ➔ Chart Title ➔ Above 
Chart. 
	 2.	In the box that is added to the chart, select the words 
“Chart Title” and enter an appropriate title.
To add a title to a horizontal axis missing a title:
	 1.	In Excel 2013, select Design ➔ Add Chart Element 
➔ Axis Titles ➔ Primary Horizontal. Otherwise, click 
on the chart and then select Layout ➔ Axis Titles ➔ 
Primary Horizontal Axis Title ➔ Title Below Axis.
	 2.	In the box that is added to the chart, select the words 
“Axis Title” and enter an appropriate title.
To add a title to a vertical axis missing a title:
	 1.	In Excel 2013, select Design ➔ Add Chart Element 
➔ Axis Titles ➔ Primary Vertical. Otherwise, click 
on the chart and then select Layout ➔ Axis Titles ➔ 
Primary Vertical Axis Title ➔ Rotated Title.
	 2.	In the box that is added to the chart, select the words 
“Axis Title” and enter an appropriate title.
Chart Axes
To turn on the display of the X axis, if not already shown:
(2013) Design ➔ Add Chart Element ➔ Axes ➔ 
­Primary Horizontal
Layout ➔ Axes ➔ Primary Horizontal Axis ➔ 
Show Left to Right Axis (or Show Default Axis, 
if listed)
To turn on the display of the Y axis, if not already shown:
(2013) Design ➔ Add Chart Element ➔ Axes ➔ 
­Primary Vertical
Layout ➔ Axes ➔ Primary Vertical Axis ➔ Show 
­Default Axis
For a chart that contains secondary axes, to turn off the sec-
ondary horizontal axis title:
(2013) Design ➔ Add Chart Element ➔ Axis Titles ➔   
Secondary Horizontal
Layout ➔ Axis Titles ➔ Secondary Horizontal ➔ 
Axis Title ➔ None

	
Appendix B  Required Excel Skills	
755
For a chart that contains secondary axes, to turn on the sec-
ondary vertical axis title:
(2013) Design ➔ Add Chart Element ➔ Axis Titles ➔  
Secondary Vertical
Layout ➔ Axis Titles ➔ Secondary Vertical Axis  
Title ➔ Rotated Title
Correcting the Display of the X Axis
In scatter plots and related line charts, Microsoft Excel dis-
plays the X axis at the Y axis origin 1Y = 02. For plots that 
have negative values, this causes the X axis not to appear 
at the bottom of the chart. To relocate the X axis so that it 
appears at the bottom of a scatter plot or line chart, open to 
the chart sheet containing the chart and:
	 1.	Right-click the Y axis and click Format Axis from the 
shortcut menu.
In the Format Axis dialog box:
	 2.	Click Axis Options in the left pane. In the Axis Options 
pane on the right, click Axis value and in its box enter 
the value shown in the dimmed Minimum box (near 
the top of the pane).
	 3.	Click Close.
Emphasizing Histogram Bars
To better emphasize each bar in a histogram, open to the 
chart sheet containing the histogram and:
	 1.	Right-click over one of the histogram bars and click 
Format Data Series in the shortcut menu.
In the Format Data Series dialog box:
	 2.	Click Border Color in the left pane. In the Border 
Color right pane, click Solid line. From the Color drop-
down list, click the darkest color in the same column as 
the currently selected (highlighted) color.
	 3.	Click Border Styles in the left pane. In the Border 
Styles right pane, click the up spinner button to set the 
Width to 3 pt.
	 4.	Click OK.
B.7  Selecting Cell Ranges 
for Charts
Selecting Cell Ranges for Chart Labels 
and Series
As a general rule, you either type that cell range or select the 
cell range by using the mouse pointer to enter a cell range in 
a Microsoft Excel dialog box. You are free to choose to enter 
the cell range either using relative or absolute references 
(see Section B.2). The Axis Labels and Edit Series dialog 
boxes, associated with chart labels and data series, are two 
exceptions. These dialog boxes and their contents for the 
Pareto chart sheet of the Pareto workbook are shown below.
To enter a cell range into these two dialog boxes, you 
must enter the cell range as a formula that uses absolute 
cell references in the form WorksheetName!UpperLeftCell: 
LowerRightCell. This is best done using the mouse-pointer 
method to enter these cell ranges. Typing the cell range, as 
you might normally do, will often be frustrating, as keys 
such as the cursor keys will not function as they do in other 
dialog boxes.
Selecting a Non-contiguous  
Cell Range
Typically, you enter a non-contiguous cell range such as 
the cells A1:A11 and C1:C11 by typing the cell range of 
each group of cells, separated by commas—for example, 
A1:A11, C1:C11. In certain contexts, such as using the 
dialog boxes discussed in the preceding section, you will 
need to select that non-contiguous cell range using the 
mouse pointer method. To use the mouse-pointer method 
with such ranges, first, select the cell range of the first 
group of cells and then, while holding down Ctrl, select 
the cell range of the other groups of cells that form the 
non-contiguous cell range.
B.8  Deleting the “Extra” 
Bar from a Histogram
As explained in “Classes and Excel Bins” on page 73, you 
use bins to approximate classes. One result of this approxi-
mation is that you will always create an “extra” bin that will 
have a frequency of zero. Because, by definition, this extra 
bin considers values that are less than the lowest value that 
exists in your set of data and therefore will always have the 
frequency zero, you can safely and properly eliminate the 
“extra” bar that represents this bin.
To do so, you need to edit the cell range that Excel uses 
to construct the histogram. Right-click the histogram back-
ground and click Select Data. In the Select Data Source 
Data dialog box, first click Edit under the Legend Entries 
­(Series) heading. In the Edit Series dialog box, edit the ­Series 
values cell range formula to begin with the second cell of the 

756	
Appendices
original cell range and click OK. Then click Edit under the 
Horizontal (Categories) Axis Labels heading. In the Axis 
Labels dialog box, edit the Axis label range to begin with the 
second cell of the original cell range and click OK.
B.9  Creating Histograms 
for Discrete Probability 
Distributions
You can create a histogram for a discrete probability distribu-
tion based on a discrete probabilities table. For example, to 
create the Figure 5.3 histogram of the binomial probability  
distribution on page 227, open to the COMPUTE worksheet 
of the Binomial workbook. Select the cell range B14:B18, 
the probabilities in the Binomial Probabilities Table, and:
	 1.	Select Insert ➔ Column and select the first 2-D 
Column gallery choice (Clustered Column).
	 2.	Right-click the chart background and click Select Data.
In the Select Data Source dialog box:
	 3.	Click Edit under the Horizontal (Categories) Axis 
Labels heading.
	 4.	In the Axis Labels dialog box, enter the cell range 
formula =COMPUTE!A14:A18 as the Axis label 
range. (See Section B.7 to learn how to best enter this 
cell range formula.) Click OK to return to the Select 
Data Source dialog box.
	 5.	Back in the Select Data Source dialog box, click OK.
In the chart:
	 6.	Right-click inside a bar and click Format Data Series 
in the shortcut menu.
In the Format Data Series dialog box:
	 7.	Click Series Options in the left pane. In the Series 
Options right pane, change the Gap Width slider to 
Large Gap. Click Close.
Relocate the chart to a chart sheet and adjust the chart for-
matting by using the instructions in Section B.6.

757
C.1  About the Online 
Resources for This Book
Online resources support your study of business statistics 
and your use of this book. Online resources are available 
from a special download web page for this book as well as 
in a MyStatLab course for this book. On the download page, 
these resources are packaged as a series of zip archive files, 
one zip file for each of the categories listed below. In the 
MyStatLab course for this book, online resources are also 
available on a chapter-by-chapter basis. Categories of online 
resources are:
• Excel and Minitab Data files  The files that contain 
the data used in chapter examples, named in prob-
lems, or used in the end-of-chapter cases, including 
the Managing Ashland MultiComm Services running 
case. A complete list of these files and their contents 
appears in Section C.3.
• Excel Guide Workbooks  Excel workbooks that 
contain templates or model solutions for applying 
Excel to a particular statistical method. A complete 
list of the Excel Guide Workbooks appear in Excel 
Guide Workbooks in Section C.3.
• Files for the Digital Cases  The set of PDF files that 
support the end-of-chapter Digital Cases. Some of the 
Digital Case PDF files contain attached or embedded 
Excel data workbooks for use with particular case 
questions.
• Online Topics  The set of PDF format files that 
present additional statistical topics. Included in 
this set is the full text of two chapters, “Statistical 
Applications in Quality Management” and “Decision 
Making.”
• Short Takes  The set of PDF files that extend the 
discussion of specific concepts or further document 
the results presented in the book.
• Visual Explorations Workbooks  The workbooks 
that interactively demonstrate various key statisti-
cal concepts. Three of these workbooks are add-in 
workbooks stored in the .xlam Excel add-in format. 
See Visual Explorations in Section C.3 for additional 
information.
As explained in Section EG.1 on page 36, this book also 
supports the use of PHStat, the Pearson Education statistics 
add-in for Microsoft Excel. If you plan to use PHStat with 
this book, see Section C.4.
Accessing the Online Resources
Online resources for this book are available either on the stu-
dent download page for this book or inside the MyStatLab 
course for this book (see Section C.3). To access resources 
from the student download page for this book:
	 1.	Visit www.pearsonhighered.com/levine.
	 2.	In that web page, find the entries for this book, Basic 
Business Statistics, thirteenth edition, and click the stu-
dent download page link.
	 3.	In the download page, click the link for the desired 
items. Most items will cause the web browser to prompt 
you to save the (zip archive) that you can save and later 
unzip. Some download links may require an access code 
(see Section C.2). 
C.2  Accessing the MyStatLab 
Course Online
The MyStatLab course for this book contains all the online 
resources for this book. Log into the course and in the 
left panel of the course page for this book, click Tools for 
Success. On that page, click the link for one of the online 
resource categories listed in Section C.1. Selected files and 
data workbooks may also be available in the chapter-by-
chapter resource pages.
Using MyStatLab requires an access code. An access 
code may have been packaged with this book. If your book 
did not come with an access code, you can obtain one at 
mypearson.com.
C.3  Details of 
Downloadable Files
Data Files
Data workbooks contain the data used in chapter exam-
ples or named in problems. Throughout this book, the 
names of data workbooks appear in a special inverted color 
­typeface—for example,  Retirement Funds . Data files are 
stored as worksheets in both the .xlsx Excel workbook and 
the .mtw Minitab worksheet file formats. (For files that con-
tain more than one worksheet, Minitab versions are stored 
as .mpj Minitab project files.) Worksheets organize the data 
for each variable by column, using the rules discussed in 
Sections EG.2 and MG.2.
A p p e n d i x  C    Online Resources

758	
Appendices
In the following alphabetical list, the variables for each 
data file are presented in the order of their appearance, 
starting with first column (A in Excel and C1 in Minitab). 
­Chapter references indicate the chapter or chapters that use 
the data file in an example or problem. A trailing (E) notes a 
file exclusive to Excel. A trailing (M) notes a file exclusive 
to Minitab.
311CALLCENTER  Day and abandonment rate (%) 
(Chapter 3)
ACCOUNTINGPARTNERS  Firm and number of part-
ners (Chapter 3)
ACCOUNTINGPARTNERS2  Region and number of 
partners (Chapter 10)
ACCOUNTINGPARTNERS4  Region and number of 
partners (Chapter 11)
ACCOUNTINGPARTNERS6  Region, revenue ($mil-
lions), number of partners, number of professionals, MAS 
(%), southeast 10 = no, 1 = yes2 Gulf coast southeast 
10 = no, 1 = yes2 (Chapter 15)
ACT  Method (online or traditional), ACT scores for 
­condensed course, ACT scores for regular course  
(Chapter 11)
ACT-ONEWAY  Group 1 ACT scores, group 2 ACT scores, 
group 3 ACT scores, group 4 ACT scores (Chapter 11)
ADINDEX  Respondent, cola A Adindex, and cola B 
Adindex (Chapter 10)
ADVERTISE  Sales ($thousands), radio ads ($thousands), 
and newspaper ads ($thousands) for 22 cities (Chapters 14, 
15, and 17)
ADVERTISING  Sales ($millions) and newspaper ads 
($thousands) (Chapter 15)
AMS2-1  Types of errors and frequency, types of errors and 
cost, types of wrong billing errors and cost (as three sepa-
rate worksheets) (Chapter 2)
AMS2-2  Days and number of calls (Chapter 2)
AMS8  Rate willing to pay ($) (Chapter 8)
AMS9  Upload speed (Chapter 9)
AMS10  Update times for email interface 1 and email inter-
face 2 (Chapter 10)
AMS11-1  Update time for system 1, system 2, and system 
3 (Chapter 11)
AMS11-2  Technology (cable or fiber) and interface (sys-
tem 1, system 2, or system 3) (Chapter 11)
AMS13  Number of hours spent telemarketing and number 
of new subscriptions (Chapter 13)
AMS14  Week, number of new subscriptions, hours spent 
telemarketing, and type of presentation (formal or infor-
mal) (Chapter 14)
AMS16  Month and number of home delivery subscriptions 
(Chapter 16)
ANSCOMBE  Data sets A, B, C, and D, each with 11 pairs 
of X and Y values (Chapter 13)
ATM TRANSACTIONS  Cause, frequency, and percent-
age (Chapter 2)
AUDITS  Year and number of audits (Chapters 2 and 16)
AUTOMAKER1  Automaker and number of complaints 
(Chapters 2 and 17)
AUTOMAKER2  Category and number of complaints 
(Chapter 2)
AUTOSALES  Manufacturer, sales, and change percentage 
(Chapter 17)
BANK1  Waiting time (in minutes) of 15 customers at a bank 
located in a commercial district (Chapters 3, 9, 10, and 12)
BANK2  Waiting time (in minutes) of 15 customers at a 
bank located in a residential area (Chapters 3, 10, and 12)
BANKMARKETING  Age, type of job, marital status 
(divorced, married, or single), education (primary, sec-
ondary, tertiary, or unknown), is credit in default, mean 
yearly balance in account, is there a housing loan, is 
there a personal loan, last contact duration in seconds, 
number of contacts performed during this campaign, has 
the client purchased a term deposit (also contains the 
BinaryLogisticDATA worksheet that contains recoded 
variables) (Chapter 18)
BASEBALL  Team; attendance; high temperature on 
game day; winning percentage of home team; oppo-
nent's winning percentage; game played on week-
end day 10 = no, 1 = yes2 and promotion held 
10 = no, 1 = yes2 (Chapter 18)
BB2012  Team, league 10 = American, 1 = national2 
wins, earned run average, runs scored, hits allowed, walks 
allowed, saves, and errors (Chapters 13, 14, 15, and 17)
BBCOST2012  Team and fan cost index (Chapters 2, 6,  
and 17)
BBREVENUE2013  Team, revenue ($millions), and value 
($millions) (Chapter 13)
BBSALARIES  Year and average major league baseball 
salary ($millions) (Chapter 16)
BED & BATH  Year, coded year, and number of stores open 
(Chapter 16)
BESTFUNDS1  Fund type (short-term or long-term), 
1-year return, and 3-year return (Chapters 10 and 17)
BESTFUNDS2  Fund type (short-term, long-term, or 
world), 1-year return, and 3-year return (Chapter 11)
BESTFUNDS3  Fund type (small, mid-cap, or large), 
1-year return, and 3-year return (Chapter 11)

	
Appendix C  Online Resources	
759
BOOKPRICES  Author, title, bookstore price, and online 
price ($) (Chapter 10)
BRAKES  Part, gauge 1, and gauge 2 (Chapter 11)
BRANDZTECHFIN  Brand, brand value in 2011 ($mil-
lions), % change in brand value from 2010, region, and 
sector (Chapters 10 and 12)
BRANDZTECHFINTELE  Brand, brand value in 2011 
($millions), % change in brand value from 2010, region, 
and sector (Chapter 11)
BREAKFAST  Type (Continental or American), delivery 
time difference for early time period, and delivery time 
difference for late time period (Chapter 11)
BREAKFAST2  Type (Continental or American), delivery 
time difference for early time period, and delivery time 
difference for late time period (Chapter 11)
BRYNNEPACKAGING  WPCT score and rating  
(Chapter 13)
BULBS  Manufacturer (1 = A, 2 = B) and length of life 
(hours) (Chapters 2, 10, and 12)
BUNDLE  Restaurant, bundle score, and typical cost ($) 
(Chapter 2)
BUSINESSVALUATION  Drug company name, price to 
book value ratio, return on equity (ROE), and growth% ­ 
(Chapters 14 and 17)
BUSINESSVALUATION2  Company, ticker symbol, 
Standard Industrial Classification 3 (SIC3) code, Standard 
Industrial Classification 4 (SIC4) code, price to book 
value ratio, price to earnings ratio, natural log of assets 
(as a measure of size), return on equity (ROE), growth 
percentage (GS5), debt to EBITDA ratio, dummy variable 
indicator of SIC 4 code 2834 11 = 2834, 0 = not 28242, 
and dummy variable indicator of SIC 4 code 2835 
11 = 2835, 0 = not 28352 (Chapter 15)
CABERNET  California wine rating, Washington wine rat-
ing, California wine ranking, and Washington wine rank-
ing (Chapter 12)
CAFFEINE  Caffeine per fluid ounce (mg/oz) (Chapter 2)
CALLCENTER  Month and call volume (Chapter 16)
CAMERAS  Subcompact battery life, compact battery life 
(Chapters 10 and 12)
CANDIDATE ASSESSMENT  Salary, competence rating, 
gender of candidate (F or M), gender of rater (F or M), 
rater/candidate gender (F to F, F to M, M to M, M to M), 
school (Private, Public), department (Biology, Chemistry, 
Physics), and age of rater (Chapter 18)
CARDIOGOODFITNESS  Product purchased (TM195, 
TM498, TM798), age in years, gender (Male or Female), 
education in years, relationship status (Single or 
Partnered), average number of times the customer plans 
to use the treadmill each week, self-rated fitness on a 
1-to-5 ordinal scale 11 = poor to 5 = excellent2, annual 
household income ($), and average number of miles the 
customer expects to walk/run each week (Chapters 2, 3, 
6, 8, 10, 11, and 12)
CARDSTUDY  Upgraded 10 = no, 1 = yes2, pur-
chases ($thousands), and extra cards 10 = no, 1 = yes2 
(Chapters 14 and 17)
CARPRODUCTION  Year, coded year, and number of 
units produced (Chapter 16)
CATFOOD  Ounces eaten of kidney, shrimp, chicken liver, 
salmon, and beef cat food (Chapters 11 and 12)
CATFOOD2  Piece size 1F = fine, C = chunky2, coded 
weight for low fill height, and coded weight for current 
fill height (Chapter 11)
CDRATE  Bank, 1-year CD rate, and 5-year CD rate 
(Chapters 2, 3, 6, and 8)
CEO-COMPENSATION  Company, CEO compensation 
($millions), and return in 2012 (Chapters 2, 3, and 13)
CEREALS  Cereal, calories, carbohydrates, and sugar 
(Chapters 3, 13, and 17)
CHALLENGING  Data and charts for Figure 2.19 (Chapter 2)
CHURN  Customer ID, churn coded 10 = no, 1 = Yes2, 
churn, calls, and visits (Chapters 14 and 17)
CIGARETTETAX  State and cigarette tax ($) (Chapters 2 
and 3)
CIRCUITS  Batch, position 1, position 2, position 18, posi-
tion 19, and position 28 (Chapter 11)
COCA-COLA  Year, coded year, and revenues ($billions) 
(Chapter 16)
COFFEE  Expert and rating of coffees by brand A, B, C, 
and D (Chapters 10 and 11)
COFFEEPRICESPORTUGAL  Year and retail price of 
coffee in Portugal (€/kg) (Chapter 16)
COFFEESALES  Coffee sales at $0.59, $0.69, $0.79, and 
$0.89 (Chapters 11 and 12)
COFFEESALES2  Coffee sales and price (Chapter 15)
COLA  Beverage end-cap sales and produce end-cap sales 
(Chapters 10 and 12)
COLLEGE FOOTBALL  Head coach, school, conference, 
school pay of head coach, other pay, total pay, max bonus, 
and football net revenue (Chapters 2, 3, and 13)
COMPUTERSALES  Year, coded year, and computer and 
software sales ($millions) (Chapter 16)
CONCRETE1  Sample number and compressive strength 
after two days and seven days (Chapter 10)
CONCRETE2  Sample number and compressive strength 
after 2, 7, and 28 days (Chapter 11)

760	
Appendices
CONGESTION  City, annual time waiting in traffic (hours), 
and cost of waiting in traffic ($) (Chapters 2 and 3)
COSTESTIMATION  Units produced and total cost ($) 
(Chapter 15)
CPI-U  Year, coded year, and value of CPI-U (the consumer 
price index) (Chapter 16)
CREDIT SCORES  City, state, and average credit score 
(Chapters 2 and 3)
CURRENCY  Year, coded year, and exchange rates (against 
the U.S. dollar) for the Canadian dollar, Japanese yen, and 
English pound sterling (Chapters 2 and 16)
CURRENCY2  Currency and value of US$1 for years from 
2002 through 2012 (Chapter 17)
DELIVERY  Customer, number of cases, and delivery time 
(Chapter 13)
DENSITY  Ammonium percentage, density for stir rate of 
100, and density for stir rate of 150 (Chapter 11)
DOINGBUSINESS  Region, country name, 2012 GDP per 
capita, Internet users 2011 (per 100 people), and mobile 
cellular subscriptions 2011 (per 100 people) (Chapter 17)
DOMESTICBEER  Brand, alcohol percentage, calories, 
and carbohydrates (Chapters 2, 3, 6, and 15)
DOMESTICBEER2  Brand, calories, and carbohydrates 
(Chapter 17)
DOWDOGS  Stock and 1-year return (Chapter 3)
DOWMARKETCAP  Company and market capitalization 
($billions) (Chapters 3 and 6)
DRILL  Depth, time to drill additional 5 feet, and type of 
hole (dry or wet) (Chapters 14 and 17)
DRINK  Amount of soft drink filled in 2-liter bottles 
(Chapters 2 and 9)
DRIVE-THRUSPEED  Year and drive-thru speed in  
seconds (Chapter 16)
ENERGY  State and per capita kilowatt hour use  
(Chapter 3)
ENTREE  Type and number served (Chapter 2)
ERWAITING  Emergency room waiting time (in minutes) 
at the main facility and at satellite 1, satellite 2, and satel-
lite 3 (Chapters 11 and 12)
ESPRESSO  Tamp (inches) and time (seconds) (Chapter 13)
EUROTOURISM  Country, employment in tourism 2012, 
tourism establishments (Chapter 15)
EUROTOURISM2  Country, employment in tourism 2012, 
business travel & tourism spending 2012 (US$millions), 
international visitors 2012, tourism establishments 
(Chapter 18)
FACEBOOKTIME  Gender (F or M) and amount of time 
in minutes spent on Facebook per day (Chapter 9)
FACEBOOKTIME2  Gender (F or M) and amount of time 
in minutes spent on Facebook per day (Chapter 10)
FALSE IMPRESSIONS  Data and charts for selected 
Section 2.7 figures (Chapter 2) (E)
FASTFOOD  Amount spent on fast food ($) (Chapters 2 3, 
8, and 9)
FEDRECEIPT  Year, coded year, and federal receipts 
($billions current) (Chapter 16)
FIFTEENWEEKS  Week number, number of customers, 
and sales ($thousands) over a period of 15 consecutive 
weeks (Chapter 13)
FIVEYEARCDRATE  Five-year CD rates in New York 
and Los Angeles (Chapter 10)
FLYASH  Fly ash percentage and strength (PSI) (Chapter 15)
FOODS  Type, bland/spicy rating, light/heavy rating, low/
high calories rating (Chapter 17)
FORCE  Force required to break an insulator (Chapters 2, 
3, 8, and 9)
FOREIGNMARKET  Country, level of development 
(Emerging or Developed), and time required to start a 
business (days) (Chapter 10)
FOREIGNMARKET2  Country, region, cost to export con-
tainer (US$), cost to import container (US$) (Chapters 11 
and 12)
FREEPORT  Address, fair market value ($thousands), 
property size (acres), house size, age, number of rooms, 
number of bathrooms, and number of cars that can be 
parked in the garage (Chapter 15)
FRESHFOOD  Fresh food, United States pounds per capita 
consumed, Japan pounds per capita consumed, and Russia 
pounds per capita consumed (Chapter 2)
FTGLOBAL500  Sector (Automobiles & parts, Financial 
services, Health care equipment & services, or Software 
& computer services), country, company, market cap 
($billions), and 52-week change (%) (Chapter 17)
FTMBA  School name, program cost ($), total students 
per program, job offers (%), and mean starting salary ($) 
(Chapters 15 and 17)
FURNITURE  Days between receipt and resolution of 
complaints regarding purchased furniture (Chapters 2, 3, 
8, and 9)
GASPRICES  Month and price per gallon ($) (Chapter 16)
GCFREEROSLYN  Address, location (Glen Cove, 
Freeport, or Roslyn), fair market value ($thousands), 
property size (acres), age, house size (sq. ft.), number of 
rooms, number of bathrooms, and number of cars that can 
be parked in the garage (Chapter 15)
GCROSLYN  Address, location (Glen Cove or Roslyn), 
fair market value ($thousands), property size (acres), 
age, house size (sq. ft.), number of rooms, number of 
bathrooms, and number of cars that can be parked in the 
garage (Chapters 14 and 15)
GDP  Year and gross domestic product (Chapter 16)

	
Appendix C  Online Resources	
761
GLENCOVE  Address, fair market value ($thousands), 
property size (acres), age, house size (sq. ft.), number of 
rooms, number of bathrooms, and number of cars that can 
be parked in the garage (Chapters 14, 15, and 17)
GLOBALSOCIALMEDIA  Country, GDP, and social 
media usage (%) (Chapters 2, 3, 13, and 17)
GOLD  Quarter, coded quarter, price ($), Q1, Q2, and Q3 
(Chapter 16)
GOLFBALL  Distance for designs 1, 2, 3, and 4 (Chapters 11 
and 12)
GPIGMAT  GMAT scores and GPA (Chapter 13)
GRADSURVEY  ID, gender (Female or Male), age (as of last 
birthday), graduate major (Accounting, CIS, Economics/
Finance, International Business, Management, Retailing/
Marketing, or Other), current graduate GPA, undergradu-
ate major (Biological Sciences, Business, Engineering, or 
Other), undergraduate GPA, current employment status 
(Full-Time, Part-Time, or Unemployed), number of dif-
ferent full-time jobs held in the past 10 years, expected 
salary upon completion of MBA ($thousands), amount 
spent for books and supplies this semester ($), advisory 
rating, type of computer owned (Desktop or Laptop), 
text messages per week, wealth accumulated to feel rich 
(Chapters 2, 3, 6, 8, 10, 11, and 12)
GRANULE  Granule loss in Boston and Vermont shingles 
(Chapters 3, 8, 9, and 10)
HEATINGOIL  Monthly consumption of heating oil (gal-
lons), temperature (degrees Fahrenheit), attic insulation 
(inches), ranch-style (0 =  not ranch-style, 1 =  ranch-
style) (Chapters 14 and 15)
HEMLOCKFARMS  Asking price, hot tub 10 = no,
1 = yes2, rooms, lake view 10 = no, 1 = yes2, bath-
rooms, bedrooms, loft/den 10 = no, 1 = yes2, finished 
basement 10 = no, 1 = yes2, number of acres (Chapter 15)
HOTELAWAY  Nationality and cost (British pounds ster-
ling) (Chapter 3)
HOTELPRICES  City and average price (US$) of a hotel 
room at a 2-star price, 3-star price, and 4-star hotel 
(Chapters 2 and 3)
HOUSEHOLDS  ID, Gender, age, Hispanic origin (N 
or Y), dwelling type (AB, AH, DH, or Other), age of 
dwelling (years), years living at dwelling, number of 
bedrooms, number of vehicles kept at dwelling, fuel 
type at dwelling (Electric, Gas, Oil, or Other), monthly 
cost of fuel at dwelling ($), U.S. citizenship (N or Y), 
college degree (N or Y), marital status (D, M, NM, S, or 
W), work for pay in previous week (N or Y), mode of 
transportation to work (Bus, Car, Home, Subway/Rail, 
Taxi, Other, or NA), commuting time ( minutes), hours 
worked per week, type of organization(GOV, NA, PP, 
PNP, or SE), annual earned income ($), and total annual 
income ($) (Chapter 18)
HYBRIDSALES  Year and number sold (Chapter 18)
ICECREAM  Daily temperature (in degrees Fahrenheit) 
and sales ($thousands) for 21 days (Chapter 13)
INDICES  Year, change in DJIA, S&P500, and NASDAQ 
(Chapter 3)
INSURANCE  Processing time in days for insurance poli-
cies (Chapters 3, 8, and 9)
INSURANCECLAIMS  Claims, buildup (0 = buildup not 
indicated, 1 = buildup indicated), excess payment ($) 
(Chapter 8)
INSURANCEFRAUD  ID, fraud coded 10 = no, 1 = yes2, 
fraud (No or Yes), new business coded 10 = no, 1 = yes2, 
new business (No or Yes), and claims/year (Chapters 14 
and 17)
INVOICE  Number of invoices processed and amount of 
time (hours) for 30 days (Chapter 13)
INVOICES  Amount recorded (in dollars) from sales 
invoices (Chapter 9)
LUGGAGE  Delivery time (in minutes) for luggage in 
Wing A and Wing B of a hotel (Chapters 10 and 12)
MANAGERS  Sales (ratio of yearly sales divided by the 
target sales value for that region), Wonderlic Personnel 
Test score, Strong-Campbell Interest Inventory Test score, 
number of years of selling experience prior to becoming a 
sales manager, whether the sales manager has a degree in 
electrical engineering (No or Yes) (Chapter 15)
MARKET PENETRATION  Country and Facebook pen-
etration (in percentage) (Chapters 3 and 8)
MCDONALDS Year, coded year, and annual total revenues 
($billions) at McDonald's Corporation (Chapter 16)
MEDICALWIRES1  Machine type, narrow, and wide 
(Chapter 11)
MEDICALWIRES2  Narrow, and wide (Chapter 11)
METAL INDICES  Metal and the total rate of return (%) 
for the years 2006 through 2012 (Chapter 17)
METALS  Year and the total rate of return (in percentage) 
for platinum, gold, and silver (Chapter 3)
MINING  Day, amount stacked, and downtime (Chapter 18)
MINING2  Day; hours of downtime due to mechanical, 
electrical, tonnage restriction, operator, and no feed; and 
total hours (Chapter 18)
MMCDRATE  Bank, and interest rates for money market, 
1-year CD, 2-year CD, and 5-yr CD (Chapter 11)
MOBILE ELECTRONICS STACKED  Stacked version 
of Mobile Electronics (Chapter 11) (M)
MOBILE ELECTRONICS  In-aisle sales, front sales, 
kiosk sales, and expert area sales (Chapter 11) (E)
MOBILE ELECTRONICS2  Mobile payments (No or 
Yes), in-aisle sales, front sales, kiosk sales, and expert 
area sales (Chapter 11)

762	
Appendices
MOISTURE  Moisture content of Boston shingles and 
Vermont shingles (Chapter 9)
MOLDING  Vibration time (seconds), vibration pressure 
(psi), vibration amplitude (%), raw material density (g/mL),  
quantity of raw material (scoops), product length in  
cavity 1 (in.), product length in cavity 2 (in.), product 
weight in cavity 1 (gr.), and product weight in cavity 2 
(gr.) (Chapter 15)
MOTIVATION  Factor, mean rating by global employees, 
and mean rating by U.S. employees (Chapter 10)
MOVIE  Title, box office gross ($millions), and DVD rev-
enue ($millions) (Chapter 13)
MOVIE ATTENDANCE  Year and movie attendance (bil-
lions) (Chapters 2 and 16)
MOVIE ATTENDANCE2  Movie attendance (billions) for 
years 2002 through 2012 (Chapter 17)
MOVIE REVENUES  Year and revenue ($billions)  
(Chapter 2)
MOVING  Labor hours, cubic feet, number of large pieces 
of furniture, and availability of an elevator (Chapters 13, 
14, and 17)
MYELOMA  Patient, before transplant measurement, after 
transplant measurement (Chapter 10)
NATURAL GAS  Month, wellhead price ($/thousands cu. 
ft.), and residential price ($/thousands cu. ft.) (Chapter 2)
NATURAL GAS2  Month, wellhead price ($/thousands cu. 
ft.), and residential price ($/thousands cu. ft.) (Chapter 16)
NBA2012  Team, number of wins, points scored, points 
allowed, point difference, field goal %, field goal % allowed, 
field goal % difference, own turnovers, opponent turnovers, 
turnover difference, and rebounds % (Chapters 14 and 18)
NBAVALUES  Team, team code, annual revenue ($mil-
lions), and value ($millions) and 1-year change in value 
(%) (Chapters 2, 3, 13, and 17)
NEEDS  Need and frequency (Chapter 2)
NEIGHBOR  Selling price ($thousands), number of rooms, 
neighborhood location 10 = east, 1 = west2 (Chapter 14)
NEWHOMESALES  Month, sales in thousands, and mean 
price ($thousands) (Chapter 2)
OIL&GASOLINE  Week, price of a gallon of gasoline ($), 
and price of oil per barrel, ($) (Chapter 13)
OMNIPOWER  Bars sold, price (cents), and promotion 
expenses ($) (Chapters 14 and 17)
ONLINE SHOPPING  Main reason and percentage 
(Chapter 2)
ORDER  Time in minutes to fill orders for a population of 
200 (Chapter 8)
ORGANICFOOD  Customer, organic food purchaser 
10 = no, 1 = yes2, age, online health wellness e-news-
letters subscriber 10 = no, 1 = yes2 (Chapter 14)
O-RING  Flight number, temperature, and O-ring damage 
index (Chapter 13)
PACKAGEDFOOD  Category, United States sales, Japan 
sales, Russia sales  (Chapter 17)
PACKAGINGFOAM1  Die temperature, 3 mm. diameter, 
and 4 mm. diameter (Chapter 11)
PACKAGINGFOAM2  Die temperature, 3 mm. diameter, 
and 4 mm. diameter (Chapter 11)
PACKAGINGFOAM3  Die temperature, die diameter, and 
foam density (Chapter 14)
PACKAGINGFOAM4  Die temperature, die diameter, and 
foam diameter (Chapter 14)
PAINRELIEF  Temperature and dissolve times for Equate, 
Kroger, and Alka-Seltzer tablets (Chapter 11)
PALLET  Weight of Boston shingles and weight of Vermont 
shingles (Chapters 2, 8, 9, and 10)
PARACHUTE1WAY  Tensile strength of parachutes from 
suppliers 1, 2, 3, and 4 (Chapter 11)
PARACHUTE2WAY  Loom and tensile strength of para-
chutes from suppliers 1, 2, 3, and 4 (Chapter 11)
PEN  Ad and product rating (Chapters 11, 12)
PHILLY  Zip code, population, median sales price 2012 
($000), average days on market 2012, units sold 2012, 
median household income ($), percentage of residents 
with a BA or higher, and hotness 10 = not hot, 1 = hot2 
(Chapter 18)
PHONE  Time (in minutes) to clear telephone line prob-
lems and location 11 = I, 2 = II2 (Chapters 10 and 12)
PIZZAHUT  Gender coded 10 = Female, 1 = Male2, 
gender (Female or Male), price ($), and purchase (0 =  
student selected another pizzeria, 1 =  student selected 
Pizza Hut) (Chapters 14 and 17)
PIZZATIME  Time period, delivery time for local restau-
rant, and delivery time for national chain (Chapter 10)
POLIO  Year and incidence rates per 100,000 persons of 
reported poliomyelitis (Chapter 16)
POTATO  Percentage of solids content in filter cake, acidity 
(pH), lower pressure, upper pressure, cake thickness, var-
idrive speed, and drum speed setting for 54 measurements 
(Chapter 15)
POTTERMOVIES  Title, first weekend gross ($millions), 
U.S. gross ($millions), and worldwide gross ($millions) 
(Chapters 2, 3, 13, and 17)
PROPERTYTAXES  State and property taxes per capita 
($) (Chapters 2 and 3)
PROTEIN  Type of food, calories (in grams), protein, per-
centage of calories from fat, percentage of calories from 
saturated fat, and cholesterol (mg) (Chapters 2, 3 and 17)
PUMPKIN  Circumference and weight of pumpkins 
(Chapter 13)

	
Appendix C  Online Resources	
763
QSR  Company, average sales per unit, market segment 
(Chapters 11 and 12)
QSRCHAIN  Evaluator and ratings for Henry St, Surf Av, 
Granby, and Blvd N restaurants (Chapter 11)
REDANDWHITE  Fixed acidity, volatile acidity, citric 
acid, residual sugar, chlorides, free sulfur dioxide, total 
sulfur dioxide, density, pH, sulphates, alcohol, wine type 
coded 10 = White, 1 = Red2, wine type (Red or White), 
quality (Chapter 14)
REDWOOD  Height (ft.), breast height diameter (in.), and 
bark thickness (in.) (Chapters 13 and 14)
REGISTRATIONERROR  Registration error, tempera-
ture, pressure, and supplier (Chapter 15)
REGISTRATIONERROR-HIGHCOST  Registration 
error and temperature (Chapter 15)
RENTSILVERSPRING  Apartment size (sq. ft.) and 
monthly rental cost ($) (Chapter 13)
RESTAURANTS  Location (City or Suburban), food rat-
ing, decor rating, service rating, summated rating, coded 
location 10 = City, 1 = Suburban2, and cost of a meal 
(Chapters 2, 3, 10, 13, and 14)
RESTAURANTS2  Location, food rating, decor rating, 
service rating, cost of a meal, popularity index, and cui-
sine [American (New), Chinese, French, Indian, Italian, 
Japanese, or Mexican] (Chapter 18)
RETIREMENT FUNDS  Fund number, market cap (Small, 
Mid-Cap, or Large), type (Growth or Value), assets ($mil-
lions), turnover ratio, beta (measure of the volatility of a 
stock), standard deviation (measure of returns relative to 
36-month average), risk (Low, Average, or High), 1-year 
return, 3-year return, 5-year return, 10-year return, expense 
ratio, star rating (Chapters 2, 3, 6, 8, 10, 11, 12, 15, and 17)
ROSLYN  Address, fair market value ($thousands), prop-
erty size (acres), house size (sq. ft.), age, number of 
rooms, number of bathrooms, and number of cars that can 
be parked in the garage (Chapter 15)
SATISFACTION  Satisfaction code 10 = not satisfied,
1 = satisfied2, Satisfaction (No or Yes), delivery time 
difference (minutes), previous coded 10 = no, 1 = yes2, 
and previous (No or Yes) (Chapter 17)
SECOND EXPERIMENT  In-aisle sales, front sales, kiosk 
sales, and expert area sales (Chapter 12) (E)
SECOND EXPERIMENT STACKED  Stacked version of 
Second Experiment (Chapter 12) (M)
SEDANS  Miles per gallon for 2013 midsized sedans 
(Chapters 3 and 8)
SERVICETIME  Fast food chain and mean service time 
for years 1 through 12 (Chapter 17)
SILVER  Year and price of silver ($) (Chapter 16)
SILVER-Q  Quarter, coded quarter, price of silver ($), Q1, 
Q2, and Q3 (Chapter 16)
SILVERSPRING  Address, asking price ($000), assessed 
value ($000), taxes ($), size (thousands sq. ft.) fireplace 
coded 10 = no, 1 = yes2, number of bedrooms, number of 
bathrooms, age (years), fireplace (No or Yes) (Chapters 13,  
14, and 15)
SITESELECTION  Store number, profiled customers, and 
sales ($millions) (Chapter 13)
SMARTPHONES  Price ($) (Chapter 3)
SMARTPHONES SALES  Type, and market share per-
centage for the years 2011 through 2013 (Chapter 2)
SOCCERVALUES2013  Team, country, revenue ($mil-
lions), and value ($millions) (Chapter 13)
SOLAR POWER  Year and amount of solar power gener-
ated (megawatts) (Chapter 16)
SPILLS  Year and number of oil spills in the Gulf of Mexico 
(Chapter 16)
SPORTS  Sport, movement rating, speed rating, rules rat-
ing, team oriented rating, and amount of contact rating 
(Chapter 17)
STANDBY  Standby hours, total staff present, remote hours, 
Dubner hours, and total labor hours (Chapters 14 and 15)
STARBUCKS  Tear, viscosity, pressure, plate gap ­(Chapters 13, 
14 and 17)
STEEL  Error in actual length and specified length 
(Chapters 2, 6, 8, and 9)
STOCK INDICES  Stock index and percentage change for 
2006 through 2012 (Chapter 17)
STOCK PERFORMANCE  Decade and stock perfor-
mance (%) (Chapters 2 and 16)
STOCKPRICES2012  Date, S&P 500 value, and closing 
weekly stock price for GE, Discovery Communications, 
and Google (Chapter 13)
STUDYTIME  Gender and study time in hours (Chapter 10)
SUPERMARKETPRICES  Shopping item and price at 
Publix, Winn-Dixie, Target, and Walmart (Chapter 11)
SUV  Miles per gallon for 2013 small SUVs (Chapters 3, 6, 
and 8)
TABLE_5.1  X and P(X) (Chapter 5) (M)
TARGETWALMART  Shopping item, Target price ($), 
and Walmart price ($) (Chapter 10)
TAX  Quarterly sales tax receipts ($thousands) (Chapter 3)
TEABAGS  Weight of tea bags in ounces (Chapters 3, 8, and 9)
TELECOM  Provider, TV rating, and Phone rating (Chapter 10)
TELECOM2 Provider, TV rating, Phone rating, and Internet 
rating (Chapter 11)
TESTRANK  Rank scores and training method 10 =  
traditional, 1 = experimental2 for 10 people (Chapter 12)
THICKNESS  Thickness, catalyst, pH, pressure, tempera-
ture, and voltage (Chapters 14 and 15)

764	
Appendices
THREE-HOTEL SURVEY  Choose again? (No or Yes) 
and Golden Palm, Palm Royale, and Palm Princess tallies 
(Chapter 12) (M)
TIMES  Get-ready times (Chapter 3)
TOYS R US  Quarter, coded quarter, revenue, and the 
dummy variables Q1, Q2, and for quarters (Chapter 16)
TRAVEL  Month and amount of travel (Chapter 16)
TROUGH  Width of trough (Chapters 2, 3, 8, and 9)
TRSNYC  Year, unit value of Diversified Equity funds, and 
unit value of Stable Value funds (Chapter 16)
TSMODEL1  Year, coded year, and three time series (I, II, 
and III) (Chapter 16)
TSMODEL2  Year, coded year, and two time series (I and 
II) (Chapter 16)
TWITTERMOVIES  Movie, Twitter activity, and receipts 
($) (Chapter 13)
TWO-HOTEL SURVEY  Choose again? (No or Yes) and 
Beachcomber and Windsurfer tallies (Chapter 12) (M)
UNDERGRADSURVEY  ID, gender (Female or Male), age 
(as of last birthday), class designation (Sophomore, Junior, 
or Senior), major (Accounting, CIS, Economics/Finance, 
International Business, Management, Retail/Marketing, 
Other, or Undecided), graduate school intention (No, Yes, 
or Undecided), cumulative GPA, current employment 
status (Full-Time, Part-Time, or Unemployed), expected 
starting salary ($thousands), number of social networking 
sites registered for, satisfaction with student advisement 
services on campus, amount spent on books and sup-
plies this semester, type of computer preferred (Desktop, 
Laptop, or Tablet), text messages per week, wealth accu-
mulated to feel rich (Chapters 2, 3, 6, 8, 10, 11, and 12)
UNDERWRITING  End-of-training exam score, pro-
ficiency exam score, and training method (classroom, 
courseware app, or online) (Chapter 14)
UNSTACKED 1YRRETURN  One-year return percentage 
for growth funds and one-year return percentage for value 
funds (Chapter 2) (M)
USEDCARS  Car, year, age, price ($), mileage, power (hp), 
fuel (mpg) (Chapter 18)
UTILITY  Utilities charges ($) for 50 one-bedroom apart-
ments (Chapters 2 and 6)
VB  Time to complete program (Chapter 10)
VINHOVERDE  Fixed acidity, volatile acidity, citric acid, 
residual sugar, chlorides, free sulfur dioxide, total sul-
fur dioxide, density, pH, sulphates, alcohol, and quality 
(Chapters 13, 14 and 15)
VINHOVERDE POPULATION  Fixed acidity, volatile 
acidity, citric acid, residual sugar, chlorides, free sulfur 
dioxide, total sulfur dioxide, density, pH, sulphates, alco-
hol, wine type (Red or White), and quality (Chapter 17)
WAIT  Waiting time and seating time (Chapter 6)
WALMART  Quarter and Wal-Mart Stores quarterly rev-
enues ($billions) (Chapter 16)
WARECOST  Distribution cost ($thousands), sales ($thou-
sands), and number of orders (Chapters 13, 14, and 15)
WIP  Processing times at each of two plants 11 = A, 2 = B2 
(Chapter 18)
WL_SOCIALDATA Land, ride, comments, rating, and 
alternate scale (Chapter 17)
WL_WAITDATA Land, ride, and wait (minutes) (Chapter 17)
WL_WAITHISTORY Ride and historical wait times (min-
utes) at 21 half-hour intervals (Chapter 17)
WORKFORCE  Year, population, and size of the work-
force (Chapter 16)
YARN  Side-by-side aspect and breaking strength scores for 
30 psi, 40 psi, and 50 psi (Chapter 11)

	
Appendix C  Online Resources	
765
Excel Guide Workbooks
Excel Guide workbooks contain templates or model solu-
tions for applying Excel to a particular statistical method. 
Chapter examples and the In-Depth Excel instructions of the 
Excel Guides feature worksheets from these workbooks and 
PHStat constructs many of the worksheets from these work-
books for you.
Workbooks are stored in the .xlsx Excel workbook for-
mat. Most contain a COMPUTE worksheet (often shown 
in this book) that presents results as well as a COMPUTE_
FORMULAS worksheet that allows you to examine all of 
the formulas used in the worksheet. The Excel Guide work-
books (with the number of the chapter in which each is first 
mentioned) are:
Recoded (1)
Random (1)
Data Cleaning (1)
Summary Table (2)
Contingency Table (2)
Distributions (2)
Pareto (2)
Stem-and-leaf (2)
Histogram (2)
Polygons (2)
Scatter Plot (2)
Time Series (2)
MCT (2)
Descriptive(3)
Quartiles (3)
Boxplot (3)
Parameters(3)
VE-Variability (3)
Covariance (3)
Probabilities (4)
Bayes (4)
Discrete Variable (5)
Portfolio (5)
Binomial (5)
Poisson (5)
Hypergeometric (5)
Normal (6)
NPP (6)
Exponential (6)
SDS (7)
CIE sigma known (8)
CIE sigma unknown (8)
CIE Proportion (8)
Sample Size Mean (8)
Sample Size Proportion (8)
Z Mean workbook (9)
T mean workbook (9)
Z Proportion (9)
Pooled-Variance T (10)
Separate-Variance T (10)
Paired T (10)
F Two Variances (10)
Z Two Proportions (10)
One-Way ANOVA (11)
Levene (11)
Randomized Block (11)
Chi-Square (12)
Chi-Square Worksheets (12)
Wilcoxon (12)
Kruskal-Wallis Worksheets 
(12)
Simple Linear Regression 
(13)
Package Delivery (13)
Multiple Regression (14)
Logistic Regression add-in 
(14)
Moving Averages (16)
Exponential Smoothing (16)
Exponential Trend (16)
Differences (16)
Lagged Predictors (16)
Forecasting Comparison (16)
GaugeBullet (17)
Treemap (17)
Slicers (17)
The Slicers workbook works only with Microsoft Win-
dows versions Excel 2010 and Excel 2013. The Logistic 
­Regression add-in workbook requires the Solver ­add-in 
and, if you use a Microsoft Windows version of Excel, 
the security settings discussed in Appendix Section D.3. 
­(Appendix Section D.6 discusses how to check for the pres-
ence of the Solver add-in.)
Visual Explorations
Visual Explorations are workbooks that interactively 
demonstrate various key statistical concepts. Three work-
books are add-in workbooks that are stored in the .xlam 
Excel add-in format. Using these add-in workbooks with 
Microsoft Windows Excels requires the security settings 
discussed in Appendix Section D.3. The visual exploration 
workbooks are:
VE-Normal Distribution (add-in)
VE-Sampling Distribution (add-in)
VE-Simple Linear Regression (add-in)
VE-Variability
PDF Files
PDF files use the Portable Document Format that can  
be viewed in most web browsers or with PDF utility pro-
grams, such as Adobe Reader, a program available at  
get.adobe.com/reader/. Both the Digital Case files and the 
online sections use this format.
C.4  PHStat
PHStat is the Pearson Education statistics add-in for 
Microsoft Excel that simplifies the task of using Excel as 
you learn business statistics. PHStat comes packaged as a 
zip file archive that you download and unzip to the folder of 
your choice. The archive contains: 
PHStat.xlam,  the actual add-in workbook that is further 
discussed in Appendix Sections D.2 and G.1, and these four 
supporting files:
PHStat readme.pdf  Explains the technical requirements, 
and setup and troubleshooting procedures for PHStat (PDF 
format).
PHStatHelp.chm  The integrated help system for users of 
Microsoft Windows Excel.
PHStatHelp.pdf  The help system as a PDF format file.
PHStatHelp.epub  The help system for eBook reader ap-
plications (Open Publication Structure eBook format).
Downloading PHStat requires an access code. If your 
book was packaged with an access code, then click the 
PHStat link on either the student download page or the 
MyStatLab Tools for Success page (see Sections C.1 and 
C.2) to be taken to the PHStat home page. On that page, 
click the download link and follow the instructions for 
entering the access code.
If your book was not packaged with an access code, 
visit the PHStat home page (www.pearsonhighered.com/
phstat) to learn how you can obtain a code.

766
A p p e n d i x  D    Configuring Microsoft Excel
This appendix seeks to eliminate the common types of technical problems that could compli-
cate your use of Microsoft Excel as you learn business statistics with this book. You will want 
to be familiar with the contents of this appendix—and follow all its directives—if the copy 
of Microsoft Excel you plan to use runs on a computer system that you control and maintain. 
If you use a computer system that is maintained by others, such as a computer system in an 
academic computer lab, this appendix can be a useful resource for those in charge of solving 
technical issues that may arise.
Not all sections of this appendix apply to all readers. Sections with the code (WIN) apply 
to you if you use Microsoft Excel with Microsoft Windows, while sections with the code (OS 
X) apply to you if you use Microsoft Excel with OS X (formerly, Mac OS X). Some sections 
apply to all readers (ALL). (If you use Minitab, there are no configuration issues that you need 
to address.)
D.1  Getting Microsoft Excel Ready for Use (ALL)
You must have an up-to-date, properly licensed copy of Microsoft Excel in order to work 
through the examples and solve the problems in this book as well as to take advantage of the 
Excel-related workbooks and add-ins described in Appendix C. To get Microsoft Excel ready 
for use, follow this checklist:
❑  If necessary, install Microsoft Excel on your computer system.
❑  Check and apply Microsoft-supplied updates to Microsoft Excel and Microsoft Office.
❑  After you first use Microsoft Excel, recheck for Microsoft-supplied updates at least 
once every two weeks.
If you need to install a new copy of Microsoft Excel on a Microsoft Windows computer 
system, choose the 32-bit version and not the 64-bit version even if you have a 64-bit version of a 
Microsoft Windows operating system. Many people mistakenly believe that the 64-bit version is 
somehow “better,” not realizing that the OS X Excel 2011 is a 32-bit version and that Microsoft 
advises you to choose the 32-bit version for reasons the company details on its website. (The 
64-bit WIN version exists primarily for users who need to work with Excel workbooks that are 
greater than 2GB in size. What would a 2GB workbook store? By one informal calculation, the 
contents of over 60 copies of this book—in other words, big data, as defined in Section GS.3.)
Checking For and Applying Updates
Microsoft Excel updates require Internet access and the process to check for and apply updates 
differs among Excel versions. If you use a Microsoft Windows version of Excel and use Windows 
7 or 8, checking for updates is done by the Windows Update service. If you use an older version 
of Microsoft Windows, you may have to upgrade to this service (Visit the Microsoft Download 
Center at http://www.microsoft.com/download/default.aspx for further details.)
Windows Update can automatically apply any updates it finds, although many users prefer 
to set Windows Update to notify when updates are available and then select and apply updates 
manually.
In OS X Excel versions and some Microsoft Windows versions, you can manually check 
for updates. In Excel 2011 (OS X), select Help ➔ Check for Updates and in the dialog box 
that appears, click Check for Updates. In Excel 2007 (WIN), first click the Office Button and 
then Excel Options at the bottom of the Office Button window. In the Excel options dialog 
box, click Resources in the left pane and then in the right pane click Check for Updates and 
follow the instructions that appear on the web page that is displayed.

	
Appendix D  Configuring Microsoft Excel	
767
You normally do not manually check for updates in either Excel 2010 (WIN) or  
Excel 2013 (WIN). However, in some installations of these versions, you can select File ➔  
Account ➔ Update Options (2013) or File ➔ Help ➔ Check for Updates (2010) and select 
options or follow instructions to manually check for updates.
If all else fails, you can open a web browser and go to the Microsoft Office part of the 
­Microsoft Download Center at www.microsoft.com/download/office.aspx?q=office and 
manually select and download updates. On the web page that gets displayed, filter the down-
loadable files by specifying the Excel version you use. Discover the version number and up-
date status by these means:
• In Excel 2013 (WIN), select File ➔ Account and then click About Microsoft Excel. In 
the dialog box that appears note the numbers and codes that follow the phrase “Microsoft 
Excel 2013.”
• In Excel 2010 (WIN), select File ➔ Help. Under the heading “About Microsoft Excel” 
click Additional Version and Copyright Information and in the dialog box that appears 
note the numbers and codes that follow “Microsoft Excel 2010.”
• In Excel 2011 (OS X), click Excel ➔ About Excel. The dialog box that appears displays 
the Version and Latest Installed Update.
• In Excel 2007 (WIN), first click the Office Button and then click Excel Options. In 
the Excel options dialog box, click Resources in the left pane. In the right pane note the 
numbers and codes that follow Microsoft Office Excel 2007 under the “about Microsoft 
Office Excel 2007” heading.
Special Note for Office 365 Users
If you use Office 365, you are using the most current version of Excel for your system. At the 
time of publication, the most current version for Microsoft Windows systems was Excel 2013. 
For OS X, the most current version was Excel 2011.
D.2  Getting PHStat Ready 
for Use (ALL)
If you plan to use PHStat, the Pearson Education add-
in workbook that simplifies the use of Microsoft Excel 
with this book (see Section EG.1 on page 36), you must 
first download PHStat using an access code as discussed 
in Section C.4. The PHStat download comes packaged 
as a zip file archive that you unzip to the folder of your 
choice.
PHStat is fully compatible with these Excel ver-
sions: Excel 2007 (WIN), Excel 2010 (WIN), Excel 2011 
(OS X), and Excel 2013 (WIN). PHStat is not compatible 
with Excel 2008 (OS X), an Excel version that did not in-
clude the capability of running add-in workbooks. If you 
are using Microsoft Excel with Microsoft Windows (any  
version), then you must first configure the Microsoft  
Excel security settings as discussed in Section D.3. If you 
are using Microsoft Excel with OS X, no additional steps are  
required.
D.3  Configuring Excel 
Security for Add-In 
Usage (WIN)
The Microsoft Excel security settings can prevent add-ins 
such as PHStat and the Visual Explorations add-in work-
books from opening or functioning properly. To configure 
these security settings to permit proper PHStat functioning:
	 1.	In Excel 2010 and Excel 2013, select File ➔ Options. In 
Excel 2007, first click the Office Button and then click 
Excel Options.

768	
Appendices
In the Excel Options dialog box (shown below):
	 2.	Click Trust Center in the left pane and then click Trust 
Center Settings in the right pane.
In the Trust Center dialog box:
	 3.	Click Add-ins in the next left pane, and in the Add-ins 
right pane clear all of the checkboxes (shown below).
	 4.	Click Macro Settings in the left pane, and in the Macro 
Settings right pane click Disable all macros with no-
tification and check Trust access to the VBA object 
model (shown below).
	 5.	Click OK to close the Trust Center dialog box.
Back in the Excel Options dialog box:
	 6.	Click OK to finish.
On some systems that have stringent security settings, 
you might need to modify step 4. For such systems, in 
step 4, also click Trusted Locations in the left pane and 
then, in the Trusted Locations right pane, click Add new 
location to add the folder path that you chose to store the 
PHStat files.
D.4  Opening PHStat (ALL)
Open the PHStat.xlam file to use PHStat. As you open 
the file, by any of the means discussed in Section EG.3 on 
page 37, Microsoft Excel will display a warning dialog 
box. The dialog boxes for Excel 2013 (WIN) and Excel 
2011 (OS X) are shown below. Click Enable Macros, 
which is not the default choice, to enable PHStat to func-
tion properly.
After you click Enable Macros, you can verify that 
PHStat has opened properly by looking for a PHStat menu 
in the Add-Ins tab of the Office Ribbon (WIN) or in the 
menu at top of the display (OS X).
If you have skipped checking for and applying neces-
sary Excel updates, or if some of the updates were un-
able to be applied, when you first attempt to use PHStat, 
you may see a “Compile Error” message that talks about 
a “hidden module.” If this occurs, repeat the process of 
checking for and applying updates to Excel. Review the 
PHStat FAQs in Appendix G for additional assistance, if 
necessary, and occasionally check for PHStat updates by 
revisiting the page from which you originally downloaded 
the PHStat files.

	
Appendix D  Configuring Microsoft Excel	
769
D.5  Using a Visual 
Explorations Add-in 
Workbook (ALL)
To use any of the Visual Explorations add-in workbooks, 
you must first download them using one of the methods dis-
cussed in Appendix Section C.1. If your download is pack-
aged as a zip archive file, you must unzip that archive and 
store the add-in workbook files together in a folder of your 
choosing. Then apply the Section D.3 instructions, if nec-
essary. When you open a Visual Explorations add-in work-
book, you will see the same type of warning dialog box that 
Section D.4. describes. Click Enable Macros to enable the 
workbook to function properly.
D.6  Checking for the 
Presence of the 
Analysis ToolPak or 
Solver Add-Ins (ALL)
If you choose to perform logistic regression using the 
Section EG14.7 PHStat or In-Depth Excel instructions, you 
will need to ensure that the Solver add-in has been installed. 
Similarly, if you choose to use the Analysis ToolPak 
Excel Guide instructions, you will need to ensure that the 
Microsoft Excel Analysis ToolPak add-in has been installed. 
(This add-in is not available if you use Microsoft Excel with 
OS X.)
To check for the presence of the Solver (or Analysis 
ToolPak) add-in, if you use Microsoft Excel with Microsoft 
Windows:
	 1.	Select File ➔ Options. (In Excel 2007, click the Office 
Button and then click Excel Options.)
In the Excel Options dialog box:
	 2.	Click Add-Ins in the left pane and look for the entry 
Solver Add-in (or Analysis ToolPak) in the right pane, 
under Active Application Add-ins.
	 3.	If the entry appears, click OK.
	 4.	If the entry does not appear in the Active Application 
Add-ins list, select Excel Add-ins from the Manage 
drop-down list and then click Go.
	 5.	In the Add-Ins dialog box, check Solver Add-in (or 
Analysis ToolPak) in the Add-Ins available list and 
click OK.
If Analysis ToolPak (or Solver Add-in) does not appear in 
the list, rerun the Microsoft ­Office setup program to install 
this component.
To check for the presence of the Solver add-in, if you 
use Microsoft Excel with OS X, ­select Tools ➔ Options. In 
the Add-Ins dialog box, check Solver.Xlam in the Add-Ins 
available list and click OK.

770
A p p e n d i x  E    Tables
T a b l e  E . 1
Table of Random 
Numbers
Column
00000
00001
11111
11112
22222
22223
33333
33334
Row
12345
67890
12345
67890
12345
67890
12345
67890
01
49280
88924
35779
00283
81163
07275
89863
02348
02
61870
41657
07468
08612
98083
97349
20775
45091
03
43898
65923
25078
86129
78496
97653
91550
08078
04
62993
93912
30454
84598
56095
20664
12872
64647
05
33850
58555
51438
85507
71865
79488
76783
31708
06
97340
03364
88472
04334
63919
36394
11095
92470
07
70543
29776
10087
10072
55980
64688
68239
20461
08
89382
93809
00796
95945
34101
81277
66090
88872
09
37818
72142
67140
50785
22380
16703
53362
44940
10
60430
22834
14130
96593
23298
56203
92671
15925
11
82975
66158
84731
19436
55790
69229
28661
13675
12
30987
71938
40355
54324
08401
26299
49420
59208
13
55700
24586
93247
32596
11865
63397
44251
43189
14
14756
23997
78643
75912
83832
32768
18928
57070
15
32166
53251
70654
92827
63491
04233
33825
69662
16
23236
73751
31888
81718
06546
83246
47651
04877
17
45794
26926
15130
82455
78305
55058
52551
47182
18
09893
20505
14225
68514
47427
56788
96297
78822
19
54382
74598
91499
14523
68479
27686
46162
83554
20
94750
89923
37089
20048
80336
94598
26940
36858
21
70297
34135
53140
33340
42050
82341
44104
82949
22
85157
47954
32979
26575
57600
40881
12250
73742
23
11100
02340
12860
74697
96644
89439
28707
25815
24
36871
50775
30592
57143
17381
68856
25853
35041
25
23913
48357
63308
16090
51690
54607
72407
55538
26
79348
36085
27973
65157
07456
22255
25626
57054
27
92074
54641
53673
54421
18130
60103
69593
49464
28
06873
21440
75593
41373
49502
17972
82578
16364
29
12478
37622
99659
31065
83613
69889
58869
29571
30
57175
55564
65411
42547
70457
03426
72937
83792
31
91616
11075
80103
07831
59309
13276
26710
73000
32
78025
73539
14621
39044
47450
03197
12787
47709
33
27587
67228
80145
10175
12822
86687
65530
49325
34
16690
20427
04251
64477
73709
73945
92396
68263
35
70183
58065
65489
31833
82093
16747
10386
59293
36
90730
35385
15679
99742
50866
78028
75573
67257
37
10934
93242
13431
24590
02770
48582
00906
58595
38
82462
30166
79613
47416
13389
80268
05085
96666
39
27463
10433
07606
16285
93699
60912
94532
95632
40
02979
52997
09079
92709
90110
47506
53693
49892
41
46888
69929
75233
52507
32097
37594
10067
67327
42
53638
83161
08289
12639
08141
12640
28437
09268
43
82433
61427
17239
89160
19666
08814
37841
12847
44
35766
31672
50082
22795
66948
65581
84393
15890
45
10853
42581
08792
13257
61973
24450
52351
16602
46
20341
27398
72906
63955
17276
10646
74692
48438
47
54458
90542
77563
51839
52901
53355
83281
19177
48
26337
66530
16687
35179
46560
00123
44546
79896
49
34314
23729
85264
05575
96855
23820
11091
79821
50
28603
10708
68933
34189
92166
15181
66628
58599

	
Appendix E  Tables	
771
Column
00000
00001
11111
11112
22222
22223
33333
33334
Row
12345
67890
12345
67890
12345
67890
12345
67890
51
66194
28926
99547
16625
45515
67953
12108
57846
52
78240
43195
24837
32511
70880
22070
52622
61881
53
00833
88000
67299
68215
11274
55624
32991
17436
54
12111
86683
61270
58036
64192
90611
15145
01748
55
47189
99951
05755
03834
43782
90599
40282
51417
56
76396
72486
62423
27618
84184
78922
73561
52818
57
46409
17469
32483
09083
76175
19985
26309
91536
58
74626
22111
87286
46772
42243
68046
44250
42439
59
34450
81974
93723
49023
58432
67083
36876
93391
60
36327
72135
33005
28701
34710
49359
50693
89311
61
74185
77536
84825
09934
99103
09325
67389
45869
62
12296
41623
62873
37943
25584
09609
63360
47270
63
90822
60280
88925
99610
42772
60561
76873
04117
64
72121
79152
96591
90305
10189
79778
68016
13747
65
95268
41377
25684
08151
61816
58555
54305
86189
66
92603
09091
75884
93424
72586
88903
30061
14457
67
18813
90291
05275
01223
79607
95426
34900
09778
68
38840
26903
28624
67157
51986
42865
14508
49315
69
05959
33836
53758
16562
41081
38012
41230
20528
70
85141
21155
99212
32685
51403
31926
69813
58781
71
75047
59643
31074
38172
03718
32119
69506
67143
72
30752
95260
68032
62871
58781
34143
68790
69766
73
22986
82575
42187
62295
84295
30634
66562
31442
74
99439
86692
90348
66036
48399
73451
26698
39437
75
20389
93029
11881
71685
65452
89047
63669
02656
76
39249
05173
68256
36359
20250
68686
05947
09335
77
96777
33605
29481
20063
09398
01843
35139
61344
78
04860
32918
10798
50492
52655
33359
94713
28393
79
41613
42375
00403
03656
77580
87772
86877
57085
80
17930
00794
53836
53692
67135
98102
61912
11246
81
24649
31845
25736
75231
83808
98917
93829
99430
82
79899
34061
54308
59358
56462
58166
97302
86828
83
76801
49594
81002
30397
52728
15101
72070
33706
84
36239
63636
38140
65731
39788
06872
38971
53363
85
07392
64449
17886
63632
53995
17574
22247
62607
86
67133
04181
33874
98835
67453
59734
76381
63455
87
77759
31504
32832
70861
15152
29733
75371
39174
88
85992
72268
42920
20810
29361
51423
90306
73574
89
79553
75952
54116
65553
47139
60579
09165
85490
90
41101
17336
48951
53674
17880
45260
08575
49321
91
36191
17095
32123
91576
84221
78902
82010
30847
92
62329
63898
23268
74283
26091
68409
69704
82267
93
14751
13151
93115
01437
56945
89661
67680
79790
94
48462
59278
44185
29616
76537
19589
83139
28454
95
29435
88105
59651
44391
74588
55114
80834
85686
96
28340
29285
12965
14821
80425
16602
44653
70467
97
02167
58940
27149
80242
10587
79786
34959
75339
98
17864
00991
39557
54981
23588
81914
37609
13128
99
79675
80605
60059
35862
00254
36546
21545
78179
100
72335
82037
92003
34100
29879
46613
89720
13274
Source: Partially extracted from the Rand Corporation, A Million Random Digits with 100,000 Normal Deviates 
(Glencoe, IL, The Free Press, 1955).
T a b l e  E . 1
Table of Random 
Numbers (continued )

772	
appendices
T a b l e  E . 2
The Cumulative Standardized Normal Distribution
Entry represents area under the cumulative standardized  
normal distribution from - ∞ to Z
Cumulative Probabilities
Z
0.00
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
-6.0
0.000000001
-5.5
0.000000019
-5.0
0.000000287
-4.5
0.000003398
-4.0
0.000031671
-3.9
0.00005
0.00005
0.00004
0.00004
0.00004
0.00004
0.00004
0.00004
0.00003
0.00003
-3.8
0.00007
0.00007
0.00007
0.00006
0.00006
0.00006
0.00006
0.00005
0.00005
0.00005
-3.7
0.00011
0.00010
0.00010
0.00010
0.00009
0.00009
0.00008
0.00008
0.00008
0.00008
-3.6
0.00016
0.00015
0.00015
0.00014
0.00014
0.00013
0.00013
0.00012
0.00012
0.00011
-3.5
0.00023
0.00022
0.00022
0.00021
0.00020
0.00019
0.00019
0.00018
0.00017
0.00017
-3.4
0.00034
0.00032
0.00031
0.00030
0.00029
0.00028
0.00027
0.00026
0.00025
0.00024
-3.3
0.00048
0.00047
0.00045
0.00043
0.00042
0.00040
0.00039
0.00038
0.00036
0.00035
-3.2
0.00069
0.00066
0.00064
0.00062
0.00060
0.00058
0.00056
0.00054
0.00052
0.00050
-3.1
0.00097
0.00094
0.00090
0.00087
0.00084
0.00082
0.00079
0.00076
0.00074
0.00071
-3.0
0.00135
0.00131
0.00126
0.00122
0.00118
0.00114
0.00111
0.00107
0.00103
0.00100
-2.9
0.0019
0.0018
0.0018
0.0017
0.0016
0.0016
0.0015
0.0015
0.0014
0.0014
-2.8
0.0026
0.0025
0.0024
0.0023
0.0023
0.0022
0.0021
0.0021
0.0020
0.0019
-2.7
0.0035
0.0034
0.0033
0.0032
0.0031
0.0030
0.0029
0.0028
0.0027
0.0026
-2.6
0.0047
0.0045
0.0044
0.0043
0.0041
0.0040
0.0039
0.0038
0.0037
0.0036
-2.5
0.0062
0.0060
0.0059
0.0057
0.0055
0.0054
0.0052
0.0051
0.0049
0.0048
-2.4
0.0082
0.0080
0.0078
0.0075
0.0073
0.0071
0.0069
0.0068
0.0066
0.0064
-2.3
0.0107
0.0104
0.0102
0.0099
0.0096
0.0094
0.0091
0.0089
0.0087
0.0084
-2.2
0.0139
0.0136
0.0132
0.0129
0.0125
0.0122
0.0119
0.0116
0.0113
0.0110
-2.1
0.0179
0.0174
0.0170
0.0166
0.0162
0.0158
0.0154
0.0150
0.0146
0.0143
-2.0
0.0228
0.0222
0.0217
0.0212
0.0207
0.0202
0.0197
0.0192
0.0188
0.0183
-1.9
0.0287
0.0281
0.0274
0.0268
0.0262
0.0256
0.0250
0.0244
0.0239
0.0233
-1.8
0.0359
0.0351
0.0344
0.0336
0.0329
0.0322
0.0314
0.0307
0.0301
0.0294
-1.7
0.0446
0.0436
0.0427
0.0418
0.0409
0.0401
0.0392
0.0384
0.0375
0.0367
-1.6
0.0548
0.0537
0.0526
0.0516
0.0505
0.0495
0.0485
0.0475
0.0465
0.0455
-1.5
0.0668
0.0655
0.0643
0.0630
0.0618
0.0606
0.0594
0.0582
0.0571
0.0559
-1.4
0.0808
0.0793
0.0778
0.0764
0.0749
0.0735
0.0721
0.0708
0.0694
0.0681
-1.3
0.0968
0.0951
0.0934
0.0918
0.0901
0.0885
0.0869
0.0853
0.0838
0.0823
-1.2
0.1151
0.1131
0.1112
0.1093
0.1075
0.1056
0.1038
0.1020
0.1003
0.0985
-1.1
0.1357
0.1335
0.1314
0.1292
0.1271
0.1251
0.1230
0.1210
0.1190
0.1170
-1.0
0.1587
0.1562
0.1539
0.1515
0.1492
0.1469
0.1446
0.1423
0.1401
0.1379
-0.9
0.1841
0.1814
0.1788
0.1762
0.1736
0.1711
0.1685
0.1660
0.1635
0.1611
-0.8
0.2119
0.2090
0.2061
0.2033
0.2005
0.1977
0.1949
0.1922
0.1894
0.1867
-0.7
0.2420
0.2388
0.2358
0.2327
0.2296
0.2266
0.2236
0.2206
0.2177
0.2148
-0.6
0.2743
0.2709
0.2676
0.2643
0.2611
0.2578
0.2546
0.2514
0.2482
0.2451
-0.5
0.3085
0.3050
0.3015
0.2981
0.2946
0.2912
0.2877
0.2843
0.2810
0.2776
-0.4
0.3446
0.3409
0.3372
0.3336
0.3300
0.3264
0.3228
0.3192
0.3156
0.3121
-0.3
0.3821
0.3783
0.3745
0.3707
0.3669
0.3632
0.3594
0.3557
0.3520
0.3483
-0.2
0.4207
0.4168
0.4129
0.4090
0.4052
0.4013
0.3974
0.3936
0.3897
0.3859
-0.1
0.4602
0.4562
0.4522
0.4483
0.4443
0.4404
0.4364
0.4325
0.4286
0.4247
-0.0
0.5000
0.4960
0.4920
0.4880
0.4840
0.4801
0.4761
0.4721
0.4681
0.4641
0
Z
2`

	
Appendix E  Tables	
773
Cumulative Probabilities
Z
0.00
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0.0
0.5000
0.5040
0.5080
0.5120
0.5160
0.5199
0.5239
0.5279
0.5319
0.5359
0.1
0.5398
0.5438
0.5478
0.5517
0.5557
0.5596
0.5636
0.5675
0.5714
0.5753
0.2
0.5793
0.5832
0.5871
0.5910
0.5948
0.5987
0.6026
0.6064
0.6103
0.6141
0.3
0.6179
0.6217
0.6255
0.6293
0.6331
0.6368
0.6406
0.6443
0.6480
0.6517
0.4
0.6554
0.6591
0.6628
0.6664
0.6700
0.6736
0.6772
0.6808
0.6844
0.6879
0.5
0.6915
0.6950
0.6985
0.7019
0.7054
0.7088
0.7123
0.7157
0.7190
0.7224
0.6
0.7257
0.7291
0.7324
0.7357
0.7389
0.7422
0.7454
0.7486
0.7518
0.7549
0.7
0.7580
0.7612
0.7642
0.7673
0.7704
0.7734
0.7764
0.7794
0.7823
0.7852
0.8
0.7881
0.7910
0.7939
0.7967
0.7995
0.8023
0.8051
0.8078
0.8106
0.8133
0.9
0.8159
0.8186
0.8212
0.8238
0.8264
0.8289
0.8315
0.8340
0.8365
0.8389
1.0
0.8413
0.8438
0.8461
0.8485
0.8508
0.8531
0.8554
0.8577
0.8599
0.8621
1.1
0.8643
0.8665
0.8686
0.8708
0.8729
0.8749
0.8770
0.8790
0.8810
0.8830
1.2
0.8849
0.8869
0.8888
0.8907
0.8925
0.8944
0.8962
0.8980
0.8997
0.9015
1.3
0.9032
0.9049
0.9066
0.9082
0.9099
0.9115
0.9131
0.9147
0.9162
0.9177
1.4
0.9192
0.9207
0.9222
0.9236
0.9251
0.9265
0.9279
0.9292
0.9306
0.9319
1.5
0.9332
0.9345
0.9357
0.9370
0.9382
0.9394
0.9406
0.9418
0.9429
0.9441
1.6
0.9452
0.9463
0.9474
0.9484
0.9495
0.9505
0.9515
0.9525
0.9535
0.9545
1.7
0.9554
0.9564
0.9573
0.9582
0.9591
0.9599
0.9608
0.9616
0.9625
0.9633
1.8
0.9641
0.9649
0.9656
0.9664
0.9671
0.9678
0.9686
0.9693
0.9699
0.9706
1.9
0.9713
0.9719
0.9726
0.9732
0.9738
0.9744
0.9750
0.9756
0.9761
0.9767
2.0
0.9772
0.9778
0.9783
0.9788
0.9793
0.9798
0.9803
0.9808
0.9812
0.9817
2.1
0.9821
0.9826
0.9830
0.9834
0.9838
0.9842
0.9846
0.9850
0.9854
0.9857
2.2
0.9861
0.9864
0.9868
0.9871
0.9875
0.9878
0.9881
0.9884
0.9887
0.9890
2.3
0.9893
0.9896
0.9898
0.9901
0.9904
0.9906
0.9909
0.9911
0.9913
0.9916
2.4
0.9918
0.9920
0.9922
0.9925
0.9927
0.9929
0.9931
0.9932
0.9934
0.9936
2.5
0.9938
0.9940
0.9941
0.9943
0.9945
0.9946
0.9948
0.9949
0.9951
0.9952
2.6
0.9953
0.9955
0.9956
0.9957
0.9959
0.9960
0.9961
0.9962
0.9963
0.9964
2.7
0.9965
0.9966
0.9967
0.9968
0.9969
0.9970
0.9971
0.9972
0.9973
0.9974
2.8
0.9974
0.9975
0.9976
0.9977
0.9977
0.9978
0.9979
0.9979
0.9980
0.9981
2.9
0.9981
0.9982
0.9982
0.9983
0.9984
0.9984
0.9985
0.9985
0.9986
0.9986
3.0
0.99865
0.99869
0.99874
0.99878
0.99882
0.99886
0.99889
0.99893
0.99897
0.99900
3.1
0.99903
0.99906
0.99910
0.99913
0.99916
0.99918
0.99921
0.99924
0.99926
0.99929
3.2
0.99931
0.99934
0.99936
0.99938
0.99940
0.99942
0.99944
0.99946
0.99948
0.99950
3.3
0.99952
0.99953
0.99955
0.99957
0.99958
0.99960
0.99961
0.99962
0.99964
0.99965
3.4
0.99966
0.99968
0.99969
0.99970
0.99971
0.99972
0.99973
0.99974
0.99975
0.99976
3.5
0.99977
0.99978
0.99978
0.99979
0.99980
0.99981
0.99981
0.99982
0.99983
0.99983
3.6
0.99984
0.99985
0.99985
0.99986
0.99986
0.99987
0.99987
0.99988
0.99988
0.99989
3.7
0.99989
0.99990
0.99990
0.99990
0.99991
0.99991
0.99992
0.99992
0.99992
0.99992
3.8
0.99993
0.99993
0.99993
0.99994
0.99994
0.99994
0.99994
0.99995
0.99995
0.99995
3.9
0.99995
0.99995
0.99996
0.99996
0.99996
0.99996
0.99996
0.99996
0.99997
0.99997
4.0
0.999968329
4.5
0.999996602
5.0
0.999999713
5.5
0.999999981
6.0
0.999999999
T a b l e  E . 2
The Cumulative Standardized Normal Distribution (continued)
Entry represents area under the cumulative standardized  
normal distribution from - ∞ to Z
0
Z
2`

774	
appendices
T a b l e  E . 3
Critical Values of t
For a particular number of degrees of freedom, entry represents  
the critical value of t corresponding to the cumulative probability  
11 - a2 and a specified upper-tail area 1a2.
Degrees of 
Freedom
Cumulative Probabilities
0.75
0.90
0.95
0.975
0.99
0.995
Upper-Tail Areas
0.25
0.10
0.05
0.025
0.01
0.005
1
1.0000
3.0777
6.3138
12.7062
31.8207
63.6574
2
0.8165
1.8856
2.9200
4.3027
6.9646
9.9248
3
0.7649
1.6377
2.3534
3.1824
4.5407
5.8409
4
0.7407
1.5332
2.1318
2.7764
3.7469
4.6041
5
0.7267
1.4759
2.0150
2.5706
3.3649
4.0322
6
0.7176
1.4398
1.9432
2.4469
3.1427
3.7074
7
0.7111
1.4149
1.8946
2.3646
2.9980
3.4995
8
0.7064
1.3968
1.8595
2.3060
2.8965
3.3554
9
0.7027
1.3830
1.8331
2.2622
2.8214
3.2498
10
0.6998
1.3722
1.8125
2.2281
2.7638
3.1693
11
0.6974
1.3634
1.7959
2.2010
2.7181
3.1058
12
0.6955
1.3562
1.7823
2.1788
2.6810
3.0545
13
0.6938
1.3502
1.7709
2.1604
2.6503
3.0123
14
0.6924
1.3450
1.7613
2.1448
2.6245
2.9768
15
0.6912
1.3406
1.7531
2.1315
2.6025
2.9467
16
0.6901
1.3368
1.7459
2.1199
2.5835
2.9208
17
0.6892
1.3334
1.7396
2.1098
2.5669
2.8982
18
0.6884
1.3304
1.7341
2.1009
2.5524
2.8784
19
0.6876
1.3277
1.7291
2.0930
2.5395
2.8609
20
0.6870
1.3253
1.7247
2.0860
2.5280
2.8453
21
0.6864
1.3232
1.7207
2.0796
2.5177
2.8314
22
0.6858
1.3212
1.7171
2.0739
2.5083
2.8188
23
0.6853
1.3195
1.7139
2.0687
2.4999
2.8073
24
0.6848
1.3178
1.7109
2.0639
2.4922
2.7969
25
0.6844
1.3163
1.7081
2.0595
2.4851
2.7874
26
0.6840
1.3150
1.7056
2.0555
2.4786
2.7787
27
0.6837
1.3137
1.7033
2.0518
2.4727
2.7707
28
0.6834
1.3125
1.7011
2.0484
2.4671
2.7633
29
0.6830
1.3114
1.6991
2.0452
2.4620
2.7564
30
0.6828
1.3104
1.6973
2.0423
2.4573
2.7500
31
0.6825
1.3095
1.6955
2.0395
2.4528
2.7440
32
0.6822
1.3086
1.6939
2.0369
2.4487
2.7385
33
0.6820
1.3077
1.6924
2.0345
2.4448
2.7333
34
0.6818
1.3070
1.6909
2.0322
2.4411
2.7284
35
0.6816
1.3062
1.6896
2.0301
2.4377
2.7238
36
0.6814
1.3055
1.6883
2.0281
2.4345
2.7195
37
0.6812
1.3049
1.6871
2.0262
2.4314
2.7154
38
0.6810
1.3042
1.6860
2.0244
2.4286
2.7116
39
0.6808
1.3036
1.6849
2.0227
2.4258
2.7079
40
0.6807
1.3031
1.6839
2.0211
2.4233
2.7045
41
0.6805
1.3025
1.6829
2.0195
2.4208
2.7012
42
0.6804
1.3020
1.6820
2.0181
2.4185
2.6981
43
0.6802
1.3016
1.6811
2.0167
2.4163
2.6951
44
0.6801
1.3011
1.6802
2.0154
2.4141
2.6923
45
0.6800
1.3006
1.6794
2.0141
2.4121
2.6896
46
0.6799
1.3002
1.6787
2.0129
2.4102
2.6870
47
0.6797
1.2998
1.6779
2.0117
2.4083
2.6846
48
0.6796
1.2994
1.6772
2.0106
2.4066
2.6822
49
0.6795
1.2991
1.6766
2.0096
2.4049
2.6800
50
0.6794
1.2987
1.6759
2.0086
2.4033
2.6778
0
t
a

	
Appendix E  Tables	
775
Degrees of 
Freedom
Cumulative Probabilities
0.75
0.90
0.95
0.975
0.99
0.995
Upper-Tail Areas
0.25
0.10
0.05
0.025
0.01
0.005
51
0.6793
1.2984
1.6753
2.0076
2.4017
2.6757
52
0.6792
1.2980
1.6747
2.0066
2.4002
2.6737
53
0.6791
1.2977
1.6741
2.0057
2.3988
2.6718
54
0.6791
1.2974
1.6736
2.0049
2.3974
2.6700
55
0.6790
1.2971
1.6730
2.0040
2.3961
2.6682
56
0.6789
1.2969
1.6725
2.0032
2.3948
2.6665
57
0.6788
1.2966
1.6720
2.0025
2.3936
2.6649
58
0.6787
1.2963
1.6716
2.0017
2.3924
2.6633
59
0.6787
1.2961
1.6711
2.0010
2.3912
2.6618
60
0.6786
1.2958
1.6706
2.0003
2.3901
2.6603
61
0.6785
1.2956
1.6702
1.9996
2.3890
2.6589
62
0.6785
1.2954
1.6698
1.9990
2.3880
2.6575
63
0.6784
1.2951
1.6694
1.9983
2.3870
2.6561
64
0.6783
1.2949
1.6690
1.9977
2.3860
2.6549
65
0.6783
1.2947
1.6686
1.9971
2.3851
2.6536
66
0.6782
1.2945
1.6683
1.9966
2.3842
2.6524
67
0.6782
1.2943
1.6679
1.9960
2.3833
2.6512
68
0.6781
1.2941
1.6676
1.9955
2.3824
2.6501
69
0.6781
1.2939
1.6672
1.9949
2.3816
2.6490
70
0.6780
1.2938
1.6669
1.9944
2.3808
2.6479
71
0.6780
1.2936
1.6666
1.9939
2.3800
2.6469
72
0.6779
1.2934
1.6663
1.9935
2.3793
2.6459
73
0.6779
1.2933
1.6660
1.9930
2.3785
2.6449
74
0.6778
1.2931
1.6657
1.9925
2.3778
2.6439
75
0.6778
1.2929
1.6654
1.9921
2.3771
2.6430
76
0.6777
1.2928
1.6652
1.9917
2.3764
2.6421
77
0.6777
1.2926
1.6649
1.9913
2.3758
2.6412
78
0.6776
1.2925
1.6646
1.9908
2.3751
2.6403
79
0.6776
1.2924
1.6644
1.9905
2.3745
2.6395
80
0.6776
1.2922
1.6641
1.9901
2.3739
2.6387
81
0.6775
1.2921
1.6639
1.9897
2.3733
2.6379
82
0.6775
1.2920
1.6636
1.9893
2.3727
2.6371
83
0.6775
1.2918
1.6634
1.9890
2.3721
2.6364
84
0.6774
1.2917
1.6632
1.9886
2.3716
2.6356
85
0.6774
1.2916
1.6630
1.9883
2.3710
2.6349
86
0.6774
1.2915
1.6628
1.9879
2.3705
2.6342
87
0.6773
1.2914
1.6626
1.9876
2.3700
2.6335
88
0.6773
1.2912
1.6624
1.9873
2.3695
2.6329
89
0.6773
1.2911
1.6622
1.9870
2.3690
2.6322
90
0.6772
1.2910
1.6620
1.9867
2.3685
2.6316
91
0.6772
1.2909
1.6618
1.9864
2.3680
2.6309
92
0.6772
1.2908
1.6616
1.9861
2.3676
2.6303
93
0.6771
1.2907
1.6614
1.9858
2.3671
2.6297
94
0.6771
1.2906
1.6612
1.9855
2.3667
2.6291
95
0.6771
1.2905
1.6611
1.9853
2.3662
2.6286
96
0.6771
1.2904
1.6609
1.9850
2.3658
2.6280
97
0.6770
1.2903
1.6607
1.9847
2.3654
2.6275
98
0.6770
1.2902
1.6606
1.9845
2.3650
2.6269
99
0.6770
1.2902
1.6604
1.9842
2.3646
2.6264
100
0.6770
1.2901
1.6602
1.9840
2.3642
2.6259
110
0.6767
1.2893
1.6588
1.9818
2.3607
2.6213
120
0.6765
1.2886
1.6577
1.9799
2.3578
2.6174
∞
0.6745
1.2816
1.6449
1.9600
2.3263
2.5758
T a b l e  E . 3
Critical Values of t (continued )
For a particular number of degrees of freedom, entry represents the critical value of t corresponding to the 
cumulative probability 11 - a2 and a specified upper-tail area 1a2.

776	
appendices
T a b l e  E . 4
Critical Values of x2
For a particular number of degrees of freedom, entry represents the critical value of x2  
corresponding to the cumulative probability 11 - a2 and a specified upper-tail area 1a2. 
Degrees of 
Freedom
Cumulative Probabilities
0.005
0.01
0.025
0.05
0.10
0.25
0.75
0.90
0.95
0.975
0.99
0.995
Upper-Tail Areas (A)
0.995
0.99
0.975
0.95
0.90
0.75
0.25
0.10
0.05
0.025
0.01
0.005
1
0.001
0.004
0.016
0.102
1.323
2.706
3.841
5.024
6.635
7.879
2
0.010
0.020
0.051
0.103
0.211
0.575
2.773
4.605
5.991
7.378
9.210
10.597
3
0.072
0.115
0.216
0.352
0.584
1.213
4.108
6.251
7.815
9.348
11.345
12.838
4
0.207
0.297
0.484
0.711
1.064
1.923
5.385
7.779
9.488
11.143
13.277
14.860
5
0.412
0.554
0.831
1.145
1.610
2.675
6.626
9.236
11.071
12.833
15.086
16.750
6
0.676
0.872
1.237
1.635
2.204
3.455
7.841
10.645
12.592
14.449
16.812
18.548
7
0.989
1.239
1.690
2.167
2.833
4.255
9.037
12.017
14.067
16.013
18.475
20.278
8
1.344
1.646
2.180
2.733
3.490
5.071
10.219
13.362
15.507
17.535
20.090
21.955
9
1.735
2.088
2.700
3.325
4.168
5.899
11.389
14.684
16.919
19.023
21.666
23.589
10
2.156
2.558
3.247
3.940
4.865
6.737
12.549
15.987
18.307
20.483
23.209
25.188
11
2.603
3.053
3.816
4.575
5.578
7.584
13.701
17.275
19.675
21.920
24.725
26.757
12
3.074
3.571
4.404
5.226
6.304
8.438
14.845
18.549
21.026
23.337
26.217
28.299
13
3.565
4.107
5.009
5.892
7.042
9.299
15.984
19.812
22.362
24.736
27.688
29.819
14
4.075
4.660
5.629
6.571
7.790
10.165
17.117
21.064
23.685
26.119
29.141
31.319
15
4.601
5.229
6.262
7.261
8.547
11.037
18.245
22.307
24.996
27.488
30.578
32.801
16
5.142
5.812
6.908
7.962
9.312
11.912
19.369
23.542
26.296
28.845
32.000
34.267
17
5.697
6.408
7.564
8.672
10.085
12.792
20.489
24.769
27.587
30.191
33.409
35.718
18
6.265
7.015
8.231
9.390
10.865
13.675
21.605
25.989
28.869
31.526
34.805
37.156
19
6.844
7.633
8.907
10.117
11.651
14.562
22.718
27.204
30.144
32.852
36.191
38.582
20
7.434
8.260
9.591
10.851
12.443
15.452
23.828
28.412
31.410
34.170
37.566
39.997
21
8.034
8.897
10.283
11.591
13.240
16.344
24.935
29.615
32.671
35.479
38.932
41.401
22
8.643
9.542
10.982
12.338
14.042
17.240
26.039
30.813
33.924
36.781
40.289
42.796
23
9.260
10.196
11.689
13.091
14.848
18.137
27.141
32.007
35.172
38.076
41.638
44.181
24
9.886
10.856
12.401
13.848
15.659
19.037
28.241
33.196
36.415
39.364
42.980
45.559
25
10.520
11.524
13.120
14.611
16.473
19.939
29.339
34.382
37.652
40.646
44.314
46.928
26
11.160
12.198
13.844
15.379
17.292
20.843
30.435
35.563
38.885
41.923
45.642
48.290
27
11.808
12.879
14.573
16.151
18.114
21.749
31.528
36.741
40.113
43.194
46.963
49.645
28
12.461
13.565
15.308
16.928
18.939
22.657
32.620
37.916
41.337
44.461
48.278
50.993
29
13.121
14.257
16.047
17.708
19.768
23.567
33.711
39.087
42.557
45.722
49.588
52.336
30
13.787
14.954
16.791
18.493
20.599
24.478
34.800
40.256
43.773
46.979
50.892
53.672
For larger values of degrees of freedom (df) the expression Z = 22x2 - 221df2 - 1 may be used and the resulting upper-tail area can be found 
from the cumulative standardized normal distribution (Table E.2).
0
a
x2
1 2 a

T a b l e  E . 5
Critical Values of F
For a particular combination of numerator and denominator degrees of freedom, entry represents the critical values 
of F corresponding to the cumulative probability (1 - a) and a specified upper-tail area (a).
Cumulative Probabilities = 0.95
Upper@Tail Areas = 0.05
Numerator, df1
Denominator, 
df2
1
2
3
4
5
6
7
8
9
10
12
15
20
24
30
40
60
120
H
1
161.40
199.50
215.70
224.60
230.20
234.00
236.80
238.90
240.50
241.90
243.90
245.90
248.00
249.10
250.10
251.10
252.20
253.30
254.30
2
18.51
19.00
19.16
19.25
19.30
19.33
19.35
19.37
19.38
19.40
19.41
19.43
19.45
19.45
19.46
19.47
19.48
19.49
19.50
3
10.13
9.55
9.28
9.12
9.01
8.94
8.89
8.85
8.81
8.79
8.74
8.70
8.66
8.64
8.62
8.59
8.57
8.55
8.53
4
7.71
6.94
6.59
6.39
6.26
6.16
6.09
6.04
6.00
5.96
5.91
5.86
5.80
5.77
5.75
5.72
5.69
5.66
5.63
5
6.61
5.79
5.41
5.19
5.05
4.95
4.88
4.82
4.77
4.74
4.68
4.62
4.56
4.53
4.50
4.46
4.43
4.40
4.36
6
5.99
5.14
4.76
4.53
4.39
4.28
4.21
4.15
4.10
4.06
4.00
3.94
3.87
3.84
3.81
3.77
3.74
3.70
3.67
7
5.59
4.74
4.35
4.12
3.97
3.87
3.79
3.73
3.68
3.64
3.57
3.51
3.44
3.41
3.38
3.34
3.30
3.27
3.23
8
5.32
4.46
4.07
3.84
3.69
3.58
3.50
3.44
3.39
3.35
3.28
3.22
3.15
3.12
3.08
3.04
3.01
2.97
2.93
9
5.12
4.26
3.86
3.63
3.48
3.37
3.29
3.23
3.18
3.14
3.07
3.01
2.94
2.90
2.86
2.83
2.79
2.75
2.71
10
4.96
4.10
3.71
3.48
3.33
3.22
3.14
3.07
3.02
2.98
2.91
2.85
2.77
2.74
2.70
2.66
2.62
2.58
2.54
11
4.84
3.98
3.59
3.36
3.20
3.09
3.01
2.95
2.90
2.85
2.79
2.72
2.65
2.61
2.57
2.53
2.49
2.45
2.40
12
4.75
3.89
3.49
3.26
3.11
3.00
2.91
2.85
2.80
2.75
2.69
2.62
2.54
2.51
2.47
2.43
2.38
2.34
2.30
13
4.67
3.81
3.41
3.18
3.03
2.92
2.83
2.77
2.71
2.67
2.60
2.53
2.46
2.42
2.38
2.34
2.30
2.25
2.21
14
4.60
3.74
3.34
3.11
2.96
2.85
2.76
2.70
2.65
2.60
2.53
2.46
2.39
2.35
2.31
2.27
2.22
2.18
2.13
15
4.54
3.68
3.29
3.06
2.90
2.79
2.71
2.64
2.59
2.54
2.48
2.40
2.33
2.29
2.25
2.20
2.16
2.11
2.07
16
4.49
3.63
3.24
3.01
2.85
2.74
2.66
2.59
2.54
2.49
2.42
2.35
2.28
2.24
2.19
2.15
2.11
2.06
2.01
17
4.45
3.59
3.20
2.96
2.81
2.70
2.61
2.55
2.49
2.45
2.38
2.31
2.23
2.19
2.15
2.10
2.06
2.01
1.96
18
4.41
3.55
3.16
2.93
2.77
2.66
2.58
2.51
2.46
2.41
2.34
2.27
2.19
2.15
2.11
2.06
2.02
1.97
1.92
19
4.38
3.52
3.13
2.90
2.74
2.63
2.54
2.48
2.42
2.38
2.31
2.23
2.16
2.11
2.07
2.03
1.98
1.93
1.88
20
4.35
3.49
3.10
2.87
2.71
2.60
2.51
2.45
2.39
2.35
2.28
2.20
2.12
2.08
2.04
1.99
1.95
1.90
1.84
21
4.32
3.47
3.07
2.84
2.68
2.57
2.49
2.42
2.37
2.32
2.25
2.18
2.10
2.05
2.01
1.96
1.92
1.87
1.81
22
4.30
3.44
3.05
2.82
2.66
2.55
2.46
2.40
2.34
2.30
2.23
2.15
2.07
2.03
1.98
1.91
1.89
1.84
1.78
23
4.28
3.42
3.03
2.80
2.64
2.53
2.44
2.37
2.32
2.27
2.20
2.13
2.05
2.01
1.96
1.91
1.86
1.81
1.76
24
4.26
3.40
3.01
2.78
2.62
2.51
2.42
2.36
2.30
2.25
2.18
2.11
2.03
1.98
1.94
1.89
1.84
1.79
1.73
25
4.24
3.39
2.99
2.76
2.60
2.49
2.40
2.34
2.28
2.24
2.16
2.09
2.01
1.96
1.92
1.87
1.82
1.77
1.71
26
4.23
3.37
2.98
2.74
2.59
2.47
2.39
2.32
2.27
2.22
2.15
2.07
1.99
1.95
1.90
1.85
1.80
1.75
1.69
27
4.21
3.35
2.96
2.73
2.57
2.46
2.37
2.31
2.25
2.20
2.13
2.06
1.97
1.93
1.88
1.84
1.79
1.73
1.67
28
4.20
3.34
2.95
2.71
2.56
2.45
2.36
2.29
2.24
2.19
2.12
2.04
1.96
1.91
1.87
1.82
1.77
1.71
1.65
29
4.18
3.33
2.93
2.70
2.55
2.43
2.35
2.28
2.22
2.18
2.10
2.03
1.94
1.90
1.85
1.81
1.75
1.70
1.64
30
4.17
3.32
2.92
2.69
2.53
2.42
2.33
2.27
2.21
2.16
2.09
2.01
1.93
1.89
1.84
1.79
1.74
1.68
1.62
40
4.08
3.23
2.84
2.61
2.45
2.34
2.25
2.18
2.12
2.08
2.00
1.92
1.84
1.79
1.74
1.69
1.64
1.58
1.51
60
4.00
3.15
2.76
2.53
2.37
2.25
2.17
2.10
2.04
1.99
1.92
1.84
1.75
1.70
1.65
1.59
1.53
1.47
1.39
120
3.92
3.07
2.68
2.45
2.29
2.17
2.09
2.02
1.96
1.91
1.83
1.75
1.66
1.61
1.55
1.50
1.43
1.35
1.25
∞
3.84
3.00
2.60
2.37
2.21
2.10
2.01
1.94
1.88
1.83
1.75
1.67
1.57
1.52
1.46
1.39
1.32
1.22
1.00
 
777
(continued )
0
F
a = 0.05

Cumulative Probabilities = 0.975
Upper@Tail Areas = 0.025
Numerator, df1
Denominator, 
df2
1
2
3
4
5
6
7
8
9
10
12
15
20
24
30
40
60
120
H
1
647.80
799.50
864.20
899.60
921.80
937.10
948.20
956.70
963.30
968.60
976.70
984.90
993.10
997.20
1,001.00
1,006.00
1,010.00
1,014.00
1,018.00
2
38.51
39.00
39.17
39.25
39.30
39.33
39.36
39.39
39.39
39.40
39.41
39.43
39.45
39.46
39.46
39.47
39.48
39.49
39.50
3
17.44
16.04
15.44
15.10
14.88
14.73
14.62
14.54
14.47
14.42
14.34
14.25
14.17
14.12
14.08
14.04
13.99
13.95
13.90
4
12.22
10.65
9.98
9.60
9.36
9.20
9.07
8.98
8.90
8.84
8.75
8.66
8.56
8.51
8.46
8.41
8.36
8.31
8.26
5
10.01
8.43
7.76
7.39
7.15
6.98
6.85
6.76
6.68
6.62
6.52
6.43
6.33
6.28
6.23
6.18
6.12
6.07
6.02
6
8.81
7.26
6.60
6.23
5.99
5.82
5.70
5.60
5.52
5.46
5.37
5.27
5.17
5.12
5.07
5.01
4.96
4.90
4.85
7
8.07
6.54
5.89
5.52
5.29
5.12
4.99
4.90
4.82
4.76
4.67
4.57
4.47
4.42
4.36
4.31
4.25
4.20
4.14
8
7.57
6.06
5.42
5.05
4.82
4.65
4.53
4.43
4.36
4.30
4.20
4.10
4.00
3.95
3.89
3.84
3.78
3.73
3.67
9
7.21
5.71
5.08
4.72
4.48
4.32
4.20
4.10
4.03
3.96
3.87
3.77
3.67
3.61
3.56
3.51
3.45
3.39
3.33
10
6.94
5.46
4.83
4.47
4.24
4.07
3.95
3.85
3.78
3.72
3.62
3.52
3.42
3.37
3.31
3.26
3.20
3.14
3.08
11
6.72
5.26
4.63
4.28
4.04
3.88
3.76
3.66
3.59
3.53
3.43
3.33
3.23
3.17
3.12
3.06
3.00
2.94
2.88
12
6.55
5.10
4.47
4.12
3.89
3.73
3.61
3.51
3.44
3.37
3.28
3.18
3.07
3.02
2.96
2.91
2.85
2.79
2.72
13
6.41
4.97
4.35
4.00
3.77
3.60
3.48
3.39
3.31
3.25
3.15
3.05
2.95
2.89
2.84
2.78
2.72
2.66
2.60
14
6.30
4.86
4.24
3.89
3.66
3.50
3.38
3.29
3.21
3.15
3.05
2.95
2.84
2.79
2.73
2.67
2.61
2.55
2.49
15
6.20
4.77
4.15
3.80
3.58
3.41
3.29
3.20
3.12
3.06
2.96
2.86
2.76
2.70
2.64
2.59
2.52
2.46
2.40
16
6.12
4.69
4.08
3.73
3.50
3.34
3.22
3.12
3.05
2.99
2.89
2.79
2.68
2.63
2.57
2.51
2.45
2.38
2.32
17
6.04
4.62
4.01
3.66
3.44
3.28
3.16
3.06
2.98
2.92
2.82
2.72
2.62
2.56
2.50
2.44
2.38
2.32
2.25
18
5.98
4.56
3.95
3.61
3.38
3.22
3.10
3.01
2.93
2.87
2.77
2.67
2.56
2.50
2.44
2.38
2.32
2.26
2.19
19
5.92
4.51
3.90
3.56
3.33
3.17
3.05
2.96
2.88
2.82
2.72
2.62
2.51
2.45
2.39
2.33
2.27
2.20
2.13
20
5.87
4.46
3.86
3.51
3.29
3.13
3.01
2.91
2.84
2.77
2.68
2.57
2.46
2.41
2.35
2.29
2.22
2.16
2.09
21
5.83
4.42
3.82
3.48
3.25
3.09
2.97
2.87
2.80
2.73
2.64
2.53
2.42
2.37
2.31
2.25
2.18
2.11
2.04
22
5.79
4.38
3.78
3.44
3.22
3.05
2.93
2.84
2.76
2.70
2.60
2.50
2.39
2.33
2.27
2.21
2.14
2.08
2.00
23
5.75
4.35
3.75
3.41
3.18
3.02
2.90
2.81
2.73
2.67
2.57
2.47
2.36
2.30
2.24
2.18
2.11
2.04
1.97
24
5.72
4.32
3.72
3.38
3.15
2.99
2.87
2.78
2.70
2.64
2.54
2.44
2.33
2.27
2.21
2.15
2.08
2.01
1.94
25
5.69
4.29
3.69
3.35
3.13
2.97
2.85
2.75
2.68
2.61
2.51
2.41
2.30
2.24
2.18
2.12
2.05
1.98
1.91
26
5.66
4.27
3.67
3.33
3.10
2.94
2.82
2.73
2.65
2.59
2.49
2.39
2.28
2.22
2.16
2.09
2.03
1.95
1.88
27
5.63
4.24
3.65
3.31
3.08
2.92
2.80
2.71
2.63
2.57
2.47
2.36
2.25
2.19
2.13
2.07
2.00
1.93
1.85
28
5.61
4.22
3.63
3.29
3.06
2.90
2.78
2.69
2.61
2.55
2.45
2.34
2.23
2.17
2.11
2.05
1.98
1.91
1.83
29
5.59
4.20
3.61
3.27
3.04
2.88
2.76
2.67
2.59
2.53
2.43
2.32
2.21
2.15
2.09
2.03
1.96
1.89
1.81
30
5.57
4.18
3.59
3.25
3.03
2.87
2.75
2.65
2.57
2.51
2.41
2.31
2.20
2.14
2.07
2.01
1.94
1.87
1.79
40
5.42
4.05
3.46
3.13
2.90
2.74
2.62
2.53
2.45
2.39
2.29
2.18
2.07
2.01
1.94
1.88
1.80
1.72
1.64
60
5.29
3.93
3.34
3.01
2.79
2.63
2.51
2.41
2.33
2.27
2.17
2.06
1.94
1.88
1.82
1.74
1.67
1.58
1.48
120
5.15
3.80
3.23
2.89
2.67
2.52
2.39
2.30
2.22
2.16
2.05
1.94
1.82
1.76
1.69
1.61
1.53
1.43
1.31
∞
5.02
3.69
3.12
2.79
2.57
2.41
2.29
2.19
2.11
2.05
1.94
1.83
1.71
1.64
1.57
1.48
1.39
1.27
1.00
 
778
T a b l e  E . 5
Critical Values of F (continued)
For a particular combination of numerator and denominator degrees of freedom, entry represents the critical values 
of F corresponding to the cumulative probability (1 - a) and a specified upper-tail area (a).
0
F
a = 0.025

Cumulative Probabilities = 0.99
Upper@Tail Areas = 0.01
Numerator, df1
Denominator, 
df2
1
2
3
4
5
6
7
8
9
10
12
15
20
24
30
40
60
120
H
1
4,052.00 4,999.50 5,403.00 5,625.00 5,764.00 5,859.00 5,928.00 5,982.00 6,022.00 6,056.00 6,106.00 6,157.00 6,209.00 6,235.00 6,261.00 6,287.00 6,313.00 6,339.00 6,366.00
2
98.50
99.00
99.17
99.25
99.30
99.33
99.36
99.37
99.39
99.40
99.42
99.43
44.45
99.46
99.47
99.47
99.48
99.49
99.50
3
34.12
30.82
29.46
28.71
28.24
27.91
27.67
27.49
27.35
27.23
27.05
26.87
26.69
26.60
26.50
26.41
26.32
26.22
26.13
4
21.20
18.00
16.69
15.98
15.52
15.21
14.98
14.80
14.66
14.55
14.37
14.20
14.02
13.93
13.84
13.75
13.65
13.56
13.46
5
16.26
13.27
12.06
11.39
10.97
10.67
10.46
10.29
10.16
10.05
9.89
9.72
9.55
9.47
9.38
9.29
9.20
9.11
9.02
6
13.75
10.92
9.78
9.15
8.75
8.47
8.26
8.10
7.98
7.87
7.72
7.56
7.40
7.31
7.23
7.14
7.06
6.97
6.88
7
12.25
9.55
8.45
7.85
7.46
7.19
6.99
6.84
6.72
6.62
6.47
6.31
6.16
6.07
5.99
5.91
5.82
5.74
5.65
8
11.26
8.65
7.59
7.01
6.63
6.37
6.18
6.03
5.91
5.81
5.67
5.52
5.36
5.28
5.20
5.12
5.03
4.95
4.86
9
10.56
8.02
6.99
6.42
6.06
5.80
5.61
5.47
5.35
5.26
5.11
4.96
4.81
4.73
4.65
4.57
4.48
4.40
4.31
10
10.04
7.56
6.55
5.99
5.64
5.39
5.20
5.06
4.94
4.85
4.71
4.56
4.41
4.33
4.25
4.17
4.08
4.00
3.91
11
9.65
7.21
6.22
5.67
5.32
5.07
4.89
4.74
4.63
4.54
4.40
4.25
4.10
4.02
3.94
3.86
3.78
3.69
3.60
12
9.33
6.93
5.95
5.41
5.06
4.82
4.64
4.50
4.39
4.30
4.16
4.01
3.86
3.78
3.70
3.62
3.54
3.45
3.36
13
9.07
6.70
5.74
5.21
4.86
4.62
4.44
4.30
4.19
4.10
3.96
3.82
3.66
3.59
3.51
3.43
3.34
3.25
3.17
14
8.86
6.51
5.56
5.04
4.69
4.46
4.28
4.14
4.03
3.94
3.80
3.66
3.51
3.43
3.35
3.27
3.18
3.09
3.00
15
8.68
6.36
5.42
4.89
4.56
4.32
4.14
4.00
3.89
3.80
3.67
3.52
3.37
3.29
3.21
3.13
3.05
2.96
2.87
16
8.53
6.23
5.29
4.77
4.44
4.20
4.03
3.89
3.78
3.69
3.55
3.41
3.26
3.18
3.10
3.02
2.93
2.81
2.75
17
8.40
6.11
5.18
4.67
4.34
4.10
3.93
3.79
3.68
3.59
3.46
3.31
3.16
3.08
3.00
2.92
2.83
2.75
2.65
18
8.29
6.01
5.09
4.58
4.25
4.01
3.84
3.71
3.60
3.51
3.37
3.23
3.08
3.00
2.92
2.84
2.75
2.66
2.57
19
8.18
5.93
5.01
4.50
4.17
3.94
3.77
3.63
3.52
3.43
3.30
3.15
3.00
2.92
2.84
2.76
2.67
2.58
2.49
20
8.10
5.85
4.94
4.43
4.10
3.87
3.70
3.56
3.46
3.37
3.23
3.09
2.94
2.86
2.78
2.69
2.61
2.52
2.42
21
8.02
5.78
4.87
4.37
4.04
3.81
3.64
3.51
3.40
3.31
3.17
3.03
2.88
2.80
2.72
2.64
2.55
2.46
2.36
22
7.95
5.72
4.82
4.31
3.99
3.76
3.59
3.45
3.35
3.26
3.12
2.98
2.83
2.75
2.67
2.58
2.50
2.40
2.31
23
7.88
5.66
4.76
4.26
3.94
3.71
3.54
3.41
3.30
3.21
3.07
2.93
2.78
2.70
2.62
2.54
2.45
2.35
2.26
24
7.82
5.61
4.72
4.22
3.90
3.67
3.50
3.36
3.26
3.17
3.03
2.89
2.74
2.66
2.58
2.49
2.40
2.31
2.21
25
7.77
5.57
4.68
4.18
3.85
3.63
3.46
3.32
3.22
3.13
2.99
2.85
2.70
2.62
2.54
2.45
2.36
2.27
2.17
26
7.72
5.53
4.64
4.14
3.82
3.59
3.42
3.29
3.18
3.09
2.96
2.81
2.66
2.58
2.50
2.42
2.33
2.23
2.13
27
7.68
5.49
4.60
4.11
3.78
3.56
3.39
3.26
3.15
3.06
2.93
2.78
2.63
2.55
2.47
2.38
2.29
2.20
2.10
28
7.64
5.45
4.57
4.07
3.75
3.53
3.36
3.23
3.12
3.03
2.90
2.75
2.60
2.52
2.44
2.35
2.26
2.17
2.06
29
7.60
5.42
4.54
4.04
3.73
3.50
3.33
3.20
3.09
3.00
2.87
2.73
2.57
2.49
2.41
2.33
2.23
2.14
2.03
30
7.56
5.39
4.51
4.02
3.70
3.47
3.30
3.17
3.07
2.98
2.84
2.70
2.55
2.47
2.39
2.30
2.21
2.11
2.01
40
7.31
5.18
4.31
3.83
3.51
3.29
3.12
2.99
2.89
2.80
2.66
2.52
2.37
2.29
2.20
2.11
2.02
1.92
1.80
60
7.08
4.98
4.13
3.65
3.34
3.12
2.95
2.82
2.72
2.63
2.50
2.35
2.20
2.12
2.03
1.94
1.84
1.73
1.60
120
6.85
4.79
3.95
3.48
3.17
2.96
2.79
2.66
2.56
2.47
2.34
2.19
2.03
1.95
1.86
1.76
1.66
1.53
1.38
∞
6.63
4.61
3.78
3.32
3.02
2.80
2.64
2.51
2.41
2.32
2.18
2.04
1.88
1.79
1.70
1.59
1.47
1.32
1.00
  
779
0
F
a = 0.01
(continued )
T a b l e  E . 5
Critical Values of F (continued)
For a particular combination of numerator and denominator degrees of freedom, entry represents the critical values 
of F corresponding to the cumulative probability (1 - a) and a specified upper-tail area (a).

Cumulative Probabilities = 0.995
Upper −Tail Areas = 0.005
Numerator, df1
Denominator, 
df2
1
2
3
4
5
6
7
8
9
10
12
15
20
24
30
40
60
120
H
1
16,211.00 20,000.00 21,615.00 22,500.00 23,056.00 23,437.00 23,715.00 23,925.00 24,091.00 24,224.00 24,426.00 24,630.00 24,836.00 24,910.00 25,044.00 25,148.00 25,253.00 25,359.00 25,465.00
2
198.50
199.00
199.20
199.20
199.30
199.30
199.40
199.40
199.40
199.40
199.40
199.40
199.40
199.50
199.50
199.50
199.50
199.50
199.50
3
55.55
49.80
47.47
46.19
45.39
44.84
44.43
44.13
43.88
43.69
43.39
43.08
42.78
42.62
42.47
42.31
42.15
41.99
41.83
4
31.33
26.28
24.26
23.15
22.46
21.97
21.62
21.35
21.14
20.97
20.70
20.44
20.17
20.03
19.89
19.75
19.61
19.47
19.32
5
22.78
18.31
16.53
15.56
14.94
14.51
14.20
13.96
13.77
13.62
13.38
13.15
12.90
12.78
12.66
12.53
12.40
12.27
12.11
6
18.63
14.54
12.92
12.03
11.46
11.07
10.79
10.57
10.39
10.25
10.03
9.81
9.59
9.47
9.36
9.24
9.12
9.00
8.88
7
16.24
12.40
10.88
10.05
9.52
9.16
8.89
8.68
8.51
8.38
8.18
7.97
7.75
7.65
7.53
7.42
7.31
7.19
7.08
8
14.69
11.04
9.60
8.81
8.30
7.95
7.69
7.50
7.34
7.21
7.01
6.81
6.61
6.50
6.40
6.29
6.18
6.06
5.95
9
13.61
10.11
8.72
7.96
7.47
7.13
6.88
6.69
6.54
6.42
6.23
6.03
5.83
5.73
5.62
5.52
5.41
5.30
5.19
10
12.83
9.43
8.08
7.34
6.87
6.54
6.30
6.12
5.97
5.85
5.66
5.47
5.27
5.17
5.07
4.97
4.86
4.75
4.61
11
12.23
8.91
7.60
6.88
6.42
6.10
5.86
5.68
5.54
5.42
5.24
5.05
4.86
4.75
4.65
4.55
4.44
4.34
4.23
12
11.75
8.51
7.23
6.52
6.07
5.76
5.52
5.35
5.20
5.09
4.91
4.72
4.53
4.43
4.33
4.23
4.12
4.01
3.90
13
11.37
8.19
6.93
6.23
5.79
5.48
5.25
5.08
4.94
4.82
4.64
4.46
4.27
4.17
4.07
3.97
3.87
3.76
3.65
14
11.06
7.92
6.68
6.00
5.56
5.26
5.03
4.86
4.72
4.60
4.43
4.25
4.06
3.96
3.86
3.76
3.66
3.55
3.41
15
10.80
7.70
6.48
5.80
5.37
5.07
4.85
4.67
4.54
4.42
4.25
4.07
3.88
3.79
3.69
3.58
3.48
3.37
3.26
16
10.58
7.51
6.30
5.64
5.21
4.91
4.69
4.52
4.38
4.27
4.10
3.92
3.73
3.64
3.54
3.44
3.33
3.22
3.11
17
10.38
7.35
6.16
5.50
5.07
4.78
4.56
4.39
4.25
4.14
3.97
3.79
3.61
3.51
3.41
3.31
3.21
3.10
2.98
18
10.22
7.21
6.03
5.37
4.96
4.66
4.44
4.28
4.14
4.03
3.86
3.68
3.50
3.40
3.30
3.20
3.10
2.99
2.87
19
10.07
7.09
5.92
5.27
4.85
4.56
4.34
4.18
4.04
3.93
3.76
3.59
3.40
3.31
3.21
3.11
3.00
2.89
2.78
20
9.94
6.99
5.82
5.17
4.76
4.47
4.26
4.09
3.96
3.85
3.68
3.50
3.32
3.22
3.12
3.02
2.92
2.81
2.69
21
9.83
6.89
5.73
5.09
4.68
4.39
4.18
4.02
3.88
3.77
3.60
3.43
3.24
3.15
3.05
2.95
2.84
2.73
2.61
22
9.73
6.81
5.65
5.02
4.61
4.32
4.11
3.94
3.81
3.70
3.54
3.36
3.18
3.08
2.98
2.88
2.77
2.66
2.55
23
9.63
6.73
5.58
4.95
4.54
4.26
4.05
3.88
3.75
3.64
3.47
3.30
3.12
3.02
2.92
2.82
2.71
2.60
2.48
24
9.55
6.66
5.52
4.89
4.49
4.20
3.99
3.83
3.69
3.59
3.42
3.25
3.06
2.97
2.87
2.77
2.66
2.55
2.43
25
9.48
6.60
5.46
4.84
4.43
4.15
3.94
3.78
3.64
3.54
3.37
3.20
3.01
2.92
2.82
2.72
2.61
2.50
2.38
26
9.41
6.54
5.41
4.79
4.38
4.10
3.89
3.73
3.60
3.49
3.33
3.15
2.97
2.87
2.77
2.67
2.56
2.45
2.33
27
9.34
6.49
5.36
4.74
4.34
4.06
3.85
3.69
3.56
3.45
3.28
3.11
2.93
2.83
2.73
2.63
2.52
2.41
2.29
28
9.28
6.44
5.32
4.70
4.30
4.02
3.81
3.65
3.52
3.41
3.25
3.07
2.89
2.79
2.69
2.59
2.48
2.37
2.25
29
9.23
6.40
5.28
4.66
4.26
3.98
3.77
3.61
3.48
3.38
3.21
3.04
2.86
2.76
2.66
2.56
2.45
2.33
2.21
30
9.18
6.35
5.24
4.62
4.23
3.95
3.74
3.58
3.45
3.34
3.18
3.01
2.82
2.73
2.63
2.52
2.42
2.30
2.18
40
8.83
6.07
4.98
4.37
3.99
3.71
3.51
3.35
3.22
3.12
2.95
2.78
2.60
2.50
2.40
2.30
2.18
2.06
1.93
60
8.49
5.79
4.73
4.14
3.76
3.49
3.29
3.13
3.01
2.90
2.74
2.57
2.39
2.29
2.19
2.08
1.96
1.83
1.69
120
8.18
5.54
4.50
3.92
3.55
3.28
3.09
2.93
2.81
2.71
2.54
2.37
2.19
2.09
1.98
1.87
1.75
1.61
1.43
∞
7.88
5.30
4.28
3.72
3.35
3.09
2.90
2.74
2.62
2.52
2.36
2.19
2.00
1.90
1.79
1.67
1.53
1.36
1.00
780
T a b l e  E . 5
Critical Values of F (continued)
For a particular combination of numerator and denominator degrees of freedom, entry represents the critical values 
of F corresponding to the cumulative probability (1 - a) and a specified upper-tail area (a).
0
F
a = 0.005

	
Appendix E  Tables	
781
T a b l e  E . 6
Lower and Upper 
Critical Values, T1, of the 
Wilcoxon Rank Sum Test
A
n1
n2
One-tail
Two-tail
4
5
6
7
8
9
10
4
0.05
0.10
11,25
0.025
0.05
10,26
0.01
0.02
—,—
0.005
0.01
—,—
5
0.05
0.10
12,28
19,36
0.025
0.05
11,29
17,38
0.01
0.02
10,30
16,39
0.005
0.01
—,—
15,40
6
0.05
0.10
13,31
20,40
28,50
0.025
0.05
12,32
18,42
26,52
0.01
0.02
11,33
17,43
24,54
0.005
0.01
10,34
16,44
23,55
7
0.05
0.10
14,34
21,44
29,55
39,66
0.025
0.05
13,35
20,45
27,57
36,69
0.01
0.02
11,37
18,47
25,59
34,71
0.005
0.01
10,38
16,49
24,60
32,73
8
0.05
0.10
15,37
23,47
31,59
41,71
51,85
0.025
0.05
14,38
21,49
29,61
38,74
49,87
0.01
0.02
12,40
19,51
27,63
35,77
45,91
0.005
0.01
11,41
17,53
25,65
34,78
43,93
9
0.05
0.10
16,40
24,51
33,63
43,76
54,90
66,105
0.025
0.05
14,42
22,53
31,65
40,79
51,93
62,109
0.01
0.02
13,43
20,55
28,68
37,82
47,97
59,112
0.005
0.01
11,45
18,57
26,70
35,84
45,99
56,115
10
0.05
0.10
17,43
26,54
35,67
45,81
56,96
69,111
82,128
0.025
0.05
15,45
23,57
32,70
42,84
53,99
65,115
78,132
0.01
0.02
13,47
21,59
29,73
39,87
49,103
61,119
74,136
0.005
0.01
12,48
19,61
27,75
37,89
47,105
58,122
71,139
Source: Adapted from Table 1 of F. Wilcoxon and R. A. Wilcox, Some Rapid Approximate Statistical 
Procedures (Pearl River, NY: Lederle Laboratories, 1964), with permission of the American Cyanamid 
Company.

T a b l e  E . 7
Critical Values of the Studentized Range, Q
Upper 5% Points (A = 0.05)
Denominator,  
df
Numerator, df
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
1
18.00
27.00
32.80
37.10
40.40
43.10
45.40
47.40
49.10
50.60
52.00
53.20
54.30
55.40
56.30
57.20
58.00
58.80
59.60
2
6.09
8.30
9.80
10.90
11.70
12.40
13.00
13.50
14.00
14.40
14.70
15.10
15.40
15.70
15.90
16.10
16.40
16.60
16.80
3
4.50
5.91
6.82
7.50
8.04
8.48
8.85
9.18
9.46
9.72
9.95
10.15
10.35
10.52
10.69
10.84
10.98
11.11
11.24
4
3.93
5.04
5.76
6.29
6.71
7.05
7.35
7.60
7.83
8.03
8.21
8.37
8.52
8.66
8.79
8.91
9.03
9.13
9.23
5
3.64
4.60
5.22
5.67
6.03
6.33
6.58
6.80
6.99
7.17
7.32
7.47
7.60
7.72
7.83
7.93
8.03
8.12
8.21
6
3.46
4.34
4.90
5.31
5.63
5.89
6.12
6.32
6.49
6.65
6.79
6.92
7.03
7.14
7.24
7.34
7.43
7.51
7.59
7
3.34
4.16
4.68
5.06
5.36
5.61
5.82
6.00
6.16
6.30
6.43
6.55
6.66
6.76
6.85
6.94
7.02
7.09
7.17
8
3.26
4.04
4.53
4.89
5.17
5.40
5.60
5.77
5.92
6.05
6.18
6.29
6.39
6.48
6.57
6.65
6.73
6.80
6.87
9
3.20
3.95
4.42
4.76
5.02
5.24
5.43
5.60
5.74
5.87
5.98
6.09
6.19
6.28
6.36
6.44
6.51
6.58
6.64
10
3.15
3.88
4.33
4.65
4.91
5.12
5.30
5.46
5.60
5.72
5.83
5.93
6.03
6.11
6.20
6.27
6.34
6.40
6.47
11
3.11
3.82
4.26
4.57
4.82
5.03
5.20
5.35
5.49
5.61
5.71
5.81
5.90
5.99
6.06
6.14
6.20
6.26
6.33
12
3.08
3.77
4.20
4.51
4.75
4.95
5.12
5.27
5.40
5.51
5.62
5.71
5.80
5.88
5.95
6.03
6.09
6.15
6.21
13
3.06
3.73
4.15
4.45
4.69
4.88
5.05
5.19
5.32
5.43
5.53
5.63
5.71
5.79
5.86
5.93
6.00
6.05
6.11
14
3.03
3.70
4.11
4.41
4.64
4.83
4.99
5.13
5.25
5.36
5.46
5.55
5.64
5.72
5.79
5.85
5.92
5.97
6.03
15
3.01
3.67
4.08
4.37
4.60
4.78
4.94
5.08
5.20
5.31
5.40
5.49
5.58
5.65
5.72
5.79
5.85
5.90
5.96
16
3.00
3.65
4.05
4.33
4.56
4.74
4.90
5.03
5.15
5.26
5.35
5.44
5.52
5.59
5.66
5.72
5.79
5.84
5.90
17
2.98
3.63
4.02
4.30
4.52
4.71
4.86
4.99
5.11
5.21
5.31
5.39
5.47
5.55
5.61
5.68
5.74
5.79
5.84
18
2.97
3.61
4.00
4.28
4.49
4.67
4.82
4.96
5.07
5.17
5.27
5.35
5.43
5.50
5.57
5.63
5.69
5.74
5.79
19
2.96
3.59
3.98
4.25
4.47
4.65
4.79
4.92
5.04
5.14
5.23
5.32
5.39
5.46
5.53
5.59
5.65
5.70
5.75
20
2.95
3.58
3.96
4.23
4.45
4.62
4.77
4.90
5.01
5.11
5.20
5.28
5.36
5.43
5.49
5.55
5.61
5.66
5.71
24
2.92
3.53
3.90
4.17
4.37
4.54
4.68
4.81
4.92
5.01
5.10
5.18
5.25
5.32
5.38
5.44
5.50
5.54
5.59
30
2.89
3.49
3.84
4.10
4.30
4.46
4.60
4.72
4.83
4.92
5.00
5.08
5.15
5.21
5.27
5.33
5.38
5.43
5.48
40
2.86
3.44
3.79
4.04
4.23
4.39
4.52
4.63
4.74
4.82
4.91
4.98
5.05
5.11
5.16
5.22
5.27
5.31
5.36
60
2.83
3.40
3.74
3.98
4.16
4.31
4.44
4.55
4.65
4.73
4.81
4.88
4.94
5.00
5.06
5.11
5.16
5.20
5.24
120
2.80
3.36
3.69
3.92
4.10
4.24
4.36
4.48
4.56
4.64
4.72
4.78
4.84
4.90
4.95
5.00
5.05
5.09
5.13
∞
2.77
3.31
3.63
3.86
4.03
4.17
4.29
4.39
4.47
4.55
4.62
4.68
4.74
4.80
4.85
4.89
4.93
4.97
5.01
782

Upper 1% Points (A = 0.01)
Denominator, 
df
Numerator, df
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
1
90.03
135.00
164.30
185.60
202.20
215.80
227.20
237.00
245.60
253.20
260.00
266.20
271.80
277.00
281.80
286.30
290.40
294.30
298.00
2
14.04
19.02
22.29
24.72
26.63
28.20
29.53
30.68
31.69
32.59
33.40
34.13
34.81
35.43
36.00
36.53
37.03
37.50
37.95
3
8.26
10.62
12.17
13.33
14.24
15.00
15.64
16.20
16.69
17.13
17.53
17.89
18.22
18.52
18.81
19.07
19.32
19.55
19.77
4
6.51
8.12
9.17
9.96
10.58
11.10
11.55
11.93
12.27
12.57
12.84
13.09
13.32
13.53
13.73
13.91
14.08
14.24
14.40
5
5.70
6.98
7.80
8.42
8.91
9.32
9.67
9.97
10.24
10.48
10.70
10.89
11.08
11.24
11.40
11.55
11.68
11.81
11.93
6
5.24
6.33
7.03
7.56
7.97
8.32
8.61
8.87
9.10
9.30
9.49
9.65
9.81
9.95
10.08
10.21
10.32
10.43
10.54
7
4.95
5.92
6.54
7.01
7.37
7.68
7.94
8.17
8.37
8.55
8.71
8.86
9.00
9.12
9.24
9.35
9.46
9.55
9.65
8
4.75
5.64
6.20
6.63
6.96
7.24
7.47
7.68
7.86
8.03
8.18
8.31
8.44
8.55
8.66
8.76
8.85
8.94
9.03
9
4.60
5.43
5.96
6.35
6.66
6.92
7.13
7.32
7.50
7.65
7.78
7.91
8.03
8.13
8.23
8.33
8.41
8.50
8.57
10
4.48
5.27
5.77
6.14
6.43
6.67
6.87
7.06
7.21
7.36
7.49
7.60
7.71
7.81
7.91
7.99
8.08
8.15
8.23
11
4.39
5.15
5.62
5.97
6.25
6.48
6.67
6.84
6.99
7.13
7.25
7.36
7.47
7.56
7.65
7.73
7.81
7.88
7.95
12
4.32
5.04
5.50
5.84
6.10
6.32
6.51
6.67
6.81
6.94
7.06
7.17
7.26
7.36
7.44
7.52
7.59
7.66
7.73
13
4.26
4.96
5.40
5.73
5.98
6.19
6.37
6.53
6.67
6.79
6.90
7.01
7.10
7.19
7.27
7.35
7.42
7.49
7.55
14
4.21
4.90
5.32
5.63
5.88
6.09
6.26
6.41
6.54
6.66
6.77
6.87
6.96
7.05
7.13
7.20
7.27
7.33
7.40
15
4.17
4.84
5.25
5.56
5.80
5.99
6.16
6.31
6.44
6.56
6.66
6.76
6.85
6.93
7.00
7.07
7.14
7.20
7.26
16
4.13
4.79
5.19
5.49
5.72
5.92
6.08
6.22
6.35
6.46
6.56
6.66
6.74
6.82
6.90
6.97
7.03
7.09
7.15
17
4.10
4.74
5.14
5.43
5.66
5.85
6.01
6.15
6.27
6.38
6.48
6.57
6.66
6.73
6.81
6.87
6.94
7.00
7.05
18
4.07
4.70
5.09
5.38
5.60
5.79
5.94
6.08
6.20
6.31
6.41
6.50
6.58
6.66
6.73
6.79
6.85
6.91
6.97
19
4.05
4.67
5.05
5.33
5.55
5.74
5.89
6.02
6.14
6.25
6.34
6.43
6.51
6.59
6.65
6.72
6.78
6.84
6.89
20
4.02
4.64
5.02
5.29
5.51
5.69
5.84
5.97
6.09
6.19
6.29
6.37
6.45
6.52
6.59
6.65
6.71
6.77
6.82
24
3.96
4.55
4.91
5.17
5.37
5.54
5.69
5.81
5.92
6.02
6.11
6.19
6.26
6.33
6.39
6.45
6.51
6.56
6.61
30
3.89
4.46
4.80
5.05
5.24
5.40
5.54
5.65
5.76
5.85
5.93
6.01
6.08
6.14
6.20
6.26
6.31
6.36
6.41
40
3.83
4.37
4.70
4.93
5.11
5.27
5.39
5.50
5.60
5.69
5.76
5.84
5.90
5.96
6.02
6.07
6.12
6.17
6.21
60
3.76
4.28
4.60
4.82
4.99
5.13
5.25
5.36
5.45
5.53
5.60
5.67
5.73
5.79
5.84
5.89
5.93
5.97
6.02
120
3.70
4.20
4.50
4.71
4.87
5.01
5.12
5.21
5.30
5.38
5.44
5.51
5.56
5.61
5.66
5.71
5.75
5.79
5.83
∞
3.64
4.12
4.40
4.60
4.76
4.88
4.99
5.08
5.16
5.23
5.29
5.35
5.40
5.45
5.49
5.54
5.57
5.61
5.65
Source: Extracted from H. L. Harter and D. S. Clemm, “The Probability Integrals of the Range and of the Studentized Range—Probability Integral, Percentage Points, and Moments of the Range,” 
Wright Air Development Technical Report 58–484, Vol. 1, 1959.
T a b l e  E . 7
Critical Values of the Studentized Range, Q (continued)
783

T a b l e  E . 8
Critical Values, dI and dU, of the Durbin–Watson Statistic, D (Critical Values Are One-Sided)a
A = 0.05
A = 0.01
k = 1
k = 2
k = 3
k = 4
k = 5
k = 1
k = 2
k = 3
k = 4
k = 5
n
dL
dU
dL
dU
dL
dU
dL
dU
dL
dU
dL
dU
dL
dU
dL
dU
dL
dU
dL
dU
15
1.08
1.36
.95
1.54
.82
1.75
.69
1.97
.56
2.21
.81
1.07
.70
1.25
.59
1.46
.49
1.70
.39
1.96
16
1.10
1.37
.98
1.54
.86
1.73
.74
1.93
.62
2.15
.84
1.09
.74
1.25
.63
1.44
.53
1.66
.44
1.90
17
1.13
1.38
1.02
1.54
.90
1.71
.78
1.90
.67
2.10
.87
1.10
.77
1.25
.67
1.43
.57
1.63
.48
1.85
18
1.16
1.39
1.05
1.53
.93
1.69
.82
1.87
.71
2.06
.90
1.12
.80
1.26
.71
1.42
.61
1.60
.52
1.80
19
1.18
1.40
1.08
1.53
.97
1.68
.86
1.85
.75
2.02
.93
1.13
.83
1.26
.74
1.41
.65
1.58
.56
1.77
20
1.20
1.41
1.10
1.54
1.00
1.68
.90
1.83
.79
1.99
.95
1.15
.86
1.27
.77
1.41
.68
1.57
.60
1.74
21
1.22
1.42
1.13
1.54
1.03
1.67
.93
1.81
.83
1.96
.97
1.16
.89
1.27
.80
1.41
.72
1.55
.63
1.71
22
1.24
1.43
1.15
1.54
1.05
1.66
.96
1.80
.86
1.94
1.00
1.17
.91
1.28
.83
1.40
.75
1.54
.66
1.69
23
1.26
1.44
1.17
1.54
1.08
1.66
.99
1.79
.90
1.92
1.02
1.19
.94
1.29
.86
1.40
.77
1.53
.70
1.67
24
1.27
1.45
1.19
1.55
1.10
1.66
1.01
1.78
.93
1.90
1.04
1.20
.96
1.30
.88
1.41
.80
1.53
.72
1.66
25
1.29
1.45
1.21
1.55
1.12
1.66
1.04
1.77
.95
1.89
1.05
1.21
.98
1.30
.90
1.41
.83
1.52
.75
1.65
26
1.30
1.46
1.22
1.55
1.14
1.65
1.06
1.76
.98
1.88
1.07
1.22
1.00
1.31
.93
1.41
.85
1.52
.78
1.64
27
1.32
1.47
1.24
1.56
1.16
1.65
1.08
1.76
1.01
1.86
1.09
1.23
1.02
1.32
.95
1.41
.88
1.51
.81
1.63
28
1.33
1.48
1.26
1.56
1.18
1.65
1.10
1.75
1.03
1.85
1.10
1.24
1.04
1.32
.97
1.41
.90
1.51
.83
1.62
29
1.34
1.48
1.27
1.56
1.20
1.65
1.12
1.74
1.05
1.84
1.12
1.25
1.05
1.33
.99
1.42
.92
1.51
.85
1.61
30
1.35
1.49
1.28
1.57
1.21
1.65
1.14
1.74
1.07
1.83
1.13
1.26
1.07
1.34
1.01
1.42
.94
1.51
.88
1.61
31
1.36
1.50
1.30
1.57
1.23
1.65
1.16
1.74
1.09
1.83
1.15
1.27
1.08
1.34
1.02
1.42
.96
1.51
.90
1.60
32
1.37
1.50
1.31
1.57
1.24
1.65
1.18
1.73
1.11
1.82
1.16
1.28
1.10
1.35
1.04
1.43
.98
1.51
.92
1.60
33
1.38
1.51
1.32
1.58
1.26
1.65
1.19
1.73
1.13
1.81
1.17
1.29
1.11
1.36
1.05
1.43
1.00
1.51
.94
1.59
34
1.39
1.51
1.33
1.58
1.27
1.65
1.21
1.73
1.15
1.81
1.18
1.30
1.13
1.36
1.07
1.43
1.01
1.51
.95
1.59
35
1.40
1.52
1.34
1.58
1.28
1.65
1.22
1.73
1.16
1.80
1.19
1.31
1.14
1.37
1.08
1.44
1.03
1.51
.97
1.59
36
1.41
1.52
1.35
1.59
1.29
1.65
1.24
1.73
1.18
1.80
1.21
1.32
1.15
1.38
1.10
1.44
1.04
1.51
.99
1.59
37
1.42
1.53
1.36
1.59
1.31
1.66
1.25
1.72
1.19
1.80
1.22
1.32
1.16
1.38
1.11
1.45
1.06
1.51
1.00
1.59
38
1.43
1.54
1.37
1.59
1.32
1.66
1.26
1.72
1.21
1.79
1.23
1.33
1.18
1.39
1.12
1.45
1.07
1.52
1.02
1.58
39
1.43
1.54
1.38
1.60
1.33
1.66
1.27
1.72
1.22
1.79
1.24
1.34
1.19
1.39
1.14
1.45
1.09
1.52
1.03
1.58
40
1.44
1.54
1.39
1.60
1.34
1.66
1.29
1.72
1.23
1.79
1.25
1.34
1.20
1.40
1.15
1.46
1.10
1.52
1.05
1.58
45
1.48
1.57
1.43
1.62
1.38
1.67
1.34
1.72
1.29
1.78
1.29
1.38
1.24
1.42
1.20
1.48
1.16
1.53
1.11
1.58
50
1.50
1.59
1.46
1.63
1.42
1.67
1.38
1.72
1.34
1.77
1.32
1.40
1.28
1.45
1.24
1.49
1.20
1.54
1.16
1.59
55
1.53
1.60
1.49
1.64
1.45
1.68
1.41
1.72
1.38
1.77
1.36
1.43
1.32
1.47
1.28
1.51
1.25
1.55
1.21
1.59
60
1.55
1.62
1.51
1.65
1.48
1.69
1.44
1.73
1.41
1.77
1.38
1.45
1.35
1.48
1.32
1.52
1.28
1.56
1.25
1.60
65
1.57
1.63
1.54
1.66
1.50
1.70
1.47
1.73
1.44
1.77
1.41
1.47
1.38
1.50
1.35
1.53
1.31
1.57
1.28
1.61
70
1.58
1.64
1.55
1.67
1.52
1.70
1.49
1.74
1.46
1.77
1.43
1.49
1.40
1.52
1.37
1.55
1.34
1.58
1.31
1.61
75
1.60
1.65
1.57
1.68
1.54
1.71
1.51
1.74
1.49
1.77
1.45
1.50
1.42
1.53
1.39
1.56
1.37
1.59
1.34
1.62
80
1.61
1.66
1.59
1.69
1.56
1.72
1.53
1.74
1.51
1.77
1.47
1.52
1.44
1.54
1.42
1.57
1.39
1.60
1.36
1.62
85
1.62
1.67
1.60
1.70
1.57
1.72
1.55
1.75
1.52
1.77
1.48
1.53
1.46
1.55
1.43
1.58
1.41
1.60
1.39
1.63
90
1.63
1.68
1.61
1.70
1.59
1.73
1.57
1.75
1.54
1.78
1.50
1.54
1.47
1.56
1.45
1.59
1.43
1.61
1.41
1.64
95
1.64
1.69
1.62
1.71
1.60
1.73
1.58
1.75
1.56
1.78
1.51
1.55
1.49
1.57
1.47
1.60
1.45
1.62
1.42
1.64
100
1.65
1.69
1.63
1.72
1.61
1.74
1.59
1.76
1.57
1.78
1.52
1.56
1.50
1.58
1.48
1.60
1.46
1.63
1.44
1.65
an =  number of observations; k = number of independent variables.
Source: Computed from TSP 4.5 based on R. W. Farebrother, “A Remark on Algorithms AS106, AS153, and AS155: The Distribution of a Linear Combination of Chi-Square Random Variables,” 
Journal of the Royal Statistical Society, Series C (Applied Statistics), 1984, 29, p. 323–333.
784

	
Appendix E  Tables	
785
T a b l e  E . 9
Control Chart Factors
Number of Observations 
in Sample/Subgroup (n)
d2
d3
D3
D4
A2
  2
1.128
0.853
0
3.267
1.880
  3
1.693
0.888
0
2.575
1.023
  4
2.059
0.880
0
2.282
0.729
  5
2.326
0.864
0
2.114
0.577
  6
2.534
0.848
0
2.004
0.483
  7
2.704
0.833
0.076
1.924
0.419
  8
2.847
0.820
0.136
1.864
0.373
  9
2.970
0.808
0.184
1.816
0.337
10
3.078
0.797
0.223
1.777
0.308
11
3.173
0.787
0.256
1.744
0.285
12
3.258
0.778
0.283
1.717
0.266
13
3.336
0.770
0.307
1.693
0.249
14
3.407
0.763
0.328
1.672
0.235
15
3.472
0.756
0.347
1.653
0.223
16
3.532
0.750
0.363
1.637
0.212
17
3.588
0.744
0.378
1.622
0.203
18
3.640
0.739
0.391
1.609
0.194
19
3.689
0.733
0.404
1.596
0.187
20
3.735
0.729
0.415
1.585
0.180
21
3.778
0.724
0.425
1.575
0.173
22
3.819
0.720
0.435
1.565
0.167
23
3.858
0.716
0.443
1.557
0.162
24
3.895
0.712
0.452
1.548
0.157
25
3.931
0.708
0.459
1.541
0.153
Source: Reprinted from ASTM-STP 15D by kind permission of the American Society for Testing and Materials. 
Copyright ASTM International, 100 Barr Harbor Drive, Conshohocken, PA 19428.

786	
appendices
T a b l e  E . 1 0
The Standardized Normal Distribution
Entry represents area under the standardized normal  
distribution from the mean to Z
Z
.00
.01
.02
.03
.04
.05
.06
.07
.08
.09
0.0
.0000
.0040
.0080
.0120
.0160
.0199
.0239
.0279
.0319
.0359
0.1
.0398
.0438
.0478
.0517
.0557
.0596
.0636
.0675
.0714
.0753
0.2
.0793
.0832
.0871
.0910
.0948
.0987
.1026
.1064
.1103
.1141
0.3
.1179
.1217
.1255
.1293
.1331
.1368
.1406
.1443
.1480
.1517
0.4
.1554
.1591
.1628
.1664
.1700
.1736
.1772
.1808
.1844
.1879
0.5
.1915
.1950
.1985
.2019
.2054
.2088
.2123
.2157
.2190
.2224
0.6
.2257
.2291
.2324
.2357
.2389
.2422
.2454
.2486
.2518
.2549
0.7
.2580
.2612
.2642
.2673
.2704
.2734
.2764
.2794
.2823
.2852
0.8
.2881
.2910
.2939
.2967
.2995
.3023
.3051
.3078
.3106
.3133
0.9
.3159
.3186
.3212
.3238
.3264
.3289
.3315
.3340
.3365
.3389
1.0
.3413
.3438
.3461
.3485
.3508
.3531
.3554
.3577
.3599
.3621
1.1
.3643
.3665
.3686
.3708
.3729
.3749
.3770
.3790
.3810
.3830
1.2
.3849
.3869
.3888
.3907
.3925
.3944
.3962
.3980
.3997
.4015
1.3
.4032
.4049
.4066
.4082
.4099
.4115
.4131
.4147
.4162
.4177
1.4
.4192
.4207
.4222
.4236
.4251
.4265
.4279
.4292
.4306
.4319
1.5
.4332
.4345
.4357
.4370
.4382
.4394
.4406
.4418
.4429
.4441
1.6
.4452
.4463
.4474
.4484
.4495
.4505
.4515
.4525
.4535
.4545
1.7
.4554
.4564
.4573
.4582
.4591
.4599
.4608
.4616
.4625
.4633
1.8
.4641
.4649
.4656
.4664
.4671
.4678
.4686
.4693
.4699
.4706
1.9
.4713
.4719
.4726
.4732
.4738
.4744
.4750
.4756
.4761
.4767
2.0
.4772
.4778
.4783
.4788
.4793
.4798
.4803
.4808
.4812
.4817
2.1
.4821
.4826
.4830
.4834
.4838
.4842
.4846
.4850
.4854
.4857
2.2
.4861
.4864
.4868
.4871
.4875
.4878
.4881
.4884
.4887
.4890
2.3
.4893
.4896
.4898
.4901
.4904
.4906
.4909
.4911
.4913
.4916
2.4
.4918
.4920
.4922
.4925
.4927
.4929
.4931
.4932
.4934
.4936
2.5
.4938
.4940
.4941
.4943
.4945
.4946
.4948
.4949
.4951
.4952
2.6
.4953
.4955
.4956
.4957
.4959
.4960
.4961
.4962
.4963
.4964
2.7
.4965
.4966
.4967
.4968
.4969
.4970
.4971
.4972
.4973
.4974
2.8
.4974
.4975
.4976
.4977
.4977
.4978
.4979
.4979
.4980
.4981
2.9
.4981
.4982
.4982
.4983
.4984
.4984
.4985
.4985
.4986
.4986
3.0
.49865
.49869
.49874
.49878
.49882
.49886
.49889
.49893
.49897
.49900
3.1
.49903
.49906
.49910
.49913
.49916
.49918
.49921
.49924
.49926
.49929
3.2
.49931
.49934
.49936
.49938
.49940
.49942
.49944
.49946
.49948
.49950
3.3
.49952
.49953
.49955
.49957
.49958
.49960
.49961
.49962
.49964
.49965
3.4
.49966
.49968
.49969
.49970
.49971
.49972
.49973
.49974
.49975
.49976
3.5
.49977
.49978
.49978
.49979
.49980
.49981
.49981
.49982
.49983
.49983
3.6
.49984
.49985
.49985
.49986
.49986
.49987
.49987
.49988
.49988
.49989
3.7
.49989
.49990
.49990
.49990
.49991
.49991
.49992
.49992
.49992
.49992
3.8
.49993
.49993
.49993
.49994
.49994
.49994
.49994
.49995
.49995
.49995
3.9
.49995
.49995
.49996
.49996
.49996
.49996
.49996
.49996
.49997
.49997
0
Z

787
A p p e n d i x  F    Useful Excel Knowledge
This appendix reviews knowledge that you will find useful if you plan to be more than a casual 
user of Microsoft Excel. While none of the content in this appendix needs to be mastered in 
order to use the instructions in the Excel Guides in this book, reviewing this appendix as neces-
sary will help you make better sense of your Excel results. If you are using a version of Excel 
that is older than Excel 2010, you will need to be familiar with Section F.3 so that you can 
modify the names of functions used in worksheet templates and models as necessary.
Section F.4 presents an enhanced explanation of some the statistical worksheet functions 
that recur in two or more chapters. This section also discusses functions that either serve pro-
gramming purposes or are used in novel ways to compute intermediate results. If you have a 
particular interest in developing application solutions, you will want to be familiar with this set 
of functions.
This appendix assumes that you are familiar with Excel and have mastered the basic con-
cepts presented in Appendix B. If you are a first-time user of Excel, do not make the mistake of 
trying to comprehend the material in this appendix before you gain experience using Excel and 
familiarity with Appendix B.
F.1  Useful Keyboard Shortcuts
In Microsoft Office programs including Microsoft Excel, certain individual keys or combina-
tions of keys held down as you press another key are shortcuts that allow you to execute com-
mon operations without having to select choices from menus or click in the Ribbon. As first 
explained in Table GS.2 on page 35, in this book, keystroke combinations are shown using plus 
signs; for example, Ctrl+C means “while holding down the Ctrl key, press the C key.”
Editing Shortcuts
Pressing Backspace erases typed characters to the left of the current position, one character 
at a time. Pressing Delete erases characters to the right of the cursor, one character at a time.
Ctrl+C copies a worksheet entry, and Ctrl+V pastes that entry into the place that the edit-
ing cursor or worksheet cell highlight indicates. Pressing Ctrl+X cuts the currently selected 
entry or object so that you can paste it somewhere else. Ctrl+C and Ctrl+V (or Ctrl+X and 
Ctrl+V) can also be used to copy (or cut) and paste certain workbook objects such as charts. 
(Using copy and paste to copy formulas from one worksheet cell to another is subject to the 
adjustment discussed in Section B-2.)
Pressing Ctrl+Z undoes the last operation, and Ctrl+Y redoes the last operation. Pressing 
Enter or Tab finalizes an entry typed into a worksheet cell. Pressing either key is implied by 
the use of the verb enter in the Excel Guides.
Formatting Shortcuts
Pressing Ctrl+B toggles on (or off) boldface text style for the currently selected object. 
Pressing Ctrl+I toggles on (or off) italic text style for the currently selected object. Pressing 
Ctrl+Shift+% formats numeric values as a percentage with no decimal places.
Utility Shortcuts
Pressing Ctrl+F finds a Find what value, and pressing Ctrl+H replaces a Find what value 
with the Replace with value. Pressing Ctrl+A selects the entire current worksheet (useful as 
part of a worksheet copy or format operation). Pressing Esc cancels an action or a dialog box. 
Pressing F1 displays the Microsoft Excel help system.

788	
appendices
F.2  Verifying Formulas and Worksheets
If you use formulas in your worksheets, you should review and verify formulas before you use 
their results. To view the formulas in a worksheet, press Ctrl+` (grave accent key). To restore 
the original view, the results of the formulas, press Ctrl+` a second time.
As you create and use more complicated worksheets, you might want to visually examine 
the relationships among a formula and the cells it uses (called the precedents) and the cells 
that use the results of the formula (the dependents). Select Formulas ➔ Trace Precedents (or 
Trace Dependents) to examine relationships. When you are finished, clear all trace arrows by 
selecting Formulas ➔ Remove Arrows.
After verifying formulas, you should test, using simple numbers, any worksheet that you 
have modified or constructed from scratch.
F.3  New Function Names
Beginning in Excel 2010, Microsoft renamed many statistical functions and reprogrammed 
a number of functions to improve their accuracy. Generally, with exceptions noted, this book 
uses the new function names in worksheet cell formulas. The new function names used in this 
book are listed in Table F.1, along with the place of first mention in this book and correspond-
ing older function name.
Because the new function names are not compatible with Excel versions older than 
Excel 2010, alternative worksheets have been included in the Excel Guide workbooks, as 
explained in “Alternative Worksheets,” later in this section. If compatibility with older Excel 
versions is important to you, you should use the older function names (and the alternative 
worksheets).
T a b l e  F. 1
New Function Names 
Used in This Book and 
Older (“Compatible”) 
Names
New Name
First Mention
Older Name
BINOM.DIST
EG5.3
BINOMDIST
CHISQ.DIST.RT
EG12.1
CHIDIST
CHISQ.INV.RT
EG12.1
CHIINV
CONFIDENCE.NORM
EG8.1
CONFIDENCE
COVARIANCE.S
EG3.5
none*
EXPON.DIST
EG6.5
EXPONDIST
F.DIST.RT
EG10.4
FDIST
F.INV.RT
EG10.4
FINV
HYPGEOM.DIST
EG5.5
HYPGEOMDIST
NORM.DIST
EG6.2
NORMDIST
NORM.INV
EG6.2
NORMINV
NORM.S.DIST
EG9.1
NORMSDIST
NORM.S.INV
EG6.2
NORMSINV
POISSON.DIST
EG5.4
POISSON
STDEV.S
EG3.2
STDEV
STDEV.P
EG3.2
STDEVP
T.DIST.RT
EG9.3
TDIST
T.DIST.2T
EG9.2
TDIST
T.INV.2T
EG8.2
TINV
VAR.S
EG3.2
VAR
VAR.P
EG3.2
VARP
*COVARIANCE.S is a function that was new to Excel 2010. The COVARIANCE.P function  
(not used in this book) replaces the older COVAR function.

	
appendix F  Useful Excel Knowledge	
789
Quartile Function
In this book, you will see the older QUARTILE function and not the newer QUARTILE.EXC 
function. In Microsoft’s Function Improvements in Microsoft Office Excel 2010 (available at 
bit.ly/RkoFIf), QUARTILE.EXC is explained as being “consistent with industry best prac-
tices, assuming percentile is a value between 0 and 1, exclusive.” Because there are several 
established but different ways of computing quartiles, there is no way of knowing exactly how 
the new function works.
Because of this lack of specifics, this book uses the older QUARTILE function, whose 
programming and limitations are well known, and not the new QUARTILE.EXC function or 
QUARTILE.INC function, which is the QUARTILES function renamed for consistency with 
QUARTILES.EXC. As noted in Section EG3.3, none of the three functions compute quar-
tiles using the rules presented in Section 3.3, which are properly computed in the COMPUTE 
worksheet of the Quartiles workbook that uses the older QUARTILE function. If you are using 
Excel 2010 or a newer version of Excel, the COMPARE worksheet illustrates the results using 
the three forms of QUARTILES for the data found in column A of the DATA worksheet.
Alternative Worksheets
If a worksheet in an Excel Guide workbook uses one or more of the new function names, the 
workbook contains an alternative worksheet for use with Excel versions that are older than 
Excel 2010. Three exceptions to the rule are the Simple Linear Regression 2007, Multiple 
Regression 2007, and Exponential Trend 2007 workbooks. As explained in Chapters 13, 14, 
and 16, respectively, these workbooks serve as alternatives to the Simple Linear Regression, 
Multiple Regression, and Exponential Trend workbooks. Alternative worksheets and work-
books work best in Excel 2007.
The following Excel Guide workbooks contain an alternative worksheet named COM-
PUTE_OLDER. Numbers that appear in pare ntheses are the chapters in which these work-
books are first mentioned.
Parameters (3)
Covariance (3)
Hypergeometric (5)
Normal (6)
Exponential (6)
CIE sigma known (8)
CIE sigma unknown (8)
CIE Proportion (8)
Z Mean workbook (9)
T mean workbook (9)
Z Proportion (9)
Pooled-Variance T (10)
Separate-Variance T (10)
Paired T (10)
One-Way ANOVA (11)
Chi-Square (12)
The following Excel Guide workbooks have alternative worksheets with various names:
Descriptive (3)
CompleteStatistics_OLDER
Binomial (5)
CUMULATIVE_OLDER
Poisson (5)
CUMULATIVE_OLDER
NPP (6)
PLOT_OLDER and NORMAL_PLOT_OLDER
One-Way ANOVA (11)
TK4_OLDER
Chi-Square Worksheets (12)
Various worksheets, including ChiSquare2x3_
OLDER and Marascuilo2x3_OLDER
Wilcoxon (12)
COMPUTE_ALL_OLDER
Kruskal-Wallis  
Worksheets (12)
KruskalWallis3_OLDER and KruskalWallis4_
OLDER
As explained in Chapters 13, 14, and 16, respectively, the Simple Linear Regression 
2007, Multiple Regression 2007, and Exponential Trend 2007 workbooks contain a number 
of alternative worksheets for Excel versions that are older than Excel 2010. (These alternative 
workbooks work best in Excel 2007.)

790	
appendices
F.4  Understanding the Nonstatistical Functions
Although this book focuses on Excel statistical functions, selected Excel Guide and PHStat 
worksheets use a number of nonstatistical functions that either compute an intermediate result 
or perform a mathematical or programming operation. These functions are explained in the fol-
lowing alphabetical list:
CEILING(cell, round-to value)  takes the numeric value in cell and rounds it to the next multi-
ple of the round-to value. For example, if the round-to value is 0.5, as it is in several column 
B formulas in the COMPUTE worksheet of the Quartiles workbook, then the numeric value 
will be rounded either to an integer or a number that contains a half such as 1.5.
COUNT(cell range)  counts the number of cells in a cell range that contain a numeric value. 
This function is often used to compute the sample size, n, for example, in cell B9 of the 
COMPUTE worksheet of the Correlation workbook. When seen in the worksheets pre-
sented in this book, the cell range will typically be the cell range of variable column, such 
as DATA!A:A. This will result in a proper count of the sample size of that variable if you 
follow the Section EG.2 rules for entering data (see pages 36 and 37).
COUNTIF(cell range for all values, value to be matched)  counts the number of occurrences of 
a value in a cell range. For example, the COMPUTE worksheet of the Wilcoxon workbook 
uses COUNTIF(SortedRanks!A2:A21, "Beverage") in cell B7 to compute the sample 
size of the Population 1 Sample by counting the number of occurrences of the sample name 
Beverage in column A of the SortedRanks worksheet. In the KruskalWallis4 worksheet of 
the Kruskal–Wallis Worksheets workbook, the function counts the number of occurrences in 
column A of the SortedRanks worksheet of a supplier name that appears in a column D row.
DEVSQ(variable cell range)  computes the sum of the squares of the differences between a 
variable value and the mean of that variable. For example, in Equations (3.6) on page 141 
that defines the sample variance, DEVSQ(X variable cell range) computes the value of the 
term in the numerator of the fraction.
FLOOR(cell, 1)  takes the numeric value in cell and rounds down the value to the nearest integer.
IF(logical comparison, what to display if comparison holds, what to display if comparison 
is false)  uses the logical comparison to make a choice between two alternatives. In the 
worksheets shown in this book, the IF function typically chooses from two text values, such 
as Reject the null hypothesis and Do not reject the null hypothesis, to display, but in 
Chapter 16, the function is used to create dummy variables for quarterly or monthly data.
MMULT(cell range 1, cell range 2)  treats both cell range 1 and cell range 2 as matrices 
and computes the matrix product of the two matrices. When each of the two cell ranges 
is either a single row or a single column, MMULT can be used as part of a regular for-
mula. If the cell ranges each represent rows and columns, then MMULT must be used as 
part of an array formula (see Appendix Section B.3). One exception to these rules occurs 
in cell B21 of the CIEandPI worksheet of the Multiple Regression worksheet, in which 
MMULT(TRANSPOSE(B5:B7), COMPUTE!B17:B19) has been entered as part of an 
array formula because of how Excel treats the results of the TRANSPOSE function.
ROUND(cell, 0)  takes the numeric value in cell and rounds to the nearest whole number.
SMALL(cell range, k)  selects the kth smallest value in cell range.
SQRT(value)  computes the square root of value, where value is either a cell reference or an 
arithmetic expression.
SUMIF(cell range for all values, value to be matched, cell range in which to select cells for 
summing)  sums only those rows in cell range in which to select cells for summing in which 
the value in cell range for all values matches the value to be matched. SUMIF provides 
a convenient way to compute the sum of ranks for a sample in a worksheet that contains 
stacked data. For example, the COMPUTE worksheet of the Wilcoxon workbook uses 
SUMIF(SortedRanks!A2:A21, "Beverage", SortedRanks!C2:C21) in cell B8 to com-
pute the sum of ranks for the Beverage (End-cap) sample by summing only the rows in the 
SortedRanks worksheet column C whose column A value is Beverage. In the KruskalWallis4 

	
appendix F  Useful Excel Knowledge	
791
worksheet of the Kruskal–Wallis Worksheets workbook, SUMIF sums only the rows in the 
SortedRanks worksheet column C whose column A value matches the value that appears in 
a column D row.
SUMPRODUCT(cell range 1, cell range 2)  multiplies each cell in cell range 1 by the cor-
responding cell in cell range 2 and then sums those products. If cell range 1 contains 
a column of differences between an X value and the mean of the variable X, and cell  
range 2 contains a column of differences between a Y value and the mean of the variable 
Y, then this function would compute the value of the numerator in Equation (3.16) that 
defines the sample covariance. In Section EG16.6, SUMPRODUCT(ABS(cell range of 
residual values)) uses the function in a novel way with only one cell range to efficiently 
compute the sum of the absolute values of the values found in the cell range of residual 
values.
TRANSPOSE(horizontal or vertical cell range)  takes the cell range, which must be either a 
horizontal cell range (cells all in the same row) or a vertical cell range (cells all in the same 
column) and transposes, or rearranges, the cell in the other orientation such that a horizon-
tal cell range becomes a vertical cell range and vice versa. When used inside another func-
tion, Excel considers the results of this function to be an array, not a cell range.
VLOOKUP(lookup value cell, table of lookup values, table column to use)  function displays 
a value that has been looked up in a table of lookup values, a rectangular cell range. In 
the ADVANCED worksheet of the Recoded workbook, the function uses the values in the 
second column of table of lookup values (an example of which is shown below) to look 
up the Honors values based on the GPA of a student (the lookup value cell). Numbers in 
the first column of table of lookup values are implied ranges such that No Honors is the 
value displayed if the GPA is at least 0, but less than 3; Honor Roll is the value displayed 
if the GPA is at least 3, but less than 3.3; and so on:
0
No Honors
3
Honor Roll
3.3
Dean’s List
3.7
President’s List

792
A p p e n d i x  G    Software FAQs
G.1  PHStat FAQs
What is PHStat?
PHStat is the macro-enabled workbook that you use with 
Excel to help build solutions to statistical problems. With 
PHStat, you fill in simple-to-use dialog boxes and watch 
as PHStat creates a worksheet solution for you. PHStat 
allows you to use the Microsoft Excel statistical functions 
without having to first learn advanced Excel techniques or 
worrying about building worksheets from scratch. As a stu-
dent studying statistics, you can focus mainly on learning 
­statistics and not worry about having to fully master Excel 
as well.
PHStat executes for you the low-level menu selection 
and worksheet entry tasks that are associated with imple-
menting statistical analysis in Microsoft Excel. PHStat cre-
ates worksheets and chart sheets that are identical to the ones 
featured in this book. From these sheets, you can learn real 
Excel techniques at your leisure and give yourself the ability 
to use Excel effectively outside your introductory statistics 
course. (Other add-ins that appear similar to PHStat report 
results as a series of text labels, hiding the details of using 
Microsoft Excel and leaving you with no basis for learning 
to use Excel effectively.)
Which versions of Excel are compatible with PHStat?
PHStat works best with Microsoft Windows Excel 2010 and 
Excel 2013 and with OS X Excel 2011. PHStat is also compat-
ible with Excel 2007 (WIN), although the accuracy of some 
Excel statistical functions PHStat uses varies from Excel 2010 
and can lead to (minor) changes in the results reported.
PHStat is partially compatible with Excel 2003 (WIN). 
When you open PHStat in Excel 2003, you will see a file 
conversion dialog box as Excel translates the .xlam file into 
a format that can be used in Excel 2003. After this file con-
version completes, you will be able to see the PHStat menu 
and use many of the PHStat procedures. As documented in 
the PHStat help system some advanced procedures construct 
worksheets that use Excel functions that were added after 
Excel 2003 was published. In those cases, the worksheets 
will contain cells that display the #NAME? error message 
instead of results.
PHStat is not compatible with Excel 2008 (OS X), 
which did include the capability of running add-in work-
books.
How do I get PHStat ready for use?
Section D.2 explains how to get PHStat ready for use. 
You should also review the PHStat readme file (available 
for download as discussed in Appendix C) for any late-
breaking news or changes that might affect this process.
When I open PHStat, I get a Microsoft Excel error mes-
sage that mentions a “compile error” or “hidden work-
book.” What is wrong?
Most likely, you have not applied the Microsoft-supplied 
updates to your copy of Microsoft Excel (see Section D.1). If 
you are certain that your copy of Microsoft Excel is fully up-
to-date, verify that your copy is properly licensed and undam-
aged. (If necessary, you can rerun the Microsoft Office setup 
program to repair the installation of Excel.)
When I use a particular PHStat procedure, I get an error 
message that includes the words “unexpected error.” 
What should I do?
“Unexpected error” messages are typically caused by improp-
erly prepared data. Review your data to ensure that you have 
organized your data according to the conventions PHStat 
expects, as explained in the Section EG.5 on page 40 and the 
PHStat help system, and “clean” your data, as discussed in 
Section 1.3, if necessary.
Where can I get further news and information about 
PHStat? Where can I get further assistance about using 
PHStat?
Several websites can provide you with news and information 
or provide you with assistance that supplements the readme 
file and help system included with PHStat.
www.pearsonhighered.com/phstat is Pearson 
­Education’s official web page for PHStat. From this page, you 
can download PHStat (requires an access code as ­explained 
in Section C.4) or contact Pearson 24/7 Technical Support 
­directly about any technical issue that you cannot resolve.
phstat.davidlevinestatistics.com is a website main-
tained by the authors of this book that contains general news 
and information about PHStat. 
phstatcommunity.org is a new website organized by 
PHStat users and endorsed by the developers of PHStat. 
You can click News on the home page to display the latest 
news and developments about PHStat. Other content on the 
website explains some of the “behind-the-scenes” technical 
workings of PHStat.
How can I make sure that my version of PHStat is 
­up-to-date? How can I get updates to PHStat when they 
become available?
PHStat is subject to continuous improvement. When enhance-
ments are made, a new PHStat zip archive is posted on the 
PHStat home page (see Section C.4) and, if you hold a valid 

	
appendix G  Software FAQs	
793
access code, you can download that archive and overwrite 
your older version. To discover the version number of your 
copy of PHStat, select About PHStat from the PHStat menu. 
(The version number for the PHStat version supplied for use 
with this book will always be a number that begins with 4.)
G.2  Microsoft Excel FAQs
Do all Microsoft Excel versions contain the same fea-
tures and functionality? Which Microsoft Excel version 
should I use?
Unfortunately, features and functionality vary across ver-
sions still in use (including versions no longer supported by 
Microsoft). This book works best with Microsoft Windows 
versions Excel 2010 and Excel 2013 and OS X version Excel 
2011. However, even among these current versions there 
are variations in features. For example, the slicer function-
ality discussed in Section 17.1 is found only in Excel 2010 
and Excel 2013 and is missing in OS X Excel 2011 as well 
as in older Microsoft Windows versions. PivotTables have 
subtle differences across versions, none of which affect the 
instructions and examples in this book, and PivotCharts, not 
discussed in this book, are not included in Excel 2011 (see 
related PivotChart FAQ).
This book identifies differences among versions when 
they are significant. In particular, this book supplies, when 
necessary, special instructions and alternative worksheets 
(discussed in Appendix Section F.3) designed for versions 
that are both older than Excel 2010 and currently supported 
by Microsoft. If you plan to use Microsoft Windows Excel 
2007, an upgrade will give you access to the newest features 
and provide a version with significantly increased statistical 
accuracy.
If you use OS X Excel 2008, you must upgrade to use 
PHStat or any of the other add-in workbooks mentioned in 
this book. Even if you plan to avoid using any add-ins, you 
should consider upgrading to OS X Excel 2011 for the same 
reasons that Excel 2003 and Excel 2007 face.
What does “Compatibility Mode” in the title bar mean?
Excel displays “Compatibility Mode” when you open and 
use a workbook that has been previously stored using the 
older .xls Excel workbook file format. Compatibility Mode 
does not affect Excel functionality but will cause Excel to 
review your workbook for exclusive-to-xlsx formatting 
properties and Excel will question you with a dialog box 
should you go to save the workbook in this format.
To convert a .xls workbook to the .xlsx format, select 
File ➔ Save As and select Excel Workbook (*.xlsx) from 
the Save as type (WIN) or the Format (OS X) drop-down 
list in Excel 2010, 2011, or 2013. To do so in Excel 2007, 
click the Office Button, move the mouse pointer over Save 
As, and, in the Save As gallery, click Excel Workbook to 
save the workbook in the .xlsx file format.
One quirk in Microsoft Excel is that when you convert 
a workbook by using Save As, the newly converted .xlsx 
workbook stays temporarily in Compatibility Mode. To 
avoid possible complications and errors, close the newly 
converted workbook and then reopen it.
Using Compatibility Mode can cause minor differences 
in the objects such as charts and PivotTables that Excel cre-
ates and can cause problems when you seek to transfer data 
from other workbooks. Unless you need to open a work-
book in a version of Excel that is older than Excel 2007, you 
should avoid using Compatibility Mode.
What Excel security settings will allow the PHStat or a 
Visual Explorations add-in workbook to function prop-
erly when using a Microsoft Windows version of Microsoft 
Excel?
The security settings are explained in the Appendix Section 
D.3 instructions. (These settings do not apply to OS X 
Excel.)
What is a PivotChart? Why doesn't this book discuss 
PivotCharts?
PivotCharts are charts that Microsoft Excel creates automat-
ically from a PivotTable. This type of chart is not discussed 
in this book because Excel will typically create a “wrong” 
chart that takes more effort to fix than the effort needed to 
create a proper chart and because PivotChart functionality 
varies very significantly among the current Excel versions—
and is missing from OS X Excel 2011.
The special instructions for selecting a PivotTable cell 
or cell range that appear in selected Section EG2.3 In-Depth 
Excel instructions help you avoid creating an unwanted  
PivotChart. (PHStat never creates a PivotChart.)
What is Microsoft SkyDrive?
Microsoft SkyDrive is an Internet-based service that offers 
you online storage that enables you to access and share your 
files anytime and anywhere there is an Internet connection 
available. In Excel 2013, you will see SkyDrive listed as a 
choice along with Computer in the Open, Save, and Save 
As panels. In Excel 2011, you use the Document Connection 
to access SkyDrive files and select File ➔ Share ➔ Open 
SkyDrive  to save to a SkyDrive folder.
You must sign in to the SkyDrive service using a  
“Microsoft account,” formerly known as a “Windows Live 
ID.” If you use the Microsoft Office Web Excel app, or cer-
tain other special versions of Excel, you may need to sign 
into the SkyDrive service to use Excel itself.
What is Office 365?
Office 365 is a subscription-based service that supplies you 
with the latest version of Microsoft Office programs for 
your system. Office 365 requires you to be signed in using a 
Microsoft account in the same way as you would sign in to use 
SkyDrive (see previous answer). Using Office 365 gives you 
access to the latest version of Microsoft Excel, which, at the 
time of publication of this book, is Excel 2013 for Microsoft 

794	
appendices
Windows systems and Excel 2011 for OS X systems. If you 
use Office 365, use either the Excel 2013 or Excel 2011 
instructions, as appropriate. appropriate.
G.3  FAQs for New Users of 
Microsoft Excel 2013
When I open Excel 2013, I see a screen that shows panels 
that represent different workbooks and not the Ribbon 
interface. What do I do?
Press Esc. That screen, called the Start screen, will disap-
pear and a screen that contains an Excel window similar to 
the ones in Excel 2010 and Excel 2011 will appear. For a 
more permanent solution, select File ➔ Options and in the 
General panel of the Excel Options dialog box that appears 
clear Show the Start screen when this application starts 
and then click OK.
Are there any significant differences between Excel 2013 
and its immediate predecessor, Excel 2010?
There are no significant differences, but several File tab 
commands present restyled panes (with the same or similar 
information), and opening and saving files differs slightly, as 
described in the Excel Guide for the Let's Get Started chapter.
The Excel 2013 Ribbon, featured in a number of Ap-
pendix B illustrations, looks slightly different than the Excel 
2010 Ribbon. However, these differences are so slight that the 
Excel 2013 Ribbon illustrations in Appendix B will be rec-
ognizable to you if you choose to use Excel 2010.The Excel 
2013 Ribbon also contains a number of new icons and groups 
in some of its tabs, but those additions do not affect any of the 
Ribbon selection sequences presented in the Excel Guides.
In the Insert tab, what are Recommended PivotTables 
and Recommended Charts? Should I use these features?
Recommended PivotTables and Recommended Charts 
display one or more “recommended” PivotTables or charts 
as shortcuts. Unfortunately, the recommended PivotTables 
can include statistical errors such as treating the categories 
of a categorical variable as zero values of a numerical vari-
able and the recommended charts often do not conform to 
best practices (see Appendix Section B.6).
As programmed in Excel 2013, you should ignore and 
not use these features as they will likely cause you to spend 
more time correcting errors and formatting mistakes than 
the little time that you might otherwise save.
G.4  Minitab FAQs
Can I use Minitab Release 14 or 15 with this book?
Yes, you can use the Minitab Guide instructions, written for 
Minitab 16, with Release 14 or 15. For certain methods, there 
may be minor differences in labeling of dialog box elements. 
Any difference that is not minor is noted in the instructions.
Can I save my Minitab worksheets or projects for use 
with Release 14 or 15?
Yes. Select either Minitab14 or Minitab 15 (for a work-
sheet) or Minitab 14 Project (*.MPJ) or Minitab 15 
Project (*.MPJ) (for a project) from the Save as type drop-
down list in the save as dialog box. See Section MG1.3 
on page 81 for more information about using the Save 
Worksheet As and Save Project As dialog boxes.

795
Self-Test Solutions and Answers to 
Selected Even-Numbered Problems
The following sections present worked-out solutions to Self-Test Problems and brief answers to most of the even-numbered problems in the text. For 
more detailed solutions, including explanations, interpretations, and Excel and Minitab results, see the Student Solutions Manual.
CHAPTER 1
1.2 Comfort, deluxe and luxury rooms are classified into distinct cat-
egories with ranking so the type of hotel rooms is an example of ordinal 
scaled variable.
1.4 (a) The number of cellphones is a numerical variable that is discrete 
because the outcome is a count. It is ratio scaled because it has a true zero 
point. (b) Monthly data usage is a numerical variable that is continuous 
because any value within a range of values can occur. It is ratio scaled 
because it has a true zero point. (c) Number of text messages exchanged 
per month is a numerical variable that is discrete because the outcome is 
a count. It is ratio scaled because it has a true zero point. (d) Voice usage 
per month is a numerical variable that is continuous because any value 
within a range of values can occur. It is ratio scaled because it has a true 
zero point. (e) Whether a cellphone is used for email is a categorical 
variable because the answer can be only yes or no. This also makes it a 
nominal-scaled variable.
1.6 (a) Categorical, nominal scale. (b) Numerical, continuous, ratio scale. 
(c) Categorical, nominal scale. (d) Numerical, discrete, ratio scale.  
(e) Categorical, nominal scale.
1.8 (a) Numerical, continuous, ratio scale. (b) Numerical, discrete, ratio 
scale. (c) Numerical, continuous, ratio scale. (d) Categorical, nominal 
scale.
1.10 The underlying variable, ability of the students, may be continuous, 
but the measuring device, the test, does not have enough precision to 
­distinguish between the two students.
1.18 Simple random sampling.
1.20 A simple random sample would be less practical for personal 
­interviews because of travel costs (unless interviewees are paid to go to a 
central interviewing location).
1.22 Here all members of the population are equally likely to be selected, 
and the sample selection mechanism is based on chance. But selection of 
two elements is not independent; for example, if A is in the sample, we 
know that B is also and that C and D are not.
1.24 (a)
Row 16: 2323 6737 5131 8888 1718 0654 6832 4647 6510 4877
Row 17: 4579 4269 2615 1308 2455 7830 5550 5852 5514 7182
Row 18: 0989 3205 0514 2256 8514 4642 7567 8896 2977 8822
Row 19: 5438 2745 9891 4991 4523 6847 9276 8646 1628 3554
Row 20: 9475 0899 2337 0892 0048 8033 6945 9826 9403 6858
Row 21: 7029 7341 3553 1403 3340 4205 0823 4144 1048 2949
Row 22: 8515 7479 5432 9792 6575 5760 0408 8112 2507 3742
Row 23: 1110 0023 4012 8607 4697 9664 4894 3928 7072 5815
Row 24: 3687 1507 7530 5925 7143 1738 1688 5625 8533 5041
Row 25: 2391 3483 5763 3081 6090 5169 0546
Note: All sequences above 5,000 are discarded. There were no repeating 
sequences.
(b)
089
189
289
389
489
589
689
789
889
989
1089
1189
1289
1389
1489
1589
1689
1789
1889
1989
2089
2189
2289
2389
2489
2589
2689
2789
2889
2989
3089
3189
3289
3389
3489
3589
3689
3789
3889
3989
4089
4189
4289
4389
4489
4589
4689
4789
4889
4989
(c) With the single exception of invoice 0989, the invoices selected in 
the simple random sample are not the same as those selected in the 
­systematic sample. It would be highly unlikely that a simple random 
­sample would select the same units as a systematic sample.
1.26 You cannot assume that people who do not respond to surveys will 
have the same responses as those who do. Therefore you need to follow 
up on the non-responses after a specified period of time. You should make 
several attempts to convince such individuals to complete the survey and 
possibly offer an incentive to participate. 
The mode of response you use, such as face-to-face interview, tele-
phone interview, paper questionnaire, or computerized questionnaire, affects 
the rate of response. Personal interviews and telephone interviews usually 
have a higher response rate than do mail surveys—but at a higher cost.
1.28 The results are based on an online survey. If the frame is supposed 
to be smartphone and tablet users, how is the population defined? This 
is a self-selecting sample of people who responded online, so there is an 
undefined nonresponse error. Sampling error cannot be determined since 
this is not a random sample.
1.30 Before accepting the results of the survey, you might want to know, 
for example: Who funded the study? Why was it conducted? What was 
the population from which the sample was selected? What sampling 
design was used? What mode of response was used: a personal interview, 
a telephone interview, or a mail survey? Were interviewers trained? Were 
survey questions field-tested? What other questions were asked? Were 
the questions clear, accurate, unbiased, and valid? What was the response 
rate? What was the margin of error? What was the sample size? What 
frame was used?

796	
Self-Test Solutions and Answers to Selected Even-Numbered Problems
(d) Powertrain has the most complaints, followed by steering, interior 
electronics/hardware, fuel/emission/exhaust system, airbags and seatbelts, 
body and glass, brakes, and, finally, tires and wheels.
2.6 (a) The percentages are 4.00, 10.58, 25.91, and 59.51. (b) More than 
half the oil produced is from non-OPEC countries. More than 25% is 
­produced by OPEC countries other than Iran and Saudi Arabia.
2.8 (a) Table of row percentages:
Enjoy Shopping 
for Clothing
Gender
Male
Female
 Total
Yes
46%
54%
100%
No
53
47
100
Total
50
50
100
Table of column percentages:
Enjoy Shopping 
for Clothing
Gender
Male
Female
  Total
Yes
  44%
  51%
  47%
No
  56
  49
  53
Total
100
100
100
Table of total percentages:
Enjoy Shopping 
for Clothing
Gender
Male
Female
Total
Yes
22%
25%
  47%
No
28
25
  53
Total
50
50
100
(b) A higher percentage of females enjoy shopping for clothing.
2.10 Social recommendations had very little impact on correct recall. 
Those who arrived at the link from a recommendation had a correct recall 
of 73.07% as compared to those who arrived at the link from browsing 
who had a correct recall of 67.96%.
2.12 Ordered array: 3 4 4 4 4 5 5 5 5 5 5 6 6 6 6 6 7 7 7 7 8 8 8 8 8 9 9 9 9 9
The GPA of 30 students ranges from 3 to 9, however frequency tabulation 
would add more value and help draw more meaningful conclusions.
2.14 (a) 0 but less than 5 million, 5 million but less than 10 million,  
10 million but less than 15 million, 15 million but less than 20 million,  
20 million but less than 25 million, 25 million but less than 30 million. 
(b) 5 million. (c) 2.5 million, 7.5 million, 12.5 million, 17.5 million,  
22.5 million, and 27.5 million.
2.16 (a) 
Electricity Costs
Frequency
Percentage
$80 up to $99
  4
     8%
$100 up to $119
  7
14
$120 up to $139
  9
18
$140 up to $159
13
26
$160 up to $179
  9
18
$180 up to $199
  5
10
$200 up to $219
  3
  6
1.44 (a) All benefitted employees at the university. (b) The 3,095 employ-
ees who responded to the survey. (c) Gender and marital status are 
­categorical. Age (years), education level (years completed), and house-
hold income ($) are numerical.
CHAPTER 2
2.2 (a) Table of frequencies for all student responses:
    Student Major Categories
Gender
A
C
M
Totals
Male
14
  9
2
25
Female
  6
  6
3
15
Totals
20
15
5
40
(b) Table based on total percentages:
 Student Major Categories
Gender
A
C
M
Totals
Male
35.0%
22.5%
  5.0%
  62.5%
Female
15.0
15.0
  7.5
  37.5
Totals
50.0
37.5
12.5
100.0
Table based on row percentages:
Student Major Categories
Gender
A
C
M
Totals
Male
56.0%
36.0%
  8.0%
100.0%
Female
40.0
40.0
20.0
100.0
Totals
50.0
37.5
12.5
100.0
Table based on column percentages:
Student Major Categories
Gender
A
C
M
Totals
Male
  70.0%
  60.0%
  40.0%
  62.5%
Female
  30.0
  40.0
  60.0
  37.5
Totals
100.0
100.0
100.0
100.0
2.4 (a) The percentage of complaints for each automaker:
General Motors
Other
Nissan Motors Corporation
Chrysler LLC
Ford Motor Company
Toyota Motor Sales
American Honda
439
440
551
516
467
169
332
Frequency
82.81%
67.74%
18.91%
36.62%
52.64%
100.00%
94.20%
Cumulative Pct.
15.07%
15.10%
18.91%
17.71%
16.03%
 5.80%
11.39%
Percentage
Automaker
(b) General Motors has the most complaints, followed by Other, Nissan 
Motors Corporation, Ford Motor Company, Chryler LLC, Toyota Motor 
Sales and American Honda.
(c) The percentage of complaints for each category:
Powertrain
Steering
Interior Electronics/Hardware
Airbags and Seatbelts
Fuel/Emission/Exhaust System
Body and Glass
Brakes
Cumulative Pct.
Category
Tires and Wheels
Frequency
201
240
1148
397
279
163
182
71
Percentage
7.50%
8.95%
42.82%
14.81%
10.41%
6.08%
6.79%
2.65%
 84.48%
 76.99%
42.82%
57.63%
68.03%
 97.35%
91.27%
100.00%

	
Self-Test Solutions and Answers to Selected Even-Numbered Problems	
797
2.22 (a) 
 
Bulb Life (hours)
Percentage, 
Mfgr A
Percentage, 
Mfgr B
  6,500–7,499
     7.5%
     0.0%
  7,500–8,499
12.5
  5.0
  8,500–9,499
50.0
20.0
  9,500–10,499
22.5
40.0
10,500–11,499
  7.5
22.5
11,500–12,499
  0.0
12.5
(b) 
 
% Less Than
Percentage Less 
Than, Mfgr A
Percentage Less 
Than, Mfgr B
  7,500
      7.5%
     0.0%
  8,500
  20.0
  5.0
  9,500
  70.0
  25.0
10,500
  92.5
  65.0
11,500
100.0
  87.5
12,500
100.0
100.0
(c) Manufacturer B produces bulbs with longer lives than Manufacturer A. 
The cumulative percentage for Manufacturer B shows that 65% of its bulbs 
lasted less than 10,500 hours, contrasted with 92.5% of Manufacturer 
A’s bulbs. None of Manufacturer A’s bulbs lasted at least 11,500 hours, 
but 12.5% of Manufacturer B’s bulbs lasted at least 11,500 hours. At the 
same time, 7.5% of Manufacturer A’s bulbs lasted less than 7,500 hours, 
whereas none of Manufacturer B’s bulbs lasted less than 7,500 hours.
2.24 (b) The Pareto chart is best for portraying these data because it not 
only sorts the frequencies in descending order but also provides the cumu-
lative line on the same chart. (c) You can conclude that primary branding 
accounts for the largest percentage, 45%. When a mix of ­branding and 
direct response is added to primarily branding, this accounts for 84%.
2.26 (b) The top 47.17% complaints were received for cleaning issues, 
and the rest 52.83% of the complaints were for towels, heating, room ser-
vice, noise and theft, in that order. (c) Based on the results, the manage-
ment should prioritize the improvement of the cleaning services.
2.28 (b) Since energy use is spread over many types of appliances, a bar 
chart may be best in showing which types of appliances used the most 
energy. (c) Heating, water heating, and cooling accounted for 72% of the 
residential energy use in the United States.
2.30 (a) Since, the requirement is to compare two variables, a side-by-
side bar chart will be best for the given data.
2.32 (b) A higher percentage of dogs of the specific breed resemble their 
owners.
2.34 50 74 74 76 81 89 92.
2.36 (a) 
Stem 
Unit
10
Stem 
Unit
10
12
23
  0 3
13
24
  2
14
6
25
  7
15
1 2 3
26
16
0 6
27
17
2 2 4 6
28
18
4 4 5 7
29
19
6 8
30
  0
20
31
21
3 7
32
  4
22
3 4 4 5 5
33
  7
(b) The results are concentrated between $172 and $225.
(b) 
Electricity 
Costs
 
Frequency
 
Percentage
Cumulative 
%
$  99
  4
   8.00%
     8.00%
$119
  7
14.00
  22.00
$139
  9
18.00
  40.00
$159
13
26.00
  66.00
$179
  9
18.00
  84.00
$199
  5
10.00
  94.00
$219
  3
  6.00
100.00
(c) The majority of utility charges are clustered between $120 and $180.
2.18 (a), (b)
 
Credit Score
 
Frequency
 
Percentage
Cumulative 
%
695 but less than 705
  3
  2.10%
2.10%
705 but less than 715
12
  8.39%
10.49%
715 but less than 725
12
  8.39%
18.88%
715 but less than 735
19
13.29%
32.17%
735 but less than 745
18
12.59%
44.76%
745 but less than 755
24
16.78%
61.54%
755 but less than 765
22
15.38%
76.92%
765 but less than 775
20
13.99%
90.91%
775 but less than 785
10
  6.99%
97.90%
795 but less than 795
  3
  2.10%
100.00%
(c) The average credit scores are concentrated around 750.
2.20 (a) 
Width
Frequency
Percentage
8.310–8.329
  3
   6.12%
8.330–8.349
  2
  4.08
8.350–8.369
  1
  2.04
8.370–8.389
  4
  8.16
8.390–8.409
  5
10.20
8.410–8.429
16
32.65
8.430–8.449
  5
10.20
8.450–8.469
  5
10.20
8.470–8.489
  6
12.24
8.490–8.509
  2
  4.08
(b) 
Width
Percentage Less Than
8.310
0
8.330
  6.12
8.350
  10.20
8.370
  12.24
8.390
  20.40
8.410
  30.60
8.430
  63.25
8.450
  73.45
8.470
  83.65
8.490
  95.89
8.51
100.00
(c) All the troughs will meet the company’s requirements of between 8.31 
and 8.61 inches wide.

798	
Self-Test Solutions and Answers to Selected Even-Numbered Problems
(b) Patterns of star rating conditioned on market cap:
For the growth funds as a group, most are rated as four-star, fol-
lowed by three-star, two-star, five-star, and one-star. The pattern of star 
rating is the same across the different market caps within the growth 
funds with most of the funds receiving a four-star rating, followed by 
three-star, two-star, five-star, and one-star with the exception of small-cap 
funds with most of the funds receiving a four-star or three-star rating, 
­followed by two-star, one-star, and five-star.
For the value funds as a group, most are rated as three-star, followed 
by four-star, two-star, one-star, and five-star. Within the value funds, the 
large-cap funds follow the same pattern as the value funds as a group. 
Most of the mid-cap funds are rated as three-star, followed by two-star, 
four-star, five-star, and one-star while most of the small-cap funds are 
rated as three-star, followed by either two-star or four-star, and either  
one-star or five star.
Patterns of market cap conditioned on star rating:
Most of the growth funds are large-cap, followed by mid-cap and 
small-cap. The pattern is similar among the five-star, four-star, three-star, 
and two-star growth funds, but among the one-star growth funds, most are 
small-cap, followed by large-cap and mid-cap.
The largest share of the value funds is large-cap, followed by small-
cap and mid-cap. The pattern is similar among the four-star and one-star 
value funds. Among the three-star value funds, most are large-cap, fol-
lowed by mid-cap and then small-cap while most are large-cap, followed 
by equal portions of mid-cap and small-cap among the two-star value 
funds and most are either large-cap or small-cap followed by mid-cap 
among the five-star value funds.
2.60 (a) Pivot table of tallies in terms of counts:
Five
Four
One
Three
Two
Grand Total
18
Growth
76
16
74
3
Average
15
6
28
High
1
5
1
15
Low
60
5
45
5
Value
22
7
36
1
Average
3
7
High
2
4
Low
22
2
29
23
Grand Total
98
23
110
43
22
3
18
19
6
1
12
62
227
74
10
143
89
17
3
69
316
Pivot table of tallies in terms of percentage of grand total:
Five
Four
One
Three
Two
Grand Total
5.70%
Growth
24.05%
5.06%
23.42%
0.95%
Average
4.75%
1.90%
8.86%
0.00%
High
0.32%
1.58%
0.32%
4.75%
Low
18.99%
1.58%
14.24%
1.58%
Value
6.96%
2.22%
11.39%
0.32%
Average
0.00%
0.95%
2.22%
0.00%
High
0.00%
0.63%
0.00%
1.27%
Low
6.96%
0.63%
9.18%
7.28%
Grand Total
31.01%
7.28%
34.81%
13.61%
6.96%
0.95%
5.70%
6.01%
1.90%
0.32%
3.80%
19.62%
71.84%
23.42%
3.16%
45.25%
28.16%
5.38%
0.95%
21.84%
100.00%
(b) Patterns of star rating conditioned on risk:
For the growth funds as a group, most are rated as four-star, fol-
lowed by three-star, two-star, five-star, and one-star. The pattern of star 
rating is the same among the low-risk growth funds. The pattern is dif-
ferent among the high-risk and average-risk growth funds. Among the 
2.38 (c) The majority of utility charges are clustered between $120 and 
$180.
2.40 Property taxes seem concentrated between $1,000 and $1,500 and 
also between $500 and $1,000 per capita. There were more states with 
property taxes per capita below $1,500 than above $1,500.
2.42 The average credit scores are concentrated around 750.
2.44 (c) All the troughs will meet the company’s requirements of between 
8.31 and 8.61 inches wide.
2.46 (c) Manufacturer B produces bulbs with longer lives than 
Manufacturer A.
2.48 (b) Yes, there is a strong positive relationship between X and Y. As X 
increases, so does Y.
2.50 (c) There appears to be a linear relationship between the first week-
end gross and either the U.S. gross or the worldwide gross of Harry Potter 
movies. However, this relationship is greatly affected by the results of the 
last movie, Deathly Hallows, Part II.
2.52 (a), (c) There appears to be a positive relationship between the 
coaches’ total pay and revenue. Yes, this is borne out by the data.
2.54 (b) There is a great deal of variation in the returns from decade to 
decade. Most of the returns are between 5% and 15%. The 1950s, 1980s, 
and 1990s had exceptionally high returns, and only the 1930s and 2000s 
had negative returns.
2.56 (b) There was a slight decline in movie attendance between 2001 
and 2012. During that time, movie attendance increased from 2002 to 
2004 but then decreased to a level below that in 2001.
2.58 (a) Pivot table of tallies in terms of counts:
Five
Four
One
Three
Two
Grand Total
18
Growth
76
16
74
9
Large
31
5
37
7
Mid-Cap
28
4
20
2
Small
17
7
17
5
Value
22
7
36
2
Large
13
5
21
1
Mid-Cap
4
9
2
Small
5
2
6
23
Grand Total
98
23
110
43
21
13
9
19
9
5
5
62
227
103
72
52
89
50
19
20
316
Pivot table in terms of % of total
Five
Four
One
Three
Two
Grand Total
5.70%
Growth
24.05%
5.06%
23.42%
2.85%
Large
9.81%
1.58%
11.71%
2.22%
Mid-Cap
8.86%
1.27%
6.33%
0.63%
Small
5.38%
2.22%
5.38%
1.58%
Value
6.96%
2.22%
11.39%
0.63%
Large
4.11%
1.58%
6.65%
0.32%
Mid-Cap
1.27%
0.00%
2.85%
0.63%
Small
1.58%
0.63%
1.90%
7.28%
Grand Total
31.01%
7.28%
34.81%
13.61%
6.65%
4.11%
2.85%
6.01%
2.85%
1.58%
1.58%
19.62%
71.84%
32.59%
22.78%
16.46%
28.16%
15.82%
6.01%
6.33%
100.00%

	
Self-Test Solutions and Answers to Selected Even-Numbered Problems	
799
Dessert 
Ordered
Beef Entrée
Yes
No
Total
Yes
52%
48%
100%
No
25
75
100
Total
31
69
100
Dessert 
Ordered
Beef Entrée
Yes
No
Total
Yes
   38%
   16%
   23%
No
  62
  84
  77
Total
100
100
100
Dessert 
Ordered
Beef Entrée
Yes
No
Total
Yes
11.75%
10.79%
22.54%
No
19.52
57.94
   77.46
Total
31.27
68.73
100
(b) If the owner is interested in finding out the percentage of males and 
females who order dessert or the percentage of those who order a beef 
entrée and a dessert among all patrons, the table of total percentages is 
most informative. If the owner is interested in the effect of gender on 
ordering of dessert or the effect of ordering a beef entrée on the order-
ing of dessert, the table of column percentages will be most informative. 
Because dessert is usually ordered after the main entrée, and the owner 
has no direct control over the gender of patrons, the table of row per-
centages is not very useful here. (c) 17% of the men ordered desserts, 
compared to 29% of the women; women are almost twice as likely to 
order dessert as women. Almost 38% of the patrons ordering a beef entrée 
ordered dessert, compared to 16% of patrons ordering all other entrées. 
Patrons ordering beef are more than 2.3 times as likely to order dessert as 
patrons ordering any other entrée.
2.86 (a) Most of the complaints were against the airlines. (c) Most of 
the complaints against U.S. airlines were about flight problems, fol-
lowed by reservations/ticketing/boarding, customer service, and bag-
gage. (d) Most of the complaints against foreign airlines were about 
baggage, then reservations/ticketing/boarding, flight problems, and 
customer service.
2.88 (c) The alcohol percentage is concentrated between 4% and 6%, 
with more between 4% and 5%. The calories are concentrated between 
140 and 160. The carbohydrates are concentrated between 12 and 15. 
There are outliers in the percentage of alcohol in both tails. The outlier 
in the lower tail is due to the non-alcoholic beer O’Doul’s, with only a 
0.4% alcohol content. There are a few beers with alcohol content as high 
as around 11.5%. There are a few beers with calorie content as high as 
around 327.5 and carbohydrates as high as 31.5. There is a strong posi-
tive relationship between percentage of alcohol and calories and between 
calories and carbohydrates, and there is a moderately positive relationship 
between percentage alcohol and carbohydrates.
2.90 (c) There appears to be a positive relationship between the yield of 
the one-year CD and the five-year CD.
high-risk growth funds, most are rated as one-star, followed by two-star, 
equal portions of three-star and four-star, with no five-star. Among the 
average-risk growth funds, most are rated as three-star, followed by two-
star, four-star, one-star, and five-star.
For the value funds as a group, most are rated as three-star, followed by 
four-star, two-star, one-star and five-star. Among the average-risk value funds, 
most are three-star, followed by two-star, five-star, and one-star with no four-
star. Among the high-risk value funds, most are one-star, followed by two-star 
with no three-star, four-star, or five-star. Among the low-risk value funds, 
most are three-star, followed by four-star, two-star, five-star, and one-star.
Patterns of risk conditioned on star rating:
Most of the growth funds are rated as low-risk, followed by average-
risk and then high-risk. The pattern is the same among the three-star, 
four-star, and five-star growth funds. Among the one-star growth funds, 
most are average-risk, followed by equal portions of high-risk and low-
risk. Among the two-star growth funds, most are average-risk, followed 
by low-risk and high-risk.
Most of the value funds are rated as low-risk, followed by average-
risk and then high-risk. The pattern is the same among the two-star, three-
star, and five-star value funds. Among the one-star value funds, most are 
­average-risk, followed by equal portions of high-risk and low-risk. Among 
the four-star value funds, all are low-risk with no average-risk or high-risk.
2.72 Determining the class interval width
Interval width = highest value - lowest value
number of classes
2.80 (c) The publisher gets the largest portion (64.8%) of the revenue. 
About half (32.3%) of the revenue received by the publisher covers manu-
facturing costs. The publisher’s marketing and promotion account for the 
next largest share of the revenue, at 15.4%. Author, bookstore employee 
salaries and benefits, and publisher administrative costs and taxes each 
account for around 10% of the revenue, whereas the publisher after-tax 
profit, bookstore operations, bookstore pretax profit, and freight consti-
tute the “trivial few” allocations of the revenue. Yes, the bookstore gets 
twice the revenue of the authors.
2.82 (b) The pie chart may be best since with only three categories, it 
enables you to see the portion of the whole in each category. (d) The pie 
chart may be best since, with only four categories it enables you to see 
the portion of the whole in each category. (e) The online content is not 
copy-edited or fact-checked as carefully as print content. Only 41% of the 
online content is copy-edited as carefully as print content, and only 57% 
of the online content is fact-checked as carefully as the print content.
2.84 (a) 
Dessert 
Ordered
   Gender
Male
Female
Total
Yes
34%
66%
100%
No
52
48
100
Total
48
52
100
Dessert 
Ordered
   Gender
Male
Female
Total
Yes
    17%
    29%
    23%
No
  83
  71
  77
Total
100
100
100
Dessert 
Ordered
   Gender
Male
Female
Total
Yes
      8%
15%
   23%
No
40
37
  77
Total
48
52
100

800	
Self-Test Solutions and Answers to Selected Even-Numbered Problems
falling between 24 and 32 grams. Spareribs and fried liver are both very 
different from other foods sampled—the former on calories and the latter 
on cholesterol content.
2.96 (b) There is a downward trend in the amount filled. (c) The amount 
filled in the next bottle will most likely be below 1.894 liters. (d) The 
scatter plot of the amount of soft drink filled against time reveals the 
trend of the data, whereas a histogram only provides information on the 
distribution of the data.
CHAPTER 3
3.2 (a) Mean = 7, median = 6, mode = None (b) Range = 10,  
S2 = 12.66667, S = 3.559026, CV = (3.559026>7)*100% = 50.84323% 
(c) Z scores: 0.280976, -1.1239, 0.842927, -0.28098, -0.84293, 
1.685855, -0.56195. None of the Z scores are larger than 3.0 or smaller 
than -3.0. There is no outlier. (d) Right skewed because mean 7 median.
3.4 (a) Mean = 2, median = 2.5, mode = None (b) Range = 2.5,  
S2 = 30, S = 5.477225575, CV = (5.477225575>2)*100% = 273.86%  
(c) Z scores: -1.278, 1.0954, 0.3651, -0.183 (d) Left-skewed because 
mean 6 median.
3.6 RG = 311 + 0.22211 - 0.2824
1
2 - 1 = -6.28%
3.8 (a) 
Measure
Type 1 
Type 2 
Mean
12.17
12.83
Median
12
13
Standard Deviation
1.169
1.169
Range
3
3
(b) Both sets have the same standard deviation but the Type 1 average is 
closer to the expected value of 12 mm. Using this parameter the Type 1 
brake pads seem to meet the set expectations. But if consistency is the 
measure, then both types have equal values for the standard deviation and 
there would be no difference in the brake pads. Also, the range for type 
1 and type 2 show no difference in value. (c) If the last value of Type 2 
is changed to 24 mm, the mean for Type 2 becomes 14.5 mm, which is 
larger than the diameter of Type 1 (12.17 mm). The standard deviation 
for Type 2 gets affected (4.764) in comparison with Type 1 (1.169) and is 
therefore not acceptable when compared with Type 1.
3.10 (a)
Mean
6.647
Median
6.42
(b)
Variance
6.734593
Standard Deviation
2.595109
Range
10.2
Coefficient of Variation
39.04181
(c) The mean is only slightly smaller than the median which indicates that 
the data is skewed left. (d) The mean of the data and median are $6.73 and 
$6.88 respectively. The average data dispersion around the mean is $2.595. 
The data shows a range of $10.2.
2.92 (a) 
Frequency (Boston)
Weight (Boston)
Frequency
Percentage
3,015 but less than 3,050
2
0.54%
3,050 but less than 3,085
44
11.96
3,085 but less than 3,120
122
33.15
3,120 but less than 3,155
131
35.60
3,155 but less than 3,190
58
15.76
3,190 but less than 3,225
7
1.90
3,225 but less than 3,260
3
0.82
3,260 but less than 3,295
1
0.27
(b) 
Frequency (Vermont)
Weight (Vermont)
Frequency
Percentage
3,550 but less than 3,600
4
1.21%
3,600 but less than 3,650
31
9.39
3,650 but less than 3,700
115
34.85
3,700 but less than 3,750
131
39.70
3,750 but less than 3,800
36
10.91
3,800 but less than 3,850
12
3.64
3,850 but less than 3,900
1
0.30
(d) 0.54% of the Boston shingles pallets are underweight and 0.27% are 
overweight. 1.21% of the Vermont shingles pallets are underweight and 
3.94% are overweight.
2.94 (c)
 
Calories
 
Frequency
 
Percentage
 
Limit
Percentage 
Less Than
50 but less than 100
3
12%
100
12%
100 but less than 150
3
12
150
  24
150 but less than 200
9
36
200
  60
200 but less than 250
6
24
250
  84
250 but less than 300
3
12
300
  96
300 but less than 350
0
  0
350
  96
350 but less than 400
1
  4
400
100
 
Cholesterol
 
Frequency
 
Percentage
 
Limit
Percentage 
Less Than
0 but less than 50
  2
8%
  50
     8%
50 but less than 100
17
68
100
  76
100 but less than 150
  4
16
150
  92
150 but less than 200
  1
  4
200
  96
200 but less than 250
  0
  0
250
  96
250 but less than 300
  0
  0
300
  96
300 but less than 350
  0
  0
350
  96
350 but less than 400
  0
  0
400
  96
400 but less than 450
  0
  0
450
  96
450 but less than 500
  1
  4
500
100
The sampled fresh red meats, poultry, and fish vary from 98 to 397 
­calories per serving, with the highest concentration between 150 and  
200 calories. One protein source, spareribs, with 397 calories, is more 
than 100 calories above the next-highest-caloric food. The protein content 
of the sampled foods varies from 16 to 33 grams, with 68% of the values 

	
Self-Test Solutions and Answers to Selected Even-Numbered Problems	
801
3.14 (a)
Mean
33.65
Median
32.54
Mode
NA
(b)
Variance
160.375
Standard Deviation
12.66393
Range
48.09
Coefficient of Variation
37.63425
X
Z-Score
42.56
0.703573
32.09
-0.12318
  5.37
-2.23311
19.51
-1.11656
32.54
-0.08765
42.69
0.713838
51.63
1.41978
30.14
-0.27717
39.47
0.459573
30.52
-0.24716
38.16
0.35613
19.35
-1.12919
27.14
-0.51406
53.46
1.564285
40.12
0.5109
(c) Right-skewed as mean 7 median (d) The mean cost is $33.65 and the 
median cost is $32.54. The difference between the two extreme values is 
$48.
3.16 (a)
Mean
156
Median
157.5
Mode
156
(b)
Variance
256.8571
Standard Deviation
  16.02676
(c) Mean is smaller than the median which indicates that the data is 
skewed left and the majority of the people attending the function spent 
lesser money than the median.
(d) (a), (b)
Mean
168.5
Median
157.5
Mode
156
Variance
1849.714
Standard Deviation
    43.0083
The new data summary shows that the data became right skewed 
due to the change in data, also the amount of data dispersion 
increased.
3.12 (a)
Mean
36
Median
35
Mode
35
(b)
Variance
9.5
Standard Deviation
3.082207
Range
12
Coefficient of Variation
8.561686
Score
Z-Score
35
-0.32444
30
-1.94666
37
0.324443
35
-0.32444
34
-0.64889
35
-0.32444
35
-0.32444
42
1.946657
40
1.297771
37
0.324443
42
1.946657
38
0.648886
35
-0.32444
34
-0.64889
35
-0.32444
34
-0.64889
34
-0.64889
(c) Mean is larger than both median and mode, hence the data is skewed 
right. 
(d) 
Measure
Data
Mean
39
Median
38
Mode
38
Variance
9.5
Standard Deviation
3.082207
Range
12
Coefficient of Variation
8.561686
Score
Z-Score
38
0.648886
33
-0.97333
40
1.297771
38
0.648886
37
0.324443
38
0.648886
38
0.648886
45
2.919986
43
2.2711
40
1.297771
45
2.919986
41
1.622214
38
0.648886
37
0.324443
38
0.648886
37
0.324443
37
0.324443

802	
Self-Test Solutions and Answers to Selected Even-Numbered Problems
and five-star, mid-cap and four-star, mid-cap and one-star, small-cap and 
five-star, small-cap and four-star, and small-cap and two-star.
3.26 (a)
Five
Type
Four
One
Three
Two
Grand Total
16.5544
Growth
15.2193 10.3575
13.9957
16.5333
Average
16.2233 11.6467
13.0514
High
14.6100
9.3620
14.5900
16.5587
Low
14.9785
9.8060
14.5700
17.2820
Value
12.7295 13.4957
15.3603
28.2700
Average
13.9800
16.4786
High
12.0500
14.5350
Low
12.7295 14.2150
15.0903
16.7126
14.6604 11.3126
14.4423
13.6058
10.8005
33.7200
13.6822
15.4863
17.5267
22.1400
13.9117
14.1821
14.2780
13.0524
17.7170
14.6717
14.6982
17.1012
15.4133
14.0751
14.3963
Average of 1YrReturn% Star Rating
Grand Total
(b)
Five
Type
Four
One
Three
Two
Grand Total
4.0813
Growth
3.6946
5.0187
3.8308
3.0735
Average
4.9524
7.6948
4.9654
High
#DIV/0!
2.6945
#DIV/0!
4.3448
Low
3.3483
3.0114
2.8818
6.9822
Value
4.5679
4.3343
4.1815
#DIV/0!
Average
4.0506
2.9673
High
8.5843
3.8335
Low
4.5679
0.5445
4.4251
4.6722
4.0202
4.9474
3.9820
7.6709
6.9272
0.2946
2.1220
3.6530
3.8277
#DIV/0!
2.4852
6.7243
5.0041
6.0163
11.3821
3.3562
4.4651
4.4488
8.4131
4.1475
4.8551
Grand Total
StdDev of 1YrReturn% Star Rating
(c) In general, the mean one-year return of the five-star rated growth 
funds is highest, followed by that of the four-star, three-star, two-star, 
and one-star rated growth funds across the various risk levels. However, a 
similar pattern does not hold through among the value funds.
There is no obvious pattern in the standard deviation of the one-year 
return.
3.28 (a) 6, 11, 5 (b) 3, 6, 8, 11, 15 (c) The data is skewed to the right as 
the distance between the median and highest value is slightly greater than 
the distance between the lowest value and the median, and the right tail is 
slightly longer than the left tail.
3.30 (a) -6.5, 8, 14.5. (b) -8, -6.5, 7, 8, 9. (c) The shape is left-skewed. 
(d) This is consistent with the answer in Problem 3.4 (d).
3.32 (a), (b)
Five-Number Summary
Minimum
5.37
First Quartile
30.12
Median
38.16
Third Quartile
49.35
Maximum
53.45
Interquartile Range
19.23
The penetration value is left-skewed.
3.34 (a) 34.5, 37.5, 3 (b) 30, 34.5, 35, 37.5, 42 (c) The data is skewed to 
the right as the distance between the median and highest value is slightly 
greater than the distance between the lowest value and the median, and 
the right tail is much longer than the left tail.
3.18 (a) Mean = 7.11, median = 6.68. (b) Variance = 4.336,
standard deviation = 2.082, range = 6.67, CV = 29.27%. (c) Because 
the mean is greater than the median, the distribution is right-skewed.  
(d) The mean and median are both greater than five minutes. The distribu-
tion is right-skewed, meaning that there are some unusually high values. 
Further, 13 of the 15 bank customers sampled (or 86.7%) had waiting 
times greater than five minutes. So the customer is likely to experience a 
waiting time in excess of five minutes. The manager overstated the bank’s 
service record in responding that the customer would “almost certainly” 
not wait longer than five minutes for service.
3.20 (a) 311 + 0.02402 * 11 + 0.7460241>2 - 1 = 1.3371 - 1 = 33.71% 
per year. (b) = 1+1,0002 * 11 + 0.33712 * 11 + 0.33712 =
+1,787.84. (c) The result for Taser was better than the result for GE, 
which was worth $1,188.41.
3.22 (a) Platinum = 12.90%, gold = 15.41%, silver = 27.58% per year. 
(b) Silver had a much higher return than gold or platinum. (c) Silver had 
a much higher return than the DJIA, the S&P 500, and the NASDAQ; 
gold’s return was worse than the NASDAQ but better than the S&P 500 
and DJIA; platinum’s return was better than S&P 500 and DJIA but 
worse than NASDAQ.
3.24 (a)
Five
Type
Four
One
Three
Two
Grand Total
16.5544
Growth
15.2193 10.3575
13.9957
18.0756
Large
15.4971 12.3320
14.8743
15.5200
Mid-Cap
15.0400 10.0875
13.4140
13.3300
Small
15.0082
9.1014
12.7676
17.2820
Value
12.7295 13.4957
15.3603
16.4150
Large
11.7515 12.1120
14.5648
16.4400
Mid-Cap
16.1625
16.7267
18.5700
Small
12.5260 16.9550
16.0950
16.7126
14.6604 11.3126
14.4423
13.6058
17.1257
8.7046
12.4722
15.4863
14.1633
17.4680
15.8860
14.1821
14.2780
15.6771
13.2160
12.9771
14.6982
13.5898
16.7879
15.4840
14.3963
Average of 1YrReturn% Star Rating
Grand Total
(b)
Five
Type
Four
One
Three
Two
Grand Total
4.0813
Growth
3.6946
5.0187
3.8308
4.3119
Large
4.1374
4.6690
2.7064
3.3099
Mid-Cap
3.1017
8.7458
4.8023
4.4265
Small
3.9244
2.2479
4.3906
6.9822
Value
4.5679
4.3343
4.1815
1.1384
Large
3.6990
4.3732
4.5739
#DIV/0!
Mid-Cap
4.1910
3.1676
13.7179
Small
6.3546
1.6476
3.9994
4.6722
4.0202
4.9474
3.9820
7.6709
7.4925
7.6199
2.9127
3.6530
2.5803
4.5127
4.1620
6.7243
5.0041
4.7615
5.4705
4.0854
4.4651
4.0592
3.4837
5.4861
4.8551
StdDev of 1YrReturn% Star Rating
Grand Total
(c) The mean one-year return of small-cap value funds is higher than that 
of the small-cap growth funds across the different star ratings with the 
exception of those rated as four-star. On the other hand, the mean one-
year return of large-cap value funds is lower than that of the growth funds 
across the different star ratings, but the mid-cap value funds are higher 
across the different star ratings.
The standard deviation of the one-year return of growth funds is 
generally higher than that of the value funds across all the star ratings and 
market caps with the exception of the large-cap and three-star, mid-cap 

	
Self-Test Solutions and Answers to Selected Even-Numbered Problems	
803
(b) 8.312, 8.404, 8.42, 8.459, 8.498. (c) Even though the mean = median, 
the left tail is slightly longer, so the distribution is slightly left-skewed. 
(d) All the troughs in this sample meet the specifications.
3.68 (a), (b)
Bundle Score
Typical Cost ($)
Mean
54.775
24.175
Standard Error
4.3673
2.8662
Median
62
20
Mode
75
8
Standard Deviation
27.6215
328.6096
Sample Variance
762.9481
18.1276
Kurtosis
-0.8454
2.7664
Skewness
-0.4804
1.5412
Range
98
83
Minimum
2
5
Maximum
100
88
Sum
2,191
967
Count
40
40
First Quartile
34
9
Third Quartile
75
31
Interquartile Range
41
22
CV
50.43%
74.98%
(c) The typical cost is right-skewed, while the bundle score is left-skewed. 
(d) r = 0.3465. (e) The mean typical cost is $24.18, with an average 
spread around the mean equaling $18.13. The spread between the low-
est and highest costs is $83. The middle 50% of the typical cost fall over 
a range of $22 from $9 to $31, while half of the typical cost is below 
$20. The mean bundle score is 54.775, with an average spread around 
the mean equaling 27.6215. The spread between the lowest and highest 
scores is 98. The middle 50% of the scores fall over a range of 41 from 
34 to 75, while half of the scores are below 62. The typical cost is right-
skewed, while the bundle score is left-skewed. There is a weak positive 
linear relationship between typical cost and bundle score.
3.70 (a) Boston: 0.04, 0.17, 0.23, 0.32, 0.98; Vermont: 0.02, 0.13, 0.20, 
0.28, 0.83. (b) Both distributions are right-skewed. (c) Both sets of shin-
gles did quite well in achieving a granule loss of 0.8 gram or less. Only 
two Boston shingles had a granule loss greater than 0.8 gram. The next 
highest to these was 0.6 gram. These two values can be considered outli-
ers. Only 1.176% of the shingles failed the specification. Only one of the 
Vermont shingles had a granule loss greater than 0.8 gram. The next high-
est was 0.58 gram. Thus, only 0.714% of the shingles failed to meet the 
specification.
3.72 (a) The correlation between calories and protein is 0.4644. (b) The 
correlation between calories and cholesterol is 0.1777. (c) The correlation 
between protein and cholesterol is 0.1417. (d) There is a weak positive 
linear relationship between calories and protein, with a correlation coef-
ficient of 0.46. The positive linear relationships between calories and  
cholesterol and between protein and cholesterol are very weak.
3.74 (a), (b)
Property Taxes per Capita ($)
Mean
1,332.2353
Median
1,230
Standard deviation
577.8308
Sample variance
333,888.4235
Range
2,479
First quartile
867
Third quartile
1,633
Interquartile range
766
Coefficient of variation
43.37%
3.36 (a) Commercial district five-number summary: 0.38 3.2 4.5 5.55 
6.46. Residential area five-number summary: 3.82 5.64 6.68 8.73 10.49. 
(b) Commercial district: The distribution is left-skewed. Residential area: 
The distribution is slightly right-skewed. (c) The central tendency of the 
waiting times for the bank branch located in the commercial district of a 
city is lower than that of the branch located in the residential area. There 
are a few long waiting times for the branch located in the residential area, 
whereas there are a few exceptionally short waiting times for the branch 
located in the commercial area.
3.38 (a) Population mean, m = 6. (b) Population standard deviation, 
s = 1.673, population variance, s2 = 2.8.
3.40 (a) 68%. (b) 95%. (c) Not calculable, 75%, 88.89%. (d) m - 4s to 
m + 4s or -2.8 to 19.2.
3.42 (a) 
Mean = 662,960
51
= 12,999.22, variance = 762,944,726.6
51
= 14,959,700.52, 
standard deviation = 214,959,700.52 = 3,867.78. (b) 64.71%, 
98.04%, and 100% of these states have mean per capita energy  
consumption within 1, 2, and 3 standard deviations of the mean,  
respectively. (c) This is consistent with 68%, 95%, and 99.7%,  
according to the empirical rule. (d) (a) Mean = 642,887
50
= 12,857.74, 
variance = 711,905,533.6
50
= 14,238,110.67, standard deviation 
= 214,238,110.67 = 3,773.34. (b) 66%, 98%, and 100% of these states 
have a mean per capita energy consumption within 1, 2, and 3 standard 
deviations of the mean, respectively. (c) This is consistent with 68%, 
95%, and 99.7%, according to the empirical rule.
3.44 (a) Covariance = 64.11818 
(b) Sx
- = 4.665151, Sy = 13.75103, r =
Cov1X, Y2
Sx * Sy
= 0.999494
(c) There is a strong positive linear relationship between the two data sets.
3.46 (a) cov1X, Y2 =
a
n
i = 1
 1Xi - X21Yi -  Y2
n-1
= 800
6
= 133.3333.
(b) r =
cov1X, Y2
SXSY
=
133.3333
146.9042213.38772 = 0.8391.
(c) The correlation coefficient is more valuable for expressing the rela-
tionship between calories and sugar because it does not depend on the 
units used to measure calories and sugar. (d) There is a strong positive 
linear relationship between calories and sugar.
3.48 (a) cov1X, Y2 = 1.4115 * 1013 (b) r = 0.7752 (c) There is a posi-
tive linear relationship between the coaches’ total pay and revenue.
3.64 (a) Mean = 43.89, median = 45, 1st quartile = 18, 
3rd quartile = 63. (b) Range = 76, interquartile range = 45, variance = 
639.2564, standard deviation = 25.28, CV = 57.61%. (c) The distri-
bution is right-skewed because there are a few policies that require an 
exceptionally long period to be approved. (d) The mean approval process 
takes 43.89 days, with 50% of the policies being approved in less than 
45 days. 50% of the applications are approved between 18 and 63 days. 
About 67% of the applications are approved between 18.6 and 69.2 days.
3.66 (a) Mean = 8.421, median = 8.42, range = 0.186, S = 0.0461. 
The mean and median width are both 8.42 inches. The range of the widths 
is 0.186 inch, and the average scatter around the mean is 0.0461 inch.  

804	
Self-Test Solutions and Answers to Selected Even-Numbered Problems
(c) The data are symmetrical.
(d) The mean of the average credit scores is 746.2238. Half of the average 
credit scores are less than 749. One-quarter of the average credit scores 
are less than 730 while another one-quarter is more than 763. The overall 
spread of average credit scores is 89. The middle 50% of the average 
credit scores spread over 33. The average spread of average credit scores 
around the mean is 21.7807.
CHAPTER 4
4.2 (a) Selecting a red pen or a green pen. (b) A red pen.
4.4 (a) 0.56 (b) 0.20 (c) 0.24 (d) 0.74
4.6 (a) Mutually exclusive, not collectively exhaustive. (b) Not mutually 
exclusive, not collectively exhaustive. (c) Mutually exclusive, not collec-
tively exhaustive. (d) Mutually exclusive, collectively exhaustive.
4.8 (a) Is a male. 
(b) Is a male and feels tense or stressed out at work.  
(c) Does not feel tense or stressed out at work.  
(d) Is a male and feels tense or stressed out at work is a joint event 
because it consists of two characteristics.
4.10 (a) People who are aware of the oil spill. (b) People who are not 
aware of the spill and who are from Europe. (c) The set of respondents 
from the USA. (d) It describes an event with two different characteristics 
(unaware of the oil spill and hailing from Europe)
4.12 (a) 8,007>14,074 = 0.5689. (b) 6,264>14,074 = 0.4451.  
(c) 8,007>14,074 + 6,264>14,074 - 3,633>14,074 = 0.7559  
(d) The probability of saying that analyzing data is critical or is a 
­manager includes the probability of saying that analyzing data is critical 
plus the probability of being a manager minus the joint probability of 
­saying that analyzing data is critical and is a manager.
4.14 (a) 514>1,085. (b) 276>1,085. (c) 781>1,085.  
(d) 1,085>1,085 = 1.00.
4.16 (a) 10>30 = 1>3 = 0.33. (b) 20>60 = 1>3 = 0.33.  
(c) 40>60 = 2>3 = 0.67. (d) Because P1A0 B2 = P1A2 = 1>3, events A 
and B are independent.
4.18 1
2 = 0.5.
4.20 Because P1A and B2 = 0.20 and P1A2P1B2 = 0.12, events A and B 
are not independent.
4.22 (a) P1Aware>USA2 = 67
100 (b) P1Aware>Europe2 = 52
100
(c) No, they are not independent
4.24 (a) 4,374>7,810 = 0.5601. (b) 3,436>7,810 = 0.4399.  
(c) 3,633>6,264 = 0.5800. (d) 2,631>6,264 = 0.4200.
4.26 (a) 0.025>0.6 = 0.0417. (b) 0.015>0.4 = 0.0375. (c) Because 
P1Needs warranty repair   Manufacturer based in U.S.2 = 0.0417 and 
P1Needs warranty repair2 = 0.04, the two events are not independent.
4.28 (a) 0.0045. (b) 0.012. (c) 0.0059. (d) 0.0483.
4.30 0.3277.
4.32 (a) 0.736. (b) 0.997.
(c), (d) The distribution of the property taxes per capita is right-skewed, 
with a mean value of $1,332.24, a median of $1,230, and an average 
spread around the mean of $577.83. There is an outlier in the right tail at 
$2,985, while the standard deviation is about 43.37% of the mean. 25% 
of the states have property tax that falls below $867 per capita, and 25% 
have property taxes that are higher than $1,633 per capita.
3.76 (a), (b)
Mean
Standard Error
Mode
Median
Sample Variance
Standard Deviation
Kurtosis
Skewness
Range
Minimum
Maximum
Count
Sum
First Quartile
Third Quartile
Interquartile Range
CV
1.1807
29
5
305
34
22
9
20
11
54.99%
Abandonment rate in % (7:00AM–3:00PM)
7.6238688759
9
13.8636
1.625414306
10
0.7235687396
58.1233
(c) The data are right-skewed.
(d) r = 0.7575
(e) The mean abandonment rate is 13.86%. Half of the abandonment rates 
are less than 10%. One-quarter of the abandonment rates are less than 9% 
while another one-quarter are more than 20%. The overall spread of the 
abandonment rates is 29%. The middle 50% of the abandonment rates are 
spread over 11%. The average spread of abandonment rates around the 
mean is 7.62%. The abandonment rates are right-skewed.
3.78 (a), (b) 
Mean
Standard Error
Mode
Median
Sample Variance
Standard Deviation
Kurtosis
Skewness
Range
Minimum
Maximum
Count
Sum
First Quartile
Third Quartile
Interquartile Range
CV
–0.22982
89
700
106710
789
143
730
763
33
2.92%
Average Credit Score
21.78073
760
746.2238
1.821396
749
–0.83035
474.4003

	
Self-Test Solutions and Answers to Selected Even-Numbered Problems	
805
5.6 (b) s = 1.55.
5.8 (a) E1X2 = 92.5, E1Y2 = 30 (b) sx = 96.53, sy = 115 (c) -150 
(d) 122.5
5.10 (a) 9.5 minutes. (b) 1.9209 minutes.
5.12 
X * P1X2 Y * P1Y2
1X −MX22 
P1X2
1Y −MY22 
P1Y2
1X −MX2 
1Y −MY2
: P1x, y2
-10
5
2,528.1
129.6
-572.4
0
45
1,044.3
5,548.8
-2,407.2
24
-6
132.3
346.8
-214.2
45
-30
2,484.3
3,898.8
-3,112.2
(a) E1X2 = mX = a
N
i - 1
 XiP1Xi2 = 59, E1Y2 = mY = a
N
i - 1
 YiP1Yi2 = 14.
(b) sX = A a
N
i - 1
 3Xi - E1X242P1Xi2 = 78.6702.
	
sY = B a
N
i = 1
 3Yi - E1Y242P1Yi2 = 99.62.
(c) sXY = a
N
i = 1
 3Xi - E1X243Yi - E1Y24P1xi , yi2 = -6,306.
(d) Stock X gives the investor a lower standard deviation while yielding a 
higher expected return, so the investor should select stock X.
5.14 (a) E1X2 = 77.5, E1Y2 = 3.05 (b) sx = 89.77  
(c) sxy = -6156.38 (d) Stock X gives a lower standard deviation while 
yielding a higher expected return thus, stock X should be selected.
5.16 (a) E1X2 = +66.20; E1Y2 = +63.01. (b) sX = +57.22; sY =  
+195.22. (c) sXY = +10,766.44. (d) Based on the expected value criteria, 
you would choose the common stock fund. However, the common stock 
fund also has a standard deviation more than three times higher than 
that for the corporate bond fund. An investor should carefully weigh the 
increased risk. (e) If you chose the common stock fund, you would need 
to assess your reaction to the small possibility that you could lose virtu-
ally all of your entire investment.
5.18 (a) 0.55840594. (b) 0.0276964. (c) 0.20880069. (d) 0.0838603.
5.20 (a) 0.5, 0.67082. (b) 2.5, 1.11803. (c) 3, 0.86603. (d) 1.35, 0.86168.
5.22 (a) 0.0425. (b) 0.0004. (c) 0.0492. (d) m = 1.62, s = 1.0875.  
(e) Each under 25 year old either owns a tablet or does not own a tablet 
and each person surveyed is independent of every other person.
5.24 (a) 0.395291799. (b) 0.378470871. (c) 0.94286668. (d) 0.05713332.
5.26 (a) 0.5718. (b) 0.0049. (c) 0.9231. (d) m = 2.49, s = 0.6506.
5.28 (a) 0.251045. (b) 0.130377. (c) 0.268128. (d) 0.022371.
5.30 (a) 0.014873. (b) 0.002479. (c) 0.982649. (d) 0.017351.
4.34 (a) P1B′O2 =
10.5210.32
10.5210.32 + 10.25210.72 = 0.4615. 
(b) P1O2 = 0.175 + 0.15 = 0.325.
4.36 (a) P1Huge success   Favorable review2 = 0.099>0.459 = 0.2157;
P1Moderate success   Favorable review2 = 0.14>0.459 = 0.3050;
P1Break even   Favorable review2 = 0.16>0.459 = 0.3486;
P1Loser   Favorable review2 = 0.06>0.459 = 0.1307. 
(b) P1Favorable review2 = 0.459.
4.38 310 = 59,049.
4.40 (a) 27 = 128. (b) 67 = 279,936. (c) There are two mutually exclu-
sive and collectively exhaustive outcomes in (a) and six in (b).
4.42 (8)(4)(3)(3) = 288.
4.44 5! = (5)(4)(3)(2)(1) = 120. Not all the orders are equally likely 
because the teams have a different probability of finishing first through 
fifth.
4.46 6! = 720.
4.48 10!
4!6! = 210.
4.50 4,950.
4.52 1,048,576 possible answers
4.60 30,240
4.62 (a) 84/200. (b) 126/200. (c) 141/200. (d) 33/200. (f) 16/100.
4.64 (a) 202>447 = 0.4519. (b) 95>237 = 0.4008. (c) 107>210 =
0.5095. (d) 217>447 = 0.4855. (e) 122>237 = 0.5148. (f) 95>210
= 0.4524. (g) IT executives were more likely to identify big data as criti-
cal while marketing executives were more likely to identify functional 
silos as an issue.
CHAPTER 5
5.2 (a) 
m = 010.102 + 110.202 + 210.452 + 310.152 + 410.052 + 510.052 = 2.0.
(b) s = B
10-22210.102 + 11-22210.202 + 12-22210.452 +
13-22210.152 + 14-22210.052 + 15-22210.052 = 1.183.
5.4 (a) 
(b) 
(c) 
(d) -+0.167 for each method of play.
X
P1X2
+ - 1
21>36
+ + 1
15>36
X
P1X2
+ - 1
21>36
+ + 1
15>36
X
P1X2
+ - 1
30>36
+ + 1
6>36

806	
Self-Test Solutions and Answers to Selected Even-Numbered Problems
(d) If n = 6, E = 10, and N = 100,
    P1X Ú 22 = 1 - 3P1X = 02 + P1X = 124
 
= 1 - ≥
a10
0 b a100-10
6-0
b
a100
6 b
+
a10
1 b a100-10
6-1
b
a100
6 b
¥
 
= 1 - 30.5223 + 0.36874 = 0.1090.
(e) The probability that the entire group will be audited is very sensitive 
to the true number of improper returns in the population. If the true num-
ber is very low 1E = 52, the probability is very low (0.0279). When the 
true number is increased by a factor of 6 1E = 302, the probability the 
group will be audited increases by a factor of more than 20 (0.5854).
5.46 (a) P1X = 42 = 1.09421 * 10-5. (b) P1X = 02 = 0.64454.  
(c) P1X Ú 12 = 0.3555. (d) (a) P1X = 42 = 0.00016413.  
(b) P1X = 42 = 0.00016413. (c) P1X Ú 12 = 0.49254842.
5.48 (a) P1X = 12 = 0.2424. (b) P1X Ú 12 = 0.9697.
(c) P1X = 32 = 0.2424. (d) Because there were now 12 funds to con-
sider, the probability that 3 would be growth funds decreased from 0.3810 
to 0.2424.
5.54 (a) 0.65. (b) 0.65. (c) 0.3124. (d) 0.0053. (e) The assumption of 
independence may not be true.
5.56 (a) If p = 0.50 and n = 13, P1X Ú 102 = 0.0461.  
(b) If p = 0.75 and n = 13, P1X Ú 102 = 0.5843.
5.58 (a) 0.0060. (b) 0.2007. (c) 0.1662. (d) Mean = 4.0, standard 
deviation = 1.5492. (e) Since the percentage of bills containing an error 
is lower in this problem, the probability is higher in (a) and (b) of this 
problem and lower in (c).
5.60 (a) m = np = 8.2 (b) s = 1.55.  
(c) P1X = 102 = 0.1268. (d) P1X … 52 = 0.1079.  
(e) P1X Ú 52 = 0.9577.
5.62 (a) If p = 0.50 and n = 40, P1X Ú 352 = 0.000000691. (b) If 
p = 0.70 and n = 40, P1X Ú 352 = 0.008618. (c) If p = 0.90 and 
n = 40, P1X Ú 352 = 0.793727. (d) Based on the results in (a)–(c), the 
probability that the Standard & Poor’s 500 Index will increase if there is 
an early gain in the first five trading days of the year is very likely to be 
close to 0.90 because that yields a probability of 79.37% that at least 35 
of the 40 years the Standard & Poor’s 500 Index will increase the entire 
year.
5.64 (a) The assumptions needed are (i) the probability that a question-
able claim is referred by an investigator is constant, (ii) the probability 
that a questionable claim is referred by an investigator approaches 0 as 
the interval gets smaller, and (iii) the probability that a questionable claim 
is referred by an investigator is independent from interval to interval.  
(b) 0.0378. (c) 0.5830. (d) 0.4170.
CHAPTER 6
6.2 (a) 0.8856. (b) 0.15028. (c) Z = 1.96 or Z = -1.96. (d) Z = 1.96.
6.4 (a) 0.35569. (b) 0.0197. (c) 0.38811. (d) Z = 1.
6.6 (a) 0.02275. (b) 0.15866. (c) 23.4. (d) 37.52.
5.32 (a) 
P1X 6 52 = P1X = 02 + P1X = 12 + P1x = 22 + P1X = 32
	
+ P1X = 42
=
e-61620
0!
+
e-61621
1!
+
e-61622
2!
+
e-61623
3!
+
e-61624
4!
= 0.002479 + 0.014873 + 0.044618 + 0.089235  
      + 0.133853
= 0.2851.
(b) P1X = 52 =
e-61625
5!
= 0.1606.
(c) P1X Ú 52 = 1 - P1X 6 52 = 1 - 0.2851 = 0.7149.
(d) P1X = 4 or X = 52 = P1X = 42 + P1X = 52 =
e-61624
4!
+
e-61625
5!
	
= 0.2945.
5.34 (a) 0.1287. (b) 0.8713. (c) 0.6074.
5.36 (a) 0.1165. (b) 0.2504. (c) 0.6331. (d) 0.3669.
5.38 (a) 0.3263. (b) 0.8964. (c) Because Ford had a higher mean rate of 
problems per car than Toyota, the probability of a randomly selected Ford 
having zero problems and the probability of no more than two problems 
are both lower than for Toyota.
5.40 (a) 0.3535 (b) 0.9122. (c) Because Toyota had a lower mean rate 
of problems per car in 2009 compared to 2010, the probability of a ran-
domly selected Toyota having zero problems and the probability of no 
more than two problems are both higher in 2009 than in 2010.
5.42 (a) 0.3968. (b) 0.3535. (c) 0.4242. (d) 0.5333.
5.44 (a) If n = 6, E = 25, and N = 100,
P1X Ú 22 = 1 - 3P1X = 02 + P1X = 124
= 1 - ≥
a25 
0 b a 100-25
6-0
b
a 100
6 b
+
a25
1 b a100-25
6-1
b
a100
6 b
¥
= 1 - 30.1689 + 0.36204 = 0.4691.
(b) If n = 6, E = 30, and N = 100,
    P1X Ú 22 = 1 - 3P1X = 02 + P1X = 124
 
= 1 - ≥
a 30
0 b a100-30
6-0
b
a100
6 b
+
a30
1 b a100-30
6-1
b
a100
6 b
¥
 
= 1 - 30.1100 + 0.30464 = 0.5854.
(c) If n = 6, E = 5, and N = 100,
    P1X Ú 22 = 1 - 3P1X = 02 + P1X = 124
 
= 1 - ≥
a5
0b a100-5
6-0 b
a100
6 b
+
a5
1b a100-5
6-1 b
a100
6 b
¥
 
= 1 - 30.7291 + 0.24304 = 0.0279.

	
Self-Test Solutions and Answers to Selected Even-Numbered Problems	
807
6.32 (a) For l = 2, P1X … 12 = 0.8647. (b) For l = 2,
P1X … 52 = 0.99996. (c) For l = 1, P1X … 12 = 0.6321, for 
l = 1, P1X … 52 = 0.9933.
6.34 (a) 0.4512. (b) 0.3012. (c) 0.1816.
6.36 (a) 0.8647. (b) 0.3297. (c) (a) 0.9765. (b) 0.5276.
6.46 (a) 0.4772. (b) 0.9544. (c) 0.0456. (d) 1.8835. (e) 1.8710 and 2.1290.
6.48 (a) 0.1405. (b) 0.0256. (c) $2,179.78. (d) $898.22 to $2,179.78.
6.50 (a) Waiting time will more closely resemble an exponential distribu-
tion. (b) Seating time will more closely resemble a normal distribution. 
(c) Both the histogram and normal probability plot suggest that waiting 
time more closely resembles an exponential distribution. (d) Both the his-
togram and normal probability plot suggest that seating time more closely 
resembles a normal distribution.
6.52 (a) 0.0426. (b) 0.0731. (c) 0.9696. (d) 1.2127. (e) 1.6891 to 6.7850. 
(f) 0.125, 0.125, 0.90.
CHAPTER 7
7.2 Always normal for sample size 7 30
7.4 (a) Both means are equal to 6. This property is called unbiasedness. 
(c) The distribution for n = 3 has less variability. The larger sample size 
has resulted in sample means being closer to m.
7.6 (a) The population matches the criteria of a uniform distribution. The 
sample distribution of mean for uniform population becomes normal for 
as small as 5 units of sample size. (b) Distribution of sample mean is nor-
mal with a large sample. (c) The average weight is 7000>100 = 70 kg 
P1X 7 702 = P1Z 7 -2.52 = 1.0 - 0.9938 = 0.0062.  
(d) The average weight is 7800>100 = 78 kg  
P1X 6 782 = P1Z 7 1.52 = 1.0 - 0.9332 = 0.0668.
7.8 (a) P1X 7 262 = P1Z 7 -1.002 = 1.0 - 0.1587 = 0.8413.  
(b) P1Z 6 1.042 = 0.85; X = 27 + 1.0411.02 = 28.04. (c) To 
be able to use the standardized normal distribution as an approxi-
mation for the area under the curve, you must assume that the 
population is approximately symmetrical. (d) P1Z 6 1.042 = 0.85; 
X = 27 + 1.0410.502 = 27.52.
7.10 (a) 0.40. (b) 0.0704.
7.12 (a) p = 0.501, sp = B
p11 - p2
n
= B
0.50111 - 0.5012
100
= 0.05
	
P1p 7 0.552 = P1Z 7 0.982 = 1.0 - 0.8365 = 0.1635.
(b) p = 0.60, sp = B
p11 - p2
n
= B
0.611 - 0.62
100
= 0.04899
	
P1p 7 0.552 = P1Z 7 -1.0212 = 1.0 - 0.1539 = 0.8461.
(c) p = 0.49, sp = B
p11 - p2
n
= B
0.4911 - 0.492
100
= 0.05
	
P1p 7 0.552 = P1Z 7 1.202 = 1.0 - 0.8849 = 0.1151.
6.8 (a) 49.379% (b) 0.15866 (c) 752,000 km (d) 0.49865, 0.10565, 
741,600 km
6.10 (a) P1Z 6 0.62 = 0.72575. (b) P1-0.47 6 Z 6 -0.072 = 0.15292. 
(c) x = 61.95. (d) Z = 85 - 72
15
= 1 and Z = 65 - 55
2
= 5
Comparing the z scores the second student scored better than the first 
with reference to the group.
6.12 (a) 1X 7 152 = 0.97257. (b) P110 6 X 7 202 = 0.1215.  
(c) P1X 6 102 = P1Z 6 -2.692 = 0.00357. (d) P1Z 6 ?2 =   
0.9900, X = 42.645 kg.
6.14 The smallest of the standard normal quartile values covers an area 
under the normal curve of 0.025. The corresponding Z value is -1.96. 
The middle (20th) value has a cumulative area pf = 0.50 and a corre-
sponding Z curve of 0.0. The largest of the standard normal quantile val-
ues covers an area under the normal curve of 0.975, and its corresponding 
Z is 1.96.
6.16 (a) Mean = 22.5294, median = 22, S = 1.8411, range = 7, 
6S = 611.84112 = 11.0466, interquartile range = 2.0, 1.3311.84112 =
2.4487. The mean is slightly more than the median. The range is much 
less than 6S, and the interquartile range is less than 1.33S. (b) The normal 
probability plot appears to be approximately normally distributed.  
The kurtosis is 0.3402, indicating very little departure from a normal 
distribution.
6.18 (a) Mean = 1,332.24, median = 1,230, range = 2,479, 
61S2 = 3,466.98, interquartile range = 766, 1.331S2 = 768.51. Because 
the mean is greater than the median, the interquartile range is slightly less 
than 1.33 times the standard deviation, and the range is much smaller than  
6 times the standard deviation, the data appear to deviate from the normal dis-
tribution. (b) The normal probability plot suggests that the data appear to be 
right-skewed. The kurtosis is 0.5395 indicating a distribution that is slightly 
more peaked than a normal distribution, with more values in the tails.
6.20 (a) Interquartile range = 0.0025, S = 0.0017, range = 0.008,
1.331S2 = 0.0023, 61S2 = 0.0102. Because the interquartile range is 
close to 1.33S and the range is also close to 6S, the data appear to be 
approximately normally distributed. (b) The normal probability plot sug-
gests that the data appear to be approximately normally distributed.
6.22 (a) Five-number summary: 82 127 148.5 168 213; mean = 147.06, 
mode = 130, range = 131, interquartile range = 41, standard deviation
= 31.69. The mean is very close to the median. The five-number sum-
mary suggests that the distribution is approximately symmetric around the 
median. The interquartile range is very close to 1.33S. The range is about 
$50 below 6S. In general, the distribution of the data appears to closely 
resemble a normal distribution. (b) The normal probability plot confirms 
that the data appear to be approximately normally distributed.
6.24 (a) P1X 6 372 = 0.20 (b) P135 6 X 6 402 = 0.50  
(c) P1X 7 382 = 0.70 (d) Mean = 40, Standard Deviation = 8.33
6.26 (a) P1X 6 372 = 0.067. (b) P138 6 X 6 652 = 0.90.  
(c) P138 6 X 6 622 = 0.80. (d) Mean = 50, Standard Deviation = 75.
6.28 (a) 0.6321. (b) 0.3679. (c) 0.2326. (d) 0.7674.
6.30 (a) 0.7769. (b) 0.2231. (c) 0.1410. (d) 0.8590.

808	
Self-Test Solutions and Answers to Selected Even-Numbered Problems
8.14 3 { 2.447 10.467> 172 = 3.432, 2.568. Given this scenario, the 
range of the confidence interval varies more because 1 can contains 
1.5gms.
8.16 (a) 75 { 12.00492192> 255; 72.57 … m … 77.43 (b) You can 
be 95% confident that the population mean amount of one-time gift is 
between $72.57 and $77.43.
8.18 (a) 6.31 … m … 7.87. (b) You can be 95% confident that the popu-
lation mean amount spent for lunch at a fast-food restaurant is between 
$6.31 and $7.87.
8.20 (a) 21.58 … m … 23.48. (b) You can be 95% confident that the 
population mean miles per gallon of 2013 small SUVs is between 21.58 
and 23.48. (c) Because the 95% confidence interval for population mean 
miles per gallon of 2013 small SUVs does not overlap with that for the 
population mean miles per gallon of 2013 family sedans, you can con-
clude that the population mean miles per gallon of 2013 small SUVs is 
lower than that of 2013 family sedans.
8.22 (a) 31.12 … m … 54.96. (b) The number of days is approximately 
normally distributed. (c) No, the outliers skew the data. (d) Because 
the sample size is fairly large, at n = 50, the use of the t distribution is 
appropriate.
8.24 (a) 28.97 … m … 43.59. (b) That the population distribution is 
normally distributed. (c) Both the normal probability plot and the boxplot 
show that the distribution of the Facebook penetration is left-skewed, so 
with the small sample size, the validity of the confidence interval is in 
question.
8.26 0.19 … p … 0.31.
8.28 (a) p = X
n = 135
500 = 0.27, p { ZB
p11 - p2
n
= 0.27 {
2.58B
0.2710.732
500
; 0.2189 … p … 0.3211. (b) The manager in charge 
of promotional programs can infer that the proportion of households that 
would upgrade to an improved cellphone if it were made available at a 
substantially reduced cost is somewhere between 0.22 and 0.32, with 
99% confidence.
8.30 X1 = Master’s degree, X2 = Bachelor’s degree, 
(a) p = X1
n = 28069
47626 = 0.59. Confidence Intervals = 0.59 { 1.96 
A
10.59211 - 0.592
47626
= 0.59 { 0.004 = 0.594, 0.586  
(b) p = X2
n = 19557
47626 = 0.41; Confidence Intervals = 0.4062, 0.4151 
(c) p = X2
n = 19557
47626 = 0.41; Confidence Intervals at 99% = 0.4048 
and 0.4164 (d) Confidence interval for master degree holders at 99% is 
0.5836 and 0.5952 (e) As the level of significance decreases, the range in 
which the proportion lies increases.
8.32 (a) The manager is 95% confident that the proportion of employees 
favoring Plan B is between 0.47, 0.21 (b) The manager is 99% confident that 
the proportion of employees favoring plan B is 0.17 and 0.51 (c) Comparing 
(a) and (b), as the confidence level increases, the range increases.
(d) Increasing the sample size by a factor of 4 decreases the standard 
error by a factor of 2.
(a) P1p 7 0.552 = P1Z 7 1.962 = 1.0 - 0.9750 = 0.0250.
(b) P1p 7 0.552 = P1Z 7 -2.042 = 1.0 - 0.0207 = 0.9793.
(c) P1p 7 0.552 = P1Z 7 2.402 = 1.0 - 0.9918 = 0.0082.
7.14 (a) 0.8944. (b) 0.7887. (c) 0.3085. (d) (a) 0.9938. (b) 0.9876.  
(c) 0.1587.
7.16 (a) 0.7661. (b) The probability is 90% that the sample percentage 
will be contained between 0.1085 to 0.0.1915. (c) The probability is 95% 
that the sample percentage will be contained between 0.1005 and 0.1995.
7.18 (a) 0.0326. (b) 0.0001. (c) Increasing the sample size by a factor of 4 
decreases the standard error by a factor of 2. The sampling distribution of 
the proportion becomes more concentrated around the true proportion of 
0.39 and, hence, the probability in (b) becomes smaller than that in (a).
7.24 (a) 0.3849. (b) 0.1151. (c) 0.1151. (d) 2.937, 3.263. (e) 29.
7.26 (a) 0.8944. (b) 4.617; 4.783. (c) 4.641.
7.28 (a) 0.0002. (b) 0.0577. (c) 0.9423.
CHAPTER 8
8.4 (15.87, 14.13) In order to have 100% confidence, the entire popula-
tion (sample size N) would have to be selected.
8.6 (a) Z at 95% is 1.96. Confidence intervals are 67 + 1.96 a 15
150b 
and 67 - 1.96 a 15
150b = 171.16, 62.842 (b) The population parameter 
in unknown and the sample size is greater than 30, which is considered 
a large sample size. Thus, using Central Limit Theorem, we can assume 
that the population is normally distributed.
8.8 Sampling is an efficient, cost-effective and practical way of quality 
control. Using Equation 8.1, he would compute the mean first and then 
the sample standard deviation. The population mean and population 
standard deviation is already known, he doesn’t need a sample.
8.10 (a) X { Z # s
2n
= 7,500 { 1.96 # 1,000
264
; 7,255 … m … 7,745. 
(b) No, since the confidence interval does not include 8,000 hours the 
manufacturer cannot support a claim that the bulbs have a mean of 8,000 
hours. (c) No. Because s is known and n = 64, from the Central Limit 
Theorem, you know that the sampling distribution of X is approximately 
normal. (d) The confidence interval is narrower, based on a population 
standard deviation of 800 hours rather than the original standard deviation 
of 1,000 hours. X { Z *
s
2n
= 7,500 { 1.96 *
800
264
, 
7.304 … m … 7,696. No, since the confidence interval does not include 
8,000 the manufacturer cannot support a claim that the bulbs have a mean 
life of 8,000 hours.
8.12 (a) 10. (b) 3.46. (c) 7. (d) 12.89, 7.11. (e) 19.89, 0.10.

	
Self-Test Solutions and Answers to Selected Even-Numbered Problems	
809
CHAPTER 9
9.2 Because ZSTAT = +2.21 7 1.96, reject H0.
9.4 Reject H0 if ZSTAT 6 -2.58 or if ZSTAT 7 2.58.
9.6 p@value = 0.0456.
9.8 p@value = 0.1676.
9.10 H0: Defendant is guilty; H1: Defendant is innocent. A Type I error 
would be not convicting a guilty person. A Type II error would be con-
victing an innocent person.
9.12 H0: m = 32%. The packet contains 32% cashews. H1: m ≠32%. 
The packet does not contain 32% cashews.
9.14 (a) ZSTAT = 7,250 - 7,500
1,000
264
= -2.0. Because ZSTAT = -2.00
6 -1.96, reject H0. (b) p@value = 0.0456. (c) 7,005 … m … 7,495.
(d) The conclusions are the same.
9.16 (a) Because -2.58 6 ZSTAT = -1.7678 6 2.58, do not reject H0. 
(b) p@value = 0.0771. (c) 0.9877 … m … 1.0023. (d) The conclusions 
are the same.
9.18 tSTAT = 2.00.
9.20 {2.1315.
9.22 No, you should not use a t test because the original population is left-
skewed, and the sample size is not large enough for the t test to be valid.
9.24 (a) tSTAT = 13.57 - 3.702>0.8> 164 = -1.30. Because 
-1.9983 6  tSTAT = -1.30 6 1.9983 and p@value = 0.1984 7 0.05, 
there is no evidence that the population mean waiting time is different 
from 3.7 minutes. (b) Because n = 64, the sampling distribution of the  
t test statistic is approximately normal. In general, the t test is appropriate 
for this sample size except for the case where the population is extremely 
skewed or bimodal.
9.26 (a) Decision: Since tSTAT  7 2.0096 and the p-value of 0.0202 6
0.05, reject H0. tSTAT  = 2.05 and p-value = 0.0202. Thus, there is suf-
ficient evidence to conclude that average property prices are different. (b) 
The manager can conclude that the published prices do not prevail in the 
current market scenario. 
9.28 (a) Because -2.1448 6 tSTAT = 1.6344 6 2.1448, do not reject H0. 
There is not enough evidence to conclude that the mean amount spent for 
lunch at a fast food restaurant, is different from $6.50. (b) The p-value 
is 0.1245. If the population mean is $6.50, the probability of observing a 
sample of nine customers that will result in a sample mean farther away 
from the hypothesized value than this sample is 0.1245. (c) The distribu-
tion of the amount spent is normally distributed. (d) With a sample size 
of 15, it is difficult to evaluate the assumption of normality. However, the 
distribution may be fairly symmetric because the mean and the median 
are close in value. Also, the boxplot appears only slightly skewed so the 
normality assumption does not appear to be seriously violated.
9.30 (a) Because -2.0096 6 tSTAT = 0.114 6 2.0096, do not reject H0. 
There is no evidence that the mean amount is different from 2 liters.  
(b) p@value = 0.9095. (d) Yes, the data appear to have met the normality 
assumption. (e) The amount of fill is decreasing over time so the values 
are not independent. Therefore, the t test is invalid.
8.34 Sufficient n is important to construct appropriate confidence intervals.
8.36 Population parameter is the proportion of the population that meets a 
certain criteria. However, in the real business world, this is the parameter 
we try to determine by studying a sample.
8.38 (a) n =
1.96211.25211 - 1.252
0.052
= 288.12 = 289.  (b) When p is 
unknown, p = 0.5 is assumed 
1.96210.5211 - 0.52
0.052
= 384.16 = 385. 
8.40 n = 97.
8.42 (a) 61.46 or 62 (b) 106.09 = 107
8.44 (a) n = 246. (b) n = 385. (c) n = 554. (d) When there is more 
variability in the population, a larger sample is needed to accurately 
­estimate the mean.
8.46 (a) p = 0.18; 0.1365 … p … 0.2235. (b) p = 0.13; 0.0919 …  
p … 0.1681. (c) p = 0.09; 0.0576 … p … 0.1224. (d) (a) n = 1,418. 
(b) n = 1,087. (c) n = 787.
8.48 (a) If you conducted a follow-up study to estimate the population 
proportion of individuals who say that banking on their mobile device is 
convenient, you would use p = 0.77 in the sample size formula because 
it is based on past information on the proportion. (b) n = 756.
8.54 (a) p = 0.88; 0.8667 … p … 0.8936 
p = 0.58; 0.5597 … p … 0.6005. 
p = 0.61; 0.5897 … p … 0.6300. 
p = 0.18; 0.1643 … p … 0.1961.
p = 0.18; 0.1643 … p … 0.1961.
(b) Most adults have a cellphone. Many adults have a desktop computer 
or a laptop computer. Some adults have an e book reader or a tablet 
­computer.
8.56 (a) 39.88 … m … 42.12. (b) 0.6158 … p … 0.8842. (c) n = 25.
(d) n = 267. (e) If a single sample were to be selected for  both purposes, 
the larger of the two sample sizes 1n = 2672 should be used.
8.58 (a) 3.19 … m … 9.21. (b) 0.3242 … p … 0.7158. (c) n = 110.  
(d) n = 121. (e) If a single sample were to be selected for both purposes, 
the larger of the two sample sizes 1n = 1212 should be used.
8.60 (a) 0.2459 … p … 0.3741. (b) 3.22 … m … +3.78.  
(c) +17,581.68 … m … +18,418.32.
8.62 (a) +36.66 … m … +40.42. (b) 0.2027 … p … 0.3973.  
(c) n = 110. (d) n = 423. (e) If a single sample were to be selected for 
both purposes, the larger of the two sample sizes 1n = 4232 should be used.
8.64 (a) 0.4643 … p … 0.6690. (b) +136.28 … m … +502.21.
8.66 (a) 8.41 … m … 8.43. (b) With 95% confidence, the population 
mean width of troughs is somewhere between 8.41 and 8.43 inches.
(c) The assumption is valid as the width of the troughs is approximately 
normally distributed.
8.68 (a) 0.2425 … m … 0.2856. (b) 0.1975 … m … 0.2385. (c) The 
amounts of granule loss for both brands are skewed to the right, but the 
sample sizes are large enough. (d) Because the two confidence intervals 
do not overlap, you can conclude that the mean granule loss of Boston 
shingles is higher than that of Vermont shingles.

810	
Self-Test Solutions and Answers to Selected Even-Numbered Problems
9.60 (a) H0: p Ú 0.31. The proportion who respond that shared 
organizational goals and objectives linking the team is the supported 
driver of alignment is greater than or equal to 0.31. H1: p 6 0.31. The 
proportion who respond that shared organizational goals and objec-
tives linking the team is the supported driver of alignment is less than 
0.31. (b) ZSTAT = -0.6487 7 -1.645; p@value = 0.2583. Because 
ZSTAT = -0.6487 7 -1.645 or p@value = 0.2583 7 0.05, reject H0. 
There is insufficient evidence that the proportion who respond that shared 
organizational goals and objectives linking the team is the supported 
driver of alignment is less than 0.31.
9.70 (a) Concluding that a firm will go bankrupt when it will not.  
(b) Concluding that a firm will not go bankrupt when it will go bankrupt. 
(c) Type I. (d) If the revised model results in more moderate or large  
Z scores, the probability of committing a Type I error will increase. Many 
more of the firms will be predicted to go bankrupt than will go bankrupt. 
On the other hand, the revised model that results in more moderate or 
large Z scores will lower the probability of committing a Type II error 
because few firms will be predicted to go bankrupt than will actually go 
bankrupt.
9.72 (a) Because tSTAT = 3.3197 7 2.0010, reject H0.
(b) p@value = 0.0015. (c) Because ZSTAT = 0.2582 6 1.645, do not 
reject H0. (d) Because -2.0010 6 tSTAT = -1.1066 6 2.0010, do not 
reject H0. (e) Because ZSTAT = 2.3238 7 1.645, reject H0.
9.74 (a) Because tSTAT = -1.69 7 -1.7613, do not reject H0. (b) The 
data are from a population that is normally distributed. (d) With the 
exception of one extreme value, the data are approximately normally dis-
tributed. (e) There is insufficient evidence to state that the waiting time is 
less than five minutes.
9.76 (a) Because tSTAT = -1.47 7 -1.6896, do not reject H0.  
(b) p@value = 0.0748. If the null hypothesis is true, the probability 
of obtaining a tSTAT of -1.47 or more extreme is 0.0748. (c) Because 
tSTAT = -3.10 6 -1.6973, reject H0. (d) p@value = 0.0021. If the 
null hypothesis is true, the probability of obtaining a tSTAT of -3.10 or 
more extreme is 0.0021. (e) The data in the population are assumed to be 
normally distributed. (g) Both boxplots suggest that the data are skewed 
slightly to the right, more so for the Boston shingles. However, the very 
large sample sizes mean that the results of the t test are relatively insensi-
tive to the departure from normality.
9.78 (a) tSTAT = -3.2912, reject H0. (b) p@value = 0.0012.  
(c) tSTAT = -7.9075, reject H0. (d) p@value = 0.0000. (e) Because of the 
large sample sizes, you do not need to be concerned with the normality 
assumption.
CHAPTER 10
10.2 (a) t = 3.8959. (b) df = 21. (c) 2.5177. (d) Because tSTAT =
tSTAT = 3.8959 7 2.5177, reject H0.
10.4 3.73 … m1 - m2 … 12.27.
10.6 Because tSTAT = 2.6762 6 2.9979 or p@value = 0.0158 7 0.01, do 
not reject H0. There is no evidence of a difference in the means of the two 
populations.
10.8 (a) 234.84 … m … 237.16. You are 90% confident that the popu-
lation mean for the alloy CGI is somewhere between 234.84 and 
237.16. (b) 10.225 … mD … 13.775. You are 95% confident that the 
mean difference between the manufacture of engine blocks using the 
given two alloys somewhere between 10.225 and 13.775. (c) Because 
tSTAT = 13.4164 7 1.6620, reject null hypothesis. There is evidence of 
9.32 (a) Because tSTAT = -5.9355 6 -2.0106, reject H0. There is 
enough evidence to conclude that mean widths of the troughs is different 
from 8.46 inches. (b) The population distribution is normal. (c) Although 
the distribution of the widths is left-skewed, the large sample size means 
that the validity of the t test is not seriously affected. The large sample 
size allows you to use the t distribution.
9.34 (a) Because -2.68 6 tSTAT = 0.094 6 2.68, do not reject H0. 
There is no evidence that the mean amount is different from 5.5 grams. 
(b) 5.462 … m … 5.542. (c) The conclusions are the same.
9.36 p@value = 0.0228.
9.38 p@value = 0.0838.
9.40 p@value = 0.9162.
9.42 tSTAT = 2.7638.
9.44 tSTAT = -2.5280.
9.46 (a) tSTAT = 2.7273 7 1.6604. There is evidence that the population 
mean bus miles is greater than 3,900 miles. (b) p@value = 0.0038 6
0.05. The probability of getting a tSTAT statistic greater than 2.7273 given 
that the null hypothesis is true, is 0.0038.
9.48 (a) tSTAT = 123.05 - 252>16.83> 1355 = -2.1831. 
Because tSTAT = -2.1831 7 -2.3369, do not reject H0. 
p@value = 0.0148 7 0.01, do not reject H0. (b) The probability of 
­getting a sample mean of 23.05 minutes or less if the population mean is 
25 minutes is 0.0148.
9.50 (a) tSTAT = 4.1201 7 2.3974. There is evidence that the population 
mean one-time gift donation is greater than $70. (b) The probability of 
getting a sample mean of $75 or more if the population mean is $70 is 
0.0001.
9.52 p = 0.22.
9.54 Do not reject H0.
9.56 (a) H0: p … 0.5, H1: 0.5; Decision rule: p-value 60.05, reject H0.
(b) ZSTAT = 1.34, p = 0.0901. Since, p 7 0.05, do not reject H0. There 
is not enough evidence that the awareness regarding the test is greater 
than 50%. (c) ZSTAT = 0.6, p = 0.2743. Since, p 7 0.05, do not reject 
H0. There is not enough evidence that the awareness regarding the test 
is greater than 50%. (d) The Centre for Disease Control and Prevention, 
using the results, can presume that the awareness regarding the test is 
not greater than 50%. Thus, the Center can take appropriate measures to 
spread the awareness to reach the expected awareness level.
9.58 H0: p = 0.35; H1: p ≠0.35. Decision rule: If ZSTAT 7 1.96 or 
ZSTAT 6 -1.96, reject H0.
p = 328
801 = 0.4095
Test statistic:
ZSTAT =
p - p
B
p11 - p2
n
=
0.4095 - 0.35
B
0.409511 - 0.40952
801
= 3.5298.
Because ZSTAT = 3.5298 7 1.96 or p@value = 0.0004 6 0.05, reject 
H0 and conclude that there is evidence that the proportion of all LinkedIn 
members who plan to spend at least $1,000 on consumer electronics in 
the coming year is different from 35%.

	
Self-Test Solutions and Answers to Selected Even-Numbered Problems	
811
10.22 (a) Because  tSTAT = 1.7948 7 1.6939 reject H0. There is evidence 
to conclude that the mean at Super Target is higher than at Walmart.  
(b) You must assume that the distribution of the differences between the 
prices is approximately normal. (c) p@value = 0.0411. The likelihood 
that you will obtain a tSTAT statistic greater than 1.7948 if the mean price 
at Super Target is not greater than Walmart is 0.0411.
10.24 (a) Because tSTAT = 1.8425 6 1.943, do not reject H0. There is 
not enough evidence to conclude that the mean bone marrow microvessel 
density is higher before the stem cell transplant than after the stem cell 
transplant. (b) p@value = 0.0575. The probability that the t statistic for 
the mean difference in microvessel density is 1.8425 or more is 5.75% 
if the mean density is not higher before the stem cell transplant than 
after the stem cell transplant. (c) -28.26 … mD … 200.55. You are 95% 
confident that the mean difference in bone marrow microvessel density 
before and after the stem cell transplant is somewhere between -28.26 
and 200.55. (d) That the distribution of the difference before and after the 
stem cell transplant is normally distributed.
10.26 (a) Because tSTAT = -9.3721 6 -2.4258, reject H0. There is 
evidence that the mean strength is lower at two days than at seven days. 
(b) The population of differences in strength is approximately normally 
distributed. (c) p = 0.000.
10.28 (a) Because -2.58 … ZSTAT = -0.58 … 2.58, do not reject H0. 
(b) -0.273 … p1 - p2 … 0.173.
10.30 (a) Populations proportion: 1 = females, 2 = males
1-0.152 { 2.5758 A
0.426511 - 0.42652
80
+ 0.612511 - 0.61252
80
 
= 1-0.351, 0.0512 
10.32 (a) H0: p1 = p2. H1: p1 ≠p2. Decision rule: If   ZSTAT  7 2.58, 
reject H0.
Test statistic: p = X1 + X2
n1 + n2
=
930 + 230
1,000 + 1,000 = 0.58
ZSTAT =
1p1 - p22 - 1p2 - p22
Bp11 - p2a 1
n1
+ 1
n2
b
=
10.93 - 0.232 - 0
B0.5811 - 0.582a
1
1,000 +
1
1,000b
.
ZSTAT = 31.7135 7 2.58, reject H0. There is evidence of a difference 
in the proportion of Superbanked and Unbanked with respect to the 
proportion that use credit cards. (b) p@value = 0.0001. The probability 
of obtaining a difference in proportions that gives rise to a test statistic 
below -31.7135 or above +31.7135 is 0.0000 if there is no difference  
in the proportion of Superbanked and Unbanked who use credit cards.  
(c) 0.6599 … 1p1 - p22 … 0.7401. You are 99% confident that the dif-
ference in the proportion of Superbanked and Unbanked who use credit 
cards is between 0.6599 and 0.7401.
10.34 (a) Because ZSTAT = 3.8512 7 1.96, reject H0. There is evidence 
of a difference in the proportion of gamification-user sales organizations 
and non-gamification-user sales organizations that provide mobile access 
to CRM. (b) p@value = 0.0001. The probability of obtaining a difference 
in proportions that is 0.336 or more in either direction is 0.0001 if there is 
no difference between the proportion of gamification-user sales organiza-
tions and non-gamification-user sales organizations that provide mobile 
access to CRM.
10.36 (a) 2.20. (b) 2.57. (c) 3.50.
10.38 (a) Population B: S2 = 25. (b) 1.5625.
the difference in the melting points of the two alloys. (d) p-value 
 = 0.0000 6 0.05 reject null hypothesis.
10.10 (a) H0: m1 = m2, where Populations: 1 = Southeast, 2 =
Gulf Coast. H1: m1 ≠m2. Decision rule: df = 29. If tSTAT 6 -2.0452 or 
tSTAT 7 2.0452, reject H0.
Test statistic:
 S2
p =
1n1 - 121S2
 12 + 1n2 - 121S2
22
1n1 - 12 + 1n2 - 12
 =
1132140.990622 + 1162132.090622
14 + 17
= 1,321.3748
 tSTAT =
1X1 - X22 - 1m1 - m22
BS2
pa 1
n1
+ 1
n2
b
 =
140.0714 - 27.94122 - 0
B1,321.3748a 1
14 + 1
17b
= 0.9246.
Decision: Because -2.0452 6 tSTAT = 0.9246 6 2.0452, do not  
reject H0. There is not enough evidence to conclude that the mean ­ 
number of ­partners between the Southeast and Gulf Coast is different.  
(b) p@value = 0.3628. (c) In order to use the pooled-variance t test, you 
need to assume that the ­populations are normally distributed with equal 
variances.
10.12 (a) Because tSTAT = -4.1343 6 -2.0484, reject H0.  
(b) p@value = 0.0003. (c) The populations of waiting times 
are ­approximately normally distributed. (d) -4.2292 …
m1 - m2 … -1.4268.
10.14 (a) Because tSTAT = -1.4707 7 -2.0484, do not reject H0. There 
is insufficient evidence of a difference in the mean time to start a busi-
ness between developed and emerging countries. (b) p@value = 0.1525. 
The probability that two samples have a mean difference of 11.20 
or more is 0.1525 if there is no difference in the mean time to start 
a business between developed and emerging countries. (c) You need 
to assume that the population distribution of the time to start a busi-
ness of both developed and emerging countries is normally distributed. 
(d) -26.7996 … m1 - m2 … 4.3996.
10.16 (a) Because tSTAT = -2.0036 6 -2.0017 or p@value =
0.0498 6 0.05, reject H0. There is evidence of a difference in the mean 
Facebook time per day between males and females. (b) You must assume 
that each of the two independent populations is normally distributed.
10.18 df = 19.
10.20 (a) tSTAT = 1-1.55662>11.4242> 19 = -3.2772. Because 
tSTAT = -3.2772 6 -2.306 or p@value = 0.0112 6 0.05, reject H0. 
There is enough evidence of a difference in the mean summated  
­ratings between the two brands. (b) You must assume that the distribution 
of the differences between the two ratings is approximately normal.  
(c) p@value = 0.0112. The probability of obtaining a mean  
­difference in ratings that results in a test statistic that deviates  
from 0 by 3.2772 or more in either direction is 0.0112 if there is no 
­difference in the mean summated ratings between the two brands.  
(d) -2.6501 … mD … -0.4610. You are 95% confident that the mean dif-
ference in summated ratings between brand A and brand B is ­somewhere 
between -2.6501 and -0.4610.

812	
Self-Test Solutions and Answers to Selected Even-Numbered Problems
data value, the standard deviation went from 1.8 to 13.2, which reduced 
the value of t statistic. (d) Because FSTAT = 1.2308 6 3.8549, do not 
reject H0. There is not enough evidence to conclude that the population 
variances are different for the Introduction to Computers students and 
computer majors. Hence, the pooled-variance t test is a valid test to 
determine whether computer majors can write a VB.NET program in 
less time than introductory students, assuming that the distributions of 
the time needed to write a VB.NET program for both the Introduction 
to Computers students and the computer majors are approximately nor-
mally distributed. Because tSTAT = 4.0666 7 1.7341, reject H0. There 
is enough evidence that the mean time is higher for Introduction to 
Computers students than for computer majors. (e) p@value = 0.000362. 
If the true population mean amount of time needed for Introduction to 
Computer students to write a VB.NET program is no more than  
10 minutes, the probability of observing a sample mean greater than 
the 12 minutes in the current sample is 0.0362%. Hence, at a 5% level 
of significance, you can conclude that the population mean amount of 
time needed for Introduction to Computer students to write a VB.NET 
program is more than 10 minutes. As illustrated in (d), in which there is 
not enough evidence to conclude that the population variances are dif-
ferent for the Introduction to Computers students and computer majors, 
the pooled-variance t test performed is a valid test to determine whether 
computer majors can write a VB.NET program in less time than intro-
ductory students, assuming that the distribution of the time needed to 
write a VB.NET program for both the Introduction to Computers stu-
dents and the computer majors are approximately normally distributed.
10.64 From the boxplot and the summary statistics, both ­distributions 
are approximately normally distributed. FSTAT = 1.056 6 1.89. 
There is insufficient evidence to conclude that the two population 
variances are significantly different at the 5% level of significance. 
tSTAT = -5.084 6 -1.99. At the 5% level of significance, there is suf-
ficient evidence to reject the null hypothesis of no difference in the mean 
life of the bulbs between the two manufacturers. You can conclude that 
there is a significant difference in the mean life of the bulbs between the 
two manufacturers.
10.66 (a) Because ZSTAT = -3.6911 6 -1.96, reject H0. There is 
enough evidence to conclude that there is a difference in the proportion of 
men and women who order dessert. (b) Because ZSTAT = 6.0873 7 1.96, 
reject H0. There is enough evidence to conclude that there is a difference 
in the proportion of people who order dessert based on whether they 
ordered a beef entree.
10.68 The normal probability plots suggest that the two populations 
are not normally distributed. An F test is inappropriate for testing 
the difference in the two variances. The sample variances for Boston 
and Vermont shingles are 0.0203 and 0.015, respectively. Because 
tSTAT = 3.015 7 1.967 or p@value = 0.0028 6 a = 0.05, reject H0. 
There is sufficient evidence to conclude that there is a difference in the 
mean granule loss of Boston and Vermont shingles.
CHAPTER 11
11.2 (a) SSW = 150. (b) MSA = 15. (c) MSW = 5. (d) FSTAT = 3.
11.4 (a) 2. (b) 18. (c) 20.
11.6 (a) Reject H0 if FSTAT 7 2.95; otherwise, do not reject H0.  
(b) Because FSTAT = 4 7 2.95, reject H0. (c) The table does not have 
28 degrees of freedom in the denominator, so use the next larger critical 
value, Qa = 3.90. (d) Critical range = 6.166.
10.40 dfnumerator = 24, dfdenominator = 24.
10.42 Because FSTAT = 1.2109 6 2.27, do not reject H0.
10.44 (a) Because FSTAT = 1.2995 6 3.18, do not reject H0. (b) Because 
FSTAT = 1.2995 6 2.62, do not reject H0.
10.46 (a) H0: s2
 1 = s2
2. H1: s2
 1 ≠s2
2.
Decision rule: If FSTAT 7 2.8506, reject H0.
Test statistic: FSTAT = S2
 1
S2
2
=
140.990622
132.090622 = 1.6316.
Decision: Because FSTAT = 1.6316 6 2.8506, do not reject H0. There is 
insufficient evidence to conclude that the two population variances are 
different. (b) p@value = 0.3509. (c) The test assumes that each of the two 
populations is normally distributed. (d) Based on (a) and (b), a pooled-
variance t test should be used.
10.48 (a) Because FSTAT = 3.8179 6 4.0721 or p@value =
0.0609 6 0.05, do not reject H0. There is no evidence of a difference in 
the variability of the battery life between the two types of digital cameras. 
(b) p@value = 0.0609. The probability of obtaining a sample that yields a 
test statistic more extreme than 3.8179 is 0.0609 if there is no difference 
in the two population variances. (c) The test assumes that each of the two 
populations are normally distributed. The boxplots appear fairly symmet-
rical and the skewness and kurtosis statistics are not dramatically different 
from 0. Thus, the distributions do not appear to be substantially different 
from a normal distribution. (d) Based on (a) and (b), a pooled-variance t 
test should be used.
10.50 Because FSTAT = 1.1583 6 4.8232, or p@value = 0.8658 7 0.05, 
do not reject H0. There is insufficient evidence of a difference in the 
­variance of the yield in the two cities.
10.58 (a) Because FSTAT = 1.2221 6 1.7462, or p@value = 0.4688
7 0.05, do not reject H0. There is not enough evidence of a ­difference 
in the variance of the salary of Black Belts and Green Belts. (b) The 
pooled-variance t test. (c) Because tSTAT = 4.2412 7 1.6554 or 
p@value = 0.0000 6 0.05, reject H0. There is evidence that the mean 
­salary of Black Belts is greater than the mean salary of Green Belts.
10.60 (a) Because FSTAT = 1.5625 6 Fa = 1.6854, do not reject H0. 
There is not enough evidence to conclude that there is a difference 
between the variances in the talking time per month between women and 
men. (b) It is more appropriate to use a pooled-variance t test. Using the 
pooled-variance t test, because tSTAT = 11.1196 7 2.6009, reject H0. 
There is enough evidence of a difference in the mean talking time per 
month between women and men. (c) Because FSTAT = 1.44 6 1.6854, 
do not reject H0. There is not enough evidence to conclude that there is 
a difference between the variances in the number of text messages sent 
per month between women and men. (d) Using the pooled-variance t test, 
because tSTAT = 8.2456 7 2.6009, reject H0. There is enough evidence 
of a difference in the mean number of text messages sent per month 
between women and men.
10.62 (a) Because tSTAT = 3.3282 7 1.8595, reject H0. There is enough 
evidence to conclude that the introductory computer students required 
more than a mean of 10 minutes to write and run a program in VB.NET 
(b) Because tSTAT = 1.3636 6 1.8595, do not reject H0. There is not 
enough evidence to conclude that the introductory computer students 
required more than a mean of 10 minutes to write and run a program in 
VB.NET (c) Although the mean time necessary to complete the assign-
ment increased from 12 to 16 minutes as a result of the increase in one 

	
Self-Test Solutions and Answers to Selected Even-Numbered Problems	
813
11.14 (a) Because FSTAT = 53.03 7 2.92, reject H0. (b) Critical 
range = 5.27 (using 30 degrees of freedom). Designs 3 and 4 are 
­different from designs 1 and 2. Designs 1 and 2 are different from each 
other. (c) The assumptions are that the samples are randomly and inde-
pendently selected (or randomly assigned), the original populations of 
distances are approximately normally distributed, and the variances are 
equal. (d) Because FSTAT = 2.093 6 2.92, do not reject H0. There is 
insufficient evidence of a difference in the variation in the distance  
among the four designs. (e) The manager should choose design 3 or 4.
11.16 (a) SSE = 75. (b) MSA = 15, MSBL = 12.5, MSE = 3.125.  
(c) FSTAT = 4.8. (d) FSTAT = 4.
11.18 (a) df numerator = 5, df denominator = 24. (b) Qa = 4.17.  
(c) Critical range = 2.786.
11.20 (a) MSE = 3, SSE = 36, because FSTAT = 4 6 F0.01,6,12 = 4.82, 
do not reject H0.
11.22 (a) Because FSTAT = 5.185 7 3.07, reject H0. (b) Because 
FSTAT = 5 7 2.49, reject H0.
11.24 (a)
Verizon FIOS
WOW
A T & T U-verse
Bright House Networks
SuddenLink
Cox
Cablevision/Optimum
Insight
RCN
Comcast
TimeWarner
Mediacom
Charter
TV
Phone
Internet
SUMMARY
Anova: Two-Factor Without Replication
Count
Sum
Average
Variance
21
21
21
37.33333
30.33333
21
34.14103
27.02564
30.39744
12.33333
5.333333
4
20.33333
13
32.33333
66.33333
3
3
3
3
3
3
13
13
13
3
3
3
3
3
3
3
204
198
195
193
196
174
843
959
880
211
208
228
235
219
206
215
68
66
65
64.33333
65.33333
58
64.84615
73.76923
67.69231
70.33333
69.33333
76
78.33333
73
68.66667
71.66667
Rows
Columns
Total
Error
Source of 
Variation
ANOVA
ss
df
F
P-value
0.0000
0.0000
F crit
3.402826
2.18338
70.51282
540.1538
1,028.256
1,638.923
24
2
12
38
MS
2.938034
270.0769
85.68803
91.92436
29.16509
FSTAT = 91.9243 7 3.4028, p-value is 0.0000, reject H0. There is 
­evidence of a difference in the mean score between TV, phone, and Internet.
(b) Critical range = 3.53A
2.938034
13
= 1.6781; TV - phone: 64.8462
- 73.7692 = -8.9230; TV - Internet: 64.8462 - 67.6923 = -2.8461;
Phone - Internet: 73.7692 - 67.6923 = 6.0769. The mean for TV is 
significantly lower than for phone and for Internet. Then mean for phone 
is significantly higher than for Internet.
11.26 (a) H0: m.1 = m.2 = m.3 = m.4 H1: Not all m.j are equal where 
j = 1, 2, 3, 4 FSTAT = 99.8046 7 2.8115, p@value = 0.0000 6 0.05, 
reject H0. There is evidence of a difference in the mean rates for the dif-
ferent investments. (b) The assumptions needed are (i) samples are ran-
domly and independently drawn, (ii) populations are normally ­distributed, 
11.8 (a) H0: mA = mB = mC = mD and H1: At least one mean is different.
 MSA =
SSA
c - 1 = 7,854,648
3
= 2,618,216.2.
 MSW =
SSW
n - c = 15,137,801.8
36
= 420,494.4944.
 FSTAT = MSA
MSW =
2,618,216.2
420,494.4944 = 6.2265.
 F0.05,3,36 = 2.8663.
Because the p-value is 0.0016 and FSTAT = 6.2265 7 2.8663, reject H0. 
There is sufficient evidence of a difference in the mean import cost across 
the four global regions. (b) Critical range = QaB
MSW
2
a 1
nj
+ 1
nj′
b
= 3.79B
420,494.4944
2
a 1
10 + 1
10b = 205.0596.
From the Tukey-Kramer procedure, there is a difference in the mean 
import cost between the East Asia and Pacific region and each of the 
other regions. None of the other regions are different. (c) ANOVA output 
for Levene’s test for homogeneity of variance:
 MSA =
SSA
c - 1 = 1,387,853.3
3
= 462,617.7667
 MSW =
SSW
n - c = 7,654,784.8
36
= 212,632.9111
 FSTAT = MSA
MSW = 462,617.7667
212,632.9111 = 2.1757
 F0.05,3,36 = 2.8663
Because p@value = 0.1078 7 0.05 and FSTAT = 2.1757 6 2.8663, do 
not reject H0. There is insufficient evidence to conclude that the variances 
in the import cost are different. (d) From the results in (a) and (b), the 
mean import cost for the East Asia and Pacific region is lower than for the 
other regions.
11.10 (a) Because FSTAT = 12.56 7 2.76, reject H0. (b) Critical 
range = 4.67. Advertisements A and B are different from Advertisements 
C and D. Advertisement E is only different from Advertisement D.  
(c) Because FSTAT = 1.927 6 2.76, do not reject H0. There is no 
­evidence of a significant difference in the variation in the ratings among 
the five advertisements. (d) The advertisements underselling the pen’s 
characteristics had the highest mean ratings, and the advertisements over-
selling the pen’s characteristics had the lowest mean ratings. Therefore, 
use an advertisement that undersells the pen’s characteristics and avoid 
advertisements that oversell the pen’s characteristics.
11.12 (a)
 
Source
Degrees of 
Freedom
Sum of 
Squares
Mean 
Squares
 
F
Among groups
  2
  1.879
0.9395
8.7558
Within groups
297
31.865
0.1073
Total
299
33.744
(b) Since FSTAT = 8.7558 7 3.00, reject H0. There is evidence of a 
­difference in the mean soft-skill score of the different groups.
(c) Group 1 versus group 2: 0.072 6 Critical range = 0.1092; 
group 1 versus group 3: 0.181 7 0.1056; group 2 versus group 3: 
0.109 6 0.1108. There is evidence of a difference in the mean soft-skill 
score between those who had no coursework in leadership and those who 
had a degree in leadership.

814	
Self-Test Solutions and Answers to Selected Even-Numbered Problems
There is evidence that the mean strength of suppliers is different. Critical 
range = 3.359. Supplier 1 has a mean strength that is less than suppliers 
2 and 3.
11.54 (a) Because FSTAT = 0.075 6 3.68, do not reject H0. (b) Because 
FSTAT = 4.09 7 3.68, reject H0. (c) Critical range = 1.489. Breaking 
strength is significantly different between 30 and 50 psi.
11.56 (a) Because FSTAT = 0.1899 6 4.1132, do not reject H0. 
There is insufficient evidence to conclude that there is any inter-
action between type of breakfast and desired time. (b) Because 
FSTAT = 30.4434 7 4.1132, reject H0. There is sufficient evidence to 
conclude that there is an effect due to type of breakfast. (c) Because 
FSTAT = 12.4441 7 4.1132, reject H0. There is sufficient evidence to 
conclude that there is an effect due to desired time. (e) At the 5% level of 
significance, both the type of breakfast ordered and the desired time have 
an effect on delivery time difference. There is no interaction between the 
type of breakfast ordered and the desired time.
11.58 Interaction: FSTAT = 0.2169 6 3.9668 or p@value =
0.6428 7 0.05. There is insufficient evidence of an interaction between 
piece size and fill height. Piece size: FSTAT = 842.2242 7 3.9668 or 
p@value = 0.0000 6 0.05. There is evidence of an effect due to piece 
size. The fine piece size has a lower difference in coded weight. Fill 
height: FSTAT = 217.0816 7 3.9668 or p@value = 0.0000 6 0.05. There 
is evidence of an effect due to fill height. The low fill height has a lower 
difference in coded weight.
11.60 Population 1 = short term 2 = long term, 3 = world; One-year 
return: Levene test: Since the p@value 0.5220 7 0.05 do not reject H0 .  
There is insufficient evidence to show a difference in the variance of the 
return among the three different types of bond funds at a 5% level of signif-
icance. Since the p-value is 0.0000, reject H0. There is sufficient evidence 
to show a difference in the mean one-year returns among the three different 
types of bond funds at a 5% level of significance. Critical range = 2.8194. 
At the 5% level of significance, there is sufficient evidence that the mean 
one-year returns of the short-term bond funds are significantly lower than 
the others. Three-year return: Levene test: FSTAT = 1.0557. Since the 
p@value = 0.3619 7 0.05, do not reject H0. There is insufficient evidence 
to show a difference in the variance of return among the three different 
types of bond funds at a 5% level of significance. FSTAT = 37.1365. Since 
the p-value is 0.0000, reject H0. There is sufficient evidence to show a dif-
ference in the mean three-year returns among the three different types of 
bond funds at a 5% level of significance. Critical range = 1.9394. At the 
5% level of significance, there is sufficient evidence that the mean three-
year returns of the short-term bond funds is significantly higher than the 
others. Also, the mean three-year returns of the long-term bond funds are 
significantly higher than that of the world bond funds.
CHAPTER 12
12.2 (a) For df = 1 and a = 0.05, x2
a = 3.841. (b) For df = 1 and 
a = 0.025, x2 = 5.024. (c) For df = 1 and a = 0.01, x2
a = 6.635.
12.4 (a) Because XSTAT
2
= 1.83 6 3.841, do not reject H0. (b) The own-
ers do not use more fertilizers in comparison to tenants. 
12.6 (a) H0: p1 = p2; H1: p1 ≠p2, where p1 = specific breed and 
p2 = general breed (b) Decision rule: df = 1. If x2
STAT 7 3.8415,  reject H0.  
Test statistic: x2
STAT = 3.2226  
Decision: Since x2
STAT = 3.2226 is less than the upper critical bound of 
3.8415, do not reject H0. There is not enough evidence of a difference in 
the resemblance level with specific breed dogs. 
(c) Decision rule: p-value 6 0.05, 0.0726 7 0.05, do not reject H0. The 
result is same as in (b).
(iii) populations have equal variances, and (iv) no interaction effect 
between treatments and blocks. (c), (d) H0: m1. = m2. = c = m16. H0: 
Not all mi. are equal where i = 1, 2, c, 16, FSTAT = 12.2177 7 1.8949, 
and p@value = 0.0000, reject H0. There is evidence of a significant block 
effect in this experiment. The blocking has been advantageous in reducing 
the experimental error.
11.28 (a) Because FSTAT = 268.26 7 3.114, reject H0. There is enough 
evidence to conclude that there is a difference in the mean compressive 
strength after 2, 7, and 28 days. (b) Critical range = 0.1651. At the  
0.05 level of significance, all of the comparisons are significant.  
(c) RE = 2.558. (e) The compressive strength of the concrete increases 
over the three time periods.
11.30 (a) 40. (b) 60 and 55. (c) 10. (d) 10.
11.32 (a) Because FSTAT = 6.00 7 3.35, reject H0. (b) Because 
FSTAT = 5.50 7 3.35, reject H0. (c) Because FSTAT = 1.00 6 2.73, do 
not reject H0.
11.34 dfB = 4, dfTOTAL = 44, SSA = 160, SSAB = 80, SSE = 150, 
SST = 610, MSB = 55, MSE = 5. For A: FSTAT = 16. For 
B: FSTAT = 11. For AB: FSTAT = 2.
11.36 (a) Because FSTAT = 3.4032 6 4.3512, do not reject H0.  
(b) Because FSTAT = 1.8496 6 4.3512, do not reject H0. (c) Because 
FSTAT = 9.4549 7 4.3512 reject H0. (e) Die diameter has a significant 
effect on density, but die temperature does not. However, the cell means 
plot shows that the density seems higher with a 3 mm die diameter at  
155 C but that there is little difference in density with a 4 mm die diam-
eter. This interaction is not significant at the 0.05 level of significance.
11.38 (a) H0: There is no interaction between brand and water temperature. 
H1: There is an interaction between brand and water temperature. 
Because FSTAT = 253.1552
12.2199 = 20.7167 7 3.555 or the p@value =
0.0000214 6 0.05, reject H0. There is evidence of interaction between 
brand of pain reliever and temperature of the water. (b) Because there 
is an interaction between brand and the temperature of the water, it is 
inappropriate to analyze the main effect due to brand. (c) Because there 
is an interaction between brand and the temperature of the water, it is 
inappropriate to analyze the main effect due to water temperature. (e) The 
difference in the mean time a tablet took to dissolve in cold and hot water 
depends on the brand, with Alka-Seltzer having the largest difference and 
Equate the smallest difference.
11.40 (a) FSTAT = 0.1523, p@value = 0.9614 7 0.05, do not reject H0. 
There is not enough evidence to conclude that there is an ­interaction 
between the brake discs and the gauges. (b) FSTAT = 7.7701, p-value is 
virtually 0 6 0.05, reject H0. There is sufficient ­evidence to conclude 
that there is an effect due to brake discs. (c) FSTAT = 0.1465, p@value =  
0.7031 7 0.05, do not reject H0. There is inadequate evidence to con-
clude that there is an effect due to the gauges. (d) From the plot, there is 
no obvious interaction between brake discs and gauges. (e) There is no 
obvious difference in mean temperature across the gauges. It appears that 
Part 1 has the lowest, Part 3 the second lowest, and Part 2 has the highest 
average temperature.
11.52 (a) Because FSTAT = 0.0111 6 2.9011, do not reject H0.  
(b) Because FSTAT = 0.8096 6 4.1491, do not reject H0. (c) Because 
FSTAT = 5.1999 7 2.9011, reject H0. (e) Critical range = 3.56. Only 
the means of Suppliers 1 and 2 are different. You can conclude that the 
mean tensile strength is lower for Supplier 1 than for Supplier 2, but 
there are no statistically significant differences between Suppliers 1 and 
3, Suppliers 1 and 4, Suppliers 2 and 3, Suppliers 2 and 4, and Suppliers 
3 and 4. (f ) FSTAT = 5.6998 7 2.8663 (p@value = 0.0027 6 0.05). 

	
Self-Test Solutions and Answers to Selected Even-Numbered Problems	
815
12.18 (a) Because x2
STAT = 9.0485 7 5.9915, reject H0. There is 
­evidence of a difference in the percentage who use their cellphones  
while watching TV between the groups. (b) p@value = 0.0108.  
(c) Group 1 ­versus group 2: 0.0215 6 0.0788. Not significant. Group 1 
versus group 3: 0.0905 7 0.0859 significant. Group 2 versus group 3: 
0.0691 7 0.06457 significant. The rural group is different from the urban 
and suburban groups.
12.20 df = 1r - 121c - 12 = 13 - 1214 - 12 = 6.
12.22 Since the calculated test statistic 12.3902 is less than the critical value 
of 12.59159, you do not reject H0 and conclude that there is not enough 
evidence of a relationship between mode of commuting and stress level.
12.24 H0: There is no relationship between the frequency of posting on 
Facebook and age. H1: There is a relationship between the frequency of 
posting on Facebook and age.
fo
fe
1  fo - fe2
1  fo −fe22,fe
20
8.895459
11.10454
13.8622
28
13.83738
14.16262
14.4955
32
19.43823
12.56177
8.1179
32
26.35692
5.643083
1.2082
22
27.83949
-5.83949
1.2249
16
34.59345
-18.5935
9.9937
  6
25.03907
-19.0391
14.4768
22
17.90496
4.095037
0.936574
37
27.85216
9.147835
3.004538
46
39.12566
6.87434
1.207815
69
53.05174
15.94826
4.794318
66
56.0359
9.964097
1.771779
59
69.63041
-10.6304
1.622935
15
50.39916
-35.3992
24.86352
  9
13.34319
-4.34319
1.413702
14
20.75607
-6.75607
2.199092
31
29.15734
1.842661
0.116451
35
39.53537
-4.53537
0.520284
47
41.75924
5.24076
0.657712
56
51.89018
4.10982
0.325507
42
37.55861
4.441394
0.525205
  2
10.5491
-8.5491
6.928282
  4
16.40971
-12.4097
9.384747
  7
23.05174
-16.0517
11.1774
17
31.2566
-14.2566
6.502647
28
33.01478
-5.01478
0.761721
61
41.02429
19.97571
9.726655
66
29.69377
36.30623
44.39121
  1
3.307286
-2.30729
1.609649
  1
5.144667
-4.14467
3.339043
  2
7.227033
-5.22703
3.78051
  7
9.799366
-2.79937
0.79969
  6
10.35058
-4.35058
1.828646
18
12.86167
5.138332
2.052801
23
9.309398
13.6906
20.13369
Decision rule: If x2
STAT 7 42.9798, reject H0.
Test statistic: x2
STAT =
a
all cells
 
1f0 - fe22
fe
= 229.7554.
Decision: Because x2
STAT = 229.7554 7 42.9798 reject H0. There is 
­evidence to conclude that there is a relationship between the ­frequency of 
Facebook posts and age.
12.8 (a) H0: p1 = p2. H1: p1 ≠p2. Because x2
STAT = 1930 - 58022> 
580 + 170 - 42022>420 + (230 - 58022>580 + 1770 - 420)2 =
1,005.7471 7 6.635, reject H0. There is evidence of a difference in the 
proportion of Superbanked and Unbanked with respect to the propor-
tion that use credit cards. (b) p@value = 0.0000. The probability of 
obtaining a difference in proportions that gives rise to a test statistic 
above 1,005.7471 is 0.0000 if there is no difference in the proportion 
of Superbanked and Unbanked who use credit cards. (c) The results 
of (a) and (b) are exactly the same as those of Problem 10.32. The 
x2 in (a) and the Z in Problem 10.32 (a) satisfy the relationship that 
x2 = 1,005.7471 = Z2 = 131.713522, and the p-value in (b) is exactly 
the same as the p-value computed in Problem 10.32 (b).
12.10 (b) Since x2
STAT = 14.8319 7 3.841, reject H0. There is ­evidence 
that there is a significant difference between the ­gamification-user sales 
organizations and non-gamification-user sales organizations in the pro-
portion that provide mobile access to CRM.  
(c) p@value 0.0001. The probability of obtaining a test statistic of 14.8319 
or larger when the null hypothesis is true is 0.0001. (d) The results are 
identical since 13.851222 = 14.8319.
12.12 (a) The expected frequencies for the first row are 20, 30, and 40. 
The expected frequencies for the second row are 30, 45, and 60.  
(b) Because x2
STAT = 12.5 7 5.991, reject H0.
12.14 (a) Because the calculated test statistic x2
STAT = 44.6503 7 11.0705, 
reject H0 and conclude that there is a difference in the proportion who  
buy lunch between the age groups. (b) The p-value is virtually 0. 
The probability of a test statistic greater than 44.6503 or more is 
­approximately 0 if there is no difference between the age groups in the 
proportion who buy lunch. (c) The 18–24 and 25–34 groups are different 
from the 45–54, 55–64, and 65+ groups, and the 35–44 group is different 
from the 65+ group.
12.16 (a) H0: p1 = p2 = p3. H1: At least one proportion differs.
f0
fe
1 f0 - fe2
1f0 - fe22>fe
118
  72
46
29.3889
  82
128
-46
16.5313
  72
  72
0
       0
128
128
0
       0
  26
  72
-46
29.3889
174
128
46
16.5313
91.8403
Decision rule: df = 1c - 12 = 13 - 12 = 2. If x2
STAT 7 5.9915, reject H0.
Test statistic: x2
STAT =
a
all cells
1 f0 - fe2
fe
= 91.8403. 
Decision: Because x2
STAT = 91.8403 7 5.9915, reject H0. There is a 
significant difference in the age groups with respect to using a cellphone 
to access social networking. (b) p@value = 0.0000. The probability that 
the test statistic is greater than or equal to 91.8403 is 0.0000, if the null 
hypothesis is true.
(c)
Pairwise Comparisons
Critical Range
pj - pj′
1 to 2
0.1189
0.23
1 to 3
0.1031
0.46
2 to 3
0.1014
0.23
There is a significant difference between all the groups. (d) Marketers can 
use this information to target their marketing to the 18- to 34-year-old group 
since they are more likely to use their cellphones to access social media.

816	
Self-Test Solutions and Answers to Selected Even-Numbered Problems
with those of Problem 11.10. (c) Because the combined scores are not 
true continuous variables, the nonparametric Kruskal-Wallis rank test is 
more appropriate because it does not require that the scores be normally 
distributed.
12.50 (a) Because H = 22.0357 7 7.815 or the p-value is approximately 
0, reject H0. There is sufficient evidence of a difference in the median cost 
associated with importing a standardized cargo of goods by sea transport 
across the four global regions. (b) The results are the same.
12.56 (a) Because x2
STAT = 0.412 6 3.841, do not reject H0. 
There is insufficient evidence to conclude that there is a relation-
ship between a student’s gender and pizzeria selection. (b) Because 
x2
STAT = 2.624 6 3.841, do not reject H0. There is insufficient evidence 
to conclude that there is a relationship between a student’s gender and 
pizzeria selection. (c) Because x2
STAT = 4.956 6 5.991, do not reject 
H0. There is insufficient evidence to conclude that there is a relationship 
between price and pizzeria selection. (d) p@value = 0.0839. The prob-
ability of a sample that gives a test statistic equal to or greater than 4.956 
is 8.39% if the null hypothesis of no relationship between price and piz-
zeria selection is true.
12.58 (a) Because x2
STAT = 11.895 6 12.592, do not reject H0. There 
is not enough evidence to conclude that there is a relationship between 
the attitudes of employees toward the use of self-managed work teams 
and employee job classification. (b) Because x2
STAT = 3.294 6 12.592, 
do not reject H0. There is insufficient evidence to conclude that there is 
a relationship between the attitudes of employees toward vacation time 
without pay and employee job classification.
CHAPTER 13
13.2 (a) Yes. (b) No. (c) No. (d) Yes.
13.4 (a) The scatter plot shows a positive linear relationship.  
(b) For each increase in alcohol percentage of 1.0, mean ­predicted  
mean wine quality is estimated to increase by 0.5624.  
(c) Yn = -0.3529 + 0.5624X = -0.3529 + 0.5624(10) = 5.2715. 
(d) Wine quality appears to be affected by the alcohol percentage. Each 
increase of 1% in alcohol leads to a mean increase in wine quality of a 
­little more than half a unit.
13.6 (b) b0 = -2.37, b1 = 0.0501. (c) For every cubic foot increase in 
the amount moved, predicted mean labor hours are estimated to increase 
by 0.0501. (d) 22.67 labor hours. (e) That as expected, the labor hours are 
affected by the amount to be moved.
13.8 (b) b0 = -601.9291, b1 = 5.9316. (c) For each additional million-
dollar increase in revenue, the mean value is predicted to increase by an 
estimated $5.9316 million. Literal interpretation of b0 is not meaningful 
because an operating franchise cannot have zero revenue. (d) $880.9832 
million. (e) That the value of the franchise can be expected to increase as 
revenue increases.
13.10 (b) b0 = 4.8445, b1 = 0.1631. (c) For each increase of $1 ­million  
of box office gross, the predicted DVD revenue is estimated to 
increase by $0.1631 million. (d) Yn = b0 + b1X. Yn = 4.8445 +
0.163111002 = +21.1588 million. (e) You can conclude that the mean 
predicted increase in DVD sales is $163,100 for each million-dollar 
increase in movie gross.
13.12 r 2 = 0.90. 90% of the variation in the dependent variable can be 
explained by the variation in the independent variable.
13.14 r 2 = 0.75. 75% of the variation in the dependent variable can be 
explained by the variation in the independent variable.
12.26 Because x2
STAT = 6.6876 6 12.5916, do not reject H0. There is 
insufficient evidence of a relationship between consumer segment and 
geographic region.
12.28 (a) 31. (b) 29. (c) 27. (d) 25.
12.30 40 and 79.
12.32 (a) The ranks for Sample 1 are 1, 2, 4, 5, and 10. The ranks for 
Sample 2 are 3, 6.5, 6.5, 8, 9, and 11. (b) 22. (c) 44.
12.34 Because T1 = 22 7 20, do not reject H0.
12.36 (a) The data are ordinal. (b) The two-sample t test is ­inappropriate 
because the data can only be placed in ranked order. (c) Because 
ZSTAT = -2.2054 6 -1.96, reject H0. There is evidence of a 
­significance difference in the median rating of California Cabernets  
and Washington Cabernets.
12.38 (a) H0: M1 = M2, where Populations: 1 = Wing A, 2 = Wing B. 
H1: M1 ≠M2.
Population 1 sample: Sample size 20, sum of ranks 561
Population 2 sample: Sample size 20, sum of ranks 259
 mT1 =
n11n + 12
2
=
20140 + 12
2
= 410
 sT1 = B
n1n21n + 12
12
= B
201202140 + 12
12
= 36.9685
 ZSTAT =
T1 - mT1
ST1
= 561 - 410
36.9685
= 4.0846
Decision: Because ZSTAT = 4.0846 7 1.96 (or p@value = 0.0000
6 0.05), reject H0. There is sufficient evidence of a difference in the 
median delivery time in the two wings of the hotel.
(b) The results of (a) are consistent with the results of Problem 10.65.
12.40 (a) Because ZSTAT = 1.5965 6 1.96, do not reject H0. There is 
insufficient evidence to conclude that there is a difference in the median 
brand value between the two sectors. (b) You must assume approximately 
equal variability in the two populations. (c) Using the pooled-variance  
t test you rejected the null hypothesis and the separate-variance t test did 
not allow you to reject the null hypothesis and conclude in Problem 10.17 
that the mean brand value is different between the two sectors. In this test, 
using the Wilcoxon rank sum test with large-sample Z approximation did 
not allow you to reject the null hypothesis and conclude that the median 
brand value differs between the two sectors.
12.42 (a) Because -1.96 6 ZSTAT = 0.7245 6 1.96 (or the 
p@value = 0.4687 7 0.05), do not reject H0. There is not enough  
evidence to conclude that there is a difference in the median battery  
life between subcompact cameras and compact cameras. (b) You must 
assume approximately equal variability in the two populations.  
(c) Using the pooled-variance t-test, you do not reject the null hypothesis 
1t = -2.1199 6 -0.6181 6 2.1199; p@value = 0.5452 7 0.052 and 
conclude that there is insufficient evidence of a difference in the mean 
­battery life between the two types of digital cameras in Problem 10.11 (a).
12.44 (a) Decision rule: If H 7 x2
U = 15.086, reject H0. (b) Because 
H = 13.77 6 15.086, do not reject H0.
12.46 (a) H = 13.517 7 7.815, p@value = 0.0036 6 0.05, reject H0. 
There is sufficient evidence of a difference in the median waiting time in 
the four locations. (b) The results are consistent with those of Problem 11.9.
12.48 (a) H = 19.3269 7 9.488, reject H0. There is evidence of a 
­difference in the median ratings of the ads. (b) The results are consistent 

	
Self-Test Solutions and Answers to Selected Even-Numbered Problems	
817
13.38 (a) b0 = -2.535, b1 = 0.06073. (b) $2,505.40. (d) D = 1.64
7 dU = 1.42, so there is no evidence of positive autocorrelation among 
the residuals. (e) The plot shows some nonlinear pattern, suggesting that 
a nonlinear model might be better. Otherwise, the model appears to be 
adequate.
13.40 (a) 3.00. (b) {2.1199. (c) Reject H0. There is evidence that the fit-
ted linear regression model is useful. (d) 1.32 … b1 … 7.68.
13.42 (a) tSTAT = b1-b1
Sb1
= 0.5624
0.1127 = 4.9913 7 2.0106. Reject H0. 
There is evidence of a linear relationship between the ­percentage of 
­alcohol and wine quality.  
(b) b1 {  ta>2Sb1 = 0.5624 { 2.010610.11272 0.3359 … b1 … 0.7890.
13.44 (a) tSTAT = 16.52 7 2.0322; reject H0. There is evidence of a lin-
ear relationship between the number of cubic feet moved and labor hours. 
(b) 0.0439 … b1 … 0.0562.
13.46 (a) tSTAT = 11.3668 7 2.0484 or because the p-value is 0.0000, 
reject H0 at the 5% level of significance. There is evidence of a ­linear 
relationship between annual revenue and franchise value.  
(b) 4.8627 … b1 … 7.0006.
13.48 (a) tSTAT = 6.5626 7 2.0195 or because the p-value 
= 0.0000 6 0.05; reject H0. There is evidence of a linear relationship 
between box office gross and sales of DVDs. (b) 0.1129 … b1 … 0.2133.
13.50 (a) (% daily change in MDU) = b0 + 3.0 (% daily change in 
S&P Midcap 400 index). (b) If the S&P Midcap 400 gains 10% in a year, 
MDU is expected to gain an estimated 30%. (c) If the S&P Midcap 400 
loses 20% in a year, MDU is expected to lose an estimated 60%. (d) Risk 
takers will be attracted to leveraged funds, and risk-averse investors will 
stay away.
13.52 (a), (b) First weekend and U.S. gross: r = 0.7264, tSTAT =
2.5893 7 2.4469, p@value = 0.0413 6 0.05. reject H0. At the 0.05 
level of significance, there is evidence of a linear relationship between 
first weekend sales and U.S. gross. First weekend and worldwide gross: 
r = 0.8234, tSTAT = 3.5549 7 2.4469, p@value = 0.0120 6 0.05. 
reject H0. At the 0.05 level of significance, there is ­evidence 
of a linear relationship between first weekend sales and 
­worldwide gross. U.S. gross and worldwide gross: r = 0.9629,
tSTAT = 8.7456 7 2.4469, p@value = 0.0001 6 0.05. Reject H0. At 
the 0.05 level of significance, there is evidence of a linear relationship 
between U.S. gross and worldwide gross.
13.54 (a) r = 0.7042. There appears to be a moderate positive linear 
relationship between social media networking and the GDP per capita. 
(b) tSTAT = 4.3227, p@value = 0.0004 6 0.05. Reject H0. At the 0.05 
level of significance, there is a significant linear relationship between 
social media networking and the GDP per capita. (c) There appears to be 
a strong relationship.
13.56 (a) 15.95 … mYX = 4 … 18.05. (b) 14.651 … YX = 4 … 19.349.
13.58 (a) Yn = -0.3529 + (0.5624)(10) = 5.2715 Yn { ta>2SYX1hi
= 5.2715 { 2.010610.9369210.0249
4.9741 … mYX = 10 … 5.5690.
(b) Yn { ta>2SYX21 + hi
= 5.2715 { 2.010619,369221 + 0.0249
3.3645 … YX = 10 … 7.1786.
(c) Part (b) provides a prediction interval for the individual response 
given a specific value of the independent variable, and part (a) provides 
13.16 (a) r 2 = SSR
SST = 21.8677
64.0000 = 0.3417, 34.17% of the variation in 
wine quality can be explained by the variation in the percentage of 
­alcohol.
(b) SYX = A
SSE
n-2 = R
a
n
i = 1
1Yi-Yni22
n-2
= A
42.1323
48
= 0.9369.
(c) Based on (a) and (b), the model should be somewhat useful for 
­predicting wine quality.
13.18 (a) r 2 = 0.8892. 88.92% of the variation in labor hours can be 
explained by the variation in cubic feet moved. (b) SYX = 5.0314. 
(c) Based on (a) and (b), the model should be very useful for predicting 
the labor hours.
13.20 (a) r 2 = 0.8219, 82.19% of the variation in the value of a baseball 
franchise can be explained by the variation in its annual revenue. 
(b) SYX = 165.3106. (c) Based on (a) and (b), the model should be useful 
for predicting the value of a baseball franchise.
13.22 (a) r 2 = 0.5123, 51.23% of the variation in DVD revenue can be 
explained by the variation in box office gross. (b) SYX = 12.2279.  
The variation of DVD revenue around the prediction line is $12.2279 million. 
The typical difference between actual DVD revenue and the ­predicted 
DVD revenue using the regression equation is approximately  
$12.2279 million. (c) Based on (a) and (b), the model may not be useful 
for predicting DVD revenue. (d) Other variables that might explain the 
variation in DVD revenue could be the amount spent on advertising, the 
timing of the release of the DVDs, and the type of movie.
13.24 A residual analysis of the data indicates a pattern, with sizable 
clusters of consecutive residuals that are either all positive or all negative. 
This pattern indicates a violation of the assumption of linearity. A curvi-
linear model should be investigated.
13.26 There does not appear to be a pattern in the residual plot. The 
assumptions of regression do not appear to be seriously violated.
13.28 Based on the residual plot, there does not appear to be a curvilinear 
pattern in the residuals. The assumptions of normality and equal variance 
do not appear to be seriously violated.
13.30 Based on the residual plot, there appears to be an outlier in the 
residuals, but no evidence of a pattern.
13.32 (a) An increasing linear relationship exists. (b) There is evidence of 
a strong positive autocorrelation among the residuals.
13.34 (a) No, because the data were not collected over time. (b) If data 
were collected at a single store had been selected and studied over a 
period of time, you would compute the Durbin-Watson statistic.
13.36 (a)
b1 = SSXY
SSX = 201,399.05
12,495,626 = 0.0161
b0 = Y - b1X = 71.2621 - 0.0161 14,3932 = 0.458.
(b) Yn = 0.458 + 0.0161X = 0.458 + 0.016114,5002 = 72.908, or 
$72,908. (c) There is no evidence of a pattern in the residuals over time.
(d) D =
a
n
i = 2
 1ei- ei - 122
a
n
i = 1
 e2
i
= 1,243.2244
599.0683
= 2.08 7 1.45. There is no
evidence of positive autocorrelation among the residuals. (e) Based on a 
residual analysis, the model appears to be adequate.

818	
Self-Test Solutions and Answers to Selected Even-Numbered Problems
because an operating franchise cannot have zero ­revenue.  
(c) $651.7731 million. (d) r 2 = 0.836. 83.6% of the variation in the 
value of an NBA franchise can be explained by the variation in its  
annual revenue. (e) There does not appear to be a pattern in the residual 
plot. The assumptions of regression do not appear to be seriously 
­violated. (f) tSTAT = 11.9452 7 2.0484 or because the p-value  
is 0.0000, reject H0 at the 5% level of significance. There is evidence  
of a linear relationship between annual revenue and franchise value.  
(g) 613.6103 … mYX = 150 … 689.9359. (h) 486.9282 … YX = 150 …
816.618. (i) The strength of the relationship between revenue and value 
is about the same for NBA franchises, European soccer teams, and Major 
League Baseball teams.
13.84 (a) b0 = -2,629.222, b1 = 82.472. (b) For each additional 
­centimeter in circumference, the weight is estimated to increase by 
82.472 grams. (c) 2,319.08 grams. (d) Yes, since circumference is a very 
strong predictor of weight. (e) r 2 = 0.937. (f) There appears to be a 
nonlinear relationship between circumference and weight. (g) p-value is 
virtually 0 6 0.05; reject H0. (h) 72.7875 … b1 … 92.156.
13.86 (a) The correlation between compensation and stock performance 
is 0.1719. (b) tSTAT = 2.2615 7 1.9742; p@value =  0.025 6 0.05. The 
correlation between compensation and stock performance is ­significant, 
but only 2.95% of the variation in compensation can be explained 
by return. (c) The small correlation between compensation and stock 
­performance was surprising (or maybe it shouldn’t have been!).
CHAPTER 14
14.2 (a) Income and need are independent variables and demand is a 
dependent variable. Thus, given the values of income and need, you 
can estimate the demand. When income and need change, the demand 
changes. (b) Demand = 1.5 + 0.7102 + 0.5112 = 2. This situation 
shows that even given no income individuals have some basic needs.
14.4 (a) Yn = -2.72825 + 0.047114X1 + 0.011947X2. (b) For a  
given number of orders, for each increase of $1,000 in sales, the mean 
distribution cost is estimated to increase by $47.114. For a given amount 
of sales, for each increase of one order, the mean distribution cost is  
estimated to increase by $11.95. (c) The interpretation of b0 has no  
practical meaning here because it would represent the estimated  
distribution cost when there were no sales and no orders.  
(d) Yn = -2.72825 + 0.04711414002 + 0.011947145002 = 69.878, or 
$69,878. (e) +66,419.93 … mYX … +73,337.01. (f) +59,380.61 … YX …
+80,376.33 (g) The interval in (e) is narrower because it is estimating the 
mean value, not an individual value. (h) The model uses both the number 
of orders and the amount of sales to predict warehouse distribution cost. 
This may produce a better model than if only one of these independent 
variables is included.
14.6 (a) Yn = 156.4 + 13.081X1 + 16.795X2. (b) For a given amount 
of newspaper advertising, each increase by $1,000 in radio advertis-
ing is estimated to result in an increase in mean sales of $13,081. For a 
given amount of radio advertising, each increase by $1,000 in newspaper 
­advertising is estimated to result in an increase in mean sales of $16,795. 
(c) When there is no money spent on radio advertising and newspaper 
advertising, the estimated mean sales is $156,430.44. (d) Holding the 
other independent variable constant, newspaper advertising seems to be 
more effective because its slope is greater.
14.8 (a) Yn = 532.2883 + 407.1346X1 - 2.8257X2, where 
X1 = land area,  X2 = age. (b) For a given age, each increase by one 
acre in land area is estimated to result in an increase in the mean fair 
­market value by $407.1346 thousands. For a given land area, each 
an interval estimate for the mean value, given a specific value of the inde-
pendent variable. Because there is much more variation in predicting an 
individual value than in estimating a mean value, a prediction interval is 
wider than a confidence interval estimate.
13.60 (a) 20.799 … mYX = 500 … 24.542. (b) 12.276 … YX = 500 … 33.065. 
(c) You can estimate a mean more precisely than you can predict a single 
observation.
13.62 (a) 814.3841 … mYX = 250 … 947.5823. (b) 535.8727 … YX = 250
… 1,226.094. (c) Part (b) provides a prediction interval for an individual 
response given a specific value of X, and part (a) provides a confidence 
interval estimate for the mean value, given a specific value of X. Because 
there is much more variation in predicting an individual value than in 
estimating a mean, the prediction interval is wider than the confidence 
interval.
13.74 (a) b0 = 24.84, b1 = 0.14. (b) For each additional case, the 
­predicted delivery time is estimated to increase by 0.14 minute.  
(c) 45.84. (d) No, 500 is outside the relevant range of the data used to fit 
the regression equation. (e) r 2 = 0.972. (f) There is no obvious pattern 
in the residuals, so the assumptions of regression are met. The model 
appears to be adequate. (g) tSTAT = 24.88 7 2.1009; reject H0.  
(h) 44.88 … mYX = 150 … 46.80. 41.56 … YX = 150 … 50.12. (i) The 
­number of cases explains almost all of the variation in delivery time.
13.76 (a) b0 = 276.848, b1 = 50.8031. (b) For each additional 1,000 
square feet in the size of the house, the mean assessed value is ­ 
predicted to increase by $50,803.10. The estimated selling price of a 
house with a 0 size is +276,848 thousand. However, this ­interpretation  
is not meaningful because the size of the house cannot be 0.  
(c) Yn = 276.848 + 50.8031(2) = 378.4542 thousand dollars.  
(d) r 2 = 0.3273. So 32.73% of the variation in assessed value be 
explained by the variation in size. (e) Neither the residual plot nor the 
normal probability plot reveals any potential violation of the linearity, 
equal variance, and normality assumptions. (f) tSTAT = 3.6913 7 2.0484, 
p-value is 0.0009. Because p@value 6 0.05, reject H0. There is evidence 
of a linear relationship between assessed value and size.  
(g) 22.6113 … b1 … 78.9949. (h) The size of the house is somewhat 
useful in predicting the assessed value, but since only 32.73% of the vari-
ation in assessed value is explained by variation in size, other variables 
should be considered.
13.78 (a) b0 = 0.30, b1 = 0.00487. (b) For each additional point on the 
GMAT score, the predicted GPA is estimated to increase by 0.00487. 
Because a GMAT score of 0 is not possible, the Y intercept does not 
have a practical interpretation. (c) 3.222. (d) r 2 = 0.798. (e) There is 
no obvious pattern in the residuals, so the assumptions of regression 
are met. The model appears to be adequate. (f) tSTAT = 8.43 7 2.1009; 
reject H0. (g) 3.144 … mYX = 600 … 3.301, 2.866 … YX = 600 … 3.559. 
(h) .00366 … b1 … .00608. (i) Most of the variation in GPA can be 
explained by variation in the GMAT score.
13.80 (a) There is no clear relationship shown on the scatter plot.  
(c) Looking at all 23 flights, when the temperature is lower, there is 
likely to be some O-ring damage, particularly if the temperature is below 
60 degrees. (d) 31 degrees is outside the relevant range, so a predic-
tion should not be made. (e) Predicted Y = 18.036-0.240X, where 
X = temperature and Y = O-ring damage. (g) A nonlinear model would 
be more appropriate. (h) The appearance on the residual plot of a non-
linear pattern indicates that a nonlinear model would be better. It also 
appears that the normality assumption is invalid.
13.82 (a) b0 = -132.5214, b1 = 5.2286. (b) For each additional 
million-dollar increase in revenue, the franchise value will increase by an 
estimated $5.2286 million. Literal interpretation of b0 is not meaningful 

	
Self-Test Solutions and Answers to Selected Even-Numbered Problems	
819
(radio advertising) and X2 (newspaper advertising) should be included  
in the model.
14.30 (a) 274.1702 … b1 … 540.0990. (b) For X1 : tSTAT = 6.2827 
and p@value = 0.0000. Because p@value 6 0.05, reject H0. There is 
evidence that X1 contributes to a model already containing X2. For 
X2 : tSTAT = -4.1475 and p@value = 0.0003. Because p@value 6 0.05 
reject H0. There is evidence that X2 contributes to a model already 
­containing X1: FSTAT =  30.4533 p@value = 0.0000. Both X1 (land area) 
and X2 (age) should be included in the model.
14.32 (a) For X1: FSTAT = 1.25 6 4.96; do not reject H0. For 
X2: FSTAT = 0.833 6 4.96; do not reject H0. (b) 0.1111, 0.0769.
14.34 (a) For X1: SSR1X1X22 = SSR 1X1 and X22 - SSR1X22 =
3,368.087 - 3,246.062 = 122.025, FSTAT =
SSR1X1X22
MSE
=
122.025
477.043>21 = 5.37 7 4.325. Reject H0. There is evidence that 
X1 contributes to a model already containing X2. For  
X2: SSR1X2X12 = SSR 1X1 and X22 - SSR1X12 = 3,368.087 - 2,726.822
= 641.265, FSTAT =
SSR1X2X12
MSE
=
641.265
477.043>21 = 28.23 7 4.325.
Reject H0. There is evidence that X2 contributes to a model already 
­containing X1. Because both X1 and X2 make a significant contribution to 
the model in the presence of the other variable, both variables should be 
included in the model.
(b) r 2
Y1.2 =
SSR1X1X22
SST - SSR1X1 and X22 + SSR1X1X22
=
122.025
3,845.13 - 3,368.087 + 122.025 = 0.2037.
Holding constant the effect of the number of orders, 20.37% of the 
­variation in distribution cost can be explained by the variation in sales.
 r 2
Y2.1 =
SSR1X2X12
SST - SSR1X1 and X22 + SSR1X2X12
 =
641.265
3,845.13 - 3,368.087 + 641.265 = 0.5734
Holding constant the effect of sales, 57.34% of the variation in 
­distribution cost can be explained by the variation in the number  
of orders.
14.36 (a) For X1: FSTAT = 55.28 7 4.381. Reject H0. There 
is evidence that X1 contributes to a model containing X2. For 
X2: FSTAT = 32.12 7 4.381. Reject H0. There is evidence that X2 con-
tributes to a model already containing X1. Because both X1 and X2 make 
a significant contribution to the model in the presence of the other vari-
able, both variables should be included in the model. (b) r 2
Y1.2 = 0.7442. 
Holding constant the effect of newspaper advertising, 74.42% of the 
variation in sales can be explained by the variation in radio advertising. 
r 2
Y2.1 = 0.6283. Holding constant the effect of radio advertising, 62.83% 
of the variation in sales can be explained by the variation in newspaper 
advertising.
14.38 (a) To incorporate the effect of a categorical variable, dummy vari-
ables are used so that female = 1, male = 0 and similarly for education. 
(b) Wagei = 200 + 40 experiencei + 70 femalei + 20 educationi 
Wagei = 200 + 40*5 + 70*1 + 20*5 = 200 + 200 + 70 + 100 = 
570 (c) There can be instances of interaction in the sense that better edu-
cated employees have more experience.
increase of one year in age is estimated to result in a decrease in the mean 
fair ­market value by $2.8257 thousands.
(c) The interpretation of b0 has no practical meaning here because 
it would represent the estimated fair market value of a new house 
that has no land area. (d) Yn = 5,332.2883 + 407.134610.252
-2.82571552 = +478.6577 thousands. (e) 446.8367 … mYX … 510.4788. 
(f) 307.2577 … YX … 650.0577.
14.10 (a) MSR = 15, MSE = 12. (b) 1.25. (c) FSTAT = 1.25 6 4.10; do 
not reject H0. (d) 0.20. (e) 0.04.
14.12 (a) FSTAT = 97.69 7 3.89. Reject H0. There is evidence of 
a ­significant linear relationship with at least one of the independ-
ent ­variables. (b) p@value = 0.0001. (c) r 2 = 0.9421. 94.21% of the 
­variation in the long-term ability to absorb shock can be explained by 
variation in forefoot-absorbing capability and variation in midsole impact. 
(d) r 2
adj = 0.935.
14.14 (a) FSTAT = 74.13 7 3.467; reject H0. (b) p@value = 0.  
(c) r 2 = 0.8759. 87.59% of the variation in distribution cost can be 
explained by variation in sales and variation in number of orders.  
(d) r 2
adj = 0.8641.
14.16 (a) FSTAT = 40.16 7 3.522. Reject H0. There is evidence of a 
significant linear relationship. (b) p@value 6 0.001. (c) r 2 = 0.8087. 
80.87% of the variation in sales can be explained by variation in radio 
advertising and variation in newspaper advertising. (d) r 2
adj = 0.7886.
14.18 (a)–(e) Based on a residual analysis, there is no evidence of  
a violation of the assumptions of regression. (f) D = 2.26  
(g) D = 2.26 7 1.55. There is no evidence of positive  
autocorrelation in the residuals.
14.20 (a) There appears to be a quadratic relationship in the plot of the 
residuals against both radio and newspaper advertising. (b) Since the data 
are not collected over time, the Durbin-Watson test is not appropriate.  
(c) Curvilinear terms for both of these explanatory variables should be 
considered for inclusion in the model.
14.22 (a) The residual analysis reveals no patterns. (b) Since the data  
are not collected over time, the Durbin-Watson test is not appropriate.  
(c) There are no apparent violations in the assumptions.
14.24 (a) Variable X2 has a larger slope in terms of the t ­statistic 
of 3.75 than variable X1, which has a smaller slope in terms of 
the t statistic of 3.33. (b) 1.46824 … b1 … 6.53176. (c) For 
X1 :  tSTAT = 4>1.2 = 3.33 7 2.1098, with 17 degrees of freedom for 
a = 0.05. Reject H0. There is evidence that X1 contributes to a model 
already containing X2. For X2 : tSTAT = 3>0.8 = 3.75 7 2.1098, with  
17 degrees of freedom for a = 0.05. Reject H0. There is evidence  
that X2 contributes to a model already containing X1. Both X1 and X2 
should be included in the model.
14.26 (a) 95% confidence interval on b1 : b1 { tSb1, 0.0471 { 2.0796 
10.02032, 0.0049 … b1 … 0.0893. (b) For X1 : tSTAT = b1>Sb1 =  
0.0471>0.0203 = 2.32 7 2.0796. Reject H0. There is evidence that X1 
contributes to a model already containing X2. For X2 : tSTAT = b1>Sb1 =  
0.0112>0.0023 = 5.31 7 2.0796. Reject H0. There is evidence that X2 
contributes to a model already containing X1. Both X1 (sales) and X2 
(orders) should be included in the model.
14.28 (a) 9.398 … b1 … 16.763. (b) For X1 : tSTAT = 7.43 7 2.093. 
Reject H0. There is evidence that X1 contributes to a model already 
­containing X2. For X2 : tSTAT = 5.67 7 2.093. Reject H0. There is 
­evidence that X2 contributes to a model already containing X1. Both X1 

820	
Self-Test Solutions and Answers to Selected Even-Numbered Problems
between total staff present and remote hours, the model in Problem 14.7 
should be used.
14.50 Holding constant the effect of other variables, the natural ­logarithm 
of the estimated odds ratio for the dependent categorical response will 
increase by 2.2 for each unit increase in the particular independent 
­variable.
14.52 0.4286.
14.54 (a) ln1estimated odds ratio2 = -6.9394 + 0.1395X1 +
2.7743X2 = -6.9394 + 0.13951362 + 2.7743102 = -1.91908. 
Estimated odds ratio = e-1.91908 = 0.1470. Estimated Probability of 
Success = Odds Ratio>11 + Odds Ratio2 = 0.1470>11 + 0.14702 =
0.1260. (b) From the text discussion of the example, 70.2% of the 
individuals who charge $36,000 per annum and possess additional 
cards can be expected to purchase the premium card. Only 12.60% 
of the individuals who charge $36,000 per annum and do not pos-
sess additional cards can be expected to purchase the premium card. 
For a given amount of money charged per annum, the likelihood 
of purchasing a premium card is substantially higher among indi-
viduals who already possess additional cards than for those who 
do not possess additional cards. (c) ln(estimated odds ratio) =
-6.9394 + 0.13957X1 + 2.7743X2 = -6.9394 + 0.13951182 +
2.7743102 = -4.4298. Estimated odds ratio = e-4.4298 = 0.0119. 
Estimated Probability of Success = Odds Ratio>11 + Odds Ratio2 =
0.0119>11 + 0.01192 = 0.01178. (d) Among individuals who do not 
purchase additional cards, the likelihood of purchasing a premium card 
diminishes dramatically with a substantial decrease in the amount charged 
per annum.
14.56 Using Microsoft Excel (a) ln(estimated odds) = -47.4723 +
1.3099 fixed acidity + 90.5722 chlorides + 9.777 pH. (b) Holding 
constant the effect of chlorides and pH, for each increase of one point 
in fixed acidity, ln(estimated odds) increases by an estimate of 1.3099. 
Holding constant the effect of fixed acidity and pH, for each increase 
of one point in chlorides, ln(estimated odds) increases by an estimate 
of 90.5722. Holding constant the effect of fixed acidity and chlorides, 
for each increase of one point in pH, ln(estimated odds) increases by an 
estimate of 9.777. (c) 0.3686. (d) Deviance = 54.456, p@value = 1.0000, 
do not reject H0, so model is adequate. (e) For fixed acidity: 
ZSTAT = 3.17 7 1.96, reject H0. For chlorides: ZSTAT = 4.00 7 1.96, 
reject H0. For pH: ZSTAT = 2.9738 7 1.96, reject H0. Each variable 
makes a significant contribution to the model. (f) Fixed acidity, chlorides, 
and pH are all important factors in distinguishing between white and red 
wines.
14.58 (a) Using Microsoft Excel (a) ln1estimated odds2 = -0.6048 +
0.0938 claims>year + 1.8108 new business (b) Holding constant the 
effects of whether the policy is new, for each increase of the number of 
claims submitted per year by the policy holder, ln(odds) increases by an 
estimate of 0.0938. Holding constant the number of claims submitted per 
year by the policy holder, ln(odds) is estimated to be 1.8108 higher when 
the policy is new as compared to when the policy is not new.  
(c) ln1estimated odds ratio2 = -0.6048 + 0.0938112 + 1.8108112 =
1.2998. Estimated odds ratio = e1.2998 = 3.6684 Estimated
Probability of the Event of Interest = Estimated Odds Ratio> 
11 + Estimated Odds Ratio2 = 0.7858 (d) The deviance statistic is 
119.4353 with a x2 distribution of 95 d.f. and p@value = 0.0457 6 0.05. 
Reject H0. The model is not a good fitting model. (e) For claims/year: 
ZSTAT = 0.1865, p@value = 0.8521 7 0.05. Do not reject  
H0. There is insufficient evidence that the number of claims 
­submitted per year by the policy holder makes a significant 
contribution to the logistic regression model. For new busi-
ness: ZSTAT = 2.2261, p@value = 0.0260 6 0.05. Reject H0. 
14.40 (a) Yn = 243.7371 + 9.2189X1 + 12.6967X2, where X1 = number  
of rooms and X2 = neighborhood 1east = 02. (b) Holding 
­constant the effect of neighborhood, for each additional room, the 
mean ­selling price is estimated to increase by 9.2189 ­thousands 
of ­dollars, or $9,218.9. For a given number of rooms, a west 
­neighborhood is estimated to increase the mean selling price over an 
east neighborhood by 12.6967 thousands of dollars, or $12,696.7. 
(c) Yn = 243.7371 + 9.2189192 + 12.6967102 = 326.7076, or 
+326, 707.6. +309,560.04 … YX … 343,855.1. +321,471.44 …  
mYX … +331, 943.71. (d) Based on a residual analysis, the model 
appears to be adequate. (e) FSTAT = 55.39, the p-value is ­virtually 
0. Because p@value 6 0.05, reject H0. There is evidence of a sig-
nificant relationship between selling price and the two independ-
ent variables (rooms and neighborhood). (f) For X1: tSTAT = 8.9537, 
the p-value is virtually 0. Reject H0. Number of rooms makes a 
significant contribution and should be included in the model. For 
X2: tSTAT = 3.5913, p@value = 0.0023 6 0.05. Reject H0. Neighborhood 
makes a significant contribution and should be included in the model. 
Based on these results, the regression model with the two ­independent 
variables should be used. (g) 7.0466 … b1 … 11.3913.  
(h) 5.2378 … b2 … 20.1557. (i) r 2
adj = 0.851. (j) r 2
Y1.2 = 0.825. Holding 
constant the effect of neighborhood, 82.5% of the variation in selling 
price can be explained by variation in number of rooms. r 2
Y2.1 = 0.431. 
Holding constant the effect of number of rooms, 43.1% of the variation  
in selling price can be explained by variation in neighborhood.  
(k) The slope of selling price with number of rooms is the same, 
­regardless of whether the house is located in an east or west neigh-
borhood. (l) Yn = 253.95 + 8.032X1 - 5.90X2 + 2.089X1X2. For 
X1 X2,  p@value =  0.330. Do not reject H0. There is no evidence that the 
interaction term makes a contribution to the model. (m) The model in (b) 
should be used. (n) The number of rooms and the neighborhood both sig-
nificantly affect the selling price, but the number of rooms has a greater 
effect.
14.42 (a) Predicted time = 8.01 + 0.00523 Depth - 2.105 Dry.  
(b) Holding constant the effect of type of drilling, for each foot increase 
in depth of the hole, the mean drilling time is estimated to increase by 
0.00523 minutes. For a given depth, a dry drilling hole is estimated to 
reduce the drilling time over wet drilling by a mean of 2.1052 minutes. 
(c) 6.428 minutes, 6.210 … mYX … 6.646, 4.923 … YX … 7.932.  
(d) The model appears to be adequate. (e) FSTAT = 111.11 7 3.09; reject 
H0. (f) tSTAT = 5.03 7 1.9847; reject H0. tSTAT = -14.03 6 -1.9847; 
reject H0. Include both variables. (g) 0.0032 … b1 … 0.0073. (h) 
-2.403 … b2 … -1.808. (i) 69.0%. (j) 0.207, 0.670. (k) The slope of 
the additional drilling time with the depth of the hole is the same, regard-
less of the type of drilling method used. (l) The p-value of the interaction 
term = 0.462 7 0.05, so the term is not significant and should not be 
included in the model. (m) The model in part (b) should be used. Both 
variables affect the drilling time. Dry drilling holes should be used to 
reduce the drilling time.
14.44 (a) Yn = 31.5594 + 0.0296X1 + 0.0041X2 + 0.000017159X1X2, 
where X1 = sales, X2 = orders, p@value = 0.3249 7 0.05. Do not 
reject H0. There is not enough evidence that the interaction term makes 
a ­contribution to the model. (b) Because there is insufficient evidence of 
any interaction effect between sales and orders, the model in  
Problem 14.4 should be used.
14.46 (a) The p-value of the interaction term = 0.002 6 0.05, so the 
term is significant and should be included in the model. (b) Use the 
model developed in this problem.
14.48 (a) For X1 X2, p@value = 0.2353 7 0.05. Do not reject H0. There is 
insufficient evidence that the interaction term makes a contribution to the 
model. (b) Because there is not enough evidence of an interaction effect 

	
Self-Test Solutions and Answers to Selected Even-Numbered Problems	
821
furniture. (l) r 2
Y1.2 = 0.5930. Holding constant the effect of the number 
of pieces of large furniture, 59.3% of the variation in labor hours can be 
explained by variation in the amount of cubic feet moved. r 2
Y2.1 = 0.3927. 
Holding constant the effect of the number of cubic feet moved, 39.27% of 
the variation in labor hours can be explained by variation in the number 
of pieces of large furniture. (m) Both the number of cubic feet moved and 
the number of large pieces of furniture are useful in predicting the labor 
hours, but the cubic feet removed is more important.
14.78 (a) Yn = 257.9033 + 53.3606X1 + 0.2521X2, where 
X1 = house size and X2 = age. (b) Holding constant the age, for 
each additional thousand square feet in the size of the house, the mean 
assessed value is estimated to increase by 53.3606 thousand dollars. 
Holding constant the size of the house, for each additional year in age, 
the assessed value is estimated to increase by 0.2521 thousand dol-
lars. (c) Yn = 257.9033 + 53.3606122 + 0.25211552 = 378.4093 
thousand dollars. (d) Based on a residual analysis, the model appears 
to be adequate. (e) FSTAT = 6.6459, the p@value = 0.0045. Because 
p@value 6 0.05, reject H0. There is evidence of a significant relation-
ship between assessed value and the two independent variables (size 
of the house and age). (f) The p-value is 0.0045. The probability of 
obtaining a test statistic of 6.6459 or greater is virtually 0 if there is no 
significant relationship between assessed value and the two independ-
ent variables (size of the house and age). (g) r 2 = 0.3299. 32.99% of 
the variation in assessed value can be explained by variation in the size 
of the house and age. (h) r 2
adj = 0.2803. (i) For X1: tSTAT = 3.3128, 
the p-value is 0.0026. Reject H0. The size of the house makes a 
significant contribution and should be included in the model. For 
X2: tSTAT = 0.3203, p@value = 0.7512 7 0.05. Do not reject H0. Age 
does not make a significant contribution and should not be included in the 
model. Based on these results, the regression model with only the size of 
the house should be used. (j) For X1: tSTAT = 3.3128, the p-value is virtu-
ally 0. The probability of obtaining a sample that will yield a test statistic 
farther away than 3.3128 is 0.0026 if the house size does not make a 
significant contribution, holding age constant. For X2: tSTAT = 0.3203, the 
p-value is 0.7512. The probability of obtaining a sample that will yield a 
test ­statistic farther away than 0.3203 is 0.7512 if the age does not make  
a significant contribution holding the effect of the house size constant.  
(k) 20.3109 … b1 … 86.4104. You are 95% confident that the assessed 
value will increase by an amount somewhere between $20.3109 thousand 
and $86.4104 thousand for each additional thousand square foot increase 
in house size, holding constant the age of the house. In Problem 13.76, 
you are 95% confident that the assessed value will increase by an amount 
somewhere between $22.6113 thousand and $78.9949 thousand for each 
additional 1,000 square foot increase in house size, regardless of the age 
of the house. (l) r 2
Y1.2 = 0.2890. Holding constant the effect of the age of 
the house, 28.9% of the variation in assessed value can be explained by 
variation in the size of the house. r 2
Y2.1 = 0.0038. Holding constant the 
effect of the size of the house, 0.38% of the variation in assessed value 
can be explained by variation in the age of the house.
14.80 (a) Yn = 694.9557 + 8.6059X1 + 2069X2, where X1 =
assessed value and X2 = age. (b) Holding age constant, for each addi-
tional $1,000, the taxes are estimated to increase by a mean of $8.61 
thousand. Holding assessed value constant, for each additional year, the 
taxes are estimated to increase by $2.069
(c) Yn = 694.9557 + 8.605914002 + 2.0691502 = 4,240.542  
­dollars. (d) Based on a residual analysis, the errors appear to be  
normally distributed. The equal-variance assumption appears 
to be valid. (e) FSTAT = 22.0699, p@value = 0.0000. Because 
p@value = 0.0000 6 0.05, reject H0. There is evidence of a significant 
relationship between taxes and the two independent variables (assessed 
value and age). (f) p@value = 0.0000. The probability of obtaining 
an FSTAT test statistic of 22.0699 or greater is virtually 0 if there is no 
There is sufficient evidence that whether the policy is new 
makes a significant contribution to the logistic model regres-
sion. (f) ln1estimated odds2 = -1.0125 + 0.9927 claims>year 
(g) ln1estimated odds2 = -0.5423 + 1.9286 new business (h) The 
deviance statistic for (f) is 125.0102 with a x2 distribution of 96 d.f. and 
p@value = 0.0250 6 0.05. Reject H0. The model is not a good fitting 
model. The deviance statistic for (g) is 119.4702 with a x2 distribution of 
96 d.f. and p@value = 0.0526 7 0.05. Do not reject H0. The model is a 
good fitting model. The model in (g) should be used to predict a fraudu-
lent claim.
14.60 Largest Cook’s D = 0.5637 6 0.8149 so no need for deletion of 
any cases.
14.62 Largest Cook’s D = 0 .652 6 0.8177 for observation 2. hi for this 
observation = 0.2924 7 0.2727 and ti = 2.4431 7 1.7291, so you may 
wish to delete observation 2 and determine if that affects the fit of the 
model.
14.64 Largest Cook’s D = 0.5344 6 0.8149 so no need for deletion of 
any cases.
14.76 (a) Yn = -3.9152 + 0.0319X1 + 4.2228X2, where X1 =  ­ 
number cubic feet moved and X2 = number of pieces of large ­ 
furniture. (b) Holding constant the number of pieces of large  
furniture, for each additional cubic foot moved, the mean labor  
hours are estimated to increase by 0.0319. Holding constant the  
amount of cubic feet moved, for each additional piece of large  
furniture, the mean labor hours are estimated to increase by 4.2228.  
(c) Yn = -3.9152 + 0.031915002 + 4.2228 122 = 20.4926. (d) Based 
on a residual analysis, the errors appear to be normally distributed. 
The equal-variance assumption might be violated because the vari-
ances appear to be larger around the center region of both independent 
­variables. There might also be violation of the linearity assumption. A 
model with quadratic terms for both independent variables might be fit-
ted. (e) FSTAT = 228.80, p-value is virtually 0. Because p@value 6 0.05, 
reject H0. There is evidence of a significant relationship between labor 
hours and the two independent variables (the amount of cubic feet moved 
and the number of pieces of large furniture). (f) The p-value is virtually 0. 
The probability of obtaining a test statistic of 228.80 or greater is virtu-
ally 0 if there is no significant relationship between labor hours and the 
two independent variables (the amount of cubic feet moved and the num-
ber of pieces of large furniture). (g) r 2 = 0.9327.93.27% of the variation 
in labor hours can be explained by variation in the number of cubic feet 
moved and the number of pieces of large furniture. (h) r 2
adj = 0.9287.  
(i) For X1: tSTAT = 6.9339, the p-value is virtually 0. Reject H0. The 
­number of cubic feet moved makes a significant contribution and should 
be included in the model. For X2: tSTAT = 4.6192, the p-value is virtually 0.  
Reject H0. The number of pieces of large furniture makes a significant 
contribution and should be  included in the model. Based on these results, 
the regression model with the two independent variables should be used. 
(j) For X1: tSTAT = 6.9339, the p-value is virtually 0. The probability of 
obtaining a sample that will yield a test statistic farther away than 6.9339 
is virtually 0 if the number of cubic feet moved does not make a sig-
nificant contribution, holding the effect of the number of pieces of large 
furniture constant. For X2: tSTAT = 4.6192, the p-value is virtually 0. The 
probability of obtaining a sample that will yield a test statistic farther 
away than 4.6192 is virtually 0 if the number of pieces of large furniture 
does not make a significant contribution, holding the effect of the amount 
of cubic feet moved constant. (k) 0.0226 … b1 … 0.0413. You are 95% 
confident that the mean labor hours will increase by between 0.0226 and 
0.0413 for each additional cubic foot moved, holding constant the number 
of pieces of large furniture. In Problem 13.44, you are 95% confident 
that the labor hours will increase by between 0.0439 and 0.0562 for each 
additional cubic foot moved, regardless of the number of pieces of large 

822	
Self-Test Solutions and Answers to Selected Even-Numbered Problems
5% level of significance, there is not enough evidence to conclude that 
pressure and/or temperature affect thickness. The p-value of the t test for 
the significance of pressure is 0.8307 7 0.05. Hence, there is insufficient 
evidence to conclude that pressure affects thickness, holding constant the 
effect of temperature. The p-value of the t test for the significance of tem-
perature is 0.0820, which is also 7 0.05. There is insufficient evidence 
to conclude that temperature affects thickness at the 5% level of signifi-
cance, holding constant the effect of pressure. Hence, neither pressure nor 
temperature affects thickness individually.
The normal probability plot does not suggest any potential violation 
of the normality assumption. The residual plots do not indicate potential 
violation of the equal variance assumption. The temperature residual plot, 
however, suggests that there might be a nonlinear relationship between 
temperature and thickness.
The r 2 of the multiple regression model that includes the interaction 
of pressure and temperature is very low, at 0.0734. Only 7.34% of the vari-
ation in thickness can be explained by the variation of pressure, tempera-
ture, and the interaction of the two. The F test statistic for the model that 
includes pressure and temperature and their interaction is 1.214, with a 
p-value of 0.3153. Hence, at a 5% level of significance, there is insufficient 
evidence to conclude that pressure, temperature, and the interaction of the 
two affect thickness. The p-value of the t test for the significance of pres-
sure, temperature, and the interaction term are 0.5074, 0.4053, and 0.5111, 
respectively, which are all greater than 5%. Hence, there is insufficient evi-
dence to conclude that pressure, temperature, or the interaction individually 
affects thickness, holding constant the effect of the other variables.
The pattern in the normal probability plot and residual plots is simi-
lar to that in the regression without the interaction term. Hence the arti-
cle’s suggestion that there is a significant interaction between the pressure 
and the temperature in the tank cannot be validated.
14.86 b0 = 18.2892 (die temperature), b1 = 0.5976, (die diameter), 
b2 = -13.5108. The r 2 of the multiple regression is 0.3257 so 32.57% 
of the variation in unit density can be explained by the variation of die 
temperature and die diameter. The F test statistic for the combined sig-
nificance of die temperature and die diameter is 5.0718 with a p-value of 
0.0160. Hence, at a 5% level of significance, there is enough evidence to 
conclude that die temperature and die diameter affect unit density. The 
p-value of the t test for the significance of die temperature is 0.2117, 
which is greater than 5%. Hence, there is insufficient evidence to con-
clude that die temperature affects unit density holding constant the effect 
of die diameter. The p-value of the t test for the significance of die diame-
ter is 0.0083, which is less than 5%.There is enough evidence to conclude 
that die diameter affects unit density at the 5% level of significance hold-
ing constant the effect of die temperature. After removing die temperature 
from the model, b0 = 107.9267 (die diameter), b1 = -13.5108. The r 2 
of the multiple regression is 0.2724. So 27.24% of the variation in unit 
density can be explained by the variation of die diameter. The p-value of 
the t test for the significance of die diameter is 0.0087, which is less than 
5%. There is enough evidence to conclude that die diameter affects unit 
density at the 5% level of significance. There is some lack of equality in 
the residuals and some departure from normality. None of the observa-
tions have a Cook’s Di 7 Fa = 0.8002 with df = 2 and 22. Hence, 
using the Studentized deleted residuals, hat matrix diagonal elements, 
and Cook’s distance statistic together, there is insufficient evidence for 
removal of any observation from the model.
CHAPTER 15
15.2 (a) Predicted feelings are -2.28, 4.52, 10.12, 14.52, 17.72, 19.72, 
20.52, 20.12, 18.52, 15.72, 11.72, 6.52, 0.12, -7.48, -16.28, and -26.28 
(b) The graph is curvilinear, which shows the feelings first increase 
with increase in height, and after a certain level, they start decreasing. 
­significant relationship between taxes and the two independent variables 
(assessed value and age). (g) r 2 = 0.6205. 62.05% of the variation in 
taxes can be explained by variation in assessed value and age.  
(h) r 2
adj = 0.5924. (i) For X1: tSTAT = 6.5271, p@value = 0.0000 6 0.05. 
Reject H0. The assessed value makes a significant contribu-
tion and should be included in the model. For X2: tSTAT = 0.3617, 
p@value = 0.7204 7 0.05. Do not reject H0. The age of a house does not 
make a significant contribution and should not be included in the model. 
Based on these results, the regression model with only assessed value 
should be used. (j) For X1: p@value = 0.0000. The probability of obtain-
ing a sample that will yield a test statistic farther away than  
6.5271 is 0.0000 if the assessed value does not make a significant 
­contribution, holding age constant. For X2: p@value = 0.7204. The 
­probability of obtaining a sample that will yield a test statistic farther 
away than 0.3617 is 0.7204 if the age of a house does not make a signifi-
cant contribution, holding the effect of the assessed value constant.  
(k) 5.9005 … b1 … 11.3112. You are 95% confident that the mean taxes 
will increase by an amount somewhere between $5.90 and $11.31 for 
each additional $1,000 increase in the assessed value, holding constant 
the age. In Problem 13.77, you are 95% confident that the mean taxes will 
increase by an amount somewhere between $5.91 and $11.07 for each 
additional $thousand increase in assessed value, regardless of the age.  
(l) r 2
Y1.2 = 0.6121. Holding constant the effect of age, 61.21% of the 
variation in taxes can be explained by variation in the assessed value. 
r 2
Y2.1 = 0048. Holding constant the effect of the assessed value, 0.48% of 
the variation in taxes can be explained by variation in the age. (m) Based 
on your answers to (b) through (k), the age of a house does not have an 
effect on its taxes.
14.82 (a) Yn = 163.8291 - 20.0106X1 - 4.8622X2, where X1 = ERA 
and X2 = league 1American = 0 National = 12. (b) Holding constant 
the effect of the league, for each additional ERA, the number of wins 
is estimated to decrease by 20.0106. For a given ERA, a team in the 
National League is estimated to have 4.8622 fewer wins than a team in 
the American League. (c) 73.7813 wins. (d) Based on a residual analysis, 
there is no pattern in the errors. There is no apparent violation of other 
assumptions. (e) FSTAT = 31.7812 7 3.35, p@value = 0.0000. Because 
p@value 6 0.05, reject H0. There is evidence of a significant relation-
ship between wins and the two independent variables (ERA and league). 
(f) For X1: tSTAT = -7.9253 6 -2.0518, the p@value = 0.0000. Reject 
H0. ERA makes a significant contribution and should be included in the 
model. For X2: tSTAT = -1.9487 7 -2.0518, p@value = 0.0618 7 0.05. 
Do not reject H0. The league does not make a significant contribution and 
should not be included in the model. Based on these results, the regres-
sion model with only the ERA as the independent variable should be 
used. (g) -25.1913 … b1 … -14.83. (h) -9.9816 … b2 … 0.2573.  
(i) r 2
adj = 0.6798. 67.98% of the variation in wins can be explained by the 
variation in ERA and league after adjusting for number of independent 
variables and sample size. (j) r 2
Y1.2 = 0.6994. Holding constant the effect 
of league, 69.94% of the variation in number of wins can be explained 
by the variation in ERA. r 2
Y2.1 = 0.1233. Holding constant the effect of 
ERA, 12.33% of the variation in number of wins can be explained by the 
variation in league. (k) The slope of the number of wins with ERA is the 
same, regardless of whether the team belongs to the American League 
or the National League. (l) For X1X2: tSTAT = -0.7514 7 -2.0555 the 
p-value is 0.4592 7 0.05. Do not reject H0. There is no evidence that the 
interaction term makes a contribution to the model. (m) The model with 
one independent variable (ERA) should be used.
14.84 The r 2 of the multiple regression is very low, at 0.0645. Only 
6.45% of the variation in thickness can be explained by the variation of 
pressure and temperature. The F test statistic for the model including 
pressure and temperature is 1.621, with p@value = 0.2085. Hence, at a 

	
Self-Test Solutions and Answers to Selected Even-Numbered Problems	
823
(f) 85.53%. (g) Choose the model from Problem 15.6. That model has a 
much higher adjusted r 2 of 98.61%.
15.14 1.25.
15.16 R2
1 = 0.64, VIF1 =
1
1 - 0.64 = 2.778, R2
2 = 0.64,
VIF2 =
1
1 - 0.64 = 2.778. There is no evidence of collinearity.
15.18 VIF = 1.0 6 5. There is no evidence of collinearity.
15.20 VIF = 1.0105. There is no evidence of collinearity.
15.22 (a) 35.04. (b) Cp 7 3. This does not meet the criterion for 
­consideration of a good model.
15.24 Let Y = assessed value, X1 = size, X2 = fireplace (0 = no  
1 = yes and X3 = number of bedrooms. X4 = number of bathrooms. 
Based on a full ­regression model involving all of the variables, all the 
VIF values (2.3355, 1.1873, 1.9885, and 1.4428, respectively) are less 
than 5. There is no reason to suspect the existence of collinearity. Based 
on a best-subsets regression and examination of the resulting Cp values, 
the best models appear to be a model with variables X1 and X2, which 
has Cp = 1.1712, and the regression model with only X1. Based on a 
stepwise regression analysis with all the original variables, only vari-
able X1 makes a significant contribution to the model at the 0.05 level. 
Thus, the best model is the model using the size of the house 1X12 as the 
independent variable. A residual analysis shows no strong patterns. The 
final model is Yn = 276.848 + 508,031X1, r 2 = 0.3273, r 2
adj = 0.3033. 
Overall significance of the model: FSTAT = 13.326, p 6 0.001.
15.30 (a) An analysis of the linear regression model with all of the three 
possible independent variables reveals that the highest VIF is only 1.06. 
A stepwise regression model selects only the supplier dummy variable 
for inclusion in the model. A best-subsets regression produces only one 
model that has a Cp value less than or equal to k + 1 which is the model 
that includes pressure and the supplier dummy variable. This model is 
Yn = -31.5929 + 0.7879X2 + 13.1029 X3. This model has F = 5.1088 
(2 and 11 degrees of freedom) with a p@value = 0.027. r 2 = 0.4816, 
r 2
adj = 0.3873. A residual analysis does not reveal any strong patterns. 
The errors appear to be normally distributed.
15.32 (a) Best model: Cp = 2.1558, predicted fair market 
value = 260.6791 + 362.8318 land + 0.1109 house size 1sq ft2 -
1.7543 age. (b) The adjusted r 2 for the best model in 15.32 (a), 15.33 (a), 
and 15.34 (a) are, respectively, 0.8242, 0.9047, and 0.8481. The model in 
15.33 (a) has the highest explanatory power after adjusting for the num-
ber of independent variables and sample size.
15.34 (a) Predicted fair maket value = 145.1217 + 149.9337
land + 0.0913 house size (sq. ft.). (b) The adjusted r 2 for the best model 
in 15.32(a), 15.33(a), and 15.34 (a) are, respectively, 0.8242, 0.9047, and 
0.8481. The model in 15.33 (a) has the highest explanatory power after 
adjusting for the number of independent variables and sample size.
15.36 Let Y = fair market value, X1 = land area, X2 = interior, X3 =  age, 
X4 = number of rooms, X5 = number of bathrooms, X6 =  garage size, 
X7 = 1 if Glen Cove and 0 otherwise, and X8 = 1 if Roslyn and 0 other-
wise. (a) The VIFs of X2, X3, and X7 are greater than 5. Dropping X2 with 
the largest VIF, X3 still has a VIF greater than 5. After dropping X2 and X3, 
all remaining VIFs are less than 5 so there is no reason to suspect collinearity 
between any pair of variables. The following is the multiple regression model 
that has the smallest Cp14.32112 and the highest adjusted r 210.68152:
Fair Market Value = 49.2379 + 579.0105 Land + 109.5767 Baths
+ 48.2282 Garage + 213.2326 Roslyn
(c) Feelings initially improved, then improved at a diminishing rate to 
become stable at 220, and then started reducing. The same scenario has 
been explained in the law of diminishing marginal utility. (d) Given the 
data, r 2 = 1. This suggests that the 100% of the variation in an individu-
al’s feelings can be explained by the quadratic multiple regression model 
between height and feelings.
15.4 (a) Yn = -4.628 + 21.2075X1 + 4.004X2 where X1 = alcohol %  
and X2 = carbohydrates. FSTAT = 2,719.1957 p@value = 0.0000  
6 0.05, so reject H0. At the 5% level of ­significance, the linear 
terms are significant together. (b) Yn = 9.8109 + 15.2538X1 +
4.466X2 + 0.477 X2
1 - 0.017 X2
2, where X1 = alcohol % and 
X2 = carbohydrates. (c) FSTAT = 5.526 p@value = 0.0000 6 0.05, 
so reject H0. At the 5% level of significance, the model with quad-
ratic terms are significant. tSTAT = 3.3086, and the p@value = 0.0012. 
Reject H0. There is enough evidence that the quadratic term for alco-
hol % is significant at the 5% level of significance. tSTAT = -1.1674, 
p@value = 0.2449. Do not reject H0. There is insufficient evidence that 
the quadratic term for carbohydrates is significant at the 5% level of 
significance. Hence, since the quadratic term for alcohol is significant, 
the model in (b) that includes this term is better. The normal probability 
plot suggests some left-skewness in the errors. However, because of the 
large sample size, the validity of the results is not seriously impacted. 
The residual plots of the alcohol percentage and carbohydrates in the 
quadratic model do not reveal any remaining nonlinearity. (d) The num-
ber of calories in a beer depends quadratically on the alcohol percentage 
but linearly on the number of carbohydrates. The alcohol percentage and 
number of carbohydrates explain about 97.454% of the variation in the 
number of calories in a beer.
15.6 (b) Predicted cost = 710.00 + 607.9773 units - 1.3693 units2.
(c) Predicted cost = 710.00 + 607.977311452 - 1.3693114522 =
+60.076.79. (d) There appears to be a curvilinear pattern in the residual 
plot of the units and the units squared. (e) FSTAT = 320.5955 7 4.74; 
reject H0. (f) p@value = 0.0000 6 0.05, so the model is significant.  
(g) tSTAT = -5.5790 6 -2.3646; reject H0. There is a significant 
quadratic effect. (h) p@value = 0.0008 6 0.05, so the quadratic term is 
significant. (i) 98.92% of the variation in yield can be explained by the 
quadratic model. (j) 98.61%.
15.8 (a) 215.37. (b) For each additional unit of the logarithm of X1, the 
logarithm of Y is estimated to increase by 0.9 unit, holding all other 
variables constant. For each additional unit of the logarithm of X2, the 
logarithm of Y is estimated to increase by 1.41 units, holding all other 
variables constant.
15.10 (a) Yn = -154.089 + 95.26282X1 + 27.35 2X2, where 
X1 = alcohol % and X2 = carbohydrates. (b) The normal ­probability 
plot suggests that the errors are right-skewed. However, because of  
the large sample size, the validity of the results is not seriously  
impacted. The residual plots of the square-root transformation of alcohol 
­percentage and carbohydrates reveal some remaining nonlinearity.  
(c) FSTAT = 1,158.2378. Because the p-value is 0.0000, reject H0 at the 
5% level of significance. There is evidence of a significant linear relation-
ship between calories and the square root of the percentage of alcohol 
and the square root of the number of carbohydrates. (d) r 2 = 0.9396. 
So 93.96% of the variation in calories can be explained by the variation 
in the square root of the percentage of alcohol and the square root of the 
number of carbohydrates. (e) Adjusted r 2 = 0.9388. (f) The model in 
Problem 15.4 is slightly ­better because it has a higher r 2.
15.12 (a) Predicted ln1Cost2 = 9.7664 + 0.0080 Units (b) $55,471.75. 
(c) A quadratic pattern exists, so the model is not adequate.  
(d) tSTAT = 7.362 7 2.306; reject H0. (e) 87.14%. 87.14% of the varia-
tion in the natural log of the cost can be explained by the number of units. 

824	
Self-Test Solutions and Answers to Selected Even-Numbered Problems
(d) W = 0.5: Yn2013 = E2012 = 183.8854; W = 0.25: Yn2013 = E2012 =
177.0569. (f) The exponentially smoothed forecast for 2013 with 
W = 0.5 is higher than that with W = 0.25. A smoothing coefficient of 
W = 0.25 smooths out the average time more than W = 0.50. The expo-
nential smoothing with W = 0.5 assigns more weight to the more recent 
values and is better for forecasting, while the exponential smoothing with 
W = 0.25, which assigns more weight to more distance values, is better 
suited for eliminating unwanted cyclical and irregular variations.
16.6 (b), (c), (e)
 
Decade
Performance  
(%)
 
MA(3)
 
ES1W = 0.52
 
ES1W = 0.252
1830s
2.8
2.8000
2.8000
1840s
12.8
7.4000
7.8000
5.3000
1850s
6.6
10.6333
7.2000
5.6250
1860s
12.5
8.8667
9.8500
7.3438
1870s
7.5
8.6667
8.6750
7.3828
1880s
6.0
6.3333
7.3375
7.0371
1890s
5.5
7.4667
6.4188
6.6528
1900s
10.9
6.2000
8.6594
7.7146
1910s
2.2
8.8000
5.4297
6.3360
1920s
13.3
4.4333
9.3648
8.0770
1930s
-2.2
6.9000
3.5824
5.5077
1940s
9.6
8.5333
6.5912
6.5308
1950s
18.2
12.0333
12.3956
9.4481
1960s
8.3
11.0333
10.3478
9.1611
1970s
6.6
10.5000
8.4739
8.5208
1980s
16.6
13.6000
12.5370
10.5406
1990s
17.6
11.2333
15.0685
12.3055
2000s
-0.5
7.2842
9.1041
(d) Yn2010 = E2000 = 7.2842 (e) Yn2010 = E2000 = 9.1041. (f) The expo-
nentially smoothed forecast for the 2010s with W = 0.5 is lower than 
that with W = 0.25. The exponential smoothing with W = 0.5 assigns 
more weight to the more recent values and is better for forecasting, while 
the exponential smoothing with W = 0.25 which assigns more weight 
to more distant values is better suited for eliminating unwanted cyclical 
and irregular variations. (g) According to the exponential smoothing with 
W = 0.25, there appears to be a general upward trend in the performance 
of the stocks in the past.
16.8 (b), (c), (e)
Year Audits
MA(3)
ES1W = 0.52
ES1W = 0.252
2001
3,305
3,305.00
3,305.00
2002
3,749
3,461.33
3,527.00
3,416.00
2003
3,330
3,821.67
3,428.50
3,394.50
2004
4,386
4,191.67
3,907.25
3,642.38
2005
4,859
4,407.00
4,383.13
3,946.53
2006
4,276
4,186.33
4,329.56
4,028.90
2007
3,424
3,784.67
3,876.78
3,877.67
2008
3,654
3,616.33
3,765.39
3,821.76
2009
3,771
3,625.00
3,768.20
3,809.07
2010
3,450
3,630.00
3,609.10
3,719.30
2011
3,669
3,736.00
3,639.05
3,706.72
2012
4,089
3,864.02
3,802.29
(d) W = 0.5: Yn2013 = E2012 = 3,864.02; W = 0.25: Y n2013 =  
E2012 = 3,802.29. (f) The exponentially smoothed forecast for 2013 with 
W = 0.5 is higher than that with W = 0.25. The exponential smoothing 
with W = 0.5 assigns more weight to the more recent values and is better 
for forecasting, while the exponential smoothing with W = 0.25, which 
assigns more weight to more distant values, is better suited for eliminat-
ing unwanted cyclical and irregular variations.
The individual t test for the significance of each independent variable at 
the 5% level of significance concludes that only property size, baths, and 
the dummy variable Roslyn are significant given that the others are in the 
model. The following is the multiple regression result for the model cho-
sen by stepwise regression:
Fair Market Value = 30.3016 + 611.6910 Land + 130.7788 Baths
+ 214.2567 Roslyn
All the variables are significant individually at the 5% level of signifi-
cance. Combining the stepwise regression and the best-subsets regres-
sion results along with the individual t test results, the most appropriate 
multiple regression model for predicting the fair market value is the 
stepwise regression model. (b) The estimated fair market value in Roslyn 
is $214.2567 thousands above Glen Cove or Freeport for two otherwise 
identical properties.
15.38 In the multiple regression model with catalyst, pH, pressure, tem-
perature, and voltage as independent variables, none of the variables has 
a VIF value of 5 or larger. The best-subsets approach showed that only 
the model containing X1, X2, X3, X4, and X5 should be considered, where 
X1 = catalyst, X2 = pH, X3 = pressure, X4 = temp, and X5 = voltage. 
Looking at the p-values of the t statistics for each slope coefficient of the 
model that includes X1 through X5 reveals that pH level is not significant at 
the 5% level of significance 1p@value = 0.28622. The multiple regression 
model with pH level deleted shows that all coefficients are significant indi-
vidually at the 5% level of significance. The best linear model is determined 
to be Yn = 3.6833 + 0.1548X1 - 0.04197X3 - 0.4036X4 + 0.4288X5. 
The overall model has F = 77.0793 (4 and 45 degrees of freedom), with a 
p-value that is virtually 0. r 2 = 0.8726, r 2
adj = 0.8613. The normal prob-
ability plot does not suggest possible violation of the normality assumption. 
A residual analysis reveals a potential nonlinear relationship in temperature. 
The p-value of the squared term for temperature (0.1273) in the following 
quadratic transformation of temperature does not support the need for a 
quadratic transformation at the 5% level of significance. The p-value of the 
interaction term between pressure and temperature (0.0780) indicates that 
there is not enough evidence of an interaction at the 5% level of signifi-
cance. The best model is the one that includes catalyst, pressure, tempera-
ture, and voltage, which explains 87.26% of the variation in thickness.
CHAPTER 16
16.2 (a) 100% emphasis to historic values (b) 100% emphasis on the cur-
rent values (c) W = equal importance.
16.4 (b), (c), (e)
 
Year
Drive-Thru 
Speed
 
MA(3)
 
ES1W = 0.52
 
ES1W = 0.252
1998
177.59
177.5900
177.5900
1999
167.02
171.4967
172.3050
174.9475
2000
169.88
169.2500
171.0925
173.6806
2001
170.85
167.8167
170.9713
172.9730
2002
162.72
163.4967
166.8456
170.4097
2003
156.92
157.3867
161.8828
167.0373
2004
152.52
159.1133
157.2014
163.4080
2005
167.90
161.4400
162.5507
164.5310
2006
163.90
166.3000
163.2254
164.3732
2007
167.10
163.2567
165.1627
165.0549
2008
158.77
166.6967
161.9663
163.4837
2009
174.22
170.753
168.0932
166.1678
2010
179.27
179.2300
173.6816
169.4433
2011
184.20
184.1000
178.9408
173.1325
2012
188.83
183.8854
177.0569

	
Self-Test Solutions and Answers to Selected Even-Numbered Problems	
825
Linear trend: Yn2014 = 2.0500 + 0.13211142 = +3.8992 millions
Exponential trend: Yn2014 = 100.3274 + 0.01981142 = +4.0184 millions
16.20 (b) There has been an upward trend in the CPI in the United States 
over the 48-year period. The rate of increase became faster in the late 70s 
but tapered off in the early 80s.
(c) Linear trend: Yn =  16.2540 + 4.4919X. (d) Quadratic trend: 
Yn = 19.7334 + 4.0381X + 0.0097X2. (e) Exponential trend: 
log10Yn = 1.5612 + 0.0192X. (f) None of the trend models appear appro-
priate according to the first-, second- and percentage-difference but the 
second-difference has the smallest variation in general. So a quadratic 
trend model is slightly preferred over the others. (g) Quadratic trend:
For 2013: Yn2013 = 19.7334 + 4.03811482 + 0.009714822 = 235.8101
For 2014: Yn2014 = 19.7334 + 4.03811492 + 0.009714922 = 240.7848
16.22 (a) For Time Series I, the graph of Y versus X appears to be more 
linear than the graph of log Y versus X, so a linear model appears to be 
more appropriate. For Time Series II, the graph of log Y versus X appears 
to be more linear than the graph of Y versus X, so an exponential model 
appears to be more appropriate.
(b) Time Series I: Yn = 100.0731 + 14.9776X, where X = years relative 
to 2001
Time Series II: Yn = 101.9982 + 0.0609X, where X = years relative to 2001.
(c) X = 12 for year 2012 in all models. Forecasts for the year 2012:
 Time Series I: Yn = 100.0731 + 14.97761122 = 279.8045
 Time Series II: Yn = 101.9982 + 0.06091122 = 535.6886.
16.24 tSTAT = 2.40 7 2.2281; reject H0.
16.26 (a) Second-order autoregressive model (b) 7.4
16.28 (a) 
 
Coefficients
Standard 
Error
 
t Stat
 
P-value
Intercept
73.0383
79.1710
   0.9225
0.3780
YLag1
2.2358
  1.5648
   1.4288
0.1835
YLag2
-2.8050
  2.9255
-0.9588
0.3603
YLag3
1.6443
  1.5045
   1.0929
0.3001
Since the p-value = 0.3001 7 0.05 level of significance, the third-order 
term can be dropped.
(b) 
 
Coefficients
Standard 
Error
 
t Stat
 
P-value
Intercept
70.1584
  60.1327
1.1667
0.2660
YLag1
  0.9157
    0.6571
1.3935
0.1888
YLag2
  0.1243
    0.6541
0.1900
0.8525
Since the p@value = 0.8525 and is greater than 0.05, the second-order 
term can be dropped.
(c) 
 
Coefficients
Standard 
Error
 
t Stat
 
p-value
Intercept
53.9415
31.1693
  1.7306
0.1055
YLag1
  1.0482
  0.0418
25.0917
0.0000
Since the p@value = 0.0000, the first-order term cannot be dropped.
(d) The most appropriate model for forecasting is the first-order 
­autoregressive model:
 Yn2014 = 53.9415 + 1.0482Y2013 = 1,596 stores.
 Yn2015 = 53.9415 + 1.0482Yn2014 = 1,727 stores.
16.10 (a) The value of initial year is presumed to be base year and is 
coded as 0 and subsequently 1, 2, 3 ... (b) The predicted values of Y after 
coding the values of X are 8, 82, 155, 228, 301, 375, 448, 521, 595, 668, 
741, 815. (c) Substituting the value of X as 12 in the trend model, the pre-
dicted number of airline travelers are 887.8 = 888 people.
16.12 (b) Linear trend: Yn = 35.9216 + 82.5686X, where X is relative to 
1997. (c) Quadratic trend: Yn = 53.6925 + 75.4603X + 0.4443X2, where 
X is relative to 1997. (d) Exponential trend:
log10Yn =  2.1981 + 0.0671X, where X is relative to 1997. (e) Linear trend: 
Yn2014 = 35.9216 + 82.56861172 = 1,439.5882 = 1,440
Yn2015 = 35.9216 + 82.56861182 = 1,522.1569 = 1,522
Quadratic trend: Yn2014 = 53.6925 + 75.46031172 + 0.444311722
= 1,464.9118 = 1,465
Yn2015 = 53.6925 + 75.46031182 + 10.4443211822 = 1,555.9216 = 1,556
Exponential trend: Yn2014 =  102.1981 + 0.06711172 = 2,180.1029 = 2,180
Yn2015 =  102.1981 + 0.06711182 = 2,544.2451 = 2,544.
(f) The linear and quadratic trend model fit the data better than the 
­exponential trend model and, hence, either forecast should be used.
16.14 (b) Yn = 315.7662 + 65.2175X where X = years relative to 1978.
(c) X = 2013 - 1978 = 35, Yn = 315.7662 + 65.21751352 =
+ 2,598.3770 billion X = 2014 - 1978 = 36, Yn = 3,105.7662 +
65.21751362 = +2,663.5944 billion.
(d) There is an upward trend in federal receipts between 1978 and 2012. 
The trend appears to be linear.
16.16 (b) Linear trend: Yn = -95 + 245.49098X, where X is relative to 
2002. (c) Quadratic trend: Yn = 930.2098 - 437.9823X + 68.3473X2, 
where X is relative to 2002. (d) Exponential trend: log10Yn = 2.5659 +  
0.0747X, where X is relative to 2002. (e) Linear trend: 
Yn2013 = -95.0000 + 245.49091112 = 2,605.4 million KWh
Yn2014 = -95.0000 + 245.49091122 = 2,850.89 million KWh
Quadratic trend: Yn2013 = 930.2098 - 437.98231112 + 68.347311122
= 4,382.43 million KWh
Yn2014 = 930.2098 - 437.98231122 + 68.347311222
= 5,516.44 millions of KWh
Exponential trend: Yn2013 = 102.5659 + 0.07471112 = 2,438.44 million KWh
Yn2014 = 102.5659 + 0.07471122 = 2,895.82 million KWh.
16.18 (b) Linear trend: Yn = 2.0500 + 0.1321X, where X is relative to 
2000. (c) Quadratic trend: Yn = 2.1471 + 0.0835X - 0.0037X2, where X 
is relative to 2000. (d) Exponential trend: log10Yn = 0.3274 + 0.0198X, 
where X is relative to 2000.
(e)
1st Difference
2nd Difference
%Difference
0.30
15.08
0.09
-0.21
3.93
0.20
0.11
8.40
-0.09
-0.29
-3.49
0.14
0.23
5.62
0.20
0.06
7.60
0.09
-0.11
3.18
0.21
0.12
7.19
0.13
-0.08
4.15
0.01
-0.12
0.31
0.05
0.04
1.53
0.06
0.01
1.81
0.87
0.81
25.74
The linear and exponential models should be investigated.
(f) The forecasts using the two models are:

826	
Self-Test Solutions and Answers to Selected Even-Numbered Problems
(d) The residuals in the three trend models show strings of consecutive 
positive and negative values. The autoregressive model performs well 
for the historical data and has a fairly random pattern of residuals. The 
autoregressive model also has the smallest values in MAD and SYX. Based 
on the principle of parsimony, the autoregressive model would be the best 
model for forecasting.
16.38 (b), (c)
Linear
Quadratic
Exponential
AR1
SSE
0.3977
0.3571
0.3538
0.6299
Syx
0.1821
0.1802
0.1717
0.2393
MAD
0.1193
0.1182
0.1165
0.1490
(d) The residuals in the linear, quadratic, and exponential trend models 
show strings of consecutive positive and negative values. The autoregres-
sive model performs well for the historical data and has a fairly random 
pattern of residuals. The exponential trend model, however, has the small-
est values in MAD and SYX. The autoregressive model would be the best 
model for forecasting due to its fairly random pattern of residuals even 
though it has slightly larger MAD and SYX than the exponential model.
16.40 (a) log bn0 = 2, bn0 = 102 = 100. This is the fitted value for 
January 2009 prior to adjustment with the January multiplier.
(b) log bn1 = 0.01, bn1 = 100.01 = 1.0233. The estimated monthly com-
pound growth rate is 2.33%. (c) log bn2 = 0.1, bn2 = 100.1 = 1.2589. The 
January values in the time series are estimated to have a mean 25.89% 
higher than the December values.
16.42 (a) log bn0 = 3.0, bn 
0 = 103.0 = 1,000. This is the fitted value for 
the first quarter of 2009 prior to adjustment by the quarterly ­multiplier. 
(b) log bn1 = 0.1, bn1 = 100.1 = 1.2589. The estimated quarterly 
­compound growth rate is 1bn1 - 12100% = 25.89%.
(c) log bn3 = 0.2, bn3 = 100.2 = 1.5849.
16.44 (a) The retail industry is heavily subject to seasonal variation  
due to the holiday seasons and so are the revenues for Toys R Us.
(b) There is obvious seasonal effect in the time series.
(c) log10Yn = 3.6291 + 0.0025X - 0.3670Q1 - 0.3676Q2 - 0.3438Q3. 
(d) log10bn1 = 0.0025. bn1 = 100.0025 = 1.0058. The estimated quarterly 
compound growth rate is 1bn1 - 12 100% = 0.58%.  
(e) log10bn2 = -0.3670. bn2 = 10-0.3670 = 0.4296.  
1bn2 - 12 100% = -57.04%. The 1st quarter values in the time series are 
estimated to have a mean 57.04% below the 4th quarter values.  
log10bn3 = -0.3676. bn3 = 10-0.3676 = 0.4289. 1bn3 - 12 100% =
-57.11%.
The 2nd quarter values in the time series are estimated to have a mean 
57.11% below the 4th quarter values.  
log10bn4 = -0.3438. bn = 10-0.3438 = 0.4531. 1bn4 - 12 100% =
-54.69%.
The 3rd quarter values in the time series are estimated to have a mean 
54.69% below the 4th quarter values. (f) Forecasts for the last three 
quarters of 2013 and all of 2014 are 2,723.336, 2,893.926, 6,423.647, 
2,775.405, 2,787.204, 2,961.794, and 6,574.295 millions
16.30 (a) 
 
Coefficients
Standard 
Error
 
t Stat
 
P-value
Intercept
-0.1927
0.7859
-0.2452
0.8133
YLag1
0.4966
1.0298
0.4822
0.6444
YLag2
0.3137
1.1551
0.2716
0.7938
YLag3
0.3507
0.8621
0.4068
0.6963
Since the p@value = 0.6963 7 0.05 level of significance, the third-order 
term can be dropped.
(b) 
 
Coefficients
Standard 
Error
 
t Stat
 
P-value
Intercept
-0.2268
0.6014
-0.3771
0.7149
YLag1
   0.7751
0.8212
   0.9438
0.3699
YLag2
   0.3759
0.7459
   0.5039
0.6264
Since the p@value = 0.6264 7 0.05 level of significance, the second-
order term can be dropped.
(c) 
 
Coefficients
Standard 
Error
 
t Stat
 
P-value
Intercept
-0.0479
0.4358
-0.1100
0.9144
YLag1
   1.0791
0.1535
   7.0278
0.0000
Since the p-value is 0.0000, the first-order term cannot be dropped.
(d) The most appropriate model for forecasting is the first-order 
­autoregressive model:
Yn2014 = -0.0479 + 1.0791Y2013 = +4.5380 million.
16.32 (a) plot the residuals. (b) SYX or MAD.
16.34 (a) The residuals in the linear and exponential trend model show 
strings of consecutive positive and negative values.
(b), (c)
Linear
Quadratic Exponential
AR1
SSE
6,275,572.218 2,267,548.717
5,613,646
760,222.4864
Syx
835.0364
532.3942
789.7711
308.2658
MAD
580.8397
367.8580
364.4034
222.0914
(d) The residuals in the three trend models show strings of consecutive 
positive and negative values. The autoregressive model performs well for 
the historical data and has a fairly random pattern of residuals. It has the 
smallest values in MAD and SYX. Based on the principle of parsimony, 
the autoregressive model would be the best model for forecasting.
16.36 (b), (c)
Linear
Quadratic
Exponential
AR1
SSE
43,144.3137
41,614.2394
547,380.1537
53,441.7002
Syx
53.6310
54.5201
191.0288
61.7840
MAD
40.1915
40.6096
133.0124
37.0922

	
Self-Test Solutions and Answers to Selected Even-Numbered Problems	
827
 
Coefficients
Standard 
Error
 
t Stat
 
P-value
Intercept
-2.5854
0.6244
-4.1404
0.0002
Coded Yr
0.7197
0.0290
 24.7863
0.0000
(c) Quadratic trend: Yn = 1.24247 + 0.0847X + 0.0172X2, where X is 
relative to 1975.
 
Coefficients
Standard 
Error
 
t Stat
 
P-value
Intercept
1.2247
0.2405
    5.0932
0.0000
Coded Yr
0.0847
0.0301
  2.8160
0.0079
Coded Yr Sq
0.0172
0.0008
21.8464
0.0000
Exponential trend: log10Yn = 0.1741 + 0.0374X, where X is relative to 
1975.
 
Coefficients
Standard 
Error
 
t Stat
 
P-value
Intercept
0.1741
0.0216
  8.0477
0.0000
Coded Yr
0.0374
0.0010
37.1890
0.0000
AR(3): Yni = 0.4887 + 1.1725Yi - 1 - 0.8248Yi - 2 + 0.7121Yi - 3
 
Coefficients
Standard 
Error
 
t Stat
 
P-value
Intercept
   0.4887
0.1561
   3.1301
0.0038
YLag1
   1.1725
0.1543
   7.5975
0.0000
YLag2
-0.8248
0.2592
-3.1817
0.0033
YLag3
   0.7121
0.1833
   3.8855
0.0005
Test of A3: p@value = 0.0005 6 0.05. Reject H0 that A3 = 0. Third-
order term cannot be deleted. A third-order autoregressive model is 
appropriate.
Linear
Quadratic
Exponential
AR3
SSE
138.6905
9.4759
206.5249
7.7632 
Syx
  1.9628
0.5203
    2.3952
0.5004
MAD
  1.6711
0.3576
  1.3060
0.3682
(h) The residuals in the first three models show strings of consecutive 
positive and negative values. The autoregressive model performs well for 
the historical data and has a fairly random pattern of residuals. It also has 
the smallest values in the standard error of the estimate, MAD and SSE. 
Based on the principle of parsimony, the autoregressive model would 
probably be the best model for forecasting.
(i) Yn2013 = 0.4887 + 1.1725Y2012 - 0.8248Y2011 + 0.7121Y2010 =
$27.7445 billions.
CHAPTER 17
17.2 (b) Gauges take up too much visual space that would be needed for 
the 20 funds. (c) The one-year return and the three-year return is much 
higher for the long-term funds than for the short-term funds.
17.4 (c) The bullet graph enables you to see the names of the individual 
teams and which teams are inexpensive, typical, and expensive whereas 
the stem-and-leaf display only shows the distribution of the costs. (d) The 
bullet graphs shows the costs for each team and which teams fall into the 
inexpensive, typical, and expensive categories.
16.46 (b) 
 
Coefficients
Standard 
Error
 
t Stat
 
P-value
Intercept
-0.0916
0.0070
-13.1771
0.0000
Coded Month
-0.0037
0.0001
-64.4666
0.0000
M1
   0.1245
0.0086
14.5072
0.0000
M2
   0.0905
0.0086
10.5454
0.0000
M3
   0.0806
0.0086
9.3975
0.0000
M4
   0.0668
0.0086
7.7810
0.0000
M5
   0.0793
0.0086
9.2457
0.0000
M6
   0.1001
0.0086
11.6725
0.0000
M7
   0.1321
0.0086
15.3949
0.0000
M8
   0.1238
0.0086
14.4279
0.0000
M9
   0.0577
0.0088
6.5372
0.0000
M10
   0.0356
0.0088
4.0317
0.0001
M11
   0.0103
0.0088
1.1646
0.2472
(c) Yn103 = 0.4451.
(d) Forecasts for the last four months of 2012 are 0.3790, 0.3571, 0.3340, 
and 0.3234.
(e) log10bn1 = -0.0037; bn1 = 10-0.0037 = 0.9915. The estimated 
monthly compound growth rate is 1bn1 - 12 100% = -0.8542%.
(f) log10bn8 = 0.1321; bn8 = 100.1321 = 1.3554. 
1bn8 - 12 100% = 35.5382%. The July values in the time series are 
­estimated to have a mean 35.5382% above the December values.
16.48 (b) 
 
Coefficients
Standard 
Error
 
t Stat
 
P-value
Intercept
   0.7884
0.0362
21.8058
0.0000
Coded Quarter
   0.0213
0.0013
16.5210
0.0000
Q1
   0.0545
0.0378
  1.4406
0.1597
Q2
   0.0044
0.0377
  0.1165
0.9080
Q3
-0.0080
0.0377
  0.2117
0.8338
(c) log10bn1 = 0.0213; bn1 = 100.0213 = 1.05226; 1bn1 - 12 100% =
5.0226%. The estimated quarterly compound mean growth rate in the 
price of silver is 5.0226%, after adjusting for the seasonal component.
(d) log10bn2 = 0.0545; bn2 = 100.0545 = 1.1337; 1bn2 - 12 100% =  
13.37%. The first-quarter values in the time series are estimated to have a 
mean 13.37% above the fourth-quarter values.
(e) Last quarter, 2012: Yn35 = +34.1414.
(f) 2013: 40.6502, 38.0402, 40.2810, 41.5349.
16.60 (b) Linear trend: Yn = 174,015.2828 + 2,436.3739X, where X is 
relative to 1984.
(c) 2013: Yn2013 = 174,015.2828 + 2,436.37391292 = 244,670.1256
thousands
2014: Yn2014 = 174,015.2828 + 2,436.37391302 = 247,106.4995
thousands.
(d) (b) Linear trend: Yn = 115,574.3034 + 1,553.1828X, where X is 
­relative to 1984.
(c) 2012:  Yn2013 =  115,574.3034 + 1,553.18281292 = 160,616.6034 
thousands.
2013: Yn2014 =  115,574.3034 + 1,5853.18281302 = 162,169.7862
thousands.
16.62 Linear trend: Yn = -2.5854 + 0.7197X, where X is relative to 
1975.

828	
Self-Test Solutions and Answers to Selected Even-Numbered Problems
17.24 Because half the data will be used for a validation sample, the 
results will differ depending on which values are in the training sample 
and which are in the validation sample.
17.26 The r 2 for the regression tree model is 0.373. The first split is based 
on a plate gap of 1.8. For those bags with a plate gap less than 1.8, the 
mean tear is 0.3107. For those bags with a plate gap at least 1.8, the mean 
tear is 1.98. For those bags with a plate gap less than 0.0, the mean tear is 
0.06. For those bags with a plate gap less than 1.8 but greater than 0, the 
mean tear is 0.45. Thus, you would recommend that a plate gap of less 
than 0 be used to minimize tears in the bag.
17.28 The r 2 for the regression tree model is 0.789. The first split is based 
on 831 square feet. Moves of at least 831 sq. ft. have a mean moving time 
of 51.1875 hours. Moves of less than 831 square feet have a mean moving 
time of 22.6071 hours. Among moves of less than 831 sq. ft., moves of 
less than 486 sq. ft., have a mean moving time of 15.7955 hours. Moves 
of less than 344 sq. ft. have a mean moving time of 12.75 hours. Moves 
of between 344 and 486 sq. ft. have a mean moving time of 18.3333 
hours. Moves of between 486 and 830 sq. ft. have a mean moving time of 
27.0147 hours. Moves between 486 and 599 sq. ft. have a mean moving 
time of 24.825 hours. Moves between 600 and 830 have a mean moving 
time of 30.1429 hours. Moves between 557 and 599 sq. ft. have a mean 
moving time of 24.05 hours. Moves between 486 and 557 sq. ft have a 
mean moving time of 25.6 hours.
17.30 Because some of the data will be used for a validation sample, the 
results will differ depending on which values are in the training sample 
and which are in the validation sample.
17.32 Because some of the data will be used for a validation sample, the 
results will differ depending on which values are in the training sample 
and which are in the validation sample.
17.34 Because some of the data will be used for a validation sample, the 
results will differ depending on which values are in the training sample 
and which are in the validation sample.
17.36 Because some of the data will be used for a validation sample, the 
results will differ depending on which values are in the training sample 
and which are in the validation sample.
17.38 (b) The first two cereals to cluster are Wheaties and Nature’s Path 
Organic Multigrain Flakes followed by Post Shredded Wheat Vanilla 
Almond and Kellogg’s Mini Wheats. At the two cluster level, one  
cluster ­contains Post Shredded Wheat Vanilla Almond and Kellogg’s  
Mini Wheats and the other cluster contains the other five cereals.
17.40 (b) The first two countries to cluster are Egypt and Jordan followed 
by Lithuuania and Poland. At the two cluster level one cluster is France, 
Germany, Spain, United Kingdom, Japan, Israel, and the United States 
and the other cluster contains the remaining countries. The first cluster 
appears to contain the western European countries and the United States 
and Israel.
17.42 (b) Since the stress statistic is 0.0973 in three dimensions, 0.1308 
in two dimensions, and 0.3147 in one dimension, it is reasonable to try 
to interpret a two-dimensional mapping of the cereals. Looking at a 45° 
rotation, one dimension separates Post Shredded Wheat Vanilla Almond 
and Kellogg’s Mini Wheats based on their higher calorie and sugar con-
tent. A second dimension does not seem to be interpretable. In addition, 
All Bran, which has lower calories and higher sugar is separated from the 
other cereals.
17.44 (b) Since the stress statistic is 0.2773 in four dimensions, 0.3151 
in three dimensions, 0.3787 in two dimensions, and 0.5323 in one 
­dimension, it is reasonable to try to first try interpret a two-dimensional 
17.6 (b) The rates of return of the three indices vary a great deal from 
year to year, but the pattern for the three indices are similar except for the 
NASDAQ in 2009 which had a much higher return in that year than the 
DJIA or the S & P500. (c) Unlike the three stock indices which had simi-
lar patterns between 2006–2012, the returns of the three metals differed 
greatly from year to year.
17.8 (b) Wendy’s consistently has the fastest service time. The service 
time at McDonald’s and Burger are similar over the years. The service 
time at Chick-Fil-A was slower in the earlier and later years than the 
other fast food chains.
17.10 (b) The values of the teams varied from $315 million for the 
Charlotte Hornets to $1,100 million for the New York Knicks. The change 
in values was not consistent across the teams. The two most valuable 
teams, the Los Angeles Lakers, and the New York Knicks had very differ-
ent increases in value (11% and 41% respectively).
17.12 (c) Almost all the countries that had lower GDP had lower Internet 
use except for the Republic of Korea. The pattern of mobile cellular sub-
scriptions does not seem to depend on the GDP of the country.
17.14 (a) 
(b) There are 37 funds.
17.16 (a)
(b) There is only one fund.
17.18 The five-year returns for the four funds that are small market cap 
funds that have a rating of five stars are 5.29, 6.97, 10.75, and 11.35.
17.20 The highest five-year return of 12.33 is for a large cap growth fund.
17.22 (b) The r 2 for the classification tree model is 0.434. The first split 
is for the 8 customers who called 50 or more times. Among customers 
who called fewer than 50 times, those who called at least seven times and 
visited two or more times are more likely to churn.

	
Self-Test Solutions and Answers to Selected Even-Numbered Problems	
829
17.56 Because half the data will be used for a validation sample, the 
results will differ depending on which values are in the training sample 
and which are in the validation sample.
17.58 (c) The first two foods to cluster are Cantonese and American, fol-
lowed by French and Mandarin, followed by Spanish and Greek. At the 
two cluster level, the first cluster includes Japanese, French, Mandarin, 
Szechuan, and Mexican. The second cluster includes Cantonese, 
American, Spanish, Greek, and Italian.Since the stress statistic is 0.0468 
in four dimensions, 0.1164 in three dimensions, 0.2339 in two dimen-
sions, and 0.4079 in one dimension, it is reasonable to try to first try 
interpret a two-dimensional mapping of the foods. There does not seem 
to be a clear interpretation of the dimensions along the lines of the three 
scales. The two spicy foods, Mexican and Szechuan are close to each 
other as are French anhd Greek, and Japanese and American. Italian is 
separated by itself as is Spanish.
mapping of the countries. There does not seem to be a clear interpreta-
tion of the dimensions along the lines of GDP and social media usage. 
Pakistan seems very separated from the other countries with Indonesia on 
the other side of the graph. Russia and Lithuania are close as are Mexico 
and Spain, and Israel and Egypt.
17.52 (b) The sparklines show differences in the values of the U. S. dol-
lar in terms of the Canadian dollar, the English pound, and the Euro over 
the time period 2002–2012. The value of the U. S. dollar in terms of the 
Canadian dollar declined drastically from 2002 to 2007, but has remained 
steady since 2009. The value of the U. S. dollar in terms of the English 
pound has remained relatively steady between 2002 and 2012. The value 
of the U. S. dollar in terms of the Euro declined between 2002 and 2007 
(with an increase in 2005) but has remained steady since 2008.
17.54 Because half the data will be used for a validation sample, the 
results will differ depending on which values are in the training sample 
and which are in the validation sample.


Index
831
A
a (level of significance), 340
A priori probability, 181
Addition rule, 186 
Adjusted r, 578
Akaike information criterion, 713
Algebra, rules for, 744 
Alternative hypothesis, 337
Among-block variation, 439
Among-group variation, 425, 439
Analysis of means (ANOM), 432
Analysis of proportions (ANOP), 488
Analysis of variance (ANOVA), 
Kruskal-Wallis rank test for differences in c medians, 501–504 
assumptions of, 504
One-way, 
assumptions, 433 
F test for differences in more than two means, 426 
F test statistic, 426
Levene’s test for homogeneity of variance, 433–434
summary table, 427
Tukey-Kramer procedure, 430–432 
Randomized block,
Assumptions, 443
Testing for factor and block effects, 438–443
F test for factor effect, 440
F test for block effect, 441
Two-way, 
cell means plot, 454
factorial design, 446 
interpreting interaction effects, 454–455 
multiple comparisons, 452–453 
summary table, 450
testing for factor and interaction effects, 449–450
Analysis ToolPak,
Checking for presence, 769
Frequency distribution, 116 
Histogram, 120
Descriptive statistics, 174 
Exponential smoothing, 697
F test for ratio of two variances, 418 
Multiple regression, 617
One-way ANOVA, 468
paired t test, 416–417
pooled-variance t test, 414–415 
randomized block design, 471
random sampling, 62
sampling distributions, 298
separate-variance t test, 415–416 
simple linear regression, 567
two-way ANOVA, 470–471
Residual analysis, 568
Sampling distributions, 298
Analyze, 30, 42
ANOVA. See Analysis of variance (ANOVA)
Area of opportunity, 230
Arithmetic mean. See Mean
Arithmetic operations, rules for, 744
Assumptions
analysis of variance (ANOVA), 433 
of the confidence interval estimate for the mean (sunknown), 307 
of the confidence interval estimate for the proportion, 315
of the F test for the ratio of two variances, 403
of the paired t test, 390–391 
of Kruskal-Wallis test, 504
of regression, 535
for 2 * 2 table, 481
for 2 * c table, 494
for r * c table, 494
for the t distribution, 308
t test for the mean (s unknown), 352–353 
in testing for the difference between two means, 379 
of the Wilcoxon rank sum test, 495
of the Z test for a proportion, 360
Autocorrelation, 539
Autoregressive modeling, 675
steps involved in, on annual time-series data, 679–680
for trend fitting and forecasting, 675–681 
Average linkage, 720
B
Bar chart, 79–80
Bayes’ theorem, 197
Best-subsets approach in model building, 640–641
b Risk, 340
Bias
nonresponse, 53
selection, 53
Big data, 32
Binomial distribution, 223–229 
mean of, 228
properties of, 223
shape of, 227
standard deviation of, 228
Binomial probabilities
calculating, 225–227
Boxplots, 152–153
Brynne packaging, 564–565
Bullet graph, 705
Business analytics, 32, 703 
C
CardioGood Fitness, 58–59, 111, 172,210, 274, 331, 412, 466, 512
Categorical data
chi-square test for the difference between two proportions, 476–481 
chi-square test of independence, 489–494 
chi-square test for c proportions, 483–486 

832	
Index ﻿
organizing, 65–68, 96–97 
visualizing, 79–83 
Z test for the difference between two proportions, 399–400 
Categorical variables, 42
Causal forecasting methods, 658
Cell means plot, 454 
Cell, 34
Central limit theorem, 285 
Central tendency, 130–135
Certain event, 180
Challenges in organizing and visualizing variables,
Obscuring data, 98–99
Creating false impressions, 99
Chartjunk, 100–102
Charts. 
bar, 79–80 
Pareto, 81–83
pie, 80–81
side-by-side bar, 83 
Chebyshev Rule, 158 
Chi-square (x2) distribution, 478
Chi-square (x2) test for differences
between c proportions, 483–486 
between two proportions, 476–482 
Chi-square (x2) test for the variance or standard deviation, 506
Chi-square (x2) test of independence, 489–494 
Chi-square (x2) table, 776
Choice Is Yours Followup, 111, 210
Class boundaries, 71
Class intervals, 71
Class midpoint, 72
Class interval width, 71
Classes, 71
And Excel bins, 73
Classification trees, 711–712
Clear Mountain State Surveys, 59, 112, 172, 210, 274, 331, 413,  
466, 512
Cluster analysis, 719
Cluster sample, 51
Coefficient of correlation, 162, 547
inferences about, 547–548 
Coefficient of determination, 531–532
Coefficient of multiple determination, 577–578, 630 
Coefficient of partial determination, 590 
Coefficient of variation, 140 
Collectively exhaustive events, 185
Collect, 30, 42
Collinearity of independent variables, 636–637 
Combinations, 204, 224 
Complement, 184
Complete linkage, 720
Completely randomized design, 423. See also One-way analysis of 
variance
Computing conventions used in this book, 35
Cook’s distance statistic Di, 607–608
Conditional probability, 189–190
Confidence coefficient, 341
Confidence interval estimation, 301
connection between hypothesis testing and, 347–348
for the difference between the means of two independent 
groups, 381
for the difference between the proportions of two independent 
groups, 399–400
for the mean difference, 393
ethical issues and, 323–324
for the mean (s known), 301–304
for the mean (s unknown), 307–313 
for the mean response, 551–552 
for the proportion, 315–317 
of the slope, 546, 584
Contingency tables, 67, 183, 476
Continuous probability distributions, 298
Continuous variables, 43
Control chart factors, 
tables, 785
Convenience sampling, 49
Counting rules, 202–204
Correlation coefficient. See Coefficient of correlation
Covariance, 160 
of a probability distribution, 218 
Coverage error, 53
Craybill Instrumentation Company case, 652
Critical range, 431, 444, 453
Critical value approach, 342–345 
Critical values, 339
of test statistic, 338–339 
Cross-product term, 594
Cross validation, 644
Cumulative percentage distribution, 75–76 
Cumulative percentage polygons, 89–90 
Cumulative standardized normal distribution, 251, 
tables, 772–773 
Cyclical effect, 694
D
Dashboards, 704
Data, 31
sources of, 46
Data cleaning, 48
Data collection, 46–49
Data formatting, 47
Data mining, 710
Data discovery, 706
DCOVA, 30
Decision trees, 191 
Define, 30, 42
Degrees of freedom, 309
Dependent variable, 520
Descriptive analytics, 703
Descriptive statistics, 32
Deviance statistic, 604
Digital Case, 59–60, 111, 172, 210, 242, 274, 297, 330, 370,  
412, 465–466, 511, 564, 616, 652, 696
Directional test, 356
Discrete probability distributions
binomial distribution, 225 
covariance, 218 
hypergeometric distribution, 234 
Poisson distribution, 231
Discrete variables, 43
expected value of, 215
probability distribution for, 214
variance and standard deviation of, 215–216 
Dispersion, 135
Downloading files for this book, 757–764 
Drill-down, 706–707

	
Index	
833
Dummy variables, 592–594 
Durbin-Watson statistic, 540–542 
tables, 784
E
Electronic formats and encoding, 47
Empirical probability, 181
Empirical rule, 157 
Ethical issues
confidence interval estimation and, 323
in hypothesis testing, 365 
in multiple regression, 646
in numerical descriptive measures, 165–166
for probability, 205
for surveys, 54
Euclidean distance, 720, 722
Events, 181
Expected frequency, 477 
Expected value, 214
of discrete variable, 215 
of sum of two variables, 219
Explained variation or regression sum of squares (SSR), 530–531
Explanatory variables, 521
Exponential distribution, 268 
Mean of, 268
Standard deviation of, 268
Exponential growth
with monthly data forecasting equation, 688
with quarterly data forecasting equation, 689
Exponential smoothing, 662–664 
Exponential trend model, 669–670 
Extrapolation, predictions in regression analysis and, 525
F
Factor, 423
Factorial design. See Two-way analysis of variance
F distribution, 426 
tables, 777–780
Finite population correction facvtor, 235, 293
First-order autoregressive model, 676
First quartile, 148
Five-number summary, 151
Fixed effects models, 459
Forecasting, 
autoregressive modeling for, 675–681
choosing appropriate model for, 683–685 
least-squares trend fitting and, 665–673 
seasonal data, 686–691 
Frame, 49
Frequency distribution, 71–72
Friedman rank test, 506
F test for the ratio of two variances, 402–405 
F test for the block effect, 441
F test for the factor effect, 440
F test for factor A effect, 449 
F test for factor B effect, 449
F test for interaction effect, 450
F test for the slope, 545–546 
F test in one-way ANOVA, 426
G
Gauges, 705
Gaussian distribution, 298
General addition rule, 186–187 
General multiplication rule, 196 
Geometric mean, 134
Geometric mean rate of return, 135 
Grand mean, 424
Greek alphabet, 749
Groups, 423
Guidelines for developing visualizations, 102
H
Hat matrix diagonal elements hi, 607
Hierarchical clustering, 719
Histograms, 87–88 
Homogeneity of variance, 433
Levene’s test for, 433–434 
Homoscedasticity, 535
Hyperbolic tangent function, 717
Hypergeometric distribution, 234
mean of, 235
standard deviation of, 235
Hypergeometric probabilities
calculating, 235–236
Hypothesis. See also One-sample tests of hypothesis
alternative, 337
null, 337
I
Impossible event, 180
Independence, 193
of errors, 535
x2 test of, 489–494 
Independent events, multiplication rule for, 196 
Independent variable, 520
Index numbers, 692
Inferential statistics, 32
Interaction, 447, 594–595
Interaction terms, 594
Interpolation, predictions in regression analysis and, 525
Interquartile range, 150 
Interval scale, 44 
Influence analysis, 606–608
Irregular effect, 659
J
Joint probability, 184
Joint event, 182
Joint response, 66
JMP
Classification and regression trees, 732–733
Cluster analysis, 734
Graph builder, 732
Multidimensional scaling, 734
Neural networks, 733–734
treemaps, 730–731
Judgment sample, 50
K
k-means clustering, 719
Kruskal-Wallis rank test for differences in c medians,  
501–504 
assumptions of, 504
Kurtosis, 142–143

834	
Index ﻿
L
Lagged predictor variable, 675
Least-squares method in determining simple linear regression, 
522–523
Least-squares trend fitting
and forecasting, 665–673 
Left-skewed, 142
Leptokurtic, 143
Level of confidence, 304
Level of significance (a), 340
Levels, 423
Levene’s test
for homogeneity of variance, 433–434 
Linear regression. See Simple linear regression
Linear relationship, 521
Linear trend model, 665–666 
Logarithms, rules for, 745
Logarithmic transformation, 633 
Logistic regression, 601–604
Logworth statistic, 713
M
Main effects, 451
Main effects plot, 428
Managing the Managing Ashland MultiComm Services, 58, 110, 172, 
241, 273–274, 297, 329–330, 369, 411–412, 465, 510–511, 
564, 615, 696 
Marascuilo procedure, 486–488 
Marginal probability, 185, 195
Margin of error, 53 
Matched samples, 387
Mathematical model, 223
McNemar test, 505
Mean, 130–132 
of the binomial distribution, 228
confidence interval estimation for, 
geometric, 134–135
of hypergeometric distribution, 235
population, 155 
sample size determination for, 318
sampling distribution of, 279–289 
standard error of, 281
unbiased property of, 279
Mean absolute deviation, 684
Mean squares, 425
Mean Square Among (MSA), 426
Mean Square A (MSA), 440, 448
Mean Square B (MSB), 449
Mean Square Blocks (MSBL), 440
Mean Square Error (MSE), 440, 449 
Mean Square Interaction (MSAB), 449
Mean Square Total (MST), 426
Mean Square Within (MSW), 426
Measurement
types of scales, 43–45 
Measurement error, 53 
Median, 132–133
Microsoft Excel, 
Absolute and relative cell references, 751
Autocorrelation, 568
autogressive modeling, 698–699
bar charts, 117
Bayes’ theorem, 211
basic probabilities, 211
binomial probabilities, 244
bins, 73
boxplots, 175
bullet graphs, 729
cells, 34
cell means plot, 472
cell references, 750 
central tendency, 173
chart formatting, 755 
checking for and applying Excel updates, 766–767 
checklist for using, 34
chi-square tests for contingency tables, 514–515 
coefficient of variation, 174
computing conventions, 35
confidence interval esimate for the difference between the means of 
two independent groups, 415
confidence interval for the mean, 332 
confidence interval for the proportion, 333
configuring Excel security for add-ins, 767–768
contingency tables, 114–115 
correlation coefficient, 175
counting rules, 211
covariance, 175 
covariance of a probability disdtribution, 243
creating histograms for discrete probability distributions, 756
creating and copying worksheets, 38 
cross-classification table, 114–115 
cumulative percentage distribution, 116–117
cumulative percentage polygon, 121–122
descriptive statistics, 173–175 
drilldown, 731
dummy variables, 619
entering data, 36–37 
entering array formulas, 752
entering formulas into worksheets, 751
establishing the variable type, 61
expected value, 243
exponential probabilities, 276
exponential smoothing, 697
FAQs, 793–794
frequency distribution, 115–116 
functions, 751–752 
F test for the ratio of two variances, 418 
Gauges, 728
Geometric mean, 173
Getting ready to use, 766
Guide workbooks, 765
Histogram, 120 
Hypergeometric probabilities, 244
Kruskal-Wallis test, 516
least-squares trend fitting, 698
Levene test, 469–470
Logistic regression, 620
Marascuilo procedure, 515
moving averages, 697
multidimensional contingency tables, 122–123 
multiple regression, 617–619 
mean absolute deviation, 699
model building, 654
new function names, 788
normal probabilities, 275
normal probability plot, 275–276 

	
Index	
835
one-tail tests, 372
one-way analysis of variance, 468 
opening workbooks, 37
ordered array, 115
quartiles, 174
Paired t test, 416
Pareto chart, 119
Pasting with Paste Special, 752
Percentage distribution, 116–117
Percentage polygon, 121
pie chart, 117 
PivotTables, 114
Poisson probabilities, 244
Pooled-variance t test, 414 
Population parameters, 174 
portfolio expected return, 243 
prediction interval, 568
preparing and using data, 
printing worksheets, 38 
probability, 211
probability distribution for a discrete random variable, 243
quadratic regression, 653
randomized block, 470
range, 174
recalculation, 750–751
recoding, 61
relative frequency distribution, 116–117
residual analysis, 567–568, 618–619
sample size determination, 333 
sampling distributions, 298
saving workbooks, 37
scatter plot, 122 
seasonal data, 699
selecting cell ranges for charts, 755–756
separate-variance t test, 415
side-by-side bar chart, 119
simple linear regression, 566–567
simple radom samples, 61–62
skill set needed, 36
slicers, 731
sparklines, 728
special note for Office 365 users, 767
standard deviation, 174
stem-and-leaf display, 119
summary tables, 113–114
t test for the mean (a unknown), 372
templates, 36, 751
time-series plot, 122
transformations, 653
treemaps, 729
two-way analysis of variance, 471 
Tukey-Kramer multiple comparisons, 469
understanding nonstatistical functions, 790–791
useful keyboard shortcuts, 787
variance, 174
variance inflationary factor (VIF), 
verifying formulas and worksheets, 788
Wilcoxon rank sum test, 516
Workbooks, 34
Worksheet entries and references, 750 
Worksheets, 34
Worksheet formatting, 752–754
Z test for the difference between two proportions, 417
Z test for the mean (a known), 371
Z scores, 174
Z test for the proportion, 372
Midspread, 150
Minitab
autoregressive modeling, 701
bar chart, 124
best-subsets regression, 656
binomial probabilities, 245
boxplot, 177
chi-square tests for contingency tables, 517
collinearity, 655
confidence interval for the mean, 334
confidence interval for the proportion, 334
contingency table, 124
correlation coefficient, 178
counting rules, 212
covariance, 177
creating and copying worksheets, 40
cross-tabulation table, 124
cumulative percentage polygon, 127–128
descriptive statistics, 176–177
dummy variables, 622
entering data, 39
establishing the variable type, 62
exponential probabilities, 277
exponential smoothing, 700
F test for the difference between variances, 420–421
FAQs, 794
histogram, 126–127
geometric mean, 176
hypergeometric probabilities, 246
Influence analysis, 623
Kruskal-Wallis test, 518
least-squares trend fitting, 701
Levene test, 473
logistic regression, 623
main effects plot, 472
model building, 655
moving averages, 700
multidimensional contingency tables, 128
multiple regression, 620–621
normal probabilities, 276
normal probability plot, 277
one-tail tests, 373
one-way analysis of variance, 472–473
opening worksheets and projects, 39–40
ordered array, 124
percentage polygon, 127
paired t test, 419–420
Pareto plot, 125
pie chart, 125
Poisson probabilities, 245–246
probability distribution for a discrete random variable, 
printing worksheets, 40
project, 34
quadratic regression, 654
randomized block design, 473–474
recoding variables, 63
residual analysis, 569
saving worksheets, 39–40
sampling distributions, 299 
sample size, 335

836	
Index
saving worksheets and projects, 39–40
scatter plot, 128
seasonal data, 701
side-by-side bar chart, 125
simple linear regression, 569
simple random samples, 63
stacked data, 124
stem-and-leaf display, 126
stepwise regression, 655
summary table, 123–124
t test for the difference between two means, 419
t test for the mean ( unknown), 373
three-dimensional plot, 620
time-series plot, 128
transforming variables, 655
Tukey-Kramer procedure, 473
two-way ANOVA, 474
unstacked data, 124
variance inflationary factors, 655
Wilcoxon rank sum test, 517
Z test for the mean (a known), 373
Z test for the difference between two proportions, 420
Z test for the proportion, 373–374
Mixed effects models, 459
Mode, 133–134
Models. See Multiple regression models
More Descriptive Choices Follow-up, 172, 274, 331, 413, 466, 
512–513, 652
Mountain States Potato Company case, 651, 727 
Moving averages, 660–662
Multidimensional contingency tables, 96–97 
Multidimensional scaling, 721
Multiple comparisons, 430
Multiple regression models, 572
Adjusted r, 578
best-subsets approach to, 640–641 
coefficient of multiple determination in, 577–578, 630 
coefficients of partial determination in, 590 
collinearity in, 636–637 
confidence interval estimates for the slope in, 583–584
dummy-variable models in, 592–594 
ethical considerations in, 646
interpreting slopes in, 573
interaction terms, 594–595
with k independent variables, 573
model building, 637–639
model validation, 644
net regression coefficients, 575
partial F-test statistic in, 586–589
pitfalls in, 646
predicting the dependent variable Y, 575–576
quadratic, 625–629 
residual analysis for, 581–582 
stepwise regression approach to, 639–640
testing for significance of, 579
testing portions of, 586–590
testing slopes in, 583–584
transformation in, 633–634 
variance inflationary factors in, 636 
Multilayer perceptons, 716
Multiplication rule, 196
Mutually exclusive events, 185
Mystatlab course outline,
Accessing, 757
N
Net regression coefficient, 575
Neural networks, 716
Nominal scale, 43–44
Nonparametric methods, 495
Nonprobability sample, 49
Nonresponse bias, 53
Nonresponse error, 53
Normal approximation to the binomial distribution, 270
Normal distribution, 248
cumulative standardized, 251 
properties of, 248 
Normal probabilities
calculating, 250–258 
Normal probability density function, 250
Normal probability plot, 263
constructing, 263 
Normality assumption, 433 
Null hypothesis, 337
Numerical descriptive measures
coefficient of correlation, 161–162 
measures of central tendency, variation, and shape, 130–144 
from a population, 155–159 
Numerical variables, 43 
Organizing, 70–77 
Visualizing, 86–90, 93–95 
O
Observed frequency, 477
Odds ratio, 601
Ogive, 89–90
One-tail tests, 356 
null and alternative hypotheses in, 356 
One-way analysis of variance (ANOVA), 
assumptions, 433 
F test for differences in more than two means, 426 
F test statistic, 426
Levene’s test for homogeneity of variance, 433–434
summary table, 427
Tukey-Kramer procedure, 430–432 
Online resources, 757, 
Operational definitions, 42
Ordered array, 70
Ordinal scale, 43–44 
Organize, 30, 42
Outliers, 141
Overall F test, 579
P
Paired t test, 388–393
Parameter, 47
Pareto chart,  81–83
Pareto principle, 81
Parsimony, 638
principle of, 684
Partial F-test statistic, 586–589
PDF files, 765
Percentage distribution, 73–75 
Percentage polygon, 88–89 
Percentiles, 149 
Permutation, 203 

	
Index﻿	
837
PHStat2
Autocorrelation, 568
bar chart, 117
basic probabilities, 211
best subsets regression, 654
binomial probabilities, 243–244
boxplot, 175
cell means plot, 472
chi-square (x2) test for contingency tables, 514–515 
confidence interval 
for the mean (s known), 332
for the mean (s unknown), 332 
for the difference between two means, 415
for the mean value, 568
for the proportion, 333
contingency tables, 114
covariance of a probability distribution, 243
cumulative percentage distributions, 115–116
cumulative polygons, 121–122 
exponential probabilities, 276
FAQs, 792–793
Files, 765
F test for ratio of two variances, 418
frequency distributions, 115 
histograms, 119–120 
hypergeometric probabilities, 244
installing, 765
Kruskal-Wallis test, 516
kurtosis, 174
Levene’s test, 469
Logistic regression, 620
Marascuilo procedure, 514–515
Mean, 173
Median, 173
Mode, 173
Model building, 654 
multiple regression, 617–620 
normal probabilities, 275
normal probability plot, 275
one-way ANOVA, 468
one-way tables, 113
one-tail tests, 372
opening, 768
paired t test, 416
Pareto chart, 118
Percentage distribution, 115–116
Percentage polygon, 121–122
pie chart, 117 
Poisson probabilities, 244
pooled-variance t test, 414
portfolio expected return, 243
portfolio risk, 243
prediction interval, 568
quartiles, 174
randomized block design, 470
Random sampling, 62 
Residual analysis, 567, 618
Sample size determination,
for the mean, 333
for the proportion, 333
sampling distributions, 298
scatter plot, 122 
separate-variance t test, 415
side-by-side bar chart, 118
simple linear regression, 566
simple probability, 211
simple random samples, 62
skewness, 174
stacked data, 115
standard deviation, 174
stem-and-leaf display, 119
stepwise regression, 654
summary tables, 113
t test for the mean (s unknown), 371
two-way ANOVA, 471
Tukey-Kramer procedure, 469
Unstacked data, 115
Wilcoxon rank sum test, 516
Z test for the mean (s known), 371
Z test for the difference in two proportions, 417
Z test for the proportion, 372 
Pie chart, 80–81 
PivotTables, 96–97
Platykurtic, 143
Point estimate, 301
Poisson distribution, 230
calculating probabilities, 231–232 
properties of, 230
Polygons, 121–122 
cumulative percentage, 121–122 
Pooled-variance t test, 376–381 
Population(s), 47
Population mean, 155, 280
Population standard deviation, 156, 280
Population variance, 156
Portfolio, 219
Portfolio expected return, 220
Portfolio risk, 220 
Power of a test, 341 
Practical significance, 364–365
Prediction interval estimate, 552–553 
Prediction line, 522
Predictive analytics, 703
Prescriptive analytics, 703
Primary data source, 46
Probability, 180
a priori, 181
Bayes’ theorem for, 197 
conditional, 189–191 
empirical, 181
ethical issues and, 205
joint, 184
marginal, 185
simple, 183
subjective, 181
Probability density function, 248
Probability distribution function, 223
Probability distribution for discrete random variable, 214
Probability sample, 49
Processing elements, 716
Proportions, 73
chi-square (x2) test for differences between two, 476–482 
chi-square (x2) test for differences in more than two, 483–486 
confidence interval estimation for, 315–317
sample size determination for, 320 
sampling distribution of, 290–292 

838	
Index
Z test for the difference between two, 399–400
Z test of hypothesis for, 360–363 
pth-order autoregressive model, 676
p-value, 345
p-value approach, 345–347 
Q
Quadratic regression, 625–629 
Quadratic trend model, 667–668 
Qualitative forecasting methods, 658
Qualitative variable, 42
Quantitative forecasting methods, 658
Quantitative variable, 43
Quartiles, 148 
Quantile-quantile plot, 263 
R
Random effect, 659
Random effects models, 459
Randomized block design 438–444
Randomness and independence, 433 
Random numbers, table of, 50, 770–771 
Range, 135–136 
interquartile, 150
Ratio scale, 45
Recoded variable, 48–49
Rectangular distribution, 265
Region of nonrejection, 339
Region of rejection, 339
Regression analysis. See Multiple regression models; Simple linear 
regression
Regression coefficients, 523
Regression trees, 714
Relative frequency, 73–74
Relative frequency distribution, 73–75
Relevant range, 525
Repeated measurements, 387
Replicates, 447
Residual analysis, 535, 683 
Residual plots
in detecting autocorrelation, 539–540 
in evaluating equal variance, 538
in evaluating linearity, 536
in evaluating normality, 537 
in multiple regression, 581–582 
Residuals, 535
Resistant measures, 151
Response variable, 521
Right-skewed, 142
Robust, 379 
S
Sample, 47
Sample mean, 130
Sample proportion, 315 
Sample standard deviation, 136–137
Sample variance, 136
Sample size determination
for mean, 318
for proportion, 320 
Sample space, 182
Samples, 
cluster, 51
convenience, 49
judgment, 50
nonprobability, 49
probability, 49
simple random, 50 
stratified, 51
systematic, 51
Sampling
from nonnormally distributed populations, 285–289 
from normally distributed populations, 282–285 
with replacement, 50
without replacement, 50
Sampling distributions, 279
of the mean, 279–289 
of the proportion, 290–292 
Sampling error, 53, 304
Scale
interval, 44
nominal, 43
ordinal, 43
ratio, 45
Scatter diagram, 520
Scatter plot, 93–94, 520
Seasonal effect, 659
Secondary data source, 46
Selection bias, 53
Separate-variance t test for differences in two means, 382–384 
Shape, 142 
Side-by-side bar chart, 83 
Simple event, 181
Simple linear regression, 
assumptions in, 535
avoiding pitfalls in, 557
coefficient of determination in, 531–532 
coefficients in, 523
computations in, 525–527 
Durbin-Watson statistic, 540–542 
equations in, 523
estimation of mean values and prediction of individual  
values, 551–553 
inferences about the slope and correlation coefficient, 543–548
least-squares method in, 522–523
pitfalls in, 555
residual analysis, 535–538 
standard error of the estimate in, 533–534
sum of squares in, 530–531 
Simple probability, 183
Simple random sample, 50
Single linkage, 720 
Skewness, 142–143
Slicers, 707–708
Slope, 525
inferences about, 544–547 
interpreting, in multiple regression, 573
Solver add-in,
Checking for presence, 769
Sources of data, 46
Sparklines, 704
Spread, 135
Square-root transformation, 633 
Stacked data, 77 
Standard deviation, 136–137 
of binomial distribution, 228
of discrete random variable, 216

	
Index	
839
of hypergeometric distribution, 235
of population, 156
of sum of two random variables, 219
Standard error of the estimate, 533–534 
Standard error of the mean, 281
Standard error of the proportion, 291
Standardized normal random variable, 250
Statistic, 31
Statistics, 31, 47
descriptive, 32
inferential, 32
Statistical inference, 32
Statistical package, 34
Statistical symbols, 749
Stem-and-leaf display, 86 
Stepwise regression
approach to model building, 639–640 
Strata, 51
Stratified sample, 51
Stress statistic, 722
Structured data, 47
Studentized deleted residuals, ti, 607
Studentized range distribution, 
	
tables, 782–783 
Student’s t distribution, 307 
Student tips, 31, 42, 47, 65, 67, 72, 74, 86, 133, 137, 140, 148, 180, 
181, 182, 186, 190, 214, 218, 223, 226, 250, 252, 253, 281, 
290, 301, 306 , 315, 337, 339, 342, 343, 345, 350, 356, 360, 
376, 377, 388, 395, 402, 423, 424, 425, 428, 430, 433, 448, 
477, 478, 487, 490, 495, 496, 501, 523, 524, 532, 533, 536, 
574, 575, 578, 579, 581, 590, 593, 595, 602, 625, 627, 630, 
633, 642, 660, 669, 676, 680, 681, 706, 711, 719, 720
Subjective probability, 181
Summary table, 66 
Summation notation, 746–748 
Sum of squares, 136
Sum of squares among blocks (SSBL), 439
Sum of squares among groups (SSA), 425 
Sum of squares due to factor A (SSA), 447 
Sum of squares due to factor B (SSB), 448
Sum of squares due to regression (SSR), 531
Sum of squares of error (SSE), 439, 448, 531
Sum of squares to interaction (SSAB), 448
Sum of squares total (SST), 423, 424, 439, 448, 530
Sum of squares within groups (SSW), 425
SureValue Convenience Stores, 331, 370, 412, 466, 511, 651
Survey errors, 52–54 
Symmetrical, 142
Systematic sample, 51
T
Tableau public
Bullet graphs, 729–730
Treemaps, 731
Tables
chi-square, 776
contingency, 67 
Control chart factors, 785
Durbin-Watson, 784
F distribution, 777–780 
for categorical data, 65–68 
cumulative standardized normal distribution, 772–773
of random numbers, 50, 770–771 
standardized normal distribution, 786
Studentized range, 782–783 
summary, 66
t distribution, 774–775 
Wilcoxon rank sum, 781
t distribution, properties of, 308
Test statistic, 339
Tests of hypothesis
Chi-square (x2) test for differences
between c proportions, 483–486 
between two proportions, 476–482 
Chi-square (x2) test of independence, 489–494 
F test for the ratio of two variances, 402–405
F test for the regression model, 588
F test for the slope, 545–546
Kruskal-Wallis rank test for differences in c medians, 501–504 
Levene test, 433–434 
Paired t test, 388–393 
pooled-variance t test, 376–381 
separate-variance t test for differences in two means, 382–384 
t test for the correlation coefficient, 547–548 
t test for the mean (s unknown), 349–352 
t test for the slope, 544–545, 583
Wilcoxon rank sum test for differences in two medians, 495–499 
Z test for the mean (s known), 342 
Z test for the difference between two proportions, 395–399 
Z test for the proportion, 360–363
Think About This, 200, 259, 384, 692 
Third quartile, 148
Times series, 658
Time-series forecasting
autoregressive model, 675–681 
choosing an appropriate forecasting model, 683–685 
component factors of classical multiplicative, 658–659
exponential smoothing in, 662–664 
least-squares trend fitting and forecasting, 665–673 
moving averages in, 660–662 
seasonal data, 686–691 
Times series plot, 94–95
Total variation, 424, 439, 447
Training data, 717
Transformation formula, 250
Transformations in regression models
logarithmic, 633–634 
square-root, 633 
Treemap, 705–706
Trend, 659
t test for a correlation coefficient, 547–548 
t test for the mean (s unknown), 349–352 
t test for the slope, 544–545, 583
Tukey-Kramer multiple comparison procedure, 430–432 
Tukey multiple comparison procedure, 443–444
Two-factor factorial design, 446
Two-sample tests of hypothesis for numerical data, 
F tests for differences in two variances, 402–405 
Paired t test, 388–393   
t tests for the difference in two means, 376–384 
Wilcoxon rank sum test for differences in two medians, 495–499 
Two-tail test, 342
Two-way analysis of variance
cell means plot, 454
factorial design, 446 
interpreting interaction effects, 454–455 

840	
Index
multiple comparisons, 452–453 
testing for factor and interaction effects, 449–450
Type I error, 340
Type II error, 340
U
Unbiased, 279
Unexplained variation or error sum of squares (SSE), 530–531 
Uniform probability distribution, 265
mean, 265
standard deviation, 266
Unstacked data, 77
Unstructured data, 47
V
Variables, 31
categorical, 42
continuous, 43
discrete, 43
dummy, 592–594 
numerical, 43
Variance inflationary factor (VIF), 636 
Variance, 
of discrete random variable, 215 
F-test for the ratio of two, 402–405
Levene’s test for homogeneity of, 433–434 
of the sum of two random variables, 219
population, 156
sample, 136–137 
Variation, 130 
Venn diagrams, 183
Visual Explorations, 765
descriptive statistics, 145 
normal distribution, 259
sampling distributions, 289
simple linear regression, 527–528
using, 769
Visualize, 30, 42
Visualizations,
Guidelines for constructing, 102
W
Wald statistic, 604
Ward’s minimum variance method, 720
Width of class interval, 71
Wilcoxon rank sum test
for differences in two medians, 495–499 
Tables, 774–775
Wilcoxon signed ranks test, 506
Within-group variation, 425
X
 
Y
Y intercept b0, 522
Z
Z scores, 141
Z test,
for the difference between two proportions, 399–400 
for the mean (s known), 342 
for the proportion, 360–363 


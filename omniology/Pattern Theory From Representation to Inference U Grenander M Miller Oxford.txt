
PATTERN THEORY

This page intentionally left blank 

PATTERN THEORY: FROM
REPRESENTATION
TO INFERENCE
Ulf Grenander and Michael I. Miller
1

3
Great Clarendon Street, Oxford OX2 6DP
Oxford University Press is a department of the University of Oxford.
It furthers the University’s objective of excellence in research, scholarship,
and education by publishing worldwide in
Oxford NewYork
Auckland Cape Town Dar es Salaam Hong Kong Karachi
Kuala Lumpur Madrid Melbourne Mexico City Nairobi
New Delhi Shanghai Taipei Toronto
With ofﬁces in
Argentina Austria Brazil Chile Czech Republic France Greece
Guatemala Hungary Italy Japan Poland Portugal Singapore
South Korea Switzerland Thailand Turkey Ukraine Vietnam
Oxford is a registered trade mark of Oxford University Press
in the UK and in certain other countries
Published in the United States
by Oxford University Press Inc., New York
© Oxford University Press, 2007
The moral rights of the authors have been asserted
Database right Oxford University Press (maker)
First published 2007
All rights reserved. No part of this publication may be reproduced,
stored in a retrieval system, or transmitted, in any form or by any means,
without the prior permission in writing of Oxford University Press,
or as expressly permitted by law, or under terms agreed with the appropriate
reprographics rights organization. Enquiries concerning reproduction
outside the scope of the above should be sent to the Rights Department,
Oxford University Press, at the address above
You must not circulate this book in any other binding or cover
and you must impose the same condition on any acquirer
British Library Cataloguing in Publication Data
Data available
Library of Congress Cataloging in Publication Data
Data available
Typeset by Newgen Imaging Systems (P) Ltd., Chennai, India
Printed in Great Britain
on acid-free paper by
Antony Rowe Ltd., Chippenham, Wiltshire
ISBN 0–19–850570–1
978–0–19–850570–9
ISBN 0–19–929706–1
978–0–19–929706–1
1 3 5 7 9 10 8 6 4 2

CONTENTS
1
Introduction
1
1.1
Organization
3
2
The Bayes Paradigm, Estimation and Information Measures
5
2.1
Bayes Posterior Distribution
5
2.1.1
Minimum Risk Estimation
6
2.1.2
Information Measures
7
2.2
Mathematical Preliminaries
8
2.2.1
Probability Spaces, Random Variables, Distributions,
Densities, and Expectation
8
2.2.2
Transformations of Variables
10
2.2.3
The Multivariate Normal Distribution
10
2.2.4
Characteristic Function
11
2.3
Minimum Risk Hypothesis Testing on Discrete Spaces
12
2.3.1
Minimum Probability of Error via Maximum A Posteriori
Hypothesis Testing
13
2.3.2
Neyman–Pearson and the Optimality of the Likelihood Ratio Test
14
2.4
Minimum Mean-Squared Error Risk Estimation in Vector Spaces
16
2.4.1
Normed Linear and Hilbert Spaces
17
2.4.2
Least-Squares Estimation
20
2.4.3
Conditional Mean Estimation and Gaussian Processes
22
2.5
The Fisher Information of Estimators
24
2.6
Maximum-Likelihood and its consistency
26
2.6.1
Consistency via Uniform Convergence of Empirical Log-likelihood
27
2.6.2
Asymptotic Normality and √n Convergence Rate of the MLE
28
2.7
Complete–Incomplete Data Problems and the EM Algorithm
30
2.8
Hypothesis Testing and Model Complexity
38
2.8.1
Model-Order Estimation and the d/2 log Sample-Size Complexity
38
2.8.2
The Gaussian Case is Special
41
2.8.3
Model Complexity and the Gaussian Case
42
2.9
Building Probability Models via the Principle of Maximum Entropy
43
2.9.1
Principle of Maximum Entropy
44
2.9.2
Maximum Entropy Models
45
2.9.3
Conditional Distributions are Maximum Entropy
47
3
Probabilistic Directed Acyclic Graphs and Their Entropies
49
3.1
Directed Acyclic Graphs (DAGs)
49
3.2
Probabilities on Directed Acyclic Graphs (PDAGs)
51
3.3
Finite State Markov Chains
54
3.4
Multi-type Branching Processes
56
3.4.1
The Branching Matrix
59
3.4.2
The Moment-Generating Function
60
3.5
Extinction for Finite-State Markov Chains and Branching Processes
62
3.5.1
Extinction in Markov Chains
62
3.5.2
Extinction in Branching Processes
63
3.6
Entropies of Directed Acyclic Graphs
64
3.7
Combinatorics of Independent, Identically Distributed Strings via the
Aymptotic Equipartition Theorem
65
v

vi
CONTENTS
3.8
Entropy and Combinatorics of Markov Chains
66
3.9
Entropies of Branching Processes
68
3.9.1
Tree Structure of Multi-Type Branching Processes
69
3.9.2
Entropies of Sub-Critical, Critical, and Super-Critical Processes
70
3.9.3
Typical Trees and the Equipartition Theorem
71
3.10
Formal Languages and Stochastic Grammars
74
3.11
DAGs for Natural Language Modelling
81
3.11.1
Markov Chains and m-Grams
81
3.11.2
Context-Free Models
82
3.11.3
Hierarchical Directed Acyclic Graph Model
84
3.12
EM Algorithms for Parameter Estimation in Hidden
Markov Models
87
3.12.1
MAP Decoding of the Hidden State Sequence
88
3.12.2
ML Estimation of HMM parameters via EM Forward/Backward
Algorithm
89
3.13
EM Algorithms for Parameter Estimation in Natural
Language Models
92
3.13.1
EM Algorithm for Context-Free Chomsky Normal Form
93
3.13.2
General Context-Free Grammars and the Trellis Algorithm
of Kupiec
94
4
Markov Random Fields on Undirected Graphs
95
4.1
Undirected Graphs
95
4.2
Markov Random Fields
96
4.3
Gibbs Random Fields
101
4.4
The Splitting Property of Gibbs Distributions
104
4.5
Bayesian Texture Segmentation: The log-Normalizer Problem
110
4.5.1
The Gibbs Partition Function Problem
110
4.6
Maximum-Entropy Texture Representation
112
4.6.1
Empirical Maximum Entropy Texture Coding
113
4.7
Stationary Gibbs Random Fields
116
4.7.1
The Dobrushin/Lanford/Ruelle Deﬁnition
116
4.7.2
Gibbs Distributions Exhibit Multiple Laws with the Same Interactions
(Phase Transitions): The Ising Model at Low Temperature
117
4.8
1D Random Fields are Markov Chains
119
4.9
Markov Chains Have a Unique Gibbs Distribution
120
4.10
Entropy of Stationary Gibbs Fields
121
5
Gaussian Random Fields on Undirected Graphs
123
5.1
Gaussian Random Fields
123
5.2
Difference Operators and Adjoints
124
5.3
Gaussian Fields Induced via Difference Operators
126
5.4
Stationary Gaussian Processes on Zd and their Spectrum
133
5.5
Cyclo-Stationary Gaussian Processes and their Spectrum
134
5.6
The log-Determinant Covariance and the Asymptotic Normalizer
137
5.6.1
Asymptotics of the Gaussian processes and their Covariance
138
5.6.2
The Asymptotic Covariance and log-Normalizer
142
5.7
The Entropy Rates of the Stationary Process
142
5.7.1
Burg’s Maximum Entropy Auto-regressive Processes on Zd
143
5.8
Generalized Auto-Regressive Image Modelling via Maximum-Likelihood
Estimation
144
5.8.1
Anisotropic Textures
147

CONTENTS
vii
6
The Canonical Representations of General Pattern Theory
154
6.1
The Generators, Conﬁgurations, and Regularity of Patterns
154
6.2
The Generators of Formal Languages and Grammars
158
6.3
Graph Transformations
162
6.4
The Canonical Representation of Patterns: DAGs, MRFs, Gaussian Random Fields
166
6.4.1
Directed Acyclic Graphs
167
6.4.2
Markov Random Fields
169
6.4.3
Gaussian Random Fields: Generators induced via difference operators
170
7
Matrix Group Actions Transforming Patterns
174
7.1
Groups Transforming Conﬁgurations
174
7.1.1
Similarity Groups
174
7.1.2
Group Actions Deﬁning Equivalence
175
7.1.3
Groups Actions on Generators and Deformable Templates
177
7.2
The Matrix Groups
177
7.2.1
Linear Matrix and Afﬁne Groups of Transformation
177
7.2.2
Matrix groups acting on Rd
179
7.3
Transformations Constructed from Products of Groups
181
7.4
Random Regularity on the Similarities
184
7.5
Curves as Submanifolds and the Frenet Frame
190
7.6
2D Surfaces in R3 and the Shape Operator
195
7.6.1
The Shape Operator
196
7.7
Fitting Quadratic Charts and Curvatures on Surfaces
198
7.7.1
Gaussian and Mean Curvature
198
7.7.2
Second Order Quadratic Charts
200
7.7.3
Isosurface Algorithm
201
7.8
Ridge Curves and Crest Lines
205
7.8.1
Deﬁnition of Sulcus, Gyrus, and Geodesic Curves on
Triangulated Graphs
205
7.8.2
Dynamic Programming
207
7.9
Bijections and Smooth Mappings for Coordinatizing
Manifolds via Local Coordinates
210
8
Manifolds, Active Models, and Deformable Templates
214
8.1
Manifolds as Generators, Tangent Spaces, and Vector Fields
214
8.1.1
Manifolds
214
8.1.2
Tangent Spaces
215
8.1.3
Vector Fields on M
217
8.1.4
Curves and the Tangent Space
218
8.2
Smooth Mappings, the Jacobian, and Diffeomorphisms
219
8.2.1
Smooth Mappings and the Jacobian
219
8.2.2
The Jacobian and Local Diffeomorphic Properties
221
8.3
Matrix Groups are Diffeomorphisms which are a Smooth Manifold
222
8.3.1
Diffeomorphisms
222
8.3.2
Matrix Group Actions are Diffeomorphisms on the
Background Space
223
8.3.3
The Matrix Groups are Smooth Manifolds (Lie Groups)
224
8.4
Active Models and Deformable Templates as Immersions
226
8.4.1
Snakes and Active Contours
226
8.4.2
Deforming Closed Contours in the Plane
226
8.4.3
Normal Deformable Surfaces
227
8.5
Activating Shapes in Deformable Models
229
8.5.1
Likelihood of Shapes Partitioning Image
229
8.5.2
A General Calculus for Shape Activation
229

viii
CONTENTS
8.5.3
Active Closed Contours in R2
232
8.5.4
Active Unclosed Snakes and Roads
234
8.5.5
Normal Deformation of Circles and Spheres
236
8.5.6
Active Deformable Spheres
236
8.6
Level Set Active Contour Models
237
8.7
Gaussian Random Field Models for Active Shapes
240
9
Second Order and Gaussian Fields
244
9.1
Second Order Processes (SOP) and the Hilbert Space of Random Variables
244
9.1.1
Measurability, Separability, Continuity
244
9.1.2
Hilbert space of random variables
247
9.1.3
Covariance and Second Order Properties
249
9.1.4
Quadratic Mean Continuity and Integration
251
9.2
Orthogonal Process Representations on Bounded Domains
252
9.2.1
Compact Operators and Covariances
253
9.2.2
Orthogonal Representations for Random Processes and Fields
257
9.2.3
Stationary Periodic Processes and Fields on Bounded Domains
258
9.3
Gaussian Fields on the Continuum
262
9.4
Sobolev Spaces, Green’s Functions, and Reproducing
Kernel Hilbert Spaces
264
9.4.1
Reproducing Kernel Hilbert Spaces
265
9.4.2
Sobolev Normed Spaces
266
9.4.3
Relation to Green’s Functions
267
9.4.4
Gradient and Laplacian Induced Green’s Kernels
267
9.5
Gaussian Processes Induced via Linear Differential Operators
271
9.6
Gaussian Fields in the Unit Cube
274
9.6.1
Maximum Likelihood Estimation of the Fields: Generalized ARMA
Modelling
278
9.6.2
Small Deformation Vector Fields Models in the Plane and Cube
280
9.7
Discrete Lattices and Reachability of Cyclo-Stationary Spectra
283
9.8
Stationary Processes on the Sphere
285
9.8.1
Laplacian Operator Induced Gaussian Fields on the Sphere
289
9.9
Gaussian Random Fields on an Arbitrary Smooth Surface
293
9.9.1
Laplace-Beltrami Operator with Neumann Boundary Conditions
293
9.9.2
Smoothing an Arbitrary Function on Manifolds by Orthonormal Bases
of the Laplace-Beltrami Operator
297
9.10
Sample Path Properties and Continuity
299
9.11
Gaussian Random Fields as Prior Distributions in Point
Process Image Reconstruction
303
9.11.1
The Need for Regularization in Image Reconstruction
304
9.11.2
Smoothness and Gaussian Priors
304
9.11.3
Good’s Roughness as a Gaussian Prior
305
9.11.4
Exponential Spline Smoothing via Good’s Roughness
306
9.12
Non-Compact Operators and Orthogonal Representations
309
9.12.1
Cramer Decomposition for Stationary Processes
311
9.12.2
Orthogonal Scale Representation
312
10 Metrics Spaces for the Matrix Groups
316
10.1
Riemannian Manifolds as Metric Spaces
316
10.1.1
Metric Spaces and Smooth Manifolds
316
10.1.2
Riemannian Manifold, Geodesic Metric, and Minimum Energy
317
10.2
Vector Spaces as Metric Spaces
319
10.3
Coordinate Frames on the Matrix Groups and
the Exponential Map
320

CONTENTS
ix
10.3.1
Left and Right Group Action
320
10.3.2
The Coordinate Frames
321
10.3.3
Local Optimization via Directional Derivatives and the
Exponential Map
323
10.4
Metric Space Structure for the Linear Matrix Groups
324
10.4.1
Geodesics in the Matrix Groups
324
10.5
Conservation of Momentum and Geodesic Evolution of the
Matrix Groups via the Tangent at the Identity
326
10.6
Metrics in the Matrix Groups
327
10.7
Viewing the Matrix Groups in Extrinsic Euclidean Coordinates
329
10.7.1
The Frobenius Metric
329
10.7.2
Comparing intrinsic and extrinsic metrics in SO(2,3)
330
11 Metrics Spaces for the Inﬁnite Dimensional Diffeomorphisms
332
11.1
Lagrangian and Eulerian Generation of Diffeomorphisms
332
11.1.1
On Conditions for Generating Flows of Diffeomorphisms
333
11.1.2
Modeling via Differential Operators and the Reproducing
Kernel Hilbert Space
335
11.2
The Metric on the Space of Diffeomorphisms
336
11.3
Momentum Conservation for Geodesics
338
11.4
Conservation of Momentum for Diffeomorphism Splines
Speciﬁed on Sparse Landmark Points
340
11.4.1
An ODE for Diffeomorphic Landmark Mapping
343
12 Metrics on Photometric and Geometric Deformable Templates
346
12.1
Metrics on Dense Deformable Templates: Geometric Groups
Acting on Images
346
12.1.1
Group Actions on the Images
346
12.1.2
Invariant Metric Distances
347
12.2
The Diffeomorphism Metric for the Image Orbit
349
12.3
Normal Momentum Motion for Geodesic Connection Via
Inexact Matching
350
12.4
Normal Momentum Motion for Temporal Sequences
354
12.5
Metric Distances Between Orbits Deﬁned Through
Invariance of the Metric
356
12.6
Finite Dimensional Landmarked Shape Spaces
357
12.6.1
The Euclidean Metric
357
12.6.2
Kendall’s Similitude Invariant Distance
359
12.7
The Diffeomorphism Metric and Diffeomorphism Splines on Landmark Shapes
361
12.7.1
Small Deformation Splines
361
12.8
The Deformable Template: Orbits of Photometric and
Geometric Variation
365
12.8.1
Metric Spaces for Photometric Variability
365
12.8.2
The Metrics Induced via Photometric and Geometric Flow
366
12.9
The Euler Equations for Photometric and Geometric Variation
369
12.10 Metrics between Orbits of the Special Euclidean Group
373
12.11 The Matrix Groups (Euclidean and Afﬁne Motions)
374
12.11.1 Computing the Afﬁne Motions
376
13 Estimation Bounds for Automated Object Recognition
378
13.1
The Communications Model for Image Transmission
378
13.1.1
The Source Model: Objects Under Matrix Group Actions
379
13.1.2
The Sensing Models: Projective Transformations in Noise
379
13.1.3
The Likelihood and Posterior
379

x
CONTENTS
13.2
Conditional Mean Minimum Risk Estimation
381
13.2.1
Metrics (Risk) on the Matrix Groups
381
13.2.2
Conditional Mean Minimum Risk Estimators
382
13.2.3
Computation of the HSE for SE(2,3)
384
13.2.4
Discrete integration on SO(3)
385
13.3
MMSE Estimators for Projective Imagery Models
385
13.3.1
3D to 2D Projections in Gaussian Noise
385
13.3.2
3D to 2D Synthetic Aperture Radar Imaging
389
13.3.3
3D to 2D LADAR Imaging
392
13.3.4
3D to 2D Poisson Projection Model
393
13.3.5
3D to 1D Projections
395
13.3.6
3D(2D) to 3D(2D) Medical Imaging Registration
397
13.4
Parameter Estimation and Fisher Information
398
13.5
Bayesian Fusion of Information
402
13.6
Asymptotic Consistency of Inference and Symmetry Groups
405
13.6.1
Consistency
405
13.6.2
Symmetry Groups and Sensor Symmetry
406
13.7
Hypothesis Testing and Asymptotic Error-Exponents
407
13.7.1
Analytical Representations of the Error Probabilities and the
Bayesian Information Criterion
408
13.7.2
m-ary Multiple Hypotheses
412
14 Estimation on Metric Spaces with Photometric Variation
414
14.1
The Deformable Template: Orbits of Signature and
Geometric Variation
414
14.1.1
The Robust Deformable Templates
414
14.1.2
The Metric Space of the Robust Deformable Template
415
14.2
Empirical Covariance of Photometric Variability
via Principle Components
416
14.2.1
Signatures as a Gaussian Random Field Constructed from
Principle Components
417
14.2.2
Algorithm for Empirical Construction of Bases
418
14.3
Estimation of Parameters on the Conditionally Gaussian
Random Field Models
422
14.4
Estimation of Pose by Integrating Out EigenSignatures
424
14.4.1
Bayes Integration
427
14.5
Multiple Modality Signature Registration
429
14.6
Models for Clutter: The Transported Generator Model
431
14.6.1
Characteristic Functions and Cumulants
432
14.7
Robust Deformable Templates for Natural Clutter
438
14.7.1
The Euclidean Metric
439
14.7.2
Metric Space Norms for Clutter
439
14.7.3
Computational Scheme
442
14.7.4
Empirical Construction of the Metric from Rendered Images
444
14.8
Target detection/identiﬁcation in EO imagery
445
15 Information Bounds for Automated Object Recognition
447
15.1
Mutual Information for Sensor Systems
447
15.1.1
Quantifying Multiple-Sensor Information Gain Via Mutual
Information
447
15.1.2
Quantifying Information Loss with Model Uncertainty
449
15.1.3
Asymptotic Approximation of Information Measures
452

CONTENTS
xi
15.2
Rate-Distortion Theory
456
15.2.1
The Rate-Distortion Problem
456
15.3
The Blahut Algorithm
457
15.4
The Remote Rate Distortion Problem
459
15.4.1
Blahut Algorithm extended
460
15.5
Output Symbol Distribution
465
16 Computational Anatomy: Shape, Growth and Atrophy Comparison via
Diffeomorphisms
468
16.1
Computational Anatomy
468
16.1.1
Diffeomorphic Study of Anatomical Submanifolds
469
16.2
The Anatomical Source Model of CA
470
16.2.1
Group Actions for the Anatomical Source Model
472
16.2.2
The Data Channel Model
473
16.3
Normal Momentum Motion for Large Deformation Metric
Mapping (LDDMM) for Growth and Atrophy
474
16.4
Christensen Non-Geodesic Mapping Algorithm
478
16.5
Extrinsic Mapping of Surface and Volume Submanifolds
480
16.5.1
Diffeomorphic Mapping of the Face
481
16.5.2
Diffeomorphic Mapping of Brain Submanifolds
481
16.5.3
Extrinsic Mapping of Subvolumes for Automated
Segmentation
481
16.5.4
Metric Mapping of Cortical Atlases
483
16.6
Heart Mapping and Diffusion Tensor Magnetic
Resonance Imaging
484
16.7
Vector Fields for Growth
488
16.7.1
Growth from Landmarked Shape Spaces
488
17 Computational Anatomy: Hypothesis Testing on Disease
494
17.1
Statistics Analysis for Shape Spaces
494
17.2
Gaussian Random Fields
495
17.2.1
Empirical Estimation of Random Variables
496
17.3
Shape Representation of the Anatomical Orbit Under Large Deformation
Diffeomorphisms
496
17.3.1
Principal Component Selection of the Basis from
Empirical Observations
497
17.4
The Momentum of Landmarked Shape Spaces
498
17.4.1
Geodesic evolution equations for landmarks
498
17.4.2
Small Deformation PCA Versus Large Deformation PCA
499
17.5
The Small Deformation Setting
502
17.6
Small Deformation Gaussian Fields on Surface Submanifolds
502
17.7
Disease Testing of Automorphic Pathology
503
17.7.1
Hypothesis Testing on Disease in the Small Noise Limit
503
17.7.2
Statistical Testing
505
17.8
Distribution Free Testing
510
17.9
Heteromorphic Tumors
511
18 Markov Processes and Random Sampling
514
18.1
Markov Jump Processes
514
18.1.1
Jump Processes
515
18.2
Random Sampling and Stochastic Inference
516
18.2.1
Stationary or Invariant Measures
517
18.2.2
Generator for Markov Jump Processes
519
18.2.3
Jump Process Simulation
520
18.2.4
Metropolis–Hastings Algorithm
521

xii
CONTENTS
18.3
Diffusion Processes for Simulation
523
18.3.1
Generators of 1D Diffusions
525
18.3.2
Diffusions and SDEs for Sampling
527
18.4
Jump-Diffusion Inference on Countable Unions of Spaces
528
18.4.1
The Basic Problem
529
19 Jump Diffusion Inference in Complex Scenes
532
19.1
Recognition of Ground Vehicles
533
19.1.1
CAD Models and the Parameter Space
533
19.1.2
The FLIR Sensor Model
534
19.2
Jump Diffusion for Sampling the Target Recognition Posterior
536
19.2.1
The Posterior distribution
536
19.2.2
The Jump Diffusion Algorithms
536
19.2.3
Jumps via Gibbs’ Sampling
539
19.2.4
Jumps via Metropolis–Hastings Acceptance/Rejection
541
19.3
Experimental Results for FLIR and LADAR
543
19.3.1
Detection and Removal of Objects
543
19.3.2
Identiﬁcation
543
19.3.3
Pose and Identiﬁcation
544
19.3.4
Identiﬁcation and recognition via High Resolution Radar (HRR)
546
19.3.5
The Dynamics of Pose Estimation via the Jump–Diffusion Process
546
19.3.6
LADAR Recognition
548
19.4
Powerful Prior Dynamics for Airplane Tracking
549
19.4.1
The Euler-Equations Inducing the Prior on Airplane Dynamics
550
19.4.2
Detection of Airframes
552
19.4.3
Pruning via the Prior distribution
552
19.5
Deformable Organelles: Mitochondria and Membranes
553
19.5.1
The Parameter Space for Contour Models
553
19.5.2
Stationary Gaussian Contour Model
554
19.5.3
The Electron Micrograph Data Model: Conditional Gaussian Random
Fields
555
19.6
Jump–Diffusion for Mitochondria
556
19.6.1
The jump parameters
557
19.6.2
Computing gradients for the drifts
557
19.6.3
Jump Diffusion for Mitochondria Detection and Deformation
558
19.6.4
Pseudolikelihood for Deformation
560
References
563
Index
581

1
INTRODUCTION
This book is to be an accessible book on patterns, their representation, and inference. There
are a small number of ideas and techniques that, when mastered, make the subject more
accessible. This book has arisen from ten years of a research program which the authors
have embarked upon, building on the more abstract developments of metric pattern theory
developed by one of the authors during the 1970s and 1980s. The material has been taught
over multiple semesters as part of a second year graduate-level course in pattern theory,
essentially an introduction for students interested in the representation of patterns which are
observed in the natural world. The course has attracted students studying biomedical engi-
neering, computer science, electrical engineering, and applied mathematics interested in speech
recognition and computational linguistics, as well as areas of image analysis, and computer
vision.
Now the concept of patterns pervades the history of intellectual endeavor; it is one of the
eternal followers in human thought. It appears again and again in science, taking on different
forms in the various disciplines, and made rigorous through mathematical formalization. But the
concept also lives in a less stringent form in the humanities, in novels and plays, even in everyday
language. We use it all the time without attributing a formal meaning to it and yet with little risk
of misunderstanding. So, what do we really mean by a pattern? Can we deﬁne it in strictly logical
terms? And if we can, what use can we make of such a deﬁnition?
These questions were answered by General Pattern Theory, a discipline initiated by Ulf
Grenander in the late 1960s [1–5]. It has been an ambitious effort with the only original sketchy
program having few if any practical applications, growing in mathematical maturity with a mul-
titude of applications having appeared in biology/medicine and in computer vision, in language
theory and object recognition, to mention but a few. Pattern theory attempts to provide an algebraic
framework for describing patterns as structures regulated by rules, essentially a ﬁnite number of
both local and global combinatory operations. Pattern theory takes a compositional view of the
world, building more and more complex structures starting from simple ones. The basic rules for
combining and building complex patterns from simpler ones are encoded via graphs and rules on
transformation of these graphs.
In contrast to other dominating scientiﬁc themes, in Pattern Theory we start from the
belief that real world patterns are complex: Galielaen simpliﬁcation that has been so successful in
physics and other natural sciences will not sufﬁce when it comes to explaining other regulari-
ties, for example in the life sciences. If one accepts this belief it follows that complexity must
be allowed in the ensuing representations of knowledge. For this, probabilities naturally enter,
superimposed on the graphs so as to express the variability of the real world by describing its
ﬂuctuations as randomness. Take as a goal the development of algorithms which assist in the
ambitious task of image understanding or recognition. Imagine an expert studying a natural scene,
trying to understand it in terms of the awesome body of knowledge that is informally avail-
able to humans about the context of the scene: identify components, relate them to each other,
make statements about the ﬁne structure as well as the overall appearance. If it is truly the
goal to create algorithmic tools which assist experts in carrying out the time-consuming labor
of pattern analysis, while leaving the ﬁnal decision to their judgment, to arrive at more than ad
hoc algorithms the subject matter knowledge must be expressed precisely and as compactly as
possible.
This is the central focus of the book: ‘How can such empirical knowledge be represented
in mathematical form, including both structure and the all important variability?’ This task of
presenting an organized and coherent view of the ﬁeld of Pattern theory seems bewildering at
best. But what are today’s challenges in signal, data and pattern analysis? With the advent of
1

2
1 INTRODUCTION
geometric increases in computational and storage resources, there has been a dramatic increase
in the solution of highly complex pattern representation and recognition problems. Historically
books on pattern recognition present a diverse set of problems with diverse methods for building
recognition algorithms, each approach handcrafted to the particular task. The complexity and
diversity of patterns in the world presents one of the most signiﬁcant challenges to the pedagogical
approach to the teaching of Pattern theory. Real world patterns are often the results of evolutionary
change, and most times cannot be selected by the practitioner to have particular properties. The
representations require models using mathematics which span multiple ﬁelds in algebra, geometry,
statistics and statistical communications.
Contrasting this to the now classical ﬁeld of statistical communications, it might appear that
the task seems orders of magnitude bigger than modelling signal ensembles in the communica-
tion environment. Thinking historically of the now classical ﬁeld of statistical communications,
the discipline can be traced back far, to Helmholtz and earlier, but here we are thinking of its
history in the twentieth century. For the latter a small number of parameters may be needed,
means, covariances, for Gaussian noise, or the spectral density of a signal source, and so on.
The development of communication engineering from the 1920s on consisted in part of formal-
izing the observed, more or less noisy, signals. Statistical signal processing is of course one of
the great success stories of statistics/engineering. It is natural to ask why. We believe that it was
because the pioneers in the ﬁeld managed to construct representations of signal ensembles, mod-
els that were realistic and at the same time tractable both analytically and computationally (by
analog devices at the time). The classical signalling models: choose s0(t), s1(t) to be orthogonal
elements in L2, with the noise model additive stationary noise with covariance representation via
a complete orthonormal basis. Such a beautiful story, utilizing ideas from Fourier analysis, sta-
tionary stochastic processes, Toeplitz forms, and Bayesian inference! Eventually this resulted in
more or less automated procedures for the detection and understanding of noisy signals: matched
ﬁlters, optimal detectors, and the like. Today these models are familiar, they look simple and
natural, but in a historical perspective the phenomena must have appeared highly complex and
bewildering.
We believe the same to be true for pattern theory. The central challenge is the formalization of
a small set of ideas for constructing the representations of the patterns themselves which accommodate
variability and structure simultaneously. This is the point of view from which this book is written.
Even though the ﬁeld of pattern theory has grown considerably over the past 30 years, we have
striven to emphasize its coherence. There are essentially two overarching principles. The ﬁrst is the
representation of regularity via graphs which essentially encode the rules of combination which
allow for the generation of complex structures from simpler ones. The second is the application of
transformations to generate from the exemplars entire orbits. To represent typicality probabilistic
structures are superimposed on the graphs and the rules of transformation. Naturally then the
conditional probabilities encode the regularity of the patterns, and become the central tool for
studying pattern formation.
We have been drawn to the ﬁeld of pattern theory from backgrounds in communication
theory, probability theory and statistics. The overall framework ﬁts comfortably within the source-
channel view of Shannon. The underlying deep regular structures are descriptions of the source,
which are hidden via the sensing channel. We believe that the principle challenge is the represen-
tation of the source of patterns, and for that reason the majority of the book is focused precisely
on this topic. A multiplicity of channels or sensor models will be used throughout the book, those
appropriate for the pattern class being studied. They are however studied superﬁcially, drawn
from the engineering literature and taken as given, but certainly studied more deeply elsewhere.
The channel sensor models of course shape the overall performance of the inference algorithms;
but the major focus of our work is on the development of stochastic models for the structural
understanding of the variabilities of the patterns at the source. This also explains the major devia-
tion of this pattern theory from that which has come to be known as pattern recognition. Only in
the ﬁnal chapters will pattern recognition algorithms be studied, attempting to answer the ques-
tion of how well the algorithm can estimate (recognize) the source when seen through the noisy
sensor channel.

1.1 ORGANIZATION
3
1.1 Organization
Throughout this book we use methods from estimation, stochastic processes and information
theory. Chapter 2 includes the basic stalwarts of statistics and estimation theory which should be
familiar to the reader, including minimum-risk estimation, Fisher Information, hypothesis testing
and maximum-likelihood, consistency, model order estinmation, and entropies.
Chapters 3–6 bring into central prominence the role of representation of patterns via con-
ditioning structure. Chapter 3 examines discrete patterns represented via probabilistic directed
acyclic graphs (DAGs) emphasizing the pivoting properties and conditional factorizations of
DAGs which are familiar for Markov chains and random branching processes. This provides
ample opportunity to construct and study the syntactic theory of Chomsky Languages via the
classical formulation of graphs and grammatical transformation. Chapter 4 relaxes away from the
pivoting property of directed graphs to the conditioning structure of neighborhoods in Markov
random ﬁelds. Chapter 5 brings the added structure of of the Gaussian law for representing real-
valued patterns via Gaussian ﬁelds. In this context entropy and maximum entropy distributions
are examined in these three chapters as a means of representing conditioning information for rep-
resenting patterns of regularity for speech, language, and image analysis. Chapter 6 presents the
abstract representation of patterns via generators and probabilistic structures on the generators.
The generator representation is explored as it provides a uniﬁed way of dealing with DAGs and
random ﬁelds.
Chapters 7 and 8 begin examining in their own right the second central component of pattern
theory, groups of geometric transformation applied to the representation of geometric objects. The
patterns and shapes are represented as submanifolds of Rn, including points, curves, surfaces and
subvolumes. They are enriched via the actions of the linear matrix groups, studying the patterns
as orbits deﬁned via the group actions. In this context active models and deformable templates
are studied. The basic fundamentals of groups and matrix groups are explored assuming that the
typical engineering graduate student will not be familiar with their structure.
Chapter 9 makes the ﬁrst signiﬁcant foray into probabilistic structures in the continuum,
studying random processes and random ﬁelds indexed over subsets of Rn. Classical topics
are examined in some detail including second order processes, covariance representation, and
Karhunen-Loeve transforms. This is the ﬁrst chapter where more signiﬁcant understanding is
required for understanding signal and patterns as functions in a Hilbert space.
Chapters 10 and 11 continue the major thrust into transformations and patterns indexed over
the continuum. In this context, the ﬁnite dimensional matrix groups are studied as diffeomorphic
actions on Rn, as well their inﬁnite dimensional analogues are established. It is in these chapters
in which the substantial bridge between Pattern theory, mechanics, and differential geometry is
established. The links come through the study of ﬂows of diffeomorphisms. Chapter 10 focuses
on the study of the ﬁnite dimensional matrix groups, and Chapter 11 on the inﬁnite dimensional
diffeomorphisms acting on manifolds of Rn as a Riemannian metric space. The metric is induced
by the geodesic length between elements in the space deﬁned through the Riemannian length
of the ﬂow connecting one point to another. Herein the classical equations of motion for the
geodesics in the ﬁnite dimensional case are expanded to include the Euler formulation of the
inﬁnite dimensional case.
Chapter 12 expands this view to examine the orbit of imagery as a deformable template
under diffeomorphic action; the orbit is endowed with the metric structure through the length
minimizing geodesics connecting images. In this chapter the photometric orbit is studied as
well, adding notions from transport to deﬁne a metric on the product space of geometric and
photometric variation.
Chapters 13–15 extend from the pure representations of shapes to the Bayes estimation of
shapes and their parametric representation. Classical minimum-mean-squared error and maxi-
mum a-posteriori estimators of shapes are explored in these chapters as viewed through various
remote sensing models. Chapter 13 focuses on estimating the pose of rigid objects; chapter 14
focuses on accommodating photometric variability superimposed on the geometric variability of

4
1 INTRODUCTION
rigid pose. Chapter 15 focuses on information bounds for quantifying estimation accuracy of the
matrix group, comparing mean-squared error bounds with capacity and rate-distortion bounds
for codebooks.
Chapters 16 and 17 turn from the estimation of ﬁnite dimensional matrix groups to the study
of the estimation of inﬁnite dimensional shape in the newly emergent ﬁeld of Computational
Anatomy. Chapter 15 focuses on estimating landmark and image based shape metrics in volumes,
with Chapter 16 focusing on submanifolds and on the inference of disease and hypothesis testing
in Computational Anatomy.
The last two Chapters 18 and 19 conclude on inference, exploring random sampling
approaches for estimation of model order and parametric representing of shapes. Chapter 18
reviews jump and diffusion processes and their use in random sampling of discrete and continuum
spaces. Chapter 19 examines a series of problems in object recognition.
We have made an attempt to keep the theory at a consistent level. The mathematical level is
a reasonably high one, ﬁrst-year graduate level, with a background of at least one good semester
course in probability and a solid background in mathematics. We have, however, been able to
avoid the use of measure theory.
Appendices outlining proofs, theorems and solutions to exercises together with a com-
phrehive list of ﬁgures, tables and plates are freely available on an accompanying website
www.oup.com/uk/academic/companion/mathematics/patterntheory
In this book plates 1–16 appear between pages 180–181, plates 17–34 appear between pages
372–373, and plates 35–53 appear between 564–565.

2
T H E B AY E S PA R A D I G M , E S T I M AT I O N A N D
I N F O R M AT I O N M E A S U R E S
ABSTRACT
The basic paradigm is the Bayesian setup, given is the source of parameters X ∈X
which are seen through a noisy channel giving observations Y ∈Y. The posterior distribution
determines the bounds on estimation of X given Y, the risk associated with estimating it, as
well as a characterization of the information in the observation in Y about X.
2.1 Bayes Posterior Distribution
The basic set up throughout is we are given a model of the source of possible objects X ∈X.
These are observed through a noisy channel giving observations Y ∈Y. The source X ∈X is
modelled with distribution and density PX(dx) = p(x)dx,

X p(x)dx = 1. Generally the source
can only be observed with loss of information due to observational noise or limited accuracy in
the sensor. The mapping from the input source X ∈X to the observed output Y ∈Y expresses
the physics of the sensing channels; the data Y ∈Y will in general contain multiple components
corresponding to several sensors Y =

Y1, Y2, . . .

. The observation process is characterized via
a statistical transition law, transferring X →Y PY|X(·|·) : X × Y →R+, summarizing completely
the transition law mapping the input model parameters X to the output Y, the likelihood of Y
given X.
ThisBayesianparadigm, separatingthesourcefromthechanneliswhathasbecomethemod-
ern view of communications developed out of Shannon’s theory of communications [6]. Figure 2.1,
clearly delineates the separation of the source of information and the channel through which the
messages are observed. To infer the transmitted message at the output of the channel, the obser-
vation Y must be processed optimally. The inference engine is a decoder, working to recover
properties of the original message from the source.
Given such a communications source/channel decomposition, we shall be interested in both
specifying optimal procedures for inferring properties of pattern generation systems given the
observable measurements, and quantifying the information content and information gain of the
observation system. Pattern deduction becomes an enterprise consisting of constructing the source
and channel models, and essentially has several parts: (i) selection and ﬁtting of parameters
parametrizing the models representing the patterns, and (ii) construction of the family of prob-
ability models representing the knowledge about the pattern classes. For this we shall examine
classical minimum-risk estimators, such as minimum-mean-squared-error (MMSE) estimators,
maximum-aposteriori and likelihood (MAP, MLE). For constructing the models we shall examine
various forms and principles of entropy and mutual information.
At the most fundamental level, the posterior distribution represents the information con-
tained in the observables about the underlying imagery. All provably optimal structured methods
of inference and information gathering fundamentally involve the posterior density or distribution
of the random variables X ∈X given the observed deformed image Y ∈Y.
Source
Noisy Channel
Inference Engine
X e
Y e
X(Y) e
^
^
Figure 2.1 Shannon’s source channel model for communications systems
5

6
2 THE BAYES PARADIGM
Deﬁnition 2.1
Deﬁne the prior distribution and density on the source of random vari-
ables P(dx) = p(x)dx, on X,

X p(x)dx; deﬁne the likelihood or transition density on
the observations Y ∈Y given the input X as PY|X(dy|x) = p(y|x)dy, Y ∈Y. Deﬁne the
posterior as PX|Y(dx|y) = p(x|y)dx given by
p(x|y) = p(x)p(y|x)
p(y)
,

X
p(x|y)dx = 1.
(2.1)
If the random variables X and Y are discrete, then Bayes formula for the posterior is
interpreted as a probability mass function.
2.1.1 Minimum Risk Estimation
Now in solving problems in pattern theory we will want to perform inference; how should this
be done? It is not just a matter of calculating the posterior probabilities, we also have to operate
on them to distill the essence of the knowledge they contain. This leads to various tasks in pattern
inference:
1. Synthesis: to simulate the patterns from the probability distribution representing the source
X ∈X of patterns, P(dX) = p(X)dx.
2. Restoration: to recover the original source pattern X from the observed one Y suppressing
noise and other measurement errors as far as possible.
3. Recognition: to determine to which pattern class X is likely to belong when Y has been
observed.
4. Extrapolation: to extrapolate some part of the image that could not be observed because of
obscuration or other reasons.
5. Understanding: both intrinsic understanding, assuming the knowledge representation to be
essentially correct, and extrinsic understanding to spot misrepresentation and other model
errors.
In solving problems in pattern theory a variety of techniques have to be developed and modiﬁed,
examples including the estimation of unknown nuisance parameters in the representation, as well
as model selection, i.e. choice of the actual representation being used. These can all be treated in
a similar uniﬁed manner, all exploiting the basic properties of the likelihood function and prior
distribution.
One of the most fundamental aspects is to construct estimators X⋆that in some sense (to
be made precise) are a reasonable approximation to the true but unknown objects X. In statistical
terminology, generate a point estimator, a function X⋆(Y), X⋆: Y →X. But what should be meant
by reasonable? Within the Bayesian paradigm, we could ask for an estimator which satisﬁes some
optimality principle, or is of minimum risk. Fundamental to such an approach is the deﬁnition of a
distortion measure, or distance between the estimator and the random source vector. If the source
space X can be embedded in a vector space, Rd, Cd, then the concept of the squared-error metric
can be introduced, and the conditional mean value could be computed. The risk associated with
such metrics is often taken for granted, and has become part of the culture. The classic example is
the Kalman ﬁlter, a ﬁxture in modern estimation theory.
In such a setting, the choice to be made is the deﬁnition of the metric distance or risk function
R : X × X →R+ quantifying the relative merit between the approximating estimator and the
truth.

2.1 BAYES POSTERIOR DISTRIBUTION
7
y =|x|
y
x
y = x2
x
y
y = x2, |x|< θ
x
y
y = θ2, |x|≥θ
Figure 2.2 Left shows absolute value; middle shows squared error; right panel shows a threshold-
ing function
Deﬁnition 2.2
Then the positive-valued risk measure (loss) R : X × X →R+ has
expected risk R = E(R(X, ˆX(Y))) with the minimum risk-estimator satisfying
X⋆
R(Y) = arg min
ˆX∈X
E{R(X, ˆX(Y))}
(2.2)
= arg min
ˆX∈X

X ×Y
R(x, ˆx(y))p(x, y)dx dy.
(2.3)
Shown in Figure 2.2 are various examples.
2.1.2 Information Measures
Information measures quantify performance of the inference algorithms including accuracy of the
estimators, exponential error bounds for decision, and combinatoric complexity of representation
required. We introduce these information measures here following Cover and Thomas [7], where
additional details may be found. Deﬁne the information measures for both discrete and continuous
random variables.
Deﬁnition 2.3
Given a discrete random variable X ∈X with probability mass function PX
on X, the entropy is
H(X) = −EPX log PX = −

x∈X
PX(x) log PX(x),
where log is base e giving nats or base 2 giving bits.1
Given a second discrete random variable Y ∈Y jointly distributed with X according to
the PX,Y on X × Y the joint entropy of X and Y and the conditional entropy of X given
Y is
H(X, Y) = −EPX,Y log PX,Y = −

x,y
PX,Y(x, y) log PX,Y(x, y);
(2.4)
H(X|Y) = −EPX,Y log PX|Y = −

x,y
PX,Y(x, y) log PX|Y(x|y) .
(2.5)
1 Throughout we shall use log with its base interpreted depending on the context, so that log e = 1 when
log is base e, and log 2 = 1 when base 2.

8
2 THE BAYES PARADIGM
Given two random variables X and Y with joint probability PX,Y on X × Y and
marginals PX on X and PY on Y the mutual information between X and Y is
H(X; Y) = H(X) −H(X|Y) = H(Y) −H(Y|X)
= EPX,Y log
 PX,Y
PXPY

.
The relative entropy or Kullback Leibler divergence between the two probability
mass functions PX, PY, on X is deﬁned as
D(PX∥PY) = EPX log
PX
PY

.
By manipulation H(X; Y) = D(PX,Y∥PXPY).
For continuous random variables we use the convention of using small letters.
Deﬁnition 2.4
Given a continuous random variable X ∈X with probability density
P(dx) = p(x)dx, the differential entropy is
h(X) = −EPX log p(X) = −

X
p(x) log p(x)dx .
GivenasecondcontinuousrandomvariableY ∈Y jointlydistributedwithdensityP(dx, dy) =
p(x, y)dx dy on X × Y the joint differential entropy of X and Y and the conditional
entropy of X given Y is
h(X, Y) = −EPX,Y log p(X, Y);
(2.6)
h(X|Y) = −EPX,Y log p(X|Y).
(2.7)
Given two random variables X and Y with joint density p(X, Y) on X ×Y and marginals
p(X) on X and p(Y) on Y the mutual information between X and Y is
h(X; Y) = h(X) −h(X|Y) = h(Y) −h(Y|X)
= EPX,Y log
 p(X, Y)
p(X)p(Y)

.
The relative entropy or Kullback-Leibler divergence between two densities (assumed to
be absolutely continuous) PX(dx) = p(x)dx, QY(dx) = q(x)dx becomes
D(P∥Q) = EPX log
p(X)
q(X)

.
Similarly, h(X; Y) = EPX,Y log

p(X, Y)/p(X)p(Y)
	
.
2.2 Mathematical Preliminaries
2.2.1 Probability Spaces, Random Variables, Distributions,
Densities, and Expectation
First deﬁne the basic components of probabilities.
Deﬁnition 2.5
A probability space (, A, µ) is comprised of three quantities, a sample
space , a collection of subsets A ∈A the ﬁeld of events, and a probability measure µ.

2.2 MATHEMATICAL PRELIMINARIES
9
The ﬁeld of events has the properties that (i) A is not empty, (ii) if A ∈A then Ac ∈A,
and (iii) if A1, A2, . . . are events then ∪iAi ∈A.
The probability measure µ : A →[0, 1] satisﬁes (i) µ() = 1 and (ii) if A1, A2 . . . are
disjoint then µ(∪iAi) = 
i µ(Ai).
The sample space  with elements ω ∈ are possible outcomes of random experiments,
and may be denumerable (outcomes of a dice throw) and nondenumerable (random events in the
continuum such as the random time to failing of a system). As we shall see complicated spaces
will be studied as well including function spaces.
A subset A ⊂ termed an event is generally associated with describing the outcomes of the
random experiment. The collection of subsets A will almost always contain all possible subsets
for  ﬁnite. However, for nondenumerable sample spaces, A contains a subset of what we shall
term the measurable events, those for which µ(·) can be applied. Throughout the book, rather than
working explicitly with the underlying probability space we will often refer to it through notation
like Pr{ω ∈A}. We shall also suppress the dependence on ω throughout.
Real-valued random variables and random vectors will be studied throughout.
Deﬁnition 2.6
A function X :  →R is a real-valued random variable with respect to
(, A, µ) if every set {ω : X(ω) ≤x} ∈A.
A function (X1, . . . , Xn) :  →Rn is an Rn valued random variable with respect
to (, A, µ) if every set {ω : (X1(ω), . . . , Xn(ω)) ≤(x1, . . . , xn)} ∈A.
Notice in the above deﬁnition intersections and unions of sets generated from such basic
events ∪i{ω : X(ω) ≤xi}, ∩i{ω : X(ω) ≤xi} are elements of A implying that sets generated from
countable intersections and unions of intervals in R are legal sets as well to be asking for mem-
bership of random variables. Such sets we shall term measurable, meaning they are constructing
from the basic intervals. The probability distribution associated with the random variable directly
will be used throughout the entire book thereby suppressing the dependence on the underlying
sample probability space. Densities, distributions, and joint distributions are deﬁned as follows.
Deﬁnition 2.7
The probability distribution function for the real-valued random
variable X(ω) as the function such that for all sets A ⊂R (Lebesgue measurable)
PX(A) = µ{ω : X(ω) ∈A} .
(2.8)
The random variable has density PX(dx) = p(x)dx if for all such A,
Pr{X(ω) ∈A} = PX(A) =

A
p(x)dx .
(2.9)
The joint probability distribution for the Rn valued X1(ω), . . . , Xn(ω) is the
function such that for all sets A ⊂Rn,
PX(A) = µ{ω : (X1(ω), . . . , Xn(ω)) ∈A} .
(2.10)
Then X = (X1, . . . , Xn) has joint density PX(dx) = p(x1, . . . , xn)dx if for all A ∈A,
PX(A) =

A
p(x1, . . . , xn)dx1 · · · dxn .
(2.11)
In Rn, the probability elements p(x1, . . . , xn)x1 · · · xn is approximately Pr{x1 ≤X1 ≤x1 +
x1, . . . , xn ≤Xn ≤xn + xn} if pX1,...,Xn is continuous.
Statistical independence and conditional probability are deﬁned through relationships on
the joint distribution.
Deﬁnition 2.8
Random variables X, Y with joint distribution PXY are mutually inde-
pendent if
PXY(A, B) = PX(A)PY(B) ,
(2.12)
where PX, PY are the marginal distributions.

10
2 THE BAYES PARADIGM
The conditional distribution PX|Y is given for all A, B (measurable),
PX|Y(A|B) = PXY(A, B)
PY(B)
.
(2.13)
The moments are generated by taking the expected value with respect to the distribution.
Deﬁnition 2.9
The expected value of a random function f(X) with respect to distribution
with density PX(dx) = p(x)dx will often be written EPf(X) given by
EPf(X) =

X
f(x)p(x)dx .
(2.14)
The nth moment is given by the expectation of the nth power of X:
EPXn =

X
xnp(x)dx,
n = 1, 2, . . . .
(2.15)
2.2.2 Transformations of Variables
Transforming random variables is something we will do all the time. Let X1, X2, . . . have joint
density PX(dx) = p(x1, x2, . . . )dx, and consider the real-valued function yi = gi(x1, x2, . . . ), i =
1, . . . , n with g : Rn →Rn one-to-one with inverse xi = g−1
i
(y1, y2, . . . ). Then PY(dy) becomes
PY(dy) = p(g−1
1 (y1, y2, . . . ), . . . , g−1
n (y1, y2, . . . ))
det Dg−1(y1, y2, . . . )
 dy,
(2.16)
where the n × n Jacobian matrix Dg−1 =

∂g−1
i
/∂yj

.2
To prove this simply note the probability that
PY1,Y2,...(A) = PX1,X2,...(g−1(A)) =

g−1(A)
p(x1, x2, . . . )dx · · ·
(2.17)
=

g−1(A)
p(g−1(y1, y2, . . . ))
det Dg−1(y)
 dy.
(2.18)
Equating densities gives the result.
2.2.3 The Multivariate Normal Distribution
Normal random variables will be studied quite a bit. The univariate normal density with mean
EX = µ and variance E(X−µ)2 is written as PX(dx) =
1
√
2πσ 2 e−(1/2)(x−µ)2/σ 2dx. We shall of course
be interested in the multivariate case. For this we deﬁne the n × 1 and n × n vector notation3:
X =


X1
X2
...

µ =


EX1
EX2
...

,
KXX = E(X−µ)(X−µ)∗=

E(Xi −µi)(Xj −µj)

. (2.19)
2 We use the notation

Aij

to denote the matrix with i, j entry Aij; then

∂g−1
i
/∂yj

represents the n × n
Jacobian matrix.
3 Here (A)∗denotes matrix transpose of real vectors and matrices A, and matrix hermitian transpose for the
complex case.

2.2 MATHEMATICAL PRELIMINARIES
11
Deﬁnition 2.10
Then we shall say that X1, X2, . . . is multivariate normal with
mean vector µ = EX = (µi) and covariance matrix KXX
having density P(dX) =
p(X1, . . . , Xn)dX with
p(X1, . . . , Xn) = det−1/2(2πKXX)e−(1/2)(X−µ)∗K−1
XX(X−µ) .
(2.20)
The multivariate normal can be derived from the change of variables formula. Let X =


X1
X2
...

bezero-mean, independent, normal, varianceonewithpX(x) =
1
(2π)n/2 e−(1/2)x∗x. Deﬁne
the random vector Y =


Y1
Y2
...

by the linear transformation Y = QX + µ, with Q an invertible
n × n matrix. Deﬁning K = QQ∗, then a change of the density formula on PY(dy) gives
PY(dy) = p

Q−1(y −µ)
 det Q−1 dy
(2.21)
= det−1/2(2πK)e−(1/2)(y−µ)∗K−1(y−µ)dy.
(2.22)
This also provides the following result that invertible linear transformations of Gaussian random
variables are Gaussian.
Theorem 2.11
Let X =


X1
X2
...

be jointly normal with mean vector µ and covariance K.
Then invertible linear transformations Y = AX + b with (A invertible) are normal with mean
Aµ + b and covariance AKA∗.
The proof follows as above.
2.2.4 Characteristic Function
Random variables which are complex valued will be studied as well. For a complex variable to be
well deﬁned as a random variable, the real and imaginary parts must be measurable functions with
respect to the underlying probability space, in which case the complex sum is also well deﬁned.
The characteristic function generated from a random variable is a complex random variable which
we shall study a good bit.
Deﬁnition 2.12
Given a random vector X, v ∈Rn with distribution PX then the charac-
teristic function of X is
MX(jv) = EPXejv∗X,
v∗X =
n

i=1
viXi .
(2.23)
Notice that moments can be determined from the characteristic function. Suppose that X is
a scalar random variable, then the mth moment is generated by evaluating the mth derivative at
v = 0:
dm
d(jv)m MX(jv)|v=0 = EPXXm .
(2.24)

12
2 THE BAYES PARADIGM
Example 2.13 (The Gaussian)
For random vectors with densities PX(dx) = p(x)dx,
then
MX(jv) =

Rn p(x)ejv∗xdx .
(2.25)
Let X be a multivariate Gaussian with mean µ, covariance K, then MX(jv) =
ejv∗µ−(1/2)v∗Kv. To see this, deﬁne Y = Q−1(X −µ) where K = QQ∗. The Yi’s are inde-
pendent, univariate Gaussian, zero-mean random variables, with the characteristic
function of the univariate zero-mean Gaussian given by
MYi(jv) =

1
√
2π
e−(1/2)y2ejvydy
=

1
√
2π
e−(1/2)(y−jv)2e(1/2)(jv)2dy = e−(1/2)v2 .
(2.26)
The characteristic function of the Y vector becomes the product of characteristic
functions since the Yi’s are independent:
MY(jv) = EPYejv∗Y =
n

i=1
EPYi ejviYi =
n

i=1
e−(1/2)v2
i = e−(1/2)v∗v .
(2.27)
Now X = QY + µ, so that
Eejv∗X = Eejv∗QYejv∗µ = Eej(Q∗v)∗Yejv∗µ (a)
= e−(1/2)v∗QQ∗vejv∗µ ,
(2.28)
with (a) following from Eqn. 2.27, giving MX(jv) = ejv∗µ−(1/2)v∗Kv.
Example 2.14 (The Poisson)
Let N be Poisson with mean λ, so that
PN(n) = e−λ λn
n! ,
n = 0, 1, . . . .
(2.29)
Then
MN(jv) =
∞

n=0
e−λ λn
n! ejvn =
∞

n=0
e−λ (λejv)n
n!
(2.30)
= eλ(ejv−1) .
(2.31)
2.3 Minimum Risk Hypothesis Testing on Discrete Spaces
Minimum-risk will take various forms for decision making on discrete and continuous parameter
spaces. A good deal of inference in pattern theory requires deciding between discrete alternatives;
object detection and identiﬁcation is a fundamental example. The generalized hypothesis testing
problem involves choosing between alternative hypotheses X = 1, 2, . . . , each with own probabil-
ity law. Given a random sample Y1, Y2, . . . , Yn, drawn from one of the populations, choose one of
the hypotheses. Such hypothesis testing chooses between discrete hypotheses; thus the parameter
spaces will be discrete and countable.

2.3 MINIMUM RISK HYPOTHESIS TESTING
13
2.3.1 Minimum Probability of Error via Maximum A Posteriori
Hypothesis Testing
For minimum probability of error testing on discrete spaces, it is therefore natural to identify the
hypothesis spaces X with Z+, and work with the discrete metric, R = δ(·, ·) : Z+ ×Z+ →0, 1, with
δ(X, X′) = 1 unless X = X′. This naturally reduces to a minimum probability of error estimation,
the problem being given hypothesis X and random observation Y, generate estimators ˆX(Y) : Y →
X which minimize the desired risk:
R = Eδ( ˆX(Y), X) = Pr( ˆX ̸= X) .
(2.32)
Assume throughout the hypothesis space is ﬁnite, let X = 1, . . . , m be populations with den-
sities p(y|X = i), y ∈Y, with prior probability π(i), i = 1, . . . , m. We can view the optimum
hypothesis testing as deﬁning disjoint decision regions D1, . . . , Dm on the space of observations
Y ∈∪m
i=1Di = Y. Assuming identical unit costs for miscategorization corresponds to decision
region design to minimize the probability of error:
R(D) = Pr{ ˆX(Y) ̸= X}
(2.33)
=
m

i=1
π(i)Pr{ ˆX(Y) ̸= X|X = i}
(2.34)
=
m

i=1
π(i)

j̸=i

Dj
p(y|X = i)dy .
(2.35)
The goal is to choose the optimum regions D⋆
1, . . . , D⋆m such that R(D⋆) is the minimum risk. This
gives the following theorem demonstrating maximum a posteriori estimation as the minimum
risk estimator for identical costs (minimum probability of error).
Theorem 2.15
Given are random observations Y ∈Y from populations X = i with densities
p(y|X = i) with a priori probabilities π(i), i = 1, . . . with equal costs of misclassiﬁcation.
Then the optimum decision regions D⋆
1, . . . , D⋆m to minimize classiﬁcation error are deﬁned by
assigning observation Y = y to D⋆
i , i = 1, . . . , m so that
D⋆
i = {y ∈Y : i = arg max
j=1,...,m
π(j)p(y|X = j)} .
(2.36)
Notice, if there are multiple indices which have identical posterior probability, assignment to
either of the decision regions leaves the cost unchanged.
Proof
To calculate such optimum decision regions rewrite the above cost Eqn. 2.35
according to
m

i=1
π(i)

j̸=i

Dj
p(y|X = i)dy =
m

j=1

Dj

i̸=j
π(i)p(y|X = i)dy ,
(2.37)
and notice that minimizing total risk amounts to minimizing the risk for each mea-
surement y since this is a convex combination. The goal becomes choosing the decision
regions for measurement y so that the inner quantity is minimized:
j⋆= arg min
j

i̸=j
π(i)p(y|X = i)
(2.38)
= arg max
i
π(i)p(y|X = i) ,
(2.39)

14
2 THE BAYES PARADIGM
where the quantity is made smallest by removing the largest contribution. Let fj(y) =

i̸=j π(i)p(y|X = i) and deﬁning f(y|D) = fj(y), y ∈Dj, then the expected loss for
decision procedure D is
m

j=1

Dj
fj(y)dy =

Y
f(y|D)dy .
(2.40)
Taking two procedures D and optimum Bayes region D⋆, then the difference in
expected risk is

(f(y|D) −f(y|D⋆))dy =
m

j=1

Dj
(fj(y) −min
i
fi(y))dy ≥0 .
(2.41)
Remark 2.3.0
We point out that for unequal costs C(j|i) of saying X = j given X = i,
then the risk of Eqn. 2.35 becomes
RC(D) =
m

i=1
π(i)

j̸=i
C(j|i)

Dj
p(y|X = i)dy .
(2.42)
The decision regions are constructed by placing y in Dj⋆choosing j⋆so that the sum
is minimized:
j⋆= arg min
j=1,...,m

i̸=j
π(i)p(y|X = i)C(j|i) .
(2.43)
2.3.2 Neyman–Pearson and the Optimality of the Likelihood Ratio Test
Almost without exception all hypothesis testing studied in this book will be based on the likelihood
ratio because of its optimality associated with the Neyman–Pearson Lemma.
Deﬁnition 2.16
Given Y1, Y2, . . . drawn independent and identically distributed from
a model with density p(·|X = i), i = 1, . . . , m corresponding to one of the hypotheses
X = i, i = 1, . . . , m. Then we shall say that likelihood-ratio testing is any decision strategy
for choosing between hypotheses X = i, X = j which has decision region determined by the
test statistic of the form
Lij(Y1, . . . , Yn) = p(Y1, . . . , Yn|X = i)
p(Y1, . . . , Yn|X = j) .
(2.44)
The fundamental role of likelihood-ratio testing is that it minimizes the risk of the error
types.
Deﬁnition 2.17
Deﬁne αij to be the risk of claiming X = j given that X = i is true.
In the binary setting then α01 is usually called error of type I or probability of a
false alarm. α10 is usually called error of type II or probability of a miss.
Now for the optimality from Neyman and Pearson of such testing.
Theorem 2.18 (Neyman–Pearson)
Let Y be a random variable with density PY(dy) =
p(y)dy. For choosing among hypotheses X = 0, X = 1 each from model with densities

2.3 MINIMUM RISK HYPOTHESIS TESTING
15
p(·|X = 0), p(·|X = 1), respectively, deﬁne the decision region for choosing X = 0 and
X = 1 to be deﬁned, respectively, according to
D(θ) =
p(y|X = 0)
p(y|X = 1)
X=0
>
θ

(2.45)
D(θ)c =
p(y|X = 0)
p(y|X = 1)
X=1
≤
θ

.
(2.46)
Given the error types
α⋆
01 = P(D(θ)c|X = 0) ,
(2.47)
then for any other decision region B(θ) with error probabilities α01, α10, then if α01 ≤α⋆
01
implies α10 ≥α⋆
10.
Proof
Deﬁning 1D(·) as the indicator function on decision region D ⊂Y, 4 then for
B any other decision region, then
(1D(y) −1B(y))(p(y|X = 0) −θp(y|X = 1)) ≥0 .
(2.48)
Multiplying and integrating over Y gives
0 ≤

Y

1D(y)p(y|X = 0) −θ1D(y)p(y|X = 1)
−1B(y)p(y|X = 0) + θ1B(y)θp(y|X = 1)
	
dy
(2.49)
=

D
(p(y|X = 0) −θp(y|X = 1))dy
−

B
(p(y|X = 0) −θp(y|X = 1))dy
(2.50)
= (1 −α⋆
01) −θα⋆
10 −(1 −α01) + θα10
(2.51)
= θ(α10 −α⋆
10) −(α⋆
01 −α01) .
(2.52)
This implies
α10 −α⋆
10 ≥
α⋆
01 −α01
θ
≥0 .
(2.53)
Example 2.19 (Stein’s Lemma and Error Exponents)
Examine the role of entropy
and cross entropy in quantifying the error types. Clearly the threshold trades one
error for another. Examine the case when the error of the ﬁrst kind is made to asymp-
totically go to zero, and the error of the second kind is quantiﬁed in terms of the
best possible exponential decay rate. This is Stein’s lemma, demonstrating that the
Kullback distance controls the best possible exponential decay of the second error.
Theorem 2.20
(Stein’s Lemma) Let Y1, Y2, . . . be independent and identically distributed
distribution P. Consider the hypotheses governed by two distributions with densities P0(dy) =
p0(y)dy, P1(dy) = p1(y)dy continuous with respect to each other with ﬁnite Kullback–Leibler
distance
D(P0||P1) = EP0

log p0
p1

< ∞.
(2.54)
4 The indicator function 1A : Y →{0, 1} on a set A ⊂Y is deﬁned as 1A(y) = 1 if y ∈A, and 0 otherwise.

16
2 THE BAYES PARADIGM
Deﬁne α01(n), α10(n) to be the errors of type I,II indexed by sample size n.
Given that the probability of claiming X = 1 given X = 0 (type I error) goes to zero
with sample size, limn→∞α01(n) = 0, then the best error exponent for missing X = 1 and
claiming X = 0 (type II error) is given by the Kullback–Leibler distance between the two
probabilities:
lim
n→∞
1
n log α10(n) = −D(P0||P1).
(2.55)
Proof
The proof follows by ﬁrst deﬁning an acceptance region, which satisﬁes the
properties, and then showing you can not do better. Deﬁne the acceptance region
An =

(y1, . . . , yn) ∈Yn : n(D(P0∥P1) −δ)
≤log p0(y1, . . . , yn)
p1(y1, . . . , yn) ≤n(D(P0∥P1) + δ)

,
(2.56)
where
D(P0∥P1) =

Y
p0(y) log p0(y)
p1(y)dy .
(2.57)
Denote the probability that (y1, . . . , yn) ∈An as P(An), where P is associated
with the density p(y1, . . . , yn) = p(y1)p(y2) · · · p(yn). Note α01 = 1 −P0(An), and
clearly P0(An) →1 as n →∞by the strong law of large numbers since D(P0∥P1) =
EP0(log p0(Y)/p1(Y)). So this decision region ﬁts the ﬁrst part of the bill. Now examine
the error exponent for the second part using the deﬁnition of An to write
P1(An) =

An⊂Yn p1(y1, . . . , yn)dy
≤

An⊂Yn p0(y1, . . . , yn)dy2−n(D(P0||P1)−δ)
(2.58)
= 2−n(D(P0||P1)−δ)P0(An) ;
(2.59)
similar arguments give the upper bound P1(An) ≥2−n(D(P0∥P1)+δ)P0(An). Since
P0(An) →1, taking logarithms yields for all δ, there exists n(δ) large enough
so that
−D(P0∥P1) −δ ≤1
n log α10 ≤−D(P0∥P1) + δ ,
(2.60)
implying limn→∞(1/n) log α10 = −D(P0∥P1).
2.4 Minimum Mean-Squared Error Risk Estimation in Vector Spaces
Minimum risk will take various forms for decision making on discrete and continuous parameter
spaces. We now study continuous parameter spaces like Rn. In vector spaces the familiar metrics
willbethenaturalwaytomeasureerrorforestimation. Thesemetricswillbecometheriskfunctions
for minimum-risk estimation. Begin with linear vector spaces and minimum-mean-squared error
estimation.

2.4 MINIMUM MEAN-SQUARED ERROR RISK ESTIMATION
17
2.4.1 Normed Linear and Hilbert Spaces
The objects being estimated are oftentimes vectors or elements in a normed vector space.
Deﬁnition 2.21
A normed linear vector space is a vector space of elements x ∈X
on which there is deﬁned a norm which is a function ∥· ∥: X →R+ satisfying for all
x, y ∈X:
1. ∥x∥≥0 with ∥x∥= 0 if and only if x = 0;
2. ∥x + y∥≤∥x∥+ ∥y∥for each x, y ∈X (triangle inequality);
3. ∥αx∥= |α|∥x∥for all scalars α and each x ∈X.
Here are several examples of metric spaces which are familiar.
Example 2.22 (Euclidean)
The Euclidean space of n-tuples x = (x1, . . . xn) ∈Rn is
one of the principal normed spaces we are familiar with (and a Hilbert space as well,
see below) with norm given by ∥x∥=

n
i=1 |xi|2.
Example 2.23 (Continuous functions)
The normed linear space of continuous func-
tions C[0, 1] on [0, 1] with the norm ∥x∥= maxt∈[0,1] |x(t)|. Clearly it is a vector space,
and addition of continuity gives continuous functions. To see the triangle inequality
is satisﬁed it follows
max
t∈[0,1] |x(t) + y(t)| ≤max
t∈[0,1] |x(t)| + max
t∈[0,1] |y(t)| .
(2.61)
Oftentimes we require differentiability, then C1[0, 1] denotes one time continu-
ously differentiable functions with
∥x∥= max
t∈[0,1] |x(t)| + max
t∈[0,1] |˙x(t)|.
(2.62)
Example 2.24 (Triangle Inequality in lp)
The
square-summable
and
square-
integrable spaces are used often. Let p be a real number, p ≥1, the space lp con-
sisting of vectors x = (x1, x2, . . . ) for which the positive quantity ∥· ∥p is ﬁnite deﬁned
as ∥x∥p = (
∞
i=1 |xi|p)1/p < ∞. For p-arbitrary, see Luenberger [8]; let us do here
p = 1, 2, showing the triangle inequality. For p = 1 it is straightforward:
n

i=1
|xi + yi| ≤
n

i=1
|xi| +
n

i=1
|yi| .
(2.63)
Since this is true for every n, ∥x + y∥1 ≤∥x∥1 + ∥y∥1. For p = 2, we use the Holder
inequality (see proof below), for 1 ≤p, q ≤∞with x, y ∈Rn, ∥x∥p =

i |xi|p	1/p,
then
n

i=1
|xiyi| ≤∥x∥p∥y∥q
with 1
p + 1
q = 1 .
(2.64)

18
2 THE BAYES PARADIGM
Using the Holder inequality5 then,
n

i=1
|xi + yi|2 ≤
n

i=1
|xi + yi||xi| +
n

i=1
|xi + yi||yi|
(2.68)
(a)
≤


n

i=1
|xi + yi|2


1/2 



n

i=1
|xi|2


1/2
+


n

i=1
|yi|2


1/2
,
(2.69)
with (a) following for the Holder inequality with p = q = 2. Dividing through gives


n

i=1
|xi + yi|2


1/2
≤


n

i=1
|xi|2


1/2
+


n

i=1
|yi|2


1/2
;
(2.70)
since this is true for every n, we have the triangle inequality.
The square summable and integrable spaces l2, L2 will be used extensively for modeling. The
extra structure afforded to them as a Hilbert space with inner product will be used extensively.
The square-summable inner product associated with the inner product on l2 will be ⟨x, y⟩2 =

i xiyi; the square-integrable space L2 has an inner product associated with the norm which is
⟨x, y⟩=
 1
0 x(t)y⋆(t)dt.
Deﬁnition 2.25
A Hilbert space is a complete6 linear vector space H together with the
inner product producing a scalar ⟨·, ·⟩: H×H→C satisfying for all x, y ∈H, complex scalars
1. ⟨x, y⟩= ⟨y, x⟩∗;
2. ⟨x + y, z⟩= ⟨x, z⟩+ ⟨y, z⟩, ⟨αx, y⟩= α⟨x, y⟩
3. ⟨x, x⟩≥0 and equals 0 if and only if x = 0.
We note, that from the normed-distance to the origin the metric distances between elements
is induced by the inner product. To see this, we only need prove the triangle inequality, which we
do using the Cauchy–Schwartz inequality.
Theorem 2.26 (Cauchy–Schwartz)
In the Hilbert space H denotes ∥x∥= ⟨x, x⟩1/2, then
for all x, y ∈H,
|⟨x, y⟩| ≤∥x∥∥y∥,
(2.71)
5 Holder Inequality: Consider f(t) = tλ −λt + λ −1, t ≥1 and 0 < λ < 1, then ˙f(t) = λtλ−1 −1 giving ˙f(t) > 0
for 0 < t < 1 and ˙f(t) < 0 for t > 1. Thus, for t ≥0, f(t) ≤f(1) = 0 implying tλ ≤λt + 1 −λ. Substituting
t = (|xi|/∥x∥p)p/(|yi|/∥y∥q)q with λ = 1/p, 1 −λ = 1/q gives

(|xi|/∥x∥p)p
(|yi|/∥y∥q)q
1/p
≤1
p

(|xi|/∥x∥p)p
(|yi|/∥y∥q)q

+ 1
q .
(2.65)
Multiplying through by (|yi|/∥y∥q)q gives
|xiyi|
∥x∥p∥y∥q
≤1
p

|xi|
∥x∥p
p
+ 1
q

|yi|
∥y∥q
q
(2.66)
and summing over i gives

i |xiyi|
∥x∥p∥y∥q
≤1
p + 1
q = 1 .
(2.67)
6 A space for which Cauchy sequences converge within the space under the norm.

2.4 MINIMUM MEAN-SQUARED ERROR RISK ESTIMATION
19
and ∥· ∥is a norm satisfying the triangle inequality:
∥x + y∥≤∥x∥+ ∥y∥.
(2.72)
Proof
For all x, y ∈H then
0 ≤

x −⟨x, y⟩
⟨y, y⟩y, x −⟨x, y⟩
⟨y, y⟩y

(2.73)
= ⟨x, x⟩−|⟨x, y⟩|2
⟨y, y⟩
,
(2.74)
giving the Cauchy–Schwartz. The triangle inequality becomes
∥x + y∥2 = ∥x∥2 + 2|⟨x, y⟩| + ∥y∥2
(2.75)
(a)
≤∥x∥2 + 2∥x∥∥y∥+ ∥y∥2
(2.76)
= (∥x∥+ ∥y∥)2 ,
(2.77)
(a) following from the Cauchy–Schwartz inequality.
The added structure of the inner product in the Hilbert space with the squared-error metric
provides the projection theorem for which errors from linear estimators are orthogonal to the
approximating set of vectors.
Theorem 2.27 (Classical Projection Theorem)
Let M ⊂H be the closed subspace of the
Hilbert space generated from the linear independent basis y1, . . . , yn. Then for all x ∈H
there exists a unique vector ˆx ∈M of the form ˆx = 
i αiyi such that ∥x −ˆx∥≤∥x −˜x∥
for all ˜x ∈M. A necessary and sufﬁcient condition for ˆx ∈M to be the unique minimizing
vector is that x −ˆx is orthogonal to M; that is, for ˆx = 
n
j=1 ajyj the normal equations
⟨x −ˆx, yi⟩= 0, i = 1, . . . , n:


⟨y1, y1⟩
⟨y1, y2⟩
. . .
⟨y1, yn⟩
⟨y2, y1⟩
⟨y2, y2⟩
. . .
⟨y2, yn⟩
...
...
...
...
⟨yn, y1⟩
⟨yn, y2⟩
. . .
⟨yn, yn⟩




a1
a2
...
an

=


⟨x, y1⟩
⟨x, y2⟩
...
⟨x, yn⟩

.
(2.78)
Proof
First, x −ˆx must be orthogonal to the y′s or any element m ∈M. Suppose it
isn’t, so that ⟨x −ˆx, m⟩= δ ̸= 0, for some m ∈M. Then deﬁning ˜x = ˆx + δm, with
∥m∥= 1, then
∥x −ˆx −δm∥2 = ∥x −ˆx∥2 −2δ⟨x −ˆx, m⟩+ δ2∥m∥2
(2.79)
= ∥x −ˆx∥2 −δ2 < ∥x −ˆx∥2 ;
(2.80)
apparently this can not be since ˆx is the minimum estimator. That ˆx is unique follows
from the property of the norm:
∥x −˜x∥2 = ∥x −ˆx + ˆx −˜x∥2
(2.81)
= ∥x −ˆx∥2 + ∥ˆx −˜x∥2 .
(2.82)
Thus ∥x −˜x∥> ∥x −ˆx∥for ˜x ̸= ˆx.

20
2 THE BAYES PARADIGM
2.4.2 Least-Squares Estimation
We can construct linear least-square error (l.l.s.e.) estimator as a minimum norm problem in the
Hilbert space of random vectors, in which the correlation plays the role of the inner product. In
this way the geometric interpretation of the classical projection theorem will hold. Throughout
we assume the ﬁnite dimensional setting; we return to this in Chapter 9 when we extend these
notions to the inﬁnite dimensional setting in which projection in the associated function spaces of
L2, l2 becomes fundamental.
For now, construct the ﬁnite m-dimensional Hilbert space H(Y1, Y2, . . . , Ym) of random vari-
ables generated from the random n-vectors Y1, Y2, . . . , Ym ∈Rn of zero-mean random variables
with ﬁnite second moment. Deﬁne the Hilbert space to be the space of random variables generated
as linear combinations of the Yis, with the inner product including the expectation.
Deﬁnition 2.28
Deﬁne the ﬁnite dimensional Hilbert space of random variables
generated from Y1, . . . , Ym as H(Y1, . . . , Ym) given by the set of all elements Z
=

m
i=1 aiYi, ai ∈R with inner product and norm
⟨X, Y⟩= tr EXY∗= E⟨X, Y⟩Rn,
∥X∥2 = tr EXX∗= E⟨X, X⟩Rn ,
(2.83)
with ∗denoting the transpose for real vectors so that KXY = EXY∗, and ⟨X, Y⟩Rn is the Rn
inner product.
Now examine a classical problem illustrating least-squares estimation as an orthogonal projection.
Theorem 2.29
Let Yi, i = 1, . . . , m ∈Rn be zero-mean random vectors jointly distributed
with covariances KYiYj with the optimum linear estimate
ˆX = 
m
i=1 aiYi of X ∈H
(Y1, . . . , Ym), has a1, . . . , am satisfying


E⟨Y1, Y1⟩Rn
. . .
E⟨Y1, Ym⟩Rn
...
...
...
E⟨Ym, Y1⟩Rn
. . .
E⟨Ym, Ym⟩Rn




a1
...
am

=


E⟨X, Y1⟩Rn
...
E⟨X, Ym⟩Rn

.
(2.84)
Proof
The projection theorem with inner product ⟨X, Y⟩= tr EXY∗implies orthog-
onality according to
⟨X −ˆX(Y), Yj⟩=

X −

i
aiYi, Yj

= 0,
j = 1, . . . , m.
(2.85)
The normal equations become


E⟨Y1, Y1⟩Rn
. . .
E⟨Y1, Ym⟩Rn
...
...
...
E⟨Ym, Y1⟩Rn
. . .
E⟨Ym, Ym⟩Rn




a1
...
am

=


⟨X, Y1⟩
...
⟨X, Ym⟩

.
(2.86)
Corollary 2.30
If X, Y1, Y2, · · · ∈R are scalar valued, then denoting Y =


Y1
...
Ym

with
K−1
YY existing, then the l.l.s.e and error covariance satisﬁes
ˆX = KXYK−1
YYY,
with K ˆX ˆX = E|X −ˆX|2 = KXX −KXYK−1
YYKYX .
(2.87)

2.4 MINIMUM MEAN-SQUARED ERROR RISK ESTIMATION
21
Proof
For the Yi ∈R1 scalar case with Y ∈Rm the m-vector of scalars, then each
inner product reduces ⟨Yi, Yj⟩= EYiYj so that the normal matrix becomes KYY; with
the inverse existing then ˆX = KXYK−1
YYY. The covariance follows from computing
E(X −KXYK−1
YYY)(X −KXYK−1
YYY)∗
= KXX −2KXYK−1
YYKYX + KXYK−1
YYKYX ,
(2.88)
which gives the result.
Example 2.31 (Adding the Mean)
GivenrandomvariableX ∈Rann-vectorY ∈Rn,
with means µX, µY and cross covariance KXY = E(X−µX)(Y−µY)∗, KYY = E(Y−µY)
(Y −µY)∗, the linear-least-square-error estimator is adjusted by the mean values
ˆX(Y) = µX + KXYK−1
YY(Y −µY).
(2.89)
Notice the adjustment by the mean. To see this, linear estimators are of the form
ˆX(Y) = 
i aiYi + b. Using matrix notation, then deﬁne the vector a∗Y = 
i aiYi, and
the squared error gives
E|X −ˆX(Y)|2 = E|X −a∗Y −b|2
(2.90)
implying b = µX −a∗µY. Thus
E|(X −µX) −a∗(Y −µY)|2 = KXX + a∗KYYa −2a∗KYX
(2.91)
a = K−1
YYKYX.
(2.92)
Example 2.32
Hereisaclassicproblemindeconvolution. LettheobservablesY bethe
matrix superposition via the linear operator P of the underlying unobserved random
variables X with noise W:
Y = PX + W ,
(2.93)
with X and W zero-mean with covariance KXX and KWW and uncorrelated. Then the
least-square estimator is given by
ˆX = KXXP∗(PKXXP∗+ KWW)−1Y .
(2.94)
Example 2.33 (Recursive Estimation via Kalman Filtering)
Examine the discrete-
time dynamical system with state variables Xi and observables Yi,i = 0, 1, . . . , n:
Xi+1 = iXi + Ui ,
(2.95)
Yi+1 = MiXi + Wi ,
(2.96)
with initial random vector X0 with initial estimate ˆX0 having error covariance K0 =
E( ˆX0−X0)( ˆX0−X0)∗, and input Uk and observation noise Wk zero-mean uncorrelated
processes with covariances EUkU∗
l = Qkδ(k −l), EWkW∗
l = Rkδ(k −l), with Qk, Rk
positive deﬁnite matrices.
Examine the estimation problem of obtaining an l.l.s.e. of the state X from the
measurements Y. Introduce the special notation ˆXk|j to denote the l.l.s.e. of Xk from
measurements {Yi, j ≥i ≥0}. For prediction we examine the case k ≥j.
Theorem 2.34 (Kalman)
With initial conditions ˆX0|−1 = ˆX0 with covariance K0, the
recursive algorithm which the l.l.s.e. and its covariance satisﬁes for i ≥1 is given as follows:
ˆXi+1|i = iKiM∗
i (MiKiM∗
i + Ri)−1(yi −Mi ˆXi|i−1) + i ˆXi|i−1 ;
(2.97)
Ki+1 = iKi

I −M∗
i (MiKiM∗
i + Ri)−1MiKi

∗
i + Qi .
(2.98)

22
2 THE BAYES PARADIGM
Proof
Suppose that Y0 = y0, . . . , Yi−1 = yi−1 have been observed with ˆXi|i−1 the
l.l.s.e. projected onto the space generated by the measurements yj, j ≤i −1 with
covariance matrix Ki. Given observation Yi = yi satisfying
yi = Mixi + wi ,
(2.99)
then ˆXi|i is the l.l.s.e. projected on the space generated to time i. Since Mi is a linear
transformation, the updated estimator ˆXi|i is the old estimate ˆXi|i−1 plus the best
estimate of Xi in the subspace generated by yi −Mi ˆXi|i−1. From the above l.l.s.e.
theorem the updated estimator to time i and error covariance becomes
ˆXi|i = ˆXi|i−1 + KiM∗
i (MiKiM∗
i + Ri)−1(yi −Mi ˆXi|i−1) ,
(2.100)
Ki|i = Ki −KiM∗
i (MiKiM∗
i + Ri)−1MiKi .
(2.101)
Since i is a linear transformation, then the l.l.s.e. of iXi given yj, j ≤i is i ˆXi|i. Since
Ui is uncorrelated with Yj = yj, j ≤i and ˆXi|i, then we have the l.l.s.e. of Xi+1 given by
ˆXi+1|i = i ˆXi|i,
(2.102)
with the error covariance given according to
Ki+1 = iKi|i∗
i + Qi.
(2.103)
But substituting Eqns. 2.100 into 2.102 and Eqn. 2.101 into 2.103 gives the Kalman ﬁlter
equations 2.97 and 2.98.
2.4.3 Conditional Mean Estimation and Gaussian Processes
For elements of Hilbert spaces, the minimum-mean-squared error estimators have overwhelm-
ingly become the most widely used estimators, principally because (i) they are ﬁrmly linked to
conditional mean estimators, and (ii) for Gaussian random variables least-squared error estimators
are linear functions of the data. For random variables and random vectors, the conditional mean
is deﬁned as should be expected, the average value using the conditional density.
Deﬁnition 2.35
The conditional mean of X given Y is denoted by E(X|Y).
Theorem 2.36
Given random vectors X, Y ∈Rn with inner product ⟨X, Y⟩= 
n
i=1 XiYi
and associated norm ∥X∥= |⟨X, X⟩|1/2, then the conditional mean E(X|Y) has the particu-
larly beautiful property that the correlation of the error X −E(X|Y) with any other function
(measurable) of the data ψ(Y) is zero.
The conditional mean also provides the minimum mean-squared error estimator over all
estimators ˆX : Y →X.
Proof
To see this, examine
E⟨X −E(X|Y), ψ(Y)⟩= E⟨X, ψ(Y)⟩−E⟨E(X|Y), ψ(Y)⟩
(2.104)
= E⟨X, ψ(Y)⟩−E⟨X, ψ(Y)⟩= 0 .
(2.105)

2.4 MINIMUM MEAN-SQUARED ERROR RISK ESTIMATION
23
To show the minimum error property, let ˆX(Y) be any other estimator. Then
E∥X −ˆX(Y)∥2 = E∥X −E(X|Y) + E(X|Y) −ˆX(Y)∥2
= E∥X −E(X|Y)∥2 + E∥E(X|Y) −ˆX(Y)∥2
−2E⟨X −E(X|Y), E(X|Y) −ˆX(Y)⟩
(a)
= E∥X −E(X|Y)∥2 + E∥E(X|Y) −ˆX(Y)∥2
≥E∥X −E(X|Y(·))∥2 ,
with (a) following from the orthogonality property of the conditional mean.
We now show that the conditional mean for Gaussian processes is precisely the least-squared
error estimator.
Theorem 2.37
Let the random vectors be zero-mean with components divided into two
subvectors [X, Y] with covariance matrix K =
 KXX
KXY
KYX
KYY

.
Then if the distribution of X, Y is jointly normal, then the conditional distribution of X
given Y is normal with mean KXYK−1
YYY and covariance KXX −KXYK−1
YYKYX.
Proof
Deﬁne Y(1) = X −KXYK−1
YYY, Y(2) = Y then Y(1), Y(2) are Gaussian and
independent with joint density
p(Y(1), Y(2)) = det(−1/2) 2π(KXX −KXYK−1
YYKYX)e−(1/2)Y(1)∗(KXX−KXYK−1
YYKYX)−1Y(1)
(2.106)
× det(−1/2) 2πKYYe−(1/2)Y(2)∗K−1
YYY(2) .
(2.107)
The density on X, Y is obtained via the change of density formula using the transfor-
mation Y(1) = X −KXYK−1
YYY, Y(2) = Y with the Jacobian being one gives the joint
density in X, Y according to
p(X, Y) = det(−1/2) 2π(KXX −KXYK−1
YYKYX)e−(1/2)(X−KXYK−1
YYY)∗(KXX−KXYK−1
YYKYX)−1(X−KXYK−1
YYY)
× det−1/2 2πKYYe−(1/2)Y∗K−1
YYY.
(2.108)
The conditional density divides by the Gaussian density on Y giving the result.
Example 2.38
Let X and Y be jointly Gaussian random variables with the density
p(X, Y) =
1
2πσXσY

(1 −ρ2)
× exp −

1
2(1 −ρ2)
(X −µX)2
σ 2
X
+ (Y −µY)2
σ 2
Y
−2ρ(X −µX)(Y −µY)
σXσY

(2.109)
also denoted as N(µX, µY, σ 2
X, σ 2
Y, ρ). Anice property of the bivariate Gaussian density
is that normality is preserved under conditioning. The conditional density of random

24
2 THE BAYES PARADIGM
variable X given Y is
p(X|Y) = p(X, Y)
p(Y)
=
1
2πσXσY
√
(1−ρ2) exp −

1
2(1−ρ2)
(X−µX)2
σ 2
X
+ (y−y)2
σ 2
Y
−2ρ(X−µX)(Y−µY)
σXσY

1

2πσ 2
Y
exp −1/2

(Y−µY)2
σ 2
Y

(2.110)
=
1
σX

2π(1 −ρ2)
exp −
1
2(1 −ρ2)
(X −µX)
σX
−ρ(Y −µY)
σY
2
(2.111)
whichbyobservationisGaussiandistributedasN(µX+ρ(σX/σY)(Y−µY), σ 2
X(1−ρ2)).
The conditional mean E(X|Y) = µX + ρ(σX/σY)(Y −µY), which is linear in Y.
For estimators which are linear combinations of the data, the least-square estimators are par-
ticularly simple involving only mean and covariances of the processes. Of course for the Gaussian
process, conditional mean estimators are linear-least-squared error estimators.
Remark 2.4.1
That an estimator satisﬁes the orthogonality property is a natural way
for deﬁning the conditional mean which extends even to the case when the condition-
ing events are inﬁnite. That is, given a random variable X and stochastic process Y(·),
the conditional expectation of X given Y(·) is the unique random variable that is a
functional of Y(·) satisfying the orthogonality condition E(X −ˆX(Y(·))ψ(Y(·)) = 0 for
all measurable functions ψ(·).
2.5 The Fisher Information of Estimators
Thus far we have examined on discrete parameter spaces maximizing likelihood and posterior
probability for minimum risk. For optimizing mean-squared error on the continuum the con-
ditional mean ﬁts the job. In general, however, there are many other point estimators as the
conditional mean may be difﬁcult to compute; maximum likelihood, MAP, method of moments
estimators, andmanyothers. Ingeneraltheywillnotbeoptimalinthemean-squaresense, although
we would still like to know how good they can possibly be. For this reason Fisher information is
extremely powerful as it allows us to bound the performance of any estimator in the mean-squared
error sense.
Deﬁnition 2.39
Let X have density PX(dx) = pθ(x)dx indexed by parameter θ ∈ ⊂Rd.
An estimator ˆθ : X → of the real-valued parameter θ ∈ has bias deﬁned by the true
value θ and its average value:
Bias θ = E{ ˆθ(X) −θ} .
(2.112)
The estimator is unbiased if Bias(θ) = 0.
It turns out that the mean-squared error for any unbiased estimator is lower bounded by
the inverse of the Fisher information.

2.5 THE FISHER INFORMATION OF ESTIMATORS
25
Deﬁnition 2.40
Let θ ∈ be an m-dimensional real parameter, density pθ(X). Then the
d × d Fisher information matrix is deﬁned as F(θ) = (Fjk(θ)), where
Fjk(θ) = E

∂log pθ(X)
∂θj
∂log pθ(X)
∂θk

.
(2.113)
Theorem 2.41
Given sufﬁcient smoothness of the density so that it is twice differentiable,
then
Fjk(θ) = E

−∂2 log pθ(X)
∂θj∂θk

.
(2.114)
Proof
To see that the Fisher information is given by the negative second derivative
follows from the following argument:
E

−
∂2
∂θj∂θk
log pθ(X)

= E

−
(∂2/∂θj∂θk)pθ(X)
pθ(X)
+
(∂/∂θj)pθ(X)(∂/∂θk)pθ(X)

pθ(X)
	2

(2.115)
(a)
= E

(∂/∂θj)pθ(X)(∂/∂θk)pθ(X)

pθ(X)
	2

,
(2.116)
with (a) following from the interchange of differentiation7 and integration in the ﬁrst
term with the integral a constant,

X (∂2/∂θj∂θk)pθ(x)dx = 0.
The mean-squared error performance is bounded by the inverse Fisher information. We shall
sometimes denote the covariance of ˆθ as Cov ( ˆθ).
Theorem 2.42 (Cramer–Rao inequality)
Let ˆθ(X) ∈ be an unbiased estimator of
θ ∈. Then
Cov ˆθ(X) ≥F(θ)−1 .
(2.117)
Let X1, . . . , Xn be i.i.d random variables. Then the Fisher information is n-times the single
sample information with the covariance lower bound decreased by n:
Cov ˆθ(X1, . . . , Xn) ≥(nF(θ))−1 .
(2.118)
Proof
Deﬁne the score function Vθ(X) = ∇θ log pθ(X), then EVθ(X) = 0 since
EVθ(X) =

X
∇θpθ(x)
pθ(x) pθ(x)dx
(2.119)
= ∇θ

X
pθ(x)dx = 0 .
(2.120)
with the smoothness of Theorem 2.41 allowing interchange of differentiation for
bounded convergence. Using the Cauchy–Schwartz inequality componentwise on the
7 A sufﬁciently strong condition is if the second-derivative is continuous on  compact then it is bounded
and so dominated convergence allows swapping of the derivative and integral.

26
2 THE BAYES PARADIGM
entries in the matrix gives

E(Vθ(X) −EVθ(X))( ˆθ −E ˆθ)∗2
≤E

(Vθ(X) −EVθ(X))(Vθ(X) −EVθ(X))∗
E

( ˆθ −E ˆθ)( ˆθ −E ˆθ)∗
= F(θ) Cov ˆθ.
(2.121)
Note, this inequality is componentwise. Now use the fact that EVθ(X) = 0 according
to Eqn. 2.120 simplifying the left-hand side according to

E(Vθ(X) −EVθ(X))( ˆθ −E ˆθ)∗2
= E(Vθ(X) ˆθ∗)
(2.122)
(a)
= ∇θ

X
pθ(x) ˆθ∗(x)dx
(2.123)
(b)
= ∇θθ∗= id ,
(2.124)
with (a) following from the smoothness as above and (b) following from unbiased
assumption and where id is the d × d identity matrix.
To complete the proof, given X1, X2, . . . independent random variables, then
E

∂
∂θj
log pθ(X1, X2, . . . ) ∂
∂θk
log pθ(X1, X2, . . . )

= nFjk(θ) .
(2.125)
Monotonicity of the Fisher information in sample size is important in multi-sensor fusion work;
estimators get better when information is combined optimally.
Example 2.43
Let X be Poisson distributed, mean θ, then pθ(X) = e−θ+X log θ+log X!,
and H(n, θ) = (θ −¯X log θ), with the inverse Fisher information F(θ) = 1/θ.
2.6 Maximum-Likelihood and its consistency
The most commonly used estimation when not in a mean-squared error setting is one based on
Bayesian inference which assumes that (i) a priori probabilities exist on the parameters, and (ii)
they are known at least approximately. When this is not the case, maximum likelihood becomes
the method of choice. If the data is strong, and the parameter space is of low-dimension the two
perform similarly. In high dimensional settings the prior can play a fundamental role with special
care required (see [9] for a detailed examination of such issues).
Let us begin with maximizing likelihood. Similar arguments hold for the maximum a pos-
teriori formulation where the parameter is random with a priori distribution. Here is the basic
setup. Let X be a random variable with density Pθ(dX) = pθ(X)dX parameterized by θ ∈ ⊆Rd.
Deﬁne ML estimators as follows.
Deﬁnition 2.44
GivenasampleX1, X2, . . . withjointdensityPθ(dX) = pθ(X1, X2, . . . )dX
parameterized by θ ∈ ⊆Rd. Then we shall say ˆθ is a maximum-likelihood (ML)
estimator if ˆθ ∈Mn ⊂, Mn the set of maximizers of the likelihood:
Mn = { ˆθ ∈ : p ˆθ(x1, . . . , xn) = sup
θ∈
pθ(x1, . . . , xn)} .
(2.126)

2.6 MAXIMUM-LIKELIHOOD AND ITS CONSISTENCY
27
There are many classical proofs of optimality of likelihood estimators. In any case, what is the
reason for maximizing likelihood? Why does it work? It is the law of large numbers and the
fundamental role of the cross-entropy function which makes it work.
Consider the use of the normalized log-likelihood function deﬁned on an indepen-
dent identically distributed (i.i.d.) sample of n-terms with real-valued parameters θ
∈,
(1/n) 
n
i=1 log pθ(Xi). This forms an i.i.d. sample with expected value the negative cross-entropy
with respect to the density parameterized by the true parameter θ⋆∈. Thus if all goes well, and
the log-likelihood function converges then limn→∞(1/n) 
n
i=1 log pθ(Xi) = EPθ⋆(X) log pθ(X).
But we know that this has maximum for θ = θ⋆, so that
EPθ⋆(X) log pθ(X) ≤EPθ⋆(X) log pθ⋆(X) = lim
n→∞
1
n
n

i=1
log pθ(Xi)|θ=θ⋆.
(2.127)
We should then expect that maximizing with respect to θ ∈ should give a cross-entropy which
is close to that given by the true parameter, and if the parameter space is well behaved should
have maximum in the parameter space which is close to the true parameter θ⋆. Essentially all
proofs of consistency follow this line of argument, that the cross entropy is maximum when the
true parameter is selected.
We shall now look at this a bit more carefully, ﬁrst assuming a stronger form of uniform
convergence for the empirical log-likelihood, and the second assuming a sufﬁcient smoothness so
that a rate of convergence can be established for the consistency.
2.6.1 Consistency via Uniform Convergence of Empirical Log-likelihood
We now examine this more precisely. Essentially we employ uniform convergence of the empirical
log-likelihood function to establish consistency. We ﬁrst deﬁne consistency as follows.
Deﬁnition 2.45
Let X1, X2, . . . be a random sample jointly distributed with density
Pθ(dX) = pθ(X1, X2, . . . )dX, θ ∈. Then we shall say that the MLE ˆθ(X1, X2, . . . ) is
asymptotically consistent if it converges in probability to θ⋆the true parameter with
n →∞.
We shall establish uniform convergence for Hn(θ) using a Uniform Weak Law of Large
numbers.
Theorem 2.46
Let X1, X2, ... be identically distributed with density Pθ(dX)
=
pθ(X1, X2, . . . ), θ ∈, and deﬁne the sequence obtained from maximization of the log-
likelihoods
ˆθn = arg max
θ∈
log pθ(X1, X2, . . . ) = arg min
θ∈
Hn(θ)
(2.128)
where Hn(θ) = −1
n
n

i=1
log pθ(Xi) .
(2.129)
Assume  is a compact subset of Rk, and H(θ) = −EPθ⋆(X) log pθ(X) is continuously
differentiable in θ ∈ so that {Hn(θ)} converges to H(θ) uniformly in probability.8
8 [The Uniform Weak Law [10]] Let Xi ∈Rm be i.i.d. random samples, fθ(X) a random variable on  × Rm
which is continuous in θ for each X. Let  be a compact subset of Rm and E supθ∈
fθ(X)
 < ∞. Then,
for all ǫ > 0,
θ ∈,
lim
n→∞Pr




1
n
n

i=1
fθ(Xi) −Efθ(X)

> ǫ


= 0.
(2.130)

28
2 THE BAYES PARADIGM
Then if H(θ) has a unique minimum θ⋆∈ with H(θ) > H(θ⋆) for all θ ̸= θ⋆, then
the maximum-likelihood estimator is consistent in probability, also denoted ˆθn
p→θ⋆.
Remark 2.6.2
We point out that H(θ) is only the true entropy of X (as in Eqn. 2.1.2)
when θ = θ⋆, i.e.
−EPθ⋆(X) log pθ⋆(X) ≤−EPθ⋆(X) log pθ(X) .
(2.131)
Proof
Since θ⋆is the unique minimum, for an open neighborhood N(θ⋆) ⊂ of θ⋆,
then H(θ) > H(θ⋆) for all θ ∈Nε(θ⋆). Choose ε = minNε(θ⋆)c #  H(θ) −H(θ⋆) > 0.
Deﬁne the event n as the set
n = {ω ∈ : |Hn(θ) −H(θ)| < ǫ/2, ∀θ ∈} .
(2.132)
Then in n, we have
H( ˆθn) −ε
2
(a)
< Hn( ˆθn)
(2.133)
(b)
< Hn(θ⋆)
(c)
< H(θ⋆) + ε
2 ,
(2.134)
with (a,c) following from 2.132, and with (b) since ˆθn is an MLE of log p(X1, . . . , Xn) it
minimizes Hn(θ). Thus we have H( ˆθn) < H(θ⋆) + ε, and apparently ˆθn ∈Nε(θ⋆), with
ε arbitrary. Thus ˆθn →θ⋆converges in probability.
2.6.2 Asymptotic Normality and √n Convergence Rate of the MLE
Here is another attack on consistency, in which by adding smoothness a convergence rate can
be obtained. More restrictions on smoothness and deﬁniteness of the Hessian actually gives a
convergence rate.
Theorem 2.47
Let the parameter space  be as above with H(θ) twice continuously differ-
entiable in θ ∈ with Hn(θ) converging to H(θ) uniformly in probability with H(θ) having
a unique minimum θ∗∈ as above.
Then if F(θ⋆)
p= limn→∞((∂2Hn(θ))/(∂θj∂θk))θ=θ⋆is a non-singular matrix, then
the MLE is consistent converging in distribution to a Gaussian with covariance given by the
inverse Fisher information:
√n

ˆθn −θ∗ d
→N

0, F(θ⋆)−1
.
(2.135)
Proof
The mean value theorem implies that there exists λ ∈(0, 1) with θ′ = θ⋆+
λ( ˆθn −θ⋆) such that
∂Hn( ˆθn)
∂θ
−∂Hn(θ⋆)
∂θ
= ∂2Hn(θ′)
∂θj∂θk
( ˆθn −θ⋆) .
(2.136)
Since ˆθn is an MLE estimator, (∂Hn( ˆθn))/∂θ = 0, implying
√n

ˆθn −θ⋆
= −√n

∂2Hn(θ′)
∂θj∂θk
−1 ∂Hn(θ⋆)
∂θ
.
(2.137)

2.6 MAXIMUM-LIKELIHOOD AND ITS CONSISTENCY
29
Examine the terms in the right-hand side of Eqn. 2.137. Asymptotic consistency
of MLE implies ˆθn, θ′ p
→θ⋆, which implies

∂2Hn(θ′)
∂θj∂θk
−1 p
→

∂2Hn(θ⋆)
∂θj∂θk
−1 p
→F(θ⋆)−1 .
(2.138)
Consider the second term in Eqn. 2.137,
−√n∂Hn(θ⋆)
∂θ
= √n ∂
∂θ

1
n
n

i=1
log pθ(Xi)


θ=θ⋆
.
(2.139)
The random variables (∂log pθ(Xi))/∂θ|θ=θ⋆are i.i.d. with zero mean and common
covariance:
E
∂log pθ(Xi)
∂θ

=

X
∂pθ(x)
∂θ
pθ(x)
pθ(x)dx =

X
∂pθ(x)
∂θ
= ∂
∂θ

X
pθ(x) = 0,
(2.140)
E
∂log pθ⋆(Xi)
∂θ
∂log pθ⋆(Xi)
∂θ
′
= E

−∂2 log pθ⋆(Xi)
∂θj∂θk

= F(θ⋆).
(2.141)
where smoothness allows us to swap expectation and differentiation. The central limit
theorem implies that and by the Uniform Weak Law theorem (see footnote 8) gives
−√n∂Hn(θ⋆)
∂θ
d
→N

0, F(θ⋆)
	
.
(2.142)
Then, by Slutsky’s theorem9 applied to Eqn. 2.137 gives
√n

ˆθn −θ⋆ d
→F(θ⋆)−1N

0, F(θ⋆)
	
∼N

0, F(θ⋆)−1
.
(2.144)
Example 2.49 (Gaussian)
FortheGaussiansettingthenpθ(X) =
1
√
2πσ 2 e−∥X−θ∥2/2σ 2,
with
H(n, θ) = 1
n
n

i=1
∥Xi −θ∥2
2σ 2
,
(2.145)
and (∂2/∂θ2)H(θ) = 1/σ 2. The MLE (1/n) 
n
i=1 Xi converges in probability to θ at
rate σ/√n.
Example 2.50
LetX1, X2, . . . bedrawni.i.dwithexponentialdensitypθ(X) = (1/θ)e−X/θ,
then the MLE is unbiased:
ˆθ = arg max
λ
−n log θ −

n
i=1 Xi
θ
=

n
i=1 Xi
n
.
(2.146)
9
Theorem 2.48
Slutsky’s Theorem [11] Suppose a sequence of random vectors Xn converges in distri-
bution to a random vector X, Xn
d
→X, and a sequence of random vectors Yn converges in probability to
a constant vector C, Yn
p
→C. Then for any continuous function g,
g(Xn, Yn) d
→g(Xn, C).
(2.143)

30
2 THE BAYES PARADIGM
2.7 Complete–Incomplete Data Problems and the EM Algorithm
We shall employ the expectation-maximization (EM) algorithm of Dempster, Laird and Rubin [12]
to solve many problems in the estimation of parameters. Problems involving the EM algorithm
are called complete–incomplete data problems, in which a function is estimated which parameter-
izes a known probability density; the actual data (denoted as the complete-data) described by the
density are not observed. Rather, observations consist of data (denoted as incomplete-data ) which
nonuniquely speciﬁes the complete-data via some set of many-to-one mappings. The motivation
is that for many classic problems parameters are estimated from measurements which are both
noisy, i.e. samples of a stochastic process, as well as incomplete.
The problem set up is to assume a prior density fθ(X) describing the complete-data X,
parameterized by some function θ, and observations Y = h(X) where h(·) is a many-to-one vector
mapping h : X →Y from the complete data observations. Then we deﬁne the complete-data
random variable X as a measurable function with density f(X) so that Pr(X ∈B) =

B f(x)dx
which is absolutely continuous with density parameterized by θ. The family of densities fθ(X)
parameterized by θ are termed the complete-data densities. We say that we are given incomplete-data
if instead of observing X in X, only the sample Y is available, where Y = h(X) for some measurable
m-dimensional vector mapping h(·); this mapping is, in general, many to one, so X is not uniquely
speciﬁed by Y. Thus the incomplete data Y results in the existence of m + 1 sample spaces, the
complete data space X and the m incomplete data spaces Y1, Y2, . . . , Ym. Denote the product space
describing the incomplete data vector Y = Y1 × Y2 × · · · × Ym. The subset χ(Y) ⊂X is given by
χ(Y) = {X : h(X) = Y}.
The family of densities gθ(Y) describing the incomplete data are given by gθ(Y) =

χ(Y) fθ(x)dx, with the conditional density on X given Y then
kθ(X|X ∈χ(Y)) =
fθ(X)

χ(Y) fθ(x)dx = fθ(X)
gθ(Y),
X ∈χ(Y) .
(2.147)
Now it follows directly that maximum-likelihood estimation over parameterized families
of log-likelihood density gθ(·) may be posed as a joint-entropy maximization.
Lemma 2.51
Given is incomplete data Y = h(X) with density parameterized by θ, gθ(Y) =

χ(Y) fθ(x)dx. Then
arg max
{θ}
log gθ(Y) = arg max
{θ}
max
{q:

χ(Y) q(x)dx=1}
−

χ(Y)
q(x) log q(x)
fθ(x)dx .
(2.148)
Proof
The proof results from the following equalities using Eqn. 2.147:
log gθ(Y) = log fθ(X) −log kθ(X|X ∈χ(Y))
(2.149)
= Ekθ(X|X∈χ(Y)){log fθ(X) −log kθ(X|X ∈χ(Y), θ)}
(2.150)
= −

χ(Y)
kθ(x|X ∈χ(Y)) log kθ(x|X ∈χ(Y))
fθ(x)
dx
(2.151)
=
max
{q:

χ(Y) q(x)dx=1}
−

χ(Y)
q(x) log q(x)
fθ(x)dx .
(2.152)
Because of the equivalence between the conditional and maximum entropy densities, the
incomplete-data log-likelihood is simply the joint-maximum with respect to q, θ of the entropy
function. This results in the estimation problem being expanded to what appears to be a larger

2.7 COMPLETE–INCOMPLETE DATA PROBLEMS
31
problem in which both the parameters θ as well as density q must be estimated, which implies the
following iterative algorithms of Dempster et al. [12] and Csiszar and Tusnady [13].
Theorem 2.52
Given is incomplete data Y = h(X) with density parametrized by θ, gθ(Y) =

χ(Y) fθ(X)dx. Deﬁnethesequenceofiteratesq(n), θ(n); n = 1, 2, . . .accordingtothefollowing
joint maximization:
q(n+1) =
arg max
{q:

χ(Y) q(x)dx=1}
−

χ(Y)
q(x) log
q(x)
fθ(n)(x)dx
(2.153)
=
fθ(n)

χ(Y) fθ(n)(x)dx = kθ(n);
(2.154)
EM −step
θ(n+1) = arg max
{θ}
−

χ(Y)
q(n+1)(x) log q(n+1)(x)
fθ(x)
dx
(2.155)
= arg max
{θ}
Ekθ(n) log fθ(X) .
(2.156)
Then this deﬁnes an expectation–maximization algorithm with iterates θ(n); n = 1, 2, ...
which are monotone nondecreasing in the incomplete data likelihood sequence
log gθ(1)(Y) ≤log gθ(2)(Y) ≤· · · .
(2.157)
Proof
This iteration is a Csiszar and Tusnady alternating minimization of the cross
entropy, where the parametrized family fθ(·) is varied through the parameters θ. The
monotonicity of the log-likelihood for the EM algorithm is inherited from the fact that
q(n+1)(X) = kθ(n)(X|X ∈χ(Y)) implying
log gθ(n−1)(Y) (a)
= −

χ(Y)
q(n)(x) log
q(n)(x)
fθ(n−1)(x)dx
(2.158)
(b)
≤−

χ(Y)
q(n)(x) log q(n)(x)
fθ(n)(x)dx
(2.159)
≤−

χ(Y)
q(n+1)(x) log q(n+1)(x)
fθ(n)(x) dx (a)
= log gθ(n) ,
(2.160)
with (a) using Eqns. 2.151 evaluated at θ(n−1), θ(n) and with (b) the EM-Step Eqn. 2.156.
Remark 2.7.3
Calculating the negative of the entropy function results in the alternat-
ing minimization of the K–Ldivergence of Csiszar and Tusnady [13] and Musicus [14].
The EM sequence is a particular example of an alternating minimization.
We now explicitly examine several maximum-likelihod estimation problems working with
the conditional mean and the EM algorithm.
Example 2.53 (Maximum-Likelihood Estimation of Sinusoids in Gaussian Noise)
Aclassic problem in spectrum estimation involves stationary Gaussian noise in which
the spectrum is unknown, and may contain time-varying means. This arises in many
contexts in spectrum estimation [15] including direction of arrival processing [16–18],
multi-dimensionalmagneticresonanceimagingspectroscopy[19–24], andradarimag-
ing [25–28] to name a few. See Kay and Marple [29] for an extensive introduction to the
problems. For the basic sinusoids in noise model, the measured ﬁeld Y(t), t ∈[0, T] is

32
2 THE BAYES PARADIGM
a superposition of exponentially decaying sinusoids in additive noise, with unknown
amplitudes, frequencies and phases:
Y(t) =

m
ame−λt + W(t) ,
(2.161)
and W(t) is “white noise” with spectral density σ 2.
The log-likelihood is coupled across the sinusoids:
log gθ(Y) = −2
2σ 2

Y(t)

m
ame−λmtdt +
1
2σ 2
 

m
ame−λmt

2
dt .
(2.162)
Direct maximization involves coupling of parameters in the log-likelihood.
The EM algorithm [12] divides the problem into M independent maximiza-
tions. Deﬁne the complete data space to be a set of independent Gaussian processes
Xm(t), m = 1, 2, . . . , one for each signal component, each having mean ame−λmt with
each noise component having spectral intensity σ 2m and σ 2 = 
m σ 2m. The map-
ping between the complete and incomplete data is given by superposition Y(t) =

m Xm(t). Then the EM algorithm is given as follows.
Theorem 2.54
Deﬁning the complete-data Xm(t) = ame−λt + Wm(t) with Wm(t), m =
1, 2, . . . , independent white noise instantaneous variance EWm(t)2 = σ 2m, with the many
to one mapping Y(t) = h(X1(t), X2(t), . . . ) = 
m Xm(t), then the sequence of iterates
generated according to
anew
m
=

¯Xold
m (t)e−λnew
m
tdt

|e−λnew
m
t|2
;
λnew
m
= arg max
{λm}


¯Xold
m (t)e−λmt
 dt
2

|e−λmt|
2
,
(2.163)
where ¯Xold
m (t) = σ 2m
σ 2

Y(t) −

m′
aold
m′ e−λold
m′ t

+ aold
m e−λold
m t,
(2.164)
are an instance of an EM algorithm implying that the likelihoods are monotonically increasing.
In turn, stable points of the iteration satisfy the necessary maximizer conditions maximizing
the likelihood of Eqn. 2.162.
Proof
The expectation of the complete data log-likelihood becomes
Ekθold log fθ(X1, X2 . . . ) =

m
2

¯Xold
m (t)ame−λmtdt −

m
 ame−λmt2dt ,
where ¯Xold
m
= E(Xm|θold, Y). Now we have to prove Eqn. 2.164 for the conditional
mean. The complete-data is Gaussian as is the incomplete data, implying that from
Eqn. 2.31 of Example 2.31 the conditional mean becomes
¯Xold
t
= µXt + KXtYtK−1
YtYt(Y −µYt) .
(2.165)
Clearly µXt = ame−λold
m t. Since Y(t) = 
m Xm(t) and X1, X2, . . . are independent,
we have KXtYt = (σ 2
1 , σ 2
2 , . . . ), K−1
YtYt = diag(1/σ 2, 1/σ 2, . . . ) giving ¯Xold
m
in the form
Eqn. 2.164. Using the conditional mean and maximizing gives the update EM iteration,
Eqns. 2.163.
Notice, theEMalgorithmbreaksthemaximizationintoindependentproblemsacrossthesinusoids.

2.7 COMPLETE–INCOMPLETE DATA PROBLEMS
33
F2
F1
F2
F1
Figure 2.3 Left column: The ﬁgure shows the results from the N-butyl alchohol experiment. The
top panel shows the original 2D spectrum of the N-butyl alcohol data. The bottom panel shows
the spectrum reconstructed from the estimates of the EM-algorithm parameters. The data are
taken from [23]. Right column: The ﬁgures show the EM algorithm reconstruction of the COSM
data taken from Dr. Keeling of Washington University; the data reconstructions are from [30]. The
top row shows X–Z sections through the COSM amoeba data. The bottom row shows the 200th
EM algorithm iteration for sections through the COSM amoeba data (see also Plate 1).
In 1- and 2D nuclear magnetic resonance (NMR) spectroscopy time series are segmented
into their constituent chemical shifts, modeled as sinusoids in noise [19–21] and modeled as expo-
nentially decaying sinusoids signals in Gaussian noise [18,22,24]. In d-dimensional spectroscopy,
the decaying sinusoids are indexed over Rd so that t = (t1, . . . , td), and
Y(t) =

m
ame−⟨λm,t⟩ddt + W(t),
where ⟨λm, t⟩d =
d

i=1
λmiti .
(2.166)
Shown in Figure 2.3 are results from 2D magnetics resonance spectroscopy [22–24] for the analysis
of N-butyl alcohol. In using the EM algorithm to determine the amplitude and signal parame-
ters, twenty-two peaks were found. Column 1 of Figure 2.3 shows the original spectrum of the
N-Butanol analysis data (top panel). The bottom panel shows the reconstructed spectrum from the
maximum-likelihood ﬁt of the peaks. The reconstructed spectrum closely resembles the original
spectrum. There are minor differences observed in these two spectra, including small noise peaks
present in the original spectrum not found in the reconstructed spectrum, and minor differences
in the height and width of the peaks in the two spectra. Otherwise, the ML algorithm reconstructs
the signal in the N-Butanol spectrum.
Example 2.55 (Image Restoration in Point Processes)
Examine the image restora-
tion problem via conditionally Poisson models. A variety of imaging applications
involving measurement errors of various kinds have been studied on Poisson pro-
cesses including forward looking infra-red imaging (FLIR) via CCD arrays [31, 32],
computational optical sectioning microscopy (COSM) [30, 33], electron-microscopic-
autoradiography [34], and emission tomography both positron (PET) [35–37] and
single-photon (SPET) [38–40]. The image restoration must take into account two
fundamental components characteristic of the imaging systems: (i) the number of

34
2 THE BAYES PARADIGM
measurement points are low and therefore dominated by Poisson statistics, and (ii) the
physics of the measurement systems introduced in the creation of the observed data.
The restoration is based on the model of Snyder [41] which hypothesizes the existence
of two point-processes, hence the connection to complete–incomplete data.
Examine the discrete version of the counting process here. The photon counts are
modeled as a Poisson counting process Xi the number of points with mean EXi = λi
in discrete position i in the discrete lattice. The counts in subsets A are deﬁned by the
counting process X(A) = 
i∈A Xi, with mean EX(A) = 
i∈A λi. The Poisson law
is given by $
i(e−λiλXi
i /Xi!), with complete-data log-likelihood only a function of λ
given by
log fλ(X) = −

i
λi +

i
Xi log λi.
(2.167)
Generally, these are observed with measurement errors reﬂecting the uncertainty due
to optical focus or detector uncertainty in optical sectioning and the line- and time-of-
ﬂight uncertainty in positron-emission tomography. The error vectors in measurement
are assumed to be independent of the creation of the counting process, described
via point-spread functions conditioned on voxel i, p(·|i), 
j p(j|i) = 1. The measure-
ments Yj are modeled as Poisson-distributed [42] with mean EYj = 
i p(j|i)λi, and
log-likelihood
log gλ(Y) = −

i
λi +

j
Yj log


i
p(j|i)λi

.
(2.168)
The EM iteration is as follows.
Theorem 2.56
The sequence of iterates generated according to
λnew
i
= λold
i


j
p(j|i)

i′ p(j|i′)λold
i′
Yj

,
(2.169)
are an instance of an EM algorithm with stable points of the iteration satisfying the necessary
maximizer conditions for the maximizer of Eqn. 2.168.
Proof
The new iterates maximize the conditional expectation of the complete-data
log-likelihood
Ekλold (X|Y) log fλ(X) = −

i
λi +

i
¯Xold
i
log λi ,
(2.170)
where ¯Xold
i
= E(Xi|Y, λold) = λold
i


j
p(j|i)

i′ p(j|i′)λold
i′
Yj

.
(2.171)
Maximizing gives the iterates of Eqn. 2.169. The convergence point λnew = λold gives
the ﬁxed point condition10
1 =


j
p(j|i)

i′ p(j|i′)λi′ Yj

,
(2.172)
10 [Convergence of the EM-algorithm to MLEs] Vardi et al. [43] proved that the discrete Eqn. 2.169 has
global convergence properties; the initial estimate λ(0)
i
can be any positive bounded function with the sequence
converging to a an MLE satisfying the necessary and sufﬁcient maximizer conditions. The neat proof of Vardi
et al. breaks into two parts: (i) showing that if the iteration of 2.169 converges, the Kuhn–Tucker conditions are
satisﬁed and therefore the convergence point of the algorithm maximizes the log-likelihood; and (ii) showing
that every sequence converges. Proof of (i) follows since the log-likelihood is concave and Shepp and Vardi [36]

2.7 COMPLETE–INCOMPLETE DATA PROBLEMS
35
whichisthenecessarymaximizerconditionforinteriorpointsλi > 0. Fortheboundary
term λi = 0, see [36].
The maximization of the discrete log-likelihood via the EM algorithm was concurrently derived
and implemented by Shepp and Vardi [36] for positron-emission tomography and Lange and
Carson [44] for transmission tomography, and subsequently for tomography systems with time-
of-ﬂight by Snyder and Politte [45]. Similar solutions have been derived for single-photon
tomography and electron microscopic autoradiography, with the appropriate imaging models
chosen in each [34,46].
In the statistical models proposed for time-of-ﬂight positron emission tomography [35] and
computational optical sectioning microscopy (COSM) [30, 47, 48], the point-spread function p(·)
reﬂects the 3D function in the source volume. In COSM the specimen ﬂuoresces incoherently
when illuminated with the photons detected only after they have undergone random transla-
tions as a result of the microscopes point spread function (see [30, 47, 48]). The point-spread
function p(k)(j|i), k = 1, . . . , K, reﬂects the conditional probability-density that a photon incoher-
ently ﬂourescing at point x is detected at point y when the microscope is focused to plane k. Much
work has been done on characterizing optical transfer (see [47,48], e.g.).
Shown in the right column of Figure 2.3 are the results of the EM algorithm reconstruction
in COSM of amoeba. The top row shows X–Z sections through the 3D data measurement of an
amoeba collected using the optical sectioning microscope. Notice the blurriness of the measured
data resulting from the optical sectioning point-spread function. Each measurement Y(k) has a
mean corresponding to its point-spread function EY(k)
j
= 
x p(k)(j|i)λi, with 
k

j p(k)(j|i) = 1.
The set of measurements are Poisson distributed according to incomplete-data log-likelihood
log gλ(Y(1), Y(2), . . . ) = −

i
λi +
K

k=1

j
Y(k)
j
log


i
p(k)(j|i)λi

.
(2.173)
The EM algorithm above is modiﬁed only slightly from Eqn. 2.169 adjusted to incorporate
the multiple point-spread orientation terms according to
λnew
i
= λold
i


k

j
Y(k)
j
pk(j|i)

i′ pk(j|i′)λold
i′

.
(2.174)
Shown in the right column of Figure 2.3 (bottom row) are the results of the reconstructed
intensity of ﬂuorescence resulting from 200 iterations of the EM algorithm for several sections.
Notice the increased structure resulting from the deconvolution algorithm.
Example 2.57 (Segmentation of Brains)
Gaussian mixture modeling has been ubiq-
uitous in cortical brain analysis by several groups [49–57]. The brain is modelled as
having multiple compartments including gray matter (G), white matter (W), and cere-
brospinal ﬂuid (CSF) with different tissue types having different mean and variance
parameters.
Model the image as a Gaussian mixture of different regions each with its own
parameters. Then the image Xi, i = 1, . . . , n of n = |D| discrete voxels is modeled
as a Gaussian density fµ,σ 2(Xi) =
1
√
2πσ 2 e−(Xi−µ)2/2σ 2 given the means and vari-
ances µ, σ 2. The data forming the histogram from the measured imagery is modeled
showed that the convergence point satisﬁes the necessary Kuhn–Tucker conditions. Proof of (ii) is more subtle,
requiring the results of Csiszar and Tusnady [13]. For the particular alternating maximization of 2.169, the K–L
divergence between any limit point of the sequence and successive iterates of the algorithm decreases. This
coupled with the fact that every sequence has a set of subsequential limit points due to the compactness of
the iteration set and the fact that the limit points are stable, implies global convergence for the full sequence
of iterates.

36
2 THE BAYES PARADIGM
as conditionally independent samples X1, . . . , Xn from a Gaussian mixture density
gθ(X) = 
M
m=1 amd(X; θm), with d(X; θm) the Gaussian density with parameters
θm = (µm, σ 2m, am).
Model the measured data forming the histogram as conditionally independent
samples Y1, Y2, . . . from a Gaussian mixture density
gθ(Y1, . . . , Yn) =
n

i=1

m
amdθm(Yi),
with dθm(Y) =
1
√
2πσm
e
−(Y−µm)2
2σ 2m
.
(2.175)
The goal is to estimate the parameters making up the mixture θm = (am, µm, σ 2m),
m = 1, 2, . . . .
Model the complete data as the pairs X1 = (Y1, M1), X2 = (Y2, M2) . . . , with
Mi labeling which of the models the data arises from. The many to one mapping
from complete to incomplete data discards the compartment tags. The complete data
density becomes
fθ((Y1, M1), (Y2, M2), . . . ) =
n

i=1

m
(amdθm(Yi))δ(Mi−m)
(2.176)
with the indicator functions δ(M −m) = 1 if M = m, and 0 otherwise. The EM
algorithm is as follows.
Theorem 2.58
The sequence of iterates generated according to
anew
m
= aold
m
n

i=1
dθold
m (Yi)

θm′ aold
m′ dθold
m′ (Yi)
,
µnew
m
=

n
i=1 Yi(aold
m dθold
m (Yi)
% 
m′ aold
m′ dθold
m′ (Yi))

n
i=1(dold
θm (Yi)
% 
m′ aold
m′ dθold
m′ (Yi))
,
(σ 2
m)new =

n
i=1(Yi −µnew
m
)2(aold
m dθold
m (v)
% 
m′ aold
m′ dθold
m′ (Yi))

n
i=1(dold
θm (Yi)
% 
m′ aold
m′ dθold
m′ (Yi))
,
(2.177)
are an instance of an EM algorithm with nondecreasing likelihood. The stable points of the
iterations satisfy the necessary maximizer conditions of the likelihood density of Eqn. 2.175.
Proof
The expectation step of the EM algorithm requires the conditional expectation
of the log-likelihood according to
Ekθold log fθ(X1, X2, . . . ) =
n

i=1

m
E
&
δ(Mi −m)|Y, θold'
log amdθold
m (Yi) (2.178)
where E{δ(M −m)|Y, θ} =
amdθold
m (Y)

m′ am′dθold
m′ (Y) .
(2.179)

2.7 COMPLETE–INCOMPLETE DATA PROBLEMS
37
The maximization step is carried out with the constraint 
m am = 1 giving the new
maximizers of the EM iteration of Eqn. 2.177:
n

i=1
aold
m dθold
m (Yi)

m′ aold
m′ dθold
m′ (Yi)
1
am
+ γ = 0,
n

i=1
aold
m dold
m (Yi)

m′ aold
m′ dold
θm′ (Yi)

∂
∂µm
dθm(Yi)

1
dθm(Yi) = 0,
n

i=1
aold
m dold
m (Yi)

m′ aold
m′ dθold
m′ (Yi)

∂
∂σ 2m
dθm(Yi)

1
dθm(Yi) = 0 .
(2.180)
To ﬁnish the proof, clearly at a ﬁxed point θold = θnew then Eqns. 2.180
are the necessary maximizer conditions of the incomplete data log-likelihood

n
i=1 log 
m amdθm(Yi; θ) of Eqn. 2.175.
Shown in Figure 2.4 are results from the cingulate and prefrontal gyrus. Panel 1 shows
the EM algorithm ﬁt of G,W,CSF, and partial volume compartments to a cingulate gyrus brain
histogram to illustrate the parameter ﬁtting. The top solid curve is the mixture model with ﬁtted
parameters from the EM algorithm. It superimposes the measured histogram data almost exactly.
Shown below via the lower dashed lines are each of the compartment ﬁts taken separately. Panel 2
shows a single MRI section through the cingulate gyrus. The minimum Bayes risk for segmentation
(as in Section 2.3, Chapter 2) in the m = 1, . . . , M compartments selects for each i the model-type:
ˆHi = arg max
Hi∈{1,...,M}
−1
2 log 2πσ 2(Hi) −1
2
(Xi −µ(Hi))2
σ 2(Hi)
.
(2.181)
0
50
100
150
200
250
0
1
2
3
4
5
6
7
8 x 104
Intensity
Frequency
Thresholds: Cingulate s1003
A
B
Figure 2.4 Panel 1 shows the EM algorithm ﬁt of G,W,CSF and partial volume compartments
to brain tissue histograms to illustrate the segmentation calibration. The top solid curve super-
imposes the measured histogram data almost exactly. The lower dashed lines depict each of the
compartment ﬁts taken separately. Panel 2 shows an MRI section of the cingulate; panel 3 shows
the Bayes segmentation into G,W, CSF compartments of coronal sections of the cingulate gyrus;
Panels 4 and 5 show the same as above (row 1) for the medial prefrontal gyrus. Data taken from
the laboratories of Dr. John Csernansky and Dr. Kelly Botteron of Washington University (see also
Plate 2).

38
2 THE BAYES PARADIGM
Panel 3 shows the solution to the Bayes hypothesis testing problem for selecting G,W, and CSF in
each image voxel Xi in the coronal section of the cingulate gyrus. The bottom row panels 4 and 5
show the same for the prefrontal gyrus.
2.8 Hypothesis Testing and Model Complexity
Consider the multihypothesis testing problem of determining model m = 1, 2, . . . from random
observation Y1, Y2, . . . . Proceeding in a Bayesian manner, the obvious solution is through mini-
mum risk implying the fundamental role of Bayesian hypothesis testing and likehood ratio testing
on the model given the observation. Given the sequence of conditional densities pm(Y) across
models with priors πm, m = 1, 2, . . . then Bayesian model selection is from Theorem 2.15 the mini-
mum risk. Most often, however, the models involve random nuisance parameters with some prior
density πm(X), X ∈Xm ⊆Rdm of dimension dm, requiring the calculation of the so-called nuisance
integral of the conditional density on the random sample Y1, Y2, . . . for every m taking the form
ˆm = arg max
m
πmpm(Y1, . . . , Yn),
(2.182)
where pm(Y1, . . . , Yn) =

Xm
pm(Y1, . . . , Yn|x)πm(x)dx .
(2.183)
Such celebrated integrals have received tremendous attention, and only in special cases can
they be performed analytically. They have been focused on in the context of Bayesian integration of
nuisance parameters via the Laplace’s method (see Schwarz [58]) which is precisely the approach
we shall follow. We return to this below in the context of model complexity and Rissanen’s [59]
pioneering work. Most inﬂuential has been the study of the information-theoretic asymptotics of
Bayes methods by Barron and Clark [60].
2.8.1 Model-Order Estimation and the d/2 log Sample-Size Complexity
Since its introduction by Fisher, the method of maximum-likelihood has certainly proven to be one
of the most ubiquitous and effective methods for vector parameter estimation when the dimension
of the parameter space is ﬁxed and ﬁnite. When the dimension of the parameter space itself needs
to be estimated, maximum-likelihood techniques tend to be “greedy,” consistently picking the
models of greatest complexity to yield overly tight ﬁts to the data. This is the so called model
order estimation problem. The challenges of model order estimation were originally addressed by
Akaike(1973), andsubsequentlySchwarz[58] andRissanen[59]. Rissanen’sminimumdescription
length principle (MDL) seeks to remedy the problem by incorporating the complexity of the model
in the calculation; greedy selection is moderated by the complexity of the model.
The exact formulation of the complexity for the Gaussian case results from the quadratic
expansion. We should expect, if the posterior density is smooth then for large sample size the
integrand can be approximated via Laplace’s method (Polya & Szego [61], p. 96). The technical
requirements for this are smoothness and identiﬁability.
Condition 2.59 (Smoothness Conditions)
Assume the parameter space from model
m = 1, 2, . . . is closed and bounded with prior πm(x), x ∈Xm ⊂Rdm. Deﬁne the joint

2.8 HYPOTHESIS TESTING AND MODEL COMPLEXITY
39
density on the i.i.d. sample Y1, . . . , Yn for all x ∈Xm ⊂Rdm to be of the form
pm(Y1, . . . , Yn, x) = e−nHm(n,x), Hm(n, x) = −1
n
n

i=1
log pm(Yi|x) −1
n log πm(x).
(2.184)
AssumeHm(n, x)satisﬁesthefollowingsmoothnessandidentiﬁabilityconditionsforeveryn:
1. Hm : x →Hm(x) are third derivative continuous, and
2. for all (Y1, . . . , Yn) ∈Yn
0 , there exists a unique ˆx ∈Xm such that
Hm(n, ˆx) < Hm(n, x)
for all x ∈Xm/{ˆx},
and for any closed set G ⊂Xm not containing ˆx,
inf
x∈G Hm(n, x) > Hm(n, ˆx); and
3. positive deﬁniteness of the dm × dm Hessian matrix at the MAP estimator so that
Fm(ˆx) =

∂2
∂xj∂xk
Hm(n, ˆx)

,
(2.185)
is non-singular in a neighborhood of ˆx ∈Xm.
Now the asymptotics on the nuisance integral follows. Laplace’s approach employs a Taylor
series expansion around the maximizer ˆx.
Theorem 2.60 (Nuisance Integral)
Assume models m
=
1, 2, . . . with closed and
bounded parameter spaces with priors πm(x), x ∈Xm ⊂Rdm. Deﬁne the joint density on
Y1, . . . , Yn to be
pm(Y1, . . . , Yn, x) = e−nHm(n,x),
Hm(n, x) = −1
n
n

i=1
log pm(Yi|x) −1
n log πm(x) ,
(2.186)
satisfying the smoothness conditions 2.59 with Hessian
Fm(x) =

∂2
∂xj∂xk
Hm(n, x)

.
(2.187)
Deﬁning A(n) ∼B(n) to mean A(n) is asymptotically equal to B(n), then as n →∞with
pm(Y1, . . . , Yn) ∼pm(Y1, . . . , Yn, ˆx)
2π
n
dm/2
det−1/2 Fm(ˆx)
(2.188)
where ˆx = arg max
x∈Xm
pm(Y1, . . . , Yn, x) .
(2.189)
The asymptotic Bayes testing procedure is equivalent to
max
m πmpm(Y1, . . . , Yn) ∼max
m πmpm(Y1, . . . , Yn, ˆx)
2π
n
dm/2
det−1/2 Fm(ˆx).
(2.190)

40
2 THE BAYES PARADIGM
Proof
Expand Hm in a Taylor series attaining a minimum at ˆx with ∇xHm(n, ˆx) = 0
so that for some x1 ∈Bǫ(ˆx) a local neighborhood of ˆx ∈Xm,
Hm(n, x) = Hm(n, ˆx) + 1
2(x −ˆx)∗

∂2
∂xj∂xk
Hm(n, ˆx)

(x −ˆx).
(2.191)
Now use the uniqueness of the MLE of property 2 from 2.59 to get for all x ∈X \Bǫ(ˆx)
there exists a δ so that H(n, x) > H(n, ˆx) + δ giving

Xm
e−nHm(n,x)dx =

Bǫ(ˆx)
e−nHm(n,x)dx +

Xm/Bǫ(ˆx)
e−nHm(n,x)dx
(2.192)
= e−nHm(n,ˆx) 
Bǫ(ˆx)
e−(n/2)(x−ˆx)∗Fm(n, ˆx)(x−ˆx)dx + O(e−δn)

.
(2.193)
Third derivative continuity gives k = supx∈Bǫ(ˆx)(∂3/∂xixjxk)Hm(x) which with the
intermediate value theorem implies for x ∈Bǫ(ˆx),
Fm(x)jk −Fm(ˆx)jk
 < k∥x −x⋆∥= O(ǫ),
giving

Xm
e−nHm(n,x)dx
= e−nHm(n,ˆx)

Bǫ(ˆx)
e−(n/2)(x−ˆx)∗
Fm(ˆx)+O(ǫ)
	
(x−ˆx)dx + O(e−δn)

(2.194)
= e−nHm(n,ˆx)

Rdm e−(n/2)(x−ˆx)∗
Fm(ˆx)+O(ǫ)
	
(x−ˆx)dx + O

e−(n/2)c∥ǫ2∥	
+ O(e−δn)

.
(2.195)
Using property 3 then Fm(ˆx) is positive deﬁnite in the neighborhood of ˆx to induce
the density transformation of the Gaussian law. Fixing ˆx, substitute u = (nFm(ˆx) +
O(ǫ))1/2x giving

Rdm e(−1/2)u∗udu
 1
n
dm/2
det−1/2 
Fm(ˆx) + O(ǫ)

=
2π
n
dm/2
det−1/2 
Fm(ˆx) + O(ǫ)

,
(2.196)
since

e−(1/2)u∗udu = (2π)dm/2. Since this is true for arbitrarily small ǫ, this completes
the proof.
Model selection by this asymptotic procedure is often called the Bayesian information
criterion. It accomodates the prior distribution in a straightforward manner.
In i.i.d. large sample cases, as n →∞then the model selection problem is dominated by the
terms which are functions of n, yielding the classic approximation
log pm(Y1, . . . , Yn) ≈log pm(Y1, . . . , Yn|ˆx) −dm
2 log n,
(2.197)
giving the complexity of the model is dimension dm log sample −size. We choose the model m
which maximizes (2.197).

2.8 HYPOTHESIS TESTING AND MODEL COMPLEXITY
41
2.8.2 The Gaussian Case is Special
Examine the Gaussian case which can be handled analytically and demonstrates the fundamental
role of the Fisher information in evaluating the integral. For the Gaussian case the nuisance integral
of Eqn. 2.183 can be calculated directly. Examine the following time honored model, let Y ∈Rn
be a real-valued n-dimensional conditionally Gaussian process, Y = 
d
i=1 Xiφi + W, W white
Gaussian noise variance σ 2, {φi}d
i=1 an orthonormal set of n×1 vectors spanning the d-dimensional
subspace of Rn. If the Xi’s are Gaussian variates as well, then Y is Gaussian with a particularly
interesting decomposition in the Fisher information. Then the MAP estimators ˆXi, i = 1, . . . , d are
given by
ˆXi = arg max
Xi∈Rd
log p(X1, . . . , Xd|Y) =
λ2
i
λ2
i + σ 2 ⟨Y, φi⟩Rn.
(2.198)
The d × d empirical Fisher information (including the prior) are given by
F =

−E
∂2
∂xi∂xj
log p(X1, . . . , Xd|Y)

=

λ2
i + σ 2
λ2
i σ 2
δ(i −j)

.
(2.199)
Theorem 2.61
Given conditionally Gaussian process, Y = 
d
i=1 Xiφi + W, W white
Gaussian noise variance σ 2, {φi}d
i=1 an orthonormal set of n × 1 vectors. Let Xi be zero-
mean independent Gaussian, variance λ2
i , i = 1, . . . , d. The density in terms of the Fisher
information and MAP estimator are exact:
p(Y) = p(Y, ˆX1, . . . , ˆXd)(2π)d/2 det−1/2 F .
(2.200)
As σ →0, the penalty goes as
log det−1/2 F ∼−d/2 log σ 2 + O(1) .
(2.201)
Proof
The MAP estimator becomes
0 =
∂
∂xj
∥Y −
d
i=1 Xiφi∥2
Rn
2σ 2
+
d

i=1
|Xi|2
2λ2
i
= −2
2σ 2

Y −
d

i=1
Xiφi, φj

Rn
+
2Xj
2λ2
j
(2.202)
implying ˆXj =
λ2
j
λ2
j +σ 2 ⟨Y, φj⟩Rn.
Deﬁning
H(X1, . . . , Xd) =
∥Y −
d
i=1 Xiφi∥2
Rn
2σ 2
+
d

i=1
|Xi|2
2λ2
i
,
(2.203)

42
2 THE BAYES PARADIGM
then rewriting the conditional density gives
p(Y) =

Rd p(Y|x1, . . . , xd)π(x1, . . . , xd)dx1 · · · dxd
(2.204)
=

Rd
1
(2πσ)n/2 e−
∥Y−
d
i=1 xiφi∥2
Rn
2σ2
1
(2π)d/2 $d
i=1 λi
e
−
d
i=1
|xi|2
2λ2
i dx1 · · · dxd
(2.205)
=

Rd
1
(2πσ)n/2
1
(2π)d/2 $d
i=1 λi
e−H(x1,...,xd)dx1 · · · dxd.
(2.206)
Expand H(x1, . . . , xd) in a Taylor series around the MAP estimators ˆX1, . . . using the
fact that the gradient term is zero at the MAP estimator gives
H(x1, . . . , xd) = H( ˆX1, . . . , ˆXd) + 1
2
d

i,j=1
(xi −ˆXi)(xj −ˆXj)Fij .
(2.207)
Substituting for the MAP estimator gives
p(Y) = p(Y, ˆX1, . . . , ˆXd)

Rd e−1/2 
d
i,j=1(xi−ˆXi)(xj−ˆXj)Fijdx1 · · · dxd
(a)
= p(Y, ˆX1, . . . , ˆXd)(2π)d/2 det−1/2 F = p(Y, ˆX1, . . . , ˆXd)(2πσ 2)d/2 d

i=1
λ2
i
λ2
i + σ 2
1/2
,
(2.208)
where (a) follows from the fact that the function (2π)−d/2 det1/2 Qe(−1/2)x∗Qx inte-
grates over Rd to 1 where Q is a positive deﬁnite d × d matrix.
2.8.3 Model Complexity and the Gaussian Case
To introduce MDL, let us formulate it in a coding context as in Lanterman [62] in discrete and
ﬁnite hypothesis space context. Suppose the data Y and parameters X are elements of discrete
spaces, with the goal being to encode the data Y with a two-part message. The ﬁrst part indicates
the parameters X for that model, and the second encodes the data Y; the total message length
becomes
len(Y, X) = len(Y|X) + len(X, m).
(2.209)
The MDL principle selects the X and m which minimize (2.209) for the collected data Y. Shannon’s
theory dictates that for a given model m and parameter X, a uniquely decodable code exists for Y
with codewords of length len(Y|X, m) = ⌈−log p(Y|X, m)⌉. (For convenience, drop the notation
for “next largest integer” in the remaining discussion.) If X could somehow be transmitted cost-
free, then the maximum-likelihood estimate would be selected for X since it would minimize
len(Y|X, m). However, the parameter X must be encoded as well as it indexes the law. In this data
transmission viewpoint, the code for X must be a preﬁx (also called “self-punctuating”) code. This
means that the stream representing Y given X may follow the stream representing X without an
additional “comma” symbol. This implies the code for X must satisfy the Kraft inequality (Cover
and Thomas, 1991, section 5.2, p. 82 [7])

x
e−len(X) ≤1.
(2.210)

2.8 HYPOTHESIS TESTING AND MODEL COMPLEXITY
43
Hence, π(X) ∝e−len(X) gives a proper prior distribution on X. Similarly, if π(X) is on hand
then it can be used to ﬁnd the code lengths len(X).
To
quantitatively
measure
the
model
complexity
associated
with
parameters
(X1, X2, . . . , Xd) ∈Xd, deﬁne the complexity of model m as given by the difference between the
information gained about the outputs of the model with and without knowledge of the parameters.
Deﬁnition 2.62
Given model m outputting random sample Y parametrized by X1, X2, . . .
with conditional density pm(Y|X1, . . . , Xdm), then the complexity of model m is given by
the mutual information
C(m) = h(Y) −h(Y|X1, . . . , Xdm) .
(2.211)
For the quadratic (Gaussian) case the complexity can be calculated exactly.
Theorem 2.63
Given is Y ∈Rn a real-valued n-dimensional Gaussian vector with Gaussian
mean 
dm
i=1 Xiφi, φ ∈Rn, in additive white-noise variance σ 2, with Xi zero-mean Gaussian
variates with variances λ2
i . The complexity of the dm-parameter Gaussian model is
C(m) = h(Y) −h(Y|X1, . . . , Xdm) =
dm

i=1
1
2 log
σ 2 + λ2
i
σ 2
.
(2.212)
As σ 2 →0, C(m) ∼dm
2 log σ 2.
Proof
The set of Yi = ⟨Y, φi⟩, i = 1, . . . , d are zero-mean, Gaussian variance σ 2 + λ2
i ,
with Yi, i = d + 1, . . . , n having variance σ 2. The entropy of Y becomes
h(Y) = 1
2
dm

i=1
log 2π(σ 2 + λ2
i ) + n −dm
2
log 2πσ 2 .
(2.213)
The conditional entropy h(Y|X1, . . . , Xdm) becomes
h(Y|X1, . . . , Xdm) = n
2 log 2πσ 2 .
(2.214)
Computing their difference gives the result.
2.9 Building Probability Models via the Principle of Maximum Entropy
As one proceeds in pattern theory we basically have two possible tools for solving inference prob-
lems. The ﬁrst we have already seen, minimum-risk estimation which when given parametrized
probability models delivers estimators of the random parametric functions. However, one might
ask the fundamental question: Where do the probability models over which the inference problems are
constructed come from. This involves our second basic tool, gathering context from the patterns and
using it to construct probabilistic models which are of maximum entropy. Estimation and model
building co-exist; almost without exception the estimation problems are solved in the probability
models which are constructed to be of maximum entropy.

44
2 THE BAYES PARADIGM
2.9.1 Principle of Maximum Entropy
There has, over the past several decades, been a tremendous increase in the application of
maximum-entropy techniques to constraint problems with nonunique solutions. The rational
has been most elequently formulated by Jaynes [63]: of all candidates consistent with a set of
constraints the maximum-entropy (maxent) solution is the one which occurs with greatest multi-
plicity. The success of the entropy function is due to the property that the candidate solutions are
concentrated strongly near the maxent one; solutions with appreciably lower entropy are atypical
of those speciﬁed by the data [64].
Throughout, given is a prior density p(x), x ∈X,

X p(x)dx = 1. In Jaynes principle of maximum
entropy, probability densities q(x), x ∈X,

X q(x)dx = 1 are the objects being estimated, and are
chosen to maximize the entropy function −D(q||p) = −

X q(x) log (q(x))/(p(x))dx. Incomplete
data observations come in the form of mean values of known functions h with respect to the
unknown density H =

X q(x)h(x)dx. Jaynes principle is to choose the density which is consistent
with the prior and satisﬁes the incomplete data observables.
Theorem 2.64 (Jaynes Principle of Maximum Entropy)
Given
prior
p(·)
on
X,

X p(x)dx = 1, with moment constraints via observation functions hm, m = 1, . . . , M with
expected values Hm = Eq{hm}, m = 1, . . . , M, then the unique density maximizing the
entropy (minimizing Kullback–Leibler distance) −D(q||p) satisﬁes
ˆq(·) =
arg max
q(·):

X q(x)dx=1
−

X
q(x) log q(x)
p(x)dx , subject to Hm
=

X
q(x)hm(x)dx ,
m = 1, . . . , M
(2.215)
= eλ0+
M
m=1 λmhm(·)p(·).
(2.216)
Proof
For this use the proof of Cover and Thomas [7]. Take ˆq(·) = eλ0+
M
m=1 λmhmp(·),
and examine any other q satisfying the moment-constraints which satisfy the following
set of inequalities:
−D(q||p) = −

X
q(x) log q(x)
p(x)dx = −

X
q(x) log q(x)
ˆq(x)dx −

X
q(x) log ˆq(x)
p(x)dx
(2.217)
= −D(q||ˆq) −

X
q(x) log ˆq(x)
p(x)dx
(2.218)
(a)
≤−

X
q(x)

λ0 +
M

m=1
λmhm

dx = −

X
ˆq(x)

λ0 +
M

m=1
λmhm

dx
(2.219)
= −

X
ˆq(x) log ˆq(x)
p(x)dx = −D(ˆq||p),
(2.220)
where (a) follows from log x ≥1 −(1/x), with equality attained if q = ˆq almost
everywhere in X.

2.9 BUILDING PROBABILITY MODELS
45
2.9.2 Maximum Entropy Models
Almost all of the distributions we shall study will be of maximum-entropy. Maximum-entropy
distributions are tilted, tilted by the expectations or moment constraints of the observable
functions h.
The probability models we use are maximum entropy.
Theorem 2.65
1. Independent processes: Let X1, . . . , XN, be ﬁnite valued, Xi ∈{1, 2, . . . , J}. For ﬁrst
order marginals which are stationary, E{1j(Xn)} = pj, j = 1, . . . , J, the maxent
distribution is the product law of independence
P(X1, . . . , XN) =
N

n=1
P(Xn) ;
(2.221)
and the sequence X1, . . . , Xn is i.i.d.
2. First order Markov: Given joint stationary marginals, E{1ij(Xn, Xn+1)} = Qi,j,
i, j = 1, . . . , J, the maxent distribution is 1st order Markov; with initial distribution
Pr(X1 = j) = π(j),
P(X1, . . . , XN) =
N−1

n=2
P(Xn+1|Xn)π(X1) .
(2.222)
3. Multivariate Gaussian: Let X be real valued with ﬁrst two moments E[X] = µ,
EX2 = α, the maximum entropy density is Gaussian with mean and variance µ, α−µ2.
Let X1, . . . , XN, be real valued, then given mean and covariance constraints
E{Xn} = µn, E{(Xm −µm)(Xn −µn)} = Kmn, the maximum entropy density is
multivariate normal:
p(X1, . . . , XN) =
1
(
√
2π)n det1/2 e−(1/2) 
N
m,n=1 XmXnλmn
(2.223)
where  = K−1 =

Kij
−1
.
(2.224)
4. Whitening Model: Let X = X1, . . . , XN, be a real valued zero-mean process, L = (Lij)
an n × n nonsingular matrix. Then given second moments E|LXi|2 = σ 2 for all i =
1, . . . , n, LXi = 
j LijXj, then X is a Gaussian vector process satisfying the equation
LXi = Wi ,
i = 1, . . . , n ,
(2.225)
W = “white Gaussian noise” variance σ 2.
5. Exponential Interarrivals and Poisson density: Let N[0,t), t ≥0 be a counting pro-
cess with arrival times W0 = 0, W1, W2, . . . and interarrival times T1 = W1, T2 =
W2 −W1, . . . . Then, if E(Ti) = 1/λ, then the maximum entropy density for waiting
times are exponential
p(Ti) = λe−λTi .
(2.226)
The counting process N[0,t) is Poisson with density
P(N[0,t)) = e−λt(λt)N[0,t)
N[0,t)!
.
(2.227)

46
2 THE BAYES PARADIGM
Proof
Marginal Independence:
P(X1, . . . , XN) = eλ0+
J
j=1

N
n=1 λj(n)1j(Xn)
(2.228)
= eλ0
N

n=1
eλXn(n) .
(2.229)
This is a product density, and choosing eλj(n) = P(j) satisﬁes the constraints.
Joint marginal Markov chain: To prove the Markov property,
P(XN|X1, . . . XN−1) =
P(X1, . . . , XN)

J
XN=1 P(X1, . . . , XN)
=
$N−1
n=1 eλ0eλXn,Xn+1(n)π(X1)

J
XN=1
$N−1
n=1 eλ0eλXn,Xn+1(n)π(X1)
(2.230)
=
eλXN−1,XN (N−1)

J
XN=1 eλXN−1,XN (N−1) .
(2.231)
This is a function of XN−1, XN implying the ﬁrst order Markov property.
Multivariate Gaussian:
p(X1, . . . , XN) = eλ0+
N
n=1 λnXn+
N
m=1

N
n=1 λmn(Xm−µm)(Xn−µn).
(2.232)
Whitening Process: The second moment constraints give
p(X1, . . . , Xn) =
1
Z(λ)
n

i=1
e−λi(LXi)2 ,
(2.233)
and choosing the λi = 1/2σ 2 gives the Gaussian density. Choosing Wi = LXi gives the
product law for W implying it is a white process, which is Gaussian (linear function
of X).
Poisson Counting Process: The expected value on waiting times being given by the
mean E(T) = 1/λ implies the maximum entropy density is of the form p(T) = λe−λT.
Independence of interarrivals follows from the maximum entropy principle giving a
product density
p(T1, . . . , Tn) = λn
n

i=1
e−λTi .
(2.234)
This is sufﬁcient to imply the process is the number of arrivals is Poisson with
parameter λ (see Snyder [65]). Deﬁning the event times to be Wi, then
Pr(N[0,t) = n) = Pr(Wn < t, Wn+1 ≥t) =
 t
0
 ∞
t
λe−λ(y−x) λnxn−1e−λx
(n −1)!
dx dy
(2.235)
(a)
=
 t
0
 ∞
t
λn+1xn−1e−λy
(n −1)!
dx dy = λne−λt
n!
,
(2.236)
where (a) follows from the Gamma distribution on the n-th interarrival.

2.9 BUILDING PROBABILITY MODELS
47
2.9.3 Conditional Distributions are Maximum Entropy
Maximizing entropy has played an important role in the solution of problems in which the
measurements correspond to moment constraints on some many-to-one mapping h(·). We now
explore its role in estimation problems in which the measured data are statistical observations
deﬁned via many-to-one maps delimiting the domain of the density of inference, the so called
complete-incomplete data problems. We conclude the density maximizing entropy is identical to
the conditional density of the complete data given the incomplete data. The principle of max-
imum entropy is consistent with the rules of formal conditional probability for complete and
incomplete data problems. Such a view is informative as it gives rise to the iterative approaches
popularized by Dempster, Laird and Rubin [12] and Csiszar and Tusnady [13] already exam-
ined in the EM algorithm section. This equivalence results by viewing the measurements as
specifying the domain over which the density is deﬁned, rather than as a moment constraint
on h(·).
Throughout, a function θ is estimated which parametrizes a known probability density; the
actual data (denoted as the complete-data) described by the density are not observed. Rather, obser-
vations consist of data (denoted as incomplete-data ) which nonuniquely speciﬁes the complete-data
via some set of many-to-one mappings.
Theorem 2.66
Given the observable incomplete data y ∈Y speciﬁed via the many-to-one
mapping y = h(x) on the complete data x a particular realization in X, then the maximum-
entropy density satisfying domain constraint

χ(y) q(x)dx, χ(y) = {x : h(x) = y} is the
conditional density of x given y:
ˆq(·) =
arg max
{q:

X q(x)dx=1}
−

X
q(x) log q(x)
p(x)dx
subject to

χ(y)
q(x)dx = 1.
(2.237)
(a)
=
pθ(·)

χ(y) pθ(x)dx
(b)
= kθ(·|x ∈χ(y)).
(2.238)
Proof
Using the log x ≥1 −1/x inequality with equality x = 1 almost everywhere
gives (a) with Bayes rules Eqn. 2.147 giving (b).
Remark 2.9.4
[Conditional probability] The thesis work of Musicus [14] was the ﬁrst
to reveal the maximum entropy connection to one of the authors. The 1981 paper of
Van Campenhout and Cover [66] is one of the more complete developments on the
fundamental connection between conditional probability and the Jaynes principle.
Viewing the measurements y as determining the domain over which the density is
deﬁned, rather than as a moment constraint, then the density closest to the prior f in the
cross-entropy sense is the conditional density of x given y. By way of Van Campenhout
and Cover, choose one particularly simple (their results are more general) many-to-one
mapping for the incomplete data and apply their Theorems I and II.Assume x1, . . . , xN
are the complete-data which are independent, identically distributed discrete random
variables with mass function p(x) on the range x ∈1, 2, . . . , m. Given the incomplete
data y = 
N
i=1 xi, the conditional probability of x1, x2, . . . , xN given y is
p(x1 = j1, . . . , xN = jN|y) =
p(x1 = j1, . . . , xN = jN)

(j1,...,jN):j1+···+jN=y p(x1 = j1, . . . , xN = jN),
(2.239)

48
2 THE BAYES PARADIGM
and the independence of the x′
is it follows that
p(x1 = j1|y) = p(x1 = j1)



(j2,...,jN):j2+···+jN=y−j1
$N
i=2 p(xi = ji)

(j1,...,jN):j1+···+jN=y
$N
i=1 p(xi = ji)

.
(2.240)
For N →∞, by Theorems I and II of Van Campenhout and Cover, the above density
converges to the maxent one of 2.216, with h(x) replaced by x.

3
P R O B A B I L I S T I C D I R E C T E D A C Y C L I C G R A P H S A N D
T H E I R E N T R O P I E S
ABSTRACT
Probabilistic structures on the representations allow for expressing the variation of
natural patterns. In this chapter the structure imposed through probabilistic directed graphs
is studied. The essential probabilistic structure enforced through the directedness of the
graphs is sites are conditionally independent of their nondescendants given their parents.
The entropies and combinatorics of these processes are examined as well. Focus is given to
the classical Markov chain and the branching process examples to illustrate the fundamentals
of variability descriptions through probability and entropy.
3.1 Directed Acyclic Graphs (DAGs)
Pattern theory builds complex structures from simpler ones. The structure is imposed via graph
structures, directed and undirected, deﬁning how the variables interact. Begin with discrete state
spaces and for discrete graphs.11
Thebasicstructureswhichgluethepatternstogetherarethegraphs, directedandundirected.
Figure 3.1 depicts the contrast between directed (left panel) and undirected graphs (right panel).
The patterns will be constructed via the graphs deﬁned by their domain of sites D and edge system
E. The graph structure is denoted using the shorthand notation σ = {D, E}.
We begin our study with the less general directed graphs, in which the arcs associated with
the sites have a speciﬁc direction. For many of the examples, the ﬁnite state and tree graphs for
languages, there is a natural root node associated with the start state or sentence state from which
the direction is implicitly deﬁned. Many graphs will be partially ordered (POSET graphs) in which
the order is not total, but is transitive only. For this associate with the directed graph σ = {D, E} an
order deﬁned by the set of edges, so that the edges e ∈E are directed. For directed graphs, assume
at least a partial ordering, depicted via < or >, so that two sites which have an edge between
them are either > or < than one another. The ordering is denoted graphically via a directed arc,
so that if there exists an edge e ∈E in the graph i•
e−−
j• with direction of the arrow i•→
j• implying
the ordering i < j, and i•←
j• implying the ordering j < i.
(1)
(2)
Figure 3.1 Panel 1 shows a directed graph consisting of sites D and edges E; panel 2 shows an
undirected graph in which all edges have an orientation with no cycles.
11 Standard measurability is assumed with respect to the underlying probability space (, A, P), with 
being the set of sample points and A the sigma-algebra of events containing the P-measurable sets. The
random processes on uncountable index sets will be studied in the following chapters and will require more
careful examination of properties of measurablity, separability and continuity. Throughout the underlying
measure theoretic issues are suppressed.
49

50
3 PDAGS AND THEIR ENTROPIES
The left panel 1 of Figure 3.1 depicts a directed graph, with sites and edges with orientations
which deﬁne a partial ordering. The right panel shows an undirected graph in which the edges
have no direction.
Throughout these oriented sites, paths and cycles can be deﬁned.
Deﬁnition 3.1
A n-path pn(i, j) on a directed graph connecting sites i, j is a list of n + 1
sites i0, i, . . . , in, with i0 = i, in = j such that there exists oriented pairs
ij•→
ij+1
• of sites
(ij, ij+1), j = 0, . . . , n −1.
A cycle in the directed graph σ is an n-path pn(i, j) for some n with i = j.
Such paths induce the partial ordering >, then j > i if there exists a path connecting i to j.
This is a only a partial ordering, with the relation > being transitive only, so that if i > j and j > k
then this implies i > k. Parents and descendents can be deﬁned as well.
Deﬁnition 3.2
The parents of a site i ∈D, denoted i = {j ∈D :
j•→i•}, are the sites
connected via directed edges emanating from them. The descendents of a site are the set of
nodes in the graph for which there exists ﬁnite length paths from the site to the node which
respect the directed arcs:
i = {j ∈D : ∃pn(i, j) for some n ﬁnite}.
(3.1)
Probabilities will be put on acyclic graphs, implying the directed graphs will have no cycles.
Deﬁnition 3.3
In summary, the graph σ = {D, E}, is a directed graph with parent
system  = ∪i and descendent system  = ∪ii, if (1) i /∈i, (2) i ∈j =⇒∃e ∈
E with i• e→
j•, (3) i ∈j =⇒∃pn(j, i) for some n ﬁnite.
It is an acyclic DAG if ∄pn(i, j) for any n with i = j for any (i, j) ∈D.
There are many examples of directed graphs, and here are three.
1. Panel 1 of Figure 3.2 shows a linear graph, σ = LINEAR, in which all the sites are ordered,
with identically one parent for each internal site, and all sites are descendents and parents
exceptforthespecialrootandleafnodeswhichhavenoparentsandnochildren, respectively.
For this reason, σ = LINEAR has a total ordering. For all i, j ∈D, either i > j or i < j. We
shall call a graph r-linear if internal sites have i-parents, i = {i −r, i −r + 1, . . . , i −1}.
2. Panel 2 of Figure 3.2 shows a more general tree graph σ = TREE, in which all internal nodes
have identically one parent. Each node internal to the tree has multiple children, the number
called the branching degree. For this reason, σ = TREE has only a partial ordering which is
transitive.
3. Panel 3 of Figure 3.2 shows the most general direct acyclic graphs which we will be studying,
termed a POSET, partially ordered set. Note that the nodes may have multiple parents.
Linear
Tree
DAG
Figure 3.2 Left panel shows a directed graph LINEAR associated with a Markov chain; middle
panel a graph type TREE associated with a random branching process; right panel most general
DAG directed acyclic graph.

3.2 PROBABILITIES ON DIRECTED ACYCLIC GRAPHS
51
3.2 Probabilities on Directed Acyclic Graphs (PDAGs)
There is an exquisite connection of the picture associated with the graph showing arcs connecting
the nodes and particular conditional independence structure providing natural and simple factor-
izations. This launches us into the notion of directed acylic graphs and familiar probabilistic models
such as Markov chains, random branching processes and most generally probabilities on directed acyclic
graphs (DAGs). For probabilities on DAGs we follow the inﬂuential paper of Dawid [67] and book
by Pearl [68].
Begin assuming ﬁnite graphs σ −{D, E} with a number of sites n(σ) = n. Assign to each
point in the graph one of a ﬁnite set of random variables, {Xi, i ∈D} denoting the family of
random variables indexed over D, each Xi ∈X0 a ﬁnite state space. Deﬁne the set of all possible
conﬁgurations X = X n
0 . To assign probability distributions to the set of all possible conﬁgurations
X, denote by XG the conﬁguration X restricted to the subgraph G ⊂D:
XG = {Xi, i ∈G ⊂D}.
(3.2)
For directed acyclic graphs, the full distribution P on X is determined by the conditional
probability of a site i given its parents P(Xi|Xj, j ∈i). Essentially, sites are conditionally inde-
pendent of nondescendent nodes in the graph given their parents. This is the splitting property of
directed graphs.
Deﬁnition 3.4
X has realizations from a probabilistic directed acyclic graph
(PDAG) σ = {D, E} if for all nodes i ∈D, parents split sites with their nondescendents:
P(Xi, XD\(i∪i) | Xi) = P(Xi | Xi)P(XD\(i∪i) | Xi).
(3.3)
Notice, for all i ∈D conditional independence Eqn. 3.3 is equivalent to
P(Xi | Xj, j /∈(i ∪i)) −P(Xi | Xj, j ∈i).
(3.4)
This provides us with a product of conditionals which is so familiar!
Theorem 3.5
Given a directed acyclic graph σ = {D, E} with parent system  = ∪ii and
n(σ) = n sites, then
P(X1, X2, . . . , Xn) =
n

i=1
P(Xi|Xi).
(3.5)
Proof
The proof involves recursively peeling the leaf nodes off the graph σ = {D, E}
by deﬁning the sequence of graphs σ0 = σ, σ1, . . . , with sites D0 = D ⊃D1 ⊃. . . , the
sites Dj from the graph σj with all of the leaf nodes removed. For any ﬁnite graph with
n(σ) sites, there are at most j = 0, 1, . . . , max < n(σ) such peelings. See Figure 3.3.
Deﬁne the set of leaf nodes of each graph σj to be Lj ⊂Dj, with L0 ⊂D the
leaf nodes for D0 = D. Then, deﬁne the sites and extended parent system recursively
according to
Dj = Dj−1 \ Lj−1,
D0 = D,
(3.6)
˜Lj−1 = ∪i∈Lj−1i,
˜0 = ∪i∈L0i,
(3.7)
for j = 1, 2, . . . , m = max, with Dmax +1 = ∅=⇒˜Lmax = ∅.
This gives
P(XD) = P(XL0|XD\L0)P(XD\L0)
(3.8)
= P(XL0|XD0\L0)P(XL1|XD1\L1)P(XD1\L1) = · · ·
(3.9)
=
max

j=0
P(XLj|XDj\Lj).
(3.10)

52
3 PDAGS AND THEIR ENTROPIES
5
6
7
2
3
4
1
L0
s
L1
2
3
4
1
s1
s2
2
1
L2
s3
1
L3
Figure 3.3 Showing a sequence of graphs generated by sequentially peeling the leaves
(those sites with children).
Clearly ˜Lj ⊂Dj \ Lj, and since the elements of the leaf set Lj are nondescendents of
each other, they are conditionally independent given their parent set:
P(XD) =
max

j=0
P(XLj|X ˜Lj ) =
max

j=0

i∈Lj
P(Xi|Xi).
(3.11)
Example 3.6 (Bayesian Belief Networks)
A beautiful example is illustrated via
Bayesian belief networks in which the causes and effects of events of interest in expert
systems can be characterized probabilistically using Bayesian belief networks (see [69],
e.g.). Belief networks are an example of general pattern theoretic graphs. The sites
in the graph represent hypotheses. A directed arc from site X to site Y indicates
that Y is probabilistically dependent on X. These dependences are characterized by
conditional distributions. Sites with no in-bonds are characterized by their marginal
distributions. Independence is assumed wherever it is consistent with the speciﬁed
conditional distributions. For instance, if the graph indicates that X causes Y and
Y causes Z, but there is no direct connection between X and Z, then X and Z are
conditionally independent given Y. Note that the causal relationships may be pos-
itive, i.e. X true means Y is likely true, or negative, i.e. X true means Y is likely
false.
Figure3.4showsanexamplefromCowelletal.[70] involvingbinaryhypotheses.
Suppose we wish to infer whether a patient has tuberculosis, lung cancer, or bronchitis,
given the knowledge of possible causes of these conditions, such as smoking or a recent
visit to Asia, and possible symptoms, such as a positive chest X-ray and dyspnoea.
This would correspond to determining the probability distributions of sites T, L, and
B, conditioned on the knowledge of A, S, X, and D. E enters as a nuisance variable.
The joint probability of the conﬁguration is given by
P(ASTLBEXD) = P(X|E)P(D|ESB)P(E|TL)P(T|A)P(L|S)P(A)P(S)P(B)
(3.12)
Remark 3.2.0
Why restrict to acyclic graphs? Essentially, cycles imply trivial independ-
ence. Let X = (X1, X2, . . . , Xn), since each site is a descendant.
Example 3.7 (Computational Biology)
The advent of gene expression microchips
and similar devices has recently opened a number of new avenues for investigators
in medicine and biology. The elucidation of part of the intricacies of the intracellular
mechanisms that lead to a given cellular state is one such example.
Biological information is stored as genes in the form of deoxyribonucleic acids
(DNA). Genes from DNA can be read and copied (in a process termed transcription)

3.2 PROBABILITIES ON DIRECTED ACYCLIC GRAPHS
53
Visit to Asia? 
(A)
Tuberculosis?
(T)
Lung cancer? 
(L)
Smoking?
(S)
Bronchitis?
(B)
Either tub. 
or lung can?
(E)
Posit. X-ray? 
(X)
Dyspnoea?
(D)
Figure 3.4 Bayesian belief network for medical diagnosis. From Cowell et al. [70].
into molecules called messenger ribonucleic acids (mRNA). mRNAs then serve as the
blueprints to build proteins (in a process called translation), and these proteins are
responsible for almost all biological functions.
The state of a cell can thus be thought of as the set of concentrations of all mRNAs
and proteins as well as a variety of other molecules or ions described generally as
metabolites. Now, genes and their expressed products (mRNAs, proteins) may interact
with one another, either positively or negatively. That is, the expression of certain
genes may activate the expression of certain genes while repressing that of others. The
mapping of these interactions is referred to as the genetic regulatory network.
DNA microarray technologies enable one to acquire data related to the concen-
trations of the mRNAs for large numbers of genes in a given biological preparation.
The challenge is then to ﬁnd ways to interpret this information in the hope of identify-
ing the nature of the interactions, if any, between genes and possibly even to quantify
these. Among many other approaches that have been considered, directed acyclic
graphs have been used. The motivation in using Bayesian networks rather than other
approaches resides in their ability to integrate interactions patterns much more com-
plex than pairwise, their robustness to noise (a major concern with gene expression
data) and their ability to factor in latent or hidden variables.
In the case of galactose metabolism in S. cerevisiae (yeast), candidate DAGs
of interactions between ﬁve mRNAs and two proteins were screened for the one
that best accounts for the observed data. This work is by Hartemink et al. [71].
Two of the possible graphs of structures which are plausible candidates from a bio-
logical standpoint are shown in Figure 3.5. They differ by a single edge, that is
whether Gal80p directly interacts with Gal4p or does so only via Gal4m. The graphs
being directed and acyclic, the joint probability distribution can be factored into
the product of the conditional probability distributions of each variable, given its
parents.
P(XD) =

i∈D
P(Xi|Xi).
(3.13)

54
3 PDAGS AND THEIR ENTROPIES
Gal80m
Model 1
Model 2
Gal80p
Gal4p
Gal2 m
Gal4m
Gal5 m
Gal7 m
Gal80 m
Gal80 m
Gal2 m
Gal5 m
Gal7 m
Gal4 p
Gal80 p
Figure 3.5 Genetic regulatory network candidate for yeast; taken from Hartemink et al. [71].
Now, each candidate graph XD can be scored with the available data Y according to
the conditional log-posterior
log P(XD|Y) ∝log π(XD) + log P(Y|XD),
(3.14)
where log π(XD) and log P(Y|XD) are the log prior distribution and log likelihood
of realization Y given XD. The evaluation of these terms and the choice of prior may
vary, particularly with the size of the graph of interest. Hartemink et al. [71] have been
able to correctly infer the most generally agreed upon candidate genetic regulatory
networks (assessed through different biological means) using this method and yeast
gene expression data obtained using Affymetrix’s GeneChip technology.
3.3 Finite State Markov Chains
Finite state Markov chains are a classic example of probabilistic directed acyclic graphs which
are of greatest familiarity. The special structure associated with Markov chains is that parents have
at most one child, and children have only one parent in their parent system. Thus the Markov
chain sites inherit a complete ordering > on the site set D ⊂Z the integers, with ∀i, j ∈Z, i < j
or j < i. When the sites are totally ordered, the global probability is rewritten as products of local
probabilities in a familiar manner.
Begin by associating with D ⊂Z the natural complete ordering 1 < 2 < · · · < n . . . .
Examine the left panel of Figure 3.2. This corresponds to Markov processes having a special
structure implying that the past and future are conditionally independent given the present. This
is a property which shall be exploited with the total ordering associated with the integers.
Deﬁnition 3.8
A ﬁnite state X0-valued random process X1, X2, . . . is a Markov chain
with initial distribution π(·) if for i = 1, 2 . . .
P(Xi|Xi−1, . . . , X0) = P(Xi|Xi−1)
(3.15)
with P(X0) = π(·) for all X0, X1, . . . , Xn ∈X0.
The Markov chain is said to be time-invariant with 1-step state transition
probabilities Q =

Qjk

, with 
k Qjk = 1, if for each j, and all times n,
Pr{Xn = k|Xn−1 = j} = Qjk.
(3.16)

3.3 FINITE STATE MARKOV CHAINS
55
With the conditional independence of Eq. 3.15 in hand, the familiar approach of rewriting
joint probabilities as products of conditionals becomes available. The stationary Markov process is
characterized by the initial distribution π(·) on X0, and the transition distribution between states.
Now lets connect to the splitting X0, and the 1-step transition property.
Theorem 3.9
Then Xi, i = 0, 1, . . . is a ﬁnite-state Markov chain with the joint probability
given by
P(X0, . . . , Xn) =
n

i=1
P(Xi|Xi−1)π(X0),
(3.17)
if and only if it satisﬁes the splitting property with the past and future conditional
independence, i.e. for all ordered combinations of n-times j = 0, . . . , n and all j, then
P(X0, . . . , Xj−1, Xj+1, . . . , Xn|Xj) = P(X0, . . . , Xj−1|Xj)P(Xj+1, . . . , Xn|Xj).
(3.18)
Proof
Using the splitting property gives
P(X0, . . . , Xn−2, Xn|Xn−1) = P(X0, . . . , Xn−2|Xn−1)P(Xn|Xn−1)
(3.19)
P(X0, . . . , Xn)
P(Xn−1)
= P(X0, . . . , Xn−1)P(Xn|Xn−1)
P(Xn−1)
.
(3.20)
Continuing in this manner gives P(X0, . . . , Xn) = $n
i=1 P(Xi|Xi−1)π(X0).
The product condition, Eqn. 3.17 implies the splitting property Eqn. 3.18
according to
P(X0, . . . , Xj−1, Xj+1, . . . , Xn|Xj) = P(X0, . . . , Xn)
P(Xj)
=
$n
i=1 P(Xi|Xi−1)π(X0)
P(Xj)
=
$j
i=1 P(Xi|Xi−1)π(X0)
P(Xj)
n

i=j+1
P(Xi|Xi−1)
=
P(X0, . . . , Xj)
P(Xj)
P(Xj+1, . . . , Xn|Xj)
= P(X0, . . . , Xj−1|Xj)P(Xj+1, . . . , Xn|Xj).
The factoring property results in the Chapman–Kolmogorov equations. In the time-invariant
case, it is natural to introduce the matrix representing the multi-step transition probabilities. Deﬁne
(Qi)jk to be the j, k entry of the ith power.
Corollary 3.10
The Chapman–Kolmogorov equations take the form, for all 1 ≤i ≤n, then
P(Xn|X0) =

x∈X0
P(Xn|Xi = x)P(Xi = x|X0).
(3.21)
For the time invariant case, then
P(Xn|X0) = (Qn)X0,Xn =

x∈X0
(Qn−j)X0,x(Qj)x,Xn.
(3.22)
Proof
Conditional probability implies
P(Xn|X0) =

x∈X0
P(Xn, Xi = x|X0) =

x∈X0
P(Xn|Xi = x, X0)P(Xi|X0),
(3.23)
from which using the splitting property of Markov chains gives the result.

56
3 PDAGS AND THEIR ENTROPIES
For the time-invariant case use induction. For n = 1 it is clearly true, assume
n = k −1 is true, then for n = k it follows by writing the probability of the event
P(Xk|X0) =

x
P(Xk, Xk−1 = x|X0) =

x
P(Xk|Xk−1 = x, X0)P(Xk−1 = x|X0)
(3.24)
(a)
=

x
P(Xk|Xk−1 = x)P(Xk−1 = x|X0)
(3.25)
(b)
=

x
P(X1|X0)P(Xk−1 = x|X0) =

x
(Qk−1)X0xQxX1,
(3.26)
with (a) following from the Markov property, and (b) following from time-invariance.
Doing this j-times, gives the general results.
An m-memory Markov chain simply has the splitting property of an m-linear graph.
Deﬁnition 3.11
Let D ⊂Z be ﬁnite subsets of the integers. Then a discrete time, ﬁnite state
X0-valued random process {Xi, i = 0, 1, . . . } on the integers is an m-memory Markov chain
if for all ordered combinations of n-times and m-conditioning times then
P(X0, . . . , Xj−m+1, Xj+1, . . . , Xk|Xj, . . . Xj−m)
= P(X0, . . . , Xj−m+1|Xj, . . . Xj−m)P(Xj+1, . . . , Xk|Xj, . . . Xj−m),
(3.27)
with marginals adjusted at the boundary to the initial distribution P(X0, . . . , Xm−1) =
π(X0, . . . , Xm−1).
Equivalently the product condition follows as in Theorem 3.9:
P(X0, . . . , Xn) =
n

i=m
P(Xi|Xi−1, . . . , Xi−m)π(X0, . . . , Xm−1).
(3.28)
We can predict that the directed acyclic graphs with a complete ordering of the type shown
in the left panel of Figure 3.2 are Markov chains, i.e. the splitting property of deﬁnition 3.8 holds
since Theorem 3.5 gives the factored probability for the DAG.
Corollary 3.12
Let {Xi, i ∈D} be a ﬁnite X0-valued random directed graph σ = {D, E}.
Then with the complete order of the LINEAR graph of panel 1 of Fig. 3.2 with parent system
 = (n
i=1{i −1}, then
P(Xi|XD\(i∪i)) = P(Xi|Xi−1),
(3.29)
and is statistically equivalent to a Markov chain satisfying the splitting property with the same
transition probabilities.
3.4 Multi-type Branching Processes
We now construct the random branching process model historically termed the multi-type Galton
Watson processes (for an extensive development see [72,73]). Begin with the Markov chain view.
Imagine that particles can generate additional objects of the same kind, and denote a particle type A.
The initial set of particles is called the 0th generation, denoted by Z0, which have children which are
called the ﬁrst generation, Z1; their children, are the second generation Z2, and so on. The simplest

3.4 MULTI-TYPE BRANCHING PROCESSES
57
Z0 = 1
Z1 = 3
Z2 = 2
Figure 3.6 Panels showing two family generation trees from the 1-type process with particles A,
evolving under different choices of birthing rules. Note that both families have the identical states
Z0 = 1, Z1 = 3, Z2 = 2.
model ﬁrst proposed by Galton and Watson has indistinguishable particles with identical birthing
laws; only the size of the successive generations are tracked. The more general model will have
multiple particles. Shown in Figure 3.6 are two examples of family trees corresponding to particles
generating offspring according to various offspring mechanisms, with both family trees having
identical generations: Z0 = 1, Z1 = 3, Z2 = 2.
We make the following critical assumptions:
1. Ifthesizeofthenthgenerationisknown, thentheprobabilitylawgoverninglatergenerations
does not depend on the size of generations preceeding the nth. Thus, the splitting property
holds, and Z0, Z1, . . . forms a Markov chain on the positive integers.
2. Different particles within a generation do not interfere with one another. The number of
children born to a particle, does not depend on how many other particles are present in
that generation. Nodes are conditionally independent of their non-descendents given their
parents.
The 1-step transition function of the Markov chain P(Zn+1|Zn) which for the 1-type case is
determined by a single probability law p·, 
∞
k=0 pk = 1, where pk is the probability that a particle
existing in the nth generation has k children in the n + 1st generation. The set of transformation
laws to particle A takes the form
rk : A
pk
−→A A . . . A
)
*+
,
k times
,
k = 0, 1, . . . .
(3.30)
The 1-type processes consist of indistinguishable particles. To accomodate V-distinguishable
particles each with their own probabilistic behavior we introduce V−multi-type processes. Such
processes arise as genetic types in animal populations, or bacterial types, or throughout as syntactic
structures in computational linguistics: noun-phrases, verb-phrases, determiners etc.. Enumerate the
V particle types A1, A2, . . . with their speciﬁc set of transformation (birthing rules) probability laws
for generating offspring pv, v = 1, 2, . . . the probability laws of transforming particles of type-v.
The rules transforming each particle identify the speciﬁed number of the various children types.
Deﬁning n(j)
vk to be the number of type Aj created by substitution for particle v via rule k then
rvk : Av
pvk
−→
A1 A2 A3 . . .
)
*+
,
n(1)
vk of type A1,n(2)
vk of type A2,...
,

k
pvk = 1,
v = 1, . . . , V.
(3.31)

58
3 PDAGS AND THEIR ENTROPIES
3
4
+
2
*
Z0=[1,0]
Z1=[1,1]
Z2=[1,1]
x1
x1
x2
x1
x2
x1
x1
x2
x1
x1
x2
x1
Z0=[1,0]
Z1=[2,1]
Z2=[2,1]
Z3=[0,0]
Figure 3.7 Left panel showing an example family tree from the 2-type reverse polish, with particles
that birth according to the two rules A1 →A1 A1 A2, A1 →N, A2 →{+, ∗}. The derivation
is ((3 4 +) 2 ∗) with state sequence Z0 = [1, 0], Z1 = [2, 1], Z2 = [2, 1]. Right panel showing
a 3-generation realization from a 2-type process, p1,(1,1) = 1, p2,(0,0) = 1, demonstrating the
recurrence of the state [1, 1] starting from Z0 = [1, 0].
The left panel of Figure 3.7 shows an example family tree from the 2-type reverse polish
branching process, with particles birthing according to the rules A1 →A1 A1 A2, A1 →N,
A2 →{+, ∗}. Notice, A1 represents natural numbers, A2 procedures. The derivation is ((3 4 +) 2 ∗);
the state sequence Z0 = [1, 0], Z1 = [2, 1], Z3 = [2, 1].12
Associate with each generation the state vector Zn = [Z(1)
n , . . . , Z(V)
n
] the number of part-
icles of each type in the nth generation. Since particles reproduce independently, the conditional
distribution of Zn+1 given Zn is distributed as the sum of independent random variables. The
probability law becomes the convolution of probability laws. The state 0 is an absorbing state, so
that if Zn = 0, then Zn+1 = 0 with probability 1. Deﬁne the single particle state vectors
1(v) = [0, . . . , 0, . . . ,
1
)*+,
vth entry
, . . . , 0].
Let pvk, v = 1, . . . , V be the probabilities that the particles type-v have k = (k1, k2, . . . , kV) children
type j1, j2, . . . , jV; 
k∈I+V pvk = 1. For branching processes, since there can be more than one
particle born to any generation, the 1-step probability of transition from one state to another is
given by the convolution of the basic single particle transitions.
Deﬁnition 3.13
AGalton–Watson1-typebranchingprocessisaMarkovchainZn, n =
0, 1, . . . with inﬁnite state space X0 = I+ the nonnegative integers, with particle substitution
law p., 
k pk = 1, if for all times n,
Pr{Zn+1 = k|Zn = j} = (p∗j)k
if j ≥1,
k ∈I+
δ(k)
if j = 0,
k ∈I+ ,
(3.32)
with p∗j
· the j-fold convolution of the probability p·, 
k pk = 1.
A V-multi-type (vector) branching process Zn = [Z(1)
n , . . . , Z(V)
n
], n = 0, 1, . . .
is a Markov chain with inﬁnite state space X0 = I+V, and particle substitution laws pv,
12 Throughout this section the state vectors are taken in a row form.

3.4 MULTI-TYPE BRANCHING PROCESSES
59

k pvk = 1, v = 1, . . . , V, if for all times n,
Pr{Zn+1 = k|Zn = j} =

(p1)∗j1 ∗(p2)∗j2 ∗· · · ∗(pV)∗jV

k
if j ̸= 0, k ∈I+V
= δ(k)
if j = 0, k ∈I+V,
(3.33)
with p· ∗q· the convolution of distributions p·, q·, and p∗i
· is the i-fold convolution.13
Note that the deﬁnition of the transition law assumes the Markov structure of conditional
independence of non-descendents given their parents, so that if Zn = (j1, . . . , jV) then Zn+1 is the
sum of j1 + j2 + · · · + jV independent random vectors.
There are two tools for studying the branching processes, the branching rates given by the
branching matrix and the moment generating function.
3.4.1 The Branching Matrix
The combinatorial expansion of the number of trees is given by the moments of generations of the
process as determined by the mean branching matrix.
Deﬁnition 3.14
The mean branching matrix is the positive V × V matrix M = (Mjk),
deﬁned to be the matrix of ﬁrst moments:
Mjl = E

Z(l)
1 |Z0 = 1(j)	
=

k
pjkn(l)
jk .
(3.36)
Deﬁne ρ to be the largest eigenvalue of M. For the 1-type, the mean is the scalar M = ρ =
E

Z1|Z0 = 1
	
.
If the process has exactly one offspring for each of its substitutions, then M is a stochastic
matrix 
l Mvl = 1, v = 1, . . . , V corresponding to a Markov chain. Problem A.2 calculates the
mean and variance of Zn given Z0 = 1 for the single type case. More generally, the branching
matrix directly determines the expected generations.
Lemma 3.15
Let Z0, Z1, . . . correspond to a branching process of the V-type. Then,
E

Zn+m|Zm
	
= ZmMn,
n, m = 0, 1, 2, . . . .
(3.37)
Proof
Using conditioning and the Markov property gives
E

Zn+m|Zm
	
= E

E

Zn+m|Zn+m−1, Zm
	
|Zm
	
= E

E

Zn+m|Zn+m−1
	
|Zm
	
.
(3.38)
To compute the V × 1 vector E

Zn+m|Zn+m−1
	
use the fact that for any n,
E

Z(v)
n |Zn−1
	
=
V

j=1
Z(j)
n−1

k
n(v)
jk pjk
)
*+
,
Mjv
,
(3.39)
implying E

Zn+m|Zm
	
= E

Zn+m−1M|Zm
	
. Doing this n-times completes the proof.
13 Let p·, q· have domains I+V, then
(p· ∗q·)k =

j∈I+V
pjqk−j,
(3.34)
(p∗j
· )k = (p· ∗p∗j−1
·
)k,
(p∗0
· )k = δ(k).
(3.35)

60
3 PDAGS AND THEIR ENTROPIES
The branching rates and entropies are determined by the critical eigenvalue of M being
greater than 1.
Deﬁnition 3.16
The random branching processes are called supercritical, critical, or
subcritical according to the largest eigenvalue of M, ρ >, =, or < 1, respectively.
3.4.2 The Moment-Generating Function
Since the state vector involves convolutions of conditionally independent variates, it is natural
that the moment or probability generating function plays an important role in studying branching
processes.
Deﬁnition 3.17
The 1-single-type probability generating function f(s), s complex, is
given by
f(s) =
∞

k=0
pksk,
|s| ≤1,
(3.40)
with fn(s) the generating function of Zn given Z0 = 1:
fn(s) =
∞

k=0
Pr{Zn = k|Z0 = 1}sk.
(3.41)
For the V-multi-type, deﬁne the vth probability generating function associated
with each particle type-v = 1, . . . , V as
f(v)(s1, s2, . . . , sV) =

(k1···kV)∈I+V
pv,(k1,...,kV)sk1sk2 · · · skV.
(3.42)
The generating function of Zn given Z0 = 1(v) is deﬁned as
f(v)
n (s1, . . . , sV) =

(k1···kV)∈I+V
Pr{Zn = (k1, . . . , kV) | Z0 = 1(v)}sk1
1 · · · skV
V ,
(3.43)
fn(s1, . . . , sV) = (f(1)
n (s), f(2)
n (s), . . . , f(V)
n
(s)).
(3.44)
The shorthand notation sk = sk1
1 sk2
2 · · · skV
V shall be employed, so that f(v)(s) = 
k pvksk.
Generating functions provide a fundamental link to extinction as Pr{Zn = 0|Z0 = 1(v)} =
f(v)
(n)(0).Aswellthemomentsoftheprocessaregivenbythegeneratingfunctions. Deﬁningj =
√
−1,
the moments of the nth generation of the process are given by
E

Z(k1)
n
Z(k2)
n
· · · Z(kV)
n
| Z0 = 1(v)	
= 1
jr
∂r
∂ωk1 · · · ∂ωkV
f(v)
(n)

s =
V

k=1
e (jωk)


ω=0
.
(3.45)
Such events as the probability that a random sequence goes to zero, i.e. the particle family
goes extinct, and the behavior if the family does not become extinct are characterized by the
moment or probability generating function. Watson discovered the basic result that for the single
type the nth generating function is the nth iterate.

3.4 MULTI-TYPE BRANCHING PROCESSES
61
Theorem 3.18
Deﬁning the nth iterate recursively according to
f(1)(s) = f(s),
f(n)(s) = f(f(n−1)(s)),
(3.46)
then for the 1-type (Watson-1874) the generating function of Zn given Z0 = 1 becomes
fn+1(s) = f(f(n)(s)) = f(n+1)(s),
n = 1, 2, . . . .
(3.47)
For the V-multi-type, with f(v)
(1) (s) = f(v)(s), then
f(v)
n+1(s1, . . . , sV) = f(v) 
f(1)
(n)(s1, . . . , sV), . . . , f(V)
(n) (s1, . . . , sV)

.
(3.48)
Proof
The 1-step transition law Pr{Zn = k|Zn−1 = j} is the j-fold convolution of the
1-particle transition probability, implying from the product rule for the transform (see
Homework Problem F.2)

k
Pr{Zn = k|Zn−1 = j}sk = (f(s)) j.
(3.49)
Then

k
Pr{Zn+1 = k|Z0 = 1}sk (a)
=

k

m
Pr{Zn+1 = k|Z1 = m}Pr{Z1 = m|Z0 = 1}sk
(b)
=

m
Pr{Z1 = m|Z0 = 1}(fn(s))m = f(fn(s))
(3.50)
with (a) coming from Chapman–Kolmogorov and (b) from the m-fold convolution.
Applying this recursively completes the ﬁrst part of the proof.
For the V-multi-type, similar arguments follow:
f(v)
n+1(s1, . . . , sV)
=

k∈I+V
Pr{Zn+1 = k|Z0 = 1(v)}sk1
1 sk2
2 · · · skV
V
=

k∈I+V

m∈I+V
Pr{Zn+1 = k|Z1 = m}Pr{Z1 = m|Z0 = 1(v)}sk
=

m∈I+V


k∈I+V
Pr{Zn+1 = k|Z1 = m}sk

Pr{Z1 = m|Z0 = 1(v)}
=

m∈I+V

f(1)
n
m1 
f(2)
n
m2 · · ·

f(V)
(n)
mV Pr{Z1 = m|Z0 = 1(v)} = f(v)(f(n)(s)).
(3.51)
Example 3.19
Let V = 3, {A1, A2, A3} with
A1
4/7
−→A1A1
A1
3/7
−→A2
A2
1/35
−→φ
A2
16/35
−→A1A3
A2
18/35
−→A2A2
A3
3/7
−→A2
A3
4/7
−→A3A3
,
(3.52)

62
3 PDAGS AND THEIR ENTROPIES
giving the probability generating function and branching matrix
f(s1, s2, s3) =

f(1)(s) = 4
7 s2
1 + 3
7 s2, f(2)(s) = 1
35 + 16
35s1s3 + 18
35s2
2, f(3)(s) = 3
7 s2 + 4
7 s2
3

,
M =


8
7
3
7
0
16
35
36
35
16
35
0
3
7
8
7

.
3.5 Extinction for Finite-State Markov Chains and Branching Processes
We shall be interested in understanding the circumstances under which strings are ﬁnite, and
when they are inﬁnitely extended. For this we will have to understand the extinction of the ﬁnite
length sequences. This has a particularly straightforward characterization for ﬁnite-state Markov
chains; the typical sequences simply divide as probability 0 or 1. It will be more challenging
for branching processes. Essentially, stationary Markov chains do not go extinct (or they would
not be stationary); extinction for Markov chains will correspond to the transition matrix Q being
sub-stochastic which will be characterized via Perron–Frobenius.
Deﬁnition 3.20
Let M = (Mij) ≥0 be a nonnegative matrix, then it is said to be positively
regular if there exists a ﬁnite integer n such that Mn > 0 is a strictly positive matrix.
Clearly if M = Q is the 1-step transition matrix of a Markov chain, then if Mn is strictly
positive then there exists an n such that there is a nonzero probability of getting from any state
to any other (the relationship to connectedness). Now we will use the well-known results from
Perron (1909) and Frobenius (1912)
Theorem 3.21 (Perron–Frobenius)
Let M be positively regular. Then M has a simple,
largest eigenvalue ρ, with right and left positive eigenvectors µ , ν, such that
Mn = ρnµν + Mn
2,
ν µ = 1,
(3.53)
where |Mn
2| = O(|α|n)14 with 0 ≤|α| < ρ, and µ is a column vector and ν a row vector.
See Gantmacher [74, volume 2] or Cinlar [75].
3.5.1 Extinction in Markov Chains
Let X1, X2, . . . be a time-invariant ﬁnite-state Markov chain with transitions Q = (Qjk). Deﬁne
extinction by introducing the absorbing or dead state D = 0. Stationarity of course corresponds
to extinction probability 0, and the process never entering the terminating-state. For this, deﬁne
Zn = 0 (Xn = 0 the dead state); extinction is Zn = 0 for some n.
Deﬁnition 3.22
The probability of extinction γ = [γ (1), γ (2), . . . ] is the probability
that Zn = 0 (Xn = 0 in dead-state) for any initial non-terminated state:
γ (X0) = Pr{Zn = 0 for some n |X0}.
(3.54)
The extinction probability for ﬁnite-state Markov chains is either 0 or 1 depending upon
whether any of the states are connected to the 0 dead-state; in this case the largest eigenvalue of
14 A function f(n) is O(αn) if limn→∞f(n)/αn < ∞.

3.5 EXTINCTION FOR FINITE-STATE MARKOV CHAINS AND BRANCHING
63
the time-invariant transition out of states is ρ < 1. In the stationary case, ρ = 1, with transition
matrix positively regular.
Theorem 3.23
Let X1, X2, . . . be a ﬁnite state time-invariant Markov chain with dead-state
0 and transition matrix ˜Q of the form
˜Q =
 1
0
δ
Q

,
0 = [0 0 0 · · · 0 0
)
*+
,
V times
],
δ =


δ1
...
δV

,
(3.55)
where Q =

Qjk

is positively regular (states connected) with eigenelements νQ = ρν,
Qµ = ρµ.
1. If δ ̸= 0 then Q is sub-stochastic with ρ < 1 and extinction probability γ = 1.
2. If δ = 0 then Q is stochastic with ρ = 1 with extinction probability γ = 0.
Proof
For δ ̸= 0, then
lim
n→∞
˜Qn = lim
n→∞
 1
0
γ
ρnµν

,
(3.56)
which if γ ̸= 0 implies ρ < 1. Thus from the limit, γ = 1.
For δ = 0, then
lim
n→∞
˜Qn = lim
n→∞
 1
0
0
Qn

,
(3.57)
implying ρ = 1, Q is stochastic, γ = 0.
3.5.2 Extinction in Branching Processes
Extinction in branching processes is far more subtle. Certain realizations branch quickly enough
that the probability of termination is non-zero. The extinctions probability is determined by the
critical maximal eigenvalue ρ of M and the moment generating function. Deﬁne the absorbing
or terminating state [0, 0, . . . , 0], with the extinction probability determined by the generating
function and its iterates.
Theorem 3.24
1. 1-single-type: The extinction probability γ is the limit γ = limn→∞fn(0). If ρ ≤
1 =⇒γ = 1. If ρ > 1 =⇒0 ≤γ ≤1 and γ is the unique non-negative solution of
the equation γ = f(γ ).
2. V-multi-type: If the process is positively regular and not a ﬁnite-state Markov chain,
then ρ ≤1 =⇒γ = 1. If ρ > 1 =⇒0 ≤γ ≤1, and γ satisﬁes the equation γ = f(γ ).
Proof
We only prove the 1-type case, see Harris [p. 72, 41] for the V-type vector case.
γ = Pr{Zn = 0 for some n|Z0 = 1} = Pr{∪∞
n=0{Zn = 0}|Z0 = 1}
(a)
= lim
n→∞Pr{∪n
i=1{Zi = 0}|Z0 = 1}
(b)
= lim
n→∞Pr{Zn = 0|Z0 = 1} = lim
n→∞fn(0)
= lim
n→∞f(n)(0).

64
3 PDAGS AND THEIR ENTROPIES
Equalities (a,b) follow from the semi-continuity of probability and the fact that
{Z1 = 0} ⊂{Z2 = 0} ⊂· · · . To see that γ = f(γ ), since f(n+1)(0) = f(f(n)(0)) and
γ = limn→∞f(n)(0) = limn→∞f(n+1)(0),
lim
n→∞f(f(n)(0)) = lim
n→∞f(n)(0) = γ
(3.58)
giving f(γ ) = γ . That there is a unique solution 0 ≤γ ≤1 for ρ > 1 and that γ = 1
for ρ ≤1 is proved in Harris [72].
Example 3.25 (Positive Regularity is Important!)
Positive regularity is important
for the various extinction and convergence theorems. Examine the V = 2 process
A1, A2, with birthing probabilities
A1
1
−→A1A2,
A2
1
−→φ,
(3.59)
with moment generating function and mean matrix
f(s1, s2) = (f(1)(s1, s2) = s1s2, f(2)(s1, s2) = 1),
M =
 1
1
0
0

.
(3.60)
This is not positively regular, M = Mn for any n, and the state Z = [1, 1] is
recurrent, since
Pr{Zn = [1, 1] inﬁnitely often |Z0 = [1, 0]} = 1.
(3.61)
One such three generation realization is shown in the right panel of Figure 3.7 demon-
strating the recurrent state. Notice ρ = 1, yet γ ̸= 1 contrary to Theorem 3.24 since it
is not positively regular.
Example 3.26 (Reverse Polish)
Here is the basis for the Reverse Polish language,
with two rewriting rules
A
p→a A
1−p
→AAb,
a ∈{Z, R}, b ∈{+, −, ×}
(3.62)
with moment generating function and mean
f(s) = p + (1 −p)s2,
ρ = m = ∂f(s)
∂s
s=1 = 2(1 −p).
(3.63)
The extinction probability is the root 0 ≤s ≤1 solving f(s) = s giving
s = p + (1 −p)s2,
(3.64)
then γ is the unique positive value
γ = 1
2
1
1 −p ±
-
1
(1 −p)2 −
4p
1 −p =
p
1 −p
for p < 1
2
(3.65)
= 1
for p ≥1
2.
(3.66)
3.6 Entropies of Directed Acyclic Graphs
As Shannon and Weaver [6,76] established, entropy characterizes the combinatoric number of pat-
terns generated by random sources generating from a random family of strings. Directed graphs

3.6 ENTROPIES OF DIRECTED ACYCLIC GRAPHS
65
have particularly simple forms for their entropies; with this comes the classical theorems on num-
bers of strings. Begin with standard deﬁnitions of entropies and conditional entropies of families
of random variables.
Deﬁnition 3.27
Let X be a X0-valued process on a probabilistic directed acyclic graph σ =
{D, E} with n(σ) = n sites with joint distribution P on X n
0 . The joint entropy of X is deﬁned
to be
H(X1, X2, . . . , Xn) = −EP(X1,...,Xn) log P(X1, . . . , Xn),
(3.67)
with the joint entropy rate normalized by the number of random variables.
For two jointly distributed random variables X, Y, the entropy of the conditional is
deﬁned by
H(X|y) = −EP(X|Y=y) log P(X|Y = y).
(3.68)
Then the conditional entropy of X given Y is deﬁned to be
H(X|Y) = −EP(X,Y) log P(X|Y),
(3.69)
=

y
P(y)H(X|y).
(3.70)
Then the realizations from the directed graphs inherit the factoring property from their
parent systems.
Theorem 3.28
Given the directed acyclic graph σ = {D, E} with parent system  = ∪ii
and n(σ) = n sites, then the entropy is given by
H(X) =
n

i=1
H(Xi|Xi).
(3.71)
Proof
Write the joint probability via its factorization,
P(X1, . . . , Xn) =
n

i=1
P(Xi|Xi) ,
(3.72)
use the additive property of logarithms, and take the negative expectation.
3.7 Combinatorics of Independent, Identically Distributed Strings via the
Asymptotic Equipartition Theorem
We will use the entropy measures in various ways, one of the most prominent being to use it
to count the typical strings which are likely to occur. For independent and identically distributed
(i.i.d.) samples X1, X2, . . . , the law of large numbers states that under reasonable conditions empir-
ical averages of functions of the random variables 1/n 
n
i=1 f(Xi) converge to their expectation
EP(X)f(X). The asymptotic equipartition theorem (AEP) exploits this for the special −log P(·)
function, implying −1/n 
n
i=1 log P(Xi) converge to the entropy constant. This being the case
implies that the number of such sequences whose empirical log-probability is close to the entropy
must be exponential in the entropy thus providing a method for counting these sequences. Let us
deﬁne typical sets.

66
3 PDAGS AND THEIR ENTROPIES
Deﬁnition 3.29
Let X1, X2, . . . , be i.i.d. with P(X) on X0. The typical set T (n)
ǫ
⊂X n
0
with respect to P(X) with entropy H(X) is the set of sequences (X1, . . . , Xn) ∈X n
0 with the
property
2−n(H(X)+ǫ) ≤P(X1, . . . , Xn) ≤2−n(H(X)−ǫ).
(3.73)
The AEP and its implications are stated as follows.
Theorem 3.30
1. For all ǫ > 0, Pr{T (n)
ǫ
} > 1 −ǫ for n sufﬁciently large.
2. For all n, 2n(H(X)+ǫ) ≥|T (n)
ǫ
| and for n sufﬁciently large, |T (n)
ǫ
| > (1−ǫ)2n(H(X)−ǫ).
Proof
From the law of large numbers,
lim
n→∞Pr

| −1
n log P(X1, . . . , Xn) −H(X)| < ǫ

= 1,
(3.74)
implying part 1. Part 2 follows from
1 ≥

T (n)
ǫ
P(X1, . . . , Xn) ≥

T (n)
ǫ
2−n(H(X)+ǫ)
(3.75)
= 2−n(H(X)+ǫ)|T (n)
ǫ
|.
(3.76)
The second part of the inequality follows from limn→∞Pr{|−(1/n) log P(X1, . . . , Xn)−
H(X)| < ǫ} = 1 giving for n sufﬁciently large,
1 −δ < Pr{T (n)
ǫ
}
(3.77)
≤

T (n)
ǫ
2−n(H(X)−ǫ) = 2−n(H(X)−ǫ)|T (n)
ǫ
|.
(3.78)
Choose δ = ǫ.
3.8 Entropy and Combinatorics of Markov Chains
Let us now examine the combinatorics of large graphs using entropy and the AEP beginning with
Markov chains. First begin with the entropy of n-length strings. The entropy of the process will
be a mixture of the visitation probability with the entropy of the conditional transition probability
from each state in the transition matrix Q. For this, reintroduce the state occupancy vector
Zn = 1(Xn) = [0, . . . , 0, . . . ,
1
)*+,
Xn entry
, . . . , 0].
(3.79)
It will be convenient to exploit the matrix property that (ZnQ)j = P(Xn+1 = j|Xn).
Lemma 3.31
Let X0, X1, . . . be a time-invariant ﬁnite-state Markov chain with matrix of
transition probabilities Q =

Qjk

, 
k Qjk = 1 and conditional transition entropy
hj = −

k
Qjk log Qjk,
j ∈X0.
(3.80)

3.8 ENTROPY AND COMBINATORICS OF MARKOV CHAINS
67
The entropy conditioned on the initial state Z0 (X0) is given by
HZ0(X1, . . . , Xn) = E(−log P(X1, . . . , Xn|X0)|X0)
(3.81)
=
n−1

i=0
Z0Qih .
(3.82)
Proof
Computing the entropies using the conditional expectation
E(−log P(X1, . . . , Xn|X0)|X0) =
n

i=1
E(E(−log P(Xi|Xi−1, X0))|X0)
(3.83)
=
n

i=1
E(E(−log P(Xi|Xi−1))|X0)
(3.84)
=
n

i=1
E(Zi−1|Z0)h) =
n

i=1
Z0Qi−1h.
(3.85)
Examining Theorem 3.30 it is clear that the typical sets for which the entropy rate describes
the exponential growth rate log |T (n)
ǫ
| ∼nH(X) are realizations which are inﬁnitely long, so
that subsequences for arbitrarily large n can always be found for the limit. The complement of
the typical set will certainly contain those strings which terminate and of ﬁnite length. Thus
understanding the extinction of the ﬁnite length sequences is essential. As we have seen this
has a particularly straightforward characterization for Markov chains; the typical sequences sim-
ply divide as probability 0 or 1. Stationary Markov chains do not go extinct (or they would
not be stationary); extinction for Markov chains corresponds to the transition matrix Q being
sub-stochastic.
Let X1, X2, . . . be a time-invariant ﬁnite-state Markov chain with transitions Q =

Qjk

.
Stationarity corresponds to the extinction probability 0, and the process never entering the
terminating-state, i.e. extinction is Zn = 0 for some n.
For positively regular, stationary ﬁnite-state Markov chains, the entropy rate is given by the
familiar mixture of conditional entropies in each state mixed against the stationary distribution.
Theorem 3.32
Let X1, X2, . . . be a ﬁnite state time-invariant Markov chain with Q =

Qjk

positively regular and eigenelements νQ = ρν, Qµ = ρµ. With 1 deﬁned as the all 1’s
vector, then the normalized entropy rate becomes
HZ0(X) = lim
n→∞
−E(log P(X1, . . . , Xn|Z0)|Z0)
E(
n
i=1 Zi1|Z0)
.
(3.86)
Then
1. if γ = 1 and Q is sub-stochastic with ρ < 1, then the normalized entropy rate is a
function of the initial state:
HX0(X) = Z0(I −Q)−1h
Z0(I −Q)−11 ;
(3.87)
2. if γ = 0 and Q is stochastic, ρ = 1, 1/n 
n
i=1 Zi →ν, ˜Qn →
 1
0
0
µν

, and the
entropy rate is independent of initial state:
H(X) = νh.
(3.88)

68
3 PDAGS AND THEIR ENTROPIES
Proof
Proof for q ̸= 0 (ρ < 1): The geometric series in Q of Eqn. 3.82 gives the ﬁrst
entropy rate since the numerator and the denominator each have terms
n−1

i=0
Qi = (I −Q)−1(I −Qn) →(I −Q)−1
as n →∞.
(3.89)
Proof for γ = 0(ρ = 1): Since Qn →µν as n →∞with µ the all 1’s vector, this
implies
(Qn)X0,j →νj
as n →∞,
(3.90)
which gives the second entropy rate.
Proving the normalized empirical negative log-probability converges to the entropy rate
requires the convergence of the occupancy rate of states to the stationary distribution.
Theorem 3.33
Let X1, X2, . . . , be a ﬁnite-state positively regular Markov chain with trans-
ition Q =

Qjk

and unique stationary distribution ν = νQ, entropy rate H(X) = 
j νjhj.
The typical set T (n)
ǫ
⊂X n
0 deﬁned as
2−n(H(X)+ǫ) ≤P(X1, . . . , Xn|X0) ≤2−n(H(X)−ǫ),
(3.91)
satisﬁes the AEP Theorem 3.30.
Proof
Now to prove that Pr{T (n)
ǫ
> 1 −ǫ} use the fact that the process is ergodic
(unique stationary probability, [p. [77], 136]) so that the state occupancy deﬁned as
Nj(n) = 
n
i=1 Z(j)
i−1 when normalized converges (Nj(n)/n) →νj with probability one.
As well, deﬁning the subsequence Xi1, Xi2, . . . , XiNj(n) of states immediately following
state j in the sequence X1, X2, . . . , Xn, then
Nj(n)

l=1
f(Xil) →

k
Qjkf(k) w.p. 1,
(3.92)
implying
lim
n→∞−1
n log P(X1, . . . , Xn|X0) = lim
n→∞−1
n
n

i=1
log P(Xi|Xi−1)
(3.93)
= −

j
Nj(n)
n

Nj(n)
l=1
log QjXil
Nj(n)
(3.94)
=

j
νjhj.
(3.95)
3.9 Entropies of Branching Processes
While entropies of the Markov chains have been calculated since the time of Shannon, the entropies
of the branching processes have been more recently developed [78–80]. Thus far in studying

3.9 ENTROPIES OF BRANCHING PROCESSES
69
the branching processes, the role of Zn, n = 0, 1, . . . have been emphasized as a Markov chain.
These vector processes do not completely characterize the complete properties of the random
realizations, for example their entropies. For this, the random processes as manifest by their
description as tree graphs are required. Throughout particles Ai are assumed to have a ﬁnite
set of particle generation probabilities, {pij, j = 1, . . . J}, J ﬁnite. Figure 3.6 illustrates the basic
issue. Note that the two realizations are different trees although the state vectors are identical,
Z0 = 1, Z1 = 3, Z2 = 2, irrespective of the fact that the trees were generated with different
rule choices. To characterize realizations of the entire family tree and its underlying probability
space we follow Harris by deﬁning each tree by a sequence of ﬁnite sequences which specify the
generation numbers including the offspring in each level of the tree.
3.9.1 Tree Structure of Multi-Type Branching Processes
Identify with the random n-generation base Tn = X1, . . . , Xn of a perhaps inﬁnite random
tree T(ω) consisting of a sequence of sequences of rules specifying each of the generations
X1 = (X11, X12, . . . ), X2 = (X21, X22, . . . ), with Xil deﬁned as a substitution rule-valued ran-
dom variable generating offspring for the lth node in the ith generation of T(ω). The random
n-generation base is identiﬁed with the sequence of random variables
Tn =
X1
)*+,
X1 consisting of 1 rewrite rule of Z0
,
X2,1, X2,2, . . . ,
)
*+
,
X2 consisting of Z(1)
1 ,Z(2)
1 ,..., rewrite rules
, . . . ,
(3.96)
Xn,1, Xn,2, . . . ,
)
*+
,
Xn consisting of Z(1)
n−1,Z(2)
n−1,..., rewrite rules
.
Random realizations are generated by successively applying the generation rules to the
variables in the tree which can have children, until a tree is generated consisting solely of particles
at the leaves which are infertile. The conditional independence structure of the random branching
processes is imposed via the fact that offspring rules are chosen conditionally independent of their
siblings given their parents. Consistency in the trees is maintained by choosing the probability law
which is appropriate for the particular node of the tree. This directed graph structure is depicted
in the middle panel of Figure 3.2.
The probability of an n generation base Tn given the start node Z0 is given by
P(Tn|Z0) = P(X1, . . . , Xn|Z0)
(3.97)
=
n

i=1
V

v=1
Jv

k=1
(pvk)


j Z(j)
i
l=1
δrvk (Xil(ω)).
(3.98)
These n-generation bases form the natural cylinders from which the probability law on the
inﬁnite realizations are constructed. This extends naturally to a distribution on inﬁnitely extended
trees as shown in [2]. Let T be the space of all realizations of n-generation trees, for all n, including
inﬁnitely extended trees. Then the cylinder set C(Tn) ⊂T is the set of all inﬁnitely extended
trees with base Tn. The probability distribution is extended to the sigma-algebra generated by all
cylinder sets by deﬁning the probability of cylinders C(Tn) ⊂T according to Eqn. 3.98 applied to
the ﬁrst n generation base
P(Tn) = Pr{T ∈C(Tn)}.
(3.99)

70
3 PDAGS AND THEIR ENTROPIES
3.9.2 Entropies of Sub-Critical, Critical, and Super-Critical Processes
Examine the entropies of the V-type branching processes. Let Tn be the base of a perhaps inﬁn-
ite random tree generated by successively applying the generation rules to the variables in the
tree which can have children. Then Tn = X1, . . . , Xn is a sequence of sequences X1 = (X1l, l =
1, . . . ), X2 = (X2l, l = 1, . . . , ), . . . with Xil the rule used to generate offspring at node l of generation
i. The probability of an n generation base conditioned on Z0 is given by Eqn. 3.98.
Calculation of the entropies is essentially determined by the rate at which the tree branches.
Roughly speaking, each path to a leaf node is a Markov chain; the entropy of the tree is the
number of paths multiplied by the Markov chain-like entropy of each path. This of course links
the branching rates and eigenvalues ρ of M the mean branching matrix and the entropies as deﬁned
originally in [78,79].
Theorem 3.34
Letting the entropy of the set of rewrite rules of the v-type, v = 1, . . . , V, be
deﬁned as
hv = −
Jv

k=1
pvk log pvk,
h =


h1
...
hV

,
(3.100)
thentheentropyofngenerationtreesconditionedontheinitialstateZ0 oftherandombranching
process with mean matrix M is given by
HZ0(Tn) = −E(log P(Tn|Z0)|Z0) =
n−1

i=0
Z0Mi h.
(3.101)
Proof
Evaluating the conditional expectation of −log P(Tn|Z0) yields
HZ0(Tn) = E(−log P(Tn|Z0)|Z0) = E(−log P(X1, . . . , Xn|Z0)|Z0)
(3.102)
=
n

i=1
E(E(−log P(Xi|Zi−1, Z0))|Z0)
(3.103)
=
n

i=1
E(E(−log P(Xi|Zi−1))|Z0)
(3.104)
=
n

i=1
E(Zi−1|Z0)h =
n−1

i=0
Z0Mih.
(3.105)
Since these directed graphs have random numbers of nodes in n-generation graphs deﬁned
by the normalized entropy per parent in the tree H. As in the Markov chain case, there are two
limits.
Theorem 3.35
Given a branching process with positively regular mean matrix M, then
deﬁne
HZ0(T) = lim
n→∞
−E(log P(Tn|Z0)|Z0)
E(
n−1
i=0 Zi1|Z0)
.
(3.106)
The normalized entropy per branching variable has different forms for the sub-critical,
and the critical, super-critical branching processes:
(1) for M having largest positive eigenvalue ρ < 1 then HZ0(T) = ((Z0 (I −M)−1h)/
(Z0 (I −M)−1 1));
(2) for M having largest positive eigenvalue ρ ≥1 then H(T) = νh/ν 1.

3.9 ENTROPIES OF BRANCHING PROCESSES
71
Proof
Proof for ρ < 1: The denominator in the last expression of Eqn. 3.106 results
from Eqn. 3.37. Since the largest eigenvalue of M is less than 1, I −M is invertible.
Using the geometric series on M in 3.106 yields the result that
H = lim
n→∞
Z0 (I −M)−1(I −Mn)h
Z0 (I −M)−1(I −Mn)1.
(3.107)
Examining the limit as n →∞with the largest eigenvalue ρ < 1, yields the ﬁrst part.
Proof for ρ ≥1: For ρ > 1, substituting into Eqn. 3.106 yields
H = lim
n→∞

n−1
i=0

ρi Z0 µ ν h + Z0 Mi
2 h


n−1
i=0

ρi Z0 µν 1 + Z0 Mi
2 1

= lim
n→∞
((ρL −1)/(ρ −1))Z0 µ νh + O(αn)
((ρn −1)/(ρ −1))Z0 µ ν1 + O(αn) = lim
n→∞
Z0 µ νh + O(αn/ρn)
Z0µν1 + O(αn/ρn) = νh
ν1.
(3.108)
For ρ = 1, (ρn −1)/(ρ −1) is replaced by n and O(αn) is replaced by O(1) in 3.108,
with the conclusion identical as for ρ > 1.
3.9.3 Typical Trees and the Equipartition Theorem
The typical strings correspond to trees which have inﬁnite extent. Characterizing the combinatoric
properties of such processes requires an understanding of extinction. Do the family trees die out?
This is more subtle for branching processes than for Markov chains where if there was a connected
terminating state, then all the connected states are transient and with probability one the process
terminates. For branching processes the possibility of birthing multiple offspring implies that
extinction can occur with probability between zero and 1. By extinction we mean the event that
the random sequence Z0, Z1, . . . consists of zero for all but a ﬁnite number of values of n.
Deﬁnition 3.36
FortheV-multi-typeextinctionprobabilitydeﬁneγ =[γ (1), γ (2), . . . , γ (V)]∈
[0, 1]V to be the extinction probability corresponding to the initial state Z0 = 1(v):
γ (v) = Pr{ Zn = 0 for some n |Z0 = 1(v)},
v = 1, . . . , V.
(3.109)
For the 1-type extinction probability, γ is a scalar.
The moments of generations of the process given by the mean matrix M will determine
the extinction probability and entropy properties. The mean matrix M is a positive matrix (all
entries ≥0). In order to obtain standard limiting forms we restrict to the class of positively regular
branching processes. Clearly if Mn is strictly positive, then there exists an n generation birthing
route of nonzero probability from each parent type to every other parent type.
Clearly Zn has the potential to grow. For ρ ≤1, the limiting behavior has Zn →0 with
probability 1 (γ = 1). Surprisingly, Zn in fact converges with probability 1 for ρ > 1. This will be
apparent in the asymptotics section below.
Now an equipartition theorem is proved showing in essence that the set of trees are cut into
two sets, the typical trees corresponding to the non-extinct set of inﬁnite, non-terminated trees of
probability 1 −q, and the a-typical trees of all trees terminating in a ﬁnite number of derivations
of probability q. For typical inﬁnitely extended trees, the state vectors Zi converge, with typicality
corresponding to the relative node occurrences following the rule birthing probabilities.

72
3 PDAGS AND THEIR ENTROPIES
Deﬁnition 3.37
Let I be the set of non-terminating, or inﬁnitely extended trees given
according to I = {T(ω) : Zn ̸= 0 for any n}.
For Markov chains which are stationary, the empirical averages of state occupancy equal the
stationary probability given by the left eigenvector of the transition probability. This is true as
well for branching processes even though the number of variables at every generation is a random
number determined by the branching rate.
Lemma 3.38
Given a random branching process with positively regular mean branching
matrix M, with largest eigenvalue ρ ≥1 and associated left eigenvector ν, then the occupancy
vectors have ratios given by the left eigenvector of the branching matrix:
lim
n→∞

n−1
i=0 Zi

n−1
i=0 ρi = c(ω)ν,
c(ω) ≥0, a.e. ω ∈ (w.p. 1),
(3.110)
with c(ω) > 0 for inﬁnitely extended trees implying
lim
n→∞

n−1
i=0 Z(j)
i

j

n−1
i=0 Z(j)
i
= νj,
a.e. ω ∈I(w.p. 1 −γ ),
(3.111)
Proof
This follows by using Theorem 9.2 of Harris [72]:
lim
n→∞
Zn
ρn = c(ω)ν,
a.e. ω ∈(w.p. 1),
(3.112)
Then the triangle inequality gives almost everywhere ω ∈, for every ǫ > 0, ∃n(ǫ, ω)
such that with i > n(ǫ, ω), |Zi −ρic(ω)ν| < ǫ/2 implying with N > n(ǫ, ω) chosen large
enough and the triangle inequality:


N−1
i=0 Zi

N−1
i=0 ρi −c(ω)ν
 ≤


N−1
i=0 Zi

N−1
i=0 ρi −

N−1
i=n(ǫ,ω) Zi

N−1
i=n(ǫ,ω) ρi

)
*+
,
ǫ/2
+


N−1
i=n(ǫ,ω) Zi

N−1
i=n(ǫ,ω) ρi −c(ω)ν

)
*+
,
ǫ/2
= ǫ.
(3.113)
Since ǫ is arbitrary the ﬁrst part is completed.
To show that c(ω) is strictly positive a.e. I follows from its connection to the
extinction probability (Theorem 7.1 of Harris [72]) implying
Pr{c(ω) = 0|Z0 = 1(1),
T(ω) ∈I} = Pr{Zi(ω) = 0
for some i|Z0, Zi ̸= 0 for any i} = 0.
Theorem 3.39 (AEP)
Given is a random branching process with positively regular mean
branching matrix M, with largest eigenvalue ρ ≥1 and associated left eigenvector ν. Deﬁne
the inﬁnitely extended trees with typical set T (n)
ǫ
with respect to P(·) with entropy rate H(T)
as the trees Tn with the property
2−(
n−1
i=0 Zi1)(H(T)+ǫ) ≤P(Tn) ≤2−(
n−1
i=0 Zi1)(H(T)−ǫ).
(3.114)
Then, for all ǫ > 0, P(T (n)
ǫ
) > 1 −ǫ.

3.9 ENTROPIES OF BRANCHING PROCESSES
73
Proof
Factoring the probability of an n-generation tree gives
log P(Tn|Z0) =
n

i=1
log P(X1, . . . , Xn|Z0) =
n

i=1
log P(Xi|Zi−1, Z0)
(3.115)
=
n

i=1
log P(Xi1, Xi2, . . . |Zi−1).
(3.116)
Deﬁning Nj(n) = 
n
i=1 Z(j)
i−1, N(n) = 
j Nj(n), then from Lemma 3.38, Eqn. 3.111
Nj(n)/N(n) →νj a.e. I. As well on the subsequence of substitutions of particle type
j, Xil1, . . . , Xi
Z(j)
i−1
,
Z(j)
i−1

m=1
f(Xilm) →

k
pjkf(k) a.e. I,
(3.117)
giving almost everywhere in inﬁnitely extended trees (probability 1 −q),
lim
n→∞−
1
N(n) log P(Tn|Z0) = −lim
n→∞
n

i=1

j
Z(j)
i−1

l=1
log P(Xil|Z(j)
i−1)
(3.118)
= −lim
n→∞

j
Nj(n)
N(n)
1
Nj(n)
n

i=1
Z(j)
i−1

l=1
log pjXil
(3.119)
=

j
νjhj.
(3.120)
The branching rate and entropy are the deﬁnitive quantity describing the combinatorics.
Theorem 3.40
Given is a random branching process with positively regular mean branching
matrix.
1. If each birthing law has identically ρ≥1 children, then for all ǫ>0, for all n,
2(
n−1
i=0 ρi)(H(T)+ǫ) ≥|T (n)
ǫ
|andfornsufﬁcientlylarge, |T (n)
ǫ
|>(1−ǫ)2(
n−1
i=0 )(H(T)−ǫ).
2. If ρ > 1, then typical trees grow at super-geometric rate ρ: for all ǫ > 0, for
all n, (
n−1
i=0 (ρ + ǫ)i) ≥log |T (n)
ǫ
| and for n sufﬁciently large, log |T (n)
ǫ
| >
(
n−1
i=0 (ρ −ǫ)i).
Proof
(Part 1): From the fact that 
n−1
i=0 Zi1 = 
n−1
i=0 ρi then the identical argument
as above Theorem 3.30 in which P(T (n)
ǫ
) is upper bounded by 1, and lower bounded
by 1 −ǫ in the limit n →∞for any ǫ > 0.

74
3 PDAGS AND THEIR ENTROPIES
(Part 2): For the combinatoric statement, the deﬁnition of T (n)
ǫ
implies that for all
Tn(ω) ∈T (n)
ǫ
, log P(Tn(ω)) ≥−(ρ + δ)n, and since P(T (n)
ǫ
) ≤1,
1 ≥

T(ω)∈T (n)
ǫ
P(Tn) ≥|T (n)
ǫ
|
min
T(ω)∈T (n)
ǫ
P(Tn)
(3.121)
≥|T (n)
ǫ
|2−(ρ+δ)n,
=⇒2(ρ+δ)n ≥|T (n)
ǫ
|.
(3.122)
The lower bound follows from the convergence in probability implying for all ǫ > 0,
∃n(ǫ) with n > n(ǫ),
1 −γ −ǫ < P(T (n)
ǫ
) < |T δ
n |
max
T(ω)∈T (n)
ǫ
P(Tn)
(3.123)
< |T (n)
ǫ
|2−(ρ−ǫ)n.
(3.124)
Remark 3.9.1
Part (1) of the theorem is stronger than part (2) for the family of special
cases in which the number of descendents below every node is identically ρ, the
branching rate, such as for the singular Markov case ρ = 1, and the binary case
ρ = 2 [80] having exactly two non-terminal variables for every rule. For these cases,
ν1 = 1 and c(ω) = 1 for all ω ∈. The combinatorics resulting is strengthened simply
because the number of nodes in an n depth tree is a deterministic quantity growing at
a geometric rate.
3.10 Formal Languages and Stochastic Grammars
The formal languages of Chomsky regular, context-free and context-sensitive [81–83] are an
exquisite example of directed acyclic graphs; they should be studied in their own right. The
regular and context-free correspond to ﬁnite-state and multi-type random branching processes;
context-sensitive correspond to more general directed graphs.
Following Chomsky, deﬁne the class of strings and the languages or conﬁgurations
associated with a regular or ﬁnite-state grammar,15 context-free grammar, and context-sensitive
grammar.
Deﬁnition 3.41
A formal grammar G is a quadruple, G = ⟨VN, VT, R, S⟩, where VN is
a ﬁnite set of non-terminal symbols (also called states in the ﬁnite-state case), VT is a ﬁnite
set of terminal symbols, R is a ﬁnite set of rewrite rules, S ∈VN is a sentence start symbol.
The ﬁnite set of non-terminal symbols and combinations of them VN = {A1, A2, . . . } are the
variables which can be rewritten, with the terminal symbols VT = {w1, w2, . . . } the words
forming the strings in the language.
The grammatical rules R take the following form:
1. For a ﬁnite-state grammar the rules R rewrite the non-terminals (these are states in
the ﬁnite-state case) and are independent of the context in which the substitutions to the
nonterminal symbols are applied, of the form
A →wB,
A →w
where w ∈VT, A, B ∈VN.
(3.125)
15 Generally these grammars are called regular, as well as their languages. We prefer to use ﬁnite-state as all
the conﬁgurations which we shall deal with are regular in the sense of general pattern theory associated with
the ﬁrst structure formula Eqn. 6.2 of Chapter 6.1.

3.10 FORMAL LANGUAGES AND STOCHASTIC GRAMMARS
75
2. For a context-free grammar rules R extend the form of the ﬁnite state rules to include
combinations of non-terminals on their right-hand side, taking the form
A →ψ,
A ∈VN,
ψ ∈(VN ∪VT)∗,
(3.126)
ψ being a collection of terminal and non-terminal symbols.
3. For a context-sensitive grammar the rules R extend the form of the context-free rules
to include context around the non-terminal being written on the left hand side, giving
rules of the form
αAβ →αψβ,
A ∈VN, α, β, ψ ∈(VN ∪VT)∗.
(3.127)
The language L(G) generated by the grammar G is the set of all sequences consisting of
terminal symbols that can be generated by starting in S and recursively applying production
rules to the nonterminating expressions until the expressions contain only terminal symbols.
Now examine the probabilistic versions of the grammars and associated stochastic lan-
guages. The languages correspond to the set of strings generated by probabilistic application of
the rules. For the ﬁnite-state (regular) and context-free languages, these are Markov chains and
multi-type random branching processes.
Deﬁnition 3.42
A stochastic formal grammar G(P) is a formal grammar G
=
⟨VN, VT, R, S⟩with a family of probability laws P = {pv, 
k pvk = 1, v = 1, . . . , V} on
the production rules R = {rvk, v = 1, . . . , k = 1, . . . }:
αAβ
p(αAβ),k
→
αψβ,
A ∈VN, α, β, ψ ∈(VN ∪VT)∗,

k
p(αAβ),k = 1.
(3.128)
The stochastic language L(G(P)) generated by the stochastic grammar G(P) is the
set of all sequences consisting of terminal symbols that can be generated by starting in S
and recursively applying production rules applied with probability P to the nonterminating
expressions until the expressions contain only terminal symbols.
Formal grammars are an exquisite illustration of the pattern theory. They directly provide a
mechanism for building the space of directed graphs recursively: the grammar speciﬁes the legal rules
of transformation of one graph type to another. Naturally, ﬁnite-state grammars provide transforma-
tions which build the space of linear graphs, the context-free grammars trees and ﬁnally context
sensitive grammars provide transformations through the partially ordered set graphs.
Example 3.43 (Run-Length and Parity Finite-State Languages)
Stringscorrespond-
ing to ﬁnite-state languages can all be accepted by ﬁnite-state machines [84], and
therefore correspond to paths through ﬁnite state graphs. For ﬁnite-state grammars,
the ﬁnite set of non-terminal symbols VN = {A(1), A(2), . . . } are the states of the
equivalent ﬁnite-state graph which can be used to generate the strings.
Examine the run-length and parity languages. From the alphabet X0 = {0, 1},
the state graphs are depicted in Figure 3.8.
Begin with the 1-1 constraint language, consisting of strings containing no more
than a single 1 symbol in a row. The grammar G = ⟨VT = {0, 1}, VN = {ZERO, ONE},
S = ZERO⟩, with the rule set (generators) simply the transitions in the state graph:
R = {ONE r1
→0ZERO, ZERO r2
→0ZERO, ZERO r3
→1ONE}.
(3.129)
Elements in the language can end in either states, augment the rule set with the
terminating rules ZERO r4
→1, ZERO r5
→0, ONE r6
→0.

76
3 PDAGS AND THEIR ENTROPIES
0
1
0
0
0
1
1
Zero
One
Even
Odd
Figure 3.8 Showing two ﬁnite-state graphs corresponding to run-length (left panel) and parity
(right panel) languages. For the 1-1 languages, both states are accepting; for even parity the even
state is accepting.
For parity languages, VN = {EVEN, ODD} with S = EVEN, and the rules
R = {EVEN r1
→0EVEN, EVEN r2
→1ODD, ODD r3
→0ODD, ODD r4
→1EVEN}.
(3.130)
Since only strings ending in EVEN parity are in the language, augment with the
terminating rules EVEN r5
→0, ODD r6
→1.
Example 3.44 (M-Gram Language Models)
Consider bigram and trigram Markov
models for natural language as in Shannon [85]. The m-gram model treats a string of
words as a realization of an mth-order Markov chain in which the transition probability
is the conditional probability of a word in the string given by the previous m−1 words.
The probability of the word string is then the product of the transition probabilities
for each word, P(W) = $n
i=1 p(Wi|Wi−1, . . . , Wi−m+1).
The grammatical rules for the m-ary processes are
wi−m, wi−(m−1), . . . , wi
)
*+
,
A
→wi−(m−1), wi−(m−2), . . . , wi+1
)
*+
,
wi+1 A′
wi−m, wi−(m−1), . . . , wi
)
*+
,
A
→wi+1−m, wi+1−(m−1), wi+1−(m−2), . . . , wi, φ)
)
*+
,
wi+1
where φ = null (punctuation). For each such transition there are then two rule types
A →wA′,
w ∈VT,
A, A′ ∈VN,
A →w
w ∈VT,
A ∈VN.
(3.131)
The regular grammar rewrites the non-terminals which are states in the ﬁnite-state case
independently of the context in which the substitutions to the nonterminal symbols
are applied.
Example 3.45 (Finite-State Languages)
Strings corresponding to ﬁnite-state lan-
guages can all be accepted by ﬁnite-state machines [84], and therefore correspond
to paths through ﬁnite state graphs. For ﬁnite-state grammars, the ﬁnite set of non-
terminal symbols VN = {A1, A2, . . . } are the “states” of the equivalent ﬁnite-state
graph which can be used to generate the strings.
Examine run-length and parity languages of previous examples from the alpha-
bet  = {0, 1}. The state graphs are depicted in Figure 3.8; strings are generated as
paths through the ﬁnite-state graph shown in Figure 3.8.
Begin with the 1-1 constraint language, strings containing no more than a single
1 symbol in a row. The grammar G = ⟨VT = {0, 1}, VN = {ZERO, ONE}, S = ZERO⟩,
with the rule set (generators) simply the transitions in the state graph:
R = {ONE r1
→0ZERO, ZERO r2
→0ZERO, ZERO r3
→1ONE}.
(3.132)

3.10 FORMAL LANGUAGES AND STOCHASTIC GRAMMARS
77
Elements in the language can end in either states, augment the rule set with the
terminating rules ZERO r4
→1, ZERO r5
→0, ONE r6
→0.
For parity languages, VN = {EVEN, ODD} with S = EVEN, and the rules
(generators)
R = {EVEN r1
→0EVEN, EVEN r2
→1ODD, ODD r3
→0ODD, ODD r4
→1EVEN}.
(3.133)
Since only strings ending in EVEN parity are in the language, augment with the
terminating rules EVEN r5
→0, ODD r6
→1.
In terms of combinatoric constraints placed by the grammars, notice L(G) ⊂2n
shrinks at an exponential rate. Using a standard argument dating back to at least
Shannon [76], deﬁne Nn =
 Nn(ZERO)
Nn(ONE)

to be the 2 × 1 vector of the number of
strings ending in states ZERO, ONE, then
 Nn(ZERO)
Nn(ONE)

)
*+
,
Nn
=
 1
1
1
0

)
*+
,
A
 Nn(ZERO)
Nn(ONE)

)
*+
,
Nn−1
.
(3.134)
The exponential growth rate is given by the largest eigenvalue ρ = ((1 +
√
5)/2) of A
since by Perron–Frobenius A is strictly positive. This log 2-exponential growth rate is
in Shannon’s words termed the capacity of the language of strings.
Note that the EVEN-parity language of strings ending only in the EVEN state
has the capacity ρ = 1.
Example 3.46 (Finite-State Markov Chain: 3-1’s Language)
Let the V
=
4-type,
{A1, A2, A3, A4}, and birthing laws
A1
0.382
→0A2
A2
1→1A3
A3 →1A4
A4
1→0A1
A1
0.618
→1A3
A3 →0A1
.
(3.135)
Deﬁne Nn interpreted as the total number of derivation trees. The branching
matrices M are stochastic with largest eigenvalue ρ = 1, νM = ν, since every rewrite
rule gives rise to one syntactic variable in the strongly connected set, 
k n(l)
vk = 1:
M =


0
0.382
0.618
0
0
0
1
0
0.618
0
0
0.618
1
0
0
0

.
(3.136)
To ﬁnd the growth rates of the start state A1, deﬁne the initial vector Z0 = [1, 0, 0, 0],
then
lim
n→∞
log Nn(A1)
n
= lim
n→∞
Z0 log Nn
n
= νh = 0.4812,
(3.137)
where H = νh is the entropy of the Markov chain. Since M is stochastic, ν is the limit
distribution of the Markov chain induced by the particular choice of probabilities on
the rewrite rules.
Example 3.47 (Adding Closure: Sentences to Paragraphs)
Extendtheregulargram-
mar example forbidding three consecutive ones or zeros by adding rules which
generate the language consisting of the concatenation of all sentences in the run-length
language, commonly called the closure of the language. Deﬁne G∗
1 = ⟨VT, V∗
N, R∗, A0⟩,
with terminal symbols as above, non-terminals V∗
N = VN ∪A0, and rewrite rules

78
3 PDAGS AND THEIR ENTROPIES
R∗= R ∪{A0 →A1A0, A0 →A1}. G∗
1 is not strongly connected since there is no
derivation originating in A1 which results in a A0. It is, however, context-free and
contains one rewrite rule with more than one non-terminal variable. To calculate the
combinatorics note that the number of derivations starting in Ak for k ∈{1, 2, 3, 4} is
the same as in the ﬁrst regular grammar example and are given above, with ρ = 1.
Using these, proceed by examining log Nn(A0) given by
log Nn(A0) = log Nn−1(A1) + log (Nn−1(A0) + 1).
(3.138)
By ignoring the “+1”, this equation may be used to get a lower bound on log Nn(A0)
which is asymptotically tight. With increasing n, Nn(A0) →∞so log (Nn(A0)+1) →
log Nn(A0). Since, log (Nn(A1))/n →H as n →∞, this implies that log (Nn(A1) +
1)/n →H with n, yielding (log (Nn(A0)) −log (Nn−1(A0)))/n −1 →H. From this
it follows that the branching rate ρ = 1 and that
gn(A0) −gn−1(A0) →(n −1)H.
(3.139)
Thus gn(A0) is asymptotically a polynomial in n of degree 2.
Example 3.48 (Pseudolinear Grammars)
Now examine the set of pseudolinear lan-
guages [86, 87] which demonstrate the important role of the strong connectedness
property. In these examples the grammars have more than one non-terminal on their
right-hand side, yet ρ = 1, but have higher growth rates than regular in that log Nn
are polynomial in n. This language lies between the context-free languages having
branching rate ρ > 1, and the regular languages for which their log-rate of growth is
linear in n, and illustrates properties of the entire family of pseudolinear grammars
characterized by Kuich [87].
A grammar G is called pseudolinear if it is never the case that for any ﬁnite set
of substitutions to any non-terminal Av can a derivation Av →φ1 Av φ2 Av φ3 be
generated, where φ’s are strings of terminals and non-terminals, possibly empty. As
shown by Kuich such a grammar can be decomposed into a set of strongly connected
subcomponents each of which contains rewrite rules which have at most one non-
terminal on their right-hand side. For each of these subcomponents, the branching
rates are at most 1, and the combinatorics can be calculated directly as has been done
for the regular component in the above example. To determine the branching rate
of all rewrite rules having more than one non-terminal on its right side, recursive
equations for the log-rates of growth are derived, where the homogeneous growth
parameter is 1 (since there can be at most one syntactic variable equal to itself on
the right side of any of these rewrite rules or it would not be pseudolinear). The
additive driving function on this linear equation is n-varying and given by the log-
rate of growth of the linear, strongly connected subcomponents. For example, in the
above the homogeneous equation log Nn(A0) = log Nn−1(A0) has ρ = 1, and driving
function log Nn−1(σ) = (n −1)H, with the driving function growing with n giving
the polynomial properties for log Nn.
To illustrate this approach more comprehensively, now examine a more com-
plex example in the class of pseudolinear grammars from
Kuich [86]. Deﬁne
G4 = ⟨VT, VN, R, A1⟩with terminal symbols VT = {a, b, c, d, e}, syntactic variables
VN = {A1, A2, A3, A4, A5, A6}, and production rules
R = {A1 →A1A2A5
A2 →aA2a
A2 →A3A6
A3 →A5A4
(3.140)
A4 →A3A6
A5 →cA5c
A5 →d
A6 →bA6b
A6 →}.
(3.141)
The combinatorics for A5, A6 are given by linear equations Nn(A5) = Nn−1(A5) + 1,
Nn(A6) = Nn−1(A6) + 1, giving Nn(A5) = Nn(A6) = n. Next, for A3,
Nn(A3) = Nn−1(A5)Nn−1(A4) = (n −1)(n −2)Nn−2(A3).
(3.142)

3.10 FORMAL LANGUAGES AND STOCHASTIC GRAMMARS
79
This is a linear equation in the log with solution log Nn(A3) = 
n−1
i=2 log i, yielding
from Stirling’s formula for large n
(n −1
2) log (n −1) −(n −1) + k2 < log Nn(A3) < (n −1
2) log (n −1) −(n −1) + k1
(3.143)
implying that ρ = 1 and that log Nn(A3) grows as (n −1
2) log (n −1) −(n −1). Now
for A2,
Nn(A2) = Nn−1(A2) + Nn−1(A3)Nn−1(A6) = Nn−1(A2) + (n −2)!(n −1),
(3.144)
=
n

i=1
(i −1)! = (n −1)!(1 + O(n−1)).
(3.145)
Clearly ρ = 1 and log Nn(A2) is similar to log Nn(A3). Finally, the start state has
combinatorics determined by
Nn(A1)
Nn−1(A1) = Nn−1(A2)Nn−1(A5) =


n−1

i=1
(i −1)!

(n −1).
(3.146)
giving
Nn(A1) =
n

k=1

(k −1)
n

i=1
k −1(i −1)!

= (n −1)!
n−1

i=1
((i −1)!)n−l.
(3.147)
Asymptotically, this implies log Nn(A1) grows at least as a quadratic in n times log n.
Example 3.49 (Binary Trees)
Let V = 2, {A1, A2}, with birthing laws
A1
p→A1A1
A1
1−p
→A2A2
A2
1→A1A2
.
(3.148)
Now we can calculate the probabilities to maximize the entropy H. Start with the
branching matrix
M =
 2p
2 −2p
1
1

,
(3.149)
having largest eigenvalue ρ = 2 irrespective of p.
Example 3.50 (Arithmetic Expression Grammar)
Now the branching correspond-
ing to the arithmetic expression language [84, 88] is examined and is demonstrated
to have branching rate ρ
= 1.75488. Let V
= 4, {A1, A2, A3, A4} with A1
=
roman expression, A2 = roman term, A3 = roman factor, A4 = roman variable, with
birthing rules
A1
p11
→A1 + A2
A2
p21
→A2 ∗A3
A3
p31
→( A1 )
A4
p41
→a
A1
p12
→A1 −A2
A2
p22
→A2/A3
A3
p32
→A4
A4
p42
→b
A1
p13
→A2
A2
p23
→A3
A4
p43
→c
A4
p44
→d
.
(3.150)

80
3 PDAGS AND THEIR ENTROPIES
A derivation of the expression a ∗b takes the form
A1 →A2 →A2 ∗A3 →A3 ∗A3 →roman ∗A3
→roman ∗roman →a ∗roman →a ∗b.
(3.151)
Associate with the ﬁrst two rewrite rules of A1 probabilities p11 and p12 with the third
rule probability p(1)
3
= 1 −p11 −p12. Let the probabilities for rewriting A2 be given by
p21, p22 and 1 −p21 −p22, respectively. The mean matrix becomes
M =


p11 + p12
1
0
0
p21 + p22
1
1
0
0

.
(3.152)
Clearly, choosing p11 + p12 = 1 and p21 + p22 = 1 gives the matrix ¯M with largest
eigenvalue, implying the branching rate is the largest eigenvalue of


1
1
0
0
1
1
1
0
0

.
(3.153)
The branching rate is the largest root of the characteristic equation λ3 −2λ2 +λ−1 = 0
which is ρ = 1.75488.
To emphasize the signiﬁcance of the branching parameter, shown in Figure 3.9
are the results of counting arithmetic expression programs as a function of genera-
tion level n. Panel 1 shows a plot of log (log (Nn(1))) versus n, with Nn the number
of terminated arithmetic expressions. This plot is linear in n for large n (note that
n
=
12 is as far as could be calculated using exponents less than 300). Panel 2
shows log Nn(1)/ log Nn−1(1) converging to ρ
= 1.75488, demonstrating that the
branching rate calculated for the branching process equals the branching rate for the
context-free language as predicted by the theory. Panel 3 demonstrates a determin-
istic large deviation result showing that for δ = 0.08, (ρ ± δ)n bounds log Nn(1) for
large n.
6
(1)
(2)
(3)
800
600
400
200
0
4
6
8
10
12
2.0
2.2
1.8
6
Derviation level (L)
Derviation level (L)
8
10
12
4
0
4
Derivation Level (L)
log (log(NL))  
log (NL/log(NL–1) and r
log (NL) and bounds  
for r +/– 0.08
6
8
10
12
2
Figure 3.9 Figure shows super-exponential growth rates of the arithmetic expression language.
Panel 1 shows log – log growth rate with derivation depth n. Panel 2 shows the near line ratio of
the logarithms of successive generations. Panel 3 shows branching rate upper and lower bounds
for ρ. Results taken from O’Sullivan [79].

3.11 DAGS FOR NATURAL LANGUAGE MODELLING
81
3.11 DAGs for Natural Language Modelling
There are two fundamental approaches to language modeling: knowledge-based approaches and
statistical approaches. In recent years, the success of statistical approaches has renewed interest
in the statistical analysis of text [89]. In particular, the success of statistical part-of-speech taggers
[90,91] and of trigram models in speech recognition systems [92] has promoted the usefulness of
statistical models. The two fundamental statistical language models are the m-gram model and
the stochastic context-free model. Both models have their distinct advantages and disadvantages
which will be discussed after presenting each model.
3.11.1 Markov Chains and m-Grams
The m-gram language models of Shannon [6] model the language as an mth-order Markov chain
on the word strings. A drawback of this model is its failure to capture the hierarchical structure of
language. For applications such as message understanding or machine translation, the syntactic
structure of a sentence is useful and desired. In order to address the problem of incorporating
syntactic structure, recent research [93,94] has focused on stochastic context-free language models.
Figure 3.10 depicts several of these language models. Panel 1 shows the Markov chain
models, and panel 2 shows the context-free random branching process models.
Considering the dependencies between words as Shannon did in his discussion of successive
approximations to English [85] results in the Markov chain graph. Since language is produced as
a time sequence of words, a ﬁrst approach towards modeling it is a stationary Markov chain. The
m-gram model treats a string of words as a realization of an mth-order Markov chain. The joint
probability is then the product of the conditional transition probabilities according to
P(W) =
n

i=1
P(Wi|Wi−1, . . . , Wi−m+1).
(3.154)
For ﬁtting model parameters to the statistics of natural language, standard ML estimators
are used. For bigrams, deﬁne the counting function Nij(W) = 
g
k=2 1ij(Wk−1, Wk) the num-
ber of occurrences of the sequence ij in the string W1, W2, . . . . The MLE of the model transition
The
(1)
(2)
(3)
ran
dog
The
ran
dog
Art
N
V
NP
VP
S
The
ran
dog
Art
N
V
NP
VP
S
Figure 3.10 Three graph structures for natural language: Panel 1 Markov chain graph, panel 2
tree graph, and panel 3 directed acyclic graph.

82
3 PDAGS AND THEIR ENTROPIES
Table 3.1 Data support for bigram and trigram estimation
Data Support
Bigrams With Support
Trigrams With Support
1
250, 146
587, 339
2–9
83, 129
83, 043
10–99
11, 393
6, 334
100–999
612
222
>1000
32
7
Total
345, 312
676, 945
probabilities maximizes
log P(W1, . . . , Wn) = log P(W1) +

i

j
Nij(W) log Qij,
(3.155)
and adding the summability condition 
j Qij = 1 gives the ML solution ˆQij = ((Nij(W))/

j Nij(W))

. Similarly, the trigram MLEs are given by ˆQ(ij)k = ((N(ij)k(W))/

k N(ij)k(W)

.
While these estimates are asymptotically unbiased, it is often the case that there is not enough
data to adequately support an estimate. The data support for trigrams and bigrams taken from
the Penn TreeBank are shown in Table 3.1. Of particular interest is the fact that for over 72% of the
bigrams and 86% of the trigrams there is only one occurrence in our dataset of a million words.
In fact in a small held-out test set of 43,000 words, 30% of the bigram types did not appear in
the training set and 60% of the trigram types did not appear in the training set. For sentences
containing these missing bigrams or trigrams, zero probability would be assigned to them by the
corresponding Markov chain model.
Several approaches have been taken to resolve this problem including deleted estimation
and the use of the Good–Turing method [95, 96]. For this the deleted interpolation method of
the IBM group is used to interpolate the parameters.16 This method is used for the interpolation
of the bigram and trigram models from the Penn TreeBank shown below. In the part-of-speech
tagging problem, trigram models (m = 3) have achieved high performance for estimating the
parts-of-speech of a word string with >95% accuracy [90]. The trigram model has also proven to
be valuable in speech recognition systems because of its accuracy. However, this accuracy comes
at a cost. The number of parameters in the trigram model is proportional to N3w where Nw is
the number of distinct words in the domain of interest. For example, the number of possible
parameters in a typical 20,000 word vocabulary is 20, 0003 = 8 × 1012! Although a large number
of these parameters are zero, there is still a large number of parameters. In the Penn Treebank data
used, there are 744,000 nonzero trigrams out of a possible 90, 0003 parameters.
3.11.2 Context-Free Models
The m-gram models have large numbers of parameters and fail to capture the hierarchical structure
of language. For applications such as message understanding or machine translation, the syntactic
16 The deleted interpolation method [97, 98] makes use of lower order models to smooth our parameters.
In this case, the interpolated probabilities are given by
ˆp(W3|W1, W2) = λ1(W1, W2)p(W3) + λ2(W1, W2)p(W3|W2) + (1 −λ1 −λ2)p(W3|W1, W2)
The interpolated probability of a trigram is given in terms of weighted unigram, bigram and trigram terms. The
estimation of the interpolation weights is done by using the forward-backward algorithm for hidden Markov
models on held-out training data [99].

3.11 DAGS FOR NATURAL LANGUAGE MODELLING
83
structure of a sentence is useful and desired. In order to address the problem of incorporating
syntactic structure, recent research [93,94] has focused on stochastic context-free language models.
This shortcoming has serious implications for higher level speech processing such as mess-
age understanding or machine translation. What is needed for these applications in the context-
free model based on a stochastic context-free grammar which is a quintuple, ⟨VN, VT, R, S, P⟩,
where VN is a ﬁnite set of non-terminal symbols, VT a ﬁnite set of terminal symbols, R the set
of rewrite rules, S the start symbol (S ∈VN), and P the parameter vector such that if r ∈R,
then Pr is the probability of using the rewrite rule r. Strings in the language deﬁned by grammar
are derived by successive applications of the rewrite rules to nonterminals beginning with the
start symbol S. Associated with each terminal sequence is a labeled tree corresponding to the
hierarchical application of the rewrite rules.
Figure 3.11 shows a small grammar written by Steve Abney of BellCore. This core grammar
contains rules that form a foundation for most context-free grammars. Abney has also provided a
more substantial grammar containing 411 rules. The probability associated with each rule is shown
to the left of the rule. Note that the probabilities for rules rewriting a given left-hand side symbol,
such as “Matrix”, add up to one.
The tree T = (R1, R2, . . . , RnT) is a sequence of rules applied in order to the leftmost non-
terminal which derives the preterminal string γ1, γ2, . . . , γn where γi ∈VP and the word string
W = W1, W2, . . . , Wn are the terminal symbols. The probability of a derivation tree for a given
tree T, W
π(T, W) = π(T)
n

i=1
π(Wi|γi)
(3.156)
=
nT

i=1
PRi(T)
n

i=1
π(Wi|γi),
(3.157)
where nT is the number of rules in tree T and Ri(T) is the ith rule used in tree T.
For the tree in Figure 3.10,
π(T, W) = PS →NP VPPNP →Art NPVP →V
π(‘The′|Art)π(‘dog’|N)π(‘ran′|V).
0.85
Matrix →S
0.05
VP →VP adv
0.05
Matrix →Wh-Question
0.10
VP →VP PP
0.05
Matrix →Yes-No-Question
0.30
VP →v
0.05
Matrix →Imperative
0.20
NP →NP-Core
0.60
Wh-Question →Wh-NP aux NP VP
0.20
NP →det NP-Core
0.40
Wh-Question →Wh-NP VP
0.20
NP →pron
1.00
Yes-No-Question →aux NP VP
0.20
NP →NP PP
1.00
Imperative →VP
0.20
NP →n
1.00
S →NP VP
0.30
NP-Core →adj NP-Core
1.00
That-Clause →that S
0.70
NP-Core →n
1.00
Inﬁnitive →to VP
0.30
Wh-NP →wh det NP-Core
0.30
VP →v NP
0.60
Wh-NP →whpron
0.10
VP →v NP PP
0.10
Wh-NP →Wh-NP PP
0.10
VP →v That-Clause
1.00
PP →p NP
0.05
VP →v Inﬁnitive
Figure 3.11 Stochastic context-free grammar constructed by Steven Abney.

84
3 PDAGS AND THEIR ENTROPIES
The probability of a word string W1,n = W1W2 . . . Wn is given by
P(W1, . . . , Wn) =

T∈Parses(W1,...,Wn)
π(T)
(3.158)
where Parses(W) is the set of parse trees for the given word string. For an unambiguous grammar,
Parses(W) consists of a single parse.
3.11.3 Hierarchical Directed Acyclic Graph Model
Examine the class of stochastic language models introduced by Mark [100,101] incorporating both
the lexical dependence in context-free natural language models with the Markov chain statistical
dependences of word relations in sentences. This is depicted in panel 3 of Figure 3.10. Such a
model is a hierachical, layered directed acyclic graph in which word-tree derivations are given by
a stochastic context-free prior on trees down to the preterminal (part-of-speech) level and word
attachment is made by non-homogeneous Markov chain models. The hierarchical digraph model
of Mark [100, 101] addresses the inadequacies of the two basic language models providing the
hierarchical structure of context-free languages while maintaining the
The Mark hierarchical digraph language model with conditional word dependencies is a
directed acyclic graph. The strengths of both the m-gram model and the stochastic context-free
modelarecombinedbyaddingbigram/trigramrelativefrequenciesasconstraintsonthestochastic
context-free trees. The directed acyclic graph structure is depicted in panel 3 of Figure 3.10. The
word string with its bigram relationships and its underlying parse tree structure is depicted.
In Mark’s digraphs [101] the preterminals or parts-of-speech in the word-tree conﬁguration
are considered to be the boundary between two Markov processes, one the random branching
process which generates the preterminal sequence and the other the Markov chain which attaches
words to the generated preterminal sequence. The stochastic context free grammar generates the
parts-of-speech, Art N V P Art N. Now, given this preterminal sequence, words are attached
according to a Markov chain.
The stochastic context free grammar. An element T ∈T is a tree with part-of-speech leaves,
γi, i = 1, . . . , n. The probability of the word sequence given the tree, P(W|T) is considered here to
be an inhomogeneous Markov chain in the words given the preterminal sequence, i.e. P(W|T) =
$n
i=1 P(Wi|γi, Wi−1) where γi is the ith preterminal. The resulting DAG is neither context-free nor
strictly a Markov chain, with probability of (W, T) written
P(W, T) = 1
K exp
 N

k=1
log P(Wk|γk, Wk−1) +
Nt

i=1
log Pri(T)

.
(3.159)
The normalizer is calculated by exploiting the fact that the sum of the probabilities P(W|T) over all
word strings may be either 0 or 1. If, for a given tree T, there does not exist a word string that may
be attached to it, then P(W|T) = 0 for all W ∈W. We shall say that for this case “the tree is non-
parseable”. Hence, partitioning the space of trees T = τP ∪τc
P where τP is the set of parseable-trees
(exists at least one non-zero probability word string attachment), then 
W∈W P(W|T) = 1τP(T),
so for a complete word-tree conﬁguration (w, t), the probability is given by the probability of all
consistent parses that have word strings K = π(τP) for τP = {t : t ∈Parse(W) for some W ∈W}.
In order to gain a better understanding of these two models we consider the following example.
First, we are given a simple noun phrase grammar:
0.8
NP →DET N
1.0
ADJ →red
0.2
NP →DET ADJ N
0.5
N →car
1.0
DET →the
0.5
N →dog

3.11 DAGS FOR NATURAL LANGUAGE MODELLING
85
Suppose have the following observations and parameters,
Noun Phrase
Number of Occurrences
σaσb
ˆHσa,σb
ασa,σb
The car
40
The car
0.4
0.0
The dog
40
The dog
0.4
0.0
The red car
18
The red
0.1
0.0
The red dog
2
Red car
0.09
1.176
red dog
0.01
−3.219
With these parameters we have the following tilted distributions Note that under the prior,
T
p(T)
p*(T)
p(T)
p(T)
p*(T)
p(T)
DET
N
the
car
NP
0.4
0.4
0.40
DET
N
the
dog
NP
0.4
0.4
0.40
T
DET
N
the
car
NP
ADJ
red
0.1
0.18
0.18
DET
N
the
dog
NP
ADJ
red
0.1
0.02
0.02
the two trees for “the red car” and “the red dog” are equally likely. However, when we add the
bigram constraints, the likelihood of “the red dog” is greatly reduced.
The entropies of natural languages have been looked at historically in the information theory
community by many, including originally Shannon [85], then later by Cover and King [102],
and [103]. Mark [101, 104] was the ﬁrst to calculate the entropies of the directed acyclic graph
model.
Theorem 3.51
TheentropyoftheDAGmodelgivenbythejointentropyH(W, T) = H(T)+
H(W|T) where τP is the set of parseable trees is given by
1.
H(T) =
1
π(τP)Hπ(T) +
1
π(τP)

t∈τ c
P
π(t) log π(t) + log π(τP),
(3.160)
where Hπ(T) is the entropy of the random branching process Theorem 3.34, and
2.
H(W|T) =

t∈τP
π(t)
π(τP)H(W|T = t),
(3.161)
where H(W|T = t) is the entropy of the non-stationary Markov chain.
Proof
The second term on the right-hand side is given by the Markov chain. In order
to compute this entropy, H(T), the marginal probability of a tree t is given by
p(T) = π(T)1τP(T)
π(τP)
.
(3.162)

86
3 PDAGS AND THEIR ENTROPIES
The entropy of the trees which can be parsed into legal bigram/trigram leaf
strings
H(T) = E{−log P(T)} = −

t∈T
π(t)1τP(t)
π(τP)
log π(t)1τP(t)
π(τP)
(3.163)
= −
1
π(τP)

t∈τP
π(t) log π(t) + log π(τP)
(3.164)
=
1
π(τP)Hπ(T) +
1
π(τP)

t∈τ c
P
π(t) log π(t) + log π(τP),
(3.165)
where Hπ(T) is the entropy of the random branching process Theorem 3.34, Eqn. 3.101.
The second term is dependent on the partition τP of the tree space T and must be
computed numerically.
Such an entropy measure can be used to compare the entropies of the four language models:
bigrams, trigrams, context-free, and the layered Markov model. The parameters for these models
were estimated from a subset of Dow Jones newswire articles from the Penn TreeBank corpus.
This data has 1,013,789 words in 42,254 sentences which have been machine-parsed and hand-
corrected. The number of rules in the underlying context-free grammar are 24,111 down to the
preterminal level and 78,929 down to the word level. There are 389,440 distinct bigrams in the data
set and 744,162 distinct trigrams.
The performance of such models for limiting the uncertainty of the language can be studied
by calculating the empirical entropy of the probability law generated from empirical representa-
tions of the underlying statistics. This was done by Mark for the bigram model with parameters
estimated from a subset of Dow Jones newswire articles from the Penn TreeBank corpus. The
entropy of the purely context-free model can be calculated from the same corpus as for the bigram
model. Using the parameters derived from the corpus, (see Table 3.1). Figure 3.12 shows the
model entropies for the purely bigram/trigram (m = 2, 3) models and the context-free model.
The model entropy has the interpretation of the number of bits required to code a conﬁgu-
ration (word string, tree, or word-tree) under the given language model. The entropy of the
Art
N
V
NP
VP
S
ran
dog
The
t
Wn
0
200,000
400,000
600,000
800,000
0
100
200
300
400
500
600
Number of Parameters
Model Entropy
Context-Free Grammar
Bigram Model
CFG w/ Bigrams
Trigram Model
Figure 3.12 Left panel shows two components of a derivation tree T: the tree t deriving the preter-
minal string “Art N V” and the word string Wn = “The dog ran”. Right panel shows comparison of
the model entropy of four language models, bigrams, trigram, context-free, and directed acyclic
graph.

3.11 DAGS FOR NATURAL LANGUAGE MODELLING
87
Markov chain bigram model is given according to Lemma 3.31, Eqn. 3.82. The state transition para
meters of the model were estimated using the ML estimates from the bigram counts and trigram
counts using the deleted interpolation method to interpolate the transitions for which there was
insufﬁcient data. The bigram model has entropy of 99 bits; the trigram model has entropy of
53 bits.
The entropy of the purely context-free model can be calculated from the same corpus as for
the bigram model. Using the conditional probabilities generated via the ML estimates from the
empirically estimated Penn Treebank training corpus, Figure 3.12 shows the model entropy for the
context-free model Hπ(T) given by Theorem 3.34, Eqn. 3.101. The context-free model has entropy
Hπ(T) = 525 bits.
Figure 3.12 also shows the model entropy for the layered digraph model compared to the
context-free and bigram models alone plotted versus the number of parameters in the corre-
sponding model. The model entropy has the interpretation of the number of bits required to code
a conﬁguration (word string, tree, or word-tree) under the given language model. A dramatic
reduction in the model entropy is seen in the layered digraph model with entropy of 41 bits. This
order-of-magnitude reduction in entropy from that of the SCFG demonstrates the effect of the
bigram constraints. These constraints place probability only on conﬁgurations that satisfy the lex-
ical preferences in the training corpus which rules out a very large number of trees which violate
these preferences.
This reduction in entropy does come at the cost of additional parameters since approximately
400,000 parameters have been added in the form of bigram constraints. However, the entropy of the
trigram model is 53 bits which is greater than that of the layered Markov model. So, even though
it has almost twice as many parameters as the layered Markov model, the trigram model has
higher entropy. The number of additional parameters added via context-free rules to the bigram
model is a fraction of those added via the trigram model, yet a comparable reduction in entropy
results.
3.12 EM Algorithms for Parameter Estimation in Hidden
Markov Models
Typical speech recognition systems use hidden Markov models to estimate strings of phonemes
fromtheacousticsignal(seethereview[105]).Atahigherlevelofprocessingmodelsareusedwhich
describe the way in which words or subwords (phonemes) are put together to form sentences.
Hidden Markov models (HMMs) with directed graph structures often provide a computationally
efﬁcient framework for computing probabilities of sequences. Methods based on such compu-
tationally efﬁcient structures have been given many names, dynamic programming, Dijkstra’s
algorithm, Viterbi algorithm [106]. For a beautiful paper on this topic, see Forney’s 1973 inci-
sive paper [107]. The particular model descriptions presented here follow the developments of
V. Goel and S. Kumar of the Center for Language and Speech Processing at Johns Hopkins Univer-
sity. Examine the HMM DAG depicted in Figure 3.13 showing the hidden process a Markov
chain X1, X2, . . . of states in X0 with transition probabilities QXi−1Xi = P(Xi|Xi−1), and the
output or observation process Y1, Y2, . . . taking values in Y with output probability stationary
oXi→Yi = P(Yi|Xi), and initial state distribution π(X1).
The practical usability of HMMs arise from their efﬁcient solution to the following two basic
problems: (i) how to choose a state sequence (X1, X2, . . .) that is maximum a posteriori probability
given the observation sequence (Y1, Y2, . . .), (ii) how to efﬁciently compute Pφ(Y1, Y2, . . . ) and
adjust the parameters φ of the model so as to maximize Pφ(Y1, Y2, . . . ) given the observation
sequence (Y1, Y2, . . . ).

88
3 PDAGS AND THEIR ENTROPIES
2
3
4
5
1
2
3
4
5
1
X
X
X
X
X
Y
Y
Y
Y
Y
Figure 3.13 The Hidden Markov model with hidden state sequence X1, X2, . . . and conditionally
independent outputs Y1, Y2, . . . given the state-sequence.
3.12.1 MAP Decoding of the Hidden State Sequence
Examine the graph structure dictating the conditional independence of the time evolution of the
process. This is best represented by a structure called trellis, as shown in Figure 3.14.
Examine the problem of deciding on a hidden state sequence X1, X2, . . . that best explains
the observation Y1, Y2, . . . . The MAP state sequence is given by
( ¯X1, ¯X2, . . . ) =
arg max
(X1,...,Xn)∈X n
0
P(X1, . . . , Xn, Y1, . . . , Yn).
(3.166)
Brute force calculation of the MAP estimator is O(|X0|n). Alternatively, dynamic programming
based solutions based on the Viterbi algorithm [107] computes the MAP sequence in O(n|X0|2)
computations. Examine Figure 3.14 illustrating a three-state graph for an n = 5 length state
sequence X1, . . . X5. The most likely state sequences are scored. The Viterbi algorithm exploits
the fact that given the highest scoring paths to time k ending in all of the states, then to compute
the solution to time k +1 one has to examine all of |X0| states sequentially trying |X0| new costs for
each state, hence producing an order |X0|2 algorithm per sequence length, and n|X0|2 complexity
algorithm for n-length strings.
Lemma 3.52 (Viterbi Algorithm)
Denote the score of the most likely state sequences up
to time k ending in state Xk = x as Sk(x), deﬁned as
Sk(x) =
max
X1,...,Xk−1∈X k−1
0
P(X1, . . . , Xk−1, Xk = x, Y1, . . . , Yk).
(3.167)
The maximizing state sequence is generated recursively in order O(|X0|2) via the recursion
with S1(x) = π(x)ox→Y1, then Sn = maxx∈X0 Sn(x) with
Sk+1(x) = max
Xk∈X0
QXkxox→Yk+1Sk(Xk),
x ∈X0.
(3.168)
Proof
Sk+1(x) =
max
(X1,...,Xk)∈X k
0
P(X1, . . . , Xk, Xk+1 = x, Y1, . . . , Yk, Yk+1)
(3.169)
=
max
(X1,...,Xk)∈X k
0
P(Xk+1 = x, Yk+1|X1, . . . , Xk, Y1, . . . , Yk)P(X1, . . . , Xk, Y1, . . . , Yk)
(3.170)
= max
Xk∈X0
P(Xk+1 = x|Xk)P(Yk+1|x)Sk(Xk) = max
Xk∈X0
QXkxox→Yk+1Sk(Xk). (3.171)

3.12 EM ALGORITHMS FOR PARAMETER ESTIMATION IN HMMS
89
X3
X2
X1
X3
X1
X3
X2
X1
X3
X2
X1
X3
X2
X1
S1(X)1
S2(X1)
S2(X2)
S2(X3)
S3(X3)
S4(X3)
S5(X3)
S3(X2)
S4(X2)
S5(X2)
S3(X1)
S4(X1)
S5(X1)
X2
Figure 3.14 Representation of the Viterbi algorithm on a three-state state space. The arcs in bold
show the path which obtains the state sequence with the highest output probability.
3.12.2 ML Estimation of HMM parameters via EM Forward/Backward
Algorithm
Examine ML estimation of the model parameters π(x), Qxx′, ox→y. For estimating the parameters
of the hidden state sequences, procedures based on the EM algorithm have been used extensively
(see Section 2.7). To see the difﬁculty, a direct assault on maximizing likelihood with respect to the
parameters φ = (π, Qxx′, ox→y) requires the computation of the observation sequence
Pφ(Y1, . . . , Yn) =

(X1,...,Xn)∈X n
0
Pφ(X1, . . . , Xn, Y1, . . . , Yn)
(3.172)
=

(X1,...,Xn)∈X n
0
π(X1)oX1→Y1
n

i=2
QXi−1XioXi→Yi,
(3.173)
where X n
0 is the set of all n length state sequences. Because of the hidden variates, direct calculation
of maximizers is difﬁcult.
Here is the Baum–Welch algorithm [108] commonly used in speech recognition. It is an EM
algorithm beneﬁting from the properties of monotonicity of likelihood.
Algorithm 3.53 (Baum–Welch)
Deﬁning φm = (πm, Qm
xx′, omx→y) and the sufﬁcient
statistics Nxx′(X) the number of times state x′ follows state x and Nx→y(X, Y) is the number
of times state x outputs observation y, then the sequence of iterates φ1, φ2, . . . deﬁned by the
following iteration are an EM algorithm:
Qnew
xx′
= E

Nxx′|φold, Y
	
=

n−1
k=1 Pφold(Xk = x, Xk+1 = x′|Y)

n−1
k=1 Pφold(Xk = x|Y)
(3.174)
onew
x→y = E

Nx→y|φold, Y
	
=

n
k=1 Pφold(Xk = x|Y)1{y}(yk)

n
k=1 Pφold(Xk = x|Y)
,
(3.175)
with initial state estimate π(X)new = Pφold(X1|Y).
Proof
That this is an EM algorithm (Theorem 2.52) follows with complete data
(X, Y) = (X1, . . . , Xn, Y1, . . . , Yn) and complete data likelihood written in terms of

90
3 PDAGS AND THEIR ENTROPIES
the sufﬁcient statistics
P(X, Y) = π(X1)
n

k=2
P(Xk|Xk−1)
n

k=1
P(Yk|Xk)
= π(X1)

x∈X0

x′∈X0
QNxx′(X,Y)
xx′

x∈X0

y∈Y
o
Nx→y(X,Y)
x→y
.
The expectation E-step in the EM algorithm (Eqn. 2.154) computes the expected value
of the complete data log-likelihood given the data Y = Y1, Y2, . . . and the previous
estimates of the parameters φ = {π, Qxx′, ox→y} according to
E

log P(X, Y)|Y, φ
	
=

x∈X0

x′∈X0
E

Nxx′(X)|Y, φ
	
log Qxx′
+

x∈X0

y∈Y
E

Nx→y(X, Y)|Y, φ
	
log ox→y
+

x∈X0
E

1{x}(X1)|Y, φ
	
log π(x).
For the maximization M-step (Eqn. 2.156), the conditional expectation is maximized
subjecttonormalizationconstraintsontheprobabilities. Forthestatetransitionparam-
eter Qxx′, maximize the ﬁrst term of Eqn. 3.176 with the constraint 
x′∈X0 Qxx′ = 1
giving
Qxx′ =
E

Nxx′(X)|Y, φ
	

x′∈X0 E

Nxx′(X)|Y, φ
	 =

n−1
k=1 Pφ(Xk = x, Xk+1 = x′|Y)

n−1
k=1 Pφ(Xk = x|Y)
.
For the output distribution parameter ox→y, maximize the second term of Eqn. 3.176
with constraint 
y∈Y ox→y = 1 giving
ox→y =
E

Nx→y(X, Y)|Y, φ
	

x∈X0 E

Nx→y(X, Y)|Y, φ
	 =

n
k=1 Pφ(Xk = x|Y)1{y}(yk)

n
k=1 Pφ(Xk = x|Y)
.
Fortheinitialstateparameterπ(x), ˆπ(x) = ((E{1{x}(X1)|Y, φ})/(
x∈X0 E{1{x}(X1)|Y, φ})) =
Pφ(X1 = x|Y).
To implement the EM algorithm, there is a core computation, the conditional probability of
occupying a pair of states, Pφ(Xk = x, Xk+1 = x′|Y). From this the single state conditional prob-
ability Pφ(Xk = x|Y) can be directly calculated. Brute force calculation would involve summing
out the hidden variables of Eqn. 3.173, a prohibitive O(|X0|n) calculation. The recursive structure
exploited by the Viterbi algorithm associated with the trellis diagram is commonly used. This is
the so-called Forward/Backward algorithm commonly used in speech recognition.
Lemma 3.54 (Forward/Backward)
Deﬁning the forward and backward scores
αk(x) = P(Y1, . . . , Yk, Xk = x),
βk(x) = P(Yk+1, . . . , Yn|Xk = x)
(3.176)
the joint state probabilities are computed order O(n|X0|2) according to
ξk(x, x′) = Pφ(Xk = x, Xk+1 = x′|Y)
(3.177)
=
αk(x)Qxx′ox′Yk+1βk+1(x′)

x∈X0 αn(x)
.
(3.178)

3.12 EM ALGORITHMS FOR PARAMETER ESTIMATION IN HMMS
91
The EM algorithm parameters are computed according to
Qxx′ =

n−1
k=1 Pφ(Xk = x, Xk+1 = x′|Y)

n−1
k=1 Pφ(Xk = x|Y)
=

n−1
k=1 ξk(x, x′)

n−1
k=1

x′∈X0 ξk(x, x′)
ox→y =

n
k=1 Pφ(Xk = x|Y)1{y}(yk)

n
k=1 Pφ(Xk = x|Y)
=

n
k=1

x′∈X0 ξk(x, x′)1{y}(yk)

n
k=1

x′∈X0 ξk(x, x′)
.
with initial state estimate π(X1) = Pφ(X1|Y) = 
x′∈X0 ξ1(X1, x′).
Proof
Using the forward and the backward scores, then the joint state probability
becomes
ξk(x, x′) = P(Xk = x, Xk+1 = x′|Y)
= P(Y1, . . . , Yk, Xk = x, Xk+1 = x′, Yk+1, . . . , Yn)
P(Y)
(3.179)
=
αk(x)Qxx′ox′Yk+1βk+1(x′)

x∈X0 αn(x)
.
(3.180)
To demonstrate efﬁcient procedures O(n|X0|2) for computing forward and back-
ward scores exploit the conditioning structure of the graph. For αk(xk) with α1(x) =
π(x)P(Y1|x), then
αk+1(xk+1) =

xk∈X0
P(Y1, . . . , Yk, Yk+1, xk, xk+1)
=

xk∈X0
P(Yk+1, xk+1|xk, Y1, . . . , Yk)P(Y1, . . . , Yk, xk)
=

xk∈X0
oxk+1Yk+1Qxkxk+1αk(xk)
with P(Y1, . . . , Yn) =

x∈X0
αn(x).
(3.181)
Similarly for the backward score initializing with βn(x) = 1, then
βk(x) = P(Yk+1, Yk+2, . . . , Yn|Xk = x)
(3.182)
=

x′∈X0
P(Yk+1, Yk+2, . . . , Yn, Xk+1 = x′|Xk = x),
(3.183)
=

x′∈X0
P(Yk+2, . . . , Yn, |Xk+1, Xk = x, Yk+1, )P(Yk+1, Xk+1 = x′|Xk = x),
(3.184)
=

x′∈X0
P(Xk+1 = x|Xk = x′)P(Yk+1|Xk+1 = x)βk+1(x).
(3.185)
Example 3.55
(HMM Based System for Large Vocabulary Conversational Speech
Recognition)An automatic speech recognition(ASR) system was designed at the Cen-
ter for Language and Speech Processing (CLSP) summer workshop [109] with the
goal of producing a word-transcription of an acoustic input signal. We now describe
a HMM-based ASR system for the Switchboard corpus [110]. Switchboard is a large
vocabulary conversational speech recognition corpus where speech data is collected

92
3 PDAGS AND THEIR ENTROPIES
Table 3.2 A summary of the automatic speech recognition system from a training corpus of 45 h with 2.2
million words from the 1997 CLSP Workshop [109] on a test corpus of 93 minutes with the dimension of the
features 39
Switchboard Corpus
Front-End Features
PLP Cepstral Coefﬁcients [111]
HMM Topology
Three-state , Left-to-Right
Number of Gaussians/HMM State
12
Number of HMM states
7461
Number of Gaussian Mixtures
89,544
Word Error Rate
40.4%
over the telephone lines. The acoustic signal is processed by a front-end and converted
into a sequence of acoustic feature vectors, typically 100 vectors or frames per second.
Given this sequence of acoustic feature vectors O, the system seeks from amongst all
word sequences W, the sequence ˆW satisfying:
ˆW = arg max
W∈WP(W|O) = arg max
W∈WP(O|W)P(W),
(3.186)
where P(W) is the a priori probability of a word sequence W and P(O|W) is the condi-
tional probability of observing O when the word sequence W is uttered by the speaker.
The estimates of these probabilities are computed using parametric models and the
parameters of these models are estimated from data. An acoustic model gives the
estimate of P(O|W) while a language model gives an estimate of P(W).
The state-of-the-art ASR systems use a phonetic HMM as the acoustic model.
Context dependent versions of phones are modeled by concatenating HMMs. In
our system, each HMM state corresponds to a subphonetic unit with a multivariate
Gaussian mixture as the output distribution. These Gaussian mixtures have diago-
nal covariances. A decision tree-based procedure was used to cluster the HMM states
based on phonetic and word boundary information (Table 3.2).
The Language Model used in the CLSP system is a trigram model where the
probability of the word string W is given by
P(W) =
n

i=3
P(Wi|Wi−1, Wi−2)π(W1, W2).
(3.187)
The classiﬁcation performance in this task is measured by a weighted Lev-
enshtein distance [112] between speech recognizer transcript and the manual
transcription.
3.13 EM Algorithms for Parameter Estimation in Natural
Language Models
An important problem in both of the proposed language models is the estimation of the model
parameters. The Inside-Outside algorithm, ﬁrst established by Baker [113], is used to estimate
the rule probabilities in the stochastic context-free model (see Lari and Young [93] and Jelinek
for reviews [94]). This estimation algorithm is a parallel of the forward-backward algorithm for
hidden Markov models. It can be shown that the Inside-Outside algorithm is an EM algorithm.

3.13 EM ALGORITHMS
93
These models clearly take care of the problem of linguistic structure. Primary among the problems
is the failure to model lexical preferences such as those expressed by bigrams and trigrams.
3.13.1 EM Algorithm for Context-Free Chomsky Normal Form
For a grammar in Chomsky Normal Form, the familiar Inside/Outside Algorithm is used to
estimate the stochastic grammar substitution rule probabilities. The Inside/Outside algorithm is
an extension of the Baum–Welch re-estimation algorithm for hidden Markov models in which the
complete data is the word string with its underlying parse tree, and the incomplete data is the
observed word string.
For the estimation of rule probabilities from a word string W1, . . . , Wn, examine ML estima-
tion of the model parameters pσ→r0r1 the probabilities of rewriting non-terminal σ as r0r1. Assume
the context-free grammar comes in Chomsky Normal form [84,94], with each non-terminal rewrit-
ing into either two non-terminals σ →σ1σ2 or into a terminal or word σ →w. Again the EM
algorithm is used, appealing to the Inside/Outside Algorithm as ﬁrst introduced by Baker [113]
generalizing the Forward/Backward Baum–Welch algorithm for Markov chains (see 3.12.2).
Here the hidden state sequences correspond to the underlying parse of the word string; for
nonambiguous grammars there would be a unique parse for each word string, although context-
free grammars are generally ambiguous. Maximizing likelihood with respect to the parameters
θ = (pσ→σ1σ2, pσ→w) uses the Inside/Outside Algorithm commonly used in language parsing which
is an EM algorithm beneﬁting from the properties of monotonicity of likelihood.
Algorithm 3.56
Deﬁning θm = (pmσ→σ1σ2, pmσ→w), the sufﬁcient statistics in the complete-
data are the number of instantiations of the rewrites rules Nσ→σ0σ1(T), Nσ→w(T) in the
underlying tree T. The sequence of iterates θ1, θ2, . . . deﬁned by the following iteration are an
EM algorithm:
pnew
σ→σ1σ2 = E

Nσ→σ1σ2(T)|W1, . . . Wn, θold	
E

Nσ (T)|W1, . . . , Wn, θold	
,
(3.188)
pnew
σ→w = E

Nσ→w(T)|W1, . . . , Wn, θold	
E

Nσ (T)|W1, . . . , Wn, θold	
.
(3.189)
Proof
That this is an EM algorithm (Theorem 2.52) follows with complete data the
words and tree parse (W, T) and complete data probability given by
P(W, T) =

σ→σ0σ1
p
Nσ→σ0σ1(T)
σ→σ0σ1

σ→w
pNσ→w(T)
σ→w
.
The E-step in the EM algorithm (Eqn. 2.154) computes the expected value of the com-
plete data log-likelihood given the incomplete data W and the previous estimates of
the parameters θold
E

log P(W, T)|W1, . . . , Wn, θold	
=

σ→σ0σ0
E

Nσ→σ0σ1(W)|W1, . . . , Wn, θold	
log pσ→σ0σ1
+

σ→w
E

Nσ→w(T)|W1, . . . , Wn, θold	
log pσ→w
For the maximization M-step (Eqn. 2.156), the conditional expectation is maxi-
mized subject to normalization constraints on the probabilities, 
σ→σ0σ1 pσ→σ0σ1 +

σ→w pσ→w = 1.
Maximizing
subject
to
the
constraint
gives
the
theorem
statement.

94
3 PDAGS AND THEIR ENTROPIES
3.13.2 General Context-Free Grammars and the Trellis Algorithm
of Kupiec
However, most grammars are not in this normal form. Although the grammar could be easily
converted to CNF, maintaining its original form is necessary for linguistic relevance. Hence, we
need an algorithm that can estimate the probabilities of rules in the more general form given above.
The algorithm derived by Mark [101] is a speciﬁc case of Kupiec’s trellis-based algorithm
[114]. Kupiec’s algorithm estimates parameters for general recursive transition networks. In our
case, we only have rules of the following two types:
1. H →G1G2 · · · Gk where H, Gi ∈VN and k = 1, 2, . . .;
2. H →T where H ∈VN and T ∈VT.
For this particular topology, we derived the following trellis-based algorithm.
Trellis-based Algorithm
1. Compute inner probabilities α(i, j, σ) = Pr[σ derives Wij] where σ ∈VN the set of non-
terminals and Wij denotes the substring Wi . . . Wj.
α(i, i, σ) = P old
σ→wi +

σ1:σ→σ1
P old
σ→σ1α(i, i, σ1),
α(i, j, σ) =

σn:σ→...σn
αnte(i, j, σn, σ)
αnte(i, j, σm, σ) =



P old
σ→σm...α(i, j, σm)
if σ →σm . . . or m = 1

j−1
k=i+1 αnte(i, k, σm−1, σ)α(k, j, σm)
if σ →. . . σm−1σm . . . .
2. Compute outer probabilities β(i, j, σ) = Pr[S
∗⇒W1,i−1 σ Wj+1,n] where σ ∈VN. Choose
β(1, n, S) = 1.0,
β(i, j, σ) =

n→σ...
P old
n→σ...βnte(i, j, σ, n) +

n→...pσ...
i−1

k=0
αnte(k, i, p, n)βnte(k, j, σ, n)
βnte(i, j, σm, σ) =



β(i, j, σ)
if σ →. . . σm

L
k=j+1 α(j, k, σm+1)βnte(i, k, σm+1, σ)
if σ →. . . σmσm+1 . . .
3. Re-estimate P.
P new
σ→σ1σ2...σn =

N−1
i=1

N
j=i+1 αnte(i, j, σn, σ)β(i, j, σ)

N
i=1

N
j=i α(i, j, σ)β(i, j, σ)
P new
σ→T =

i:wi=T α(i, i, σ)β(i, i, σ)

N
i=1

N
j=i α(i, j, σ)β(i, j, σ)
.
Note that for CNF grammars, the trellis-based algorithm reduces to the Inside-Outside
algorithm. As an EM algorithm, the trellis-based algorithm has the important property that the
sequence of likelihood values converges monotonically, that is, the likelihood increases after each
iteration unless Pnew = Pold which indicates convergence of the sequence of estimates.

4
M A R K O V R A N D O M F I E L D S O N U N D I R E C T E D
G R A P H S
ABSTRACT
This chapter focuses on random ﬁelds on lattices and undirected graphs. Discrete
ﬁnite state spaces are examined in the context of Markov and Gibbs ﬁelds. The subsequent
chapter studies state spaces on the continuum through Gaussian ﬁelds.
This chapter examines texture representation and segmentation, exploring the Gibbs
random ﬁelds. For dealing with the partition function asymptotics are derived allowing for
the explicit calculation of the asymptotic approximation to the log-normalizer required for
the Bayes solution. Zhu’s maximum-entropy model is explored for representing random ﬁeld
textures.
4.1 Undirected Graphs
Thus far we have examined only directed graphs. Now examine the most general undirected
graphs such as shown in the left panel of Figure 4.1. The patterns are constructed via the graphs
σ = {D, E} deﬁned by their domain of sites D and edge system E. For undirected graphs, the edges
of the graph will have no orientation, but play the role of deﬁning the neighborhood and clique
structure of the graph. To begin with, two points which are sites are neighbors if there is an edge
connecting them. A set of points form a clique if every pair of points in the set are neighbors.
Deﬁnition 4.1
Undirected graph σ = {D, E} has neighborhood system N = ∪iNi with
neighborhoods if (1) j /∈Nj, (2) j ∈Ni ⇔i ∈Nj, and (3) j ∈Ni =⇒∃e ∈E with
j•
e−−i•.
Deﬁne C = ∪C as the clique system of σ = {D, E} with subsets C ⊂D the cliques
if every two distinct sites in C are neighbors: i, j ∈C =⇒i ∈Nj, j ∈Ni.
There are many examples of patterns with undirected graph structures; a few are depicted
in Figure 4.1.
Lattice
All
Cyclic
Discrete
Torus
Tree
Forest
Linear
2-Connection linear
Figure 4.1 Figure showing various undirected graphs.
95

96
4 MARKOV RANDOM FIELDS ON UNDIRECTED GRAPHS
i1
i2
i3
i6
i5
i4
i7
i8
i2
i2
i5
i6
i5
i4
i8
i9
i5
i8
i4
i5
i5
i6
Figure 4.2 Left panel shows a 3 × 3 lattice. Middle panel shows the neighborhood system N5 =
{i2, i4, i6, i8} for i5. Right panel shows the clique system of i5, {i2, i5} ∪{i5, i8} ∪{i4, i5} ∪{i5, i6}.
1. If there are no edges or bonds connecting sites in the graph then, σ = DISCRETE. If, on the
other hand all sites are connected with segments then, σ = ALL.
2. Begin with the integer lattice, nearest neighbor connected, σ = LINEAR. Let D = {i : 1 ≤
i ≤m} be the integer lattice. Let the neighborhood system be of the form
N = ∪1≤i≤mNi; Ni = {j ∈D : 0 < |j −i| ≤c}.
(4.1)
with c = 1, these are neighborhoods corresponding to a linear graph, with cliques subsets of
the form {i}, {i, i + 1}. Notice sites at the boundary have 1 less neighbor. With c = 2, it is the
2-connection linear structure shown. Notice the boundary sites have 2 less neighbors, and
1 site in from the boundary has 1 less neighbor.
3. The cyclic graph σ = CYCLIC is similar to the linear one, with the ﬁrst and last sites
connected, so that neighborhoods are identical throughout the graph.
4. Let σ = LATTICE, then let D = {1 ≤i1, i2 ≤m} be the m × m integer lattice. Let the
neighborhood system be of the form
N = ∪1≤i1,i2≤mNi; Ni = {j ∈D : ∥j −i∥≤c}.
(4.2)
for c = 1, cliques are subsets of the form {(i1, i2)}, {(i1, i2), (i1 + 1, i2)}, or {(i1, i2), (i1, i2 + 1)}.
Changing to the σ = TORUS, the neighborhoods only change along the boundary.
5. Multiple trees correspond to σ = FOREST also as depicted. Notice, there are multiple root
nodes.
To illustrate the neighborhood and clique structures for a regular lattice, shown in Figure 4.2
is a 3 × 3 set of sites D = {i1, . . . , i9}, with its edges. The middle panel shows the neighbors of i5,
N5 = {i2, i4, i6, i8}; the right panel shows the cliques. Now examine the placement of probabilities
on the graphs to respect the directed edge structure.
4.2 Markov Random Fields
Thusfarconditionalindependenceandsplittingpropertiesofdirectedgraphshavebeenexamined.
We now examine random ﬁelds on lattices and arbitrary graphs which are not directed.
Since graphs play a fundamental role in the description of patterns, Markov random ﬁelds
provide an important probabilistic structure. Of course Markov random ﬁelds extend Markov pro-
cesses to settings in which there are no natural ordering supporting ﬁnite, ﬁxed size boundaries for
arbitrary cylinders during the peeling process. This allows us to study multidimensional lattices.
The earliest applications of Markov random ﬁelds are contained in the works of Ising (1925) [115]
and later Onsager on the now classic Ising random ﬁeld models for the characterization of mag-
netic domains. Of course much of the formal mathematics on the existence of probability measures

4.2 MARKOV RANDOM FIELDS
97
on the inﬁnite graph can be credited to Dobrushin [116–119] and Ruelle [120]. These mathematical
treatments notwithstanding, much of the popularization of MRFs in the 1980s can be credited to
Besag’s early 1974 paper [121], Geman and Geman’s subsequent inﬂuential paper [122] on image
processing, and the must-read monographs of Kinderman and Snell [123] and Geman [124].
Now examine the placement of probabilities on the graphs to respect the neighborhood and
edge structure.
MRFs are an important probabilistic tool for the conﬁgurations of pattern theory. Begin
assuming ﬁnite graphs σ = {D, E} with number of sites n(σ) = n. Assign to each point in the
graph one of a ﬁnite set of random variables, {Xi, i ∈D} denoting the family of random variables
indexed over D, each random variable taking values in X0 a ﬁnite state space. The set of possible
conﬁgurations becomes X = X n
0 . To assign probability distributions to the set of all possible
conﬁgurations of X ∈X, begin with the local characteristics of the probability distribution on X
which are the conditional probabilities of the form
P(Xi|Xj, j ∈D/i).
(4.3)
Then P will be said to deﬁne a Markov random ﬁeld with respect to a graph with neighborhood and
cliques if the local characteristic depends only on the neighbors.
Deﬁnition 4.2
X is a X0 = {0, 1, . . . , m−1} valued Markov random ﬁeld on σ = {D, E}
with n = |D| and neighborhoods N = ∪iNi, if for all X ∈X = X n
0 , and Xi, i ∈D,
P(X) > 0,
(4.4)
P(Xi|Xj, j ̸= i) = P(Xi|Xj, j ∈Ni).
The conditional probabilities of an MRF globally determine the joint distribution.
Theorem 4.3
The distribution on X is determined by its local characteristics.
Proof
We will show that for all x, y ∈X,
Pr{X = x}
Pr{X = y}
=
$n
i=1 Pr{Xi = xi|X1 = x1, . . . , Xi−1 = xi−1, Xi+1 = yi+1, . . . , Xn = yn}
$n
i=1 Pr{Xi = yi|X1 = x1, . . . , Xi−1 = xi−1, Xi+1 = yi+1, . . . , Xn = yn}.
(4.5)
This determines the distribution P(X) since taking another candidate P′(X) satisfying
P(x)/P(y) = P′(x)/P′(y) implies

x∈X
P(x)
P(y) =

x∈X
P′(x)
P′(y) =⇒P(y) = P′(y),
∀y ∈X.
(4.6)
Now to Pr{X = x}/Pr{X = y}:
Pr{X = x} =
Pr{X1 = x1, . . . , Xn = xn}
Pr{X1 = x1, . . . , Xn−1 = xn−1}
× Pr{X1 = x1, . . . , Xn−1 = xn−1}
Pr{X1 = x1, . . . , Xn = yn}
Pr{X1 = x1, . . . , Xn = yn}
= Pr{Xn = xn|X1 = x1, . . . , Xn−1 = xn−1}
Pr{Xn = yn|X1 = x1, . . . , Xn−1 = xn−1}Pr{X1 = x1, . . . , Xn = yn}.
(4.7)

98
4 MARKOV RANDOM FIELDS ON UNDIRECTED GRAPHS
Do it again with
Pr{X1 = x1, . . . , Xn = yn} = Pr{Xn−1 = xn−1|X1 = x1, . . . , Xn = yn}
Pr{Xn=1 = yn−1|X1 = x1, . . . , Xn = yn}
× Pr{X1 = x1, . . . , Xn−1 = yn−1 · Xn = yn}.
(4.8)
Continuing in this manner n-times gives
Pr{X1 = x1, . . . , Xn = xn}
=
$n
i=1 Pr{Xi = xi|X1 = x1, . . . , Xi−1 = xi−1, Xi+1 = yi+1, . . . , Xn = yn}
$n
i=1 Pr{Xi = yi|X1 = x1, . . . , Xi−1 = xi−1, Xi+1 = yi+1, . . . , Xn = yn}
× Pr{X1 = y1, . . . , Xn = yn}.
(4.9)
Dividing by Pr{X1 = y1, . . . , Xn = yn} gives the result.
Example 4.4 (Markov Chains)
Let X1, X2, . . . be a Markov chain, then the neighbor-
hoods are the parents and children for Ni = {i −1, i + 1}, i = 2, . . . .
P(Xi|Xj, j ̸= i) =
P(X)

Xi P(X) =
$n
k=2 P(Xk|Xk−1)π(X1)

Xi
$n
k=1 P(Xk|Xk−1)π(X1)
=
P(Xi|Xi−1)P(Xi+1|Xi)

x P(Xi = x|Xi−1)P(Xi+1|Xi = x).
(4.10)
Example 4.5 (MRF Representation of Textured Images)
Randomﬁeldmodelshave
been used extensively in image processing. The number of applications are extensive;
here is one. Examine the local texture representations via Markov Random Fields
described in[125–128]Exploit the local MRF characterization by associating with each
region type in an image one of M-models θ1, θ2, . . . , θM. Then X is modeled as a
Markov random ﬁeld with local probabilities
Pθk(Xi|Xj, j ̸= i) = Pθk(Xi|Xj, j ∈Ni),
i ∈D.
The neighborhood system N = ∪i∈DNi is taken to be nearest neighbors on the regular
square lattice.
The left two panels 1 and 2 of Figure 4.3 are several electron micrograph images
at a magniﬁcation of approximately 30,000 used in an experiment for representing the
textures of mitochondria and cytoplasm in subcellular regions viewed via electron
microscopy. The conditional probabilities P(Xi|Xj, j ∈Ni) are estimated from a set
(1)
(2)
(3)
(4)
Figure 4.3 Panels 1 and 2 show electron micrographs at 30,000 magniﬁcation containing mito-
chondria and cytoplasm. Panels 3 and 4 show the Bayes segmentation via an MRF model of
nearest-neighbors with zero boundary and four gray levels. Results from [127, 128]; data from
Jeffrey Safﬁtz of the Department of Pathology at Washington University.

4.2 MARKOV RANDOM FIELDS
99
of training micrographs labeled into mitochondria and cytoplasm based on the local
average gray-level feature and a four-gray level texture feature, X0 = {0, 1, 2, 3} with
nearest-neighbor structure [127, 128]. Estimating the local conditional probabilities
for the organelles is straight forward. The Von–Mises estimators for the conditional
probabilities are computed by counting the relative frequency of occurrence of Xi ∈
{0, 1, 2, 3}, given its neighboring conﬁguration. This allows for the direct estimation
of the essential characteristics as encoded via the local conditional probabilities of the
various organelles.
The right two panels 3 and 4 of Figure 4.3 show the pixel based optimal Bayesian
hypothesis test performed pixel by pixel solving
θi =
arg max
θ∈{mito,cyto}
log Pθ(Xi|Xj, j ∈Ni).
(4.11)
Each shows the segmentation into two regions; white corresponds to mitochondria
and black corresponds to cytoplasm.
Example 4.6 (Hidden Markov Chains)
Hidden processes are very important as
they change the graph structures. Consider the hidden Markov model used exten-
sively in speech recognition in which there is a hidden state sequence X
=
(X1, X2, . . . , Xn) assumed generated by a Markov chain and a sequence of observables
Y
= (Y1, Y2, . . . , Yn) generated independently conditioned on the state sequence.
The probability of the state sequence and the probability of the observation sequence
conditioned on the states become
P(X) =
n

k=1
P(Xk|Xk−1),
P(Y) =
n

k=1
P(Yk|Xk).
(4.12)
The joint probability is then deﬁned as
P(X, Y) = P(X)P(Y|X) =
n

k=1
P(Xk|Xk−1)P(Yk|Xk).
If the state sequence is known, the neighborhood structure is simple, that is,
P(Yi|(X, Y)/Yi) = P(Yi|Xi)
and
P(Xi|(X, Y)/Xi)
= P(Xi|Xi−1, Xi+1, Yi).
Shown in the left panel of Figure 4.4 is the directed graph structure for the joint X, Y
process.
In general, however, the state sequence is hidden and only the observation
sequence is measured with marginal probability P(Y) = 
X P(X, Y) where the sum
is over all possible state sequences.
x1
x2
x3
x4
y1
y2
y3
y4
y1
y2
y3
y4
Figure 4.4 Left
panel
shows
the
directed
graph
structure
corresponding
to
the
joint
X, Y process. Right panel shows the completely connected undirected graph corresponding
to the marginal process Y only.

100
4 MARKOV RANDOM FIELDS ON UNDIRECTED GRAPHS
Under the marginal probability, the neighborhood structure is revealed by
considering
P(Yi|Y/Yi) =
P(Y)

Yi P(Y)
=

X1

X2 · · · 
Xn
$n
k=1 P(Xk|Xk−1)P(Yk|Xk)

Yi

X1

X2 · · · 
Xn
$n
k=1 P(Xk|Xk−1)P(Yk|Xk)
= P(Yi|Y1, . . . , Yi−1, Yi+1, . . . , Yn).
Shown in the right panel of Figure 4.4 is the graph structure for the marginal of
Y of the joint X, Y process, a fully connected graph.
Example 4.7 (Hidden Random Branching Processes)
Now consider a hidden
branching process model in which there is a branching process speciﬁed by a sequence
of rules T = (R1, R2, . . . , RN), Ri ∈R, a set of context-free rules, and a sequence of
observed words W = (W1, W2, . . . , Wn) generated conditioned on the preterminating
leaves of the tree γ1, . . . , γn derived by the rules above. The probability of the rule
sequence and the probability of the word sequence conditioned on the rule sequence is
P(T) =
N

j=1
P(Rj),
P(W|T) =
n

j=1
P(Wj|γj),
with the joint probability
P(W, T) = P(T)P(W|T) =
N

j=1
P(Rj)
n

k=1
P(Wk|γk).
If the underlying branching process T is known, the neighborhood structure is
P(Wi|(W, T)/Wi) = P(Wi|γi).
(4.13)
Shown in the left panel of Figure 4.5 is the directed graph structure for the marginal
on Y of the joint X, Y process.
If the underlying tree is not observed but only the word sequence, then the
marginal probability on the words W becomes, P(W) = 
T P(W, T) where the sum is
r1
r3
g1
w1
g2
w2
r2
w3
g3
w1
w2
w3
Figure 4.5 Left panel shows the directed graph branching process structure for the joint T = R, W
process. The γ1, γ2, . . . , γn denote the pre-terminal symbols. Right panel shows resulting completely
connected undirected graph corresponding to the marginal process on the word W.

4.2 MARKOV RANDOM FIELDS
101
over all possible trees. In the case of an unambiguous context-free grammar, such as for
programming languages, there is only one tree associated with each word sequence.
In general, however, context-free languages are ambiguous [84].
Under the marginal probability, the neighborhood structure is shown by
considering
P(Wi|W/Wi) =
P(W)

wi P(W)
=

t
$N
j=1 P(Rj) $n
k=1 P(Wk|γk)

wi

t
$N
j=1 P(Rj) $n
k=1 P(Wk|γk)
= P(Wi|W1, . . . , Wi−1, Wi+1, . . . , Wn).
So, the underlying graph structure is fully connected. Shown in the right panel
of Figure 4.5 is the graph structure for marginal process on the words W for the joint
T, W process, a fully connected graph.
4.3 Gibbs Random Fields
Now examine the equivalence between Gibbs distributions and Markov random ﬁelds as proved
ﬁrst for lattices in [129], and extended to arbitrary graphs in [130–132]. To connect Gibbs represen-
tations with probabilities of the form P(X) = e−U(X)/Z having U(X) the potential function to the
graph σ = {D, E} it must be established that the neighborhood and clique structure of the MRF
determines the form of the potential in the Gibbs representation. Proceeding ﬁrst descriptively, the
potential U : X →R will be constructed from locally deﬁned potentials which associates values
to every conﬁguration of sites XG, G ⊂D. In particular, the Gibbs probability on the entire graph
having potential U is
P(X) = e−U(X)
Z
where Z =

X∈X
e−U(X).
(4.14)
The exquisite connection between the probabilistic structure of the Gibbs ﬁeld with potential U(·)
and the picture of a graph with neighborhood and clique system is available. Most importantly, as
ﬁrst shown in [129-132], a Gibbs random ﬁeld with potential U(·) must involve all cliques in assign-
ing energy for the Gibbs distribution to have the neighborhood structure and local characteristics
of the MRF.
Deﬁnition 4.8
A Gibbs distribution is a probability distribution on X with
P(X) = 1
Ze−U(X)
with Z =

X∈X
e−U(X),
(4.15)
with the normalizing constant Z called the partition function and U : X →R the energy
function.
We shall say that P(X) is a Gibbs distribution respecting the graph σ = {D, E}
with neighborhood and cliques C ∈C if the potential U : X →R is of the form
U(X) =

C∈C
C(X)
with C(X) = C(XC),
(4.16)
where C : X →R depends only on those coordinates in C ⊂D.
Naturally this desire to have ﬁnite potentials links to the non-zero probabilities of Eqn. 4.4.

102
4 MARKOV RANDOM FIELDS ON UNDIRECTED GRAPHS
Theorem 4.9
X is an MRF with respect to graph σ = {D, E} with neighborhood and clique
structure N, C if and only if P(X) is a Gibbs distribution with respect to N, C.
Proof
If P(X), X = (X1, . . . , Xn) ∈X = X n
0 is a X0 = {0, 1, . . . , m −1}-valued Gibbs
distribution with respect to neighborhood and clique system N, C then
P(X) = 1
Ze−U(X),
Z =

X∈X n
0
e−U(X),
(4.17)
and U(X) is the energy function. Fix i ∈D and Xi, then by rules of conditional
probability
P(Xi|Xj, j ̸= i) =
P(X)

Xi∈X0 P(X) =
e−
C∈C C(X)

Xi∈X0 e−
C∈C C(X)
=
e−(

C∈C:i∈C C(X)+
C∈C:i/∈C C(X))

Xi∈X0 e−(

C∈C:i∈C C(X)+
C∈C:i/∈C C(X)) .
(4.18)
Notice the right-hand side of Eqn. 4.18 depends only on Xi and on Xj, where j ∈Ni,
since any site in a clique containing i must be a neighbor of i. Hence,
P(Xi|Xj, j ̸= i) = P(Xi|Xj, j ∈Ni).
(4.19)
Converse: Deﬁne x, = 0 to be an entire realization of 0s, and X(i) the realization
(X1, . . . , Xi−1, 0, Xi+1, . . . , Xn). Then with the assumption that P(0) > 0, deﬁne Q(X) =
log

P(X)/P(0)

. Introduce all subsets of X of the form
Ei,j,...,ir = {X ∈X|Xi > 0, Xj > 0, . . . , Xir > 0, all other Xi = 0},
r = 1, 2, . . . , n.
It is convenient to assume the ordering i < j < · · · < ir. Denote the indicator function
of Ei,j,...,ir by 1i,j,...,ir(·). In an E-set the x-components that are non-zero can be divided
out, and since the E-sets are disjoint and cover X this gives the identity
Q(X) =
n

r=1

i:i<j...ir
Q(X)1i<j...ir(X) =
n

r=1

i:i<j...ir
Xi · · · XirF(X),
(4.20)
where F(X) = (Q(X)/(Xi · · · Xir)). Reordering the variables gives
Q(X) =

1≤i≤n
XiFi(Xi) +

1≤i<

j≤n
XiXjFi,j(Xi, Xj) + · · ·
+ X1X2 . . . XnF1,2,···,n(X1, X2, . . . , Xn).
(4.21)
Since
exp

Q(X)−Q(X(i))

=
P(X)
P(X(i)) =
P(Xi|X1, . . . , Xi−1, Xi+1, . . . , Xn)
P(Xi = 0|X1, . . . , Xi−1, Xi+1, . . . , Xn), (4.22)
and this is a Markov random ﬁeld, the last equation suggests that exp

Q(X)−Q(X(i))

in Eqn. 4.22 can depend only on Xi and its neighborhood. Without loss of generality

4.3 GIBBS RANDOM FIELDS
103
assume i = 1, then
Q(X) −Q(X(1)) =X1

F1(X1) +

2≤j≤n
XjF1,j(X1, Xj)
+

s
2 ≤j <

k≤n
XjXkF1,j,k(X1, Xj, Xk)
+ X2X3 · · · XnF1,2,...,n(X1, X2, . . . , Xn)

.
Now suppose that site l ̸= 1 is not a neighbor of site 1. Then Q(X)−Q(X(1)) must
be independent of Xl for all X ∈X. Putting Xi = 0 for i ̸= 1 or l gives F1,l(X1, Xl) = 0
on X. Similarly by other suitable choices of X it is deduced that all 3−, 4−, . . . , n−
variable F-functions involving X1 and Xl must be null. The analogous result holds
for any pair of sites which are not neighbors of each other and hence, in general,
Fi,j,...,s can only be non-null if the sites i, j, . . . , s form a clique. Thus, Q(X) has the form
Q(X) = 
C∈C C(X) and is hence Gibbsian.
Given the global Gibbs probability it is straightforward to calculate the local conditionals of
the MRF.
Example 4.10 (The Ising Model on Regular Lattices)
The Ising model is a cele-
brated example. Consider an n × n lattice of sites, D = {1 ≤i, j ≤n}. At each
site place a dipole, “up” or “down” element of the state space X0 = {+1, −1}, with
xij ∈{+1, −1}, (i, j) ∈D, with conﬁguration space X = {+1, −1}n2 of n2 dipole orien-
tations. Shown in the left panel of Figure 4.6 is such a conﬁguration of dipoles. Ising’s
probability on X is constructed from the local potential
ij(X) = −JXi,j(Xi,j−1 + Xi−1,j) −mHXij.
(4.23)
Then to each conﬁguration X ∈X n2
0
assign the potential
U(X) = −J

1≤i,j≤n
Xi,j(Xi,j−1 + Xi−1,j) −mH

1≤i,j≤n
Xi,j.
(4.24)
with J > 0 this is termed the attractive case encouraging neighboring spins to
be identically aligned. H encourages spins to be in the same direction associated
Figure 4.6 Left panel shows a 2D conﬁguration of dipoles; right panel shows the graph structure
for the Ising model.

104
4 MARKOV RANDOM FIELDS ON UNDIRECTED GRAPHS
with an external ﬁeld resulting in net magnetization. The probability assigned to a
conﬁguration becomes
P(X) = 1
Ze
−U(X)
KT
,
where Z =

X∈X
e
−U(X)
KT
.
(4.25)
Notice the ij for internal sites are a function of only sites in the cliques of Xij:
{(ij), (i + 1, j)} ∪{(ij), (i −1, j)} ∪{(ij), (i, j + 1)} ∪{(ij), (i, j −1)} ∪{(ij)}.
(4.26)
The neighborhood structure for internal sites is straightforwardly calculated
using the conditional probabilities
P(Xij|Xkl, (kl) ̸= (ij)) =
P(X)

Xij∈{−1,+1} P(X)
=
e−JXij(Xi+1,j+Xi−1,j+Xi,j+1+Xi,j−1)

Xi,j∈{−1,+1} e−JXij(Xi+1,j+Xi−1,j+Xi,j+1+Xi,j−1) .
(4.27)
4.4 The Splitting Property of Gibbs Distributions
The crucial aspect of examining Gibbs distribution is of course understanding the role of the
boundary in determining probabilities of event on cylinders in the interior. This is of course the
signiﬁcant departure in multiple dimensions from the 1D Markov process case.
For this take a more careful look at Gibbs distributions on ﬁnite state spaces associated with
inﬁnite lattices. Choose a regular lattice D = Zd with state space X = X Zd
0 , X taking values of
d-dimensional array of elements from X0. As before, for G ⊂Zd, take the conﬁguration associated
with the subset of sites G ⊂Zd as XG = {Xi, i ∈G}.
For the stationary setting the full Gibbs potential is constructed by shifting the local 
potential around the subgraphs of G ⊂Zd, with interactions at the boundaries. The potentials will
have ﬁnite support; we quantify the range of support of the Gibbs potential as R as follows.
Deﬁnition 4.11
Deﬁne the support set S ⊂Zd positioned to the left of the origin of size Rd:
S ⊂{i ∈Zd : (−R + 1, −R + 1, . . . ) ≤(i1, i2, . . . ) ≤(0, 0, . . . )}.
(4.28)
Then R is deﬁned to be the range of the interaction if the potential is constructed from
locally XS supported functions, and  : X →R having the property that  depends only on
the random subconﬁguration XS, so that (X) = (XS).
Given set G, then deﬁne its interior G0 = {i ∈G ⊂Zd : (i + S) ⊂G}, its
closure ¯G = ∪i∈G(i + S), and interior boundary ∂G = G/G0 and exterior boundary
∂¯G = ¯G/ ¯G0.
Examine Figure 4.7. Notice the interior boundary deﬁned ∂G here is the internal
boundary including only points inside the set.
The conﬁguration determining the potential associated with the ﬁnite set G with
the inﬁnitely extended boundary conﬁguration y ∈X is deﬁned to be XG ∨y:
(XG ∨y)i =
 Xi
i ∈G
yi
i /∈G .
(4.29)
Examine distributions generated from ﬁxed boundaries.

4.4 THE SPLITTING PROPERTY OF GIBBS DISTRIBUTIONS
105
G
i
i +S
S
G
G0
–
Figure 4.7 Showing the interior and closure of the sets G0, ¯G. Double bold line denotes the set G,
with G0 the interior square, and ¯G the exterior square.
Deﬁnition 4.12
Deﬁne τ the shift operator so that τi(XG), i ∈G is the conﬁguration
obtained by shifting the coordinates to the origin:
τi(XG) = {Xi′ : i′ = j −i, j ∈G}.
(4.30)
The ﬁnite volume Gibbs distribution P(XG|y) with interaction  and ﬁxed
boundary y on ﬁnite cylinders XG is deﬁned to be
P(XG|y) =e−Uy
G(XG)
Zy
G
,
where Uy
G(XG) =

i∈G0
(τi(XG)) +

i∈∂G
(τi(XG ∨y)),
(4.31)
with the normalizer Zy
G = 
XG∈X |G|
0
e−Uy
G(XG).
In the context of conditional probability, the range R of the local support of  can be given
a precise interpretation as the critical distance17 between subsets G, G′ so that events on these
subsets are conditionally independent.
This provides the basic splitting property for Markov random ﬁelds, split by the external
boundaries. ∂¯G, ∂¯G′.
Theorem 4.13
Let P be constructed from locally supported potentials (x) = (xS) with
range R, then the probabilities on sets G, G′ ⊂Zd with d(G, G′) ≥R are split by conditioning
on the boundary:
P(XG, XG′|X(G∪G′)c) = P(XG|X∂¯G)P(XG′|X∂¯G′)
(4.32)
in particular, the probability of a conﬁguration on set G is determined by its external boundary:
P(XG|XGc) = P(XG|X∂¯G).
(4.33)
17 The distance between sets G, G′ ⊂Zd is deﬁned as the minimum distance between coordinates: d(G, G′) =
mind
k=1 mini∈G,j∈G′ |ik −jk|.

106
4 MARKOV RANDOM FIELDS ON UNDIRECTED GRAPHS
Proof
By deﬁnition
P(XG, XG′|X(G∪G′)c) =
1
Z
X(G∪G′)c
G∪G′
exp −U
X(G∪G′)c
G∪G′
(XG, XG′)
(4.34)
=
1
Z
X(G∪G′)c
G∪G′
exp

−

i∈(G∪G′)0
(τi(XG∪G′))
−

i∈∂(G∪G′)
(τi(X(G∪G′) ∨X(G∪G′)c))

.
Since d(G, G′) > R then for all i ∈∂(G ∪G′), then either
(i + S) ∩G = ∅
if i ∈∂G′
(i + S) ∩G′ = ∅
if i ∈∂G ,
(4.35)
implying the separation of the potentials:

i∈(G∪G′)0
(τi(X(G∪G′))) =

i∈G0
(τi(XG)) +

i∈G′o
(τi(XG′)),

i∈∂(G∪G′)
(τi(X(G∪G′) ∨X(G∪G′)c)) =

i∈∂G
(τi(XG ∨X(G∪G′)c))
+

i∈∂G′
(τi(XG′ ∨X(G∪G′)c)).
Substituting into 4.34 gives
P(XG,XG′|X(G∪G′)c)
=
1
Z
X(G∪G′)c
G∪G′
exp

−

i∈G0
(τi(XG)) −

i∈∂G
(τi(XG ∨X(G∪G′)c))


exp

−

i∈G′0
(τi(XG′)) −

i∈∂G′
(τi(XG′ ∨X(G∪G′)c))

.
(4.36)
The ﬁrst multiplier terms are only a function i ∈¯G, the second term a function i ∈¯G′.
Therefore, the partition function factors as ZX ¯G
G ZX ¯G′
G′
completing the proof giving the
factorization and the splitting property of Eqn. 4.32:
P(XG|X(G∪G′)c) = P(XG|X∂¯G),
P(XG′|X(G∪G′)c) = P(XG′|X∂¯G′).
(4.37)
Corollary 4.14
The Markov chain splitting takes the form for Markov random ﬁelds
P(XG, XG′|X∂¯G) = P(XG|X∂¯G)P(XG′|X∂¯G).
(4.38)

4.4 THE SPLITTING PROPERTY OF GIBBS DISTRIBUTIONS
107
Proof
To see this form of the splitting property
P(XG|XG′, X∂¯G) =
exp

−U(XG′,X∂¯G)
G
(XG)
	
Z(XG′,X∂¯G)
G
=
exp

−
i∈G0 (τi(XG)) −
i∈∂G (τi(XG ∨(XG′, X∂¯G)))
	
Z(XG′,X∂¯G)
G
=
exp

−UX∂¯G
G
	
ZX∂¯G
G
.
(4.39)
This gives the splitting property of Eqn. 4.38 according to
P(XG, XG′|X∂¯G) = P(XG|XG′, X∂¯G)P(XG′|X∂¯G) (a)
= P(XG|X∂¯G)P(XG′|X∂¯G),
(4.40)
with the ﬁnal equality (a) following from Eqn. 4.39 above.
The identical argument gives P(XG, XG′|X∂¯G′) = P(XG|X∂¯G′)P(XG′|X∂¯G′).
Example 4.15 (Markov chains)
Choose D = {0 ≤i ≤n}, then the 1-memory Markov
chain is a range R = 2 random ﬁeld. The energy U : X →R is constructed from
S = {−1, 0} with (X) = (X−1, X0), (τi(X)) = (Xi−1, Xi).
The m-memory Markov chain is a range R = m + 1 random ﬁeld; the energy
U : X →R constructed from locally S = {−m, . . . , 0} supported functions
(X) = (X−m, X−m+1, . . . , X0),
(τi(X)) = (Xi−m, Xi−m+1, . . . , Xi).
(4.41)
The open sets and closed sets become
D0 = {m, . . . , n},
¯D = {−m, . . . , 0, . . . , n},
(4.42)
with internal boundary ∂D0 = {0, . . . , m −1}, ∂¯D = {−m, . . . , −1}.
Example 4.16 (Segmentation via the Ising Model [133])
The Ising model has been
studied extensively, not only for the description of magnetic spins but also in image
analysis for capturing low-level aggregation features in segmentation. As proposed
by Geman and Geman [122], the Ising model can be used for segmentation to treat the
case where the image has 2 parts, one light and one darker, so that the segmentation
variables will be +1 on the light part, and −1 on the darker. An example is illustrated
in Figure 4.8 from Mumford [133].
Take the graph to be on D = {1 ≤i, j ≤m} the m × m square-lattice with
two layers, the bottom layer observable random variables Yij ∈R, 1 ≤i, j ≤m
associated to the image measurement, and the top layer hidden random variable
Xij ∈{+1, −1} associated to the segmentation. Connect by edges each Yi,j vertex to
the Xij vertex above and to Xi,j its 4 neighbors Xi±1,j, Xi,j±1 in the X−graph (adjusted
to the boundary) and no others. The cliques are just the pairs of vertices connected by
edges. The clique potentials become
(1)
ij (X) = Xi,j · Xi′,j′
for (i, j), (i′, j′) for two adjacent vertices,
(4.43)
(2)
ij (X) = Yi,j · Xi,j
for (i, j).
(4.44)

108
4 MARKOV RANDOM FIELDS ON UNDIRECTED GRAPHS
Figure 4.8 Figure taken from Mumford [133] showing the solutions from the Ising model
following the cooling schedule.
The potentials become
U(X, Y) = α

i,j
Xi,j(Xi−1,j + Xi,j−1) +

ij
Yi,jXi,j,
(4.45)
with probability P(X, Y) = 1
Ze−U(X,Y). Notice the observed image Y is being applied
as the external ﬁeld locally to each site X of the hidden ﬁeld.
The modes of P(X|Y) seek to make adjacent X vertices equal, and have the same
sign as the corresponding Ys. These are in conﬂict with rapidly varying sign changes
of the external image ﬁeld. The probable values of X will align with the large areas
where Y is consistently of one sign.
Shown in Figure 4.8 are results from Mumford [133] showing the solutions from
the Ising model. Panel 1 shows the original image which was scaled to make the dark
and light areas have opposite signs. Panel 2 shows the ﬁnal segmentation resulting
from the sequence of Ising segmentations. Panels 3–8 show successive realizations
representing local minimum from the potential P(X, Y)1/T, where T = temperature
in a cooling schedule. Panel 3 shows T large, and panels 4–8 show the realizations
coming from successive decreasing of T on the probability law. The decreasing T
cooling schedule forces the probability to concentrate on fewer and fewer realizations
each occupying higher and higher probability.
Example 4.17 (Hierarchical Fields via Line Processes)
We have seen that hidden
processes change the graph structures. As emphasized by Geman and Geman [122],
this same notion holds for the regular lattice random ﬁeld case. Deﬁne the n × n pixel
lattice D(1) = {1 ≤i, j ≤n} and “dual” lattice D(2) = {1 ≤i, j ≤n}, the dual
sites placed midway between each vertical or horizontal pair of pixels. The dual lat-
tice represents the possible locations of edge elements. Associate with each lattice site
the joint MRF (X, L) consisting of the pixel intensity ﬁeld X = {Xi,j, 1 ≤i, j ≤n}
with eight gray level values and the line site ﬁeld L = {Li,j, 1 ≤i, j ≤n} which is
{0, 1} valued denoting the existence or non-existence of an edge at site (i, j). Associate
nearest neighborhoods with the pixel lattice and dual lattice, respectively, N (1) =
∪(i,j)∈D(1)N(1)
i
, N (2) = ∪(i,j)∈D(2) N(2)
i,j , as depicted in Figure 4.9. The neighborhood

4.4 THE SPLITTING PROPERTY OF GIBBS DISTRIBUTIONS
109
Figure 4.9 Left panel shows pixel sites marked with circles and edge sites marked with crosses;
right panel shows the dual lattice with edges and pixels.
system is constructed from the basic neighborhoods for the k = 1, 2 lattices:
N(k)
i,j = {(i −1, j)(k), (i + 1, j)(k), (i, j −1)(k), (i, j + 1)(k)} ⊂D(k).
(4.46)
The set of cliques becomes
C(1) = {(i, j)(1)},
C(2) = {((i, j)(1), (i + 1, j)(1)), ((i, j)(1), (i, j + 1)(1))},
(4.47)
C(3) = {((i, j)(1), (i −1, j)(2)), ((i, j)(1), (i + 1, j)(2)), ((i, j)(1), (i, j −1)(2)),
× ((i, j)(1), (i, j + 1)(2))}
with the potential
U(X) = α

i,j
Xi,j(Xi+1,j + Xi,j+1)
+ β

i,j
Xi,j(Li−1,j + Li+1,j + Li,j−1 + Li,j+1) + γ

i,j
Xi,j.
(4.48)
The joint probability takes the form
P(X, L) = 1
Zeα 
i,j Xi,j(Xi+1,j+Xi,j+1)eβ 
i,j Xi,j(Li−1,j+Li+1,j+Li,j−1+Li,j+1) eγ 
i,j Xi,j,
(4.49)
with L ∈{0, 1}m2 taking 0, 1-values corresponding to the absence or presence of an
edge.
Marginalizing with respect to X = x corresponds to summing the density over
all values of line sites. Note that summation of a term of the form exp(xl) over all
values of l gives (1 + exp(x)). Hence we have to only sum the exponent term in the
probability density which contains y. The other two exponents factor out. Summation
over all l gives

li,j,1≤i,j≤n
e(β 
i,j xi,j(li−1,j+li+1,j+li,j−1+li,j+1))
=

i,j
(1 + exi−1,j+xi,j)(1 + exi+1,j+xi,j)(1 + exi,j−1+xi,j) (1 + exi,j+1+xi,j).

110
4 MARKOV RANDOM FIELDS ON UNDIRECTED GRAPHS
Hence the marginal density has the form
P(Xi,j, 1 ≤i, j ≤n)
= 1
Z


i,j
{(1 + eXi−1,j+Xi,j)(1 + eXi+1,j+Xi,j)(1 + eXi,j−1+Xi,j)(1 + eXi,j+1+Xi,j)}
× e(γ 
i,j Xi,j)e(α 
i,j Xi,j(Xi+1,j+Xi,j+1))

.
Note the density is no longer of Gibbs form with local neighborhood structure.
In fact it is a sum of exponents now. If we take the conditional expectation with respect
to a particular lattice site the density instead of having a local neighborhood structure
shows a fully connected graph.
4.5 Bayesian Texture Segmentation: The log-Normalizer Problem
Bayesian approaches have enjoyed increasing prominence throughout the spectrum of com-
puter vision paradigms, ranging from traditional bottom-up “early vision” algorithms to systems
exploiting high-level deformable shape models [134–136]. Statistical characterizations of the avail-
able data are fundamental to any Bayesian approach. In ideal cases, different objects may be
effectively identiﬁed by their intensities; more commonly, they will be differentiated via their tex-
tures. A tremendous amount of research has been performed and published in the area of texture
analysis, much of which seeks to characterize the relationships between pixels in a textured object.
Clearly, in the independent pixel models, the basic object being modeled is the pixel. Under such
circumstances the log-normalizer will present no difﬁculty. However, for textures of extended
objects having interiors forming a disjoint partition of the image this changes dramatically.
Model the interiors of the shapes of real-valued images as scalar random ﬁelds deﬁned
on the index sets of integer lattices. We take the viewpoint of Bayesian testing calculating the
likelihood of shapes from Gaussian and Gibbs ﬁelds. Because of splitting, the partitions are con-
ditionally independent given their boundaries. The goal of taking a Bayesian approach presents a
substantial conceptual hurdle. Since the normalizing partition function must be calculated for each
partition D(m) ⊂Zd, d = 2, 3, m = 1, . . . , M the number of random ﬁelds depending on the bound-
aries of the random shapes. To avoid the difﬁculty with the partition function, Besag suggested
replacing the likelihood with a pseudolikelihood, a product of conditional probabilities, yielding a
modiﬁed Bayesian paradigm as described in [137]. In this chapter we examine direct assaults on
the normalization constants using asymptotics thus providing a purely Bayesian formulation.
4.5.1 The Gibbs Partition Function Problem
To illustrate, examine the goal of understanding the partition function over randomly shaped
objects, roads, lakes, etc. Model their interiors as scalar real-valued random ﬁelds. The Bayesian
probability of accepting one shape over another is obtained by evaluating the likelihood of a par-
ticular disjoint partition of the scene, each region a random ﬁeld. Conditioned on the boundaries,
the subregion Markov ﬁelds are assumed independent.

4.5 BAYESIAN TEXTURE SEGMENTATION: THE LOG-NORMALIZER PROBLEM
111
For the Gibbs distribution let X on D have state space X |D|
0
, taking the form
P(X) = e−U(X)
ZD
,
where ZD =

x∈X |D|
0
e−U(x).
(4.50)
Choosing model type α from a group of Gibbs ﬁelds with potentials Uα requires computation of
log Zα
D:
ˆα ←arg max
α
−Uα(x) −log Zα
D.
(4.51)
Notice the role the partition function plays. Clearly, if the region corresponds to the interior of an
unknown shape which is to be inferred and is evolving during the inference procedure, calculation
of the partition function over the sub-graph becomes impossible.
Nicely enough, the normalized partition functions are relatively independent of the bound-
ary condition as proved in [138, 139]. This is essentially the same as the convergence of the
log-determinant of the covariance for Gaussian ﬁelds in the large graph limit which we show
in the next chapter (see Corollary 5.21, Chapter 5).
Theorem 4.18
Let X correspond to a Gibbs distribution P with potential U =

i  ◦τi. Then there exists a constant c depending on the range R and the interaction
∥∥= supx∈X (x) such that for any D ⊂Zd, the normalized log-partition functions have
the following boundary to volume dependence:
1
|D|
log
Zy
D
Zz
D
 ≤c|∂D|
|D| .
(4.52)
The distributions under the various boundary conditions have the form
1
|D|
log
Py
D
Pz
D
 ≤c|∂D|
|D| ,
1
|D|
log PD
Pz
D
 ≤c|∂D|
|D| .
(4.53)
Proof
Begin with the inequality |Uy
D −Uz
D)| ≤2∥∥|∂D| on the arbitrary boundaries
y, z ∈X Zd/D
0
giving
e−2∥∥|∂D| ≤e−Uy
D
e−Uz
D
≤e+2∥∥|∂D|;
(4.54)
e−Uz
De−2∥∥|∂D| ≤e−Uy
D ≤e−Uz
De+2∥∥|∂D|.
(4.55)
Using Eqn. 4.55 gives
Zy
D =

xD
e−Uy
D(xD) (a)
≤e+2∥∥|∂D| 
xD
e−Uz
D(xD) = e+2∥∥|∂D|Zz
D
(4.56)
=

xD
e−Uy
D(xD) (b)
≥e−2∥∥|∂D|Zz
D,
(4.57)
where (a) follows from R.H.S. Eqn. 4.55 and (b) follows from L.H.S. Eqn. 4.55. This
gives | log(Zy
D)/(Zz
D)| ≤2∥∥|∂D|, completing the ﬁrst part of the Theorem with the
constant c = 2∥∥.
From this,
e−Uy
D
Zy
D
(a)
≤e−Uz
De2∥∥|∂D|
Zy
D
(b)
≤e−Uz
De2∥∥|∂D|
Zz
De−2∥∥|∂D| = Pz
De4∥∥|∂D|,
(4.58)

112
4 MARKOV RANDOM FIELDS ON UNDIRECTED GRAPHS
where (a) follows from the R.H.S. Eqn. 4.55 and (b) follows from Eqn. 4.57. This gives
the second part
log
Py
D
Pz
D
 ≤4∥∥|∂D|.
(4.59)
To ﬁnish, then
Pz
De−4∥∥|∂D| ≤Py
D ≤Pz
De4∥∥|∂D|,
(4.60)
Pz
De−4∥∥|∂D| ≤E{Py
D} = PD ≤Pz
De4∥∥|∂D|
(4.61)
giving | log(PD/Pz
D)| ≤4∥∥|∂D|, completing the proof.
4.6 Maximum-Entropy Texture Representation
Markov Random ﬁeld models are all maximum entropy models (see Chapter 2, Section 2.9). As
discussed in Section 2.9 maximum-entropy techniques provide methods for constructing repre-
sentations of the world. As Jaynes [63] eloquently formulated, given empirical observations, the
maximum-entropy model occurs with greater multiplicity than any other choice. Zhu and Mum-
ford have exploited this for texture representation in natural imagery. They address the following
problem in pattern representation and texture modeling: Assume there exists a joint probability
density pX(·) on X the space of images X ∈X with X = Xi, i ∈D the discrete graph D, and the
objective is to estimate p(·) via some density q(·). Assume throughout that the only direct obser-
vation of p(·) is through realizations of it, or samples x(s) = {x(s)
i , i ∈D}, s = 1, . . . , S the size of
the empirical sample. The basic strategy is to construct ˆp(·) so that it reproduces features of the
empirical samples. Deﬁning the M-features hm(X), m = 1, . . . , M, then the model ˆp(·) approximat-
ing p(·) is constructed so that the expectation under the approximating model equals the empirical
average of the feature under the sample set. Then the maximum entropy density takes the familiar
form from Jaynes theorem on moment constraints.
How should the features be chosen? Zhu et al. [140] exploit the fact that probability densities
can be represented through their marginals. This is given by the following theorem characterizing
any smooth density via its characteristic functional.
Theorem 4.19
Let X = X1, . . . , Xn associated with discrete lattice D of size |D| = n have
joint density pX(·) on X = Rn. Then given ﬁlters fi, i = 1, . . . , n and ﬁltered versions of the
process,
Xf =
n

i=1
Xifi
(4.62)
have marginal densities
pXf (r) = E{δ(r −Xf )},
r ∈R,
which determine pX(·).
Proof
Let |D| = n, then relate pX(·) to its Fourier transform ˆpX(·) according to
pX(x1, . . . , xn) =

Rn ej2π 
i=1n xifi ˆpX(f1, . . . , fn) df
(4.63)

4.6 MAXIMUM-ENTROPY TEXTURE REPRESENTATION
113
with
ˆpX(f1, . . . , fn) =

Rn e−j2π 
n
i=1 xifipX(x1, . . . , xn) dx
(4.64)
=

R
e−j2πr dr

Rn δ(r −
n

i=1
xifi)pX(x1, . . . , xn) dx
(4.65)
=

R
e−j2πrpXf (r) dr,
(4.66)
where pXf (r) =

Rn δ(r −
n

i=1
xifi)pX(x1, . . . , xn) dx
(4.67)
= E{δ(r −Xf )}.
(4.68)
Zhu takes these ﬁltered versions as the empirical test functions in the maximum
entropy formalism.
Theorem 4.20
Given are moment constraints on marginal densities of ﬁltered versions of
the process:
Hm(r) = E{δ(r −Xf(m))},
m = 1, . . . M, r ∈R
(4.69)
=

Rn δ(r −
n

i=1
xif(m)
i
)pX(x1, . . . , xn) dx.
(4.70)
Then the density maximizing entropy with moment equalling the observation functions takes
the form
pX(x1, . . . , xn) =
1
Zm()e−
M
m=1 λ(m)(
n
i=1 xif(m)
i
),
(4.71)
where λ(m)(r), r ∈R are real-valued functions.
4.6.1 Empirical Maximum Entropy Texture Coding
For constructing texture ﬁelds Zhu exploits ergodic properties of stationary random ﬁelds to model
textures from individual realizations by generating empirical averages of the marginal densities
from realizations of the single pictures. For this, given the random process XD = {xi, i ∈D} a
realization of the stationary random ﬁeld X on the inﬁnite lattice, with D ⊂Zk assumed large.
Then take as the empirical estimator ˆHm(r) of Hm(r) = E{δ(r −Xf(m))} the function
ˆHm(r) =
1
|D|

j∈D
δ

r −

i∈D
xi+jf(m)
i

.
(4.72)
While this is a powerful theorem demonstrating the characterization of processes through
ﬁltered versions, it requires knowledge of an inﬁnite number of such ﬁlter functions to completely
specify the inverse transform. For texture representations in visual coding Zhu et al. exploit the fact
that small numbers of selective ﬁltering functions may be adequate for explaining large numbers

114
4 MARKOV RANDOM FIELDS ON UNDIRECTED GRAPHS
(1)
(2)
(3)
(4)
(5)
(6)
Figure 4.10 Panel 1 shows animal fur texture, panel 2 shows white noise, panel 3 shows maximum-
entropy with 5 pixel Laplacian of a Gaussian ﬁlter f1, panel 4 shows added ﬁlters f6,120◦, f2,30◦,
then panel 5 adding the ﬁlter f12,60◦, and panel 6 adds the ﬁlter f10,120◦and DC component.
of textures in the world. Examples of ﬁlters include the frequency and orientation selective Gabor
ﬁlters [141] modeling cells in the mammalian visual cortex. Zhu et al. deﬁne a general ﬁlter bank
including (i) a DC component f(·) = δ(·), (ii) the Laplacian of Gaussian ﬁlters which are isotropic
center-surrounded and are used to model retinal cells with impulse response functions
fσ (x, y) ∝(x2 + y2 −σ 2)e−(x2+y2)/2σ 2,
(4.73)
where σ controls the scales of the ﬁlters, and (iii) the Gabor ﬁlters modeling frequency and
orientation sensitivity:
fσ,θ(x, y) ∝e−1/(2σ 2)4(x cos θ+y sin θ)2+(−x sin θ+y cos θ)2e−j(2π/σ)(x cos θ+y sin θ),
(4.74)
where σ controls the scale and θ controls the orientation.
Examine texture ﬁelds from Zhu et al. Figure 4.10 shows various examples of animal fur
and its synthesis. Panel 1 shows the fur texture. Panel 2 shows a white noise ﬁeld. Panel 3 shows
a texture ﬁeld synthesized from the maximum entropy distribution resulting from the ﬁve pixel
Laplacian of a Gaussian ﬁlter f1 with scale σ = 1.0. Panels 4, 5 and 6 show synthesized textures with
the dc component added and the Gabor ﬁlters added, panel 4 adding two f6,120◦, f2,30◦, then panel
5 adding the ﬁlter f12,60◦, and panel 6 adding f10,120◦. With more ﬁlters added, the synthesized
texture image moves closer to the observed one.
When model complexity is ﬁxed, the ﬁlters can be chosen based on the MDLprinciple. Given
a maximum entropy distribution pX(x1, . . . , xn), which meets moment constraints on marginal
densities of ﬁltered versions of the process, the goodness of this distribution is given by the
Kullback–Leibler distance between the true density f(x1, . . . , xn) and pX(x1, . . . , xn):
D(f∥pX) = Ef(x1,...,xn) log f(x1, . . . , xn) −Ef(x1,...,xn) log pX(x1, . . . , xn).
(4.75)
Thus, minimizing the Kullback–Leibler distance between f(x1, . . . , xn) and pX(x1, . . . , xn) is
equivalent to minimizing −Ef(x1,...,xn) log pX(x1, . . . , xn) which is the expected coding length of the
data. In other words, one should make use of all the information to specify pX(x1, . . . , xn). Let F be

4.6 MAXIMUM-ENTROPY TEXTURE REPRESENTATION
115
(1)
(2)
(3)
(4)
(5)
(6)
(7)
Figure 4.11 Panel 1: observed texture of mud. Panels 2–7 show synthesized versions from Zhu
model.
(1)
(2)
(3)
(4)
(5)
(6)
(7)
Figure 4.12 Panel 1: observed texture of fabric. Panels 2–7 show synthesized versions from Zhu
model.
the set of all possible ﬁlters. Then the optimal set F∗(K) of K ﬁlters should be chosen according to
F∗(K) = arg min
|F(K)|=K
−Ef(x1,...,xn) log pX(x1, . . . , xn).
(4.76)
Figure 4.11 shows an example of mud texture and the results of its synthesis. Panel 2 shows a
white noise ﬁeld. Panel 3 shows a texture ﬁeld synthesized from the maximum entropy distribution
resulting from adding the ﬁlter f8,30. Panels 4, 5, 6, 7 and 8 show synthesized textures with the
ﬁlters cosine component of f7,90, Laplacian of a Gaussian ﬁlter f2, Laplacian ﬁlter with mask size
5x5, cosine component of the Gabor ﬁlter f3,90 and cosine component of f3,0, respectively.
Figure 4.12 shows similar results for fabric textures.

116
4 MARKOV RANDOM FIELDS ON UNDIRECTED GRAPHS
4.7 Stationary Gibbs Random Fields
Thus far the speciﬁcation of the Gibbs ﬁelds has been in terms of an assumed boundary; the
probabilities on ﬁnite graphs G ⊂Zd are determined by the value of the ﬁeld on the boundary of
the graph. Now deﬁne the underlying Gibbs distribution P associated with the inﬁnite lattice Zd.
This is far more than just an interesting theoretical indulgence. If we hope to be able to understand
what is happening “in the large graph limit,” that is for large patterns understanding the inﬁnite
distribution and its properties, the entropy and partition function will determine the properties of
the ﬁnite graph patterns.
Let D ⊂Zd, with X deﬁned on D with state space X0. The Gibbs distribution takes the form
P(X) = e−U(X)
ZD
,
where ZD =

x∈X |D|
0
e−U(x).
(4.77)
The partition function is determined by values of the ﬁelds on the boundary. Similar issues apply
as above for the Gaussian case. The probabilities on ﬁnite graphs D ⊂Zd are determined by the
value of the ﬁeld on the boundary ∂D. Fortunately, in the large graph limit there is a diminishing
effect of the boundary on the partition function.
4.7.1 The Dobrushin/Lanford/Ruelle Deﬁnition
Deﬁne the underlying Gibbs distribution P associated with the inﬁnite lattice Zd. To deﬁne proba-
bilities on the cylinders for any cylinder, the inﬁnite distribution must be deﬁned consistently with
the ﬁnite cylinder distributions of the same potentials. The most natural deﬁnition is to construct
it from the ﬁxed boundary distributions of the same potentials:
PD(xD) = EPY{E{1xD(XD)|Y}}.
(4.78)
Deﬁnition 4.21 (Dobrushin, Lanford, Ruelle)
Then P is the Gibbs distribution cor-
responding to interaction  on Zd if for any D ⊂Zd the marginal PD of P on D is
given by
PD(XD) =

y∈X Zd/D
0
PD(XD|y)PZd/D(y).
(4.79)
Denote the set of stationary Gibbs probabilities with interaction  as G
s .
This Eqn. 4.79 is known as the Dobrushin, Lanford, and Ruelle equation. One of the remark-
able things about Gibbs distributions, which is unlike irreducible Markov chains, is that there may
be multiple distributions in G
s with precisely the same interactions ; the set may contain more than
one element. In particular, the deﬁnition implies
PD(XD|yZd/D) = PD(XD|y∂¯D),
for all P ∈G
s .
(4.80)
All of the Gibbs distributions with the same interaction attach the same conditional probabilities
to ﬁnite sets [120].

4.7 STATIONARY GIBBS RANDOM FIELDS
117
4.7.2 Gibbs Distributions Exhibit Multiple Laws with the Same Interactions
(Phase Transitions): The Ising Model at Low Temperature
Thus far, as we have deﬁned it, the ﬁnite volume Gibbs distributions are determined by their
boundaries. Perhaps the most critical difference between 1D random ﬁelds which are essentially
Markov chains and two or higher dimensional ﬁelds is the fact that on the inﬁnite lattice there can
be multiple Gibbs distributions with exactly the same local conditionals. We would be delinquent
not to highlight this fundamental departure. Since this is an advanced topic, we do it by illustrating
the issue on the Ising model.
Examine the proof originally constructed by Peierls [142] following Kinderman and Snell
[123] showing that the 2D Ising model does have a phase transition—that is, it exhibits spontaneous
magnetization at sufﬁciently low temperatures. Examine the pure +1 boundary, setting Xi = +1
on the boundary—then letting the boundary move off to inﬁnity, or equivalently, letting the size
of the lattice go to inﬁnity. For low temperatures the probability that X0 = +1 (or−1) for any site
O is no longer 1
2 but dependent on the boundary condition, no matter how far away the site O is
from the boundary. In other words, the boundary condition has a strong effect on internal sites if
the temperature is sufﬁciently low. Recall that in the 2D Ising model, the probability of being in a
particular conﬁguration X = (X1, . . . , XN) is given by
P(X) = 1
Ze1/KT
J

⟨i,j⟩
XiXj + mH

i
Xi
	
,
(4.81)
with H the strength of the external ﬁeld, J > 0, m > 0 are weights of the internal and exter-
nal energies, with K Boltzmann’s constant and T the temperature. The ﬁrst summation is taken
over all pairs of sites that are connected by a bond (i.e. nearest neighbors), and the second sum-
mation is over all sites in the lattice. Deﬁning the number of even bonds ne(X) = XiXj > 0,
and the number of odd bonds no(X) = XiXj < 0 and nb = ne + no, with H = 0 Eqn. 4.81 is
simpliﬁed to
P(X) = 1
ZeJ/KT

ne(X)−no(X)
	
= 1
ZeJ/KT

nb(X)−2no(X)
	
(4.82)
= 1
Z′ e−bno(X).
(4.83)
Z′ is a new normalization factor and b = 2J/KT. In the following, we consider a positive boundary
condition, and show that at low enough temperatures Pr{X0 = −1} < 1
2 and Pr{X0 = +1} >
Pr{X0 = −1}. Similar reasoning shows that Pr{X0 = +1} < Pr{X0 = −1} under a negative
boundary condition. We consider O to be in the middle of the lattice, that is, the site deepest in
the interior. The left panel of Figure 4.13 illustrates one conﬁguration that site O takes a value of
−1. In the ﬁgure, we have drawn lines separating sites of opposite signs, or odd bonds. Because
of the boundary conditions, the lines must form closed curves. It is clear that the number of odd
bonds no(X) is just the total length of all the curves in Figure 4.13. This was the essential device
introduced by Peierls to study the probability distribution. Since X0 = −1, there must be one
closed curve that encloses O. Let’s call this curve a circuit and denote it by S, and say that its
length is L(S). Suppose we ﬁx S, and consider the set S of conﬁgurations having S as the circuit
that encloses O. Then for any conﬁguration ¯X ∈S, we associate a new conﬁguration X′ which
agrees with ¯X except that all sites inside S are changed to +1. This has the effect of removing the
circuit S and decreasing the number of odd bonds by L(S). That is, no(X′) = no( ¯X) −L(S). The

118
4 MARKOV RANDOM FIELDS ON UNDIRECTED GRAPHS
L
S
L
+
–
–
–
–
–
–
–
–
–
–
–
–
–
–
–
–
–
–
–
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
Figure 4.13 Left panel shows a conﬁguration with positive boundary condition. Right panel shows
the bounding box for the circuit of length L.
mapping ¯X →X′ is one-to-one. Then the probability that the circuit S around O occurs is
P(S) =

¯X e−bno( ¯X)

X e−bno(X) ≤

¯X e−bno( ¯X)

X′ e−bno(X′)
(4.84)
=

¯X e−bno( ¯X)

¯X e−bno( ¯X)+bL(S) = e−bL.
(4.85)
Now consider the set  of all possible circuits which surround the lattice site O, and we get
Pr{X0 = −1} =

S∈
P(S) ≤

S∈
e−bL
=

L=4,6,...
r(L)e−bL,
(4.86)
where r(L) is the number of circuits of length L which surround the lattice site O. Consider such
a circuit. Since it is required to surround the lattice site 0, this circuit must have each point at a
distance no more than L/2 from the site O. See panel 2 of Figure 4.13. There are at most L2 ways
to choose a starting point for the circuit. Having chosen a starting point, there are at most four
ways to choose the next point and three ways to choose each succeeding point moving around the
circuit. Thus there are at most 1
L4L23L−1 circuits of length L surrounding O. (The factor 1/L comes
from the fact that each circuit gets counted n times since any point along it can be considered as
the starting point). Thus r(L) ≤4L3L−1 ≤L4L implying
Pr{X0 = −1} ≤

L=4,6,...
L4Le−bL ≤
∞

L=1
L(4e−b)L
(4.87)
(a)
=
4e−b
(1 −e−b)2
(4.88)
with (a) coming from
x
(1 −x)2 = x(1 + 2x + 3x2 + · · · ) =
∞

n=1
nxn,
if |x| < 1.
since b = 2J/KT, we can make the R.H.S. of the inequality (4.88) arbitrarily small (strictly less than
1
2) by letting the temperature T →0. Thus, phase transition is guaranteed at low temperature.

4.8 1D RANDOM FIELDS ARE MARKOV CHAINS
119
4.8 1D Random Fields are Markov Chains
The DAG models are a special case of Gibbs random ﬁelds, and Markov chains are a 1D (linear)
Gibbs random ﬁeld with transition matrix determined by the potentials.
Theorem 4.22
Let X1, X2, . . . be a ﬁnite-valued m-range Gibbs random ﬁeld with locally
supported potential . Then there is a statistically equivalent Markov chain with transition
matrix determined by the locally supported clique function .
Proof
TheenergyU : X →Risconstructedfromlocallysupportedfunctions(x) =
(xS), S ⊂Z with range R = m, max{i,j∈S} |i −j| ≤m, implying
P(X) = 1
Ze−U(X) = 1
Ze−
n
j=m (τj(X))π(X0, . . . , Xm−1)
(4.89)
= 1
Ze−
n
j=m (Xj−m,...,Xj)π(X0, . . . , Xm−1),
(4.90)
with the  function corrected at the boundary by the initial distribution π(·). The
Markov property follows from
P(Xn|Xn−1, Xn−2, . . . ) =
P(Xn, Xn−1, Xn−2, . . . )

Xn∈X0 P(Xn, Xn−1, Xn−2, . . . )
(4.91)
=
e(Xn−m,...,Xn)

Xn∈X0 e(Xn−m,...,Xn) .
(4.92)
Example 4.23 (Ising Model)
The locally supported potentials  : X →R determine
the transition matrix for the chain. Let us do it explicitly for the 1D Ising case of
Section 4.10 following Kinderman and Snell [123]. Let D = {0 ≤i ≤n}, X0 = {−1, +1}
and associate no external ﬁeld. Then
P(X) = e
−J
KT

n
i=1 XiXi−1+f(X0)
Zn+1
.
(4.93)
Deﬁne ne(X), no(X) to be the number of even and odd bonds; a bond even corresponds
to either (Xi = 1) ∧(Xi−1 = 1) or (Xi = −1) ∧(Xi−1 = −1), with ne + no = n. Then
P(X) = e−2(J/KT)ne(X)e(J/KT)n+f(X0)
(4.94)
=
1
Zn+1
e(J/KT)n+f(X0)e(−2J/KT)ne(X).
Deﬁne a two state Markov chain with 2 × 2 transition matrix Q
=

p
1 −p
1 −p
p

, then
P(X) = π(X0)pne(X)(1 −p)no(X) = π(X0)pne(X)(1 −p)(n−ne(X))
= π(X0)(1 −p)n

p
1 −p
ne(X)
= π(X0)(1 −p)nene(X)(log
p
1−p ).
(4.95)
Choose −2J/KT = log(p)/(1 −p).

120
4 MARKOV RANDOM FIELDS ON UNDIRECTED GRAPHS
Apparently, the partition function may be calculated for this example. Equating
the normalizers gives
1
Zn+1
e(J/KT)n+f(X0) = π(X0)(1 −p)n.
(4.96)
Choosing π(X0) = ef(X0) as the distribution on the initial state, and substituting for
J/KT = 1
2 log(1 −p)/(p) gives
1
Zn+1
1 −p
p
n/2
= (1 −p)n =⇒Zn+1 =
1
[(1 −p)p]n/2 .
(4.97)
Using the fact that e2J/KT = (1 −p)/p implies p = 1/(1 + e2J/KT), 1 −p = e2J/KT/(1 +
e2J/KT) giving
Zn+1 = (1 + e2J/KT)n
(e2J/KT)n/2 .
(4.98)
Notice, with n →∞, (log Zn)/n →log Z = 1
2 log (1 + e2J/KT)/(e2J/KT).
4.9 Markov Chains Have a Unique Gibbs Distribution
Clearly this claim that there can be multiple stationary probabilities with the same interactions
is a departure from what we usually think about the Markov chain setting. This has something
to do with the dimension of the boundary for random ﬁelds in Zd, d > 1. Let us now prove
that for irreducible Markov chains there is only one stationary probability with the interactions
corresponding to the transition law of the chain.
Theorem 4.24
Given a Markov chain on ﬁnite states space X0 with positively regular tran-
sition matrix Q = (qkj), there is a unique stationary probability PS satisfying the Dobrushin,
Lanford, Ruelle condition Eqn. 4.79, where PSn on the ﬁnite cylinders is given by
PS
n(X1, . . . , Xn) = π(X1)
n−1

i=1
QXi,Xi+1,
(4.99)
with π = πQ the left eigenvector of Q.
Proof
Let us show the DLR condition is satisﬁed by PS; deﬁning Zn to be the ﬁrst
n-integers, then

y∈X Z/Zn
0
PS(X1, . . . , Xn|y)PS(y) =

X0∈X0
PS(X1, . . . , Xn|x0)PS(X0)
(4.100)
=

X0∈X0
n−1

i=0
QXi,Xi+1π(x0)
(4.101)
= π(X1)
n−1

i=1
QXi,Xi+1 = PS
n(X1, . . . , Xn).
(4.102)
Uniqueness
follows
from
the
fact
that
π
is
the
unique
left
eigenvector
(Perron–Frobenius) for the irreducible case.

4.10 ENTROPY OF STATIONARY GIBBS FIELDS
121
4.10 Entropy of Stationary Gibbs Fields
For stationary ﬁelds the entropy rates on large graphs are well deﬁned.
Theorem 4.25
Let Xi, i ∈Zd be a stationary Gibbs random ﬁeld. The entropy rate deﬁned as
H(X) =
lim
|D|→∞
H(Xi, i ∈D)
|D|
(4.103)
exists and letting Dn ⊂Zd be an increasing family of cylinders size |Dn| = n1 × · · · × nd
with Dn ↑D∞, then the limiting entropy rate H(X) is given by
H(X) = lim
n→∞H(X1|XDn/1) = lim
n→∞H(Xn|XDn/n)
(4.104)
= lim
n→∞
1
|Dn|H(XDn).
(4.105)
Proof
The proof generalizes the 1D proof in [7]. The ﬁrst limit follows since
conditioning reduces entropy, H(X|Y, Z) ≤H(X|Y), implying
H(X1|XDn/1) ≤H(X1|XDn−1/1).
(4.106)
Thus, H(X1|XDn/1), H(X1|XDn+1/1) is a decreasing sequence of non-negative numbers
implying it has a limit, call it H(1)(X). Also, H(Xn|XDn/n), H(Xn−1|XDn−1/n−1) is a
decreasing positive sequence which has a limit as well, call it H(2)(X), since
H(Xn|XDn/n) ≥H(Xn|X1+Dn−1/n) (a)
= H(Xn−1|XDn−1/n−1),
(4.107)
with the equality (a) using the stationarity of the ﬁeld. To prove that these limits are the
same and equal to the normalized entropy of the cylinders, factor the entropy using
the chain rule:
H(XDn) = H(X1,1,...,1|XDn/1) + H(X2,1,...,1|XDn/{1,(2,1,...,1)}) + · · ·
=
nd

id=1
· · ·
n1

i1=1
H(X(i1,...,id)|XDn/∪i
j=1{(j1,...,jd)}).
(4.108)
Choosing m(ǫ) large enough so that |H(X1|XDm/1)−H(1)| < ǫ, then construct cylinder
Dn with n > m(ǫ) so that for all i ∈Dn−m(ǫ) ⊂Dn,
H(1)(X) + ǫ ≥H(X1|XDm/1)
(4.109)
(a)
= H(Xi+1|Xi+Dm/i+1)
(4.110)
(b)
≥H(Xi+1|XDn/∪i+1
j=1j) ≥H(1)(X),
(4.111)

122
4 MARKOV RANDOM FIELDS ON UNDIRECTED GRAPHS
where (a) follows from stationarity, and (b) follows since conditioning decreases
entropy. Since limn→∞(|Dn−m(ǫ)|)/|Dn| = 1,
H(1)(X) + ǫ ≥lim
n→∞
1
|Dn|H(XDn)
(4.112)
= lim
n→∞

i∈Dn/Dn−m(ǫ) H(Xi|XDn/∪i
j=1j)
|Dn|
+

i∈Dn−m(ǫ) H(Xi|XDn/∪i
j=1j)
|Dn|
(4.113)
≥H(1)(X).
(4.114)
Since ǫ is arbitrary, the limit is H(1)(X). The identical limit is obtained by factoring the
joint entropy using H(Xn|XDn/n), implying H(1)(X) = H(2)(X) = H(X).

5
G A U S S I A N R A N D O M F I E L D S O N U N D I R E C T E D
G R A P H S
ABSTRACT
This chapter focuses on state spaces of the continuum studying Gaussian random
ﬁelds on discrete lattices. Covariances are induced via difference operators and the associated
neighborhood structure of the resulting random ﬁeld is explored. Texture representation and
segmentation are studied via the general Gaussian random ﬁeld structures. For dealing with
the partition function determined by the log-determinant of the covariance asymptotics are
derived connecting the eigenvalues of the ﬁnite covariance ﬁelds to the spectrum of the
inﬁnite stationary extension.
5.1 Gaussian Random Fields
In pattern theoretic representations the graphs often arise in the process of performing discrete
computation associated with continuum representations associated with differential operators
expressing interactions in the continuum. The differential operators induce the graphs, the ran-
dom ﬁelds with covariance structure induced via differential operators arising in the continuum.
Naturally the order of the differential operator determines the neighborhood and graph structure.
Inducing Gaussian time series via differential operators is familiar through auto-regressive
processes. In a similar manner this can be done for generalizations to ﬁelds on more general
lattices D ⊂Zd. For this, Gaussian ﬁelds on regular lattices play an important role. Gaussian
ﬁelds in general correspond to index spaces perhaps on the continuum D ⊂Rd, and state spaces
vector valued, X0 = Rm. For now, examine a discrete lattice D ⊂Zd, scalar valued X0 = R.
The most familiar view of the speciﬁcation of Gaussian processes and Gaussian random ﬁelds
is through arbitrary linear combinations of samples of the ﬁeld that are Gaussian. The Gaussian
ﬁelds studied will be associated with Hilbert spaces H = L2, l2; therefore it will be powerful to
deﬁne the Gaussian ﬁelds through their properties in inner products with elements of the Hilbert
space.
Deﬁnition 5.1
A scalar valued stochastic process or random ﬁeld {Xi(ω), i ∈D ⊆Z(Zd)} is
a Gaussian process (ﬁeld) with mean mX : D →R, and covariance KX : D × D →C if
ﬁnite linear combinations are normally distributed; for any integer n and any n-tuple a1, . . . , an
and points i1, . . . , in ∈D, Y(ω) = 
n
k=1 akXik(ω) is Gaussian with mean and variance
n

k=1
akmX(ik),
n

k=1
n

j=1
akKX(ik, ij)aj.
(5.1)
We shall say X is a Gaussian process (ﬁeld) with respect to the Hilbert space of square
summable sequences l2(Zd), l2(D ⊂Zd) with inner product ⟨f, g⟩= 
i∈D figi with mean
mX ∈H and covariance operator KX, if for all f ∈l2, ⟨f, X⟩is normally distributed with mean
and variance
mf = ⟨f, mX⟩,
σ 2
f = ⟨f, KXf⟩.
(5.2)
This illustrates the more general case which is speciﬁed according to arbitrary operators on lattices
D ⊂Zd. If the operators are linear, they induce Gaussian ﬁelds with particular graph structures
dependent on the differential order.
123

124
5 GAUSSIAN RANDOM FIELDS ON UNDIRECTED GRAPHS
Remark 5.1.1
It is usual to specify a Gaussian process as having all marginals which
are Gaussian, so that the process is zero-mean Gaussian if for any n variates X1, . . . , Xn
with covariance K has density
p(X1, . . . , Xn) = det−(1/2) 2πKe−(1/2) 
ij XiK−1
ij Xj.
(5.3)
That the above deﬁnition is equivalent is sufﬁcient to examine the characteristic func-
tion, using the fact that if Y is Gaussian with variance σ 2
Y then Eeiγ Y = e−(1/2)γ 2σ 2
Y (see
Example 2.13, Chapter 2). Use the fact that for any coefﬁcient 
n
i=1 aiXi is Gaussian
with variance 
ij aiajKX(i, j), so choosing ω = (ω1, . . . , ωn), then the characteristic
function becomes
Eei 
n
i=1 ωiXi = e−(1/2) 
ij ωiωjKX(i,j),
(5.4)
since 
n
i=1 ωiXi is Gaussian with variance 
ij ωiωjKX(i, j). This is the joint character-
istic function of a Gaussian vector with covariance KX (see [143], p. 168), which is a
unique identiﬁcation of the density, so X1, . . . , Xn have joint density given by 5.3.
5.2 Difference Operators and Adjoints
The Gaussian ﬁelds are studied induced by stationary linear difference operators adjusted at the
boundary to reﬂect boundary conditions. Let L be a ﬁnite difference, bounded support stationary
operator with its action on functions f ∈l2(Zd) of the form
(Lf)i =

j∈Zd
L(i, j)fj =

j∈Zd
aj−ifj
(5.5)
=

s∈S
asfi+s,
i ∈Zd,
(5.6)
with bounded support implied by |S| ﬁnite.
To induce Gaussian processes on ﬁnite domains D ⊂Zd, follow the approach as above
for Gibbs ﬁelds in which boundaries are speciﬁed determining the form of the operator at the
boundary conﬁguration. The ﬁnite domain operator Lg
D extended with inﬁnite boundary g ∈l2(Zd)
is constructed from L according to
(Lg
Df)i = (Lfg)i =

s∈S
as(fi+s ∨gi+s),
(5.7)
where fg = (f ∨g)i =
 fi
i ∈D
gi
i /∈D .
(5.8)
As before deﬁne the interior and boundary associated with the operator determined by the support
set S:
D0 = {i ∈D : i + S ⊂D},
¯D = ∪i∈D(i + S),
∂D = D/D0.
(5.9)
In what follows the 0-boundary will be used extensively, and will be denoted for convenience
as LD corresponding to the operator L on the interior of D extended with the zero boundary. The
covariance of the ﬁelds generated will be deﬁned by the operator and its adjoint. The adjoint
operator is deﬁned through the inner product and its action on functions (see Luenberger [8]).

5.2 DIFFERENCE OPERATORS AND ADJOINTS
125
Deﬁnition 5.2
Let H1, H2 be two Hilbert spaces of functions with inner products ⟨·, ·⟩H1,
⟨·, ·⟩H2, respectively. Then the adjoint L∗of L : H1 →H2 is a mapping L∗: H2 →H1
satisfying
for all f ∈H1, g ∈H2,
⟨f, L∗g⟩H1 = ⟨Lf, g⟩H2.
(5.10)
We study difference operators, therefore H1 = H2 = l2(Zd), and with the same inner product
denoted ⟨·, ·⟩l2. The adjoint operators for the inﬁnite and ﬁnite processes become as follows:
Theorem 5.3
Given are L on Zd and LD L extended with 0-boundary on D deﬁned
according to
(Lf)i =

s∈S
asfi+s,
i ∈Zd,
(5.11)
(LDf)i =

s∈S
asf0
i+s,
i ∈D.
(5.12)
1.
The adjoint operators L∗on Zd and L∗
D on D are given by
(L∗f)i =

s∈S
asfi−s,
i ∈Zd,
(5.13)
(L∗
Df)i =

s∈S
asf0
i−s,
i ∈D.
(5.14)
2. The operators L∗L on Zd and L∗
DLD on D are given by
(L∗Lf)i =

s′∈S

s∈S
asas′fi+s−s′,
i ∈Zd,
(5.15)
(L∗
DLDf)i =

s∈S

s′∈S
asas′f0
i−s+s′,
i ∈D.
(5.16)
Proof
First part follows from the identity
⟨g, L∗f⟩l2 =

i∈Zd
gi(L∗f)i = ⟨Lg, f⟩l2 =

i′∈Zd
(Lg)i′fi′ =

i′∈Zd

s∈S
asgs+i′fi′
=

i∈Zd
gi

s∈S
asfi−s
)
*+
,
(L∗f)i
(i = s + i′).
(5.17)
The adjoint operator L0∗
D is constructed similarly. For all f, g ∈l2(D) with f0, g0
extended to Zd with the 0-boundary, then
⟨g, L0∗
D f⟩=

i∈D
gi(L0∗
D f)i = ⟨LDg, f⟩=

i∈D

s∈S
asg0
i+s
)
*+
,
(LDg)i
fi =

i∈Zd

s∈S
asg0
i+sf0
i
(i = s + i′)
=

i′∈Zd

s∈S
asf0
i′−sg0
i′ =

i′∈D

s∈S
asf0
i′−s
)
*+
,
(L∗
Df)i′
gi′.
(5.18)

126
5 GAUSSIAN RANDOM FIELDS ON UNDIRECTED GRAPHS
Then L∗L is given by
(L∗Lf)i =

s∈S
asLfi−s =

s∈S
as

s′∈S
as′fs′+i−s.
Similarly L∗
DLD is derived as above.
5.3 Gaussian Fields Induced via Difference Operators
We shall look only at the 0-boundary case, so again it is implicitly assumed LD is the L operator
deﬁned on D with 0-boundary. Begin by deﬁning the Minkowski set difference.
Deﬁnition 5.4
Given a set S ⊂D ⊂Zd, then the Minkowski set difference and
addition is
S ⊖S = {s −s′; ∀s ̸= s′ ∈S} ⊂Zd,
(5.19)
S ⊕S = {s + s′; ∀s ̸= s′ ∈S} ⊂Zd.
(5.20)
Theorem 5.5
Assume LD is an invertible operator on l2(D) and let X satisfy the stochastic
difference equation
(LDX)i = Wi,
(5.21)
withWi, i ∈Dawhitenoisezeromeanprocessofindependent, identicallydistributedGaussian
variables, variance σ 2.
Then {Xi(ω), i ∈D ⊂Zd} is a real-valued Gaussian random ﬁeld having density
p(X) =
1
(2π)|D|/2 det1/2 KX

i∈D
e
−|(LDX)i|2
2σ2
,
(5.22)
with the inverse covariance the product of the operator with its adjoint K−1
X
= (1/σ 2)L∗
DLD,
with Markov structure on graph σ = {D, E} with neighborhoods Ni = D ∩(i + S ⊖S).
Proof
First we must prove for any vector of coefﬁcients fi, i ∈D, then the linear
combination 
i∈D fiXi is Gaussian distributed. Use the fact that LDX = W deﬁnes a
bijection with inverse L−1
D implying

i∈D
fiXi =

i∈D
fi(L−1
D W)i
= ⟨f, L−1
D W⟩= ⟨L−∗
D f
)*+ ,
g
, W⟩,
(5.23)
which is Gaussian distributed since by deﬁnition ⟨g, W⟩is Gaussian for all g. Thus
⟨f, X⟩is Gaussian, with covariance following from Eqn. 5.23:
E⟨f, X⟩2 = ⟨f, KXf⟩
(5.24)
= ⟨L−∗
D f, KWL−∗
D f⟩= σ 2⟨f, L−1
D L−∗
D f⟩.
(5.25)
This is true for all a, implying K−1
X
= L∗
DLD/σ 2.

5.3 GAUSSIAN FIELDS INDUCED VIA DIFFERENCE OPERATORS
127
To calculate the graph structure deﬁned by the neighborhood system, examine
the case that i + S ⊖S ⊂D. Use the adjoint and write the density according to
1
Ze−⟨LDX,LDX⟩/2σ 2 = 1
Ze−⟨X,L∗
DLDX⟩/2σ 2 = 1
Z

i∈D
e−Xi(L∗
DLDX)i/2σ 2.
(5.26)
Applying the deﬁnitions gives L∗
DLD as
(L∗
DLDX)i =

s∈S

s′∈S
asas′X0
i−s+s′.
(5.27)
The number of edges in the graph emanating from internal sites is |S|2−|S|. Only terms
in the quadratic form involving Xi are those sites j ∈i+S⊖S giving the neighborhood
structure for i ∈D0 according to
p(Xi|Xj, j ̸= i) =
e
−(1/2σ 2) 
j∈D Xj

s∈S

s′∈S asas′X0
j−s+s′

R e
−(1/2σ 2) 
j∈D Xj

s∈S

s′∈S asas′X0
j−s+s′ dxi
=
e−(1/2σ 2)Xi

s∈S

s′∈S asas′Xj−s+s′

R e−(1/2σ 2)X 
s∈S

s′∈S asas′Xj−s+s′ dx
.
(5.28)
The edges internal to the graph are constructed by taking a site i ∈D0 and
equipping it with bonds enumerated by j = 1, 2, . . . |S⊖S|. Two sites, i, i′ are connected
by an edge in the graph if i = i′ + k for some k = s −s′ ∈S ⊖S.
Remark 5.3.1
The same result can be obtained using the standard change of distri-
bution formula. Since Wi, i ∈D is independent Gaussian with product density having
the form p(W) = $
i∈D p(Wi), then
p(X) = 1
Z

i∈D
e−|(LDX)i|2/2σ 2,
(5.29)
with the constant 1/Z determined by the Jacobian of the transformation. The Gaussian
case is a special case of a linear operator with p(Wi) = ef(Wi) where f(·) = ∥· ∥2. Let
p(Wi) = ef(Wi) with LD a linear or non-linear difference operator. Then
p(X) = 1
Z

i∈D
ef((LDX)i),
(5.30)
with Z given by the Jacobian of the transformation LD.
Example 5.6 (Auto-Regression: 1 Derivative, 0-boundary)
In order to motivate the
general theorem on graphs and generator structure induced via difference operators,
examine the simplest auto-regressive model which is familiar.
Take the index set on the real line (time), D = [0, ∞), L = (d/dt) + a with zero
boundary X(0) = 0, associated with the differential equations ˙X(ω, t) + aX(ω, t) =
W(ω, t), and W(·) “white noise”. This induces a Gaussian process with particular
covariance; we will return to these continuum processes in some detail in Chapter 9.
For now, focus on the associated difference equation and the resulting nearest neighbor
Markov chain graph. Then, taking backward differences with zero boundary X0 = 0
gives
(1 + a)Xi −Xi−1 = Wi,
i ≥1,
(5.31)

128
5 GAUSSIAN RANDOM FIELDS ON UNDIRECTED GRAPHS
implying
Xi =
i

k=1

1
1 + a
i+1−k
Wk
i ≥1, X0 = 0.
(5.32)
This is a Gaussian process, which is ﬁrst order Markov; its directed graph structure
is as shown in the left panel of Figure 3.2, with parents of internal sites πi = {i −1}.
The linear graph structure is induced by the ﬁrst order operator L = (d/dt) + a. If W
is zero-mean, covariance EWnWj = δ(n −j), then
KX(i, j) = EXiXj =
i−1

k=0
j−1

k′=0

1
1 + a
i−k 
1
1 + a
j−k′
EWkWk′
(5.33)
=
min(i,j)−1

k=0

1
1 + a
i+j−2k
.
(5.34)
Examine the operator as inducing a Gaussian ﬁeld {Xi, 1 ≤in} with backward
differences giving the stochastic equation
(LX)i =

s∈S
asXs+i = Xi −Xi−1 + aXi = Wi,
X0 = 0.
Applying Theorem 5.5
S = {0, −1},
S ⊖S = {−1, 1},
(5.35)
N = ∪n−1
i=2 {i −1, i + 1}
)
*+
,
i+S⊖S
∪{2} ∪{n −1}.
(5.36)
Viewing Ln : Rn →Rn as a matrix operator in equation LnX = W, the Ln is invertible
with
L0
n =


1 + a
0
0
0
· · ·
−1
1 + a
0
0
· · ·
0
−1
1 + a
0
· · ·
...
...
...
...
...
0
0
· · ·
−1
1 + a


,
L0∗
n =


1 + a
−1
0
0
· · ·
0
1 + a
−1
0
· · ·
0
0
1 + a
−1
· · ·
...
...
...
...
...
0
0
· · ·
0
1 + a


.
(5.37)
Using the standard density transformation with zero boundary gives
p(X1, . . . , Xn) = 1
Z
n

i=1
e(−|(L0
nX)i|2/2σ 2)
= 1
Ze(−|(1+a)X1|2/2σ 2)
n

i=2
e(−|(1+a)Xi−Xi−1|2/2σ 2),
(5.38)

5.3 GAUSSIAN FIELDS INDUCED VIA DIFFERENCE OPERATORS
129
Z absorbing the normalization given by the Jacobian of the coordinate transformation,
which is the determinant of Ln. In this case, Ln is lower triangular and the determinant
is just (1 + a)n, giving Z = (2πσ 2)n/2(1 + a)−n.
Example 5.7 (Derivative operator: Periodic boundary)
Reconsider the same basic
operator L = (∂/∂t) + a, discretized with backward differences as before, but with
periodic boundary conditions on 1 ≤i ≤n:
(LX)i =

s∈S
asXs+i = Xi −Xi−1 + aXi = Wi,
X0 = Xn.
This is the cyclic torus graph structure shown in Figure 4.1. Then
Ln =


1 + a
0
0
· · ·
0
−1
−1
1 + a
0
· · ·
0
0
0
−1
1 + a
0
· · ·
0
...
0
0
0
· · ·
−1
1 + a


,
L∗
n =


1 + a
−1
0
· · ·
0
0
0
1 + a
−1
0
· · ·
0
0
0
1 + a
−1
· · ·
0
...
−1
0
0
0
· · ·
1 + a


,
(5.39)
with eigenfunctions for Ln, L∗n being φi(ωk = (2π/n)k) = e(−j2π/n)ki and eigenvalues
λk = 1 −e(+j2πk/n) + a = −2jejπk/n sin(πk/n) + a and eigenvalues for L∗n given by
λ∗
k = 2je(−jπk)/n sin(πk/n) + a. The inverse covariance is
K−1
n
= L∗nLn
σ 2
(5.40)
= 1
σ 2


(1 + a)2 + 1
−(1 + a)
0
· · ·
0
−(1 + a)
−(1 + a)
(1 + a)2 + 1
−(1 + a)
0
· · ·
0
0
−(1 + a)
(1 + a)2 + 1
−(1 + a)
· · ·
0
...
−(1 + a)
0
0
· · ·
−(1 + a)
(1 + a)2 + 1


,
which is symmetric having real eigenvalues
λkλ∗
k = (1 + a)2 + 1 −(1 + a)e−j2πk/n −(1 + a)e−j2π(n−1)k/n
(5.41)
= 2 + 2a + a2 −2(1 + a) cos 2π
n k = λkλ∗
k.
(5.42)
These illustrate 1D cases. Now to operators on lattices D ⊂Z2.
Example 5.8 (2D Derivative Operator)
Examine the operator L = (∂/∂x1) + (∂/∂x2)
+a, with backward differences with 0-boundary X0,j = Xi,0, inducing the random ﬁeld
X(i,j), (i, j) ∈D = {1 ≤i, j ≤n} according to
−Xi−1,j + (2 + a)Xi,j −Xi,j−1 = Wi,j,
(5.43)
and Wi,j a zero-mean white noise ﬁeld, independent identically distributed Gaussian
variables, variance σ 2. Thus (LmX)i = 
s∈S asXs+i, and applying Theorem 5.5, gives
S = {(0, 0), (0, −1), (−1, 0)},
S ⊖S = {(0, −1), (−1, 0), (0, 1), (−1, 1), (1, 0), (1, −1)},
Ni = D ∩i + S ⊖S = {(i, j −1), (i −1, j), (i, j + 1), (i −1, j + 1), (i + 1, j), (i + 1, j −1)}.

130
5 GAUSSIAN RANDOM FIELDS ON UNDIRECTED GRAPHS
Then the interior neighborhoods are of size |S ⊖S| = 6.
The operator equation is invertible, since there is a bijection between the X and
W vectors in (LmX)i,j = Wi,j, (i, j) ∈D, as evidenced by
(i = 1, j = 1)
(2 + a)X1,1 = W1,1
(i = 1, j = 2)
(2 + a)X1,2 −X1,1 = W1,2
...
...
(i = 1, j = m)
(2 + a)X1,m −X1,m−1 = W1,1
(i = 2, j = 1)
(2 + a)X2,1 −X1,1 = W2,1
(i = 2, j = 2)
(2 + a)X2,2 −X1,1 −X2,1 = W2,2
...
.
(5.44)
Thus Lm is invertible, and using the change of density formula p(Xi,j, (i, j) ∈D)
the density becomes
1/Z

i∈D0
e−1/2σ 2(Xi−1,j−(2+a)Xi,j + Xi,j−1)2 
j∈D
e−1/2σ 2(X1,j)2

i∈Zm
e−1/2σ 2(Xi,1)2.
(5.45)
The left panel in Figure
5.1 shows the induced graph structure for the 2D
derivative operator.
Example 5.9 (1D Self-adjoint Laplacian: Periodic boundary)
Examine the Lapla-
cian L = −(∂2/∂x2) + a with periodic boundary conditions. Assume the derivative is
approximated according to ((∂/∂x)x)i = xi+(1/2) −xi−(1/2) giving L = L∗with
(LX)i = −Xi−1 + (a + 2)Xi −Xi+1 = Wi
(5.46)
giving
Ln =


(a + 2)
−1
0
· · ·
0
−1
−1
(a + 2)
−1
0
· · ·
0
...
...
...
...
...
−1
0
0
· · ·
−1
a + 2


(5.47)
Figure 5.1 Left panel shows the graph induced by the ﬁrst order difference operator; right panel
shows the graph associated with the second order Laplacian difference operator.

5.3 GAUSSIAN FIELDS INDUCED VIA DIFFERENCE OPERATORS
131
with eigenvalues λk = a + 2 −2 cos(2πk/n). Notice, a makes the operator positive
deﬁnite.
Since this is symmetric, Ln = L∗n, the inverse covariance is given by
K−1
n
= 1
σ 2 L2
n
= 1
σ 2


(a + 2)2 + 2
−2(a + 2)
1
0
· · ·
0
1
−2(a + 2)
−2(a + 2)
(a + 2)2 + 2
−2(a + 2)
1
0
· · ·
0
1
...
−2(a + 2)
1
0
. . .
0
1
−2(a + 2)
(a + 2)2 + 2


.
(5.48)
The normalizer is given by
det−1/2 Kn = det−1/2 Kn = (det L∗L)1/2
= det L =
n

k=1
λk =
n

k=1

a + 2 −2 cos 2πk
n

.
Example 5.10 (2D Laplacian: O-boundary)
Examine the Laplacian operator L =
− + a = −(∂2/∂x2
1) −(∂2/∂x2
2) + a, with 0-boundary, inducing the random ﬁeld
Xi,j, (i, j) ∈D = {1 ≤i, j ≤n} according to
−

Xi+1,j + Xi−1,j + Xi,j+1 + Xi,j−1
	
+ (4 + a)Xi,j = Wi,j,
(5.49)
X0,j = Xi,0 = Xm,j = Xi,m = 0, and Wi,j, (i, j) ∈D a white noise ﬁeld. The density
p(Xi,j, (i, j) ∈D) becomes
1
Z

(i,j)∈D0
e−1/2σ 2(−(Xi+1,j+Xi−1,j+Xi,j+1+Xi,j−1)+(4+a)Xi,j)2
×

i∈{1,n},1≤j≤n
e−1/2σ 2(Xi,j)2

j∈{1,n},1≤i≤n
e−1/2σ 2(Xi,j)2,
giving
S = {(−1, 0), (1, 0), (0, 1), (0, −1), (0, 0)}
(5.50)
S ⊖S = {(−1, 0), (−2, 0), (1, 0), (2, 0), (−1, −1), (1, 1), (0, −1), (0, −2),
(0, 1), (0, 2), (−1, 1), (1, −1)},
with interior neighborhood Ni = i + S ⊖S of size |S ⊖S| = 12. The graph for the
Laplacian induced random ﬁeld is shown in the right panel of Figure 5.1. Figure 5.2
illustrates the result of synthesizing such random ﬁelds. For this the noise ﬁelds were
generated as a set of independent and identically distributed Gaussian random vari-
ables with mean µ and standard deviation σ. The random ﬁeld was generated by
solving Eqn. 5.49. The top row, panel 1 shows a realization of the white noise ﬁelds
with σ = 100, µ = 128. Panels 2–4 show the random ﬁeld solved from the stochastic
differential equation with a = 1 (panel 2), a = 4 (panel 3), and a = 12 (panel 4). The
independent nature of the noise-image pixels is clearly evident by the high degree of
granularity (left panel). The inter-pixel dependence induced by the Laplacian is also
clear. The bottom row of Figure 5.2 shows the histograms of the statistics generated
from the ﬁelds.
Figure 5.3 illustrates the effect that varying the noise standard deviation and restor-
ing force have on the resulting textures for ﬁxed noise mean µ = 128. The columns 1,2

132
5 GAUSSIAN RANDOM FIELDS ON UNDIRECTED GRAPHS
mean=128, noise sd=100, a=1 
0
100
200
300
400
500
600
700
800
50
100
150
200
250
0
mean=128, noise sd=100, a=4 
0
100
200
300
400
500
600
700
800
0
50
100
150
200
250
0
50
100
150
200
250
mean=128, noise sd=100, a=12
0
100
200
300
400
500
600
700
800
(1)
(2)
(3)
(4)
Figure 5.2 Top row: Panel 1 shows a noise realization driving the stochastic difference equation
with σ = 100, µ = 128. Panels 2–4 show the solution of the stochastic difference equation with
a = 1 (panel 2), a = 4 (panel 3), and a = 12 (panel 4). Bottom row: Histograms of statistics of the
images.
mean=128, noise sd=50, a=1 
mean=128, noise sd=50, a=4 
0
50
100
150
200
250
mean=128, noise sd=50, a=12
100
200
300
400
500
600
700
800
900
0
0
50
100
150
200
250
100
200
300
400
500
600
700
800
0
0
50
100
150
200
250
100
200
300
400
500
600
700
800
900
0
mean=128, noise sd=200, a=1 
0
100
200
300
400
500
600
700
800
900
1000
0
50
100
150
200
250
100
200
300
400
500
600
700
800
0
50
100
150
200
250
0
mean=128, noise sd=200, a=4 
100
200
300
400
500
600
700
800
900
0
50
100
150
200
250
0
mean=128, noise sd=200, a=12
Figure 5.3 Random ﬁelds and histograms resulting from different sets of parameters. Rows have
restoring forces a = 1, a = 4 and a = 12 (top to bottom) for ﬁxed noise mean µ = 128, with noise
standard deviations of σ = 50 (columns 1, 2) and σ = 200 (columns 3, 4).

5.3 GAUSSIAN FIELDS INDUCED VIA DIFFERENCE OPERATORS
133
show noise standard deviations of σ = 50 and columns 3,4 show standard deviations of σ = 200
with the three rows showing restoring forces a = 1 (left column), a = 4 (middle column), and
a = 12 (right column). Columns 2 and 4 of Figure 5.3 present the histograms of the images. Note
how different sets of parameters result in similar histograms, but the images are distinguished by
their different textures.
5.4 Stationary Gaussian Processes on Zd and their Spectrum
For the independent pixel model (e.g. Example 2.57, Chapter 2), the normalizing variance may be
calculated for each pixel separately. There is no difﬁculty. As clusters of pixels that are correlated
are examined , the closed form calculation of the normalizer becomes far more challenging. The
basic paradigm now examined provides the techniques for handling the normalizing covariance of
stationary Gaussian random ﬁelds on arbitrary but large ﬁnite subgraphs. Clearly the boundaries
of the random ﬁelds determine the covariance; the question of how signiﬁcant the boundary is in
determining the log-determinant of the normalizer can be answered deﬁnitively. We do this by
demonstrating asymptotics via the fully stationary case.
The asymptotics will correspond to an approximation of the ﬁnite processes by their station-
ary analogue. Stationary Gaussian random ﬁelds are characterized through their covariance and
spectrum.
Deﬁnition 5.11
Given a stationary real-valued zero mean stochastic process {Xi, i ∈Zd},
then deﬁne the autocovariance function for lag τ ∈Zd as
Kτ = EXiXi+τ .
(5.51)
Let ω = (ω1, . . . , ωd) ∈[−π, π]d and ⟨ω, k⟩= 
d
i=1 ωiki then the power spectral density
is deﬁned as the Fourier transform of the covariance:
S(ω) =

τ∈Zd
Kτ e−j⟨ω,τ⟩,
ω ∈[−π, π]d.
(5.52)
Induce the stationary Gaussian ﬁelds via the translation invariant operator L : l2(Zd) →
l2(Zd), with its action on functions deﬁned to be invariant to shift where L(i, j) = aj−i, j−i ∈S ⊂Zd.
The calculation of the eigenelements {φ(ω), σ(ω)} of the operator L associated with Zd (inﬁnite
boundary) is determined by the Fourier transform, since the operator is shift invariant.
Theorem 5.12
Let Xi, i ∈Zd satisfy the stochastic difference equation LXi = Wi with L
the shift invariant invertible operator L(i, j) = aj−i given by

j∈Zd
L(i, j)Xj =

j∈Zd
aj−iXj = Wi,
(5.53)
W an i.i.d Gaussian, zero-mean, variance 1 process. Then X is a Gaussian process (inverse
covariance LL∗) with spectral density the inverse of the spectrum of LL∗:
S(ω) =
1
σ 2(ω)
=
1
| 
s∈S ase−j⟨ω,s⟩|2 ,
where σ(ω) =

s∈S
asej⟨ω,s⟩.
(5.54)

134
5 GAUSSIAN RANDOM FIELDS ON UNDIRECTED GRAPHS
Proof
The Gaussianity is proven as in Theorem 5.5, extended to the stationary case
on Zd. Using the fact that L is invertible, then for all f ∈l2(Zd),
⟨f, X⟩= ⟨f, L−1W⟩= ⟨L−∗f, W⟩.
(5.55)
Thus ⟨f, X⟩is Gaussian with inverse covariance operator K−1
X
= L∗L.
The eigenfunctions are the complex exponentials φ(ω) = ej⟨ω,·⟩, from the discrete
Fourier transform given from the deﬁnition of the operator and its adjoint Eqn. 5.15
(Lφ(ω))i =

s∈S
asej⟨ω,(i+s)⟩=

s∈S
asej⟨ω,s⟩ej⟨ω,i⟩
(5.56)
= σ(ω)ej⟨ω,i⟩
(L∗Lφ(ω))i =

s∈S
as(Lφ(ω))i−s =

s∈S
asσ(ω)ej⟨ω,i−s⟩
(5.57)
= |σ(ω)|2ej⟨ω,i⟩.
Deﬁning the covariance of the X process to be KX(i, j) = EXiXj, then we have
EWiWj = ELXiLXj =

h′

h
ah′ahEXi+h′Xj+h
=

h′

h
ah′ahKX(i + h′, j + h)
(5.58)
=

r

h
ar+hahKX(i + r + h, j + h).
(5.59)
Now, EWiWj is independent of i, j so deﬁne j = i + τ implying KX(i, j) = KX(0, τ) is a
function of the difference of its arguments, so the process is Gaussian and stationary.
Calling the covariance KX(i, i + τ) = Kτ , τ ∈Zd, then we have
δ(τ) (a)
= ELXiLXi+τ =

r

h
ar+hahKτ−r,
(5.60)
with (a) following from the independent properties of W. Taking the Fourier transform
completes the proof:


h
ahej⟨ω,h⟩

2
S(ω) = 1,
ω ∈[−π, π]d.
(5.61)
5.5 Cyclo-Stationary Gaussian Processes and their Spectrum
For cyclic conditions on cubic lattices D ⊂Zd
N there is no difﬁculty with the normalizer. The spec-
trum of the process and the eigenvalues of the covariance are given by the discrete periodic Fourier
transform.
Cyclo-stationary Gaussian random ﬁelds are characterized through periodic covariances
and spectra.

5.5 CYCLO-STATIONARY GAUSSIAN PROCESSES
135
Deﬁnition 5.13
Given a cyclo-stationary real-valued zero mean stochastic process {Xi, i ∈
Zd
N} with Xi = Xi+mN, m integer, then deﬁne the autocovariance function for lag τ ∈
Zd
N as
Kτ = EXiXi+τ = Kτ+mN,
m integer.
(5.62)
Let ωk = (2πk1/N1, . . . , 2πkd/Nd), k ∈Zd
N and ⟨ωk, τ⟩= 
d
i=1(2πki/Ni)τi then the power
spectral density is deﬁned as the periodic Fourier transform of the covariance:
S
2πk
N

=

τ∈Zd
N
Kτ e−j⟨2πk/N,τ⟩,
k ∈Zd
N.
(5.63)
Induce the stationary Gaussian ﬁelds via the square-root of the inverse covariance which
is cyclo-stationary to shift, L(i, j) = aj−i, j −i ∈Zd
N. The calculation of the eigenelements
{φ(2πk/N), σ(2πk/N)} of L associated with Zd (inﬁnite boundary) is determined by the periodic
Fourier transform. This is a direct corollary of Theorem 5.12.
Corollary 5.14 (Theorem 5.12)
Let Xi, i ∈Zd
N satisfy the stochastic difference equation
LXi = Wi with L the cyclo-shift invariant invertible operator L(i, j) = aj−i given by

j∈Zd
N
L(i, j)Xj =

j∈Zd
N
aj−iXj = Wi,
(5.64)
W an i.i.d Gaussian, zero-mean, variance 1 process. Then X is a Gaussian process (inverse
covariance LL∗) with spectral density the inverse of the spectrum of LL∗:
S
2πk
N

=
1
σ 2(2πk/N)
=
1
| 
s∈Zd
N ase−j⟨2πk/N,s⟩|2 ,
where σ
2πk
N

=

s∈Zd
N
asej⟨2πk/N,s⟩.
(5.65)
Example 5.15 (Cyclo-Stationary Gaussian Texture Modelling)
Examine models of
the textured images in which the covariances are unconstrained other than to full
cyclo-stationarity, then K(i, j), i, j ∈D ⊂Zd a cyclic K(i + mN, j + nN) = K(i, j) and
toeplitz K(i, j) = K(i −j, 0) having square root factorization K = (LL∗)−1. Then the
eigenelements are
K(i, j) =

k
λ2
kej
2
2πk
N ,(i−j)
3
,
(5.66)
implying L(i, j) = 
k(1/λk)ej
2
2πk
N ,(i−j)
3
. With this model in mind, let X solve
LX = W with L generated as the inverse square root of the cyclo-stationary covariance
L−1 =
√
K. The Toeplitz, cyclic covariance satisﬁes K(i, j) = K(i −j, 0) = EXiXj with
the added property that Xi = Xi+N. The Toeplitz covariance K(i, j) is estimated from
the training data according to K(i, 0) = 1/|D| 
j∈D Xi+jXj.
Due to cyclo-stationarity of the process X(·), the eigenvalues of the Toeplitz
covariance are given by complex exponentials. The random process is generated from
the i.i.d. Gaussian white process transformed by L−1 =
√
K so that the random Gaus-
sianprocesssatisﬁesLX = W withX =
√
KW = L−1W. ShowninFigure5.4areresults
of cyclo-stationary Gaussian process modelling of various textures. In each panel a

136
5 GAUSSIAN RANDOM FIELDS ON UNDIRECTED GRAPHS
(1)
(2)
(3)
(4)
(5)
Figure 5.4 Panels show textures being synthesized with stationary Gaussian process. Top row:
Panel 1 shows the original fur synthesis, panel 2 shows synthesis with the largest 8 eigenvalues,
panel 3 shows synthesis with the largest 128 eigenvalues, panel 4 shows synthesis with the largest
512 eigenvalues, and panel 5 shows synthesis with all of the eigenvalues. Bottom row is similar
but for fabric. Taken from Dimitri Bitouk.
0
64
128
0
64
128
0
64
128
0
50
100
150
0
50
100
150
0
1
2
3
4
5
x 10
5
0
50
100
150
0
50
100
150
0
1
2
3
4
5
x 10
5
0
64
128
Figure 5.5 Figure presents the animal spectrum and the spectrum estimated from the synthesized
textures. The two panels on the left show sparsity pattern of the animal fur texture eigenvalues
and the eigenvalues estimated from the synthesized texture images; the two panels on the right
show empirical versus synthesized spectra. Taken from Dimitri Bitouk.
different number of eigenvalues were used for generating the empirical covariance.
Since the complex exponentials are the eigenfunctions, the covariances were gener-
ated from increasing numbers of empirically generated eigenvalues. Panels 1, 6 show
the original fur and fabric textures. Panels 2, 7 show synthesis using the largest 8
eigenvalues. Panels 3, 8 show synthesis with the largest 128 eigenvalues. Panels 4, 9
show synthesis via the largest 512 eigenvalues. Panels 5, 10 show synthesis via all of
the eigenvalues.

5.5 CYCLO-STATIONARY GAUSSIAN PROCESSES
137
(1)
(2)
(3)
Figure 5.6 Panel 1 shows the micrograph data including mitochondria and cytoplasmic structures.
Panel 2 shows a decomposition into two region types, mitochondria (white) and background
(black). Panel 3 shows the pixel-by-pixel segmentation of the image based on the Bayesian hypoth-
esis test under the two models. Data taken from Dr. Jeffrey Safﬁtz of the Department of Pathology
at Washington University.
Figure 5.5 presents the animal spectrum and the spectrum estimated from the
synthesized textures. Left two panels show sparsity pattern of the animal fur texture
eigenvalues and the eigenvalues estimated from the synthesized texture images; right
two panels show empirical versus synthesized spectra.
5.6 The log-Determinant Covariance and the Asymptotic Normalizer
For segmentation of textured objects, model the image as a collection of objects with textured
random ﬁelds of speciﬁc type over the interior. Deﬁne the background space D ⊂Zd is made
up of a collection of textured interiors forming a disjoint partition of the image, D = ∪jDj. The
goal is to segment the interiors Dj forming a covering of the complete image. Shown in Figure 5.6
are examples of micrographs showing textured mitochondria and cytoplasm. The data are from
rabbit-kidney cells at 20,000 times magniﬁcation. Panels 1–3 show the micrograph data, with the
two region types, mitochondria (white) and background (black).
Generally, the interiors of the shapes are modelled as real-valued scalar random ﬁelds. We
take the viewpoint of Bayesian testing for calculating the likelihood of the shapes and construct the
regions Dj, j = 1, 2, . . . so that the probability of the observed imagery P(XD) is a maximum. Panel
2 of Figure 5.6 shows such a decomposition of the micrograph image into a disjoint partition. For
Gaussian random ﬁelds under the independent pixel model the global probability over the shape
is simply the product of the probabilities over the interior, with the normalizer just the product of
the variances:
p(XD) =

i
1
2πσ 2 e−|Xi−µ|2/2σ 2.
(5.67)
This is straightforwardly calculated. Panel 3 of Figure 5.6 shows the segmentation performed by
ﬁtting the mean and variance statistics to the mitochondria and membrane. For this an independent
pixel to pixel hypothesis test is performed selecting the two model types. Notice the granularity of
the segmentation; there is no notion of the global structure associated with the connected regions.
The deformable active models attempt to model the global structure.
We model the interiors of the shapes of real-valued images as scalar random ﬁelds deﬁned on
the index sets of integer lattices. Because of splitting, the partitions are conditionally independent

138
5 GAUSSIAN RANDOM FIELDS ON UNDIRECTED GRAPHS
given their boundaries. The Bayesian approach presents a substantial conceptual hurdle. Since the
normalizing partition function must be calculated for each partition Dj ⊂Zd, d = 2, 3, j = 1, . . . , J
the number of object regions. The partition function and log-normalizer must be calculated for
each shape. Let XD be a real-valued Gaussian ﬁeld on D ⊂Zd, with mean and covariance µ, KD.
We model the inverse covariance via the difference operator which is zero at the boundary of the
shape so that LDL∗
D = K−1
D ; the probability density requires the computation of the determinant
of the covariance corresponding to the log-partition function for Gaussian ﬁelds:
p(XD) = det−1/2(2πKD)e−1/2∥LD(XD−µ)∥2,
(5.68)
with ∥LD(X −µ)∥2 the integral-square of the ﬁeld over the domain D. Notice the role the partition
functions play; segmentation requires computation of log KD for the random shape. Construct
the background space D ⊂Zd as a disjoint partition of simply connected regions D(gj) with ran-
dom image ﬁeld XD(gj). Conditioned on the parametric models, g1, g2, . . . , the subregion Markov
random ﬁelds are assumed conditionally independent. Write the potential via energy densities
Ej(x), x ∈D(gj) representing the log-normalizer and quadratic data statistic according to
p(XD1, . . . , XDJ ) =
J
j=1
det−(1/2)(2πKDj)e−1/2σ 2∥L(XDj−µj)∥2
.
(5.69)
5.6.1 Asymptotics of the Gaussian processes and their Covariance
Let XD be a real-valued Gaussian ﬁeld on D ⊂Zk, with mean and covariance µ, KD. let D ⊂Zk,
with X real-valued on D since the determinant of the covariance replaces the log-partition func-
tion. The probability density requires the computation of the determinant of the covariance
corresponding to the log-partition function for Gaussian ﬁelds:
p(XD) = det−(1/2)(2πKD)e−(1/2)⟨X−µ,K−1
D X−µ⟩.
(5.70)
As we noted in Chapter 4 the problem isn’t any better for Gibbs distributions. Now examine
Gaussian random ﬁelds {Xi(ω), i ∈Dn ⊂Zd} deﬁned on a sequence of the ﬁnite increasing graphs
Dn ⊂Dn+1 ⊂· · · , with lim
n→∞Dn = Zd. Associate with Dn ⊂Zd the operator Ln : l2(Dn) →l2(Dn)
constructed from the stationary operator L restricted to Dn according to LnX = LX0, where X0
i =
Xi, i ∈Dn and 0 otherwise. Then X on Dn solves the stochastic difference equation
(LnX)i = Wi,
i ∈Dn,
(5.71)
with Wi white noise with variance 1. As proven in Theorem 5.5, the operator Ln determines the
covariance structure with KXD = (L∗nLn)−1. Of course the fundamental issue is that for every
domain shape Dn, the boundary of the operator ∂Dn determines the normalizing determinant of
the covariance.
The asymptotics are a direct assault on the inﬂuence of the boundary on the covariance,
appealing to results familiar to the Toeplitz aﬁcionados, similar in spirit to Weyl’s results. For this,
crucially the operators deﬁning the ﬁelds are stationary with respect to shift over the interior sites.
This allows for the relationship of the determinant of the covariance on Dn of the ﬁnite operator to
the Fourier transform of the stationary operator associated with Zd. Assuming the boundaries are
thin relative to their internal volumes, so that lim
n→∞|∂Dn|/|Dn| = 0, then the asymptotic moments
of the eigenvalue distribution hold irrespective of the boundary conditions. It is natural to study

5.6 THE LOG-DETERMINANT COVARIANCE
139
the 0-boundary. Now we prove a theorem on the trace and log-trace of the operator which provides
the normalizer for the Gaussian case.
Deﬁnition 5.16
Let the operator Ln on Dn have eigenvalues λm, m = 1, . . . , |Dn|, with the
pth power operator deﬁned as
Lp
n(i0, ip) =

i1∈Dn
· · ·

ip−1∈Dn
Ln(i0, i1)Ln(i1, i2) · · · Ln(ip−1, ip).
(5.72)
The operator trace is deﬁned as the sum of the eigenvalues tr Lp
n = 
|Dn|
m=1 λp
m; the boundary
of the pth power of Lp
n as
∂D(p)
n
= Dn/D(p)o
n
where D(p)o
n
= {i ∈Dn : i + (S ⊖S ⊖· · · S)
)
*+
,
p times
⊂Dn}.
(5.73)
We now show that the normalized pth moment of eigenvalues of the ﬁnite-domain operator
converges to the pth moment of the Fourier transform of the inﬁnite domain operator.
Theorem 5.17 (Dedicated to the Memory of Gabor Szego)
Let Ln : l2(Dn) →l2(Dn)
according to Lnf = Lf0 where (Lf0)i = 
s∈S asf0
i+s, i ∈Dn with eigenvalues of Ln given by
λm, m = 1, . . . , |Dn|, and the Fourier transform of L given by σ(ω) = 
s asej⟨ω,s⟩.
Assuming the boundary is thin so that lim
n→∞|∂D(p)
n |/|Dn| = 0 for all p, then the
asymptotic trace is given by
lim
n→∞
tr Lp
n
|Dn| = lim
n→∞

n
m=1 λp
m
|Dn|
=
1
(2π)d

[−π,π]d σ p(ω) dω.
(5.74)
Assuming that the domain of eigenvalues does not include 0, then
lim
n→∞

|Dn|
m=1 log λm
|Dn|
=
1
(2π)d

[−π,π]d log σ(ω) dω.
(5.75)
Proof
lim
n→∞
tr Lp
n
|Dn| = lim
n→∞

i=k∈Dn Lp
n(i, k)
|Dn|
(5.76)
= lim
n→∞



i=k∈D(p)o
n
Lp
n(i, k)
|Dn|
+ O(|∂D(p)
n |)
|Dn|

.
(5.77)
Now clearly, Lp(i, ·) and σ p(·) form a Fourier transform pair, since the complex
exponentials are eigenfunctions of the stationary operator L:
Lp(i, k) =
1
(2π)d

[−π,π]d σ p(ω)ej⟨ω,(k−i)⟩dω,
(5.78)
and substituting in Eqn. 5.77 completes the ﬁrst part of the theorem:
lim
n→∞
tr Lp
n
|Dn| = lim
n→∞

|D(p)o
n
|
|Dn|
1
(2π)d

[−π,π]d σ p(ω) dω + O(|∂D(p)
n |)
|Dn|

(5.79)
=
1
(2π)d

[−π,π]d σ p(ω) dω.
(5.80)

140
5 GAUSSIAN RANDOM FIELDS ON UNDIRECTED GRAPHS
The second part follows since the eigenvalues λ are contained in the convex hull
CW of the set W = {σ(ω); ω ∈[−π, π]d} ⊂C a closed subset of the complex plane.
This follows since if λ is an eigenvalue of Ln with eigenvector φ ∈l2(Dn) of norm
one, then
λ = λ∥φ∥2 =

i∈Dn
(Lnφ)iφ⋆
i
(5.81)
=

i∈Dn


k∈Dn
ak−iφk

φ⋆
i
=

i∈Dn



[−π,π]d
1
(2π)d

k∈Dn
φkej⟨ω,k−i⟩σ(ω) dω

φ⋆
i
(5.82)
=
1
(2π)d

[−π,π]d σ(ω)


i∈Dn
φie−j⟨ω,i⟩

2
dω.
(5.83)
But the function g(ω) = 1/(2π)d| 
i∈Dn φie−j⟨ω,i⟩|2 is non-negative and has integral
one

[−π,π]d g(ω) dω = 1 since the φ-vector had l2 norm one. Hence λ ∈CW the convex
hull.
DeﬁningtheeigenvaluedistributionaccordingtoEn(dλ) = 1/|Dn| 
|Dn|
m=1 δλm(dλ),
on CW, then rewrite the pth moment of the eigenvalues according to
lim
n→∞

|Dn|
m=1 λp
m
|Dn|
= lim
n→∞

CW
λpEn(dλ)
(5.84)
(a)
= lim
n→∞
1
(2π)d

[−π,π]d σ p(ω) dω,
(5.85)
where (a) follows from Eqn. 5.74. This is true for all p, implying for any polynomial
P(·) on CW,
lim
n→∞

CW
P(λ)En(dλ) =
1
(2π)d

[−π,π]d P(σ(ω)) dω.
(5.86)
From the Stone–Weierstrass theorem, choose a sequence of polynomials Pj, j = 1, . . .
converging uniformly to h a continuous function on the compact domain CW implying
lim
n→∞

|Dn|
m=1 h(λm)
|Dn|
=
1
(2π)d

[−π,π]d h(σ(ω)) dω.
(5.87)
This is true for all continuous h, and since 0 /∈CW, then choose h(·) = log (·) as the
particular continuous function in the complex plane. Since log λk is unambiguously
deﬁned by systematically taking the main branch of the log-function for 0 /∈CW,
the function log z is analytic in CW and from Chebychev the function can then be
uniformly approximated by polynomials in CW completing the proof of the theorem.
Remark 5.6.1
If L is self-adjoint (symmetric) L = L∗, the Fourier transform σ(·) is real.
In the above theorem it requires the condition 0 /∈CW which is satisﬁed, for example,
when the values of σ(ω) fall in a half plane not including imaginary axis, for example
when the spectrum is never identically zero. Note, if the operator is self-adjoint, then
this is equivalent to saying it is bounded away from 0, so that λ ∈[a, B], a > 0, for

5.6 THE LOG-DETERMINANT COVARIANCE
141
all eigenvalues, and as long as the eigenvalues are bounded away from 0 lying in a
closed set, log is a particular continuous function.
Example 5.18 (Periodic Boundary)
The claim is that, for example, the discrete coefﬁ-
cients for the periodic Fourier transform converge into the inﬁnite transform spectrum.
This would correspond, for example, to the periodic boundary condition most often
studied for cyclo-stationary and the fully stationary limit. The periodic boundary
condition on a rectangular domain is generated via shift on the domain D = {i =
(i1, . . . , id) ∈Zd : (0, . . . , 0) ≤i ≤(n1 −1, . . . , nd −1)} according to
xP
D = {xP
i = x(i mod n), i ∈Zd} ∈Zd.
(5.88)
Remark 5.6.2
The conclusion is qualitatively the same as the classical Toeplitz theo-
rem in a rectangle: the shape does not matter as long as the discretized boundaries are
thin. It is also related to Weyl’s result on the asymptotics of the eigenvalues associated
with an elliptic differential operator.
Example 5.19
Let us examine the following 1D example to understand the asymp-
totics. Let Ln be an n × n operator matrix with i, j entries Ln(i, j) = aj−i, j −i ∈S. Then,
L2n is given by
L2
n(i, j) =
n

k=1
Ln(i, k)Ln(k, j)
implying tr(L2
n) =
n

i=1
n

k=1
ai−kak−i.
(5.89)
The trace reduces to
lim
n→∞
1
n tr(L2
n) = lim
n→∞a2
0 + 2

1 −1
n

a1a−1 + 2

1 −2
n

a2a−2 . . .
+ 2

1 −n −1
n

an−1a−(n−1),
(5.90)
= a2
0 + 2
∞

j∈S
aja−j = (a ∗a)0 = 1
2π
 2π
0
σ 2(ω) dω.
(5.91)
This is Eqn. 5.74 for p = 2.
Example 5.20
Examine the Laplacian Example 5.9 in 1D −(∂2/∂x2) + a with periodic
boundary conditions:
(LnX)i = −Xi−1 + (a + 2)Xi −Xi+1
(5.92)
with eigenvalues λk = a + 2 −2 cos(2πk/n) and therefore covariance normalizer
det−1/2 Kn = det Ln =
n

k=1

a + 2 −2 cos 2πk
n

.
(5.93)
Theorem 5.17 states that since σ(ω) = (a + 2) −2 cos ω,
lim
n→∞
1
n log det K−1
n
= lim
n→∞
2 
n
k=1 log (a + 2 −2 cos(2πk/n))
n
= 1
2π
 2π
0
log (a + 2 −2 cos ω) dω.
(5.94)

142
5 GAUSSIAN RANDOM FIELDS ON UNDIRECTED GRAPHS
5.6.2 The Asymptotic Covariance and log-Normalizer
We shall now examine the log-determinant of the covariance (log-normalizer) of the stationary
Gaussian ﬁelds generated from the ﬁnite graphs. We shall exploit the fact that for stationary ﬁelds
limits on increasing cylinders given a log-determinant are given by the integral of the spectrum
independent of the boundary.
Corollary 5.21
Let X be a Gaussian process induced by operator Ln on l2(Dn) satisfying
Theorem 5.17 solving the difference equation LnX = W on Dn, W white Gaussian noise.
Then the normalized log-determinant of the covariance of the process converges to the integral
of the log-spectrum, so that
lim
n→∞
1
|Dn| log det KXD =
1
(2π)d

(−π,π)d log S(ω) dω,
(5.95)
where S(ω) =
1
|σ(ω)|2 =
1
| 
h ahej⟨ω,h⟩|2 .
(5.96)
Proof
From the deﬁnition of the operator Eqn. 5.15 KXDn = (L∗
DnLDn)−1, and from
Theorem5.17(Eqn.5.75), thenormalizingasymptotictraceisgivenbythelogspectrum
of the stationary operator (LL∗)−1. Then S(ω) = 1/|σ(ω)|2 is the eigenvalue of this
operator satisfying
(L∗Lφ(ω))i =

s∈S
as(Lφ(ω))i−s =

s∈S
asσ(ω)ej⟨ω,i−s⟩= |σ(ω)|2ej⟨ω,i⟩.
5.7 The Entropy Rates of the Stationary Process
We shall now examine the entropy of stationary Gaussian ﬁelds generated from the ﬁnite graphs
exploiting the fact that for stationary ﬁelds limits on increasing cylinders of the normalized
entropies converge to the so-called entropy rate. Denote the differential entropy rates for the
continuum-valued Gaussian ﬁelds in small letters h(X). The entropy of the real-valued Gaussian
process Xi, i ∈D is given by
1
|Dn|h(Xi, i ∈D) = 1
2
log det 2πeKXD
|Dn|

.
(5.97)
As we now show, there is a limiting entropy rate for the large graphs of the stationary processes
generated via shift invariant operators. Theorem 5.17 tells us that the limiting entropy is given by
the logarithm of the spectrum.
Corollary 5.22
Let X be a Gaussian process induced by operator Ln on l2(Dn) satisfying
Theorem 5.17 solving the difference equation LnX = W on Dn, W white Gaussian noise.
Then the normalized log-determinant of the covariance of the process converges to the integral
of the log-spectrum, and the differential entropy rate becomes
lim
n→∞
1
|Dn|h(Xi, i ∈Dn) = 1
2

1
(2π)d

log 2πeS(ω) dω

.
(5.98)
Proof
Substituting into Eqn. 5.97 the result for the log-determinant covariance from
Corollary 5.21 given by the log-spectral density gives the result.

5.7 THE ENTROPY RATES OF THE STATIONARY PROCESS
143
5.7.1 Burg’s Maximum Entropy Auto-regressive Processes on Zd
We will now show that stationary processes induced through auto-regressive operators are
maximum entropy. This is Burg’s maximum-entropy principle for Gaussian stationary processes.
Theorem 5.23
Let Xi, i ∈Zd be a real-valued stationary Gaussian process with covariances
Kτ , τ ∈S ⊂Zd, with Kτ = K−τ . Then the process maximizing entropy rate 1/(2π)d 
[−π,π]d
log 2πeS(ω) dω with covariance constraints
Kτ =
1
(2π)d

[−π,π]d S(ω)ej⟨ω,τ⟩dω,
τ ∈S −S
(5.99)
satisﬁes the difference equation
LX = W,
(5.100)
with the difference operator L(i, j) = aj−i, j −i ∈S where
as =

1
√S(ω)e−j⟨ω,h⟩dω,
s ∈S.
(5.101)
Proof
Maximizing the entropy subject to the constraints gives the equation which
the maximum entropy solution must satisfy
max
S(·)
1
(2π)d

[−π,π]d log S(ω) dω +

τ∈S−S
λτ
1
(2π)d

[−π,π]d S(ω)ej⟨ω,τ⟩dω
(5.102)
giving
1
S(ω) +

τ∈S−S
λτ ej⟨ω,τ⟩= 0,
(5.103)
implying the maximum entropy spectrum is of the form
S(ω) =
1

τ∈S−S λτ ej⟨ω,τ⟩.
(5.104)
The spectrum of the ﬁnite-difference process is
S(ω) =
1
| 
s∈S asej⟨ω,s⟩|2 =
1

s∈S

s′∈S asas′ej⟨ω,s−s′⟩
(5.105)
=
1

τ∈S−S

s∈S asaτ+sej⟨ω,τ⟩.
(5.106)
Choosing
λτ =

s∈S
asaτ+s,
τ ∈S
(5.107)
demonstrates the maximum entropy form and clearly as ↔1/√S(ω) form a Fourier
transform pair.

144
5 GAUSSIAN RANDOM FIELDS ON UNDIRECTED GRAPHS
5.8 Generalized Auto-Regressive Image Modelling via
Maximum-Likelihood Estimation
With the above asymptotic results we have turned texture modelling into spectral analysis analo-
gous to ARMA modelling for stationary time series. The integral of the log-spectrum is precisely
what is needed for the normalizer in the likelihood function of any kind of estimation. For
this examine the maximum-likelihood estimation. Let XD = {Xi(ω), i ∈D} be a realization of
mean µ-Gaussian random ﬁeld with covariance KD = (LDL∗
D)−1 solving LDX(ω) = W(ω), W(ω)
white noise and LD the stationary difference operator interior to the graph D determined by the
coefﬁcients as, s ∈S:
(LDf)i =

s∈S
asfi+s,
i ∈D/∂D,
(5.108)
and with LD having boundary conditions on ∂D.
Parametric maximum-likelihood estimates of the operator representation of the ﬁeld are
deﬁned in the standard way.
Deﬁnition 5.24
Then the maximum-likelihood estimate LMLE determining the covari-
ance KMLE maximizing likelihood are given by the parameters
{aMLE
s
, s ∈S} = arg max
{as,s∈S}
−log det KD −∥LDX∥2.
(5.109)
Then we have the following.
Theorem 5.25
The asymptotic maximum likelihood estimators
{aAMLE
s
, s ∈S} = arg max
{as,s∈S}
−

[−π,π]d log |

s∈S
asej⟨ω,s⟩| dω −∥LDX∥2
|D|
,
(5.110)
are asymptotic MLE’s in the sense that

−

[−π,π]d log |

s∈S
aAMLE
s
ej⟨ω,s⟩| dω −
∥LAMLE
D
X∥2
|D|
+
log det KMLE
D
|D|
+
∥LMLE
D
X∥2
|D|
 ≤O(|∂D|)
|D|
.
(5.111)
Example 5.26 (Isotropic Laplacian)
Examine as the ﬁrst example the Laplacian in R2
made non-singular − + a, where a > 0 is the constant in Hooke’s law corresponding
to a restoring force returning the texture ﬁeld to its original average displacement,
and  = (∂2/∂x2) + (∂2/∂y2). Model regions in the image {Xi, i ∈Zm2} as Gaussian
random ﬁelds with mean µ and covariance structures induced via the operator, the
discrete Laplacian operator inducing nearest-neighbor structure:
L Xi = (− + a)Xi i ∈Z2,
(5.112)
where Xi = Xi1−1,i2 + Xi1+1,i2 + Xi1,i2−1 + Xi1,i2+1 −4Xi1,i2,
(5.113)
The noise process is taken to be white Gaussian noise with mean µ and variance
σ 2. The three parameters, a and σ 2, completely specify the model. With 0-boundary,
L induces the random ﬁeld according to
−

Xi1+1,i2 + Xi1−1,i2 + Xi1,i2+1 + Xi1,i2−1
	
+ (4 + a)Xi1,i2 = Wi1,i2,
(5.114)

5.8 GENERALIZED AUTO-REGRESSIVE IMAGE MODELLING
145
with X0,i2 = Xi1,0 = Xn,i2 = Xi1,n = 0.
For L = − + a, the spectral density σ(ω), ω = (ω1, ω2) of L is
σ(ω) = 2
2

k=1
(1 −cos ωk) + a,
(5.115)
yielding the normalization used in the large size limit of the shape with n = m2 pixels:
log det1/2 KD
n
= 1
n
n

m=1
log λm
(5.116)
=
1
4π2

[−π,π]2 log

2
2

k=1
(1 −cos ωk) + a

dω + O(|∂D|)
n
.
(5.117)
Apply ML-estimation to the normalized log-likelihood function
(ˆσ 2, ˆa) = arg max
{σ 2,a}
−1
2 log σ 2 + 1
2n
n

m=1
log λ2
m −
1
2nσ 2 ∥LX∥2.
(5.118)
Since the log-partition function is independent of the variance, the maximum
likelihood estimate of σ 2 becomes
ˆσ 2 = arg max
σ 2
−1
2 log σ 2 + 1
2n

m
log λ2
m −
1
2nσ 2 ∥LX∥2
(5.119)
= ∥LX∥2
n
.
(5.120)
Upon substitution, then estimating the restoring force and using the asymptotic
eigenvalue expression gives
ˆa = arg max
a
−1
2 log ∥LX∥2
n
+ 1
2n
n

m=1
log λ2
m −1
2
= arg max
a
−1
2 log ∥LX∥2
n
+
1
4π2

[−π,π)2 log |σ(ω)|dω −1
2
= arg max
a

−1
2 log

i∈D | −(X)i + aXi|2
n
+ 1
4π2

[−π,π]2 log

2
2

k=1
(1 −cos ωk) + a

dω

.
(5.121)
One of the integrations can be done in closed form, yielding
1
4π2

[−π,π]2 log

2
2

k=1
(1 −cos ωk) + a

dω
= 1
2π

[−π,π]
log 4 + a −2 cos ω1 +

(4 + a −2 cos ω1)2 −4
2
dω1
(5.122)
with the remaining integration performed by numerical quadrature.

146
5 GAUSSIAN RANDOM FIELDS ON UNDIRECTED GRAPHS
124
125
126
127
128
129
130
131
132
0
10
20
30
Estimated Mean
Histograms of 200 trials
44
46
48
50
52
54
56
58
60
62
0
10
20
30
Estimated Noise Std Deviation
0.6
0.8
1
1.2
1.4
1.6
1.8
2
Estimated Restoring Force
0
10
20
30
Figure 5.7 Showing maximum-likelihood estimates of µ, σ, a from an experiment of 200 random
ﬁelds generated with µ = 128, σ = 50, and a = 1. Results courtesy of David Mafﬁt, Washington
University at St. Louis.
(1)
(2)
(3)
(4)
Figure 5.8 Panels 1–3 show example micrographs containing mitochondria and actin–myosin com-
plexes; panel 4 illustrates the hand segmentations of the mitochondria and actin–myosin shapes
used to estimate the oriented texture parameters. Everything not labelled in gray is not mito-
chondria or actin–myosin. Data taken from Dr. Jeffrey Safﬁtz of the Department of Pathology at
Washington University.
Results of experiments measuring how well various model parameters can be
estimated from random ﬁelds with known parameters are presented in Figure 5.7.
Experiments were performed corresponding to the data shown in Figures 5.2 and 5.3
illustrating the effect of varying the noise standard deviations and the various param-
eters of the differential operator. For this the noise ﬁelds were generated as a set of
independent and identically distributed Gaussian random variables with mean µ and
standard deviation σ, and solved for the random ﬁeld as a ﬁxed point of Eqn. 5.112.
Figure 5.7 shows the distributions of the estimates from random ﬁeld realizations
generated with µ = 128, σ = 50, and a = 1. Shown in Figure 5.8 are examples of esti-
mating the parameters of Eqn. 5.112 with two different model types: Mitochondria and

5.8 GENERALIZED AUTO-REGRESSIVE IMAGE MODELLING
147
Table 5.1a Showing the maximum-likelihood estimates of the parameters µ, variance σ 2, and restoring
force a in the isotropic model.
Mean Gray Level
Variance of White Noise σ 2
Restoring Force a
Mitochondria
58.7
847.3
0.62
Cytoplasm
130.3
2809.9
0.15
Table 5.1b Estimates of the texture parameters a11; a12; a22 for the actin–myosin complex and the
mitochondria for the orientation dependent model.
Texture
Mean
a11
a12
a22
Standard Deviations
Actin–myosin
87.70
0.87
−0.68
0.82
64.16
Mitochondria
179.56
0.98
0.02
1.03
55.45
cytoplasm. The data are from rabbit-kidney cells at 20,000 times magniﬁcation. Panels
1–3 show the micrograph data, with the two region types, mitochondria (white) and
background (black). Panel 4 shows a pixel-by-pixel segmentation of the image based
on the likelihood of each pixel under the two models solving the Bayesian hypothesis
test for each pixel separately. The parameters estimated from the micrograph data for
these regions are shown in Table 5.1 assuming the Gaussian model (− + a)X = W
has parameters the noise mean µ, noise power σ 2, and restoring force a.
Remark 5.8.3
The beneﬁts of maximum-likelihood applied directly are obvious. Rea-
soning heuristically for a moment, it appears to be common sense that a should be
estimated by the following ratio of inner products a∗= (X, X)/(X, X) that minimizes
the quadratic form ∥−X + aX∥2. After all, this is just the least squares estimate which
ought to perform fairly well. This, however, is not the case and we must be a bit more
careful. Now notice that − is a negative operator (with the proper boundary condi-
tions) so that this will give a negative value for the estimate, but we know that a is
positive. Therefore this estimate must be discarded; the MLE has to be used instead.
5.8.1 Anisotropic Textures
Thus far Gaussian ﬁelds which are isotropic and homogeneous have been described with textures
which are rotationally invariant (no preferred direction). However, there are clearly applications
for which this is not appropriate. Shown in the bottom row of Figure 5.8 are exquisite examples
of actin–myosin complexes with an orientation to the texture ﬁeld. A great deal of work has been
done on the representation of oriented textured ﬁelds, such as those evidenced in wood grains
and or ﬁnger prints (e.g., see [144]). Now we extend to the more general operator formulation
L = −
2
i,j=1 aij(∂2/(∂xi∂xj)) + a0, modelling the texture ﬁeld Xi(ω), i ∈Dn = {1 ≤i1, i2 ≤n} and
solve the stochastic equation LX = W with the discrete operator becoming
LXi1,i2 = −a11(Xi1+1,i2 −2Xi1,i2 + Xi1−1,i2) −a22(Xi1,i2+1 −2Xi1,i2 + Xi1,i2−1)
−1
2a12(Xi1+1,i2+1 −Xi1+1,i2−1 −Xi1−1,i2 + Xi1−1,i2−1) + a0Xi1,i2.
(5.123)

148
5 GAUSSIAN RANDOM FIELDS ON UNDIRECTED GRAPHS
For maximum-likelihood ﬁtting, treat the constant a0 and the 2 × 2 symmetric matrix L =
 a11
a12
a21
a22

as unknowns to be estimated. It is convenient to interchange the role of L as an
operator, and L the matrix representation of the operator. The log-determinant normalizer is given
by the Fourier transform
σ(ω) = a0 + 4a11 sin2 ω1
2 + 2a12 sin ω1 sin ω2 + 4a22 sin2 ω2
2 .
(5.124)
Theorem 5.17 obtains
1
n2
n2

m=1
log λm =
1
(2π)2

[−π,π]2 log σ(ω) dω + O(|∂Dn|)
n2
.
(5.125)
Using maximum likelihood the matrix of parameters a0, L can be estimated directly from
the data. The eigenvectors of the estimated matrix ˆL give the principle directions of the texture;
the ratio between the eigenvalues determine how anisotropic it is.
Consider the micrographs shown in panels 4–6 of Figure 5.8. The hand segmentation used
to extract the regions for estimation of the mitochondria and actin–myosin are shown on the
right, with the region means subtracted out. Note it could be assumed that σ is ﬁxed, with the a
parameters then estimated. Since the ML estimate for σ can be computed in closed form, a0 = 1 is
set; this reduces the number of parameters to be searched over. As well, the maximum-likelihood
estimate of the average pixel value ˆµ is subtracted from all the object pixels before analysis.
ThebottomrowofTable5.1liststhemaximum-likelihoodestimatesofthetextureparameters
for this image. For the mitochondria, a11 ≈a22 and a12 ≈0, suggesting the isotropic models are
appropriate. The parameters for the actin–myosin complex suggest a highly oriented texture; the
ratio of eigenvalues is 1.5278/0.1670 = 9.1485, with the principle eigenvector pointing in the
direction (−0.7193, 0.6947), which is nearly diagonal as can be easily observed in the micrograph.
It is reasonable to synthesize the actin–myosin complexes under the model to determine
the appropriateness via visual inspection. For this a white random ﬁeld W is generated, with the
Gaussian random ﬁeld solving LX = W. The random ﬁeld X is solved for using a standard ﬁxed-
point iteration with “checkerboard” interchange, updating the red and black squares on alternate
iterations. Updating all pixel values simultaneously can result in divergence.
The top row of Figure 5.9 shows results of such solutions. Panel 1 shows a 128×128 random
ﬁeld synthesized with zero mean and the texture parameters given in Table 5.1. Panel 2 uses the
same values for a0 and the noise variance a0 but employs a11 = 1.5278, a22 = 0.1670, a12 = 0, which
is analogous to rotating the actin–myosin texture to align its principal directions along the axis. The
MRF model effectively captures the concept of directionality, which here is the essential property
that differentiates the different textures. Panel 3 has a0 = 1 and the same noise variance as the
actin–myosin complex but employed a11 = 2, a22 = 6, and a12 = 0, resulting in a thicker-grained
appearance. Panel 4 has the thicker-grained texture operator rotated by 60◦(as described in the
next section), yielding a11 = 5, a12 = 1.7321, and a22 = 3. The maximum-likelihood estimates of
the texture parameters deduced from these simulations are shown in Table 5.2.
Example 5.27 (Anisotropic Filters: 3-Dimensions)
In 3-dimensions, extend to the
more general operator formulation L = 
3
i,j=1 aij(∂2/(∂xi∂xj)) + a0. Treat the constant
a0 and the 3 × 3 symmetric matrix L as unknowns
L =


a11
a12
a13
a21
a22
a23
a31
a32
a33

.
(5.126)
Given the texture ﬁeld Xi(ω), (i1, i2, i3) ∈Dn = {1 ≤i1, i2, i3 ≤n} with boundary
conditions, ∂X, the unknown parameters are estimated by maximum likelihood. The

5.8 GENERALIZED AUTO-REGRESSIVE IMAGE MODELLING
149
(1)
(2)
(3)
(4)
(5)
(6)
(7)
(8)
(9)
(10)
(11)
(12)
Figure 5.9 Top row: Synthetic Gaussian texture images with directional components. Panels 1–
4 show simulated random ﬁelds with varying parameter choices. Panel 1 shows ﬁelds based on
parameters estimated from micrographs of the actin–myosin complexes; panel 2 shows similar
parameters rotated to align with the Y-axis; panel 3 shows the thicker grain parameters; panel 4
shows these rotated by 60◦. Middle row: Simulated mitochondria textures; panels 5 and 6 show the
preferable ﬁrst-order model and the less-likely second order model, with panels 7 and 8 showing
the mixed and ﬁrst-order models, respectively. Bottom row: Panel 9 shows cityscape observed via
an infrared imager. Panel 10 shows a subimage of urban clutter extracted from the left portion
of the cityscape. (Data courtesy Howard McCauley, Naval Air Warfare Center Weapons Division,
NAWCWPNS, China Lake, CA. Approved for public release; distribution is unlimited.) Panels 11,12
show simulated infrared urban clutter; panel 11 shows the preferable second-order model; panel
12 shows the less-likely ﬁrst order model.
Table 5.2 Maximum-likelihood estimates derived from the simulated images of the top row of Figure 5.9.
Left 3 columns show the parameters used for the simulations; right 3 columns show the estimates.
a11
a12
a22
a11
a12
a22
σ
0.87
−0.68
0.82
0.87
−0.69
0.82
63.57
1.53
0
0.17
1.50
−0.02
0.17
63.35
2
0
6
1.89
0.05
5.73
61.51
5
1.73
3
4.90
1.76
2.99
61.51

150
5 GAUSSIAN RANDOM FIELDS ON UNDIRECTED GRAPHS
log-eigenvalue normalizer given by Theorem 5.17 is given according to
σ(ω) =
1
8π3
&
a0 +

2j sin ω1
2 , 2j sin ω2
2 , 2j sin ω3
2

×


a11
a12
a13
a21
a22
a23
a31
a32
a33




2j sin(ω1/2)
2j sin(ω2/2)
2j sin(ω3/2)




,
(5.127)
with Theorem 5.17 giving
1
n2

m
log λm =
1
8π3

[−π,π]3 log σ(ω) dω + O(|∂Dn|)
n2
.
(5.128)
Example 5.28 (Higher-Order Isotropic Texture Models)
Examine sums of powers
of the Laplacian of the form
L =
M

m=0
(−1)mm = (−1)MaM + · · · + a22 −a1 + a0,
(5.129)
where  is the Laplacian, 0 is deﬁned to be the identity operator, and m = (m−1)
are powers of the Laplacian. Then the Fourier–Bessel transform of the Green’s function
of (5.129) is
H(ω) =
1

M
m=0 amω2m =
1
aMω2M + · · · + a2ω4 + a1ω2 + a0
.
(5.130)
This is a low-pass ﬁlter with DC gain 1/a0. The a’s must be non-negative for H
to be well-behaved. If am = 0 for 0 < m < M, then the resulting ﬁlter
H(ω) =
1
aMω2M + a0
(5.131)
is commonly referred to in the medical imaging ﬁeld as an Mth order Butterworth
ﬁlter with cutoff frequency ωc = (a0/aM)1/M. Eqn. (5.131) is actually the power versus
frequency response of the 1D causal ﬁlters, with associated phase shift, originally
studied by Butterworth, so (5.131) is not strictly a Butterworth ﬁlter in the traditional
sense.
Discretize the second order operator L = a2 △2 −a1 △+ a0 according to
LXi1,i2 = a0Xi1,i2 −a1(Xi1,i2 + Xi1,i2 + Xi1,i2 + Xi1,i2 −4Xi1,i2)
+ a2[Xi1+2,i2 + Xi1−2,i2 + Xi1,i2+2 + Xi1,i2−2
−8(Xi1+1,i2 + Xi1−1,i2 + Xi1,i2+2 + Xi1,i2−2)
+ 2(Xi1+1,i2+1 + Xi1−1,i2+1 + Xi1−1,i2+1 + Xi1−1,i2−1) + 20Xi1,i2].
(5.132)
The corresponding Fourier representation used to compute the log-eigenvalue
sum normalizer is
σ(ω) = a0 −a1(2(cos ω1 + cos ω2) −4) + a2(2(cos 2ω1 + cos 2ω2)
(5.133)
−16(cos ω1 + cos ω2) + 8(cos ω1 cos ω2) + 20).

5.8 GENERALIZED AUTO-REGRESSIVE IMAGE MODELLING
151
Table 5.3 Maximum-likelihood parameter estimates derived from the simulated images
Experiment
a1
a2
Standard Deviation
a1
a2
Standard Deviation
Mitochondria
2.77
0
63.55
2.64
0
61.06
Mitochondria w/a1 = 0
0
0.23
29.92
0
0.22
29.91
Cytoplasm
5.91
0.16
340.01
5.24
0.17
311.93
Cytoplasm w/a2 = 0
7.67
0
395.25
7.16
0
371.36
Cytoplasm w/a2 = 0
7.67
0
395.25
6.54
0.16 (set)
372.72
Urban clutter
0
1.73
136.86
0
1.72
136.27
Urban clutter w/a2 = 0
105.25
0
2913
82.60
0
2294.3
The (set) tag denotes that a parameter was set to the speciﬁed value in order to explore different modeling choices. Means chosen for mitochondria
µ1 = 57.2922, cytoplasm µ2 = 135.2851 and urban clutter µ3 = 106.6847.
Returning to the mitochondria example shown in Figure 5.8, the ﬁrst line of
Table 5.3 shows ML values a1 = 2.77, a2 = 0 supporting the ﬁrst-order Laplacian
models explored previously. The second line shows the results of artiﬁcially setting
a1 = 0. Shown in panels 5 and 6 of the middle row of Figure 5.9 are random ﬁelds
from these parameters. As in the previous section on nonisotropic operators, we set
a0 = 1 and use the closed-form solution of the ML estimate for σ, so we only have
two parameters to maximize over. Again we subtract the average pixel value before
analysis.
The second set of three lines of Table 5.3 show the results of analysing the cyto-
plasm background in the left panel of Figure 5.8. This is the ﬁrst example of a truly
mixed model in which a1 and a2 are both deduced to be nonzero. The last line shows the
result of employing just a ﬁrst-order model. Synthesized textures with the mixed and
ﬁrst-order models are shown in panels 7 and 8 of Figure 5.9. The ﬁfth line illustrates
the deduction from the ﬁrst-order cytoplasm simulation which results if a2 is artiﬁ-
cially set to the value deduced from the original mixed model. As might be expected,
a1 is estimated to be at a lower value, closer to that found in the original mixed
model.
Consider the forward-looking infrared (FLIR) image displayed in the bottom
row panel 9 of Figure 5.9. A 50 × 50 subimage of urban clutter extracted from the left
side of the image is shown in panel 10. Interestingly, the ML parameter estimates for
the urban clutter sample yield a1 = 0, a2 = 1.7278, as listed in the bottom two lines
of Table 5.3, suggesting a second-order Butterworth ﬁlter model; this is a special case
of the “Model A” clutter proposed by US Army Missile Command [145]. As shown
in the last line of the table, if the coefﬁcient a2 = 0 is artiﬁcially restricted and a ﬁrst-
order model is enforced, maximum-likelihood chooses an extremely large a1 value.
Simulated IR clutter using the higher-likelihood second order model and the lower-
likelihood ﬁrst order are shown in the panels 11 and 12 of Figure 5.9. Even at ﬁrst
glance, the clutter synthesized from the second-order model appears to more closely
capture the characteristics of the real clutter than the ﬁrst-order model.
Example 5.29 (Anisotropic Textures with Variable Orientation)
Examine the case
where the principal directions of the texture are variable. Parameterize the matrix
L = L(O) via the unknown rotation O ∈SO(3) describing the orientation of the tex-
ture for the particular instance. The standard way in which differentiation depends
upon the choice of coordinate system choose y = Ox, O an orthogonal matrix with
O =


o11
o12
o13
o21
o22
o23
o31
o32
o33

,
(5.134)

152
5 GAUSSIAN RANDOM FIELDS ON UNDIRECTED GRAPHS
then (∂/∂Xi) = 
3
j=1(∂/∂yj)oji or in shorthand notation (∂/∂x) = O∗(∂/∂y). Iterating
gives (∂2/∂xi∂xj) = 
k,l(∂2/(∂yk∂yl))okiolj, therefore
3

i,j=1
aij
∂2
∂xi∂xj
=
3

k,l=1
bkl
∂2
∂yk∂yl
with bkl = 
ij aijokiolj. Then
B =


b11
b12
b13
b21
b22
b23
b31
b32
b33

= OLO∗,
(5.135)
dictating the relationship between L(O) and the rotation O in the continuum R3.
This transfers directly to ﬁnite differences. Without loss of generality assume L is
of diagonal form and introduce the nuisance parameter O to estimate the matrix
L′(O) = OLO∗. To determine the rotation that makes the estimate L′, use the
Hilbert–Schmidt norm ∥M∥2 = tr(M2) for symmetric matrices M corresponding to
the inner product ⟨M1, M2⟩= tr(M1M2). Then
arg min
O
∥L′ −OLO∗∥2 = arg max
O
tr(L′OLO∗).
(5.136)
To solve the maximum problem express L and L′ in their spectral representation
L =
3

ν=1
λνEν,
L′ =
3

ν=1
λ′
νE′
ν,
(5.137)
with the E’s projection operators corresponding to respective resolutions of the
identity. Then
tr(L′OLO∗) =
3

ν,µ=1
λνλ′
µcνµ
(5.138)
with the coefﬁcients cνµ = tr(EνOE′µO∗).
Express the projections Eν, OE′µO∗expressed in terms of the unit vectors e =
(ei; i = 1, 2, 3) and e′ = (e′
i; i = 1, 2, 3), we have
cνµ = tr(EνE′
µ) =

i,j
eieje′
ie′
j =


i
eie′
i


2
≥0,
(5.139)
so that the c-coefﬁcients are non-negative. On the other hand
3

µ=1
cνµ =

µ
tr(EνOE′
µO∗) = tr(Eν) = 1,
since 
µ E′µ = Identity and any projection operator to a 1D sub-space has trace 1.
Similarly 
ν cνµ = 1, ∀µ.
Hence matrix C belongs to the set D of doubly stochastic matrices. Since D is
convex and compact any of its elements can be expressed as a convex combination of
the extreme points of D, i.e. the permutation matrices k, so that
C =
6

k=1
wkk;
wk ≥0,

wk = 1.

5.8 GENERALIZED AUTO-REGRESSIVE IMAGE MODELLING
153
The maximum is attained by solving maxk

ν λLνλB
(k)ν , where the vector k means
the reordering of (1, 2, 3) by the permutation matrix k. This is a classical problem
(see Chapter 10 [146]). The answer is that if we have ordered the eigenvalues λL
i in
non-decreasing order then we should choose the k that orders the λ also in increasing
order.
Now we have the solution. After having estimated the L-matrix solve for its
eigenelements. Order the eigen-vectors so that the corresponding eigen-values form
a nondecreasing sequence and use them to form an orthogonal matrix (in the same
order): This rotation is the solution. Since eigen-vectors are determined up to its sign,
there will typically be 8 solutions, it does not matter which one we choose.
Extending to differential operators of order greater than two presents some
new difﬁculties. Indeed, instead of quadratic forms higher degree forms will be
encountered with various complications.

6
T H E C A N O N I C A L R E P R E S E N TAT I O N S O F
G E N E R A L PAT T E R N T H E O RY
ABSTRACT
Pattern theory is combinatory in spirit or, to use a fashionable term, connection-
ist: complex structures are built from simpler ones. To construct more general patterns, we
will generalize from combinations of sites to combinations of primitives, termed genera-
tors, which are structured sets. The interactions between generators is imposed via the
directed and undirected graph structures, deﬁning how the variables at the sites of the
graph interact with their neighbors in the graph. Probabilistic structures on the representa-
tions allow for expressing the variation of natural patterns. Canonical representations are
established demonstrating a uniﬁed manner for viewing DAGs, MRFs, Gaussian random ﬁelds
and probabilistic formal languages.
6.1 The Generators, Conﬁgurations, and Regularity of Patterns
To construct more general patterns, the random variables and state spaces are generalized
via the introduction of primitives called generators which are structured sets. The generators
become the random variables at the nodes of the graphs; the structure is imposed via the
edge relations in the graphs constraining how the sets at the vertices of the graph interact with
their neighbors. The graphs imply the conditional probabilities associated with the directed and
undirected random ﬁelds.
When moving to the more general patterns on arbitrary graphs σ = {D, E} it is clear that
building structure requires the aggregation of the ﬁeld variables. This is precisely what the gen-
erators do! Examine the goal of moving from unstructured representations of pixelated images to
those containing edge or line sites and continuing to object elements and highly structured sets. In
an attempt to add structure, more complex abstractions are deﬁned; the edge vertices which are
aggregations of the pixels, and line and boundary vertices which are in turn aggregations of the
edge and line vertices, respectively. The generators arise as aggregations of the ﬁeld elements.
Inanatomisticspiritwebuildtherepresentationsbycombiningsimpleprimitives, generators.
To begin with the generators will be treated as abstract mathematical entities whose only structure
willbeexpressedintermsofbondsdeﬁninghowtheycommunicateonthegraph. Themathematical
objects, the generators, are from the generator space G and will appear in many forms: they can be
positive pixel values in an image, states in a Markov chain, geometric objects such as vectors and
surface elements, or rewriting rules in language theory.
As before, begin with the connector graph σ with n = |σ| the number of vertices in σ. At each
vertex place one of the generators g ∈G the generator space. A pattern or conﬁguration is denoted
c(σ) = σ(g1, . . . , gn). Associate with the structured generators bonds, information which is used to
communicate to other neighbors in the graph. The bonds establish the local rules of interaction;
an edge in the graph corresponds to a bond between generators. As the generators are stitched
together, only certain patterns will have the structure regularity of the pattern class. To enforce
rigid structure, a bond function β : G →B is associated to each generator and its target deﬁned by
edges e = (i, j) between pairs of generators which are neighbors in the graph. If two vertices i, j ∈D
of the graph are neighbors, j ∈Ni, i ∈Nj, then the generator gi has a bond βj(gi) which must agree
with the bond βi(gj) from gj. For sites i, j which have no edge, we assume the null bonds, with
agreement trivially satisﬁed. The value of the bond couples determine how the generators interact
and therefore how the patterns form.
The conﬁgurations are illustrated as in Figure 6.1. Panel 1 shows two generators in the graph
with their bonds. Panel 2 shows a graph with the set of 6 vertices in D with associated edges
dictating where the bonds must form; the right panel, the set of generators with their bonds.
154

6.1 GENERATORS, CONFIGURATIONS AND REGULARITY OF PATTERNS
155
j
1
2
βj
i
βi
gi
g1
gj
g3
g4
g2
g6
g5
Figure 6.1 Panel 1 shows two generators with their bonds; panel 2 shows a conﬁguration with
the set of generators, bonds, and edges in the graph.
The bonds constrain the conﬁguration deﬁning which ones are regular; the space of regular
conﬁgurations will generally be a subset of the full conﬁguration space Gn.
Deﬁnition 6.1
Deﬁne the unconstrained conﬁgurations on graph σ = {D, E} as a
collection of n-generators,
C(σ) = {c(σ) = σ(g1, g2, . . . , gn) : gi ∈G(i), i = 1, . . . , n}.
(6.1)
A conﬁguration is determined by its content, the set of generators in G making it up,
content(c) = (g1, g2, . . . , gn), and its combinatory structure determined by the graph with
the internal bonds of the conﬁguration denoted int(c), and the set of the remaining ones, the
external bonds, denoted by ext(c).
Deﬁning the bond function ρ(·, ·) : B × B →(TRUE, FALSE), then a conﬁguration
c(σ) = σ(g1, g2, · · · , gn) ∈C is said to be regular when the bond relation is satisﬁed over
the graph σ = (D, E):
4
e=(i,j)∈E
ρ(βj(gi), βi(gj)) = TRUE.
(6.2)
The space of regular conﬁgurations, a subset of the full conﬁguration space, is
denoted as
CR ⊆C(σ) = G(1) × · · · × G(n).
For many problems, we will also be interested in conﬁguration spaces associated with the
collection of graphs CR() = ∪σ∈CR(σ).
Formula 6.2 is a structural formula expressing relations between the generators at the nodes
of the graph. Sometimes local regularity may be distinguishable in which all of the bonds in the
graph type σ are satisﬁed, although the graph type itself may not be in the set of allowed graphs.
The connected bonds are essentially the internal bonds of the conﬁguration, with the set of the
remaining ones, the external bonds.
In general, connector graphs may be sparse, so that many of the sites do not have an edge.
The right graph in Figure 6.1 is such a case. It is then natural to explicitly index the bonds at each
generator βk(g), k = 1, . . . , ω(g), with total arity ω(g) designating the number of bonds attached to g.
Example 6.2 (Regular Lattice Magnetic Spins)
In the magnetic spin models such as
the Ising model from statistical physics, σ = LATTICE, the generators are the magnetic
dipoles in plus or minus orientation G = {+, −}, and the bond values are the terms
which multiply to form the energy function: B = {−1, +1} with ω(g) = 2, 4, 6 for the

156
6 THE CANONICAL REPRESENTATIONS OF GENERAL PATTERN THEORY
. . .
g1
g2
bin(g1)
bin(g1)
bin(g2)
bin(g2)
bout(g2)
bout(g1)
bout(g1)
bout(g2)
bin(gn)
bin(gn)
bout(gn)
bout(gn)
. . .
g1
gn
gn
g2
Figure 6.2 Top
and
bottom
rows
show
conﬁgurations
LINEAR(n, g1, . . . , gn)
and
CYCLIC(n, g1, . . . , gn).
1,2,3D models, respectively. For the Ising case, ρ(β, β′) = TRUE for all (β, β′) ∈B2;
all +1, −1 conﬁgurations are allowed.
In 1D let D = {1 ≤i ≤n} and choose the two bond values to be β1(g) = β2(g)
deﬁned by β(+) = 1, β(−) = −1, with the truth table taking the form for the generators
ρ : B × B =
+1
−1
+1
ρ = T
ρ = T
−1
ρ = T
ρ = T
,
and
CR = C = {+, −}n.
(6.3)
Example 6.3 (Unclosed and Closed Contours)
Examine unclosed contours in the
plane, and generate the boundaries from line segments. Then σ = LINEAR, ω(g) = 2,
generators are arcs in the plane g = (z1, z2); z1, z2 ∈R2, G = R4, and bond-values
are start and end-points of the generators, βin(g) = z1; βout(g) = z2. As depicted in
Figure 6.2, notice that the boundary vertex generators have one less in and out bond
for LINEAR. For cyclic, the last generator interacts with the ﬁrst generator.
The continuity constraints mean that consecutive line segments are joined to each other and
enforced via the regularity of bond consistency R(LINEAR) so that the outbond of the ith generator
equals the inbond of the i + 1st:
ρ(βout(gi), βin(gi+1)) = TRUE
if and only if
βout(gi) = βin(gi+1).
(6.4)
For closed contours, σ = CYCLIC, with the ﬁrst and last vertices having an arc in the graph,
adding the cyclic regularity enforced by the added bond relation ρ(βout(gn), βin(g1)) = TRUE
(bottom row of Figure 6.2).
Here the generators are chosen as directed line segments, which could be generalized to other
arc elements from conic sections or other curve families. For curves in R3, z1, z2 ∈R3, G = R6.
Example 6.4 (Triangulated Graphs)
In cortical surface generation the surfaces are
generated from vertices using triangulations of the sphere. Panel 1 of Figure 6.3 depicts
a triangulated sphere, the generators being
gi = (v(1)
i
, v(2)
i
, v(3)
i
) ∈(S2)3,
(6.5)
with ω(g) = 3, and βj(gi) = v(j)
i ; j = 1, 2, 3. Thus G = R9. The graph family  = ∪n
TRIANG(n) expresses the topology corresponding to patches which are topolog-
ically connected according to the triangulated sphere topology. A complete tri-
angulation with congruent triangles is only possible for n = 4, 6, 8, 12, 20, the
Platonic solids. Generally large n-values are used, so that noncongruent triangles form
thepatterns. Panels2and3showthetriangulatedgraphsassociatedwiththebounding
closed surface of the hippocampus in the human brain and the macaque cortex.

6.1 GENERATORS, CONFIGURATIONS AND REGULARITY OF PATTERNS
157
Rotation
Generator
(2)
(3)
Object Type
Translation
(1)
Figure 6.3 Top row: Panel 1 shows the triangulated graph for the template representing amoeba
corresponding to spherical closed surfaces. Generators are gi =(v1, v2, v3), elements of R9. Panel
2 shows the triangulated for the closed surface representing the bounding closed surface of the
hippocampus in the human brain. Panel 3 shows the surface representation of a macaque brain
from the laboratory of David Van Essen. Bottom row: Shows a generator shown in standard
position, orientation under rotation and translation transformation. (see plate 3).
Alternatively, choose the generators to be the vertices in G = R3, and the
generating points are the n-points sampled around the sphere:
gi,j =

sin 2πi
m cos 2πj
m , sin 2πi
m
sin 2πj
m , cos 2πi
m

,
(i, j) ∈Zm2.
(6.6)
Example 6.5
In the setup for observing moving bodies, say for tracking an aircraft,
deﬁne each generator to an aircraft, G = {AIRCRAFTS} of speciﬁed types in speciﬁed
positions and orientation. The center of gravity is located at some (x1, x2, x3) ∈R3;
the attitude is the natural coordinate axes of the aircraft forming given angles with
those of the inertial frame. Then dim(G) = 6 . A convenient choice of generator space
is to let each generator consist of an arc, for example a line segment, in location and
orientation space.
A system of moving bodies need not be constrained by local regularity unless
they do not occupy common volume. Then a generator has an indeﬁnite number of
bonds, all of whose values should be the set occupied by the body. The bond relation
takes the form
β1 ∩β2 = ∅.
(6.7)
A conﬁguration for tracking is a linear graph LINEAR(m1) of m1 generators
(airplanes) speciﬁed via the special Euclidean motion. Multiple airplanes correspond
to unions of linear graphs, MULT(m2, LINEAR). Examine the set of graph transfor-
mations associated with discovering tracks. These consist of such transformations as
depicted in the Figure in the right panel of Figure 6.7 and in Figure 6.4 for airplane
tracking.
Example 6.6 (Finite State Graphs)
Let Xi, i = 1, 2, . . .
be a realization from the
binary 1-1s languages X0 = {0, 1} of binary strings not containing 2 consecutive
1 symbols in a row. Let the generators be the binary values, G = {0, 1}.

158
6 THE CANONICAL REPRESENTATIONS OF GENERAL PATTERN THEORY
Figure 6.4 Figure showing graph transformations forming and deleting tracks and track segments.
With  = LINEAR, ω(g) = 2 with β1(g) = β2(g) = g ∈B = G, choose the
bond-relation truth table to respect the 1-1 constraint:
ρ : B × B =
0
1
0
ρ = T
ρ = T
1
ρ = T
ρ = F
.
(6.8)
Theregularconﬁgurationsbecomec(LINEAR) ∈CR(LINEAR)ifandonlyifβout(gi) =
βin(gi+1), i = 1, . . . , n −1.
Example 6.7 (Model-Order, Sinusoid estimation)
In a sinusoid estimation such as
for NMR [22], signals are constructed from sinusoids x(t) = 
n
i=1 ai sin ωite−λit, gen-
erators are sinusoids. Then a conﬁguration c(LINEAR) = LINEAR(a1, . . . , an), linear
graph denoting order so that ai is associated with sin ωit, and CR(n) = C(n) = Rn. All
choices of amplitudes are allowed: it is a vector space. The dimension of the model
will not generally be known, so that C() = ∪∞
n=0Rn.
6.2 The Generators of Formal Languages and Grammars
The formal grammars of Chapter 3, Section 3.10 are an exquisite illustration of the pattern theory.
They directly provide a mechanism for building the space of directed graphs  recursively: the
grammar speciﬁes the legal rules of transformation of one graph type to another. Naturally, ﬁnite-state
grammars provide transformations which build the space of linear graphs  = LINEAR, the
context-free grammars the space of trees  = TREE and ﬁnally context-sensitive grammars provide
transformations through the partially ordered set graphs  = POSET.
What is so beautiful about the formal language theory of Chomsky is that the grammars
precisely specify the generators for the conﬁgurations and the rules of transformation. They also
illustrate one more non-trivial movement to more abstract generators. The patterns (strings) are
built by connecting the generators in the various graph structures to satisfy the consistency rela-
tionship deﬁned by the bond structure: LINEAR, TREE, DAG. The connection of the Chomsky
transformation rules is that the rules of transformation are the generators. These are the formal
grammars! The transformations allowed by the rules are constrained by the consistency placed
via the bonds between the generating rules.
The generators are the grammatical rules themselves, and the bond-values are subsets of the
non-terminals and terminals on the right and left hand sides of the rules, elements of the terminal
and non-terminal set. To see the forms for the rules of the three grammar types, refer to Figure 6.5
which shows various examples. Left panel shows a ﬁnite-state grammar production rule A →B,
ωin = ωout = 1. The middle panel shows a production rule (generator) S →NP VP for a context-
free grammar; ωin(g) = 1, ωout(g) = 2, βin(g) = S, βout1(g) = NP, and βout2(g) = VP. The right
panel shows a context-sensitive production rule A B →C D.

6.2 THE GENERATORS OF FORMAL LANGUAGES AND GRAMMARS
159
A
A
A
B
Regular
Context-Free
Context-Sensitive
g
a
a
b
b
C
C
g
g
Figure 6.5 Rewriting rules corresponding to the ﬁnite-state, context-free, and context-sensitive
generators.
The local regularity comes from the fact that for two bonds β, β′ joined in the graph, then
outbond β = inbond β′.
(6.9)
In other words, when a rewriting rule has produced certain non-terminal syntactic variables as
their daughters, then this rule can only be connected to a rule with those syntactic variables on
their left hand side: no jumps are allowed. In languages, we shall be interested in the set of strings
having only terminals.
Deﬁnition 6.8
Associated with every conﬁguration deﬁne the closed or internal bonds
of the conﬁguration, denoted int(c), and the set of the remaining ones, the external bonds
denoted as ext(c).
Theorem 6.9
Given is a formal grammar G(P) = < VN, VT, R, S > with language L(G).
Then let the regular conﬁgurations be constructed from generators the rules G = R, with
the bonds the non-terminals and terminals on the right and left hand sides of the rules in
B = VN ∪VT:
1. for ﬁnite-state, σ = LINEAR with generators and bonds
A
g→wB, βin(g) = A, βout(g) = A′, ω(g) = 2;
(6.10)
2. for context-free, σ = TREE with generators and bonds
A
g→ψ, βin(g) = A, βout(g) = ψ, ω(g) = 1 + |ψ|;
(6.11)
3. for context-sensitive, σ = DAG with generators and bonds
αAβ
g→αψβ, βin(g) = αAβ, βout(g) = ψ, ω(g) = |αAβ| + |ψ|.
(6.12)
The regular conﬁgurations c(σ) = σ(n, g1, g2, . . . , gn) ∈CR(σ) satisfy
4
e=(i,i′)∈E
ρ(βj(e)(gi), βj′(e)(gi′)) = TRUE, where ρ(β, β′) = TRUE ⇐⇒β = β′.
(6.13)
Then the regular conﬁgurations CR(σ) ⊂CR(σ), σ ∈ = {LINEAR, TREE, DAG}
having no unclosed external bonds is the formal language:
¯CR(σ) = {c(σ) = σ(n, g1, . . . , gn) ∈CR(σ) : ext(c) = φ} = L(G).
(6.14)
Figure 6.5 depicts the various forms of the generators for the three grammar types. The
special rewrite rules involving the sentence symbol, S →ψ have ωin(g) = 0, and the rules
αAβ
g→ψ ∈V∗
T with only terminal symbols on the right hand side, βout(g) = 0.

160
6 THE CANONICAL REPRESENTATIONS OF GENERAL PATTERN THEORY
The probabilistic versions of formal languages, the stochastic languages, the set of strings
generated by probabilistic application of the rules. For the ﬁnite-state (regular) and context-free
languages, these are Markov chains and multi-type random branching processes.
Example 6.10 (Finite-State Languages)
Examine
the
ﬁnite-state
languages
of
Example 3.43. Begin with the 1-1 constraint language, VN = {ZERO, ONE} with
S = ZERO >, with the generators the transitions in the state graph:
G = R = {ONE r1
→0ZERO, ZERO r2
→0ZERO, ZERO r3
→1ONE}.
(6.15)
Elements in the language can end in either states, augment the rule set with the
terminating rules ZERO r4
→1, ZERO r5
→0, ONE r6
→0.
For parity languages, VN = {EVEN, ODD} with S = EVEN, and the rules
(generators)
G = R = {EVEN r1
→0EVEN, EVEN r2
→1ODD, ODD r3
→0ODD, ODD r4
→1EVEN}.
(6.16)
Since only strings ending in EVEN parity are in the language, augment with the
terminating rules EVEN r5
→0, ODD r6
→1. The graph is σ = LINEAR, arity ω(g) = 2
with in- and out-bond values
βin(g) = LHS(g), βout(g) = RHS(g),
(6.17)
with
ρ(βout(gi), βin(gi+1)) = TRUE
if and only if RHS(gi) = LHS(gi+1).
The string 001001 is regular in the 1-1 language with generator representation
LINEAR(r2, r2, r3, r1, r2, r4). The parity string 001001 in the EVEN parity language is
regular with generator representation LINEAR(r1, r1, r2, r3, r3, r6). They both satisfy
the structure relations βout(gn) = φ with
βout(gi) = βin(gi+1)
for i = 1, . . . , n −1.
(6.18)
For the bigram and trigram language models the m-gram model treats a string of
words as a realization of an mth-order Markov chain in which the transition probability
is the conditional probability of a word in the string given by the previous m−1 words.
The generators for the m-ary processes are
wi−m, wi−(m−1), . . . , wi
)
*+
,
A
→wi−(m−1), wi−(m−2), . . . , wi+1
)
*+
,
wi+1 A′
wi−m, wi−(m−1), . . . , wi
)
*+
,
A
→wi+1−m, wi+1−(m−1), wi+1−(m−2), . . . , wi, φ)
)
*+
,
wi+1
where φ = null (punctuation). Notice, for m = 1, m = 2 these rules are precisely the
generators of node i, (Xi, Xπi) in Theorem 6.19,with in-arity 1 over the entire graph.
An element of the conﬁguration space C(LINEAR) consists of a set of generators
which are the rewrite rules G = R placed at the vertices of the directed graph. A
conﬁguration consists of n nodes in a linear graph c = LINEAR(n, r1, . . . , rn); rule
ri ∈R transforms node i.
Example 6.11 (Phrase Structure Grammars)
Examine the structures associated with
context free languages and their associated tree graphs.

6.2 THE GENERATORS OF FORMAL LANGUAGES AND GRAMMARS
161
The generators are the rewrite rules, and for each generator g ∈G, the arity
of the in-bonds ωin(g) = 1 and the arity of the out-bonds ωout(g) ≥1, 18 the
in-bonds corresponding to the left hand side of the production rule, the out-bonds the
right hand side of the production rule determined by the number of non-terminals.
Context-free means  = TREE, as depicted in 6.6. An element of the conﬁguration
space c(TREE) = TREE(n, g1, g2, . . . , gn) ∈C consists of a set of n-vertices at which
the rewriting rule generators are placed. The in- and out-bonds of a generator are
β1(g), the in-bond value the LHS of the production rule, and β2(g), . . . , βω(g)(g), the
out-bond values which are elements of the set of bond values B = VN ∪VT, where
VN is the set of non-terminal symbols and VT is the set of terminal symbols.
Examine the simple phrase-structure grammar with seven basic rules
R =



S r1
→NP VP
NP r2
→ART N
VP r3
→V
N r4,r5
→frogs ∨eggs
ART r6
→the
V r7
→jump



.
(6.19)
Deﬁne the sentence–language as the set of all context-free trees rooted in the syn-
tactic variable S. Panel 1 of Figure 6.6 shows a sentence TREE(6, r1, r2, r3, r6, r4, r7).
Notice, it is a single tree rooted in the sentence root node S = sentence with all
r2
r3
r4
r6
r7
r1
S
1
2
3
4
Det
Det
N
N
NP
NP
VP
VP
V
V
N
V
S
Det
Det
N
N
NP
NP
VP
VP
r2
r3
r4
r6
r5
r1
r7
V
V
jump
r3
VP
Det
Det
r6
r4
N
N
r2
NP
NP
Det
N
NP
Det
N
NP
VP
N
V
S
NP
VP
N
V
r1
r8
r2
r4
r3
r7
r6
r5
frogs
the
dog
the
eats
meat
eggs
the
frogs
jump
the
frogs
Figure 6.6 Panel 1 shows the conﬁguration diagram TREE (6, r1, r2, r3, r6, r4, r7) for the sentence
The frogs jump; panel 2 shows the conﬁguration diagram TREE (6, r1, r2, r3, r6, r4, r5) for the sen-
tence The frogs eggs, which is not locally regular. Panel 3 shows the conﬁguration PHRASE–FOREST
(2, c1, c2) of two phrases PHRASE (3, r2, r6, r4),PHRASE (2, r3, r7) each locally regular. Panel 4 shows
the conﬁguration DIGRAPH (8, r1, r2, r4, r5, r6, r7, r3, r8) which is context-sensitive and is a DIGRAPH.
18 As proven by Chomsky a context-free language is fundamentally not ﬁnite-state, if at least one rule in the
connected set has more than one non-terminal in the right-hand side (ωout(g) ≥2), the so called self-embedding
property.

162
6 THE CANONICAL REPRESENTATIONS OF GENERAL PATTERN THEORY
bonds agreeing (local regularity). Panel 2 of Figure 6.6 shows a conﬁguration
c = TREE(6, r1, r2, r3, r6, r4, r5) which is not regular. Notice the disagreement of the
VP →V, N →jump rules. Notice, βout(r3) = V ̸= βin(r5) = N. Panel 3 of Figure 6.6
showsaconﬁgurationconsistingoftwotreeseachofwhicharephrases. Thegraphtype
PHRASE–FOREST, consists of all tree derivations rooted in any syntactic variable with
leaves as the terminal strings. The conﬁguration c=PHRASE–FOREST(2, c1, c2) where
c1 = PHRASE(3, r2, r6, r4),
c2 = PHRASE(2, r3, r7),
(6.20)
is locally and globally regular.
Example 6.12 (Context-Sensitive Languages and POSET Graphs)
An example of a
context-sensitivegrammarisVN = {S, NP, VP, V, DET, N}, VT = {The, dog, eat, meat}
with rules
R =



S r1
→NP VP,
NP r2
→DETN,
NP r3
→N,
VP r4
→V NP,
DET r5
→the,
the N r6
→the dog,
dog V r7
→dog eats,
eats N r8
→eats meat



.
(6.21)
The context sensitive rules are those that rewrite a part of speech in the context
of the previous word. Context-sensitive grammars derive conﬁgurations that are par-
tially ordered sets. Examine panel 4 of Figure 6.6 showing the corresponding graph.
Notice the choice of grammar places context at the terminal leaves resulting in loss of
the conditional independence structure at the bottom of the tree.
6.3 Graph Transformations
Now explore graph transformations as the mechanism for constructing more complex structures
from simpler ones. The construction of complex patterns requires rules of transformation so that
more complex patterns may be constructed from simpler ones. This is certainly one of the most
fundamental aspects of pattern theory, building more complex graphs from simpler ones. Graph
transformations are familiar, as in linguistic parsing, where the role of the grammatical rules deﬁne
the structure of the transformation. Such grammatical transformations are chosen to be rich enough
to represent valid structures in the language, while at the same time restrictive enough to enforce
regular grammatical structure associated with the language.
During transformation of graphs, bonds will be left unclosed, the so called external bonds,
(ext(c)), and bonds will be closed during combination. For such changes, introduce the set of graph
transformations T : CR →CR of either birth or death type.
Deﬁnition 6.13
Deﬁne the discrete graph transformation T ∈T to be either of the birth or
death type according to the following:
T ∈T (c) : c(σ) birth
→c′(σ ′) = σ(c, c′′);
c, c′′, c′ ∈CR;
(6.22)
T ∈T (c) : c = σ(c′, c′′) death
→c′(σ ′);
c, c′, c′′ ∈CR.
(6.23)
For any regular conﬁguration c then the upper neighborhood N+(c) consists of all
conﬁgurations obtained from c by a birthing and the lower neighborhood N−(c) of all
conﬁgurations obtained via a death transformation to c, with the full neighborhood
N(c) = N+(c) ∪N−(c).
Then the set of graphs will be traversed by the family of graph transformations
T ∈T = ∪c∈CT (c) of simple moves consisting of either the birth or death types.

6.3 GRAPH TRANSFORMATIONS
163
MON
MULT
gi
zi , 2
zi , 1
Figure 6.7 Left panel depicts MON and MULT extensions. Right panel shows ﬁgure tracking
airplanes as LINEAR graphs with generators.
We shall see in deﬁning inference algorithms with desirable properties that it is helpful
to require that the transformations T give the graph space a reversibility and connectedness
property.
Deﬁnition 6.14
T : CR →CR shall be reversible in the sense that
T ∈T (c) : c →c′ ∈CR =⇒∃T ∈T (c) ⊂T : c′ →c = T(c′) ∈CR
T shall act transitively on CR in the sense that for any c, c′ ∈CR there shall exist a
sequence of Ti ∈T ; i = 1, 2, . . . such that ci+1 = Tici with c1 = c and cn = c′.
As the graphs are traversed, they are transformed into unions of simpler and more
complex graphs.
Deﬁnition 6.15
It is called monotonic if σ ∈ implies that any subgraph of σ also belongs
to ; it is then also monatomic.
For any given connection type , it can be extended by adding all subgraphs of the
connectors it contains, as well as repetitions containing multiple bodies. Deﬁne the monotonic
extension, denoted by MON(), to be the extension of  generated by adding all subgraphs
of the connectors it contains.
Deﬁne the multiple extension of , denoted by MULT(), to be the family of graphs
generated by unions with a ﬁnite and arbitrary number of repetitions, for all σi ∈,
σ1 ∪σ2 ∪σ3 · · · .
(6.24)
Figure 6.7 shows various examples to illustrate transformations which give rise to MON ()
and MULT() . Panel 1 shows a monotonic extension transformation; panel 2 shows a multiple
extension transformation. A conﬁguration for tracking is a linear graph LINEAR(m1) of m1.
Example 6.16 (Multiple Object Discovery)
For multiple object recognition where
the number of shapes and objects are unknown it is natural to deﬁne a hierarchy or
nested family of graphs corresponding to unions of multiple graphs. The generators in
the higher level graphs become the single objects themselves.
Begin with the object graphs σ ∈OBJ = {CYCLIC, LINEAR}. Deﬁne m to
be the number of objects present in the scene. Extend the graph space to include
MULT(m1, LINEAR) and MULT(m2, CYCLIC), the disconnected union of m1, m2
LINEAR and CYCLIC graphs. Then a scene σ ∈SCENE is an element of the graph set
SCENE = ∪m1≥0MULT(m1, LINEAR) × ∪m2≥0MULT(m2, CYCLIC)
(6.25)
consisting of all graphs that are ﬁnite disconnected unions of graphs from
OBJ. The entire conﬁguration space becomes the union of conﬁguration spaces
C = ∪σ∈SCENEC(σ).

164
6 THE CANONICAL REPRESENTATIONS OF GENERAL PATTERN THEORY
A conﬁguration c(MULT(m)) ∈C(MULT(m)) associated with an m-object graph
is just a collection of single object conﬁgurations making up the scene.
The multiple object scenes are synthesized by transformation of new objects
in the multiple object graph or dropping already existing objects. The graph
transformations become
MULT(m1) →MULT(m1 + 1)
MULT(m2) →MULT(m2 −1)
(6.26)
where the ﬁrst adds a new object from OBJ, and the second deletes.
Example 6.17 (Parsing and Language Synthesis)
Examine standard English pars-
ing using rules from the phrase structure grammar. Then  = PHRASE–FOREST and
is constructed recursively as the set of all phrases in the language which terminate in a
given English sentence and can be generated by parsing the English string. Parsing is
the process of transforming graph types σ →σ ′ so that the parse pushes towards a tree
rooted in a complete sentence. Aset of parse trees resulting from such transformations
are depicted in Figure 6.8.
The set  =PHRASE–FOREST is generated recursively through the family
T ∈T = ∪c∈CT (c) of simple moves, beginning with the starting conﬁguration, a
sequence of isolated words. This is depicted in the left panel of Figure 6.8 showing
the discrete graph corresponding to the three words The frogs jump. This is the starting
conﬁguration during the parsing process. Simple moves are allowed that add or delete
exactly one generator at the boundary ∂σ of the connector. From c only conﬁgurations
in the neighborhood Nadd(c) = {c′|Tc = c′, T ∈T (c)} ⊂C where
Nadd(c) = set of all c′ such that c′ is obtained
from c by adding a new generator.
the
frogs
jump
r6
r4
Det
N
V
the
frogs
jump
V
V
jump
VP
Det
Det
N
N
the
frogs
NP
r1
S
Det
Det
N
N
NP
NP
VP
VP
V
V
the
frogs
jump
S
S
NP
VP
S
Det
N
NP
NP
VP
VP
V
S
Det
Det
N
N
NP
NP
VP
VP
VV
the
frogs
jump
r7
r6
r2
r4
r7
r3
r2
r4
r7
r3 
r6
r1
r7
r4
r6
r3
r2
r1
r3
r2
r1
Figure 6.8 Top row shows graphs from  = PHRASE-FOREST corresponding to a sequence of
graph transformations generated during the parsing process of the sentence The frogs jump.
Left panel shows the starting conﬁguration; successive panels show parsing transformations from
the phrase–structure context-free grammar. Bottom row shows a sequence of transformations
through  = PHRASE-FOREST to the root node S resulting in the synthesis of the sentence The
frogs jump.

6.3 GRAPH TRANSFORMATIONS
165
Notice, during parsing, external(c) ̸= φ; upon completion of the parse to the
root node external(c) = φ.
Language synthesis involves sampling elements in the language. For this, start
with the root node S as the sole conﬁguration. Using identical grammar generators are
added to external bonds, with Nadd(c) as above. See Figure 6.8 illustrating synthesis
of the string in the language The frogs jump.
Example 6.18 (Computational Biology)
Learning genetic regulatory networks from
microarray data using Bayesian Networks is an area of active research in the Compu-
tational Biology community. The use of DAGs is an active area of research. Friedman
et al. [2000]. use a Bayesian approach for the scoring of learned graphical structures
based on the use of a conditional log-posterior and the choice of Dirichlet distributions
for the prior. Once an appropriate scoring function has been deﬁned, the space of all
possible directed acyclic graph is explored using graph transformations in order to
ﬁnd the structure that best ﬁts the data. For this, simple moves or operations resulting
Gene A
Gene B
Gene C
Gene D
Edge addition
edge addition
Edge removal
Edge reversal
edge reversal
+
Gene A
Gene B
Gene C
Gene D
Gene A
Gene B
Gene C
Gene D
Gene A
Gene B
Gene C
Gene D
Gene A
Gene B
Gene C
Gene D
Graph transformations
X
Gene A
Gene B
Gene C
Gene D
Original DAG
Example of illegal sequence of transformations
Figure 6.9 Top panel shows the set of elementary graph transformations over a sample DAG.
Lower panel depicts a sequence of transformations resulting in an illegal structure.
Sst 2
Tec1
Mfa1
Ste8
Ndj1
Yir949w
Ye050w
Yir334c
Kss1
Aga2
Aga1
Fus1
Kar4
Prm1
Fig1
Fus3
Tom6
Figure 6.10 Gene subnetwork for mating response in Saccharomyces cerevisiae, from [D. Pe’er:
From Gene Expression to Molecular Pathways Ph D dissertation Hebrew University]. The widths of
the arcs correspond to feature conﬁdence and they are oriented only when there is high conﬁdence
in their orientation.

166
6 THE CANONICAL REPRESENTATIONS OF GENERAL PATTERN THEORY
in so-called neighboring structures are used (namely the addition, removal or reversal
of an edge, see ﬁgure 6.9 for some examples). At each step of the learning process,
only operations leading to “legal” structures (i.e. structures with no directed cycles)
are permitted.
Friedman et al. [2000] and Pe’er [D. Pe’er: From Gene Expression to Molecular
Pathways Ph D dissertation Hebrew University]. have applied these ideas to the study
of gene regulatory networks in Yeast (Saccharomyces cerevisiae) and they have been able
to correctly identify some generally agreed upon relations, as shown in ﬁgure 6.10.
6.4 The Canonical Representation of Patterns: DAGs, MRFs, Gaussian
Random Fields
In the pattern theory, the ﬁrst job in constructing the solution to a problem is the construction of
the generators and the graphs on which they sit. This is representation. It would be nice if for
each problem there were a unique deﬁnition of generators, bonds and their respective graphs.
This will not be the case; there are no unique choices in general. There is, however, a canonical
representation in which the generators are deﬁned from the ﬁeld variables in such a way as to
accommodate the arbitrary nature of cliques in at most a pairwise interaction on the graph. This
is analogous to Gaussian ﬁelds in which there is at most pairwise interactions of the cliques via
the pairwise product in the potentials. The challenge is that the regular structures associated with
the variability of Markov random ﬁelds on an arbitrary graph can in general have binary, ternary,
arbitrary dependencies. The order depends upon the locally supported potential functions and
the size of the cliques in the graph. To construct a canonical representation in pairwise interactions
only, the generators must aggregate the ﬁeld variables so that arbitrary dependency structure of
the cliques is accommodated. This is a very general idea: essentially that the potentials of the
MRFs of regular patterns can always be realized as the products of pairs of generators. To this
end an explicit construction is established incorporating all order dependencies reduced to binary
interactions by the introduction of the more abstract generators.
Thus far, rigid regularity has been introduced through the bond function. To accommodate
variability over the regular conﬁgurations we introduce a Gibbs probability P(·) on the conﬁgura-
tion space, CR, the regular conﬁgurations. To place probabilities on the patterns assume that the
generator spaces G are discrete collections making them directly connected to Gibbs distributions.
The probabilities are replaced with densities once the generators move to the continuum.
Deﬁnition 6.19
Deﬁne the local potential functions expressing interaction between gen-
erators ij : Bi × Bj →R+ governing the probabilistic behavior of the conﬁgurations as
strictly ﬁnite so that in exponential form e−(β,β′).
Then the Gibbs probability of a conﬁguration P(C) restricted to CR for discrete generator
spaces G can be written as the product of potentials over pairwise interactions in the graph:
P(c) = 1
Z

e=(i,j)∈E
e−ij(βj(gi),βi(gj)),
(6.27)
with Z normalizing to CR so that 
c∈CR P(c) = 1.
It is often useful to highlight Eqn. 6.27 via a modiﬁcation which distinguishes the coupling
between generators and their marginal frequency of occurence in the graph:
1
Z

e=(i,j)∈E
e−0(βj(gi),βi(gj))
n(σ)

i=1
Q(gi).
(6.28)

6.4 THE CANONICAL REPRESENTATION OF PATTERNS
167
The role of e−0 is to control the couplings between generators and the weight function Q
governs the frequencies of occurrence of different generators. In many applications, the A, A0, Q
functions are node dependent, depending upon the generator index such as for non-stationary. The
above formulas 6.27, 6.28 specify that the random structures on the generators can be constructed
to involve at most pairwise interactions on the graph.
For generator spaces G on the continuum, interpret the left-hand side as a density p(·) with
q(·) a density on G:
p(c) = 1
Z

e=(i,j)∈E
e−0(βj(gi),βi(gj))
n(σ)

i=1
q(gi).
(6.29)
Let us now establish that the question "is binary enough?" can be answered in the afﬁrmative
for all of the probabilistic models discussed thus far. The probabilistic models of directed acylic
graphs, Markov random ﬁelds, and Gaussian random ﬁelds all have a canonical representation
via pairwise generator cliques. Our claim is that we can view all of the so far discussed random
ﬁelds on graphs in a uniﬁed manner, in at most pairwise interactions. It is not a matter of principle
but of convenience what form is preferred, binary or higher order. This is an old idea well known
to systems theorists using state variable descriptions to study ODEs where one can always assume
that the order of the equation is one. Any nth order differential equation can be studied as a ﬁrst
order differential equation, simply via expansion of the state space. As we show in the general
digraph case, and in particular for Markov chains, this is familiar to the reader as the standard
way in which the state space is enlarged to accommodate higher order dependencies. For Markov
chains this corresponds to changing the graph structure from n-nearest neighbor to 1-neighbor
with the expanded state space (nth power). This is reminiscent of the principle expressed by the
great probabilist William Feller that any process is a Markov process with a sufﬁciently large state
space.
6.4.1 Directed Acyclic Graphs
The conditional probability formulas Eqns. 6.27, 6.28 provide a common representation that only
involves nearest neighbor interactions. For directed acylic graphs, the factoring as a product sup-
ports a straightforward representation in products of probabilities (independent) with conditional
dependence determined through the regularity R of the graph.
Theorem 6.20
Let X be a realization from a ﬁnite X0-valued directed acylic graph σ =
{D, E} with parent system  with probability
P(X) =
n(σ)

i=1
P(Xi|Xπi).
(6.30)
The probability law P(X) written in the pairwise formula Eqn. 6.27 has generators of
the form determined by the parent system, gi = (Xi, Xπi), with arity ω(gi) = 1 + |πi|,
βin(gi) = Xi, βout = Xπi, ρ(β, β′) = TRUE deﬁning local regularity.
The conditioned probability on conﬁgurations c(DAG) = DAG(g1, . . . , gn) becomes
P(c) =
n(σ)

i=1
Q(gi),
where Q(gi) = P(Xi|Xπi).
(6.31)
Proof
Proof by construction: Divide the sites of the DAG into classes
D = {1, 2, . . . , n} = ∪k≥0Dk

168
6 THE CANONICAL REPRESENTATIONS OF GENERAL PATTERN THEORY
where the sites in subset Dk ⊂D have in-arity k; that is i ∈Dk ⇒|πi| = k. Deﬁne
the maximum parent set size kmax ≤n −1. Then the joint probability of the random
vector X = (X1, X2, . . . , Xn) can be written as
P(X) =

i0∈D0
P(Xi0) ×

i1∈D1
P(Xi1|Xπi1 ) · · · ×

ikmax∈Dkmax
P(Xikmax |Xπikmax )
where the functions P(·|·) depend upon l parents, πil = {j1, j2, . . . , jl} and the site i. The
factorization suggests the form of the generators. If l > 1 construct a new generator g
carrying the information Xil; Xj1, Xj2, . . . , Xjl and associate it with the Q-function:
Q(gi) = P(Xil|Xj1, Xj2, . . . , Xjl).
(6.32)
Consistency requires that if two new generators contain the same Xj add a segment to
the graph linking the sites with a bond relation ρ = EQUAL.Also link g with any other
site to which one of the xs constituting g was already connected. Then the probability
density can be written as in the product structure formula with the new graph and
generators.
Notice that the regularity constrained conﬁgurations are strictly contained in
the full conﬁguration space except for the independent DAG case:
CR = X n
0 ⊂C =
kmax

j=0
(X j+1
0
)|Dj|.
(6.33)
Example 6.21 (m-memory Markov Chain)
TheX0-valuedm-memoryMarkovchains
have canonical representation in the regular conﬁgurations CR deﬁned by the
σ =LINEAR graph structures. Let Xi, i = 1, 2, . . . be an m-memory Markov chain,
P(X1, . . . , Xn) =
n

i=1
P(Xi|Xi−1, Xi−2, . . . , Xi−m) ,
(6.34)
with transition law Q(Xi−m,...,Xi−1),Xi.
For the LINEAR graph with two in and out bonds with values βin(g) = (yj,
j = 1, . . . , m), βout(g) = (Xj, j = 2, . . . , m+1), with regularity R if and only if βout(gi) =
βin(gi+1)), i = 1, . . . , n.
Figure 6.11 shows the 1-memory and 2-memory cases,
1-memory having generators g = (x, y) ∈G = X 2
0 , and 2-memory g = (x, y, z) ∈G =
X 3
0 . Regularity implies the in- and out-bond relations are satisﬁed reducing the space
x1
x2
x3
x4
x5
x6
x7
x8
x1
x2
x3
x4
x5
x6
x7
x8
x
x
y
y
x, y
x
y
z
y, z
Figure 6.11 Top row shows m = 1-memory Markov chain with generators g = (x, y) the pairs in
the boxes and bond values βin(g) = x, βout(g) = y. Bottom row shows for m = 2-memory, with
g = (x, y, z) and βin(g) = (x, y), βout(g) = (y, z).

6.4 THE CANONICAL REPRESENTATION OF PATTERNS
169
of allowable strings to CR(n) = X n
0 ⊂C = (X m+1
0
)n. Such a string of generators in CR
has probability
P(LINEAR(g1, . . . , gn)) =
n

i=1
Q(Xi−m,...,Xi−1),Xi.
(6.35)
6.4.2 Markov Random Fields
The conditional probability formulas Eqns. 6.27 and 6.28 provide a representation involving pair-
wise interaction of cliques. This must interface to the Hammersley Clifford result which allows
arbitrary interactions.
Theorem 6.22
Consider a ﬁnite X0-valued Markov random ﬁeld {Xi, i ∈D} with respect
to graph σ = {D, E} size n(σ), with Gibbs form
P(X) = e−
C∈{set of cliques} C(x)
Z
,
x ∈X .
(6.36)
Then P(X) is of the form of the pairwise probability formula, Eqn. 6.27 with graph σ ′,
generators g ∈G, and acceptors A(·, ·) to be deﬁned within the construction below.
Proof
Deﬁne the sets of cliques of size κ to be Cκ ⊂2n(σ), κ = 1, 2, . . . n, with the
complete set of cliques ∪κCκ. The general form of the probability of a conﬁguration
becomes
P(X) = 1
Z
n

κ=1

{i1,i2,...,iκ}∈Cκ
e−i1,i2,...,iκ (Xi,Xj,...,Xiκ ).
(6.37)
Now deﬁne max ≤n to be the largest clique size in the graph σ. If max ≤2,
then the interactions are in the pairwise form and nothing need be done. Otherwise,
for any clique (i1, i2, . . . , imax) ∈Cκ replace the sites (generators) in the clique by a
new generator g = (Xi1, Xi2 . . . , Ximax) and connect g with bond value
β(g) = (Xi1, . . . , Ximax) ∈Gmax = X max
0
(6.38)
to each site value i ∈D that was connected to any of the Xi1, Xi2 . . . , Ximax, in the
original graph σ. The factors in the clique probabilities are replaced by functions
Q(gi), i = 1, . . . , |Cmax| depending upon the single (new) generators
Q(gi) = e−i1,i2,...imax(Xi1,Xi2,...,Ximax), i = 1, . . . , |Cmax|.
Figure 6.12 illustrates the reduction of the various largest cliques of size four
and size three. Do this recursively; this is guaranteed to terminate since max < n. At
this point σ ′ has largest cliques of size ≤max −1. Now repeat this process replacing
max by max −1. This procedure is repeated until all cliques are of size 2 or less.
The resulting graph σ ′ is binary and will consist of some old and m new gen-
erators. The reformulated graph is shown on the right. For the other segments in
σ ′ the associated factor in the product depends upon at most two generators, old
or new. Hence the probability of a conﬁguration becomes a mixture of the origi-
nal random ﬁeld elements, call them (Xi, Xj, . . . , Xim), ij ∈σ, all entering into the

170
6 THE CANONICAL REPRESENTATIONS OF GENERAL PATTERN THEORY
1
3
4
5
6
7
8
2
9
10
11
1
3
4
5
6
7
2
8
4
5
6
7
1
8
4
5
8
7
Figure 6.12 Left panel showing various cliques of size three and four transformed to the binary
graph case (right panel).
cliques of size κ ≤2, as well as the m′-newly deﬁned generators, g1, g2, . . . , gm′. Then
c(σ ′) = σ ′(Xi, . . . , Xim, g1, . . . , gm′) is of a form
P(c) =

i∈C1
e−i(Xi)

{i,i′}∈C2
e−(i,i′)(Xi,Xi′)
m′

i=1
Q(gi).
Figure 6.12 shows the new graph σ ′ for the creation of m′ new generators. The density
has been reformulated into binary form as claimed.
There are many ways of reducing the representation to binary form. Another way is to
introduce new generators so that each clique consists of at most two new generators. This is more
efﬁcient in the sense that the multiplicity of the new generators can be made smaller. Apparently,
the question "is binary enough?" can be answered in the afﬁrmative.
6.4.3 Gaussian Random Fields: Generators induced via difference operators
Returning to Gaussian random ﬁelds, we see directly that the difference operators specify the
generators in the representations. Here the generators and bond values G, B are continua for
Gaussian random ﬁelds, with the conditional probability formulas Eqns. 6.27, 6.28 interpreted as
probability densites with respect to Lebesgue measure.
Theorem 6.23
Given is a Gaussian random ﬁeld {Xi, i ∈D ⊂Zd} satisfying the stochastic
difference equation
LDXi = Wi
with LDXi =

s∈S
as Xi+s,
(6.39)
S ⊂Zd, W a white noise process of independent, identically distributed Gaussian
variables, variance σ 2, with LD with associated boundary conditions, an invertible difference
operator. The probability distribution can be represented as pairwise cliques with generators
gi = {Xi+s, s ∈S}, g ∈G = R|S| having bond structure as deﬁned below within the proof
and determined by the Minkowski addition set K = S ⊖S ⊂Zd of Eqn. 5.19 with arity
ω(g) = |K|.

6.4 THE CANONICAL REPRESENTATION OF PATTERNS
171
Proof
Deﬁne the new generator at site i ∈Zd as the vector of length |S|
gi = (Xi+s1, Xi+s2, . . . , Xi+s|S|),
implying that the generator deﬁnes the ﬁeld on a patch of Zd. With Q(gi)
=
exp −1/2τ2 6
s∈S asXi+s
72, the desired probability density is obtained as a product
of Q’s. Consistency is established in the sense that if two patches have a non-empty
intersection the X-ﬁelds deﬁned by the two generators must coincide on this intersec-
tion. The bond structure of a generator is constructed by placing it at the origin of the
lattice Zd and equipping it with |K| bonds enumerated by j, j = 1, 2, . . . |K|. Deﬁne the
bonds as the vector
βj(gi) = (Xi+l; l ∈K ∩{K + kj}), kj ∈K, j = 1, 2 . . . |K|.
Two generators, gi, gi′ situated at i, i′ are connected by a segment if i = i′ + k for some
k = h −h′ ∈K.
With the bond relation ρ = TRUE when βj(gi) = βj′(gi′) implies consistency, the
X-ﬁelds deﬁned on patches for the two generators coincide on the intersection of the
patches.
Example 6.24 (The derivative operator)
Examine the non-self-adjoint differential
operator L = (∂/∂x1)+(∂/∂x2), with 0-boundary conditions X0,j = Xi,0 = 0 with oper-
ator LXi = 
s∈S asXi+s, given in Eqn. 5.43 inducing the random ﬁeld {Xi,j, (i, j) ∈Zn2}.
Then
S = {(0, 0), (0, 1)(1, 0)} ,
(6.40)
K = S ⊖S = {(0, −1), (−1, 0), (0, 1), (−1, 1), (1, 0), (1, −1)}.
(6.41)
Theorem 5.5 dictates the neighborhood structure of the random ﬁeld interior to the
graph as Ni = i + S ⊖S. The generators carry information in the form of 3-vectors
gi = Xi+s, s ∈S = (Xi,j, Xi+1,j, Xi,j+1)
according to panel 1 in Figure 6.13 where the dashed triangle represents a generator.
Then with
Q(gi) = e−
1
2σ2 (Xi+1,j−(2−c)Xi,j+Xi,j+1)2
the probability density is obtained as the product $
i Q(gi). To enforce consistency,
choose the |K| = 6 bond values as illustrated in the right panel of Figure 6.13, the
bonds given by
β1(gi) = Xi,j; β2(g) = Xi+1,j; β3(g) = Xi+1,j; β4(g) = Xi,j+1; β5(g) = Xi,j+1;
β6(g) = Xi,j.
The bond relation will be EQUAL as consistency demands that the 6th bond of gi+1,j
is equal to the 3rd bond of gi,j, so that β3(gi,j) = β6(gi+1,j), and so on for the other
relations between bonds.

172
6 THE CANONICAL REPRESENTATIONS OF GENERAL PATTERN THEORY
g i
x i1, i2+1
x i1, i2+1
x i1+1, i2
x i1+1, i2
x i1, i2
x i1, i2
g i
1
2
Figure 6.13 Top row: Panel 1 shows the grouping of lattice sites into generators; panel 2 shows
the bond values for generator gi,j =(Xi,j, Xi+1,j, Xi,j+1) for the derivative operator. Bottom row
shows the same for the Laplacian operator.
Example 6.25 (The Laplacian operator)
Examine the Laplacian operator L = − +
a = −∂2/∂x2
1 −∂2/∂x2
2 + a , with 0-boundary according to Eqn. 5.49. Then
S ={(1, 0), (−1, 0), (0, 0), (0, 1), (0, −1)},
K =S ⊖S = {(−1, 0), (−2, 0), (1, 0), (2, 0), (−1, −1), (−1, 1)
(6.42)
(0, −1), (0, −2), (0, 1), (0, 2), (−1, 1), (1, −1)},
with generators g = {Xi+s, s ∈S} given by
gi1,i2 = (Xi1,i2, Xi1−1,i2, Xi1+1,i2, Xi1,i2+1, Xi1,i2−1),
(6.43)
and ω(g) = |K| = 12 the number of bonds. Use a generator coordinate i = (i1, i2) and
a bond coordinate j = 1, 2, 3, . . . , 12 which will be treated modulo 4 enumerating the
directions S,E,N,W. Let
L = − + a
with the discrete 4-neighbor Laplacian . Introduce generators of the form
gi = (xi, y0
i , y1
i , y2
i , y3
i ) ∈R5,

6.4 THE CANONICAL REPRESENTATION OF PATTERNS
173
and with arity ω(gi) = 4. Associate the bond values
βj(gi) = (xi, yj
i; j = 0, 1, 2, 3).
Consider two neighboring generators gi; i = (i1, i2) and gi′; i′ = (i′
1, i′
2), for example
i′ = i+1, j = j′; the other three possibilities are treated in the same way. Then the bond
relation ρ is of the following form:
ρ(β1(gi), β3(g′
i)) = {y3
i′ = xi} ∧{y1
i = xi′}.
(6.44)
Then with
Q(gi) = e−|(1+a)xi−1
4 (y0
i +y1
i +y3
i +y3
i )|2,
(6.45)
the probability density obtained is 1/Z $
i Q(gi) which is a Gaussian process.

7
M AT R I X G R O U P A C T I O N S T R A N S F O R M I N G
PAT T E R N S
ABSTRACT
Thus far Pattern theory has been combinatore constructing complex patterns by
connecting simpler ones via graphs. Patterns typically occurring in nature may be extremely
complex and exhibit invariances. For example, spatial patterns may live in a space where the
choice of coordinate system is irrelevant; temporal patterns may exist independently of where
time is counted from, and so on. For this matrix groups as transformations are introduced,
these transformations often forming groups which act on the generators.
7.1 Groups Transforming Conﬁgurations
Pattern theory is transformational. One of the most fundamental transformation types for study-
ing shape is deﬁned via groups and group actions acting on the generators, providing a vehicle for
representing the natural invariances of the real world. The fundamental role of groups as transfor-
mations on a background space what we term the generators is familiar and is at the very core of
much of the geometry which is familiar to us. Even though the concept of groups did not appear in
Euclid’s original axioms, congruence did. For Euclid, congruence was deﬁned through the equiv-
alence deﬁned by the rigid motions carrying one ﬁgure into another. Klein [147] discovered the
central role of groups in all the classical geometries (see Boothby [148]). The Erlangen program
was essentially that. Geometry becomes the study of the groups which leave geometric properties
invariant. The emphasis is transferred from the things being acted upon to the things doing the
action. This is familiar to the Kolmogoroff complexity model. With each geometry is associated a
group of transformations on the background space in which the properties of the geometry and
its theorems are invariant. In Euclid’s geometry the subgroup of rigid motions of the afﬁne group
leaves distances invariant and preserves the congruence relation. Two ﬁgures are congruent if they
are equal modulo an element of the rigid motion group.
7.1.1 Similarity Groups
In Pattern theory, patterns or shapes are understood through exemplars or representers of equivalence
classes. Elements in the equivalence classes will often be called congruent, with the examplar congru-
ent to all elements of its congruence class. The formal notion of pattern is based on congruence
modulo the similarity group. This section follows Artin [149] and Boothby [148].
Deﬁnition 7.1
A group is a set S together with a law of co(S, ◦)mposition ◦: S × S →S
which is associative and has an identity element e ∈S and such that every element s ∈S has
an inverse element s−1 ∈S satisfying s ◦s−1 = e.
A subset H of a group S, H ⊂S, is called a subgroup if
1. it is closed: a◦∈H, b ∈H =⇒a ◦b ∈H,
2. it has an identity e ∈H and
3. the inverse is in H: a ∈H =⇒a−1 ∈H.
As a notational shorthand, a group will be represented by a set, leaving the law of compo-
sition implicit, when it can be done so without ambiguity. Subgroups will be important. Direct
products of groups will allow for increasing dimensions.
174

7.1 GROUPS TRANSFORMING CONFIGURATIONS
175
Deﬁnition 7.2
The direct product group
(S, ◦) = (S1, ◦1) × (S2, ◦2) × · · · × (Sn, ◦n)
(7.1)
has law of composition s = (s1, . . . , sn), s′ = (s′
1, . . . , s′n) ∈S1 × S2 × · · · × Sn given by
s ◦s′ = (s1 ◦1 s′
1, . . . , sn ◦n s′
n)
(7.2)
with ◦i the law of composition for group Si.
It will be helpful to partition groups according to their subgroups and associated cosets.
Deﬁnition 7.3
A left (right) coset of subgroup H ⊂S is a subset of the form
aH = {ah|h ∈H}
(Ha = {ha|h ∈H}).
(7.3)
Example 7.4 (Familiar groups and subgroups.)
Groups and subgroups are very
familiar.
1. The group (Z, +), (R, +), (C, +), the integers, reals and complex numbers with
addition, e = 0. A subgroup of the integers is H = mZ, the subset of integers of
multiples of m, mZ = {n ∈(Z, +) : n = mk, k ∈(Z, +)}.
2. The groups (R −{0}, ×), (C −{0}, ×), the nonzero real and complex numbers,
with multiplication, e = 1. Obvious subgroups include H = {c ∈C× : |c| = 1},
the points on the unit circle in the complex plane, or the discrete subgroup of
equally spaced points on the unit circle H = {ei(2πk/n), k = 0, 1, . . . , n −1}.
Unlike the matrix groups we study below, all these commute giving them the special
name of Abelian groups.
7.1.2 Group Actions Deﬁning Equivalence
Congruence or sameness is an equivalence relation.
Deﬁnition 7.5
Let X be a set. An equivalence relation on X is a relation which holds between
certain elements of X, written as x ∼y, having the properties
1. transitivity: if x ∼y and y ∼z then x ∼z,
2. symmetric: if x ∼y then y ∼x and
3. reﬂexive: x ∼x for all x ∈X.
Equivalence classes, i.e. subsets of X which contain equivalent elements, form a disjoint
covering, or partition of X. This is an important property which we now prove.
Theorem 7.6
Let ∼be an equivalence relation on set X. Then the equivalence classes
partition X.
Proof
For x ∈X, let [x]∼= {y ∈X : y ∼x} denote the equivalence class containing
a. By the reﬂexive property ∀x ∈X, x ∼x, so x ∈[x]∼. Thus, the family or collection
of equivalence classes covers X. We need only show that two equivalence classes are
disjoint or equal. Suppose we have two equivalence classes identiﬁed by [x]∼and [y]∼,
respectively, and that there exists a shared element z ∈[x]∼, z ∈[y]∼. By deﬁnition,
x ∼z and y ∼z so by commutativity z ∼y, and by transitivity, x ∼y. Now suppose
w ∈[x]∼, then by the deﬁnition of equivalence class, w ∼x, but by transitivity w ∼y,
so w ∈[y]∼. Thus, [y]∼contains [x]∼. Similarly, [x]∼contains [y]∼. So, [x]∼and [y]∼
are equal, i.e. they represent the same equivalence class.

176
7 MATRIX GROUP ACTIONS TRANSFORMING PATTERNS
A classic example are the EVEN,ODD integers, represented by ¯0, ¯1, equivalent mod2.
Through the group action on sets, particularly beautiful equivalence relationships emerge.
Deﬁnition 7.7
Let S be a group with group operation ◦, and X be a set. Then deﬁne a group
action  on X which is a mapping  : S × X →X, (s, x) = s · x, s ∈S, x ∈X, with the
properties that
1. if e is the identity element of S then
(e, x) = x
∀x ∈X,
(7.4)
2. if s1, s2 ∈S then the associative law holds according to
(s1, (s2, x)) = (s1 ◦s2, x).
(7.5)
Ashorthand notation used throughout for the group action will be (s, x) = s·x, s ∈S, x ∈X.
This is extremely familiar for matrices.
The group action S allows the decomposition of the set X into its orbits.
Deﬁnition 7.8
The orbit Sx ⊂X, x ∈X of the group S is just the set of all images of x
under arbitrary group action of s ∈S:
Sx = {y ∈X : y = sx for some s ∈S}
(7.6)
Theorem 7.9
Let S denote a group, X a set and  : S × X →X a group action. Deﬁne the
relation
x ∼y
if ∃s ∈S : (s, x) = y.
(7.7)
With the equivalence classes under this relation denoting [x]S, the orbits are the equivalence
classes, [x]S = Sx, and the orbits {Sx} of the group S partition X.
Proof
We need to show that this is an equivalence relation. First, x ∼x since
x = e x,e ∈S the identity. x ∼y implies y ∼x (reﬂexivity) according to x ∼y ⇒y =
sx implying x = s−1y, s−1 ∈S. Finally, x ∼y, y ∼z ⇒y = sx, z = s′y giving
z = (s′s)x, (s′s) ∈S ⇒x ∼z.
That the equivalence classes and orbits are equal, [x]S = Sx follows since x ∼y
implies y = sx and thus y ∈Sx giving [x]S ⊂Sx. Conversely, y ∈Sx implies x ∼y so
Sx ⊂[x]S.
Deﬁnition 7.10
Deﬁne the set of equivalence classes as X/S called the orbits of the
action.
In pattern theory, the set of equivalence classes X/S will arise often, S expressing the natural
invariances in which the pattern is viewed.
Certain sets are essentially the same from the point of view of the group actions, i.e. if we
look for example at X/S there is but one equivalence class X = [x]S. This is formalized through
the notion transitive action and homogeneous spaces.
Deﬁnition 7.11
Let  : S × X →X be a group action. Then  is transitive if for all
x, y ∈X there exists s ∈S such that (s, x) = y.
In turn, X is said to be a homogeneous space of the group S if there exists a transitive
group action on X.
Example 7.12
Equivalence relations are fundamental to many things that we do
in image analysis. Examine an oriented and piecewise linear closed curve in the
plane. Assume that it does not intersect itself so that Jordan’s curve theorem dic-
tates that it divides the plane into two parts, an inside and an outside. Deﬁne
set(c) = inside of the curve and an equivalence relation ∼by
c ∼c′ implies set(c) = set(c′).
(7.8)

7.1 GROUPS TRANSFORMING CONFIGURATIONS
177
This deﬁnes the equivalence relation,
with [c]∼the orbit representing
the image.
Example 7.13 (Null Space of a Linear Operator)
Let A : H →H, A a linear operator
A : x ∈H →Ax, H a Hilbert space, either ﬁnite or inﬁnite dimensional. Two elements
x, x′ can be identiﬁed via the identiﬁcation rule if x = x′+null, where null is an element
from the null space of A.
In particular, let A : Rn →Rn, with A = 
m
i=1 αiφi⟨φi, ·⟩Rn, with m < n and
{φi}n
i=1 a C.O.N. basis for Rn. Then x ∼x′ if and only if Ax = Ax′. Divide Rn into
the null space of the operator and its orthogonal complement, Rn = N ⊕N ⊥. To
construct a disjoint partition via equivalence classes deﬁne x⊥= 
m
i=1 φi⟨φi, x⟩Rn,
and x⊥is the projection onto the orthogonal complement of the null space, x⊥∈N ⊥.
The equivalence classes or images = [x]∼become
[x]∼= x⊥⊕N =


x′ ∈Rn : x′ = x⊥+
n

i=m+1
αiφi, (αm+1, . . . , αn) ∈Rn−m


.
7.1.3 Groups Actions on Generators and Deformable Templates
The patterns and shapes will be represented via the exemplars or generators; the natural invariances
are expressed through the groups acting on the generators. Equivalence being deﬁned through
the orbits and partition of the pattern space into the orbits.
For this, let the similarity group be of some ﬁxed dimension acting on the generator space.
We shall use the shorthand notation to deﬁne the group action (s, g) = sg.
Deﬁnition 7.14
The group action on a single generator  : S × G →G is deﬁned as
(s, (s′, g)) = (s ◦s′, g) = (s ◦s′, g).
(7.9)
7.2 The Matrix Groups
Shapes and structures will be studied using the low-dimensional matrix groups translations,
rotations, rigid motions of translations and rotations acting on the ﬁnite dimensional X = Rd
background spaces.
7.2.1 Linear Matrix and Afﬁne Groups of Transformation
For the geometry of shape, the ﬁnite dimensional groups and their subgroups generated from the
group of matrices, the generalized linear group is used throughout.
Deﬁne the matrix groups explicitly as follows.

178
7 MATRIX GROUP ACTIONS TRANSFORMING PATTERNS
Deﬁnition 7.15 (Generalized Linear Group)
The d×d general linear group, denoted
as GL(d), is the group of all d × d matrices
GL(d) = {d × d real matrices A with det A ̸= 0},
(7.10)
with non-zero determinant (invertible) and with law of composition matrix multiplication,
A ◦B = AB =


j
AijBjk

,
(7.11)
with the identity element I = diag[1, . . . , 1].
That this is a group follows from the fact that the identity I ∈GL(d), the product of
two matrices, is in the group A ◦B = AB ∈GL(d) and the inverse is in the group as well
A−1 ∈GL(d).
Subgroups of GL(d) are used as well.
1. Deﬁne the special linear group SL(d) ⊂GL(d) to be the subgroup of volume
preserving transformations:
SL(d) = {A ∈GL(d) : det A = 1}.
(7.12)
2. Deﬁne the orthogonal group O(d) ⊂GL(d) to be the orthogonal subgroup of
matrices and SO(d) ⊂O(d) ⊂GL(d) to be the special orthogonal subgroup with
determinant 1:
O(d) = {A ∈GL(d) : A∗A = I},
(7.13)
SO(d) = {A ∈O(d) : det A = 1} = O(d) ∩SL(d).
(7.14)
3. Deﬁne the uniform scale group US(d) ⊂GL(d) of diagonal matrices:
US(d) = {A ∈GL(d) : A = ρ I, ρ > 0}.
(7.15)
Notice, the group operation for GL(d) does not commute. There are various groups which
are generated as products of the subgroups.
Deﬁnition 7.16
The afﬁne group A(d) is the semi-direct product of groups GL(d) ⊗Rd
with elements {(A, a) : A ∈GL(d), a ∈Rd} and law of composition semi-direct product
A(d) = GL(d) ⊗Rd,
with
(A, a) ◦(B, b) = (AB, Ab + a).
(7.16)
The Euclidean and special Euclidean groups E(d), SE(d), respectively, are the sub-
groups of the Afﬁne Group consisting of the rigid motions generated from orthogonal and
special orthogonal O(d), SO(d) motions with group operation, the semi-direct product:
E(d) = O(d) ⊗Rd,
SE(d) = SO(d) ⊗Rd.
(7.17)
The similitudes are given by the direct product of uniform scale with orthogonal
motions
Sim(d) = US(d) × SO(d).
(7.18)

7.2 THE MATRIX GROUPS
179
7.2.2 Matrix groups acting on Rd
When the matrix groups act on the inﬁnite background space X = Rd according to the usual con-
vention of identifying points x ∈X = Rd with column vectors, the action is matrix multiplication
on the vectors.
Deﬁnition 7.17
Let X be the background space and the afﬁne group A ∈GL(d) acts on
the background space according to
(A, x) = Ax =



j A1jxj
...

j Adjxj

∈X.
(7.19)
Let (A, a) ∈S ⊂A(d), then
((A, a), x) = Ax + a.
(7.20)
For homogeneous coordinates, then represent
¯A =
 A
a
0
1

∈¯A,
A ∈GL(d), a ∈Rd,
¯x =
 x
1

,
(7.21)
with group action ( ¯A, ¯x) = ¯A¯x.
We emphasize that the semi-direct product of Eqn. (7.16) matches ((A, a), x) of Eqn. 7.20 a group
action. We leave it to the reader to verify this.
Example 7.18 (Polar Decomposition)
Polar decomposition is useful for clarifying
the degrees of freedom inherent in the class of operators. Any invertible matrix
A ∈GL(d) can be written as
A = PO = O′∗diag(λ1, . . . , λd)O′
)
*+
,
P
O,
(7.22)
P being a positive deﬁnite symmetric matrix, with O, O′ ∈O(d) the orthogonal
matrices. With that said, rewrite elements of GL(2) to have 4 parameters, noticing
O, O′ ∈SO(2) have one each, with two for the diagonal entries. Similarly elements of
GL(3) have 9 free paramaters, 3 each for the orthogonal groups and 3 diagonal entries.
Example 7.19 (Tracking via Euclidean motions)
To accomodate arbitrary position
and pose of objects appearing in a scene introduce the subgroups of the generalized
linear group GL(3) : X ↔X. The generators G are the CAD models of various types
as depicted in Figure 7.1 showing 3D renderings of sample templates. In this case
each template consists of a set of polygonal patches covering the surface, the material
description (texture and reﬂectivity), and surface colors.
The Euclidean group E including rigid translation and rotation operate on the
generators 2D surface manifolds in R3. The Euclidean group of rigid motions denoted
E(n) = O(n) ⊗Rn, where O ∈O(n) are n × n orthogonal matrices; the group action
on X = Rn is ((O, a), ·) : O(n) ⊗Rn →Rn is ((O, a), x) = Ox + a, with the law of
composition given by the semi-direct product: That Euclidean distance is preserved
d(((O, a), x), ((O, a), y)) = d(x, y) where d(x, y) = 
n
i=1 |xi−yi|2 for rotation around
the origin followed by translation is clear, hence the name rigid motions.
Example 7.20 (Flip Group for Symmetry)
Biologicalshapesexhibitoftensymmetries.
The right panel of Figure 7.1 illustrates symmetry seen in the human brain. The cosets of

180
7 MATRIX GROUP ACTIONS TRANSFORMING PATTERNS
(1)
(3)
(4)
(2)
Figure 7.1 The panels 1–3 show various CAD models for objects. Panel 4 shows the human brain
section depicting symmetry.
the orthogonal group deﬁned by the ﬂips which are appropriate for studying symmetry,
D(2, 3) = {R, I} ⊂O(2, 3), given by the 2 × 2, 3 × 3 matrices
D =

I =
 1
0
0
1

,
R =
 −1
0
0
1

,
(7.23)
D =


I =


1
0
0
0
1
0
0
0
1

,
R =


−1
0
0
0
1
0
0
0
1




.
(7.24)
with the cosets I{SO(2, 3), RSO(2)(3)}.
Example 7.21 (Similar and Congruent Triangles)
Congruence: Let S = E(2) be the
set of Euclidean rigid motions in the plane, X the set of points, lines, or triangles in the
plane. For X = TRIANGLES, then [△]S is the subset of TRIANGLES congruent to △.
Let a nondegenerate △= (v1, v2, v3) ∈R6 be identiﬁed with a 2×3 matrix constructed
from the vertices, the triangle nondegenerate so that △is a rank 2 matrix. Position the
triangle △0 at the origin so that v1 = (0, 0) ∈R2.
Let G = TRIANGLES ⊂R6 be generated by the afﬁne motions on △0:
TRIANGLES = {(A, a)△0, (A, a) ∈A(2)},
(7.25)
where the group action applied to the 2 × 3 matrix generators becomes
(A, a)△= A△+
 a1
a1
a1
a2
a2
a2

.
(7.26)
Deﬁne equivalence classes by the orbits of E, [△0]E. Then two triangles △1 ∼△2
if both are in the same orbit: △2 = s△1 for some s ∈E. Since orbits form disjoint
partitions we have essentially established the Euclidean partition of TRIANGLES into
congruence classes.
Notice in this example a triangle △is not a picture but rather a point in the
manifold of 2 × 3 matrices.
Example 7.22 (Similar Triangles)
Now return to Euclid’s more general notion of
similarity and notice that the subgroups are normal allowing us to use quotient group
ideas. Examine the orbit of △0 under the scale-rotation group matrices of the type

7.2 THE MATRIX GROUPS
181
s =

u
v
−v
u

, (u, v) ̸= (0, 0). Then, both SO(2), US(2) are normal subgroups imply-
ing, for example, SR(2)/SO(2) is the quotient group. This is the union of all congruence
classes (left cosets of SO(2)) in the sense of Euclid:
[△o]SR(2)/SO(2) = {s△o : s ∈ρSO(2), ρ > 0}.
(7.27)
These are the congruence classes of triangles rooted at the origin. Each class is similar
to each other in the extended sense of Euclid but not congruent.
Example 7.23 (Target and Object Symmetry)
Examine target and object symmetry,
the generators G the targets. Man-made objects are characterized by well deﬁned
shapes with symmetries. The standard notion of symmetry of an object is studied
through the subgroup for the object αǫA,
S(α) = {s ∈S : sgα = gα} ⊂S.
Due to this equivalence, the inference set has to be reduced to the quotient space
S/S(α). For this deﬁne the identiﬁcation rule corresponding to the set of symmetries:
[g]∼= {g′ : g′ = sg, s ∈S(g)} .
(7.28)
Let S = SE(3) the special Euclidean group, for α = cylinder, then S(cylinder) =
SO(2) and the inference is performed over the 2-sphere, SO(3)/SO(2) = S2.
If on the other hand more structure is added to the cylinder such as for a missile
which also has four tail ﬁns separated by the angle 90◦then the relative symmetry set
instead includes the discrete 4-torus.
7.3 Transformations Constructed from Products of Groups
Since complex patterns are often constructed by combining simpler ones, it will be necessary to
work with the products of groups, each one applied to a single generator in the scene, thereby
representing the variability of multiple subconﬁgurations. To extend the domain of the group
transformations to the full conﬁgurations combining multiple generators, deﬁne the product group
acting on the multiple generators making up the conﬁgurations. We now generalize the transfor-
mations so that they can be applied locally throughout the space required. The product groups are
extended to act on the regular conﬁgurations c(σ) = σ(g1, . . . , gn) ∈CR in the obvious way. The
graph type σ will remain unaffected by the transformations; however, the conﬁguration may not
be regular after the similarities S = Sn
0 act on them.
For this, let the similarity group be of dimension d acting on the ith generator S0 : G0 ↔G0,
with the product S = S0 × S0 × · · · of dimension d times the number of generators.
Deﬁnition 7.24
The group action on multiple generators n : Sn
0 ×Gn →Gn becomes
n((s1, s2, . . . ), n((s′
1, s′
2, . . . ), (g1, g2, . . . ))) = ((s1 ◦s′
1)g1, (s2 ◦s′
2)g2, . . . ).
(7.29)
Deﬁne the action on the conﬁgurations Sn
0 : CR →C as
sσ(g1, g2, . . . ) = σ(s1g1, s2g2, . . . , ).
(7.30)
Denote a particular regular conﬁguration within the conﬁguration space as the template, one
ﬁxed element in the orbit:
c0 = σ(g0
1, g0
2, . . . , g0
n).
(7.31)
The deformable template becomes the orbit [c0]S.

182
7 MATRIX GROUP ACTIONS TRANSFORMING PATTERNS
The signiﬁcant extension being made here is to accommodate the glueing together of the
structured generating sets, with the transformations acting locally upon them. Thus far the group
transformations have acted globally on the entire generating background space. This presents
several signiﬁcant difﬁculties for the pattern theory; clearly with multiple groups acting regular
conﬁgurations may not be regular. Examine the application to the representation of a semi-rigid
chair swiveling around its base. Imagine that a translation and axis-ﬁxed rotation group is applied
globally via the Euclidean matrix E(2). However, the chair should rotate independently around
the same axis ﬁxed orientation. Should the cartesian product E(2)×E(2) unconstrained be applied
to the rigid substructure? Apparently not, otherwise the chair would tear apart. It is natural to wonder
why this difﬁculty does not arise in the application of single group transformations. For the matrix
groups acting globally local structure is mapped with its topology preserved. This is the property
of the matrix groups when viewed as smooth 1–1 and onto transformations (diffeomorphisms).
We will return to this later.
It will be necessary to deﬁne the subspace of transformations for which the regular
conﬁgurations stay regular.
Deﬁnition 7.25
Then the regularity constrained subset of similarities SR ⊂S = Sn
0
become
SR = {s ∈Sn
0 : sc ∈CR, ∀c ∈CR}
(7.32)
= ∩c∈CRSR(c)
where SR(c) = {s ∈Sn
0 : sc ∈CR}.
(7.33)
From the conﬁgurations which have acted upon it is natural to denote a particular regular
conﬁguration within the conﬁguration space of the template.
Deﬁnition 7.26
The template is one ﬁxed element in the orbit:
c0 = σ(g0
1, g0
2, · · · , g0
n).
(7.34)
The deformable template is the full orbit [c0]S.
The template expresses typical structure, the similarities variability around it. These subsets
play an important role in pattern theory, and in general they will not be subgroups of S! Typically
they are of lower dimension than that of S, but equality can hold:
dim(SR(c)), dim(SR) ≤dim(S).
(7.35)
Although they are not generally a subgroup,SR ⊂S : CR →CR is a semi-group with
identity. Clearly e ∈SR(c) for all c. If s, s′ ∈SR then this implies
(s ◦s′)c = s(s′c) = sc′,
c′ ∈CR
(7.36)
and since s ∈SR then sc′ ∈CR. It is sometimes the case that SR is a subgroup.
Example 7.27 (TANKS)
Examine the almost rigid body case corresponding to the
representation of tanks with movable turrets atop tractor assemblies. Let G
=
{tractors, turrets} the set of tractors and turrets at all possible orientations and posi-
tions, G0 = {g0
1, g0
2} ⊂G the tractors and turrets at the origin at 0◦orientation. The
graph type DIMER consists of bonds between the mounting support of the tractor
and the pivot point of the turret: the conﬁgurations c = DIMER(g1, g2) are regular if
βout(g1) = βin(g2) meaning the turret is mounted on the tractors. The basic similarity
group E(2) = SO(2)⊗R2 : G ↔G rotates the tractors and turrets around the body ref-
erence frame, and translates. The full product similarity S = E(2)2 does not generate
regular conﬁgurations; the similarity constrained subset SR = E(2) × SO(2), with its
action rotating the turret around the center point of contact of the tractor according to
s = ((O1, a1), (O2, a1)) ∈SR : DIMER(g1, g2) →
DIMER((O1, a1)g1, (O2, a1)g2).

7.3 TRANSFORMATIONS CONSTRUCTED FROM PRODUCTS OF GROUPS
183
Example 7.28 (Unclosed Contours: SNAKES and ROADS on the Plane)
Examine
the representation such as in HANDS [150] for unclosed ROAD contours in which
the generators are vectors G = R2, essentially directed line segments from the origin,
g0
i =

g0
i1
g0
i2

, with graph type σ = LINEAR(n) denoting the order of generators. The
polygonal representations are generated by adding the generators sequentially; hence
the ordering is required in the conﬁguration. To generate n-length curves in the plane,
attach the basic similarities rotations S0 = SO(2) rotating each of the generators:
O(θ) : gk →
 cos θ
sin θ
−sin θ
cos θ
  gk1
gk2

.
(7.37)
The regularity constrained set is SR = SO(2)n, and the conﬁguration space CR is the
space of all n-length piece-wise linear curves rooted at the origin. This is a homoge-
neous space under the group action SR on CR; the identity are copies of the 0-degree
rotations, with law of composition
s ◦s′ = (θ1 + θ′
1, . . . , θn + θ′
n) ∈SR.
To generate all n-length, piece-wise linear curves, add the global translation
group SR = SO(2)n × R2. An obvious template is a straight line along the x-axis of
length n so that the conﬁguration c0 = LINEAR(g0
1, g0
2, . . . ), g0
k =
 1
0

.
Example 7.29 (Closed Contours)
Adding the closure condition, the regularity con-
strained product transformation SR on CR is no longer a subgroup, just a lin-
ear submanifold.
Take as generators G
=
R2 chords from the circle g0
k
=
 cos(2πk/n) −cos(2π(k −1)/n)
sin(2πk/n) −sin(2π(k −1)/n)

; with these similarity groups S0 = US(2) × O(2)
the scales/rotations s =

u1
u2
−u2
u1

, where u1 = ρ cos φ, u2 = ρ sin φ, u1, u2 ∈R2,
ρ the scale parameters, and φ the rotation parameter. The full space of generators is
generated via the transitive action s ∈S0 : g0 →g = sg0 according to
gk =
 g1k
g2k

=

u1k
u2k
−u2k
u1k
 
g0
1k
g0
2k

.
The full space of transformations (regular and irregular conﬁgurations) becomes S =
Sn
0 = (US(2)×SO(2))2n = R2n. Figure 7.2 shows the circular closed contour templates
and an example deformation via the scale-rotation groups.
The template becomes c0 = (g0
1, g0
2, . . . , g0n). The closure condition means the
endpoint of the nth generator must be 0 giving 
n
i=1 gi = 0 implying SR = R2n−2 ⊂
R2n = Sn
0 , and SR is not a subgroup but is a submanifold! The loss of two-dimensions
from the closure condition is a linear manifold constraint given by the discrete Fourier
transform (DFT) condition, for j =
√
−1, then
n

k=1
 u1k
−u2k
u2k
u1k
  cos(2πk/n) −cos(2π(k −1)/n)
sin(2πk/n) −sin(2π(k −1)/n)

=
n

k=1
(u1k −ju2k)(ej(2πk/n) −ej(2π(k−1)/n)).
(7.38)

184
7 MATRIX GROUP ACTIONS TRANSFORMING PATTERNS
f (0)
f (1)
f(2)
f (3)
f (0)
f (1)
f (2)
f (3)
Figure 7.2 Showing the circular closed contour template and its deformation via the scale-rotation
groups.
This implies the highest frequency DFT component is zero:
n

k=1
(u1k −ju2k)e−j(2πk(n−1)/n) = e−j(2π/n)
n

k=1
(u1k −ju2k)e−j(2πk(n−1)/n = 0.
(7.39)
This SR is not a subgroup acting on CR, hence s ◦s′ /∈S(c, R) since the closure
condition is a linear constraint on the scale/rotation elements according to Eqn. 7.39
which is not satisﬁed. Addition of the vector of similarities would maintain closure
since this would maintain the highest frequency DFT coefﬁcient being zero. However,
composition is not addition.
7.4 Random Regularity on the Similarities
A fundamental interplay in pattern theory is the interaction betweeen probability laws on the
generators and probability laws on the transformations or similarities. The probability law on
transformations may be induced directly via the probabilities on the similarities S and the reg-
ularity constrained similarities SR. We are reminded of the work by Kolmogoroff in deﬁning
Kolmogoroff complexity, transferring the focus from the program (template) to the input to the
program (transformation).
Assume that the conﬁguration spaces can be identiﬁed with Euclidean vector spaces, C(σ) =
Rm, with S : Rm →Rm which is a bijection. Then the two probability densities are related via the
standard transformation of probability formulas.
Since the roles of the generators and the similarities are so often interchanged in so many
of the examples in pattern theory, it is instructive to be more explicit about how these densities
are related. For this, assume there is a bijection between generators and similarities, so that the
equation g = sg0, g, g0 ∈G has a unique solution in s ∈S. Then the density on the similarities
induces a density on the conﬁgurations.
Theorem 7.30
Assume c = sc0 ∈Rn with a bijection between conﬁgurations and simi-
larities. The two densities on the similarities pS(s) on S and conﬁgurations pC(c) on C(σ)
are related via the standard probability transformation formula with m(ds) the measure on
similarities:
pC(c|sc0)dc = pS(s) |det Dsc| m(ds),
(7.40)
with Dsc the Jacobian matrix of the transformation c = sc0.
Here are several examples.

7.4 RANDOM REGULARITY ON THE SIMILARITIES
185
Example 7.31 (Closed Contours with Directed Arcs as Generators)
Examineclosed
contours, the graph σ = CYCLIC(n) with generators the directed arcs gk = (g1k, g2k) ∈
G = R4, g1k = start point, g2k = end point, with bond values βin(g) = g1, βout(g) = g2
start and end points of the generators in the plane and bond function
βout(gk) = βin(gk+1)) so that g2k = g1k+1.
The start point of the k +1st arc equals the endpoint of the kth arc. For the graph
type CYCLIC, unlike LINEAR, the in-bond of the ﬁrst generator g1 is connected to the
out-bond of the last generator gn as well.
Apply the similarity of scales-rotations-translations S0 extending the template
generators S0 : G0 →G = R4 rotating and scaling the generators around their origin,
followed by translation, so that s = (A, a) ∈S0 : g′
k →gk = skg′
k,
gk = sg′
k =

g′
1k + ak
	
,

u1k
u2k
−u2k
u1k

(g′
2k −g′
1k) + g′
1k + ak

.
(7.41)
This is a transitive action on R4. Choose as generators the chords from the circle
and similarities so that
g0
k =
8 cos(2π(k −1)/n)
sin(2π(k −1)/n)

,
 cos(2πk/n)
sin(2πk/n)
9
,
s0
k =

I,
 cos(2π(k −1)/n)
sin(2π(k −1)/n)

;
then the identity I
=
 1
0
0
1

and the template is a connected circle c0
=
CYCLIC(s0
1g0
1, s0
2g0
2, . . . , s0ng0n). The graph type CYCLIC associated with the closure
condition implies g1 given the ﬁrst translation, all others are ﬁxed giving the set of 2n
rotation/scale elements implying SR = R2 × R2n−2 ⊂S = (US(2) × SO(2)
)
*+
,
R2
×R2)n
which is not a group. The loss of two-dimensions follows from the closure condition
on the sum of the generators, which is a linear manifold constraint.
The probability on the transformations induces the probability on the genera-
tors. Deﬁne the generators as 4 × 4 matrices operating on the similarity according to
gk = skg0
k =


g0
21,k
g0
22,k
1
0
g0
22,k
−g0
21,k
0
1
0
0
1
0
0
0
0
1




u1k
u2k
a1k
a2k

;
(7.42)
pS(s1, s2, . . . , sn)m(ds) induces a probability on the generators according to
p(g1, . . . , gn)dg1, . . . , dgn = pS(s1|s1g0
1=g1, . . . , sn|sng0n=gn)
n

k=1
det Dskgk
 dsk.
(7.43)
Since
| det Dskgk| =

det


g0
21,k
g0
22,k
g0
22,k
−g0
21,k
1
0
0
1
0
1
0
0
1



= (g0
21,k)2 + (g0
22,k)2,
(7.44)
then the determinant of the Jacobian matrix is the product of the lengths of the
generators squared, $n
k=1(g0
1,k)2 + (g0
2,k)2.

186
7 MATRIX GROUP ACTIONS TRANSFORMING PATTERNS
Example 7.32 (Unclosed Contours: Vectors as generators)
Now take as the genera-
tors vectors g = (g1, g2) ∈R2 and S0 the two-parameter scale rotation group plane
S0 = US(2) × SO(2), each subset [g] ˜S0 identiﬁed with a vector in R2. Thus a genera-
tor is an equivalence class. The elements of S0 are 2 × 2 matrices s =

u1
u2
−u2
u1

,
u1, u2 ∈R2. Two vectors (generators) are similar if one can be brought into the other
by rotating and stretching it.
Deﬁning the template generators to be

g0
1,k
g0
2,k

notice that
gk = skg0
k =

u1k
u2k
−u2k
−u1k
 
g0
1k
g0
2k

;
(7.45)
then for the template generators nondegenerate, gk, g0
k uniquely determine sk.
The density on the transformations induces the density on the conﬁguration same as
above:
pC(c(σ))dc(σ) = pS(s1|g1=s1g0
1, . . . , sn|gn=sng0n)

det
n

i=1
Dsigi

ds1, . . . , dsn.
(7.46)
Then the determinant of the Jacobian is the product of determinants of matrices
Dskgk =
 ∂g1k/∂u1k
∂g1k/∂u2k
∂g2k/∂u1k
∂g2k/∂u2k

=

g0
1k
g0
2k
g0
2k
−g0
1k

(7.47)
and the absolute value of the determinant of the Jacobian is the product of the lengths
of the generators squared as above.
Example 7.33 (Triangulated Graphs)
Consider triangulations Tn of the 2-sphere
S2 ⊂R3 and let the connection type  express the topology of Tn. Generators
are patches which are topologically connected according to the triangulated sphere
topology TRIANG(n) ∈ shown in Figure. 6.3, Chapter 6.1. Let the n generators
represent nondegenerate triangles with vertices vji, and identify the generators g
with 3 × 3 matrices which are nonsingular if the vertices are not on a great circle,
so that gi = (v1i, v2i, v3i); vji ∈S2; i = 1, 2, . . . , n with arity ω(g) = 3 and the bonds
βj(gi) = vji; j = 1, 2, 3. The connector σ connects βj(gi) with βj′(gi′) if the two triangles
gi and gi′ are contiguous with vji = vj′i′ which shall also be the bond relation. The
same generator sometimes appears in more than one of the closed loops in the system.
Choose the general linear group as the similarity group, S = GL(3). Then, for
any two generators g1 and g2 none of which is on a great circle, the equation g1 = sg2
has a unique solution in s. Denote the number of edges and vertices by ne and nv so
that Euler’s relation can be written as
nv −ne + n = 2.
But each triangle has three edges, each of which appears in two triangles, so that
ne = 3
2n and nv = (n/2) + 2. Each vertex has 3 degrees of freedom so that for ﬁxed n
the manifold CR will have dimension 3
2n + 6. The dimension of the similarity group
we have chosen is dim(S) = 9n, implying that 15
2 n−6 independent constraints on the
group elements are required.

7.4 RANDOM REGULARITY ON THE SIMILARITIES
187
The template becomes c0 = TRIANG(n, g0
1, g0
2, . . . , g0n); the deformed template
becomes
c = TRIANG(n, g1, g2, . . . , gn) = TRIANG(n, s1g0
1, s2g0
2, . . . , sng0
n).
(7.48)
Solving for the similarities in matrix form gives si = gi(g0
i )−1, establishing a
linear relation with constant Jacobian between the vertices and the similarities, so far
unconstrained.
Adensity p(s1, s2, . . . , sn) on the similarities induces a density on the generators,
p(g1, . . . , gn)dg1, . . . , dgn = pS(s1|g1=s1g0
1, . . . , sn|gn=sng0n)ds1, . . . , dsn,
where the vertices in the gis are not constrained.
Example 7.34 (LEAVES)
Real world systems can have topological structure with
complex connection types. A moderately complex connector is shown in panel 1 of
Figure 7.3 showing a template for stylized maple leaves. The generators are directed
line segments of arity ω(g) = 2, the bond relation ρ = EQUAL, and the basic similarity
group S = US(2) × O(2). The connector σ is the directed graph in the ﬁgure, where
n(σ) = 24 has been chosen small for descriptive purposes.
Note that the same generator appears in more than one of the closed loops in the
system. To express closure of loops we obtain linear vector relations generalizations
of the closure condition in example 7.29.
Introduce the adjacency matrix of length n = 24 with entries 0, 1, −1, for each
loop that we are considering. The ﬁrst loop gives a vector with zeros everywhere
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
2
4
7
8
12
14
15
16
17
18
19
20
21
22
23
24
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
1
2
3
4
5
6
9
10
11
12
13
14
15
16
17
18
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
(1)
(2)
(3)
(4)
(5)
Figure 7.3 Top row: Panel 1 shows the basic leaf connector; panel 2 shows the forest of the maple
leaf with panel 3 showing the corresponding independent loops. Bottom row: Panel 4 shows a
second forest with panel 5 showing its associated independent loops.

188
7 MATRIX GROUP ACTIONS TRANSFORMING PATTERNS
except at locations 13, 14, 15, where the entry is 1, and location 22 with entry −1. In
the same way the second loop would have zeros except at locations 11,12,16,22 and
location 23 with the entry −1. The closure of loops are encapsulated via the following
sets of linear vector relations:
0 = g15 + g14 + g13 −g22,
0 = g16 + g22 + g12 + g11 −g23,
0 = g17 + g23 + g10 −g24,
0 = g18 + g24 + g9 + g8,
0 = g7 + g6 −g21 −g18,
0 = g21 + g5 −g20 −g17,
0 = g20 + g4 + g3 −g19 −g16,
0 = g19 + g2 + g1 −g15,
0 = g14 + g13 + g12 + g11 + g10 + g9 + g8 + g7 + g6 + g5 + g4 + g3 + g2 + g1.
(7.49)
Now form matrix Awhich is a 9×24 matrix, a composition of the rows denoting
the nine coefﬁcient vectors, and the columns denoting the generators. A = (aij) is the
matrix whose elements have the following values:
aij = 1
if generator j is in vector i and their orientations concide,
(7.50)
aij = −1
if generator j is in vector i and their orientations do not concide,
(7.51)
aij = 0
if generator j is not in vector i.
(7.52)
The A matrix for the leaf connector becomes
A =


0
0
0
0
0
0
0
0
0
0
0
0
1
1
1
0
0
0
0
0
0
−1
0
0
0
0
0
0
0
0
0
0
0
0
1
1
0
0
0
1
0
0
0
0
0
1
−1
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
1
0
0
0
0
0
1
−1
0
0
0
0
0
0
0
1
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
1
0
0
0
0
0
1
1
0
0
0
0
0
0
0
0
0
0
−1
0
0
−1
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
−1
0
0
−1
1
0
0
0
0
0
1
1
0
0
0
0
0
0
0
0
0
0
0
−1
0
0
−1
1
0
0
0
0
1
1
0
0
0
0
0
0
0
0
0
0
0
0
−1
0
0
0
1
0
0
0
0
0
1
1
1
1
1
1
1
1
1
1
1
1
1
1
0
0
0
0
0
0
0
0
0
0


.
(7.53)
To ﬁnd the rank of the matrix A we have to ﬁnd the number of linearly inde-
pendent rows. Here all the rows are linearly independent except row 9, which is just
a linear addition of the ﬁrst eight rows. So the rank of the matrix A is 8. There are of
course other loops such as
g24 + g9 + g8 + g7 + g6 −g21 = 0.
(7.54)
The probability on the similarities is induced as follows. The generators are
directed line segments of arity ω(g) = 2, with the bond relation ρ = EQUAL, and the
basic similarity group S0 = US(2)×O(2) with elements

u1
u2
−u2
u1

. The connector
σ is the directed graph as shown in Figure 7.4.

7.4 RANDOM REGULARITY ON THE SIMILARITIES
189
B(s)
N(s)
T(s)
U(p)
X
S(v)
v
(1)
(2)
Figure 7.4 (i) Panel 1 shows the three orthonormal vector ﬁelds T, N, B of the curve x. Panel 2
shows a surface with the normal vector ﬁeld and the corresponding shape operator S(v).
To construct the probability on the full similarity group S = S24
0 , begin with the
potential function on si = (ui1ui2) with
e−ij(si,sj) = e−(1/2τ 2)((ui1−1)−a(uj1−1))2−(1/2τ 2)(ui2−auj2)2,
(i, j) ∈σ,
a ̸= 1.
Let s denote the 48-vector of similarities s = (u11, u12, u21, u22, . . . ) and introduce
the quadratic form

ij
s∗
i K−1
ij sj =

(i,j)∈σ

((ui1 −1) −a(uj1 −1))2 + (ui2 −auj2)2
,
(7.55)
and the density is Gaussian proportional to e−(1/2τ 2)Q(s), Q(s) = 
ij s∗
i K−1
ij sj with
similarity s ∈S = R48. Now to condition the closure constraint express the closed
cycle conditions in matrix vector form via the constraint matrix C of size ncyclo × n,
where ncyclo is the so-called cyclomatic number, and C is of full rank:
C


g1
...
gn

= 0 ,
(7.56)
with the generator matrix of size n × 2. Now apply known properties of conditioned
Gaussian distributions implying that the s-vector, conditioned as above, will have the
modiﬁed covariance matrix
K −KC(C∗KC)−1C∗K .
(7.57)
This implies that pattern synthesis can be achieved by a fast direct algorithm
without using iterative methods. Indeed, if we choose the Cholesky decomposition
of the covariance matrix, K = LL∗, in terms of a lower triangular matrix L, the above
expression can be written as
L[I −L∗C(C∗KC)−1C∗L]L∗= LPL∗.
(7.58)
Note that P is symmetric and idempotent so that the whole expression takes the
form of a “square” K = (LP)(LP)∗so that s = LPx where x is a Gaussian vector with
independent components.
Example 7.35 (Graph Theory)
To determine the set of independent loops, recall
somedeﬁnitionsingraphtheory.Aspanningtreeisaconnectedsubgraphofaconnected
graph containing all the nodes of the graph but containing no loops. The branches of

190
7 MATRIX GROUP ACTIONS TRANSFORMING PATTERNS
a tree are called twigs and those branches not in a tree are called links. If a graph is not
connected, the object corresponding to a tree is a forest.
Now let us write the procedure for determining the independent or fundamental
loops. Given a graph, a forest is selected. Then, one at a time, the union of a link with
the forest is considered. Each resulting subgraph contains one loop. In each case, the
loop will be characterized by the fact that all but one of its branches are twigs of the
chosen forest. The orientation of the loop is chosen to coincide with that of its deﬁning
link. Each of the loops is an independent one and the total number of such loops
which will be equal to the number of links is b −n −1. The rank of the matrix is then
rank = b −n −1 where b is the number of branches and n is the number of nodes
in the graph generated by our generators. In the case above b = 24 and n = 17; the
rank is 8.
Returning to the problem, we ﬁrst consider the ﬁrst forest shown in the panels 2
and 3 of the top row of Figure 7.3. One at a time, take the union of links 13,11,10, 9,6,5,3
and 1 with the forest to get the eight independent loops corresponding to each link.
The eight independent loops with their orientations are {13,22,15,14} {11,23,16,22,12},
{10,24,17,23}, {9,8,18,24}, {6,21,18,7}, {5,20,17,21}, {3,19,16,20,4}, and {1,15,19,2}. They
are shown in panel 3 of Figure 7.3.
Although the total number of independent loops is always ﬁxed for a given
graph, the independent loops are not unique. Depending on the forest we choose we
obtain different independent loops corresponding to the different links of the chosen
forest. Consider the forest shown in the bottom row panels of Figure 7.3. Like the
previous steps, one at a time take the unions of links 19,20,21,7,8,24,23, and 22 with
the forest obtaining the eight independent loops corresponding to each link. The set
of these eight loops are {19,2,1,15}, {20,4,3,19,16}, {21,5,20,17}, {7,6,21,18}, {8,18,24,9},
{24,10,23,17}, {23,11,12,22,16}, and {22,13,14,15}.
7.5 Curves as Submanifolds and the Frenet Frame
In the following two sections let us examine curves and surfaces which often provide generators
for the patterns we study in shape. Almost all the geometric work which we do in pattern theory
concentrates on submanifolds of Rd, d = 1, 2, 3, corresponding to points, lines, surfaces, and
subvolumes. Begin with the geometry of curves deﬁned in the continuum based on their deﬁnition
via curvature, torsion through the Frenet representation of curves.
Deﬁnition 7.36
A regular curve x : D = [0, 1] →R3 is a differentiable function
x : s ∈D = [0, 1] ⊂R →x(s) ∈R3
(7.59)
assumed to be regular so that ˙x(s) ̸= 0, s ∈[0, 1] with velocity vector ˙x(s). and speed the
norm ∥˙x(s)∥.
Curves will be deﬁned by their speed, curvature, torsion.
Deﬁnition 7.37
Let x(·) be a regular smooth curve, x(s), s ∈[0, 1], then the speed,
curvature, and torsion are given by
α(s) = ∥˙x(s)∥R3,
κ(s) = ∥˙x(s) × ¨x(s)∥
α(s)3
R3,
τ(s) = ⟨(˙x(s) × ¨x(s)), ...x(s)⟩R3
∥˙x(s) × ¨x(s)∥
.
(7.60)

7.5 CURVES AS SUBMANIFOLDS AND THE FRENET FRAME
191
For unit speed curves α = 1 with
κ(s) = ∥¨x(s)∥R3,
τ(s) =
1
κ2(s)det


˙x1(s)
˙x2(s)
˙x3(s)
¨x1(s)
¨x2(s)
¨x3(s)
...x 1(s)
...x 2(s)
...x 3(s)

.
(7.61)
The arc length of x from s = a to s = b is given by l(a, b) =
 b
a ∥˙x(s)∥ds. Length does not
change under reparameterization, and since the curvature and torsion are simpler we will work
in the arc-length parameterization.
Lemma 7.38
Let φ : [0, 1] →[0, 1] be any diffeomorphism, φ0 = 0, φ1 = 1, ˙φ > 0, then
the length of any curve x in Rd is invariant to all such φ, so that
 1
0
::::
d
dtx ◦φt
::::Rd dt =
 1
0
∥˙xt∥Rd dt.
(7.62)
In particular on deﬁning φt = (
 t
0 ∥˙xt∥dt)/(
 1
0 ∥˙xt∥dt), x(·) in arc-length parame-
terization x ◦φ−1
t
, t ∈[0, 1] is constant speed:
::::
d
dtx ◦φ−1
t
::::Rd =
 1
0
∥˙xt∥Rd dt , t ∈[0 1].
(7.63)
Proof
Since φ is a diffeomorphism, ˙φ > 0, then
 1
0
::::
d
dtx ◦φt
::::Rd dt =
 1
0
∥˙x ◦φt∥Rd| ˙φt| dt (a)
=
 1
0
∥˙xs∥Rd ds,
(7.64)
with (a) following from the substitution of variables s = φt. Let ψt = φ−1
t
, then
˙ψt = 1/( ˙φ(φ−1
t
)), giving | ˙ψt| = (
 1
0 ∥˙xt∥Rd dt)/(∥˙x ◦ψt∥Rd) implying
::::
d
dtx ◦φ−1
t
::::Rd = ∥˙x ◦ψt∥Rd| ˙ψt| dt =
 1
0
∥˙xt∥Rd dt.
(7.65)
Now examine the Frenet representation following [151, 152] describing the mathematical
measurements of the turning (curvature) and twisting (torsion) curves in R3. It follows that in the
Frenet representation, the derivatives of the tangents, curvature, and torsion form an orthogonal
frame.
Theorem 7.39 (Frenet)
Let x : D ⊂R3 →R3 be a unit speed curve, so that ∥˙xs∥R3 = 1
for each s in D. Then the three vector ﬁelds associated with the curve x, (i) the unit tangent
vector ﬁeld T = ˙xs, (ii) the unit normal curvature vector ﬁeld N = ˙T/∥˙T∥R3, and
(iii) the binormal vector ﬁeld B = T × N, provide an orthonormal frame T(s), N(s), B(s),
s ∈[0, 1],
˙T = κN,
˙N = −κT + τB,
˙B = −τN,
(7.66)
Proof
First for the orthonormality of T, N, B. Since T has constant length 1, differen-
tiation of ⟨T, T⟩R3 gives 2⟨˙T, T⟩R3 = 0, so ˙T is always orthogonal to T, that is, normal
to x. Thus the unit normal ﬁeld N = ˙T/κ is normal to T and tells the direction in which
x is turning. B is orthonormal to T, N by deﬁnition of the cross product.

192
7 MATRIX GROUP ACTIONS TRANSFORMING PATTERNS
Now for the three equations. The ﬁrst equation is the deﬁnition of curvature
ﬁelds ˙T = κN with κ = ∥¨x∥. The third equation follows since ⟨˙B, B⟩R3 = 0 as B is unit
length, and since ⟨B, T⟩R3 = 0 differentiating gives
0 = ⟨˙B, T⟩R3 + ⟨B, ˙T⟩R3 = ⟨˙B, T⟩R3 + ⟨B, κN⟩R3 = ⟨˙B, T⟩R3.
(7.67)
Thus ˙B is orthogonal to B, T implying ˙B = αN with α = −τ.
For the second equation expand ˙N in terms of the orthonormal vectors T, N, B:
˙N = ⟨˙N, T⟩R3T + ⟨˙N, N⟩R3N + ⟨˙N, B⟩R3B.
(7.68)
The ﬁrst term is −κT since differentiation of ⟨N, T⟩R3 = 0 gives
⟨˙N, T⟩R3 + ⟨N, ˙T⟩R3 = 0 ⇒⟨˙N, T⟩R3 = ⟨−N, ˙T⟩R3 = ⟨−N, κN⟩R3 = −κ.
(7.69)
The second term is 0 since ⟨˙N, N⟩R3 = 0 since N is a unit vector ﬁeld. The third term
follows differentiating ⟨N, B⟩R3 = 0 giving
⟨˙N, B⟩R3 = ⟨−N, ˙B⟩R3 = ⟨−N, −τN⟩R3 = τ.
(7.70)
The ﬁelds T, N, B are depicted in panel 1 of Figure 7.4. The Frenet equations show that the
rotating orthonormal frame ﬁelds T, N, B, and therefore the curves themselves, are completely
determined by the torsion and curvature functions up to rigid motions. If κ = 0 the curve is a
straight line; if τ = 0, the curve is contained in a plane. The values of κ are always nonnegative;
those of τ can take both signs.
Example 7.40 (Frenet representation of curves in the cortex [153,154])
External
features of great importance in the brain are the curves depicting the regions of folding
in the neocortex called the sulcal curves. These have been well characterized in the
macaque [155], and are actively being studied in humans by various groups [156].
Despite their anatomic and functional signiﬁcance, the sulcus folding of the neocor-
tex in mammals appear consistently and exhibit pronounced variability in size and
conﬁguration. Khaneja [153, 154] has examined representations of sulcal curve vari-
ation via the Frenet representation of curves. Figure 7.5 shows three macaque brains
(panels 1–3) depicting curves delineating the folds of the neocortical surface termed
the sulcal curves, including the Arcuate Sulcus, Lateral Sulcus, Central Sulcus, and
others. Let the sulcal curve in parametric form be x(s), s ∈[0, L] with arclength param-
eter s. Adiscrete representation supporting curvature and torsion is obtained from the
xi = (xi1, xi2, xi3), i = 1, 2, . . . , N, equidistant points spaced some distance δ = 1 apart,
from which ﬁrst, second, and third difference vectors are generated, ˙xδ
i , ¨xδ
i , ...x δ
i . The
orthogonal moving frame Ti, Ni, Bi, i = 1, . . . are obtained via Gram-Schmidt orthog-
onalization. The discrete curvatures and torsions are expressed in terms of the frame
vectors via the discretized Frenet relations to solve
(Ti+1 −Ti) = κiNi,
(Ni+1 −Ni) = −κiTi + τiBi,
(Bi+1 −Bi) = −τiNi,
(7.71)
with torsions and curvatures generated using discrete versions of Eqn. 7.61.
To associate a distribution, deﬁne the sequence of triples curvatures, tor-
sions, and lengths as a Gaussian process {(li, κi, τi), i = 1, . . . , N}, mean process

7.5 CURVES AS SUBMANIFOLDS AND THE FRENET FRAME
193
Figure 7.5 Panels 1–3 show the three macaque brain sulcal curves used for estimating the random
sulcal model. Panel 4 shows the mean sulcal curves computed from the three brains displayed
on the average brain generated from the 3 in the left panels; panel 5 shows samples of the
sulcal curves from the distribution on curvature and torsion. Data courtesy of David Van Essen,
Washington University.
¯li = (1/M) 
j lj
i, ¯κi = (1/M) 
j κj
i , ¯τi = (1/M) 
j τj
i , and independent variances.
This gives the potential for the log-prior distribution of the form
log π(li, κi, τi, i = 1, . . . , N) = η

i
(li −¯li)2 + β

i
(κi −¯κi)2 + γ

i
(τi −¯τi)2.
(7.72)
Panel 1–3 of Figure 7.5 shows the loci of sulcal curves on the three macaque
brains depicting the locations of the deep folds. From these the mean and vari-
ability of the length, curvature and torsion functions of the sulcus are empirically
computed using the standard empirical mean formulas across the hand traced
sulci.
To synthesize the sulcus curves associated with this distribution generate the
random variables li, κi, τi i = 1, 2, . . . , N from the Gaussian distribution and then solve
the discretized Frenet equations sequentially. Note the starting vectors v1
1, v2
1 are unde-
termined expressing the fact that the patterns are in the equivalence set modulo
the special Euclidean group SE(3). Each principal sulcal curve was sampled into 50
points. Panel 4 shows the mean sulcus curves displayed on the mean brain. Panel
5 shows the sulcal curves synthesized from the Frenet equations with random cur-
vature and torsion functions sampled from the empirically estimated means and
variances.
Example 7.41 (Representations in Pattern Theory are Not Unique)
The represen-
tations in pattern theory are not unique. Alternative to the Frenet representation
of the motion of the orthogonal frame through the instantaneous skew-symmetric
matrix acting on it, the orthogonal frame can be represented directly as vectors.
Let xi ∈R3; i = 1, 2, . . . denote equidistant points on the space curve representing
the sulcus curves assumed to be equi-spaced some distance l apart. The transfor-
mations are deﬁned from the generalized linear group GL(3) and constructed as a
Gauss-Markov process. Then generators g ∈G = R9 are deﬁned to be the triplet of
vectors
gi = (v(1)
i
= xi+1 −xi, v(2)
i
= xi+2 −xi+1, v(3)
i
= xi+3 −xi+2).
(7.73)
Then the similarity transformation si ∈GL(3) takes its action
g′
i = sigi, with gi = (v(1)
i
, v(2)
i
, v(3)
i
),
g′
i = (v(1)′
i
, v(2)′
i
, v(3)′
i
).
(7.74)
Attach to a generator gi an inbond as (ρin(gi) = xi) and outbond as (ρout(gi) = xi+3)
with relation ρ=EQUAL sticking generators together.

194
7 MATRIX GROUP ACTIONS TRANSFORMING PATTERNS
DeﬁneS0(ω), S1(ω), . . . tobea9×1Gassianvectorprocesswithmeansµ0, µ1, . . .
and covariance induced by the ﬁrst order autoregression
(Sj −µj) = α(Sj−1 −µj−1) + Wj,
(7.75)
where µj is the mean of the jth transformation group ESj = µj, and Wj represents a
zero-mean 3 × 1 white noise process with covariance EWiW∗
j = δ(i −j) σ 2I, I the 9 × 9
identity matrix. Reparameterizing in Yj = Sj −µj gives
Yi−1 = αYi−2 + Wi−1 ,
with Y0 = W0.
(7.76)
Drawing Y0 = W0 from the stationary distribution so that EY0Y∗
0 = EW0W∗
0
= σ 2I, then the Yi are stationary, implying EYiY∗
i = E(αYi−1 + Wi)(αYi−1 + Wi)∗=
EYi−1Y∗
i−1, giving
Kii = EYiY∗
i
=
σ 2
1 −α2 I,
Ki,i+j = EYi+jY∗
i = αjKii,
(7.77)
with the 9n × 9n covariance matrix K = (Kij), and the I the 9 × 9 identity matrices
representing assumed independence between the similarities. The inverse covariance
has special tri-diagonal structure
K−1 = 1
σ 2


I
−α I
−α I
(1 + α2) I
−α I
...
...
...
−α I
(1 + α2) I
−α I
−α I
I


)
*+
,
J
= 1
σ 2 J.
(7.78)
Given M realizations of the above process each of length N, the joint density has
the form of M independent Gaussian vectors with the log-likelihood given by
log p(Y1, Y2, . . . , YM; K) = −M
2 log det 2πK −1
2
M

m=1
Y∗
mK−1Ym,
(7.79)
= −M
2 log det 2 πK −M
2 tr(K−1),
(7.80)
where tr(·) is the matrix trace with sample covariance matrix  = (1/M) 
M
m=1 ymy∗m.
The maximum likelihood estimate of the parameters ˆα, ˆσ ←arg max(α,σ) log p(; K),
is given by
ˆσ 2 = arg max
σ 2
log p(; K) = arg max
σ 2
−9n log(σ 2) + log |J| −tr[J]
σ 2
(7.81)
= tr[J]
9n
= tr() + ˆα2ϒ() −ˆα()
9n
,
(7.82)
where ϒ(B) =
i=N−1

i=2
tr(Bi,i) ,
(B) =
i=N−1

i=1
tr(Bi,i+1) +
i=N

i=2
tr(Ai,i−1).
(7.83)
To derive an equation only in σ 2, deﬁne the block matrices B = (Bij); then the
second equation for calculating ˆα arises computing the variation with respect to α.

7.5 CURVES AS SUBMANIFOLDS AND THE FRENET FRAME
195
The variation of log |J| can be written as δ log |J|
= tr(J−1δJ), implying that
maximizing with respect to α and keeping σ 2 constant gives
∂log p(; K)
∂α
ˆα
= tr
8
(J−1 −/ˆσ 2) ∂J
∂α
9
= 0
where
δJ
δα =


0
−1
−1
2α
−1
...
...
...
−1
2α
−1
−1
0


.
(7.84)
This gives 2ˆαϒ(J−1 −(/ˆσ)) = (J−1 −(/ˆσ 2)). Using ϒ(J−1) = (9(n−2))/(1−ˆα2),
(J−1) = (2ˆα9(n −1))/(1 −ˆα2) yields a second equation which the MLE ˆα, ˆσ 2 must
satisfy:
ˆσ 2 = 1 −ˆα2
18ˆα () −(1 −ˆα2)
9
ϒ().
(7.85)
Solving Eqns. 7.82 and 7.85 simultaneously in ˆσ 2, ˆα gives the maximum-likelihood
estimates.
Figure 7.5 shows three brains which were used for estimating sulcal curve vari-
ability (panels 1–3). Using these three macaque brain maps the mean sulcal curves
were computed, with the ˆα, ˆσ 2 estimates generated by maximizing the log-likelihood.
Panel 3 shows different realizations of sulcal curves superimposed over the template.
The parameters of the Markov process ˆα, ˆσ 2 were estimated from the population of
three brains for the Gaussian distributions for each sulcus on the space GL(3)M × R3.
Each sulcus was named following the nomenclature in Felleman and Van Essen [155].
The values found for the sulci were Superior Temporal Sulcus α = 0.25, σ = 0.01,
Arcuate Sulcus α = 0.07, σ = 0.02, Central Sulcus α = 0.32, σ = 0.01, Intra-Parietal
Sulcus α = 0.19, σ = 0.02, Inferior Occipital Sulcus α = 0.30, σ = 0.01.
7.6 2D Surfaces in R3 and the Shape Operator
Curves and surfaces in R3 provide a rich source of patterns, and provide concrete examples of
manifolds, in particular 2D manifolds, which are familiar. Now it is natural that for a surface
M ⊂R3 there exists a tangent space at each of its points which resembles a region in the plane R2.
Deﬁnition 7.42
A 2D surface is a subset M ⊂R3 having the property that for each point
p ∈M there exists a neighborhood O ⊂M of p, an open set D ⊂R2, and a smooth
coordinate patch (inﬁnitely differentiable) x : D →O which is a diffeomorphism
x : (u, v) ∈D ⊂R2 →x(u, v) = (x1(u, v), x2(u, v), x3(u, v)).
(7.86)
Deﬁne the tangent space Tp(M) of the surface at p ∈M as the set of all vectors
generated by the 2D span of tangent vectors (∂x(u, v))/∂u, (∂x(u, v))/∂v. These are often
called the coordinate frames spanning the tangent space.

196
7 MATRIX GROUP ACTIONS TRANSFORMING PATTERNS
7.6.1 The Shape Operator
Just as the shape of a curve in R3 can be measured via its curvature and torsion functions, similarly
the shape of a surface M in R3 is described inﬁnitesimally by a feature associated with the tangent
plane called the shape operator S, a 2×2 linear operator describing how the tangent plane changes
as one follows particular curves on the surface. The algebraic invariants (determinant, trace, etc)
of the shape operator encode the geometric meaning for the surface M.
Now natural local coordinates which we shall use to understand the surface are devel-
oped as follows. Deﬁne the partial derivative notation of a function of two variables f(u, v) as
fu = ∂f/∂u, fv = ∂f/∂v.
Theorem 7.43
Given is smooth surface submanifold M ⊂R3, with local coordinate patch
x : D ⊂R2 →O ⊂M with orthogonal frame E1, E2, E3 at p ∈M with E1, E2 spanning the
tangent plane Tp(M). Then the local coordinate representation representing the surface locally
around p ∈O given by
x(u, v) = p + uE1 + vE2 + E3f(u, v)
(7.87)
has tangent vectors and surface normal
∂x(u, v)
∂u
= E1 + E3fu(u, v),
∂x(u, v)
∂v
= E2 + E3fv(u, v),
(7.88)
n = −fuE1 −fvE2 + E3

1 + f2u + f2v
.
(7.89)
Proof
Over the patch the tangent space Tx(M) is clearly spanned by the natural
coordinate vectors E1 + fuE3, E2 + fvE3. The normal vector to the surface n is given by
the cross product
n = det


E1
−E2
E3
1
0
fu
0
1
fv

= −fuE1 −fvE2 + E3

1 + f2u + f2v
.
(7.90)
Following the usual conventions the coordinate frame dependence is suppressed with local
coordinate representation x(u, v) = (u, v, f(u, v)) and orthogonal frame on (u, v) ∈D written as
∂x
∂u =


1
0
fu

,
∂x
∂v =


0
1
fv

,
n =
1

f2u + f2v + 1


−fu
−fv
1

.
(7.91)
Notice, in this local coordinate system p = x(0, 0) and f(0, 0) = 0, and fu(0, 0) = fv(0, 0) = 0,
since the tangent vectors along the u and v curves (keeping ﬁrst v ﬁxed, and alternately u ﬁxed)
must be in the tangent space given by the E1, E2 plane. Since understanding the shape operator
which involves curvature will only be quadratic terms, so that f can be approximated near (0, 0)
via the quadratic Taylor series
f(u, v) = fuu(0)u2
2 + 2fuv(0)uv
2 + fvv(0)v2
2 +
2

k=0
o(ukv2−k).
(7.92)

7.6 2D SURFACES IN R3 AND THE SHAPE OPERATOR
197
Deﬁnition 7.44
The shape-operator of M at p is a linear operator Sp : Tp(M) →Tp(M)
attaching to each tangent vector vp ∈Tp(M), the vector Sp(vp) given by the negative of the
directional derivative along vp:
Sp(vp) = −d
dtn(p + tvp)|t=0 .
(7.93)
The directional derivative of the vector ﬁeld n in the direction of vp according to (d/dt)n(p+
tvp)|t=0.
The right panel of Figure 7.4 shows the shape operator.
With this the following can be established.
Lemma 7.45
Let the surface M have local coordinate patch x : D ⊂R2 →O ⊂M with
orthogonal frame E1p, E2p, E3p at p ∈O and E1p, E2p spanning the tangent space Tp(M).
Then with n = (−fuE1 −fvE2 + E3)/

1 + f2u + f2v the unit normal vector ﬁeld on M over
the patch, the shape operator at x(0, 0) = p the 2 × 2 symmetric operator is
Sx(0,0) =
 fuu(0, 0)
fuv(0, 0)
fvu(0, 0)
fvv(0, 0)

.
(7.94)
The shape operator over the local coordinate patch takes the form
Sx(u,v) =
1

1 + f2u + f2v
 1 + f2u
fufv
fufv
1 + f2v
−1  fuu
fuv
fvu
fvv

,
u, v ∈D.
(7.95)
Proof
To compute the shape operator at the origin (0, 0), take perturbations along
tangent vectors of the form v0 = α1E1 + α2E2 giving
S0(α1E1 + α2E2) = −d
dtn(tα1E1 + tα2E2)|t=0
(7.96)
= (α1fuu + α2fuv)E1 + (α1fvu + α2fvv)E2.
(7.97)
Therefore, identifying the shape operator with 2 × 2 matrices, it is a symmetric linear
operator uniquely represented by the 2×2 symmetric matrix of the form S0 of Eqn. 7.94.
Example 7.46 (The Sphere)
Return to the sphere Example 8.8, S2 = {y ∈R3 :
∥y∥=

y2
1 + y2
2 + y2
3 = r}, and examine the coordinate patch x : (u, v) ∈D →
x(u, v) = (u, v,

r −u2 −v2) at the north pole p = (0, 0, 1) with coordinate frames
E1 = (1, 0, 0), E2 = (0, 1, 0). The tangents and curvatures correspond to
xu(u, v) =

1, 0,
−u

r2 −u2 −v2

,
xv(u, v) =

0, 1,
−v

r2 −u2 −v2

,
(7.98)
xuu(u, v) =

0, 0,
−1

r2 −u2 −v2 +
u2
(r2 −u2 −v2)3/2

=

0, 0, −
r2 −v2
(r2 −u2 −v2)3/2

,
0xvv(u, v) =

0, 0, −
r2 −u2
(r2 −u2 −v2)3/2

,
xuv(u, v) = xvu(u, v) = 0.
(7.99)

198
7 MATRIX GROUP ACTIONS TRANSFORMING PATTERNS
u
v
e2
e1
U(p)
p
c
Figure 7.6 Shape operator on surfaces.
Then n the “outward normal” over the disk D orthogonal to the span of the
tangent space xu, xv of Eqn. 7.98. becomes
n = 1
r (u, v,

r2 −u2 −v2).
(7.100)
The shape operator S(v) in the direction of the tangent vector v = (vx, vy, vz) becomes
∇vn = d
dt(u + tvx, v + tvy, z + tvz)|t=0 = (vx, vy, vz)
r
,
(7.101)
where z =

r2 −u2 −v2. The outward normal is shown in the left panel of Figure
7.6. Then S(v) = −v/r for all v, so the shape operator is merely scalar multiplication
by −1/r. This uniformity reﬂects that a sphere bends equally in all the directions.
Example 7.47 (The Cylinder)
Examine the circular cylinder C = {y : y2
1 + y2
2 = r2}
in R3. At any point p of C, let E1p, E2p be unit orthonormal tangent vectors to the
surface at p, E1 tangent to the ruling of the cylinder through p, and E2 tangent to the
cross-sectional circle. The outward normal is shown in the right panel of Figure 7.6.
Now, when n moves from p in the E1 direction, it stays parallel to itself just as on the
plane; hence S(E1) = 0. When n moves forward in the E2 direction it topples forward
exactly as in case of a sphere of radius r; hence S(E2) = −E2/r. In this way S describes
the half ﬂat, half-round shape of the cylinder.
7.7 Fitting Quadratic Charts and Curvatures on Surfaces
7.7.1 Gaussian and Mean Curvature
For ﬁtting curvatures, we write the local coordinate patch up to second order in the Taylor series
expansion, so that
f(u, v) = 1
2(s2,0u2 + 2s1,1uv + s0,2v2).
(7.102)
The shape operator eigenvalues and eigenvectors, its trace and determinant all have geometric
meaning of ﬁrst importance for the surface M ⊂R3. The shape operator and curvature at a point

7.7 FITTING QUADRATIC CHARTS AND CURVATURES ON SURFACES
199
p ⊂M deﬁne the surface shape locally around p. To construct the local coordinate patches around
points p ∈M deﬁne the following.
Deﬁnition 7.48
Let Tp be the tangent space of M at the point p with the orthonormal
basis of the tangent plane at p E1p, E2p ∈Tp(M), with E3p = E1p × E2p the unit normal.
The quadratic patch approximating the surface passing through the point p is
written as
x(u, v) = p + uE1p + vE2p +

(u, v)Sp
 u
v

E3p,
(7.103)
where the shape operator Sp is the symmetric 2 × 2 matrix of the form Sp =
 s20
s11
s11
s02

.
Let u = (u1, u2)∗be the 2×1 vector of expansion coefﬁcients of the unit vector tangent
to M ⊂R3 at p representing the expansion coefﬁcients expanding the tangent in the coordinate
basis E1p, E2p. Then the normal curvature in the u direction is deﬁned to be the number
κ(u) = u∗Su.
The principal curvatures κ1, κ2 of M at p are the maximum and minimum values of
the normal curvature κ(u) of M at p. The directions in which these extreme values occur are
the principal directions denoted by ⃗t1 and ⃗t2.
The maximum and minimum values of the normal curvature are the eigenvalues of the
matrix S, and the principal vectors are the corresponding eigenvectors, i.e. S⃗ti = κi⃗ti, i = 1, 2. The
sign of the normal curvature has a geometric meaning. If x is a curve in M with initial velocity
˙x(0) = u and N is the normal to the curve as shown in Figure 7.7 then if κ(u) > 0 then N(0) = U(p),
so that the surface is bending towards U(p). If κ(u) < 0 then N(0) = −U(p), so that surface is
bending away from U(p) as shown in the Figure.
Return to the cylinder example Fig 7.6. It is clear that along the ruling κ1 = 0 and κ2 < 0
occur in the direction tangent to the cross section. A point p of M ⊂R3 is umbilic provided the
normal curvature κ(u) is constant on all unit tangent vectors u at p, as is the case of a sphere where
κ1 = κ2 = −1/r.
Deﬁnition 7.49
The gaussian and mean curvature of M ⊂R3 are real-valued functions
of the eigenvalues, the determinant and arithmetic mean, respectively.
The gaussian curvature is the product of the two principal curvatures which is the
determinant given by the product of the eigevalues,
K = κ1κ2 = s02s20 −s11s11.
The mean curvature of M ⊂R3 is the trace of S given by the mean of the eigenvalues.
H = 1
2 tr S = s20 + s02
2
= (κ1 + κ2)/2.
(7.104)
The Gaussian curvature K(p) > 0 implies the principal curvatures κ1(p) and κ2(p) have the
same sign. Thus κ(u) > 0 for all tangent directions or κ(u) < 0. Thus M is either bending towards
or away in all directions. The surface locally looks like that shown in panel 2 of Figure 7.7. The
Gaussian curvature K(p) < 0, then the principal curvatures κ1(p) and κ2(p) have opposite signs.
Thus the quadratic approximation of M near p is a hyperboloid, so M also is saddle shaped near
p, as shown in panel 3 of Figure 7.7.
If the Gaussian curvature K(p) = 0 then there are two cases. (i) If one principal curvature
is zero, the shape of the surface then looks as shown in panel 4 of Figure 7.7. (ii) If both principal
curvatures are zero, the surface is planar.

200
7 MATRIX GROUP ACTIONS TRANSFORMING PATTERNS
N(0) = U(p)
p
p
U(p)
N(0)
e1
e2
p
M
K1(p) < 0
K2(p) < 0
U(p)
U(p)
e1
e2
p
K1(p) < 0
K2(p) > 0
M
e1
e2
U(p)
M
K1(p) >0
K2(p) =0
p
(1)
(2)
(3)
(4)
Figure 7.7 Panel 1 shows the positive and negative normal curvatures. Panel 2 shows the case of
K(p) > 0. Panel 3 shows the case of K(p) < 0; panel 4 shows the case K(p) = 0.
7.7.2 Second Order Quadratic Charts
Upon these triangulated graphs calculus is performed, surface areas are computed, torsions and
curvatures of various curves on the surface are generated. For this, local quadratic charts associated
withthetangentspacesaregenerated. ThesurfacemanifoldM representingthestructureofinterest
is assumed to be a smooth 2D differentiable submanifold of R3.
The problem of ﬁtting a second order local coordinate chart is to estimate the symmetric
2 × 2 matrix Sp at each point p ∈M. Based on the neighborhood Np of the point p, a minimum
mean squared error estimate can be generated [157].
Algorithm 7.50 (Hamann [157])
Let {xi ∈M ⊂R3, i = 1, . . . , M} be the vertices of the
triangulated graph M△with neighborhoods ∪M
i=1Ni. The coordinate frames and neighborhoods
Np of point p are depicted in Figure 7.8.
1. Let Tp be the tangent plane at p, with coordinate frames E1p, E2p and unit normal
E3p = E1p × E2p. Then, Tp = {x ∈R3 : ⟨E3p, (x −p)⟩R3 = 0}.
2. The orthonormal basis vectors are chosen based on the outward unit normal vector E3p.
Choose a vector v perpendicular to E3p, that is, ⟨v, E3p⟩= 0.
The orthonormal basis vectors E1p, E2p are given by E1p = v/∥v∥, E2p = E3p × E1p ,
where × is the cross product in R3. Figure 7.8 shows the neighborhood of a point p and
the three vectors E3p, E1p, E2p.
3. Let Np = {xj ∈M△, j = 1, . . . , n} be the neighborhood of the point p . For each point
xj ∈Np deﬁne hj to be the distance of the point xj to the tangent plane Tp. Deﬁne a set
of n × 1 vector of distances, H = [h1, . . . , hn]∗.
4. Let xp
j be the projection of the point xj ∈Np on to the tangent plane Tp. Let uj and vj
be the local coordinates of the points xp
j in the tangent plane given by
(uj, vj) = (⟨(xp
j −p), E1p⟩R3, ⟨(xp
j −p), E2p⟩R3).
(7.105)

7.7 FITTING QUADRATIC CHARTS AND CURVATURES ON SURFACES
201
xi
ni
xj
hj
Txi(M)
x
p
j
b2xi
b1xi
Figure 7.8 Figure
depicts
the
neighborhood
of
a
point
p
and
the
three
vectors
E3p, E1p, E2p used for ﬁtting the local quadratic charts; taken from Joshi et al. [158].
5. The minimum mean squared estimate of the matrix Sp in it is the solution of the
minimization
Sp = arg
min
{S:2×2 symmetric matrices}

xj∈Np
||xj
−(p + ujE1p + vjE2p + E3p[uj, vj]Sp[uj, vj]∗)||2
(7.106)
Identifying the symmetric 2 × 2 shape matrix Sp with a 3 × 1 column vec-
tor S =


s11
s02
s20

, and deﬁning the n × 3 matrix of the local coordinates A =


u2
1
2u1v1
v2
1
...
...
...
u2n
2unvn
v2n

, then the minimization of Eqn. 7.106 yields
ˆS = arg min
S
∥AS −H∥2 = (A∗A)−1A∗H.
7.7.3 Isosurface Algorithm
Numerous computer scientists are studying the folding and tangent structures of Biologically
related surfaces and manifolds. The basic idea behind isosurface generation in volumetric data is
to determine where and how the surface representing the image value of constancy will intersect
a given voxel. A popular approach to this problem is called the “Marching Cubes” algorithm
[159] which was later reﬁned [160] to handle ambiguities in the previous. The algorithm details
follow.
The space in which the volume lies is subdivided into a series of small cubes, each the size of
a voxel and centered at the intersection of every 2×2×2 group of voxels (thus for voxel neighbors
(1,1,1), (2,1,1), (1,2,1), …, (2,2,2) the cube center will be at (1.5,1.5,1.5)). The eight vertices of each
cube are assigned the voxel value of the volume data they lie in. By comparing each vertex of
the cube with a user-deﬁned isovalue, the vertex can be considered “in” or “out” of the surface.
There are 28 or 256 different arrangements of in/out labeled vertices that each cube can have in
this manner. For each of the 256 cases, a lookup table is consulted to see how the surface would
intersect a particular cube arrangement. This lookup table pre-deﬁnes a set of polygons that will
be located within that cube. Each of the polygons have vertices that lie at the midpoint of an edge
between “in” and “out” vertices of a cube centered at the origin. The most trivial cases — no
vertices in or all vertices in — have zero polygons that will contribute to the surface. A simple case
where 1 vertex is in and all others are out (or vice versa) results in a single triangle whose vertices
lie along each of the three edges emanating from the “in” vertex of the cube (Figure 7.9). The other
cases are more complex and contain between 1 and 30 triangles.

202
7 MATRIX GROUP ACTIONS TRANSFORMING PATTERNS
Figure 7.9 Panel 1 shows the resulting single triangle (shown as thick lines) from a cube where only
1 vertex (solid circle) is considered inside the surface. Panel 2 shows a slightly more complicated
example where 3 vertices are considered inside the surface.
For each cube the contributing polygons from the lookup table are added to the ﬁnal surface
ﬁrst by shifting polygon vertices along the edge of the cube corresponding to how close each edge
vertex value is to the isovalue (using interpolation) and then by adding a translation using the
center of the cube. Since neighboring cubes will have the same interpolated polygon vertices along
each edge this results in a continuous surface crossing the cubes.
Algorithm 7.51 (Isosurface Algorithm)
Here is the pseudocode algorithm. Deﬁne I as
volume data, S is the new (empty) surface, nx is the number of voxels in the X dimension
of I, ny is the number of voxels in the Y dimension of I. nz is the number of voxels in the
Z dimension of I. θ = isovalue threshold for surface (within range of I values); table is the
lookup table with 256 entries, each deﬁning some number of polygons (see [159, 160] for a
sample table).
for k = 1...nz-1 ; for j = 1...ny-1 ; for i = 1...nx-1
vertex[0] = I(i-1,j-1,k-1), vertex[1] = I(i,j-1, k-1), vertex[2] = I(i,j,k-1),
vertex[3] = I(i-1,j,k-1), vertex[4] = I(i-1,j-1,k), vertex[5] = I(i,j-1,k),
vertex[6] = I(i,j,k), vertex[7] = I(i-1,j,k)
index = 8 bit bitmask where bit i is 1 iff vertex[i] ≤
 (threshold), else 0
for each polygon P in table[index]
for each vertex V of P
E = edge of cube V lies on
a = cube index of first vertex in E (0..7)
b = cube index of second vertex in E (0..7)
[ by definition of table one of vertex[a] or vertex[b] will be
’in’, the other will be ’out’]
shift V along E by interpolating beween vertex[a] and vertex[b]
using 
endfor
translate P by (i-0.5,j-0.5,k-0.5)
add P to S
endfor
endfor; endfor; endfor
Example 7.52 (Human Hippocampus Surface and Macaque Cerebral Cortex)
The
folding structure of the neocortex and other subvolumes of the brain is being stud-
ied by constructing triangulated graphs representing such surface submanifolds. To
construct a triangulated graph, M△, surface contours are generated deﬁning the
boundaries of interest, which are pieced together, via isocontouring algorithms (e.g.
Marching Cubes [161]) or hand tracing in the tissue sections. Shown in Figure 7.10 are
the triangulated graphs representing the human hippocampus (top row, panel 1) and
macaque cortex (bottom row, panel 3).
Shown in Figure 7.10 are the mean curvature maps superimposed on the tri-
angulated graphs representing the hippocampus in the human (top row) and the
macaque neocortex (bottom row). The left column shows the triangulated meshes

7.7 FITTING QUADRATIC CHARTS AND CURVATURES ON SURFACES
203
(1)
(2)
(3)
(4)
Figure 7.10 Top row: Panel 1 shows the triangulated graph representing the hippocampus gener-
ated by isocontouring; panel 2 shows the mean curvature map superimposed on the hippocampus
surface of the human. Data taken from Haller et al. [162]. Bottom row: Panels 3 and 4 show the
same as above for macaque cortical surface reconstructed from cryosection data from the labo-
ratory of Dr. David Van Essen. Bright areas represent areas of high positive mean curvature; dark
areas represent areas of high negative mean curvature (see Plate 4).
representing the surfaces; the right column shows the mean curvature representing
the curving nature of the surfaces. To compute the mean curvature map, the shape
matrix Sp is estimated at each point p ∈M△on the triangulated graph. With the local
charts established at each vertex, then the mean curvature deﬁned as the mean of the
principle curvatures given by the eigenvalues of the shape matrix Sp, p ∈M△can be
computed. The mean curvature maps (right column) were generated by ﬁtting with
the least-squares Algorithm 7.50. The mean curvature is depicted through a intensity
scale. Note that the brightly lit areas correspond to areas of high curvature (valleys)
and dark regions are areas of negative curvature (ridges). Notice in the cortex the
places of deep folding.
Example 7.53 (Occipital Cortex)
The mammalian cerebral cortex has the form of a
layered, highly convoluted thin shell of grey matter surrounding white matter, with
its folding pattern a current topic of intense scientiﬁc investigation. Coordinatizing
the local gyral and sulcal submanifolds of the human brain is a signiﬁcant area of
research in the study of human shape. Figure 7.11 depicts reconstructions of the occip-
ital cortex. Panel 1 shows the medial wall of the cortex depicting the occipital cortex
(back right). Panel 2 shows the surface reconstruction (panel 2) of the occipital cortex
depicting the major sulcal and gyral principal curves generated via dynamic pro-
gramming including the inferior Calcarine sulcus and Lingual and Parietal sulci.
The heat scale color map depicts the Gaussian curvature proﬁles. Data taken from
Dr. Steven Yantis of the Johns Hopkins University.

204
7 MATRIX GROUP ACTIONS TRANSFORMING PATTERNS
(1)
(2)
Figure 7.11 Top row: Panel 1 shows the atlas depiction of the occipital cortex. Panel 2 shows
the reconstruction of the occipital cortex depicting the major sulcal and gyral principal curves
including the inferior Calcarine sulcus and Lingual and Parietal gyri. Data taken from Dr. Steven
Yantis of the Johns Hopkins University (see Plate 5).
Figure 7.12 Left column shows the medial prefrontal cortex section from the Duvernoy atlas.
Middle column top and bottom panels show the isosurface reconstruction of the prefrontal medial
cortex. Sections through the two different MRI brains show the embedded surfaces. Right column,
top and bottom panels, shows the medial prefrontal cortex reconstructions. Data taken from Dr.
Kelly Botteron of Washington University (see Plate 6).
Example 7.54 (Medial Prefrontal Cortex Reconstruction)
Figure 7.12 shows results
from the segmentation and iscontour triangulated graph construction for the medial
prefrontal cortex (MPFC) taken from Dr. Kelly Botteron of Washington University.
The left column shows the medial prefrontal cortex section from the Duvernoy atlas.
The middle column top and bottom panels show the isosurface reconstruction of
the prefrontal medial cortex. Sections through the two different MRI brains show

7.7 FITTING QUADRATIC CHARTS AND CURVATURES ON SURFACES
205
the embedded surfaces. The right column top and bottom panels shows the medial
prefrontal cortex reconstructions.
7.8 Ridge Curves and Crest Lines
Ridge curves are important features in computational vision. Intuitively ridge curves on a sur-
face are where the curvature is changing most rapidly. For a detailed description of ridge
curves see [151, 163]. Ridge curves can be deﬁned in terms of the shape operator. Begin
with the local surface representation in R3 by the equation z = f(u, v). The tangent space
of the point p(u, v) = (u, v, f(u, v)) in local coordinates is given by the two vectors over the
patch E1 = (1, 0, fu(u, v))∗, E2 = (0, 1, fu(u, v))∗. The principal curvature and directions are
the eigenvalues and eigenvectors of the shape operator S, respectively, the shape operator taking
the form S = A−1B, where
A =
 1 + f2u
fufv
fufv
1 + f2v

,
B = (1 + f2
u + f2
v )−1/2
 fuu
fuv
fvu
fvv

.
Principal curves on the surface are the curves whose tangent directions always point in
the principal direction. There are two principal curves passing through a point corresponding to
the larger and smaller eigenvalues of the shape operator at that point. Along a principal curve the
value of the principal curvature changes and at certain points has local extrema; these points are
called the ridge points, and set of such points form the collection of curves called the ridge curves.
There are two set of curves one for the larger, and one for the smaller eigenvalue. Each ridge point
can be classiﬁed according to whether the curvature is maximal or minimal. We will deal with a
special subset of the ridge curves called the crest lines, as described by [164]. Let us deﬁne an
algebraic condition for a curve to be ridge curve.
Deﬁnition 7.55
If ⃗t = (t1, t2) is a principal direction for the surface in R3 represented by
z = f(u, v), with non-zero principal curvature κ1, then the point p = (u, v, f(u, v)) is on a
ridge curve if and only if
R(u, v) =(t3
1fuuu + 3t2
1t2fuuv + 3t1t2
2fuvv + t3
2fvvv)
−3(1 + f2
u + f2
v )1/2(t2
1fuu + 2t1t2fuv + t2
2fvv)(t1fu + t2fv)κ1 = 0.
(7.107)
Crest lines are the loci of points whose maximal (in absolute value) principal curvature
is a local extremum along a corresponding principal curve, expressed as
⟨∇κm, tm⟩R3 = 0,
(7.108)
where tm is the principal direction corresponding to maximal principal curvature κm.
7.8.1 Deﬁnition of Sulcus, Gyrus, and Geodesic Curves on
Triangulated Graphs
Modern whole brain cryosection imaging provides excellent high resolution data required for
the study of such anatomical features of cortex [155, 165] such as the arrangement of the sul-
cal ﬁssures visible throughout the cortical surface of a mammalian brain with major sulci and
gyri now being cataloged in human atlases which are becoming available [166, 167]. Computa-
tional metrics deﬁned by cortical geometry such as geodesic length is drawing the attention of

206
7 MATRIX GROUP ACTIONS TRANSFORMING PATTERNS
the neuroscience community in terms of the role of wiring length in the general layout of the
nervous system [168]. Despite their anatomic and functional signiﬁcance, the gyri, sulci, and
many stable cortical pathways consistently appearing in all normal anatomies exhibit pronounced
variability in size and conﬁguration [169]. Methods are beginning to appear for characterizing
their variation [156]. The sulci and gyri exhibit strong features associated with their extrema of
bending. The deepest beds of the sulci are called the fundus beds; associated with the gyri are
the crowns. Empirical evidence [156, 166, 167] suggests that in most part of their length fundus
beds resemble crest lines (see [151, 170]) corresponding to points where the maximal absolute
principal curvature has, a local maximum. There exist algorithms for extracting ridge and crest
lines from surface geometries [151, 163, 170] deﬁned as the loci of points x ∈S where the maxi-
mal absolute principal curvature κmax(x) has a maximum. At these points, ⟨∇κmax, tmax⟩R3 = 0
for tmax the principal unit direction corresponding to the maximal principal curvature, κmax.
Such zero tracing methods are sensitive to noise. An alternate approach based on dynamic pro-
gramming for tracking optimal trajectories on surfaces has noise immunity. Instead of ﬁnding
the extremum of principal curvature using higher derivatives of curvature, we deﬁne a sequen-
tially additive energy associated with candidate curves and use dynamic programming for its
minimization. For this deﬁne the cost of a candidate curve α(s, t) to be

α(s,t) (κmax(x) −K)2 dα,
with K assigned the largest maximal curvature on the surface, and minimize over all such paths on
thetriangulatedgraphrepresentationofthesurface. Forasurfacesymmetricalaboutacrestline, i.e.
onewheretmax isperpendiculartothecrestline, minimizinggives(κmax(x)−K)⟨∇κmax, tmax⟩R3 =
0 implying ⟨∇κmax, tmax⟩R3 = 0 which is precisely the equation for the crest line. For regions of
the sulcus where the basin is ﬂat, κmax is constant, and the minimizer of the above functional
produces shortest paths through these regions.
For generating geodesics, we adapt the continuum deﬁnition associated with length on the
surface as measured by the integral of the norm of the tangent vector along the curve. If s and t
are points of a smooth connected surface M ⊂R3, the intrinsic distance ρ(s, t) from s to t in M
is the lower bound of the lengths of these curves. The curve ˆα for which the minimum length is
achieved is called a geodesic. Throughout we denote the triangulation of points on the surface as
i, j, and the set of coordinates in R3 taken by the sites of the graph and or positions of the triangle
vertices as xi, xj ∈R3.
Deﬁnition 7.56
Given a 2D triangulation of the surface {xi ∈M} deﬁne the platelet Pi of
point i as the set of triangles (with index-triples (j1, j2, j3) specifying their vertices) sharing
xi as a common vertex Pi = ∪{(j1, j2, j3)|xi = xj1 or xi = xj2 or xi = xj3}.
Deﬁne a path on the surface α(s, t) routed and terminated, respectively, in nodes s on
the surface as
α(s, t) = (s = j1, j2), (j2, j3), . . . , (jk−1, jk), . . . , (jN−1, t = jN),
such that
jk ∈Pjk−1, ∀k,
and the collection of all paths connecting (s, t) as α(s, t) ∈Ps,t(S).
Deﬁne the N-length discrete geodesic and discrete fundus bed as cost minimizing paths
given by
fundus(s, t) = arg
min
α(s,t)∈Ps,t(S)
N

k=1
dfundus(jk, jk+1) , where
(7.109)
dfundus(jk, jk+1)
=



κmax(xjk) + κmax(xjk+1)

−K2
2

∥xjk −xjk+1∥;
(7.110)

7.8 RIDGE CURVES AND CREST LINES
207
geodesic(s, t) = arg
min
α(s,t)∈Ps,t(S)
N

k=1
dgeo(jk, jk+1), where
(7.111)
dgeo(jk, jk+1) =

(x1,jk −x1,jk+1)2 + (x2,jk −x2,jk+1)2 + (x3,jk −x3,jk+1)2.
Notice the cost of a discrete curve α(s) connecting (s, t) is deﬁned by assuming piecewise constant
function between successive nodes for the curvature integral

α(s,t) (κmax(x) −K)2 dx.
7.8.2 Dynamic Programming
Examine an approach based on dynamic programming for tracking optimal trajectories on surfaces
similar to that done in boundary and artery tracking [171, 172]. We search for curves that pass
through regions of highest maximal curvature joining the prespeciﬁed start and end points in
the surface. For generating these curves and geodesics we follow Khaneja et al. [153, 154] using
dynamic programming adapted to optimization on triangulated surfaces. Denote the ﬁnite state
space S of size ∥S∥= N; on these triangulated graphs the positions of the nodes of the surface itself
are the states. The goal is to compute optimal shortest paths between the speciﬁed initial states s
and the ﬁnal state t. Assuming that the optimal path has no more than K nodes, the total number
of paths of length K between points s and t are of the order NK. If the cost is additive over the
length of the path, dynamic programming reduces the complexity of the search algorithm to order
of KN2. Let ck(xk, xk+1) denote the cost incurred for the transition from state xk ∈S to xk+1 ∈S at
each time k. Suppression of k dependence in c(i, j) means the cost is independent of time. We shall
assume that c(i, j) ≥0, and arcs of inﬁnite cost c(i, j) = ∞signify that there is no arc from node i
to node j. An optimal path need not have more than N arcs ( number of nodes in the graph) and
hence take no more than N moves. We formulate the problem as one of ﬁnding the optimal path
in exactly N moves allowing degenerate moves from a node i to itself with cost c(i, i) = 0. The
degenerate moves signify that the length of the path may be less than N.
The efﬁciency of dynamic programming on the triangulated graphs representing the surface
is that the states spaces Sk ⊂S can be dynamically deﬁned and of reduced complexity. Curves
passing through a point on the graph must pass through one of it neighbors (analogos to being in
the tangent space for the continuum representation).
Algorithm 7.57 (Dynamic Programming Algorithm)
Denote the optimal cost for get-
ting from node i to node t in (N −k) moves as Jk(i), i ∈S, k = 0, 1, . . . , N −1. Then the
optimal N-length path J0(i) from i to t is given by the ﬁnal step of the following algorithm,
with JN−1(i) = cN−1(i, t),
and
(7.112)
Jk(i) =
min
j=1,...,N {ck(i, j) + Jk+1(j)},
k = 0, 1, . . . , N −2,
i ∈M.
(7.113)
Deﬁne the state spaces dynamically SN−1 = {i|i ∈Pt}, Sk = {i|i ∈Pj, j ∈Sk+1},
and implement the algorithm according to Initialize: Jk(i) ←∞i ̸= t, for all k, SN ←t,
Jk(t) ←0;
For k ←N −1 down to 0 do
Sk ←{i | i ∈Pj, j ∈Sk+1},
set ck(i, j), j ∈Sk+1, i ∈Sk,
(7.114)

208
7 MATRIX GROUP ACTIONS TRANSFORMING PATTERNS
with JN−1(i) = cN−1(i, t), i ∈SN−1,
(7.115)
Jk(i) =
min
j∈{Sk+1
# Pi}
{ck(i, j) + Jk+1(j)},
i ∈Sk,
k = 0, 1, . . . , N −2.
(7.116)
Theorem 7.58
1. Geodesic generation. Given the costs for transition
ck(i, j) = dgeo(i, j),
j ∈Pi,
ck(i, j) = ∞for j ̸∈Pi,
then the length J0(s) in the algorithm is a a geodesic (not necessarily unique) between
nodes s and t:
geodesic(s, t) = arg
min
α(s,t)∈Ps,t(S)

k
dgeo(k, k + 1).
2. Fundus curve generation. Given the costs for transition
ck(i, j) = dfundus(i, j),
j ∈Pi,
ck(i, j) = ∞,
j ̸∈Pi,
then the length J0(s) in the algorithm is an optimal principal curve from nodes s to t:
ˆα(s, t) = arg
min
α(s,t)∈Ps,t(S)

k
dfundus(k, k + 1).
Proof
The original proof was provided by Khaneja et al. [154] which follows since the
costs are additive over the curves. For k = N −1, JN−1(i) = c(i, t), i = 1, 2, . . . , N, with
c(i, t) = ∞for i ̸∈Pt, implying that there is no one arc path from i to t. Therefore the
onlyvaluesofJ thatgetupdatedintheﬁrststepareforSN−1 = {i ∈Pt}. Writingthekth
step in the original algorithm Jk(i) = minj=1,...,N {c(i, j) + Jk+1(j)}, k = 0, 1, . . . , N −2,
observe that by deﬁnition Sk consists of all j from which the terminal node t can be
reached via a curve with less than or equal to N −k arcs or N −k moves (degenerate
moves allowed). Therefore if j ̸∈Sk+1 then Jk+1(j) = ∞, and as in previous case if
i ̸∈Pj, c(i, j) = ∞, in either case Jk(i) = ∞. Hence only i ∈Sk need be considered
Jk(i) = minj∈{Sk+1
# Pi}{ck(i, j) + Jk+1(j)}, i ∈Sk, k = 0, 1, . . . , N −1.
By using the Riemannian length function for the minimizing cost then the shortest length
geodesic paths on the surface can be generated. Shown in Figure 7.13 are examples of such shortest
length paths on the macaque cortex. The bottom row of Figure 7.13 depicts results on the opti-
mality of DP on the superior temporal gyrus (STG) and also shows a reconstruction of the STG.
Panels 4–7 show the DP generation of principal curves on the superior temporal gyrus of the Vis-
ible Human. Panel 4 shows various cortical gyri depicted in different colors by Van Essen in his
reconstruction of the Visible Human cryosection. Panel 5 shows the temporal gyrus extracted from
the Visible Human in R3 with dynamic programming generated fundus curves. Panel 6 shows
principal curves choosing multiple start and end points for the dynamic programming solution
ﬁnding common ﬂows on the surface. The paths are illustrated on the planar representation of the
STG illustrating robustness of the solution. The DP algorithm was run for multiple starting and
end points. Notice how the trajectories initiated at different starting points merge into a common
trajectory, illustrating the optimality of the path. Panel 7 shows the dynamic programming gener-
ation of the superior temporal sulcus jumping across the break connecting the start and end points
which were manually selected.
Example 7.59 (Cutting Surfaces with Dynamic Programming)
Figure 7.14 shows a
depiction of the surface of the superior temporal gyrus and generation of its bound-
aries via dynamic programming. The triangulated graphs represent the gray/white
boundary of the STG in MRI data. Panels 1 and 2 show the whole brain and STG

7.8 RIDGE CURVES AND CREST LINES
209
1
2
3
4
5
6
1
0
47.07
63.62
68.75
39.29
22.39
2
47.07
0
70.75
61.85
70.76
63.62
3
63.62
70.75
0
19.15
36.1
55.88
4
68.25
61.85
19.15
0
49.0
66.30
5
31.29
70.76
36.1
49.0
0
19.77
6
22.39
63.62
55.88
66.30
19.77
0
1
4
5
6
2
3
(1)
(4)
(5)
(6)
(7)
(2)
(3)
Figure 7.13 Top row: Panel 1 shows eight geodesics generated on the neocortex by picking the
start and end points manually. Panel 2 depicts geographical landmarks on the macaque cortex;
labels 1, 2, 3, 4, 5, 6. Panel 3 shows a table of Riemannian distances in millimeters between the
predeﬁned points. Data taken from the laboratory of David Van Essen, Washington University.
Bottom row: Figure shows optimality of dynamic programming. Panel 4 shows the Visible Human
cortex extracted by David Van Essen; panel 5 shows choosing multiple terminal points for the DP
solution; panels 6, 7 show the DP generation of the superior temporal sulcus jumping across the
break connecting the start and end points which were manually selected (see Plate 7).
(1)
(2)
(3)
(4)
(5)
(6)
Figure 7.14 Top row panel 1 shows the external view of the Superior Temporal Gyrus (STG). Panel
2 shows Heschl’s gyrus and the posterior boundary of the plenum temporale (PT) deﬁned via
dynamic programming. Panel 3 shows the delineation of the STG surface into two with the PT as
the blue region which is extracted. Bottom row shows the application of dynamic programming
to extract the PT from STG surface. Panel 4 tracks Heschl’s gyrus; panel 5 tracks the STG as far as
the posterior ascending (or descending) ramus; panel 6 tracks the geodesic from the end of the
STG to the retro-insular end of the Heschl’s gyrus. Data taken from the laboratory of Drs. Godfrey
Pearlson and Patrick Barta, reconstructions from Dr. Tilak Ratnanather (see Plate 8).

210
7 MATRIX GROUP ACTIONS TRANSFORMING PATTERNS
reconstructions. To extract the planum temporale (PT) from the reconstructed STG
surface, we use dynamic programming to track the gyral and sulcal principal curves
deﬁning the boundaries of the submanifold graphs. The PT is delineated by the HG
and the STG as far as at the start of either the posterior ascending ramus or posterior
descending ramus. The posterior boundary of PT is deﬁned by the geodesic from the
end of the STG and the retro-insula point of the HG. Panel 3 shows the delineation of
the STG surface into two with the PT as the blue region that is extracted. The bottom
row shows the application of dynamic programming to extract the PT from STG sur-
face. Panel 4 tracks Heschl’s gyrus with panel 5 tracking the STG as far as the posterior
ascending (or descending) ramus. Panel 6 tracks the geodesic from the end of the STG
to the retro-insular end of the Heschl’s gyrus.
7.9 Bijections and Smooth Mappings for Coordinatizing
Manifolds via Local Coordinates
Various investigators have been studying methods for mapping the surface manifold to spherical
and planar coordinates [166–168, 173–177]. The approach being taken is to deﬁne local diffeomor-
phisms between the submanifolds and their locally Euclidean representation. To construct the local
diffoemorphisms φ : S →D ⊂R2 between the 2D manifold S ⊂R3 of the cortical surface to the
plane we describe methods based on quasi-conformal ﬂat mapping which is a computational pro-
cedure for implementing the Riemann mapping theorem developed [178, 179]. The computational
approach for generating φ relies on the “ﬂattening” of the triangulated graphs representation S(△)
of the original surface S. Once a “ﬂattened graph” has been generated then the diffeomorphism
is associated with the bijection between the vertices in the original triangulate surface and the
discrete points in D ⊂R2. The quasi-conformal ﬂat mapping is computed via the circle packing
algorithms developed on triangulated graphs, which control angular distortion [180, 181] .
Let the triangulated graph be denoted by S(△) = (N, V) where N is the neighborhood
structure or connectivity array of the graph vertices and V is the set of coordinates of the graph
vertices. The goal is to generate an equivalent graph S′ = (N, V′) such that the curvature at each
vertex of V′ is zero, i.e. a ﬂattened graph with a one-to-one correspondence between vertices of V
and V′. This is achieved by circle packing for S′ in which the vertices are the center of circles that
are tangent to each other. Circle packing [180] is based on the Riemann Mapping Theorem [181]
in which the angular measure between the edges is preserved.
Let |V| be the number of vertices in S and R = {rv, v = 1, . . . , |V|} be the set of radii of circles
centered at the vertices. Suppose the vertex v has k faces (i.e. triangles) with associated vertices
Fv = {v : v1, . . . , vk+1 s.t. vk+1 = v1}, then the angle sum at v is
θ(v; R) =

⟨v,u,w⟩:u,w∈Fv
α(rv; ru, rw),
where the triple ⟨v, u, w⟩, denotes the face with vertices v, u, w, and α is the angle of the face at v
subtended by the edge joining u and w readily computed via the cosine rule
α(rv; ru, rw) = arccos

(rv + rw)2 + (rv + ru)2 −(ru + rw)2
2(rv + rw)(rv + ru)

.
The angle sum at the vertex is a measure of the curvature that is concentrated at the vertex.
Thus for ﬂatness, it is required that θ(v; R) = 2π. If θ(v; R) < 2π, curvature at v is positive like

7.9 BIJECTIONS AND SMOOTH MAPPINGS
211
Figure 7.15 Panel 1 (Panel 3) show the radius of circle at vertex with positive (negative) curvature
must be reduced (increased) to ensure that circles at all vertices are tangent to each other as shown
in Panel 2. Figure from Ratnanather et al. (2003).
Figure 7.16 Left column: Top two panels show automatically and hand generated PT surfaces
from one MRI brain; bottom two panels show a second PT generated automatically and by hand
contouring. Right column shows the surface of the left PT from the STG shown superimposed
with the mean-curvature map drawn over the planar coordinates at the location deﬁned by the
bijection φ : S(△) →D. The retro-insular end of the HG and the positive y-axis passes through the
posterior STG where the ramus begins. Also superimposed is the Heschl’s sulcus in blue generated
by dynamic programming tracking on the original surface. Data taken from the laboratory of Drs.
Godfrey Pearlson and Patrick Barta; reconstructions from Dr. Tilak Ratnanather (see Plate 9).
a cone point alongwith a gap between circles around v as in panel 1 of Figure 7.15 which is rectiﬁed
by reducing the radii of the circles around v until tangency is attained as in panel 2 of Figure 7.15.
If θ(v; R) > 2π, curvature at v is negative like a saddle point with overlapping circles as in panel 3
of Figure 7.15 which is rectiﬁed by expanding the radii of the circles around v until all circles are
tangent. For our computational procedure we use the methods of Hurdal et al. [178,182].
Thus conﬁguration S′ = (N, V′) is achieved by iteratively computing radii of the circles at
interiorvertices, withﬁxedradiusattheboundaryverticesequaltotheaverageofhalfofthelengths
of the two edges of the vertex. In practice, instead of repeated reﬁnement of triangulation giving
zero angular distortion, angular distortion is minimized. The iteration is initialized by assigning
arbitrary radii at interior vertices and convergence is accelerated based on the algorithm developed
by Collins and Stephenson [180]. The result is an Euclidean ﬂat map which preserves Euclidean

212
7 MATRIX GROUP ACTIONS TRANSFORMING PATTERNS
(1)
(3)
(4)
(5)
(2)
Figure 7.17 Top row: Panel 1 shows the reconstruction of the cortical surface with the curvature
map superimposed. Shown depicted are various sulcal principal curves generated via dynamic
programming. Panel 2 shows the planar representation of the medial cortex. Bottom row: Panel 3
shows the reconstruction of the left and right MPFC from Dr. Kelly Botteron. Panels 4 and 5 show
the planar maps of the MPFC reconstructions superimposed curvature proﬁles. Data taken from
Dr. Kelly Botteron of Washington University (see Plate 10).
lengths on the edges. The quasi-conformal ﬂat map is normalized by assigning a landmark to an
origin and another vertically above the origin.
Example 7.60 (Planum Temporale in the Temporal Gyrus)
Figure 7.16 depicts such
results on the planum temporal in the superior temporal gyrus from Ratnanather
et al. [183]. The left column depicts automatically (panels 1 and 3) and hand contour
generated (panels 2 and 4) PT surfaces from two MRI brains. The middle column
shows the left PT surface from the STG with the superimposed mean-curvature map.
The blue line depicts Heschl’s sulcus in blue generated by dynamic programming.
The right column shows the same curvature map drawn over the planar coordinates
at the location deﬁned by the diffeomorphism φ : S(△) →D ⊂R2. The retro-insular
end of the HG and the positive y-axis passes through the posterior STG where the
ramus begins. Also superimposed is the Heschl’s sulcus in blue generated by dynamic
programming tracking on the original surface.

7.9 BIJECTIONS AND SMOOTH MAPPINGS
213
The quasi-conformal mapping algorithm of Hurdal permits the deﬁnition of a
coordinate system on the PT manifold. The origin must be unambiguously present.
Thus the retro-insular end of the HG where the HG meets the SF is deﬁned as the
origin. The y-axis is aligned such that it passes through the point on the posterior STG
where the ramus begins.
Example 7.61 (Medial Prefrontal Cortex Reconstruction)
Shown in Figure 7.17 are
the global reconsructions of the Medial Prefrontal cortex from Dr. Kelly Botteron.
Shown in the top row panel 1 is the reconstructions of the MPFC with superimposed
heat scale color map showing the Gaussian curvature proﬁles superimposed on the
medial walls deﬁning the MPFC. Panel 2 shows the same curvature maps shown
mapped to planar coordinates D ⊂R2 via the diffeomorphisms φ : S →D generated
via the circle packing algorithm of Hurdal applied to the left and right MPFC. Shown
superimposed are dynamic programming generated contours depicting the various
sulcal principle curves. The color map depicts the Gaussian curvature proﬁles of the
MPFC. Shown in panels 4 and 5 are the curvature maps shown mapped to planar
coordinates D ⊂R2 via the diffeomorphisms φ : S →D generated via the circle pack-
ing algorithm applied to the left and right MPFC. Shown superimposed are dynamic
programming generated principal curves.

8
M A N I F O L D S , A C T I V E M O D E L S , A N D
D E F O R M A B L E T E M P L AT E S
ABSTRACT
To study shape we introduce manifolds and submanifolds examined in the contin-
uum as the generators. Transformations are constructed which are built from the matrix
groups and inﬁnite products. This gives rise to many of the widely used structural models
in image analysis often termed active models, essentially the deformable templates. These
deformations are studied as both diffeomorphisms as well as immersions. A calculus is intro-
duced based on transport theory for activating these deformable shapes by taking variations
with respect to the matrix groups parameterizing them. Segmentation based on activating
these manifolds is examined based on Gaussian random ﬁelds and variations with respect to
the parameterizations.
8.1 Manifolds as Generators, Tangent Spaces, and Vector Fields
From the previous two Chapters 6, 7 it is clear that for studying shape the generators are often the
structured sets of line segments, curves, quadrics, subvolumes, to name some. Thus the quantitive
study of shape and variability behoves us to study quantitatively manifolds in Rn, and their
transformation.
8.1.1 Manifolds
Roughly speaking, an n-dimensional smooth manifold will locally look like Rn with the property
that it has local coordinates which overlap smoothly.
Deﬁnition 8.1
A manifold M of dimension m is a set with a set of open sets (topology)19
which is locally Euclidean of dimension m, i.e. for each p ∈M, there exists a smooth local
coordinate chart which is a diffeomorphism φ = (φ1, . . . , φm) : O →D ⊂Rm (a 1-1, onto
bijection with both φ, φ−1 continuous and differentiable) from an open neighborhood O ⊂M
containing p to an open set D ⊂Rm.
A manifold is called a C∞smooth manifold if there exists a family of coordinate charts
(Oα, φα) covering the manifold, pairwise compatible, so that if Oα
# Oβ nonempty implies
that the maps φα ◦φ−1
β
: φβ(Oα
# Oβ) →φα(Oα
# Oβ) giving the change of smooth
coordinates.
This is depicted in the following equation:
p ∈O ⊂M
(8.1)
⇓φ
(8.2)
(x1, . . . , xm) = φ(p) ∈D ⊂Rm
(8.3)
19 Atopology T on a set X is a collection of open subsets of X satisfying the following axioms: unions of open
sets are open, ﬁnite intersections are open, and X, ∅are both open (closed). See Boothby [148] for additional
properties of Hausdorff and countability of basis in the deﬁnition of the manifold. We will take these as given.
214

8.1 MANIFOLDS AS GENERATORS, TANGENT SPACES, AND VECTOR FIELDS
215
Example 8.2 (The Circle)
The circle S1 is M = {(x, y) ∈R2 : x2 + y2 = 1}, choose for
the point p = (1, 0) the chart O ⊂N, φ with O the right half circle given by the open
set {(x, y) ∈R2 : x > 0} ∩S1, φ : O →D ⊂R, φ−1 : D →O ⊂N to be
φ−1(x) = (cos x, sin x)
x ∈D =

−π
2 , π
2

,
φ(p1, p2) = arcsin(p2)
(p1, p2) ∈O = φ−1
−π
2 , π
2

.
Example 8.3 (Linear Manifolds and Equality Constraints)
Linear
manifolds
are
well known through equality constraints in vector spaces. Let X = Rn, and M ⊂X
deﬁned by
M =


y ∈Rn :

i
αiyi = 0


.
(8.4)
Then M is an m = n −1 dimensional manifold with coordinate chart (O = M, φ) with
φ : M →Rn−1 given by
φ−1 : (x1, . . . , xn−1) ∈Rn−1 →
n−1

i=1
xibi
(8.5)
where {bi ∈Rn, i = 1, . . . , n −1} span the orthogonal complement of the 1D
vector space (α1, . . . , αn).
Assuming the bi’s are orthogonal,
then φ(y)
=
(⟨b1, y⟩Rn, . . . , ⟨bn−1, y⟩Rn) ∈Rn−1 for all y ∈M.
8.1.2 Tangent Spaces
Roughly speaking, a tangent vector to a manifold M at point p ∈M will correspond to tangents to
curves on the manifold through the point p. The collection of all tangents generated from tangent
curves on M through p will be denoted Tp(M) the tangent space which will be a vector space.
Figure 8.1 shows an example of a 2-D manifold M with an associated curve.
Tangent vectors to M at p ∈M, and the tangent vector space Tp(M) is deﬁned as the vector
space generated from a basis {Ei, i = 1, . . . , m} of the tangent space.
Deﬁnition 8.4
Deﬁne the local coordinate frames for M = Rm to be Ei = ∂/∂xi,
i = 1, . . . , m, with action on smooth functions
Eif = ∂f
∂xi
,
i = 1, . . . , m.
(8.6)
M
p
v
Figure 8.1 Panel shows a curved submanifold M ⊂R3 with tangent plane Tp(M) at point
p ∈M.

216
8 MANIFOLDS, ACTIVE MODELS, AND DEFORMABLE TEMPLATES
For manifold M curved with local coordinates (O ⊂M, φ) containing p, then deﬁne the
m-local coordinate frames at p ∈M as Eip, i = 1, . . . , m taking action
Eipf =
∂
∂xi
(f ◦φ−1)
x=φ(p)
,
i = 1, . . . , m.
(8.7)
The coordinate frames form a basis for the tangent space Tp(M) of M at p which is a
vector space given by the set of all vectors generated from the basis:
Vp =
m

i=1
αiEip.
(8.8)
The most important properties of the basis are that they are an m-dimensional span, and
they satisfy the Liebnitz rule of ordinary calculus. 20
Theorem 8.5
The coordinate frames E1p, . . . , Emp of Tp(M) in Deﬁnition 8.4 satisfy (i)
independence implying dimTp(M) = m, and (ii) Liebnitz rule
Ei(fg) = gEif + fEig.
(8.9)
Proof
To see that the Eip, i = 1, . . . m are independent assume that they are not. This
would imply that for some i ̸= j, then for all f ∈C∞(p) Eipf = 
j̸=i αjEjpf, and choose
f = φi the ith local coordinate function of φ = (φ1, . . . , φm) implying
Eipf =
∂
∂xi
(φi ◦φ−1)
φ(p)
=
∂
∂xi
xi = 1
(8.10)
=

j̸=i
αjEjpf =

j̸=i
αj
∂
∂xj
(φi ◦φ−1)

φ(p)
= 0.
(8.11)
Contradiction, so the Eip must be independent. For Liebnitz rule,
Eip(fg) =
∂
∂xi
(fg) ◦φ−1 =
∂
∂xi

f ◦φ−1 
g ◦φ−1
= g ◦φ−1φ(p)
∂
∂xi
f ◦φ−1
φ(p)
+ f ◦φ−1φ(p)
∂
∂xi
g ◦φ−1
φ(p)
= gEipf + fEipg.
(8.12)
Example 8.6
Of course, then any Vp ∈Tp(M) can be written Vp = 
m
i=1 αiEip, with
the αi calculated by operating the tangent vector on the ith local coordinate function
φi of φ = (φ1, . . . , φm) according to
Vpφi =
m

j=1
αjEjpφi =
m

j=1
αj
∂
∂xj
(φi ◦φ−1)

φ(p)
=
m

j=1
αj
∂
∂xj
xi

φ(p)
= αi.
(8.13)
Example 8.7 (Directional Derivatives)
These are familiar as directional derivatives
usedallthetimeinoptimizationviaLagrangemultiplierstoconstraintheoptimization
20 A standard way to deﬁne the tangent space of M at p abstractly (as in Boothby [148]) is to deﬁne it as the
space all tangent vectors Vp at p ∈M is a map Vp : C∞(p) →R satisfying linearity and Liebnitz rule, so that
for all f, g smooth, then (i) Vp(af + bg) = aVpf + bVpg , a, b ∈R, and (ii) Vp(fg) = f(p)Vp + g(p)Vpf.

8.1 MANIFOLDS AS GENERATORS, TANGENT SPACES, AND VECTOR FIELDS
217
to a constraint space (manifold). Let M = Rm, with tangent space Rm itself so that
φ = id the identity map, and the global coordinate frames (independent of p) become
Eip = ∂/∂xi , i = 1, . . . , m. Then for Vp = 
m
i=1 αi(∂/∂xi) and Vpf is just the directional
derivative in the direction of α:
Vpf = ⟨α, ∇f⟩Rm.
(8.14)
Note that the basis is independent of p, the position at which the tangent space
Tp(Rm) is attached: the special property of Euclidean spaces.
Example 8.8 ( Circle and Sphere)
Return to Example 8.2. The natural coordinate
frame E1p becomes E1pf = (∂/∂x)f ◦φ−1 giving
E1pf =
∂
∂xf ◦φ−1
φ(p)
=
∂f
∂y1
d cos x
dx

arcsin p2
+ ∂f
∂y2
d sin x
dx
arcsin p2
.
(8.15)
The coordinate frame becomes Ep = −p2(∂/∂y1) + p1(∂/∂y2) which is a globally
deﬁned, dimension 1 vector ﬁeld on all of S1.
For surfaces, then for each point p ∈M there exists a local coordinate system
φ−1 : x ∈R2 →M given by
φ−1(x1, x2) = (φ−1
1 (x1, x2), φ−1
2 (x1, x2), φ−1
3 (x1, x2)).
(8.16)
For the 2-sphere S2 = {(y1, y2, y3) : y2
1 + y2
2 + y2
3 = 1}, use the azimuth-elevation
coordinates
φ−1(x1, x2) = (cos x1 sin x2, cos x1 cos x2, sin x1).
The two coordinate frames applied to smooth functions Eipf = (∂/∂xi)f ◦φ−1φ(p) are
E1p = −sin x1 sin x2
∂
∂y1
−sin x1 cos x2
∂
∂y2
+ cos x1
∂
∂y3
,
= −
p3p1

1 −p2
3
∂
∂y1
−
p3p2

1 −p2
3
∂
∂y2
+

1 −p2
3
∂
∂y3
,
(8.17)
E2p = cos x1 cos x2
∂
∂y1
−cos x1 sin x2
∂
∂y2
=
p2

1 −p2
3
∂
∂y1
−
p1

1 −p2
3
∂
∂y2
.
(8.18)
Notice, this is not a globally deﬁned basis of dimension 2; at x1 = π/2, E2p = 0.
8.1.3 Vector Fields on M
Associating to every point of the manifold a vector which varies smoothly across the manifold is
a vector ﬁeld.
Deﬁnition 8.9
A smooth vector ﬁeld V on M assigns to each p ∈M a tangent vector
Vp ∈Tp(M) with components in the coordinate frames which are smooth.

218
8 MANIFOLDS, ACTIVE MODELS, AND DEFORMABLE TEMPLATES
The coordinate frames being Eip, then the function Vpf = 
m
i=1 αi(p)Eipf is a smooth
function.
Vector ﬁelds will be important for us in their role in deﬁning a basis for the tangent spaces.
Aset of m-vector ﬁelds on a manifold of dimension m which are linearly independent at every point
in the manifold form a basis for the tangent spaces at each point of the manifold. In general it is not
possible to ﬁnd a set of m independent vector ﬁelds on an arbitrary manifold. For Rm it is straight-
forward, ∂/∂xi, i = 1, . . . , m are such an example; for matrix groups it will be possible as well.
This property is so signiﬁcant that it is given a name.
Deﬁnition 8.10
A manifold M of dimension m with the property that there exists a full set
of m coordinate frames is called parallelizable.
8.1.4 Curves and the Tangent Space
The tangent space can be deﬁned via the curves as the set of all tangent vectors of curves x(·) on
M. Depicted in Figure 8.1 is an example of a curve with tangent vector vx(t)=p.
Deﬁnition 8.11
A curve on the manifold M of dimension m is a smooth mapping from an
open real interval to the manifold M according to x : D ⊂R1 →M.
The tangents to these curves will be elements of the tangent space of the manifold.
Theorem 8.12
Given the curve t ∈M on the manifold M rooted in p ∈M so that
0(p) = p, then the vectors Vt0(p) for p ∈M deﬁned by tangents to curves through the
manifold
Vt0(p)f = d
dtf ◦t(p)
t0
,
(8.19)
are elements of the tangent space of the manifold Vt0(p) ∈Tt0(p)(M).
Proof
Applying the deﬁnition assuming the coordinate frames Eit0(p) spanning the
tangent space at t0(p) gives
d
dtf ◦t
t0
= d
dtf ◦φ−1 ◦φ ◦t
t0
=
m

i=1
∂f ◦φ−1
∂xi
φ◦t0(p)
˙xi(t0)
(8.20)
=
m

i=1
˙xi(t0)Eit0(p)f,
(8.21)
with ˙x = (d/dt) φ ◦ the derivative of the curve in local coordinates.
Example 8.13 (Translation)
For illustration examine the simple version of transla-
tion in the plane, then t : p ∈Rn →p + at ∈Rn; then
d
dtf ◦t|p = d
dtf(p1 + a1t, . . . , pn + ant) =
n

i=1
∂f
∂xi
|t(p)( ˙t(p))i,
(8.22)
with Vt0(p) =
n

i=1
ai
∂
∂xi
.
(8.23)
Notice that it is independent of p ∈Rn.

8.2 SMOOTH MAPPINGS, THE JACOBIAN, AND DIFFEOMORPHISMS
219
8.2 Smooth Mappings, the Jacobian, and Diffeomorphisms
We have already started looking at shapes corresponding to generators which are sub-manifolds
sewn together; the tranformations were built from groups acting locally on submanifolds of the
whole. This is our general construction of deformable templates and active models. The graph
based regularity on the transformations constrains the local action of these transformations so that
the total manifold stays regular (connected) (see Chapter 7, Section 7.3). In the continuum this
corresponds to smoothness of the mapping, and local bijective properties. For computation, the
number of submanifolds (generators) are chosen to be ﬁnite; to understand them analytically we
now study these transformations as smooth mappings on the continuum in which the number of
generating manifolds goes to inﬁnity as well as the groups acting on them. This takes us into the
study of homeomorphisms and diffeomorphisms.
8.2.1 Smooth Mappings and the Jacobian
The basic model to be studied is pictured in Figure 8.2 below in which manifolds are studied via
mappings of one to the other, F : M ⊂Rm →N ⊂Rn with local coordinates (φ, M), (ψ, N).
p ∈M
F
=⇒
F(p) ∈N
(8.24)
⇓φ
⇓ψ
(8.25)
φ(p) ∈Rm
ψ ◦F(p) ∈Rn
(8.26)
Figure 8.2 The basic model of transforming manifold M with local coordinates φ under smooth
mapping F : M →N with local coordinates ψ.
Deﬁnition 8.14
Let F : M →N with coordinate neighborhoods (O, φ) and (P, ψ)
with F(O) ⊂P. Then F in local coordinates, denoted ˆF : φ(O) →ψ(P) is
ˆF(p) = (y1, . . . , yn) = ψ ◦F ◦φ−1|φ(p)=(x1,...,xm) .
(8.27)
There are various properties to be understood corresponding to smoothness, continuity, and
bijective properties which we shall examine again below more carefully.
Deﬁnition 8.15
A mapping F : M →N is Ck smooth if for each p ∈M there exists local
coordinate (O, φ) of M and (P, ψ) of N with p ∈O, F(p) ∈P such that ˆF is Ck. For X1, X2
are topological spaces, with F : X1 →X2, then various properties of F are as follows:
(i) F is continuous if the inverse image of every open set in X2 is an open set in X1;
(ii) F is an open mapping if F takes open sets in X1 to open sets in X2;
(iii) F is a homeomorphism if F and F−1 are both 1-1 and onto (bijections), continuous
and open;
(iv) F is a smooth Ck map from Rn to Rm if F : x ∈Rn →F(x) = (F1(x), . . . , Fm(x)) ∈
Rm has each component Ck, k-times continuously differentiable;
(v) a smooth Ck mapping F : Rn →Rm is a diffeomorphism iff it is a homeomorphism
(1-1 and onto and continuous) with F−1 smooth Ck.

220
8 MANIFOLDS, ACTIVE MODELS, AND DEFORMABLE TEMPLATES
Note that F, a homeomorphism, implies that F, F−1 are both open mappings. To see that F−1
is open, for example, take an open set O2 ∈T2, since F is continuous, F−1(O2) is open, thus F−1
is an open mapping. The identical argument shows that F is open.
The Ck-differentiability of each component is deﬁned in the standard iterative way from
vector calculus. Let F : O ⊂Rm →R be in Ck and let DkF represent the m × m × · · · × m (k times)
vector of its kth partial derivatives. Then F ∈Ck+1 on O if and only if for each component of
that vector, say DkFi, there exists an m-vector of functions, Ai and an m-tuple Ri(x, a) of functions
deﬁned on O × O such that ∥Ri(x, a)∥→0 as x →a and for each x ∈O we have
DkFi(x) = DkFi(a) + Ai(x −a) + ∥x −a∥Ri(x, a).
Then F : Rm →Rn is smooth if each component is smooth. Clearly the Jacobian of the transfor-
mation will play an important role in determining at least local 1-1 and onto characteristics of the
maps. For this the rank of the mapping as deﬁned through the Jacobian in local coordinates is
important.
Deﬁnition 8.16
The rank of F at p is deﬁned to be the rank of ˆF = (y1, . . . , yn) at φ(p) =
(x1, . . . , xm) of the Jacobian matrix of the transformation in local coordinates:
DˆF(x1, . . . , xm) =


∂y1
∂x1
. . .
∂y1
∂xm
...
...
...
∂yn
∂x1
. . .
∂yn
∂xm


.
(8.28)
We will be interested in understanding how the tangent spaces transform under smooth
mappings F : M →N, M, N smooth manifolds of dimension m, n. For this we deﬁne the differential
of the map F.
Deﬁnition 8.17
Let M, N be smooth manifolds with F : M →N a smooth mapping. Then
the differential of F at p ∈M is the linear map F⋆: Tp(M) →TF(p)(N) deﬁned as, for all
Vp ∈Tp(M) and f smooth,
F⋆(Vp)f = Vp(f ◦F).
(8.29)
Let us in fact verify that the map F⋆is a linear map into the tangent space TF(p). Assume
coordinate charts (O, φ) and (P, ψ) on M, N, with p ∈O, F(p) ∈P. For this assume coordinate
frames
˜EjF(p) = ψ−1
⋆
∂
∂yj
,
j = 1, . . . , n.
(8.30)
Let us establish that F⋆Eip is an element of the tangent space TF(p)(N). Choosing f ∈C∞(F(p)),
then by deﬁnition
F⋆Eipf = Eipf ◦F =
∂
∂xi
f ◦ψ−1 ◦ψ ◦F ◦φ−1
φ(p)
=
n

j=1
∂
∂yj
(f ◦ψ−1)
ψ◦F(p)
∂yj
∂xi
=
n

j=1
∂yj
∂xi
˜EjF(p)f.
The Jacobian matrix in local coordinates is ψ ◦F ◦φ−1; the matrix determining the basis
transformation of the linear transformation from F⋆: Tp(M) →TF(p)(N) is the Jacobian matrix.
Example 8.18 (Matrix Group Action)
Let A =

aij

∈GL(n) : x →Ax, then
∂
∂xi
f ◦(Ax) =
n

j=1
∂f
∂yj
(Ax)
∂(Ax)j
∂xi
=
n

j=1
aij
∂f
∂yj
(Ax).
(8.31)

8.2 SMOOTH MAPPINGS, THE JACOBIAN, AND DIFFEOMORPHISMS
221
8.2.2 The Jacobian and Local Diffeomorphic Properties
The Jacobian provides the necessary condition (not sufﬁcient) for F : M →N to be a diffeomorphism
from M →N. This is illustrated via the following standard example for mappings between
ﬁnite dimensional vector spaces; the rank of the Jacobian matrix must equal the dimension of the
manifolds being mapped, i.e. dim M = dim N = rankF. Let F : M = Rm →N ⊂Rn be a linear
mapping via matrix multiplication of the vector spaces, F an n × m matrix: Fx = y, x ∈Rm, y ∈Rn.
Then, if dim N ̸= dim M either the forward or reverse mapping cannot be onto all of Rn (hence
not invertible). Examine dim M = dim N = m, with F an m × m matrix. Since F is identically the
Jacobian, the rank of the Jacobian < m implies that F has a non-trivial null-space, and obviously
has no inverse; thus it is not a diffeomorphism of Rm →Rm if rankF ̸= m.
The Jacobian of F : M →N being full rank globally over M does imply that locally F
describes a diffeomorphism between M and N, but not globally. Essentially, the global 1-1 nature
of the transformation can fail.
Examine the mapping from the real line onto the circle, a subset of R2, F : R →F(R) ⊂R2
according to F(t) = (cos t, sin t). The Jacobian globally has rank 1, i.e. DF(t) =
 −sin t
cos t

. Locally,
this describes a diffeomorphism from small neighborhoods of diameter strictly less than 2π to the
image of F, subsets of R2.
The local result is essentially the inverse function theorem for Rn, stated here, with the proof
found in Boothby (see p. 42, [148]).
Theorem 8.19 (Local Diffeomorphisms via the Inverse Function Theorem)
Let W
be an open subset of Rn, F : W →Rn a smooth mapping. If the Jacobian of F at w ∈W is
nonsingular, then there exists an open neighborhood O ⊂W of w such that V = F(O) is open
and F : O →V is a diffeomorphism.
See proof in Boothby, p. 42 [148].
Corollary 8.20
Then, let M, N be manifolds both of dimension dim M = dim N = m with
smooth mapping F : M →N, and an open set W ⊂M. If w ∈W and the Jacobian of F at w
has rank DF(w) = m, then there exists an open neighborhood O ⊂W with V = F(O) open
and F : O →V a diffeomorphism.
Proof
For the manifolds apply the deﬁnition of the local coordinates, choose φ and
ψ to be the local coordinate maps for M, N, then ˆF = ψ ◦F ◦φ−1 : Rm →Rm.
Apply the inverse function theorem to the point φ(w) ∈Rm; then there exists an open
neighborhood O′ ⊂Rm containing φ(w) such that V′ = ˆF(O′) is open, and ˆF : O′ →V′
is a diffeomorphism. Then, F : O = φ−1O′ →ψ−1V′ is a diffeomorphism from an
open set O ⊂M containing w to an open set F(W) ⊂N.
Knowing only properties of the Jacobian implies that we will have to accept the fact that our
transformations may not be globally invertible. This motivates our understanding of mappings
which are only locally diffeomorphisms, such as are associated with immersions. See the example
below.
Deﬁnition 8.21
Given manifolds M, N, then the mapping F : M →N is said to be an
immersion if rankF = dim M = m everywhere.
Example 8.22 (Circle, Figure Eight)
The circle corresponding to F : R →R2, F(t) =
(cos 2πt, sin 2πt), is an immersion.
The ﬁgure eight corresponding to F : R →R2, F(t) = (2 cos(t −(1/2)π), sin(t −
(1/2)π)), is not.

222
8 MANIFOLDS, ACTIVE MODELS, AND DEFORMABLE TEMPLATES
8.3 Matrix Groups are Diffeomorphisms which are a Smooth Manifold
We shall work extensively to require the mappings of manifolds in the study of shape F : M ⊂
Rm →N ⊂Rn to be not just smooth but to be diffeomorphisms. Why should we be so fascinated
with diffeomorphisms? Well, one of the more intuitive reasons is that diffeomorphic maps preserve
features. For us this will correspond to such intuitive ideas as connected sets stay connected
(structures are not broken apart), and local maxima in the images remain local maxima under the
transformations (both proved below).
8.3.1 Diffeomorphisms
Almost all of the subsequent geometric work concentrates on submanifolds of Rn, corresponding
to points, lines, surfaces, and subvolumes, and actions upon them by diffeomorphisms. The
most important property that we shall exploit is that diffeomorphisms carry the submanifolds
diffeomorphically maintaining their topological structure.
For this we need to ﬁrst deﬁne more exactly the interpretation of the submanifolds and the
open sets. Since we will almost exclusively study manifolds which are subsets of Rn, we shall be
thinking of the subspace toplogy as the collection of open sets.
Deﬁnition 8.23
We shall say M is an m-dimensional submanifold of Rn with sub-
space topology TM if it is a set which is locally Euclidean of dimension m, with the topology
TM of open sets given by
TM = {O ⊂M : O = M ∩U, U open in Rn}.
(8.32)
The subspace topology is consistent with our everyday notions. The open sets in the subman-
ifold are just the open sets associated with the extrinsic background space intersected with the
submanifold.
Then the most celebrated property that we will require is that diffeomorphic mappings
carry submanifolds so as to preserve their topology (open sets remain open sets). This of course
corresponds to such intuitively natural properties as connected sets remain connected, i.e. two
eyes remain two, and do not become three.
Theorem 8.24
Diffeomorphisms map connected sets to connected sets.
Proof
Let F : M →N be a diffeomorphism and let M be a connected set. We need
to show that F(M) ⊂N is also a connected set. Suppose not, i.e. there exist two sets
N1 and N2, open in F(M), such that N1 ∩N2 = ∅and N1 ∪N2 = F(M). Therefore,
F−1(N1 ∩N2) = F−1(N1) ∩F−1(N2) = ∅. Also, as F is a diffeomorphsim F−1(N1) and
F−1(N2) open sets in M. Since N1 ∪N2 = F(M), F−1(N1)∪F−1(N2) = M, union being
a disjoint union. So we have two disjoint open subsets of M whose union is M, i.e. M
is not a connected set. Contradiction.
More generally, diffeomorphisms map smooth submanifolds diffeomorphically.
Theorem 8.25
Let F be a diffeomorphism from X →Y with M ⊂X a submanifold with
subspace topology.
Then F : M ⊂X →F(M) ⊂Y is a diffeomorphism from M →F(M) with the
subspace topology.
Proof
Global 1-1, onto and differentiability properties of F : M →F(M) follows
from the fact that F is a diffeomorphism from X →Y. We really need only show
continuity of F, F−1 and thus the homeomorphism properties. Let O be open in the
subspace topology of F(M), implying O = V ∩F(M), V open in the topology of Y.
Then
F−1(O) = F−1(V) ∩M
(8.33)

8.3 MATRIX GROUPS ARE DIFFEOMORPHISMS
223
which is in the subspace topology of M (since F is continuous in X giving F−1(V) is
open). So we have shown F is continuous on M. In precisely the same way it follows
F−1 is continuous on the subspace topology of F(M), so F is a homeomorphism from
M →F(M).
Clearly deﬁniteness of the Jacobian and full rank condition, although not implying global
properties, implies that such features as maxima of functions on M are preserved under the
mapping. To obtain a global property, the 1-1 condition needs to be added.
Theorem 8.26
Let M, N be manifolds both of dimension dim M = dim N = m. Then a
smooth mapping F : M →N is a diffeomorphism if and only if F is a bijection (1-1 and onto)
and rankF = m everywhere.
8.3.2 Matrix Group Actions are Diffeomorphisms on the
Background Space
Group transformations of background spaces which are manifolds are central to the pattern theory.
The major examples of the matrix groups which shall be used are obtained from subgroups
of the generalized linear group GL(n). Now matrix groups acting as transformations S : X →X
are diffeomorphisms on the backround space X. For this reason we examine the group action as a
transformation of the background spaces.
Theorem 8.27
Let S = GL(n) and S = Rn the generalized linear group and the translation
group, and Rn the differentiable manifold with the group actions viewed as linear mappings
F : S × Rn →Rn by FA(x) = Ax or Fa(x) = x + a, multiplication by the n × n matrix
A ∈GL(n) or translation by a ∈Rn.
Then these transformations F are group actions which are smooth (C∞) maps from
Rn →Rn, and FA, Fa : Rn →Rn are diffeomorphisms.
Proof
That these are group actions follows from e = I ∈GL(n) and e = 0 ∈Rn with
Ix = x, x + 0 = x. Associativity of matrix multiplication (A ◦B) · x = A · (B · x) and
associativity and commutativity of addition of vectors satisﬁes the group action.
These are smooth maps since Ax = 
m
j=1 a1jxj, 
m
j=1 a2jxj, . . . , 
m
j=1 amjxj
viewed as a column vector is just a polynomial in the entries of A. Addition is also
obviously a smooth map.
The generalized linear group action FA or translation Fa are 1 −1 and onto. The
onto property follows since for all y ∈Rn, there exists an x = F−1
A y with FAx = y. The
translation group is 1-1 and onto as well.
Clearly Fs is 1-1 since for any x, x′, if Fs(x) = Fs(x′) then
x = Fs−1◦s(x) = Fs−1(Fs(x)) = Fs−1(Fs(x′)) = x′.
(8.34)
For subgroups of GL(n), s = A =

aij

, then FA(x) = Ax and
∂FA(x)
∂xi

j
=
∂Ax
∂xi

j
= aij.
(8.35)
Since A is invertible, rank F(A,b)s(·) = n. For s = (A, b) with F(A,b)(x) = Ax + b, the
Jacobian is the same.
Using Theorem 8.25 then it deﬁnes a diffeomorphism on smooth submanifolds.
One of the most important implications of the matrix group action is that it deﬁnes a diffeo-
morphism, and therefore by Theorem 8.25 it carries smooth submanifolds smoothly maintaining

224
8 MANIFOLDS, ACTIVE MODELS, AND DEFORMABLE TEMPLATES
their submanifold structure. Examine such a result for the subgroups of the afﬁne group applied
to submanifolds of Rn.
Corollary 8.28
Let S be a subgroup of the afﬁne group. Let M be a submanifold of Rn.
Then the linear mappings Fs : M →Fs(M) are for all s ∈S diffeomorphisms from
M →Fs(M).
8.3.3 The Matrix Groups are Smooth Manifolds (Lie Groups)
Now the matrix groups are differentiable manifolds, providing the opportunity to perform
differentiation of group elements.
Deﬁnition 8.29
Let S be a group which is at the same time a differentiable manifold. For
x, y ∈S, let x ◦y denote their product and x−1 its inverse.
Then S is a Lie group provided that the mapping from S × S →S deﬁned by
(x, y) →x ◦y and the mapping S →S deﬁned by x →x−1 are smooth (C∞) mappings.
Example 8.30 (Vector Lie Groups)
Let S = Rn with addition. The mapping Rn ×
Rn →Rn given by (x, y) →x◦y = x+y, and inverse Rn →Rn deﬁned by x →x−1 =
−x. To show that these are smooth maps, deﬁne the local coordinates by the identity
map φ1 : Rn →Rn: φ1(x) = x. Let M = Rn × Rn have the product topology with the
local coordinate map φ : M →Rn the identity map componentwise φ(x, y) = (x, y),
F : M →Rn. Then, F(x, y) = x + y is smooth since ˆF = φ1 ◦F ◦φ−1 = x + y which is
smooth in x, y!
The inverse map F : Rn →Rn, F(x) = −x is similarly smooth.
Example 8.31 (Multiplicative and Complex Number Lie Group)
Let S = R× the
multiplication group of reals (not including 0) is a Lie group, x ◦y = xy, x−1 = 1/x.
Identical argument as in example 8.30.
The set of non-zero complex numbers S = C×, the multiplication group of
complex numbers (not including 0) is a Lie group. C× is a group with respect to
matrix multiplication of complex numbers, the inverse being z−1 = 1/z. Also, C× is
a smooth (C∞) manifold covered by a single coordinate neighborhood U = C× with
the coordinate map given by φ(z) = (x, y) for z = x + iy. Using these coordinates the
product w = zz′, z = x + iy, z′ = x′ + iy′, is given by
((x, y), (x′, y′)) →(xx′ −yy′, xy′ + yx′)
and the mapping z →z−1 by
(x, y) →

x
x2 + y2 ,
−y
x2 + y2

.
These two maps are smooth; therefore C× is a Lie group.
Our two major sources of Lie groups will correspond to the afﬁne group: that composed of
subgroups of the generalized linear group GL(n) and the translation group Rn.
Theorem 8.32
Then
1. GL(n) as the matrix product group with group operation A◦B = AB deﬁned by matrix
product for A, B ∈GL(n), and A−1 matrix inverse; and

8.3 MATRIX GROUPS ARE DIFFEOMORPHISMS
225
2. Rn as the translation group with group operation addition a ◦b = a + b and inverse
a−1 = −a
are both differentiable manifolds which are Lie groups.
Proof
To see that GL(n) is a differentiable manifold identify A ∈GL(n) with Rn2
and we need only prove that it is an open subset of a differentiable manifold. To see
that it is open, use the distance d(A, B) = 
i,j |Ai,j −Bi,j|2. Clearly if det A ̸= 0, then
there exists a neighborhood Nǫ(A) ⊂GL(n) of A since det is a continuous function.
To see that the matrix product is a smooth map of GL(n) × GL(n) it sufﬁces
to see that the components of the product matrix are polynomial evaluations in
the elements of matrices under multiplication and polynomials are C∞-functions.
Similarly, matrix inversion is also a polynomial function (see below) and hence a
smooth map.
Example 8.33 (Special Orthogonal Group)
The matrix subgroups are Lie groups. It
is informative to prove this for the heavily used case of SO(3) ⊂GL(3).
Theorem 8.34
SO(3) is a Lie group.
Proof
It is a topological manifold with the subset topology inherited from the space
of 3 × 3 matrices with non-zero determinants, the generalized linear group GL(3). It
is very often studied as the submanifold of GL(3), the metric used being the regular
matrix 2-norm, ∥A−B∥= 
ij(aij−bij)2.Also, SO(3) is locally euclidean, of dimensions
3, with the local coordinate chart given by the mapping
φ−1(x1, x2, x3) =


1
0
0
0
cos x1
sin x1
0
−sin x1
cos x1




cos x2
0
−sin x2
0
1
0
sin x2
0
cos x2


×


cos x3
sin x3
0
−sin x3
cos x3
0
0
0
1

.
(8.36)
We need to prove that this map is a homeomorphism on small neighborhoods. Sines
and cosines are homeomorphic away from the integer multiple of π/2 and near the
integer multiples of π/2 sine or cosine is homeomorphic depending upon whether
it is the even or odd multiple. Hence on neighborhoods small enough this map is a
homeomorphism. Besides, SO(3) also has a group structure with the group operation
being the regular matrix product, as elements of R3×3, and the inverse given by matrix
inversion.
Now we must show that matrix multiplication and inversion in SO(3) are C∞
maps. For A, B ∈SO(3) the product AB has entries which are polynomial in the entries
of A and B. The entries of A and B are C∞maps from the local coordinates of A and B
(by our choice of local charts which are made up of sines and cosines terms). Similarly,
the entries of the product AB are C∞in terms of its local coordinates. By concatenation,
there is C∞map from the local coordinates of A and B to the local coordinates of AB.
The inverse of A = (ai,j) may be written as A−1 = (1/det(A))(˜ai,j), where (˜ai,j) are
cofactors of A (hence polynomials in the entries of A). The det(A) is a polynomial in
the entries of A, which does not vanish on SO(3). It follows that A−1 has entries which
are rational functions with non-vanishing denominators, hence C∞. Therefore, SO(3)
is a Lie group.

226
8 MANIFOLDS, ACTIVE MODELS, AND DEFORMABLE TEMPLATES
8.4 Active Models and Deformable Templates as Immersions
Groups transforming generators which are manifolds locally are an area of active research in the
computer vision and pattern theory literature. For single global groups actions — scale, rotation,
translation — they are diffeomorphisms; for products of groups which act locally such as for active
models they are immersions.
A great deal of work has been done in the area of segmentation via Active Deformable
Models, including active snakes and contours [137, 150, 184–195], active surfaces and deformable
models [56, 196–203]. These are global shape methods in that they deﬁne global generators which
aggregate multiple local features into single objects. In this approach generators g ∈G are deﬁned
as submanifolds, curves, surfaces, and subvolumes. The transformations making them active ﬁll
out the orbit of all shapes as deﬁned through the vector ﬁelds which move the boundary manifolds
and their connected interiors.
For using these models the image is assumed to be a collection of generators or submanifolds
(object) deﬁned parametrically. The collection of generating submanifolds form a disjoint partition
of the image.
8.4.1 Snakes and Active Contours
Here is a transformation deﬁned by SO(2)[0,L] acting on a straight line generator corresponding
to snakes. Let F : (0, L) ⊂R →R2,
F(t) =
 t
0

cos θ(l)
−sin θ(l)
sin θ(l)
cos θ(l)
 
1
0

dl =
 t
0

cos θ(l)
sin θ(l)

dl.
(8.37)
Then F is an immersion in R2 since rankF = 1. Clearly, this is not necessarily a global diffeomor-
phism onto its image F(0, L). Allow the curve to turn 2π radians in length L/2. For example, choose
θ(l) = 2 · 2πl, l ∈[L/4, 3/4L].
Adding scale, then we have the following.
Theorem 8.35
Let the mapping F : M ⊂R2 →N ⊂R2 generated from the product group
(US × SO(2))[0,T] according to
F(t) =
 t
0
ρ(l)

cos θ(l)
−sin θ(l)
sin θ(l)
cos θ(l)
 
1
0

dl.
(8.38)
Then if ρ ̸= 0, the mapping F is an immersion.
Proof
The Jacobian matrix DF(t) = ρ(t)

cos θ(t)
−sin θ(t)
sin θ(t)
cos θ(t)
 
1
0

.
8.4.2 Deforming Closed Contours in the Plane
Here is an interesting example of deforming closed contours as mapping of circular templates.
Corollary 8.36
Let the mapping F : M = S1 ⊂R2 →N ⊂R2 generated from the product
group (US × SO(2))[0,T] according to
F(t) =
 t
0
ρ(l)
 cos θ(l)
−sin θ(l)
sin θ(l)
cos θ(l)
 
cos θ(l)
−sin θ(l)

dl,
t ∈[0, T].
(8.39)

8.4 ACTIVE MODELS AND DEFORMABLE TEMPLATES AS IMMERSIONS
227
Then this is an immersion if it satisﬁes ˙ρ ̸= 0 from above; the mapping F : S1 →M is an
immersion to a closed curve in the plane if the processes ρ(t), θ(t), t ∈[0, 2π] satisfy the the
Fourier transform condition
 2π
0
e−jtρ(t)ejθ(t) dt = 0.
(8.40)
Proof
Expanding the Fourier transform in its real and imaginary parts gives
 2π
0
(cos t −j sin t)ρ(t)(cos θ(t) + j sin θ(t))dt
=
 2π
0
(ρ(t) cos θ(t) cos t + ρ(t) sin θ(t) sin t)dt
+ j
 2π
0
(ρ(t) sin θ(t) cos t −ρ(t) cos θ(t) sin t)dt.
(8.41)
The Fourier transform Eqn. 8.40 set to 0 means that the real and imaginary parts are
zero giving the closure condition
F(2π) =
 2π
0
ρ(t)
 cos θ(t)
−sin θ(t)
sin θ(t)
cos θ(t)
 
cos t
−sin t

dt = 0.
(8.42)
8.4.3 Normal Deformable Surfaces
Thus far we have examined group action on the tangents. Now examine normal deformations of
curves and surfaces as deﬁned by products of translation groups.
Theorem 8.37
Given is smooth surface submanifold M ⊂R3, with local coordinate patch
x : D ⊂R2 →O ⊂M
x(u, v) = p + uE1 + vE2 + E3f(u, v),
(8.43)
with orthogonal frame E1, E2, E3 and normal n and shape operator Su,v. Let the mapping
F : M →N generated from the translation group deﬁned by
F(u, v) = x(u, v) + h(u, v)n(u, v),
(8.44)
with h a scalar ﬁeld specifying the normal translation motion. Then F : M →N is an immer-
sion with Jacobian matrix DF(u, v) = (∂F/∂u), (∂F/∂v)) with columns
∂F
∂u =


1 −(fuhu/

1 + f2u + f2v ) + hS11
−(fvhu/

1 + f2u + f2v ) + hS21
+fu + (hu/

1 + f2u + f2v )


, ∂F
∂v =


(−fuhv/

1 + f2u + f2v ) + hS12
1 −(fvhv/

1 + f2u + f2v ) + hS22
+fv + (hv/

1 + f2u + f2v )


.
(8.45)
Proof
The tangents (∂x/∂u) = E1 + fuE3, (∂x/∂v) = E2 + fvE3, and normal n =
(−fuE1 −fvE2 + E3)
%
1 + fu2 + fv2, then

228
8 MANIFOLDS, ACTIVE MODELS, AND DEFORMABLE TEMPLATES
ADINA-PLOT VERSION 6.1.4, 26 JUNE 1996
FREQUENCY ANALYSIS OF A HIPPOCAMPUS
X
Y
Z
ADINA
MODE_SHAPE
MODE 13
F = 47.74
MODESHAPE
3.484
XVMIN -1
XVMAX 1
YVMIN -1
YVMAX 1
X
Y
Z
ADINA
MODE_SHAPE
MODE 14
F = 47.91
MODESHAPE
4.309
XVMIN -1
XVMAX 1
YVMIN -1
YVMAX 1
X
Y
Z
ADINA
MODE_SHAPE
MODE 15
F = 48.71
MODESHAPE
3.184
XVMIN -1
XVMAX 1
YVMIN -1
YVMAX 1
X
Y
Z
ADINA
MODE_SHAPE
MODE 16
F = 48.71
MODESHAPE
3.635
XVMIN -1
XVMAX 1
YVMIN -1
YVMAX 1
Figure 8.3 Left: shows the spherical template (top panel) and its deformed version (bottom panel)
resulting from translation group applied to 4096 generators on the template. Right: Panels show
deformations via the ﬁrst four surface harmonics of the surface of the sphere (see also Plate 11).
∂F
∂u = ∂x
∂u + ∂h
∂un + h∂n
∂u
(8.46)
= E1 + fuE3 + hu
−fuE1 −fvE2 + E3

1 + f2u + f2v
+ h
 S11
S21

1
E1 + h
 S12
S22

2
E2.
(8.47)
Similarly as above, ∂F
∂v follows.
Example 8.38 (Active Deformable Spheres)
Joshi and McNally [33, 198] have used
active deformable spheres for the study of 3D cell motion.
Generate new shapes via normal deformation with local coordinate representation (see
example 7.46)
x(u, v) + h(u, v)n(u, v) = (1 + h(u, v))x(u, v),
(8.48)
withx(u, v) = (u, v,

r2 −u2 −v2), xu = (1, 0, −(u/

r2 −u2 −v2)), xv = (0, 1, −(v/

r2 −u2 −v2)),
n(u, v) = (1/r)(u, v,

r2 −u2 −v2). The Jacobian matrix in local coordinates is in the form
∂F
∂u = hu(u, v)x(u, v) + (1 + h(u, v))xu(u, v),
∂F
∂v = hv(u, v)x(u, v) + (1 + h(u, v))xv(u, v).
(8.49)

8.4 ACTIVE MODELS AND DEFORMABLE TEMPLATES AS IMMERSIONS
229
Figure 8.3 shows the spherical template containing 4096 nodes. Column 1 shows the sphere
(top) and an example of the template shape deformed via the normal vector ﬁelds. Column
2 shows examples of the sphere deformed via the ﬁrst four eigenfunctions of the Laplacian
operator.
8.5 Activating Shapes in Deformable Models
Now examine the calculus for activating Active Deformable Models. Deﬁne generators g ∈G
which are smooth submanifolds; activating them arise by constructing transformations which are
vector ﬁelds which move the protypical shape to ﬁll out the orbit of all shapes.
8.5.1 Likelihood of Shapes Partitioning Image
For using these models for segmentation, the image is assumed to be a collection of generators
or objects deﬁned parametrically. Deﬁne the background space D ⊂Zd, Rd to be a collection of
objects g1, g2, . . ., with textured interiors forming a disjoint partition of the image, D = ∪jD(gj). The
goal is to infer the objects gj, j = 1, 2, . . . forming a covering of the complete image. Such a disjoint
partition is illustrated by panel 1 of Figure 8.4 showing an electron micrograph of the mitochondria.
Model the interiors of the shapes of real-valued images as scalar random ﬁelds, either Markov or
Gaussian ﬁelds deﬁned on the index sets of the integer lattices. Because of splitting, the partitions
are conditionally independent given their boundaries. To formulate the inference of the active
shapes gj, j = 1, 2, . . . in the continuum, associate an energy density with each shape; the energy
under any model for shape g takes the form

D(g) E(x)dx, E(·) representing the interior model for
shape g as a density in space. Of course, the smooth manifold of the boundary ∂D(g) of each shape
intersects obliquely the discrete lattice Zd, and in calculating the energy associated with each shape
the partial voxels are interpolated in the calculation.
Then the conditional probabililty of the partition D(gj), j = 1, 2, . . . takes the form
p(I|D(g1), D(g2), . . . ) ∝

j
e
−

D(gj) Ej(x)dx
.
(8.50)
8.5.2 A General Calculus for Shape Activation
ActiveModelsforsegmentationofcurves, surfaces, andsubvolumesaremadeactivebyassociating
with the bounding manifold of the shapes some velocity of motion which is essentially a vector
ﬁeld acting on the shape. The vector ﬁeld is constructed so that the shape covers with its interior the
region in the image corresponding to the model of the interior of the shape. Aprincipled way to do
this is to send the parameters of the active model along a trajectory which follows the gradient of
the energy representing the Bayesian likelihood of the image given the parametric representation
as depicted in Eqn. 8.50.
Without loss of generality assume that there is one global model deﬁned by g(γ ) with interior
and parameterize its interior directly in the parameter D(γ ) and complement D(γ )c = D\D(γ ); the
parameterization of g is associated to γ . For group parameters γ represents the local coordinates.
To activate the shape so that it evolves to an optimum covering of the background space we now
calculate the general form for the variation of the energy densities making up the potential so as

230
8 MANIFOLDS, ACTIVE MODELS, AND DEFORMABLE TEMPLATES
(1)
(2)
Figure 8.4 Panel 1 depicts an electron micrograph at 30,000 times magniﬁcation showing con-
nected subregions of mitochondria. Panel 2 shows the disjoint partition of the connected
subregion micrograph images into ∪jD(gj) and the bounding contours ∂D(gj) representing the
closed active contour models representing these regions. Data taken from the laboratory of
Dr. Jeffrey Safﬁtz of Washington University.
to evolve the shape towards a minimum. Model the energy density for the interior as E1(x; γ ), x ∈
D(γ ) and exterior E2(x), x ∈D(γ )c the energy becomes
H(γ ) =

D(γ )
E1(x; γ ) dx +

D\D(γ )
E2(x) dx.
(8.51)
Notice, the interior energy density of the model is generally an explicit function of γ the parameter
to be inferred, the exterior is not.
Let (∂1, ∂2, . . . , ∂γ , . . .) be a basis of the tangent space of the transformation group. A differ-
ential ∂γ will correspond to the generation of some vector ﬁeld V(x), x ∈D so that a point in the
shape x ∈D(γ ) is mapped by the vector ﬁeld x →x+V(x)ǫ under a small perturbation γ →γ +ǫ.
Thus V(·) is essentially the velocity of the particle being acted upon by the 1-parameter variation
of the group transforming the shape under the perturbation of γ →γ + ǫ. Now interestingly, the
variation of the energy representing the active shapes will have two components arising essentially
from the fact that both the region of integration of the active model as well as the energy density
contained within the interior change with the group transformation.
For our derivation we will need to interpret the delta-dirac acting through a function
following the standard developments in [204].
Lemma 8.39
With (·) a real-valued function on Rd with smooth level set {x : (x) = 0}
a smooth submanifold with ∥∇∥= 1 and surface measure ds, then

φ(x)δ((x)) dx =

=0
φ(s) ds.
(8.52)
Proof
Deﬁne the curvilinear coordinates u1, . . . , ud−1 on the smooth manifold
{(x1, . . . , xd)
:
(x)
=
0}.
Then introduce the change of coordinate system
x(u1, . . . , ud−1, ud = ); notice x(u1, . . . , ud−1, ud = 0) is a point on the manifold
 = 0. Locally, ud =  is the distance to the level set, since  is constant along
u1, . . . , ud−1 and ∂/∂ud = 1. In these coordinates,
dx(u) = | det Dx(u)|du1, du2, . . . , dud,
(8.53)

8.5 ACTIVATING SHAPES IN DEFORMABLE MODELS
231
where Dx(u) is the Jacobian matrix of the transformation x(u) evaluated at u =
(u1, . . . , ud). Then the integral becomes

φ(x)δ((x))dx =

φ(x(u1, . . . , ud))δ(ud)| det Dx(u1, . . . , ud−1, ud)|
× du1 · · · dud−1dud
(8.54)
=

=0
φ(x(u1, . . . , ud−1, 0))| det Dx(u1, . . . , ud−1, 0)|
× du1 · · · dud−1
(8.55)
=

=0
φ(s) ds,
(8.56)
where the surface measure ds is du1, · · · , dud−1 weighted by the determinant of the
Jacobian Dx evaluated at (u1, . . . , ud−1, 0).
Example 8.40
To illustrate, take the unit circle  =

x2 + y2 −1, then choose u1 ∈
[0, 2π), u2 = , then x(u1, u2) = (u2 + 1) cos u1, y(u1, u2) = (u2 + 1) sin u1 and
Dx(u1, u2) =
 −(u2 + 1) sin u1
(u2 + 1) cos u1
cos u1
sin u1

.
(8.57)
Then ds = du1, and

φ(x)δ((x))dx =
 2π
0
φ(cos u1, sin u1)du1.
(8.58)
Now do the sphere,  =

x2 + y2 + z2 −1, then choose u1 ∈[0, 2π), u2 ∈[0, π],
u3 = , with
x(u1, u2, u3) = (u3 + 1) cos u1 sin u2
(8.59)
y(u1, u2, u3) = (u3 + 1) sin u1 sin u2
(8.60)
z(u1, u2, u3) = (u3 + 1) cos u2.
(8.61)
The Jacobian matrix becomes
Dx(u1, u2, u3) =


−(u3 + 1) sin u1 sin u2
(u3 + 1) cos u1 sin u2
0
(u3 + 1) cos u1 cos u2
(u3 + 1) sin u1 cos u2
−(u3 + 1) sin u2
cos u1 sin u2
sin u1 sin u2
cos u2

.
(8.62)
Then ds = sin u2du1du2, and

φ(x)δ((x))dx =
 2π
0
 π
0
φ(x(u1, u2, 0), y(u1, u2, 0)) sin u2du2du1.
(8.63)
Now we compute the variation of the deformable template with the transport theorem perturbing
the shape g(γ ) parametrically according to v(γ ) = dg(γ )/dγ .
Theorem 8.41 (Transport Theorem for Deformable Shapes)
Given simply connected
regions g(γ ) with interior D(γ ) and smooth boundary ∂D(γ ) and associated smoothly varying
unit normal n, vector perturbation v = ∂g(γ )/∂γ and associated surface measure ds. Then
with interior and exterior models E1(γ ), E2 smooth in space and parameterization with energy

232
8 MANIFOLDS, ACTIVE MODELS, AND DEFORMABLE TEMPLATES
H(γ ) =

D(γ ) E1(x; γ )dx +

D(γ )c E2(x)dx then the differential of the energy is given by the
sum of the boundary and interior integrals
∂H(γ )
∂γ
=

∂D(γ )
(E1(s; γ ) −E2(s))⟨n(s), v(s)⟩Rd ds +

D(γ )
∂
∂γ E1(x; γ )dx.
(8.64)
Proof
Deﬁne the function (·) with level set {x : (x) = 0} the boundary ∂D(γ )
with  > 0 in D, and the normal to the boundary ∇ = n (∥∇∥= 1). Rewrite
the potential using the heaviside function h(x) = 1 for x > 0, h(x) = 0 for x < 0
according to
H(γ ) =

h((x))E1(x; γ )dx +

(1 −h((x)))E2(x)dx.
(8.65)
The exterior model is not a function of γ , therefore differentiating by parts gives
∂H(γ )
∂γ
=

∂
∂γ h((x))(E1(x) −E2(x))dx +

h((x)) ∂
∂γ E1(x)dx
(8.66)
=

δ((x))⟨∇(x), v(x)⟩Rd(E1(x) −E2(x))dx +

D(γ )
∂
∂γ E1(x)dx.
(8.67)
Now use the lemma,

δ((x))g(x)dx =

∂D g(s)ds (since ∥∇∥= 1) to give
∂H(γ )
∂γ
=

∂D(γ )
⟨n(s), v(s)⟩Rd(E1(s) −E2(s))ds +

D(γ )
∂
∂γ E1(x)dx.
(8.68)
The calculation of the surface measure for integration on ∂D has to be done for each particular
case. What is clearly the case is that ⟨n, (∂g(γ )/∂γ )⟩Rdds is the local volume element that gets
integrated around the boundary parameterization against the difference in energies under the
two models.
8.5.3 Active Closed Contours in R2
Assume that the continuum D ⊂Rd is partitioned into a set of closed connected regions with
smooth boundary manifolds simple C1 closed curves in the plane gt(γ ), t ∈[0, 1) ∈R2. The
potential takes the form
H(γ ) =

D(γ )
E1(x; γ )dx+

D\D(γ )
E2(x)dx,
(8.69)
where the potential E1 is associated with the interior x ∈D(g(γ )) of g, and E2 with the exterior
x ∈D(g(γ ))c. Depicted in Figure 8.5 is an example of a small perturbation of the parameter causing
a change in energy which involves the integral along the boundary of the difference of the models.
Notice, in this model, that the action of the transformation does not affect the interior energy
density.
The one parameter variations of the potential with respect to the parameters γ are given by
the following curvilinear integral.

8.5 ACTIVATING SHAPES IN DEFORMABLE MODELS
233
g(g)
D(g +e)
D(g)    D(g+ e)c
g(g +e)
D(g)c


Figure 8.5 Panel shows the computation required for a small perturbation of the active
model g(γ ) →g(γ + ǫ) depicts the regions over which the energy density in the integral is
computed.
Theorem 8.42
For smooth C1 curve gs(γ ), s ∈[0, 1] with simply connected interior and
smooth potentials E1, E2, then with the normal n =
 −˙gy
˙gx

and velocity v = ∂g/∂γ , then
the 1-parameter variation of the potential is the integral
∂H(γ )
∂γ
=
 1
0
(E1(gs) −E2(gs))⟨n(s), v(s)⟩ds .
(8.70)
Let us examine the Chan–Vese model [205] in which only mean interior and exterior values
between the objects are modelled with the random ﬁeld having identity covariance.
Corollary 8.43
Let the potential representation of the interior–exterior modelling of
gs(γ ), s ∈[0, 1] taking the form
H(γ ) =

D(γ )
(I(x) −µ1)2 dx +

D\D(γ )
(I(x) −µ2)2 dx.
(8.71)
The optimizing µ1 and µ2 are the mean intensities of the image inside the interior and
exterior:
µ1 =

D(γ ) I(x)dx

D(γ ) dx
,
µ2 =

D\D(γ ) I(x)dx

D\D(γ ) dx
;
(8.72)
the variation of the potential takes the form
∂H(γ )
∂γ
=

∂D(γ )
⟨n(s), v(s)⟩[(I(x) −µ1)2 −(I(x) −µ2)2] ds.
(8.73)
The functional perturbation in the steepest descent direction g →g + ǫψ is given by
ψ(s) = (µ1 −µ2)(2I(x) −µ1 −µ2)n(s).
(8.74)
Proof
From Theorem 8.41, we have E1(x; γ ) = (I(x) −µ1)2, and E2(x; γ ) = (I(x) −
µ2)2, giving the variation of H(γ ) as
∂H(γ )
∂γ
=

∂D(γ )
⟨n(s), v(s)⟩[(I(x) −µ1)2 −(I(x) −µ2)2]ds
+

D(γ )
∂
∂γ (I(x) −µ1)2dx +

D\D(γ )
∂
∂γ (I(x) −µ2)2 dx.
(8.75)
The second and third terms equal zero since

D(γ )
∂
∂γ (I(x) −µ1)2dx = −2

D(γ )
(I(x) −µ1) ∂
∂γ µ1 dx
= −2 ∂
∂γ µ1

D(γ )
I(x)dx −µ1

D(γ )
dx

= 0;
(8.76)
similarly for the third term.

234
8 MANIFOLDS, ACTIVE MODELS, AND DEFORMABLE TEMPLATES
Using the deﬁnition of µ1 and the Schwartz inequality [206], the steepest descent
direction that reduces the energy functional H most rapidly is given by Eqn. 8.74.
In the literature often an additional boundary integral term is added to the energy functional
to improve taking the form Hb(g(γ )) =

∂D(γ ) qds where q(x) is a scalar function deﬁned on the
image domain D. The boundary integral can be interpreted as the weighted length of the boundary
contour, when q(x) = 1 it gives the Euclidean length of the contour.
Corollary 8.44
Adding the boundary integral for smoothness
Hb(g(γ )) =

∂D(γ )
q ds,
(8.77)
the variation of Hb(g(γ )) with v = ∂g/∂γ is given by
∂Hb(g(γ ))
∂γ
=

∂D(γ )
−(∇q · n(s) + q∇· n) < n(s),
v(s) > ds.
(8.78)
The steepest descent direction for boundary contour evolution becomes
(s) = (∇q · n(s) + q∇· n)n(s);
(8.79)
combined with the Chan–Vese region functional, the steepest descent direction is given by
(s) = [(µ1 −µ2)(2I(x) −µ1 −µ2) + ∇q · n(s) + q∇· n]n(s).
(8.80)
See [207] for proof of this; the derivation of the variation in terms of level sets is derived below in
section 8.6.
8.5.4 Active Unclosed Snakes and Roads
Linear structures such as roads or membranes with constant narrow width relative to their length
have been examined extensively in the community. Model the roads as structures of constant,
known width 2w pixels parameterized by its midcurve fs, s ∈[0, L] (see Figure 8.6), assumed
to be of unit speed ∥(∂/∂s)fs∥= constant, with length encoded through the index length [0, L].
Perturbing the boundary corresponds to wiggling a structure of constant width (therefore constant
area) with position which snakes through R2.
Perturbing the boundary wiggles the linear structure of constant width and length L, with
position snaking through R2 and parameterized via its midcurve fs, s ∈[0, L]. To compute the
curvilinear integral around the boundary of the membrane it is divided into two major components
f+w
s
, f−w
s
(see left panel of Figure 8.6) determined by the midcurve fs and its normal ns:
f+w
s
= fs + wns ,
f−w
s
= fs −wns ,
with ns =
 −∂fys/∂s
∂fxs/∂s

.
(8.81)
Then a simple formula arises for computing the variation of these linear roads with respect to
1-parameter perturbations of the midcurve. Essentially since the Jacobian along either of the long
boundaries are equal, then variation amounts to comparing the difference between texture poten-
tials representing the interior and exterior of the roads E = E1 −E2 as it runs along each of the
boundaries.

8.5 ACTIVATING SHAPES IN DEFORMABLE MODELS
235
Theorem 8.45
Given is a simply connected road of ﬁxed width w ≪L (much less than
its length) with smooth boundary and interior and exterior potentials satisfying Theorem
8.42 with E = E1 −E2. Parameterize the shape via its smooth midcurve in unit speed
fs =
 fxs
fys

, s ∈[0, L] having normal ns, ∥˙f∥= 1, and curvature ∥¨f∥= κ with ¨f = κn.
Then the plus and minus boundary normal-velocity terms are given by
⟨nw, vw⟩= (1 −κw)⟨n, v⟩,
⟨n−w, v−w⟩= (1 + κw)⟨n, v⟩.
(8.82)
The 1-parameter variation of the potential is given by the integral
∂H(f(γ ))
∂γ
=
 L
0
⟨n(s), v(s)⟩[(1 −wκ(s))E(f+w
s
)
−(1 + wκ(s))E(f−w
s
)]ds + O(w).
(8.83)
Proof
Let the midcurve and normal f =
 fx
fy

, n =
 −˙fy
˙fx

, then the plus w
boundary and tangent are given by
fw = f + wn =
 fx −w˙fy
fy + w˙fx

,
˙fw = ˙f + w˙n =
 ˙fx −w¨fy
˙fy + w¨fx

.
(8.84)
Using the fact that ¨f = κn, the plus w normal becomes
nw =
 −˙fy −w¨fx
˙fx −w¨fy

= (1 −wκ)n.
(8.85)
Since (∂/∂γ )⟨n, n⟩= 0 we have
⟨nw, vw⟩=

(1 −wκ)n, ∂
∂γ fw

(8.86)
=

(1 −wκ)n, ∂
∂γ (f + wn)

= (1 −wκ)⟨n, v⟩.
(8.87)
Similarly for the −w boundary. Substituting into the closed curve Theorem 8.42
and integrating along the midcurve give the result.
Example 8.46 (Piecewise Linear Roads)
Let the roads have zero curvature and be
built from linear sections as in the left panel of Figure 8.6. Let the midcurve
parameterization be
fs = (s −n)
 cos θn+1
sin θn+1

+
n

k=1
 cos θk
sin θk

+
 x0
y0

s ∈[n, n + 1).
(8.88)
Then the variation ∂fs/∂θm =
 −sin θm
cos θm

1>m(s) with the normal ns =
 −sin θk
cos θk

for
s ∈[k −1, k) giving ⟨n(s; θm), v(s; θm)⟩= cos(θm −θs)1>m(s); the 1-parameter variation
becomes
∂H(θm)
∂θm
=
L

k=m+1
2 cos(θk −θm)
 k
k−1
[E(f+w
s
) −E(f−w
s
)]ds + O(2w).
(8.89)
Shown in the right panel of Figure 8.6 is a linear membrane depicted using the
road model.

236
8 MANIFOLDS, ACTIVE MODELS, AND DEFORMABLE TEMPLATES
fi
2w
2w
fl
w+
fl
w–
Figure 8.6 Left panel shows the linear membrane model with midline curve fs shown dashed,
and two boundary components deﬁned by the midline curve and normals f+w
s
= fs + wns and
f−w
s
= fs −wns. Right panel shows results from the linear active contour model.
8.5.5 Normal Deformation of Circles and Spheres
Corollary 8.47
For normal deformation of the circle, with us = 
∞
n=0 unej2πns, then
gs =
 x0
y0

+
 cos s
sin s

+ us
 cos s
sin s

,
s ∈[0, 2π],
(8.90)
and the gradient is given by
 ∂H(γ )/∂x0
∂H(γ )/∂y0

=
 2π
0
(E1(s) −E2(s))
 −cos s
−sin s

ds;
∂H(γ )
∂un
=
 1
0
(E1(s) −E2(s))ej2πnsds.
(8.91)
Proof
The normal to the curve is given by n(s) =
 −∂gys/∂s
∂gxs/∂s

= −
8 cos s
sin s
9
, with
the velocity v(s) = ∂g/∂un = −ej2πnsn(s). Substituting gives the result.
8.5.6 Active Deformable Spheres
Return to example 8.38 on active spheres under normal deformation with active models. Joshi
and McNally [33, 198] have used active deformable spheres to study 3D cell motion during
embryogenesis in the slime mold Dictyostelium discoideum in which individual cells are labeled
with a ﬂuorescent dye and analyzed during an aggregation phase of their life cycle. Joshi uses
azimuth-elevation representation and spherical harmonics. Let
g(θ, ψ) =


cos θ sin ψ
sin θ sin ψ
cos ψ

+ u(θ, ψ)


nx(θ, ψ)
ny(θ, ψ)
nz(θ, ψ)

+


x0
y0
z0

, ψ ∈[0, π], θ ∈[0, 2π).
(8.92)

8.5 ACTIVATING SHAPES IN DEFORMABLE MODELS
237
with n(θ, ψ) ∈R3 the normal to the sphere at θ, ψ, and u(θ, ψ) is the scalar ﬁeld parameterizing
the translation vector ﬁelds applied to the generator spheres. The scalar ﬁeld on the sphere is
often parametrically deﬁned via a complete orthonormal spherical-harmonic basis analogous to
the complex exponentials on the circle (see the subsequent Chapter 9, Section 9.8).
Corollary 8.48 (The Sphere)
Assuming the interior and exterior models E1, E2, then the
likelihood of a spherical active shapes with simply connected interior D(g(γ )) has 1-parameter
variation of the energy
∂H(γ )
∂γ
=

θ∈[0,2π),ψ∈[0,π]
(E1(s) −E2(s))⟨n(s(θ, ψ)),
v(s(θ, ψ))⟩ds.
(8.93)
For scalar ﬁelds in the spherical harmonic basis expansion
u(θ, ψ) =
N

n=1
(un0φn0(θ, ψ) + (ue
nmφe
nm(θ, ψ) + uo
nmφo
nm(θ, ψ))
(8.94)
φe,o
nm the even (cosine) and odd (sine) spherical-harmonics (see Deﬁnition 9.57, Chapter 9),
then for v(θ, ψ) = ∂g(θ, ψ)/∂ue,o
nm = φe,o
nm(θ, ψ)n(θ, ψ) and


∂H(γ )/∂x0
∂H(γ )/∂y0
∂H(γ )/∂z0

=

θ∈[0,2π),ψ∈[0,π]
(E1(θ, ψ) −E2(θ, ψ))n(θ, ψ) sin ψdψdθ,
(8.95)
∂H(γ )
∂unm
=

θ∈[0,2π),ψ∈[0,π]
(E1(θ, ψ) −E2(θ, ψ))φnm(θ, ψ) sin ψdψdθ.
(8.96)
8.6 Level Set Active Contour Models
NowletusexaminelevelsetmethodschampionedintheOsherschoolasacomputationalapproach
for generating solutions to active shape problems. Starting from an initial partition of the domain
x ∈D as speciﬁed by an initial submanifold, active shapes g(γ ) evolve by generating a sequence
of shapes which converge ultimately to a partition at inﬁnite simulation time. In the level set
framework [208] for implementation, this is achieved by embedding the explicit representations of
the evolutions of the active shapes g(γ ) in their embedding background spaces. This is obtained by
formulating the solutions as the zero level set of a higher dimensional Lipschitz-continuous scalar
functiont(x), x ∈X; thezerolevelsetfunctionatanytimeinstantofsimulationtimet correspondsto
{ x | t(x) = 0 }. Although there are inﬁnitely many choices of the level set function, in practice, the
signed distance function is preferred for its stability in numerical computations. The fast marching
method proposed in [209, 210] provides an efﬁcient algorithm for constructing the signed distance
function from a given contour. Alternatively, an explicit representation of the embedded contour
can be recovered from the signed distance function by applying any isocontour algorithm.
To relate the evolution of the explicit representation of the bounding manifold of the
deformable shape g(γ ) with the level set formulation we examine the evolution of the level sets
of the extrinsic embedding space. For this we shall “interchange between” δ() with the norm of
the gradient of the embedding level set function ∥∇∥. This rescaling corresponds to a natural
extension of the evolution of the zero level set to all the other level sets of  [211], which does not
affect the evolution of the zero level set itself.

238
8 MANIFOLDS, ACTIVE MODELS, AND DEFORMABLE TEMPLATES
In this setting, deﬁne the level set function (x), x ∈X with natural boundary conditions
along the boundary of the computational domain ∇·n = 0. To calculate the variational solutions
perturbations of  → + ǫ, with  satisfying boundary conditions ∇ · n = 0.
Theorem 8.49
For the Chan-Vese Mean Model with energy of Theorem 8.43, the steepest-
descent direction is given by
t(x) = δ(t(x))(µ1 −µ2)(2I −µ1 −µ2).
(8.97)
Deﬁning δ() by |∇|, n = ∇/∥∇∥, the level set evolution in terms of the extrinsic
boundary contour evolution with initial estimate (x) is the directional derivative in the
direction of the gradient of the level set
∂t(x)
∂t
= ⟨, ∇t⟩
where  = (µ1 −µ2)(2I −µ1 −µ2)n.
(8.98)
Proof
The proof of Eqn. 8.97 follows techniques similar to that in [205, 211] to derive
the variation of H(). The Frechet derivative of ∂H() in the direction (x) is
computed as
∂H() =

δ()(I −µ1)2 dx −

δ()(I −µ2)2 dx
(8.99)
−2

h()(I −µ1)∂(µ1, )dx −2

(1 −h())(I −µ2)∂(µ2, ) dx.
(8.100)
(a)
= ⟨δ()(µ1 −µ2)(2I −µ1 −µ2), ⟩,
(8.101)
where (a) follows from the fact that the last two terms in Eqn. 8.100 evaluate to zero
(similarly to Eqn. 8.76). By the Schwartz inequality the variation in the steepest descent
direction  is therefore given by Eqn. 8.97.
Oftentimes in the literature an additional boundary integral is added to the energy functional
to improve the smoothness of the segmented boundary contour.
Corollary 8.50
Adding the boundary integral Hb() =

q(x)δ((x))dx, the level set
function following the steepest-descent direction
 = δ((x))(µ1 −µ2)(2I −µ1 −µ2) + δ()
∇q · ∇
∥∇∥
+ q∇·
 ∇
∥∇∥

.
(8.102)
The level set evolution equation becomes
t = (µ1 −µ2)(2I(x) −µ1 −µ2)∥∇∥+ ∇q · ∇ + q∇·
 ∇
∥∇∥

∥∇∥
(8.103)
= (µ1 −µ2)(2I(x) −µ1 −µ2)∥∇∥+ ∇q · ∇ + qκ∥∇∥,
(8.104)
where κ is the curvature of the level sets of  corresponding to the divergence of the normal.

8.6 LEVEL SET ACTIVE CONTOUR MODELS
239
Proof
To prove the second boundary part, the Frechet derivative ∂Hb() in the
direction (x) is computed as
∂Hb() =


δ′()∥∇∥q dx +


δ()q∇ · ∇
∥∇∥
dx,
where δ′(·) denotes the ﬁrst derivative of the delta function. Applying Green’s
formula [206] to the second term yields
∂Hb() =


δ′()∥∇∥q dx +
;
∂
δ()q∇ · n
∥∇∥ds
−


∇·

δ()q ∇
∥∇∥

dx,
where ∇· is the divergence operator, n is the normal vector to the boundary and ds is
a differential element on the boundary. Since
∇·

δ()q ∇
∥∇∥

= qδ′()∥∇∥+ δ()∇·

q ∇
∥∇∥

,
under the boundary conditions ∇ · n = 0 obtains
∂Hb() = −


δ()∇·

q ∇
∥∇∥

 dx
=

−δ()∇·

q ∇
∥∇∥

, 

.
Figure 8.7 Results from level set evolution showing different iterations for a single face (see also
Plate 12).

240
8 MANIFOLDS, ACTIVE MODELS, AND DEFORMABLE TEMPLATES
The steepest descent direction of Hb() is given by
 = δ()∇·

q ∇
∥∇∥

= δ()
∇q · ∇
∥∇∥
+ q∇·
 ∇
∥∇∥

.
Example 8.51 (Xiao Han)
Here are results from Xiao Han where he studied level
set implementations in R2, R3. Here the examples are restricted to the plane. Han’s
implementations have the smoothing boundary term with q = 1, so that ∇q = 0 and
the evolution follows the simpler form
t = (µ1 −µ2)(2I(x) −µ1 −µ2)∥∇∥+ q∇·
 ∇
∥∇∥

∥∇∥
(8.105)
= (µ1 −µ2)(2I(x) −µ1 −µ2)∥∇∥+ qκ∥∇∥.
(8.106)
Shown in Figure 8.7 are results from Han’s algorithm showing level set
evolutions as a function of iteration number.
8.7 Gaussian Random Field Models for Active Shapes
Model the interiors of the shapes of real-valued images as scalar random ﬁelds deﬁned on the
index sets of integer lattices. Because of splitting, the partitions are conditionally independent
given their boundaries. The Bayesian approach presents a substantial computational hurdle. Since
the normalizing partition function must be calculated for each partition D(gj) ⊂Zd, d = 2, 3,
j = 1, . . . , J the number of object regions. The partition function and log-normalizer must be
calculated for each shape. For this we use the asymptotic representation of the log-normalizer
via the Fourier transform of Theorem 5.17 of Chapter 5. Let ID be a real-valued Gaussian ﬁeld on
D ⊂Zd, with mean and covariance µ, KD. Model the inverse covariance via the difference operator
which is zero at the boundary of the shape so that LDL∗
D = K−1
D ; the probability density requires
the computation of the determinant of the covariance corresponding to the log-partition function
for Gaussian ﬁelds:
p(ID) = (2π)−|D|/2 det−1/2 KDe−1/2||LD(ID−µ)||2,
(8.107)
with ∥LD(X −µ)∥2 the integral-square of the ﬁeld over the domain D. Notice the role that the
partition functions play; segmentation requires computation of log KD for the random shape.
The Gaussian Markov random ﬁeld is used for segmentation of the micrographs.
Theorem 8.52
Given is the simple parametric shape model with the interior D of shape
gt, t ∈[0, 1] a random realization of a Gaussian random ﬁeld with density
p(ID) = (2π)−|D|/2 det−1/2 KDe−1/2∥LD(ID−µ)∥2,
(8.108)
with operators for the interior and exterior L1, L2 solving the difference equation
L1 Ii =

s∈S
a(1)
s Ii+s,
L2 Ii =

s∈S
a(2)
s Ii+s.
(8.109)

8.7 GAUSSIAN RANDOM FIELD MODELS FOR ACTIVE SHAPES
241
Then the 1-parameter variation of the potential H(γ ) is given by Theorem 8.42 with difference
in energies for an asymptotic in size shape
E1 −E2 ≃1
2
1
(2π)d

[−π,π]d log σ1(ω)
σ2(ω)dω + 1
2

∥L1(Is −µ1)∥2 −∥L2(Is −µ2)∥2
,
(8.110)
with σ1(ω) =

s
a(1)
s ej⟨ω,s⟩,
σ2(ω) =

s
a(2)
s ej⟨ω,s⟩.
(8.111)
Proof
The asymptotic in size partition Theorem 5.17 of Chapter 5, gives the log-
determinant covariance by the spectrum:
log det1/2 KD = |D|
2
1
(2π)d

[−π,π]d log σ(ω)dω + O(|∂D|)
with σ(ω) =

s
asej⟨ω,s⟩.
(8.112)
The energy spectrum becomes

D(γ )
E(x)dx = |D(γ )|
2
1
(2π)d

[−π,π]d log σ(ω)dω + 1
2∥L(ID(γ ) −µ)∥2.
(8.113)
Example 8.53 (Segmenting Mitochondria)
Now examine the multiple shape prob-
lem partitioning the background space as multiple object domains. Construct the
background space D ⊂Zd as a disjoint partition of simply connected regions D(gj)
with random image ﬁeld ID(gj). Conditioned on the parametric models, g1, g2, . . . , the
subregion Markov random ﬁelds are assumed conditionally independent. Write the
potential via energy densities Ej(x), x ∈D(gj) representing the log-normalizer and
quadratic data statistic according to
p(I|g1, g2, . . . ) =
J
j=1
(2π)−|D(gj)|/2 det−1/2 KD(gj)e−1/2∥L(ID(gj)−µj)∥2
(8.114)
∝
J
j=1
(2π)−|D(gj)|/2e
−

D(gj) Ej(x)dx
.
(8.115)
Clearly, if the regions corresponding to the interiors of an unknown shape which
are to be inferred the asymptotic partition approximations must be used. Notice
Ej(x), x
∈
D(gj) the energy density representing the log-normalizer using the
asymptotic representation via the Fourier transform of Theorem 5.17 of Chapter 5.
The Gaussian Markov random ﬁeld is used for segmentation of the micrographs.
Assume the mitochondrial interiors are random realization of the stochastic difference
equation for i = (i1, i2) ∈D ⊂Z2:
L (Ii −µ) = (−∇2 + a)(Ii −µ) = Wi with
(8.116)
∇2Ii1,i2 = Ii1−1,i2 + Ii1+1,i2 + Ii1,i2−1 + Ii1,i2+1 −4Ii1,i2 ,
(8.117)
where µ is the mean and a is the constant restoring force, with white Gaussian noise
variance σ 2. The Laplacian induces nearest-neighbor dependence between pixels on

242
8 MANIFOLDS, ACTIVE MODELS, AND DEFORMABLE TEMPLATES
the discrete lattice of points. The three parameters, a, µ and σ, completely specify the
model. The energy density resulting from the spectrum of the 2D Laplacian becomes
log det1/2 KDg =
|D(gj)|
2
1
4π2

[−π,π]2 log

2
2

k=1
(1 −cos ωk) + a

dω
+ O(|∂Dg|) where
(8.118)

D(g(γ ))
E(x)dx = |D(g(γ ))|
2

1
4π2

[−π,π]2 log 2πσ 2


2

k=1
(2 −2 cos ωk) + a

dω


+
1
2σ 2 ∥LID(g(γ ))∥2.
Using the asymptotic maximum-likelihood parameter estimation of Chapter 5, Example
5.26 of Theorem 5.17, parameters are estimated from sets of hand-labelled mitochondria and back-
ground cytoplasm region types. The parameters estimated from the micrograph data assuming
the Gaussian model with L = △−a having parameters of mean µ, noise power σ 2, and restoring
force a are shown in Table 8.1.
Let us use Corollary 8.42 for computing gradients of the shapes for closed curves as given
in Example 7.29 then
gt(γ ) =

x0
y0

+
 t
0

u1s
−u2s
u2s
u1s
 
−sin s
cos s

ds,
t ∈[0, 2π] ,
(8.119)
with γ ∈{u1s, u2s, x0, y0}, then
∂gs
∂s =

−u1s sin s −u2s cos s
u1s cos s −u2s sin s

,
∂gs
∂x0
=

1
0

,
∂gs
∂y0
=

0
1

(8.120)
∂gt
∂u1s
= 1≥t(s)

−sin s
cos s

,
∂gt
∂u2s
= 1≥t(s)

−cos s
−sin s

.
(8.121)
For computation there are n unique parameter vectors, {u1k, u2k}n
k=1 constant over intervals l ∈
((k−1)/n, k/n), correspondingtoapolygonalapproximationtothecircle. Then-similaritiesbecome
the scales and rotations applied to the chords, and adding global translation gives
gt= l
n =

x0
y0

+
l

k=1

u1k
u2k
−u2k
u1k
 
cos 2πk/n −cos 2π(k −1)/n
sin 2πk/n −sin 2π(k −1)/n

.
(8.122)
Table 8.1 Maximum-likelihood estimates of the parameters µ, noise variance σ 2, and restoring force as
estimated from the mitochondria data using the algorithm from Example 5.26 of Chapter 5.
µ Mean gray level
σ 2 Variance of White Noise
a restoring Force
Mitochondria
58.7
847.3
0.62
Background
130.3
2809.9
0.15

8.7 GAUSSIAN RANDOM FIELD MODELS FOR ACTIVE SHAPES
243
(1)
(2)
(3)
Figure 8.8 Panels 1, 2, and 3 show the segmentation via the closed active contour models. Mito-
chondria interiors and cytoplasmic exteriors were represented using the Gaussian random ﬁeld
asymptotic partition function. Each circular template shape was manually placed; data were taken
from the laboratory of Dr. Jeffrey Safﬁtz at Washington University.
Figure 8.8 shows examples of active model segmentation of the EM micrographs. The
active shape model is deformed to maximize the “probability” or minimize the energy by varying
the parameters of the deformable contour to match the interior compartments of mitochondria.
Examining the results it appears as if the shape models are ﬂexible enough to accomodate the
variation in organelle shapes.

9
S E C O N D O R D E R A N D G A U S S I A N F I E L D S
ABSTRACT
This chapter studies second order and Gaussian ﬁelds on the background spaces
which are the continuum limits of the ﬁnite graphs. For this random processes in Hilbert spaces
are examined. Orthogonal expansions such as Karhunen–Loeve are examined, with spec-
tral representations of the processes established. Gaussian processes induced by differential
operators representing physical processes in the world are studied.
In several of the examples studied in previous chapters the graphs have all been discrete, at most
countable. Oftentimes these graphs are representations of shapes with generators which are sub-
manifolds sewn together, with each of the groups acting only locally on submanifolds of the whole.
Then, the graph-based regularity on the transformations corresponds to the notion of constraining
the local action of these transformations so that the total manifold stays regular or connected.
For many of the examples, the number of submanifolds (generators) are chosen in some sense
arbitrarily, oftentimes based on computational convenience. However, the continuum limits are
in most applications in image processing the regular structures, not discrete sets. This important
distinction was not made systematically in the literature until the early 1980s, and is based on the
obvious observation that natural objects live in Rd-spaces, not on lattices. The lattices often appear
for computational convenience. A theory which is consistent only on lattices is not sufﬁcient since
they do not allow most of the natural invariances that are so important in the physical world. Of
course, the computations will be done discretely in the ﬁnal stage, thus intimately connecting the
continuum representations with the ﬁnite graph structures and discrete generators of previous
sections. It is our belief, however, that whenever the representations support a continuum limit, the
discrete representation which is often of implementation convenience must be consistent with that
continuum limit. For this reason, we require representations on the continuum, processes built
from products of the Lie groups which take their actions on the manifolds and sub-manifolds.
These form transformations on the continuum, requiring the study of vector and group valued
ﬁelds.
As probabilistic structures on the representations will allow us to express the variation of
natural patterns, we now move from probabilistic model representations on discrete sets to those
on the continuum. Therefore the study of random processes and random ﬁelds on both lattices as
well as the continuum becomes essential.
9.1 Second Order Processes (SOP) and the Hilbert Space of Random
Variables
First for classical measurability and continuity assumptions.
9.1.1 Measurability, Separability, Continuity
All the random processes will be examined in their coordinate representation assuming standard
measurability with respect to the underlying probability space. As the random processes begin
to be indexed on countably inﬁnite and uncountable sets, properties of measurablity, separability
and continuity become important. These are discussed in Appendix A.2 in some detail (see the
texts [212–214] for indepth development). Basic deﬁnitions are provided below for clarity. Since
244

9.1 SOP AND THE HILBERT SPACE OF RANDOM VARIABLES
245
some of the index spaces will be associated with time, we will sometimes index with the 1D time
variable t ∈T ⊂R; for multiple dimensions use x ∈X ⊂Rd.
As we extend to ﬁelds on the continuum technical conditions arise requiring the processes to
be well behaved, measurable and separable. For this reason, we deﬁne the basic measurability and
separability properties which will be assumed throughout. We shall be interested in m-dimensional
ﬁelds, real and complex Rm, Cm.
Deﬁnition 9.1
A real-valued random process Y(x), x ∈X ⊂R(Z) and is a random ﬁeld
ifX ⊂Rd(Zd)ontheprobabilityspace(, A, P)issaidtobeA-measurableif∀r ∈R, ∀x ∈X
then
{ω : Y(ω, x) ≤r} ∈A.
(9.1)
An m-vector-valued random process (ﬁeld) is A −measurable for each of the com-
ponents Y1, . . . , Ym. In particular, a complex-valued process YR + jYI is A-measurable
with respect to each of the real and imaginary components, YR, YI.
In moving from ﬁnite graphs to countable and uncountable index sets, we need to reduce
questions involving the uncountable sets associated with the continuum and countable subsets.
This is the separability program.
Suppose we were to ask about probabilities on such events as
Pr{ω : 0 ≤Y(ω, x) ≤1, ∀x ∈X ⊂R} = Pr
<
x∈X
{ω : 0 ≤Y(ω, x) ≤1}.
(9.2)
In general, this cannot be evaluated as #
x∈X{ω : 0 ≤Y(ω, x) ≤1} may not necessarily be in the
σ-algebra since X is uncountable. Now suppose the question is changed to what is
Pr{ω : 0 ≤Y(ω, s) ≤1, s ∈S} = Pr
<
s∈S
{ω : 0 ≤Y(ω, s) ≤1},
(9.3)
with S ⊂X a countable dense subset. This quantity can be computed because #
s∈S{ω : 0 ≤
Y(ω, s) ≤1} ∈A is a countable intersection of measurable sets.
The point is, for separable processes, such questions as given by Eqns. 9.2 and 9.3 are prob-
abilistically equivalent. This relies on separability allowing questions involving non-countable
unions or intersections to be reduced to questions involving countable ones; not all processes are
separable (see Example A.1 in Appendix A). More precisely, here is the deﬁnition of separability.
Deﬁnition 9.2
A set E ⊂E1 is said to be dense in E1 if for every x1 ∈E1, every open subset
of E1 containing x1 also contains an element of E.
A process (ﬁeld) Y(x), x ∈X ⊂R1(Rd) is said to be separable if there exists a countable
set S ⊂X and a set  ⊂X with P() = 0 such that for any closed set K ⊂R1 and any open
interval I the two sets
&
ω : Y(ω, x) ∈K, x ∈I
<
X
'
,
&
ω : Y(ω, x) ∈K, x ∈I
<
S
'
(9.4)
differ by a subset of .
The good news is that with continuity in probability, we can assume we work on realizations
of the process with the particular probability model which is separable. As well for this process,
any countable dense set in the time or space index is sufﬁcient on which to ask such probability
questions (e.g. see Doob [212]).
Throughout the development we assume all our processes are both measurable and separa-
ble, hence we drop such qualiﬁcations in our speciﬁcation of the process. Now for various forms
of convergence.

246
9 SECOND ORDER AND GAUSSIAN FIELDS
Deﬁnition 9.3
Various forms of convergence and continuity of the process will be
important.
1. Given are a set of P-measurable random variables Yn, n = 1, . . . , and Y. Convergence
in probability (in p.), convergence in quadratic mean (q.m.), and converge
almost surely (a.s.) for Yn to Y are deﬁned to be
∀ǫ > 0 lim
n Pr{ω : |Yn −Y| > ǫ} = 0 (in p.),
lim
n E|Yn −Y|2 = 0 (q.m.),
Pr{ω : lim
n |Yn −Y| = 0} = 1 (a.s.).
We write limn Yn
p= Y, limn Yn
q.m.
= Y, limn Yn
a.s.
= Y, respectively.
2. For a stochastic process Y(x), x ∈X on (, A, P), continuity at x in probability,
quadratic mean, and almost sure are deﬁned to be
∀ǫ > 0 , lim
x→y Pr{ω : |Y(x) −Y(y)| > ǫ} = 0,
lim
x→y E|Y(x) −Y(y)|2 = 0,
Pr{ω : lim
x→y |Y(x) −Y(y)| = 0} = 1.
For m-vector valued random processes, change the distance measure | · | to the version
∥·∥for vectors in Rd or Rm, with continuity in probability, quadratic mean, and almost
sure remaining otherwise the same.
Example 9.4 (Measurability)
Given a probability space (, A, P),  = [0, 1], A is
the Borel σ-algebra of [0, 1], and P{(a, b)} = b −a, with X = N the naturals. Let
Y(ω, n), n ∈N with Y(ω, n) = nth ordinate of the dyadic expansion (binary expansion)
of ω restricted to have an inﬁnite number of zeros:
ω
dyadic expansion of ω
0.0
0.00000000 . . .
0.25
0.01000000 . . .
0.5
0.10000000 . . .
0.75
0.11000000 . . .
This stochastic process is A-measurable because for example
{ω : Y(ω, 0) = 1} = [0.5, 1) ∈A,
{ω : Y(ω, 0) = 0} = [0, 0.5) ∈A,
{ω : Y(ω, 1) = 1} =
=
[0, 0.5)
<
[0.25, 0.5)
> ? =
[0.5, 1)
<
[0.75, 1)
>
∈A.
Example 9.5 (Separability)
The rationals are dense in the reals. In addition, the ratio-
nalsarecountable, makingtherealsseparable. Thereexistsabijectionfromthenaturals
to the rationals, with the rationals generated by the following sequence
0, 1
1, 1
2, 2
2, 2
1, 1
3, 2
3, 3
3, 3
2, 3
1, 1
4, 2
4, 3
4, 4
4, 4
3, 4
2, 4
1, 1
5 . . . .
(9.5)
Aone-to-one mapping of the naturals to the rationals is generated by assigning 1 to the
ﬁrst element above, 2 to the next, and so on, skipping any rationals that are repeated.

9.1 SOP AND THE HILBERT SPACE OF RANDOM VARIABLES
247
Let Y(ω, t), t ∈[0, 1] be a real-valued process, separable and continuous in prob-
ability. Then, using the separating set S = (∞
k=1{0, 2−k, 2 · 2−k, . . . , 2k · 2−k} which is
dense in [0, 1], the probability Pr{Y(ω, t) ≥0, 0 ≤t ≤1} can be computed:
Pr{Y(t) ≥0, 0 ≤t ≤1} = Pr

Y
 k
2n

≥0, 0 ≤k ≤2n, n = 0, 1, . . .

= Pr
∞
<
n=0

ω : Y

ω, k
2n

≥0, 0 ≤k ≤2n

) = Pr
∞
<
n=0
An,
where An = {ω : Y(ω, k/2n) ≥0, 0 ≤k ≤2n} is a decreasing sequence in n. Therefore,
by the continuity of probability,
Pr{Y(t) ≥0, 0 ≤t ≤1} = Pr{ lim
n→∞An} = lim
n→∞Pr{An}
= lim
n→∞Pr

ω : Y

ω, k
2n

≥0, 0 ≤k ≤2n

.
9.1.2 Hilbert space of random variables
There will be several Hilbert spaces which shall be worked with, in particular the Hilbert space,
HY, generated from linear combinations of random variables generated from the process Y(·).
Statements about second order representations require understanding the Hilbert space of mean
squared normed random variables and vectors.
Deﬁnition 9.6
Deﬁne L2(P) to be the set of P−measurable random m-vectors Y =


Y1
Y2
...


with inner product and ﬁnite norm for all Y, Z ∈L2(P),
E⟨Y, Z⟩= EY∗Z,
E∥Y∥2 = EY∗Y < ∞,
(9.6)
where (·)∗denotes complex transpose for vectors (for scalars use (·)∗).
For the scalar case, the norm and inner product reduce to E|Y|2, EY∗Z.
That L2(P) is a Hilbert space of random variables requires showing completeness, i.e.
quadratic mean Cauchy sequences converging in quadratic mean.
Theorem 9.7
L2(P) is a Hilbert space with norm and inner product as in Eqn. 9.6.
Proof
See Theorem B.5 in Appendix B.1 for the proof.
Second order processes and the Hilbert space constructed from their linear combinations
are as follows.
Deﬁnition 9.8
1. Deﬁne Y(x), x ∈X ⊂R(Rd) to be a second order process (ﬁeld) with respect to
P if for all x ∈X, Y(x) ∈L2(P).
2. Given a second order process (ﬁeld) Y(x), x ∈X ⊂R(Rd) then Z is said to be a
random variable derived from a linear operation on Y(·) if Z is a random variable
expressible in the form
Z(ω) =
N

n=1
αnY(ω, xn)
or if Z(ω)
q.m.
=
lim
N→∞
N

n=1
αnY(ω, xn) .
(9.7)

248
9 SECOND ORDER AND GAUSSIAN FIELDS
Theorem 9.9
For Y(x), x ∈X ⊂Rd second order with respect to P, let HY denote the set
of all random variables derived from linear operations on Y(·).
Then HY is a Hilbert space. (Furthermore, it is a Hilbert subspace of the Hilbert space
L2(P).)
Proof
Since Y(·) is second order then for all t ∈T(x ∈X), Y(t)(Y(x)) ∈L2(P).
Because L2(P) is a vector space, ﬁnite linear combinations are also in L2(P), so Z(ω)
of the ﬁrst form Eqn. 9.7 are in L2(P). Then recall that q.m. convergence of a sequence
of random variables corresponds to convergence in the norm of L2(P), so Z(ω) of the
second form Eqn. 9.7 are also in L2(P) because it is complete. Thus, HY ⊂L2(P).
Now let Z1(ω), Z2(ω) ∈HY and a ∈C. By deﬁnition there exist sequences Z(N)
1
=

N
n=1 αnY(ω, xn) and Z(N)
2
= 
N
k=1 βkY(ω, xk) which converge in quadratic mean to
Z1 and Z2, respectively, (if Z1 or Z2 is derived from ﬁnite linear combinations then for
large n, k take αn = βk = 0 and arbitrary xk, xn). Clearly, aZ(N)
1
+Z(N)
2
is in HY for each
N, and is a ﬁnite linear combination derived from Y(·). By continuity of addition and
scalar multiplication aZ(N)
1
+Z(N)
2
converges to aZ1 +Z2 in quadratic mean. Therefore
aZ1 + Z2 ∈HY by deﬁnition and we thus conclude that HY is a vector subspace of
L2(P). Furthermore, since Z was arbitrary and Z(N)
1
→Z1 in quadratic mean, HY
is closed with respect to the norm inherited from L2(P). It is left to show that HY is
complete. Let {Z(n)
1 } be a Cauchy sequence in HY. Then Z(n)
1
is also Cauchy in L2(P),
and since L2(P) is complete Z(n)
1
converges to a random variable, say Z1, which must
belong to HY since HY is closed. Thus, HY is a complete vector subspace with the
norm and inner product inherited from L2(P).
Having a complete orthonormal (CON) basis21 for generating random variables in the
Hilbert space is very useful. Therefore notions of basis representation and Gram-Schmidt orthogo-
nalization are important. Now clearly, if F = {Zk} is a countable CON set in HY then every Y ∈HY
has the property Y
q.m.
= limn→∞

n
k=1⟨Zk, Y⟩Zk (q.m. convergence and convergence in HY are
the same). But can a countable CON basis in HY always be found? As proved in the appendix,
Theorem B.6, if a second order ﬁeld Y(x), x ∈X is q.m. continuous and S ⊆X is dense in X then the
set of all random variables derived from linear operations on Y(x), x ∈S is dense in HY. That is, for
all Z(ω) ∈HY, Z(ω)
q.m.
= limn→∞˜Zn(ω)
q.m.
= limn→∞

Nn
k=1 αn,kY(ω, ˜xn,k) , where for all n, ˜xn,k ∈S.
Thus, supposing X is separable and Y(·) is q.m. continuous, then theorem 9.9 and appendix
B.6 establish the existence of a countable basis for HY, so any CON set in HY must be countable. One
may be constructed using the Gram–Schmidt procedure on the countable basis set guaranteed to
exist by the previous theorem. Thus, given these conditions, we can express any random variable
in the Hilbert space HY in the form of an orthogonal expansion. Since we are often interested in
cases where X ⊂Rd which is separable, these expansions will be useful to us.
Example 9.10
Here is an example illustrating a 2D Hilbert space of random variables
HY generated by the random process Y(·). Let
Y(t) = Y0(ω) cos 2πt −Y1(ω) sin 2πt,
t ∈[0, 1]
(9.8)
with E|Y0|2 = E|Y1|2 = 1, and EY0Y∗
1 = EY∗
0Y1 = 0. The covariance function for this
random process is K(t, s) = EY(t)Y∗(s) = cos 2π(t−s). The Hilbert space HY generated
by the given process is the space of all random variables derived from linear operations
on Y(·). This space is 2D; it has the random variables Y0, Y1 as a basis. The basis is
orthonormal since E|Y0|2 = E|Y1|2 = 1, and EY0Y∗
1 = EY∗
0Y1 = 0. That HY is two
dimensional is proved in Exercise F.7.45.
21 An orthonormal sequence in a Hilbert space is said to be a complete orthonormal (CON) basis if the
closed subspace it generates is the whole space. The only vector orthogonal to each vector in the basis is the 0
vector.

9.1 SOP AND THE HILBERT SPACE OF RANDOM VARIABLES
249
9.1.3 Covariance and Second Order Properties
Smoothness of the processes is required which can be studied through the second order statistics
summarized via the covariance function of the process [9]. We examine the scalar case more
intensely.
Deﬁnition 9.11
Let Y =


Y1
Y2
...

on X ⊂Rd(Zd) be a second order m-vector process. The
associated covariance function for ﬁelds Y on X ⊂Rd, Zd is deﬁned to be
KY : X × X →Cm × Cm
where KY(x, y) = EY(x)Y(y)∗.
For the scalar case, the KY is deﬁned with the hermitian transpose symbol (·)∗meaning
complex conjugation for non vectors.
Stationarity of the second order (wide-sense) will be used extensively.
Deﬁnition 9.12
A second-order zero-mean m-vector random process (ﬁeld) Y =


Y1
Y2
...


on X ⊂Rd(Zd) is wide-sense stationary if its covariance function is only a function of the
time difference:
EY(x)Y∗(y) = K(x −y) .
(9.9)
Properties of the covariance determine continuity and differentiability properties.
Theorem 9.13
(i) A scalar second order random ﬁeld Y(x), x ∈X ⊂Rd with d ≥1 is quadratic mean
continuous at x ∈X, if and only if KY is continuous at (x, x).
(ii) If Y(x) is quadratic mean continuous then KY(x, y) is continuous at every point (x, y)
in X × X.
(iii) For Y(x), x ∈X ⊂Rd an m-vector ﬁeld, then (i),(ii) hold.
Proof
Proof (i) if: Assume K = KY is continuous at (y, y). Then
lim
x→yE|Y(x) −Y(y)|2
= lim
x→y(EY(x)Y(x)∗−EY(x)Y(y)∗−EY(y)Y(x)∗+ EY(y)Y(y)∗)
= lim
x→y(K(x, x) −K(x, y) −K(y, x) + K(y, y) + K(y, y) −K(y, y))
= lim
x→y(K(x, x) −K(y, y)) −lim
x→y(K(x, y) −K(y, y))
−lim
x→y(K(y, x) −K(y, y))
= 0 by continuity of K at (x, x) .

250
9 SECOND ORDER AND GAUSSIAN FIELDS
(i) only if: By the Cauchy-Schwarz inequality |EYZ∗|2 ≤E|Y|2E|Z|2 implying
|K(x1, x2) −K(y, y)| = |EY(x1)Y(x2)∗−EY(y)Y(y)∗|
= |E(Y(x1) −Y(y))Y(x2)∗+ EY(y)(Y(x2) −Y(y))∗|
≤

E|Y(x1) −Y(y)|2E|Y(x2)|2
+

E|Y(y)|2E|Y(x2) −Y(y)|2.
Taking limits as (x1, x2) →(y, y) in X × X, the right side goes to zero by the
q.m. continuity of Y(y) at t. Thus, K is continuous at (y, y).
(ii) : Consider (x1, x2) →(y1, y2). Then the exact same derivation as in the proof
of (i) only if yields
|K(x1, x2) −K(y1, y2)| ≤

E|Y(x1) −Y(y1)|2E|Y(x2)|2
+

E|Y(y1)|2E|Y(x2) −Y(y2)|2.
(9.10)
Taking limits as (x1, x2) →(y1, y2) in X × X, the right side goes to zero by the
q.m. continuity of Y(y) at y1 and at y2. Thus, K is continuous at (y1, y2) for
every (y1, y2) ∈X.
(iii) vector case if: For the vector case, if K is continuous at (x, x) then
lim
x→yE∥Y(x) −Y(y)∥2
= lim
x→y EY(x)∗Y(x) −EY(x)∗Y(y) −EY(y)∗Y(x) + EY(y)∗Y(y)
= lim
x→y
m

i=1
[EYi(x)Yi(x)∗−EYi(x)Yi(y)∗−EYi(y)Yi(x)∗+ EYi(y)Yi(y)∗]
=
m

i=1
[Kii(x, x) −Kii(x, y) −Kii(y, x) + Kii(y, y)].
Since Kii is continuous at (y,y) for each i = 1, . . . , m, the right-hand limit equals
0 by the same argument as in the scalar case.
(i) only if and (ii) for the vector case are proved identically as above just
componentwise.
Example 9.14 (Covariance and Differentiation)
Covariance properties on deriva-
tives in the quadratic mean sense imply continuity almost surely. To illustrate, for
example, a scalar SOP Y(t), t ∈[0, 1] is q.m. differentiable,
lim
h→0
Y(t + h) −Y(t)
h
q.m.
= Y′(t) ∈L2(P) ,
(9.11)
if and only if the covariance K(t, s) has a generalized second derivative:
∂2K(t, s)
∂t∂s
= lim
h→0 lim
k→0
K(t + h, s + k) −K(t + h, s) −K(t, s + k) + K(t, s)
hk
.
(9.12)

9.1 SOP AND THE HILBERT SPACE OF RANDOM VARIABLES
251
The if part follows from if Y(t) is q.m. differentiable, then deﬁning Yh(t) = (Y(t + h) −
Y(t)/h) then
lim
h→0,k→0
K(t + h, s + k) −K(t + h, s) −K(t, s + k) + K(t, s)
hk
=
lim
h→0,k→0 EYh(t)Y∗
k (s) (a)
= E|Y′(t)|2.
(9.13)
with (a) following from continuity of the inner product. If the second derivative of the
covariance exists then Y(t), t ∈[0, 1] has E|Y′(t)|2 < ∞. Similarly the reverse direction
follows as well. If E|Y′(t)|2 < ∞then the second derivative of covariance exists.
The only if converse part follows if the generalized derivative exists, then
EYh(t)Yk(t)∗→limit c, so that
E|Yh(t) −Yk(t)|2 = E|Yh(t)|2 −EYh(t)Yk(t)∗−EYk(t)Yh(t)∗+ E|Yk(t)|2 →0.
(9.14)
Since this Cauchy condition holds, the limit of Yh(t) exists as an element in L2(P), and
the proposition is true.
In section 9.10 almost-sure continuity of sample-paths is examined more
completely.
9.1.4 Quadratic Mean Continuity and Integration
Computing integrals of the random process are linked to the quadratic mean continuity. Examine
the random process case with X ⊂R1 closed bounded. Associated with the index sets X ⊂Rd
there will be a Hilbert space of square-integrable functions, L2(X), norm ∥· ∥2. Throughout, the
random processes will have a basic covariance property, essentially a ﬁnite mean-square property,
which will imply the existence of linear functionals of the process. Assuming the processes Y(·) are
quadratic mean continuous, then the covariance is continuous along the diagonal. Clearly, index
set X compact, then

X K(x, x)dx < ∞, since K must be bounded from its continuity. This is the
so-called trace-class property (see discussion below, deﬁnition 9.30).
That the integral is well deﬁned in the sense that the limit is independent of the choice of
sequence of partitions has still to be established. For this, examine the joint measurability with
respect to both  as well as X.
Deﬁnition 9.15
For processes Y(ω, x), x ∈X on (, A, P) for which X is Lebesgue mea-
surable with measure m, then the process is said to be L ⊗A measurable with respect to
m × P on the product topology of X ×  (the open sets generated from the topologies L, A) if
∀r ∈R,
{(ω, x) : Y(ω, x) ≤r} ∈L ⊗A.
(9.15)
Joint measurability allows the understanding of Y(ω, x), x ∈X, ω ∈ in several ways, along
with integrals of the type ⟨φ, Y⟩2 given by ⟨φ, Y⟩2 =

X φ(x)Y(ω, x) dx. Clearly,
E∥Y(·)∥2
2 =

×X
|Y(ω, x)|2 d(P(ω) × m(x))
(9.16)
and joint measurability allows for the swapping of the order of integration via Fubini’s theorem.
From the second order process condition on bounded domains X implies the trace class condition
E∥Y(ω, ·)∥2
2 < ∞which with joint measurability and Fubini’s theorem implies P(·) almost surely,

252
9 SECOND ORDER AND GAUSSIAN FIELDS

X Y2(·, t) dt < ∞so that Y ∈L2(X) almost surely. Thus P(·) almost surely,

X |Y(·, t)| dt < ∞
giving Y ∈L1(X). For orthogonal expansions, integrals of the random process are computed
against functions φ ∈L2 and existence of the integrals as quadratic mean limits will be required.
Then

X Y(·, t)ej2πftdt exists almost surely and integrals ⟨φ, Y⟩2 well deﬁned. Being able to compute
projections onto eigenfunctions of random processes will be used quite often.
Example 9.16 (Integrals in q.m.)
Deﬁning the partition Tn = {a = t(n)
0
< t(n)
1
< · · ·
< t(n)
n } with the sum deﬁned as
In(Y, φ) =
n

i=1
Y(t(n)
i
)φ(t(n)
i
)(t(n)
i
−t(n)
i−1) .
Clearly, In(Y, φ) ∈HY and the q.m. limit is as well I(Y, φ)
q.m.
= limn→∞In(Y, φ) ∈HY
since
lim
n→∞E|In(Y, φ)|2 ≤lim
n→∞
n

i=1
E|Y(t(n)
i
)|2(t(n)
i
−t(n)
i−1)
n

i=1
|φ(t(n)
i
)|2(t(n)
i
−t(n)
i−1)
=

|φ(t)|2 dt

K(t, t) dt < ∞.
(9.17)
Example 9.17
Computing the expectation of an integral of the process involves joint
measurability.
Brownian motion is an example of an L ⊗A measurable. Let
W(n)(ω, t) = W(ω, k2−n),
k2−n ≤t < (k + 1)2−n,
(9.18)
where k = 0, 1, 2, . . . , 2n −1. Then, W(n)(ω, t) is measurable because
{(ω, t) : W(n)(ω, t) ≤x} =



2n−1
?
k=0
[k2−n, (k + 1)2−n)



× {ω : W(ω, k2−n) ≤x, k = 1, 2, . . . , 2n −1}.
Since a countable union of measurable sets is measurable, the set {(ω, t) : W(n)(ω, t)
≤x} ∈L ⊗A is measurable.
9.2 Orthogonal Process Representations on Bounded Domains
The SOP processes which are studied associated with bounded domains have covariances which
are supported on closed and bounded sets and are therefore compact as operators. This implies
they have a discrete set of eigenvalues and associated orthogonal expansion. This is important as
various orthogonal process representations such as Mercer’s representation and Karhunen-Loeve
expansions.

9.2 ORTHOGONAL PROCESS REPRESENTATIONS
253
9.2.1 Compact Operators and Covariances
For this we follow Reed and Simon’s [215] particularly incisive development. Assume operators
A : F →G on Banach spaces 22 F and G with norms ∥· ∥F, ∥· ∥G. In almost all cases these are
Hilbert spaces denoted by HF, HG, with associated inner products ⟨·, ·⟩F, ⟨·, ·⟩G. Then the bounded
operators taking their action 23 on these spaces are what shall be studied.
Deﬁnition 9.18
The operator norm ∥A∥is deﬁned to be
∥A∥= sup
f∈F
∥Af∥G
∥f∥F
=
sup
f∈F:∥f∥F=1
∥Af∥G ;
(9.19)
the space of bounded norm, linear operators are denoted as B(F, G); when F = G the
bounded operators are denoted as B(F).
The operators will be expanded via their eigenfunctions, which for the self-adjoint and
normal operators will be orthogonal. All of the covariances will be self-adjoint, and differential
operators will be normal.
Deﬁnition 9.19
1. Let A ∈B(H); then φ ̸= 0 ∈H is an eigenfunction of A if it satisﬁes Aφ = λφ for
some eigenvalue λ ∈C. The eigen-spectrum σ(A) are the set of eigenvalues.
2. The adjoint operator A∗of A ∈B(H2, H1) is the unique operator such that
⟨Af, g⟩H2 = ⟨f, A∗g⟩H1
∀f ∈H1, ∀g ∈H2 .
(9.20)
When the operators A are self-adjoint A = A∗mapping from one Hilbert space to
itself.
3. A normal operator A is one for which A∗A = AA∗.
For these cases the eigenfunctions are orthogonal.
Lemma 9.20
(i) For A self adjoint, the eigenvalues {λ} are real with eigenfunctions {φ} orthogonal.
(ii) For A normal, the eigenfunctions are orthogonal and if A has eigenelements (λ, φ) then
the operator A∗has eigenelements (λ∗, φ).
Proof
Part (i): For A self-adjoint, the eigenvalues are real valued:
⟨Aφ1, φ1⟩= λ∗∥φ1∥2 = ⟨φ1, Aφ1⟩= λ∥φ1∥2 .
(9.21)
The eigenfunctions φ1, φ2 corresponding to distinct non-zero eigenvalues λ1 ̸= λ2, are
orthogonal:
⟨Aφ1, φ2⟩= ⟨λ1φ1, φ2⟩
= ⟨φ1, A∗φ2⟩(a)
= ⟨φ1, Aφ2⟩= ⟨φ1, λ2φ2⟩,
(9.22)
22 A Banach space is a normed vector space that is complete with respect to the norm metric.
23 The operators A : f ∈F →Af ∈G will most often deﬁne mappings between Hilbert spaces. For example
L2([0, 1]), A will have a kernel representation A = (A(x, y)) on [0, 1]2, with the action deﬁned through the
kernel, Af(x) =
 1
0 A(x, y)f(y) dy. For l2, the kernel (Aij) is on Z2 and Afi = 
j∈Z Aijfj; for Rn its an n×n matrix
kernel A = (Aij) with Afi = 
n
j=1 Aijfj.

254
9 SECOND ORDER AND GAUSSIAN FIELDS
with (a) following from the self-adjoint property. This implies ⟨φ1, φ2⟩= 0 if λ1 ̸= λ2 ̸=
0. Part (ii): If the normal operator A ∈L(H) has eigenelement (λ, φ) then the operator
A∗has the eigenelement (λ∗, φ). This follows since Aφ = λφ, λ ∈C for normal operator
A, i.e. ∥(A −λ id)φ∥= 0 giving
∥(A −λid)φ∥2 = ⟨(A −λ id)φ, (A −λ id)φ⟩= ⟨(A −λ id)∗(A −λ id)φ, φ⟩
(9.23)
= ⟨(A −λ id)(A −λ id)∗φ, φ⟩= ⟨(A −λ id)∗φ, (A −λid)∗φ⟩
(9.24)
= ∥(A −λ id)∗φ∥2 = 0,
(9.25)
which implies A∗φ = λ∗φ. That the eigenfunctions are orthogonal for distinct
eigenvalues,
λ∗
1⟨φ1, φ2⟩= ⟨λ1φ1, φ2⟩= ⟨Aφ1, φ2⟩
(9.26)
= ⟨φ1, A∗φ2⟩= ⟨φ1, λ∗
2φ2⟩= λ∗
2⟨φ1, φ2⟩,
(9.27)
giving λ∗
1⟨φ1, φ2⟩−λ∗
2⟨φ1, φ2⟩= (λ∗
1 −λ∗
2)⟨φ1, φ2⟩= 0. Since λ1 ̸= λ2, this implies
that ⟨φ1, φ2⟩= 0, hence proving orthogonality for eigenfunctions φ1, φ2 of a normal
operator A.
Thus, for self-adjoint bounded linear operators, A = A∗∈B(H), then the eigen-spectrum is
real σ(T) ⊂R and bounded since
sup
λ∈σ(T)
|λ| =
sup
λ∈σ(T)
∥Aφλ∥
∥φλ∥< ∥A∥< ∞.
(9.28)
The subclass of bounded, linear operators, those which are compact, will play an important
role in the process representations. One of the most useful properties of compact operators is that
their spectrum is discrete. It is the most important property of compact operators that they can
essentially be represented via the superposition of a ﬁnite rank operator and a remainder which
has arbitrarily small norm.
Deﬁnition 9.21
An operator A ∈B(F, G) is compact if A takes bounded subsets of F into
precompact sets in G. Equivalently, A is compact if for every bounded sequence {fn} ⊂F,
{Afn} has a subsequence convergent in G.
There are two kinds of operators which are compact for which we shall exploit the discrete-
ness of their spectrum for orthogonal expansions. These are (i) ﬁnite rank operators which can be
identiﬁed with matrices, for example, and (ii) continuous covariances which support eigenfunction
expansions through Karhunen-Loeve.
Theorem 9.22
1. Let A be a ﬁnite dimensional operator on L2 with norm ∥· ∥2 and inner product ⟨·, ·⟩2,
then A : L2 →L2 according to
Af =
n

k=1
αk⟨φk, f⟩2φk ,
(9.29)
where φk is a CON basis and αk ∈C. Then A is compact.

9.2 ORTHOGONAL PROCESS REPRESENTATIONS
255
2. Let A be a continuous operator with kernel A(t, s), (t, s) ∈[0, 1]2 mapping square-
integrable functions to square-integrable functions, A
:
L2[0, 1]
→
L2[0, 1],
according to
Af(t) =
 1
0
A(t, s)f(s) ds.
(9.30)
Then A is compact. Similarly for ﬁelds, A(x, y), (x, y) ∈X2, X ⊂Rd.
Proof
For the proof, see Appendix section C, Theorem C.3.
Now it follows that such compact operators can be written as the superposition of ﬁnite
rank approximations. Self-adjoint operators (such as covariances) have ﬁnite rank approximations
constructed from the span of their eigenvectors.
Theorem 9.23
Riesz-Schauder and Hilbert-Schmidt Theorem
(a) Let A be compact in B(H). Then σ(A) is a discrete set of eiegenvalues having no limit
points except possibly λ = 0. Any λ ∈σ(A) is an eigenvalue, with the corresponding
space of eigenvectors of at most ﬁnite dimension.
(b) Let A be compact in B(H), and {φk} an orthonormal set in H. Then
A = lim
n→∞
n

k=1
Aφk⟨φk, ·⟩.
(9.31)
(c) Let A be self adjoint and compact on B(H). Then the set of eigenvectors {φk} satisfying
Aφk = λkφk are a C.O.N. basis for H with λk →0 as k →∞, and
A = lim
n→∞
n

k=1
λkφk⟨φk, ·⟩.
(9.32)
Here the limit is in the operator norm sense limn→∞supf ∥Af −
n
k=1 λk⟨φk, f⟩φk∥= 0.
Proof
For proof see the Appendix section C.2, Theorem C.4.
This gives the basic characterization of self adjoint and compact operators which we will use
in numerous places for L2 representations of stochastic processes on compact domains familiar to
the communication engineer.
Theorem 9.24 (Mercer’s Theorem )
(1909) Consider a symmetric (Hermitian) nonnega-
tive deﬁnite operator K with continuous kernel K(x, y) on X × X for X ⊂Rd, d ≥1 with
eigenelements {λk, φk} satisfying the integral equation λφ(x) =

X K(x, y)φ(y)dy then
K(x, y) =
∞

k=1
λkφk(x)φ∗
k (y), converging absolutely and uniformly.
(9.33)
Similarly for vector ﬁelds with eigenelements {λk,


φ1k
φ2k
...

} and m × m matrix
covariance K(x, y), x, y ∈X.

256
9 SECOND ORDER AND GAUSSIAN FIELDS
Proof
K is continuous and compact by Theorem 9.22. It is also self-adjoint, for by
swapping order of integrals (Fubini’s theorem) gives
⟨f, Kg⟩=

f∗(y)Kg(y) dy =

f∗(y)

K(y, x)g(x) dx dy
(9.34)
=
 
f∗(y)K(y, x) dyg(x) dx =
  
K∗(y, x)f(y) dy
∗
g(x) dx
(9.35)
=
  
K(x, y)f(y) dy
∗
g(x) dx = ⟨Kf, g⟩.
Thus, by the Hilbert–Schmidt theorem, {φi} forms a C.O.N. basis of L2(X). Since K(·, ·) ∈
L2(X × X), by Fubini’s theorem K(·, y) is in L2(X) for almost all y. Thus,
K(·, y) =
∞

i=1
⟨φi(·), K(·, y)⟩φi(·) =
∞

i=1
λiφi(·)φ∗
i (y) ∈L2(X) ,
(9.36)
Also, by the triangle inequality we have
:::K(·, y) −
n

i=1
λiφi(·)φ∗
i (y)
::: ≤∥K(·, y)∥+
:::
n

i=1
λiφi(·)φ∗
i (y)
:::
(9.37)
=
n

i=1
λi|φ∗
i (y)| ≤2∥K(·, y)∥,
(9.38)
and since 4∥K(·, y)∥2 = 4

|K(x, y)|2dx ∈L1(X) by Fubini’s theorem, we have
lim
n→∞
  K(x, y) −
n

i=1
λiφi(x)φ∗
i (y)

2
dx dy
=

lim
n→∞
  K(x, y) −
n

i=1
λiφi(x)φ∗
i (y)

2
dx

dy = 0
bythedominatedconvergencetheorem. Hence K(·, ·) = 
∞
i=1 λiφi(·)φ∗
i (·)inL2(X×X).
Consider the remainders
Kn(x, y) = K(x, y) −
n

i=1
λiφi(x)φ∗
i (y)
for n = 1, . . . . By 9.22, Kf(x) is a continuous function for all f ∈L2. In particular, for
all λi ̸= 0, the corresponding eigenfunctions, φi are continuous and thus it follows
that the partial sums of the series are continous, and hence so is Kn(·, ·). By Fubini’s
theorem we have ⟨f, Knf⟩= ⟨f(·)f∗(·), Kn(·, ·)⟩, where the second inner product is the
one associated with the joint product space, and since Kn(x, y) = 
∞
i=n+1 λiφi(x)φ∗(y)
in L2(X × X), it follows from continuity of the inner product that
 
Kn(x, y)f(y)f∗(x) dx dy =
∞

i=n+1
 
λiφi(x)φ∗
i (y)f(y)f∗(x) dx dy
=
∞

i=n+1
λi|⟨φi, f⟩|2 ≥0.

9.2 ORTHOGONAL PROCESS REPRESENTATIONS
257
Thus, Kn(·, ·) is also nonnegative deﬁnite. This implies that Kn(x, x) ≥0, for otherwise
Kn(s0, s0) < 0 for some (s0, s0), and by continuity of Kn(·, ·) we must have Kn(x, y) < 0
in some neighborhood N of (s0, s0). Setting f(x) = 1 for all x such that (s, s0) ∈N and
setting f(x) = 0 elsewhere, we ﬁnd ⟨Knf, f⟩< 0 which contradicts the non-negative
deﬁnite property of Kn(·, ·). So, by the positivity of Kn(x, x) and by the deﬁnition of
Kn(·, ·), we have 
∞
i=1 λi|φi(x)|2 ≤K(x, x). Now, let M = maxx∈X K(x, x). By Cauchy’s
inequality we have

m

i=n
λiφi(x)φ∗
i (y)

2
≤
m

i=n
λi |φi(x)|2
m

i=n
λi
φi(y)
2 ≤M
m

i=n
λi |φi(x)|2 .
(9.39)
Therefore, 
∞
i=1 λiφi(x)φ∗
i (y) converges uniformly in y for every ﬁxed value of x, and
thus the series converges to a continuous function of y, say R(x, y). We have

R(x, y)f(y) dy =
∞

i=1
λiφi(x)

φ∗
i (y)f(y) dy =

K(x, y)f(y) dy,
for all continuous functions f, where the ﬁrst equality follows from uniform con-
vergence and the second by the Hilbert-Schmidt theorem. In particular, for f(y) =
(R(x, y) −K(x, y))∗we have

|R(x, y) −K(x, y)|2 dy = 0,
and thus we conclude that for ﬁxed x, R(x, y) = K(x, y) for almost all y. Thus K(x, x) =
R(x, x) = 
∞
i=1 λi|φi(x)|2, and in fact the series is uniformly convergent by Dini’s
Theorem24 since K(x, x) and the partial sums of the series are continuous. Finally,
from (9.39) we now have joint uniform convergence in x and y.
Vector case: See homework problem F.7.46 for proof based on introduction of
Frobenius norm on matrices ⟨K, K′⟩= tr K∗K′.
9.2.2 Orthogonal Representations for Random Processes and Fields
Many of the processes examined in the pattern theoretic representations are quadratic mean con-
tinuous, and we are now in a position to state the Karhunen-Loeve expansion of second order
quadratic mean continuous processes. Not surprisingly, since the covariance kernel is continuous
on a compact interval, its discrete spectrum plays the fundamental role in the expansion.
Theorem 9.25
(Karhunen-Loeve Expansion) Let Y(ω, x), x ∈X ⊂Rd, d ≥1, be a q.m.
continuous, second order ﬁeld with continuous covariance kernel K(y, x). Then {λk, φk} are
the eigenelements satisfying

X
K(y, x)φk(x) dx = λkφk(y)
if and only if Y(y)
q.m.
=
lim
n→∞
n

k=1
Ykφk(y)
(9.40)
uniformly in y ∈X, with Yk =

X φ∗
k (y)Y(y) dy orthogonal random variables according to
EY∗
k Yk′ = λkδ(k −k′).
24 Dini’s Theorem If {fj} is a sequence of non-negative continuous functions on a compact set T such that
f1(x) ≤f2(x) ≤. . . for all x ∈X, and limj→∞fj = f is continous then {fj} converges uniformly.

258
9 SECOND ORDER AND GAUSSIAN FIELDS
The m-vector case is identical as above with Y(ω, x) =


Y1(ω, x)
Y2(ω, x)
...

, x ∈X ⊂Rd,
and the continuous covariance an m×m matrix K(x, y) = (Kij(x, y)), with m×1 eigenvectors



λk, φk(x) =


φ1k(x)
φ2k(x)
...





.
Proof
If the {φk} are eigenfunctions of Eqn. 9.40 then
E

Y(y) −
n

k=1
Ykφk(y)

2
= K(y, y) −EY∗(y)
n

k=1
Ykφk(y) −E
n

k=1
Y∗
k φ∗
k (y)Y(y)
+
n

k=1
n

k′=1
EY∗
k Yk′φ∗
k (y)φk′(y)
= K(y, y) −
n

k=1
λkφk(y)φ∗
k (y)
(9.41)
which goes to zero as n →∞uniformly in y by Mercer’s theorem.
Conversely, suppose Y(y) has the orthogonal expansion of Eqn. 9.40, then
EY(y)Y∗(x) = 
∞
k=1 λkφk(y)φ∗
k (x) giving

X
K(y, x)φm(x) dx =

X
∞

k=1
λkφk(y)φ∗
k (x)φm(x) dx
(9.42)
(a)
=
∞

k=1
λkφk(y)

X
φ∗
k (x)φm(x) dx (b)
= λmφm(y),
(9.43)
with (a) following from uniform convergence and (b) from the orthogonality of the
functions {φk}, completing the proof.
The vector case is proved in Homework problem F.7.
Notice, continuity of the covariance implies boundedness over the compact interval giving
that the process is of trace class 
∞
k=1 λk =

X K(y, y) dy < ∞. Also, in the vector case q.m.
continuity and ﬁnite trace corresponds to

X tr K(x, x) dx < ∞.
For the vector case, block orthogonal expansions are useful owing to incremental expan-
sion of the covariance and random processes generating vectors of random variables which are
independent across vectors, but correlated within a vector. See Homework problem F.7.
9.2.3 Stationary Periodic Processes and Fields on Bounded Domains
Examples of processes on bounded domains which are used often in the study of patterns are
periodic processes, processes on the sphere and others. It is well known from communications
that the eigenfunctions for shift are the Fourier series implying covariances on compact domains
which are shift invariant have countable eigenfunctions for their Karhunen–Loeve representation
which are the complex exponentials.
On compact intervals, cyclo-stationarity is associated with periodicity of the process.

9.2 ORTHOGONAL PROCESS REPRESENTATIONS
259
Deﬁnition 9.26
A second-order zero-mean periodic vector ﬁeld Y(x) =


Y1(x)
Y2(x)
...

, x ∈
X = [0, 1]d, which is cyclo (wide-sense) stationary if it is periodic Y(x) = Y(x + m), m ∈
Zd which covariance a function of shift EY(y)Y∗(x) = K(y −x, 0).
Now examine periodic processes on [0, 1]d which are cyclo-stationary. Stationary on the
Torus means the covariance is Toeplitz and a function only of modulo shift t −s; this gives the
Fourier representation of the periodic covariance process with the eigenfunctions of the covari-
ance, the complex exponentials. This is Egevary’s representation for the cyclo-stationary ﬁelds. Use
ωk = 2πk to denote the scalar angular radian frequency for the kth harmonic, and ωk =
(2πk1, 2πk2, . . . ) to denote the vector version. The interpretation will be made obvious by the
context.
Corollary 9.27
For Y(ω, x), x ∈X = [0, 1]d a quadratic mean continuous periodic vector
ﬁeld with continuous covariance kernel K(x, y) = K(x −y, 0), then the Karhunen-Loeve
expansion of Theorem 9.25 holds with the eigenelements
λk =

[0,1]d K(τ, 0)e−j⟨ωk,τ⟩Rd dτ,
φk(τ) = ej⟨ωk,τ⟩Rd ,
ωk = (2πk1, 2πk2, . . . ),
k ∈Zd . .
(9.44)
Proof
Karhunen Loeve holds giving the orthogonal expansion of Eqn. 9.44. The
eigenfunctions, φk, of the covariance are the complex exponentials:

[0,1]d K(t, s)ej⟨ωk,s⟩Rd ds =

[0,1]d K(t −s, 0)ej⟨ωk,s⟩Rd ds
=

[0,1]d K(τ, 0)ej⟨ωk,(t−τ)⟩Rd dτ
(9.45)
= ej⟨ωk,t⟩Rd

[0,1]d K(τ, 0)e−j⟨ωk,τ⟩Rd dτ.
(9.46)
Remark 9.2.0 (FFT)
Notice, for the discrete periodic process case (period N), then
φk(n) = ej(2π/N)⟨k,n⟩Rd , λk = 
N
τ=1 K(τ, 0)e−j2π⟨k,τ⟩Rd with Yk = ⟨φk, Y⟩, k ∈ZN an
orthogonal process with spectrum EYkY∗
k = λk.
Example 9.28
For representing biological shapes in electron micrographs examine
Figure 9.1 (left panel) illustrating mitochondrial connected shapes with closed bound-
aries curves in R2. Stationary processes have been used as in [137, 216] with Fourier
expansions arising for representing closed contours in [185].
For closed curves of arbitrary lengths introduce scales and rotations of the form

ρ(t)
0
0
ρ(t)
 
cos θ(t)
sin θ(t)
−n sin θ(t)
cos θ(t)

=

U1(t)
U2(t)
−U2(t)
U1(t)

.
(9.47)
Curves are generated by applying these to the tangent vectors of a template circle
giving the closed curves f(t), t ∈[0, 1], f(0) = f(1) according to
f(m) = 2π
 m
0

U1(t)
U2(t)
−U2(t)
U1(t)
 
−sin 2πt
cos 2πt

dt +

x0
y0

,
m ∈[0, 1] .
(9.48)

260
9 SECOND ORDER AND GAUSSIAN FIELDS
0
500
1000
1500
2000
0
10
20
30
40
50
60
(1)
(2)
(3)
(4)
Figure 9.1 Top row: Panel 1 shows an electron-micrograph of rat-heart myocyte containing a
linear organelle at 20,000× magniﬁcation taken from the laboratory of Jeffrey Safﬁtz of the
department of Pathology at Washington University. Panel 2 shows the average mitochondria
generated from 41 micrographs and hand tracing 497 mitochondria. Bottom row: Panel 3
shows the spectrum of the covariance of the stationary Gaussian prior on the deformations
of the mitochondria. Panel 4 shows 8 mitochondria generated from the prior distribution
placed uniformly in the scene.
Now use Egevary’s result Corollary 9.27 in the block form for representing the
random 2-vector process on the Torus {U(t) =
 U1(t)
U2(t)

∈R2, t ∈T = [0, 1]}. Model
the process U(·) as cyclo-stationary with 2 × 2 covariance matrices which are Toeplitz
K(t, s) = K(t −s, 0) = EU(t)U(s)∗.
Periodicity implies the expansion of the random vector ﬁelds U(t), t ∈[0, 1]
in the C.O.N. block diagonalizing basis constructed from the complex exponentials
φ(1)
k
=
 1
0

) *+ ,
1(1)
ej2πkt, φ(2)
k
=
 0
1

) *+ ,
1(2)
ej2πkt giving the orthogonal expansion

U1(t)
U2(t)

q.m.
=

k∈Z

U1k
U2k

ej2πkt, where
U1k =
 1
0 U1(t)e−j2πkt dt
U2k =
 1
0 U2(t)e−j2πkt dt
.
(9.49)
Notice Uk =

U1k
U2k

∈C2 are complex orthogonal vector process (Homework
problem F.7,
Corollary F.2) with spectrum EUkU∗
k′
=
kδ(k −k′),
and

9.2 ORTHOGONAL PROCESS REPRESENTATIONS
261
k =
 1
0 K(t)e−j2πkt dt. Discretizing to n unique scale, rotation groups gives piece-
wise polygonal curves f(l/n), l = 0, 1, . . . , n −1, with f(0) = f(1) and keeping the
scale/rotation parameters over constant sampling intervals
f
 l
n

=

x0
y0

+
l

i=1

U1(i)
U2(i)
−U2(i)
U1(i)
 
cos(2πi/n) −cos(2π(i −1)/n)
sin(2πi/n) −sin(2π(i −1)/n)

.
(9.50)
The Toeplitz covariance has n, 2 × 2 blocks K(k, j)
=
EU(k)U(j)∗
=
K(k −j, 0); the block diagonalizing transformation is the block DFT matrix. Then
Uk = 
n
i=1 U(i)e−j2πik/n is a block-orthogonal process with 2 × 2 block covariances
k =
 λ11k
λ∗
21k
λ21k
λ22k

= 
n
i=1 e−j2πik/NK(i), k = 1, . . . , n. A method of synthesis is
via the set of complex 2 × 1 Gaussian vectors {Uk, k ∈Zn} of Goodman class [217].
The closure constraint on the simple curves f(0) = f(1) implies the 2D linear manifold
constraint associated with the discrete Fourier transform (DFT) is zero:

0
0

=
n

i=1

U1(i)
−U2(i)
U2(i)
U1(i)
 
cos(2πi/n) −cos(2π(i −1)/n)
sin(2πi/n) −sin(2π(i −1)/n)

,
(9.51)
implying the complex zero equation
0 =
n

i=1

U1(i) −jU2(i)

ej(2πi/n) −ej(2π(i−1)/n)
.
(9.52)
This implies the highest frequency discrete Fourier transform coefﬁcient is zero since
n

i=1

U1(i) −jU2(i)

e−j(2πi(n−1)/n) = e−j(2π/n)
n

i=1

U1(i) −jU2(i)

e−j(2πi(n−1)/n).
(9.53)
The effect of closure is to reduce the dimension of the 2n, U−variables to 2n −2
dimensions.
Figure 9.1 shows the results of experimental generation of mitochondrial bound-
ary shapes representing the empirical covariance structure of 41 images containing 497
mitochondria which were hand-traced. Each boundary was discretized and sampled
to constant arc length. The template was mapped to the hand tracing from which the
parameters were ﬁtted and discrete Fourier transformed for all M of the mitochondria.
The 2 × 2 blocks k, k ∈Zn are calculated according to
k =
 a
b∗
b
d

where
a = 1
M

M
m=1 |U(m)
1k
−¯U1k|2
b = 1
M

M
m=1(U(m)∗
1k
−¯U1k)∗(Um
2k −¯U2k)
d = 1
M

M
m=1 |U(m)
2k
−¯U2k|2
,
(9.54)
where ¯U represents the sample means of the complex Fourier transforms ¯U1k =
(1/M) 
M
m=1 U(m)
1k , ¯U2k = (1/M) 
M
m=1 U(m)
2k .
Panel 2 of Figure 9.1 shows the average mitochondria generated by hand tracing
the population of mitochondria and estimating the covariance parameters. Panel 3
(bottom row) shows the spectrum of the covariance of the stationary Gaussian process
on the mitochondria shapes. Since there are two parameters at each boundary point,
the spectrum has two components at each Fourier transform frequency corresponding
to the eigenvalues of k. Panel 3 shows a plot of these two spectral components.

262
9 SECOND ORDER AND GAUSSIAN FIELDS
Because the shapes are smooth, the spectra have low frequency content, with the
familiar symmetry around the midpoint of the spectrum being due to the fact that
the scale/rotation process is real. The mean shape and its covariance represents the
typical structure and its range of variation. Panel 4 (bottom row) shows 8 mitochondria
generated from the empirical distribution placed uniformly throughout the scene.
9.3 Gaussian Fields on the Continuum
Orthogonal expansions require speciﬁcation of second order properties, mean and covariance;
it is natural to return to Gaussian ﬁelds, completely determined by their mean and covariance
operators. The familiar view of Gaussian processes is that arbitrary linear combinations of samples
of the ﬁeld are Gaussian. The Gaussian ﬁelds studied here are associated with Hilbert spaces
H = L2, l2; therefore it will be powerful to deﬁne the Gaussian ﬁelds through their properties in
inner products with elements of the Hilbert space. We give both deﬁnitions.
Deﬁnition 9.29
Then Y(ω, x), x ∈X is an m-dimensional Gaussian vector ﬁeld on X ⊂
Rd(Zd), m ≥1, d ≥1 with mean and covariance ﬁelds mY, KY, mY : X →Cm, KY : X ×
X →Cm ×Cm, if for any n and n-tuple of m-vectors a1, . . . , an ∈Cm, points x1, . . . , xn ∈X,
Z(ω) = 
n
k=1 a∗
kY(ω, xk) is Gaussian with mean and variance
ma =
n

i=1
a∗
i mY(xi) ,
σ 2
a =
n

i,j=1
a∗
i KY(xi, xj)aj.
(9.55)
The Hilbert space deﬁnition is useful.
Deﬁnition 9.30
Deﬁne the complex m−vector Hilbert space H on X ⊂Rd, m ≥1, d ≥1
with
⟨f, g⟩H =

X
⟨f(x), g(x)⟩Cm dx =
m

i=1

X
f∗
i (x)gi(x) dx.
(9.56)
Then Y is a Gaussian m-vector ﬁeld on H with mean and covariance mY ∈H, KY :
H →H, if for all f ∈H, ⟨f, Y⟩H is normally distributed with mean and variance mf , σ 2
f
given by
mf = ⟨f, mY⟩H,
σ 2
f = ⟨f, KYf⟩H.
(9.57)
We shall say Y is trace class (covariance operator KY is trace class) if for any C.O.N.
basis {φk} ⊂H,
trKY =
∞

k=1
⟨φk, KYφk⟩H < ∞.
(9.58)
KY is a positive deﬁnite and self-adjoint operator; this is homework problem F.7.52. The
covariance properties required for the orthogonal expansions require the trace class condition.
The covariance trace class connects to the notion of SOP’s, E∥Y(ω)∥2 < ∞.
Theorem 9.31
Given a process on H (zero mean) with covariance trace class, tr KY < ∞,
then
tr KY = E∥Y∥2 < ∞.
(9.59)

9.3 GAUSSIAN FIELDS ON THE CONTINUUM
263
Proof
Deﬁne the C.O.N. base of H to be {φk}, then
E∥Y∥2 = E lim
n→∞
n

k=1
|⟨Y, φk⟩|2 = lim
n→∞
n

k=1
E|⟨Y, φk⟩|2 (monotone convergence)
= lim
n→∞
n

k=1
⟨φk, KYφk⟩=
∞

k=1
⟨φk, KYφk⟩= tr KY < ∞.
Of particular interest is L2(X), X ⊂Rd compact. Take X = [0, 1] for example, then Y on X
being trace class implies E

X |Y(ω, t)|2 dt < ∞. The connection to q.m. continuity is clear, Y being
q.m. continuous implies K is bounded over X (Theorem 9.13), implying

X K(t, t) dt < ∞which
by Fubini’s theorem implies Y(ω, ·) is of trace class!
Notice, integrals against functions in the Hilbert space are a direct extension from ﬁnite
linear combinations. Homework problem F.7 examines the equivalence of the two deﬁnitions
9.29 and 9.30.
Example 9.32 (Brownian Motion)
The Wiener process is such an important exam-
ple of a random process with so many beautiful properties: let us single it out. Let
Y(t), t ≥0 be a real-valued Gaussian process. Then it is a standard Wiener process or
a standard Brownian motion if it has mean and covariance
EY(t) = 0
and
EY(t)Y(s) = min(t, s) .
(9.60)
Since it is a Gaussian process, ﬁnite linear combinations of the form Z =

N
k=1 αkY(tk) are Gaussian. The nth order density on samples are Gaussian (see
Remark 5.1.1 of the Gaussian ﬁelds Section 5.1, Chapter 5) giving
p(y(t1), · · · , y(tn)) = det−1/2 2πKe−1/2 
ij y(ti)y(tj)(K−1)ij
(9.61)
with n × n covariance K =

min(ti, tj)
	
.
Observe that this process has independent increments, i.e. Y(t1) ∼N(0, t1),
Y(t2) −Y(t1) ∼N(0, t2 −t1), Y(tn) −Y(tn−1) ∼N(0, tn −tn−1), and if s < t < u < v
then
E(Y(t) −Y(s))(Y(v) −Y(u)) = 0 .
(9.62)
There is a much stronger property of Brownian motion which actually provides
the basis for its role in diffusions and stochastic integration. The variance of an incre-
ment varies as the length of the increment, E(Y(t) −Y(s))2 = t −s, suggesting that
(dY(t))2 ∼O(dt). This is in fact the case in an almost sure sense, and provides the
basis for many results on diffusions and stochastic integrals (see Theorem A.2 in the
Appendix).
Example 9.33 (Eigen Spectrum of Brownian Motion)
Brownianmotionwithbound-
ary condition Y(0) = 0 is a second order process, and is quadratic mean continuous
as well since K(t, t) = min(t, t) = t is continuous. Then K(t, s) = EY(t)Y(s) is
continuous on [0, T]2 (see Theorem 9.13), and as an operator is compact and self
adjoint:
Kf(t) =
 T
0
K(t, s)f(s) ds =
 t
0
sf(s) ds + t
 T
t
f(s) ds,
t ∈[0, T].
(9.63)
The eigenfunctions of the covariance satisfy
λφ(t) =
 T
0
K(t, s)φ(s) ds =
 t
0
sφ(s) ds + t
 T
t
φ(s) ds ,
t ∈[0, T] .
(9.64)

264
9 SECOND ORDER AND GAUSSIAN FIELDS
Differentiation twice gives
 T
t
φ(s) ds = λ ˙φ(t)
0 < t < T
−φ(t) = λ ¨φ(t)
0 < t < T.
To expand the Brownian motion around Y(0) = 0, then the ﬁrst boundary condition
gives φ(0) = 0, with the second boundary condition ˙φ(T) = 0, then φ(t) = A sin t/
√
λ,
cos T/
√
λ = 0, and the eigenelements become λn = T2/((n + (1/2))2π2), φn(t) =
√
2/T sin((n + (1/2))πt)/T.
Example 9.34 (White Noise)
White-noise is tricky, its not continuous anywhere.
Assume Y(t), t ∈[0, 1] on (, A, P) is an i.i.d. collection of normal random variables.
Let Y(t) be continuous at some point t0 ∈[0, 1]. The i.i.d. condition implies
Pr{ω : Y(ω, t) > ǫ, Y(ω, s) < −ǫ} =
 ∞
ǫ
1
√
2π
e−z2/2 dz
2
,
(9.65)
with the event Pr{ω : Y(ω, t) > ǫ, Y(ω, s) < −ǫ} not depending on s and t, implying as
s approaches t the limit is a non-zero constant for any choice of ǫ > 0. Then if Y(ω, t0)
is continuous at t0 ∈[0, 1], it implies
lim
n→∞An = lim
n→∞

ω : Y(ω, t0) > ǫ, Y

ω, t0 + 1
n

< −ǫ

= ø.
(9.66)
Thus,
P

lim
n→∞An

= P(ø) = 0 ̸= lim
n→∞P(An) =
  ∞
ǫ
1
√
2π
e−z2/2 dz
2
.
(9.67)
P is not sequentially continuous. Impossible! Apparently there is no such continuity
point t0 ∈[0, 1].
9.4 Sobolev Spaces, Green’s Functions, and Reproducing
Kernel Hilbert Spaces
For vector ﬁelds differential operators which are physically based will be used to model the structure
ofphysicalsystems. Theoperatorsexpressphysicalpropertieswhichrepresentthetransformations
of the dynamical system being modeled.
Herein linear differential operators are studied supporting orthogonal process descriptions.
Assume throughout the domain is compact. The Gaussian random ﬁelds are induced as solutions
of partial differential equations of the type
LU(x) = W(x) ,
x ∈X ⊂Rd ,
(9.68)
with W a Gaussian noise process, and L a linear differential operator. Smoothness properties
on the solution U are inherited from smoothness properties associated with the differential
operators L.
We have already seen from our various decomposition theorems that smoothness on covari-
ance (continuity for example) and existence of smooth eigenelements are fundamental. This
smoothness will be determined by the existence of the Green’s function and its smoothness.
Assume G is the Green’s function of L, so that LG = δ. For the engineers amongst us, think of

9.4 REPRODUCING KERNEL HILBERT SPACES
265
L as representing the transformation of the linear time invariant system on the input W trans-
forming to the output U. Let us assume for the moment that W is well deﬁned, a simple smooth
function. In this case G is the “impulse response” of the linear time invariant system, thus linear
solutions (initial conditions at minus inﬁnity) take the form
U(x) =

G(x, y)W(y) dy .
(9.69)
Thus, the smoothness of the Green’s function G determines the smoothness of U. Even in the
“white noise case”, if we interpret U weakly, then clearly the covariance of U is determined by the
smoothness of the Green’s function according to KU =

G(·, y)G(y, ·) dy.
9.4.1 Reproducing Kernel Hilbert Spaces
So what is the space of functions associated with solutions of the equations associated with the
differential operators? They are a Hilbert space H ⊂L2, generally smoother than L2 functions,
whichweshallcallareproducingkernelHilbertspace(RKHS)whichcanbegeneratedbyoperating
a smooth function (the Green’s function) on the L2 functions.
The general deﬁnition of an RKHS is deﬁned by the existence of a continuous linear
functional which sifts on functions.
Deﬁnition 9.35
A Hilbert space H(X, R) of real-valued scalar functions on X ⊂Rd is said
to be a reproducing kernel Hilbert space (RKHS) if for each x ∈X, the linear evaluation
functional given by δx : h ∈H →δx(h) = h(x) ∈R is continuous 25 and there exists a
unique element K(x, ·) ∈H such that
δx(h) = ⟨K(x, ·), h⟩H = h(x),
for all h ∈H;
(9.70)
it is reproducing since
⟨K(x, ·), K(y, ·)⟩H = K(y, x).
(9.71)
If H(X, Rm) is the space of m-vector functions then it is an RKHS if for each x ∈X, α ∈Rm;
then δαx (h) = ⟨h(x), α⟩Rm is continuous and there exists a unique element K(x, ·)α ∈H such
that
δα
x (h) = ⟨K(x, ·)α, h⟩H = ⟨α, h(x)⟩Rm
for all h ∈H;
(9.72)
it reproduces since
⟨K(x, ·)α, K(y, ·)β⟩H = ⟨α, K(x, y)β⟩Rm.
(9.73)
The kernels for such spaces come in many forms, for us they will often be the convolution
of the Green’s kernel of a shift-invariant differential operator with itself. In the one dimensional
signal analysis case this corresponds to the convolution of the impulse response with itself.
Remark 9.4.1
So how do we see that this Hilbert space H ⊂L2 is smoother than L2
functions. Well, essentially they are functions which can be generated by operating a
25 Continuity of a linear function F : H →H is equivalent to boundedness in the sense that ∃M ∈R such
that |δαx (h)| ≤M∥h∥H for all h ∈H, implying |F(h −hn)| ≤M∥h −hn∥H →0 as ∥hn →h∥H. The existence of
the kernel follows from the Riesz Fréchet Theorem; if F : H →R is a continuous linear functional on a Hilbert
space H then there exists a unique f ∈H such that F(h) = ⟨f, h⟩H for all h ∈H. Since the linear functional δx
is continuous then there exists a unique element kx(·) ∈H such that δx(h) = ⟨kx, h⟩H = h(x) for all h ∈H. The
same argument holds for the vector case.

266
9 SECOND ORDER AND GAUSSIAN FIELDS
smooth function on the L2 functions. When the kernel is sufﬁciently smooth so that
its square root is smooth, then there exists a G(x, ·), x ∈X which is continuous and
positive deﬁnite with reproducing kernel K(·, ·) =

X G(·, z)G(z, ·) dz such that the
Hilbert space H is constructed according to
H(X, Rm) =

h ∈L2(X, Rm) : ∃f ∈L2(X, Rm), h(·) =

X
G(·, x)f(x) dx

.
(9.74)
The inner product on H is deﬁned as < h, h′ >H=< f, f′ >2.
9.4.2 Sobolev Normed Spaces
Sobolev spaces and the Sobolev inclusion theorems will allow us to relate the L2 existence of
the derivatives to the pointwise differentiabilty of continuous and differentiable functions with
sup-norm. The connection to RKHS is with sufﬁcient derivative then the space has a smooth kernel.
Deﬁnition 9.36
Deﬁne the Sobolev space Hs
0(X, Rm) to be the closure of the compactly
supported with zero-boundary continuously differentiable m-vector functions on the X ⊂Rd
with s > 0 the integer order of the Sobolev norm; ∥· ∥Hs
0 given by
∥h∥2
Hs
0 =
m

i=1

α≤s
∥Dαhi∥2
2 ,
(9.75)
with the notation Dα representing the derivative on the scalar function with the index α
selecting the derivatives and cross derivatives up to order α.
Now to guarantee the existence of smooth kernels we use the Sobolev embedding theorems
allowing us to show that if the order s of differentiability in the Sobolev norm satisﬁes s > k+(d/2)
then functions h ∈Hs
0 agree almost everywhere with functions in Ck
0, written as Hs
0 ֒→Ck
0. This
condition we say is the inclusion map being continuous.
Theorem 9.37
If X is bounded in Rd, k ≥0 and s > k + (d/2) then Hs
0(X) ֒→Ck
0(X) is
continuous and Hs
0(X) ֒→Ck(X) is compact.
In particular X ⊂Rd open, s > (d/2), so that Hs
0(X) ֒→C0
0(X); then Hs
0 is a
reproducing kernel Hilbert space with m × m reproducing kernel K satisfying, for all h ∈H,
α ∈Rm,
⟨K(x, ·)α, h⟩H = ⟨α, h(x)⟩Rm.
(9.76)
It follows from Theorem 9.37 that if s > d/2 then the evaluation functionals on Hs
0 are continuous.
Indeed, for any x ∈X, the continuity of the inclusion map implies that there exists a constant M
such that for all h ∈Hs
0,26
|h(x)| ≤sup
y
|h(y)| ≤M∥h∥Hs
0.
26 By the inclusion map of H into B where H ⊂B, we mean the map id : H →M with id h = h. To say that
the inclusion map is continuous, (we write this concisely as H ֒→B) we mean that operator id is continuous
in the usual sense (which is equivalent to bounded as deﬁned above, i.e. there exists a constant M such that
∥h∥B ≤M∥h∥H for all h ∈H.) To say that the inclusion map is compact, we mean that the operator id is compact.
That is, id is compact if id maps bounded sets in H to precompact sets (sets whose closure is compact) in B. id is
just the identity; so this means that sets in H which are bounded with respect to the norm on H are precompact
with respect to the norm on B. Another equivalent statement is that bounded sequences in H have convergent
subsequences with respect to the norm on B.

9.4 REPRODUCING KERNEL HILBERT SPACES
267
9.4.3 Relation to Green’s Functions
For modeling purposes, it is useful to use the differential operators L to dominate the Sobolev
norm of sufﬁcient derivatives, since then, from the above Sobolev embedding theorem 9.37, the space
is a reproducing kernel Hilbert space with the kernel of smoothness determined by s. This kernel
is generated from the Green’s operator according to LL∗since for all h ∈Hs, then if ∥h∥2
Hs = ∥Lh∥2
2
implying if there exists a kernel K then
∥h∥2
Hs =

X
⟨L∗LK(x, y)α, h(y)⟩Rm dy = ⟨K(x, ·)α, h(·)⟩H = ⟨α, h(x)⟩Rm .
(9.77)
Thus we have, if the kernel exists, then K = GG∗generated from the Green’s kernels.
To illustrate the condition which satisﬁes the s > k + d/2 condition, with s = 2, k = 1, d = 1.
Let L = id −α2∇2 in R1, then the Green’s kernel satisfying L∗LK = δ is given by
K(x, y) =
1
4α2

e−1
α |x−y|(|x −y| + α)

and K(x, y) is C1 and an element of the Sobolev space of two derivatives (reproducing kernel
space).
To see this, to calculate the Green’s kernel L∗L = (id −α2)2, then let K(x, y) = g(x −y) and
deﬁne the Fourier transform pair g(x) ↔G(f). Then L∗LK = δ, G(f) is the inverse spectrum of LL∗
given by
G(f) =
1
(α2f2 + 1)2 .
Clearly, the kernel is in the Sobolev space of two derivatives since differentiating twice gives a
Fourier transform which is integrable going as f−2.
Since the transform is the square, the kernel is given by the convolution
g(x) = 1
2α e−|x|
α ∗1
2α e−|x|
α
=
1
4α2
 ∞
−∞

e−(t−τ)
α
u(t −τ) + e
(t−τ)
α
u(−t + τ)
 
e−τ
α u(τ) + e
τ
α u(−τ)

dτ
=
1
4α2

e−1
α (|x| + α)

To show that K(x, y) is C1, we observe g′(x) is continuous according to
g′(x) =



−1
4α3 e−x
α (x + α) +
1
4α2 e−x
α
x ≥0
1
4α3 e
x
α (−x + α) −
1
4α2 e−x
α
x < 0
with g′(0+) = g′(0−) = 0
9.4.4 Gradient and Laplacian Induced Green’s Kernels
Here are three examples deriving the Green’s operator; in R1, R3 k = 0 and the evaluation kernels
are continuous only with s > d/2 condition; in R2, then k = 1 with s = 2; so the evaluation
functional has a non-continuous derivative.

268
9 SECOND ORDER AND GAUSSIAN FIELDS
Theorem 9.38
Let L = id + α(∂/∂x) in R1 and let L = id −α2∇2 in R3, then the Green’s
kernel satisfying L∗LK = δ is given by
K(x, y) = βe−1/α∥x−y∥R1,3 ,
(9.78)
with β providing the normalizer given by β = 1/2α in R1, β = 1/(2α)3π in R3, respectively.
For L = id −α2∇2 in R2,
K(x, y) =
1
4α2π
 ∞
0
e−t−∥x−y∥2
R2/4α2t dt = ∥x −y∥R2
|α|
B1
∥x −y∥R2
|α|

,
where B1 denotes the modiﬁed Bessel function of the second kind of order 1[218].
Proof
In 1-dimension R1, the adjoint L∗is required to compute the Green’s kernel;
its deﬁned by
⟨Lg, h⟩2 = ⟨g, L∗h⟩2
(9.79)
with g, h differentiable functions vanishing at inﬁnity. Solving gives
 
id + α ∂
∂x

g(x)h(x) dx =

g(x)h(x) dx −

g(x)α ∂
∂xh(x) dx
(9.80)
=

g(x)

id −α ∂
∂x

h(x) dx,
(9.81)
implying L∗= id −α(∂/∂x). Then L∗L = id −α2(∂2/∂x2). That the Green’s kernel in
1-D is the exponential spline with β = 1/2α follows from the second derivative:
∂
∂x
1
2α e−1
α |x| = 1
2α
1
α e
1
α xu(−x) −1
2α
1
α e−1
α xu(x),
(9.82)
∂2
∂x2
1
2α e−1
α |x| = 1
α2 K(x, 0) −1
α2 δ(x),
(9.83)
with u(x) the unit step function. Then L∗LK = δ as an operator.
To calculate the Green’s kernel in R3 use Fourier identities. Start with L∗L =
(id −α2∇2)2, then let K(x, y) = g(x −y), and deﬁne the Fourier transform pair g(x) ↔
G(f), x = (x1, x2, x3) and f = (f1, f2, f3). Since L∗Lg = δ, then G(f) is the inverse
spectrum of L∗L given by G(f) = 1/(c2∥f∥2
R3 + 1)2 with c = 2πα implying the Green’s
function is given by the following integral:
g(x) =
 ∞
−∞
 ∞
−∞
 ∞
−∞
1
(c2∥f∥2
R3 + 1)2 ej2π⟨x,f⟩R3 df.
(9.84)
Note that G(·) is rotationally invariant, so g(·) is rotationally invariant, and only needs
to be evaluated at x = (0, 0, z), z > 0. Making the change to spherical coordinates

9.4 REPRODUCING KERNEL HILBERT SPACES
269
f1 = r sin ψ sin θ, f2 = r sin ψ cos θ, f3 = r cos ψ, then the integral becomes
1
2π
 ∞
0
 2π
0
 π
0
1
(c2r2 + 1)2 ej2πrz cos ψr2 sin ψ dψdθ dr
(a)
= 2π
 ∞
0
 1
−1
r2
(c2r2 + 1)2 ej2πrzy dy dr
(b)
= 1
z
 ∞
0
r
(c2r2 + 1)2

ej2πrz −e−j2πrz
j

dr
= 1
2z
 ∞
−∞
r
(c2r2 + 1)2

ej2πrz −e−j2πrz
j

dr
= −1
z
 ∞
−∞
jr
(c2r2 + 1)2 ej2πrz dr,
with (a) since the integrand does not depend on ψ and making the change of variable
y = cos θ, and (b) integrating with respect to y. Now from the tables
−1
z
 ∞
−∞
jr
(c2r2 + 1)2 ej2πrz dr =
1
8πα3z
 ∞
−∞
−8πjr
(4π2r2 + (1/α2) 2 ej2πrz dr
(9.85)
=
1
(2α)3πzze−(1/α)∥z∥R3
=
1
(2α)3π e−(1/α)∥z∥R3 ,
(9.86)
giving the result g(x) = 1/(2α)3πe−1/α∥x∥R3 .
For R2, then examine the Fourier transform of g(x, 0) ↔G(f) = (1 −
4α2π2∥f∥2
R2)−2. We have g ∈L1, for
∥g∥2
L1 ≤
1
4α2π
 ∞
0

R2 e−te−∥x∥2
R2/4α2t dx dt =
 ∞
0
te−t dt = 1,
where the ﬁrst equality follows from the Gaussian integral identity,

Rn e−πa∥x∥2
Rn dx =
a−n/2. Thus, by Fubini’s Theorem we have
G(f) =
1
4α2π

R2
  ∞
0
e−t−∥x∥2
R2/4α2t dt

e−2πi⟨f,x⟩R2 dx
=
1
4α2π
 ∞
0
e−t 
R2 e−∥x∥2
R2/4α2te−2πi⟨f,x⟩R2 dx

dt
=
 ∞
0
te−(1+4α2π2∥f∥2
R2)t dt
= (1 + 4α2π2∥f∥2
R2)−2,
where the third equality follows from the Fourier Transform pair of the Gauss kernel,
e−πa∥x∥2
Rn ↔a−n/2e−π∥f∥2
Rn/a.
Example 9.39 (Auto-regressive Models)
Let dY(t)/dt = −cY(t) + W(t), Y(0) = 0
with W(·) white-noise. Solving for the G function gives
LG(t, s) =
 d
dt + c

G(t, s) = δ(t −s)
(9.87)

270
9 SECOND ORDER AND GAUSSIAN FIELDS
implying G(t, s) = e−c(t−s)u(t −s). The adjoint becomes
⟨Gf, h⟩=
 ∞
0
 ∞
0
G(t, s)f(s) dsh(t) dt
=
 ∞
0
 ∞
0
e−c(t−s)u(t −s)f(s) dsh(t) dt
=
 ∞
0
f(s)
 ∞
0
e−c(t−s)u(t −s)
)
*+
,
G∗(s,t)
h(t) dt ds,
(9.88)
implying G∗(s, t) = ec(s−t)u(−(s −t)). The covariance becomes
K(t, s) = GG∗(t, s) =
 ∞
0
e−c(t−z)u(t −z)ec(z−s)u(−(z −s)) dz
=
 s
0
e−c(t−z)u(t −z)ec(z−s) dz
t>s
=
 s
0
e−c(t−z)ec(z−s) dz = e−c(t+s) e2cz
2c

s
0 = e−c(t−s) −e−c(t+s)
2c
t<s
=
 s
0
e−c(t−z)u(t −z)ec(z−s) dz
=
 t
0
e−c(t−z)ec(z−s) dz = e+c(t−s) −e−c(t+s)
2c
.
Homework problem F.7.49 shows that the covariance using the sifting property
of white-noise gives the same result.
Example 9.40 (Wiener Process)
Let (dB(t))/(dt) = W(t) weakly with B(0) = 0, t ≥0.
Then the Green’s function satisﬁes
dG(t, s)
dt
= δ(t −s)
(9.89)
implying G(t, s) = u(t −s) the unit step. Notice, for s ≥0, u(0 −s) = 0 satisfying the
boundary condition G(0, s) = 0. G is not self-adjoint since d/dt is not self adjoint:
⟨Gf, g⟩=
 ∞
0
g(t)
 ∞
0
u(t −s)f(s) ds dt
=
 ∞
0
f(s)
 ∞
0
u(t −s)
) *+ ,
G∗(s,t)
g(t) dt ds = ⟨f, G∗g⟩
(9.90)
proving G ̸= G∗. The covariance satisﬁes
K(s, t) = GG∗(s, t) =
 ∞
0
u(s −τ)u(t −τ) dτ
t, s > 0
=
 min(t,s)
0
dτ = min(t, s)
t, s > 0.
(9.91)
To see K(s, t) = min(t, s) satisﬁes the equation LKUL∗= δ identity as an operator on
function f ∈ˆC(0, ∞) vanishing on the boundary, f(0) = f(∞) = 0, then
min(t, s) = −1
2|t −s| + 1
2s + 1
2t .
(9.92)

9.4 REPRODUCING KERNEL HILBERT SPACES
271
Now L = d/dt, L∗= −d/dt, giving
LKUL∗(t, s) = d
dt

−1
2|t −s| + 1
2s + 1
2t
−d
ds =

−1
2u(t −s) + 1
2
−d
ds ,
(9.93)
implying via integration by parts of the sifting property on functions f ∈ˆC: giving
LKUL∗f(t) =
 ∞
0
LKUL∗(t, s)f(s) ds =
 ∞
0

−1
2u(t −s) + 1
2

)
*+
,
g(t,s)
−d
ds f(s)ds
= −(g(t, ∞)f(∞) −g(t, 0)f(0))
+
 ∞
0
d
ds

−1
2u(t −s) + 1
2

f(s) ds = f(t).
(9.94)
9.5 Gaussian Processes Induced via Linear Differential Operators
As in Chapter 5 we are going to induce Gaussian ﬁelds using differential operators (rather than
difference operators). The background space X ⊂Rd is assumed compact so that the random
processwillhavediscretespectrum. Thisapproachgeneralizesauto-regressivemodelingforcausal
time-series applications, and allows the parameterization of the covariance of the induced ﬁelds
to be parametric. This also completes the approach taken in the previous chapters, but extended
from discrete graphs to the continuum. If L is a shift invariant operator with cyclic boundary
conditions, then we shall be inducing cyclo-stationary Gaussian ﬁelds with covariance functions
having a spectral decomposition based on the complex exponentials.
In the ﬁnite dimensional setting, the differential operators were driven by “white nois”,
LU = W, W = white noise. Then the covariance of the ﬁeld is directly given by the inverse
operators KU = (LL∗)−1. In the inﬁnite dimensional setting, the inverse operators are given by
the Green’s operator, and care must be taken in dealing with white noise as it does not exist in the
pointwise function sense. Contrast this to the ﬁnite dimensional Problem F.5.15, Chapter 5, where
the covariance of white noise is identity. White noise is clearly not a second-order process.
For this treat white noise as a generalized Gaussian random process as in [219]. Introduce the
family of test functions, C∞(X) inﬁnitely differentiable functions for X ⊂Rd bounded domains
(vanishing at inﬁnity ˆC∞for unbounded domains).
Deﬁnition 9.41
Then W(x), x ∈X ⊂Rd is a generalized random process or random
ﬁeld if it is a continuous linear mapping that associates with each f ∈C∞, the random variable
⟨W, f⟩∈L2(P).
We shall say W(·) is Gaussian distributed in the generalized sense with mean
and covariance, mW, KW, if for all smooth test functions f ∈C∞, ⟨W, f⟩is Gaussian
distributed with mean ⟨mW, f⟩and variance ⟨f, KWf⟩.
Return to white noise. Since it is nowhere continuous (see Example 9.34), the stochastic
differential equation cannot be deﬁned pointwise. Rather it is deﬁned in the weak sense via its
distribution in integration against test functions. The weak interpretation of the equality in Eqn.
9.68 gives for all smooth f ∈C∞,
⟨W, f⟩= ⟨LU, f⟩= ⟨U, L∗f⟩
(9.95)

272
9 SECOND ORDER AND GAUSSIAN FIELDS
which implies
∥f∥2 = E|⟨W, f⟩|2 = ⟨L∗f, KUL∗f⟩
(9.96)
= ⟨f, LKUL∗f⟩.
(9.97)
Since this is true for all f this deﬁnes the operator L∗KUL to be the identity operator,
LKUL∗= δ. Thus choose KU = GG∗the covariance satisfying Eqn. 9.97.
Deﬁnition 9.42
Let L be a differential operator deﬁning the norm ∥f∥2
L =

X ∥Lf(x)∥2 dx
with
associated
Hilbert
space
of
functions
HL
with
inner
product
⟨f, g⟩L
=

X(Lf(x))∗Lg(x)dx. Suppose HL supports a sifting function K(xi, ·) ∈HL such that for
all f ∈HL then ⟨K(xi, ·), f⟩L = f(xi).
Then K with kernels K(x, ·) ∈HL for all x ∈X is the Green’s operator of L∗L according
to for all f ∈HL,
f(xi) = ⟨K(xi, ·), f⟩L =

X
(LK(xi, x))∗Lf(x) dx
(9.98)
=

X
L∗LK(xi, x)f(x) dx .
(9.99)
Theorem 9.43
Let L be a normal invertible differential operator on X
⊂Rd with
continuous eigenelements {λk, φk}, Lφk = λkφk, admitting Greens operator G. Deﬁne
U(n) = 
n
k=0 Ukφk, Uk orthogonal Gaussian variates variance E|Uk|2 = 1/|λk|2. Then
U(·)
q.m.
= limn→∞U(n)(·) is a quadratic mean continuous Gaussian ﬁeld with trace class
covariance KU = GG∗= 
∞
k=0 1/|λk|2φk⟨φk, ·⟩satisfying
LU(·) = W(·), W(·) = white noise,
(9.100)
if and only if GG∗is continuous along the diagonal.
Proof
Assume GG∗is continuous along the diagonal. Then GG∗is trace-class since
X is compact so that
tr GG∗=

X
GG∗(x, x) dx < ∞.
(9.101)
Thus with G
=

∞
k=0
1
λk φk⟨φk, ·⟩and GG∗ﬁnite trace implies trace GG∗
=

∞
k=0 1/|λk|2 < ∞.
For every n and j = 0, 1, . . . , n,
λjUj =

X
φ∗
j (x)LU(n)(x) dx =

X
φ∗
j (x)W(n)(x) dx
(9.102)
= Wj.
(9.103)
Thus, Uj = Wj/λj are independent Gaussian variates with variance 1/|λj|2 and U(n)
is an n-dimensional Gaussian ﬁeld since inner products with test functions ⟨f, U(n)⟩=

n
k=0 Wk/λk⟨f, φk⟩are Gaussian.
ThenforEqn. 9.100toholditmustbeshownthattheleft-andright-handsidesare
equal when integrated against smooth functions. Expanding W(·) = 
∞
k=0 Wkφk(·),

9.5 GAUSSIAN PROCESSES INDUCED VIA LINEAR DIFFERENTIAL OPERATORS
273
Wk zero-mean orthogonal Gaussian variates variance E|Wk|2 = 1, then
E|

f∗(x)(LU(n)(x) −W(x)) dx|2 = E

∞

k=n+1
Wk⟨f, φk⟩

2
(a)
=
∞

k=n+1
|⟨f, φk⟩|2 (b)
→0 as n →∞.
(9.104)
where (a) holds since W(·) is white noise and (b) since f ∈C∞(X) is square integrable
on X.
Then the covariance KU = GG∗directly from the deﬁnitions of the Green’s
operator.
Theorem 9.44
Let L be a normal invertible linear differential operator on D ⊂Rd with
continuous eigenelements Lφk = λkφk, with W(n)(·) = 
n
k=0 Wkφk(·), Wk zero-mean
independent Gaussian variates with variances E|Wk|2 = |αk|2, with 
∞
k=0 |αk|2 < ∞. 27
Deﬁning U(n)(·) = 
k Ukφk(·) with Uk zero-mean independent Gaussian vari-
ates with variance E|Uk|2 = |αk|2/|λk|2 satisfying 
∞
k = 0 |αk|2/|λk|2 < ∞then limn→∞
U(n)(x) q.m. = U(x) is a quadratic mean continuous process solving
LU(·) = W(·) ,
W(·)
q.m.
= lim W(n)(·),
(9.105)
with covariance operator KU = 
∞
k=0(|αk|2/|λk|2)φk⟨φk, ·⟩.
Proof
For every n and j = 0, 1, . . . , n,
λjUj =

X
φ∗
j (x)LU(n)(x) dx =

X
φ∗
j (x)W(n)(x) dx
(9.106)
= Wj.
(9.107)
Thus, Uj = Wj/λj are independent Gaussian variates with variance |αj|2/|λj|2 and U(n)
is an n-dimensional Gaussian ﬁeld since inner products with test functions ⟨f, U(n)⟩=

n
k=0 Wk/λk⟨f, φk⟩are Gaussian.
With the trace class assumption 
∞
k=0 |αk|2/|λk|2 < ∞holds, then U(n) con-
verges in quadratic mean since E∥
∞
k=n+1 Ukφk∥2 →0; call the limit U satisfying
LU = W, examine LUn −W as n →∞. Since W is trace class then 
k |αk|2 < ∞
implies
E∥LUn −W∥2 =
∞

k=n+1
|αk|2 (a)
→0
as n →∞,
(9.108)
with (a) the ﬁnite trace condition on W.
That U is a quadratic mean continuous process follows from the continuity of
the eigenfunctions with the covariance deﬁnition KU given by for all functions f,
⟨f, KUf⟩= E|⟨U, φ⟩|2
(9.109)
=
∞

k=0
|αk|2
|λk|2 ⟨φk, f⟩2 =

f,
∞

k=0
|αk|2
|λk|2 φk⟨φk, f⟩

.
(9.110)
27 Such W(n) (ﬁnite n) could be called “bandlimited” since they are in the span of a ﬁnite number of
“frequencies” φk.

274
9 SECOND ORDER AND GAUSSIAN FIELDS
9.6 Gaussian Fields in the Unit Cube
Now examine 3-valued vector ﬁelds on the unit cube [0, 1]3, with the covariance and random
structure viewed as arising through solutions to the stochastic equation of the type LU(x) = W(x),
L a cyclic, shift invariant (constant coefﬁcient) linear differential operator.
Consider the vector valued functions f : X →R3 made into a Hilbert space L2(X) with the
inner product ⟨f, g⟩(L2(X))3 = 
3
i=1

X fi(x)gi(x) dx. Various operators arise and have been used
by the community, including 3D small deformation elasticity operator, L = −a∇2 −b∇∇· +cI
[220–222], as well as the bi-harmonic (describing small deformations energetics of thin plates)
[223,224], Laplacian [225], etc. Since cyclic shift invariant operators (also termed cyclo-stationary)
on the unit-cube are assumed, they have eigenfunctions arising from complex exponentials. With
x = (x1, x2, x3) ∈X = [0, 1]3, thecomplementaryvariablebecomesωk = (ωk1, ωk2, ωk3), ωki = 2πki,
i = 1, 2, 3, and the Fourier basis for periodic functions on [0, 1]3 takes the form ej⟨ωk,x⟩,⟨ωk, x⟩=

3
i=1 ωkixi.
Since the random U-ﬁelds are vector valued, for each ωk there will be corresponding three
orthogonal eigenfunctions. It leads to a natural indexing of the eigenfunctions and eigenvalues
according to {φ(i)
k , λ(i)
k , i = 1, 2, 3}.
The general form for the eigenfunctions and eigenvalues can be derived for gen-
eral linear differential operators which determines the covariance of the resulting Gaussian
process.
Theorem 9.45
Let L be a non-singular normal shift invariant (constant coefﬁcient) linear
differential operator on X = [0, 1]3 with eigenelements {λ(i)
k , φ(i)
k } satisfying Lφ(i)
k
= λ(i)
k φ(i)
k
and of the form
L =


A11
A12
A13
A21
A22
A23
A31
A32
A33

,
(9.111)
where
Ajl
=

njl
m=1 ajl(m)(∂pjl(m)
@
(∂x
p(1)
jl (m)
1
∂x
p(2)
jl (m)
2
∂x
p(3)
jl (m)
3
)
and
pjl(m)
=

3
i=1 p(i)
jl (m).
1. The eigenelements



λ(i)
k , ej⟨ωk,x⟩


c(i)
k1
c(i)
k2
c(i)
k3





with normalizing constants ∥ck∥2 = 1
satisfy
λ(i)
k


c(i)
k1
c(i)
k2
c(i)
k3

=


A11(ωk)
A12(ωk)
A13(ωk)
A21(ωk)
A22(ωk)
A23(ωk)
A31(ωk)
A32(ωk)
A33(ωk)




c(i)
k1
c(i)
k2
c(i)
k3

,
i = 1, 2, 3,
(9.112)
Ajl(ωk) =
njl

m=1
ajl(m) (jωk1)p(1)
jl (m)(jωk2)p(2)
jl (m)(jωk3)p(3)
jl (m),
j, l = 1, 2, 3.
(9.113)

9.6 GAUSSIAN FIELDS IN THE UNIT CUBE
275
2. Let W(x), x ∈X be a random Gaussian process with covariance operator KW =

∞
k=0

3
i=1 |α(i)
k |2φ(i)
k ⟨φ(i)∗
k
, ·⟩. Then if L is such that 
∞
k=0

3
i=1

|α(i)
k |2
|λ(i)
k |2

< ∞
the solution U of the random equation
LU(x) = W(x)
(9.114)
is a quadratic mean continuous Gaussian process with orthogonal expansion
U(x)
q.m.
=
∞

k=0
3

i=1
U(i)
k φ(i)
k (x).
(9.115)
U(i)
k
zero-mean orthogonal Gaussian variates, variances

|α(i)
k |2
|λ(i)
k |2

, with U(·) having
covariance operator
KU =
∞

k=0
3

i=1
|α(i)
k |2
|λ(i)
k |2 φ(i)
k ⟨φ(i)
k , ·⟩.
(9.116)
Proof
Applying Lφ(i)
k = λ(i)
k φ(i)
k gives Eqn. 9.112. That the quadratic mean limit exists
follows from the trace class assumption 
k

3
i=1

|α(i)
k |2
|λ(i)
k |2

< ∞; the rest follows from
Theorem 9.44 and Corollary 9.43 above with U satisfying LU = W.
Remark 9.6.2
(Real expansions) In general the eigenfunctions are complex. For
expansions of real-valued ﬁelds, Uk = U∗
−k, so that the quadratic mean expansions
reduce to a real expansion in sines and cosines. The reduced set of real expansion
functions correspond to φk + φ∗
k , −j(φk −φ∗
k ), with eigenvalues L(φk + φ∗
k ) = λk + λ∗
k,
and L(φk −φ∗
k ) = (λk −λ∗
k).
Example 9.46 (Boundary Conditions)
For small deformation elasticity Christensen
[221,226] has used
L = −a∇2 −b∇∇+ ǫ id ,
(9.117)
where ∇2 and ∇are the Laplacian and divergence operators, ∇2 = (∂2/∂x2
1) +
(∂2/∂x2
2) + (∂2/∂x2
3), ∇= (∂/∂x1, ∂/∂x2, ∂/∂x2)∗, id is the identity operator, and a, b in
terms of the the Lame elasticity constants are a = µ0, b = λ0+µ0. The particular mixed
boundary conditions which have been studied correspond to the Von-Neuman and
Dirichlet boundary conditions (ﬁrst used by Amit et al. [225]) mapping the boundary
to the boundary with sliding along the sides of the cube.
Amit, Grenander, Piccioni For the 2D square domain X = [0, 1]2, examine
the operator, let u = (u1, u2)∗and consider the Laplacian operator Lu = ∇2u =
(∂2/∂x2
1)u + (∂2/∂x2
2)u satisfying the boundary conditions:
u1(0, x2) = ∂u1/∂x1(0, x2) = u1(1, x2) = ∂u1/∂x1(1, x2) = 0
u2(x1, 0) = ∂u2/∂x2(x1, 0) = u2(x1, 1) = ∂u2/∂x2(x1, 1) = 0.
(9.118)
Classical Fourier analysis suggests eigenfunctions can be expressed as a linear com-
bination of sin iπx1 sin jπx2, sin iπx1 cos jπx2, cos iπx1 sin jπx2 and cos iπx1 cos jπx2
where i, j
=
0, 1, . . . . Inspection of the boundary conditions suggest the two

276
9 SECOND ORDER AND GAUSSIAN FIELDS
eigenvectors are
φ(1)
i,j = (sin iπx1 cos jπx2, 0)∗,
φ(2)
i,j = (0, cos iπx1 sin jπx2)∗.
(9.119)
It is easy to show Lφ(k)
i,j = λi,jφ(k)
i,j where λi,j = −π2(i2 + j2).
Christensen: Laplacian Any integer multiple of the above eigenfunctions is also an
eigenfunction. Thus we can have
φ(1)
i,j
= (i sin iπx1 cos jπx2, 0)∗,
φ(2)
i,j = (0, i cos iπx1 sin jπx2)∗.
(9.120)
It is easy to show Lφ(k)
i,j = λi,jφ(k)
i,j where λi,j = −π2(i2 + j2).
Christensen: Cauchy-Navier Let ∇=
 ∂
∂x1
, ∂
∂x2
∗
. Now consider the operator
L = ∇∇. So Lu becomes


∂2u1
∂x2
1
+
∂2u2
∂x1∂x2
∂2u2
∂x2
2
+
∂2u1
∂x1∂x2


.
(9.121)
Using Christensen’s idea for the Laplacian, the eigenvectors are
φ(1)
i,j = (i sin iπx1 cos jπx2, j cos iπx1 sin jπx2)∗,
φ(2)
i,j = (−j cos iπx1 sin jπx2, i cos iπx1 sin jπx2)∗.
(9.122)
The trick is to note that the cross-derivatives involving the other component suggest
that the component of the other eigenvector must play a role in satisfying Lφ(k)
i,j
=
λi,jφ(k)
i,j where λi,j = −π2(i2 + j2). The same result is achieved for L = ∇2 + b∇∇.
Example 9.47 (1D case)
Examine the 1D setting U(t), t ∈[0, 1] satisfying
LU(t) = W(t) ,
t ∈[0, 1].
(9.123)
Choosing L a constant coefﬁcient differential operator with circulant boundary con-
ditions, and W(·) cyclo-stationary Gaussian white noise process with covariance
KW(t, s) = 
k ej2πk(t−s), gives U cyclo-stationary with spectrum induced by the
structure of the differential operator.
Let L = 
p
m=0 am(∂m/∂tm) with circulant bondary conditions be invertible with
eigenvalues non-zero: ∀k ∈Z, 
p
m=0 am(j2πk)m ̸= 0. Then U(t), t ∈[0, 1] solving
Eqn. 9.123 is a Gaussian process with Green’s function and covariance
G(t, s) =
∞

k=−∞
1

p
m=0 am(j2πk)m ej2πk(t−s) ,
(9.124)
KU(t, s) = G∗KWG(t, s) =
∞

k=−∞
1
| 
p
m=0 am(j2πk)m|2 ej2πk(t−s).
(9.125)

9.6 GAUSSIAN FIELDS IN THE UNIT CUBE
277
Example 9.48 (Laplacian circulant boundary)
Let L be the Laplacian operator over
t ∈[0, 1], L = −(∂2/∂t2) + c, with random process (−∂2U(t)/∂t2) + cU(t) = W(t).
The eigenfunctions φk(t) = ej2πkt have eigenvalues λk = (2πk)2 + c; notice the
operator is invertible for c > 0. Then L = L∗, with Green’s function and covariance
G(t, s) =
∞

k=−∞
1
(2πk)2 + cej2πk(t−s) ,
(9.126)
KU(t, s) =

[0,1]
G(t, r)G∗(r, s) dr =
∞

k=−∞
1
((2πk)2 + c)2 ej2πk(t−s).
(9.127)
Notice, LG is the identity operator since
LG(t, s) =
∞

k=−∞
L 1
λk
φk(t)φ∗
k (s) =
∞

k=−∞
ej2πk(t−s) = δ(t −s).
(9.128)
Example 9.49 (Green’s Operator Laplacian, Zero Boundary)
Take t ∈[0, 1] and the
Laplacian operator L = ∂2/∂t2 deﬁning the S.D.E. ∂2U(t)/∂t2 = W(t) with boundary
conditions U(0) = U(1) = 0 and W white noise. The Green’s function is given by
G(t, s) = 1
2|t −s| + a + b(t −s) ,
(9.129)
with a, b satisfying the boundary conditions G(0, s) = G(1, s) = 0, s ∈[0, 1].
This gives a = s2 −s, b = s −(1/2) which implies the Green’s function is
G(t, s) = 1
2|t −s| + t(s −1
2) −1
2s.
(9.130)
Checking at (0, s) or (1, s) shows the boundary conditions are satisﬁed. To show
LG(t, s) = δ(t −s), taking the ﬁrst derivative gives
∂G(t, s)
∂t
= 1
2 (u(t −s) −u(−t + s)) +

s −1
2

,
(9.131)
with the second derivative giving (∂2G(t, s))/(∂t2) = δ(t −s).
The eigenfunctions and eigenvalues are φk(t) = (1/
√
2) sin 2πkt, λk = −(2πk)2.
For W a white noise ﬁeld, then
KU(t, s) =

k
1
(2πk)4 sin 2πkt sin 2πks
(9.132)
and
∂2KU(t, s)
∂t∂s
=

k
1
(2πk)2 cos 2πkt cos 2πks.
(9.133)

278
9 SECOND ORDER AND GAUSSIAN FIELDS
Thus the second mixed partial derivative covariance condition exists on the diagonal,
and it follows that U is q.m. differentiable by Theorem 9.13.
9.6.1 Maximum Likelihood Estimation of the Fields: Generalized ARMA
Modelling
The operator L plays the role of a pre-whitening operator. To be empirical introduce the basic gener-
ating operator L0, and an associated polynomial operator consisting of powers of L0, generalizing
AR modeling:
L = p(L0) = adLd
0 + ad−1Ld−1
0
+ · · · + a0id
(9.134)
with the unknown parameters ad, ad−1, . . . a0 estimated from the data.
Assume the set
{U(1), . . . , U(N)} of vector ﬁelds are given and are generated: U(n)
k
= ⟨φk, U(n)⟩, n = 1, . . . , N.
It follows that the eigenvalues are polynomials of the original eigenvalues.
Corollary 9.50
Let L = p(L0) = 
d
m=0 amLm
0 with L0 a linear shift invariant normal
invertible operator on X = [0, 1]3 with eigenvalues and eigenfunctions {λ0
k, φk}.
Then L has identical eigenfunctions {φk} as given by Theorem 9.45, with eigenvalues
satisfying
λk = p(λ0
k) =
d

m=0
am(λ0
k)m.
(9.135)
If in turn 
k

3
i=1(|α(i)
k |2/|λk|2) < ∞, with W a Gaussian process with covariance
KW = 
k

3
i=1 |α(i)
k |2φ(i)
k ⟨φ(i)
k , ·⟩,
then
Theorem
9.45
is
in
force
and
U
q.m.
=
limn→∞

∞
k=0

3
i=1 U(i)
k φ(i)
k .
The maximum-likelihood estimate of the spectrum is given by |ˆσ (i)
k |2 = |α(i)
k |2/|ˆλk|2
where ˆλk = ˆp(λ0
k) = 
d
m=0 ˆam(λ0
k)m satisfying the MLE equations for each j = 0, 1, . . . , d:
0 = ∂
∂aj
log p(U(1), . . . , U(n); a)
(9.136)
= −N

k
(λ0
k)j
ˆp(λ0
k)
+

k
ˆp(λ0
k)(λ0
k)j
α(i)2
k
N

n=1
|U(n)
k
−¯Uk|2 = 0.
(9.137)
Differential operators which are mixtures L = 
i aiL(i) depend upon the parameters and
to solve the maximum likelihood problem iteratively in general requires re-computation of the
eigenvectors. However, if the operators commute, this will not be necessary.
Corollary 9.51
Let L = 
m amL(m), L(m) a shift invariant normal differential operator
of the form given in Theorem 9.45, with the property that each of the operators commute
L(m)L(m′) = L(m′)L(m). Then L(m), m = 1, 2, . . . have identical eigenfunctions L(m)φk =
λ(m)
k
φk for all m. The eigenvalues of L = 
m amL(m) are λk = 
m amλ(m)
k
.

9.6 GAUSSIAN FIELDS IN THE UNIT CUBE
279
The MLE equations for each j = 0, 1, . . . , d
0 = ∂
∂aj
log p(U(1), . . . , U(N); a)
(9.138)
= −N

k
λ(j)
k
 
m ˆamλ(m)
k
 +

k
 
m ˆamλ(m)
k

λ(j)
k
|α(i)
k |2
N

n=1
|U(n)
k
−¯Uk|2.
(9.139)
Proof
The only thing to be proved is that the eigenfunctions are equal
for the various operators L(m), m = 1, 2, . . . .
The eigenfunctions take the form
ej⟨ωk,x⟩
c(m)
k1 , c(m)
k2 , c(m)
k3
∗
, implying that for the operators to have the same eigen-
functions it must be shown c(m)
k
= c(m′)
k
. Since the operators L(m), L(m′) commute, the
matrices

A(m)
,

A(m′)
from Eqn. 9.113 commute implying they have the same
simple eigenvectors associated with their simple eigenvalues, and c(m)
k
= c(m′)
k
.
Example 9.52 (Stationary Navier Elasticity Operator)
Examine the elasticity oper-
ator L = −a∇2 −b∇· ∇+ cid studied extensively [220–222].Then the operator
matrix from Theorem 9.45 has entries Aii = −a∇2 −b(∂2/∂x2
i ) + c, i = 1, 2, 3, and
Ail = −b(∂2/∂xi∂xl), i ̸= l, with
Aii(ωk) = −a
3

m=1
(jωkm)2 −b(jωki)2 + c
i = 1, 2, 3,
(9.140)
Ail(ωk) = −b(jωki)(jωkl)
i ̸= l.
(9.141)
The eigenfunctions and eigenvalues take the form
φ(1)
k (x) = α1
=
ωk1ej⟨ωk,x⟩, ωk2ej⟨ωk,x⟩, ωk3ej⟨ωk,x⟩>∗
,
λ(1)
k
= (2a + b)

ω2
k1 + ω2
k2 + ω2
k3

+ c ,
φ(2)
k (x) = α2
=
−ωk2ej⟨ωk,x⟩, ωk1ej⟨ωk,x⟩, 0
>∗
,
λ(2)
k
= a

ω2
k1 + ω2
k2 + ω2
k3

+ c,
(9.142)
φ(3)
k (x) = α3
=
ωk1ωk3ej⟨ωk,x⟩, ωk2ωk3ej⟨ωk,x⟩, −

ω2
k1 + ω2
k2

ej⟨ωk,x⟩>∗
,
λ(3)
k
= a

ω2
k1 + ω2
k2 + ω2
k3

+ c,
with the coefﬁcients α scaling each eigenvector to unit energy
α1 =

1/(ω2
k1 + ω2
k2 + ω2
k3),
α2 =

1/(ω2
k1 + ω2
k2),
α3 =

1/((ω2
k1 + ω2
k2)(ω2
k1 + ω2
k2 + ω2
k3)).
(9.143)
Since the operator is self-adjoint, λk = λ−k, the real eigenelements become
{φ(i)
k
+ φ(i)
−k, 2λ(i)
k }. Now consider the maximum-likelihood estimation of the parame-
ters associated with the linear differential operator L = −aLA −bLB + c id, with LAU =
∇2U, LBU = ▽▽·U. Examine the method for calculating the eigenvalues. L depends
upon parameters and to solve the maximization problem iteratively would require
re-computation of the eigenfunctions. This is not necessary due to the fact that

280
9 SECOND ORDER AND GAUSSIAN FIELDS
LA, LB, id are normal and commute implying they have identical eigenfunctions.
Notice the eigenfunctions are independent of a, b, c as predicted by Corollary 9.51.
Denote the eigenvalues of A and B by {λ(i)A
k
} and {λ(i)B
k
}, then the eigenvalues
just add because of the commutativity, Corollary 9.51,
λ(i)
k
= (aλ(i)A
k
+ bλ(i)B
k
+ c) .
(9.144)
In this case
λ(1)A
k
= −(ω2
k1 + ω2
k2 + ω2
k3),
λ(2)A
k
= −(ω2
k1 + ω2
k2 + ω2
k3),
λ(3)A
k
= −(ω2
k1 + ω2
k2 + ω2
k3),
λ(1)B
k
= −(ω2
k1 + ω2
k2 + ω2
k3),
λ(2)B
k
= 0,
λ(3)B
k
= 0,
λ(1)C
k
= 1,
λ(2)C
k
= 1,
λ(3)C
k
= 1.
9.6.2 Small Deformation Vector Fields Models in the Plane and Cube
Now we examine a problem from Computational Anatomy corresponding to small deformations
in which the Gaussian random ﬁelds act as deformation of the underlying coordinate system,
so that U : x →x −U(x). Chapter 16 addresses this problem in great detail. Examine the unit
cube for the background space X = [0, 1]3. Since only ﬁnite many anatomies can be observed,
the class of covariances may be restricted using symmetry properties associated with the physical
deformation of the tissues. For the random models, the covariance and random structure is viewed
as arising through the fact that {U(x), x ∈X} is thought to be the solution of a stochastic PDE of
the type,
L U(x) = W(x),
(9.145)
{W(x), x ∈X} a Gaussian random process with covariance KW. Assume the noise W is “white"
with covariance I, I the identity operator. Herein we focus on the Laplacian and elasticity oper-
ator, arising via a continuum mechanics construction, and corresponding to various mixtures of
differential operators. As well, various other forms arise, including the bi-harmonic (describing
small deformations energetics of thin plates), Laplacian, etc.
To illustrate the use of the differential operator examine how it describes shape change
corresponding to the above equation. Say that a planar anatomy is shrinking or expanding,
then the group of transformations G take the form G : x = (x1, x2) →x −(U1(x), U2(x)).
Introduce the dilatation vector d = (d1, d2) ∈R2, then an anatomical shift is generated via
the solution of the random equation LU = W, with (LH0, WH0) associated with normal, and
(LH1, WH1) associated with disease. Under the two hypotheses π0, π1 correspond to the stochastic
equations
H0 : (∇2Uj)(x1, x2) = Wj(x1, x2) ,
j = 1, 2 ,
(9.146)
H1 : (∇2Uj)(x1, x2) = Wj(x1, x2) + d1 ,
j = 1, 2 ,
(9.147)
where W(·, ·) is noise and assume cyclic boundary conditions throughout. Then under the two
hypotheses, the log-priors take the form
H0 : log π ≃−
2

i=1
∥∇2Ui∥2
H1 : log π ≃−
2

i=1
∥∇2Ui −di∥2 , with
(9.148)

9.6 GAUSSIAN FIELDS IN THE UNIT CUBE
281
∇2Ui(x1, x2) = Ui(x1+1, x2)−2Ui(x1, x2)+Ui(x1−1, x2)+Ui(x1, x2+1)−2Ui(x1, x2)+Ui(x1, x2−1).
A slight modiﬁcation of this would be to allow the vectors d1, d2 to be space dependent, for
example pointing outwards or inwards from a center of the abnormality.
To illustrate such shifts, shown in the left panel of Figure 9.2 are deformations on an MRI
section from a human brain for various choices of the dilatation vector. The top left panel (a) shows
the original image with the area of dilatation depicted; the top right panel (b) shows a contracting
ﬁeld d1 < 0 and d2 < 0; the bottom left panel (c) shows an expanding ﬁeld d1 > 0 and d2 > 0; the
bottom right panel (d) shows a shearing ﬁeld d1 < 0 and d2 > 0. The right column shows similar
results for the ventricles.
Example 9.53
(Maximum-Likelihood
Estimation
of
Elasticity
Parameterized
Gaussian Vector Fields) To associate the orthogonal expansion of the Gaussian pro-
cess, Eqn. 17.1, with the stochastic PDE of Eqn. 9.145, choose {φ, λ} the eigenelements
of the variability operator L = −a∇2 −b∇· ∇+ cI, according to
Lφk(x) = λkφk(x).
(9.149)
Then with the random variables {U(d)
k
, k = 1, 2, . . . } orthogonal Gaussian random
variables with mean and variance EU(d)
k
= ¯U(d)
k
, E|U(d)
k
−¯U(d)
k
|2 = 1/|λ(d)
k |2 then
U(x)
q.m.
=

∞
k=0

3
d=1 U(d)
k
φ(d)
k (x) is a quadratic mean Gaussian process satisfying
LU(x) = E(x), with mean and covariance
¯U(x) =
∞

k=0
3

d=1
¯U(d)
k
φ(d)
k (x) ,
KU(x, y) =
∞

k=0
3

d=1
1
|λ(d)
k |2 φ(d)
k (x)φ(d)T
k
(y).
(9.150)
Deformation on MRI image
a
c
b
d
Deformation on Ventricles
a
c
b
d
Figure 9.2 Figures show deformations corresponding to the solution of the random equation,
LU = W. Left ﬁgure: Top left panel (a) shows the original image with the area of dilatation
depicted; the top right panel (b) shows a contracting ﬁeld d1 < 0 and d2 < 0; the bottom left
panel (c) shows an expanding ﬁeld d1 > 0 and d2 > 0; the bottom right panel (d) shows a shearing
ﬁeld d1 < 0 and d2 > 0. Right ﬁgure shows analogous deformations to the ventricles (see also
Plate 13).

282
9 SECOND ORDER AND GAUSSIAN FIELDS
Let u(n)
k
= ⟨u(n), φk⟩be the coefﬁcients generated from the nth map, n = 1, 2, . . . ,
then the log-likelihood written in the K−dimensional basis becomes
ℓ(a, b, c, ¯U; u(1), . . . , u(N)) = −1
2
K

k=0
3

d=1
log


2π
|λ(d)
k |2

−1
2
K

k=0
3

d=1
|λ(d)
k |2
×

1
N
N

n=1
u(d)
k (n) −¯U(d)
k

2
.
(9.151)
The elasticity coefﬁcients and other parameters parameterize the eigenvalues
λk. The extremum conditions become
(ˆa, ˆb, ˆc, ¯U) ←−arg max
a,b,c, ¯Uk
ℓ(a, b, c, ¯U; u(1), . . . , u(N)).
Asthemapsofthevariousanatomiesareassumedindependent, thentheloglikelihood
function is additive across the anatomical maps {un, n = 1, 2, . . . }.
Parameters can be estimated from the familiy of maps modeling the random
ﬁelds using the elasticity operator L = −a∇2 −b∇· ∇+ cI, with the particular Von-
Neuman and Dirichlet mixed boundary conditions chosen mapping the boundary to
the boundary [227]:
u1(0, x2, x3) = u1(1, x2, x3) = 0,
∂u1(x1, 0, x3)
∂x2
= ∂u1(x1, 1, x3)
∂x2
= ∂u1(x1, x2, 0)
∂x3
= ∂u1(x1, x2, 1)
∂x3
= 0.
u2(x1, 0, x3) = u2(x1, 1, x3) = 0
∂u2(0, x2, x3)
∂x1
= ∂u2(1, x2, x3)
∂x1
= ∂u2(x1, x2, 0)
∂x3
= ∂u2(x1, x2, 1)
∂x3
= 0.
(9.152)
u3(x1, x2, 0) = u3(x1, x2, 1) = 0
∂u3(0, x2, x3)
∂x1
= ∂u3(1, x2, x3)
∂x1
= ∂u3(x1, 0, x3)
∂x2
= ∂u3(x1, 1, x3)
∂x2
= 0.
The eigenvectors and eigenvalues of the elasticity operator L are mixtures of sines
and cosines (see example 9.46, Chapter 9 or [226]). The empirical estimation of the
variability was performed on cryosection monkey brain volumes. The three brains
used in this experiment were labeled 87A, 90C and 93G. 87A was mapped to both the
target brains 90C and 93G. The means { ¯U} and the values of the parameters a, b were
estimated from these empirical maps. The basis coefﬁcients were generated deter-
mining the displacement ﬁelds so as to correspond to the elasticity coefﬁcients (a, b)
estimated from the population. Random brains were generated from the empirical
template with these displacement vectors. Figure 9.3 shows three section (left to right)
in three randomly generated brains (top to bottom).

9.6 GAUSSIAN FIELDS IN THE UNIT CUBE
283
Figure 9.3 Three sections for each of three randomly generated macaque monkey brains.
9.7 Discrete Lattices and Reachability of Cyclo-Stationary Spectra
While the representations are formulated on the continuum, the computations are on the ﬁnite
lattices associated with the discrete images. Similar results hold as above. To illuminate, restrict for
convenience to the class of discrete operators with periodic boundary conditions. The background
space becomes a discrete 3-torus, X = ZN3 and the operators are difference operators which are
cyclo-stationary in the sense that addition is done modulo N.
The eigenfunctions and eigenvalues are of the same form as in Theorem 9.45, but with the
constants given by the discrete Fourier series of the ﬁnite difference coefﬁcients.
Corollary 9.54
Let the discrete shift invariant (cyclo-stationary) normal invertible operators
L be of the form
LU(n) =


LU1(n)
LU2(n)
LU3(n)

=



3
l=1

h∈H a1l(h)Ul(n + h)

3
l=1

h∈H a2l(h)Ul(n + h)

3
l=1

h∈H a3l(h)Ul(n + h)

,
n = (n1, n2, n3) ∈{0, 1, . . . , N −1}3,
(9.153)

284
9 SECOND ORDER AND GAUSSIAN FIELDS
withtheoperatorshavingﬁnitesupport, H ⊂Z3
N. Theeigenfunctionsandeigenvalues{φk, λk}
solve the matrix equation of Theorem 9.45, Eqn. 9.112, with the constants Ajl, the discrete
Fourier transforms of the difference coefﬁcients:
Ajl(ωk) =

h∈H
ajl(h)ej⟨ωk,h⟩,
j, l = 1, 2, 3,
ωk =
2πk1
N
, 2πk2
N
, 2πk2
N

. (9.154)
Then {U(n), n ∈ZN3} satisﬁes the random difference equation LU(n) = W(n); U(·)
is Gaussian with covariance as above.
The question arises, how general a family of operators can be obtained from such differential or
difference operators, or equivalently, how general a family of cyclo-stationary spectra?. Restrict to the
class L of discrete normal operators with periodic boundary conditions on X = ZN3. Examine
operators generated from polynomials of normal operators L0 as L(L0) ⊂L. It will be shown
that if an operator has lower multiplicities in its eigenspectrum than the generating operator L0 it
cannot be obtained as a polynomial from L0, otherwise it is reachable.
The operators commute with identical eigenvectors φk = φk1k2k3 and eigenvalues of L0 given
by λk, k = (k1, k2, k3) ∈Z3
N. Some of them may be multiple, which will be the case typically if the
operator has symmetries. Let the corresponding multiplicities be
mk = mk1k2k3,
k1, k2, k3 = 1, 2, . . . , N.
(9.155)
Now, if L0 has the eigenvalues (2, 2, 3, 3, 4, 4, 5, 5) so that the multiplicities are 2, then an operator
L ∈L with the eigenvalues (1, 1, 5, 5, 4, 4, 3, 3) or (1, 1, 1, 1, 7, 7, 6, 6) can be obtained, however one
with eigenvalues (1, 2, 9, 9, 8, 8, 7, 7) cannot.
Theorem 9.55
Given L ∈L, and generating operator L0 of L(L0). Then an operator L ∈L
can be obtained as a polynomial in L0, L ∈L(L0), if its multiplicities are not less than those
of L0:
mk ≥m0
k,
k ∈ZN3.
(9.156)
Proof
Write the operators L0 and L in spectral decomposition:
L0 =

k
λ0
kPk,
L =

k
λkPk,
(9.157)
where Pk is the projection operator down to the sub-space spanned by φk:
Pk = φk⟨φk, ·⟩: (ℓ2(X))3 →span{φk}.
(9.158)
Notice, operators being normal implies eigenfunctions are orthogonal. For any poly-
nomial p(·) transforming L0 to L, L = p(L0), we obtain the transformed eigenvalues
λL
k = p(λ0
k). It remains to show that there is a polynomial that makes the transformed
eigenvalues equal to the ones prescribed for L: p(λ0
k)
=
λk, k = 1, . . . , N3. Given
two ﬁnite sets 0 = {λ0
k},  = {λk} of numbers, the distinct eigenvalues of L0 and L,
respectively; therefore ﬁnd a polynomial of some degree d such that the numbers in
the ﬁrst set are mapped into the numbers in the second. The only case when this is
not possible is when || > |0| but this can be ruled out if the multiplicities behave
as required in the proposition.
Essentially, this theorem is a statement of operator symmetry. The new variability operator
L should not have less symmetry structure than the original one L0. This answers deﬁnitively how
general a class of auto-regressive spectra can be obtained.

9.7 DISCRETE LATTICES
285
Example 9.56 (1-D and 3-D Laplacian examples)
Examine two examples, 1D and
3D cases. In 1-D consider the second difference operator;
L0u(n) = u(n + 1) −2u(n) + u(n −1);
n ∈ZN.
(9.159)
The eigenvalues are then
λ0
k = 2

cos 2πkn
N
−1

;
k = 0, 1 . . . , N −1.
(9.160)
Say that N is even; the odd case is dealt with similarly. Then the multiplicities m0
k =
2; ∀k so that if L ∈L is to be expressed as a polynomial in L0 it must have double
eigenvalues. But the eigenvalues of any symmetric circulant matrix L = (cn−m; n, m =
1, 2 . . . , N) are proportional to λk ∝
n cn expj2πkn/N. Since the matrix is symmetric,
cn−m = cn−m, the c-sequence is even and the eigenvalues appear in pairs. Hence L can
be expressed as a polynomial and to use the class L(L0) implies no loss of generality.
The 3-D discrete Laplacian L0 =  has the eigenvalues
λ0
k = 2

cos 2πk1
N
+ cos 2πk2
N
+ cos 2πk3
N
−6

.
(9.161)
This operator has more symmetry, as not only is there the symmetry λ0
−k1k2k3 = λ0
k1k2k3
but the symmetries λ0
k2k1k3 = λ0
k1k2k3 as well, and so on. In this case the restriction to
the class L(L0) is essential.
9.8 Stationary Processes on the Sphere
Stationarity can be generalized to other background spaces. Examine the 2D manifold the sphere
X = S2, assumed centered at the origin deﬁned in the standard azimuth-elevation representation.
Here the generalization of translation to the spherical background space is rotation of points with
the orthogonal group SO(3). Random processes which are shift invariant with respect to the
orthogonal group on the sphere we shall call isotropic or stationary on the sphere. It will follow that
all stationary processes on the sphere can be written via the spherical harmonic basis; spherical
harmonics play the role of the complex exponentials in that they become the eigenfunction of
covariances which are shift invariant on the sphere.
To establish the Karhunen-Loeve representation, deﬁne the inner product on the sphere in
azimuth-elevation coordinates according to which
x(θ, ψ) = (cos θ sin ψ, sin θ sin ψ, cos ψ),
θ ∈[0, 2π),
ψ ∈[0, π].
(9.162)
The surface measure in local azimuth-elevation θ, ψ coordinates given by dx(θ, ψ) = sin θ dθ dψ:
⟨g, h⟩L2(S2) =
 2π
0
 π
0
g(x(θ, ψ))h(x(θ, ψ)) sin ψ dψ dθ.
(9.163)
The complete orthonormal base on the sphere is constructed from the even and odd spherical
harmonics. First deﬁne the normalized Legendre functions Pmn ,m = 1, . . . , n, according to
Pm
n (cos ψ) =
1
2nn! sinm ψ

dn+m((x2 −1)n)
dxn+m
x=cos ψ
,
with Pn(x) = P0
n(x).
(9.164)
Then the complex spherical harmonics are deﬁned as direct analog of the cosines and
sines.

286
9 SECOND ORDER AND GAUSSIAN FIELDS
Deﬁnition 9.57
Deﬁne the even, odd spherical harmonics to be
for m = 1, . . . , n,
φe
nm(θ, ψ) =

((2n + 1)(n −m)!)/(2π(n + m)!) Pm
n (cos ψ) cos mθ
(9.165)
φo
nm(θ, ψ) =

((2n + 1)(n −m)!)/(2π(n + m)!) Pm
n (cos ψ) sin mθ,
(9.166)
with φn0(θ, ψ) =

(2n + 1)/4π Pn(cos ψ).
(9.167)
Lemma 9.58
Then
the
complex
spherical
harmonics
deﬁned
as
φn0
with,
for
m = 0, 1, . . . n, n = 1, 2, . . . ,
φnm(θ, ψ) =
1
√
2

φe
nm(θ, ψ) + jφo
nm(θ, ψ)
	
,
(9.168)
form a complete orthonormal base.
Proof
For completeness see [228]; for orthonormality within an order n, there are
n+1 even harmonics and n odd harmonics which are orthogonal for m ̸= m′, φnm, φnm′
since the complex exponentials are orthogonal for different frequencies m ̸= m′ over
multiple cycles,
 2π
0
φnm(θ, ψ)φnm′(θ, ψ)∗dθ = δ(m −m′).
To show orthogonality across orders n ̸= n′, use the orthogonality of Legendre
polynomials:
 2π
0
 π
0
Pm
n (cos ψ)Pm′
n′ (cos ψ) sin ψ dψ dθ =
 1
−1
Pm
n (x)Pm′
n′ (x) dx = 0 for n ̸= n′,
(9.169)
which gives ⟨φn′, φn⟩L2(S2) = 0 for n ̸= n′.
Figure 9.4 shows spherical harmonics visualized on the sphere. For wide-sense stationarity
shift and distance between two points on the surface of the unit sphere x, y ∈S2 is deﬁned to be
the arc with cosine the angle between their vector representation in R3:
d(x, y) = arc cos⟨x, y⟩R3,
(9.170)
where ⟨x, y⟩R3 is the inner product of the two vectors connecting the origin to the points x, y. From
this stationarity is deﬁned as follows.
Deﬁnition 9.59
The random ﬁeld {U(ω, x), x ∈S2} is isotropic or wide-sense station-
ary on the sphere if the covariance is a function of only the distance d(x, y) between the
points x, y on the sphere:
KU(x, y) = EU(ω, x)U(ω, y)∗= KU(Ox, Oy),
O ∈SO(3).
(9.171)
Just as the complex exponentials forming the Fourier series are eigenfunctions of the shift
operator on the cube, the spherical harmonics are eigenfunctions of the shift operator on the sphere
(see Theorem 9.84 below). Shift is operation by the orthogonal group. It follows that all stationary
correlated processes on the sphere can be written via the spherical harmonic orthogonal expansion,
the so-called orthogonal Oboukhov expansion [9]. For characterizing the variation of 2D manifolds,
vector ﬁelds on smooth surfaces have been used [33,229]. Associate with the surfaces real-valued
random 3-vector ﬁelds {U(ω, x) ∈R3, x ∈S2}, and expand the vector ﬁelds in orthogonal blocks
using the C.O.N. basis constructed from the spherical harmonics φ(i)
nm = φnm1(i), i = 1, 2, 3.

9.8 STATIONARY PROCESSES ON THE SPHERE
287
0
0.5
1
1.5
2
2.5
3
3.5
–0.06
–0.04
–0.02
0
0.02
0.04
0.06
0.08
K(theta), theta=[0:pi]
Figure 9.4 Panels 1–7 show the spherical harmonics 2–8 on the unit sphere computed numerically
from the Laplacian operator on the sphere. Panel 8 shows the eigenvalues of the shift invariant
covariance associated with the Laplacian operator (see also Plate 14).
Theorem 9.60
Lettherandomscalarﬁeld{U(ω, x), x ∈S2}beq.m. continuousonthesphere
with continuous covariance kernel K(x, y), x, y ∈S2. Then {U(x), x ∈S2} is stationary on
the sphere with eigenelements {σ 2nm, φnm} the spherical harmonics and covariance kernel
KU(x, y) =
∞

n=1
σ 2
n
2n + 1
2π
Pn(cos d(x, y))
if and only if
U(x)
q.m.
=
lim
N→∞
N

n=1
n

m=0
Unmφnm(x)
(9.172)
uniformly x ∈S2 with Unm =

S2 φ∗nm(x)U(x) dx zero-mean orthogonal random
variables
EUnmU∗
n′m′ = σ 2
nm = δ(n −n′)δ(m −m′)σ 2
n σ 2
n, with

n
σ 2
n < ∞.
Let {U(x) = (U1(x), U2(x), U3(x))∗, x ∈S2} be a vector valued random ﬁeld on the
sphere. Expanding the vector ﬁeld in orthogonal vectors using the C.O.N. basis constructed
from the spherical harmonics φ(i)
nm = φnm1(i) , i = 1, 2, 3, then the q.m. representation
U(x)
q.m.
=
∞

n=1
n

m=0
3

i=1
U(i)
nmφ(i)
nm(x)
(9.173)
with the orthogonal vectors {Unm = (U(1)
nm, U(2)
nm, U(3)
nm)∗} is stationary on the sphere.
Proof
The quadratic mean continuity corresponds to the summability condition

n σ 2n < ∞with Karhunen–Loeve holding since the Legendre polynomials in the
eigenfunctions are continuous on the interval [−1, 1].

288
9 SECOND ORDER AND GAUSSIAN FIELDS
The orthogonal expansion implies the covariance is of the form
KU(x, y) = EU(x)U∗(y) = E
lim
N,N′→∞
N

n=1
N′

n′=1
n

m=0
Unmφnm(x)
n′

m′=0
Un′m′φn′m′(y)
(9.174)
=
∞

n=1
n

m=0
σ 2
nφnm(x)φ∗
nm(y) (a)
=
∞

n=1
σ 2
n
2n + 1
2π
Pn(cos d(x, y)),
(9.175)
with (a) given by the Addition Theorem of spherical harmonics.28To show sta-
tionarity, then for all rotation elements O ∈SO(3), we must show KU(Ox, Oy) =
KU(x, y) which follows since d(x, y) = d(Ox, Oy).
In part (2), let the random ﬁeld {U(x), x ∈S2} on the sphere be a stochastic
process stationary with respect to the group SO(3) and also stationarily correlated
so that
EU(x)U∗(y) = EU(Ox)U∗(Oy),
x, y ∈S2,
O ∈SO(3).
(9.177)
Oboukhov’s representation theorem establishes the expansion of the components of
the U-ﬁeld in terms of the spherical harmonics φnm(·) as
Ui(x)
q.m.
=
∞

n=0
n

m=0
U(i)
nmφnm(x),
x ∈S2,
i = 1, 2, 3,
(9.178)
with covariances E(U(i)
nmU(i)
n′m′) = δ(n −n′)δ(m −m′)σ (i)2
n
.
For an arbitrary 3-vector a = (a1, a2, a3) consider the scalar stochastic process
on the sphere Ua(x) = 
3
i=1 aiUi(x). Since Ui(·), i = 1, 2, 3 are stationarily correlated
it follows that Ua(·) is also stationary on the sphere:
EUa(Ox)Ua(Oy) =
3

i,j=1
aiajEUi(Ox)Uj(Oy) (a)
=
3

i,j=1
aiajEUi(x)Uj(y)
(9.179)
= EUa(x)Ua(y) ,
(9.180)
with (a) following from the stationarity of U(·). We can then apply the Oboukhov
orthogonal representation to it giving
Ua(x) =
∞

n=1
n

m=0
Ua
nmφnm(x) ,
EUa
nmUa
n′m′ = δ(n −n′)δ(m −m′)σ a2
n .
(9.181)
Since the {φnm} are orthogonal the expansion coefﬁcients are uniquely determined;
this gives Uanm = 
3
i=1 aiU(i)
nm and EUanmUa
n′m′ = 
3
i,j=1 aiajEU(i)
nmU(j)
n′m′. Now let one
of the components of a be zero, the others 1, for example a = (1, 1, 0). Then
EUa
nmUa
n′m′ = EU(1)
nmU(1)
n′m′ + EU(2)
nmU(2)
n′m′ + 2EU(1)
nmU(2)
n′m′.
(9.182)
28 The Addition Theorem ([228], p. 268) of spherical harmonics corresponds to for any two points x, y ∈S2
with d(x, y) = arc cos⟨x, y⟩denoting the solid angle between x, y denoted as elements R3, then
2n + 1
4π
Pn[cos d(x, y)] =
n

m=0

φe
nm(x)φe
nm(y) + φo
nm(x)φo
nm(y)
	
= 1
2
n

m=0

φnm(x)φnm(y)∗	
.
(9.176)

9.8 STATIONARY PROCESSES ON THE SPHERE
289
But if (nm) ̸= (n′m′), the left-hand side vanishes as well as the two ﬁrst terms
on the right-hand side. Thus we have shown the orthogonality of U(1)
nm, U(2)
n′m′, and in
the same way we show orthogonality for any pair U(i1)
nm , U(i2)
n′m′; i1 ̸= i2.
Introduce the 3 × 3-matrix valued spectral density
nm = EUnmU∗
nm
(9.183)
of the uncorrelated random 3-vectors Unm = (U(1)
nm, U(2)
nm, U(3)
nm)∗. Then we have
obtained a spectral representation of a vector valued stationary process on the sphere
EUnmU∗
n′m′ = δ(n −n′)δ(m −m′)n.
9.8.1 Laplacian Operator Induced Gaussian Fields on the Sphere
Examine random ﬁelds induced on the surface of the sphere through elastic shell deforma-
tions. Spherical harmonics are eigenfunctions of the Laplacian operator on the sphere (see [228],
pp. 258–263) with eigenvalue n(n + 1):
∇2φnm =
1
sin ψ
∂
∂ψ
8
sin ψ ∂φnm
∂ψ
9
+
1
sin2 ψ
∂2φnm
∂θ2
= −n(n + 1)φnm.
(9.184)
Corollary 9.61
Then the random ﬁelds {U(ω, x), x ∈S2} solving
LU(x) = W(x),
where L = ∇2 =
1
sin ψ
∂
∂ψ
8
sin ψ ∂
∂ψ
9
+
1
sin2 ψ
∂2
∂θ2 ;
(9.185)
W(·) white Gaussian noise on the sphere are quadratic mean continuous stationary on the
sphere
KU(x, y) =
∞

n=1
1
(n(n + 1))2
2n + 1
2π
Pn(cos d(x, y)).
(9.186)
Proof
The spherical harmonics of order n, φnm have eigenvalues λnm = n(n + 1),
implying with W(·) a white Gaussian process on the sphere, then from Theorem 9.43
U(·) is Gaussian with covariance kernel
KU(x, y) =
∞

n=1
n

m=0
1
(n(n + 1))2
)
*+
,
σ 2nm
φnm(x)φnm(y)
(9.187)
(a)
=
∞

n=1
1
(n(n + 1))2
2n + 1
2π
Pn(cos d(x, y)),
(9.188)
where d(x, y) is the solid angle between the points x, y ∈S2 and Pn is the Legendre
polynomial (p. 325 [230]) and (a) follows via the addition Theorem Eqn. 9.176 for
spherical harmonics [228]. Then the ﬁeld satisﬁes the trace class condition.
Bakircioglu [231] uses the following algorithm for computing the covariance. Given the
distance on the sphere of radius R between two points ρ(x, y), then the solid angle is calculated in

290
9 SECOND ORDER AND GAUSSIAN FIELDS
degrees according to θ(x, y) = (ρ(x, y))/(2πR) × 360 with the Legendre function in the covariance
calculation, Eqn. 9.188, given by Pn(cos θ(x, y)) where Pn(x) is calculated using the recursion [228]
P0(x) = 1,
P1(x) = x,
P2(x) = 1
2(3x2 −1)
(9.189)
nPn(x) = x(2n −1)Pn−1(x) −(n −1)Pn−2(x).
(9.190)
Shown in the last panel 8 of Figure 9.4 is the eigen spectrum of the shift invariant associated with
the Laplacian operator.
Example 9.62 (Active Deformable Surface Spheres for Amoeba)
Optical-sectioning
microscopy is proving to be a powerful tool for 3-D visualization of living biological
specimens for the study of cell motion which plays a critical role during embryoge-
nesis, and is crucial to a wide range of physiological processes, including embry-
onic development, and wound healing [232]. In computational optical-sectioning
microscopy (COSM) conventional ﬂourescence images are acquired in a series of focal
planes spanning the specimen. As described in Chapter 2, Example 2.55, the optical
characteristics and statistical models for various microscope/CCD/optical systems
have been characterized (e.g. see [30,235]) based on Poisson statistical models for the
data. To visualize in vivo 3D motion of cells in the slime mold Dictyostelium discoideum,
individual cells are labeled with a ﬂuorescent dye. The amoebae aggregate to form a
multicellular mass that undergoes dramatic shape changes.
Single cells under motion are modeled as active deformable spheres by
Joshi [33] generated via global translation and normal deformation of the sphere
according to
g(θ, ψ) =


x0
y0
z0

+


cos θ sin ψ
sin θ sin ψ
cos ψ

+ U(θ, ψ)


nx(θ, ψ)
ny(θ, ψ)
nz(θ, ψ)

,
ψ ∈[0, π],
θ ∈[0, 2π).
(9.191)
with n(θ, ψ) ∈R3 the normal to the sphere at θ, ψ, and U(θ, ψ) is the random scalar
ﬁeld parameterizing the translation groups.
Now choose as the prior that is induced by the energetics of elastic membranes.
From Poisson’s equation for pressure ﬁelds acting on ﬂat membranes, the energy

S2 ∥∇2U(x(θ, ψ))∥2 dx(θ, ψ), with dx(θ, ψ) = sinθ dθ dψ, denotes that which induces
a stationary random ﬁeld. The random scalar transformation ﬁeld U(θ, ψ) expanded
via the real orthogonal spherical harmonic expansion as a stationary process with
covariance induced according to Corollary 9.61 by the Laplacian operator for elasticity.
Then U is the quadratic mean limit
U(θ, ψ)
q.m.
=
lim
N→∞
N

n=0
n

m=−n
Unmφnm(θ, ψ),
(9.192)
where the φnm are spherical harmonics (even and odd) on the surface of the sphere
and the Unm are real valued, Gaussian random variables with E{UnmUn′m′} =
δ(n −n′)δ(m −m′)λn ∝(1/(n(n + 1))2).
The simplest model Joshi has explored assumes the measurements are a simple
Poisson counting process {M(dx), x ∈} with intensity λ(x) = λin, x ∈D(g(γ )), and

9.8 STATIONARY PROCESSES ON THE SPHERE
291
λ(x) = λout, x ∈D(g(γ ))c, the log-posterior with Poisson data term and Gaussian
prior, becomes
H(γ ) = −

D(g(γ )
λin(x) dx +

D(g(γ )
log λin(x)M( dx) + α

mn
(n(n + 1))2(Unm)2
−

D(g(γ ))c λoutx dx +

D(g(γ ))c log λout(x)M( dx).
Then with Theorem 8.41 and Corollary 8.48 from Chapter 6, compute the
Jacobian J(θ, ψ; γ ) = | det Dθ,ψ,γ g(γ )|, and compute the gradients in parameters:


∂H(γ )/∂x0
∂H(γ )/∂y0
∂H(γ )
∂z0

= −

θ

ψ
(λin(θ, ψ) −λout(θ, ψ))n(θ, ψ)J(θ, ψ) dψ dθ
+

θ

ψ

log λin(θ, ψ)
λout(θ, ψ)

n(θ, ψ)J(θ, ψ)M( dθ dψ)
∂H(γ )
∂unm
= −

θ

ψ
(λin(θ, ψ) −λout(θ, ψ))φnm(θ, ψ)J(θ, ψ) dψ dθ
−αn(n + 1)Unm
+

θ

ψ

log λin(θ, ψ)
λout(θ, ψ)

φnm(θ, ψ)J(θ, ψ)M( dθ dψ),
where the surface, g(θ, ψ), is parameterized with the azimuth angle, θ, and the zenith
angle, ψ. For optical sectioning microscopy the set of measurements is determined
by the physics of data collection. The emission intensity is assumed constant inten-
sity, λin, inside the shapes, with known ﬁxed background intensity, λout, and due to
imperfect optical focusing the measurements {Mj(du), u ∈D} correspond to projec-
tions of emissions in the 3D space onto the 2D CCD detector plane D intensity of the
jth measurement, µj(u), determined by the optical point-spread function focused on
plane j, denoted as pj(u|x) focused on position x:
µj(du) =

R3 pj(u|x)λ(x)dx, where
(9.193)
λin = λ(x)
∀x ∈D(g(γ )),
λout = λ(x),
∀x ∈D(g(γ ))c.
(9.194)
The posterior density includes the normal density on the expansion coefﬁcients
in the spherical harmonic basis, giving
H(γ ) =
J

j=1

D

D(g(γ ))
pj(y|x)(λin −λout) dx dy

+

mn
(n(n + 1))2(Unm)2
(9.195)
−
J

j=1

D
log
A
D(g(γ ))
pj(y|x)λin dx +

D\D(g(γ ))
pj(y|x)λout dx
B
Mj( dy)

.
Now to compute the gradient with respect to parameters of the surface; analo-
gous to the case in R2 for active contours the variations have the elegant property that
volume integrals reduce to surface integrals around the respective shapes. Assume
the one-parameter family of surfaces f(γ ) with f = 0 corresponding to the boundary
∂D(g(γ )) of the interior D(g(γ )).

292
9 SECOND ORDER AND GAUSSIAN FIELDS
Theorem 9.63
The derivative of the posterior potential ∂H(g(γ ))/∂γ of
Eqn. (9.260) with respect to parameters γ ∈{Unm}, the spherical harmonic
coefﬁcients, becomes
∂H(γ )
∂γ
= −
J

j=1

D

[0,2π)×[0,π](λin −λout)pj(y|x(θ, φ, γ ))J(θ, φ, γ ) ds

D(g(γ )) pj(y|x)λin dx +

D\D(g(γ ) pj(y|x)λout dx ]Mj( dy)

+

[0,2π)×[0,π]
(λin −λout)J(θ, φ, γ ) ds + ∂
∂γ

mn
(n(n + 1))2(Unm)2.
(9.196)
The algorithm was implemented by Joshi [33, 198]. The surface of the sphere
on which the deformations are deﬁned is discretized on to a lattice to the lattice
L = {[0, 1, · · · , N −1] × [0, 1, · · · , N −1]} in the set of discretization points

U(l1, l2) =
8
sin
2πl1
N

cos
2πl2
N

, sin
2πl1
N

sin
2πl2
N

, cos
2πl1
N
9
,
(l1, l2) ∈L

.
(9.197)
The integrals and the partial derivatives in Eqn. 9.196 are approximated on this
mesh by ﬁnite sums and differences on this lattice. Data for the shape reconstruction
was a 3D phantom with a “pseudopod" extension. Different contrasts were generated
to evaluate the performance of the algorithm with different counting rates. Panel 1
of Figure 9.5 shows the 3D phantom section. Panels 2 and 3 in Figure 9.5 show X–Z
sections through the 3-D data set from the optical sectioning microscope for the high
(1)
(2)
(3)
(4)
(5)
(6)
Figure 9.5 Top row shows 2D X-Z sections with pseudopod (panel 1) and high contrast data (panel
2) and low contrast data (panel 3). Bottom row (panel 4) shows 3D surface; (panel 5) shows high
contrast, and (panel 6) low contrast surface reconstruction using active sphere.

9.8 STATIONARY PROCESSES ON THE SPHERE
293
contrast (panel 2) and low contrast (panel 3) data. Notice the profound blurriness of
the data resulting from the optical sectioning point-spread function. Also notice the
speckle effect due to the Poisson nature of the CCD noise at low contrast. Notice also
that the structure of the phantom is lost due to the point-spread function and the
CCD noise. The bottom row of 9.5 show the 3D object surface (panel 4) along with
the reconstruction via active spheres of the high contrast (panel 5) and low contrast
(panel 6) data.
9.9 Gaussian Random Fields on an Arbitrary Smooth Surface
For the past 15 years investigators have been studying patterns via orthonormal bases on smooth
surfaces. Pentland and Sclaroff [236] pioneered such approaches in computer vision via determin-
istic approaches, and Joshi et al. [229] extended these ideas to the construction of Gaussian random
ﬁelds on anatomical manifolds. Essentially the geometry of arbitrary surfaces are studied using a
generalized notion of complete orthonormal bases on surfaces called, “surface harmonics”. These
generalize spherical harmonics to smooth 2D submanifold in R3. From such a complete orthonor-
mal base, smoothing, interpolation, and estimation driven by mean ﬁelds and covariance ﬁelds
can be performed completely analogously to the more regular cases in R3 and the sphere.
In this section we study this idea for arbitrary smooth surfaces following the work of Qiu and
Bitouk by construcing complete orthonormal bases and Gaussian random ﬁelds via the Laplace
Beltrami operator, the generalized version of the Laplacian operator. Deﬁne a smooth surface
M parameterized by u = (u1, u2). Any point on M has coordinates x = {x1(u), x2(u), x3(u)}.
Random ﬁelds are constructed as a quadratic mean limit with continuous covariance kernel on
the manifold embedded into R3, which can be represented by a complete set of orthonormal bases
of the Laplace-Beltrami operator in the Hilbert space. In the following, we shall construct the
Gaussian random ﬁelds as a quadratic mean limit using a complete orthonormal bases of Laplace-
Beltrami operator {φk, k = 1, 2, . . .}. The Gaussian ﬁeld is deﬁned to be the quadratic mean limit as
Y(u)
q.m.
= 
∞
k=1 Ykφk(u), with mean and covariance ﬁelds
¯Y =

k
µkφk, KY =

k
σ 2
k φkφ∗
k ,
(9.198)
where Yk, k = 1, 2, . . . are independent, Gaussian random variables with means and variances,
EYk = µk, E|Zk|2 = σ 2
k .
9.9.1 Laplace-Beltrami Operator with Neumann Boundary Conditions
Qiu and Bitouk solve for the orthonormal eigenelements of the Laplace-Beltrami operator λ, φ(·)
under Neumann boundary conditions. The basis solves the eigen function problem:
−φ(u) = λφ(u), subject to

φ(u)2 dM = 1 ,

M
φi(u)φj(u) dM = δ(i −j)
(9.199)
with boundary conditions ⟨∇φ(u), n⟩for all u ∈|∂M= 0, where  is the Laplace-Beltrami operator,
and n is the normal vector on the boundary of M.
As a consequence of the divergence theorem, ﬁnding the solution to the above partial
differential equation is equivalent to solving the associated variational problem.

294
9 SECOND ORDER AND GAUSSIAN FIELDS
Theorem 9.64
The eigenfunctions of Eqn (9.199) for ﬁxed eigenvalues λ with associated
boundary conditions satisfy the variational problem
E(φ(u)) =

M
∥▽φ(u)∥2 dM −λ

M
φ(u)2 dM.
(9.200)
Proof
For a ﬁxed eigenvalue λ, take the variation φ →φ + ǫν according to
∂νE(φ) = 2

M
⟨▽φ(u), ▽ν(u)⟩dM −2λ

M
⟨φ(u), ν(u)⟩dM
(9.201)
(a)
= −2

M
φ(u)ν(u) dM −2λ

M
⟨φ(u), ν(u)⟩dM = 0,
(9.202)
with (a) following from the divergence theorem and the application of the boundary
conditions.
Because of convexity, it is straightforward to see that this optimization problem
always has a unique solution.
To construct the orthonormal bases on smooth surfaces of the Laplace-Beltrami operator Qiu
and Bitouk use the ﬁnite element method. For this, in Figure 9.6, deﬁne three vertices in a triangle
T as P1, P2, P3 with coordinates (x1, y1), (x2, y2), (x3, y3), respectively. For this deﬁne triangle Ti
with area Ai. The algorithm takes the following form.
Algorithm 9.65 (Finite Element Method for Generation of the CON)
Approximation via Shape Functions. Assume the polynomial approximation for function
φ(x, y) on the plane given by
φ(x, y) = a + bx + cy,
(9.203)
where coefﬁcients a, b, and c are unknown and determined by function values at these three
vertices as


a
b
c

=


1
x1
y1
1
x2
y2
1
x3
y3


−1 

φ(x1, y1)
φ(x2, y2)
φ(x3, y3)

.
(9.204)
The determinant of the 3 × 3 matrix in Eqn. 9.204 is the area of triangle Ti, denoted as Ai.
The function value φ(x, y) at any point x, y within the triangle is approximated by a convex
combination of weights φ(x, y) = 
3
i=1 αi(x, y)φ(xi, yi), determined by the shape functions
j1
j2
j3
j1
j2
j
P
Ti
Ti
uijj2
uijj1
Figure 9.6 Left panel shows triangle element with vertices P1, P2, P3 and arbitrary point within the
triangle P. Right panel shows one ring centered at vertex j with triangle Ti having three vertices
j, j1, j2 and angle θijj1 and θijj2 opposite to edges jj1 and jj2, respectively.

9.9 GAUSSIAN RANDOM FIELDS
295
αi satisfying 
3
i=1 αi(x, y) = 1 given by the ratio of areas [237,238] of contributing triangles
according to
α1(x, y) = 1
2A
6
(x2y3 −x3y2) + (y2 −y3)x + (x3 −x2)y
7
,
α2(x, y) = 1
2A
6
(x3y1 −x1y3) + (y3 −y1)x + (x1 −x2)y
7
,
(9.205)
α3(x, y) = 1
2A
6
(x1y2 −x2y1) + (y1 −y2)x + (x2 −x1)y
7
.
For example in the left panel of Figure 9.6 α1 is equal to the ratio of the area of the shaded
triangle to that of the total area Ai.
Within triangle Ti, the discrete energy ETi(φ(u)) becomes
ETi(φ(u)) =

Ti

|| ▽φi(u)||2 −λφi(u)2
dM
=
3

j=1
3

k=1

Ti
6
⟨▽αij, ▽αik⟩−λαijαik
7
φ(vij)φ(vik) dS .
(9.206)
1. Construct Discrete Version of the Energy on each Triangle. Simplifying each of the terms,
ﬁrst

T αiαjdS becomes by change of variables [239]

T
αiαj dS = 2A
 1
0
 1−αj
0
αiαj dαi dαj =
C
A/6
i = j
A/12
i ̸= j
,
and taking in partial derivatives of αi gives

T
⟨∇αi, ∇αi⟩dS =

T
|PjPk|2
4A2
dS =
|PjPk|2
4A
(9.207)
= 1
2(cot ∠Pj + cot ∠Pk) ,
(9.208)
and for i ̸= j ̸= k,

T
⟨∇αj, ∇αk⟩dS =

T
⟨−−→
PiPj, −−→
PkPj⟩
4A2
dS
(9.209)
= −1
2 cot ∠Pi .
(9.210)
2. Matrix form of the Total Energy: Deﬁne NT the number of triangles on the triangulated
mesh, and Nv the number of vertices. with vertex locations vi1, vi2, vi3, Deﬁne NT(j) as the
set of triangles containing vertex j, and NT(i, j) as the set of triangles containing edge (i, j). θi23
denotes the angle opposite to the edge j2j3 in triangle Ti (shown in the left panel of Figure 9.6).
Let the indices of three vertices in triangle Ti be j1, j2, and j3, respectively. Assume Ti on the

296
9 SECOND ORDER AND GAUSSIAN FIELDS
plane. The left panel of Figure 9.6 shows a triangle Ti with three vertices j1, j2, and j3. Deﬁne
the 3 × 3 matrices Ki =

Ki(j, k) =

Ti αijαik dS

, Gi =

Gi(j, k) =

Ti⟨▽αij, ▽αik⟩dS

, along
with the vectors
i =


φ(vi1)
φ(vi2)
φ(vi3)


T
,
Ki = Ai
12


2
1
1
1
2
1
1
1
2

,
Gi = 1
2


cot θi12 + cot θi13
−cot θi12
−cot θi13
−cot θi12
cot θi12 + cot θi23
−cot θi23
−cot θi13
−cot θi23
cot θi13 + cot θi23

.
The total energy is obtained by assembling all triangle elements. The energy within each
triangle becomes ETi(φ(u)) = ∗
i Gii −λ∗
i Kii giving the total energy
E() =
NT

i=1
ETi(φ(u)) =
NT

i=1
6
∗
TiGiTi −λ∗
TiKiTi
7
.
(9.211)
3. Reorganize in matrix form via vertex contributions. Deﬁne the Nv × Nv sparse matrices
G =

Gij

, K =

Kij

with entries
Gjj = 1
2

i∈NT(j)

cot θijj1 + cot θijj2

,
Gjk = −1
2



i∈NT(jk)
cot θijk


(9.212)
Kjj = 1
6

i∈NT(j)
Ai ,
Kjk = 1
12

i∈NT(jk)
Ai .
(9.213)
G is a semi-positive deﬁnite matrix. As depicted in the right panel of Figure 9.6 the jth rows of
matricesG, K correspondtovertexj. K isapositivedeﬁnitematrixsinceitisstrictlydiagonally
dominant and diagonal entries are positive. The total energy in matrix form becomes
E() = ∗G −λ∗K .
(9.214)
4. Solve variational problem for CON basis of eigen-elements.
Deﬁne K a diagonal matrix with all positive eigenvalues of matrix K as entries, and V
is a normal matrix with eigenfunctions as columns associated with eigenvalues. Then the
eigenelements (λ, λ) subject to normalization minimizing the total energy in Eq. 9.214 is
given by
λ =
inf
λ:∗
λKλ=1 ∗
λGλ −λ∗
λKλ
= V−1/2λ ,
where λ satisfy
−1/2
K
V∗GV−1/2
K
λ = λλ .
−1/2
K
V∗GV−1/2
K
is a semi-positive deﬁnite matrix.
Example 9.66 (Qiu and Bitouk Surface Harmonics via Laplace Beltrami)
Qiu and
Bitouk generate these bases on the neocortex of the brain. For this they construct the
triangulated mesh for each volume using three steps: (i) segmentation of the MR tis-
sue as white matter, gray matter, and cerebrospinal ﬂuid (CSF) voxels using Bayesian

9.9 GAUSSIAN RANDOM FIELDS
297
Figure 9.7 Rows 1 and 2 show surface harmonics 1–8 of the Laplace Beltrami operator on the
planum temporale. Rows 3 and 4 are identical for the central sulcus. Central sulcus results are
visualized via a bijection to the plane. Surface harmonics taken from Qiu and Bitouk (see also
Plate 15).
segmentation [53,54], (ii) 3D isocontouring [54], and (iii) dynamic programming delin-
eation of the boundary of the associated gyral and sulcal submanifolds, from which
the submanifold of interest is automatically extracted from the surface masked by
subvolume ROI [154,183],
Shown in the top two rows of Figure 9.7 are results showing the ﬁrst eight sur-
face harmonics corresponding to the eigenfunctions of the Laplace Beltrami operator
computed for the planum temporale. The bottom two rows show similar results for
the central sulcus. The central sulcus is a highly curved sulcus in the brian and is
visualized by deﬁning a bijection to the plane. All the surface harmonics are from Qiu
and Bitouk.
9.9.2 Smoothing an Arbitrary Function on Manifolds by Orthonormal Bases of
the Laplace-Beltrami Operator
The function model on the manifold M associated with the smoothing problem is
Y(u) = ¯Y(u) + W(u),

298
9 SECOND ORDER AND GAUSSIAN FIELDS
where ¯Y(u) is the unknown function to be estimated from the observable Y, and W is zero mean
Gaussian noise with variance σ 2. Deﬁning the Green’s function
G(u1, u2) =
∞

i=1
1
λi
φi(u1)φi(u2),
(9.215)
the estimator of ¯Y is given by
ˆ¯Y = arg min
¯Y(·)

M
∥∇¯Y∥2 dM + γ
Nv

i=1
(Y(vi) −¯Y(vi))2,
(9.216)
=
Nv

k=1
βkG(u, uk),
(9.217)
¯Y(vi) and bi are the function value and the observational data at vertex i, respectively. Deﬁning
the Nv vector β and Nv × Nv matrix G on the vertices, then
β = arg min
β
β∗(G + γ G∗G)β −γβ∗G∗Y
=
 1
γ G + G∗G
−1
G∗Y.
In practice, the constant component of Y(u) is removed by extracting the average value from
Y(u). In addition, when λi/γ becomes large, the corresponding bases no longer contribute to ¯Y(u).
As for a positive function, such as cortical thickness, the logarithm of the function is smoothed.
Example 9.67 (Qiu and Bitouk)
Examine the application to the expansion of curva-
ture ﬁelds on the neocortex. Notice the ﬁrst eigenvalue is close to zero, which leads
to the trivial solution of the associated orthonormal basis. Shown in Figure 9.8 are the
curvature proﬁles expanded in the complete orthonormal basis of the Laplace Beltrami
operator. In practice, the constant component is excluded in the estimate of ¯Y(u) by
extracting the average value

X Y dx from Y(u).
Example 9.68
Examine the Laplacian operator on the discrete triangulated graph
representing a surface. Discretizing the Laplacian, assume N(x) consists of the six
closest neighbors to x and deﬁne the operator
Mf(x) = n(x)[f(x) −¯f(x)],
(9.218)
where ¯f(x) is the average of f over N(x) ∩S, i.e. ¯f(x) = 
y∈N (x) f(y) with n(x) =
card[N(x)].
The discrete Laplacian becomes L = M + aI, a > 0 a strictly positive constant.
Assume the neighborhood structure is symmetric so that y ∈N(x) implies x ∈N(y).
Then M is self-adjoint:
⟨M f, g⟩=

x
n(x)f(x)g(x) −

x
n(x)¯f(x)g(x)
(9.219)
=

x

y∈N (x)
f(x)g(x) −

x

y∈N (x)
f(y)g(x)
(9.220)
=

x

y∈N (x)
f(x)g(x) −

y

x∈N (y)
f(y)g(x) .
(9.221)

9.9 GAUSSIAN RANDOM FIELDS
299
0.3
0
–0.4
POSTCENTRAL
PRECENTRAL
PRECENTRAL
POSTCENTRAL
Figure 9.8 Figure shows planum temporal. Shown are the curvature proﬁles expanded in the
complete orthonormal basis of the Laplace Beltrami operator (see also Plate 16).
Rewriting the last term using the symmetric neighborhood relation gives the self
adjoint relationship
⟨M f, g⟩=

x

y∈N (x)
f(x)g(x) −

x

y∈N (x)
f(x)g(y)
(9.222)
= ⟨f, M g⟩.
(9.223)
That the stochastic equation LU = W has a unique solution follows from the fact
that M is non-negative deﬁnite, implying M + aI is positive deﬁnite. For this examine
⟨Mf, f⟩=

x

y∈N (x)
f2(x) −

x

y∈N (x)
f(x)f(y)
(9.224)
= 1
2

x

y∈N (x)

f(x) −f(y)
	2 ≥0 .
(9.225)
Thus, A = M + aI is a positive-deﬁnite self-adjoint operator.
9.10 Sample Path Properties and Continuity
Thus far we have examined continuity in the mean-squared sense. We will be interested in
continuity almost surely.

300
9 SECOND ORDER AND GAUSSIAN FIELDS
Deﬁnition 9.69
The process {Y(ω, t), t ∈T} is said to be almost surely sample path
continuous if
Pr{ω : lim
h→0 sup
t∈T
|Y(t + h) −Y(t)| = 0}
= 1 −Pr
?
t∈T
{ω : lim
h→0 |Y(t + h) −Y(t)| ̸= 0}
(9.226)
= 1.
(9.227)
Similarly for ﬁelds, Y(ω, x), x ∈X,
Pr{ω : lim
h→0 sup
x∈X
|Y(x + h) −Y(x)| = 0}
= 1 −Pr
?
x∈X
{ω : lim
h→0 |Y(x + h) −Y(x)| ̸= 0}
(9.228)
= 1.
(9.229)
For the m-vector case, almost sure continuity is taken componentwise.
The following theorem on almost-sure continuity of sample paths of the random processes and
ﬁelds are taken from Wong and Hajek [213] and Karatzas and Shreve [214].
Theorem 9.70
1. (Continuity for Processes [213]) Given random process {Y(t), t ∈T ⊂R}, if there exists
strictly positive constants α, β, C such that
E|Y(t + h) −Y(t)|α ≤Ch1+β
(9.230)
then
sup
t∈T
|Y(t + h) −Y(t)| a.s
→0 .
(9.231)
2. (Continuity for Fields [214]) Let {Y(x), x ∈X ⊂Rd}, if there exists strictly positive
constants α, β, C such that
E|Y(x + h) −Y(x)|α ≤C ∥h ∥d+β
(9.232)
then
sup
x∈X
|Y(x + h) −Y(x)| a.s
→0 .
(9.233)
Proof
The proof of (1) follows Proposition 4.2 of Wong and Hajek [213]; see
Appendix A.1. For the proof of the multi-dimensional ﬁeld case, see Karatzas and
Shreve [214].
The random ﬁelds are modeled as Gaussian random vector ﬁelds {U(x), x ∈[0, 1]d ⊂Rd}.
Smoothness on the ﬁelds is forced by assuming they are solutions of stochastic partial differen-
tial equations of the type L U
=
W, L a normal differential operator. Index the variables in
the d-dimensional cube x ∈[0, 1]d, where L is a constant coefﬁcient differential operator of the
form:
L =
n

m=1
am
∂p(m)
∂xp1(m)
1
, . . . , ∂xpd(m)
d
+ ǫ,
with
d

l=1
pl(m) = p(m).
(9.234)

9.10 SAMPLE PATH PROPERTIES AND CONTINUITY
301
The eigenvalues and eigenfunctions Lφk(x) = λkφk(x) are of the form
φk(x) = ej2π⟨k,x⟩,
λk =
n

m=1
am
d

l=1
(j2πkl)pl(m) + ǫ ,
k ∈Zd .
(9.235)
We will require the operator to be strongly elliptic.
Deﬁnition 9.71
Deﬁne the operator L to be strongly elliptic of order q (Reed and Simon
[240]) if for all but a ﬁnite k
Re{λk} = Re



n

m=1
am
d

l=1
(j2πkl)pl(m)


≥∥k∥2q .
(9.236)
Example 9.72
Asimple example of an elliptic operator is the q power of the Laplacian,
L = (−∇2 + I)q which has such propertes, since with
∇2 = ∂2
∂x2
1
+ ∂2
∂x2
2
+ ∂2
∂x2
3
+ · · · + ∂2
∂x2
d
,
(9.237)
then the eigenvalues are λk = ∥2πk∥2 + 1; for Lq, λk = (∥2πk∥2 + 1)q, k ∈Zd.
For almost sure-continuity we follow Amit and Piccioni [241] and do the scalar valued case.
Theorem 9.73 (Joshi [242])
Let U(x), x = (x1, x2, x3, . . . , xd) ∈X = [0, 1]d be a real-
valued Gaussian random ﬁeld deﬁned in the quadratic mean sense as above to satisfy the
partial differential equation
LU(x) = W(x) , x ∈[0, 1]d ,
(9.238)
where L is strongly elliptic of order q and the noise process W(x) = 
k Wkφk(x) is assumed
to have a spectral representation with spectral variances E|Wk|2 = |αk|2 bounded above by
∥k∥2p except on a ﬁnite set F ⊂Zd.
(i) If 4q −2p −2 > d then U(x) has sample paths which are almost surely continuous,
(ii) and if 4q −2p −4 > d then it is almost surely differentiable; similarly it is n-times
differentiable if 4q −2p −2(n + 1) > d.
Proof
To prove sample path continuity use the Kolmogorov criteria from Theorem
9.70 followingAmit and Piccioni [241]; then U(x), x ∈Rd is a.s. sample path continuous
if there exists constants α, β, c > 0 such that
E|U(x) −U(y)|α ≤c∥x −y∥d+β .
(9.239)
The proof exploits the following Lemma.
Lemma 9.74
If 4q −2p −2 > d then for all x, y ∈[0, 1]d,
E|U(x) −U(y)|2 ≤c∥x −y∥2 .
(9.240)

302
9 SECOND ORDER AND GAUSSIAN FIELDS
Proof
Proof of Lemma. Since U satisﬁes the stochastic partial differential equation
L U(x) = W(x), x ∈[0, 1]d, use the spectral representation giving
E|U(x) −U(y)|2 =


k∈Zd
Wk
λk
(φk(x) −φk(y))

(9.241)
≤

k∈Zd
|αk|2
|λk|2 |ej2π⟨k,x⟩−ej2π⟨k,y⟩|2 =

k∈Zd
|αk|2
|λk|2 (2 −2 cos 2π⟨k, x −y⟩)
=

k∈Zd
|αk|2
|λk|2 (4 sin2 π⟨k, x −y⟩) ≤

k∈Zd
|αk|2
|λk|2 c|⟨k, x −y⟩|2
≤

k∈Zd
|αk|2
|λk|2 c∥x −y∥2∥k∥2
(a)
≤c∥x −y∥2

c1 +

k∈Zd/F
∥k∥2(p+1)
∥k∥4q

,
(9.242)
where (a) uses the fact that L is strongly elliptical of order q so that |λk|2 ≥∥k∥4q, and
|αk|2 ≤∥k∥2p, and the series converges for 4q −2p −2 > d.
Proof
To establish the Kolmogorof conditions with α and d + β, since U(x) is a zero
mean Gaussian ﬁeld, then U(x)−U(y) is a zero mean Gaussian random variable with
the moment generating function given by (s) = eσ 2s2/2 = 
∞
l=1(σ 2ls2l)/(2ll!), where
σ 2 = E|U(x) −U(y)|2. Then for any positive integer l since 4q −2 −2p > d we have
E|U(x) −U(y)|2l = 2l!
2ll!
(E|U(x) −U(y)|2)l
≤c(∥x −y∥2)l = c∥x −y∥2l.
(9.243)
Thentheprocesshasa.s. samplepathcontinuityasonchoosingα = 2l, β = 2l−d.
To show it has a.s. continuously differentiablility, examine the components of
the derivative of the vector ﬁeld U′(x) with mean square derivative, U′(x) having
spectral representation given by
U′(x) =
∂U(x)
∂x1
, · · · , ∂U(x)
∂xd

=

k∈Zd
j2πkWk(ω)
λk
ej2π⟨k,x⟩.
(9.244)
Then we need to show that each component is almost surely continuous, and since
U′ is Gaussian this requires us to show that for each j, E| ∂U(x)
∂xj
−∂U(y)
∂xj |2 ≤c∥x −y∥2.
Following the same property as above,
E

∂U(x)
∂xj
−∂U(y)
∂xj

2
≤c

k∈Zd
|αk|2
|λk|2 ∥2πk∥2|ej2π⟨k,x⟩−ej2π⟨k,y⟩|2
≤c∥x −y∥2

c1 +

k∈Zd/F
∥k∥2(p+2)
∥k∥4q

.
(9.245)
The series converges for 4q −2p −4 > d.
Similarly, for n-derivatives.

9.10 SAMPLE PATH PROPERTIES AND CONTINUITY
303
Example 9.75 (Wiener Process)
The Kolmogorov-Centsov Theorem 9.70 applies for
showing Brownian motion is almost surely sample path continuous. The increments
Y(t + h) −Y(t) are zero-mean Gaussian with variance h, implying |Y(t + h) −Y(t)|2
is chi-squared with variance 3h2. This implies E|Y(t + h) −Y(t)|4 = 3h2, and
Theorem 9.70 is satisﬁed for α = 4, β = 1, C = 3.
Example 9.76
It is helpful to examine the dependence on dimensions for various
differential operators For 1D ﬁelds x ∈X = [0, 1], and for the equation dU/dx = W(x),
L = d/dx, and for W of trace class fall quadratically in spectrum E|Wk|2 = 1/k2, then
d = 1, q = 1/2, p = −1 and 4q −2 −2p = 2 > d = 1. Thus U has a.s. sample path
continuity.
Example 9.77
Let U(ω, t) satisfy the random equation △pU = W with boundary
conditions U(0) = U(1) = 0, with W(t) = 
k Wk(ω)φk(t), φk eigen functions of the
Laplacian and Wk independent, zero-mean normal variances, α2
k.
If the Gaussian random process, W, has noise spectrum dominated above by
α2
k ≤ck2m for all but ﬁnitely many k, and △p a polynomial power of the Laplacian with
2p ≥r+m+2, then the U-ﬁeld has sample paths which are almost surely continuously
r-times differentiable. To see this, the covariance of the U-ﬁeld takes the form
KU(t, s) =

k
α2
k
(2πk)2(2p) sin 2πkt sin 2πks,
(9.246)
implying the covariance of the rth derivative
K ∂rU
∂tr (t, s) =

k
α2
k
(2πk)2(2p−r) sin

2πkt + πr
2

sin

2πks + πr
2

.
(9.247)
The second mixed derivative becomes bounded by the inequality for some ﬁnite
integer l and constant c1 < ∞
∂2K∂rU/∂tr/∂tr(t, s)
∂t∂s
≤c1 + c2

k>l
k2m
(2πk)2(2p−r−1) sin

2πkt + π(r + 1)
2

× sin (2πks + π(r + 1)/2) .
(9.248)
But this exists along the diagonal for 2p−r −m−1 ≥1, implying the process is almost
surely continuous.
9.11 Gaussian Random Fields as Prior Distributions in Point
Process Image Reconstruction
During the early 1980s it was recognized that image reconstruction as a density estimation prob-
lem in functions spaces was fundamentally inconsistent with purely likelihood based methods.
An unconstrained maximization of likelihood from ﬁnite data fails when maximizing in function
spaces. There are simply too many parameters ﬂoating around. In emission tomography images
which have been produced via purely unconstrained maximum-likelihood methods have been
observed by many investigators to have seemingly random artifacts (sharp peaks and valleys
distributed throughout the image ﬁeld) [243–254]. We point out that although the image recon-
struction problem has been described via a continuous model, the problem does not go away if the

304
9 SECOND ORDER AND GAUSSIAN FIELDS
image is constrained to histogram bins. Then the implementation has the undesirable property
that as it becomes ﬁner (decreasing pixel size) the problem becomes worse, a situation termed
dimensional instability by Tapia and Thompson [255]. To remedy the difﬁculty in emission tomog-
raphy, the Method of Sieves [9], and penalized likelihood methods [255–258] are used to estimate
inﬁnite dimensional functions from ﬁnite data sets. Here we examine penalty methods (appealing
to Good’s work [257,258]) and make explicit their connection to Gaussian random ﬁeld priors.
9.11.1 The Need for Regularization in Image Reconstruction
What is the role of regularization or smoothing via a Gaussian prior in image reconstruction?
Examine the classic density estimation problem from Poisson measurements with mean λ(x), x ∈
Rd. Assume measurements points occur at random places x1, x2, . . . , xN, with intensity λ(·). The
measurement processes N(dx) is a counting process (see [42], for more details on the development
of counting processes) with measurement locations of the counting process xi, i = 1, . . . , N, with
intensity EN(A) =

A λ(x) dx, A ⊂Rd, and maximizer given by
ˆλ = arg max
λ(·) −

λ(x) dx +

N( dx) ln λ(x) =
N

i=1
δxi( dx) .
(9.249)
Eqn. 9.249 is meant in the weak sense, so that the unconstrained MLE is not a function, but rather
the unconstrained maximizer is a Dirac measure centered at the points of the N observations.
Obviously this is unacceptable because it contradicts one’s a priori knowledge about almost all
imaging scenarios, where it is expected that λ(·) is bounded, and at least piecewise continuous.
An unconstrained maximization of 9.249 fails to produce meaningful estimates. The fundamental
problem is that the likelihood is unbounded above over the set of functions (densities). For ﬁnite
data sets there are too many parameters ﬂoating around. As noted in [259] the function spaces are
too large for direct estimation. Notice, placing the problem into histogram bins does not make it
go away, only results in a series of estimated pulses with heights proportional to the number of
observations in each pixel, (i) =
 (i+1)
i
N(dt). As  →0, then the estimator converges to a set
of Dirac delta-functions, centered at the points of the N observations.
We also emphasize that while the formulation in this section for the image reconstruction
problem is placed in the the Poisson setting precisely these issues arise in maximum-likelihood
reconstruction in function spaces from Gaussian processes. In the Gaussian setting for optical
ﬂow, see for example Poggio et al. [260] and Yuille and Grzywach [261]. In Medical imaging
Gaussian priors have been used by Levitan and Herman [262], Hart and Liang [263], and Herman
and Odhner [264]. Joyce and Root [265] have written an exquisite paper on Precision Gauges,
direcly analogous to Grenander’s Method of Sieves but restricted to the least-squares setting. Preza
et al. [235] employed these Precision Gauges to control the dimensional stability problem in the
Gaussian Random ﬁeld setting for image deconvolution problem.
9.11.2 Smoothness and Gaussian Priors
To constrain the maximization problem, introduce the roughness penalized estimators. Good
and Gaskins have argued for penalties based both on ﬁrst and second derivatives (curvature).
Lanterman has examined what he terms generalized Good’s roughness [254] introducing powers
of the Laplacian to increase the smoothing and connecting the penalty in the least-squares setting
to the theory of Butterworth ﬁlters. We state that penalty here.

9.11 POINT PROCESS IMAGE RECONSTRUCTION
305
Deﬁnition 9.78
Deﬁne the penalty
(Y) = ∥LY∥2
2with L =

i
ai∇2qi.
(9.250)
Given Z a Gaussian random ﬁeld with mean EZ = Y, then the penalized estimator
ˆY is deﬁned to be
ˆY = arg max
Y
2⟨Z, Y⟩2 −(Y) .
(9.251)
Given N a Poisson random ﬁeld with mean λ = Y2, then the penalized estimator ˆY
is given by
ˆY = arg max
Y

N(dx) ln Y2(x) −(Y) .
(9.252)
Inopticalﬂow[260] andemissiontomography[243] thesmoothingisgradientonly, q1 = 1/2.
Good and Gaskins argued for the gradient and Laplacian penalty, q1 = 1/2, q2 = 1 (ﬁrst and sec-
ond derivatives). In the context of Medical imaging, [266] has principally examined curvature
constraints, q2 = 1. Lanterman [254] has examined generalized Good’s roughness introducing
powers of the Laplacian to increase the smoothing in the resulting penalized methods. The intro-
duction of these generalized smoothing methods is quantiﬁed in part by several of the following
results characterizing the smoothness introduced by the penalty when it is viewed as specifying
the potential of a Gaussian random ﬁeld.
Theorem 9.79
Let Y be random solutions of
LY = W
with circulant b.c. on X = [0, 1]d ⊂Rd ,
(9.253)
W circulant white-noise on Y, variance 1.
Then for smoothness operators which are strongly elliptic of order q (the Laplacian to the
q power in the penalty), we have the following conditions for almost surely continuous samples
paths: (i) for d = 1, X = [0, 1] ⊂R, then q > 0.75; (ii) for X = [0, 1]2 ⊂R2, d = 2 then
q > 1.0; (iii) for d = 3, X = [0, 1]3 ⊂R3, then q > 1.25; (iv) for d = 4, X = [0, 1]4 ⊂R4,
then q > 1.5.
Proof
The proof is Theorem 9.73 given by the 4q −2 > d condition.
9.11.3 Good’s Roughness as a Gaussian Prior
Good was one of the earliest to argue [257,258] that a principled basis for introducing regularity
could be derived via principles of information. Good’s roughness measure (norm-squared gra-
dient normalized by variance) is a direct measure of the Fisher information on the location of a
known waveform at unknown position. Subsequently, regularization via the norm-square gradi-
ent penalty (for constant variance ﬁelds) has been used extensively in computer vision [260,261]
and the norm-squared gradient normalized by the variance for Poisson ﬁelds [243].
Good and Gaskin originally derived their ﬁrst-order penalty via Fisher discrimination argu-
ments. For Z(·) a Gaussian random ﬁeld on Rd, variance 1 with mean ﬁeld µ(·) = Y(·) a ﬁxed
waveform at unknown position p ∈Rd which is smooth (twice differentiable) with bounded

306
9 SECOND ORDER AND GAUSSIAN FIELDS
support, then the d × d Fisher information matrix has entries Fij = ⟨(∂Y/∂pi), (∂Y/∂pj)⟩2 with
trF = ∥∇Y∥2
2. This follows from
−E
∂2
∂pi∂pj

−∥Z −µ∥2
2

= E
 ∂Y
∂pi
∂Y
∂pi

2
−2

(Z −Y), ∂2Y
∂pi∂pj

2

=

∂Y
∂pi
, ∂Y
∂pj

2
.
(9.254)
For N(·) a Poisson random ﬁeld on Rd with mean ﬁeld (smooth as above, bounded support)
λ(·) = Y2(·) a ﬁxed waveform at unknown position p ∈Rd, then the d × d Fisher information
matrix has entries Fij = ⟨(1/
√
λ)(∂λ/∂pi), (1/
√
λ)(∂λ/∂pj)⟩2, with trF = ∥(∇λ/
√
λ∥2
2 = ∥∇Y∥2
2. This
follows from
−
∂2
∂pi∂pj

−

λ(x)dx +

N(dx) ln λ(x)

=

∂2
∂pi∂pj
λ(x)dx −
 N(dx)
λ(x)
∂2
∂pi∂pj
λ(x)
+
 N(dx)
λ(x)2
∂λ(x)
∂pi
∂λ(x)
∂pj
,
(9.255)
and taking exceptions gives the result.
In 1-D the ﬁrst order Good-roughness constraint is the norm-square derivative; its analog
in 2,3-D involves curvature via the Laplacian. In such cases they give exponential splines, and
are equivalently smooth in the sense (as proven below) that when viewed as a Gaussian prior
disribution sample-paths are quadratic mean-continuous and not almost-surely continuous.
9.11.4 Exponential Spline Smoothing via Good’s Roughness
Adding the potential of the Gaussian prior density to the maximum-likelihood problem gives
Good’s penalized methods explicitly involving the Green’s kernel of the Gaussian prior. The
solutions to the penalized estimator for Good’s roughness in 1-dimension, and extended to 2,3
dimensions via the Laplacian are as derived by Snyder and miller [243] for emission tomography
exponential splines of the data.
Corollary 9.80
The penalized estimators for the Gaussian and Poisson cases with potential
as given in the above Theorem 9.38 with Green’s kernel K(x −y) = β exp−1/α∥x−y∥2
Rd are
given by the convolution with the Green’s kernel:
ˆY(·) = arg max
Y
2⟨Z, Y⟩2 −(Y) =

K(· −y)Z(y)dy,
(9.256)
ˆY(·) = arg max
Y

N(dx) ln Y2(x) −(Y) =
 K(· −y)
Y(y)
N(dy) =
N

i=1
K(· −xi)
ˆY(xi)
.
(9.257)
Proof
For the Gaussian case, the the maximization solves the quadratic form. For
the nonlinear Poisson case, the penalized likelihood becomes
−∥LY(·)∥2
2 +

N(dx) ln Y2(x) = −⟨L∗LY, Y⟩2 +

N(dx) ln Y2(x) ,
(9.258)
where L∗L has Green’s kernel K. Maximizing gives the solution of 9.257.
The regularized estimator is a sum of exponential splines with knots at the data points.
Figure 9.9 shows results in 1 dimension illustrating the smoothing by the GRF. A Poisson-process

9.11 POINT PROCESS IMAGE RECONSTRUCTION
307
(AVG CTS= 1000)
(AVG CTS= 1000, BW= 0.3)
(AVG CTS= 1000, BW= 0.1)
Intensity (Thousands)
Intensity (Thousands)
6
6
5
4
3
2
1
0
0
0.2
0.4
0.6
0.8
1
4
2
0
0.2
0.4
0.6
x
x
0
0.2
0.4
0.6
0.8
1
x
0.8
0
Intensity (Thousands)
6
4
2
0
Figure 9.9 Panel 1 shows unconstrained likelihood estimator demonstrating dimensional instabil-
ity. Panels 2 and 3 show the likelihood estimators with Gaussian prior. Results taken from Snyder
and Miller [243].
was generated with a mean in each pixel of (i), where (i) =
 (i+1)
i
λ(x)dx,  the bin size. Panel
1 shows the likelihood estimates of the smooth proﬁle based on a Poisson simulation containing
an average of 1000 counts in the 512 bin simulation. The histogram shown in Figure 9.9 results
from a direct maximization of the unconstrained discrete likelihood of 9.249.
The 1D histogram of Figure 9.9 demonstrates the “dimensional instability” that the MLEs
exhibit. Notice the occurrence of large variations between adjacent pixel estimates of ; this effect
gets worse if the pixel size or the number of measurement points are decreased. Plotted in panels
2 and 3 are the results of applying the Gaussian prior. The estimates of Figure 9.9 were obtained
by solving the exponential spline estimate of 9.257 iteratively according to
λnew(x) =

λold(x)
N

i=1
K(x −xi)

λold(xi)
, x ∈[0, T] ,
(9.259)
where K(·) is given in Eqn. 9.78. For the initial estimate
√
λ(0) the square root of the histogram
estimate was used. Panels 2 and 3 show the estimates derived with increasing weight α implying
exponential splines of inreasing width. Clearly as the exponential width increases the effect is to
smooth the variations between adjacent estimates in the histogram.
Example 9.81 (Image Deconvolution in Space R2, R3)
Examine the algorithms for
image deconvolution in time-of-ﬂight positron emission tomography as originally
developed by Snyder [35,35,46,245] (see Chapter 2, Example 2.55). The point-spread
function p(·) reﬂects both the line-of-ﬂight and the perpendicular to the line of ﬂight
ambiguity. For time-of-ﬂight PET [35] they vary as a function of the projection direc-
tion. Emissions occur at random places Xi ∈R3 with intensity λ(x), x ∈R3 and are
observed with random errors, the random errors ǫi giving rise to independent mea-
surements Yi = Xi + ǫi. Each measurement has a different line of ﬂight with the value
corresponding to its point-spread function resulting in the multiple measurement pro-
cesses N1(dy), N2(dy), ... each with mean intensity ENj(A) =

A

R3 pj(y|x)λ(x)dx dy
generalizing Eqn. 2.173, Chapter 2.
Parameterizing in Y =
√
λ and adding the Gaussian prior ∥∇2qX∥2
2 gives the
penalized estimator
ˆY = arg max
Y
−∥Y∥2
2 −α2∥∇2qY∥2
2 +
J

j=1

Nj(dy) ln

pj(y|x)Y2(x)dx .
(9.260)

308
9 SECOND ORDER AND GAUSSIAN FIELDS
The EM algorithm used for maximizing the posterior [30] generalizes Eqn. 2.174 from
Chapter 2:
Ynew = arg
max
λ=Y2≥0
−∥Y∥2
2 −α2∥∇2qY∥2
2 +

R3 Yold2(x)
J

j=1
×

Y

pj(y|x)

R3 pj(y|x)(Yold)2(x)dx
Nj(dy)

ln Y2(x)dx .
(9.261)
Shown in Figure 9.10 are results on the Hoffman brain phantom from Lanter-
man with Gaussian random ﬁeld Good’s smoothing. Column 1 shows the Hoffman
brain phantom with the 1000th iteration unconstrained reconstruction shown in the
bottom panel below. Columns 2 and 3 row show reconstructions using Good’s ﬁrst-
order roughness penalty with q = 1/2 (column 2) and the Laplacian-squared for q = 2
(column 3). The penalty has q = 1/2 (column 1) and q = 2 (column 2) as above.
Example 9.82
(Deblurring and Denoising via Gaussian Random Fields Priors in
Gaussian Noise Models) We have in the previous section studied regularization in
point process image reconstruction. Now turn to the huge area where a great deal of
work has been done in which the observations Y are a Gaussian random ﬁeld with
mean ﬁeld, Y. The degraded image Z is observed as a function of the continuous
background space and the goal is to reconstruct Y(x), x ∈D; the optimizer is given by
inf
Y

D
(Z(x) −Y(x))2dx + α

D
∥∇Y(x)∥2dx.
(9.262)
Figure 9.10 Panel 1 shows example point-spread functions from time-of-ﬂight PET. Panel 2
(top row) show the Pie Phantom. Panel 3 (bottom row, right column) shows results of 1,000
EM iterations with no smoothing (α = 0) of Eqn. 2.174 from Chapter 2. Results taken from
Lanterman.

9.11 POINT PROCESS IMAGE RECONSTRUCTION
309
Then non-linear diffusion has been studied by many (see Guichard and Morel [267])
corresponding to functional gradient descent taking the form
∂Y
∂t = div∇Y + c(Z −Y).
(9.263)
Clearly, ﬁxed points correspond to the necessary conditions for a minimizer 9.262.
9.12 Non-Compact Operators and Orthogonal Representations
There are clearly many examples of great interest in which the index spaces are not compact.
Then, Riesz Schauder and Hilbert Schmidt do not hold; the spectrum is not discrete. This is well
appreciated by communications engineers; the Fourier transform on the entire real line is built
from a continuum of complex exponentials. Thus, the projection operations of transforms are
constructed using integration; the projections turn out to be measures which are integrated over
non discrete index sets.
This takes us towards orthogonal representations over non bounded index sets such as the
Cramer representation of stationary processes. This is a familiar setting as Fourier representations
which correspond to the spectral decomposition of the shift operator over the real line.
First deﬁne unitary operators on the Hilbert space, and projection operators.
Deﬁnition 9.83
A bounded linear operator U ∈L(H) is called a unitary operator if it
satisﬁes ∀g, h ∈H,
⟨g, h⟩H = ⟨Ug, Uh⟩H.
(9.264)
An operator P ∈L(H) is called a projection operator if P2 = P, and is orthogonal
if P = P∗.
Remark 9.12.3
Eigenvalues of unitary operators have magnitude 1, and eigenvectors
corresponding to distinct non-zero eigenvalues are orthogonal, for if φ, ψ are two
distinct eigenvectors, then
⟨φ, ψ⟩H = ⟨Uφ, Uψ⟩H = λ∗
1λ2⟨φ, ψ⟩H .
(9.265)
This implies if λ1 ̸= λ2 ̸= 0 then ⟨φ, ψ⟩H = 0; if φ = ψ then |λ1|2 = 1.
Here are several operators which are unitary; shift on the real line and scale. It is their
eigenfunctions which form the basis for the orthogonal expansions studied.
Theorem 9.84
Let Us ∈L(H) be a unitary operator, with eigenfunctions and eigenvalues
satisfying the fundamental equation Usφf (t) = λ(s)
f φf (t). Then shift and scale are unitary
operators with the following eigenelements.
Shift: Let H = L2(R, dt) be the Hilbert spaces with, for each s ∈Rd, Us being the
shift operator deﬁned for all g ∈H,
Us+s′g(t) = UsUs′g(t) = g(s + s′ + t).
(9.266)

310
9 SECOND ORDER AND GAUSSIAN FIELDS
Then Us is a unitary operator on H, with eigenelements
for s ∈R,
λ(s)
f
= ej2πfs,
φf (t) = ej2πft,
f ∈R.
(9.267)
Scale:
Let H
be the Hilbert space L2((0, ∞), (1/t)dt) with inner product
⟨g, h⟩L2((1/t)dt) =
 ∞
0
g(t)h(t)(1/t)dt. Then for each s ∈(0, ∞), let Us be the scale operator
deﬁned by for all g ∈H,
Us◦s′g(t) = UsUs′g(t) = g(s ◦s′t)
(9.268)
Then Us is a unitary operator on H with eigenelements
for s ∈(0, ∞),
λ(s)
f
= sjf ,
φf (t) = tjf .
(9.269)
Proof
Proof of shift: The unitary property is well known, just following from the
substitution of variable and Jacobian of shift being 1:

g(t + s)h(t + s)dt =

g(t)h(t)dt.
(9.270)
For the Torus, the domain is compact, therefore there is a discrete spectrum corre-
sponding to Fourier series; the complex exponentials give the eigenfunctions and
eigenvalues.
Proof of scale: The unitary property follows:
⟨Usg, Ush⟩=
 ∞
0
h(st)h(st)1
t dt (a)
=
 ∞
0
g(r)h(r)1
r dr
= ⟨g, h⟩,
(9.271)
with (a) following from the substitutions r = st.
The eigenelements (λ(j)
f
= sjf , φf (t) = tjf satisfying the fundamental equation
Usφf (t) = φf (st) = (st)j f = λ(s)
f φf (t) .
(9.272)
The eigenvalues have magnitude 1, and to see orthogonality of the eigenvectors, use
the log trick. For g ∈L2[(0, ∞), 1
t dt], then ˜g ∈L2[(−∞, ∞), dt] deﬁned by the relation
˜g ◦log = g so that
˜g(log t) = g(t),
log t ∈(−∞, ∞).
(9.273)
Then, the orthogonality of φf (t) = tj f , φf′(t) = tj f′ reduces to the orthogonality of ˜φf as
complex exponentials, ˜φf (r) = ejr f . Orthogonality holds in the sense of L2[(−∞, ∞)]
functions; the inner product of two basis components acts as a delta dirac in its actions
on functions in L2. To see this, deﬁne g(s) ↔G(f) to form a scale transform pair. Then
 ∞
−∞
⟨φf , φf′⟩G(f′) df′ =
 ∞
−∞
 ∞
0
s−j2π(f−f′) 1
s dsG(f′) df′
(9.274)
=
 ∞
0
s−j2πf
 ∞
−∞
sj2πf′)G(f′) df′
)
*+
,
g(s)
1
s ds = G(f) .
(9.275)

9.12 NON-COMPACT OPERATORS
311
Deﬁnition 9.85
The second order random process {Z(df), f ∈R} is called an orthogonal
process with spectral density S(f), f ∈R if for all F1, F2 measurable in R,
1. F1 ∩F2 = ∅=⇒Z(F1
( F2) = Z(F1) + Z(F2);
(9.276)
2. EZ(F1)Z∗(F2) =

F1

F2 EZ(df)Z∗( df′) =

F1∩F2 S(f) df.
(9.277)
Example 9.86 (Compact Covariances)
On compact domains we have already seen
orthogonal expansions. Karhunen-Loeve is an orthogonal process representation. Let
Y(t), t ∈[0, 1] have covariance K(t, s), t, s ∈[0, 1] with eigenelements {φn, λn}, Kφn =
λnφn. Then the orthogonal decomposition becomes
Y(t) =

n
φn(t)Zn,
(9.278)
with Zn = ⟨φn, Y⟩with spectrum E{ZnZ∗
n′} = λnδ(n −n′).
For cyclo-stationary processes {Y(ω, t), t ∈[0, 1]} with covariance K(t, s) = K(t−
s, 0), then the eigenfunctions are φn(t) = ej2πnt, and orthogonal process representation
becomes
Y(ω, t) =

n
ej2πntZn(ω),
(9.279)
with Zn(ω) an orthogonal process with EZnZn′ = Snδ(n′ −n). To see orthogonality,
use the fact that Zn = ⟨φn, X⟩implying that for all F1, F2 ⊂Z, F1 ∩F2 = ∅,
Z(F1
?
F2) =

n∈F1
( F2
⟨φn, Y⟩= Z(F1) + Z(F2) .
(9.280)
Orthogonality follows from the eigenfunction property:
EZnZn′ = ⟨φn, KYφn′⟩= Snδ(n −n′) .
(9.281)
9.12.1 Cramer Decomposition for Stationary Processes
The Cramer decomposition in the non-compact setting now follows. Assume {Y(ω, t), t ∈R} is
a second order process which is stationary so that the covariances is only a function of shift
K(t, s) = K(t −s, 0). The Fourier representation implies the eigenfunctions of the covariance are
the complex exponentials:
K(t −s, 0) =

R
ej2πf(t−s)S(f)df .
(9.282)
As long as the Fourier transforms exists, the complex exponential provides the orthogonal process
representation. Here is the Cramer decomposition.
Theorem 9.87 (Cramer)
Assume {Y(ω, t), t ∈R} is a stationary second order process with
covariance K(t, s) = K(t −s, 0) whose Fourier transform exists:
K(t, 0) =

R
ej2πftS(f)df.
(9.283)

312
9 SECOND ORDER AND GAUSSIAN FIELDS
Then the orthogonal representation,
Y(ω, t) =

R
ej2πftZ(ω, df), with orthogonal process Z(ω, df)
=

R
e−j2πftY(ω, t)dt df,
(9.284)
has spectral density S(f) =

R e−j2πftK(t, 0)dt.
Proof
The eigenvalues and eigenfunctions for the shift operator Ut on R are λ(t)
f
=
e+j2πft, φf (t) = ej2πft. We need to show that the process being shift invariant implies
Z(ω, df) is an orthogonal increments process with variance EZ(df)Z∗(df) = S(f)df.
The additivity and orthogonal process properties follows from the properties of the
complex exponentials as eigenfunction of the covariance: that is for F1, F2 ∈R,
F1 ∩F2 = ∅, Z(F1 ∪F2) =

F1∪F2
Z(ω, df) = Z(F1) + Z(F2)
(9.285)
EZ(F1)Z(F2) =

F1

F2
EZ(df)Z∗(df′) =

F1

F2
⟨φf , KYφf′⟩df df′
=

F1

F2
S(f′)
⟨φf , φf′⟩
) *+ ,
sifting property
df df′ =

F1∩F2
S(f)df .
(9.286)
9.12.2 Orthogonal Scale Representation
The identical approach holds for scale.
Deﬁnition 9.88
Let {Y(ω, t), t ∈(0, ∞)} be a second order process. Then Y(t) is said to be
second-order stationary to scale if
EY(st)Y∗(t) = K(st, t) = K(s, 1),
t, s ∈(0, ∞).
(9.287)
Theorem 9.89
Let {Y(ω, t), t ∈(0, ∞)} be a second order process with covariance which is
stationary with respect to scale K(st, t) = K(s, 1) whose transform exists
K(t, 1) =
 ∞
−∞
tj2πf S(f)df.
(9.288)
Then the orthogonal representation,
Y(ω, t) =
 ∞
−∞
tj2πf Z(ω, df), with orthogonal process,
Z(ω, df) =
 ∞
0
t−j2πf Y(ω, t)1
t dt df,
(9.289)
has spectral density S(f) =
 ∞
0
t−j2πf K(t, 1) 1
t dt.
Proof
The eigenvalues and eigenfunctions for the scale operator Ut are λ(t)
f
=
tj2πf , φf (t) = tj2πf . Now we must show that Z(ω, df) is an orthogonal increments

9.12 NON-COMPACT OPERATORS
313
process with spectral density S(f). The additivity property follows from the integral.
For F1, F2 ∈R,
F1 ∩F2 = ∅,
Z(F1 ∪F2) =

F1∪F2
Z(df) = Z(F1) + Z(F2)
(9.290)
EZ(F1)Z(F2) =

F1

F2
EZ(df)Z∗(df′) =

F1

F2
⟨φf , KYφf′⟩df df′
(a)
=

F1

F2
S(f)⟨φf , φf′⟩df df′ (b)
=

F1∩F2
S(f)df .
(9.291)
Property (b) follows from the sifting property; property (a) must be justiﬁed that tj2πf
is an eigenfunction of the scale invariant covariance, KYφf = S(f)φf . Stationary with
respect to scale implies the covariance operator has the property KY(t, s) = KY(s−1t, 1)
which follows directly from the orthogonal representation theorem:
KY(t, s) = EY(t)Y∗(s) = E
 ∞
−∞
tj2πf Z( df)
 ∞
−∞
s−j2πf′Z∗( df′)
=
 ∞
−∞
tj2πf
 ∞
−∞
s−j2πf′EZ( df)Z∗( df′) =
 ∞
−∞
(ts−1)j2πf S(f) df. (9.292)
Then,
KYφf (t) =
 ∞
0
KY(t, s)φf (s)1
s ds (a)
=
 ∞
0
 ∞
−∞
(ts−1)j2πf′S(f′) df′sj2πf 1
s ds
(9.293)
=
 ∞
−∞
tj2πf′S(f′)
 ∞
0
s−j2πf′sj2πf 1
s ds
)
*+
,
⟨φf ,φ′
f ⟩L2(0,∞)
df′ =
 ∞
−∞
tj2πf′S(f′)δ(f −f′) df′,
(9.294)
where (a) follows from Eqn. 9.292 proving the eigenfunction property. Thus {Z(ω, ·)}
is an orthogonal process with spectral density S(f).
Example 9.90 (Stone’s representation.)
Stone’s representation represents a continu-
ous unitary operator Us, s ∈T on H(T) as
Us =

R
λ(s)
f P( df)
withaction
Usg(t) =

R
λ(s)
f P( df)g(·)|t.
(9.295)
Here are several familiar examples: the Fourier and scale transforms.
1. Fourier transform Let {Us, s ∈R} form a unitary group on s, s′ ∈R satisfying
Us+s′ = UsUs′; then with H = L2(R, Z) and inner product
⟨g, h⟩L2((−∞,∞), dt) =
 ∞
−∞
g∗(t)h(t) dt .
(9.296)
Then the Fourier transform of the unitary shift operator is
g(t + s) = Utg(s) =

R
ej2πft 
⟨ej2πf·, g(·)⟩L2(dt) ej2πf· df
 s
for
s, t ∈R.
(9.297)

314
9 SECOND ORDER AND GAUSSIAN FIELDS
2. Scale Transform Let H = L2( 1
t dt), with inner product
⟨g, h⟩L2(1/t) dt) =
 ∞
0
g∗(t)h(t)1
t dt.
For the unitary scale operator Us, s ∈(0, ∞), then the scale transform is given by
g(ts) = Utg(s) =

R
tj2πf 
⟨e(j2πf) ln ·, g(·)⟩L2(1/tdt)e(j2πf) ln s df

.
(9.298)
Example 9.91 (Elasticity Operator Induced Stationary Priors on R3)
Assume
the
transformations are deﬁned to be shift invariant h : R3 →R3, again parameterized
via the vector ﬁeld relation h(x) = x −u(x). For this the domain is not compact, and
we allow for global registration via the rigid translation group as well as the various
subgroups of GL(3). Assume again the prior is induced via the differential operators of
continuum mechanics, as above small deformation linear elasticity with the resulting
stochastic PDE again of the type.
The equation in vector form is
−b∇2U −c ▽· ▽U + aU = W
(9.299)
with A = −b∇2 −c▽▽+aI. The term aI represents the restoring elastic forces towards
the static equilibrium. The minus signs are used for notational convenience later on.
We shall show that this is satisﬁed for the random Navier equations driven
by a stochastic Gaussian process. The Cramer representation of the vector ﬁeld from
Section 9.12.1, Theorem 9.87, is appropriate since our operator is shift invariant. For
this, introduce the orthogonal process representation of the noise process so that
W(x) =

ω∈R3 ej⟨x,ω⟩dZW(ω),
(9.300)
with ⟨x, ω⟩= ω1x1
+
ω2x2
+
ω3x3 and {ZW(ω) ∈C3, ω
∈R3} a com-
plex orthogonal process ZE(1
( 2) = ZW(1) + Z(2) for 1 ∩2 = φ, and
E{ZW(1)Z∗
E(2)} =

1∩2 Sw(ω)dω. Sw(ω) is the spectral density, matrix i.e.
E{dZW(ω) dZ∗
W(ω)} = Sw(ω)dω. For w(x) a white noise process with power spectral
density, σ 2, Sw(ω) = σ 2I. More general forms are useful.
The operator A is shift invariant, so let
U(x) =

ω∈R3 ej⟨x,ω ⟩dZW(ω),
(9.301)
where S(ω)dω = E{dZ(ω)dZ∗(ω)} with covariance
K(x, y) =

ω∈R3 ej⟨x−y,ω⟩S(ω) dω .
(9.302)
Applying for example, the differential operator (∂/∂x1) to the U- process cor-
responds to multiplication by iω1 in the frequency domain, and so on. Rewriting in
component form gives
−b

∂2U1
∂x2
1
+ ∂2U1
∂2x2
+ ∂2U1
∂2x3

−c ∂
∂x1
∂U1
∂x1
+ ∂U2
∂x2
+ ∂U3
∂x3

+ aU1 = W
(9.303)
and two more similar equations.

9.12 NON-COMPACT OPERATORS
315
Solving the stochastic Navier equation gives
AU(x) =

ω∈R3 ej⟨x,ω⟩M(ω) dZ(ω),
(9.304)
with the function M(ω) a 3×3 matrix. Via straightforward formal manipulation of the
stochastic equation AU = W gives M(ω)dZ(ω) = dZW(ω) revealing
S(ω) = M−1Sw(ω)M−1.
(9.305)
To simplify the analysis, assume W(x) is white, i.e. Sw(ω) = σ 2id. The actual
form for M(ω) can be written as the sum
M(ω) = M1(ω) + M2(ω) + M3(ω),
(9.306)
with
M1(ω) = b(ω2
1 + ω2
2 + ω2
3) id,
(9.307)
M2(ω) = c(ωiωj; i, j = 1, 2, 3),
(9.308)
M3(ω) = aid.
(9.309)
Note that all M-matrices are non-negative and the third one strictly positive so
that M(ω) is non-singular. In order to show that the u-ﬁeld is deﬁned we show that
its spectral density matrix σM−2(ω) is integrable w.r.t. Lebesgue measure over R3. To
do this we note that
M(ω) ≥M1(ω) + M3(ω)
(9.310)
with the ususal partial order for matrices; just recall that M2(ω) is non-negative. Hence
M−2 ≤
1
[a + b(ω2
1 + ω2
2 + ω2
3)]2 id.
(9.311)
In polar coordinates this function behaves like r−4 for large r and is therefore integrable
since the volume element is r2 dφ dψ. Thus the prior has a meaningful continuous limit
and is therefore acceptable.

10 M E T R I C S S PA C E S F O R T H E M AT R I X G R O U P S
ABSTRACT
In this chapter the metric space structure of shape is developed. We do this by ﬁrst
studying the action of the matrix groups on the coordinate systems of shape. We begin
by reviewing the well-known properties of the ﬁnite-dimensional matrix groups, including
their properties as smooth Riemannian manifolds, allowing us to develop metric distances
between the group elements. We explore the construction of the metric structure of these
diffeomorphisms and develop many of the properties which hold for the ﬁnite dimensional
matrix groups and subsequently in the inﬁnite dimensional setting as well.
10.1 Riemannian Manifolds as Metric Spaces
Our principal study in this chapter is shapes studied via their transformation via diffeomorphisms.
Both the ﬁnite dimensional matrix group and inﬁnite dimensional diffeomorphisms will be exam-
ined. Although they cannot be added, they form a group which is as well a Riemannian manifold
on which a metric space structure can be deﬁned. The metric distance in the groups of diffeomor-
phisms is the length of the shortest curve geodesic connecting them. This induces the distance
between shapes, becoming the distance between elements of the group which generate them.
We begin by constructing metrics spaces and Riemannian manifolds.
10.1.1 Metric Spaces and Smooth Manifolds
Deﬁnition 10.1
A metric space is a set M with a function ρ : M × M →R+ which is a
metric on set M if it satisﬁes the following three properties:
1. ρ(g, h) ≥0 with ρ(g, h) = 0 if and only if g = h for each g, h ∈M;
2. ρ(g, h) = ρ(h, g) for each g, h ∈M (symmetry);
3. ρ(g, h) ≤ρ(g, f) + ρ(f, h) for each g, h, f ∈M (triangle inequality).
One of the most familiar examples of metric spaces are normed linear vector spaces as
discussed in Section 2.4.1, Chapter 2. The associated metric ρ : M × M →R+ is given by the
norm, ρ(g, h) = ∥g −h∥. Notice, the norm satisfying triangle inequality implies the associated
metric satisﬁes the triangle inequality; for all f, g, h ∈M,
ρ(g, h) = ∥g −h∥= ∥g −f + f −h∥
(10.1)
≤∥g −f∥+ ∥f −h∥= ρ(g, f) + ρ(f, h).
(10.2)
The classic examples of normed metric spaces which are familiar are the Euclidean space of
n-tuples with sum-square norm, l2, L2 with sum-square and integral square norm, and continuous
functions with sup-norm. Geometric shape is studied via transformations which are products of
mappings which do not in general form a normed vector space; they will be intrinsically curved
manifolds which do not support addition. To establish length and metric distance in such a setting
Riemannian or geodesic length is required. Distance between elements is given by shortest or
geodesic length of the curves connecting elements of the set.
316

10.1 RIEMANNIAN MANIFOLDS AS METRIC SPACES
317
10.1.2 Riemannian Manifold, Geodesic Metric, and Minimum Energy
To measure length in these perhaps curved manifolds, deﬁne paths or curves g : [0, 1] →M
connecting exists a smooth curve gt, t ∈[0, 1] with g0 = g, g1 = h, with smooth tangent (velocity)
ﬁeld vt = dgt/dt well deﬁned for all t ∈[0, 1].
If the manifold M is as well a Riemannian manifold, then the tangent spaces have inner product
⟨·, ·⟩1/2
Tg and norm ∥· ∥Tg enabling the measurement of angles between curves (dot products) and
lengths of curves (norms). This allows for the deﬁnition of the length.
Deﬁnition 10.2
A manifold M is a Riemannian manifold if there is a symmetric, positive
deﬁnite, inner product associated with each of the points of the tangent space; for all g ∈M
there exists ⟨·, ·⟩Tg : Tg(M) × Tg(M) →R with associated norm ∥· ∥Tg = ⟨·, ·⟩1/2
Tg .
Then curve g : [0, 1] →M has velocity vt = dgt/dt, t ∈[0, 1] with length deﬁned as
Length(v) =
 1
0
∥vt = dgt
dt ∥T dt.
(10.3)
Deﬁne g to be a minimal geodesic29 between g, h ∈M if
g =
arg inf
v:g0=g,g1= h
 1
0
∥vt = dgt
dt ∥T dt.
(10.4)
It is convenient to work with curves in arc-length parameterization with constant speed.
Since length does not change under reparameterization (see Lemma 7.38 of Chapter 7), geodesics
will be in the so-called arc-length parameterization.
Lemma 10.3
The inﬁmum energy curves satisfying
inf
v:g0=g,g1=h
 1
0
∥vt∥2
T dt

(10.5)
are attained with constant speed ∥vt = dgt/dt∥T = constant, t ∈[0, 1]:
Proof
The Cauchy–Schwartz inequality gives the ordering on the energy and length
of the paths on the manifold with vt = dgt/dt, t ∈[0, 1]:
Length(v)2 =
 1
0
∥vt∥T dt
2
≤
 1
0
∥vt∥2
T dt = E(v)
(10.6)
with equality only if (non-zero paths) ∥vt∥T = constant for all t ∈[0, 1]. Thus,
the square of length Length(v) of any path is a lower bound for the energy of the
path, the minimum of energy being equal to square of length of the path if the path is
traversed at constant speed ∥˙gt = vt∥T = c.
Such shortest length paths provide the metric to make the Riemannian manifold into a metric
space. Also, minimum energy curves because of their constant speed property are geodesics,
demonstrating the connection of the variational minimizers of quadratic forms and the associated
geodesic length metric.
29 There may be many length minimizing curves, in which case a minimal geodesic is one in the set.

318
10 METRICS SPACES FOR THE MATRIX GROUPS
Theorem 10.4
Let M be a Riemannian manifold with inner products ⟨·, ·⟩T(M) and norm
∥· ∥T(M) at every point of the tangent space. Deﬁne the positive function ρ : M × M →R+
on pairs g, h deﬁned as the length of the minimal geodesic gt, t ∈[0, 1] on M connecting
g to h:
ρ(g, h) =
inf
v:g0=g,g1=h
 1
0
∥vt∥T dt

.
(10.7)
Then ρ(·, ·) satisﬁes symmetry and the triangle inequality, and makes M the Riemannian
manifold into a metric space.
The inﬁmum of the energy of all paths connecting g to h is the square of the metric
(geodesic) distance:
E =
inf
v
g0=g,g1=h
 1
0
∥vt∥2
T dt

= ρ2(g, h).
(10.8)
Proof
To establish that ρ on M × M is in fact a metric distance, both symmetry and
triangle inequality follow from the geodesic property. For symmetry, let g attain the
inﬁmum length connecting g0 = g to g1 = h. Then generating a path ˜gt, t ∈[0, 1],
˜g0 = h with velocity ˜vt = −v1−t connects h to g so that ˜g1 = g and has identical length.
If there were an alternative candidate connecting h to g with shorter length then it
should be used as the solution to connecting g to h.
The triangle inequality exploits the geodesic property; we must show that for
all points g, h, k ∈M,
ρ(g, k) ≤ρ(g, h) + ρ(h, k).
(10.9)
Deﬁne the curves g0,1, g1,2 to be the shortest length curves connecting pairs
(g, h), (h, k), respectively. Then deﬁne the new curve ˆg connecting ˆg0 = g, ˆg1 = k
according to
ˆgt = g0,1
2t ,
t ∈
=
0, 1
2
>
,
(10.10)
= g1,2
2t−1 ,
t ∈
=
1
2, 1
>
.
(10.11)
Then ˆg0 = g, ˆg1 = k and has length
 1
0
∥ˆvt∥T dt =
 1/2
0
2∥v0,1
2t ∥T dt +
 1
1/2
2∥v1,2
2t−1∥T dt
(10.12)
(a)
= ρ(g, h) + ρ(h, k)
(b)
≥ρ(g, k),
(10.13)
with (a) coming from the constant speed Lemma 7.38 and (b) following from the fact
that ρ(g, k) attains an inﬁmum over all possible curves.
The second half of the proof requires the constant speed Lemma 10.3 of geodesics
demonstrating minimum energy paths are at constant speed:
E =
inf
∥v=˙g∥=c:
g0=g,g1=h
E(v) =
inf
∥v∥=∥˙g∥=c:
g0=g,g1=h

Length(v)
	2 = ρ2(g, h).
(10.14)

10.2 VECTOR SPACES AS METRIC SPACES
319
10.2 Vector Spaces as Metric Spaces
One of the normed spaces which shall be worked with in describing the patterns of random vectors
are l2, L2 the Hilbert space of square-summable and integrable sequences and functions. The
Hilbert space l2 is the set of countable vector functions g(·) = (g1, g2, . . . ) having ﬁnite two norm
∥g∥l2 =

i |gi|2 1
2 induced by the inner product: ⟨g, h⟩l2 = 
i gih⋆
i . The square-integrable Hilbert
space L2 are the functions g(·) having ﬁnite two norm ∥g∥L2 =

|gt|2 dt
 1
2 and inner product
inducing the norm ⟨g, h⟩L2 =
 1
0 gth⋆
t dt. Thinking on the continuum puts us on L2; discretizing to
pixels associates us to l2.
If H is a Hilbert space with the inner product ⟨·, ·⟩H and associated norm ∥· ∥H, then it is
a Riemannian manifold; the tangent spaces are the spaces themselves. To see this construct the
tangent space at each point g ∈H by generating curves
gt = (h −g)t + g,
for all g, h ∈H.
(10.15)
Then the tangent vectors vg ∈Tg(H) are the entire space H since ∥h −g∥H ≤∥g∥H + ∥h∥H < ∞.
For these structures shortest paths are not curved, but rather are straight lines. The minimal
geodesic lengths are then just the normed differences between the elements.
Theorem 10.5
Given the Hilbert space (H, ∥· ∥H) then, the minimal geodesics in H with
tangent norm ∥d
dtgt = vt∥H minimizing
 1
0 ∥vt∥2
H dt connecting g0 = g, g1 = h satisfy the
Euler equation
dvt
dt = 0,
implying vt = h −g.
(10.16)
The geodesics are straight lines and the metric distance ρ : H × H →R+ is given by
ρ(g, h) =
inf
v= d
dt g: g0=g
g1=h
 1
0
∥vt∥H dt

= ∥h −g∥H.
(10.17)
Proof
To calculate the minimal geodesic with minimizing energy
 1
0 ∥vt∥2 dt, we use
the calculus of variations to generate a smooth perturbation of gt →gt(ǫ) = gt + ǫηt
giving the perturbed vector ﬁeld vt →vt + ǫ(d/dt)ηt and satisfying the boundary
conditions η0 = η1 = 0 so that g0(ǫ) = g0, g1(ǫ) = g1. Then
0 = d
dǫ
 1
0
∥vt + ǫ d
dtηt∥2
H dt|ǫ=0 = 2
 1
0

vt, dηt
dt

H
dt
(10.18)
(a)
=
 1
0
⟨dvt
dt , ηt⟩H dt = 0,
(10.19)
where (a) follows via integration by parts and the boundary condition. This gives
vt =constant, and the form for the minimizing geodesics gt = (h −g)t + g with total
length given by the norm.
This suggests that any positive deﬁnite quadratic form can be introduced in the inner product
⟨·, ·⟩Q and associated norm ∥· ∥Q giving the familiar representation of the metric.
Corollary 10.6
Let Q be a positive deﬁnite quadratic form deﬁning the inner product in the
tangent space ⟨g, h⟩Q, then
ρ(g, h)2 = ∥g −h∥2
Q.
(10.20)

320
10 METRICS SPACES FOR THE MATRIX GROUPS
If the Hilbert space has closed and bounded background space, then it has a countable
orthonormal base diagonalizing Q with eigenelements {qi, φi}, Qφi = qiφi and the metric
reduces to an l2 metric:
ρ(g, h)2 =

i
qi|gi −hi|2 ,
where g =

i
giφi , h =

i
hiφi.
(10.21)
10.3 Coordinate Frames on the Matrix Groups and
the Exponential Map
We will now study curves, in particular the shortest geodesic curves on the manifolds of the
matrix groups. These will provide the metrics and allow us to deﬁne the tangent spaces for taking
derivatives.
10.3.1 Left and Right Group Action
Notice, from the curves emanating from any matrix group element then the tangent space and
coordinate frames that can be generated as in Theorem 8.12 of Chapter 8. This is how the exponen-
tial mapping will be used at t = 0 to generate the tangent space at the identity for the subgroups
of GL(n).
For this deﬁne the group action as a transformation of the manifold.
Deﬁnition 10.7
Deﬁne the left and right group actions transforming the generalized
linear group l(B, ·), r(B, ·) : GL(n) →GL(n) for B ∈GL(n):
l(B, ·) : A ∈GL(n) →l(B, A) = B ◦A
(10.22)
r(B, ·) : A ∈GL(n) →r(B, A) = A ◦B∗,
(10.23)
where ◦denotes group operation of matrix multiplication. It is usual convention to drop the
composition symbol ◦with the understanding that A ◦B = AB is matrix multiplication.
For the afﬁne motions in homogeneous coordinates ¯A ∈A(n), the group actions are
deﬁned similarly as above l( ¯A, ·), r( ¯A, ·) : A(n) →A(n).
Notice, for the right transformation to be a group action, the transpose (or inverse for that matter)
must be applied according to
r(C, r(B, A)) = (A ◦B∗) ◦C∗= A ◦(B∗◦C∗)
(10.24)
= A ◦(C ◦B)∗= r(C ◦B, A).
(10.25)
The left group action is the usual convention in rigid body mechanics (seeArnold). The right group
action will be consistent with our examinations of the matrix groups applied to functions such as
the images. In particular we will use the right action to deﬁne the coordinate frames associated
with curves emanating from points on the manifold.
The curves used to generate the tangent space will be generated from the exponential map.
Deﬁnition 10.8
Deﬁne set of n × n real-valued matrices M(n). Then the exponential
eX of the matrix X = (Xij) ∈M(n) is deﬁned to be the matrix given by the following series
when the series converges:
eX = I + X + 1
2!X2 + 1
3!X3 + · · · .
(10.26)

10.3 COORDINATE FRAMES ON THE MATRIX GROUPS
321
The exponential integral curve rooted at the identity in the direction of V = (Vij) ∈
M(n) is denoted as etV, t ∈[0, 1]. The left and right group action is deﬁned as
l(etV, A) = etVA ,
r(etV, A) = Ae−tV.
(10.27)
As proved in Boothby pages 147–150 eX converges for all X
∈M(n) implying that etV,
t ∈[0, 1] andV ∈M(n) are curves through GL(n) and can be used to generate the tangent
space. This follows from the fact that it converges absolutely (proved in Boothby), implying
that eY+Z = eYeZ if the matrices commute YZ = ZY. Clearly, V and −V commute, implying
etV−tV = etVe−tV = I. Thus, etV has an inverse implying it is an element of GL(n).
10.3.2 The Coordinate Frames
For the generalized linear group the tangent spaces are generated from the exponential curves.
For this we use the right action applied to the manifold of matrix group elements.
Theorem 10.9
For the generalized linear group GL(n) the elements of the tangent space
TA(GL(n)) at A ∈GL(n) are constructed from curves generated from the right action of the
exponential map. Identifying elements of vij with coordinate frames ∂/(∂xij), then the tangent
elements are given by

ij
(AV)ij
∂
∂xij
∈TA(GL(n)).
(10.28)
Deﬁning the n × n matrix 1ij =

δij(k, l)
	
having ij-entry of 1, and 0 otherwise, then
the m ≤n2 coordinate frames at the identity E·I, i of the subgroups are
1. for GL(n), there are m = n2 coordinate frames at the identity transformed by A:
EijI = 1ij, i, j = 1, . . . , n;
(10.29)
2. for O(n), there are m = n(n −1)/2 coordinate frames,
EijI = 1ij −1ji,
j = 1 + i, . . . , n,
i = 1, . . . , n −1;
(10.30)
3. for SL(n), there are n2 −1 coordinate frames; deﬁne the n −1 × n −1 full rank matrix

vij

, then the coordinate frames become
EijI = 1ij,
1 ≤i ̸= j ≤n;
(10.31)
EiiI =
n−1

j=1
vij1jj −


n−1

j=1
vij

1nn,
i = 1 . . . n −1.
(10.32)
4. for US(n), there is 1 coordinate frame at the identity matrix EI = 
n
i=1 1ii = I.

322
10 METRICS SPACES FOR THE MATRIX GROUPS
Proof
The coordinate frames for the tangent spaces TA(GL(n)) for the vector ﬁelds
are transferred to arbitrary points A ∈GL(n) on the entire manifold by examining
the curves emanating from A using the property of the derivative of the right group
action
d
dtr(etV, A)|t=0 = d
dtAe−tV|t=0 = −AV.
(10.33)
Then for all V =

vij

∈M(n), identifying elements of vij with coordinate frames
∂/∂xij, then the tangent elements are given by
d
dtf ◦

Ae−tV
|t=0 =

ij
∂f
∂xij

Ae−tV t=0
 d
dtAe−tV|t=0

ij
= −

ij
(AV)ij
∂f
∂xij
(A) .
(10.34)
Proof of GL(n): Let EijI = 1ij, then etEijI ∈GL(n) (Theorem 10.9 above) therefore
dt
dt |t=0 = EijI ∈TI(GL(n)). SincetheEijI = 1ij, i = 1, . . . n, j = 1, . . . nareindependent,
and n2 in number, they span the n2 dimensional tangent space TI(GL(n)) and are a
basis.
Proof of O(n):
Let O(n)
⊂
GL(n) be the orthogonal group.
Then
etV 
etV∗
= I, giving etVetV∗= I, which implies since V commutes with itself,
and etVe−tV = et(V−V) = I, V∗= −V. V is a skew symmetric matrix. Clearly V has
dimension n(n −1)/2 the number of free parameters in a skew symmetric matrix,
implying O(n) has dimension n(n −1)/2.
Proof of SL(n): For A non-singular, then
AeXA−1 = AA−1 + AXA−1 + 1
2AX2A−1 + · · ·
= I + AXA−1 + 1
2AX2A−1 + · · · (a)
= eAXA−1,
(10.35)
and (a) follows since AXA−1 ∈M(n) and Theorem 10.9, eX converges absolutely. This
implies det eX = det eXAX−1 giving det eX = etrX. Thus, X ∈SL(n) implies trX = 0.
Thus one parameter subgroups of SL(n) takes the form etA, where A is an element of
n2 −1 dimensional manifold subset M(n) satisfying trA = 
n
i=1 aii = 0.
Corollary 10.10
The coordinate frames of the tangent spaces at any point A ∈S are
determined by the coordinate frames EiI at the identity:
EiA = A ◦EiI.
(10.36)
The vector ﬁelds for GL(n) and O(n) take the following form:
1. for GL(n) the generalized linear group
EijA = A ◦1ij =


0
· · ·
0
a1i
0
· · ·
0
...
...
...
...
...
...
...
0
· · ·
0
an−1i
0
· · ·
0
0
· · ·
0
ani
0
· · ·
0
ith column


;

10.3 COORDINATE FRAMES ON THE MATRIX GROUPS
323
2. for O(n) the orthogonal group
EijA = A ◦(1ij −1ji) =


0
· · ·
0
a1i
0
· · ·
0
−a1j
0
...
...
...
...
...
...
...
...
...
0
· · ·
0
an−1i
0
· · ·
0
−an−1j
0
0
· · ·
0
ani
0
· · ·
0
−anj
0
ith column
ith column


;
(10.37)
3. for US(n) the scale group, EijA = A.
Example 10.11 (Integral Curves for Scale and Rotation)
Forscaling, etV =
 etv
0
0
etv

where V =
 v
0
0
v

, then the curve rooted at A =

aij

∈GL(n) gives
r(etV, A) =
 a11
a12
a21
a22
  e−tv
0
0
e−tv

,
with VAe−tV = −

ij
aije−tvv ∂
∂xij
.
(10.38)
For rotation in SO(2) at the identiy etV =

cos t
sin t
−sin t
cos t

where V =

0
1
−1
0

,
then
r(etV, A) =
 a11
a12
a21
a22
  cos t
−sin t
sin t
cos t

,
with VAe−tV
=

ij


 a11
a12
a21
a22
  −sin t
−cos t
cos t
−sin t


ij
∂
∂xij
.
(10.39)
10.3.3 Local Optimization via Directional Derivatives and the
Exponential Map
To generalize our local variational calculus for extreme points on curved matrix groups (viewed
as smooth manifolds) directional derivatives at any point of the tangent space must be computed.
This will become possible as we characterize the basis of the tangent space on the matrix groups
from which the directional derivatives are generated. We have all of the tools to generalize our
local variational calculus for optimization of functions on manifolds which are matrix groups.
We assume throughout that the extreme points are interior to the manifold. With the ability to
generate tangents via the curves, local optimization falls out immediately on the matrix groups as
requiring the directional derivatives with the coordinate frames being zero. The tangents to these
curves will be elements of the tangent space of the Lie group S.
Theorem 10.12
Let H be a smooth function on M, and let {Ei, i = 1, . . . , m} be the set of
m-coordinate frames from M. The necessary condition for p ∈O ⊂M to be a local maximizer
of H becomes
EipH = 0,
i = 1, . . . m.
(10.40)

324
10 METRICS SPACES FOR THE MATRIX GROUPS
Proof
To calculate the optimizer condition, let t0 = 0, with curve rooted in p ∈M
according to t(p) ∈M, 0(p) = p and f = H, then from Theorem 8.12
d
dtH ◦t =
m

i=1
˙xi(0)Ei0(p)H = 0.
(10.41)
Since this is true for all curves, the necessary condition on the coordinate frames
follows.
10.4 Metric Space Structure for the Linear Matrix Groups
We shall study shapes and structures using the low-dimensional matrix groups and high-
dimensional diffeomorphisms. First we study the metric space structure for the matrix groups.
10.4.1 Geodesics in the Matrix Groups
Now examine the geodesics within the subgroups of the matrix groups. Measure the length
in the tangent space with the quadratic form deﬁned by the d × d positive deﬁnite matrix M
according to
∥f∥2
M = ⟨M f, f⟩Rd×d = tr M ff∗.
(10.42)
For calculating the geodesics we take the variation in the group via perturbation of the
optimum trajectory. The signiﬁcant departure from the approach taken for the Hilbert vector
space (e.g. see Theorem 10.5), is that the perturbation argument is not done via addition, but
rather via group action. Deﬁne gt, t ∈[0, 1] to be the optimizing geodesic connecting g0 = g to
g1 = h; then each point along the curve is perturbed in the direction of ηt according to eǫηt :
gt →gt(ǫ) = eǫηtgt = (id + ǫηt)gt + o(ǫ). The perturbation must satisfy the boundary condition
g0(ǫ) = g, g1(ǫ) = h, implying η0 = η1 = 0.30 This is depicted in Figure 10.1.
The following Lemma calculates the perturbation of vt along the ﬂow gt. It explicitly involves
the Lie bracket which dictates how elements in the tangent space transform along the ﬂow.
Lemma 10.13
With dgt
dt
= vtgt, then transporting tangent element w0 by the group as
wt = gtw0g−1
t
gives the Lie bracket31
dwt
dt = (vtwt −wtvt) = [vt, wt].
(10.43)
The perturbation of the group element
gt →gt(ǫ) = (id + ǫηt)gt + o(ǫ) = gt + ǫηtgt + o(ǫ).
(10.44)
gives the perturbed vector ﬁeld vt →vt(ǫ) = vt + ǫ(dvt(ǫ)/dǫ) + o(ǫ) satisfying the Lie
bracket equation
dvt(ǫ)
dǫ
= dηt
dt −(vtηt −ηtvt).
(10.45)
30 Notice, in the vector space case this is addition of η rather than matrix multiplication; see Theorem 10.5.
31 The Lie bracket for matrices A, B ∈Rd×d is familiar as [A, B] = AB −BA.

10.4 METRIC SPACE STRUCTURE
325
X
gt+ht(gt)+o(ε)
g1
gt
g0
t
X
gt+ ht(gt)+o(ε)
g1
gt
g0
t
Figure 10.1 Panel 1 shows perturbation of the ﬂow gt →gt(ǫ) = gt + ǫηt(gt) + o(ǫ) in exact
matching with g1(ǫ) = g1. For the matrix case gt →gt(ǫ) = (id + ǫηt)gt + o(ǫ) = gt + ηtgt + o(ǫ).
Panel 2 shows the variation of the ﬂow gt in inexact matching with g1(ǫ) free, i.e. g1(ǫ) ̸= g1.
Proof
Part 1: Deﬁne dgt/dt = vtgt, then dg−1
t
/dt = −g−1
t
vt, implying
dwt
dt = d
dt(gtw0g−1
t
) = vtwt −wtvt = [vt, wt].
(10.46)
Part 2: The Lemma follows according to
d
dt(gt + ǫηtgt) =

vt + ǫ dvt(ǫ)
dǫ

(gt + ǫηtgt) + o(ǫ)
(a)
= vtgt + ǫvtηtgt + ǫ dvt(ǫ)
dǫ
gt + o(ǫ)
= dgt
dt + ǫ dηt
dt gt + ǫηt
dgt
dt
(b)
= vtgt + ǫ dηt
dt gt + ǫηtvtgt,
and equating (a),(b) and applying g−1
t
on the right to both terms gives
dvt(ǫ)
dǫ
ǫ=0
= dηt
dt + ηtvt −vtηt.
With this we can calculate the geodesic equation for the matrix groups.
Theorem 10.14
The minimal geodesics connecting g, h ∈GL(d) with tangent norm-square
∥vt∥2
M = ⟨Mvt, vt⟩Rd×d minimizing
 1
0 ∥vt∥2
M dt satisfying dgt/dt = vtgt with g0 = g,
g1 = h are solutions of the Euler equation given by
dMvt
dt
= Mvtv∗
t −v∗
t Mvt.
(10.47)
Proof
TheproofonthegeodesicfollowsapathshowntooneoftheauthorsbyTrouve
and Younes. Examine a smooth perturbation of the shortest length curve (extremal
energy) gt, t ∈[0, 1] to the perturbed ﬂow gt(ǫ) = (id + ǫηt + o(ǫ))gt, as depicted in
Figure 10.1. The perturbation has the property that η0 = η1 = 0 so that the boundary
conditions remain satisﬁed at g0(ǫ) = g, g1(ǫ) = h. Using Lemma 10.13 we can identify
how vt satisfying (d/dt)gt = vtgt changes to vt(ǫ) = vt + ǫ(dvt(ǫ)/dǫ) + o(ǫ) satisfying
(d/dt)gt(ǫ) = vt(ǫ)gt(ǫ). This is given according to Lemma 10.13.

326
10 METRICS SPACES FOR THE MATRIX GROUPS
To complete the theorem, the variational equation which the minimizing energy
geodesic satisﬁes
0 = d
dǫ
 1
0
::::vt + ǫ dvt(ǫ)
dǫ
::::
2
M
dt
ǫ=0
= 2
 1
0

vt, dvt(ǫ)
dǫ

M
dt
ǫ=0
(10.48)
= 2
 1
0

Mvt, dηt
dt + ηtvt −vtηt

Rd×d dt
(10.49)
= 2
 1
0

Mvt, dηt
dt

Rd×d dt + 2
 1
0
⟨Mvtv∗
t −v∗
t Mvt, ηt⟩Rd×d dt
(10.50)
(a)
= 2
 1
0

−dMvt
dt
+ Mvtv∗
t −v∗
t Mvt, ηt

Rd×d dt,
(10.51)
with (a) using integration by parts and the boundary conditions on the perturbation.
Since this is zero over all perturbations, the Euler Eqn. 10.47 on the geodesics.
10.5 Conservation of Momentum and Geodesic Evolution of the
Matrix Groups via the Tangent at the Identity
Along the geodesic there is a conservation law; essentially the momentum Mvt is constant. This
implies that by knowing momentum acting against matrix ﬁelds at the identity gives the matrix
ﬁeld and momentum along the geodesic.
Theorem 10.15
Deﬁning the matrix ﬁeld transported from the identity as wt = gtw0g−1
t
,
then along the geodesics satisfying the Euler equation (Theorem 10.14) the momentum is
conserved:
d
dt ⟨Mvt, wt⟩Rd×d = 0,
t ∈[0, 1].
(10.52)
Along the geodesic, the momentum is determined by the momentum at the identity:
Mvt = g−1∗
t
Mv0g∗
t .
(10.53)
Proof
Use the geodesic equation equalling 0 to evaluate the derivative
d
dt⟨Mvt, wt⟩Rd×d =
dMvt
dt
, wt

Rd×d +

Mvt, dwt
dt

Rd×d
(10.54)
(a)
=
dMvt
dt
, wt

Rd×d + ⟨Mvt, (vtwt −wtvt)⟩Rd×d
(10.55)
(b)
=
 d
dtMvt + v∗
t Mvt −Mvtv∗
t , wt

Rd×d
(c)
= 0,
(10.56)
with (a) from Lemma 10.13, and (b) deﬁnition of the transpose, with (c) exactly
following from the Euler Eqn. 10.47 equalling 0.
Part 2: Since wt = gtw0g−1
t
this implies g−1
t
wtgt = w0 with conservation of
momentum implying
⟨Mvt, wt⟩Rd×d = ⟨Mv0, w0⟩Rd×d = ⟨Mv0, g−1
t
wtgt⟩Rd×d
(10.57)
= ⟨g−1∗
t
Mv0g∗
t , wt⟩Rd×d,
(10.58)
which reduces to Eqn. 10.53.

10.6 METRICS IN THE MATRIX GROUPS
327
10.6 Metrics in the Matrix Groups
In particular, various metrics can be calculated in the subgroups.
Theorem 10.16
Let the norm in the tangent space have M = id.
1. For the orthogonal motions gt ∈SO(d) ⊂GL(d), then the velocity on the geodesic
is skew-symmetric, vt = −v∗
t implying the velocity is constant ∂vt/∂t = 0. With A
the d × d skew-symmetric matrix satisfying O′ = eAO ∈O(d), then geodesic distance
between O, O′ is given by
ρ2(O, O′) = 2
(d−1)d/2

i=1
a2
i ,
where A =


0
a1
. . .
ad−1
−a1
0
. . .
ad−2
...
...
...
...
−ad−1
. . .
−a1
0

.
(10.59)
For SO(2), SO(3) the distances become, respectively,
ρ2
SO(2)(O, O′) = 2 arccos2 
1
2trO′O∗
,
(10.60)
ρ2
SO(3)(O, O′) = 2 arccos2 
1
2trO′O∗−1
2

.
(10.61)
2. Thegeodesicdistancebetweendiagonalscalematricesdiag(s1, s2, . . . ), diag(s′
1, s′
2, . . . )is
ρ2(diag(s1, s2, . . . ), diag(s′
1, s′
2, . . . )) =
d

i=1
| log si
s′
i
|2.
(10.62)
3. The geodesic distance between similitudes (s, O), (s′, O′) ∈Sim = (0, ∞) × SO(d)
where O′ = eAO (A of Eqn. 10.59) is given by
ρ2((s, O), (s′, O′)) = | log s
s′ |2 + 2
(d−1)d/2

i=1
a2
i .
(10.63)
Proof
For the orthogonal group with the ﬂow through O(d), since id = gtg∗
t , then
0 = dgt
dt g∗
t + gt
dgt
dt
∗
= 0,
with g0 = O.
(10.64)
Substituting dgt/dt = vtgt, this implies that
0 = vtgtg∗
t + gtg∗
t v∗
t = vt + v∗
t ,
(10.65)
giving that vt is skew-symmetric. Clearly then the speed along vt is constant since
dvt
dt = vtv∗
t −v∗
t vt = 0.
(10.66)

328
10 METRICS SPACES FOR THE MATRIX GROUPS
Thus the general solution for the geodesics becomes gt = eAtO, g0 = O, with A an
n × n skew-symmetric matrix A = −A∗. To calculate the distances determined by the
((d −1)/2)d entries of A explicitly,
ρ2(O, O′) = ρ2(O, eAO) =
 1
0
∥vt∥2
Rd×d dt =
 1
0
trAA∗dt
(10.67)
= trAA∗= 2
(d−1)d/2

i=1
a2
i = 2β2
where β2 =
(d−1)d/2

i=1
a2
i .
(10.68)
In SO(3) with O′ = eAO with A =


0
a1
a2
−a1
0
a3
−a2
−a3
0

, deﬁne β2 = a2
1 + a2
2 + a2
3, then
ρ2(O, O′) = 2β2. Now to simplify the matrix exponential eA = id + A + 1
2A2 . . . , the
matrix sum reduces exploiting the fact that A3 = −β2A since
A2 =


(−a2
1 −a2
2)
−a2a3
a1a3
−a2a3
(−a2
1 −a2
3)
−a1a2
a1a3
−a1a2
(−a2
2 −a2
3)

.
(10.69)
Using the alternating sin, cos series gives the Rodriquez formula:
eA = id + A
β

β −1
3!β3 + · · ·

+ A2
β2

β2
2! −1
4!β4 + · · ·

= id + A
β sin β + A2
β2 (1 −cos β).
(10.70)
Since trA = 0, trA2 = −2β2, then tr eA = 1 + 2 cos β giving β = arccos( 1
2tr O′O∗−1
2)
with geodesic distance in the group
ρ2(O, O′) = 2 arccos2
1
2trO′O∗−1
2

.
(10.71)
For SO(2), it rotates around only one axis according to A =

0
a
−a
0

, giving β = a,
with tr eA = 2 cos β giving β = arccos( 1
2tr O′O∗).
Scale Subgroup: Let
gt = diag(et log
s′
1
s1, et log
s′
2
s2 , . . . )diag(s1, s2, . . . ),
(10.72)
then dgt/dt = diag(log(s′
1/s1), log(s′
2/s2), . . . )gt implying that vt = diag(log(s′
1/s1),
log(s′
2/s2), . . . ) = v∗
t and the geodesic condition is satisﬁed:
dvt
dt = vtv∗
t −v∗
t vt = 0.
(10.73)
The geodesics are exponentials eAt with A = vt; the metric distance becomes
ρ((s1, s2, . . . ), (s′
1, s′
2, . . . )) =
inf
v:g0=s,g1=s′
 1
0
∥vt∥Rd×ddt =
d

i=1
| log s′
i −log si|.
(10.74)

10.6 METRICS IN THE MATRIX GROUPS
329
The Similitudes: Examine geodesics in the similitudes Sim(4) = (0, ∞) × SO(3)
with gt = (st/s)OtsO, t ∈[0, 1], then the derivative becomes
dgt/dt =
dst/dt
st
id + dOt
dt O−1
t

gt.
(10.75)
The geodesic distance ρ((O, s), (O′, s′)) is then
ρ2((O, s), (O′, s′)) =
inf
˙g: g0=g
g1=g′
 1
0
∥vt∥2
T dt =
inf
dst
dt , dOt
dt : s0=s, O0=O
s1=s′, O1=O′
 1
0

(dst/dt)2
s2
t
+ 2tr(dst/dt)
st
dOt
dt O−1
t
+ trdOt
dt
dOt
dt
∗
dt.
Then, with dOt/dt = AOt where A is skew-symmetric, then clearly tr(dOt/dt)O−1
t
= 0
since trA = 0. This gives the geodesic
ρ2((O, s), (O′, s′)) =
inf
˙s:
s0=s
s1=s′
 1
0
(dst/dt)2
s2
t
dt +
inf
˙O:
O0=O
O1=O′
 1
0
trdOt
dt
dOt
dt
∗
dt
(10.76)
=
log s
s′

2
+ 2

i
a2
i .
(10.77)
Remark 10.6.0
Clearly logarithm plays a fundamental role since the matrices involve
simple multiplication. Look at the scalar case. Deﬁning u = log s, then ˜ρ : R×R →R+
is a metric
˜ρ2(u, u′) =
inf
u:
u0=u
u1=u′
 1
0
|˙ut|2 dt = |u −u′|2.
(10.78)
Clearly log : (0, ∞) →R is a bijection, thus ρ : (0, ∞) × (0, ∞) →R+ deﬁned by
ρ(s, s′) = ˜ρ(log s, log s′) = | log s′
s | is a metric.
10.7 Viewing the Matrix Groups in Extrinsic Euclidean Coordinates
Viewing the groups of transformation as vector subspaces of Rd×d and R(d+1)×(d+1) then the
metric distances between elements in the subgroups are just that given by the Euclidean distance.
10.7.1 The Frobenius Metric
Theorem 10.17
Viewed as vector subspaces of Rd×d with Frobenius norm ∥A −B∥Rd×d =
tr(A−B)(A−B)∗, then the matrix groups of GL(d) have geodesics gt, t ∈[0, 1], g0 = g, g1 =
h, g, h ∈GL(d) given by
gt = t(h −g) + g,
vt = h −g.
(10.79)

330
10 METRICS SPACES FOR THE MATRIX GROUPS
The metric ρ is the Frobenius metric, so that for A, A′ ∈GL(d) ⊂Rd2,
ρ2
Rd×d(A, A′) = ∥A −A′∥2
Rd×d = tr(A −A′)(A −A′)∗.
(10.80)
For SO(d) as a vector subspace of Rd×d, then
ρ2
Rd×d(O, O′) = 2d −2tr OO′∗.
(10.81)
For SE(d) in homogeneous coordinates ¯g =
 A
a
0
1

∈¯A , A ∈GL(d) , a ∈Rd, then
ρ2
R(d+1)2 (¯g, ¯g′) = ∥¯g −¯g′∥2
R(d+1)2 = ∥A −A′∥2
Rd×d + ∥a −a′∥2
Rd.
(10.82)
Proof
Viewing GL(d) ⊂Rd×d as a vector subspace, the geodesics are straight lines
(Theorem 10.5):
gt = A + t(A′ −A) , g0 = A , t ∈[0, 1].
(10.83)
Notice, the tangent space of Rd×d itself given by the slope of the line connecting the
group elements: dgt/dt = A′ −A. Then as it should be
ρ2(A, A′) =
 1
0
∥vt∥2
Rd×d dt = ∥A′ −A∥2
Rd×d.
(10.84)
For SO(d), direct calculation shows
ρ2
Rd×d(O, O′) = 2d −tr O′O∗−tr OO′∗= 2d −2tr O′O∗.
(10.85)
For the homogeneous coordinate representation of shift,
ρ2
Rd×d(¯g, ¯g′) = tr(g −g′)(g −g′)∗= tr
 A −A′
a −a′
0
1
  A −A′
a −a′
0
1
∗
(10.86)
= ∥A −A′∥2
Rd×d + ∥a −a′∥2
Rd .
(10.87)
10.7.2 Comparing intrinsic and extrinsic metrics in SO(2,3)
To appreciate that the GL(d) and SO(d) distances are fundamentally different, one through the
Euclidean space associated with GL(d), and one the geodesic distance through the curved space
of SO(d), examine SO(2), SO(3). For SO(d) the distance based on Frobenius norm for matrices
∥A∥2
Rd×d = tr AA∗becomes 2d −2 tr OO′∗. For SO(2), identify O(θ), O′(θ′) ↔θ, θ′. Then A =
(θ −θ′)
 0
−1
1
0

giving (comparing the distances)
ρ2
R2×2(O, O′) = ∥O −O′∥2
R2×2 = 4 −4 cos(θ −θ′) ,
(10.88)
ρ2
SO(2)(O, O′) = tr AA∗= 2(θ −θ′)2.
(10.89)

10.7 VIEWING THE MATRIX GROUPS
331
Notice, for small difference in angles, the two are similar. For SO(3), write O′ = eAO where
A = −A∗is a skew-symmetric matrix of the form A =


0
a1
a2
−a1
0
a3
−a2
−a3
0

. Deﬁne the normalized
skew-symmetricmatrixZ = (1/β)Awithβ2 = 
3
i=1 a2
i . ThenO′ = eβZOinthematrixsumreduces
exploiting the fact that Z3 = −Z from Eqn. 10.69. Using the alternating sin, cos series gives
eβZ = I + Z sin β + Z2(1 −cos β).
(10.90)
This gives a direct formula for obtaining Z from O ∈SO(3) according to O = eβZ. Using the
facts that tr Z = 0, tr Z2 = −2 giving tr O = 1 + 2 cos β and cos β = 1
2(tr O −1). Thus we have
obtained the Frobenius norm required which can be compared with the geodesic distance through
the group:
ρ2
R3×3(O, O′) = ρ2
R3×3(O, eβZO) = 6 −2tr eβZ = 4 −4 cos β;
(10.91)
ρ2
SO(3)(O, O′) = ρ2
SO(3)(O, eβZO) = 2β2.
(10.92)
A direct formula for Z may be obtained from eβZ = O′O∗with β = arccos 1
2(tr O −1). Let n
be the axis of rotation of O, eigenvector eigenvalue 1 so that On = n, then
On = exp(βZ)n = n + sin βZn + (1 −cos β)Z2n,
(10.93)
with Zn = 0. As n is the eigenvector of O, let Z ∝(O −O∗), with the proportionality constant
given by the normalizing constraint, then
Zn ∝On −O∗n = 0.
This gives the exact formula for Z,
Z =


0
−nz
ny
nz
0
−nx
−ny
nx
0

,
where n = [nx, ny, nz]∗is the axis of rotation and is given by
nx = O32 −O23
2 sin β
,
ny = O13 −O31
2 sin β
,
nz = O21 −O12
2 sin β
.

11 M E T R I C S S PA C E S F O R T H E I N F I N I T E
D I M E N S I O N A L D I F F E O M O R P H I S M S
ABSTRACT
In this chapter the metric space structure of shape is developed by studying the
action of the inﬁnite dimensional diffeomorphisms on the coordinate systems of shape. Rie-
mannian manifolds allow us to develop metric distances between the group elements. We
examine the natural analog of the ﬁnite dimensional matrix groups corresponding to the
inﬁnite dimensional diffeomorphisms which are generated as ﬂows of ordinary differential
equations. We explore the construction of the metric structure of these diffeomorphisms and
develop many of the properties which hold for the ﬁnite dimensional matrix groups in this
inﬁnite dimensional setting.
11.1 Lagrangian and Eulerian Generation of Diffeomorphisms
Clearly the matrix groups are diffeomorphisms acting on the background space g : X = Rd →
X. They are of ﬁnite dimension encoded via the parameters of the matrices. Now examine the
construction of the group of inﬁnite dimensional diffeomorphisms acting on bounded X ⊂Rd.
Deﬁnition 11.1
Let X be the background space, then deﬁne g : X →X is a diffeo-
morphism with inverse g−1, and deﬁne the group of transformations G, ◦as subgroups of
diffeomorphisms acting on the background space X with the law of composition of functions
g(·) ◦g′(·) = g(g′(·)).
That this is a group follows from the fact that the identity id ∈G, the composition of two
diffeomorphisms, is in the group g ◦g′ = g(g′) ∈G and the inverse is in the group as well
g−1 ∈G.
Unlike the ﬁnite dimensional matrix groups, in the inﬁnite dimensional setting it is much less
clear how to generate the mappings which are diffeomorphisms. For this, the approach has been
to construct the diffeomorphisms as a ﬂow of ordinary differential equations (ODEs) as originally
put forward by Christensen et al. [220].
Assume the diffeomorphisms g ∈G evolve in time as a ﬂow gt, t ∈[0, 1] with an associated
vector ﬁeld v : X × [0, 1] →Rd controlling the Lagrangian evolution according to
∂gt
∂t (x) = vt(gt(x)),
g0(x) = x, x ∈X,
t ∈[0, 1].
(11.1)
Figure 11.1 depicts the Lagrangian formulation of the ﬂow of diffeomorphisms.
The forward and inverse maps are linked through the fact that for all t ∈[0, 1], x ∈X,
g−1
t
(gt(x)) = x. Deﬁning the d×d Jacobian matrix Df =

∂fi/∂xj

, then differentiating the identity
gt
g1
Vt(gt)
g0=Id
Figure 11.1 Figure shows the Lagrangian description of the ﬂow depicting the ODE (dgt/dt) =
vt(gt), g0 = id.
332

11.1 LAGRANGIAN AND EULERIAN GENERATION
333
Figure 11.2 Top row: Panels show the vector ﬁelds vt(·), t1 = 0, t2, t3, t4, t5, t6 depicting the ﬂow
of a sequence of diffeomorphisms satisfying ∂gt/∂t = vt(gt), g0 = id. Middle row: Panels depict
the ﬂow of diffeomorphisms g−1
t
associated with the vector ﬁelds in the top row. Bottom row:
Panels depict the ﬂow of particles which carry black and white labels corresponding to the circular
movement of the patch under the ﬂow of diffeomorphisms I ◦g−1
t
.
gives the inverse evolution according to
0 = ∂
∂tg−1
t
(gt(x)) =∂g−1
t
∂t ( gt(x)) + Dg−1
t
(gt(x))∂gt
∂t (x)
(11.2)
=∂g−1
t
∂t (gt(x)) + Dg−1
t
(gt(x))vt( gt(x)).
(11.3)
Substituting y = gt(x) gives the transport equation, or the evolution of the inverse:
∂g−1
t
∂t (y) = −Dg−1
t
(y)vt(y), g−1
0 (y) = y,
y ∈X.
(11.4)
Example 11.2 (Rotation of Grid [268])
This example taken from Beg [268] examines
the generation of a ﬂow via the numerical integration of a speciﬁed series of vector
ﬁelds. Depicted in Figure 11.2 are a series of vector ﬁelds vt(·), t is used to construct the
ﬂow of the grid. Shown in the middle row are the integration of the ﬂow generating
g−1
t
; the diffeomorphisms were applied to a discrete lattice and depicted by drawing
the movement of the coordinate lines. Shown in the bottom row is the transport of
material according to I ◦g−1
t
which forms the basis for our modeling of the image
orbits in the next chapter.
11.1.1 On Conditions for Generating Flows of Diffeomorphisms
In the matrix groups, the diffeomorphisms are identiﬁed with the ﬁnite number of parameters
specifying the linear action of the group. For the inﬁnite dimensional case generated via the ﬂow
of Eqn. 11.1, the vector ﬁelds play this role. Naturally, since the vector ﬁelds vt, t ∈[0, 1] are not a
ﬁnite set of parameters, these mappings are inﬁnite dimensional mappings.

334
11 METRICS SPACES FOR THE INFINITE DIMENSIONAL DIFFEOMORPHISMS
To ensure that the ODEs generate diffeomorphisms the vector ﬁelds are constrained to be
sufﬁciently smooth so that vt ∈V, t ∈[0, 1], where V is a Sobolev space (Hilbert space with ﬁnite
norm on derivatives). The starting point for generating smooth ﬂows is the compactly supported
(0-boundary) continuously p-differentiable vector ﬁelds Cp
0(X, Rd) with associated sup-norm
∥· ∥p,∞. Clearly over ﬁnite times, then for vt, t ∈[0, 1] sufﬁciently smooth, Eqn. 11.1 is inte-
grable and generates diffeomorphisms (see Boothby [148]). Dupuis and Trouve have shown that
the minimal amount of smoothness required [269,270] is that for X ⊂Rd bounded the ﬁelds be at
least continuously 1-times differentiable C1
0(X, Rd) with sup-norm
∥v∥1,∞= sup
x∈X
d

i=1
|vi(x)| +
d

i=1
d

j=1

∂vi
∂xj
(x)
 .
(11.5)
The pointwise continuity and differentiability properties are controlled by controlling the
integral square of the derivatives by forcing the indexed family of vector ﬁelds vt, t ∈[0, 1] to be in
a reproducing kernel Hilbert space (V, ∥· ∥V), with s-derivatives having ﬁnite integral square and
zero boundary. Via the Sobolev embedding theorems this then relates to the sup-norm pointwise
smoothness. The norm dominates for each of the components vit, i = 1, . . . , d and dominates the
s−derivatives according to
∥v∥2
V ≥
d

i=1

X

α≤s
|Dαvi(x)|2 dx.
(11.6)
The smoothness conditions are dimension dependent: for X ⊂Rd, the number of derivatives s
required in the Sobolev space norm for the sup-norm p = 1 condition is s > d/2 + 1.32
Theorem 11.3
[Dupuis et al. [269] and Trouve [270]] Consider the Hilbert space of vector
ﬁelds (V, ∥· ∥V) on X compact with zero boundary conditions.
Then if there exists some constant c, such that for some p ≥1 and all v ∈V,
∥v∥V ≥c∥v∥p,∞,
(11.7)
with the vector ﬁelds satisfying
 1
0 ∥vt∥Vdt < ∞, then the ordinary differential equation
Eqn. 11.1 can be integrated over [0, 1], and g1(·) : X →X is a diffeomorphism of X.
The set of all such diffeomorphism solutions obtained from V = {v :
 1
0 ∥vt∥Vdt < ∞}
form the group:
G(V) =

g1 : ∂gt
∂t = vt(gt), t ∈[0, 1], v ∈V

.
(11.8)
Proof
The diffeomorphic properties rely most heavily on the most basic property of
solutions of the ODE Eqn. 11.1.
Lemma 11.4
For v sufﬁciently smooth, for any t ∈[0, 1], and any x ∈X, then Eqn. 11.1
admits a unique solution for all t ∈[0, 1].
The smoothness required is proved in [269,270]; see Lemma 2.2 of [269].
That the solutions are 1–1 mapping from X onto itself, ﬁrst since v(x) = 0, x ∈∂X implies
gt(x) = x, x ∈∂X. Uniqueness implies points inside the boundary cannot cross the boundary,
thus g1(·) is a mapping into X. Uniqueness also implies the mapping is 1–1, since if two points
cross, g1(x) = g1(x′) = y, then following the negative vector ﬁeld from y generates two different
solutions. The mapping is onto, since given any point y, then by ﬂowing the negative velocity
32 The dominating condition is background space dependent so that for X ⊂Rd, the number of derivatives
s required in the Sobolev space norm to dominate the sup-norm in Cp
0 is s > (d/2) + p.

11.1 LAGRANGIAN AND EULERIAN GENERATION
335
d˜gt/dt = −v1−t(˜gt), ˜g0 = id ﬁeld from g1(y) maps back to y (see part of proof below Eqns. 11.10,
11.11). The differentiability property follows from the sufﬁcient differentiability in space of vt(·)
for all t ∈[0, 1].
The group properties follow from manipulations on the paths. Denote the solution cor-
responding to velocity v as gv. Clearly v = 0 ∈V, with the identity element, id = g1(·). That
gv′
1 ◦gv
1 ∈G follows by deﬁning ˜v according to
˜vt = 2v2t,
t ∈[0, 1
2),
˜vt = 2v′
2t−1,
t ∈[ 1
2, 1).
(11.9)
Then ˜v ∈V is ﬁnite, and ˜g ∈G and ˜g = g′ ◦g. Constructing g−1 via the negative of the
vector ﬁeld gives the inverse in the group G. To see this, examine the semi-group property of the
ﬂows. Deﬁne φs,t to satisfy ∂φs′,t/∂t = vt(φs′,t), φs′,s′(x) = x. Then φs′,t = φst ◦φs′,s, since yt = φs′,t
and ˜yt = φs,t ◦φs′,s satisfy the same differential equation both with initial condition ys = ˜ys=φs′,s,
and the ODE has a unique solution. For gv
t with ∂gv
t /∂t = vt(gv
t ), let wt = −v1−t with gw
t satisfying
∂gw
t /∂t = −v1−t, then
gw
1−t(x) = x −
 1−t
0
v1−s ◦gw
s (y) ds = x +
 t
1
vs ◦gw
1−s ds.
(11.10)
For t = 0 then this is precisely the inverse, gw
1 = φ1,0 implying
gw
1 ◦gv
1 = φ1,0 ◦φ0,1 = φ0,0 = id.
(11.11)
Thus the inverse

gv
1
	−1 = gw
1 is an element of the set.
11.1.2 Modeling via Differential Operators and the Reproducing
Kernel Hilbert Space
As we have seen in Chapter 9, Section 9.4 on Green’s kernels and Sobolev Hilbert spaces the
pointwise continuity of the vector ﬁelds is obtained via control in the square norm on the differ-
entiability properties. The indexed family of vector ﬁelds vt, t ∈[0, 1] in X ⊂Rd are constrained to
be in a reproducing kernel Hilbert space [271] V with s-derivatives having a ﬁnite integral square;
via the Sobolev embedding theorems this then relates to the sup-norm pointwise continuity and
differentiability. For any f ∈V, α ∈Rm the kernel K sifts according to
⟨K(·, x)α, f⟩V =
D
α, f(x)
E
Rd .
(11.12)
The smoothness of f ∈V depends on the smoothness of the kernel function K; choosing a proper
kernel such that V is embedded in C1(X) assures the necessary smoothness for the generation
of diffeomorphisms. To see the explicit dependence of the smoothness of functions f ∈V on the
kernel, see Section 9.4.2 with Eqn. 9.74.
Most often the construction of the reproducing Hilbert space (with associated kernel) designs
the norm ∥· ∥V by starting with a one-to-one d × d matrix differential operator L = (Lij) with null
boundary conditions operating on Rm valued vector ﬁelds of the form (Lv)j = 
d
i=1 Lijvi, j =
1, . . . , d. The inner product in V is deﬁned by the operator and it’s adjoint,
⟨f, h⟩V = ⟨Lf, Lh⟩2 = ⟨Af, h⟩,
with A = L∗L,
(11.13)

336
11 METRICS SPACES FOR THE INFINITE DIMENSIONAL DIFFEOMORPHISMS
where L∗is the adjoint operator and ⟨·, ·⟩2 is the L2 vector inner product.33 The differentiability in
L is selected to dominate the sup norm ∥· ∥p,∞for at least p = 1 derivative of Theorem 11.3 for
each of the components vit, i = 1, . . . , d.
From a modeling point of view, the differential operators are usually chosen via analogs
based on physical models; Christensen et al. [220] originally selected particular forms for the
norm based on arguments from physical modeling analogies in ﬂuid mechanics. Powers of the
Laplacian for the classic thin-plate splines [223,224,272] have been used, and the Cauchy operator
for 3D elasticity L = −α + β∇∇· +γ ∇2 [220, 273–275], the differential operators with sufﬁcient
derivatives and proper boundary conditions ensure the existence of solutions of the transport
equation in the space of diffeomorphic ﬂows [269,276,277].
11.2 The Metric on the Space of Diffeomorphisms
Examine the construction of metric distances on the group G(V) of inﬁnite dimensional diffeo-
morphisms on the closed and bounded background space X ⊂Rd. Again the Riemannian length
is constructed deﬁning the curves gt, t ∈[0, 1] in the Riemannian manifold of diffeomorphisms
connecting group elements g0 = g, g1 = h ∈G(V). The curves are generated via the ﬂow Eqns. 11.1,
11.4. Each point along any curve is a diffeomorphism in G(V).
Thenaturalextensionfromtheﬁnitedimensionalcurvesistoconsiderpathsg : [0, 1] →G(V)
connecting elements g, h ∈G(V) with the tangent element of the evolution ∂gt
∂t (g−1
t
) = vt ∈V, t ∈
[0, 1], with norm ∥· ∥V satisfying Theorem 11.3. In contrast to the ﬁnite dimensional setting here
the tangents to the curve vt are vector ﬁelds, the tangent space a Hilbert space with ﬁnite norm
∥vt∥V < ∞. Clearly, unlike the ﬁnite dimensional setting of the matrix groups, these are curves
through the inﬁnite dimensional space of diffeomorphisms gt ∈G(V), t ∈[0, 1]. The length and
energy of these inﬁnite dimensional curves through the group of diffeomorphisms are deﬁned via
their vector ﬁelds:
Length(v) =
 1
0
::::
∂gt
∂t (g−1
t
)
::::V
dt =
 1
0
∥vt∥V dt.
(11.14)
With this norm in the tangent space now deﬁned, the Euler equation for inﬁmum length
paths as proved in [278] generalize the ﬁnite matrix group Euler equation of Theorem 10.14.
First we calculate the analog of the Lie bracket for the matrix case of Lemma 10.13, Eqn.
10.45, in this inﬁnite dimensional setting for how the vector ﬁeld changes with a perturbation of
the group ﬂow. We will also require the perturbation of the group ﬂow with a perturbation of the
vector ﬁeld.
Lemma 11.5
The perturbation of the group element gt →gt(ǫ) = gt +ǫηt(gt)+o(ǫ) gives
the perturbation of the vector ﬁeld vt →vt(ǫ) = vt + ǫ(∂vt(ǫ)/∂ǫ) + o(ǫ) satisfying the
function space equation (analagous to the ﬁnite dimensional Lie bracket of Lemma 10.13):
ψt = ∂ηvt = ∂vt(ǫ)
∂ǫ
= ∂ηt
∂t −(Dvt ηt −Dηt vt).
(11.15)
33 Strictly speaking, although Lf ∈L2, in general Af /∈L2 so that ⟨Af, g⟩is not the L2 inner product; rather
Af is a 1-form in the dual.

11.2 THE METRIC ON THE SPACE OF DIFFEOMORPHISMS
337
The perturbation of the vector ﬁeld vt →vt + ǫψt + o(ǫ) generates a perturbation of
the group element gt →gt(ǫ) = (id + ǫηt)(gt) + o(ǫ) = gt + ǫηt(gt) + o(ǫ) where
∂ψgt = ∂
∂ǫ gt(ǫ)
ǫ=0
= ηt(gt) = Dgt
 t
0
(Dgu)−1ψu(gu) du
(11.16)
=
 t
0
Dgu,t(gu)ψu(gu) du.
(11.17)
Proof
We have
∂
∂t(gt + ǫηt(gt)) =

vt + ǫ ∂vt(ǫ)
∂ǫ

(gt + ǫηt(gt)) + o(ǫ)
(a)
= vt(gt) + ǫDvt(gt) ηt(gt) + ǫ ∂vt(ǫ)
∂ǫ
(gt) + o(ǫ)
= ∂gt
∂t + ǫ ∂ηt
∂t (gt) + ǫDηt(gt) ∂gt
∂t
(b)
= vt(gt) + ǫ ∂ηt
∂t (gt) + ǫDηt(gt) vt(gt),
and equating (a),(b) and applying g−1
t
on the right to both terms gives Eqn. 11.15
∂vt(ǫ)
∂ǫ
= ∂ηt
∂t + Dηt vt −Dvt ηt.
From the deﬁnition of the perturbation ∂ψgt = ηt(gt). To prove ηt(gt) equals the right-
hand sides of Eqn. (11.16), clearly for t = 0 we have η0(g0) = 0, and they satisfy the
same differential equation.
Differentiating the left hand side ηt(gt) gives
∂ηt(gt)
∂t
= ∂ηt
∂t (gt) + Dηt(gt)vt(gt) (a)
= Dvt(gt)ηt(gt) + ψt(gt)
(11.18)
where (a) follows from Lemma 11.5, Eqn. 11.15. Differentiating the right hand side of
Eqn. 11.16 gives the identical equation
∂
∂tDgt
 t
0
(Dgu)−1ψu(gu)du = ∂Dgt
∂t
 t
0
(Dgu)−1ψu(gu) du + Dgt(Dgt)−1ψt(gt)
= Dvt(gt)Dgt
 t
0
(Dgu)−1ψu(gu) du + ψt(gt).
(11.19)
The equality of Eqn. 11.17 follows from the fact that
(D(gt ◦g−1
u ))(gu) = Dgt(g−1
u
◦gu)Dg−1
u (gu) = Dgt(Dgu)−1.
(11.20)
Theorem 11.6
Assume the inner product ∥v∥2
V = ⟨Av, v⟩2 satisﬁes the p = 1 condition of
Theorem 11.3.34 Given g, h ∈G(V), then the geodesics minimizing
 1
0
∥vt∥2
Vdt =
 1
0
⟨Avt, vt⟩2 dt,
34 There exists some constant c such that for some p ≥1 and all v ∈V, then ⟨Av, v⟩2 ≥c∥v∥p,∞.

338
11 METRICS SPACES FOR THE INFINITE DIMENSIONAL DIFFEOMORPHISMS
connecting g, h ∈G(V) are solutions of the Euler equation
∂Avt
∂t
+ (Dvt)∗Avt + div(Avt ⊗vt) = 0,
(11.21)
with divergence operator div v = 
d
i=1(∂vi/∂xi) and div(Av ⊗v) = (DAv)v + (div v)Av.
Proof
The proof on the geodesic follows a similar path as for the matrix groups.
Examine a smooth perturbation of the shortest length curve (extremal energy) gt, t ∈
[0, 1] to the perturbed ﬂow gt(ǫ) = (id + ǫηt + o(ǫ))gt as depicted in the left panel of
Figure 10.1. So that the boundary conditions are satisﬁed at g0(ǫ) = g, g1(ǫ) = h the
perturbation must satisfy η0 = η1 = 0, and η(x) = 0, x ∈∂X. From Lemma 11.5 we
require how the vector ﬁeld changes with the perturbation in the ﬂow. The variational
equation which the minimizing energy geodesic satisﬁes is
0 = ∂
∂ǫ
 1
0
::::vt + ǫ ∂vt(ǫ)
∂ǫ
::::
2
V
dt|ǫ=0 = 2
 1
0

vt, ∂vt(ǫ)
∂ǫ

V
dt.
(11.22)
(a)
= 2
 1
0

Avt, ∂ηt
∂t + Dηt vt −Dvt ηt

2
dt
(11.23)
(b)
= 2
 1
0

−∂Avt
∂t
−(Dvt)∗Avt, ηt

2
dt + 2
 1
0
⟨Avt, Dηt vt⟩2 dt,
(11.24)
with (a) from Eqn. 11.15 and (b) following from integration by parts and the boundary
condition η0 = η1 = 0. From Stoke’s theorem, since v and η vanish on ∂X,
⟨Avt, Dηt vt⟩2 = −⟨div(Avt ⊗vt), ηt⟩2
giving
0 =
 1
0

−∂Avt
∂t
−(Dvt)∗Avt −div(Avt ⊗vt), ηt

2
dt.
(11.25)
Since this is zero over all perturbations, it gives the Euler Eqn. 11.21.
Remark 11.2.0
The approach to deriving Equation 11.21 was ﬁrst derived by Arnold
for the incompressible divergence free ﬂow [equation 1 of [279]], and then by
Mumford [280] and Miller et al. [278] and in this setting. The approach shown here
which parallels the ﬁnite Lie group approach has emerged more recently through the
continued work of Trouve et al. [281]. In 1-dimension with A = id the equation reduces
to Burger’s equation as discussed by Mumford [280].
11.3 Momentum Conservation for Geodesics
The Euler equation examines the shortest path geodesic corresponding to a perturbation on the
ﬂow of diffeomorphisms. Now following [281] examine the variational problem in the tangent
space of the group minimizing with respect to the vector ﬁelds generating the shortest path ﬂows.
As we have already seen for the matrix groups corresponding to ﬁnite dimensional mechanics,
the geodesic is completely determined by the initial momentum (Theorem 10.15, Chapter 10). For
the deformable setting the generalized momentum is naturally deﬁned as Av the function of the
vector ﬁeld which when acting against the vector ﬁeld gives energy in the metric ∥v∥2
V = ⟨Av, v⟩2.

11.3 MOMENTUM CONSERVATION FOR GEODESICS
339
There are however substantive difﬁculties with the momentum. In general it is not smooth enough
to be in L2, so strictly speaking it cannot be deﬁned via the L2 inner product ⟨Av, v⟩2. However, it
acts as a 1-form on smooth enough objects such as the elements of V, and in particular Av acting
on v is well deﬁned.
For the metric mapping of the start and end point, there is a conservation law which requires
the momentum to be conserved along the path of the geodesic ﬂow. This of course implies that
the momentum at the identiy Av0 on the geodesic determines the entire ﬂow gt, t ∈[0, 1], where
˙gt = vt(gt), g0 = id. More generally for growth, the momentum is constantly being transformed
along the shortest path.
Lemma 11.7
Deﬁne
the
vector
ﬁeld
transported
from
the
identity
as
wt
=
Dgt(g−1
t
)w0(g−1
t
), then
∂wt
∂t = (Dvt)wt −(Dwt)vt.
(11.26)
Proof
Deﬁne the notation wt = (Dgtw0)(g−1
t
) then
∂wt
∂t = ∂
∂t

Dgtw0(g−1
t
)
	
= ∂Dgtw0
∂t
(g−1
t
) + D(Dgtw0)(g−1
t
)∂g−1
t
∂t
(11.27)
=

D(vt(gt))w0
	
(g−1
t
) + D(Dgtw0)(g−1
t
)(−Dg−1
t
vt)
(11.28)
=

Dvt(gt)Dgtw0
	
(g−1
t
) −D

(Dgtw0)(g−1
t
)
	
vt
= (Dvt)wt −(Dwt)vt.
(11.29)
Theorem 11.8
Deﬁning the vector ﬁeld transported from the identity as wt
=
Dgt(g−1
t
)w0(g−1
t
), then along the geodesics satisfying the Euler equation (Theorem 11.6,
Eqn. 11.21) the momentum is conserved:
∂
∂t⟨Avt, wt⟩2 = 0,
t ∈[0, 1].
(11.30)
Along the geodesic, the momentum is determined by the momentum at the identity:
Avt = (Dg−1
t
)∗Av0(g−1
t
)|Dg−1
t
|.
(11.31)
Proof
Use the Euler equation to evaluate the derivative
∂
∂t⟨Avt, wt⟩2 =
 ∂
∂tAvt, wt

2
+

Avt, ∂
∂twt

2
(11.32)
(a)
=
 ∂
∂tAvt, wt

2
+ ⟨Avt, (Dvtwt −Dwtvt)⟩2
(11.33)
(b)
=
 ∂
∂tAvt + (Dvt)∗Avt + div(Avt ⊗vt), wt

2
= 0
(11.34)
with (a) from Lemma 11.5 and (b) exactly Eqn. 11.25 which by integration of parts
gives the Euler equation equalling 0.

340
11 METRICS SPACES FOR THE INFINITE DIMENSIONAL DIFFEOMORPHISMS
Part 2: Since wt = Dgt(g−1
t
)w0(g−1
t
) this implies (Dgt)−1wt(gt) = w0 with
conservation of momentum implying
⟨Avt, wt⟩2 = ⟨Av0, w0⟩2 = ⟨Av0, (Dgt)−1wt(gt)⟩2
(11.35)
= ⟨(Dgt)−1∗Av0, wt(gt)⟩2
(11.36)
= ⟨(Dgt)−1∗(g−1
t
)Av0(g−1
t
)|Dg−1
t
|, wt⟩2.
(11.37)
This reduces to Eqn. 11.31 using the inverse function theorem Dg−1
t
= (Dgt)−1(g−1
t
).
11.4 Conservation of Momentum for Diffeomorphism Splines
Speciﬁed on Sparse Landmark Points
To illustrate the conservation of momentum and the difﬁculty of viewing the momentum as a
function, examine the fundamental question of how to choose the initial momentum to gen-
erate mappings of correspondences between one set of points and another. Assume we are
given observations of coordinatized objects through ﬁnite sets of point locations or features,
x1, x2, . . . , xN, x′
1, x′
2, . . . , x′
N; term these objects N-shapes. We return to this again in Section 12.6,
Chapter 12 when we study landmarked metric spaces in greater detail. Assume the diffeomor-
phism is known at subsets of points g0(xn) = xn, g1(xn) = x′n, then Joshi argued [242,282], that if a
vector ﬁeld vt ∈V, t ∈[0, 1] is an optimum trajectory satisfying a particular set of constraints then
to be of minimum length it must have the property that while the vector ﬁelds are constrained
along the particle paths vt(gt(xn)) it must otherwise be of minimum norm-square ∥vt∥2
V. Thus
the diffeomorphism landmark matching problem is essentially the creation of diffeomorphism
splines, analogous to the standard spline formulations [283]. The solution to the spline problem
has as the minimum energy solution a linear combination of reproducing kernels associated with
(V, ∥· ∥V) with the linear weights chosen to satisfy constraints.
Joshi [282] ﬁrst examined the diffeomorphic correspondence of landmarked shapes in
Euclidean subsets of Rd, d = 2, 3. The momentum at the identity is transported according to
the conservation law above.
Theorem 11.9 (Joshi Splines)
Along the geodesics connecting the N-shapes xn
=
g0(xn), x′n = g1(xn), n = 1, . . . , N, the vector ﬁelds are splines of the form
vt(·) =
N

n=1
K(gt(xn), ·)βnt,
(11.38)
where K is the Green’s kernel associated with A determining the norm, with the momentum
satisfying
Avt(·) =
N

n=1
δ(gt(xn) −·)βnt.
(11.39)
The optimizing ﬂow minimizing the inexact matching problem
 1
0
∥vt∥2
V dt +
N

n=1
∥x′
n −g1(xn)∥2
Rd,
(11.40)

11.4 CONSERVATION OF MOMENTUM FOR DIFFEOMORPHISM
341
has momentum of the geodesic at t = 0 given by
Av0(·) =

n
δ(xn −·)Dg1(xn)∗(x′
n −g1(xn)) ;
(11.41)
the momentum transported along the geodesic Avt = (Dg−1
t
)∗Av0(g−1
t
)|Dg−1
t
| is given by
Avt(·) =

n
δ(gt(xn) −·)(Dgt,1(gt(xn)))∗(x′
n −g1(xn)).
(11.42)
Proof
The proof exploits the fact that if vt ∈V is of minimum norm for each t ∈[0, 1],
then this implies
 1
0 ∥vt∥2
Vdt is minimum. Let vt, t ∈[0, 1] be a candidate vector ﬁeld
connecting xn, x′n of the form g0(xn) = xn, g1(xn) = x′n, n = 1, . . . , N, then vt is optimal
of minimum norm-square if it is of the form
vt(x) =
N

n=1
K(gt(xn), x)βnt,
implying Avt(x) =
N

n=1
δ(gt(xn) −x)βnt.
(11.43)
Choose another candidate solution νt = vt + ht, t ∈[0, 1] with ht(gt(xn)) = 0,
t ∈[0, 1], n = 1, 2, . . . leaving the second constraint term unchanged on the constrained
paths. Then
∥νt∥2
V = ∥vt + ht∥2
V = ∥vt∥2
V + ∥ht∥2
V + 2⟨vt, ht⟩V
(11.44)
= ∥vt∥2
V + ∥ht∥2
V + 2
 N

n=1
K(gt(xn), ·)βnt, ht

V
(11.45)
(a)
= ∥vt∥2
V + ∥ht∥2
V + 2
 N

n=1
βnt, ht(gt(xn))

Rd
= ∥vt∥2
V + ∥ht∥2
V ≥∥vt∥2
V,
(11.46)
where (a) follows from the reproducing property, and h being zero on the paths of
the particles.
To compute the variation, of the inexact matching, the ﬁrst term contributes Avt,
and the gradient of the second term in the cost Eqn. 11.40 requires the perturbation
ηt(gt) =
 t
0 Dgu,t(gu)ψu(gu)du. Then for t = 1 gives the variation of the cost term
becomes
−
N

n=1

x′
n −g1(xn),
∂gv+ǫψ
1
(xn)
∂ǫ

Rd
(a)
= −
N

n=1

x′
n −g1(xn),
 1
0
Dgu,1(gu(xn))ψu(gu(xn)) du

Rd
= −
 1
0
N

n=1
⟨(Dgu,1(gu(xn)))∗(x′
n −g1(xn)), ψu(gu(xn))⟩Rd du
(b)
= −
 1
0
N

n=1
⟨K(gu(xn), ·)(Dgu,1(gu(xn)))∗(x′
n −g1(xn)), ψu⟩V du
where (a) is the substitution from Eqn. 11.17 of Lemma 11.5 and (b) is the sifting
property of the Green’s operator, which completes the proof of Eqn. 11.42.

342
11 METRICS SPACES FOR THE INFINITE DIMENSIONAL DIFFEOMORPHISMS
To see the momentum is transported, start with Av0(x) = 
N
n=1 δ(xn −x)
Dg0,1(xn)∗(x′n −g1(xn)), giving
(Dg−1
t
(·))∗Av0(g−1
t
(·))|Dg−1
t
(·)|
=
N

n=1
δ(xn −g−1
t
(·))(Dg−1
t
(gt(xn)))∗Dg0,1(xn)∗(x′
n −g1(xn))|Dg−1
t
(·)|
=
N

n=1
δ(xn −g−1
t
(·))(Dgt,1(gt(xn)))∗(x′
n −g1(xn))|Dg−1
t
(·)|.
(11.47)
Now substituting y = g−1
t
(x) with dx = |Dgt(y)|dy gives
(Dg−1
t
(·))∗Av0(g−1
t
(·))|Dg−1
t
(·)|
=
N

n=1
δ(gt(xn) −·)(Dgt,1(gt(xn)))∗|Dg−1
t
(gt(·))||Dgt(·)|(x′
n −g1(xn))
(a)
=
N

n=1
δ(gt(xn) −·)(Dgt,1(gt(xn)))∗(x′
n −g1(xn)) = Avt(·),
(11.48)
where (a) uses (Dgt(·))−1 = Dg−1
t
(gt(·)).
Example 11.10 (Beg Example)
To compute the landmark matching vector ﬁelds for
numericalstabilityFaisalBegcomputedthegradientdirectlyonthevectorﬁeldsrather
than the momentum. Initialize vold = 0, choose constant ǫ, then for all t ∈[0, 1],
Step1 :
∂
∂tgnew
t
= vold
t
(gnew
t
), ∂
∂tg−1new
t
= −Dg−1new
t
vold
t
, gnew
t,1
= gnew
1
(g−1new
t
),
Step2 : Compute vnew
t
= vold
t
−ǫ∇vEold
t
, set vold ←vnew,
(11.49)
return to Step1 where
∇vEold
t
(x) = vold
t
(x) −
N

n=1
K(gnew
t
(xn), x)(Dgnew
t,1 (gnew
t
(xn)))∗
× (x′
n −gnew
1
(xn)),
x ∈X
(11.50)
Figure 11.3 shows examples of the landmark solution from the Faisal Beg
algorithm a ﬁnite grid approximation to the gradient Algorithm 11.10. The tan-
gent space metric for the large deformation is deﬁned through the Laplacian A =
(diag(−∇2 + c id))2 with norm-square ∥v∥2
V = ⟨Av, v⟩2 and circulant boundary con-
ditions. The columns shows the grid deformations for the compression (column 1),
swivel (column 2), and circular rotation (column 3). The top and bottom rows show
two different values of landmark placement noise σ = 0.3 (top), σ = 1.0 (bottom); cir-
cles xn = ◦depict the template landmarks, stars x′n = ∗depict the target landmarks.
The line emanating from the template landmark is its trajectory to reach the corre-
sponding target landmark. Column 1 shows a scale out of 13 pixels which is matched
using four landmarks as shown to a larger ball of radius of 28 pixels. Column 2 shows

11.4 CONSERVATION OF MOMENTUM FOR DIFFEOMORPHISM
343
0
10
20
30
40
50
60
0
10
20
30
40
50
60
Landmark noise sigma=0.3
0
10
20
30
40
50
60
0
10
20
30
40
50
60
Landmark noise sigma=0.3
0
20
40
60
80
100
120
0
20
40
60
80
100
120
Landmark noise sigma=0.3
0
10
20
30
40
50
60
0
10
20
30
40
50
60
Landmark noise sigma=1.0
0
10
20
30
40
50
60
0
10
20
30
40
50
60
Landmark noise sigma=1.0
0
20
40
60
80
100
120
0
20
40
60
80
100
120
Landmark noise sigma=1.0
Figure 11.3 Panels show the diffeomorphisms applied to the regular coordinates generated from
the landmark metric matching; xn = ◦, x′n = ∗. Figure shows the scale (column 1), S-curve (column
2), and circular rotation (column 3) for σ = 0.3 (top row) and σ = 1.0 (bottom row). The line
emanating from the landmarks are the trajectory onto the corresponding landmark.
the S-curve with the 4 landmarks forcing the deformation in opposite directions. Col-
umn 3 shows a quarter C parametrized by 6 landmarks and transformed to a similarly
parameterized half-C. As the landmark placement noise gets smaller, the matching
tends to be more exact.
11.4.1 An ODE for Diffeomorphic Landmark Mapping
The landmark problem is very special; its solution is reduced to a series of N ordinary differential
equations [281,284].
Corollary 11.11
The landmarks are transported along the geodesic with the vector ﬁelds
given by
vt(·) =
N

n=1
K(gt(xn), ·)βnt,
(11.51)
where K is the Green’s kernel associated with A; deﬁning βnt
=
(Dgt,1(gt(xn)))∗
(x′n −g1(xn)), then
d
dtβnt + Dvt(gt(xn))∗βnt = 0
(11.52)
d
dtgt(xn) −vt(gt(xn)) = 0.
(11.53)

344
11 METRICS SPACES FOR THE INFINITE DIMENSIONAL DIFFEOMORPHISMS
Figure 11.4 Results of landmark mapping of surface geometries. Top row shows the face mapping;
bottom row shows the hippocampus mapping. Column 1 shows the starting template; column 6
shows the target surfaces. Columns 2–5 show examples along the geodesic generated by solving
the diffeomorphic mapping of the faces. Results taken from Vaillant [284].
0
5
10
15
20
25
0
1000
2000
3000
4000
5000
6000
Histogram of template to target distances
Distance in mm
Number of vertices
Before warping
After warping
Before warping
After warping
0
5
10
15
20
25
30
0
1000
2000
3000
4000
5000
6000
7000
8000
Histogram of template to target distances
Distance in mm
Number of vertices
Figure 11.5 Histograms of accuracy showing distance between template and target surface
geometries after landmark mapping. Results taken from Vaillant, PhD.
Proof
Since (Dgt,1(gt(xn)))∗
=
(Dg−1
t
(gt(xn)))∗Dg0,1(xn)∗
we have βnt
=
(Dg−1
t
(gt(xn)))∗βn0. Computing the derivative gives
d
dtβnt = d
dt(Dg−1
t
(gt(xn)))∗βn0.
(11.54)
Now to compute the derivative,
d
dtDg−1
t
(gt(xn)) = d
dt(Dgt(xn))−1 = −(Dgt(xn))−1 d
dtDgt(xn)(Dgt(xn))−1
(11.55)
= −(Dgt(xn))−1Dvt(gt(xn))Dgt(xn)(Dgt(xn))−1
(11.56)
= −Dg−1
t
(gt(xn))Dvt(gt(xn)).
(11.57)

11.4 CONSERVATION OF MOMENTUM FOR DIFFEOMORPHISM
345
Subtituting in Eqn. 11.54 gives
d
dtβnt = d
dt(Dg−1
t
(gt(xn)))∗βn0 = −Dvt(gt(xn))∗(Dg−1
t
(gt(xn)))∗βn0.
(11.58)
Example 11.12 (Landmark Mapping of Faces from Vaillant [284] )
Vaillant
has
implemented the ODE of Corollary 11.11. Shown in Figure 11.4 are examples from
Vaillant [284] demonstrating geodesic landmark mapping of faces. Column 1 shows
the template starting face. Columns 2–5 show examples along the geodesic match to
the target face. Column 6 shows the three target faces.
Shown in Figure 11.5 are histograms demonstrating the distances between tri-
angles on the surface geometry before and after landmark mapping. Notice that after
landmark mapping the manifolds are largely within 1–2 mm of each other.

12 M E T R I C S O N P H O T O M E T R I C A N D G E O M E T R I C
D E F O R M A B L E T E M P L AT E S
ABSTRACT
In this chapter the metric structure developed for geometric transformations is
extended to deﬁne a metric structure on the images which they act. Deﬁned as an orbit under
the ﬁnite and inﬁnite dimensional groups we establish the necessary properties for measuring
distance in the image orbit. We extend the results from not only geometric transformation
of the images but also to photometric variation in the images.
12.1 Metrics on Dense Deformable Templates: Geometric Groups
Acting on Images
Geometric shape is studied via transformations which are products of subgroups of diffeomor-
phisms G acting on the background spaces. To establish metrics on the space of images I, I′ ∈I,
we associate group elements g, h ∈G to them and then deﬁne metric distances in group G. This
induces the metric on the orbit I. Such metrics deﬁned on the matrix groups and diffeomorphisms
require properties of geodesic length on Riemannian manifolds.
Now examine two kinds of transformations, the ﬁrst accomodating geometric or background
shape change, the second involving the photometric or intensity value changes. The actions of the
diffeomorphic transformations (invertible differentiable mappings with differentiable inverse) are
introduced on the background space G : X →X. The group of diffeomorphisms divides the
ensemble of images into geometrically distinct orbits.
The second kind of transformation is associated with photometric or intensity transforma-
tions, acting on the image values themselves. Begin by deﬁning the basic geometric group acting
on the images. Throughout the next several chapters, the basic model studied represents the source
of imagery I as an orbit under groups of transformations of exemplar templates: G : Iα →I = GIα.
The model for the source is that the images I ∈I are orbits under actions of transformation g ∈G,
and depending on the speciﬁc problem the transformations can be translations, rotations, rigid
motions of translations and rotations, scale, and Cartesian products of them.
12.1.1 Group Actions on the Images
The transformation groups of diffeomorphisms (G, ◦) generate the orbit of images through the
group action deﬁned as follows. The diffeomorphisms will be assumed to act on the inﬁnite
background space X = Rd for the matrix groups, and a bounded subset X ⊂Rd for the inﬁnite
dimensional diffeomorphisms.
Deﬁnition 12.1
Deﬁne the set of images I on X the background space I : X →V with
V the value space.
The group deﬁnes a group action on the set of images as follows.
Theorem 12.2
Deﬁne  acting on the the images  : G × I →I according to
(g, ·) : I →g · I = I ◦g−1.
(12.1)
Then with the group operation composition of functions g ◦g′(·) = g(g′(·)) is a group
action dividing I into a set of disjoint orbits.
346

12.1 DENSE DEFORMABLE TEMPLATES
347
Proof
Clearly since (id, I) = I we must show the product property of the group
action, (g, (h, I)) = (g ◦h, I) which follows according to
(g, (h, I)) = (g, (I ◦h−1)) = (I ◦h−1) ◦g−1
(12.2)
= I ◦(g ◦h)−1 = (g ◦h, I).
(12.3)
Then since it is a group action it deﬁnes an equivalence relation I1 ∼I2 if ∃g ∈G
such that I1 = gI2, dividing the I into disjoint orbits.
We can now deﬁne the source of images as a deformable template.
Deﬁnition 12.3
The deformable template Iα corresponds to the orbit under the group G
of one selected and ﬁxed image, term it the template Iα ∈I, such that
Iα = GIα = {I ∈I : I = Iα ◦g−1, g ∈G}.
(12.4)
Corresponding to multiple orbits, not necessarily connected, the full model of the source
of images becomes I = ∪αIα.
12.1.2 Invariant Metric Distances
Examine metrics on the orbits of the group actions. For the case where the images are sufﬁciently
complex (devoid of symmetries) and in the same orbit with group actions which are of sufﬁ-
ciently low dimension, then there is a unique identiﬁcation between images, and group elements,
I = Iα ◦g ↔g. This corresponds to there being exactly one g ∈G such that gI = I′, or alter-
natively gI = I implies g = id. In the unique identiﬁability case, the metric between images is
determined by the metric in the group directly. Thus a natural construction of the metric on images
˜ρ : I × I →R+ via the metric on group elements ρ : G × G analogous to the Euclidean metric in
Rn is to deﬁne an origin and compare the distance between group elements required to carry the
images back to the origin. The template Iα plays the role of the origin, and the group elements play
the role of the vectors deﬁning the mapping of points in Rn back to the origin. Then the metric
takes the form
˜ρ(I, I′) =
inf
g,h∈G:gI=hI′=Iα
ρ(g, h) = ρ(g⋆, h⋆),
where g⋆I = h⋆I′ = Iα.
(12.5)
More generally, for high dimensional diffeomorphisms there will not be a unique identiﬁ-
cation of the image I to a single group element which generates it from another candidate in the
orbit. Since there will be in general many maps between I, I′ (since the stabilizer in the group can
always be used), the image distance will be a set distance. For this deﬁne the stabilizer of some
particular image or template; the stabilizer leaves invariant the template. The natural construction
of the metric simply includes the stabilizer.
Deﬁnition 12.4
Let I be the orbit under the action of the group G with group action g · I =
I ◦g−1. Deﬁne S ⊂G to be the stabilizing subgroup of the reference object or template
Iα ∈I:
S = {s ∈G : sIα = Iα}.
(12.6)
The natural construction of the metric includes the stabilizer giving a set distance since
inf
g,h∈G
gI=hI′=Iα
ρ(g, h) ≥
inf
g′∈[g]S ,h∈G:
gI=hI′=Iα
ρ(g′, h) ≥
inf
g′∈[g]S ,h′∈[h]S :
gI=hI′=Iα
ρ(g′, h′).
(12.7)

348
12 PHOTOMETRIC AND GEOMETRIC DEFORMABLE TEMPLATES
For the resulting function to be a metric it must extend to the orbits under the stabilizer
as a set distance. The condition for the metric to extend to the orbits is that it is invariant to the
stabilizer (see Theorem below).
Theorem 12.5
Let I be the orbit under the action of the group G with group action g · I =
I ◦g−1.
Let ρ(·, ·) be a distance on G which is invariant by the left action of the stabilizer S so
that for all s ∈S, g, g′ ∈G , ρ(sg, sg′) = ρ(g, g′). The function ˜ρ : I × I →R+ deﬁned by
˜ρ(I, I′) =
inf
g′∈[g]S ,h′∈[h]S :
gI=hI′=Iα
ρ(g′, h′),
(12.8)
is a metric distance satisfying symmetry and the triangle inequality. If it attains the inﬁmum
for all I, I′ such that ˜ρ(I, I′) = 0 implies I = I′, then ˜ρ is a distance.
Proof
For the case where the stabilizer is trivially the identity, then the inﬁmum
reduces to the distance in the group which is a metric distance by deﬁnition of the
theorem. That ˜ρ is symmetric in I, I′ follows from the property that ρ is a metric on
the group G. To prove the triangular inequality let us show that for all I, I′, I′′,
˜ρ(I, I′′) ≤˜ρ(I, I′) + ˜ρ(I′, I′′).
(12.9)
Deﬁne Iα = gI = hI′ = kI′′ then for all g, h, k ∈G, s ∈S, then
ρ(g, sk)
(a)
≤ρ(g, h) + ρ(h, sk)
(12.10)
(b)
= ρ(g, h) + ρ(s−1h, k),
(12.11)
with (a) following from the triangle inequality property of ρ(·, ·) and (b) from its left
invariance. Now choose g⋆, h⋆, h⋆⋆, k⋆to satisfy the inﬁmums:
g⋆, h⋆=
arg inf
g′∈[g]S,h′∈[h]S
ρ(g′, h′)
(12.12)
(h⋆⋆, k⋆) =
arg inf
h′∈[h]S,k′∈[k]S
ρ(h′, k′).
(12.13)
There exists an s⋆∈S such that s∗−1h⋆= h⋆⋆giving
˜ρ(I, I′) + ˜ρ(I′, I′′) = ρ(g⋆, h⋆) + ρ(h⋆⋆, k⋆)
= ρ(g⋆, h⋆) + ρ(s⋆−1h⋆, k⋆)
(12.14)
(a)
≥ρ(g⋆, s⋆k⋆)
(12.15)
(b)
≥˜ρ(I, I′′),
(12.16)
with (a) following from Eqn. 12.11, and (b) following from the inﬁmum property of
the distance.
For image matching we shall extensively use a metric which satisﬁes a stronger condition
in which the metric is left invariant to the entire group (not just the stabilizer). Then, any point in
the metric space can act as the template. This is analogous to the Euclidean setting in which any
point in Rn can play the role of 0; this is because the metric is shift invariant by a rigid motion.
Corollary 12.6
Let I be the orbit under the action of the group G with group action g · I =
I ◦g−1. If the more restrictive condition holds that the distance is left invariant to the entire
group G, then all templates are equivalent, and
˜ρ(I, I′) =
inf
g∈G :gI′=I ρ(id, g).
(12.17)

12.1 METRICS ON DENSE DEFORMABLE TEMPLATES
349
Proof
Under the more general condition of left invariance to the group, then let
gI = Iα for some g ∈G, then for all g′ ∈[g]S,
inf
h′∈[h]S
ρ(g′, h′) =
inf
h′∈[h]S
ρ(id, g−1h′)
which implies
inf
g′∈[g]S,h′∈[h]S
ρ(g′, h′) =
inf
g∈G,h′∈[h]S
ρ(id, g−1h′).
(12.18)
This gives
˜ρ(I, I′) =
inf
g′∈[g]S ,h′∈[h]S :
gI=hI′=Iα
ρ(g′, h′) =
inf
g∈G,h′∈[h]S :
gI=hI′=Iα
ρ(id, g−1h′)
(12.19)
=
inf
g∈G:gI′=I ρ(id, g).
(12.20)
12.2 The Diffeomorphism Metric for the Image Orbit
The natural extension for calculating metric length between the images is to consider the shortest
paths gt ∈G, t ∈[0, 1], connecting the group elements which match one image to another via
the template with the tangent element of the evolution (∂gt/∂t)(g−1
t
). Then we have the following
invariance properties for inducing metrics on the images in the orbit.
Theorem 12.7
Given the group of diffeomorphism G(V) with norm ∥· ∥V and vector ﬁelds
having ﬁnite energy V = {v :
 1
0 ∥vt∥2
V dt < ∞}. Deﬁning ρ : G × G →R+ to be
ρ2(g, h) =
inf
v: g0=g
˙g=v(g)g1=h
 1
0
::::
∂gt
∂t

g−1
t
 ::::
2
V
= ∥vt∥2
V

dt,
(12.21)
then ρ(·, ·) is left invariant to G(V); for all h ∈G, ρ(h, hg) = ρ(id, g). Deﬁning I to be the
orbit with respect to the group with ˜ρ : I × I →R+ given by
˜ρ(I, I′) =
inf
g,h∈G:
gI=hI′=Iα
ρ(g, h) =
inf
g∈G:gI′=I ρ(id, g),
(12.22)
then ˜ρ is symmetric and satisﬁes the triangle inequality. If it attains the inﬁmum for all I, I′
such that ˜ρ(I, I′) = 0 implies I = I′, then ˜ρ is a distance on I.
Proof
Eqn. 12.22 follows from the substitution x = g−1
t
(y) in (∂gt/∂t)(x) = vt(gt(x)).
The essential property which ρ posesses is that it is not a function of where in the
group the elements are, so translation by another group element leaves the distance
unchanged; it is left invariant. If g satisﬁes (∂gt/∂t) = vt(gt), g0 = id, g1 = g, then left
action by h ∈G simply shifts the solution. That is g0 ◦h = h, g1 ◦h = g(h), and has
identical velocity:
∂gt ◦h
∂t
(x) = ∂gt
∂t (h(x))
= vt(gt(h(x))),
g0 = h, g1 = g(h).
Clearly the distance ρ(id, g) = ρ(h, hg) are the same distance giving
 1
0 ∥vt∥2
V dt.
The left invariance property of the metric implies all elements are equally good templates.
Any element in the orbit can be taken as the template.

350
12 PHOTOMETRIC AND GEOMETRIC DEFORMABLE TEMPLATES
12.3 Normal Momentum Motion for Geodesic Connection Via
Inexact Matching
In Chapter 11, we have examined the shortest path geodesic ﬂows of diffeomorphisms corre-
sponding to the Euler equation. As well in Section 11.3 we examined the variational problem in
the tangent space of the group minimizing with respect to the vector ﬁelds generating the shortest
path ﬂows.
As we have seen, along the geodesic, the momentum is determined by the momentum at
the identity:
Avt = (Dg−1
t
)∗Av0(g−1
t
)|Dg−1
t
|.
(12.23)
The natural question becomes, where does the initial momentum Av0 specifying the geodesic
connection of one shape to another come from? Naturally, it arises by solving the variational
problem of shooting one object onto another, or inexact matching via geodesic connection. In this
setting we do not require an exact boundary condition for g1 implying that the the perturbation
in η satisﬁes only the t = 0 boundary conditions and at the boundary of the background space
x ∈∂X. The inexact correspondence associated with the boundary conditions are depicted in panel
2 of Figure 10.1 of Chapter 10. Notice how at the endpoint only approximate correspondence is
enforced. For this we deﬁne an endpoint correspondence condition corresponding to the squared-
error mismatch between the mapped exemplar and the image according to C = ∥I′ −I ◦g−1
1 ∥2
2.
The inexact matching corresponds to minimizing this distance function.
To calculate the optimizing initial momentum for shooting one object to another we work
in direct perturbation of the vector ﬁeld v →v + ǫψ with a zero boundary ψ(∂X) = 0, with the
total boundary conditions taking the form
η0(·) = 0,
η1(∂X) = 0,
ψ(∂X) = 0.
(12.24)
Now we calculate the momentum at the origin solving the inexact matching problem
following [285]. We shall require the perturbation of the inverse map with the perturbation of
the ﬂow.
Lemma 12.8
The perturbation via η of the group element gt →gt(ǫ) = (id + ǫηt) ◦gt +
o(ǫ) = gt + ǫηt(gt) + o(ǫ) generates the variation of the inverse map g−1 according to
∂ηg−1
t
= ∂
∂ǫ (gt + ǫηt(gt))−1
|ǫ=0 = −Dg−1
t
ηt.
(12.25)
The perturbation of the vector ﬁeld vt →vt+ǫψt generates a perturbation of the inverse
group element g−1
t
→g−1
t
(ǫ) = g−1
t
+ ǫ∂ψg−1
t
+ o(ǫ) where
∂ψg−1
t
= ∂
∂ǫ (gv+ǫψ−1
t
)|ǫ=0 = −
 t
0
(Dgu(g−1
t
))−1ψu(gt,u)du.
(12.26)

12.3 NORMAL MOMENTUM MOTION FOR GEODESIC CONNECTION
351
Proof
Part 1: The Lemma follows from
id = (gt + ǫηt(gt) + o(ǫ))((gt + ǫηt(gt) + o(ǫ))−1)
(12.27)
= (gt + ǫηt(gt) + o(ǫ))(g−1
t
+ ǫ∂ηg−1
t
+ o(ǫ))
(12.28)
= id + ǫDgt(g−1
t
)∂ηg−1
t
+ ǫηt + o(ǫ),
(12.29)
yielding Eqn. 12.25.
Part 2: To prove the inverse perturbation, use the identity gv−1
t
(gv
t ) = id giving
id =

gv+ǫψ
t
−1
(gv+ǫψ
t
) = g−1
t
(gt + ǫηt(gt) + o(ǫ)) + ǫ∂ψg−1
t
(gt) + o(ǫ)
(12.30)
= id + ǫDg−1
t
(gt)ηt(gt) + ǫ∂ψg−1
t
(gt) + o(ǫ).
(12.31)
Substituting for ηt(gt) from Eqn. 11.17 gives
∂ψg−1
t
(gt) = −Dg−1
t
(gt)ηt(gt) = −Dg−1
t
(gt)Dgt
 t
0
(Dgu)−1ψu(gu)du
(12.32)
(a)
=
 t
0
(Dgu)−1ψu(gu)du,
(12.33)
(a) from the inverse theorem Dg−1
t
(gt)
=
(Dgt(g−1
t
◦gt))−1
=
(Dgt)−1.
Substituting g−1
t
(y) = x gives
∂ψg−1
t
= −
 t
0
(Dgu(g−1
t
))−1ψu(gu ◦g−1
t
)du.
(12.34)
Theorem 12.9 (Geodesic Connection for Inexact Dense Matching)
Given
distance
C = ∥I′ −I ◦g−1
1 ∥2
2, with the template I smooth so that ∇(I ◦g) = (Dg)∗∇I(g), and
gt,u = gu ◦g−1
t
, with div (Av ⊗v) = (DAv)v + (div v)Av. Assume the observed time-point
data I′ with the matching functional energy given by
E(vg) =
 1
0
∥vt∥2
Vdt + ∥I′ −I(g−1
1 )∥2
2.
(12.35)
1. The minimizer with respect to perturbations gt →g(ǫ) = gt + ǫηt(gt) + o(ǫ) satisﬁes
the Euler equations
∂Avt
∂t
+ (Dvt)∗Avt + div(Avt ⊗vt) = 0,
(12.36)
Av1 + (I′ −I(g−1
1 ))∇(I ◦g−1
1 ) = 0.
(12.37)
2. The momentum of the variational minimizer of the geodesic at the identity is
Av0 = −(I′(g1) −I)|Dg1|∇I;
(12.38)

352
12 PHOTOMETRIC AND GEOMETRIC DEFORMABLE TEMPLATES
along the geodesic the momentum satisﬁes Avt = (Dg−1
t
)∗Av0(g−1
t
)|Dg−1
t
| given by
Avt = −∇(I ◦g−1
t
)|Dgt,1|(I′(gt,1) −I(g−1
t
)).
(12.39)
Proof
Part 1: To compute the variation via perturbations g →g + ǫη(g) the variation of
the second inexact matching term is given by the variation of the inverse of Eqn. 12.25
of Lemma 12.8 according to
−2⟨(I′ −I(g−1
1 ))∇I(g−1
t
), ∂ηg−1
1 ⟩2 = 2⟨(I′ −I(g−1
1 ))∇I(g−1
1 ), Dg−1
1 η1⟩2
(12.40)
= 2⟨(I′ −I(g−1
1 ))∇(I(g−1
1 )), η1⟩2.
(12.41)
Computing the entire variation gives
2
 1
0
⟨Avt, ∂ηvt⟩2 dt + ⟨(I′ −I(g−1
1 ))∇(I ◦g−1
1 ), η1⟩2
(a)
= 2
 1
0

−∂Avt
∂t
−(Dvt)∗Avt −div(Avt ⊗vt), ηt

2
dt
+ 2⟨Av1, η1⟩2 + ⟨(I′ −I(g−1
1 ))∇(I ◦g−1
1 ), η1⟩2.
(12.42)
The ﬁrst term in Eqn. 12.42 is the Euler equation of Theorem 11.6, giving the variation
over the interior t ∈[0, 1) giving the Euler Eqn. 12.36. The second term is the free
boundary term at t = 1 from the integration by parts from the perturbation η1 from
Eqn. 11.24 of Theorem 11.6 of Chapter 11. The boundary term is Eqn. 12.37 completing
Part 1 of the proof.
Part 2: The variation with respect to perturbations of v →v + ǫψ of the ﬁrst term
∥vt∥2
V gives 2Avt. The second term requires the vector ﬁeld perturbation of the inverse
∂ψg−1
1
given by Lemma 12.8, Eqn. 12.26 according to
∂ψg−1
1
= −
 1
0
(Dgt(g−1
1 ))−1ψt(gt ◦g−1
1 )dt
= −
 1
0
Dg−1
1 D(gt ◦g−1
1 )−1ψt(gt ◦g−1
1 )dt.
(12.43)
Now using the substitution g1,t = gt◦g−1
1 , the variation of the endpoint term becomes
−2⟨(I′ −I(g−1
1 ))∇I(g−1
1 ), ∂ψg−1
1 ⟩2
= 2⟨(I′ −I(g−1
1 ))∇I(g−1
1 ),
 1
0
Dg−1
1 D(gt ◦g−1
1 )−1ψt(g1,t)⟩2 dt
= 2⟨(I′ −I(g−1
1 ))Dg−1∗
1
∇I(g−1
1 ),
 1
0
D(gt ◦g−1
1 )−1ψt(g1,t)⟩2 dt
= 2⟨(I′ −I(g−1
1 ))∇(I ◦g−1
1 ),
 1
0
D(gt ◦g−1
1 )−1ψt(g1,t)⟩2 dt
(a)
= 2
 1
0
⟨(I′(gt,1) −I(g−1
t
))∇(I ◦g−1
t
), ψt|D(gt,1)|⟩2 dt
= 2
 1
0
⟨∇(I ◦g−1
t
)|Dgt,1|(I′(gt,1) −I(g−1
t
)), ψt⟩2 dt
(12.44)
where (a) follows from the change of variable y = g1,t(x) in the inner product.

12.3 NORMAL MOMENTUM MOTION FOR GEODESIC CONNECTION
353
Combining this with the ﬁrst variation term 2Avt gives the momentum on the
geodesic Eqn. 12.39.
Example 12.10 (Geodesic Shooting(Beg,Trouve,Vaillant,Younes))
Shown
in
Figure 12.1 are results from using the initial momentum at the identity Av0 to generate
objects via shooting. Row 1 shows the starting objects before shooting. Row 2 shows
the density of momentum at the identity Av0 = (I′(g1) −I)|Dg1|∥∇I∥from Eqn. 12.38
as a function of position in the grid matching each image to it’s target. Row 3 shows
the image transported under the diffeomorphism I(g−1
1 ) generated via shooting the
initial momentum according to ˙gt = vt(gt), g0 = id. The density was computed using
Faisal Beg’s algorithm for solving the Euler equation for geodesic correspondence of
the shapes to derive Av0 (see Algorithm 16.3 in Chapter 16). Notice the density of the
momentum is concentrated on the boundaries of the shape.
Figure 12.2 shows three objects studied to demonstrate the momentum at the identity gen-
erated via the Beg algorithm for the smooth Gaussian bump for shift, circles for scale, and two
mitochondria with both forward and inverse mapping. Shown are comparisons between the
momentum at the identity Av0 and the gradient of the image ∇I. Column 1 shows the results
(1)
(2)
(3)
(4)
(5)
(6)
Figure 12.1 Row 1 shows objects for translation via diagonal translation (panels 1,2), scale (pan-
els 3,4), and mitochondria (panels 5,6). Row 2 shows the density of momentum at the identity
Av0 =(I′(g1)−I)|Dg1|∥∇I∥selected to match object 1 to object 2 constructed via the Faisal Beg large
deformation diffeomorphic metric mapping algorithm (see Chapter 16) to satisfy Eqn. 12.38. Row
3 shows the image transported I ◦g−1
1
by integrating the vector ﬁelds from the momentum at the
identity along the ﬂow ˙gt = vt(gt), g0 = id.

354
12 PHOTOMETRIC AND GEOMETRIC DEFORMABLE TEMPLATES
Figure 12.2 Experiments comparing momentum for translation (column 1), scale (column 2), and
mitochondria (column 3). Row 1 shows comparison between momentum at the identity Av0 gen-
erated by the Beg large deformation diffeomorphic metric mapping algorithm (see Chapter 16)
and the gradient of the image ∇I; arrows depict the direction vector of each. Row 2 shows the
vector ﬁelds v0 at the identity corresponding to the momentum Av0.
for diagonal translation, column 2 shows scale, and column 3 shows the mitochondria mapping.
Shown in row 1 at every point in the grid are the arrows depicting the direction vector of the
momentum Av0 of each point in the grid given by the geodesic solution. Superimposed is a sec-
ond arrow showing the normal level set gradient of the image. Notice how they superimpose
almost exactly at every point; for the geodesic the momentum motion is normal to the level sets
of the image as given by the conservation law. Row 2 shows the vector ﬁelds given by the Green’s
kernel of the metric operating on the momentum.
12.4 Normal Momentum Motion for Temporal Sequences
Let us now address another one of the fundamental models in which the hidden diffeomorphism
sequence gt, t ∈[0, 1] corresponds to carrying the exemplar through a time sequence of observables
I′
t, t ∈[0, 1]. Here the diffeomorphism is selected to minimize the matching function indexed
throughout time according to
C =
 1
0
∥I′
t −I ◦g−1
t
∥2
2 dt.
(12.45)
Assume throughout that (V, ∥· ∥V) is a reproducing kernel Hilbert space with operator K.
Theorem 12.11 (Normal Momentum Motion for Growth Connection of Dense Imagery)
Given is the growth sequence I′
t, t ∈[0, 1] and distance C =
 1
0 ∥I′
t −I(g−1
t
)∥2
2 dt, with the

12.4 NORMAL MOMENTUM MOTION FOR TEMPORAL SEQUENCES
355
template I continuously differentiable with ∇(I ◦g) = (Dg)∗∇I(g), and gt,u = gu ◦g−1
t
with
growth sequence energy
 1
0
∥vt∥2
V dt +
 1
0
∥I′
t −I(g−1
t
)∥2
2 dt.
(12.46)
1. The Euler-equation associated with the minimizer with respect to variations gt →
gt(ǫ) = gt + ǫηt(gt) + o(ǫ) satisﬁes for all t ∈[0, 1],
∂Avt
∂t
+ (Dvt)∗Avt + div(Avt ⊗vt) = −(I′
t −I(g−1
t
))∇(I ◦g−1
t
).
(12.47)
2. The momentum of the variational minimizer of the geodesic at the identity is
Av0 = −∇I
 1
0
(I′
u(gu) −I)|Dgu|du = 0 ;
(12.48)
along the geodesic the momentum satisﬁes
Avt = −∇(I ◦g−1
t
)
 1
t
(I′
u(gt,u) −I(g−1
t
))|Dgt,u|du = 0.
(12.49)
Proof
Part 1: The optimizer follows via perturbations gt →gt + ǫηt(gt). The perturbation
of the ﬁrst term

∥vt∥2
Vdt gives the Euler equation. The second term for inexact
matching follows using the Lemma 12.8, Eqn. 12.25 of Chapter 12:
δηC = −2
 1
0
⟨(I′
t −I(g−1
t
))∇I(g−1
t
), ∂η(g)g−1
t
⟩2dt
= 2
 1
0
⟨(I′
t −It(g−1
t
))∇I(g−1
t
), Dg−1
t
ηt⟩2dt
(a)
= 2
 1
0
⟨(I′
t −I(g−1
t
))∇(I ◦g−1
t
), ηt⟩2dt,
(12.50)
with (a) writing ∇(I ◦g−1
t
) = (Dg−1
t
)∗∇I(g−1
t
). Collecting the two components
of the gradient, the ﬁrst corresponding to the Euler equation and the second
Eqn. 12.50 gives the proof of Eqn. 12.47.
Part 2: The L2-norm variation via perturbations of v →v + ǫψ of the ﬁrst term ∥vt∥2
V
gives 2Avt. The second term requires the vector ﬁeld perturbation of the inverse
∂ψg−1
u ; from Lemma 12.8, Eqn. 12.26 gives
δψC = −2
 1
0
⟨(I′
u −I(g−1
u ))∇I(g−1
u ), ∂ψg−1
u ⟩2 du
(12.51)
(a)
= 2
 1
0
⟨(I′
u −I(g−1
u ))∇(I(g−1
u )),
 u
0
(Dgu,t)−1ψt(gu,t)dt⟩2 du
(b)
= 2
 1
0
 u
0
⟨(I′
u(gt,u) −I(g−1
t
))∇(I ◦g−1
t
), ψt|Dgt,u|⟩2 dt du
(c)
= 2
 1
0
 1
t
⟨|Dgt,u|∇(I ◦g−1
t
)(I′
u(gt,u) −I(g−1
t
)), ψt⟩2 du dt,
(12.52)

356
12 PHOTOMETRIC AND GEOMETRIC DEFORMABLE TEMPLATES
with (a) following from ∇(I ◦g−1
u ) = (Dg−1
u )∗∇I(g−1
u ), (b) follows by making the
substitution z = gt(g−1
u (y)) so that y = gu(g−1
t
(z)) giving dy = |Dgu(g−1
t
(z))|dz,
g−1
t
(z) = g−1
u (y), D(gt(g−1
u (y))) = Dz = id, (c) interchanging integrals in u and t
and setting gu(g−1
t
) = gt,u to give the gradient.
12.5 Metric Distances Between Orbits Deﬁned Through
Invariance of the Metric
Fundamental to understanding are invariances. The representation of objects as orbits with respect
to particular groups representing the invariances is therefore fundamental.
The construction of metric distances between such orbits becomes essential. For this deﬁne
O ⊂G to be the subgroup deﬁning the orbits. Then deﬁning the notion of distances between orbits
under O of group elements g, g′ ∈G is natural. The distance should be the set distance deﬁned
by the minimum between elements in the orbits. Then a sufﬁcient condition for the distance to
extend to the orbits is left invariance.
Theorem 12.12
Let O ⊂G be the group deﬁning the orbits, [g]O in G/O, with ρ a distance
on G which is invariant by the left action of O ⊂G so that for all o ∈O, g, g′ ∈G ,
ρ(og, og′) = ρ(g, g′).
(12.53)
Then the function ˜ρ : G/O × G/O →R+ deﬁned by
˜ρ([g]O, [h]O) =
inf
g′∈[g]O,h′∈[h]O
ρ(g′, h′),
(12.54)
is a metric distance on the orbits satisfying symmetry and the triangle inequality.
Proof
For the case where O is trivially the identity, then the inﬁmum reduces to the
distance in the group which is a metric distance by deﬁnition of the theorem. More
generally, that ˜ρ satisﬁes ˜ρ([g]O, [g′]O) = 0 implies [g]O = [g′]O and is symmetric
in [g]O, [g′]O follows from the property that ρ is a metric on the group and attains
its inﬁmum in the group. To prove the triangular inequality let’s show that for all
[g]O, [h]O, [k]O,
˜ρ([g]O, [k]O) ≤˜ρ([g]O, [h]O) + ˜ρ([h]O, [k]O).
(12.55)
For all g, h, k ∈G, o ∈O, then
ρ(g, ok)
(a)
≤ρ(g, h) + ρ(h, ok)
(12.56)
(b)
= ρ(g, h) + ρ(o−1h, k),
(12.57)
with (a) following from the triangle inequality property of ρ(·, ·) and (b) from its left
invariance. Now choose g⋆, o⋆, h⋆, k⋆to satisfy the inﬁmums:
g⋆, h⋆=
arg inf
g′∈[g]O,h′∈[h]O
ρ(g′, h′)
(12.58)
(o⋆, k⋆) =
arg inf
o∈O,k′∈[k]O
ρ(o−1h⋆, k′).
(12.59)

12.5 METRIC DISTANCES BETWEEN ORBITS
357
This gives
˜ρ([g]O, [h]O) + ˜ρ([h]O, [k]O) = ρ(g⋆, h⋆) + ρ(o⋆−1h⋆, k⋆)
(12.60)
≥ρ(g⋆, o⋆k⋆)
(12.61)
≥˜ρ([g]O, [k]O).
(12.62)
12.6 Finite Dimensional Landmarked Shape Spaces
In the deformable template models, variation in the image space I are accomodated by introducing
groups of transformations carrying individual elements from one to another. Thus far we have
examined dense images I with elements I ∈I which are deﬁned as complete functions on the back-
ground space I : X ⊂Rd →value space. Examine the special class of sparse images corresponding
to ﬁnite dimensional landmark shapes in which the images are a collection of points or landmarks.
Because such points give direct information about the objects shape they will be termed N-shapes.
The study of such class of images have been extensively pioneered by Bookstein [223, 224, 272].
Fix an integer N > 0 and denote by IN the set of all collections of landmarks parametrizing the
N-shapes IN ∈IN through the points
IN = (x1, . . . , xN),
I′
N = (x′
1, . . . , x′
N).
(12.63)
Then the space of N-shapes IN = RdN has metric ρ : IN × IN →R+ deﬁned by the lengths
the paths the points travel to correspond. Length is measured through the geodesic curves gt ∈
RdN, t ∈[0, 1], with Q(gt) a positive deﬁnite symmetric matrix deﬁning the quadratic form in the
tangent space (Tgt(RdN), ∥· ∥RdN ).
Deﬁnition 12.13
Deﬁne the manifold of N-shapes IN ⊂RdN with metric distance
ρ : IN × IN →R+ between shapes deﬁned via the Nd × Nd positive deﬁnite matrices
Q(gt) = (Q(gt)ij) with d × d blocks Q(gt)ij giving the metric
ρ2(IN, I′
N) =
inf
˙gt(xn): g0(xn)=xn
n=1,...,Ng1(xn)=x′n
 1
0

∥∂gt
∂t (g−1
t
)∥2
Q =
N

ij=1
˙gt(xi)∗Q(gt)ij ˙gt(xj)

dt.
(12.64)
12.6.1 The Euclidean Metric
The distance metric between N-shapes is determined by the quadratic form giving length to the
tangent element as the particles ﬂow from one shape to another. Choosing the quadratic form to
correspond to a constant independent of the paths gives the straight lines and Euclidean distance
∥· ∥Rd. Here is a relatively simple example from landmark matching illustrating left invariance of
the Euclidean metric.
Theorem 12.14
Given N-shapes IN = x1, . . . , xN, I′
N = x′
1, . . . x′
N, with curves gt ∈RNd
choose the norm-square in the tangent space to be constant Q(gt) = Q, then the metric

358
12 PHOTOMETRIC AND GEOMETRIC DEFORMABLE TEMPLATES
ρ : IN × IN →R+ measures straight line paths
ρ2(IN, I′
N) = ∥IN −I′
N∥2
Q =
N

ij=1
(xi −x′
i)∗Qij(xj −x′
j) .
(12.65)
Deﬁning the quadratic form to be identity Q(gt) = id gives the Euclidean metric
ρ : IN × IN →R+ according to ρ(IN, I′
N) = 
N
n=1 ∥x′n −xn∥2
Rd.
Then, in R3 for example, ρ is left invariant under the rigid motions SE(3) so that
ρ(IN, I′
N) = ρ(OIN + a, OI′
N + a);
(12.66)
this deﬁnes a metric distance ˜ρ between orbits [IN]SE(3), [I′
N]SE(3) according to
˜ρ([IN]SE(3), [I′
N]SE(3)) =
inf
(O,a)∈SE(3),(O′,a′)∈SE(3) ρ(OIN + a, O′I′
N + a′)
(12.67)
=
inf
(O,a)∈SE(3) ρ(IN, OI′
N + a).
(12.68)
With centered points ˜xn = xn −¯x, ˜x′n = x′n −¯x′, and means ¯x = 1/N 
N
n=1 xn, ¯x′ =
1/N 
N
n=1 x′n, then
˜ρ2([IN]SE(3), [I′
N]SE(3)) =
N

n=1
∥˜xn∥2
R3 +
N

n=1
∥˜x′
n∥2
R3
−2
max
O∈SO(3) tr


N

n=1
˜x′
n˜x∗
nO∗

.
(12.69)
Proof
This is a ﬂat vector space, with straight line curves the geodesics as in
Theorem 10.5 of Chapter 10. The left invariance condition is satisﬁed since
∥IN −I′
N∥2
R3N =
N

n=1
∥xn −x′
n∥2
R3
=
N

n=1
∥Oxn + a −Ox′
n −a∥2
R3 = ρ(OIN + a, OI′
N + a).
(12.70)
The joint minimization gives
(O, a)MS =
arg min
O,O′∈SO(3),a,a′∈R3
ρ(OIN + a, O′I′
N + a′)
=
arg min
O∈SO(3),a∈R3
ρ(IN, OI′
N + a)
(12.71)
=
arg min
O∈SO(3),a∈R3
N

n=1
∥Ox′
n −xn + a∥2
R3.
(12.72)

12.6 FINITE DIMENSIONAL LANDMARKED SHAPE SPACES
359
Minimizing with respect to translation gives the MMSEs
aMS = arg min
a∈R3
N

n=1
∥xn −(Ox′
n + a)∥2
R3
(12.73)
= arg min
a∈R3
N

n=1
∥xn −Ox′
n∥2 −2
N

n=1
⟨a, xn −Ox′
n⟩R3 + N∥a∥2 = ¯x −O¯x′.
(12.74)
Substituting aMS and the centered points ˜xn, ˜x′n gives
OMS = arg min
O∈SO(3)
N

n=1
∥˜xn −O˜x′
n∥2
R3
= arg min
O∈SO(3)
N

n=1
∥˜xn∥2
R3 −2tr
N

n=1
˜xn˜x′∗
n O∗+
N

n=1
˜x′∗
n O∗O˜x′
n
(12.75)
= arg max
O∈SO(3)
tr AO∗,
where A =
N

n=1
˜xn˜x′∗
n .
(12.76)
12.6.2 Kendall’s Similitude Invariant Distance
Now expand the orbit invariance to the similitudes following Younes developments in [286].
Kendall [287] deﬁnes distance between sets of N-shapes which is invariant to uniform scale,
rotation, and translation. Deﬁne the afﬁne similitudes, the subgroup of the afﬁne group to be
matrices A = sO, s ∈(0, ∞), O ∈SO(d) with the action on the shapes to be scale-rotation and
translation of each point of the shape (AIN + a)∗= (Ax1 + a, . . . , AxN + a). Then for all (A, a) in
the similitudes the left invariant distance satisﬁes ˜ρ(AIN + a, AI′
N + a) = ˜ρ(IN, I′
N).
Theorem 12.15 (Kendall [287])
Deﬁne the mean shape ¯gt = 1/N 
N
n=1 gt(xn) and
variance
σ 2(gt) = 1
N
N

n=1
∥gt(xn) −¯gt∥2
Rd,
(12.77)
with the Nd × Nd diagonal matrix with varying weights Q(gt) = (1/σ 2(gt)) id, then the
metric becomes
ρ2(IN, I′
N) =
inf
˙g: g0(xn)=xn
g1(xn)=x′n
N

n=1
 1
0
1
σ 2(gt)∥dgt(xn)
dt
∥2
Rddt
(12.78)
= ρ2
0((σ(g0), ¯g0), (σ(g1), ¯g1)) +

arccos
N

n=1
g0(xn) −¯g0
σ(g0)
, g1(xn) −¯g1
σ(g1)

Rd


2
(12.79)

360
12 PHOTOMETRIC AND GEOMETRIC DEFORMABLE TEMPLATES
with ρ0 a metric on RNd × RNd.
Proof
Deﬁning the normalized landmarks γt(xn) = ((gt(xn) −¯gt)/σ(gt)), then
gt(xn) = σ(gt)γt(xn) + ¯gt,
dgt(xn)
dt
= dσ(gt)
dt
γt(xn) + σ(gt)dγt(xn)
dt
+ d¯g
dt .
(12.80)
This implies
N

n=1
 1
0
1
σ 2(gt)
::::
dgt(xn)
dt
::::Rd dt is given by
 1
0
1
σ 2(gt)
dσ(gt)
dt
2 N

n=1
∥γt(xn)∥2
Rd dt + N
 1
0
1
σ 2(gt)
::::
d¯g
dt
2::::Rd dt +
 1
0
N

n=1
::::
dγt(xn)
dt
::::
2
Rd dt
+ 2
 1
0
1
σ(gt)
dσ(gt)
dt
N

n=1

γt(xn), dγt(xn)
dt

Rd dt
+ 2
 1
0
1
σ 2(gt)

d¯gt
dt ,
N

n=1
γt(xn)

Rd
dt + 2
 1
0
1
σ(gt)

¯gt,
N

n=1
dγt(xn)
dt

Rd
dt
(a)
=
 1
0
1
σ 2(gt)
dσ(gt)
dt
2
dt + N
 1
0
1
σ 2(gt)
::::
d¯g
dt
2::::Rd dt
+
 1
0
N

n=1
::::
dγt(xn)
dt
::::
2
Rd dt,
(12.81)
with (a) following from ∥γt∥2
Rd
= 1 implying the 4th term (d∥γt∥2
Rd/dt) = 0,
the 5th term has mean zero, and the ﬁnal term is the derivative of the mean
which is zero. Denote by ρ0((σ(g0), ¯g0), (σ(g1), ¯g1))2 the minimum of the ﬁrst two
terms, then the minimum of the last term can be explicitly computed (because
(γ (x1), . . . , γ (xN)) belongs to a sphere of dimension N −2 and is given by the length
of the great circle in the sphere which connects the extremities of the path, namely

arccos 
N
n=1⟨γ0(xn), γ1(xn)⟩Rd
2
. This gives Eqn. 12.79.
Corollary 12.16
Kendall’s similitude metric is left-invariant ρ(AIN + a, AI′
N + a) =
ρ(IN, I′
N), implying the metric between orbits becomes
˜ρ(IN, I′
N) =
min
A similitude,a∈Rd ρ(AIN + a, I′
N).
(12.82)
Proof
Kendall’s distance ˜ρ in equation (12.82) requires computing the minimum of
ρ(sOIN + a, I′
N), for s > 0, O ∈SO(d) and a ∈Rd. Since the action of s and a does not
affect γ (xn), one can select them in order to cancel the distance ρ0 without changing the
second term implying that ˜ρ(IN, I′
N) is the minimum of arccos 
N
n=1⟨γ0(xn), γ1(xn)⟩Rd
for all O ∈SO(d). When d = 2, there is an explicit solution
˜ρ(IN, I′
N) = arccos

N

n=1
⟨γ0(xn), γ1(xn)⟩Rd

.
(12.83)

12.7 DIFFEOMORPHISM METRIC, SPLINES ON LANDMARK SHAPES
361
12.7 The Diffeomorphism Metric and Diffeomorphism Splines on
Landmark Shapes
Now change the metric on landmarked shapes to the diffeormorphism metric. This returns us to
the landmark splines of Section 11.4 examining the metric on the geodesics in which the diffeo-
morphisms are assumed known only on the subsets of the points deﬁned by the N-shape IN, I′
N.
The group action for landmarked shapes is the most straightforward.
Deﬁnition 12.17
Deﬁne the group action for N-shapes as
g : IN = (x1, x2, . . . , xN) →g · IN =

g(x1), g(x2), . . . , g(xN)
	
.
(12.84)
The metric for diffeomorphic correspondence deﬁned for dense correspondence reduces to
terms only involving ﬂows of curves of the N-correspondence points.
Theorem 12.18 (Diffeomorphism Metric for N-Shapes)
Assume (V, ∥·∥V) is a repro-
ducing kernel Hilbert space with Green’s operator K : L2(X, Rd) →V which deﬁnes the
Nd × Nd positive deﬁnite symmetric matrix
Q(gt) =

K(gt(xi), gt(xj))
−1
.
(12.85)
The metric distance between N-shapes of Eqn. 12.64 subject to the constraints g0(xn) =
xn, g1(xn) = x′n, n = 1, 2, . . . is given by
ρ2(IN, I′
N) =
inf
vnt: ˙gt(xn)=vnt
n=1,2,...
g0(xn)=xn,
g1(xn)=x′n
 1
0
N

i,j=1
v∗
itQ(gt)ijvjt dt.
(12.86)
Proof
From Theorem 11.9 and Corollary 11.11 of Chapter 11, the optimizing spline
vector ﬁeld is given by superposition of reproducing kernels
vt(x) =
N

n=1
K(gt(xn), x)βnt,
x ∈X.
(12.87)
The norm-square becomes
∥vt∥2
V =
 N

n=1
K(gt(xn), ·)βn, vt(·)

V
=
N

n=1
β∗
ntvt(gt(xn))
(12.88)
=
N

i,j=1
vt(gt(xi))∗Q(gt)ijvt(gt(xj)).
(12.89)
12.7.1 Small Deformation Splines
For small deformations, there is a lovely approximation of the diffeomorphism metric problem
which has been pioneered by Bookstein [223,224, 272]. This approximates the metric ﬂow through

362
12 PHOTOMETRIC AND GEOMETRIC DEFORMABLE TEMPLATES
the tangent ﬂow of the landmark trajectories at the origin. In general this is not a metric; it is not
symmetric nor does it satisfy the triangle inequality since it places a special role on the origin.
Corollary 12.19
Let the mapping v0 : x →x + v(x) with vector ﬁelds (V, ∥· ∥V) be
as in Theorem 12.18 the Nd × Nd positive deﬁnite symmetric matrix Q(g0) = Q(IN) =

K(xi, xj)
−1
, then the Bookstein measure m : IN × IN →R+ satisﬁes
m2(IN, I′
N) =
inf
v0: x′n=xn+v0(xn)
n=1,2,...
∥v0∥2
V =
N

ij=1
(xi −x′
i)∗Q(IN)ij(xj −x′
j).
(12.90)
Adding the afﬁne motions x →x′ = Ax + b + v0(x) then
m2(IN, I′
N) =
inf
v0,A,b:
Axn+b+v0(xn)=x′n
∥v0∥2
V = inf
A,b
N

ij=1
(Axi + b −x′
i)∗Q(IN)ij(Axj + b −x′
j).
(12.91)
For inexact matching minimizing
∥v0∥2
V + 1
σ 2
N

n=1
∥Axn + b + v0(xn) −yn∥2
Rd,
(12.92)
with Nd × Nd quadratic form matrix M =

K(xi, xj) + σ 2δ(i −j)
−1
the optimizers satisfy
v0(x) = −
N

n=1
K(xn, x)

N

m=1
Mnm(Axm + b −ym)

,
x ∈X,
(12.93)
with A, b = min
A,b
N

nm=1
(Axn + b −yn)∗Mnm(Axm + b −ym).
(12.94)
Proof
The proof is a direct result of the diffeomorphism spline proof Theorem 12.18,
only applied for the single time t = 0. The optimizer minimizing ∥v0∥2
V superposes
Green’s kernels v0(x) = 
n K(xn, ·)βn, giving
min
v0,A,b ∥v0∥2
V +
N

n=1
∥Axn + b + v0(xn) −yn∥2
Rd
σ 2
=

ij=1
B⋆
i K(xi, xj)βj +
N

n=1
∥Axn + b + v0(xn) −yn∥2
Rd
σ 2
.
Differentiating with respect to βk gives
0 = 2
N

n=1
K(xk, xn)βn + 2
σ 2
N

n=1
K(xk, xn)(Axn + b + v0(xn) −yn),
(12.95)

12.7 DIFFEOMORPHISM SPLINES ON LANDMARK SHAPES
363
which implies
N

n=1
K(xk, xn)βn+ 1
σ 2
N

n=1
K(xk, xn)
N

j=1
K(xn, xj)βj
= −1
σ 2
N

n=1
K(xk, xn)(Axn + b −yn).
(12.96)
Rewriting in matrix notation gives K =

K(xi, xj)

, M = (K + σ 2 I)−1 and
KM−1β = −K(Ax + b −y),
(12.97)
implying β = −M(Ax + b −y). Solving for the minimum in A, b gives
∥v0∥2
V + ∥Ax + b + v0 −y∥2
σ 2
(a)
= β∗Kβ + ∥Kβ −M−1β∥2
σ 2
= β∗M−1β
(12.98)
(b)
= (Ax + b −y)∗MM−1M(Ax + b −y).
(12.99)
Notice, the minimum norm is determined by the residual error after ﬁtting the landmarks with an
afﬁne motion.
Example 12.20 (Flows and Small Deformations)
Joshi [242] ﬁrst implemented the
original algorithm for large deformation landmark mapping exploiting the fact that
the vector ﬁelds are in the span of the Green’s kernels, comparing the large deforma-
tion diffeomorphism metric mapping with the small deformation approximation. The
metric was chosen to be ∥v∥2
V = ⟨(diag(−∇2 + c id))2v, v⟩2 for inducing the Green’s
kernel K(x, y) = diag

κe−

1
c ∥x−y∥Rd

, with κ chosen to make the Green’s kernel inte-
grate to 1 (see Section 9.4.4, Chapter 9 for calculated examples). Joshi’s algorithm [242]
implements via successive differences the spline equations reducing the problem to a
ﬁnite dimensional problem by deﬁning the ﬂows on the ﬁnite grid of ﬁxed times of
size δ, tk = kδ, k = 0, 1, . . . , K = 1/δ. Figure 12.3 shows results illustrating the ﬂow of
diffeomorphisms generated in the image plane gt ∈RNd, N = 6 landmarks, d = 2,
for the sparse correspondence of points. Column 1 shows the grid test pattern for the
correspondence of A →B, C →D while ﬁxing the corners of the grid (panel 1) along
with the paths of the ﬂow gt(xi) projected onto the plane (panel 4). Notice the required
twist of the grid. Column 2 compares the large deformation ﬂow (top panel) with
the small deformation solution (bottom panel) approximating the diffeomorphism
with the vector space solution. Panel 2 shows the metric mapping g1 applied to the
grid; panel 5 shows the small deformation applied to the grid. Column 3 shows the
determinant of the Jacobian map for the large deformation ﬂow (panel 3) and small
deformation (panel 6). The determinant of the Jacobian of the small deformation is
shown in black (negative Jacobian) to white color (positive Jacobian). The variances
used were constant σ 2 = 0.01.
Notice the geometric catastrophe which occurs for the curved transformation
which is required. The resulting transformation of the small deformation approxima-
tion results in the grid crossing over. The determinant is negative in the region where
the grid lines in the landmark deformation cross. The small deformations have no
ﬂexibility to create a curved trajectory by integrating the ﬂow.

364
12 PHOTOMETRIC AND GEOMETRIC DEFORMABLE TEMPLATES
A
B
D
C
(1)
(4)
(5)
(6)
(2)
(3)
Figure 12.3 Top row: Panel 1 shows the grid test pattern matching A →B, C →D with the corners
ﬁxed. Panel 2 shows the movement of the grid under the diffeomorphism g1; panel 3 shows the
determinant of the Jacobian |Dg1|. Bottom row: Panel 4 shows the trajectories of the particles
gt(xi), i = 1, 2, t ∈[0, 1] traced out by the landmark points A,C and the four corners of the image
projected into the plane. Panel 5 shows the small deformation mapping applied to the grid; panel
6 shows the determinant of the Jacobian |Dg1|. Black to white color scale means large negative to
large positive Jacobian. The variances were σ 2 = 0.01; mappings from Joshi [282] (see also Plate
17).
LANDMARKS ONLY TRANSFORMATION
(1)
(2)
(3)
(4)
(5)
(6)
(7)
(8)
Figure 12.4 Figure shows results of large deformation diffeomorphic landmark matching.
Row 1: Panels 1 and 2 show brains 87A and target 90C, panel 3 shows 87A matched to 90C trans-
formed via landmarks. Row 2: Panels 5, 6, 7, show sections through 87A, 90C, and 87A matched
to 90C, respectively. Panel 4 shows the difference image between 87A and 90C; panel 8 shows
difference image after landmark transformation. Mapping results taken from Johis [282]; data
taken from David Van Essen of Washington University (see also Plate 18).
Example 12.21 (Whole Brain Mapping via Landmarks)
Examine the whole maca-
que cryosection brains shown in Figure 12.4 in which the gyri and associated sulci
have been labeled. The sulci and gyri can be deﬁned precisely in terms of the geomet-
rical properties of extremal curves on the cortical surface using the notions of ridge
curves and crest lines (extremal points of curvature) as in Khaneja [153]. Joshi used
the fundus curves on the sulci and gyri to constrain the transformations across brains.
Figure 12.4shows entire macaque cryosection brains (5002 × 200 voxels). Panels 1 and

12.7 DIFFEOMORPHISM SPLINES ON LANDMARK SHAPES
365
2 show brains 87Aand 90C in which the nine major principle sulcus curves were iden-
tiﬁed in the brains and represented as piecewise linear curves with 16 nodes. The gyri
and associated sulci have been labeled following the nomenclature used in Felleman
and Van Essen [155]. The deformation ﬁeld mapping the template to the target was
constrained so that the corresponding points along the extremal curves were mapped
onto each other. Panel 3 of Figure 12.4 shows the landmark matching of 87A matched
to 90C using only the linear landmarks to deﬁne the transformation. Panels 5–7 show
corresponding sections through 87A (panel 5), 90C (panel 6), and the transformed
87A→90C (panel 7). Panel 4 shows the difference image between 87Aand 90C before
transformation; panel 8 shows the same after landmark transformation. Notice that
there is a large difference in the shape and positions of the major subvolumes (the
thalamus and the cortical folds) between the undeformed template and the target.
Notice the correspondence in the alignment of the major subvolumes (panel 8) after
the deformation.
12.8 The Deformable Template: Orbits of Photometric and
Geometric Variation
Thus far the metric depends only on the geometric transformation in the background space. Vari-
ability in objects is not solely associated with geometric transformation, but as well as other
factors such as lighting conditions, object surface properties and features, texture variations and
the operational state of the object. Extend the construction of the metric to be image dependent
following [286] by deﬁning the group action to operate on the geometry and image pair.
Now examine models of variation which explicitly construct metric spaces which model
photometric variation with geometric variation. The basic model of Figure 12.5 represents the
source of imagery I as the orbit of the product of all photometric variations with geometric vari-
ations. The space of photometric intensities are modeled as a Hilbert space H. The deformable
template becomes the orbits of all photometric intensities under geometric motions, A = H × G,
with elements the pairs (I, g) ∈A = H × G. Notice, in this setting there are no ﬁxed exemplars.
The resulting images which form the mean ﬁelds become I ◦g−1 ∈I, thus the image formation
process before the noise channel is a mapping (I, g) ∈A →I ◦g−1.
12.8.1 Metric Spaces for Photometric Variability
Nowexaminethemetricspaceofphotometricvariability. Figure12.6depictstheintroductionofthe
metric dependence on both geometry and photometric evolution. Each curve depicts a particular
object under geometric deformation. The metric distance on the product space of photometric and
Source
Identification channel
(I,g)      
I   g–1 
Noisy channel
Figure 12.5 The source of images are the pair of photometric intensities and geometric transfor-
mation (I, g) ∈A = H × G. The identiﬁcation generate the images I ◦g−1, g ∈G.

366
12 PHOTOMETRIC AND GEOMETRIC DEFORMABLE TEMPLATES
I
(g, I(g))
J
(g, J(g))
(h, J(h))
Figure 12.6 Shown is a comparison of objects I and J by looking for the smallest distance within
the set (id, I) ◦G and (id, J) ◦G.
geometry evolution must take into account both the distance between the geometric change as
well as the photometric change, depicted as labelled pairs acting on the right (h, J(h)) and (g, I(g)).
The invariance condition corresponds to parallel translation by the group acting as the right. If G
is parallel translation, then the metric on the elements extends to a metric on the orbits.
Theorem 12.22
Deﬁne the product set A = G × I, and deﬁne on it the (essentially a right
action) action G : A →A according to (g, I)h = (gh, I(h)). Let ρ be a distance on A which is
invariant by the action of G; for all h ∈G, (g, I), (g′, I′) ∈A,
ρ((g, I)h, (g′, I′)h) = ρ((g, I), (g′, I′)).
(12.100)
Then the function ˜ρ deﬁned on I × I by
˜ρ(I, I′) = inf
g,h∈G ρ((g, I(g)), (h, I′(h)))
(12.101)
= inf
g∈G ρ((id, I), (g, I′(g))),
(12.102)
is symmetric and satisﬁes the triangle inequality. If it attains the inﬁmum for all I, I′ such that
˜ρ(I, I′) = 0 implies I = I′, then ˜ρ is a distance.
Proof
Since ρ is invariant, Eqn. 12.102 follows from
inf
g,h∈G ρ((g, I(g)), (h, I′(h))) = inf
g,h∈G ρ((id, I), (hg−1, I′(hg−1))
(12.103)
= inf
g∈G ρ((id, I), (g, I′(g)).
(12.104)
The triangle inequality follows from the same proof as for Theorem 12.5 since ρ is
invariant to the entire group.
12.8.2 The Metrics Induced via Photometric and Geometric Flow
The natural extension for adding the calculation of photometric evolution is to consider shortest
paths at = (gt, It) ∈A = G(V) × I, t ∈[0, 1], connecting elements (g, I), (h, I′) ∈A with the tangent

12.8 THE DEFORMABLE TEMPLATE
367
element of the evolution
∂at
∂t (g−1
t
) =
∂gt
∂t (g−1
t
), ∂It
∂t (g−1
t
)

(12.105)
We shall study exclusively the superposition norm-square in the tangent space.
Deﬁnition 12.23
Deﬁne the superposition norm-square in the tangent space of photo-
metric and geometric variation, as
::::
∂at
∂t (g−1
t
)
::::
2
T(A)
=
::::
∂gt
∂t (g−1
t
)
::::
2
V
+
::::
∂It
∂t (g−1
t
)
::::
2
2
.
(12.106)
Deﬁning the norm in the tangent space of A of photometric and geometric variation as the
superposition of the two norms, then we have the following equivalence of the differential length
associated with the tangent space norm. Examine the connection of (g, I) to (h, I′).
Lemma 12.24
Deﬁning the geometric evolution vt = ∂gt/∂t(g−1
t
), the image evolution
Jt = It(g−1
t
), then the following energies are equal:
E =
 1
0
::::
∂gt
∂t (g−1
t
)
::::
2
V
+
::::
∂It
∂t (g−1
t
)
::::
2
2

dt,
with b.c. g0 = g, g1 = h
I0 = I, I1 = I′
(12.107)
=
 1
0

∥vt∥2
V +
::::
∂Jt
∂t + ∇J∗
t vt
::::
2
2

dt
with b.c. J0 = I(g−1), J1 = I′(h−1).
(12.108)
Proof
With the deﬁnition Jt = It(g−1
t
), then the boundary conditions imply with
g0 = g, g1 = h, J0 = I(g−1), J1 = I′(h−1), then we have
J0 = I0(g−1
0 ) = I(g−1),
J1 = I1(g−1
1 ) = I′(h−1).
(12.109)
Deﬁning It = Jt(gt) then
∂It
∂t = ∂
∂tJt(gt) = ∂Jt
∂t (gt) + ∇J∗
t (gt)∂gt
∂t .
(12.110)
Substituting g−1
t
gives
∂It
∂t (g−1
t
) = ∂Jt
∂t + ∇J∗
t vt,
(12.111)
implying the norms in the tangent space are identical:
::::
∂at
∂t (g−1
t
)
::::
2
T(A)
= ∥vt∥2
V +
::::
∂Jt
∂t + ∇J∗
t vt
::::
2
2
.
(12.112)
Then, beautifully enough, the metric measuring photometric transport involves the material
or total derivative of Jt and is left invariant.

368
12 PHOTOMETRIC AND GEOMETRIC DEFORMABLE TEMPLATES
Theorem 12.25
Deﬁning the geometric and photometric evolutions, in A = G(V) × I to
be vt = ∂gt/∂t(g−1
t
), Jt = It(g−1
t
), then the function
ρ2((g, I),(h, I′))
=
inf
g(·),I(·):g0=g,g1=h
I0=I,I1=I′
 1
0
::::
∂gt
∂t (g−1
t
)
::::
2
V
+
::::
∂It
∂t (g−1
t
)
::::
2
2

dt
(12.113)
=
inf
v,J:
J0=I(g−1),J1=I′(h−1)
 1
0

∥vt∥2
V +
::::
∂Jt
∂t + ∇J∗
t vt
::::
2
2

dt,
(12.114)
and ρ is invariant so that ρ((h, I(h)), (gh, I′(gh))) = ρ((id, I), (g, I′(g)).
The induced metric ˜ρ : I × I →R+ deﬁned as
˜ρ(I, I′) = inf
g∈G ρ((id, I), (g, I′(g)))
(12.115)
is a metric distance on I.
Proof
Eqn. 12.113 equalling Eqn. 12.114 follows is from Lemma 12.24. To prove
invariance, deﬁne ˜gt = gt(h), ˜v and we ﬁrst show ˜g satisﬁes the same O.D.E. as g just
shifted by h in initial condition with identical velocity ﬁelds v = ˜v:
∂˜gt
∂t (y) = ˜vt(˜gt(y))
(12.116)
= ∂gt
∂t (h(y)) = vt(gt(h(y)))
(12.117)
= vt(˜gt(y)), implying v(·) = ˜v(·).
(12.118)
Similarly deﬁning ˜It = It(h) = Jt(gt(h)), then it follows that ˜I and I satisfy the
same differential equation:
∂˜It
∂t (y) = ∂Jt
∂t (˜gt(y)) + ∇J∗
t (˜gt(y))∂˜gt
∂t (y),
(12.119)
∂˜It
∂t (˜g−1
t
(x)) (a)
= ∂Jt
∂t (x) + ∇J∗
t (x)∂˜gt
∂t (˜g−1
t
(x)),
(12.120)
= ∂Jt
∂t (x) + ∇J∗
t (x)˜vt(x)
(12.121)
= ∂Jt
∂t (x) + ∇J∗
t (x)˜vt(x) (b)
= ∂It
∂t (g−1
t
(x)),
(12.122)
with (a) following from the substitution ˜g−1
t
(x) for y, and (b) from v = ˜v. Thus the
energies of both solutions (g, I), (˜g, ˜I) are identical:
 1
0

∥vt∥2
V +
::::
∂It
∂t (g−1
t
)
::::
2
2

dt =
 1
0

∥˜vt∥2
V +
::::
∂˜It
∂t (˜g−1
t
)
::::
2
2

dt,
(12.123)
completing the second part of the proof.
From Theorem 12.22, Eqn. 12.115 satisfying triangle inequality and therefore
giving a metric requires the left invariance of ρ which has just been shown.

12.9 THE EULER EQUATIONS FOR PHOTOMETRIC AND GEOMETRIC VARIATION 369
12.9 The Euler Equations for Photometric and Geometric Variation
The Euler equation for the photometric and geometric matching now follows from Younes.
Theorem 12.26
The geodesics minimizing distance in Eqn. 12.114 of Theorem 12.25 with
∥v∥2
V = ⟨Av, v⟩2 satisﬁes the Euler equation with respect to variations in the ﬂow gives
∂Avt
∂t
+ (Dvt)∗Avt + div(Avt ⊗vt) = −
∂Jt
∂t + ∇J∗
t vt

∇
∂Jt
∂t + ∇J∗
t vt

,
(12.124)
where div(Av⊗v) = (DAv)v+(div v)Av. The variation with respect to the tangent elements
Avt +
∂Jt
∂t + ∇J∗
t vt

∇Jt = 0,
(12.125)
∂
∂t
∂Jt
∂t + ∇J∗
t vt

+ div
∂Jt
∂t vt + (∇J∗
t vt)vt

= 0.
(12.126)
Proof
First we will derive the variation with respect to (v, J) via perturbation
v →v + ǫψ, J →J + ǫφ. To calculate the perturbation of the energy E(v, J)
=
 1
0 (∥vt∥2
V +∥∂Jt
∂t +∇J∗
t vt∥2
2)dt of Eqn. 12.114 minimizing jointly in v and J. The variation
in v takes the form
∂ψE(v, J) = 2
 1
0

⟨Avt, ψt⟩+
∂Jt
∂t + ∇J∗
t vt, ∇J∗
t ψt

dt,
(12.127)
giving the ﬁrst variation of Eqn. 12.125.
The variation in J is obtained by computing the differential in J with v ﬁxed:
∂φE(v, J) = 2
 1
0
∂Jt
∂t + ∇J∗
t v, ∂φt
∂t + v∗
t ∇φt

2
dt
= −2
 1
0
 ∂
∂t
∂Jt
∂t + ∇J∗
t vt

+ div
∂Jt
∂t vt + (∇J∗
t vt)vt

, φt

2
dt
which gives the variation of Eqn. 12.126.
To compute the Euler equation 12.124 with respect to variations of the ﬂow, ﬁrst
deﬁne
Zt = ∂Jt
∂t + ∇J∗
t vt.
(12.128)
Then we shall require the following identities.
Lemma 12.27
−∇(⟨∇J, v⟩)Z = (DAv)∗v + (Dv)∗Av + ∇Z⟨∇J, v⟩
(12.129)
(DAv)v = (DAv)∗v + ∇Z⟨∇J, v⟩−⟨∇Z, v⟩∇J.
(12.130)
Proof
The ﬁrst identity follows from
∇⟨Z∇J, v⟩= (∇Z)⟨∇J, v⟩+ Z∇⟨∇J, v⟩
(12.131)
implying
−∇(⟨∇J, v⟩)Z = −∇⟨Z∇J, v⟩+ (∇Z)⟨∇J, v⟩
(12.132)
(a)
=(DAv)∗v + (Dv)∗Av + ∇Z⟨∇J, v⟩.
(12.133)

370
12 PHOTOMETRIC AND GEOMETRIC DEFORMABLE TEMPLATES
where (a) uses Eqn. 12.125. To prove the second identity Eqn. 12.130 use the two
equalities
(DAv)v = −∇J⟨∇Z, v⟩−ZHJv
(12.134)
(DAv)∗v = −∇Z⟨∇J, v⟩−ZH∗
J v,
(12.135)
where HJ is the Hessian of J. Now since HJ = H∗
J , and equating 12.134, 12.135 gives
the second identity.
Now to ﬁnish the proof of the Theorem, to prove the rate of change of the momentum with
time use Eqn. 12.125 and the Lemma 12.27 according to
0 = ∂
∂t(Av + Z∇J) = ∂Av
∂t + ∂Z
∂t ∇J + ∇∂J
∂t Z
(12.136)
(a)
= ∂Av
∂t −(Zdivv + ⟨∇Z, v⟩)∇J −∇(⟨∇J, v⟩)Z + Z∇Z
(12.137)
= ∂Av
∂t + Avdivv −⟨∇Z, v⟩∇J −∇(⟨∇J, v⟩)Z + Z∇Z
(12.138)
(b)
= ∂Av
∂t + Avdivv + (DAv)∗v + (Dv)∗Av + ∇Z⟨∇J, v⟩
−⟨∇Z, v⟩∇J + Z∇Z
(c)
= ∂Av
∂t + Avdivv + (Dv)∗Av + (DAv)v + Z∇Z,
(12.139)
where (a) follows from Eqn. 12.126 giving ∂Z
∂t = −div(Zv) = −(Zdivv + ⟨∇Z, v⟩) and from the
deﬁnition of Z according to ∇∂J
∂t = −∇(⟨∇J, v⟩) + ∇Z; the equalities (b,c) follow from Eqns. 12.129,
12.130 of Lemma 12.27, respectively.
The ﬁrst part of the integral penalizes large variations of the homeomorphism, and the
second one penalizes variations in the material which are not due to the deformation. Both are
expressed as norms which depend only on the current object Jt(·), a consequence of the left-
invariance requirement. The energy does not track the accumulated stress from time t = 0. For
this reason, the matching procedure has been classifed as viscous matching ([220]), in opposition to
the methods involving elastic matching [288,273–296]). From the point of view of elasticity, it should
be harder to make a small deformation of an object J if it is considered to already be a deformation
of another object, than to operate the same deformation, but considering that J itself is at rest.
Technically, this means that the norms should depend in the elastic case, both on the deformed
object, and on the stress associated with the deformation, which implies that the left-invariance
assumption has to be relaxed. Most applications of elasticity in image analysis essentially assume
that small deformation approximations are valid, which means that they do not require the opti-
mization of the deformation path. Standard comparison of objects using elastic matching consider
that one object is at equilibrium and the other one is deformed, thus the special role of the initial
template state (see e.g, the Bookstein approximate metric 12.7.1). The result depends on the choice
on which object is at equilibrium. This is valid when comparing changes around an equilibrium
condition, for example facial deformation of expressions [199], or for growth [297].
However, as ﬁrst established in [286], without the left invariance the measure of mismatch
given by the minimum norm-square will not be a metric distance.
Example 12.28
(Computing Inﬁnite Dimensional Geometric and Photometric
Variation (Younes)) This section follows closely the implementations of Younes [286].
Let the image mapping be deﬁned on X = [0, 1]2 ⊂R2, with values in R, and
let the geometric group G be the group of inﬁnite dimensional diffeomorphisms
of the previous section. Deﬁne the ﬁnite norm condition for the zero boundary
via the Laplacian-squared A = L∗L with L = ∇2 = (∂/∂x2
1) + (∂/∂x2
2) giving

12.9 THE EULER EQUATIONS FOR PHOTOMETRIC AND GEOMETRIC VARIATION 371
Hilbert space for the tangent component of the geometric ﬂow having norm-square
∥vt∥2v = 
2
i=1 ∥∇2vit∥2
2 with the metric energy
 1
0
2

i=1
∥∇2vit∥2
2 dt + α
 1
0
::::
∂Jt
∂t + ∇J∗
t vt
::::
2
2
dt,
(12.140)
α > 0 a positive parameter. Comparison between two images I0, I1 is performed by
minimizing the functional over all paths which satisfy vt(y) = 0, t ∈[0, 1], y ∈∂X, and
J0 = I0, J1 = I1.
TheminimizationofEqn. 12.140isperformedaftertimeandspacediscretization,
the derivatives being estimated by ﬁnite differences. If Nd is the dimension of the space
grid and T the number of time steps, there are, because of the boundary conditions, a
little less than 3NdT variables to estimate (v has d coordinates and J has one).
For ﬁxed v (respectively ﬁxed J), the energy is quadratic in J (respectively v). For
this reason, Younes uses a two-step relaxation procedure, which alternates a conjugate
gradient descent for J with a gradient descent for v. Acomplication comes from the fact
that the discretization of the intensity conservation term ∂Jt/∂t + ∇J∗
t vt has to be done
with care. A direct linear discretization with ﬁnite difference leads to very unstable
numerical procedures, and one has to use a non-linear, but more stable approximation
for the last term, of the kind
d

k=1
(Jt(s + ek) −Jt(s))v+
t (k, s)/h −(Jt(s) −Jt(s −ek))v−
t (k, s)/h
h being the space step, (ek) the canonical basis of Rd and v+ (respectively v−) the posi-
tive (respectively negative) part of v. We also use a hierarchical, multigrid, procedure:
the deformations are ﬁrst estimated on a coarse grid (small N) and then iteratively
reﬁned until the ﬁner grid is reached. The choice of the number of time steps, T, is
quite interesting. For T = 2 (t = 0 or 1), the minimization reduces to the regular-
ized intensity-conservation cost function which is of standard use for optical ﬂow
estimation.
Shown in Figures 12.7, 12.8 are results demonstrating the transformation pro-
cess Jt(·) as functions of time t ∈[0, 1] between two images I0 and I1. Each row shows
the process of either creating pixel intensity or geometric change when a totally black
image is matched to an image containing a white disc in its center. Depending on the
choice of the parameter α, the process will allow for the creation of a large quantity
of pixel intensity, yielding a transformation which looks like fading, with almost no
Figure 12.7 Shows two rows from Younes [286] exclusively depicting photometric variation.
Top row shows photometric change during matching. Bottom row shows grid undergoing no
deformation.

372
12 PHOTOMETRIC AND GEOMETRIC DEFORMABLE TEMPLATES
Figure 12.8 Shows all grid deformation from Younes [286]. Top row shows photometric change
required to generate matching. Middle row shows photometric change put through geometric
variation. Bottom row shows deformation to the grid.
Figure 12.9 Rows show the time series of geometric and intensity transformation in brain tissue
used for tumor growth. Row 1 shows the insertion of photometric change depicted by the small
black dot. Row 2 shows the tumor developing under mostly geometry deformation. Row 3 shows
the geometric deformation of the grid.
deformation at all (α small), or will prefer introducing a small white zone in the center
of the disc, and deform it into a disc, yielding a process which resembles an explosion
(α large).
The experiments illustrate how luminance can be created during the optimal
transformation process. Depending on the value of the parameter α (which penalizes
non-conservation of luminance) the results are quite different: for small α, the disk is

12.10 METRICS BETWEEN ORBITS OF THE SPECIAL EUCLIDEAN GROUP
373
Figure 12.10 Top row shows photometric and high-dimensional geometric motion for the glasses
experiment. Bottom rows show the eye opening video. Results taken from Younes showing image
ﬂow Jtk (x).
appearing without any deformation, simply the gray levels vary. For large α, a white
spot ﬁrst appears at the center of the disk and the deformation generates the expansion.
Shown in Figure 12.7 is the time series associated with the geometric transfor-
mation and intensity transformation for the matching of the totally black image to
the white disk centered. The ﬁrst and last picture in each row are the data provided
to the algorithm, and the other ones are synthesized intermediate stages based on
photometric and geometry transformation.
To illustrate such methods on anatomical tissue, shown in Figure 12.9 is the result of matching
sections of a brain with a tumorous shape. The top row shows the insertion of photometric change
depicted by the small black dot. The middle row shows the tumor developing under mostly
geometry deformation. The botom row shows the geometric deformation of the grid. Notice in
the rightmost panel the centered black spot.
Example 12.29 (Glasses and Eyelids (Younes))
Shown in Figure 12.10 are results
from the glasses experiment and eye opening experiments from Younes. The top row
shows the glasses experiment results. Shown in the bottom row of Figure 12.10 are
results from Younes for the woman closing her eye.
12.10 Metrics between Orbits of the Special Euclidean Group
Return to Theorem 12.12 examining understanding through the invariances of the special
Euclidean group. In the context of the inﬁnite dimensional setting we construct a distance between
the orbits, the natural deﬁnition becoming
˜ρ([I]SE(d), [I′]SE(d)) =
inf
g,h∈SE(d) ρ(gI, hI′).
(12.141)
This clearly places the requirement on the distance function that it be invariant to the group SE(d)
deﬁning the orbit.

374
12 PHOTOMETRIC AND GEOMETRIC DEFORMABLE TEMPLATES
Theorem 12.30
Let
ρ2(I, I′) =
inf
g(·),I(·): g0=id,g1=g
J0=I,J1=I′
 1
0

∥vt∥2
V +
::::
∂Jt
∂t + ∇J∗
t vt
::::
2
2

dt,
(12.142)
with A = L∗L and ˜ρ deﬁned as
˜ρ([I]SE(d), [I′]SE(d)) =
inf
g,h∈SE(d) ρ((gI), (hI′))
(12.143)
If for all g ∈SE(d),
 1
0

X
∥Lg−1vt(g(y))∥2
Rd dy dt =
 1
0

X
∥Lvt(y)∥2
Rd dy dt,
(12.144)
then ˜ρ : G/SE(d) × G/SE(d) →R+ is a distance on the orbits.
Proof
The proof requires showing the invariance ˜ρ(gI, gI′) =
˜ρ(I, I′) under the
assumption Eqn. 12.144 on the ∥Lv∥energy. Deﬁne J′
t(y) = Jt(gy) giving energy
 1
0

X

∥Lvt(y)∥2
Rd + |∂Jt
∂t (y) + ∇J∗
t (y)vt(y)|2

dy dt
(12.145)
=
 1
0

X

∥Lvt(y)∥2
Rd +

∂J′
t
∂t (y) + ∇J′∗
t (y)g−1vt(gy)

2
dy dt
(12.146)
(a)
=
 1
0

X

∥Lgv′
t(g−1y)∥2
Rd +

∂J′
t
∂t (y) + ∇J′∗
t (y)v′
t(y)

2
dy dt,
(12.147)
with (a) following from the substitution v′
t(y) = g−1vt(gy). If the ﬁrst term in the
integral is unchanged, then the cost is unchanged in the distance giving the required
left invariance condition.
The examples which have been used extensively including the Laplacian and its powers,
biharmonic, satisfy the invariance condition.
12.11 The Matrix Groups (Euclidean and Afﬁne Motions)
This section follows Bitouk [298]. Given images I0 and I1 deﬁned on Rd (d = 2, 3), assume there
are rigid objects in the background under rotational motion. Introduce the computational time
variable generated by the ODE
dOt
dt
= tOt,
O0 = id,
(12.148)
where t is a skew-symmetric matrix representing the roll of the object. In 2D, for example, t =
 0
θt
−θt
0

. The following theorem gives the metric between images under rotational motions,
assuming rotation is a pure nuisance variable and assigning no metric length for rotation.

12.11 THE MATRIX GROUPS
375
Theorem 12.31
Deﬁning the group action to be the rotational motion gt : x →Otx, O
satisfying Eqn. 12.148 above, then the distance between images I and I′ is given by
ρ2(I, I′) = ρ2((id, I), (O, I′(O)))
(12.149)
=
inf
O∈SO(d)
inf
O,I: O0=id,O1=O
I0=I,I1=I′(O)
 1
0
::::
∂It
∂t (O−1
t
)
::::
2
2
dt
(12.150)
=
inf
,J:J0=I,J1=I′
O0=id,O1=O
 1
0
::::
∂Jt
∂t + ⟨∇Jt, t·⟩Rd
::::
2
2
dt =
inf
O∈SO(d) ∥I −I′(O)∥2
2.
(12.151)
Proof
Computing the change of variable for Jt = It(g−1
t
) makes Eqns. 12.150, 12.151
equivalent:
∂It
∂t (x) = ∂Jt
∂t (Otx) + ⟨∇Jt(Otx), tOtx⟩Rd
(12.152)
∂It
∂t (O−1
t
x) = ∂Jt
∂t (x) + ⟨∇Jt(x), tx⟩Rd.
(12.153)
To ﬁnd the minimum simply note that the expression to be minimized in
Eqn. 12.150 is
 1
0
::::
∂It
∂t (O−1
t
)
::::
2
2
dt =
 1
0
::::
∂It
∂t
::::
2
2
dt,
(12.154)
since the determinant of the Jacobian of the change of variables x = O−1
t
y is 1. Thus
I is the linear interpolation of (since geodesics are straight lines, Theorem 10.5) its
boundary conditions
It(·) = tI1(·) + (1 −t)I0(·) = tI′(O1·) + (1 −t)I(·);
(12.155)
Jt(·) = tI1(O−1
t
·) + (1 −t)I0(O−1
t
) = tI′(O1O−1
t
·) + (1 −t)I(O−1
t
).
(12.156)
Substituting into the cost of Eqn. 12.154 gives Eqn. 12.152
A simple algorithm involving Eqn. 12.151 can be devised: alternate minimization in J and
in . This will converge to a local minimum of the energy.
Algorithm 12.32
1. Fixing , O, then minimizing Eqn. 12.151 is linear giving
Jt(·) = tI′(O1O−1
t
·) + (1 −t)I(O−1
t
·).
(12.157)

376
12 PHOTOMETRIC AND GEOMETRIC DEFORMABLE TEMPLATES
2. Given Jt(·), t ∈[0, 1], minimize Eqn. 12.151:
arg inf

 1
0
::::
∂Jt
∂t (·) + ⟨∇Jt(·), t·⟩Rd
::::
2
2
dt.
(12.158)
Example 12.33
(Adding cost for rotational motion) To add distance within the
orthogonal group G = SO(d) then add the geodesic distance between group elements
giving
ρ2((id, I), (O, I′(O))) = inf
 1
0
::::
∂
∂tOt
::::
2
Rd2 dt +
 1
0
::::
∂It
∂t (O−1
t
(·))
::::
2
dt

(12.159)
= inf
 1
0
tr ˙Ot ˙O∗
t dt +
 1
0
::::
∂It
∂t (O∗
t )
::::
2
2
dt

(12.160)
= 2β2 + ∥I(·) −I′(O1·)∥2
2,
(12.161)
where eβZid = O, Z-antisymmetric.
Adding the translation group is straightforward. Let gt(x) = Otx + a(t), let β(t) = da(t)/dt
and minimize
 1
0
::::
∂Jt
∂t + ⟨∇Jt, t · +β(t)⟩
::::
2
2
dt
The problem still is quadratic in (t, β(t)). For the explicit expression of J, simply replace
O−1
t
by g−1
t
and O, O−1
t
by g1(g−1
t
).
12.11.1 Computing the Afﬁne Motions
To incorporate scaling, deﬁne the similitude St = ρtOt ∈Sim(d) by the ODE
dSt
dt = (λtid + t)St,
where λt = ˙ρt
ρt
,
and t is as above skew-symmetric. Since the Jacobian is (ρt)d,
d
dt log det St = d d
dt log ρt = dλt.
(12.162)
Thus with S0 = id then
det St = ed
 t
0 λu du,
implying the energy becomes
 1
0
::::
∂It
∂t (S−1
t
)
::::
2
2
dt =
 1
0
det S−1
t
::::
∂It
∂t
::::
2
2
dt
(12.163)
=
 1
0
e−d
 t
0 λs ds
::::
∂It
∂t
::::
2
2
dt.
(12.164)

12.11 THE MATRIX GROUPS
377
This still can be explicitly minimized, introducing the time change
αt =
 t
0 det S−1
u
du
 1
0 det S−1
u
du
(12.165)
yielding It(x) = αtI0(x) + (1 −αt)I1(x).
(12.166)
In all cases, the energy at the minimum is
 1
0
|dα
dt |2 dt + ∥I −I′(O1)∥2
2
implying that the problem is equivalent to the usual static problem of minimizing ∥I −I′(O1)∥2.

13 E S T I M AT I O N B O U N D S F O R A U T O M AT E D O B J E C T
R E C O G N I T I O N
ABSTRACT
Thus far we have only studied representations of the source. Now we add the
channel, pushing us into the frameworks of estimate then examine estimation bounds
for understanding rigid object recognition involving the low-dimensional matrix groups.
Minimum-mean-squared error bounds are derived for recognition and identiﬁcation.
13.1 The Communications Model for Image Transmission
In automated object recognition objects are observed at arbitrary locations and orientations via
remote sensors. Depending on the relative orientation, distance and position—henceforth called
pose—between the object and sensor the observed imagery may vary dramatically. Building object
recognition systems which are invariant to pose is a fundamental challenge to such systems.
Throughout this chapter the goal is to analyze object recognition from a principled point of view
in which estimators are studied which satisfy optimality criteria with bounds derived, which
describe optimality independent of the particular algorithm used for analysis.
The basic model for the source is that the images I ∈I = Iα ◦G are orbits under actions
of transformation g ∈G of an exemplar template, Iα the transformations studied throughout
this chapter are the matrix groups and afﬁne motions. Assume the objects are complex enough
(modulo the symmetries) that they uniquely determine the group elements, so that there is a
bijection between Iα(g) = Iα ◦g ↔g. Then there is a natural identiﬁcation of the image Iα(g) ∈I
with the group element g ∈G which generates it. Thus, the prior density in the Bayes model
is induced by the density on the groups of transformations, π(g), g ∈G. Randomness on the
transformation induces randomness on the underlying source of images I.
The ideal images are observed through various nonideal sensors as depicted in Figure 13.1.
The observable data ID ∈ID may contain multiple components corresponding to several sensors
ID =

ID1, ID2, . . .

. The observation process is characterized via the likelihood (conditional density)
function for the sensors, p(·|·) : I×ID →R+, summarizing completely the transition law mapping
the input ideal image I to the output ID; the likelihood of ID given I.
The importance of the conceptual separation of the source of possible images I with prior
density and the channel with transition law is that there is only one true underlying scene, irre-
spective of the number of sensor measurements forming ID = (ID1, ID2, . . . ). Only one inference
problem is solved, with the multiple observations due to multiple sensors viewed as providing
additional information in the posterior distribution. Sensor fusion occurs automatically in this frame-
work. Certainly, in calculating information gain and channel capacity this view determines these
estimation bounds.
Source
Noisy Channel
Image Inference
α
α
Figure 13.1 The source of images: An orbit under matrix groups of transformations. The
observable ID is from multiple sensors with ˆI the estimator generated by the inference machine.
378

13.1 THE COMMUNICATIONS MODEL
379
13.1.1 The Source Model: Objects Under Matrix Group Actions
Begin examining the matrix groups of low-dimension such as the generalized linear group and
its subgroups. In this chapter the subgroups of the afﬁne group are examined for pose. Deﬁning
g = (A, a), the image is identiﬁed with transformation on the background space of the template:
Iα(g) = Iα(Ax + a), g = (A, a) ∈G ⊂GL(d) ⊗Rd.
(13.1)
The orbit I = {Iα(g) = Iα ◦g, g ∈G} is the space of all such imagery with prior π(·).
13.1.2 The Sensing Models: Projective Transformations in Noise
The estimators are determined by the statistical models (likelihood functions) representing the
imaging modalities. Many common imagers are maps T : I →ID from d-dimensional scenes,
d = 2, 3, 4, of objects occupying subvolumes in Rd to real- and complex-valued measurements
in multiple dimensions. Generally, accurate analytical expressions for T may not be available
in all situations. In such cases, high quality simulators are used to generate samples from the
transformations. For example, for infra-red imaging the PRISM [299] simulator is often used, or
for high resolution radar XPATCH is used [300, 301]. Begin with the most ubiquitous mappings
through projections from three-dimensions to two.
For many sensors, imaging is essentially a projective mechanism operating by accumulating
responses from the scene elements which project to the same measurement position in the image.
Such mechanisms are modelled as maps T : I →ID, the observations in most cases are Rd, Cd
valued for some ﬁxed number d.
Deﬁnition 13.1
The generalized projective sensor transformation T : I ∈I →TI ∈
ID is a mapping from R3 to R2, taking images to the function deﬁned on the detector plane,
T : I(x), x ∈X ⊂R3 →TI(y), y ∈Y ⊂R2.
The projective transformation of standard optical sensors provides 2D real-valued images of
objects sampled in the projective coordinates ID(y), y ∈Y. The transformation from the 3D image
world to the 2D imaging plane for each of the projective and perspective transformations takes
the following form. Aligning the x1 × x2 axes with the imaging plane, the orthogonal projections
are mappings
(x1, x2, x3) ∈X ⊂R3 →(x1, x2) ∈Y ⊂R2 (projection)
(13.2)
(x1, x2, x3) ∈X ⊂R3 →(x1
x3
, x2
x3
) ∈Y ⊂R2 (perspective).
(13.3)
Figure 13.2 depicts the perspective projections’ 3D objects onto the 2D projective plane.
13.1.3 The Likelihood and Posterior
For video imaging in the high count limit, the additive noise model deﬁnes ID as a Gaussian
random ﬁeld with mean ﬁeld the projection ID = TIα(g) + W, W a Gaussian random ﬁeld. The

380
13 ESTIMATION BOUNDS
Figure 13.2 Panels show the projective transformations in noise. Cubes depict objects in 3D Iα ◦
g; columns show projection mean TIα(g) (bottom panel) and projection in Gaussian noise ID =
TIα(g) + W.
conditional likelihood density of the data ID conditioned on mean-ﬁeld TI(g) is given by
p(ID|Iα(g)) =
1
(2πσ 2)N/2 e−1/2σ 2∥ID −TIα(g)∥2,
(13.4)
where ∥ID −TIα(g)∥2 =

y∈Y
|ID(y) −TIα(g)(y)|2.
(13.5)
Note that throughout this chapter the imagery will be assumed to correspond to a discrete lattice
of dimension N = |Y|, and ∥· ∥will correspond to a componentwise indexing over the lattice. In
subsequent chapters we will extend to the continuum model.
Shown in Figure 13.2 is an example of the Gaussian additive noise projection model. Cubes
depict objects in 3D, Iα ◦g for the truck and airplane. Columns show the projective mean TIα(g)
(bottom panel) and projection in Gaussian noise ID = TIα(g) + W. The rightmost panels in each
column show the 3D object.
Figure 13.3 illustrates how the Gaussian log-likelihood encodes the information concerning
the objects parameters such as pose g ∈G. The top row panel 1 shows the true rendered object.
Panels 2–4 show different choices of the object at different poses and different object types. Panel
5 shows the observed projective imagery in noise, ID = TIα(g) + W. The bottom row panels 6–8
show the difference between the optical data synthesized according to the Gaussian model from
the target at its estimated pose ID −TIα(g), subtracted from the true measured data.
With the prior density35 π(g), g ∈G, the posterior is taken directly on the transformation in
the form
p(g|ID) = π(g)p(ID|Iα(g))
Z(ID)
.
(13.6)
35 The density is the derivative of the probability measure with respect to the base measure, which is
Lebesgue on Rd and the Haar measure more generally on the other subgroups of GL(d), such as SO(d) (see
Boothby [148] for a discussion of Haar measures).

13.1 THE COMMUNICATIONS MODEL
381
(1)
(2)
(3)
(4)
(5)
(6)
(7)
(8)
Figure 13.3 Top row: Panel 1 shows the true rendered object TIα(g). Panels 2–4 show different
choices of the object at different poses and different object types. Bottom row: Panel 5 shows
the observed Gaussian random ﬁeld ID = TIα(g) + W for the true object. Panels 6–8 show the
difference between the optical data synthesized according to the Gaussian model from the target
at its estimated pose, subtracted from the true measured data. Panel 6 shows the correct object
at correct pose. Panels 7,8 show mismatched pose and object type, respectively.
13.2 Conditional Mean Minimum Risk Estimation
13.2.1 Metrics (Risk) on the Matrix Groups
Using classical Bayesian techniques, optimum estimators can be generated of the object pose, and
in addition to the estimator present lower bounds on their estimation accuracy. The minimum
risk estimator requires the introduction of the risk function R : G × G →R+, then given g∗any
estimator a mapping from ID →G the risk R becomes
R = E{R(g, g∗(ID))}.
(13.7)
To deﬁne a minimum error estimator on pose estimation subgroups of the afﬁne motion are
studied utilizing the Euclidean metric from the general linear group. We call this the Frobenius or
Hilbert Schmidt metric (distance). Estimators which minimize the square of this distance we shall
call minimum-mean-squared error (MMSE) estimators (sometimes Hilbert Schmidt estimator).
The estimator is similar to the conventional least-square estimator but extended to account for the
geometry of the subgroups of the special Euclidean group.
Examine the afﬁne group A(d) = GL(d) ⊗Rd and its subgroups. It is convenient to identify
these group elements with their (d + 1) × (d + 1) homogeneous coordinate representation so
that group composition is purely matrix multiplication within their homogeneous coordinate
representation.
Deﬁnition 13.2
Deﬁne elements of g ∈A(d) = GL(d)⊗Rd represented via their (n+1)×
(n + 1) homogeneous coordinate representation ¯A according to
g =
A
a
0
1

∈¯A,
A ∈GL(d),
a ∈Rd.
(13.8)

382
13 ESTIMATION BOUNDS
This is a metric space with Frobenius (also called Hilbert–Schmidt) distance between
two elements deﬁned by
ρ2
R(d+1)2 (g, g′) = ∥g −g′∥2
R(d+1)2 = tr(g −g′)∗(g −g′)
(13.9)
= tr(A −A′)∗(A −A′) + ∥a −a′∥2
Rd.
(13.10)
The group action of the homogeneous representation on the background space
X ⊂Rd of g : x →g
x
1

with g ∈¯A operating through matrix multiplication.
13.2.2 Conditional Mean Minimum Risk Estimators
Deﬁnition 13.3
For the posterior density p(·|ID) on G ⊆GL(d) ⊗Rd, the MMSE is
deﬁned to be
ˆg = arg min
g∈G
E
&
ρ2
R(d+1)2 (g, g′)
'
(13.11)
When ρ2
R(d+1)2 (g, g′) = tr(g −g′)(g −g′)∗it is called the HS estimator.
There may be multiple solutions to this equation, and hence the estimator may be set valued
(see [302], e.g.).
The Orthogonal Group: The MMSE minimum risk estimator ˆO : ID →SO(d) on the
orthogonal group has the norm squared risk metric ∥· ∥2
Rd2 which is continuous in its parameters
and since SO(d) is compact, the minimizer lies in SO(d). Shown in Figure 13.4 are HS plots of the
error metric for SO(2) (panel 1), SO(3) (panel 2).
Theorem 13.4 (Srivastava [303])
The
MMSE
estimator
restricted
to
G = SO(d)
deﬁned by
OMS = arg min
O∈SO(d)
E
&
∥O −O′∥2
Rd2
'
,
(13.12)
with property that for any other estimator ˆO : ID →SO(d),
E
&
∥ˆO(ID) −O′∥2
Rd2
'
≥E
&
∥OMS(ID) −O′∥2
Rd2
'
.
(13.13)
The MMSE estimator OMS(ID) given data realization ID ∈ID is given by,
OMS(ID) = arg max
O∈SO(d)
tr(O · A∗) where A = E{O′|ID},
(13.14)
=



UV⋆,
if determinant(A) ≥0
ULV⋆,
L =


1
0
. . .
0
0
1
. . .
0
...
0
0
. . .
−1

, if determinant(A) < 0,
(13.15)
and A = UV⋆is the singular value decomposition of A. The mean-squared error (MSE) is
given by
MSE = E{̺(ID)},
where ̺(ID) = 2(d −tr A∗OMS(ID)).
(13.16)

13.2 CONDITIONAL MEAN MINIMUM RISK ESTIMATION
383
0
50
100 150 200 250 300 350
400
0
1
2
3
4
5
6
7
8
(1)
(2)
(3)
0
–1
–1
–0.5
0.5
1
0
0.5
1
0
1
100
200
300
400
0
100
200
300
400
0
2
4
6
8
Figure 13.4 Panel 1 shows the Hilbert Schmidt distance for SO(2); panel 2 shows the same for
SO(3). Panel 3 shows tiling of SO(3) for discrete integration. Each point on S2+ is where a rotation
vector passes.
In the case of SO(2), for det(A) ̸= 0 the MMSE reduces to
OMS =
1
√det(A)A.
(13.17)
Proof
The inequality comes from the deﬁnition of the Hilbert–Schmidt estimator.
E∥ˆO −O′∥2
Rd2 = E
&
E
&
∥ˆO −O′∥2
Rd2 |ID''
≥E
&
E
&
∥OMS −O′∥2
Rd2 |ID''
= E∥OMS −O′∥2
Rd2 .
(13.18)
Calculating OMS,
OMS(ID) = arg min
O∈SO(d)
E
&
∥O −O′∥2
Rd2 |ID'
= arg min
O∈SO(d)
(2d −2E{tr O∗O′|ID})
= arg max
O∈SO(d)
tr(O · A⋆),
where A = E{O′|ID}. The maximizer is given by the singular value decomposition
(see [304]). Substituting into the deﬁnition of the MSE, Eqn. 13.16 follows.
The Special Euclidean Group: Examine the special Euclidean group SE(2, 3) of dimensions
2 (ground plane) and 3 (air frames). For a ﬁxed ground-plane, objects have positions in R2 and
orientations in SO(2), identiﬁed with the pair (O, a) taking values in SE(2), the special Euclidean
group of dimension 3.

384
13 ESTIMATION BOUNDS
Theorem 13.5
Given is the posterior density p(·|ID) continuously and compactly supported
prior π(·) on C ⊂SE(d). The MMSE estimator in the special Euclidean group is given by
gMS =
OMS
aMS
0
1

,
where aMS(ID) = E{a|ID},
(13.19)
OMS(ID) = arg max
O∈SO(d)
tr (O · E{O′|ID}⋆).
(13.20)
The mean-squared error is given by
MSE = E{̺(ID)}
where
(13.21)
̺(ID) = 2d −2tr A∗OMS(ID) + E{∥aMS(ID) −a∥2
Rd|ID}.
(13.22)
For any other estimator ˆO, ˆa : ID →SE(d), the mean-squared error is larger,
E
&
∥ˆg −g∥2
R(d+1)2 |ID'
≥E
&
∥gMS −g∥2
R(d+1)2 |ID'
.
(13.23)
Proof
Substituting for the MSE gives
∥ˆg −g∥2
R(d+1)2 = ∥ˆO −O∥2
Rd2 + ∥ˆa −a∥2
Rd.
(13.24)
Since the terms separate in their functional dependence on (O, a), the result follows.
Note that aMS(ID) is the usual conditional-mean (or least-squares) estimate of the object
position under the marginal posterior density.
13.2.3 Computation of the HSE for SE(2,3)
For most of the situations the integrands are too complicated to be integrated analytically, so the
trapezoidal method is used dividing the domains of integration into a uniform grid and evaluating
the integrals by ﬁnite summations. On SE(2) this involves straightforward discretization of the
torus. For SE(3) the angle–axis representation of the orthogonal group is used and described
subsequently (see Section 13.2.4). This reduces the density to a discrete mass function ˜p(·, ·) on the
discrete set of rotations and translation with the estimators computed as
aMS(ID) =
M

j=1
N

i=1
ai ˜p(Oj, ai|ID),
(13.25)
OMS(ID) = arg max
O∈SO(d)
tr(O · A†),
where A =
M

j=1
N

i=1
Oj ˜p(Oj, ai|ID).
(13.26)
For SO(2), θj = 2πj/M and so Oj =
 cos(θj)
−sin(θj)
sin(θj)
cos(θj)

, and the average matrix is of the
form A =
 α
−β
β
α

with its singular values are both equal to √det(A), thus using singular value
decomposition to determine OMS gives UVT = (1/√det(A))A when det(A) ̸= 0. For SO(3), Eqn.
13.15 of Theorem 13.4 holds. The algorithm for generating numerical estimates is given as follows.

13.2 CONDITIONAL MEAN MINIMUM RISK ESTIMATION
385
Algorithm 13.6
1. Generate elements U = (O, a) ∈SE(2, 3) uniformly36 and apply each element to the
images so that the image is rotated by O and translated by a.
2. Compute the posterior density (to within the normalizing constant) p(Oi, aj|ID) for
each i = 1, 2, . . . , M and j = 1, 2, . . . , N to determine
˜p(Oi, aj|ID) =
p(Oi, aj|ID)

M
j=1

N
i=1 p(Oi, aj|ID)
.
3. Compute ˆa and the average matrix A as described in Eqns. 13.25 and 13.26, respectively.
13.2.4 Discrete integration on SO(3)
To compute the average associated with numerical integration of the expectation on SO(2), the
torus [0, 2π] is discretized in the usual way. Numerical integration of the expectation required on
SO(3) from Eqn. 13.26 is generated by identifying each element of SO(3) with an axis of rotation
(a unit vector in R3 or S2, two degrees of freedom) and an angle (the unit circle S1, one degree
of freedom; see Example 10.7.2, Chapter 11). To make the identiﬁcation unique restrict to only a
clockwise rotation and the axis of rotation only on upper hemi-sphere (S2+). The uniform samples
for the rotation angle are θi = 2πi/N2, i = 1, 2, . . . N2. To generate equi-spaced points on the upper
hemi-sphere S2
+ let xj = 2πj/N1, j = 1, 2, . . . , N1, and deﬁne φj = sin−1(xj). Then there are N1×N2
axes identiﬁed with the upper hemi-sphere according to


n1
n2
n3


ij
=


cos θi cos φj
cos θi sin φj
sin θi

,
(13.27)
with an associated set of rotation angles ψk = 2πk/N3, k = 1, 2, . . . , N3 around each of the axes.
The N1 × N2 × N3 rotation matrices Oijk are given by the formula
Oijk =


cos(ψk) + n2
1(1 −cos(ψk))
n1n2(1 −cos(ψk)) −n3 sin(ψk)
n1n3(1 −cos(ψk)) + n2 sin(ψk)
n2n1(1 −cos(ψk)) + n3 sin(ψk)
cos(ψk) + n2
2(1 −cos(ψk))
n2n3(1 −cos(ψk)) −n1 sin(ψk)
n1n3(1 −cos(ψk)) −n2 sin(ψk)
n2n3(1 −cos(ψk)) + n1 sin(ψk)
cos(ψk) + n2
3(1 −cos(ψk))

.
(13.28)
Panel 3 of Figure 13.4 illustrates the tiling of SO(3) (taken from [305]).
13.3 MMSE Estimators for Projective Imagery Models
13.3.1 3D to 2D Projections in Gaussian Noise
Examine results on minimum-risk bounds for imaging models Gaussian video imagery using
perspective projection. The projective transformation studied T : I →ID is that observed for
36 The translation group is not bounded, thus translations are generated either uniformly over a ﬁxed region,
or with respect to a compactly supported prior.

386
13 ESTIMATION BOUNDS
1.5
(1)
(2)
(3)
(4)
(5)
(6)
(7)
1
0.5
0
0
0.2 0.4 0.6 0.8
Noise std–Deviation 
Minimum Expected Error
Expected Error vs Noise Level
1
1.2 1.4 1.6 1.8
2
Figure 13.5 Top row: Panels 1–5 show CAD models within one error ball of panel 3 depicted by
the middle X at 1.0 unit HS MSE. Bottom row: Panel 6 shows ID = TIα(g) + W for noise level
σ = 0.4 corresponding to the middle X in panel 7 for HS performance of MSE=1. Panel 7 shows the
mean-squared error bound as measured by the HSB on estimating the orientation using VIDEO
projective imagery as a function of noise level.
real-valued video imagery in the Gaussian noise limit. Thus the additive noise model is used,
deﬁning ID as a Gaussian random ﬁeld with mean ﬁeld the projection ID = TIα(g) + W, W a
Gaussian random ﬁeld with conditional density
p(ID|Iα(g)) =
1
(2πσ 2)N/2 e−
1
2σ2 ∥ID−TIα(g)∥2
.
(13.29)
Example 13.7 (MMSE for Estimators on SO(2) (Srivastava [303] ))
Examine exper-
imental results on minimum-risk bounds. Begin with ground vehicles parameterized
to axis ﬁxed rotations in SO(2) in the high count limit. The imaging model deﬁnes ID as
a Gaussian random ﬁeld with mean ﬁeld the projection TI(g) with density as given in
Eqn. 13.4.Assume a uniform prior probability for orientation of objects π(·) = constant.
Figure 13.5 shows results from computing the mean-squared error performance asso-
ciated with orientation for the truck in VIDEO imagery. The top row panels 1–5 show
renderings TI(g) of a set of CAD models in the database equally spaced around the
true object (panel 3). The CAD models shown are within MSE = 1 unit distance from
the true CAD model (panel 3) in the database. Shown in the bottom row panel 6
of Figure 13.5 is the truck CAD model at a noise level σ = 1.0. Panel 7 shows the
HSB mean-squared error bound curve for orientation as a function of noise level. The
middle cross of panel 7 corresponds to the noise level in the truck image shown in
panel 6 at an MSE = 1 unit of error. The MSE was computed over [0, π/2] to avoid
symmetries; MSE=0 implies perfect placement of the truck, with maximal MSE=1.45
implying maximally unreliable placement of the truck.
Example 13.8
(Adding Dynamical Equations of Motion for SE(2, 3)(Srivastava
[306])) Examine results on MSE bounds from Srivastava incorporating dynamics.
Pose estimation for continuous relative motion between the objects and the camera
sensors requires extension to the special Euclidean group SE(2, 3). Examine the para-
metric representations of object motion for airframes composed of motions through
SE(3) as described in Srivastava [306]. The prior probability on motions are induced
using Newtonian dynamics. Assuming standard rigid body motion with respect to

13.3 MMSE ESTIMATORS
387
Body Reference Frame
X
Y
Z
(1)
(2)
Inertial Reference Frame
p
0
0.5
1
1.5
2
2.5
3
3.5
4
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
HSB vs Noise: with & without motion prior 
Noise Std
HSB
v1
q1
v2
q2
q3
v3
Figure 13.6 Panel 1 shows the CAD airframe with the inertial coordinates with linear and angular
velocities. Panel 2 shows MSE as measured by the HSB versus noise in the case of three different
motion priors: (i) no prior (broken line), (ii) motion prior, and (iii) strong prior (.-.-)(see also Plate
19).
the inertial frame (see panel 1 of Figure 13.6), v(t) ∈R3 are the linear velocities,
q(t) =


q1(t)
q2(t)
q3(t)

∈R3 are the rotational velocities, x(t) ∈R3 are the inertial posi-
tions and O(t) ∈SO(3) are the object orientations with respect to the inertial reference
frame. The standard Newtonian laws are used to construct the non-linear differential
equations between the linear and angular velocities and the force/torque functions.
The displacements relate to the velocities according to the equations, ˙O(t) = O(t)Q(t),
˙x(t) = O(t)v(t), where Q is the 3 × 3 skew-symmetric matrix of elements q1, q2, q3,
Q(t) =


0
q1(t)
q2(t)
−q1(t)
0
q3(t)
−q2(t)
−q3(t)
0

.
Use standard difference equations assuming the angular and linear velocities
are piecewise constant over the δ = 1 time increments; identifying discrete time points
tn = nδ, then v(n) = x(n + 1) −x(n), O(n + 1) = O(n)eQ(n) with O(0) = id the 3 × 3
identity matrix. Tracking in the plane implies that the rotations are axis-ﬁxed around
the z-axis with q2 = q3 = 0 giving
O(n + 1) = O(n)


cos q1(n)
sin q1(n)
0
−sin q1(n)
−cos q1(n)
0
0
0
1


=


cos 
n
i=1 q1(i)
sin 
n
i=1 q1(i)
0
−sin 
n
i=1 q1(i)
cos 
n
i=1 q1(i)
0
0
0
1

.
(13.30)
Model the differential equations on forces and torques assuming identity
moments of inertia. Then
v(n + 1) + (Q(n) −id)v(n) = f(n),
q(n + 1) + (Q(n) −id)q(n) = t(n).
(13.31)

388
13 ESTIMATION BOUNDS
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
HSB (position) versus noise level
Noise standard deviation
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Noise standard deviation
Hilbert Schmidt Bound (normalized)
One image
Two images
Three images
Six images
0
0.2
0.4
0.6
0.8
1
1.2
1.4
HSB (orientation) versus noise level
Hilbert Schmidt Bound (normalized)
One image
Two images
Three images
Six images
Figure 13.7 Panel 1 shows VIDEO tanks as the camera closes; panel 2 shows the HSB bound for
mean-squared error performance of position; panel 3 shows the orientation bound. The different
curves correspond to successive estimates of position and orientation as a function of scale as the
camera closes. The different lines show the performance curves as a function of number of images
used for the estimator. Results from Srivastava [307].
Choosing the forces and torques as a Gaussian processes with covariance 1, 2, then
the linear and angular velocities form a Markov process, with joint density
p
v(1)
q(1)

,
v(2)
q(2)

, . . .

=
n

i=1
1

(2π)3 det 1
e−1/2(v(i)−(id−Q(i−1))v(i−1))∗
−1
1 (v(i)−(id−Q(i−1))v(i−1))
×
1

(2π)3 det 2
e−1/2(q(i)−(id−Q(i−1))q(i−1))∗
(2)−1(q(i)−(id−Q(i−1))q(i−1)).
(13.32)
Shown in panel 2 of Figure 13.6 are estimates of the orientation bound for a vehicle
moving in the ground plane imaged via the video camera. The dashed line shows
the mean-squared error performance as measured by the HSB of airplane orientation
in SO(2) as a function of the noise level in the imagery. Panel 2 shows three curves
demonstrating mean-squared error as it varies with the noise standard deviation for
three different prior conditions. The broken line represents a uniform prior on the axis
ﬁxed rotations associated with SO(2). The solid line represents a moderate dynamics
prior (moderate weight relative to conditional probability data term). The dashed-dot
(.) curve represents a strong prior (large weight relative to the data likelihood term).
Notice the decrease in estimator MSE bound.
Figure 13.7 illustrates results on MSE bounds from Srivastava incorporating
translation and rotation bounds of the special Euclidean group of motions for cam-
era motion [307]. Panel 1 shows the object as the camera converges on it. Panels 2
and 3 show the MSE bounds on position and orientation as a function of the noise
level for different object distances from the sensor. Due to the perspective projection,
increasing the object distance from the sensor results in a smaller number of pixels on
the object in the observed image and a pose estimator bound which increases with
distance to the camera. Each curve represents a different object distance to the cam-
era. Assuming conditional independence between the collections of observed images
ID = ID1, . . . , IDn, the likelihood is the product of the likelihoods of the individual
images, p(ID|O, a) = $n
i=1 p(IDi|O, a).

13.3 MMSE ESTIMATORS
389
2S1
T62
BTR60
D7
ZIL 131
ZSU 23/4
Variance images
Figure 13.8 Left Half: Publicly available SAR data from the MSTAR program showing vehicles (rows
1,3) and SAR datasets (row 2,4). Right Half: Shows estimated variance for each pixel for the 72
azimuth angles spaced from 5◦to 360◦of a T72. Variances were estimated from training data in
the MSTAR database (see also Plate 20).
13.3.2 3D to 2D Synthetic Aperture Radar Imaging
SAR imaging provides information corresponding to projective transformations from R3 to R2,
T : I(x), x ∈R3 →TI(y), y ∈R2, TI the reﬂectivity of the objects. SAR images are formed by
coherently combining range proﬁles arising from high-resolution radar imagers corresponding to
projections onto the slant plane of objects being imaged. X-band range proﬁles can decorrelate as
the object orientation changes by a fraction of a degree. SAR signatures of objects are similarly
variable, in addition depending on the imaging windowing used. Target speckle results from the
coherent combination of returns from scatterers as those scatterers move through relative distances
on the order of a wavelength of the transmitted signal. Shown in Figure 13.8 are examples of
vehicles and SAR datasets found in the training datasets of the MSTAR database.
A signal model for SAR imagery must incorporate the sources of such extreme variability
and must be robust with respect to the assumptions of the model if it is to be valid under a wide
variety of imaging scenarios. O’Sullivan, et al. [308] assume that the scene region corresponding
to any given pixel is large enough to contain multiple scattering centers and invoke central limit
theorem arguments to suggest a complex conditionally Gaussian model for the voxel values with
nonoverlapping regions being conditional independent. Because the returns are complex-valued
with the phase modelled as uniformly distributed across [0, 2π), the pixel mean is modeled as
identically zero and the variance parameter is a function of the target contents and its pose relative
to the radar platform. The conditional density on the observed SAR data ID given Iα(g) is given by
p(ID|Iα(g)) =


NS

i=1
1

πσ 2α(g, yi)

e−∥
ID
σα(g) ∥2
,
(13.33)
where there are NS = |Y| pixels in the image plane.

390
13 ESTIMATION BOUNDS
The variance function σ 2α(g) is the model which can be estimated empirically from the train-
ing data. Shown in the right panel of Figure 13.8 are a collection of variance images σT−72(g)
of a T-72 tank estimated from data in the MSTAR collection which were properly registered
and correspond to 5◦incremental models of the continuum of objects. The image shows esti-
mates for 72 azimuth windows concatenated row-wise with successive images separated by 5◦of
azimuth.
Example 13.9 (Radar: Synthetic Aperture Radar (O’Sullivan))
Examine results on
MSE bounds from O’Sullivan’s group studying SAR imaging in the rotation group
of axis-ﬁxed rotations O ∈SO(2). A conditionally Gaussian model for the data was
assumed with template variances σ 2α(O) at 5◦increments. The MSTAR data set was
separated into training and testing data sets (using the recommended separation on
the MSTAR CD). The variance images in the model were estimated using the training
data, examples of which are shown in the bottom row (left half) of Figure 13.8.
Bayesian classiﬁcation across the vehicle classes in an image ID can be classiﬁed
using a Bayesian approach treating pose as a nuisance parameter. The vehicle class is
selected as
ˆα =
arg max
α∈model classes
p(α|ID)
(13.34)
An estimate of the pose can be chosen to minimize the expected squared error in terms
of the appropriate distance metric. For O ∈SO(d), this becomes
OMS = arg min
O∈SO(d)
E
&
∥O −O′∥2
Rd2
'
.
(13.35)
For each of the SAR images in the testing data, the orientation of the object
was estimated using the conditionally Gaussian model of Eqn. 13.33. Histograms of
the Hilbert–Schmidt norm squared values for the BMP and the T-72 for all of the test
images are in panel 1 of Figure 13.9. The range of values for the Hilbert–Schmidt
norm squared is compressed in order to show more detail. There were 8 out of the 698
BMP images and 32 out of the 463 T-72 images with norm squares outside the interval
shown. Six out of eight for the BMP and 29 out of 32 for the T-72 correspond to 180◦
errors indicating the fundamental resolvability issue with object ambiguity for such
orientations. For reference, the Hilbert-Schmidt norm of .1 corresponds to an error
of 12◦.
The MSTAR dataset was partitioned into two sets of images: a training set for
parameter estimation and a testing set for evaluating orientation estimation algo-
rithms. Columns 1,2 of Figure 13.8 show sample images from the training and testing
sets for the T-72 tank. Figure13.9 illustrates the results of azimuth estimation and
vehicle classiﬁcation applied to the testing set. Panel 1 shows a relative histogram of
the errors, in degrees, between the actual azimuth angles and the estimated azimuth
determined according to (13.35). Panel 2 shows a polar plot of the average squared
Hilbert–Schmidt error in the azimuth estimate and the average probability of misclas-
siﬁcation as a function of true vehicle azimuth, respectively. The plot of estimation
error shows spikes at 90◦and 270◦(vehicle broadside facing left and right, respec-
tively) indicating that these two orientations are ambiguous. The plot of classiﬁcation
error shows that signiﬁcantly more classiﬁcation errors are made when the target is
facing nominally away from the radar platform.
The table in Figure 13.10 shows the performance for Bayes ID for the 10 class
recognition problem. Shown is the confusion matrix depicted correct ID along the
diagonal. Row headings name true vehicle classes and the value in any column is the
number of images classiﬁed as the vehicle named in the column heading.

13.3 MMSE ESTIMATORS
391
0
5
10
15
0
5
10
15
20
25
30
35
Equivalent error in degrees
Percent of errors
  0.21447
  0.42893
  0.6434
30
210
60
240
90
270
120
300
150
330
180
0
  2.3077
  4.6154
  6.9231
  9.2308
  11.5385
30
210
60
240
90
270
120
300
150
330
180
0
Histogram of estimation error
HSB error versus. Azimuth
Classification error versus. Azimuth
(1)
(2)
(3)
Figure 13.9 Panel 1 shows relative histogram of azimuth estimation errors converted from squared
Hilbert–Schmidt norm to an equivalent error in units of degrees. Panel 2 shows average pose esti-
mation error as a function of true vehicle azimuth. Panel 3 shows average classiﬁcation error rate as
a function of true vehicle azimuth. Results from the PhD thesis work of Devore and O’Sullivan [309]
(see also Plate 21).
2S1
BMP 2
BRDM2
BTR60
BTR70
D7
T62
T-72
ZIL131
ZSU234
2S1
262
0
0
0
0
0
4
8
0
0
95.62%
BMP 2
0
581
0
0
0
0
0
6
0
0
98.98%
BRDM2
5
3
227
1
0
14
3
5
4
1
86.31%
BTR60
1
0
0
193
0
0
0
0
0
1
98.97%
BTR70
4
5
0
0
184
0
0
3
0
0
93.88%
D7
2
0
0
0
0
271
1
0
0
0
98.91%
T62
1
0
0
0
0
0
259
11
2
0
94.87%
T-72
0
0
0
0
0
0
0
582
0
0
100%
ZIL131
0
0
0
0
0
0
2
0
272
0
99.27%
ZSU234
0
0
0
0
0
2
0
1
0
271
98.91%
Figure 13.10 Confusion matrix for the 10-class recognition problem. Row headings name true
vehicle classes and the value in any column is the number of images classiﬁed as the vehicle
named in the column heading. Results from O’Sullivan et al. [308]

392
13 ESTIMATION BOUNDS
Unconditional HSB performance curve for long pulse LADAR
0.4
25
23
21
19
16
13
9
0
0.35
0.25
0.15
0.05
0
8
10
12
14
16
CNR (dB)
MSE
Equivalent estimation error in degrees
18
20
22
24
0.1
0.3
0.2
LADAR
(1)
(2)
(3)
(4)
(5)
(6)
Figure 13.11 Top row: Panel 1 shows wireframes of targets viewed under perspective projection
and obscuration. Panel 2 shows noise-free LADAR range image with range ambiguity. Panel 3
shows sample LADAR range image with range ambiguity, anomalous pixels, and range-dependent
measurement errors. Bottom row: Panels show results from the LADAR experiments. Panel 4
and 5 show the mean LADAR signal and with noise. Panel 6 shows the HSB mean-squared error
performance for pose as a function of CNR (see also Plate 22).
13.3.3 3D to 2D LADAR Imaging
Laser radars (LADARs) sense ﬁxed radiant intensity and range information, LADAR imaging
provides vector range and reﬂectance information corresponding to projective transformations
from R3 to R2, (T1, T2) : I(x), x ∈R3 →(T1I(y), T2I(y)), y ∈R2(Z2), T1I the range of a particular
part of the object, and T2I the intensity of reﬂection.
The laser transmits a scanning sequence of pulses; upon reﬂection from the object and
background and roundtrip propagation through the atmosphere, each pulse is mixed on a pho-
todetector with a strong local oscillator (LO) beam with frequency offset from that of the transmitter
by an intermediate frequency (IF). The IF ﬁlter’s output consists of a signal return corrupted by
additive shot noise, which is a Gaussian random processes. The IF ﬁlter is followed by a square-law
envelope detector and a peak detector, which together produce two random variables, two mea-
surements, for each pixel: the peak intensity and the maximum-likelihood (ML) range-to-reﬂector
estimate. We shall focus on the range-to-reﬂector ﬁeld.
Figure 13.11 illustrates a representation of a conﬁguration of ground-based targets being
sensed by the LADAR sensor. The LADAR observes the scene through the effects of perspective
projection, in which a point (x, y, z) in 3D space is projected onto the 2D detector according to
(x, y, z) →(x/z, y/z). Panel 1 of Figure 13.11 shows a scene of CAD models. Panel 2 presents
the mean (noiseless) range image with ambiguity. Panel 3 shows a sample image corrupted with
anomalous pixels and range-dependent measurement error.

13.3 MMSE ESTIMATORS
393
Let ID be the laser radar range image with mean ﬁeld TLIα(g), then as modeled in [310,311]
the conditional density becomes
p(ID|Iα(g)) =
NL

i=1

1 −PrA(yi)

2πσ 2
L(yi)
e(−(ID(yi)−TLIα(g)(yi))2/2σ 2
L (yi) + PrA(yi)
R

,
(13.36)
where NL = |Y| the dimension of the discrete imaging lattice and Rmin ≤ID, ID ≤Rmax. Here,
PrA(i) is the single-pixel anomaly probability—the probability that speckle and shot-noise effects
combine to yield a range measurement that is more than one range resolution cell from the
true range, assumed to be the same for all pixels. The ﬁrst term in the product represents the
local Gaussian range behavior for a pixel given that it is not anomalous. The second weights
the anomolous behavior with uniform probability over the entire range-uncertainty interval
Rmin ≤ID, ID ≤Rmax modeled to include the true range ID and to have an extent, R = Rmax−Rmin,
that is much larger than the local accuracy σL. As established in [312] for a LADAR the range
resolution Rres is roughly cT/2 for a pulse duration of T seconds (c = speed of light) and the
number of range resolution bins N = R/Rres, with the local range accuracy and anomaly
probability obeying σL(yi) = Rres/

CNR(yi) and PrA(yi) ≈(1/CNR(yi))(ln(N) −1/N + 0.577),
respectively.37
Example 13.10 (LADAR)
Examine experimental results on mean-squared perfor-
mance in LADAR. Shown in Figure 13.11 are results from the LADAR simulation.
The template is the tank Iα = TANK, placed at arbitrary orientations. The imaging
model is TL the projective transformation, with the observed image data ID with
mean TLIα(g), g ∈SO(2) a random rotation uniformly distributed. The conditional
density p(ID|Iα(g)) is given by Eqn. 13.36. Figure 13.11 shows results from comput-
ing the pose bound computed over a range of CNRs. Panel 4 shows the mean, panel
5 shows the LADAR signal ID with mean TIα(g) and conditional density obeying
Eqn. 13.36. Panel 6 shows HSB mean-squared error performance for pose as a function
of CNR.
13.3.4 3D to 2D Poisson Projection Model
CCD detectors in forward looking infrared imaging (FLIR) correspond to projective transforma-
tions from R3 to R2, T : I(x), x ∈R3 →TI(y), y ∈R2(Z2) corresponding to discrete lattices with
image noise growing as signal mean. Thus the Poisson noise models of Snyder [31, 32] become
appropriate assuming the measured 2D image ﬁeld ID is Poisson distributed with mean intensity
TIα(g) the projection on the detector plane [32,317]. Deﬁning 1 the all 1′s vector over the imaging
lattice y ∈Y, then the density takes the form [42]
p(ID|Iα(g)) = e−⟨1,TIα(g)⟩+⟨log TIα(g),ID⟩
$N
i=1 ID(yi)!
,
(13.37)
37 Following Shapiro [313–315] the CNR for LADAR is the ratio of the average radar-return power and
average local-oscillator shot noise power given by the resolved speckle-reﬂector radar equation [316] according
to CNR(i) = νe−2αI(i) where α is the atmospheric extinction coefﬁcient and ν is a constant derived from the
properties of the laser radar. Notice that the probability of anomaly PrA(i) and the local range accuracy σ(i)
increase with hypothesized distance to the target.

394
13 ESTIMATION BOUNDS
(1)
(2)
(3)
(4)
(5)
Figure 13.12 Left Half: Panel 1 shows M2 tanks through perspective projection; panel 2 shows
the scene in Poisson noise. Panel 3 shows scene with Gaussian blur; panel 4 shows Poisson noise.
Right Half: Panel 5 shows the log-likelihood of the data on the pixel array for the two matches
log p(ID(y)|I(g)). Brightness means higher relative log-probability.
where ⟨1, TIα(g)⟩=

yi∈Y
TIα(yi),
⟨log TIα(g), ID⟩=

yi∈Y
log TIα(g)(yi)ID(yi).
(13.38)
Note that the discrete lattice has dimension N = |Y|. Shown in Figure 13.12 are examples of
imagery corresponding to projective imagery via CCD detection. Panel 1 shows M2 tanks through
perspective projection; panel 2 shows the random ﬁeld corresponding to Poisson noise.
Generally the imaging device contains optics in the focal plane resulting in point-spread p(·)
(see [31, 32], e.g.). Then the mean ﬁeld in the above models gets adjusted to

Y p(y −z)TI(z)dz.
Panels 3,4 of Figure 13.12 depict such point spread measurements. Panel 3 shows perspective
projection with Gaussian blur in the optics. Panel 4 shows the random ﬁeld corresponding to
Poisson noise.
The right half of Figure 13.12 illustrates how the conditional density p(ID|I(g)) encodes
the information about the hypothesized objects. Panel 5 shows the difference in log-likelihood
probability density (plotted in gray scale). Bright pixels correspond to high evidence of object
classiﬁcation. The relative probability corresponds to an exponentiation of the integral of the
brightness pictures. The top choice M60 is far more favorable because of the bright log-probability
values.
Example 13.11 (FLIR)
Examine results on MSE bounds for CCD array correspond-
ing to FLIR imagers. Second generation FLIR imagers sense the temperature proﬁle
of the object via CCD detection. The infrared (IR) passive sensor performs direct
detection of the thermal radiation generated by the spatially-resolved object, with the
passive-channel photodetector followed by a low pass ﬁlter integrating the resulting
photocurrent over the pixel dwell time Td. The integrated intensity measured in each
pixel consists of the radiation power and shot-noise-plus-thermal-noise [318]. Repre-
sent the ﬁrst two moments of the Poisson process corresponding to the mean following
the variance. Assuming the constant offset due to dark current has been subtracted

13.3 MMSE ESTIMATORS
395
Unconditional HSB performance curve for FLIR
0.4
25
23
21
19
16
13
9
0
0.35
0.25
0.15
0.05
0–40
–30
–20
–35
–25
–15
–10
–5
0
5
10
SNR (dB)
MSL
Equivalent estimation error in degrees
0.1
0.3
0.2
FLIR
(1)
(2)
(3)
Figure 13.13 FLIR: Panels show results from the FLIR experiments. Panel 1 shows the mean, and
panel 2 shows the signal in noise. Panel 3 shows the HSB mean-squared error performance as a
function of SNR (see also Plate 23).
from the photocurrent, then ID is Gaussian with mean ﬁeld TFIα(g) according to
p(ID|Iα(g)) =


NF

i=1
1

2πσ 2
F(yi)

e−∥ID−TFIα(g)∥2/2σ 2
F ∥,
(13.39)
where the dimension of the discrete CCD lattice is taken as NF = |Y|.38
Shown in Figure 13.13 are results from the FLIR experiments. The template is
the tank Iα = TANK, placed at arbitrary orientations. Then TF is the perspective
transformation of the FLIR imaging optics, with the observed image data ID with
mean TFIα(g), g ∈SO(2) a random rotation uniformly distributed. The conditional
density p(ID|I(g)) is given by Eqn. 13.39. Panel 1 shows the mean, and panel 2 shows
the signal in noise. Panel 3 shows the HSB mean-squared error performance as a
function of SNR (see text for deﬁnition).
13.3.5 3D to 1D Projections
There are numerous examples of projective transformations from R3 to R1, T : I(x), x ∈R3 →
TI(y), y ∈R1(Z1). One example is high range-resolution radar (HRR) illuminating the object
by a sequence of radar pulses with the received echoes collected at the receiver. The radar cross
section (RCS) (effective echoing area of the object) results in reradiation towards the radar receiver.
The return signal is ﬁltered through a bandpass ﬁlter tuned at the center frequency fc and then
sampled. The resulting range proﬁles are the discrete Fourier transforms of the ﬁltered return
38 The SNR for FLIR is deﬁned as in [318] and is determined by the noise-equivalent differential temperature
NET which for hν ≫kTS (see [318]) is given approximately by SNR = (T/NET)2 where
NET ≈kT2s
iD
F,s
-
2Bp
ηhν (iD
F,s + Pd + Ptherm),
(13.40)
k is Boltzmann’s constant, Ts is the absolute temperature of the source (object or background), ν is the passive
channel’s center frequency, Pd = Idhν/ηq is the dark current equivalent power for mean dark current Id, q is
the electron charge, and Ptherm is the thermal-noise equivalent power, Ptherm = 2kTLhν/ηq2RL, with TL being
the absolute temperature and RL the resistance of the photodetector’s load, Pb is the nominal radiation power
in the ﬁeld of view with P = Pt −Pb the signal power present in a object pixel with differential temperature
T = Tt −Tb, and NET is the temperature difference which produces unity SNR.

396
13 ESTIMATION BOUNDS
40
30
20
10
–10
–20
–30
–40
–450
–360
–270
–180
–90
90
Down range (Inch)
dBsM
180
270
360
450
0
–450
–360
–270
–180
–90
90
Down range (Inch)
180
270
360
450
0
0
–50
–40
–30
–20
–10
0
–60
–70
–80
dBsM
(1)
(2)
(3)
(4)
Figure 13.14 Azimuth-elevation power spectrum of the X-29 at several poses. Panels 1,3 show
renderings of the object; panels 2,4 show high-resolution radar proﬁles generated from XPATCH
for the X-29 targets at their respective orientations.
signal [319]. The observed range proﬁle ID(t) is modelled as a complex Gaussian random process
with conditional mean and covariance TRIα(g), KR(g), respectively.
Shown in Figure 13.14 are results of azimuth-elevation power spectra of the X-29 object
at several pose positions generated via the XPATCH simulator (see [300, 301]). Panels 1,3 show
renderings of the object; panels 2,4 show high-resolution radar proﬁles generated from XPATCH
for the X-29 targets at their respective orientations.
The wide-sense-stationary uncorrelated-scatter (WSSUS) model for diffuse radar objects
[320] assumes that returns from different delays are statistically uncorrelated. In additive complex
white Gaussian noise, the magnitude of the complex envelope for the observed signal is Rice
distributed. Let ID be the random range proﬁle power with mean target RCS proﬁle Iα(g) produced
by the HRR, then the likelihood function takes the form
p(ID|Iα(g))
=
NR

i=1
J0

2

ID(yi)TRIα(g)(yi)(1 −β)
TRIα(g)(yi)β + σ 2
R

e((−ID(yi)+TRIα(g)(yi)(1−β))/(TRIα(g)(yi)β+σ 2
R))
TRIα(g)(yi)β + σ 2
R
,
(13.41)
where J0 is the zeroth-order modiﬁed Bessel function of the ﬁrst kind.39
39 The SNR deﬁned for HRR is the ratio of average object cross-section to noise-equivalent cross-section
given by
SNR =
1
NL
NL

l=1
|m(gref, yi)|2 + σ 2
R(gref, yi)
σ 2
R
,
(13.42)
where 89◦is the reference angle chosen so that the mean reference object cross-sectional area m(gref) is
maximum.

13.3 MMSE ESTIMATORS
397
0
20
40
60
80
100
120
140
160
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
Range Bins
Range Profile Magnitude (m2)
Range profile for a t1 tank at 60° azimuth
0
20
40
60
80
100
120
140
160
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
Range Bins
Range Profile Magnitude (m2)
Range profile for a t1 tank at 60° azimuth
10 dB
Unconditional HSB performance curve for HRR
0.4
25
23
21
19
16
13
9
0
0.35
0.25
0.15
0.05
0
–30
–20
–20
10
20
30
0
40
50
SNR (dB)
MSL
Equivalent Estimation Error in Degrees
0.1
0.3
0.2
HRR
(1)
(2)
(3)
Figure 13.15 Panels 1 and 2 show the HRR range proﬁle mean-ﬁelds and samples with noise for a
T1 tank at 60◦. Panel 3 shows the HSB mean-squared error performance as a function of SNR (see
text for deﬁnition).
Example 13.12 (HRR)
Examine experimental results on the MSE performance as
measured by the HS bounds for HRR. Shown in Figure 13.15 are results from the
HRR study. The imaging model is TR the ranging transformation of the HRR device
with the observed image data ID with mean TRIα(g), g ∈SO(2) a random rotation
uniformly distributed. The conditional density p(ID|Iα(g)) is given by Eqn. 13.41. Pan-
els 1 and 2 show the HRR range proﬁle mean-ﬁelds and samples with noise. Panel
3 shows the HSB mean-squared error performance as a function of SNR (see text for
deﬁnition).
13.3.6 3D(2D) to 3D(2D) Medical Imaging Registration
Many modern high resolution imagers provide fundamental 3D information of the underlying
scenes. MR imaging technology is one such example, producing highly detailed representations of
the 3D geometry, providing transformations from R3(R2) to R3(R2), T : I(x), x ∈Rd →TI(y), y ∈
Rd(Zd), d = 2, 3. TI represents various physical properties of the tissue including the proton
density, T1, or T2 time constants of spin–spin decay and spin-lattic decay.
Image registration associated with estimating parameters in the special Euclidean group is
an area of tremendous research in the Medical Imaging context. Model the imagery ID ∈ID as
a conditionally Gaussian ﬁeld with mean the MRI brain volumes of the underlying true brain Iα.
Then ID is a conditionally Gaussian random ﬁeld conditioned on Iα(g), g ∈SE(2, 3), with the orbit
of images becoming I = {Iα(g), g ∈SE(2)}. The conditional density takes the form
p(ID|Iα(g)) =
1
(2πσ 2)
N
2
e−∥ID−Iα(g)∥2/2σ 2,
(13.43)
where the number of voxels N of the template and the measured data are the same.
Example 13.13 (2D to 2D MRI Registration (Costello [305]))
Shown in Figure 13.16
are results from registration in the special Euclidean group from 2D MRI imagery.
The top row shows 2D imagery from T2 weighted MRI data collected by Dr. Scott
Nadel at Duke University. Panel 1 shows the 2D template image section Iα; panel 2
shows the same section rotated and translated with additive noise ID = Iα(g) + W.
The imagery is modelled ID as a conditionally Gaussian process with mean the brain
section in panel 1 randomly rotated and translated in noise element of the orbit of
images ID ∈ID = {Iα(g)+W}. Shown in the bottom row panel 3, 4 are results of com-
puting the mean-squared error performance in estimating the unknown rotations and

398
13 ESTIMATION BOUNDS
0–30
–28
–26
–24
–22
–20
–18
–16
–14
–12
0.5
1
1.5
2
2.5
SNR(dB)
Hilbert–Schmidt norm
Hilbert–Schmidt norm
c 
b 
0
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
SNR(dB)
b 
c 
(1)
(2)
(3)
(4)
–30
–25
–20
–15
–10
–5
Figure 13.16 Top row: Panels 1, 2 show the template section Iα MRI image, and the image after
rotation and translation with noise ID = Iα ◦g + W. Bottom row: Panels 3, 4 show the HSB for
rotation, and rotation plus translation SE(2) error.
translations as a function of noise level. Panels 3, 4 show the HSB for rotation (panel
3), and rotation plus translation (panel 4) SE(2) error as a function of noise SNR.
Figure 13.17 examines the sensitivity of the performance of the HS minimum-
mean-squared error estimator on the accuracy of the assumed template model. Panel
1 shows the MRI section taken as the template. Panels 2, 3 show the mean ﬁelds
used to generate the orbit of actual imagery ID ∈ID. Panel 2 duplicates an inhomo-
geneity artifact; panel 3 presents a tumor artifact. Panel 4 shows the degradation in
performance of the HSB estimator resulting from the model mismatch attributable to
the inacurrate template choices. The solid line shows the MSE performance as mea-
sured by the HSB for the true template (panel 1) matched to the orbit of imagery
generated from the template ID = {IPANEL1(g) + W}. The dot–dashed line shows
the HSB for the orbit generated from the tumor ID = {Itumor(g) + W} but matched
with Iα = PANEL1. The dotted line shows the HSB for the orbit generated from the
inhomogeneity ID = {Iinhomo(g) + W} but matched with Iα = PANEL1.
13.4 Parameter Estimation and Fisher Information
We should expect that in the asymptotic consistency setting the matrix of second derivatives of
the likelihood function determines the variability of estimation. As well, it will turn out that it will
inﬂuence the exponential rate of performance for hypothesis testing, linking parameter estimation
and identiﬁcation and recognition.

13.4 PARAMETER ESTIMATION AND FISHER INFORMATION
399
0–30
–25
–20
SNR (dB)
–15
–10
–5
0.5
1
1.5
2
2.5
3
3.5
Hilbert–Schmidt norm
original 
inhomogeneity 
tumor 
(1)
(2)
(3)
(4)
Figure 13.17 Panel 1 shows the T2 weighted MR image; panel 2 shows the image with a simulated
tumor. Panel 3 shows the image with simulated inhomogeneity. Panel 4 shows the HSB bounds
for rotation error for the template in panel 1 matched to the tumor and inhomogeneity data.
To calculate the Fisher information matrix the ﬁrst and second derivatives of the log-density
must be calculated as given by the following theorem. Throughout we assume the conditional
densities are sufﬁciently smooth so that the derivatives for the Fisher information are well deﬁned.
Theorem 13.14
Given is the orbit of ideal images I(g) ∈I, g ∈G a ﬁnite-dimensional
group with local parameterization g : θ ∈Rd →g ∈G of dimension d observed through
generalized projection T with data ID a conditional random ﬁeld with mean ﬁeld TI(g).
For ID a conditional Gaussian random ﬁeld with conditional density
p(ID|Iα(g)) =
1
(2πσ 2)N/2 e−(1/2σ 2)∥ID−TIα(g)∥2,
(13.44)
then the d×d Fisher information matrix and its sensitivity integrals (Fkj, k, j = 1, . . . , d)
deﬁning the d × d Fisher information matrix
F = [Fkj],
Fkj = 1
σ 2

∂
∂θk
TIα(g), ∂
∂θj
TIα(g)

.
(13.45)
For ID a Poisson ﬁeld with conditional density
p(ID|Iα(g)) = e−⟨1,TIα(g)⟩+
D
log TIα(g),IDE
$N
i=1 ID(yi)!
,
(13.46)

400
13 ESTIMATION BOUNDS
the Fisher information is given by
Fkl =

∂/∂θkTIα(g)

TIα(g)
,
∂/∂θjTIα(g)

TIα(g)

.
(13.47)
Note the similarity between the two cases. In the Poisson setting the variance in each pixel
is the normalizer rather than a single ﬁxed constant.
Proof
To calculate the Fisher information matrix (Eqn. 2.113 of deﬁnition 2.40) the
ﬁrst and second derivatives of the log-likelihood must be calculated:
∂
∂θk
log p(ID|Iα(g)) = 1
σ 2

ID −TIα(g), ∂
∂θk
TIα(g)

(13.48)
∂2
∂θk∂θj
log p(ID|Iα(g)) = 1
σ 2

ID −TIα(g),
∂2
∂θk∂θj
TIα(g)

−1
σ 2

∂
∂θj
TIα(g), ∂
∂θk
TIα(g)

.
(13.49)
Taking the negative expectation obtains the Fisher information.
In the Poisson case, deﬁne 1(·) to be all 1’s function over the detector array. The
log-probability becomes
log p(ID|Iα(g)) = −
D
1, TIα(g)
E
+
2
ID, log TIα(g)
3
+ constant.
(13.50)
Then the terms determining the Fisher information become
∂
∂θk
log p(ID|Iα(g) = −

1, ∂
∂θk
TIα(g)

+

ID, (∂/∂θk)TIα(g)
TIα(g)

,
(13.51)
∂2
∂θk∂θj
log p(ID|Iα(g) = −

1,
∂2
∂θk∂θj
TIα(g)

+
2
ID, (∂2/∂θk∂θj)TIα(g)/TIα(g)
−(∂/∂θk)TIα(g)(∂/∂θj)TIα(g)/(TIα(g))23
.
(13.52)
Taking the negative expectation gives
Fkl =

1,
∂2
∂θk∂θj
TIα(g)

−

1,
∂2
∂θk∂θjTIα(g)

+

1,
(∂/∂θk)TIα(g)(∂/∂θj)TIα(g)
TIα(g)

(13.53)
=

(∂/∂θk)TIα(g)

TIα(g)
,
(∂/∂θj)TIα(g)

TIα(g)

.
(13.54)
Example 13.15 (Fisher Information Across Orientation)
The mean squared error is
bounded below by the inverse Fisher information (see Theorem 2.42, Chapter 2). For
pose, choose the local coordinates for the axis ﬁxed rotation as θ, then the mean-
squared error is lower bounded according to
E{( ˆθ(ID) −θ)2} ≥F−1 = −

E
C
∂2 log p(ID|θ)
∂θ2
F−1
(13.55)
=
σ 2
∥(∂/∂θ)TIα(θ)∥2 .
(13.56)

13.4 PARAMETER ESTIMATION AND FISHER INFORMATION
401
Orientation = 242 deg
Dictionary tank
10
20
30
40
50
60
10
20
30
40
50
60
0
5
10
15
20
25
30
35
40
45
50
Orientation = 275 deg
Dictionary tank
10
20
30
40
50
60
10
20
30
40
50
60
0
2
4
6
8
10
12
14
16
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
CRB
HSB
(1)
(3)
(2)
Figure 13.18 Panels 1,2 show tanks at various brightnesses and orientations. Panel 3 shows
1/2HSB(θ, σ) for a set of θ (x-axis) versus σ 2/∥∂θTI(θ)∥2 (y-axis) (see also plate 24).
To compare the mean-squared error of the angle estimator to the HSB , note that
∥O(θ) −ˆO(θ)∥2
Rn×n = tr(O −ˆO(θ))(O −ˆO(θ))∗
= 4 −4 cos(θ −ˆθ)
(13.57)
= 2(θ −ˆθ)2 + o((θ −ˆθ)2).
(13.58)
Thus, for σ 2 →0 , then 1
2HSB ∼E|θ −ˆθ|2.
Figure 13.18 examines use of the inverse Fisher information bound to predict
the mean-squared error performance as measured by the Hilbert–Schmidt estimator.
Panels 1,2 show examples of vehicles observed via a FLIR imager sensing the temper-
ature proﬁle on the tanks. Panel 3 shows a plot of 1
2HSB plotted versus the inverse
Fisher information F−1 = (σ 2/∥(∂/∂θ)TI(θ)∥2). The superimposed lines are the per-
formance plots for many different orientations of the tank. Asymptotically in low
noise, the inverse Fisher information and mean-squared error measured by 1/2HSB
are identical. They diverge at high error rates.
Example 13.16 (Fisher Information as a Function of Scale)
Shown in Figure 13.19
are results of such an analysis as a function of scale and number of pixels on target.
Rows 1, 2 show images with 1000 and 250 pixels on target as a function of noise
standard deviation. The bottom row shows MSE performance for pose as measured
by HSB/2 versus inverse Fisher information σ 2/∥TI(θ)∥as a function of noise σ for
100, 500, 1000 pixels on target. The solid line shows the 1
2HSB curve; the dashed line
shows the MSE as measured by E(θ −ˆθ)2. Note the divergence at high noise because
the Taylor series approximation diverges for the fourth order term in 4−4 cos θ . Note
the near linear relation.

402
13 ESTIMATION BOUNDS
1000 PIXELS ON TARGET    NOISE STD = 0.1
1000 PIXELS ON TARGET    NOISE STD = 0.3
1000 PIXELS ON TARGET    NOISE STD = 0.5
250 PIXELS ON TARGET    NOISE STD = 0.1
250 PIXELS ON TARGET    NOISE STD = 0.3
250 PIXELS ON TARGET    NOISE STD = 0.5
0
0.002
0.004
0.006
0.008
0.01
0.012
0.014
0
0.005
0.01
0.015
0.02
0.025
INVERSE OF THE FISHER INFORMATION
4.05
5.73
7.02
8.1
0.46
0.42
0.38
0.33
0.27
0.19
HSB/2 , MSE 
250 Pixels On Target
ERROR IN DEGREES
(std)
0
0.5
1
1.5
2
2.5
3
3.5
4
x 10-3
0
1
2
3
4
5
6
INVERSE OF THE FISHER INFORMATION
(std)
0.48
0.45
0.41
0.36
0.32
0.26
0.18
1.82
2.56
3.14
3.62
4.05
HSB/2 , MSE 
ERROR IN DEGREES
500 Pixels On Target
0
0.2
0.4
0.6
0.8
1
1.2
1.4
x 10-3
x 10-3
x 10-3
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
INVERSE OF THE FISHER INFORMATION
0.49
0.45 
0.4
0.35 
0.28 
0.2
(std) 
1.15
0.81
1.40
1.62
1.81
1.98
2.14
HSB/2 , MSE 
1000 Pixels On Target
ERROR IN DEGREES
HSB/2 
MSE   
FI-1
HSB/2 
MSE   
FI-1
HSB/2 
MSE   
FI-1
Figure 13.19 Top and middle rows show images from 1000 and 250 pixels on target as a function of
σ = 0.1, 0.3, 0.5 (columns 1, 2, 3), respectively. Bottom row shows pose performance as measured
by HSB/2 versus noise σ 2/∥TIα(θ)∥showing the three curves for 100, 500, 1000 pixels on target.
The solid line shows the 1
2HSB; the dashed line shows the MSE as given by E|θ −ˆθ|2. Note the near
linear relation.
Example 13.17 (Vehicle and Brain registration (Bitouk [321]))
ShowninFigure13.20
are results exploring the relationship between the variance of the estimator and the
asymptotic prediction of the inverse Fisher information. Theorem 2.42 predicts that
the MSE is lower bounded by the inverse Fisher information. Model the data ID as
conditional Gaussian random ﬁeld with mean ﬁeld Iα(g). Columns 1 and 2 show
imagery of the vehicle and brain at varying signal to noise (SNR) ratios SNR = −10, 0,
10 dB, respectively. Columns 3, 4 explore the dependence of the pose variability in the
posterior distribution as a function of the Fisher information. Top panels shows the
MSE E(θ −ˆθ)2 as a function of the inverse Fisher Information. Over a broad range of
SNRs there is agreement between the standard deviation of object pose with the Fisher
information. Bottom row shows the square root of the the inverse Fisher Information
plotted versus half the Hilbert–Schmidt error. At low noise these should be equal.
Similar results are shown in columns 2,4 of Figure 13.20 from Bitouk [321] for MRI
brain registration.
13.5 Bayesian Fusion of Information
The importance of the conceptual separation of the source of possible images I with prior density
π(Iα(g)), Iα(g) ∈I and the channel with transition law p(ID|Iα(g)) is that there is only one true
underlying scene, irrespective of the number of sensor measurements forming ID = (ID1, ID2, . . . ).

13.5 BAYESIAN FUSION OF INFORMATION
403
Figure 13.20 Columns 1, 2 and 3 show imagery of the vehicle (top row) and brain (bottom row)
as a function of increasing SNR SNR = −10, 0, 10 dB. Column 4: panels show 1
2 HSB versus inverse
Fisher information F−1; dots show the MSE.
For ID1, ID2, . . . conditionally independent given Iα(g) = (I1(g), I2(g) . . . , ), the conditional density
takes the form
p(ID|Iα(g)) =
n

i=1
p(IDi|Iα(g)).
(13.59)
Only one inference problem is solved, with the multiple observations due to multiple sensors
viewed as providing additional information in the posterior distribution. Sensor fusion occurs auto-
matically in this framework. Certainly, in calculating information gain this view determines these
ultimate bounds.
This allows us to prove, for example, that Fisher information is monotonic in a number of
sensors. Therefore, mean-squared error bounds have to improve with consistent combination of
sensing information.
Theorem 13.18
Let ID1, ID2, . . . be conditionally independent random ﬁelds conditioned
on the mean ﬁeld Iα(g). Then the Fisher information is monotonic in the sensors given by the
superposition of each of the Fisher informations:
F(ID1, . . . , IDn; Iα(g)) =
n

i=1
F(IDi; Iα(g)) .
(13.60)
The covariance lower bound is monotonically decreasing in number of sensors (see Theo-
rem 2.42):
Cov(ˆIα(g); (ID1, . . . , IDn)) ≥


n

i=1
F(IDi; Iα(g))


−1
.
(13.61)
Proof
Proof follows directly from Bayes factorization of the conditional density
Eqn. 13.59.

404
13 ESTIMATION BOUNDS
Example 13.19 (Fusion: LADAR, FLIR, HRR)
Examine the scenario studied by
Shapiro et al. [322, 323] for the multiple sensor suite of MIT Lincoln Laboratory’s
Infrared Airborne Radar (IRAR) Program [324]. The sensors were selected a range of
2.0 km with a long-wave infrared laser radar (LADAR), a forward looking infrared
(FLIR) and a high-resolution radar (HRR). The LADAR and FLIR imaging subsystems
share common optics and are pixel-registered sensing ﬁxed radiant intensities, and
ranges; the HRR measures 1D cross sectional reﬂectance proﬁles.
The Bayesian formulation readily incorporates additional sensors into the infer-
ence. Model the sensors as conditionally independent observations of the scene, the
product of each of the individual conditional probability terms forms the joint density
for all available observations. The sensors are assumed conditionally independent so
that the joint density on the sensor data is the product
p(IDF, IDL, IDR|Iα(g)) = p(IDF | Iα(g) )p(IDL | Iα(g) )p(IDR | Iα(g) ).
(13.62)
Sensors have different signature representations which must be reﬂected in the con-
ditional probability functions. The parameters selected for the sensors were selected
to model the MIT Lincoln Laboratory Infrared Airborne Radar (IRAR) Program and
the URISD dataset parameters.40
Figure13.21depictstheperformanceofthemultiplesensorsFLIR,LADAR,HRR
operating together at various carrier-to-noise ratios (CNR) and signal-to-noise ratios
(SNR). For each of the sensors, the MSE increases as CNR or SNR decreases. Sensors
optimally fused outperform either sensor taken individually. The performance gain
that accrues from sensor fusion can be visualized as the decrease of SNRs or CNR
required to achieve the chosen performance level. Figure 13.21 depicts this. Panels
1–3 show (CNR, SNR) requirements needed to realize a ﬁxed MSE value of 0.05 (pose
estimation error of 9◦). The horizontal straight line in panel 1 is the HRR SNR required
to achieve the selected pose accuracy from the HRR sensor alone; the vertical straight
line is the LADAR CNR required for the active LADAR channel alone to achieve
the selected pose accuracy. The solid fusion curve shows the (CNR, SNR) required
to realize the pose performance when the HRR and LADAR outputs are optimally
9
10
11
12
13
14
15
16
17
0
2
4
6
8
10
12
LADAR CNR (dB)
HRR SNR (db)
HSB Performance curve for 9 degrees of error
8
9
10
11
12
13
14
15
16
17
–20
–19
–18
–17
–16
–15
–14
–13
–12
–11
–10
LADAR CNR (dB)
FLIR SNR (db)
HSB Performance curve for 9 degrees of error
0
2
4
6
8
10
12
–24
–22
–20
–18
–16
–14
–12
–10
HRR SNR (dB)
FLIR SNR (db)
HSB Performance curve for 9 degrees of error
FLIR = 17 dB
FLIR = –∞
HRR = 2 dB
HRR = –∞
LADAR = 12 dB
LADAR = –∞
(1)
(2)
(3)
Figure 13.21 HRR,FLIR,LADAR FUSION: Panels show the performance-bound curves for 9◦root-
mean-square pose estimation error for LADAR, HRR (panel 1), LADAR,FLIR (panel 2), FLIR,HRR
(panel 3).
40 The FLIR and LADAR parameters selected are receiver aperture dimension of 13 cm, receiver ﬁeld-of-
view φR = 2.4 mrad, detector quantum efﬁciency 0.25 and atmospheric extinction coefﬁcient 0.5 dB/km. The
FLIR noise-equivalent temperature NET = 0.1 K. The LADAR parameters are average transmitter power of
2 W, pulse repetition frequency of 20 kHz, pulse duration of 200 nsec, peak transmitter power of 500 W, photon
energy 1.87 × 10−20 J, IF ﬁlter bandwidth 20 MHz, range resolution 30 m, number of range bins 256. The high
resolution radar has parameters center frequency 1.5 GHz, bandwidth 739.5 MHz, frequency sample spacing
4.921 MHz, number of frequency samples 151, azimuth angle spacing 0.15◦, number of azimuth samples, 2401.

13.5 BAYESIAN FUSION OF INFORMATION
405
combined. The dashed fusion curve shows the (CNR, SNR) values required to realize
the same pose performance with the FLIR operating at SNR = −17 dB.
Panels 2, 3 are similar for the LADAR, FLIR combination and FLIR, HRR, respec-
tively. The dashed curves correspond to adding the third sensor which shifts the entire
performance.
13.6 Asymptotic Consistency of Inference and Symmetry Groups
13.6.1 Consistency
We now examine asymptotic consistency and the role of the Fisher information matrix. We examine
the similar setting as in Chapter 2, 2.60 in which consistency is obtained through increasing sample
size. Since we study the i.i.d. Gaussian setting here, we explicitly parameterize sample size through
variance, i.e. as n →∞, σ 2(n) →0. Throughout the projective imaging model is assumed in
Gaussian noise, with ID a Gaussian random ﬁeld with mean TIα(g), and independent variances
σ 2. The conditional data density becomes
p(ID|Iα(g), α) =
1
(2πσ 2)N/2 e−1/2σ 2∥ID−TIα(g)∥2.
(13.63)
For the asymptotic study of parameter estimation and identiﬁcation we shall work in the local
coordinates of the group since the density will concentrate on a particular local coordinate chart.
Deﬁne the coordinate frames to be ∂/∂gi, i = 1, . . . , d the dimension of the transformation group G.
Consistency implies that when the signal-to-noise ratio increases, the estimates restrict to a
family of parameters representing the true underlying objects. The sensor map T is, in general,
a many-to-one transformation from object occurrences Iα(g) to the observed image ID. Multiple
object occurrences can map into the same observed image because (i) the object may have inherent
symmetries so that at different pose it leads to the same image at the sensor, and (ii) the features
distinguishing different poses may be lost in the projective transformation constituting T. Deﬁne
the set of equivalent conﬁgurations through the sensor mapping as follows.
Deﬁnition 13.20
Deﬁne the set of equivalences through the sensor map associated with
object α0 at group parameter g0 as
M(g0, α0) = {(g, α) : TIα(g) = TIα0(g0)} ⊂G × A.
(13.64)
An assumption of consistency corresponds to |M(g0, α0)| = 1.
Any two pairs (g1, α1) and (g2, α2) are considered equivalent if TI1(g1) = TI2(g2), partition-
ing the set G × A into equivalence classes. The multiplicity of the equivalence set |M(g0, α0)| is
always at least one but can have any ﬁnite cardinality as well as that of the continuum. Given
an observed image, the estimation is restricted to the space of equivalence classes ∪s,αM(g, α).
In the case of multiple sensors, each possibly having a different sensor map T1, T2, . . . , Tk, let
T = (T1, T2, . . . , Tk) and the above deﬁnition holds.
If the object α⋆with the parameter g⋆is observed, then for σ 2 →0 consistency would require
that the inference leads to the set M(g⋆, α⋆). This is made precise by the following proposition.
Theorem 13.21
For any ﬁxed (α⋆, g⋆) ∈A × G, the support of the likelihood function
p(ID|Iα(g), α) as a function of g and α contracts as σ 2 ↓0 to the set M(g⋆, α⋆).
Proof
Consider the set
Gα(ǫ) = {g : ∥ID −TIα(g)∥2 < ǫ} ⊂G

406
13 ESTIMATION BOUNDS
for an arbitrary positive ǫ. Let fα : G →R be a bounded and continuous test function
which is zero inside Gα(ǫ), and evaluate

G
fα(g)p(ID|Iα(g), α)πα(g) ds =

Gα(ǫ)c fα(g)
1
(2πσ 2)N/2 e−1/2σ 2∥ID−TIα(g)∥2πα(g) ds.
(13.65)
This quantity goes to zero for σ 2 ↓0, for all ǫ > 0 ﬁxed. The limiting set lim
ǫ↓0 Gα(ǫ) =
{g : ∥ID −TIα(g)∥2 = 0}, and (
α({g : ID = Iα(g)}, α) = M(g⋆, α⋆).
One consequence of this result is that the MLE converges to the equivalence-class associated
with the true parameter g⋆α, α⋆. Accordingly the limiting probability concentrates on the proper
equivalence class and it follows that representation can be estimated consistently only if each
pair (g, α) has multiplicity one; otherwise the results are accurate only up to the set of equivalent
conﬁgurations.
13.6.2 Symmetry Groups and Sensor Symmetry
Object symmetries are a basic issue in the studies of patterns. Most man-made objects have well-
deﬁnedsymmetries. Thestandardnotionofsymmetryofaobjectα isstudiedthroughthesubgroup
Gsym ⊂G of the original group G the parameter space, with the property that the symmetry
subgroup is the set such that the imagery looks identical through the transformation:
Gsym = {g ∈G : Iα(g) = I} ⊂G.
(13.66)
The inference set is then reduced to the quotient space G/Gsym. This corresponds to the sym-
metries in underlying object shapes (in R3) but, in practice, observations are further degraded
by the sensors which capture the object features only partially. These become statistical issues of
identiﬁability, requiring the concepts of symmetry to be extended to the power and limitations of
the sensor. The concept of symmetry is extended to include identiﬁability through the sensor as
follows.
Deﬁnition 13.22
The symmetry set relative to sensor mapping T for object α is
deﬁned to be
GT(α) = {g : TIα(g) = TIα}.
(13.67)
For multiple objects indexed over A, the notion of a symmetry set of object α0 at
group parameter g0, GT(g0, α0) ⊂G × A, deﬁned as
GT(g0, α0) = {(g, α) : TIα(g) = TIα0(g0)}.
(13.68)
This divides the set G × A into equivalence classes.
To illustrate this concept of sensor induced symmetry let us consider various examples
assuming the video sensor with G = SO(3). If α = missile of ideal cylindrical form with no other
distinguishing features then S[missile] = SO(2). Viewed via a video sensor, ST[missile] = SO(2),
which is to be a subgroup of S leading to the inference space, SO(3)/SO(2) = S2. If the missile
also has four tail ﬁns separated by the angle 90◦the relative symmetry set instead includes the
discrete 4-torus.

13.6 ASYMPTOTIC CONSISTENCY
407
–200 –150 –100
–50
0
50
100
150
200
0
0.1
0.2
0.3
0.4
0.5
0.6
(1)
(2)
(3)
Figure 13.22 Panels 1, 2 show two different object orientations depicting near symmetry visu-
alized through the perspective projection of video imagery TI(g1)≈TI(g2). Panel 3 shows the
posterior density of the Gaussian projection model with a uniform prior on the orientation. Note
how they are virtually identical.
Assuming object signature TIα(g) of the object α at pose g viewed through a remote sensor
with projective mapping T. Deﬁne the quantity
Ŵ(i; j) = inf
gi,gj ∥TIαi(gi) −TIαj(gj)∥2,
(13.69)
quantifying the fundamental separation in objects α1 and α2 through the sensor map T. As we
shall see in the subsequent sections, asymptotic in small noise the probability of misidentiﬁcation
of objects is exponential with decay rate proportional to Ŵ(αi, αj) and inversely proportion to the
variance.
This is illustrated by a ﬁgure from Srivastava [302] which depicted the posterior density
for multiple views. Figure 13.22 depicts via panels 1,2 two orientations of a vehicle which is
viewed through an orthographic projection of the video camera. In this low-noise situation, the
approximate equivalence of object proﬁles is depicted in panels 1,2 at the two orientations −30◦
and 30◦(0◦being exact broadside) and results in the concentration of the posterior measure at
these two points. Panel 3 shows that the posterior distribution is concentrated at these two near
symmetry points.
13.7 Hypothesis Testing and Asymptotic Error-Exponents
Probability of detection, false alarm, and Bayes risk are the measures which can be calculated for
detection/ID. In a Bayesian approach, object detection and identiﬁcation are instances of discrete
hypothesis testing, given H the family of hypotheses, perform least risk estimation. Deﬁne the
risk functions R : H × H →R+. The detection/recognition performance is speciﬁed through the
variations of error probabilities, such as false alarm, missed detection, and misclassiﬁcation, as a
function of system parameters. The risk is given by
R = Ep(H,ID)R(H, ID).
(13.70)
In a Bayesian approach, object detection and recognition are performed via hypothesis-
testing while the unknowns, such as object pose, location, thermal proﬁle, etc., are treated as
nuisance parameters. Recognition implies integrating out or estimating these nuisance parameters.
In most practical cases, the complicated integrands make the analytical evaluations difﬁcult. One

408
13 ESTIMATION BOUNDS
possibility is to evaluate MLEs of the nuisance parameters and perform generalized likelihood-
ratio tests, thus avoiding nuisance integrals. On the other hand, a Bayesian solution requires
evaluation of the nuisance integral either exactly or through numerical approximation. For approx-
imating, some commonly used techniques are (i) quadrature evaluation (e.g. Newton-Cotes’
method), (ii) Markov chain Monte-Carlo (e.g. Metropolis-Hasting’s method), and (iii) asymptoti-
cal analysis for approximations of the nuisance integral and use them to derive the probabilities of
success/failure. Using asymptotics and Laplace’s method [4,61] we will approximate these inte-
grals to study the limiting distributions of the likelihood-ratios and the test performance, similar
to techniques used in [60] for approximating integrals to evaluate Bayes’ methods.
For binary ID and detection in additive noise there are two object types associated with the
hypotheses H0, H1:
Detection :
ID =
TIα(g∗
1) + W
: H1
W
: H0
,
(13.71)
Identiﬁcation :
ID =
TI1(g∗
1) + W
: H1
TI0(g∗
0) + W
: H0
.
(13.72)
with W additive Gaussian noise and g∗
0, g∗
1 the true parameters for hypotheized objects type H0, H1.
For an observation ID ∈ID, a Gaussian random ﬁeld the Bayesian solution is given by the
hypothesis testing problem
p(H1|ID)
p(H0|ID)
H1
><
H0
1 or equivalently,
L(ID) = log p(ID|H1)
p(ID|H0)
H1
><
H0
log π(H0)
π(H1) = ν.
(13.73)
The two error types to be characerized in the binary hypothesis problem are deﬁned as
follows.
Deﬁnition 13.23
Deﬁne the false alarm rate to be given by the probability that H1 is
selected when H0 is true; deﬁne the missed detection rate to be given by the probability that
H0 is selected when H1 is true.
Calculating the error bounds for m > 2 object types is extended straightforwardly. Let Hi be
the hypothesis associated with the object type i, with data ID a conditionally Gaussian random ﬁeld
with mean ﬁeld TIi(gi). The Bayes estimate is given by the hypothesis with maximum a-posterior
probability,
arg max
Hi
p(Hi|ID) = arg max
Hi
log p(ID|Hi)π(Hi).
(13.74)
13.7.1 Analytical Representations of the Error Probabilities and the
Bayesian Information Criterion
The evaluation of these log-likelihoods ratios of Eqn. 13.73 involves integrating over the nuisance
parameters space. In most practical cases, the complicated integrand makes analytical evaluation
difﬁcult. Assume identiﬁability through the sensor map assuming the size of the equivalence set
to be |M(g, α)| = 1. We now demonstrate the fundamental interplay of parameter estimation
and identiﬁcation and detection showing that the effective signal to noise ratio and the Hessian
associated with the empirical Fisher information play the important role of determining the expo-
nential error rates for probability of detection/ID. Detectibility is determined by the effective
signal-to-noise ratio of object projection, and identiﬁability is determined by the cross-correlation
of these projections across different objects.

13.7 HYPOTHESIS TESTING
409
Consider the multihypothesis testing problem of determining model m = 1, 2, . . . from
random observations ID. This requires evaluation of the likehood ratio tests on the model given
the observations. For the deformable template setting, the density involves the random nuisance
parameters of pose g(θ) ∈G, θ ∈Rd the local coordinates of the pose, with some prior density
πm(g(θ)), θ ∈ ⊆Rd of dimension d, requiring the calculation of the so-called nuisance integral
for the conditional density for every m:
ˆm = arg max
m
π(m)p(ID
1 , . . . , ID
n |m)
(13.75)
= arg max
m

m
p(ID|g(θ), m)πm(g(θ)) dg(θ).
(13.76)
We follow the the approach taken in Srivastava [325] in which the integrand is approximated
via Laplace’s method of Chapter 2, Theorem 2.60.
We shall assume the smoothness conditions 2.59 guaranteeing asymptotic consistency,
essentially that there is only one maximizer of the likelihood. Then as n →∞with
p(ID
1 , . . . , ID
n |m) ∼p

ID
1 , . . . , ID
n | ¯θ, m
 2π
n
dm/2
det−1/2 Fm(n, ¯θ),
(13.77)
where ¯θ = arg max
θ∈m
p(ID
1 , . . . , ID
n |θ, m) is the MLE with respect to θ ∈m and Fm is the Hessian
empirical Fisher information.
Theorem 2.60 can be exploited for computing the nuisance integral involved in the cal-
culation of the asymptotics on likelihood ratio testing. We shall be examining the conditional
Gaussian model, ID = TIα(g) + W, W white noise variance σ 2. To denote the asymptotic conver-
gence, we make the variance a function of sample size n, so that as sample size n →∞, variance
σ 2(n) →0.
First exploit a basic Lemma demonstrating the link of performance in hypothesis testing to
the Hessian.
Lemma 13.24
Given is the Gaussian random process ID = TIα(g)+W, W white Gaussian
noise variance σ 2(n) with T : Iα(g) →TIα(g) satisfying the smoothness properties and
identiﬁability properties 1,2,3 of Theorem 2.60. Denoting the d × d Fisher information
matrix
Fα(g, σ 2(n)) =

1
σ 2(n)

∂
∂θi
TIα(g), ∂
∂θj
TIα(g)

,
(13.78)
and the true parameters under H0, H1 as I0(g∗
0), I1(g∗
1), respectively, then as the noise variance
σ 2(n) goes to zero then the likelihood ratio converges in probability:
L(ID) ∼−
∥ID −TI1(g∗
1)∥2
2σ 2(n)
+
∥ID −TI0(g∗
0)∥2)
2σ 2(n)
+ log
det1/2 F0(g∗
0, σ 2(n))
det1/2 F1(g∗
1, σ 2(n))
.
(13.79)

410
13 ESTIMATION BOUNDS
Proof
With σ 2(n) going to 0, Theorem 2.60 implies asymptotic as σ 2(n) →0 the
log-likelihood ratio goes as
L(ID) = log p(ID|H1)
p(ID|H0)
(13.80)
∼−∥ID −TI1(¯g1)∥2
2σ 2(n)
+ ∥ID −TI0(¯g0)∥2
2σ 2(n)
+ 1
2 log
det

(∂2/∂θi∂θj)∥ID −I(¯g0)∥2/2σ 2(n)

det

(∂2/∂θi∂θj)∥ID −I(¯g1)∥2/2σ 2(n)
.
(13.81)
Theorem 2.60 gives with the maximum likelihood estimates ¯g converging in probabil-
ity to g∗then for TI : G →R a three times differentiable function that TIi(¯gi) converges
in probability TIi(g∗
i ), thus
log
det

(∂2/∂θi∂θj)∥ID −I(¯g0)∥2/2σ 2(n)

det

(∂2/∂θi∂θj)∥ID −I(¯g1)∥2/2σ 2(n)
 = log
det F0(g∗
0, σ 2(n))
det F1(g∗
1, σ 2(n)) + Op(σ(n)).
(13.82)
Theorem 13.25 (Exponential Error Exponents (Srivastava [325]))
Assume ID is a
conditional Gaussian random ﬁeld mean ﬁeld TIα(g) with consistency |M(g, α)| = 1 and
identiﬁability and smoothness properties of Theorem 2.60. Under such conditions, and with
the noise variance going to zero σ 2(n) →0, then the probability of misclassiﬁcation selecting
H1 when H0 is true assuming ∥TI1(g∗
1) −TI0(g∗
0)∥> 0 is given by
Pr
&
L(ID) > ν|H0
'
∼
G
2
π
σ(n)
∥TI1(g∗
1) −TI0(g∗
0)∥e−((∥TI1(g∗
1)−TI0(g∗
0)∥2)/8σ(n)2).
(13.83)
Proof
Assume H0 so that ID = TI0(g∗
0) + W, then from the previous Lemma 13.24
and substituting ID = TI0(g∗
0) + W gives
L(ID) ∼−
∥ID −TI1(g∗
1)∥2
2σ(n)2
+
∥ID −TI0(g∗
0)∥2
2σ(n)2
+ 1
2 log
det F0(g∗
0, σ(n)2)
det F1(g∗
1, σ(n)2)
∼−
∥TI1(g∗
1) −TI0(g∗
0)∥2
2σ(n)2
+ 2
D
W, TI1(g∗
1) −TI0(g∗
0)
E
2σ(n)2
+ 1
2 log
det F0(g∗
0, σ(n)2)
det F1(g∗
1, σ(n)2) + Op(σ(n)).
(13.84)
The probability of error goes as
Pr
&
L(ID) ≥ν|H0
'
= Pr

W
σ(n)2 , TI1(g∗
1) −TI0(g∗
0)

≥ν
+∥TI1(g∗
1) −TI0(g∗
0)∥2
2σ(n)2
−1
2 log det F0(g∗
0, σ(n)2)
det F1(g∗
1, σ(n)2)
F
+ O(σ(n)).
(13.85)

13.7 HYPOTHESIS TESTING
411
Choose the test statistic to be
t(ID) =

W
σ(n),
TI1(g∗
1) −TI0(g∗
0)
∥TI0(g∗
0) −TI1(g∗
1)∥

,
(13.86)
then t(ID) is standard-normal, mean 0 variance 1. Deﬁning
κ =
σ(n)
∥TI0(g∗
0) −TI1(g∗
1)∥

ν −1
2 log
det F0(g∗
0, σ(n)2)
det F1(g∗
1, σ(n)2)

+
∥TI1(g∗
1) −TI0(g∗
0)∥
2σ(n)
,
(13.87)
then asymptotically as σ(n)2 →0 the probability of selecting H1 when H0 is true
reducestotheprobabilityofastandard-normalrandom-variablebeinggreaterthanthe
threshold value as σ(n) →0 then κ →(∥TI1(g∗
1) −TI0(g∗
0)∥)/2σ(n) which converges
to ∞giving
Pr{L(ID) > ν|H0} = Pr{t(ID) > κ} + O(σ(n))
(13.88)
=
1
√
2π
 ∞
κ
e−t2/2 dt + O(σ(n)) ∼
1
√
2πκ
e−κ2/2.
(13.89)
Remark 13.7.1
(Multiple-SensorFusion)Forthemulti-sensorsettingtheFisherinfor-
mation is strengthened as well as the exponential error bound. Deﬁning the multiple
observations as
ID1 = T1I0(g0) + W1,
ID2 = T2I0(g0) + W2, . . . ,
(13.90)
where W1, W2, . . . are independent and Gaussian with variances σ(n)2
1, σ(n)2
2, . . . . For
the asymptotic situation min σ(n)d →0, Theorem 13.25 applies as well and the error
exponent in Eqn. 13.83 becomes 
d(∥IDd −TdI0(¯g0)∥2/2σ(n)2
d).
Remark 13.7.2
When ∥TI0(g0) −TI1(g1)∥= 0 for g0 ̸= g1 then M(g0, α0) > 1, and
the estimator does not distinguish between the two objects. Hence, the assumption
that ∥TI0(g0) −TI1(g1)∦= 0.
Assuming that the ratio of priors is one, the error rate (of type I error probability)
is completely determined by the factor κ (Eqn. 13.87) which taking the general form
for positive constants a, b,
κ =
a
σ(n)∥TI1(g1) −TI0(g0)∥2 −b σ(n) log det(CRB1)
det(CRB0).
(13.91)
The ﬁrst term quantiﬁes the separation between the true object and the closest occur-
rence of the incorrect hypothesis. CRB0 and CRB1 are the lower-bounds for estimating
g under the two hypotheses, respectively. This provides a fundamental connection
between the accuracy in nuisance parameter estimation and the associated hypothesis-
selection. Depending upon the value of θ, these two terms inﬂuence the probability
of type I error.
According to Theorem 13.25, the error probability is dependent upon the objects and their
images through the correlation coefﬁcients and the projective signal energies. A similar result is
derived in [326], where it is concluded that for object-recognition the asymptotic error probability
depends on a parameter which characterizes the separation between the most similar but incorrect
object and the true object.

412
13 ESTIMATION BOUNDS
10
20
30
40
50
60
10
20
30
40
50
60
70
80
90
100
10
20
30
40
50
60
70
80
90
100
10
20
30
40
50
60
10
20
30
40
50
60
70
80
90
100
10
20
30
40
50
60
70
80
90
100
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0.55
0.6
–15
–10
–5
0
Binary recognition
beta
log(Prob of Error)
Analytical
Numerical
(1)
(4)
(5)
(2)
(3)
100
10–1
10–2
10–3
0.1
0.5
0.2
1
2
5
10
20
SNR
FLIR Conditional binary recognition
Probability of error
Monte carlo trials
Laplace method
100
10–1
10–2
10–3
0.1
0.2
0.5
1
2
SNR
FLIR Conditional binary recognition
Probability of error
Monte carlo trials
Laplace method
Figure 13.23 Top row: Panels 1,2 show VIDEO images of a tank at noise levels σ/∥TIα(g)∥=
0.01, 0.1; panel 3 shows curves denoting the log-probability of misidentifying the tank as a truck
for a ﬁxed object pose. Solid curves show asymptotic analytical estimates (solid line); dashed curves
show the likelihood ratio test calculation computed via numerical integration. Bottom row: Panels
show probability of false alarm comparing Monte-Carlo simulation and asymptotic approximation
from Shapiro and Yen [327]. Panel 4 shows FLIR identiﬁcation between two targets comparing
Monte-Carlo (circles) and asymptotic approximation (solid). Panel 5 shows similar results for M-ary
recognition.
Example 13.26 (Likelihood Ratio for Tank/Truck Discrimination)
Consider a
TANK versus TRUCK recognition through the video imager. Shown in the top row
panels 1,2 of Figure 13.23 are sample VIDEO images of the tank at two noise levels
corresponding to (σ/∥TIα(g)∥) = 0.01, 0.1; shown in panel 3 are the plots for the log-
probability of misidentifying the TANK video image as the TRUCK. The numerical
calculation is based on trapezoidal integration on SO(2) and random sampling on ID.
The dashed line plots the result of the numerical integration.
Srivastava has looked at the asymptotic expression for error rate for ID of mod-
els from VIDEO imagery. The solid line of Figure 13.23 (panel 3) shows plots of the
analytical asymptotic expression for the probability of misidentiﬁcation. Note how for
the entire range of noisy tank images for SNR from 0.01−0.2 the analytical error expo-
nent expression ﬁts the numerical integration of the probability of misidentiﬁcation
error.
13.7.2 m-ary Multiple Hypotheses
Calculating the error bounds for m > 2 object types is extended straightforwardly. Let Hi be the
hypothesis associated with the object type i, with data ID a conditionally Gaussian random ﬁeld
with mean ﬁeld TIi(gi). The Bayes estimate is given by the hypothesis with maximum a-posteriori
probability,
arg max
Hi
p(Hi|ID) = arg max
Hi
log p(ID|Hi)π(Hi).
(13.92)

13.7 HYPOTHESIS TESTING
413
The probability of error, conditioned on H0 the true hypothesis, is given by Pr{ˆi ̸= 0|H0}. Deﬁning
Li(ID) = log p(ID|Hi)
p(ID|H0)
and
log π(H0)
π(Hi) = νi,
(13.93)
then the probability of error becomes
Pr{ˆi ̸= 0|H0} = 1 −Pr(
<
i̸=0
{ID : Li(ID) < νi}).
(13.94)
Following Srivastava, the error exponents for exponential probability of decay for the m-ary case
are calculated as follows.
Theorem 13.27
(Srivastava) Given ID a Gaussian random ﬁeld with mean TI0(g∗
0) of object
H0, and deﬁne the test statistics and correlations according to
ti(ID) =
D
W/σ(n), TIi(g∗
i ) −TI0(g∗
0)
E
∥TIi(g∗
i ) −TI0(g∗
0)∥
,
(13.95)
K =

Kij =
2
TIi(g∗
i ) −TI0(g∗
0), TIj(g∗
j ) −TI0(g∗
0)
3
∥TIi(g∗
i ) −TI0(g∗
0)∥∥TIj(g∗
j ) −TI0(g∗
0)∥

.
(13.96)
Then with the m × m covariance K = Kij with thresholds given by
κi =
σ(n)
∥TIi(g∗
i ) −TI0(g∗
0)∥

log(νi) −1
2 log
det F0(g∗
0, σ(n)2)
det Fi(g∗
i , σ(n)2)

+
∥TIi(g∗
i ) −TI0(g∗
0)∥
2σ(n)
,
(13.97)
the probability of correct classiﬁcation as σ(n) →0 is given by
1
(2π)m/2 det−1/2 K

t1<κ1,...,tm<κm
e−1/2 
m
ij=1 titj(K−1)ij dt1, . . . , dtm.
(13.98)
Proof
The probability of correct selection in each binary test is evaluated through
the set {ID : Li(ID) < νi} can be reduced to evaluating Pr{ti(ID) < κi} where the
test statistic ti(ID) of Eqn. 13.95 is standard normal with thresholds κi as given by
Eqn. 13.97. Then ti are a zero-mean Gaussian random vector with the m × m element
covariance matrix K.
Example 13.28 (Asymptotic Error Exponents: FLIR, LADAR)
Shown in the bottom
row of Figure 13.23 are similar results from Shapiro and Yen [327] for the FLIR and
LADAR sensors. Panel 4 shows the false alarm rates for two different orientations
of the tank in noise. The dashed line plots the analytical expression for κ. The solid
line plots the numerical integration of the posterior false alarm error probability of
Eqn. 13.73. Panels 4 and 5 choose two different orientations of the object.

14 E S T I M AT I O N O N M E T R I C S PA C E S W I T H
P H O T O M E T R I C VA R I AT I O N
ABSTRACT
Model uncertainty comes in many forms. In this chapter we shall examine extensively
the variable photometric model in which the underlying image ﬁeld I(·) is modelled as an
element of a Hilbert space I(·) ∈H(φ) constructed via basis expansion {φi}. Inference involves
both the pose and identity of the objects as well as the photometric intensity itself. This
corresponds to making the template random, expanding the deformable template to include
both the photometric variations and geometric variations.
14.1 The Deformable Template: Orbits of Signature and
Geometric Variation
Now examine models of variation which explicitly show metric spaces on photometric variation
with geometry variation. The basic model of Figure 14.1 represents the source of imagery I as
the orbit of the product of all photometric variations with geometric variations. The space of
photometric intensities are modeled as a Hilbert space H. The deformable template becomes the
orbit of all photometric intensities under geometric motions, A = H × G, with elements the pairs
(I, g) ∈A = H × G. Notice, in this setting there are no ﬁxed exemplars. The resulting images
which form the mean ﬁelds become I(g) = I ◦g ∈I. The full model on the source requires the
construction of the prior model on elements (I, g), I ∈H, g ∈G. For this we explicitly model the
metric on images via empirical methods for estimating the statistical variation of the imagery. We
examine photometric intensities generated in the Hilbert space H(φ) spanned via the basis φ, and
the special Euclidean group g ∈SE(d). The noise model associated with the estimation problems
takes the form of the observation ID as a conditionally Gaussian random ﬁeld (conditioned on
(I, g)) with mean ﬁeld I(g), I ∈H(φ), g ∈G, with additive white noise W.
14.1.1 The Robust Deformable Templates
We now examine the construction of robust deformable templates, which allow the accomodation of
both variations of the pose of objects and photometric variations, associated with occlusion, vari-
ability of lighting, and other natural effects. Throughout we shall examine the special-Euclidean
group G = SE(d) = SO(d) ⊗Rd as the basic geometric variablility. In the robust deformable
template, variability in observed images is not solely associated with geometric transformations
Source: Photometric+
Geometric Variation
Imaging Channel
(I,g)      

I (g) +W
ID =
D
Image Inference
I 
^
Figure 14.1 The source of images: an orbit under groups of transformations. The source are images
and group elements (I, g), I ∈H, g ∈G, H the Hilbert space of photometric intensities, and G the
group of geometric transformations. The observations are ID ∈ID.
414

14.1 THE DEFORMABLE TEMPLATE
415
Figure 14.2 Shows photometric variability of FLIR imagery.
deﬁning the pose of targets, but as well other factors such as lighting variations, object surface
properties and texture variations corresponding to the presence of clutter.
The variability of target type and pose is accommodated using the group of rigid motions
representing geometric variations acting on the background space X according to g ∈G : x ∈X →
g(x) ∈X, with the group action on the image space I deﬁned by
g ∈G : I ∈I →I ◦g−1 ∈I.
(14.1)
Photometric variability is accomodated by extending the rigid templates to the robust
deformable templates, which explicitly include photometric variations. The challenge is illus-
trated in Figure 14.2. In natural scenes it is impractical to explicitly represent every object and all
of the photometric variation in the scene by a ﬁxed deterministic 3D model. Unquestionably, the
main challange to rigid model-based methods in natural imagery is to acquire pose and identiﬁ-
cation of targets while, at the same time, “seeing through” the confounding nuisance variables of
photometric variation. The panels show electro-optic imagery and FLIR imagery of natural and
cluttered scenes depicting the variability of object appearance 14.2.
The group of photometric variations is modeled as a Hilbert space H ⊆L2. Since not all
of the elements of L2 correspond to observable images, the space of photometric variations H,
constructed via basis expansion {φn}, is bounded to be a very small subset of L2. Its action on the
space of images I is additive
I′ =

n
Inφn ∈H : I ∈I →I +

n
Inφn ∈I.
(14.2)
14.1.2 The Metric Space of the Robust Deformable Template
To make the robust deformable template into a metric space, deﬁne the metric distance on elements
of I taking into account the action of G and the change in photometric value.
Deﬁnition 14.1
The group A of geometric and photometric variation is a product of
groups with elements {(g, I) : g ∈G, I ∈H} with the law of composition
(g′, I′) ◦(g, I) = (g′ ◦g, I + I′ ◦g).
(14.3)
The robust deformable template A.Itemp is the orbit under the action of A:
A.Itemp =
C
I ∈I : I =

Itemp +

n
Inφn

◦g−1
F
with
(g, I).Itemp = (Itemp + I) ◦g−1.
(14.4)

416
14 ESTIMATION ON METRIC SPACES
Deﬁning the energy of a path in A associated with the family of functional norms NG, NI
on the tangent elements (∂gt/∂t ◦g−1
t
, ∂It/∂t ◦g−1
t
) to be
E(∂g·
∂t , I·) =
 1
0
NG
∂gt
∂t ◦g−1
t
2
dt +
 1
0
NI
∂It
∂t ◦g−1
t
2
dt,
(14.5)
thentherobustdeformabletemplatebecomesametricspacewithdistancedeﬁned
through the inﬁmum over all paths enerrgies gt, t ∈[0, 1], connecting elements in A.
Notice, with the law of composition 14.3, then 14.4 is a group action. In equation 14.5, there
are two terms in the metric, which takes I to I′. The ﬁrst integral penalizes transformations in
the background space associated with the group G, and the second one penalized photometric
deformations. For the group of rigid motions, G = SE(d) = SO(d) ⊗Rd, it is natural to set the
energy term due to the rigid motions of the objects, to zero.
14.2 Empirical Covariance of Photometric Variability
via Principle Components
We now explore the construction of the metrics corresponding to the functional norm NI on
photometric variation. We do this by designing the norm to match the empirical statistics of
realizations of the patterns being studied. We shall be interested in generating the eigenfunction on
submanifolds X ⊂R3 corresponding to surfaces as well as planes and volumes. As long as we have
a well deﬁned inner product over the manifold (Riemannian manifold) then the methods extend
from the regular intervals, arrays, and cubes to smooth surface submanifolds. Shown in Figure 14.3
are examples of several triangulated graphs X = S(△) ⊂R3 which represent background spaces
upon which principal components will be performed.
We begin with the standard deﬁnition of the photometric space as a Hilbert space with the
metric deﬁned through a positive deﬁnite quadratic form.
Deﬁnition 14.2
Let the images I be deﬁned on the closed and bounded background space X ⊂
Rd. Then the Hilbert space of photometric intensities H(φ) has norm ∥f∥H deﬁned by a
quadratic form Q, with elements constructed through linear combinations of the orthonormal
basis φ:
H(φ) = {I : I(·) =

n
Inφn(·) ∈H(φ), ∥I∥H < ∞},
(14.6)
where ∥I∥2
H =

n
qn|⟨I, φn⟩2|2.
(14.7)
Figure 14.3 Examples of faceted models used for studying photometric signature variation; shown
depicted are triangulated graphs X = S(△) ⊂R3 which represent the surfaces on which we will
want to understand photometric variation along with regular subvolumes.

14.2 EMPIRICAL COVARIANCE OF PHOTOMETRIC VARIABILITY
417
Identifying images with their l2 representation I ∈H(φ) ↔(I1, I2, . . . ) ∈l2 the norm
reduces to
∥I∥2
H =

n
qn|In|2,
In = ⟨I, φn⟩2.
(14.8)
14.2.1 Signatures as a Gaussian Random Field Constructed from
Principle Components
Many approaches in pattern representation use principal components and clustering via empirical
covariance estimation. The basic idea is to model the space of imagery I ∈H(φ) as realiza-
tions of a Gaussian random ﬁeld with covariance constructed to ﬁt the empirical variation. The
complete orthonormal basis for the Hilbert space representation is generated from the orthonor-
mal eigenfunctions of the empirical covariance; the metric is deﬁned by the eigenvalues of the
covariance.
As we cross back and forth between deterministic and probabilistic interpretation it is natural
to deﬁne the quadratic form through a covariance operator which expresses the statistical occur-
rence of events. In that case, the operator representing the quadratic form above Q with eigenvalues
qn will correspond to the inverse of the covariance operator K with eigenvalues λn = 1/qn.
Theorem 14.3
Let I(·) on X be a zero mean real-valued Gaussian random ﬁeld in the Hilbert
space H(X) with covariance K(x, y) on closed and bounded background space X. The set of
orthonormal functions {φn(·), n = 1, . . . , N} minimizing the mean squared error
arg min
φn∈H
E




X

I(x) −
N

n=1
Inφn(x)

2
dx



,
(14.9)
where In =

X I(x)φn(x) dx, satisfy the integral equation
λnφn(x) =

X
K(x, y)φn(y) dy where λ1 ≥λ2 ≥. . . λN ≥λN+1 ≥. . . .
(14.10)
Proof
The proof follows the standard proof for Karhunen–Loeve expansion [328].
As K is a self adjoint positive operator on X there a exists a set of complete orthonormal
functions {ψn(x), x ∈X, n = 1, 2, . . . } such that
K(x, y) =
∞

n=1
λnψn(x)ψn(y),
(14.11)
with λn ≥λn+1,λn > 0. Now use the completeness of {ψn} and orthonormality to
express each eigenfunction according to
φn(x) =
∞

j=1
α(n)
j
ψj(x),
∞

j=1
|α(n)
j
|2 = 1,
n = 1, . . . , N.
(14.12)

418
14 ESTIMATION ON METRIC SPACES
Using this representation, rewrite the minimization as
arg min
φn∈H
E{

X
|I(x) −
N

n=1
Inφn(x)|2 dx}
(14.13)
= arg max
φn∈H
N

i=1

X

X
φi(x)
∞

n=1
λiψn(x)ψn(y)φi(y) dx dy,
(14.14)
= arg max
α(n)
j
∞

j=1
N

n=1
λj(α(n)
j
)2 subject to
∞

j=1
|α(n)
j
|2 = 1,
n = 1, . . . , N.
(14.15)
This is maximized if α(n)
j
= δ(n −j) giving the result {φn(·) = ψn(·), n = 1, . . . , N}.
14.2.2 Algorithm for Empirical Construction of Bases
Nowexamineempiricalmethodsforgeneratingtheeigenfunctions. Theeigenfunctionsaredeﬁned
on the surface background space X = S(△). Given a database of observed image signatures,
I(1), I(2), . . . , then the empirical covariance K is an L × L matrix with elements K(xl, yl), xl, yl ∈X.
The eigen basis is generated directly from the N-element database of the signatures via Singular
Value Decomposition (SVD) as follows.
Algorithm 14.4
The scalar ﬁeld is expanded according to I(·) = ¯I(·)+
N
n=1 Inφn(·), where
In = 
l(I(xl) −¯I(xl))φn(xl)γ (xl) with discrete pixel size γ (xl). The φn basis are calculated
according to the following:
1. Discretize continuum λnφn(x) =

X K(x, y)φn(y)γ (dy) into the matrix equation:
λnφn(xk) =

l
K(xk, yl)φn(yl)γ (yl).
(14.16)
2. Deﬁning the sample mean ¯I(xl) = 1/J 
J
j=1 I(j)(xl), construct the covariance factoring
it into its square root K = QQ∗including the measure given by the diagonal matrix
Ŵ = diag[γ (y1), . . . , γ (yL)]:
λn
√
Ŵφn =
√
ŴQQ∗√
Ŵ
√
Ŵφn,
where Qlj = 1

J
(I(j)(xl) −¯I(xl)), l = 1, . . . .
(14.17)
3. Via SVD factor
√
ŴQ into orthogonal matrix U, V and diagonal  so (
√
ŴQ) =
U
√
V∗. The columns of U are eigenvectors of (
√
ŴQ)(
√
ŴQ)∗; the eigenvalues are
the elements of the diagonal matrix :
φn =
√
Ŵ−1Un,
U = [U1 · · · UL], λn = nn,
n = 1, . . . , N.
(14.18)
Example 14.5 (Eigenfunctions for Illumination Variations in the Image Plane)
The modeling of objects signatures approached from the point of view of empiri-
cal statistics has been done by many groups. Examine the work in the image plane
on human faces of Belhumeur, Kriegman, Mumford, Yuille and coworkers [329–331].
Here the images are deﬁned on the unit-square background space X = [0, 1]2 ⊂R2

14.2 EMPIRICAL COVARIANCE OF PHOTOMETRIC VARIABILITY
419
(1)
(2)
(3)
(4)
(5)
(6)
Figure 14.4 Column 1: Panels show eigen-images 2 and 3. Columns 2–6 show images from the
illumination variability I = 
n Inφn(·) corresponding to one (top row) and two (bottom row)
light sources from [329].
Shown in Figure 14.4 are results from eigencorrelation experiments of Belhumeur and
Kriegman. Column 1 shows eigenfunctions 2 and 3 of the empirical autocorrelation
matrix. Shown in columns 2,3,4,5, and 6 are random samples from the span of the
eigenfunctions. Each row shows sample images from the illumination variability space
corresponding to one (top row) and two (bottom row) light sources. Figures depict
elements of the orbit I = 
n Inφn(·).
Example 14.6 (Eigenfunctions on 2D Surface Manifolds)
Examineempiricaleigen-
function models associated with 2D surface submanifolds of R3 as studied by Cooper,
Lanterman and Joshi [332–335]. Surface objects are represented as triangulated graphs
on the background space X = S(△) = {△(xl), l = 1, . . . , L}, with each triangle having
surface measures γ (xl) ﬁlling the diagonal matrix Ŵ. Shown in Figure 14.3 are exam-
ples of such triangulated graphs upon which the eignfunctions are constructed. Model
the photometric intensity I as a scalar-valued Gaussian random ﬁeld with mean cor-
responding to the template at ﬁxed coordinates under the identity transformation.
For generating the data base, OpenGL or ray tracing can be used to render 3D objects
under variable lighting lumination conditions and material reﬂective properties. The
photometric variations are due to variability in the direction, color, and intensity of
light, as well as variability in the ambient and diffuse reﬂective properties of the object
material. Databases can be generated by varying the direction of the illumination and
diffuse material reﬂectivity. A database described in Figure 14.5 for a teacup and a
teapot were generated with 2592 lighting directions and diffuse reﬂectivity 1 for the
teacup and 3 for the teapot.
Shown in the top row of Figure 14.6 are the principal components of the teacup
data base. The bottom row shows the ﬁrst eigen signatures for the teapot. Shown in
the rightmost panel of Figure 14.6 is the efﬁciency of the representation as depicted by
the eigenvalues of the empirical covariances. The circles (◦) show the eigenvalues of
the static database, the dashed lines (−−) show the dynamic data base, and the solid
lines (−) for the composite data base.
Now examine empirical eigenfunction models 2D submanifolds of R3 for FLIR
imagery [332–335]. On surfaces, the objects are represented as triangulated graphs
S(△) = △(xl), l = 1, . . . , L the number of triangle, with each triangle having surface
measure γ (l). The eigenfunctions are indexed over the triangulated graph represent-
ing the CAD model; the background space becomes S(△). Shown in Figure 14.3 are

420
14 ESTIMATION ON METRIC SPACES
Database Summaries - Video
Database
Light Directions
Diffuse Reflectivities
Object Vertices
Teacup
2592
1
3749
Teapot
2592
3
1480
0
5
10 15 20 25 30 35 40 45 50
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Eigenspectrum for Signature
Random Field: Teacup
Figure 14.5 Summaries of the contents of the empirical databases used for the KL expansion of
the signature random ﬁeld in the video context. The number of signatures used in the PCA is 2592
for the teacup, and 7776 for the teapot. Right panel shows the normalized power spectrum of
the eigenbasis.
(1)
(2)
(3)
(4)
(5)
Figure 14.6 Top row shows visualizations of the ﬁrst ﬁve eigenfunctions of the teacup database.
Bottom row panels show the ﬁrst ﬁve eigenfunctions of the teapot database.
examples of CAD models which have been represented as triangulated graphs show-
ing faceted geometries of the CAD models. For infrared sensors operating in the 8–12
micron band, object material properties, object operational states, and meteorological
conditions contribute to photometric variability. Model the template signature I as a
scalar-valued Gaussian random ﬁeld of radiant intensity with mean corresponding
to the template at ﬁxed coordinates under the identity transformation. For FLIR, the
PRISM [299] simulation package is used to build the databases isolating the primary
modes of meteorological and operational photometric variation. The meteorological
in weather and atmospheric conditions were isolated in the “static” database. Opera-
tional variation resulting from changes in the operational state of the object including
land speed, motor speed, and gun state are isolated in the “dynamic” database. The
static database was generated from 3, 12-h simulation sessions of weather changes
resulting in 343 radiance proﬁles. Cooper generates the dynamic database from 18-h
of simulation sessions consisting of dynamic data resulting in 186 radiance proﬁles.
The composite database was generated using 72, 12-h simulations consisting of 7392
radiance proﬁles (Figure 14.7).
Figure 14.8 shows the ﬁrst several eigensignatures φ1,2,3,4(·). Panels 1 and 2
show the static data base signatures. The ﬁrst eigen signature of the static database
demonstrates uniform heating of the tank as the sun moves across the sky during the
twelve hour simulations. Panels 3 and 4 show the eigen signatures for the dynamic

14.2 EMPIRICAL COVARIANCE OF PHOTOMETRIC VARIABILITY
421
Database Summaries
Database
Simulations
Obs. Interval
Sets of Weather Data
Radiance Profiles
Static
3
12 h/Each
3
343
Dynamic
6
3 h/Each
1
186
Composite
72
10–12 h/Each
3
7392
0
5
10
15
20
25
30
35 40
45
50
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Normalized Eigenspectrum of
Scalar Random Field
STATIC   
DYNAMIC  
COMPOSITE
Figure 14.7 Summaries of the contents of the empirical databases used. The static database iso-
lates meteorological variability while the dynamic database isolates operational variability. The
composite database is composed of the two modes of variability. The Right panel shows the
normalized power spectrum of the eigenbasis.
Figure 14.8 Top row shows visualizations of the three eigenfunctions of the static database of
the tank for FLIR photometric variation. Left column: Top 3 panels show ﬁrst 3 static database
eigenfunctions; bottom three panels show the dynamic database eigenfunctions. Taken from
Cooper [332–334]. Right column: Figure shows Eigentanks from Lanterman. The rows show varying
orientation; the columns show increasing detail as generated by increasing numbers of eigenfunc-
tions. Shown are estimated eigensignatures for an M60 at different hypothesized orientations and
with different numbers of eigentanks. From left to right, columns display signatures with the ﬁrst
1, 3, 9, and 50 eigentanks used in the expansion (see also Plate 25).
database. Notice the localized variations in the regions of the tank containing and
adjacent to the motor.
Shown in the right panel of Figure 14.8 are eight poses (rows) of a model tank
constructed by synthesizing the tank via different numbers of principal components
(columns). This illustrates that the EigenSurface representations are 2D manifolds
associated with the entire surface of the object.

422
14 ESTIMATION ON METRIC SPACES
14.3 Estimation of Parameters on the Conditionally Gaussian
Random Field Models
Clearly, in this setting the addition of the photometric randomness associated with the param-
eterization of I = 
i Iiφ ∈H(φ) increases the difﬁculty and dimension of estimating the
geometric parameters g ∈G. Our approach in subsequent sections will be to integrate them
out as nuisance variates as well as simultaneously estimate them via MAP and MMSE estima-
tion. The basic technique will be to expand the posterior distribution around the MAP/MMSE
estimators.
The orbit is the set of all imagery generated from the template and a linear combination of
basis elements spanning the photometric intensities. The imagers are mappings T : I ∈I(φ) →
TI ∈ID, then TI(g) is the mean-ﬁeld conditioned on the geometric transformation g ∈G and
signature I. It is natural to model the data ID as a conditionally Gaussian random ﬁeld with
mean ﬁeld TI in additive noise, ID = TI + W. Then we can speak of the conditional mean which
is the MMSE, or the MAP estimator. Corresponding to the norm square ∥· ∥2
2 is the additive
white noise model, ID = TI(g) + W, with W white noise variance 1. When working on the
continuum, then the process ID will not be of trace class and strictly speaking ∥ID∥2 will not
be well-deﬁned in the L2 norm. This is properly handled by constraining the noise process to
have a covariance with the spectrum which has a ﬁnite integral. In this case the matching norm-
square cost changes to ∥· ∥2
H reﬂecting the well-deﬁned true covariance. Alternatively, for all of
the applications, the noise process is associated with a ﬁnite number of detectors (voxels in the
image), thereby constraining the image to be well deﬁned. Thus, although the L2 matching term
in the continuum has no rigorous probabilistic interpretation we will continue to use it because
it is so convenient and connects us to so many classic formulations in least-squares estimation.
We, of course, keep in mind the modeling issues between the two alternatives to make it more
precise.
Throughout, the signature and pose are assumed conditionally independent. Take the prior
on signature to be a conditional Gaussian random ﬁeld conditioned on g ∈G with Gaussian
densities on the d-dimensional cylinder expansions on I1, . . . , Id. The object photometric intensities
are modeled as a zero-mean Gaussian random ﬁeld with eigenfunctions φi, so that Ii, i = 1, . . . , n
are independent normal with zero mean and variance the eigenvalue of the ith eigensignature φi,
Kφi = λiφi.
Clearly, expanding the deformable template to include signature parameters increases the
invariance and decreases the discriminability. There is a cost. This is the same cost as for the
nuisance parameters in hypothesis testing studied in the asymptotic setting via Theorem 2.60 in
Chapter 2. Interestingly, for the Gaussian projective imagery setting, we can calculate an exact
version of the asymptotic Theorem 2.60 calculating the complexity (or cost) of introducing the nui-
sance variables of photometric variation in order to obtain robustness. This requires the projective
Gaussian image model, with ID a conditionally Gaussian ﬁeld with the mean ﬁeld TI(g) twice
differentiable (Fisher information well deﬁned).
Theorem 14.7
Given data ID = TI(g) + W a conditionally Gaussian ﬁeld with mean ﬁeld
TI(g) twice differentiable, W white noise variance σ 2, and I(g) = 
d
i=1 Iiφi(g) a Gaussian
ﬁeld, with Ii independent Gaussian, mean zero and variance λi, i = 1, . . . , d .
Deﬁning the Fisher information (including the prior) and the MAP estimator accord-
ing to
F(g) =
 1
σ 2 ⟨Tφi(g), Tφj(g)⟩+ 1
λi
δ(i −j)

(14.19)

14.3 ESTIMATION OF PARAMETERS
423


¯I1
...
¯Id

=
arg max
(I1,...,Id)∈Rd
log p(I1, . . . , Id|ID, g) = F(g)−1


⟨ID,Tφ1(g)⟩
σ 2
...
⟨ID,Tφd(g)⟩
σ 2

,
(14.20)
then
p(ID|g) = p(ID, ¯I1, . . . , ¯Id|g) det−1/2 2πF(g).
(14.21)
Proof
Deﬁning
H(I1, . . . , Id; g) = ∥ID −TI(g)∥2
2σ 2
+
d

i=1
|Ii|2
2λi
,
(14.22)
then rewriting the conditional density gives
p(ID|g) =

Rd p(ID, I1, . . . , Id|g)dI1 . . . dId
(14.23)
=

Rd p(ID|I1, . . . , Id, g)π(I1, . . . , Id|g)dI1 . . . dId
(14.24)
=

Rd
1
(2πσ 2)N/2 e−(∥ID−TI(g)∥2)/(2σ 2)
1
(2π)d/2 $d
i=1
√λi
× e−(
d
i=1 |Ii|2)/(2λi)dI1 . . . dId
(14.25)
=

Rd
1
(2πσ 2)N/2
1
(2π)d/2 $d
i=1
√λi
e−H(I1,...,Id;g)dI1 . . . dId.
(14.26)
Expanding H(I1, . . . , Id; g) in a Taylor series around the MAP estimator ¯I using
the fact that the gradient term is zero at the MAP estimator gives
H(I1, . . . , Id; g) = H(¯I1, . . . , ¯Id; g) + 1
2
d

i,j=1
(Ii −¯Ii)(Ij −¯Ij)F(g)ij.
(14.27)
Substituting in Eqn. 14.26 gives
p(ID|g) = p(ID|¯I1, . . . , ¯Id, g)π(¯I1, . . . , ¯Id|g)

Rd e−1/2 
d
i,j=1(Ii−¯Ii)(Ij−¯Ij)F(g)ijdI1 . . . dId
(a)
= p(ID, ¯I1, . . . , ¯Id|g)(2π)d/2 det−1/2 F(g),
(14.28)
where (a) follows from the fact that the function (2π)−d/2 det1/2 Qe−1/2x∗Qx
integrates over Rd to 1 where Q is a positive deﬁnite d × d matrix.
Corollary 14.8
If for every g ∈G, {Tφi(g)} are orthogonal, then
p(ID|g) = p(ID, ¯I1, . . . , ¯Id|g)(2πσ 2)d/2


d

i=1
λi
λi∥Tφi(g)∥2 + σ 2


1/2
.
(14.29)
As σ →0, the complexity penalty goes as d/2 log σ 2 + O(1), as in Eqn. 2.212 in
Theorem 2.63.

424
14 ESTIMATION ON METRIC SPACES
Example 14.9 (For the Gaussian case, MAP=MMSE)
Examine the simple case in
which TI = I and there is no projection, and the goal is to directly estimate the pho-
tometric parameters alone without any transformation. This is the clearest version of
the above problem. Let I ∈H(φ) be the mean ﬁeld of the observation ID, W white
noise variance σ 2, then the MMSE IMMSE : ID ∈ID →H, and the MAP estimator
IMAP : ID ∈ID →H are identical:
IMAP = IMMSE =

i
¯Iiφi, ¯Ii =
λi
λi + σ 2 ⟨ID, φi⟩.
(14.30)
To see this, solving for the MMSE in l2 since with ID Gaussian, then ID
n = ⟨ID, φn⟩
is Gaussian distributed, zero-mean and variance λn + σ 2. Then from Theorem 2.36,
Chapter 2 the MMSE ¯Ij = 
i α(j)
i ID
i is a linear functional of the data ID
n and for each j
must have the property that the error is orthogonal to the data; for all n,
0 = E(¯Ij −Ij)ID
n = E


i
α(j)
i ID
i −Ij

ID
n
(14.31)
=

i
α(j)
i EID
i ID
n −EIjID
n =

i
α(j)
i (σ 2 + λi)δ(i −j) −λjδ(j −n)
(14.32)
implying α(j)
i
= 0 for i ̸= j, and ¯Ij = αjID
j , with αj = (λj/(λj + σ 2)). The MAP
estimator satisﬁes for each coefﬁcient ¯Ij, ¯Ij = arg minIj:I=
i Iiφi(∥ID−
i Iiφi∥2/2σ 2)+
(∥Ii∥2/2λ2
i ) giving the ﬁxed point condition
¯Ij

1
σ 2 + 1
λj

=
⟨ID, φj⟩
σ 2
.
(14.33)
14.4 Estimation of Pose by Integrating Out EigenSignatures
Theorem 14.7 Eqn. 14.21 and Corollary 14.8 of Eqn. 14.29 provides the direct mechanism for
solving the Bayes integral problem when estimating the pose parameter g ∈G. For each pose the
Bayes factor is computed and weights the likelihood term evaluated at the MAP estimator. Several
investigators have examined this approach in object recognition.
The MAP estimator of pose accomodating photometric variation is given by the following.
Theorem 14.10
Given ID is a conditionally Gaussian ﬁeld, mean TI(g) with I(g) =

d
i=1 Iiφi(g), with priors Ii independent Gaussian variables variance λi and π(g), g ∈G
with conditional mean where ¯I(g) = 
d
i=1 ¯Iiφi(g), then
ˆg = arg max
g∈G
log p(g|ID)
(14.34)
= arg max
g∈G log π(g) −∥ID −T¯I(g)∥2
2σ 2
−

d
i=1 |¯Ii|2
2λi
−1
2 log det 2πF(g). (14.35)
The joint estimation of photometric and geometric parameters has been approached by
various groups. Examine in detail Cooper’s approach [332–334] of representing the photometric

14.4 ESTIMATION OF POSE
425
variation on the 3D surface parameterization of the CAD models themselves. By simulating a
large number of signatures, taken by varying environmental conditions populations of signatures
are generated. For simulating temperature radiance proﬁles, Cooper employs the PRISM software
originally developed by the Keweenaw Research Center at Michigan Technological University.41
For photometric variation associated with Teapots, he uses the openGL software to examine vari-
ations of lighting illuminations. To generate the eigentank models, a large database of radiance
maps are synthesized under a wide range of conditions, both meteorological (solar irradiance,
wind speed, relative humidity, etc.) and operational (vehicle speed, engine speed, gun ﬁred or
not, etc.).
Figure 14.8 shows examples of multiple eigensignatures at four different orientations (four
columns). Panel 1 of Figure 14.9 depicts an example of a FLIR image of an M60 tank oriented
at 270◦.42 The exhaust is the dominant feature, which is typical of operational M60 imagery.
Figure 14.9 shows temperature signature proﬁles resulting from MAP estimates of the expan-
sion coefﬁcients of the M60 principal components model at different hypothesized orientations.
The MAP estimates ¯I1, . . . , ¯Id = arg max(I1,...,Id)∈Rd log p(I1, . . . , Id|ID, g), computed by solving the
least-squares solution for the image signature Eqn. 14.20 for every pose g ∈G. 43 For each target
0
50
100
150
200
250
300
350
3.3495
3.35
3.3505
3.351
3.3515
3.352
3.3525
3.353
3.3535
3.354
x 106
M60 at 270 degrees in data, M60 hypothesized in analysis
Candidate orientation in degrees
Loglikelihood in nats
5
10
15
20
25
30
35
40
45
50
0
20
40
60
80
100
120
140
160
180
Number of eigentanks used in expansion
Schwarz penalty for thermodynamic variables in nats
M60 at 270 deg. in data, M60 at 270 and 0 deg. used in analysis
10
15
20
25
30
35
40
45
50
3.3533
3.3534
3.3535
3.3535
3.3535
3.3536 x 106
Number of eigentanks used in expansion
LL and penalized LL in nats
M60 at 270 degrees in data, M60 hypothesized in analysis
Figure 14.9 Left column: Panel 1 shows a FLIR image of an M60 facing away from the detector
provided by NVESD. Middle column: Panel 2 shows the log-likelihood varying with respect to
orientation for the NVESD image with an M60 at 270◦. The correct target type, an M60, is assumed.
Lines correspond to log-likelihoods computed using 1, 3, 9, and 50 eigentanks. Panel 3 shows the
log-determinant of the Fisher information penalty, with respect to the number of eigentanks
for the M60 oriented at 270◦(top line), and 0◦(bottom line). Right column: Panel 4 shows the
log-likelihood (top line) and penalized log-likelihood (bottom line) with respect to the number
of eigentanks used for a tank hypothesized at the correct orientation. Panel 5 shows the MAP
estimator of the M60 data (panel 1).
41 PRISM is currently sold and maintained by ThermoAnalytics, Inc., P.O. Box 66, Calumet, MI 49913,
website: www.thermoanalytics.com.
42 0◦represents the tank facing left, and increasing angle measurements run counter-clockwise looking
down on the tank, so 270◦is facing directly away from the detector.
43 In fact Lanterman works with an approximation to the Gaussian density for the projective image model,
capturing the ﬁrst and second moments of the Poisson model by adjusting the projective image to have local
variance given by the number of counts in the CCD cell. This corresponds to potential ∥ID−TI(g)/
√
ID∥2 where

426
14 ESTIMATION ON METRIC SPACES
pose, these equations allow for the computation of the optimum MAP signature ¯I1, . . . , ¯Id. The dif-
ferent rows correspond to hypothesized orientations at 45◦increments. The columns correspond
to different numbers of EigenTanks φi: d = 1, 3, 9, 50. The 7th row (second to last) corresponds to
the 270◦correct orientation. Figure 14.8 illustrates various poses (rows) of the model tank con-
structed by synthesizing the tank via different numbers of principal components (columns). The
ﬁrst eigentank incorporates the exhaust ports, while the remainder brightens the engine region, as
well as the treads. Note that in the third row from the top, in which the tank is facing the detector
at 90◦, the eigentanks cannot describe the data. In ﬁtting the bright part of the data, they can heat
up the barrel, but not the remaining part of the tank face. Panel 2 of Figure 14.9 illustrates the
log-likelihood surface ∥(ID −TI(g))/
√
ID∥2 with respect to orientation of the M60. The lines in
Figure. 14.9 correspond to using 1, 3, 9, and 50 eigentanks in the expansion. Each surface peaks
sharply around the 270◦true orientation.
Panel 3 shows the penalty term varying with the number of eigentanks for different hypoth-
esized orientations. The top line shows the penalty for the correct hypothesized orientation; the
bottom line, for a tank facing towards the left. The penalties for these two cases are relatively close.
Panel 4 shows the log-likelihood (top lines) and penalized log-likelihood (bottom lines), varying
with the number of eigentanks, for tanks oriented at the correct orientation (facing away from the
detector, left panel). For the correct orientation, the penalty chooses 28 eigenvalues.
0
50
100
150
200
250
300
350
1.461
1.462
1.463
1.464
1.465
1.466
1.467
1.468
1.469
1.47
1.471 x 10
8
Candidate orientation in degrees
Loglikelihood in nats
Likelihood surface for orientation estimation
2
4
6
8
10
12
14
16
9750
9760
9770
9780
9790
9800
9810
9820
9830
9840
9850
Number of eigentanks used in expansion
LL and penalized LLs in nats, minus 1.4690E8
Tank hypothesized at correct orientation and position
Figure 14.10 Left panels show synthetic T62 superimposed over a real infrared background (cour-
tesy Night Vision and electronic Sensors Directorate). Right column shows log-likelihood varying
with respect to orientation for data generated with a true orientation of 45 degrees. From bottom
to top, lines correspond to likelihoods computed using 1, 3, 5 and 17 eigentanks. Bottom panel
shows the log-likelihood (top line) and penalized log-likelihood (bottom line) with respect to the
number of eigentanks employed.
the inner product over the image space does not include voxels with zero counts, ID(yi) = 0. This modiﬁes
Eqn. 14.20 little simply providing a normalization to the inner products arising in the Fisher information:
F(ID; g) =
 Tφi(g)
√
ID ,
Tφj(g)
√
ID

+ diag
 1
λ1
, . . . , 1
λd

.
(14.36)

14.4 ESTIMATION OF POSE
427
Example 14.11 (FLIR with Complexity Term (Lanterman [336]))
Figure 14.10
shows 140 × 140 images of a synthetic T62 superimposed on infrared backgrounds.
The radiant intensities of the facets were generated from an eigentank model, with
the ﬁrst three coefﬁcients of the T62s eigenexpansion set to −2000, 900, and −700 and
the remaining coefﬁcients set to zero. In all of the experiments the background adap-
tively estimated via its local average over the background. The second frame (top row)
shows a synthesized data set with the tank at 45◦. Panel 5 of Figure 14.10 shows the
log-likelihoods computed using the ﬁrst 1, 3, 5, and 17 terms in the thermodynamic
eigenexpansion. They peak sharply at the correct orientation of 45◦. In addition, using
more terms in the expansion generates higher log-likelihoods, since the larger num-
ber of parameters gives the model greater ﬂexibility in trying to ﬁt the tank to the
background.
Panel 6 of Figure 14.10 illustrates the effect of subtracting the penalty term from
the log-likelihood. The top line is the log-likelihood associated with a given number
of terms; notice that it increases monotonically with the number of terms. The bottom
line represents the result of subtracting the logarithm of the inverse Fisher information
penalty term. The penalized log-likelihood increases rapidly at ﬁrst, reaching a peak
at three terms, and then slowly decreases.
14.4.1 Bayes Integration
Cooper [335] studies the estimation of pose through lighting and photometric variation. The
inference of I(g) reduces to the inference of pose and temperature proﬁle. The orbit is the set of
all imagery generated from the template and a linear combination of basis elements {φi} spanning
the photometric intensities. The basis is constructed as eigenfunctions of the empirical covariance
generatedfrommanyrealizationsofactualsignaturesfromvaryingilluminationsoftheobjects(see
Figure 14.8). The openGL simulation software is used for generating the database of illuminations.
The top row shows a variety of signatures corresponding to varying illumination. Under the signal
model I = 
d
i=1 Iiφi, and I(O) = 
d
i=1 Iiφi(O) and the posterior density on the pose parameter
becomes
pd(O, I1, . . . , Id|ID) =
1
Z(ID)p(ID|I(O), I1, . . . , Id)π(O)π(I1, . . . , Id)
(14.37)
=
1
(2πσ 2)d/2 e−(1/2σ 2)∥ID−TI(O)∥2
d

i=1
1

2πλi
e−(I2
i )/(2λi).
(14.38)
Example 14.12 (Teapot Photometric Variability)
Figure
14.11
illustrates
experi-
ments on the varying signatures of the teapot. Panels 1–4 of Figure 14.11 show a
variety of signatures corresponding to varying illumination. Panel 5 shows MSE
curves for each of the teapot signatures demonstrating variability in performance due
to variability in the underlying object signature. Panel 5 plots the MSE performance
of pose estimators with correct and randomly sampled signature information and this
is compared in the video imaging context. The underlying signature of the teapot is
I = 1044. The y-axis represents the conditional MSE given the underlying object ori-
entation of 30◦. The curve labeled “I = t1044” is performance with complete signature
information, and the curves labeled “Random” correspond to performance using the
random sampling approach, without assumption of the object signature. The differ-
ence between these two curves represents the cost of knowledge of the object signature,
in terms of estimator performance.

428
14 ESTIMATION ON METRIC SPACES
0
0.5
1
1.5
2
2.5
3
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
Noise STD
Hilbert-Schmidt Squared Error
MSE Curves for Mismatched Luminance: Θtrue = 30 o, Ttrue = t1044
0
5.7
8.1
9.9
11.
12.
14.
15.
Equivalent Estimation Error In Degrees
T=t1044
Random
T=t1
T=t2000
Figure 14.11 Panels 1–4 show four signatures for the teapot I = 1044, 336, 2000, 1. Panel 5 shows
performance degradation due to lighting signature mismatch. The true signature was I = 1044,
other signatures studied are I = 1, 2000, along with Bayes integration over all signatures denoted
with X’s and labelled “Random”.
Database 75
Database 45
Database 140
Random Sampling
0
0.5
1
1.5
2
2.5
3
3.5
4
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
Noise STD
MSE(30)
Performance Curves
Figure 14.12 Panels 1–4 show T62 tanks with four temperature signatures I = 8, 45, 75, 140;
panel 5 shows performance degradation due to signature mismatch. The true signature was
I = 140, other signatures studied are I = 45, 75 along with Bayes integration over all temperature
signatures depicted via X’s and denoted “Random” (see also Plate 26).
Example 14.13 (Cooper FLIR Thermal Variation)
Cooper [333, 335] studies photo-
metric variation in infrared imagery. The ﬁrst several eigensignatures φ1, φ2 are shown
in columns 2 and 3 of Figure 14.8. Figure 14.12 shows examples of FLIR signatures
and MSE performance for the ATR. Panels 1–4 of Figure 14.12 show various signa-
tures from the Prism simulator projected through the perspective projection of the
CCD imager. The rightmost panel of Figure 14.12 shows the loss in estimator perfor-
mance due to the temperature FLIR photometric variation. Shown are MSE curves
for SO(2) estimation as a function of temperature signature, for the correct (140) and
mismatched data bases.

14.5 MULTIPLE MODALITY SIGNATURE REGISTRATION
429
14.5 Multiple Modality Signature Registration
Oftentime multiple imaging modalities are used to image the same brain. Examine the setting
where the transformations to be estimated in the registration are Euclidean motions between the
coordinate systems. In this setting the template and target have different signatures, but often have
common assumed homogeneous structuring elements. For example in brain imaging, the brain
is made up of bone, CSF, gray matter, white matter all of which images in a relatively homogeneous
manner in each of the modalities, however has different signatures for the different modalities CT,
MRI, cryosection, etc. For MR, the important parameter is the number of hydrogen protons present
while for CT it is linear attenuation coefﬁcient.
Generally the imagery is a union of homogeneous regions all of which appear identically in a
particular imaging modality. This is depicted in panel 1 of Figure 14.13 depicting a 2D cryosection
of a brain depicting various subregions. The right panel 2 shows the disjoint partition into a
series of regions WM=white matter, GM=gray matter, CSF=cerebrospinal ﬂuid, and bone. Denote
these disjoint regions via the simple functions φ1, . . . , φM, M the number of disjoint compartments
covering the background space.
Model the orbit of M-modality imagery as a vector I = (I(1), . . . , I(M)), each component
constructedinspanofadisjointpartitionofthebackgroundcoordinatesφ1, φ2, . . . simplefunctions
deﬁning the coordinate locations
I(m)(·) =

i
I(m)
i
φi(·),
I(m)
i
∈R.
(14.39)
The orbit I is the vector version of Eqn. 14.5 of Deﬁnition 14.1.
Deﬁnition 14.14
Deﬁne the robust multi-modality deformable template I as the orbit
of all signatures I = (I(1), . . . , I(M)) ∈H(φ) in the span of simple functions deﬁning the
homogeneous regions {φi} under the group operation g ∈G:
I(φ) = {I(g) = (I(1)(g), . . . , I(M)(g)) ,
I(m) =

i
I(m)
i
φi, g ∈G}.
(14.40)
The cross modality registration can be straightforwardly formulated.
CSF
SC
SK
CSF
SC
GM
WM
WM
GM
GM
Tissue
T1
T2
PD
CT
f1
f2
f3
f4
= White
223
140
81
88
= Gra y
134
167
115
88
= CSF
72
96
202
2
= Skull
41
36
4
255
Figure 14.13 Panel 1 shows cryosection imagery depicting clearly the regions WM=white matter,
GM=gray matter, CSF=cerebrospinal ﬂuid, and bone. Panel 2 depicts the regions of the compart-
ments, a disjoint partition of the full brain. Data taken from the Visible Human Project of the
National Library of Medicine. Panel 3 of tissue voxel intensity values for MRI T1, MRI T2, MRI
Proton Density (PD), and CT imaging modalities. Values correspond to four disjoint compartments
White Matter, Gray Matter, CSF, and skull.

430
14 ESTIMATION ON METRIC SPACES
Algorithm 14.15 (MAP Estimation Algorithm)
Given the disjoint partition into homo-
geneous regions {φi} of the multi-modality M-vector I = I(1), . . . , I(M), IDm has mean ﬁeld

 I(m)
i
φi(g) where the compartment φi has photometric intensity I(m)
i
for m-modality.
The MAP estimator of the registration parameter g ∈G relating the compartments
between modalities
ˆgm = arg min
g∈G
::::::
IDm −

i
I(m)
i
φi(g)
::::::
2
.
(14.41)
The I(m)
i
are the decoder values transcribing the colors represented by one imaging modality
into another. This is depicted in Table 14.13 showing compartment values, White Matter, Gray
Matter, CSF, and skull for four imaging modalities T1,T2, proton density, and CT.
Example 14.16 (Cross Modality Registration)
Examine experimental results incor-
porating unknown signatures associated with medical imaging modalities. In this
setting the template and target imagery will have different signatures. The deformable
template for rigid registration accomodates the geometric properties which are invari-
ant, but must provide enough parameters to accomodate the different photometric
intensities.
Shown in Figure 14.14 are results from multiple imaging modalities showing
cross-modality registration. Panels 1 and 2 shows T2 and proton density MRI images.
Panels 3 and 4 shows multimodality registration bounds for mutual information and
the Hilbert–Schmidt estimator. Panel 5 shows the comparison between mutual infor-
mation and the HSE for T1 and T2 weighted MR images. Panel 6 shows the T2 and
proton density registration.
–35
–30
–25
–20
–15
–10
–5
0
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
SNR(dB)
Hilbert-Schmidt norm
f
mutual information 
HSE
–30
–25
–20
–15
–10
–5
0
5
10
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
SNR(dB)
Hilbert-Schmidt norm
d
mutual information 
HSE
Figure 14.14 Column 1 shows examples of segmentations of the T1 Weighted MR Image. Columns
2 and 3 show T2, and proton density images (top row) with multimodality registration bounds for
mutual information and the Hilbert–Schmidt estimator. Panel 5 shows the comparison between
mutual information and the HSE for T1 and T2 weighted MR images. Panel 6 shows the T2 and
proton density registration. Data generated at the Kennedy Krieger institute.

14.5 MULTIPLE MODALITY SIGNATURE REGISTRATION
431
Panel 4 shows an example segmentation of the T1 Weighted MR Image. The
segmentation is produced from the MR image, by Bayes estimation of the means and
variances and solution of the likelihood ratio test. Three thresholds are used for map-
ping the tissues to WM, GM, and CSF; (White matter is white, Gray matter is gray,
and CSF is black).
14.6 Models for Clutter: The Transported Generator Model
Now examine the modelling of natural scenes in the world constructed by transporting gener-
ators throughout the scene to generate natural patterns. Begin by constructing general models
of the world of images motivated by the approach formulated by David Mumford based on
the tenet that the world is made up of atoms, or in our terminology generators; the images
observed are generators transported throughout the scene. Think in terms of a city scape, for
example, so that there are small areas where something “happens”, say a city block. Such struc-
tures are naturally associated with point processes. Thinking in terms of stochastic ﬁelds, such
processes must be generated from an ergodic and stationary point process N, with dependence
between the random variables N(Ek) where the Eks are compact Borel sets in the plane. Call the
set of random locations W; to each random point Wn generated by N associate a random vari-
able describing a simple geometric object, for example a city block. This will induce a random
image.
We are essentially constructing the transported generator model, a stochastic model corres-
ponding to marked-ﬁlter point processes which transports ﬁlters, henceforth called generators
throughout the world to construct complex scenes of objects. Model the random ﬁeld {I(x); x ∈
D ⊂Rd} generated by marking a spatially inhomogeneous Poisson counting process N with
spatial intensity λ(x), x ∈D so that for subsets A ⊂D, then NA, A ⊂D is Poisson distributed with
parameter

A λ(x) dx,
Pr{NA = n} = e−

A λ(x) dx (

A λ(x) dx)n
n!
.
(14.42)
For what follows, we shall exploit the important fact that on condition there being n events
in D from the spatial Poisson process intensity λ, the joint density takes the very familiar
form.
Lemma 14.17
Given Poisson occurrences with intensity λ(·), then
p(X1, . . . , Xn, ND = n) =
n

i=1
λ(Xi)e−

D λ(x) dx.
(14.43)
Proof
Pr(X1 ∈X1, . . . , Xn ∈Xn, ND = n) = Pr(ND = n|X1 ∈X1, . . . , Xn ∈Xn)
×
n

i=1
Pr(NXi)
(14.44)
= e
−

D\∪n
i=1Xi λ(x) dx n

i=1

Xi
λ(x)dxe−

Xi λ(x) dx.
(14.45)

432
14 ESTIMATION ON METRIC SPACES
 A sample image generated with ND=100
50
100
150
200
250
300
350
400
450
500
50
100
150
200
250
300
350
400
450
500
0
50
100
150
200
250
(1)
(2)
Figure 14.15 Panel 1 shows a scene constructed from CAD models of trees placed via the trans-
ported generator clutter model. Panel 2 shows the same for 3D CAD models which were ray-traced;
taken from Bitouk (see also Plate 27).
The joint occurrence density is given by the limit
p(X1, . . . , Xn, ND = n) =
lim
|Xi|→0
1
$ Xi
Pr(X1 ∈X1, . . . , Xn ∈Xn, ND = n).
(14.46)
Then the random image model is deﬁned as follows.
Deﬁnition 14.18
Deﬁne I(x), x ∈D to be from the random image model termed the
transported generator model if I(x), x ∈D is assumed to be of the superposition form
I(x) =
C
0
ND = 0

ND
n=1 h(x, Wn; Un)
ND ≥1
,
(14.47)
with generators h(·, Wn; Un), the Wn, n = 1, 2, . . . occurrence positions from a Poisson
process with intensity λ(x), x ∈D and Un, n = 1, 2, . . . the random marks.
A process in the form 14.47 is referred to as a ﬁltered point process in Snyder [42] with ﬁlter
functions h(·). Shown in Figure 14.15 are examples of a synthesis via the point process model using
tree generators. randomly placed. Panel 1 shows the tree scene with mean END = 100.
14.6.1 Characteristic Functions and Cumulants
We work with the characteristic functional of the ﬁltered process integrated against Rieman–
Stieltjes functions.
Deﬁnition 14.19
The characteristic functional for the random process {I(x), x ∈D}
integrated against sufﬁciently smooth real-valued test functions v(x), x ∈D is deﬁned as
MI(jv(·)) = E[ej

D I(x)dv(x)].
(14.48)

14.6 MODELS FOR CLUTTER
433
The cumulant generating function ŴI(xi)(jv) is the log-characteristic functional
ŴI(xi)(jv) = log E[ejvI(xi)].
(14.49)
The cumulants κk, k = 1, . . . deﬁne the cumulant generating function according to
ŴI(xi)(jv) = log E[ejvI(xi)] =
∞

k=1
(jv)k
k
κk ;
(14.50)
to get the cumulants differentiating implies (∂kŴI(x)(jv))/(∂(jv)k)|jv=0 = κk.
The characteristic functional of the transported generator model has a particularly
lovely form.
Theorem 14.20
Let {I(x), x ∈D} be from the transported generator model I(x) =

ND
n=1 h(x, Wn; Un) corresponding to generators at Poisson points with intensity λ(·) and
marks Un independent and identically distributed. Then the characteristic functional and the
joint characteristic function on any n-cylinders I(xi), i = 1, . . . , n is given by
MI(jv(·)) = e

D λ(y)E[ej

D h(x,y;U)dv(x)−1]dy,
(14.51)
where the expectation is with respect to the distribution of the mark random variable process U.
The joint characteristic function on n-cylinders becomes
MI(jv(·))|v(·)=
n
i=1 viδ(·−xi) = Ee

n
i=1 jviI(xi) = e

D λ(y)E[ej 
n
i=1 h(xi,y;U)−1]dy.
(14.52)
Proof
Apply the deﬁnition
MI(jv(·)) = E[ej

D I(x)dv(x)] = Pr(ND = 0) +
∞

k=1
Pr(ND = k)
× E[ej 
k
n=1

D h(x,Wn;Un)dv(x)|ND=k].
Then clearly the summation is unaffected by random re-orderings of the occurrence
positions, there being n! realizations of Poisson processes with the same positions for
the n-points. Thus the probability of n-events in D at particular positions x1, x2, . . . xn
(see p. 220 of [42]) is given by $n
i=1(λ(xi)/

D λ(x)dx). Evaluating the expectation
exploiting the independence of the marks gives
MI(jv(·)) = Pr(ND = 0) +
∞

k=1
e−

D λ(x)dx (

D λ(x)dx)k
k!

D λ(y)E[ej

D h(x,y;U) dv(x)]

D λ(y)dy
k
(14.53)
= e

D λ(y)E[ej

D h(x,y;U)dv(x)−1] dy,
(14.54)
completing the proof.
The joint characteristic function on the cylinders is determined by choos-
ing the function f to sift at the n-cylinder locations throughout the process,
v(·) = 
n
i=1 viδ(· −xi).
These transported generator processes are fundamentally different from Gaussian processes.
Examine the cumulant generating function ŴI(x)(jv) for the random variable I(x) is given through

434
14 ESTIMATION ON METRIC SPACES
the series expansion in terms of the cumulants or semi-invariants deﬁned from the log-moment
generating function
ŴI(x)(jv) = log MI(x)(jv) =
∞

k=1
(jv)k
k
κk.
(14.55)
Corollary 14.21
Given the transported generator model I(x) = 
ND
n=1 h(x, Wn; Un) with
generators occuring with spatial intensity λ(x), x ∈D with the mark process U independent,
then
ŴI(x)(jv) =

D
λ(y)E[ejvh(x,y;U) −1] dy,
(14.56)
with cumulants κk =

D
λ(y)E[hk(x, y; U)] dy.
(14.57)
Proof
The form for the cumulant function is generated from the log-characteristic
functional for the particular sifting function centered at x ∈D, f(·) = vδ(· −x):
ŴI(x)(jv) = log E[ejvI(x)] = log e

D λ(y)E[ejvh(x,y;U)−1]dy
=

D
λ(y)E[ejvh(x,y;U) −1] dy.
(14.58)
To get the cumulants use the fact ŴI(x)(jv) = 
∞
k=1((jv)k/k)κk which implies
∂kŴI(x)(jv)
∂(jv)k
|jv=0 = κk.
(14.59)
Irrespective of their covariance properties, such a transport model has kurtosis which is
lower bounded by 3 the kurtosis of a Gaussian ﬁeld.
Deﬁnition 14.22
The kurtosis of the random variable X is given by the fourth moment:
kX =
E(X −EX)4

E(X −EX)2	2 .
(14.60)
The transported generator model converges in the limit to a Gaussian ﬁeld with such kurtosis
as the intensity of the generators goes to inﬁnity. First deﬁne the moments and variances of the
random variable I(x) to be
mk = EI(x)k,
µk = E(I(x) −EI(x))k.
(14.61)
Then, notice from the series expansion in the cumulant generating function the moments arise:
κn = ∂nŴI(x)(jv)
∂(jv)n

jv=0
implies mk = EI(x)k
(14.62)
m1 = κ1, m2 = κ2 + κ2
1, m3 = κ3 + 3κ1κ2 + κ3
1, m4 = κ4 + 3κ2
2 + 4κ1κ3 + 6κ2
1κ2 + κ4
1.
(14.63)
We can now evaluate the kurtosis of the transported generator clutter model for various cases.
First for the general case.

14.6 MODELS FOR CLUTTER
435
Corollary 14.23
Given the transported generator model I(x) = 
ND
n=1 h(x, Wn; Un) with
generators occuring with spatial intensity λ(x), x ∈D with the mark process U independent,
the kurtosis is given by
kI(x) =
E[I(x) −EI(x)]4

E[I(x) −EI(x)]2	2 = 3 + κ4
κ2
2
= 3 +

D λ(y)E[h4(x, y; U)]dy

D λ(y)E[h2(x, y; U)]dy
	2 .
(14.64)
For the homogeneous transported generator model with h(x, y; U) = Ag(x −y) with
I(x) =
ND

n=1
Ung(x −Wn),
(14.65)
and constant spatial intensity λ with Un independent ﬁnite second moments, the kurtosis kI
is bounded below by 3 with excess according to
kI(x) =
E[I(x) −EI(x)]4

E[I(x) −EI(x)]2	2 = 3 +
E[A4]

D g4(x −y)dy

E[A2]λ

D g2(x −y)dy
	2 ,
(14.66)
EXCESS = kI −3 ≥0.
(14.67)
Proof
The numerator becomes
E[I(x) −EI(x)]4 = EI4(x) −4EI3(x)EI(x) + 6EI2(x)E2I(x) −3E4I(x)
= m4 −4m3m1 + 6m2m2
1 −3m4
1
= κ4 + 3κ2
2 + 4κ1κ3 + 6κ2
1κ2 + κ4
1 −4(κ3 + 3κ1κ2 + κ3
1)κ1
+ 6(κ2 + κ2
1)κ2
1 −3κ4
1
= κ4 + 3κ2
2.
(14.68)
This completes the proof using the fact that
E[I(x) −EI(x)]2 = m2 −m2
1 = κ2.
(14.69)
For the homogeneous case, use the formula for the kurtosis 3 + (κ4/κ2
2). and the
covariance.
Examine the special case of the homogeneous transported generator model with the marking
process determining the amplitudes only of the ﬁlter functions which are assumed shift invariant,
so that
h(x, y; U) = Ug(x −y),
assumed that the spatial process is homogeneous λ(·) = λ. This analysis suggests that the trans-
ported clutter model differs fundamentally from a Gaussian process. The kurtosis of the Gaussian
random ﬁelds are 3; only as λ →∞does the process have kurtosis of a Gaussian, kI = 3. This
suggests in the limit of densely transported clutter, the process is Gaussian.
Figure 14.16 examines experiments illustrating the class of images produced by the trans-
ported generator model. Assume that the counting process is homogeneous with spatial intensity
λ(·) = λ, then Eqn. 14.43 of Lemma 14.17 dictates that the events are i.i.d. uniformly distributed
over the image plane. Panel 1 of Fig. 14.16 shows the various trees used to study the kurtosis. Panel
2 depicts the effect of changing the tree ﬁltering function on the average kurtosis values. We use
the set of 20 trees as shown in the right panel of Fig. 14.16 for the average kurtosis evaluation. The
trees in the plot are indexed from left to right and top to bottom in a lexicographic order. Panel 2

436
14 ESTIMATION ON METRIC SPACES
0
2
4
6
8
10
12
14
16
18
20
5
10
15
20
25
 Index of tree from the tree file
 Avg. Kurtosis of sample images
 Effect of different trees on the kurtosis of the images
 ’+’ denotes experimental kurtosis values
 ’*’ denotes theoretical kurtosis values
λ=1.2000e04
0
10
20
30
40
50
60
70
80
90
100
0
5
10
15
20
25
30
Avg. Number of trees in an image ND
Kurtosis Avergaed over 50 images
 Effect of changing λ on kurtosis 
3
Figure 14.16 Left column: Figure shows the various tree generators. Right column: Top panel
2 shows kurtosis measured for different trees. Bottom panel shows average kurtosis plot as a
function of density parameter λ.
shows the values of the kurtosis computed from the corollary are compared between the analytical
formula and the empirical calculation (+ versus *). Panel 3 shows the kurtosis as lambda density
increases for one of the tree generators.
Example 14.24 (Covariance and Method of Moments Estimation)
The covariance
from the joint characteristic functional of Theorem 14.20 becomes
KI(x1, x2) =

D
λ(y)E[h(x1, y; U1)h(x2, y; U2)] dy.
(14.70)
For the case h(x, y; U) = Ug(x −y), U’s independent, then it is particularly simple as
K(x1, x2) =

D
λ(x)Eh(x1, y; U1)h(x2, y; U2) dy
(14.71)
= λE(U2)

D
g(x1 −y)g(y −x2) dy,
(14.72)
= λE(U2)

D
g(x −y)g(y)dy = K(x, 0),
where x = x1 −x2.
(14.73)
The method of moments may be used for estimating the parameters of the model,
exploit the Toeplitz structure using properties of the Fourier transformation. Deﬁning
w = (ω1, . . . ωd), where d = 2, 3 the dimension of the image data, deﬁne the Fourier
transform pairs as
G(ω) =

D
g(x)e−j<ω,x> dx,
K(ω) =

D
K(x, 0)e−j<ω,x> dx.

14.6 MODELS FOR CLUTTER
437
Exploiting the real, even symmetry of g(x), x ∈D according to
K(ω) = λE(U2)|G(ω)|2 = λE(U2)G(ω)2,
(14.74)
then the method of moments estimators take the form
λE(U2) = K(0),
g(x) =
 -
K(ω)
K(0) ej<ω,x> dx.
(14.75)
Example 14.25 (Ray Traced Imagery)
Bitouk has examined a large data set of ray-
traced target images which represent natural clutter. Ray-tracing appears to be an
especially attractive rendering technique, since it closely resembles the formation pro-
cess of natural images. The generated dataset consists of more than 10,000 images.
The ray-traced images in the dataset contain targets in a randomly synthesized terrain
taking into account occlusion, shadows and lighting variation as well as other effects
usually encountered in natural scenes. The terrain was generated by random place-
ment of trees on grass backgrounds according to the speciﬁed distribution densities.
The trees were synthesized using a random growing process controlled by physi-
cal parameters [337]. Shown in Figure 14.17 are example images from the synthetic
dataset.
Ray-tracing is a very realistic method for generating the low order statistics of
natural images. To illustrate that, Bitouk computed single-pixel derivative statistics,
recently introduced by Huang and Mumford (see [338] for details). Working with the
log intensities, the marginal distribution of the horizontal derivatives were computed
δ ln I = ln I(xi, xj) −ln I(xi, xj+1). Figure 14.17 shows the logarithm of the histogram
of δ ln I. The histogram has a sharp peak at 0 and heavy tails. In [338], the density
function f(x) of D is modeled as a generalized Laplace distribution
f(x) = 1
Z · e−|x/s|α,
(14.76)
whose parameters α and s are related to the variance and kurtosis of D by
σ 2 = Ŵ(3/α)
Ŵ(1/α)s2,
k = Ŵ(1/α)Ŵ(5/α)
Ŵ2(3/α)
.
(14.77)
The dashed curve in panel 3 (Figure 14.17) presents the model Eqn. 14.76 with
the parameters (α, s) = (0.637, 0.254) calculated from the variance and kurtosis using
Eqn. 14.77.
log(histogram)
–2
–2.5
–3
–3.5
–4
–5
–4.5
–5.5
–6
–6.5
–7
–2
–1
0
1
2
log l(i,j)–log l(i,j+1)
Figure 14.17 Panels 1 and 2 show examples from Bitouk of synthesized target chips generated via
the ray tracing algorithm. Panel 3 shows the derivative statistic δ ln I for synthetic clutter images;
solid curve is the observed, dashed curve is the best ﬁt with the generalized Laplace distribution
(α, s) = (0.637, 0.254) (see also Plate 28).

438
14 ESTIMATION ON METRIC SPACES
14.7 Robust Deformable Templates for Natural Clutter
Let us return to the clutter model of Section 14.6. One of the central problems in Automated Target
Recognition is to accommodate the inﬁnite variety of clutter in real military environments. In
model-based approaches, identiﬁcation/classiﬁcation rates are largely determined by the accuracy
of models used to represent real-world scenes. In heavily cluttered environments, it is impractical
to explicitly represent each of the objects in the scene by a deterministic 3D model whereas applying
low-dimensional statistical description of clutter may improve ATR performance in clutter. Clearly
we must robustify the deformable template approach to accomodate photometric variations which are not
explicitly modellable, and perhaps may not be associated with the geometric motions of the objects of interest.
This is an area of increasing interest, and most certainly will emerge as one of the central themes of
a good deal of research in the coming years. Unquestionably, the main challenge to model-based
methods is to acquire pose and identiﬁcation of targets while, at the same time, “seeing through”
the confounding clutter.
Examine the construction of robust deformable templates, following the work of Bitouk which
allows the accomodation of both the variations of the pose of objects as well as the photometric
variations associated with natural clutter. Construct the metric via the empirical statistics of clutter.
The group of photometric variations is modeled as a Hilbert space H ⊆L2. Since not all of
the elements of L2 correspond to observable images, the space of photometric variations H is
constructed via basis expansions {φn}. For the clutter problem, the statistics in the background are
modeled explicitly. Shown in Figure 14.18 are examples of imagery from the orbit under geometric
transformation and clutter variation.
G.I temp
H.Itemp
A.Itemp
Figure 14.18 Orbits under actions of the groups G, H and A = G ⊗H.

14.7 ROBUST DEFORMABLE TEMPLATES
439
14.7.1 The Euclidean Metric
Let us begin by examining the most straightforward formulation of the metric corresponding
to the L2 norm, so that N((∂It/∂t) ◦g−1
t
)2 = ∥(∂It/∂t) ◦g−1
t
∥2
2. Examine mappings deﬁned by
the special-Euclidean group of rigid motions G = SE(d) with action g : x →Ox + b where
O ∈SO(d), b ∈Rd. Then it is natural to impose no energy on the geometric motion. Deﬁning the
energy of the rigid motion transformation to be zero, then the energy of the path

gt, It
	
can be
deﬁned by
 1
0

(∂It/∂t) ◦g−1
t


2
dt. Adistance constructed from this energy is clearly left invariant
as described in Theorem 12.25 in Chapter 12 under the action of the rigid motion group.
Theorem 14.26
For the rigid motions gt ∈SE(d), deﬁning
ρ2((g, I), (h, I′)) =
inf
gt, It : g0 = g, g1 = h,
I0 = I, I1 = I′
 1
0
::::
dIt
dt ◦g−1
t
::::
2
2
dt,
(14.78)
then the resulting distance between images I, I′ ∈I is invariant:
˜ρ2(I, I′) =
inf
g,h∈SE(d) ρ((g, I(g), (h, I′(h))) =
inf
g∈SE(d) ρ((id, I), (g, I′(g)))
(14.79)
and reduces to the squared error minimization
˜ρ2(I, I′) =
inf
g∈SE(d) ∥I −I′(g)∥2
2.
(14.80)
Proof
Clearly ∥(dIt/dt)◦g−1
t
∥2
2 = ∥dIt/dt∥2
2 since gt are rigid motions, Jacobian deter-
minant 1, implying It is a linear interpolation of its boundary conditions (Theorem
10.5 of Chapter 10):
It = (1 −t)I(g) + tI′(h) with dIt
dt = I′(h) −I(g).
(14.81)
Substituting this back into the deﬁnition of d(I, I′) completes the proof Eqn. 14.91
establishing the connection of the variational problem for d(I, I′) to the maximum
likelihood estimation:
˜ρ2(I, I′) =
inf
g,h∈SE(d) ∥I(g) −I′(h)∥2
2 =
inf
g∈SE(d) ∥I −I′(g)∥2
2,
(14.82)
with the last equality following from the change of variables in the norm.
Discriminability clearly involves the shortest metric distance between the orbits under the
nuisance parameter between object classes. Figure 14.19 illustrates this for the Euclidean metric
calculations for several CAD models in trees in which they were at known, ﬁxed pose. Shown in
the table are the metric distances within class and across class models in clutter using the Euclidean
norm ∥· ∥2. The diagonal entries show the within class distance in clutter; smaller numbers mean
increased similarity.
14.7.2 Metric Space Norms for Clutter
Fortheclutterproblem, itisclearthatthesquarederrormetriccannotadequatelyrepresenttheorbit
of phtometric variations. The textures of clutter do not appear as an independent increment process

440
14 ESTIMATION ON METRIC SPACES
128.1
732.9
445.3
439.9
1081
575.4
312.5
578.6
631.6
1198
427.4
680.9
220.7
515.8
1189
366.6
672.7
486.1
173.5
948.5
557.4
1018
686.2
615.7
411.7
Figure 14.19 Figure shows multiple CAD models and metric distances between within class and
across class models in clutter. The diagonal entries show the within class distance in clutter; smaller
numbers mean increased similarity providing a method for clutter independent identiﬁcation.
These distances are computed by solving the minimum over all rotations.
which would correspond to the L2 norm of ∥·∥2. For this a complementary approach can be taken
from the photometric modeling of faces to model the photometric variations of the background
space (rather than the objects themselves) for generating metric distances upon which to compute
the representations. Our approach here is to craft the Hilbert-space norm N((∂It/∂t) ◦g−1
t
)2 =
∥((∂It/∂t) ◦g−1
t
∥2
H so as to reﬂect the statistics of the clutter. Stationary Gaussian Markov Random
Field models capture important statistical properties of natural clutter. In these models, the clutter
ﬁeld W(x), x ∈X ⊂Rd is characterized by a Toeplitz covariance K(x, 0), x ∈X ⊂Rd, given by
K(x, y) = K(x −y, 0).
To build the Hilbert space of photometric variability with the norm, induced by the empirical
covariance kernel, assume the covariance kernel K(x, y) to be Hilbert-Schmidt
 
K2(x, y)dxdy <
∞; then the Hilbert–Schmidt theorem implies that there exsits a complete orthonormal sequence
of eigenfunctions {φn(x)} with corresponding eigenvalues {λn} such that

K(x, y)φn(y) dy = λnφn(x).
(14.83)
Deﬁnition 14.27
Given a positive-deﬁnite covariance kernel K(x, y), deﬁne a Hilbert space
H ⊆L2 with the norm ∥· ∥H, induced by the kernel K(x, y), according to
H =
C
f ∈L2 : ∥f∥2
H =

n
⟨f, φn⟩L2
2
λn
< ∞
F
.
(14.84)

14.7 ROBUST DEFORMABLE TEMPLATES
441
Consider an element g in the special-Euclidean group of rigid motions G = SE(d) with
action g : x →Ox + b where O ∈SO(d), b ∈Rd. The kernel K(g(x), g(y)) is also Hilbert–Schmidt
and induces a corresponding Hilbert space g.H. The following lemma establishes the connection
between the norms ∥· ∥H and ∥· ∥g.H.
Lemma 14.28
For any f ∈g.H,
f
g.H =

f ◦g−1
H .
(14.85)
Proof
Let { ˜φn} be a complete orthonormal sequence of eigenfunctions of K(g(x), g(y))
with corresponding eigenvalues {˜λn}

K(g(x), g(y)) ˜φn(y) dy = ˜λn ˜φn(x).
(14.86)
A simple substitution of variables x →g−1(x), y →g−1(y) in the integral yields

K(x, y) ˜φn(g−1(y)) dy = ˜λn ˜φn(g−1(x)),
(14.87)
implying ˜φn = φn ◦g, ˜λn = λn. Then,
f
2
g.H =

n
⟨f, φn ◦g⟩2
2
λn
=

n
⟨f ◦g−1, φn⟩2

2
λn
= ∥f ◦g−1∥2
H.
(14.88)
For the rigid motions we set the energy for geometric motion in deﬁnition 14.1 to zero, then
the energy of the path

gt, It
	
can be deﬁned by the norm associated with the empirical covariance
measuring image motion:
E =
 1
0
::::
∂It
∂t ◦g−1
t
::::
2
g−1
t
.H
dt.
(14.89)
Such a deﬁnition of energy allows for the connection of the metric to classical maximum-likelihood
estimation. For an arbitrary covariance K deﬁning the quadratic form in the metric ∥· ∥H, the
measure of similarity d(I, I′) will not be a metric distance on I, since it will not be symmetric and
would not satisfy the triangular inequality. We now establish the connection between rotation and
shift invariance properties of d(I, I′) and the covariance K. This in turn makes the distance into a
metric. In fact the covariance must be rotationally invariant.
Theorem 14.29
For the rigid motions gt ∈SE(d), deﬁning the function d between images
I, I′ ∈I
d(I, I′)2 =
inf
gt, It : g0 = id,
I0 = I, I1 = I′ ◦g1
 1
0
::::
dIt
dt ◦g−1
t
::::
2
g−1
t
.H
dt
(14.90)
= inf
g1
I −I′ ◦g1
2
H .
(14.91)
Suppose K is Toeplitz and for all O ∈SO(d), K(Ox, 0) = K(x, 0), then for any h ∈SE(d) the
function d(I, I′) is invariant under its action:
d2(I ◦h, I′ ◦h) = d2(I, I′),
(14.92)
and the function d : I × I →R+ deﬁned by Eqn. 14.90 is a metric distance on I.

442
14 ESTIMATION ON METRIC SPACES
Proof
Calculating the energy gives
d2(I, I′) =
inf
gt, It : g0 = id,
I0 = I, I1 = I′ ◦g1
 1
0
::::
dIt
dt ◦g−1
t
::::
2
g−1
t
.H
dt
(a)
=
inf
gt, It : g0 = id,
I0 = I, I1 = I′ ◦g1
 1
0
::::
dIt
dt
::::
2
H
dt,
(14.93)
with (a) following from Lemma 14.28. Thus It is a linear interpolation of its boundary
conditions
It = (1 −t)I + t(I′ ◦g1)
with dIt
dt = I′ ◦g1 −I.
(14.94)
Substituting this back into the deﬁnition of d(I, I′) completes the proof of Eqn. 14.91
establishing the connection of the variational problem for d(I, I′) to the maximum
likelihood estimation.
For an arbitrary covariance d(I, I′) is not a metric distance on I; however, from
Theorem 12.5 of Chapter 12 once it is left invariant to the group then it is a metric. To
prove left invariance,
d2(I ◦h, I′ ◦h) = inf
g
::I ◦h −I′ ◦(h ◦g)
::2
H
(14.95)
= inf
g

n
|⟨I ◦h −I′ ◦h ◦g, ψn⟩2|2
λn
(14.96)
(a)
= inf
g

n
|⟨I −I′ ◦h ◦g ◦h−1, ψn ◦h−1⟩2|2
λn
(14.97)
= inf
g
:::I −I′ ◦h ◦g ◦h−1:::
2
h−1.H
(14.98)
with (a) following from the substitution of variables x = h−1(y). Since the Toeplitz
covariance K(x, 0) is invariant with respect to rotations, then h−1.H = H and the
distance reduces to d2(I, I′).
The proof of symmetry and triangular inequality follows from the invariance of
the metric.
14.7.3 Computational Scheme
Corollary 14.30
For the rigid motions gt ∈SE(d) with Jt = It ◦g−1
t
, then deﬁning t to
be the skew-symmetric matrix satisfying dOt/dt = tOt, βt = dbt/dt −tbt, then
d(I, I′) =
inf
(t), β(t) : 0 = 0, β0 = 0
Jt : J0 = I, J1 = I′
 1
0
::::
∂Jt
∂t + ⟨∇Jt, tx + βt⟩Rd
::::
2
g−1
t
.H
dt, .
(14.99)

14.7 ROBUST DEFORMABLE TEMPLATES
443
Proof
Computing the derivative of It = Jt ◦gt yields
dIt
dt = ∂Jt
∂t ◦gt +
D
∇Jt ◦gt, t (Otx + bt) + βt
E
Rd ,
(14.100)
which substituted gives Eqn. 14.99.
This property of the metric distance d(I, I′) allow an efﬁcient alternating minimization algo-
rithm for the optimization problem 14.29. In fact, Eq. 14.94 implies that ﬂow Jt = It ◦g−1
t
is also
given as a linear combination of its boundary conditions
Jt = (1 −t)

I ◦g−1
t

+ t

I′ ◦

g1 ◦g−1
t

.
(14.101)
On the other hand, with Jt ﬁxed, velocities t and βt are given as the solution of a simple
quadratic optimization problem
(t, βt) = arg inf
t,βt
 1
0
::::
∂Jt
∂t + ⟨∇Jt, tx + βt⟩Rd
::::
2
H
dt.
(14.102)
Thus, both the minima in Jt and t, βt with, respectively, t, βt and Jt ﬁxed, are given
by closed-form expressions. This suggests the following alternating minimization algorithm for
computing d(I, I′).
Algorithm 14.31 (Bitouk)
Starting with g(0)
t
= id and J(0)
t
= (1 −t)I + tI′, we apply the
following iterative scheme
1.

(n+1)
t
, β(n+1)
t

= arg inft,βt
 1
0
::::
∂J(n)
t
∂t
+
2
∇J(n)
t
, tx + βt
3
Rd
::::
2
H
dt.
2. g(n+1)
t
=

O(n+1)
t
, b(n+1)
t

,
where O(n+1)
t
=
 t
0 exp

(n+1)
σ

dσ,
and b(n+1)
t
=
 t
0
=
O(n+1)
t
−O(n+1)
σ
>
β(n+1)
σ
dσ.
3. J(n+1)
t
= (1 −t)I ◦

g(n+1)
t
−1
+ t

I′ ◦g(n+1)
1

◦

g(n+1)
t
−1
.
The iterative process is expected to converge to the values of Jt, t and βt which attain
the inﬁmum for d(I, I′). Shown in Figure 14.20 are examples from the tanks in dense foliage
corresponding to obscuring clutter. The panels show image ﬂow along the geodesic generated
by the algorithm J(x, tk) at tk = k/5, k = 0, 1, . . . , 5.
(1)
(2)
(3)
(4)
(5)
Figure 14.20 Panel 1 shows the tanks in dense foliage corresponding to obscuring clutter. Panels
show image ﬂow along the geodesic under the Euclidean norm ∥· ∥2 for Jtk (x) at tk = k/5, k =
0, 1, . . . , 5. Results taken from Bitouk.

444
14 ESTIMATION ON METRIC SPACES
14.7.4 Empirical Construction of the Metric from Rendered Images
Although it has been pointed out by many authors that natural images are strongly non-
Gaussian [338,339], second-order statistics capture important information about spatial correlation
between pixels. To verify that, we sampled images from distributions speciﬁed by learned Toeplitz
covariances.
Given training images, deﬁned on the N × N lattice of the image plane, I(xi, xj), assume
periodic boundary conditions I(xi, xj) = I(xi+N, xj) = I(xi, xj+N). The Toeplitz covariance repre-
sented by the empirically estimated mean µ = 1/N2 
N
i=1

N
j=1 I(xi, xj) and the N ×N covariance
matrix
K(i, j) =
1
N2
N−1

k=0
N−1

l=0

I(xk, xl) −µ
	 
I(xi+k
mod N, xj+l
mod N) −µ

.
(14.103)
The power spectral density S is given by the discrete Fourier transform of the covariance
S(u, v) =
N−1

k=0
N−1

l=0
K(k, l)e(−j2π/N)(uk+vl).
(14.104)
The eigenfunctions of the cyclo-stationary covariance K(i, j) are complex exponentials φu,v(k, l) =
e−j(2π/N)(uk+vl), and the eigenvalues λu,v are given by the power spectral density, λu,v = S(u, v).
Images can be synthesized according to
Isynth(xi, xj) =
N−1

u=0
N−1

v=0

λu,vXu,vφ∗
u,v(i, j)
(14.105)
Figure 14.21 Top row : observed images. Bottom row : images synthesized using the second-order
model. Images taken from Bitouk.

14.7 ROBUST DEFORMABLE TEMPLATES
445
as second-order random ﬁelds speciﬁed by Toeplitz covariance K(i, j), where, by the Kahunen–
Loeve theorem, Xu,v is an independent increment process, E(Xu,vXu′,v′) = δ(u −u′)δ(v −v′)
Figure 14.21 shows pairs of the observed images and the corresponding images synthesized using
the GMRF model.
14.8 Target detection/identiﬁcation in EO imagery
Bitouk has examined the performance of optimum detectors in natural clutter. For the clutter
experiments conducted by Bitouk, a dataset of ray-traced target chips were generated to represent
natural clutter. The generated dataset consists of more than 10,000 images. The ray-traced images
in the dataset contain targets in a randomly synthesized terrain taking into account occlusion,
shadows and lighting variation as well as other effects usually encountered in natural scenes. Ter-
rain was generated by random placement of trees on grass backgrounds according to the speciﬁed
distribution densities. Examples of such imagery are shown in Figure 14.17.
In a statistical framework, target detection is formulated as a hypothesis testing problem
and tackled using classic detection theory [328]. Assume a binary scenario with two hypotheses
H0 and H1, where under the hypothesis H0, the observed image contains no targets, ID = W, and,
under H1, the target is present, ID = Itemp(g) + W. Note that the pose g is unknown and plays the
role of a nuisance parameter. Generalized likelihood ratio test yields the following decision rule
written via the generalized likelihood ratio or in terms of metric distances between the observed
image ID and the template:

ID

2
H
H1
>
<
H0
inf
g∈G

ID −Itemp ◦g


2
H + ν
(14.106)
d2(ID, 0)
H1
>
<
H0
d2(ID, Itemp) + ν.
(14.107)
In this equation the threshold ν is changed to generate different points along the receiver operating
curve (ROC). The probability of correct detection PD and the probability of a false alarm are given
by PD = Pr(H1|H1) and PF = Pr(H1|H0).
The top row of Figure 14.22 displays ROC curves for different clutter densities corresponding
to the covariance norm ∥.∥H where the covariance matrix K was estimated from the set of training
images of clutter and usual Euclidean norm ∥.∥.
Let H0 and H1 be the hypotheses corresponding to the presence of the class 0 or class 1
target in the observed image, respectively. In our numerical experiment class 0 corresponds to a
Jeep vehicle, and class 1 corresponds to a T72 tank. The generalized likelihood ratio test or the
GLRT decision rule involving the comparison of metric distances from the observed image ID to
the corresponding templates is given by
inf
g∈G

ID −I(0)
temp ◦g


2
H
H1
>
<
H0
inf
g∈G

ID −I(1)
temp ◦g


2
H + ν
(14.108)

446
14 ESTIMATION ON METRIC SPACES
PD
PD
PF
PF
PFC
PCC
PCC
PFC
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0 0
0.2
0.4
0.6
0.8
1
1
0.9
0.8
0.7
0.6
0.3
0.4
0.5
0.2
0.1
0 0
0.2
0.4
0.6
0.8
1
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
00
0.2
0.4
0.6
0.8
1
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
00
0.2
0.4
0.6
0.8
1
Figure 14.22 Top row: ROC curves for detection of a T72 tank for different densities of clutter. Solid
line—covariance norm; dashed line—Euclidean norm. Bottom row: ROC curves for classiﬁcation
of a T72 tank versus a Jeep for different densities of clutter. Solid line—covariance norm; dashed
line—Euclidean norm. Results from Bitouk.
d2(ID, I(0)
temp)
H1
>
<
H0
d2(ID, I(1)
temp) + ν.
(14.109)
The probability of correct classiﬁcation PCC and the probability of false classiﬁcation PFC
are given by PCC = Pr(H1|H1) and PFC = Pr(H1|H0). The bottom row of Figure 14.22 shows the
ROC curves for target classiﬁcation using the covariance norm ∥.∥H where K was estimated from
the set of training images of clutter and Euclidean norm ∥.∥.

15 I N F O R M AT I O N B O U N D S F O R A U T O M AT E D
O B J E C T R E C O G N I T I O N
ABSTRACT
Examine information bounds for understanding rigid object recognition involv-
ing the low-dimensional matrix groups. Minimum-mean-squared error bonds and mutual
information bounds are derived for recognition and identiﬁcation.
15.1 Mutual Information for Sensor Systems
15.1.1 Quantifying Multiple-Sensor Information Gain Via Mutual
Information
Examine information bounds for understanding rigid object recognition involving the low-
dimensional matrix groups. Minimum-mean-squared error bonds and mutual information bounds
may be derived for recognition and identiﬁcation. The source–channel view provides the abso-
lutely optimal basis for which the source characterization and channel characterization must be
combined for optimal information use. This source/channel decomposition allows for the consid-
eration of the information contained in the prior distribution expressing prior knowledge about
conﬁgurations of patterns before measurement, and the statistical likelihood function associated
with the observed image data expressing the way the sensor collects information from the world.
This source–channel view of pattern transformation provides the framework for the optimal basis
for characterizing the information ﬂow through remote sensors. The posterior density summa-
rizes all of the information that the sensing channel data provides about the source of imagery.
The information gain by the observer through the sensor is measured via the mutual information
measure, the difference in entropies between the prior distribution before data observations, and
after measurements.
Deﬁnition 15.1
The mutual information between I ∈I and ID ∈ID is given for
continuous parameter spaces according to
h(I; ID
1 , . . . , ID
n ) = h(I) −h(I|ID
1 , . . . , ID
n )
= Ep(I,ID
1 ,...,ID
n ) log
p(I|ID
1 , . . . , ID
n )
p(I)
.
(15.1)
Entropies and mutual information for discrete distributions are denoted in capitals:
H(I; ID
1 , . . . , ID
n ) = H(I) −H(I|ID
1 , . . . , ID
n )
(15.2)
= EP(I,ID
1 ,...,ID
n ) log
P(I|ID
1 , . . . , ID
n )
P(I)
.
(15.3)
We notice that the inequalities for conditional entropies implies information gain is
monotonic in sensor measurements.
Theorem 15.2
Let ID
1 , ID
2 , . . . , be independent observations of the image I ∈I, then the
information gain about I is monotonic in sensors:
h(I; ID
1 , . . . , ID
n ) ≥h(I; ID
1 ),
n ≥1,
(15.4)
H(I; ID
1 , . . . , ID
n ) ≥H(I; ID
1 ),
n ≥1.
(15.5)
447

448
15 INFORMATION BOUNDS
Proof
The monotonicity of the information is implied by the entropy inequality
H(I|ID
1 ) −H(I|ID
1 , . . . , ID
n ) = E{log P(I|I1, . . . , ID
n )
P(I|I1)
(15.6)
≥0,
(15.7)
implying the change in information is ordered, H(I; ID
1 , . . . , ID
n ) ≥H(I;ID
1 ). Similarly
for the mutual information involving the densities.
Operating in the Bayes setting implies that information increases performance. We have
already seen this in terms of the Fisher information which is monotonic in the number of obser-
vations. The mutual information between two sets of random variables directly measures the
dependence between them [7]; we use it to quantify the dependence between the remote obser-
vation of the scene and the object parameters of pose, position, and class. Mutual information
is a fundamental performance metric providing the bound on communication rate for errorless
communication of the parameters describing the scene via the remote-sensing channel. View
the observation image data ID as a random vector statistically processed to infer the random
parameters of object pose and signature.
Example 15.3 (Pose estimation Cooper [340])
Take as the random element being
transmitted elements of the orthogonal group O ∈SO(2) and identify with the under-
lying imagery with the rotation group generating it I(·) = Iα(O·) ↔O ∈SO(2). The
prior density p(Iα(O)), I ∈I is induced via the uniform prior p(O), O ∈SO(2). The
likelihood p(ID|Iα(O)) is determined by the various sensors: VIDEO, LADAR, and
RADAR, and the posterior summarizes all of the information that the multisensor
data provides about the source. The mutual information gain about the orthogonal
motion is given by
h(O; ID
1 , . . . , ID
n ) = h(O) −h(O|ID
1 , . . . , ID
n ).
(15.8)
For the imaging model Cooper takes the perspective projection according to the map-
ping (x1, x2, x3) →(x1/x3, x2/x3). Then TI∝(O) is the mean-ﬁeld conditioned on the
geometric transformation O and signature Iα. The measurements ID are modeled as a
Gaussian random ﬁeld with mean the projective transformation of the scene TI∝(O)
in noise, variance σ 2 in each pixel i = 1, 2, . . . d. Take the prior p(·) to be uniform on
the circle, the posterior density becomes proportional to
p(O|ID) =
1
(2πσ 2)d/2 e−1/2σ 2∥ID−TIα(O)∥2,
O ∈SO(2).
(15.9)
Sensor fusion is readily accommodated via the use of the joint likelihood. Figure 15.1
shows entropies and mutual informations for the FLIR and VIDEO sensors. To calcu-
late the entropies Cooper discretizes the integrals over SO(2) numerically evaluating
the discrete set O = {om : m = 1, . . . , M} ⊂SO(2), placing a uniform prior on
O. For this reason denote the entropies H(·) with capital symbols. Panel 1 shows
the conditional entropies of the object pose for the FLIR sensor H(O|ID
FLIR) (solid)
and video sensor H(O|ID
VIDEO) (dashed). The conditional entropy for the fusion case
H(O|ID
FLIR, ID
VIDEO) is shown via the x’s. Panel 2 shows the mutual information. Note
that posterior entropy is decreased by increasing the signal and fusing multiple sen-
sors, implying the mutual information is increased. Panel 3 examines the dependence
of the mutual information gain on object geometry ID, H(O; ID) is plotted for three
CAD models, T62 tank (dot-dashed), HMMV (solid), and truck (dashed). Note the
relative variations of the curves for the three objects.

15.1 MUTUAL INFORMATION FOR SENSOR SYSTEMS
449
–20
–10
0
10
20
30
40
0
1
2
3
4
5
6
7
0
1
2
3
4
5
6
7
SNR (dB)
H(O|D)
Conditional entropy H(O|D)
(1)
(2)
(3)
–20
–10
0
10
20
30
40
SNR (dB)
DH(O;D)
Mutual information DH(O;D)
–20 –15 –10
–5
0
5
10
15
20
25
30
0
1
2
3
4
5
6
7
SNR (dB)
I(O;ID)
Mutual information: optical imager
FLIR 
VIDEO
JOINT
FLIR 
VIDEO
JOINT
HMMV
Truck
T62
Figure 15.1 Panel 1 shows the conditional entropies H(O|ID
FLIR) (—),
H(O|ID
VIDEO) (- - -),
H(O|ID
FLIR, ID
VIDEO) (x) are plotted versus increasing SNR. Panel 2 the mutual informations H(O; ID
FLIR)
(—), H(O; ID
VIDEO) (- - -), H(O; ID
FLIR, ID
VIDEO) (x) are plotted versus increasing SNR. Panel 3 shows
Bits expressed in log base 2.
15.1.2 Quantifying Information Loss with Model Uncertainty
Model uncertainty comes in many forms. We shall examine extensively the variable signature
model in which the underlying image ﬁeld I(·) is modelled as an element of a Hilbert space in
the orbit deﬁned by a basis expansion. Inference and information gain involves both the pose
and identity of the objects as well as the signature of an inﬁnite dimensional parameter. Model
the template Itemp as a conditionally scalar-valued Gaussian random ﬁeld. The orbit is the set of
all imagery generated from the template and a linear combination of basis elements φ1, . . . , φN
spanning the temperature signatures:
I = {I(g),
I(·) =
N

n=1
Inφn(·), φ1, . . . , φN, g ∈H}.
(15.10)
The imagers are modeled using perspective projection, according to the mapping (x1, x2, x3) →
(x1/x3, x2/x3). Then TI(g) is the mean-ﬁeld conditioned on the geometric transformation g and
signatureI. ThemeasurementsID aremodeledasaGaussianrandomﬁeldwithmeantheprojective
transformation of the scene TI(g) in noise, variance σ 2. The signature and pose are assumed condi-
tionally independent implying that the posterior density is the product of the data-likelihood and
the joint prior. Take the prior p(·) to be uniform on the circle with the object signatures modeled as
a Gaussian random ﬁeld; In, n = 1, . . . , N are independent normal with zero mean and variance

450
15 INFORMATION BOUNDS
λn the eigenvalue of the nth eigensignature φn. The posterior density on I = Iαg becomes
P(I(g), I|ID) =
1
Z(ID)p(ID|I(g), I)p(I(g))p(I),
(15.11)
=
1
(2πσ 2)d/2 e−1/2σ 2∥ID−TI(g)∥2
N

n=1
1
√2πλn
e−(I2
n/2λn).
(15.12)
Note, that we are assuming the template signature has a ﬁnite expansion. 44 Several forms of
the mutual information are used to quantify performance and model uncertainty, these being the
mutual information gain given the model parameters. The forms of mutual information which play
the key role are h(I(g); ID, I) and h(Iα(g); ID), these two distinguishing between the situation
where the model signature is explicitly known and performance is the average over all conditional
models versus a high dimensional nuisance variable which must be integrated out:
h(I(g); ID, I) = −E{log p(I(g))} + E{log p(I(g)|ID, I)},
(15.13)
h(I(g); ID) = −E{log p(I(g))} + E{log p(I(g)|ID)}.
(15.14)
Naturally, model uncertainty results in loss of information in the image parameters. This is
expressed via the Kullback–Liebler under the models.
Corollary 15.4
The information loss is nonnegative given by the Kullback–Liebler distance:
h(I(g); ID, I) −h(I(g); ID) = D(p(I(g)|ID, I)||p((I(g))|ID))
(15.15)
≥0.
(15.16)
Proof
Conditioning decreases entropy, Theorem 15.2.
Example 15.5 (Pose estimation for signature variability (Cooper[335]))
Cooper[335]
has studied estimation of pose through lighting and signature variation. Deﬁne the
image signature indexed over the surface. Inference of I(s) reduces to the inference
of pose and temperature proﬁle. Model the signature I as a scalar-valued Gaussian
random ﬁeld with mean corresponding to the template at ﬁxed inertial coordinates
under the identity transformation. The orbit is the set of all imagery generated from
the template and a linear combination of basis elements φ1, . . . , φN spanning the
signatures.
The N-dimensional basis is constructed as eigenfunctions of the empirical
covariance generated from many realizations of actual signatures from varying illu-
minations of the objects (see Figure 14.8). The openGL simulation software is used for
generating the database of illuminations. The top row shows a variety of signatures
corresponding to varying illumination. The ﬁrst several eigensignatures φ1,2,3(·) are
used in generating the simulations (see Figures 14.6, 14.6, Section 14.2.1 , e.g.).
The posterior density on the pose parameter becomes
p(O, I|ID) =
1
Z(ID)p(ID|O, I)p(O)p(I),
(15.17)
=
1
(2πσ 2)d/2 e−1/2σ 2∥ID−TI(O)∥2
N

n=1
1
√2πλn
e−I2
n/2λn,
(15.18)
44 Otherwise, we must introduce the ﬁnite cylinder expansion πN = $N
n=1(1/√2πλn)e−(I2n/2λn) and pN =
p(ID|I1, . . . , IN) and examine the estimation problem in the limit lim
N→∞pNπN .

15.1 MUTUAL INFORMATION FOR SENSOR SYSTEMS
451
where I = 
N
n=1 Inφn. Cooper [333, 335] has studied signature variation in infrared
imagery (see below).
Examine the forms of mutual information which quantify model uncertainty, the
ﬁrst h(O; ID, I) measuring the average information gain conditioned on knowledge
of the model signatures, and the second h(O; ID) measuring the average information
gain unconditional:
h(O; ID, I) = h(O) −h(O|ID, I),
(15.19)
= −E{log p(O)} + E{log p(O|ID, I)}.
(15.20)
h(O; ID) = h(O) −h(O|ID),
(15.21)
= −E{log p(O)} + E{log p(O|ID)}.
(15.22)
For the second conditional entropy, the conditional density p(O|ID) is calculated using
Bayes rule over all conditioning signature ﬁelds:
p(O|ID) = E{p(O|ID, I)|ID}.
(15.23)
Cooper performs this calculation using Monte Carlo random sampling in the orbit by
exploiting the fact that the signatures I ∈I are modelled as Gaussian in the principle
component expansion. An estimate of p(O|ID) is computed by generating n = 1, . . . , N
signature samples In ∈I which are Gaussian distributed ˆpn and then computing the
empirical average exploiting the property
lim
N→∞
1
N
N

n=1
p(O|ID, In) = p(O|ID).
(15.24)
Panel 1 of Figure 15.2 shows H(O; ID) and H(O; ID, Iα), the average infor-
mation gain in object pose given the FLIR imagery and the correct object signature.
Conditioning on the signature reduces the entropy, implying that these forms of
mutual information are ordered:
H(O; ID, I) = H(O) −H(O|ID, I)
≥H(O) −H(O|ID) = H(O; ID).
The solid curve, H(O; ID, I), quantiﬁes the average information gain due to noisy
FLIR observations over the space of possible object signatures when correct signature
information is available a priori. The dashed curve, H(O; ID) quantiﬁes the average
information gain without a priori signature information. The difference between the
two, shown in panel 2 represents the average cost of the unknown signature in terms
of information regarding the object pose.
ThebottomrowofFigure15.2showstheinformationlossassociatedwithincom-
plete model speciﬁcation. The dashed lines show pose information loss assuming
improper template signatures. The x’s show the performance loss of the marginalized
model. Note that it loses the least information since it hedges its bets by comput-
ing the Bayes marginal over all possible template signatures. The information loss is
measured in each case via the Kullback–Liebler number between the models D(p||q):
H(O; ID, I) −H(O; ID) = H(O|ID) −H(O|ID, I)
(15.25)
= E
C
log p(O|ID, I)
p(O|ID)
F
.
(15.26)

452
15 INFORMATION BOUNDS
–20
–15
–10
–5
0
5
10
15
20
25
30
0
1
2
3
4
5
6
7
Mutual information curves
(1)
(2)
(3)
(4)
SNR (dB)
–20
–15
–10
–5
0
5
10
15
20
25
30
SNR (dB)
Mutual Information
0
0.05
0.1
0.15
0.2
0.25
0.3
Average information loss
Information Loss
–15
–10
–5
0
5
10
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
Information loss due to model mismatch: ttrue = t8
Information loss due to model mismatch: ttrue = t75
K-L Distance
SNR (dB)
–15
–10
–5
0
5
10
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
K-L Distance
SNR (dB)
D(π(O|ID,T=t8)||π(O|ID,T=t240))
D(π(O|ID,T=t8)||π(O|ID,T=t75))
D(π(O|ID,T=t8)||π(O|ID))
D(π(O|ID,T=t75)||π(O|ID,T=t240))
D(π(O|ID,T=t75)||π(O|ID,T=t8))
D(π(O|ID,T=t75)||π(O|ID))
I(O;ID,T)
I(O;ID)
Figure 15.2 Top row: Panel 1 shows the mutual information for the FLIR sensor H(O; ID, I) (—-)
and H(O; ID) (- - - -), showing the information loss due to the absence of knowledge of the object
signature. Panel 2 shows the average information loss due to signature variation, as measured by
the Kullback–Leibler distance, D(p(O|ID, I)∥p(O|ID)), versus increasing signal to noise ratio. Bottom
row: Panels 3, 4 measure information loss between the marginalized model and the true signature
model I = 8 (panel 3) and I = 75 (panel 4). Each panel shows the information loss taken by
choosing models with the incorrect signature as indicated by the dashed lines. The marginal model
information loss is shown by the x’s. In both panels, information loss is minimized by the Bayes
signature model without a priori assumption of the object signature. Bits expressed in log base 2.
The Bayes signature model minimizes the information loss. Modeling the object
signature via marginalizing the joint posterior density p(O, I|ID) gives the minimal
information loss.
15.1.3 Asymptotic Approximation of Information Measures
Example 15.6 (Gaussian Entropy and Fisher Information)
Wenowexamineasymp-
totic approximations to the posterior distribution following [60, 325] and calculate
asymptotic forms of the posterior entropy assuming the SNR →∞. Let θT denote the
true value of . The Fisher information is computed as
F(θT) =
::::
d
dθ TI(θT)
::::
2
2σ 2
.
(15.27)

15.1 MUTUAL INFORMATION FOR SENSOR SYSTEMS
453
–20
–10
0
10
20
30
40
0
2
4
6
8
10
12
14
SNR (dB)
Conditional entropy H(Θ|ID)  (bits)
solid – empirical
+ – quadratic (Fisher Information) 
–20
–15
–10
0
5
10
4
5
6
7
8
9
10
11
12
Figure 15.3 Top row shows CAD models. Bottom row shows empirical entropies for pose versus
analytic entropy predicted by the variance of the Fisher information (see also Plate 29).
Figure 15.4 shows the empirical entropy (solid curves) and quadratically
approximated (dashed curves).
Example 15.7 (Bitouk)
Shown in Figure 15.3 are results on approximation of the
entropy via the Fisher information. The top row shows the CAD models. The bottom
row shows the empirical entropies for pose versus analytic entropy predicted by the
variance of the Fisher information.
Example 15.8 (Relationship to the Hilbert Schmidt bound)
The second estimate of
the Fisher information is based on the HS pose estimator [341,342]. This estimate is a
function of the mean squared error
MSE(O(θT)) =

ID ∥ˆOHS(ID) −O(θT)∥2p(ID|θT) d(ID),
(15.28)
evaluated via Monte Carlo random sampling. We relate the HS squared error on SO(2)
to squared error on [0, 2π) by Taylor’s series expansion:
∥O(θ) −O(θT)∥2 = 4 −4 cos(θ −θT) = 4 −4

1 −1
2(θ −θT)2 + · · ·

(15.29)
≃2(θ −θT)2,
(15.30)

454
15 INFORMATION BOUNDS
implying
F(θT) ∼
1
2MSE(O(θT))
−1
.
(15.31)
Figure 15.4 shows the Gaussian entropy calculated using the Hilbert–Schmidt
bound as an estimate of the Fisher information. The column shows the T62 tank at
SNR = 5 dB (top) and 30 dB (bottom). For these experiments the discrete set of possi-
ble orientations is {θm = πm/1800 : m = 0, . . . , 900} to obtain more accurate results at
high SNR. The results appear in column 2 of Figure 15.4 for θT = 30◦. The numerical
results are the solid curves. The dot–dashed curves are the quadratic approximation
based on the empirical variance, and the dashed curves are the quadratic approxi-
mations based on the HS squared error. Note the range of SNR values for which the
quadratic approximations closely ﬁt the numerical results. The accuracy with which
the quadratic form approximates the numerical results suggests that for object recog-
nition estimation scenarios with low dimensional parameter spaces, the information
measures can be modeled asymptotically for general ranges of SNR. In the simulations
of Figure 15.4, H(|iD) achieves its maximum of log2(900) = 9.81378 bits.
Example 15.9 (Commanche FLIR)
Now examine the estimation of the signature and
pose simultaneously using maximum a posteriori (MAP) estimation. The approach is
to maximize the posterior probability over the joint parameter space, SO(2)×R20. The
maximization is accomplished via steepest descent using numerical approximations
to the derivative of the joint posterior. The observations are generated by PRISM.
These results are extended to real FLIR imagery in Figure 15.5. Shown in Figure
15.5 are MAP estimators for the Commanche data courtesy of Dr. J. Ratches of the
Night Vision Laboratory Electronic Sensors Directorate (NVESD). Shown in the top
row are FLIR observations of a tank model from the Commanche data set courtesy
of Dr. J. Ratches U.S. Army NVESD. The bottom row shows renderings of the MAP
estimates of the orientation and signature from the corresponding observation of the
top row generated by estimating the principle modes of temperature variation of
the PRISM data and estimating rotations, translations, and temperature variation to
match the Commanche data set. The parameters of this representation are the principle
0
5
10
15
20
25
30
35
40
45
50
0
1
2
3
4
5
6
7
8
9
10
SNR (dB)
Entropy (Bits)
Posterior entropy H(θ|iD) θT = 30
(1)
(2)
Numerical
Quadratic
Quadratic–HS
Figure 15.4 Column 1: Video imagery of the T62 tank at SNR = 5 dB (top) and 30 dB (bottom).
Column 2: Numerical (—) and analytic (- - -) approximations to H(|iD) for θT = 30◦.

15.1 MUTUAL INFORMATION FOR SENSOR SYSTEMS
455
eigenfunctions and eigenvalues which represent all of the orientations and translation
imagesfromtheCommanchedataset. Thebottomrowshowstheestimatesofposeand
temperature. In each case, the algorithm was initialized with orientation broadside,
gun facing to the right. The initial object signature was the mean signature from the
database. The signature was represented using the N = 20 dimensional subspace of
signature principal components. In all three cases the correct object orientation was
estimated.
Shown in the bottom row of Figure 15.5 are the results of studying the bounds
for the Commanche database. The SNR model for FLIR from chapter 13, Eqn. 13.40
based on the mean object and background pixel intensity is used to estimate the
operating SNR of 5.07 dB of the FLIR sensor for the NVESD data. The plots in
the bottom row of Figure 15.5 show the mean squared error loss (left panel) and
–10
–5
0
5
10
15
20
25
30
0
0.02
0.04
0.06
0.08
0.1
0.12
SNR (dB)
Mean squared HS error
MSE Curves OT = 30° tT = t8
–20 –15 –10 –5
0
5
10
15
20
25
30
0
1
2
3
4
5
6
7
SNR (dB)
Mutual information (Bits)
I(O;ID,t8) and I(O;ID) Curves
Known Signature
Unknown Signature
Known Signature
Unknown Signature
Figure 15.5 Top row shows FLIR observations courtesy of Dr. J. Ratches U.S. Army NVESD. Second
row shows renderings of the MAP estimates of orientation and signature from the corresponding
observation of the top row. Third row: Panel 7 shows the mean squared error for orientation
estimation via FLIR with and without signature information. Panel 8 shows the mutual information
curves for FLIR with and without signature information. In both cases, the solid lines correspond to
performance with signature information, and the dashed curves without signature information.
The operating SNR of the NVESD Commanche FLIR data is indicated on the curves by the solid
dots. Bits expressed in log base 2. (see also Plate 30).

456
15 INFORMATION BOUNDS
information loss (right panel) due to object signature variation, labeling the points
on the performance curves corresponding to the operating SNR of the FLIR sensor.
The mean-squared-error curves are computed using the Hilbert–Schmidt estimator
conditioning on the object orientation θT = 30◦. Note that the information loss due
to signature uncertainty is 0.86 bits with associated mean squared error performance
loss by a multiplicative factor of 2.173 = 0.0213/0.0098. This is consistent with classic
MSE bounds which relate mutual information with estimation accuracy. For example,
from Shao et al. [343] we have
E{(θ −ˆθ(ID))2} ≥
1
2πe exp(2 max
p(θ) [H(θ) −I(θ; ID)]) =
1
2πe exp(2 max
p(s) H(θ|ID)),
(15.32)
which would predict a loss by the factor of 20.86 = 1.815 which is quite similar to the
2.173 loss actually observed.
15.2 Rate-Distortion Theory
Information theory answers directly the question of how complex an object database (source
codebook) is and therefore how much computation does the object recognition system (decoder)
have to perform to achieve optimal performance. With rate distortion, we can answer the following
question precisely: Given limited computational and storage resources for object recognition, which
orientations or codebook elements are most informative so as to to minimize probability of error in
the repesentation?
Consider the problem of determining an estimate ˆX of X; clearly any estimation approach
which requires searching for the most likely over the parameter spaces X for the continuum of
scale, rotation, and translation groups is too large. Hence we need to deﬁne a database (codebook)
the set ˆX ⊂X for doing the estimation. If we choose | ˆX| << X, what is the penalty? In general,
what is the optimum choice for the database. How should ˆX be chosen? Is the choice sensitive at
all to the geometry or reﬂectivity of the object whose parameters we are trying to estimate? Clearly
if the object has symmetries with respect to the sensor, one need not store extra copies of the same
object.
15.2.1 The Rate-Distortion Problem
We now examine the problem of constructing codebooks for a random source X ∈X with density
p(x), x ∈X. One approach would be to brute force generate a codeword for every possible value
of X ∈X. Clearly, if X is the continuum this is impossible. Even for discrete sources, the goal
may be to deﬁne approximating sets which for a minimum complexity (logarithmic size) have
optimal approximation properties, i.e. they minimize distortion over all approximations of the
same complexity. This places us ﬁrmly in the rate-distortion theory in which complexity or rate of
codebooks is traded off against approximation accuracy or distortion.
Deﬁnition 15.10
Deﬁne ˆX to be the reproduction alphabet for X. Deﬁne the penalty
or distortion function
d : X × ˆX →R+,
(15.33)

15.2 RATE-DISTORTION THEORY
457
where R+ denotes the set of nonnegative reals. The distortion measure d quantiﬁes how bad it
is to assume that ˆX is the real parameter when the real parameter is X.
For each realization x ∈X, the goal is to choose that ˆx ∈ˆX that minimizes d. This is not directly
possible since X is a random variable with conditional probability p(x), x ∈X. A reasonable thing
to do is to minimize the expected distortion. This is the standard rate-distortion theory problem
(see, e.g. Chapter 9 of [344], Chapter 13 of [7], or [345]) where X corresponds to the source alphabet,
ˆX the reproduction alphabet and d(x, ˆx) is the ﬁdelity criterion. Rate-distortion theory dictates that
for any speciﬁed average cost D there exists a nonnegative number R(D) such that if R > R(D)
then for n sufﬁciently large there exist encoding and decoding functions
f : X n →{1, 2, . . . , 2nR},
(15.34)
g : {1, 2, . . . , 2nR} →ˆX n,
(15.35)
such that
E[dn(Xn, g(f(Xn)))] ≤D,
(15.36)
where Xn = X1, . . . , Xn is a sequence of n-observations from a stationary, ergodic process, and dn
is the average distortion deﬁned via d(·) by
dn(Xn, ˆXn) = 1
n
n

i=1
d(Xi, ˆXi).
(15.37)
Conversely, if R < R(D) then such functions do not necessarily exist.
Rate-distortion theory dictates that in order to estimate a sequence of parameters with aver-
age distortion D from a sequence of observations X1, . . . , Xn, we need at least 2nR(D) elements in
the n-fold cartesian product ˆX × · · · × ˆX.
The function R(D) is the rate-distortion function. When X1, . . . , Xn is a sequence of indepen-
dent and identically distributed random variables, then
R(D) =
min
q(·|x):Ed(X, ˆX)≤D
H(X; ˆX) =
min
q(·|x):Ed(X, ˆX)≤D

x

ˆx
p(x)q(ˆx|x) log
q(ˆx|x)
q(ˆx)

, (15.38)
where H(X; ˆX) is the mutual information between X and ˆX and the minimum is over all con-
ditional probabilities q(ˆx|x), ˆx ∈
ˆX, x ∈X, 
ˆx q(ˆx|x) = 1 satisfying Ed(X, ˆX) ≤D. In this case,
Eqn. 15.38 is the ﬁrst order rate-distortion function which is an upper bound to the general rate
distortion bound. Throughout we compute the ﬁrst order R(D) function which will be the rate
distortion function only in the independent and identically distributed case.
15.3 The Blahut Algorithm
In only a small number of cases can the rate-distortion curve be calculated analytically. Using the
alternating minimization of the Blahut algorithm [7, 346–347] the RD function can be calculated.
Assume that the observable space is a ﬁnite set X and density p(x) on X. The transition prob-
ability from input to reproduction alphabet q(ˆx|x) is the conditional probability distribution on
the approximating set ˆx ∈X with input x ∈X. The goal is to ﬁnd the transition density which
minimizes the mutual information between the input and output sets:
q⋆(·|x) = arg
min
q(·|x): ¯D≤D
H(X; ˆX) = arg
min
q(·|x): ¯D≤D

x

ˆx
p(x)q(ˆx|x) log
q(ˆx|x)
q(ˆx)

,
(15.39)

458
15 INFORMATION BOUNDS
where
D =

x

ˆx
d(ˆx, x)q(ˆx|x)p(x).
(15.40)
The problem is solved by the following alternating minimization procedure (Blahut).
Theorem 15.11
Choose q0(ˆx) > 0, ˆx ∈
ˆX, then deﬁne the sequence of random variables
ˆX(1), ˆX(2), . . . with densities q(1)(·), q(2)(·), . . . on ˆX according to
q(n)(ˆx|x) =
q(n−1)(ˆx)e−λd(ˆx,x)

ˆx q(n−1)(ˆx)e−λd(ˆx,x) ,
(15.41)
q(n)(ˆx) =

x
p(x)q(n)(ˆx|x) = q(n−1)(ˆx)

x
p(x)e−λd(ˆx,x)

ˆx q(n−1)(ˆx)e−λd(ˆx,x) ,
(15.42)
Then, ˆX(1), ˆX(2), . . . has the property that the mutual information is monotonically nonin-
creasing,
H(X; ˆX(1)) ≥H(X; ˆX(2)) . . . ,
(15.43)
and the sequence of distortions satisfy the constraint 
ˆx q(n)(ˆx, x)d(ˆx, x)p(x) ≤D for all n.
Fixed points satisfy the necessary minimizer conditions for the rate-distortion problem.
Proof
Rewrite the mutual information according to
H(X; ˆX) = min

x

ˆx
q(ˆx, x) log
 q(ˆx, x)
q(ˆx)p(x)

.
(15.44)
Expand the minimization by ﬁxing q(n−1)(ˆx), and minimize over q(ˆx, x) subject to the
constraint giving
q(n)(ˆx, x) = arg min
q(ˆx,x): ¯D≤D

x

ˆx
q(ˆx, x) log

q(ˆx, x)
q(n−1)(ˆx)p(x)

(15.45)
= q(n−1)(ˆx)p(x)eλ0−λd(ˆx,x),
(15.46)
with λ0 satisfying the integrability to 1 constraint, and λ determining the expected
distortion constraint. Dividing by p(x) > 0 gives the conditional density
q(n)(ˆx|x) =
q(n−1)(ˆx)e−λd(ˆx,x)

ˆx∈ˆX q(n−1)(ˆx)e−λd(ˆx,x) .
(15.47)
Now ﬁx q(n)(ˆx, x) and minimize with respect to q(ˆx):
q(n)(ˆx) = arg min
q(ˆx): ¯D≤D

x

ˆx
q(n)(ˆx, x) log

q(n)(ˆx, x)
q(ˆx)p(x)

(15.48)
= arg min
q(ˆx): ¯D≤D
−

ˆx

x
p(x)q(n)(ˆx|x)

log q(ˆx)
(15.49)
(a)
=

x
p(x)q(n)(ˆx|x)

,
(15.50)

15.3 THE BLAHUT ALGORITHM
459
with(a)followingfromthelog x inequalityimplyingthelowerboundoncrossentropy
between densities p, f, Ep{log p/f} ≥0 with equality if and only if f = p, p almost
everywhere. Thus we have the following set of monotonic inequalities:
H

X; X(n−1)
=

ˆx
q(n−1)(ˆx, x) log

q(n−1)(ˆx, x)
q(n−1)(ˆx)p(x)

(15.51)
≥

ˆx
q(n)(ˆx, x) log

q(n)(ˆx, x)
q(n−1)(ˆx)p(x)

(15.52)
≥

ˆx
q(n)(ˆx, x) log

q(n)(ˆx, x)
q(n)(ˆx)p(x)

= H(X; X(n)).
(15.53)
That ﬁxed points q∗(ˆx), q∗(ˆx, x) satisfy the necessary maximizer condition, maximizing
Eqn. 15.39 with respect to q(ˆx, x) gives
q∗(ˆx, x) = p(x)q∗(ˆx)e−λd(ˆx,x)

ˆx q∗(ˆx)e−λd(ˆx,x) .
(15.54)
Marginalizing gives the ﬁxed point condition for q∗(ˆx) > 0:
1 =

x
p(x)e−λd(ˆx,x)

ˆx q∗(ˆx)e−λd(ˆx,x) ;
(15.55)
this is satisﬁed at the ﬁxed point of Eqns. 15.41 and 15.42.
As has been shown in [346] when these equations converge to a limit point dependent on λ, this
limit is the RD function at the point where the slope is equal to λ. By choosing a number of
λ ∈[0, ∞) values, the RD function can be swept out.
15.4 The Remote Rate Distortion Problem
In pattern theory we rarely observe directly the random variables without noise. In fact, rather
than observing the underlying source X with realizations x ∈X, the observables are almost
always noisy versions of the original random variables such as those arising in remote sensing.
Let X ∈X be a random variable with probability p(·), and Y ∈Y be such a random observation
containing information about X with transition density p(y|x), y ∈Y, x ∈X assumed known. Upon
observation of Y, we would like to choose that ˆx ∈ˆX that minimizes d. This is not possible since
given that Y = y, X is a random variable with conditional probability p(x|y), x ∈X. A reasonable
thing to do is to minimize the expected distortion
d(y, ˆx) = Ep(x|y) d(X, ˆx) =

x
d(x, ˆx)p(x|y).
(15.56)
For the pattern rate distortion problem, assume the underlying space of random patterns
I ∈I with prior density p(·) on the pattern space I. Identify the patterns with the transformation
which generates it, and assume the images are in the orbit of the special Euclidean group. Writing
the group elements in homogeneous coordinates, then the random transformation X ∈X ⊂SE(n)
is an (n+1)×(n+1) matrix. We make observations ID ∈ID, where ID contains information about

460
15 INFORMATION BOUNDS
I with transition density p(·|I) on ID. The goal is to decode the original element in the pattern
class I ∈I, i.e. choose the group element. One approach would be to generate the MAP estimator
arg max
I∈I
p(I|ID).
(15.57)
This requires searching for the maximum over the entire space I, which is generally impossible.
Because of the identiﬁcation to the group element, approximating I with ˆI is constructing the
approximating subset ˆX of the special Euclidean group SE(n). We are now in the remote rate-
distortion setting described in Section 15.2.
We seek to characterize the approximating subset of the special Euclidean group ˆX ⊂SE(n)
which sits on the rate-distortion curve. Rate-distortion theory dictates that if R > R(D) then for n
sufﬁciently large there exists an approximating set ˆX n of X n such that the average distortion
1
n
n

i=1
d(ID
i , ˆI(Xi)).
(15.58)
Conversely, if R < R(D) then such functions do not necessarily exist.
Rate-distortion theory dictates that in order to approximate a sequence of images
I(X1), . . . , I(Xn) with average distortion
D = 1
n
n

i=1
d(ID
i , ˆI(Xi)).
(15.59)
from a sequence of observations ID
1 , . . . , ID
n , we need at least 2nR(D) elements in the n-fold cartesian
product ˆI(X) × · · · × ˆI(X).
When ID
1 , . . . , ID
n is a sequence of independent and identically distributed random variables,
then
R(D) =
min
pˆI|ID(·|·):Ed(ID,ˆI)≤D
H(ID; ˆI),
(15.60)
where H(ID; ˆI(X)) is the mutual information between ID and ˆI(X) and the minimum is over all
conditional probabilities pˆI|ID(·|·), satisfying Ed(ID, ˆI) ≤D.
15.4.1 Blahut Algorithm extended
Using the Blahut algorithm, Section 15.3, the RD function is calculated. Assume that the observ-
able space ID is a ﬁnite set quantized to gray level images with input probability p(ID) on ID.
The reproduction alphabet reproduces the images with density q(ˆI), ˆI ∈ˆI. The transition proba-
bility from input to reproduction alphabet q(·|ID) is the conditional probability distribution on the
approximating set ˆI ∈I. The goal is to ﬁnd the transition densities which minimize the mutual
information between the input and output sets:
q⋆(·|ID) =
arg min
q(·|ID): ¯D≤D
H(ID; ˆI) =
arg min
q(·|ID): ¯D≤D

ID

ˆI
p(ID)q(ˆI|ID) log

q(ˆI|ID)
q(ˆI)

,
(15.61)

15.4 THE REMOTE RATE DISTORTION PROBLEM
461
where
D =

ID

ˆI
d(ˆI, ID)q(ˆI|ID)p(ID),
(15.62)
with d(ˆI, ID) = Ep(I|ID)d(I, ˆI) =

I
d(I, ˆI)p(I|ID).
(15.63)
The problem is solved by an alternating minimization procedure (Blahut).
Choose
qinit(ˆI) > 0, ˆI ∈ˆI, then ﬁxed points of the following iteration satisfy the necessary mutual
information rate-distortion conditions:
qnew(ˆI|ID) =
qold(ˆI)e−λd(ˆI,ID)

ˆI qold(ˆI)e−λd(ˆI,ID) ,
(15.64)
qnew(ˆI) =

ID
p(ID)qnew(ˆI|ID) = qold(ˆI)

ID
p(ID)e−λd(ˆI,ID)

ˆI qold(ˆI)e−λd(ˆI,ID) .
(15.65)
The point on the RD curve is given by
¯D =

ˆI

ID
p(ID)
q(ˆI)e−λd(ˆI,ID)

ˆI q(ˆI)e−λd(ˆI,ID) d(ˆI, ID),
(15.66)
R( ¯D) = s ¯D −

ID
p(ID) log


ˆI
q(ˆI)e−λd(ˆI,ID)


−

ˆI
q(ˆI) log


ID
p(ID)
e−λd(ˆI,ID)

ˆI′ q( ˆI′)e−λd( ˆI′,ID)

.
(15.67)
By choosing a number of s ∈[0, ∞) values, the RD function can be swept out.
Example 15.12 (Code Book Design for CUP CAD Models)
Examine
the
ground
based imaging problem in which the object is at a ﬁxed position in space with unknown
axis-ﬁxed pose (orientation) studied extensively by Shusterman, et al. [348]. Assume
the space of imagery I ∈I is the orbit under the axis ﬁxed orthogonal group with no
translation: X(θ) =


cos θ
sin θ
0
−sin θ
cos θ
0
0
0
1

. The prior probability on rotations is taken
as uniform p(x(θ)) = 1/2π inducing a uniform prior probability p(I(θ)), I(θ) ∈I. The
observationsID ∈ID areconditionallyGaussianwithmeanﬁeldTI(θ), andlikelihood
p(ID|I(θ)) =
1
(2πσ 2)d/2 e−1/2σ 2∥ID−TI(θ)∥2;
(15.68)
the detector array will be d = 32 × 32 throughout the simulations.
Throughout the gray level images are discretized with the Gaussian error
function used to represent the posterior density p(ID|I).
Two distortion measures are examined. The ﬁrst is the Froebenius or Hilbert–
Schmidt distortion given by
d1(X(θ), ˆX( ˆθ)) .= ∥X −ˆX∥2
HS = trace(X −ˆX)(X −ˆX)t
= 2(n −traceX ˆXt)
(15.69)
= 4 −4 cos(θ −ˆθ).
(15.70)

462
15 INFORMATION BOUNDS
The second is directly in the angles themselves according to
d2(θ, ˆθ) = (θ −ˆθ)2.
(15.71)
Note that for small errors d1 is approximately 2d2:
d2(θ, ˆθ) = 4

1 −

1 −(θ −ˆθ)2
2!
+ (θ −ˆθ)4
4!
−. . .

(15.72)
= 2d2(θ, ˆθ) + o((θ −ˆθ)2).
(15.73)
Remark 15.4.1
As shown in [341], in SO(3), the rotations correspond to three Euler
angles φ, θ, ψ giving trace X ˆXt = (1 + cos θ)(cos φ cos ψ −sin φ sin ψ) + cos θ =
(1 + cos θ) cos(φ + ψ) + cos θ, which depends only upon the angles θ and γ = φ + ψ.
This gives the Hilbert–Schmidt distortion
dHS(X, ˆX) = 6 −2 cos θ −2(1 + cos θ) cos γ .
Throughout the simulations the orientation space is tiled in fractions of degrees, thus X is a
discrete subset of rotations. The measurement space Y .= R32×32 is the continuum corresponding
to the conditional Gaussian random ﬁelds on the imaging lattice; the measurement density is a
Gaussian mixture across the discrete set of angles:
p(y) =

θ∈X
p(y|θ)p(θ).
The iteration of Eqns. 15.64 and 15.65 may be rewritten in the more convenient form
c(Hθ) =

y
p(y)
e−λd(Hθ,y)

Hθ
qold(Hθ)e−λd(Hθ,y) ,
(15.74)
qnew(Hθ) = qold(Hθ)c(Hθ).
(15.75)
An estimate of the sum in 15.74 is done by the Monte–Carlo by simulating N image samples
y1(θ), . . . , yN(θ) for each possible value of θ:
Hc(Hθ) =

θ
p(θ) 1
N
N

i=1
e−λd(Hθ,yi(θ))

Hθ
qold(Hθ)e−λd(Hθ,yi(θ)) =

θ
p(θ)Hc(Hθ|θ).
(15.76)
Unlike the original Blahut algorithm, here only estimates Hc of the coefﬁcients are generated via
empirical averages over the N samples.
Shown in the accompanying ﬁgures are results for various CAD models from [348]. Imagery
at SNR = ∞dB (zero-noise), SNR = 0 dB, SNR = −6 dB, and SNR = −12 dB was simulated for

15.4 THE REMOTE RATE DISTORTION PROBLEM
463
three objects; the “JEEP”, the “T62,” and the “CUP.” Figures 15.6 and 15.7 show the 3 CAD models
at SNR = ∞and SNR = −6 dB, respectively.
For the calculation of the RD curves, the resolution of the orientation grid was taken at 1◦for
a total of 360 total poses in the database. For the ﬁrst two CAD models, the JEEP and the T62, all
orientations are different and identiﬁable through the sensor so that the RD curve does not exhibit
any “gaps” due to ambiguity as would be expected for a symmetry. This is depicted in Figure 15.8.
As we shall see below for the CUPwith a handle feature there is a gap in the rate-distortion function
since there are a set of orientations which are not identiﬁable due to obscuration of the sensor. A
remarkable thing shown in these plots is the fact that even for −6 dB SNR, the corresponding
RD curve coincides with the the curve drawn for the zero noise case. For increased noise levels,
the performance decreases rapidly as is illustrated by the −12 dB curve. Only at the highest noise
level is there a difference between curves of these two different CAD models. Such noise immunity
demonstrates the power of the global modelling approach of deformable templates in which an
entire array of pixels enter into the inference of a low-dimensional parametric representation. This
is consistent with that shown in [341] for orientation estimator bounds demonstrating that such
parametric models behave as if they are asymptotically high in signal to noise ratio over a broad
range of noise levels.
For the “JEEP” and “T62” examples the difference between various orientations is visually
apparent. The third CAD model “CUP” has a different behavior due to its symmetry properties.
Only the cup handle breaks the symmetry. Since it is a “fragile” feature consisting of a small
number of pixels at low SNR the handle is masked. The RD curves for the third CAD model are
drawn in Figure 15.9. Note that even without noise the distortion never goes to zero because there
is a range of about 85◦over which the handle is hidden behind the cup, making all orientations in
that range indistinguishable. Assuming that the CUP model is positioned with zero degrees with
the handle hidden from the sensor, and assuming that all orientations inside the identiﬁable range
Figure 15.6 CAD models JEEP, T62, and CUP—4 orientations of CAD models without noise through
camera projection (image size 32 × 32, 8 bit/pixel).
Figure 15.7 Orientations of each model at SNR = −6 dB.

464
15 INFORMATION BOUNDS
are decoded without error, the minimum average distortion is calculated by
DMSE
min = 1
2π
 85π/360
−85π/360
θ2 dθ ≈0.0433,
DHSE
min = 1
2π
 85π/360
−85π/360
4(1 −cos θ) dθ ≈0.0211
(after normalization DMSE
min
≈0.0132). RD curves in Figure 15.9 come close to these numbers.
0
0.2
0.4
0.6
0.8
1
1.2
0
0.2
0.4
0.6
0.8
1
1.2
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
Normalized distortion D/Dmax
Rate [bits/orientation]
solid line -> SNR = inf.
doted line -> SNR = –6 db
dash–dot -> SNR = –12 db
solid line -> SNR = inf.
doted line -> SNR = –6 db
dash–dot -> SNR = –12 db
solid line -> SNR = inf.
doted line -> SNR = –6 db
dash–dot -> SNR = –12 db
solid line -> SNR = inf.
doted line -> SNR = –6 db
dash–dot -> SNR = –12 db
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
Normalized distortion D/Dmax
Rate [bits/orientation]
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
Normalized distortion D/Dmax
Rate [bits/orientation]
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
Normalized distortion D/Dmax
Rate [bits/orientation]
Figure 15.8 Left column shows rate-distortion functions for MSE distortion for objects “JEEP”(top
row) and T62”(bottom row). Right column shows the same for HS distortion.
0
0.2
0.4
0.6
0.8
1
1.2
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
Normalized distortion D/Dmax
Rate [bits/orientation]
solid line -> SNR = inf.
dashed line -> SNR = 0 db
doted line -> SNR = –6 db
dash–dot -> SNR = –12 db
solid line -> SNR = inf.
dashed line -> SNR = 0 db
doted line -> SNR = –6 db
dash–dot -> SNR = –12 db
0
0.2
0.4
0.6
0.8
1
0
0.5
1
1.5
2
2.5
3
3.5
4
Normalized distortion D/Dmax
Rate [bits/orientation]
Figure 15.9 Rate-Distortion function for the model “CUP”. Left panel MSE; right panel HSE.

15.4 THE REMOTE RATE DISTORTION PROBLEM
465
This ﬁgure shows the sensitivity of the RD function to the noise level. Unlike the previous cases,
even at 0 dB SNR noticeable degradation of performance occurs, which is entirely due to the fact
that the handle feature is fragile; 0-dB SNR makes it disappear.
We have calculated rate-distortion functions of the ﬁrst order for orientation alphabets under
assumptions that object orientations are distributed uniformly over the circle, they are statistically
independent at each sample, and the noise is zero-mean Gaussian and white. For orientations
which are temporally dependent on past orientations such as those modelled in [303] for aircraft
dynamics, the RD function may be calculated by building the input alphabet from vectors of
input samples. The size of the input and output alphabets will be geometric in vector length,
substantially increasing the computation. The RD function of the ﬁrst order is an upper bound
for RD functions of the higher orders and is achieved if the input process samples are statistically
independent [344].
15.5 Output Symbol Distribution
So far, only the optimal size distortion tradeoff of the output codebook has been calculated, not
the code itself. Such performance is achieved for long sequences of data; for short sequences or
scalar codebooks the calculated performance becomes a lower bound. In the remaining ﬁgures
we examine and interpret the ﬁxed point q⋆( ˆθ), ˆθ ∈ˆ of the iteration Eqn. 15.75 of the Blahut
algorithm. Although the distribution density of output symbols is a function with support on the
interval [0, 2π] and can be plotted using Cartesian coordinates, we prefer to use polar coordinates
to give insight into the dependency of the output distribution density on the object geometry.
The results are closed curves corresponding to probability densities with area 1 and radii which
represent the distribution mass at each angle. For illustration, all plots are scaled independently,
so no absolute quantitative information can be deduced across plots. All sub-plots are indexed
by their SNR; upper left panel refers to an SNR = ∞dB, upper right SNR = 0 dB, lower left
SNR = −6 dB, and lower right SNR = −12 dB.
Shown in Figure 15.10 is the output distribution q⋆for the Hilbert–Schmidt distortion error
for the “JEEP” at the rates of 0.5 (left), 1.0 (middle), 2.0 (right) bits/orientation. At the rate 0.5
(columns 1,2) and 1.0 bit/orientation (columns 3,4), the distribution densities at most noise levels
are circles, meaning that each one of the orientations should appear in the output alphabet with
the same frequency. The situation changes in sub-plot d, where the distribution density is spread
along orientations of 90◦and 270◦.
If one decides to build a scalar quantizer at a particular rate based on the density in sub-plots
INF dB, 0 dB then one can choose any two opposite orientations as codebook words (examine
90
270
180
0
INF db
90
270
180
0
0 db
90
270
180
0
–6 db
90
270
180
0
–12 db
90
270
180
0
INF db
90
270
180
0
0 db
90
270
180
0
–6 db
90
270
180
0
–12 db
90
270
180
0
INF db
90
270
180
0
0 db
90
270
180
0
–6 db
90
270
180
0
–12 db
Figure 15.10 Output distribution densities for HSE distortion for object “JEEP” at rate 0.5 (columns
1,2), 1.0 (columns 3,4), 2.0 (columns 5,6) bits.

466
15 INFORMATION BOUNDS
upper left, right panels). However if they are chosen near 90◦and 270◦the codebook will be close
to optimal for higher noise levels too. The property determining the code orientation is the norm-
squared measure on object as demonstrated in [341] establishing ∥T(θ)∥the norm-squared of the
mean projective transformation of the object at orientation θ as one of the crucial metric for Bayes
probability of error. A simple observation of the object images shows that the highest values of the
∥TI(θ)∥2 measure are near 90◦and 270◦(see “JEEP” column 2 of Figure 15.6).
Increasing the bit rate further to 2.0 bit/orientation (columns 5,6) demonstrate allocation of
codewords at a set of discrete orientations around the circle. At SNR=−12 dB, (lower right panel)
provides evidence that the distance between two highest norm-squared valued orientations is
also important. Figure 15.11 shows plots of the output symbols distribution density for the “T62”
model for the same rates and the Hibert–Schmidt distortion at 0.5 (left), 1.0 (middle), and 2.0 (right)
bits/symbol. The results are similar as above. Note that in this example, the highest norm-squared
values are at orientations 0◦and 180◦(see “T62” columns 3 in Figure 15.6).
Both the “JEEP” and “T62” CAD models produce different images for every orientation. If
the noise level is not very high the output symbols distribution density is a circle, with each output
orientation having the same frequency of appearance in the output vector. The last example, the
CAD model “CUP”, has a high symmetry with a number of orientations which are indistinguish-
able. Thus, even without noise, the distribution density of the output is not a circle. Its shape for
various rates is shown in Figure 15.12. Note that the orientation of zero degrees is selected when
the cup handle is hidden by the cup. Therefore, for all orientations in the range of approximately
±42.5◦are mapped to zero degrees.
These ﬁgures show that the distribution density changes much faster with the noise and that
for a high noise level the best we can do is to decide whether the “CUP” handle is to the left or to
–6 db
–12 db
–6 db
–12 db
–6 db
–12 db
90
270
180
0
INFdb
90
270
180
0
0db
90
270
180
0
90
270
180
0
90
270
180
0
INF db
90
270
180
0
0db
90
270
180
0
90
270
180
0
90
270
180
0
INF db
90
270
180
0
0 db
90
270
180
0
90
270
180
0
Figure 15.11 Output distribution density for object “T62” at rates 0.5 (columns 1,2), 1.0 (columns
3,4), 2.0 (columns 5,6) bits/orientation.
90
270
180
0
INF db
90
270
180
0
0 db
90
270
180
0
90
270
180
0
90
270
180
0
INF db
90
270
180
0
0 db
90
270
180
0
90
270
180
0
90
270
180
0
INF db
90
270
180
0
0 db
90
180
0
270
90
270
180
0
–6 db
–12 db
–6 db
–12 db
–6 db
–12 db
Figure 15.12 Output distribution densities for the HSE distortion measure for the CAD object
“CUP” at rates 0.5 (columns 1,2), 1.0 (columns 3,4), 2.0 (columns 5,6) bits/orientation.

15.5 OUTPUT SYMBOL DISTRIBUTION
467
the right. For illustration the concentrated nature of the density has been scaled although the total
density is 1.
Interestingly, if the model has a high degree of symmetry, then the output symbols density
should also be symmetric. Note that sub-plots (c) in Figure 15.12 illustrate slight asymmetries. The
light in all our experiments was placed at a position of 90◦; the right part of the cup in Figure 15.6
is brighter and could give rise to the small asymmetry.

16 C O M P U TAT I O N A L A N AT O M Y: S H A P E , G R O W T H
A N D AT R O P H Y C O M PA R I S O N V I A
D I F F E O M O R P H I S M S
ABSTRACT
Computational Anatomy (CA) is the study of human anatomy I ∈I = Iα ◦G, an orbit
under groups of diffeomorphisms of anatomical exemplars Iα ∈I. The observable images
ID ∈ID are the output of medical imaging devices. There are three components CA examines:
(i) constructions of the anatomical submanifolds under study in anatomy, (ii) estimation of
the underlying diffeomorphisms g ∈G deﬁning the shape or geometry of the anatomical
manifolds, and (iii) generation of probability laws of anatomical variation P(·) on the images
I for inference and disease testing within the anatomical models. This chapter focuses on
the metric comparison of anatomical manifolds.
16.1 Computational Anatomy
Revolutionaryadvancesinthedevelopmentofdigitalimagingmodalitiescombinedwithadvances
in digital computation are enabling researchers to advance the precise study of the awesome bio-
logical variability of human anatomy. This is emerging as the exciting new ﬁeld of computational
anatomy (CA) [227,349]. CA has three principal aspects: (a) automated construction of anatomical
manifolds, points, curves, surfaces, and subvolumes; (b) comparison of these manifolds; and (c)
the statistical codiﬁcation of the variability of anatomy via probability measures allowing for infer-
ence and hypothesis testing of disease states. This chapter will focus on several of these aspects.
Although the study of structural variability of such manifolds can certainly be traced back to the
beginnings of modern science, in his inﬂuential treatise “On Growth and Form” in 1917, D’Arcy
Thompson had the clearest vision of what lay ahead.
In a very large part of morphology, our essential task lies in the comparison of related forms rather than
in the precise deﬁnition of each; and the deformation of a complicated ﬁgure may be a phenomenon easy
of comprehension, though the ﬁgure itself may have to be left unanalyzed and undeﬁned. This process of
comparison, of recognizing in one form a deﬁnite permutation or deformation of another, apart altogether
from a precise and adequate understanding of the original “type” or standard of comparison, lies within the
immediate province of mathematics and ﬁnds its solution in the elementary use of a certain method of the
mathematician. This method is the Method of Coordinates, on which is based the Theory of Transformations.
The study of shape and structure has certainly progressed a long way. Thompson’s vision
is precisely the mathematical structure we term anatomy in CA [227,349], a deformable template
in which the space of anatomical imagery is an orbit under groups of transformations. The trans-
formations have already been studied in the previous Chapters 10, 11, and 12 in which the metric
space structure of two types of transformations have been examined. The ﬁrst is of the geometric
type studied through diffeomorphic mappings of the coordinate systems of the anatomies; the
second is of the photometric type accomodating the appearance or creation of new structures.
The metric is calculated via equations of motion describing the geodesic connection between the
elements.
Now towards the ﬁrst of the three principal aspects, the automated construction of
anatomical manifolds of points, curves, surfaces, and subvolumes, the general area of the
mathematical codiﬁcation of biological and anatomical structure has expanded rapidly over
the past several decades. The automated construction of anatomical manifolds is receiving
tremendous focus by many of the groups throughout the world supporting neuromorphomet-
ric analyses which are becoming available with large numbers of anatomical samples. Deformable
and active models are being used for generating 1D manifold curves in 2 and 3 dimensions
[154,164,170,188,202,350–354]. The differential and curvature characteristics of curves and surfaces
468

16.1 COMPUTATIONAL ANATOMY
469
have been examined as well with active and deformable surface models for the neocortex and
cardiac systems [173,174,192,193,202,229,355–358]. Local coordinatized representations for cor-
tical manifolds have included both spherical and planar representations for local coordinates in
studying the brain [166,167,173–176,178,179,182,231,355,359–366]. A great deal of work has also
been focused on methods of segmentation of anatomical volumes into 3D submanifolds. Auto-
matic methods for maximum-likelihood and Bayesian segmentation are being developed across
the whole brain as well as focusing on particular gyri [51,53,54,183,356,367–378].
Towards comparison of anatomical manifolds, because of the fundamental structural vari-
ability of biological shape there has been great emphasis by groups on the study of anatomical
shape via vector and metric comparison. The earliest vector mapping of biological coordinates
via landmarks and dense imagery was pioneered in the early 1980s and is continued today by
Bookstein [223, 224, 272, 379–381], and Bajcsy, Gee and co-workers [274, 288, 290, 291, 382–384].
Comparison via vector maps based on dense imagery is being carried on by many of the aforemen-
tioned groups. Most of the efforts deﬁne a preferred origin modeling deformations “elastically”
based on small-deformation vector maps on the dense volumes; vector mappings restricted to the
cortical manifold are being computed as well [294,295,350,385–400].
Our own efforts in comparison of anatomical manifolds have been largely focussed on
via large deformation diffeomorphic ﬂows, originally proposed in [220, 401]. Unlike the vector
maps, these are not additive but provide guaranteed bijections between anatomical conﬁgurations.
This ﬂow framework has since 1997 been developed into a complete metric space formulation
[269, 276–278, 282, 286, 349, 402, 403], providing the mapping methodology with a least-energy
(shortest path) and symmetry property.
Concerning inference on statistical representations of shape, studies in CA of growth, atro-
phy, and disease have literally exploded over the past ﬁve years since the advent of automated
methods for manifold construction. Applications to growth and statistical atlas building are
evidenced by [275, 284, 297, 381, 404–407]. Applications in CA to normal and abnormal aging
in the dementia of the Alzheimer’s type have been examined in both cortex and deep brain
stuctures by many groups [408–417]. Also investigators have been looking at the neuropsychi-
atric illness of schizophrenia [418–433]. Researchers in CA continue to develop new methods of
analysis in many of the neuropsychiatric illnesses including but not exclusive to ADHD [434],
autism [435, 436], major depressive disorder [437], psychosis [438], alcohol effects [439], neuro-
logic disorders Huntington’s disease [440], and multiple-sclerosis [441–443], as well as comparison
across illnesses [444].
16.1.1 Diffeomorphic Study of Anatomical Submanifolds
Because of the sheer complexity of human neuroanatomy, in particular the human brain, the
study of brain geometry has emerged as the study of the submanifolds of anatomical signiﬁcance
including its landmarks, curves, surfaces, and subvolumes all taken together forming the com-
plete volume. The complete methodology combines mappings that carry all of the submanifolds of
points, curves, surfaces, and subvolumes together. This is precisely why the fundamental transfor-
mation groups are subgroups of diffeomorphisms, as they carry these submanifolds consistently.
The transformations are constrained to be 1–1 and onto, and differentiable with differentiable
inverse, so that connected sets remain connected, submanifolds such as surfaces are mapped as
surfaces, and the global relationships between structures are maintained. Shown in Figure 16.1 is
the methodology the CA community is using for studying coordinatized manifolds on the brain
via diffeomorphisms and bijections to local coordinates. Depicted in the top row are diffeomorphic
mappings which are used for studying the 3D structures of the brain. These mappings are deﬁned
on subvolume manifolds X ⊂R3. Shown in the bottom row are diffeomorphic mapping between
manifolds in X ⊂R2. These especially arise in the study of cortical surfaces such as the neocortex

470
16 COMPUTATIONAL ANATOMY
3D Mapping
2D Mapping
Surface Coordinates
(planar or spherical)
Hemisphere B
Hemisphere A
Figure 16.1 Figure shows the diffeomorphisms between brain volumes and submanifolds within
the brain; ﬁgure taken from Van Essen et al. [167].
and the hippocampus and other subvolumes in which the surface of the subvolume becomes
a natural index system.
The diffeomorphisms between R3 provide smooth bijections to
the submanifolds thereby allowing for the study of correspondences between the different
structures.
16.2 The Anatomical Source Model of CA
In CA many models are being examined by investigators throughout the world, in which data
enter through dense imagery as well as landmarked points, and shape themselves. In this chapter
we shall examine several of these. In almost all cases the study of anatomy will be accomplished
by identifying the anatomical picture with an unknown, hidden diffeomorphism on the background
space of coordinates. As these diffeomorphisms can almost never be directly observed, only the
observable imagery, they must be estimated; this is the quintessential hidden structure problem.
There are essentially two models being examined, the static metric mapping model, and
the dynamic metric growth model. For the static metric mapping model, the anatomical source is
a deformable template Iα corresponding to the orbit under the group G of one selected and ﬁxed
image Iα ∈I. A single observation from one time instant is observed, and the goal is to compare
an exemplar with it via diffeomorphic transformation. The diffeomorphic transformations on the
coordinate systems are deﬁned through inﬁnite dimensional diffeomorphisms generated via the
Lagrangian and Eulerian formulation of the ﬂows on X ⊂Rd the closed and bounded background
space:
∂gt
∂t (x) = vt(gt(x)) ,
∂g−1
t
∂t (x) = −Dg−1
t
(x)vt(x), x ∈X ⊂Rd,
(16.1)
with boundary conditions g0 = g−1
0
= id. The space of allowable vector ﬁelds V is restricted by
working with the V norm-square
 1
0
∥vt∥2
V dt =
 1
0
⟨Avt, vt⟩2 dt.
(16.2)

16.2 THE ANATOMICAL SOURCE MODEL OF CA
471
Assume that (V, ∥· ∥V) is a reproducing kernel Hilbert space (see Chapter 9, Section 9.4) with
operator K : L2(X, Rd) →V, satisfying for all f, h ∈L2(X, Rd), α, β ∈Rm,
⟨K(x, ·)α, f⟩V = α∗f(x) =

i
αifi(x),
⟨K(xj, ·)α, K(xi, ·)β⟩V = α∗K(xi, xj)β.
(16.3)
The orbit of imagery becomes
Iα =

g1 · Iα : ∂gt
∂t = vt(gt), t ∈[0, 1], g0 = id, v ∈V

.
(16.4)
The complete anatomy is the union over multiple orbits generated from different templates Iα,
I = ∪αIα. Throughout this chapter it is assumed that the exemplar or initial starting template is
known and ﬁxed.
Figure 16.2 depicts the basic static metric correspondence model studied throughout this
chapter. The goal is to infer the geometric image evolution connecting the observation ID to the
exemplar I0 = Iα ∈I.
In the dynamic growth model, an observation time sequence is given and the goal is to infer
the time ﬂow of geometric change which carries the known exemplar through the growth period
t ∈[0, 1]. Here the orbits are ﬂows of imagery, with growth corresponding to actual real-time,
and comparison is performed by comparing the observables It, t ∈[0, 1] to its start point I0. Here
I0 = Iα plays the role of the so-called template or exemplar. In static matching time is simulation
time only, with the single target observable at time t = 1. Throughout we take as the dynamic
growth model orbits of the exemplars under space–time ﬂows
Iα[0, 1] =

gt · Iα, t ∈[0, 1] : ∂gt
∂t = vt(gt), g0 = id

.
(16.5)
Anatomical Model
MRI, CT, CRYO
Medical Inference
^g1. Ia
D

ID
I = g1 ·
,

Ia
g1 =    yt(gt) dt,  g0 = id,  yn
1
0
Figure 16.2 The source for the anatomical model is the set of all images I of the exemplars Iα(g−1
1 ).
The observed data ID are observations seen via the sensors (MRI, CT, CRYOSECTION pictures) and
are conditional Gaussian random ﬁelds with mean ﬁelds g1 · Iα. For the static metric mapping
model there is only one observation ID the mean ﬁeld of the exemplar Iα(g−1
1 ).
Growth Model
MRI, CT, CRYO
Growth Inference
^g1. Ia

, t
[0,1]
D

ID

, t
[0,1]
t
[0,1],


, t
[0,1]
It = gt . Ia
gt =    ys(gs) ds,  g0 = id,  y n
t
0
Figure 16.3 The source for the dynamic growth model is the set of all growth ﬂows I[0, 1] of the
exemplars gt · Iα, t ∈[0, 1]. The observed data ID
t , t ∈[0, 1] are ﬂows of observations seen via the
sensors (MRI, CT, CRYOSECTION pictures) and are conditional Gaussian random ﬁelds with mean
ﬁelds gt · Iα, t ∈[0, 1].

472
16 COMPUTATIONAL ANATOMY
Each element gt · Iα, t ∈[0, 1] ∈Iα[0, 1] in the orbit is modeled as corresponding to a different
growth trajectory of the exemplar. Figure 16.3 depicts the growth model studied throughout this
chapter.
16.2.1 Group Actions for the Anatomical Source Model
Specifying the source model amounts to specifying the group actions on the elements of the orbit.
We shall look at several group actions in this chapter deﬁned on the sparse landmark points, the
dense scalar valued imagery, as well as normalized vectors from tensor measurements involving
diffusion tensor magnetic resonance imaging (DTI). The measurement at each voxel in the DTI
image volume is a symmetric second order tensor; the principal direction of the diffusion tensor
corresponds to the ﬁber orientation in heart tissue and neuron orientation in the central nervous
system. Cao and Younes deﬁne the images to be functions I : X →S2 ⊂R3 that associate to each
point a vector, the principal direction of the diffusion tensor (outside the object the vector is zero).
The group actions studied in this chapter are as follows.
Lemma 16.1
1. For the sparse landmarks points, take the orbit of imagery as N-shapes
IN =


x1
...
xN

∈IN = RdN with the group action deﬁned as, for any IN ∈IN
and g ∈G,
g · IN =


g(x1)
...
g(xN)

.
(16.6)
2. Take the orbit the dense imagery scalar-valued functions I : X →R in orbit I ∈I with
group action, for any I ∈I and g ∈G,
g · I = I ◦g−1.
(16.7)
3. Let the images be unit normal vectors I : X →R3 that associate to each point a vector,
the principal direction of the diffusion tensor (outside the object the vector is zero). Deﬁne
the group actions according to, for any I ∈I and g ∈G,
g · I =



Dg−1gI◦g−1∥I◦g−1∥
∥Dg−1gI◦g−1∥
when I ◦g−1 ̸= 0
0
when I ◦g−1 = 0
,
(16.8)
with Df g denoting the Jacobian matrix of g evaluated at f.
These are group actions.
Proof
We only prove the normalized vector version, the others have been shown
elsewhere. Clearly, id · I = I. For g and h in G, g · (h · I) = (g ◦h) · I if I ◦g−1 = 0.

16.2 THE ANATOMICAL SOURCE MODEL OF CA
473
If I ◦g−1 ̸= 0,
g · (h · I) =
Dg−1g(h · I ◦g−1)∥h · I ◦g−1∥
∥Dg−1g(h · I ◦g−1)∥
=
Dg−1g(Dh−1◦g−1hI ◦h−1 ◦g−1∥I ◦h−1 ◦g−1∥/∥Dh−1◦g−1hI ◦h−1 ◦g−1∥)∥I ◦h−1 ◦g−1∥
:::Dg−1g(Dh−1◦g−1hI ◦h−1 ◦g−1∥I ◦h−1 ◦g−1∥/∥Dh−1◦g−1hI ◦h−1 ◦g−1∥)
:::
=
Dg−1gDh−1◦g−1hI ◦h−1 ◦g−1∥I ◦h−1 ◦g−1∥
:::Dg−1gDh−1◦g−1hI ◦h−1 ◦g−1
:::
=
Dh−1◦g−1(g ◦h)I ◦h−1 ◦g−1∥I ◦h−1 ◦g−1∥
∥Dh−1◦g−1(g ◦h)I ◦h−1 ◦g−1∥
= (g ◦h) · I.
16.2.2 The Data Channel Model
In computational anatomy observations are made of the underlying coordinates. Generally the
data is observed in multiple forms, including both measurements of material properties of tissue
such as scalar and vector valued imagery derived from MRI, CT, nuclear emission, and optical
and acoustic scanners, as well as measurement made in the form of geometric properties of objects
associated with submanifolds, curves, surfaces, and subvolumes. The general model we pursue
was shown in Figure 16.2.
Take the observable ID : X ⊂Rd →Rm, in general a vector valued function. Deﬁne a
distance measure between the underlying source or anatomical structure giving rise to the mea-
surements, and the observable measurement C : I × ID →R+. We will generally study through
the Gaussian random ﬁeld model, in which the distance function arises as the potential associated
with such a model. For this, model the observables ID as a Gaussian random ﬁeld with mean ﬁeld
I ∈I generated via unknown diffeomorphism. Then the distance takes the form
C =
1
2σ 2 ∥ID −g · Iα∥2
2 ,
(16.9)
where ∥· ∥2 is the L2 vector norm ∥f∥2 = 
m
i=1

X |fi(x)|2 dx.
For the growth problem, the goal is to extract the hidden diffeomorphism sequence
gt, t ∈[0, 1] carrying the exemplar onto the static target observation sequence ID
t , t ∈[0, 1]. The
diffeomorphism is selected to minimize a matching distance function of time
C =
1
2σ 2
 1
0
∥ID
t −gt · Iα∥2
2 dt .
(16.10)
For the norm squared cost then the stochastic model corresponds to the additive white noise
model, ID = g · Iα + W, with W white noise variance 1. Strictly speaking then ∥ID −g · Iα∥2
2
is not well deﬁned on the continuum, since ID is not of trace class. This is properly handled by
constraining the noise process to have covariance with spectrum which has ﬁnite integral. In this
case the matching norm-square cost changes to ∥ID∥2
H = ⟨ID, ID⟩ with  covariance trace class
reﬂecting the ﬁnite bandwith of the noise or the tapering of the spectrum.

474
16 COMPUTATIONAL ANATOMY
Alternatively, for all of the applications, the noise process is associated with a ﬁnite number
of detectors (voxels in the image). Assume that the measured data ID
n , n = 1, 2, . . . is a Gaussian
random ﬁeld with each component representing a noisy measurement of a scalar (e.g. gray level)
integrated over a subset of the background space X forming the voxel measurements; the nth
voxel measures the integrated response from the voxel xn ⊂X. Assume the high count limit and
independence between distinct voxels, in which case the measurement is modeled as a Gaussian
random vector of ﬁxed isotropic variance σ 2, with the nth component ID
n having mean

xn
g1 · Iα(x) dx.
We use the L2 formulation (with covariance identity) in the continuum connecting us to so
many classic formulations in least-squares estimation. For a strict interpretation we must keep in
mind the piecewise continuous model just articulated or the trace class covariance.
The unifying inference problem solved throughout the chapter is to estimate properties of the
underlying anatomical structure by minimization of the data cost with the running cost associated
with geodesic correspondence. Given observables ID with mean ﬁeld g · Iα ∈I and matching cost
C, the goal is to ﬁnd the diffeomorphism g which minimizes C. As with many minimum norm
variational problems, there are an inﬁnite number of possible paths which minimize the distance
function; we choose the ones which minimize the metric length and energy in the subgroup of
diffeomorphisms solving the inﬁmum
 1
0
∥vt∥2
Vdt + C(ID, g · Iα) .
(16.11)
16.3 Normal Momentum Motion for Large Deformation Metric
Mapping (LDDMM) for Growth and Atrophy
In Chapter 11, we examined the shortest path geodesic ﬂows of diffeomorphisms corresponding
to the Euler equation. As well in Section 11.3 we examined the variational problem in the tangent
space of the group minimizing with respect to the vector ﬁelds generating the shortest path ﬂows.
As we shall see, for metric mapping of images in which the data enters only through the two
static observations at the start and endpoints of the ﬂow, the geodesic is completely determined
by the initial momentum, where in the deformable setting the generalized momentum is naturally
deﬁned as Av the function of the vector ﬁeld which is integrated against the vector ﬁeld to given
energy in the metric ∥v∥2
V = ⟨Av, v⟩2.
For the metric mapping of the start and end point, there is a conservation law which requires
the momentum to be conserved along the path of the geodesic ﬂow. This of course implies that
the momentum at the identity Av0 on the geodesic determines the entire ﬂow gt, t ∈[0, 1], where
˙gt = vt(gt), g0 = id. As proved in Theorem 11.8 deﬁning the vector ﬁeld transported from the
identity as wt = Dgt(g−1
t
)w0(g−1
t
), then along the geodesics satisfying the Euler equation (Theorem
11.6, Eqn. 11.21, Chapter 11) the momentum is conserved according to Theorem 11.8, Eqn. 11.31:
∂
∂t⟨Avt, wt⟩2 = 0 ,
t ∈[0, 1] .
(16.12)
Along the geodesic, the momentum is determined by the momentum at the identity:
Avt = (Dg−1
t
)∗Av0(g−1
t
)|Dg−1
t
| .
(16.13)
The natural question becomes, where does the initial momentum Av0 arise to specify the
geodesic connection of one shape to another? Naturally, it arises by solving the variational problem
of shooting one object onto another. In this setting we do not require an exact boundary condition
for g1 implying that the perturbation in η satisﬁes only the t = 0 boundary conditions and at the

16.3 LDDMM FOR GROWTH AND ATROPHY
475
boundary of the background space x ∈∂X. In previous Chapter 10 we described only the Euler
equations associated with variation of the diffeomorphisms. Examine the dense matching problem
calculating the variation of the energy with respect to perturbation of the vector ﬁeld directly with
the perturbation v →v + ǫψ zero on the boundary ψ(∂X) = 0. The perturbations respecting the
boundary conditions take the form
η0(·) = 0 ,
η1(∂X) = 0 ,
ψ(∂X) = 0 .
(16.14)
The inexact correspondence associated with the boundary conditions are depicted in panel 2
of Figure 10.1 of Chapter 10. Notice how at the endpoint only approximate correspondence is
enforced. Now we follow Beg’s work [285] and calculate the momentum at the origin which
solves the minimum matching condition.
From Theorems 12.9 and 12.11 of Chapter 12 we have the Euler equations and the nor-
mal momentum motion. Beg [285] computes not on the momentum Avt for shooting one object
on to another but on the more stable object the vector ﬁelds. This introduces the smooth-
ing of the Green’s operator in the solution. The variation equation essentially corresponds
to the Sobolev derivative on the vector ﬁeld. We term the algorithm of Beg the LDDMM
algorithm.
Theorem 16.2
(Large
Deformation
Diffeomorphic
Metric
Mapping:
Dense
Imagery [285]) Given template I continuously differentiable with ∇(I ◦g) = (Dg)∗∇I(g),
and gt,u = gu ◦g−1
t
.
1. Given static image data ID with cost C = (1/2σ 2)∥ID−I(g−1
1 )∥2
2, and ﬂow minimizing
energy
 1
0
∥vt∥2
V dt +
1
2σ 2 ∥ID −I(g−1
1 )∥2
2 .
(16.15)
Then the V-norm variational minimizer has vector ﬁelds satisfying
vt + K

∇(I ◦g−1
t
) 1
2σ 2 (ID(gt,1) −I(g−1
t
))|Dgt,1|

= 0,
(16.16)
where Kf(·) =

K(·, y)f(y) dy.
2. For the growth problem, given image data ID
t , t ∈[0, 1] with cost C = (1/2σ 2(
 1
0 ∥ID
t −
I(g−1
t
)∥2
2) dt, and ﬂow minimizing energy
 1
0
∥vt∥2
V dt +
1
2σ 2
 1
0
∥ID
t −I(g−1
t
)∥2
2 dt .
(16.17)
The V-norm minimizer with respect to variations of the vector ﬁeld satisﬁes
vt + K

∇(I ◦g−1
t
) 1
2σ 2
 1
t
(ID
u (gt,u) −I(g−1
t
))|Dgt,u| du

= 0 .
(16.18)
Proof
It is only necessary to show the growth version, the other version is similar. To
calculate the V-norm variation, then the ﬁrst term is 2vt (rather than 2Avt), and Eqn.
12.52 of Theorem 12.11, Chapter 12 for the second term V-norm variation becomes
−
2
2σ 2
 1
0
⟨(ID
u −I(g−1
u ))∇I(g−1
u ), ∂ψg−1
u ⟩2 du
=
2
2σ 2
 1
0

K

∇(I ◦g−1
t
)
 1
t
|Dgt,u|(ID
u (gt,u) −I(g−1
t
)) du

, ψt

V dt .

476
16 COMPUTATIONAL ANATOMY
Beg’s algorithm [285] for the static case with observation ID at one time instant, with C =
∥ID −I(g−1
1 )∥2
2 computes as a ﬁxed point a gradient solution of the inexact metric matching
equation. We term this the LDDMM algorithm; when applied to dense images the dense image
LDDMM algorithm. For numerical issues, Beg computes the optimizing vector ﬁeld rather than
the momentum.
Algorithm 16.3 (Beg Algorithm [285])
1. Static Matching: Fixed points of the following algorithm satisfy Eqn. 16.16. Initialize
vold = 0, choose constant ǫ, then for all t ∈[0, 1],
Step 1 :
∂
∂tgnew
t
= vold
t
(gnew
t
),
∂
∂tg−1new
t
= −Dg−1new
t
vold
t
,
gnew
t,1
= gnew
1
(g−1new
t
) ,
Step 2 : Compute vnew
t
= vold
t
−ǫ∇vEold
t
, set vold ←vnew, return to Step 1 where
∇vEold
t
= vold
t
+
1
2σ 2 K

∇(I ◦g−1new
t
)|Dgnew
t,1 |(ID(gnew
t,1 ) −I(g−1new
t
))

,
t ∈[0, 1].
(16.19)
2. Growth: The growth model algorithm for dense imagery is same as above with gradient
∇vEold
t
= vold
t
+ 1
σ 2 K

∇(I ◦g−1new
t
)
 1
t
|Dgnew
t,u |(ID
u (gnew
t,u ) −I(g−1new
t
)) du
 
.
(16.20)
Beg’s gradient descent algorithm discretizes Eqns. 16.19. At the ﬁxed point, the discrete
version of the Euler–Lagrange equation is satisﬁed.
Examine the static case with observation ID at one time instant, with C = ∥ID −I(g−1
1 )∥2
2.
The ﬁrst three rows of Figure 16.4 show the mapping of multiple simple shapes involving
translation, scale and rotation. The operator was chosen to be A = (−0.01∇2 + id)2 for the
experiments. The ﬂow was discretized into 20 timesteps each of size δt = 0.1. The gradient algo-
rithm was run until convergence. Shown in each column are different points along the geodesic
I(g−1
t
), t0, t6, t12, t19, target. Plotted below each row is the metric distance. Beg’s algorithm was
implemented with circulant boundary conditions. Shown in the last two rows are the results of
the Heart and Macaque cortex experiments (Figure 16.5 and 16.6).
For large deformations such as the circular ﬂow the vector ﬁelds are not static over
time on the geodesic. Clearly the momentum evolved under the map according to Avt =
(Dg−1
t
)∗Av0(g−1
t
)|Dg−1
t
|. Therefore the vector ﬁelds will have non-overlapping support. This is
depicted in Figure 11.2 of Chapter 11. This ﬂow was generated using the LDDMM algorithm of
Beg to match the 1
2C (panel 1, row 3) to the C (panel 6, row 3).
Example 16.4 (Hippocampus Mapping)
Shown in Figure 16.7 are results from the
2-dimensional(2D)hippocampus mappingexperiment. Row 1shows theimages being
transported I0 ◦g−1
t
for mapping the young control to the Schizophrenia subject; row
2 shows the Alzheimer’s subject being mapped. The last panel of each row shows the
vector ﬁelds for the Schizophrenic (columns 1–3) andAlzheimer’s (columns 4–6). Data
taken from the laboratory of Dr. John Csernansky; mapping results from Beg [285].
Shown in Figure 16.8 are examples of 3D maps of the hippocampus generated by
Faisal Beg via the diffeomorphic ﬂow algorithm generating g1(·) solving the equation
g1(x) =
 1
0 vt(gt(x)) dt + x. The time-interval along the ﬂow was discretized into
20 timesteps. Also shown in the ﬁgure are the mappings g−1
1 , g1 and the template,
target images deformed via these mappings. The mappings are smooth in space.
Example 16.5 (Mitochondria)
Shown in Figure 16.9 are examples of metric com-
putations in 2D on high resolution electron-micrographs. Column 1 shows the
mitochondria and the shapes for metric comparison. Columns 2–5 of Figure 16.9 show

16.3 LDDMM FOR GROWTH AND ATROPHY
477
Metric 0
(1)
(2)
(3)
(4)
(5)
0.26
0.50
0.76
TRANSLATE
Metric 0
1.34
2.49
3.83
SCALE
0
3.72
6.92
10.65
C
Metric 0
2.14
3.98
6.92
HEART
Metric 0
2.40
4.46
6.86
MACAQUE
Figure 16.4 Rows 1,2, and 3 show the sequence of metric maps I(g−1
t
) for t0, t6, t12, t19, target for
the TRANSLATE, SCALE, and C experiments with metric
 t
0 ∥vs∥V ds. Column 6 shows the target
through the forward map ID(g1). Rows 4 and 5 show the HEART and MACAQUE. Mappings taken
from Beg [285]; heart data taken from Dr. Raimond Winslow of Johns Hopkins University and
macaque taken from Dr. David Van Essen of Washington University.
TRANSLATE y0
SCALE y0
1
2 C y0
HEART y0
MACAQUE y0
Figure 16.5 Columns 1,2, and 3 show the vector ﬁelds v0 (row 1), v10 (row 2), and v19 (row 3) for
TRANSLATION, SCALE, and C experiments. Columns 4 and 5 show the vector ﬁelds for the HEART
experiment and MACAQUE for v0, v10, v19. The mapping results are from Beg [285]; the heart data
is courtesy of Dr. Raimond Winslow, The Johns Hopkins University. The macaque data is taken from
David Van Essen.
1
2 C y0
y12
y29
y10
y19
MACAQUE y0
Figure 16.6 Figure shows vector ﬁelds for the 1
2C (panels 1,2, and 3) and macaque (panels 4,5,
and 6) experiments.

478
16 COMPUTATIONAL ANATOMY
YOUNG Metric 0
1.71
3.17
4.89
SCHIZ.
YOUNG Metric 0
1.95
3.63
5.59
ALZH.
y0
y0
(1)
(2)
Figure 16.7 Rows 1 and 2 show results from the hippocampus mapping experiment for the YOUNG
to SCHIZOPHRENIC (row 1) and ALZHEIMER’S (row 2). Shown are the sequence of geodesic map-
pings I(g−1
t
) connecting the YOUNG to the targets for t0, t3, t6, t9, t12, t15, t19. Plotted below each
is the metric distance. Column 6 shows the vector ﬁelds v0 at the identity. Mapping results from
Beg [285]. Data taken from the laboratory of Dr. John Csernansky of Washington University.
Schizophrenia
 
Metric 6.31
Ia(g1
–1(.))
Ia
ISCHIZ.(g1(.))
ISCHIZ.
 
Metric 4.76
Iyoung(g1
–1(.))
Ia
IDAT (g1(.))
IDAT
Alzheimers
(1)
(2)
·
Figure 16.8 Figure shows 3D hippocampus mapping results from Schizophrenia (row 1) and
Alzheimer’s (row 2). Top row shows template Iα, panel 2 shows the Schizophrenic hippocampus
hand labeled ISCHIZ, panel 3 shows Iα(g−1
1 (·)), and panel 4 shows ISCHIZ(g1(·)). Row 2 shows similar
results for the Alzheimer’s. Data taken from the laboratory of Dr. John Csernansky of Washington
University.
the shapes ordered in geodesic distances from the templates (column 1). Shown below
each panel is the metric distance.
16.4 Christensen Non-Geodesic Mapping Algorithm
Whatiftheﬂowthroughthespaceofﬂowsisnottheshortestone? ChristensenandRabbittwerethe
ﬁrst to introduce ﬂows for large deformation matching of images [220–222,273,445]. Christensen
implemented a computationally efﬁcient algorithm for image matching which generates a ﬂow

16.4 CHRISTENSEN NON-GEODESIC MAPPING ALGORITHM
479
0
0
0
2.58
1.82
3.46
2.92
1.941
3.94
5.9
1.946
5.57
(1)
(2)
(3)
(4)
(5)
Figure 16.9 Column 1 shows electron microscopy images. Columns 2–5 show computed geodesic
distances between the template shapes (column 2) and other mitochondrial shapes (columns 3,4,
and 5).
through the space of diffeomorphisms. The algorithm mapping anatomic conﬁgurations separate
the joint optimization in space–time into a sequence of indexed in time optimizations solving for
the locally optimal at each time transformation and then forward integrating the solution. This
is only a locally-in-time optimal method reducing the dimension of the optimization. It does not
revisit the entire ﬂow attempting to ﬁnd the ﬂow of shortest metric length. Assume the deformation
ﬁelds are generated from vector ﬁelds which are assumed piecewise constant within quantized
time increments of size δ, tk = kδ, k = 0, . . . , K = 1/δ giving vt = vtk, for t ∈[tk−1, tk).
Algorithm 16.6 (Christensen [220])
The
sequence
of
local-in-time
transformations
g−1
tk (y), k = 1, 2, . . . , is given by g−1
0 (y) = y with
g−1
tk
=
 tk
tk−1
−Dg−1
σ vtk dσ + g−1
tk−1
(16.21)
and
vtk = arg inf
vtk
(tk −tk−1)∥vtk∥2
V +
1
2σ 2 ∥ID −I(g−1
tk )∥2
2 .
(16.22)
For each k = 1, . . . , K, solve via Algorithm 16.3 for vtk in Eqn. 16.22 for local in time
updating of Eqn. 16.21, initializing with v = 0.
After stopping, deﬁne the ﬁnal iterate as v⋆
tk(y) with g−1
tk
satisfying Eqn. 16.21.
The Christensen algorithm generates a particular ﬂow through the space of diffeomorphisms
which connects the images. Beg compared various solutions generated by the Christensen
Algorithm 16.6 to those generated by the geodesic Algorithm 16.3. Shown in Figure 16.10 are
results from the mappings. The top row shows vector ﬁelds generated from the metric corre-
spondence algorithm; bottom row shows vector ﬁelds from the Christensen algorithm. Column 1
shows translation, column 2 shows the heart, and column 3 shows the macaque sections. Shown
in each panel are the superposition of twenty vector ﬁelds along the ﬂow. Shown in Table 16.4.1
are the relative distances between Beg’s geodesic algorithm and the Christensen algorithm. The
ﬁnal image matching errors are virtually identical. The length of the ﬂow in general
is not

480
16 COMPUTATIONAL ANATOMY
Figure 16.10 Top row shows vector ﬁelds generated from the metric correspondence algorithm;
bottom row shows vector ﬁelds from the Christensen algorithm. Column 1 shows translation,
column 2 shows the heart, and column 3 shows the macaque sections. Shown in each panel are
the superposition of 20 vector ﬁelds along the ﬂow.
Table 16.1 Matching distances and geodesic distance for Christensen algorithm and LDDMM
Image Error C(ID, I ◦g−1)
Manifold Distance

∥vt∥V dt
Experiment
“Geodesic”
“GEC”
“Geodesic”
“GEC”
“Parallel Translation”
5.648%
5.65%
0.7223
0.722757
“Heart Mapping”
11.558%
11.799%
4.631
6.7145
“Macaque Cortex Mapping”
9.55%
9.53%
4.636
5.45
similar. The right columns show the distance between the geodesic algorithm and the Christensen
algorithm as a percentage for the parallel translation, heart, and macaque experiments. Notice
for the linear translation there is no difference. For the curved deformation there is signiﬁcant
difference.
16.5 Extrinsic Mapping of Surface and Volume Submanifolds
Almost all of the geometric work in CAconcentrates on smooth submanifolds of Rn, as curved and
surface submanifolds abound in human anatomy. Diffeomorphisms restricted to smooth subman-
ifolds carry the submanifolds smoothly, see Theorems 8.24 and 8.25, Chapter 8 on diffeomorphic
mappings of submanifolds. If g is a diffeomorphism from X →X with M ⊂X a submanifold with
subspace topology, then g : M ⊂X →g(M) ⊂X is a diffeomorphism from M →g(M) with the
subspace topology.
Examine the space of 2D smooth surface manifolds M such as the cortical and hippocampul
surface. Diffeomorphisms on X carry submanifolds M ⊂X smoothly, the smooth tangent struc-
tures, curvature, etc. The geometry of the quadratic charts transform in the standard way with
the Jacobian of the mapping transforming the tangent space, and curvature transformed by the
Hessian of the transformation [229].

16.5 EXTRINSIC MAPPING OF SURFACE AND VOLUME SUBMANIFOLDS
481
Left
Right
Figure 16.11 Figure shows 3D MRI–MPRAGE head matching. Left three panels show template
and target, and the 3D transformation of the template matched to the target. Right three panels
show the same for the second individual. Figure reproduced from Christensen et al., Volumetric
Transformation of Brain Anatomy, 1997
16.5.1 Diffeomorphic Mapping of the Face
The large deformation Christensen Algorithm 16.6 has been applied for generating whole head
maps imaged via MRI–MPRAGE sequences. Figure 16.11 shows results of such experiments for
two different subjects. Shown are surface renderings of the surface of the mapped volumes forming
the MRI brain volumes. Panels 1 and 2 show the template and target surface renderings. The whole
brain volume is being studied via the mapping, but because of the use of the surface rendering
only the observable external boundary of the volume is seen manifesting as the human face. Panel
3 shows the rendering of the volume after transformation of the template matched to the target.
Panels 4, 5, and 6 show the same for a second individual. Only the faces are depicted although the
entire head has been transformed.
16.5.2 Diffeomorphic Mapping of Brain Submanifolds
Restricting the diffeomorphism on R3 to the submanifolds of the brain provides smooth map-
pings of the brain substructure. The Csernansky group has been mapping the hippocampus in
populations of schizophrenic and Alzheimer’s patients [162, 408, 422, 423, 446]. Shown in Figure
16.12 are results depicting the hippocampus mappings of Washington University. Column 1
shows the MRI and associated template hippocampus. Column 2 shows the template surface
(above) and embedded in the volume (below). Column 3 shows the target surface (above) and
the target surface embedded in the target volume (below). The target surface was generated by
hand contouring. Column 4 shows the results of mapping the surface from the template into the
target.
16.5.3 Extrinsic Mapping of Subvolumes for Automated
Segmentation
The smooth maps carry the segmentation labels of the volumes consistently. Shown in Figure 16.13
are examples illustrating mapping of subvolumes for automated segmentation from Christensen
from the occipital lobe in the macaque emphasizing the importance of fusing landmarks and dense
image information for estimating the diffeomorphisms. The macaque cryosections were generated

482
16 COMPUTATIONAL ANATOMY
(1)
(2)
(3)
(4)
Figure 16.12 Column 1: Panel shows the template hippocampus (green) embedded in the MRI
volume. Columns 2,3,4: Panels show the template surface (top), the target surface (middle), and
the mapped template surface (bottom). Top row shows the hippocampus surface through the
mapping; bottom row shows the surface embedded in the mapped volumes. Mapping results
from Christensen et al. [220] and Haller et al. [162]. (see Plate 31)
(1)
(2)
(3)
(4)
Figure 16.13 Parasagittal cryosection cortex sections from the macaque occipital lobe. Left col-
umn: Panels show photographs of the right hemisphere of macaque 92K (top) and 92L (bottom).
Arrows show the cuts that were made to remove part of the visual cortex three. Right columns show
automated segmentations of sections 23, 37, 52, 61 showing 92K, 92L, and 92K →92L. Columns
2 and 3 show hand segmentations; column 4 shows automated segmentation from mapping
92K →92L. Mapping results taken from Christensen et al. [220]; data collected in the laboratory
of David Van Essen of Washington University.
in David Van Essen’s laboratory. Both occipital lobes were hand traced into complete gray and
white matter segmentations.
Figure 16.13 shows automated segmentations of tissue volumes generated by Christensen
applying the diffeomorphisms to hand segmentations of the template exemplars. Panels 1 and 2 of
Figure 16.13 show the macaque cryosection volumes generated in David Van Essen’s laboratory.

16.5 EXTRINSIC MAPPING OF SURFACE AND VOLUME SUBMANIFOLDS
483
Figure 16.13 show the automated 3D segmentation of 92K, 92L cortex volumes. Top row panels 1
and 2 show photographs of the right hemisphere of macaque monkey 92K template and 92L tar-
get. Arrows show the cuts that were made to remove part of the visual cortex. Bottom rows
show section under the mapping. Columns 1 and 2 show sections of 92K and 92L; column
3 shows 92K →92L. Columns 4 and 5 show the same sections of the gray and white mat-
ter hand segmentations of 92K and 92L. Column 6 shows the corresponding three sections
automatically segmented by transforming the 92K hand segmentation to 92L with the large defor-
mation diffeomorphic transformation resulting from the sulcus landmarks and image matching
diffeomorphism.
16.5.4 Metric Mapping of Cortical Atlases
The mammalian cerebral cortex has the form of a layered, highly convoluted thin shell of gray
matter surrounding white matter. The cortex contains a complex mosaic of anatomically and
functionally distinct areas which play a tremendously important role in understanding brain
functions [155]. As championed by Van Essen [175], to aid in the understanding of the geome-
try and the functional topography of the cortex the convoluted cerebral cortex may be mapped
to planar coordinates for visualization of the deep and buried gyri. To understand individual
variation in the cortical topography the CA community has been using the large deforma-
tion metric landmark mapping tools on spherical representations and planar representations
of the cortex to establish correspondences between the ﬂat maps of various individual cortical
surfaces.
David Van Essen has been studying cortical atlases via both spherical and planar
bijections. The left column of Figure 16.14 shows an atlas of the macaque visual cortex
constructed by David Van Essen. The top panel shows the macaque surface in 3D and
the bottom panel shows the ﬂat atlas. The right ﬁve panels of Figure 16.14 show results
from the metric transformation of the planar representations of the macaque from Van
Essen. A shows the ﬂat map of Case 1. Shading indicates cortical geography; black lines
indicate the landmark contours used to constrain the deformation (along sulcal fundi and
along the map perimeter); and white lines indicate grid lines at intervals of 5 map-mm.
Panel B shows the pattern of grid lines after deformation of Case 1 to bring it into regis-
ter with the target atlas map in panel D. Gray boxes highlight the locations of deformed
marker points a′ and b′. Panel C shows the vector ﬁeld for selected grid points (at inter-
vals of 10 map-mm). Arrow bases indicate grid positions in the source map, and arrow
tips indicate the location of grid points in the deformed source map. For example, displace-
ments are largely to the right in the lower part of the map and largely to the left in the
upper left of the map. Panel D shows the map of geography (shading) and target regis-
tration contours (black lines) on the atlas map. Panel E shows contours from the deformed
source map (black lines) and deformed reference marks a’, b’ are overlaid on the target
atlas map.
Example 16.7 (Human and Macaque Cortical Mapping)
Figure 16.15 shows an
interspecies comparison via cortical surface deformation between the macaque and
visible human (see [362]). Panel 1 shows the 3D Visible Human Male. Panels 2 and
3 show landmarks on macaque ﬂat maps and human ﬂat maps to bring them into
correspondence. Panel 4 shows the boundaries of deformed macaque visual areas
(black lines) superimposed on the fMRI activation pattern from an attentional task
from the study of Corbetta et al. [447] after deformation to the Visible Man atlas by
Drury et al. [448]).

484
16 COMPUTATIONAL ANATOMY
A. Case 1
(source map)
D. Atlas (target map)
E. Deformed borders of
Case 1 on atlas map
B. Deformed
source map
C. Displacement
field
Figure 16.14 Van Essen atlas of macaque visual cortex. Left column shows the macaque atlas
(top row) and ﬂattened version (bottom row) generated by David Van Essen. Right column
shows ﬂattened surface-based mapping from an individual ﬂat map via landmark matching
in the plane to the macaque atlas. A shows ﬂat map of Case 1, B pattern of grid lines
after deformation of Case 1, C shows the displacement vector ﬁeld, D shows the map of
geography
(shading)
and
target
registration
contours
(black
lines)
on
the
atlas
map,
and E shows the deformed borders of Case 1 on the atlas map.
Results taken from
Van Essen et al. [362] (See Plate 32).
16.6 Heart Mapping and Diffusion Tensor Magnetic
Resonance Imaging
Now we examine extending the mapping work to vector and tensor valued imagery. Vector
and tensor valued imagery arise in many different imaging modalities including diffusion ten-
sor imaging. Diffusion tensor magnetic resonance imaging (DT-MRI) quantiﬁes the anisotropic
diffusion of water molecules in biological tissues, making it possible to non-invasively infer the
architecture of the underlying structures. The measurement at each voxel in a DT-MRI image vol-
ume is a symmetric second order tensor. The principal direction of the diffusion tensor corresponds
to the ﬁber orientation of the heart. We now examine the work of Cao and Younes, working with
Raimond Winslow of the Johns Hopkins University, who have been mapping DTI volumes of the
heart based on the principal eigenvectors from DT-MRIs [449] . Figure 16.16 illustrates the use of
scalar and vector DTI for studying geometry of heart tissue. Columns 1 and 2 illustrate normal
anatomical variation as depicted in conventional MRI scalar imagery. Columns 3 and 4 show the
use of DTI for studying tissue geometry. The orientation of the DTI tensor measurements in these
sections are color coded by depicting the direction of the principal eigenvectors in the diffusion
tensor.

16.6 HEART MAPPING AND DT-MRI
485
(1)
(2)
(3)
(4)
Figure 16.15 Panel 1 shows 3D Visible Human Male. Panel 2 shows the boundaries of deformed
macaque visual areas (black lines) superimposed on the fMRI activation pattern from an attentional
task from the study of Corbetta et al. [447] after deformation to the Visible Man atlas by Drury
et al. [448] and mapping results from [362]. Panels 3 and 4 show landmarks on macaque ﬂat
map and human ﬂat map, respectively, used for performing the mappings shown in panel 2
(see Plate 33).
(1)
(2)
(3)
(4)
Figure 16.16 Columns 1 and 2 show hearts in grayscale MRI in normal (column 1) and failing
(column 2) hearts; columns 3 and 4 show DTI of those sections depicting orientation of the principal
eigenvectors by color. The top row shows coronal sections, the bottom row shows axial sections
(see Plate 34).

486
16 COMPUTATIONAL ANATOMY
To model the orbit of imagery, deﬁne the images to be functions I : X →R3 that associate
to each point a vector, the principal direction of the diffusion tensor (outside the object the vector
is zero). Cao and Younes [449] deﬁne the action G on the set I of all principal direction images as
follows.
With Df g denoting the Jacobian matrix of g evaluated at f, the group action used for LDDMM
is for any I ∈I and g ∈G,
g · I =



Dg−1gI◦g−1∥I◦g−1∥
∥Dg−1gI◦g−1∥
when I ◦g−1 ̸= 0
0
when I ◦g−1 = 0,
(16.23)
with Df g denoting the Jacobian matrix of g evaluated at f. Given two elements I and data ID linked
via ˙gt = vt(gt), t ∈[0, 1], the LDDMM algorithm minimizes the matching function
 1
0
∥vt∥2
V dt + α∥ID −Iog−1
1 ∥2 + 4(∥I∥◦g−1
1
−∥ID∥)2.
(16.24)
Cao computes the optimizer performing steepest descent by computing the gradient with the
step size ǫ computed via golden section search for the line search. The vector ﬁelds gener-
ating the maps are integrated using second-order semi-Lagrangian schemes for the transport
equation.
Example 16.8 (Scalar MRI Mapping of Hearts)
Beg and Helm have now mapped
the geometries of several normal and failing hearts based on only the scalar
MRI imagery [450] using the last term (not the vector term, α
=
0) in the
Template (Normal)
Target (Normal)
Target (Diseased)
Target (Diseased)
Displacement
0
8 mm
Figure 16.17 Figure shows the template heart and the target heart images after transformation
into the coordinate system of the template. The determinant of the Jacobian of the transformation
is superimposed as a colormap on the surface rendering. Blue colors indicate regions where the
determinant is less than unity whereas red regions are where the determinant is greater than
unity. Data courtesy of Dr. Raimond Winslow; mappings courtesy of Faisal Beg and Pat Helm
(see Plate 35).

16.6 HEART MAPPING AND DT-MRI
487
energy minimization of Eqn. 16.24. As depicted in Figure 16.17, the heart geom-
etry displays signiﬁcant anatomical variability. Using the diffeomorphic metric
mapping algorithm Beg registered the hearts to a common template coordinate sys-
tem, accomodating the large deformations associated with heart disease. The heart
datasets were both rigidly registered and diffeomorphically registered based on input
landmarks providing an initial condition for the diffeomorphism metric mapping
algorithm.
Errors in the mapping were quantiﬁed by deﬁning a normalized mismatch
deﬁned as a percentage of the error of the template heart segmentation with respect
to the target heart segmentation before the mapping given by
ǫ =
∥I0(g−1) −I1∥2
2
∥I0 −I1∥2
2
.
(16.25)
Error rates on the order of 11.5% were found in the three mappings. Shown in Figure
16.17 are three views of the diffeomorphic transformations of the hearts into the tem-
plate coordinates. The operator in the metric was chosen to be A = (−0.01∇2 + id)2,
and the ﬂow t ∈[0, 1] was discretized into 20 timesteps each of size δt = 0.05. Row 1
shows the normal and rows 2 and 3 show the diseased hearts after transformation.
Notice the similarity of the hearts to the template geometry. Overlaid on the trans-
formed geometry is the determinant of the Jacobian of the transformation at that
point. Blue colors indicate regions where the determinant is less than unity indicat-
ing regions of contraction, whereas red regions are where determinant is greater than
unity, indicating expansion.
Example 16.9 (DTI Heart Mapping (Cao))
Cao and Younes have been mapping
heart geometries based on minimizing the cost of Eqn. 16.24. A rigid motion was
ﬁrst calculated to provide global registration generated by aligning the centroids and
the principal axes of the two objects. Figure 16.18 shows sections along the geodesic
as the algorithm aligns the vector images corresponding to the largest eigen value of
the DT data.
Shown in Figure 16.19 are the 3D LDDMM-DT showing the geometries of the
two hearts superimposed after rigid motion and LDDMM. The red color in the bottom
right panel demonstrates almost total alignment as measured via the dot product of
the corresponding principal eigenvectors.
5
10
15
20
25
30
5
10
15
20
25
30
5
10
15
20
25
30
5
10
15
20
25
30
5
10
15
20
25
30
5
10
15
20
25
30
Figure 16.18 Panels show points along the geodesic for the LDDMM of the DT vector data, with
the red vectors showing the template, and the blue vectors showing the target. Taken from [449].

488
16 COMPUTATIONAL ANATOMY
0.8
0.6
0.4
0
–0.2
0.2
–0.4
–0.6
–0.8
(1)
(2)
Figure 16.19 Column 1 shows the geometries of the two hearts. Column 2, top panel shows the
two geometries superimposed after rigid motion; blue is template, red is target. Column 2, bottom
panel shows the LDDMM solution color representing the dot product of the corresponding vectors
after alignment. Red color means total alignment; taken from [449] (see Plate 36).
16.7 Vector Fields for Growth
In the dynamic growth model, an observation time sequence is given and the goal is to infer
the time ﬂow of geometric change which carries the known exemplar through the growth period
t ∈[0, 1]. Here the orbits are ﬂows of imagery, with growth corresponding to actual real-time,
and comparison is performed by comparing the observables It, t ∈[0, 1] to its start point I0. Here
I0 = Iα plays the role of the so-called template or exemplar. In static matching time is simulation
time only, with the single target observable at time t = 1. Throughout we take as the dynamic
growth model orbits of the exemplars under space–time ﬂows
Iα[0, 1] =

Iα(g−1
t
), t ∈[0, 1] : ∂gt
∂t = vt(gt), g0 = id

.
(16.26)
Each element Iα(g−1
t
), t ∈[0, 1] ∈Iα[0, 1] in the orbit is modeled as corresponding to a different
growth trajectory of the exemplar. Figure 16.3 depicts the basic static inexact matching model
studied throughout this chapter.
16.7.1 Growth from Landmarked Shape Spaces
Now examine the setting where the observables are time sequences of landmarks. Throughout
assume (V, ∥· ∥V) is a reproducing kernel Hilbert space with operator K.

16.7 VECTOR FIELDS FOR GROWTH
489
Theorem 16.10 (Growth via Metric Mapping: Sparse Landmarked Shapes)
Giventhe
sparselandmarksynt,n = 1, 2, . . . , withdatamatchingtermC = 1
σ 2

N
n=1
 1
0 ∥ytn −gt(xn)∥2
Rd dt,
then the optimizing growth ﬂow minimizing the energy
 1
0
∥vt∥2
V dt + 1
σ 2
 1
0
N

n=1
∥ytn −gt(xn)∥2
Rd dt ,
(16.27)
has momentum satisfying
Avt(x) = 1
σ 2
N

n=1
δ(gt(xn) −x)
 1
t
(Dgt,u(gt(xn)))∗(yuk −gu(xn)) du = 0 ,
x ∈X .
(16.28)
Proof
The V-gradient of the second term in the cost requires the differential ∂ψgt(xk)
given by Eqn. 11.20 giving the variation of the second term according to
−2
σ 2
 1
0
N

n=1
⟨yun −gu(xn), ∂ψgu(xn)⟩Rd du
(a)
= −2
σ 2
 1
0
 u
0
N

n=1
⟨yun −gu(xn), Dgt,u(gt(xn))ψt(gt(xn))⟩Rd dt du
(b)
= −2
σ 2
 1
0
 1
t
N

n=1
⟨(Dgt,u(gt(xn)))∗(yun −gu(xn)), ψt(gt(xn))⟩Rd du dt ,
(c)
= −2
σ 2
 1
0
 N

n=1
K(gt(xn), ·)
 1
t
(Dgt,u(gt(xn)))∗
yun −gu(xn)
	
du, ψt(·)

V
dt ,
where (a) follows from Eqn. 11.17 of Lemma 11.5, and rearranging the integral and
inner-product, (b) by interchanging the order of integration in t and u, and (c) from
the sifting property of the Green’s kernel.
Algorithm 16.11 (Gradient for Growth of Landmarked Shape)
The growth model
gradient algorithm has gradient
∇vEold
t
(x) = vold
t
(x) −1
σ 2
N

n=1
K(gnew
t
(xn), x)
 1
t
(Dgnew
t,u (gnew
t
(xn)))∗(yun −gnew
u
(xn)) du ,
x ∈X .
(16.29)
Example 16.12 (Growth in Mouse)
Jiangyiang Zhang, Susumu Mori, and Peter Van
Zijl have been characterizing brain development with DTI via 3D, digitized images
of ex-vivo mouse brain samples with high resolution ( 100 µm per pixel) and high
tissue contrast[451,452]. These technique have paved the way for quantitative charac-
terization of mouse brain anatomy during development. Brain development involves
complex sequences of tissue growth and movements in three dimensions. During post-
natal development, the mouse brain undergoes signiﬁcant volume and shape changes.
Figure 16.20 shows example of the evolving mouse brain structures from the date of
birth (P0) to postnatal day 45 (P45) in MR T2 and diffusion tensor imagery. Volumetric
measurements show that cortical and cerebellar volumes increase approximately four
and six fold, respectively, from P0 to P10. After P10, volumetric change stabilizes,

490
16 COMPUTATIONAL ANATOMY
Figure 16.20 MR images of postnatal mouse brains. Coronal MR T2 and DT images were shown
at the level of anterior commissure. Images have been aligned to ensure proper orientation and
position. Intensity in MR T2 images is related to tissue properties, such as the content of myelin
protein. MR diffusion tensor images reveal local tissue orientation. Both types of images were
utilized in our mouse brain developmental study. The color scheme for diffusion tensor images
was illustrated by color arrows in the ﬁgure, with red for local tissue whose orientation is anterior-
to-posterior (perpendicular to the current plane), green for horizontal orientation, and blue for
vertical orientation. The scalar bar represents 2 mm. Structure abbreviations are—ac: anterior
commissure; cc: corpus callosum; CPu: caudate putamen; CX: cortex (see Plate 37).
(1)
(2)
(3)
(4)
Figure 16.21 Panel 1 shows white matter tracts, anterior part of anterior commisure (aca), poste-
rior (acp), cerebral peduncle (cb), hippocampus (H), and hippocampal commisure (hc). Panels 2,3,
and 4 show visualization of landmarks on white matter tracts (green dots) and landmarks inside
hippocampus (yellow structures) (blue dots), outer brain surface (orange) with boundary of white
matter and cortical region shown in green, and landmarks depicted as intersections of white lines
on the green structure (see Plate 38).

16.7 VECTOR FIELDS FOR GROWTH
491
while changes in shape can still be appreciated from P10 to P30 [453]. These results
suggest that mouse brain development is a dynamic process with complex changes
in both volume and shape being involved. In order to capture detailed morphological
changes during development, it is necessary to employ a large number of landmarks
along structures of interest. As depicted in Figure 16.21, landmarks are placed along
several brain compartments, including white matter tracts hippocampus, cortex and
boundaries of white matter and cortex. Shown in panel 1 are 3D visualization of land-
marks on white matter tracts (green dots) and landmarks inside hippocampus (yellow
structures) (blue dots). Shown in panel 2 are 3D visualization of landmarks (orange
dots) on outer brain surface. Panel 3 shows a schematic drawing showing several
white matter tracts, anterior part of anterior commisure (aca), posterior (acp), cerebral
peduncle (cb), hippocampus (H), and hippocampal commisure (hc). Panel 4 shows the
visualization of landmarks on outer brain surface and on boundary of white matter
and cortical region (the green structure). Landmarks on the green structure are not
shown as orange dots as landmarks on the outer brain surface, but as intersections of
white lines on the green structure.
Figure 16.22 demonstrates the landmark based LDDMM of a P7 mouse brain
to a P30 mouse brain based on 270 landmarks that covered the entire forebrain
region. The accuracy of the mappings was examined by plotting manually segmented
Figure 16.22 LDDMM of P7 mouse brain images to P30 images. Coronal MR images of P7 (A)
and P30 (C) were shown with brain boundaries marked by orange curves. Deformed P7 (B) was
overlaid with the brain boundary of P30. White arrows in A, B, and C are pointing to hippocam-
pus (H). The transformation was visualized as color-coded vector plot (D) and Jacobian plot (F),
with enlarged local areas shown in E and G, respectively. For vector plots, color of vectors repre-
sents the magnitude of local displacement. For Jacobian plot, color on deformed grids represents
changes in local volume. Surface of P7, deformed P7, and P30 mouse brains were visualized
in H, from top to bottom. Vector maps show how transformations deform a local voxel from
its original location by taking the original location as the start of the vector and the destina-
tion as the end; length and color of vectors reﬂect the magnitude of tissue displacement. The
Jacobian map shows local volume changes; a value greater than unity corresponds to volume
increase, a value less than unity corresponds to volume loss, and unity for no volume change
(see Plate 39).

492
16 COMPUTATIONAL ANATOMY
tissue boundaries of P30 mouse brains over the deformed P7 mouse brain images.
Figure 16.22B shows the deformed P7 mouse brain images together with the outer
boundary of the P30 mouse brain in orange. The P30 brain boundary agrees well
with the deformed P7 mouse brain images. The shape of inner structures such as the
hippocampus (H) also agrees with corresponding structures in the P30 brain images.
The lateral cortical regions show signiﬁcant growth from P7 to P30, with large tis-
sue displacement coupled with near three-fold volume increase. The average volume
increase for the entire forebrain in this period is two-fold.
Using LDDMM, various aspects of mouse brain growth may be characterized,
including structural evolution and morphological variations. Figure 16.23 shows the
growthofthehippocampalsurfacemeasuredusingLDDMMmethods. Displacements
of the surface of the mouse hippocampus are computed and visualized. Although this
result contains not only the effects of growth, but the effects of landmark placement
variations as well, it suggests that the hippocampal growth is not spatially uniform.
The ventral part of the hippocampus experienced greater tissue displacement during
development than the dorsal part.
Example 16.13 (Huntington’s Disease)
Dr. Elizabeth Aylward of the University of
Washington is studying the patterns of neuro-degeneration in Huntington’s disease
following a time course of atrophy. Over time individuals affected display a marked
decrease in the ability to walk, think, talk, and reason. The caudate and putamen are
known to be affected by the neuro-degenerative processes. While it is well known that
Figure 16.23 Outer-shape changes of mouse hippocampus measured using LDDMM. The displace-
ment due to growth of hippocampus was color coded and the direction of growth was visualized
by 3D glyphs (see plate 40).
Figure 16.24 Series of volumes depicting aging in Huntington’s disease patient over time as man-
ifest in the Caudate. Data taken from the laboratory of Dr. Elizabeth Aylward of the University of
Washington (see Plate 41).

16.7 VECTOR FIELDS FOR GROWTH
493
there is a marked decrease in volume of these affected organs, less is known about the
pattern of neuro-degeneration, in particular whether particular regions are affected
more preferentially than others.
Thecaudatevolumesofpre-symptomaticindividuals—thosewiththemutated-
gene who have not yet manifested the clinical symptoms of the disease — are being
mapped from MRIs taken subsequent years apart. Preliminary observations suggest
that the process of neuro-degeneration in the disease, although affecting the entire
caudate, is marked in the medial region with considerable atrophy taking place in the
head and the tail regions. Recent research from other groups has shown that regions
in the posterior cortex are signiﬁcantly atrophied. Taken with the neuro-degenerative
pattern observed in the caudate, this suggests that the axonal tracts connecting the
posterior regions of the caudate and the cortex are perhaps disrupted as well. Shown
inFigure16.24areresultsfromthemappingofanindividualpatientwithHuntington’s
disease for over four years.

17 C O M P U TAT I O N A L A N AT O M Y: H Y P O T H E S I S
T E S T I N G O N D I S E A S E
ABSTRACT
Computational Anatomy (CA) is the study of human anatomy I ∈I = Iα ◦G, an orbit
under groups of diffeomorphisms of anatomical exemplars Iα ∈I. The observable images
ID ∈ID are the output of Medical imaging devices. This chapter focuses on the third of the
three components of CA: (iii) generation of probability laws of anatomical variation P(·) on
the images I within the anatomical orbits and inference of disease models. The basic random
objects studied are the diffeomorphic transformations encoding the anatomical objects in
the orbit; Gaussian random ﬁelds are constructed based on empirical observations of the
transformations. Hypothesis testing on various neuromorphometric changes are studied.
17.1 Statistics Analysis for Shape Spaces
The most common statistical methods applied for the study of shape spaces have been those based
on second-order representations and Gaussian random ﬁelds. The basis representations are often
constructed using principle components analysis (PCA) (examined in the context of Karhunen–
Loeve decompositions 14.2.1 of Chapter 14), which compute variance maximizing bases of the
linear vector (Hilbert) space modeling the random elements. These approaches fundamentally
model the measurements as random variables which are representing to second order (mean
and covariance), thus the fundamental link to Gaussian random ﬁeld models. The basis vectors
spanning the Hilbert space are oriented along directions of maximal variance, and ordered such
that the ﬁrst vector spans the 1D (ID) subspace of largest variation, and the second vector captures
the next largest variation, etc. It is therefore effective for analyzing variation in the data and for
dimensionality reduction. For example, by projecting a data point onto a subspace spanned by
the ﬁrst D basis vectors, we obtain an optimal (to second order) D dimensional representation of
the data. If a large percentage of the variation is captured in this D dimensional subspace, then the
approximation can be expected to be quite good. If in addition, D is small then the subspace may
offer considerable dimensionality reduction of the sample space.
While such second-order representations are commonly used in the analysis of shape, they
are only valid for linear spaces. However, they have already found wide applicability, in active
shape models (ASM)[199] for estimating shape variability from training sets. Of course we should
expect that for modeling ensembles which only exhibit small deformations linear subspace approx-
imations will be adequate. However, for the study of large deformation diffeomorphisms of
anatomical models such a methodology cannot be extended. The shape spaces are not a non-
linear metric space. Simply stated, transformations can be composed but not added; vector space
addition of the random variables will not generate elements in the anatomical model space. There
is no rigorous way, for example, to add two landmark conﬁgurations and be guaranteed that
the resulting conﬁguration is a meaningful combination of the originals.
Fortunately, interpreting the shape space from the Lie group point of view, its tangent space
at the identity provides the proper bridge to the linear approximation methods. For this we formu-
late the random objects being modeled to second-order as elements associated with the tangent
space at the identity of the transformation. We have already seen that the idea for comparing
shapes is to model comparison between elements in the orbit Iα via the diffeomorphic transfor-
mations in G. The optimal diffeomorphism which matches two arbitrary elements in the orbit is
chosen from all curves gt, t ∈[0, 1] in G connecting the two elements via the group action. It is
chosen as the curve which minimizes an energy with respect to a measure of inﬁnitesimal vari-
ations in G. These energy minimizing paths (geodesics) induce a metric on Iα [278]. Now we
focus on the implications of this fundamental property in providing the powerful capability to
494

17.1 STATISTICS ANALYSIS FOR SHAPE SPACES
495
represent the entire ﬂow of a geodesic in the orbit by a template conﬁguration and a momentum
conﬁguration at a single instant in time. Viewing the space of momenta as the random objects
provides us the connection to the Gaussian random process models on vector spaces. For this the
anatomical template formulation and elements of the orbit are represented by a template conﬁgu-
ration and the initial momenta. The template remains ﬁxed and the initial momenta are statistically
characterized.
We have already remarked that this approach is founded in the Lie group point of view. The
tangent space V at the identity id ∈G is considered the “Lie algebra” of the group. The idea will be
to use its vector ﬁeld elements and members of its dual space V∗the space of momenta to model
deformations, given that geodesics can be generated from elements of V or V∗. The power of
the approach comes from the dimensionality reduction of geodesic ﬂow to a single representative
element, and the fact that the representative space is linear. The linear statistical setting provides
a natural mechanism for coping with the non-linear nature of the diffeomorphic shape space.
Statistics on manifolds, in particular shape manifolds have been studied in, for example, [454,455].
The representation of shape via their Lie algebra has been applied in the statistical learning setting
in [456] and [457]. However, we now examine the diffeomorphic setting. We remark that the
approach to estimating the mean we have presented is analogous to the Lie group approaches
of computing, the so-called intrinsic mean of a probability measure on a Riemannian manifold
[455] which has been applied to shape in, for example, [456] and [457]. These other approaches
differ in that their Lie algebra representations do not necessarily generate diffeomorphisms of the
template.
17.2 Gaussian Random Fields
For constructing probability measures of anatomical variation, we shall characterize the maps as
Gaussian ﬁelds indexed over the manifolds on which the vector ﬁelds are deﬁned.
Specify the vector ﬁelds as a d-dimensional Gaussian random ﬁeld {U(x), x ∈X ⊂Rd}
completely speciﬁed by its covariance matrix ﬁeld which is a mapping K : X × X →Rd × Rd such
that K(x, y) = K∗(y, x) and for any integer n and any n-tuple of d-vectors w1, . . . , wn ∈Rd and
points x1, . . . , xn ∈X, 
n
i,j=1 w∗
i K(xi, xj)wj ≥0.
Deﬁne {U(x), x ∈X} to be a Gaussian random ﬁeld on the Hilbert space with mean ﬁeld
¯U and covariance ﬁeld KU if for all f ∈H, ⟨f, U⟩is Gaussian distributed with mean ⟨f, ¯U⟩, and
variance ⟨f, KUf⟩. Construct the {U(x), x ∈X} as a quadratic mean limit using a complete Rd-valued
orthonormal basis {ψk, k = 0, 1, . . . }, and the U-ﬁeld given according to
U(x)
q.m.
=
∞

k=0
Ukψk(x),
(17.1)
where Uk are independent Gaussian random variables with ﬁxed means ¯Uk and variances σ 2
k . The
mean and covariance operator of the ﬁeld becomes
¯U =
∞

k=0
¯Ukψk ,
KU =
∞

k=0
σ 2
k ψk⟨ψk, ·⟩.
(17.2)
Eqn. 17.1 is meant as the quadratic mean limit minimally requiring the process to have covariance
KU with ﬁnite trace so that 
k σ 2
k < ∞, 
k | ¯Uk|2 < ∞.

496
17 COMPUTATIONAL ANATOMY: HYPOTHESIS TESTING ON DISEASE
Since the random U-ﬁelds are Rd valued, for each k there will correspond d-orthogonal
eigenfunctions, leading to a natural indexing of the eigenfunctions and variances according to
{φ(i)
k , σ (i)
k , i = 1, . . . , d}.
17.2.1 Empirical Estimation of Random Variables
Given the set of displacement ﬁelds {u(1)(·), u(2)(·), . . . u(n)(·)}, then maximum-likelihood estimation
can be used directly to estimate from the anatomical maps the mean and covariance operators
of the prior distribution. Let u(i)
k
=< u(i), φk > be the coefﬁcients generated from the ith map,
i = 1, . . . n, then the log-likelihood written in the K−dimensional basis gives
ℓ( ¯U, σ; u(1), . . . , u(n)) = −n
2
K

k=0
log

2πσ 2
k

−1
2
K

k=0

n
i=1 |u(i)
k −¯Uk|2
σ 2
k
.
(17.3)
The maximum-likelihood estimates for the mean and variances of the orthogonal Gaussian
randomvariablesUk ∼N( ¯Uk, σ 2
k )giventhesetofanatomicalmapswithmean-ﬁeldandcovariance
( ¯U, KU) given by Eqn. 17.2 become
ˆ¯Uk = 1
n
n

i=1
⟨ψk, U(i)⟩,
ˆσ 2
k = 1
n
n

i=1
|⟨ψk, U(i)⟩−ˆ¯Uk|2.
(17.4)
17.3 Shape Representation of the Anatomical Orbit Under Large
Deformation Diffeomorphisms
Now we examine the representation of typicality and deviation from typicality in the orbit of
shapes and images. We study throughout the metric connection model in which single instants
are observed, from the orbit of imagery
Iα =

g · Iα = Iα(g−1
1 ) : ∂gt
∂t = vt(gt), t ∈[0, 1], g0 = id, v ∈V

.
(17.5)
It will be helpful to index the ﬂow explicitly by the vector which generates it, so that gv
t satisﬁes
˙gv
t = vt(gv
t ). According to Theorem 11.8 of Chapter 11 we know from the fundamental conservation
law (∂/∂t)⟨Avt, wt⟩2 = 0, t ∈[0, 1], thus the vector ﬁeld and momentum acting against vector ﬁelds
at the identity completely determine the geodesic carrying one object to another:
Avt = (Dgv−1
t
)∗Av0(gv−1
t
)|Dgv−1
t
|,
t ∈[0, 1].
(17.6)
Understanding variation in the anatomical orbit is reduced to understanding the vector ﬁeld
and momentum at the identity of t = 0 of the ﬂow. Thus we model the empirical space of mappings
by reducing them to their encoding via the vector ﬁeld and momentum at t = 0. Identifying the
image in the anatomical orbit I with the vector V0, which generates it accordingly, motivates our
model on V0 for anatomical ensembles as Gaussian vector ﬁelds with mean and covariances. Then
we shall model the family of vector ﬁelds as a Gaussian random ﬁeld with mean and covariance
¯V, KV. The statistical law P on I is induced by the law on V0. Specializing the study of statistics
via the initial momentum is a a natural setting for linear statistical analysis.

17.3 SHAPE REPRESENTATION OF THE ANATOMICAL ORBIT
497
Construction 17.1 (The Large Deformation Statistical Model)
Given is the anatomi-
cal orbit
I = {I = Iα ◦g−1
1
: ˙gt = vt(gt), g0 = id , v ∈V}.
(17.7)
Associating to each I the V0 such that
I(V0) = Iα ◦g−1
1
where AVt
= (DgV−1
t
)∗AV0(gV−1
t
)|DgV−1
t
|,
˙gV
1 = Vt(gV
t ),
gV
0 = id.
(17.8)
Then V0 is a Gaussian random ﬁeld with mean and covariance ¯V, KV.
The statistical law P on I is induced by the law of V0.
17.3.1 Principal Component Selection of the Basis from
Empirical Observations
We have already explored pattern representation via principle components for empirical covari-
ance estimation. Model the space of empirical vector ﬁelds as realizations of a Gaussian random
ﬁeld with covariance constructed to ﬁt the empirical variation. The complete orthonormal basis for
the Hilbert space representation is generated from the orthonormal eigenfunctions of the empirical
covariance.
Let V(·) on X be a zero mean real-valued Gaussian random ﬁeld with covariance K(x, y) on
closed and bounded background space X. The set of orthonormal functions {φk(·), k = 1, . . . , n}
minimizing the mean squared error
arg min
φk∈H
E




X
V(x) −
n

k=1
Vkφk(x)

2
dx


,
(17.9)
where Vk =

X V(x)φk(x) dx, satisﬁes the integral equation
λkφk(x) =

X
K(x, y)φk(y) dy.
(17.10)
The eigenvalues estimate the variance along the axis of the corresponding eigenvector. So,
if D is chosen such that
D

k=1
λk ≥
α
100

k
λk,
then the subspace spanned by φ1, . . . , φD retains α% of the variation in the training set, and
we can hope to model the class of shapes described by the training set by D parameters
γ1, . . . , γD.
Equipped with the evolution equations for the diffeomorphism from the initial vector
ﬁelds it is straight-forward to state PCA of shapes via the initial vector ﬁeld and momentum at
t = 0.

498
17 COMPUTATIONAL ANATOMY: HYPOTHESIS TESTING ON DISEASE
Algorithm 17.2 (Principle Components)
Consider a set of n + 1 images I0, I1, I2, . . . ,
In ∈I from the statistical orbit model.
1. Set initial template to Iα = I0.
2. Determine the vector ﬁelds v(i)
t , t ∈[0, 1], i = 1, . . . , n satisfying ˙g(i)
t
= v(i)
t (g(i)
t ) with
minimum ∥Iα ◦g(i)
1 −Ii∥2
2.
3. Compute the mean vector ﬁelds
¯v0 = 1
n
n

i=1
v(i)
0 .
(17.11)
4. Procrustees [458] Mean Generation: Regenerate template Iα = Iα ◦¯g−1
1
where ¯gt, t ∈
[0, 1], ¯g0 = id is generated from the mean vector ﬁeld:
∂¯gt
∂t = ¯vt(¯gt),
A¯vt = (D¯g−1
t
)∗A¯v0(¯g−1
t
)|D¯g−1
t
|.
(17.12)
5. If the template changes return to step 2. Otherwise continue.
6. Since the space of initial momenta is linear, apply PCA to the resulting v(i)
0 , i = 1, . . . , n.
17.4 The Momentum of Landmarked Shape Spaces
For speciﬁcity, return to landmarked shape spaces IN = (x1, . . . , xN) ∈IN ⊂RdN. One of the
most important applications of the initial momentum point of view is in effective linearization
of the space of shapes. Because of the ﬁnite N-dimensional nature of landmarked imagery, the
momentum is particularly simple and PCA boils down to the familiar SVD factorization of ﬁnite
matrices generated from the momenta.
17.4.1 Geodesic evolution equations for landmarks
Given a template conﬁguration IN−α(x1, . . . , xN) and the initial velocity v0 of a geodesic in the orbit,
we have from Chapter 11, Theorem 11.9, the momentum satisfying Av0(x) = 
N
n=1 δ(xn −x)βnt.
The elements βn0, n = 1, . . . , N represent the initial momenta. The evolution equations describing
the transport of the template along the geodesic are given as well by Theorem 11.9, Corollary 11.11
according to
vt(gt(xi)) =
N

j=1
K(gt(xj), gt(xi))βjt
(17.13)
−˙βit =

Dvt(git)
	∗βit.
(17.14)
We recognize that Eqns. 17.13 and 17.14 are an initial value ODE system. So, given initial val-
ues g0(xn), βn0, n = 1, . . . , N, we can solve this system to give the unique solution (gt(xn), βnt,
n = 1, . . . , N for all t ∈[0, 1], and hence vt(x) over all x ∈.

17.4 THE MOMENTUM OF LANDMARKED SHAPE SPACES
499
Algorithm 17.3 (Principle Components of Landmark Momentum)
The
conﬁgura-
tionspaceoflandmarks, IN ⊂RNd withinitialtemplateconﬁgurationIN−α = (x1, x2, . . . , xN)
and a set N vector momenta, β(i) = β(i)
10 , . . . , β(i)
N0, i = 1, . . . , n, the PCA landmark momentum
construction is as follows:
1. Estimate the mean and covariance:
¯β0 = 1
n
n

i=1
β(i)
0
Kjk = 1
n
n

i=1
(β(i)
j0 −¯βj0)(β(i)
k0 −¯βk0).
The mean is typically estimated in an iterative procedure which couples an alignment
procedure, called Procrustes alignment, with the computation above to give what is
called the Procrustes mean.
2. The desired basis is given by the eigenvectors, φi, i = 1, . . . , Nd, of the covariance
matrix Kij, where the order is determined by the decreasing order of the corresponding
eigenvalues, λ1 ≥λ2 ≥· · · ≥λNd.
3. A particular landmark conﬁguration or shape IN = (y1, . . . , yN) is approximated by
ﬁnding the initial momentum β0 = (β10, . . . , βN0) at t = 0 which maps the template
IN−α to the shape and then computing its orthogonal projection onto the subspace
spanned by the eigenfunctions and the approximation:
ˆβ0 =
D

j=1
γjφj
where γj =
NK

i=1
⟨β0 −¯β), φi⟩.
(17.15)
17.4.2 Small Deformation PCA Versus Large Deformation PCA
Standard small deformation linear vector space PCA approaches fail in the large deformation
setting. Figures 17.1 and 17.2 show an example from Vaillant [284] illustrating the application of
landmark based LD-DMM and the PCA analysis. Shown in Figure 17.1 is an example of 2D face
characature Vaillant has studied. Here simple curves in the plane are generated to represent the 2D
geometry (2D); shown in the panels are examples of the simple points on the curves being moved
under the geodesic diffeomorphic metric landmark matching. For this 2D example in the plane the
landmarks are annotated features in photographs of the face (data is from theAAM database [459]).
The left panel shows the template landmark conﬁguration (straight lines connecting landmarks)
overlayed on the corresponding image of the face. The far right panel shows the overlay of the
target landmark conﬁguration on its corresponding image, and the center panels show snapshots
from the time sequence of deformation of the template conﬁguration. We see a smooth deformation
as the template conﬁguration moves close to the target.
Clearly, the resulting conﬁguration in the standard small deformation linear vector space
PCA approaches fail in the large deformation setting. This is particularly evident for large coef-
ﬁcients γj. Consider three landmark conﬁgurations I(1), I(2), and I(3) shown in Figure 17.2 of 2D
face images taken from the AAM database [459]. Suppose ¯I = I(2), and we take φ1 = I(1) −¯I and
φ3 = I(3) −¯I as two basis elements. Sampling the subspace generated by φ1 and φ3 by generating
random coefﬁcients γ1 and γ3, and producing
¯I + γ1φ1 + γ3φ3,

500
17 COMPUTATIONAL ANATOMY: HYPOTHESIS TESTING ON DISEASE
Figure 17.1 2D Face Cartoon landmark LDDMM (see also Plate 42).
I(1) = I + f1
I + g1f1 + g3f3
I(3) = I + f3
I(2) = I
I
g1
g1f1 + g3f3
(1)
(4)
(5)
(2)
(3)
Figure 17.2 Non-linearity
artifact
in
panel
4
overcome
by
initial
momentum
approach
shown in panel 5. Results taken from Vaillant [284] (see also Plate 43).
we ﬁnd elements of the subspace. Examine the solution for large coefﬁcients as depicted in panel 4.
Notice it no longer represents the “shape” of a face; the mouth appears above the nose and struc-
tures run into one another. Now consider the large deformation diffeomorphism PCA. This allows
us to overcome the non-linearity of the shape space in a fundamental way. Considering again the
same three conﬁgurations in Figure 17.2, we compute the initial momentum mapping ¯I to I1 and
I3, respectively. Now, if we again generate random coefﬁcients γ1 and γ3, set
ˆβ0 = γ1φ1 + γ3φ3,
and apply the evolution Eqns. 17.13, 17.14 to the initial conditions we obtain a natural combination
of the two conﬁgurations that is guaranteed to be a diffeomorphism of ¯I. In particular, taking the
same coefﬁcients γ1 and γ3 which produced the pathological model of panel 4 of Figure 17.2, we
obtain panel 5 of Figure 17.2 which clearly retains the structure of a face.
Here are some 3-dimensional (3D) example results from Vaillant, Trouve, and Younes.
Example 17.4 (3D Face Surfaces)
Examine statistical results from the landmark
matching of Vaillant [284] from Corollary 11.11 and Example 11.12. of Chapter 11. The
hippocampi data are part of the morphology testbed for the Biomedical Infomatics
Research Network (BIRN, www.nbirn.net).
Shown in Figure 17.3 is PCA via initial momentum of 100 annotated surfaces
from the Morphable Faces database [460] of landmarks annotated on the 2D face
manifold. The top row shows the mean conﬁguration in frontal and proﬁle view and
the subsequent rows show deformation of the mean conﬁguration along the ﬁrst three
principal directions, respectively.

17.4 THE MOMENTUM OF LANDMARKED SHAPE SPACES
501
I
g1
–3
I
l1f1
 
g1
–3
I
l2f2
g1
3
I
l1f1
 
g1
3
I
l2f2
 
I
g1
–3
I
l1f1
 
g1
–3
I
l2f2
g1
3
I
l1f1
 
g1
3
I
l2f2
 
(1)
(2)
Figure 17.3 First two eigenmodes of 3D PCA applied to the Morphable Faces database of
100 faces. Row 1 shows front views; row 2 shows side views. Column 1 shows template. Columns 2
and 3 show deformation via eigenfunction 1. Columns 4 and 5 show deformation via eigenfunction
2 (see also Plate 44).
I
g1
–3
I
l1f1
 
g1
–3
I
l2f2
g1
3
I
l1f1
 
g1
3
I
l2f2
 
I
g1
–3
I
l1f1
 
g1
–3
I
l2f2
g1
3
I
l1f1
 
g1
3
I
l2f2
 
Figure 17.4 First two eigenmodes of 3D PCA applied to hippocampi from Randy Bucker of
Washington University. Column 1 shows front and side view. Top row shows deformation via
eigenfunction 1. Bottom row shows deformation via eigenfunction 2 (see also Plate 45).

502
17 COMPUTATIONAL ANATOMY: HYPOTHESIS TESTING ON DISEASE
Example 17.5 (3D Hippocampi)
Now examine 3D PCA applied to the left hip-
pocampus of 19 normal subjects. The hippocampi data are part of the morphology
testbed for the Biomedical Infomatics Research Network (BIRN, www.nbirn.net).
Shown in Figure 17.4 are 3D PCA results on the momentum mapping of two hip-
pocampi applied to the left hippocampus of 19 normal subjects. The mean is shown
in two views in the top row, and the deformation of the mean along the ﬁrst three
eigenmodes is shown in the subsequent rows.
17.5 The Small Deformation Setting
Diffeomorphisms cannot be added, only composed. Therefore average coordinates and templates
generated via averaging of diffeomorphisms is suspect! However, for small deformations a tem-
plate coordinate system can be constructed from averages of transformations. Empirical estimation
of the templates for the various subpopulations of interest use the ideas of minimum mean-squared
estimation (MMSE) for generating the template. Assume a metric distance based on the quadratic
energy ∥(g −id)∥2
V. Intuitively, the template yet to be discovered should be deﬁned to be the
element Iα ∈I which requires the lowest average energy deformation onto the population of
anatomies. The template representing the population is deﬁned to be the image I ∈I minimizing
the overall energy of the transformation of the population to the template.
17.6 Small Deformation Gaussian Fields on Surface Submanifolds
The beauty of the general deﬁnition of {U(x), x ∈X} as a Gaussian random ﬁeld through its inner
product with functions in the Hilbert space is that it immediately generalized to more general back-
ground spaces such as surface submanifolds M ⊂R3. We are interested in 2D manifolds associated
with embedded surfaces such as the hippocampus. To quantify the shape of the hippocampus, we
use complete orthonormal bases representing the normal deviations of populations of hippocampi.
To construct the variability representation, deﬁne that the Gaussian random ﬁeld U(x), x ∈S on the
domain M is completely speciﬁed by its covariance matrix ﬁeld, mapping K : M × M →R3 × R3.
Notice this is a Gaussian ﬁeld on the submanifold M. The complete orthonormal bases are gen-
erated using singular value decomposition (SVD) of the empirical covariances and then deriving
the largest eigenfunctions.
Assuming that the underlying coordinate system is a smooth 2D manifold, an example is
shown in Figure 17.5 showing a whole MRI–MPRAGE image volume, with a section through the
brain delineating the surface of the hippocampus. The left panel shows a section through the brain
delineating the surface of the hippocampus; the middle panel shows a rendering of the entire
surface bounding the full volume of the hippocampus. Left column panel 2 shows the triangu-
lated graph representing the mean state of the hippocampus. The data are courtesy of Dr. John
Csernansky of the Department of Psychiatry at Washington University. Right column four panels
show the ﬁrst four surface harmonics visualized through deformation of the template (panel 2).
Figure 17.6 shows the statistical characterization of the shape of the hippocampus. Panels 1–3
show maps of the initial hippocampus to three in a population of 30 patients, M0◦g1, M0◦g2, M0◦g3
studied by Csernansky et al. [162]. Panel 4 shows the composite template generated from the
average of 30 maps, M0 ◦¯g. Panels 5 and 6 of the bottom row show the ﬁrst two eigenshapes of
the left and right hippocampus generated from the maps of normals and schizophrenics.
Shown in the bottom right panels of Figure 17.6 are random instances of left and right
hippocampi generated via the empirically estimated covariance function for the Gaussian random
ﬁeld on the hippocampus surface.

17.7 DISEASE TESTING OF AUTOMORPHIC PATHOLOGY
503
ADINA-PLOT VERSION 6.1.4, 26 JUNE 1996
FREQUENCY ANALYSIS OF A HIPPOCAMPUS
X
Y
ADINA
MODE_SHAPE
MODE 7
F = 0.3038
MODESHAPE
0.05618
XVMIN
24.4
XVMAX 109.
YVMIN
47.8
YVMAX 100.
X
Y
ADINA
MODE_SHAPE
MODE 8
F = 0.4201
MODESHAPE
0.06988
XVMIN
28.0
XVMAX 108.
YVMIN
47.8
YVMAX 101.
X
Y
ADINA
MODE_SHAPE
MODE 9
F = 0.5720
MODESHAPE
0.05143
XVMIN
27.7
XVMAX 109.
YVMIN
46.2
YVMAX 100.
X
Y
ADINA
MODE_SHAPE
MODE 10
F = 0.6216
MODESHAPE
0.06707
XVMIN
27.0
XVMAX 111.
YVMIN
47.2
YVMAX 100.
(1)
(1)
(2)
(3)
(4)
(2)
Figure 17.5 Left column panel 1 shows a whole MRI–MPRAGE image volume with a section
through the brain delineating the surface of the hippocampus. Left column panel 2 shows the
triangulated graph representing the mean state of the hippocampus. The data are courtesy of
Dr. John Csernansky of the Department of Psychiatry at Washington University. Right column
four panels show the ﬁrst four surface harmonics visualized through deformation of the template
(panel 2). Courtesy Sarang Joshi, Ph.D. thesis [242] (see also Plate 46).
17.7 Disease Testing of Automorphic Pathology
Now examine abnormal anatomies. In formalizing pathologies we shall employ the dichotomy
automorphic pathologies versus heteromorphic pathologies following the terminology in [5]. Auto-
morphic pathologies modify anatomical conﬁgurations within the orbit G : Iα →Iα, so that
g : I ∈Iα →I′ = I(g) ∈Iα, so that the space of anatomies is not exited Iα. This is not true for
the heteromorphic pathologies for which the new conﬁguration is generated via a more drastic
change of the regular topology, perhaps adding neoplasm or eliminating structures. This requires
the expansion to multiple anatomies, I = ∪αIα, with the transformations acting G : Iα →I.
Throughout we shall use the terms normal/abnormal in a way that differs from the custom-
ary one. Abnormal signiﬁes something exceptional, an anomaly, but not necessarily indicating
improper functioning. As well, throughout we shall concentrate solely on change in topologi-
cal structure. There is of course a second type of automorphic pathology, one corresponding to
changes in the textures of the normal textbook pictures I ∈I. We shall not explore this here.
17.7.1 Hypothesis Testing on Disease in the Small Noise Limit
Automorphic pathologies though abnormal shall correspond to changes in structure which pre-
serve the topological structures of the orbit. If the transformation g ∈G is too far from the identity
element in the similarity group so that it falls outside of a conﬁdence set of the null hypothesis,
H0, we can identify this kind of abnormality by estimating the density p(U) and applying the
Neyman–Pearson lemma to arrive at an appropriate decision.

504
17 COMPUTATIONAL ANATOMY: HYPOTHESIS TESTING ON DISEASE
Subject 1
(1)
(5)
(6)
(7)
(2)
(3)
(4)
Subject 2
Subject 3
Sample Mean
Left Eigenfunction 1
Right Eigenfunction 2
Figure 17.6 Top row: Panels 1–3 show maps of the initial hippocampus to three in a population
of 30 patients, M0 ◦g1, M0 ◦g2, M0 ◦g3 studied in Csernansky et al. [162]. Panel 4 shows the
composite template generated from the average of 30 maps, M0 ◦¯g. Bottom left column shows
the ﬁrst two eigenshapes of the left and right hippocampus generated from a population of
maps of normals and schizophrenics. Lower right panels show random instances of left and right
hippocampi generated via the empirically estimated covariance function for the Gaussian random
ﬁeld on the hippocampus surface. Taken from Joshi [242] (see also Plate 47).
Under the null (normal) and disease hypotheses let the random deformation have densities
H0 : π0(U), H1 : π1(U). Assume the image ID are generated of a particular patient’s anatomy, the
goal being to diagnose disease. The likelihood function of the measured imaged data ID expresses
the physical characteristics of the sensor(s) used and therefore does not depend upon the choice
of prior, thus f(ID|U) is the same for both hypotheses H0, H1. This implies that the density f0(·) of
the observed deformed image ID under the hypotheses H0, H1 becomes
H0 : p0(ID) =

G
π0(U)f(ID|U) dU ;
(17.16)
H1 : p1(ID) =

G
π1(U)f(ID|U) dU.
(17.17)
The question of how to calculate such integrals has been addressed in [5]. The most powerful test
comes out directly from the Neyman–Pearson lemma in terms of the critical region W as
W =
C
ID : p1(ID)
p0(ID) ≥constant
F
.

17.7 DISEASE TESTING OF AUTOMORPHIC PATHOLOGY
505
Assuming the sensor to be accurate so that the posterior densities p0 ∝π0(·)f(ID|·), p1 ∝
π0(·)f(ID|·) are very peaked, then the vector ﬁeld U is observed with high accuracy, so that the
test can be organized in terms of the vector ﬁelds directly. Then the most powerful test effectively
becomes
W =

U : π1(U)
π0(U) ≥constant

.
Throughout this chapter we assume that the deformation is highly determined by the data.
To carry out disease testing via hypothesis testing we construct the distributions π0, π1 empirically
under this assumption.
17.7.2 Statistical Testing
To calculate the optimal test statistics the population under study can be grouped into n0 number of
controls and n1 number of patients. The basis coefﬁcients {U0
1, . . . , U0n0} are random samples from
a random process whose mean is ¯U0 and covariance , and {U1
1, . . . , U1n1} are random samples
from the same random process with mean ¯U1 and the same . The sample means and covariances
¯U,  formed from each group are calculated as
ˆ¯U0 = 1
n0
n0

i=1
U0
i
ˆ¯U1 = 1
n1
n1

j=1
U1
j
(17.18)
and the pooled (common) sample covariance
ˆ =
1
n0 + n1 −2


n0

i=1
(U0
i −ˆ¯U0)(U0
i −ˆ¯U0)∗+
n1

j=1
(U1
j −ˆ¯U1)(U1
j −ˆ¯U1)∗

.
(17.19)
Then, to establish a hypothesis test of the two groups means assuming an unknown but common
covariance, the null hypothesis is
H0 : ¯U0 = ¯U1,
(17.20)
and Hotelling’s T2 statistic (for two samples) is
T2 =
n0n1
n0 + n1
 ˆ¯U0 −ˆ¯U1∗ˆ−1 ˆ¯U0 −ˆ¯U1
.
(17.21)
The basis coefﬁcients {U0
1, . . . , U0n0} are distributed according to N( ¯U0, ), and {U1
1, . . . , U1n1}
according to N( ¯U1, ). The sample means ˆ¯U0 and ˆ¯U1 are distributed according to N( ¯U0, (1/n0))
and N( ¯U1, (1/n1)), respectively. Consequently,

n0n1/(n0 + n1)( ˆ¯U0−ˆ¯U1) is distributed accord-
ing to N(0, ) under the null hypothesis. Following [461, p. 109], (n0 + n1 −2) ˆ is distributed as

n0+n1−2
i=1
UiU∗
i where Ui is distributed according to N(0, ). Thus, T2 has an F distribution, and
the null hypothesis H0 is rejected with a signiﬁcance level α if
T2 ≥(n0 + n1 −2)K
n0 + n1 −K −1 F∗
K,n0+n1−K−1(α),
(17.22)
where F∗
K,n0+n1−K−1(α) denotes the upper 100α% point of the FK,n0+n1−K−1 distribution, and
K is the total number of basis functions used in calculating the T2 statistics. Logistic regressions

506
17 COMPUTATIONAL ANATOMY: HYPOTHESIS TESTING ON DISEASE
based on χ2 scores performed on the two groups of coefﬁcients (control and schizophrenia groups)
indicate that linear combinations of a subset of the basis functions sufﬁce in describing the dif-
ference of the two groups. For each subject, using the subset of basis functions, we calculate the
log-likelihood ratio:
 = −1
2

Ui −ˆ¯U schiz∗ˆ−1
Ui −ˆ¯U schiz
+ 1
2

Ui −ˆ¯U ctrl∗ˆ−1
Ui −ˆ¯U ctrl
.
(17.23)
Here, the vector coefﬁcients symbols represent the coefﬁcient vectors with the selected components
only (e.g. Ui is the vector coefﬁcient for subject i using only the second, 12th, and 17th components,
and similarly, ˆ¯U ctrl and ˆ¯U schiz are the sample means of the coefﬁcient vectors using the above
components for the control and schizophrenia groups, respectively; see example below). The log-
likelihood ratios  give rise to a classiﬁcation scheme under two hypotheses: under H0 (when
 < 0) the sufﬁcient statistics is Gaussian distributed with mean ¯U0 and variance σ 2
0 ; under H1
(when  > 0), mean ¯U1 and variance σ 2
1 .
Example 17.6 (Schizophrenia)
The Csernansky group has examined morphometric
differences in schizophrenia. For the study 15 schizophrenia subjects were studied and
compared with 15 controls. Shown in Figure 17.7 are results from the schizophrenia
study. Wang and Joshi et al. have found that scale and volume are not powerful
discriminants of group difference in the two populations—testing based on scale and
volume gives no signiﬁcant discrimination, p = 0.19 and p = 0.27; however, shape
difference is.
Panel 1 shows differences of hippocampal surface patterns between the control
and schizophrenia groups visualized as z-scores on the mean surface of the control
group. Inward surface deformations due to schizophrenia are visualized in colder
colors, outward in warmer colors, and areas which are not deformed in neutral yellow
to green colors. Panel 2 shows statistical testing of shape differences: log-likelihood
ratios for a linear combination of basis functions {1,3,4,6,10,15}. Multivariate ANOVA
indicates signiﬁcant between-group difference: p = 0.0028 (F = 4.73, df = 1, 28).
The bottom row shows asymmetry results in schizophrenia. Panel 3 shows difference
of hippocampal surface asymmetry patterns between the control and schizophrenia
groups visualized as z-scores on the mean ﬂipped right-side surface of the control
group. Inward surface deformations due to differences in asymmetry are visualized
in colder colors, outward in warmer colors, and areas which are not deformed in
neutral yellow to green colors. Panel 4 shows statistical testing of asymmetry pattern
differences: log-likelihood ratios for a linear combination of basis functions {2,12,17}.
Multivariate ANOVA indicates signiﬁcant between-group difference in asymmetry:
p = 0.0029 (F = 6.03, df = 3, 26).
Example 17.7 (Alzheimer’s and Aging)
The Csernansky group has examined mor-
phometric changes due to normal aging and dementia. For the study 15 younger
controls were studied and compared with 18 elderly controls and 18 CDR 0.5 AD
patients. Shown in Figure 17.8 are results from the Alzheimer’s and normal aging
study. Panel 1 shows difference of hippocampal surface patterns between the elderly
control (CDR 0) and AD (CDR 0.5) groups visualized as z-scores on the mean surface
of the elderly group. Inward surface deformations due to AD are visualized in colder
colors, outward in warmer colors, and areas which are not deformed in neutral yellow
to green colors. Panel 2 shows statistical testing of shape differences: log-likelihood
ratios for a linear combination of basis functions {1,5}. Multivariate ANOVA indicates
signiﬁcant between-group difference: p = 0.0002 (F = 11.4, df = 2, 33). The bottom
row shows a normal aging comparison. Panel 3 shows difference of hippocampal sur-
face patterns between the younger and the elderly control (CDR 0) groups visualized
as z-scores on the mean surface of the younger group. Inward surface deformations
due to aging are visualized in colder colors, outward in warmer colors, and areas

17.7 DISEASE TESTING OF AUTOMORPHIC PATHOLOGY
507
(1)
Control
Schizophrenia
Λ
Sufficient statistics, comcov covariance, [2  12  17] 
5
0
5
(4)
(3)
Young Control
Schizophrenia
Λ
Sufficient statistics, common covariance, [1   3   4   6  10  15]
(12/15)
(14/15)
–6
–4
–2
0
2
4
6
8
(2)
–1
0
1
–1
0
1
Figure 17.7 Top row: Schizophrenia. Panel 1 shows difference of hippocampal surface patterns
between the control and schizophrenia groups visualized as z-scores on the mean surface of
the control group. Inward surface deformations due to schizophrenia are visualized in colder
colors, outward in warmer colors, and areas which are not deformed in neutral yellow to green
colors. Panel 2 shows statistical testing of shape differences: log-likelihood ratios for a linear
combination of basis functions {1,3,4,6,10,15}. Multivariate ANOVA indicates signiﬁcant between-
group difference: p = 0.0028 (F = 4.73, df = 1, 28). Bottom row: Asymmetry in Schizophrenia.
Panel 3 shows difference of hippocampal surface asymmetry patterns between the control and
schizophrenia groups visualized as z-scores on the mean ﬂipped right-side surface of the control
group. Inward surface deformations due to differences in asymmetry are visualized in colder colors,
outward in warmer colors, and areas which are not deformed in neutral yellow to green colors.
Panel 4 shows statistical testing of asymmetry pattern differences: log-likelihood ratios for a linear
combination of basis functions {2,12,17}. Multivariate ANOVA indicates signiﬁcant between-group
difference in asymmetry: p = 0.0029 (F = 6.03, df = 3, 26) (see also Plate 48).
which are not deformed in neutral yellow to green colors. Panel 4 shows statistical
testing of shape differences: log-likelihood ratios for a linear combination of basis
functions {1,2}. Multivariate ANOVA indicates signiﬁcant between-group difference:
p < 0.0001 (F = 348, df = 2, 30).

508
17 COMPUTATIONAL ANATOMY: HYPOTHESIS TESTING ON DISEASE
CDR 0 
CDR 0.5
–4
–3
–2
–1
0
1
2
3
4
Λ
Sufficient statistics, common covariance, [1  5]
(14/18)
(13/18)
Young 
CDR 0 
Λ
Sufficient statistics, common covariance, [1  2]
(15/15)
(18/18)
–60
–40
–20
0
20
40
60
–1
0
1
–1
0
1
(1)
(2)
(3)
(4)
Figure 17.8 Top row: Alzheimer’s Disease. Panel 1 shows difference of hippocampal surface pat-
terns between the elderly control (CDR 0) and AD (CDR 0.5) groups visualized as z-scores on the
mean surface of the elderly group. Inward surface deformations due to AD are visualized in colder
colors, outwards in warmer colors, and areas which are not deformed in neutral yellow to green
colors. Panel 2 shows statistical testing of shape differences: log-likelihood ratios for a linear
combination of basis functions {1,5}. Multivariate ANOVA indicates signiﬁcant between-group
difference: p = 0.0002 (F = 11.4, df = 2, 33). Bottom row: Normal Aging. Panel 3 shows dif-
ference of hippocampal surface patterns between the younger and the elderly control (CDR 0)
groups visualized as z-scores on the mean surface of the younger group. Inward surface deforma-
tions due to aging are visualized in colder colors, outward in warmer colors, and areas which are
not deformed in neutral yellow to green colors. Panel 4 shows statistical testing of shape differ-
ences: log-likelihood ratios for a linear combination of basis functions {1,2}. Multivariate ANOVA
indicates signiﬁcant between-group difference: p < 0.0001 (F = 348, df = 2, 30) (see also Plate
49).
Example 17.8 (Alzheimer Progression)
The Csernansky group has examined mor-
phometric changes in early AD through two-year follow-up. For the study 18 CDR 0.5
AD subjects were studied and compared with 26 CDR 0 controls using scans collected
two years apart for each subject. Shown in Figure 17.9 are results from theAlzheimer’s
follow-up study. Panel 1 shows deformations from baseline to follow-up for the CDR
0.5 AD group. Inward surface deformations due to AD are visualized in colder colors,
outward in warmer colors, and areas which are not deformed in neutral yellow to
green colors. Panel 2 shows deformations from baseline to follow-up for the CDR 0
control group. Inward surface deformations due to AD are visualized in colder colors,
outward in warmer colors, and areas which are not deformed in neutral yellow to

17.7 DISEASE TESTING OF AUTOMORPHIC PATHOLOGY
509
–1
0 mm
1
p<.05(in)
p>.05
p<.05(out)
CDR 0.5
CDR 0
Log likelihood ratio, Λ
–8
–6
–4
–2
0
2
4
6
(1)
(2)
(3)
(4)
Figure 17.9 AD Progression. Panel 1 shows deformations from baseline to follow-up for the
CDR 0.5 AD group. Inward surface deformations due to AD are visualized in colder colors,
outwards in warmer colors, and areas which are not deformed in neutral yellow to green
colors. Panel 2 shows deformations from baseline to follow-up for the CDR 0 control group.
Inward surface deformations due to AD are visualized in colder colors, outward in warmer
colors, and areas which are not deformed in neutral yellow to green colors. Panel 3 shows
follow-up-versus-baseline “spread” of the between-group inward surface deformation patterns,
shown as Wilcoxon’s sign rank test map on the CDR 0 mean surface. Areas of signiﬁcant
(p < 0.05) inward deformation at baseline of CDR 0.5 group are shown in turquoise color,
representing 38% of total hippocampal surface area. By follow-up, areas of signiﬁcant inward
deformation have increased to 47% of total hippocampal surface area. The increased affected
areas are shown in purple color. Areas of non-signiﬁcant surface deformation are shown in
green color. Panel 4 shows statistical testing of shape differences: log-likelihood ratios for
a linear combination of basis functions {1,2,4,11}. Multivariate ANOVA of the ﬁrst 12 basis
functions indicates signiﬁcant between-group difference: p = 0.014 (F = 2.66, df = 12, 31)
(See also Plate 50).
green colors. Panel 3 shows follow-up-versus-baseline “spread” of the between-group
inward surface deformation patterns, shown as Wilcoxon’s sign rank test map on the
CDR 0 mean surface. Areas of signiﬁcant (p < 0.05) inward deformation at baseline of
CDR 0.5 group are shown in turquoise color, representing 38% of total hippocampal
surface area. By follow-up, areas of signiﬁcant inward deformation have increased to
47% of total hippocampal surface area. The increased affected areas are shown in pur-
ple color. Areas of non-signiﬁcant surface deformation are shown in green color. Panel
4 shows statistical testing of shape differences: log-likelihood ratios for a linear combi-
nation of basis functions {1,2,4,11}. Multivariate ANOVA of the ﬁrst 12 basis functions
indicates signiﬁcant between-group difference: p = 0.014 (F = 2.66, df = 12, 31).

510
17 COMPUTATIONAL ANATOMY: HYPOTHESIS TESTING ON DISEASE
–1
0 mm
1
Figure 17.10 Depression. Panel shows differences of hippocampal surface patterns between
the control and depression groups visualized as perpendicular displacements on the mean sur-
face of the control group. Inward surface deformations due to depression are visualized in colder
colors, outward in warmer colors, and areas which are not deformed in neutral yellow to green
colors. Multivariate ANOVA of the ﬁrst 10 basis functions indicates signiﬁcant between-group
difference: p < 0.0001 (F = 34.1, df = 10, 58) (see also Plate 51).
Example 17.9 (Depression)
Posener et al. [437] have examined morphometric differ-
ences in depression. For the study 27 depression subjects were studied and compared
with 42 controls. Shown in Figure 17.10 are results from the depression study. The
panel shows differences of hippocampal surface patterns between the control and
depression groups visualized as perpendicular displacements on the mean surface of
the control group. Inward surface deformations due to depression are visualized in
colder colors, outward in warmer colors, and areas which are not deformed in neutral
yellow to green colors. Multivariate ANOVA of the ﬁrst 10 basis functions indicates
signiﬁcant between-group difference: p < 0.0001 (F = 34.1, df = 10, 58).
17.8 Distribution Free Testing
Joshi [242] has examined methods based on distribution free testing on shape change between
populations. The above mentioned p-values were derived based on the assumptions that the pop-
ulations Control and Schizophrenics were Gaussian distributed with different means and common
covariance. Joshi uses Fisher’s method of randomization to derive a distribution free estimate of
the level of signiﬁcance of the difference. The basis coefﬁcients {U0
1, . . . , U0n0} are random samples
from a random process whose mean is ¯U0 and covariance , and {U1
1, . . . , U1n1} are random sam-
ples from the same random process with mean ¯U1 and the same . The sample means formed
from each group will be as in Eqns. 17.18 and 17.19. To establish a hypothesis test of the two groups
means assuming an unknown but common covariance, the null hypothesis is H0 : ¯U0 = ¯U1, and
Hotelling’s T2 statistic (for two samples) is
T2 =
n0n1
n0 + n1
 ˆ¯U0 −ˆ¯U1∗ˆ−1 ˆ¯U0 −ˆ¯U1
.
(17.21)
In Fisher’s method of randomization, for all permutations of the given two groups, new
means and covariances are calculated. Monte Carlo simulations are used to generate a large num-
ber of uniformly distributed random permutations (a typical number is 10,000). The collection

17.8 DISTRIBUTION FREE TESTING
511
0
1
2
3
4
5
6
7
8
0
50
100
150
200
250
300
350
400
450
500
Exact test, with Fisher"s randomization, common covariance
T2 statistics (*26/(3*28)):10000 permutations,
basis vector = [2  12  17]
Control vs. Schizophrenia
T2
orig: 19.48
Forig: 6.03 (p = 0.0031)
F (3,26)
Forig (p = 0.0031)
p = .05
p = .01
Figure 17.11 Empirical distribution ˆF from randomized Fisher’s test with 10,000 group permuta-
tions, between the group of control and schizophrenia subjects. Basis vectors {2,12,17} are selected.
The p = 0.0031 value shown is calculated from Eqn. (17.25). Also shown are (i) ˆF(T2) value (solid
blue line) of the control-versus-schizophrenia group comparison; (ii) theoretical F-distribution
(solid red curve) with (3,26) degrees of freedom superimposed on the empirical distribution; and
(iii) p = 0.05 (red dotted line) and p = 0.01 (red dot–dash line) for reference (see also Plate 52).
of T2 statistics from each permutation gives rise to an empirical distribution ˆF(·) according to
(see Eqn. 17.22)
FK,n0+n1−K−1 = n0 + n1 −K −1
(n0 + n1 −2)K T2.
(17.24)
The null hypothesis that the two groups have equal means is rejected when
p =
 ∞
T2
ˆF(f) df
(17.25)
falls below a predeﬁned signiﬁcance level (e.g. 0.05).
Figure 17.11 plots the empirical distribution ˆF from randomized Fisher’s test with 10,000
group permutations, between the group of control and schizophrenia subjects. Basis vectors
{2,12,17} are selected. The p = .0031 value shown is calculated from Eqn. 17.25. Also shown
are (i) ˆF(T2) value (solid blue line) of the control-versus-schizophrenia group comparison; (ii)
theoretical F-distribution (solid red curve) with (3,26) degrees of freedom superimposed on the
empirical distribution; and (iii) p = 0.05 (red dotted line) and p = 0.01 (red dot–dash line) for
reference.
Apparently the Gaussian assumption for the coefﬁcient vectors are valid since the empirical
distribution of the ˆF statistics follows the F-distribution curve.
17.9 Heteromorphic Tumors
Thus far the automorphic pathologies are large deviations of identical topology modifying an
anatomical conﬁguration I ∈Iα into another I′ ∈Iα. Automorphic pathologies never leave the
original orbit of anatomies Iα. This is not the case for the heteromorphic pathologies for which a

512
17 COMPUTATIONAL ANATOMY: HYPOTHESIS TESTING ON DISEASE
new conﬁguration is generated via a change of the regular topology, perhaps adding neoplasm or
eliminating structures.
Dealing with heteromorphic pathologies is a different challenge. New masses due to tumors,
for example, introduce deformations which carry the conﬁguration outside of the space Iα, requir-
ing the introduction of a larger sample space I = ∪αIα, with G : Iα →I. To represent this type of
heteromorphic pathology, where new mass pushes out the normal tissue, we shall use a deformable
template such as in [137] for the mass and then condition the vector ﬁeld. Introduce the template
conﬁguration of the domain Xpath ⊂X representing the mass and denote its central point by
xcenter. On this template let the random transformations act. To ﬁx ideas let the translation groups
act on the background spaces Xpath ⊂X, and use a stochastic difference equation LU = W to
describe the normal displacements in X \ Xpath. The normal tissue around xcenter is displaced by
the new mass.
Denote the boundary of the pathology Xpath as ∂Xpath, and solve the difference equation in
the domain outside of the new mass
(LU)(x) = W(x),
x ∈X \ Xpath,
with the boundary conditions U(x) = xcenter −x, x ∈∂Xpath. In other words, the normal material
is pushed out from xcenter and replaced by the tissue in Xpath.
To represent heteromorphic shape abnormalities assume that the anatomy is enclosed in a
bounded rectangular background space X ⊂R3 and use the operator L = ∇2 + c, c < 0, with
Dirichlet boundary conditions on ∂X. Making c negative ensures non-singularity and existence of
a solution to the problem below. For simplicity assume the three components of the deformation
ﬁeld d(x); x ∈X to be independent; for the coupled case we should replace L by the Navier operator
used above.
Let a ∈X be a center for an expanding abnormality that expands into a region Xpath ⊂X.
For example, Xpath could be a ball with the center at a but more general shapes will be allowed. The
normal remainder will be denoted N = X \ Xpath. With the usual representation for the observed
image ID(x) = Iα[x −U(x)] we are led to the following stochastic ﬁeld equation for the pathology
induced displacement with the boundary conditions
(∇2 + c)U(x) = W(x),
x ∈X \ Xpath ,
(17.26)
U(x) = UA(x) = a −x,
x ∈∂Xpath,
(17.27)
U(x) = 0,
x ∈∂X,
(17.28)
with W a white noise ﬁeld. Note that U(·) is not and should not be deﬁned in the abnormal region
Xpath.
Let G = G(x, y), x, y ∈X \ Xpath be the Green’s function for the Laplacian ∇2 in X \ Xpath
with Dirichlet conditions. In X \ Xpath introduce a function v(x) taking the values upath(x) on
∂Xpath and zero on ∂X. For the new ﬁeld g = u −v we have
Lg = Lu −Lv = w −Lv
and g satisﬁes Dirichlet boundary conditions. It follows
g(x) =

X\Xpath G(x, y)f(y) dy
with some function f to be determined. Then
Lg(x) = ∇2

X\Xpath G(x, y)f(y) dy + c

X\Xpath G(x, y)f(y) dy = f(x) +

Xpath G(x, y)f(y) dy.

17.9 HETEROMORPHIC TUMORS
513
Heteromorphic Deformation
d
b
c
a
e
g
f
h
Figure 17.12 Figure showing heteromorphic deformations. The left panel shows an expansion or
“push out”; the right panel shows a “push in” (see also Plate 53).
But Lg = Lu −Lv = w −Lv so that we get the integral equation
f(x) +

X\Xpath G(x, y)f(y) dy = g(x) −Lv(x),
and the Fredholm equation should be solved with the Fredholm alternative holding.
The left panel of Figure 17.12 shows an expansion or “push out”; the right panel shows a
“push in.” Another pathology that may be of interest is when the change in tissue is not a “push
out” but “cover,” in the sense that the new mass takes the place of some of the normal ones, and
invades it without displacing it. This is straightforwardly represented by solving the stochastic
difference equation over the whole space X, and then placing a deformed template at some xcenter
covering the rest in that part of X.

18 M A R K O V P R O C E S S E S A N D R A N D O M S A M P L I N G
ABSTRACT
The parameter spaces of natural patterns are so complex that inference must often
proceed compositionally, successively building up more and more complex structures, as well
as back-tracking, creating simpler structures from more complex versions. Inference is trans-
formational in nature. The philosophical approach studied in this chapter is that the posterior
distribution that describes the patterns contains all of the information about the underlying
regular structure. Therefore, the transformations of inference are guided via the posterior
in the sense that the algorithm for changing the regular structures will correspond to the
sample path of a Markov process. The Markov process is constructed to push towards the
posterior distribution in which the information about the patterns are stored. This provides
the deep connection between the transformational paradigm of regular structure creation,
and random sampling algorithms.
The connection to optimal inference is through the posterior distribution. Suppose we
choose to generate a minimum-risk estimator; given a risk function R : I × I →R+ quan-
tifying relative merit between the estimator and the truth, the optimal estimator I∗attains
the minimum over all estimators R = EP(dI,dID)R(I, I ∗(ID)). Rarely will these be generated
analytically; rather, they must be simulated from the posterior, revealing the remarkable but
undisputable fact that our ability to analyze patterns boils down to our ability to synthesize
patterns.
Inference based on simulation is of a fundamentally random nature. Are random algorithms really
necessary? It should be remembered that these probability laws expresses real, for example bio-
logical, variability; it is not a mathematical trick designed to achieve certain goals. Granted this, it
is clear that the output of an inference should be random representing the uncertainty that actually
exists in nature. It is only when information is available to such an extent that practically certain
inferences can be made that the randomness can be neglected. If so, deterministic approximation
can be exploited to increase computational efﬁciency.
Our random sampling and inference will be constructed from Markov processes character-
ized through their generator, or backward Kolmogoroff operator.
18.1 Markov Jump Processes
We will construct Markov processes for inference; there will be two kinds, jumping processes,
which can make large moves in state space, and diffusions, which make small moves.
Deﬁnition 18.1
Let {Xt, t ∈T ⊆R} be an X-valued (state space X) Markov process.
Deﬁne the transition law of the process
P(t, x, t + h, dy) = Pr(ω : X(t + h, ω) ∈dy|X(t) = x),
(18.1)
with the Chapman-Kolmogorov equations
P(t, x, t + h, dy) =

P(t, x, t + r, dz)P(t + r, z, t + h, dy)
t ≤r ≤t + h, h ≥0.
(18.2)
If the process is in turn homogeneous, then P(t, x, t + h, dy) = Ph(x, dy).
514

18.1 MARKOV JUMP PROCESSES
515
18.1.1 Jump Processes
We shall now focus on a homogeneous Markov process {X(t), t ≥0} on state space X with the
transition law Pt(x, ·) (appropriate references include Gikhman and Skorohod [462]).
Deﬁnition 18.2
We shall say X(t) is an X-valued homogeneous Markov jump process
with bounded jump measures q(x, ·) : X →[0, ∞) if X(t) is continuous in probability for all
t, i.e. for all measurable A ∈X,
lim
s↓t P(t, x, s, A) = lim
ǫ↓0 Pǫ(x, A) = δx(A),
(18.3)
with bounded jump measures
lim
ǫ↓0
Pǫ(x, A) −δx(A)
ǫ
= q(x, A)
uniformly in x, A.
(18.4)
The non-homogeneous setting has qt(x, A) = limǫ↓0((P(t, x, t+ǫ, A)−δx(A))/ǫ) with the uni-
formity in convergence as a function of t as well. Unlike diffusions (studied next), jump processes
wander in ǫ time to neighborhoods far from the initial state with probability that goes as ǫ:
Pǫ(x, A) = δx(A) + q(x, A\{x})ǫ + o(ǫ).
The jump processes of greatest concern herein are conservative, i.e. the sum of the intensity of
jumping out of a state and the intensity of staying in that state is zero:
q(x, {x}) = lim
ǫ→0
1
ǫ (Pt(x, {x}) −1)
= lim
ǫ→0

−Pt(x, X\{x}))
ǫ
= −q(x, X\{x}

.
It is then convenient to deﬁne the special notation q(x) = q(x, X\{x}) for the rate of jumping out of
state x. It is useful to interpret the jump process through its subordinated Markov chain jumping
on random times. Deﬁne the probability measure Q(x, ·) to satisfy
q(x)Q(x, dy) = q(x, dy),
(18.5)
with

X \{x} Q(x, dy) = 1 following from the fact that
q(x) = q(x, X\x)
(18.6)
= q(x)

X \x
Q(x, dy).
(18.7)
Example 18.3 (Jump measure for homogeneous Poisson counting process) Let{Nt, t ≥0},
on state space X = N = {0, 1, 2, . . . }, be a homogeneous Poisson counting process with
intensity λ. Deﬁne
Pr{Nt+s = n + 1|Nt = n} = λs + o(s)
Pr{Nt+s ∈A ⊂X|Nt = n} = o(s),
A ∩{n, n + 1} = ∅.
Then the jump measures are q(n, n + 1) = λ, and
q(n, n) = lim
s↓0
1 −P(Nt+s = n + 1|Nt = n) −1 + o(s)
s
= −λ ,
q(n) = q(n, X \ n)
= lim
s↓0
P(Nt+s = n + 1|Nt = n) + o(s)
s
= λ.

516
18 MARKOV PROCESSES AND RANDOM SAMPLING
Example 18.4 (Inhomogeneous PCP)
Let {Nt, t ≥0} on state space X
= N =
{0, 1, 2, . . . } be an inhomogeneous Poisson counting process {Nt, t ≥0} with intensity
λt, t ≥0. Deﬁne
Pr{Nt+s = n + 1|Nt = n} = λts + o(s)
Pr{Nt+s ∈A ⊂X|Nt = n} = o(s),
A ∩{n, n + 1} = ∅.
Then the jump measures are qt(n, n + 1) = λt, qt(n, n) = −λt, qt(n) = qt(n, X \ n) = λt.
Example 18.5 (Birth–Death Processes)
Let X = N = {0, 1, 2, . . . }, and construct a
birth–death process {Nt, t ≥0} with birth and death intensities λb(n), λd(n). Deﬁne
Pr{Nt+s = n + 1|Nt = n} = λb(n)s + o(s),
Pr{Nt+s = n −1|Nt = n} = λd(n)s + o(s),
Pr{Nt+s ∈A ⊂X|Nt = n} = o(s)
A ∩{n −1, n, n + 1} = ∅.
Then the jump measures are q(n, n + 1) = λb(n), q(n, n −1) = λd(n), and
q(n, n) = lim
s↓0
1 −P(Nt+s = n + 1 ∪Nt+s = n −1|Nt = n) −1 + o(s)
s
= −

λb(n) + λd(n)

,
q(n) = q(n, X \ n)
= lim
s↓0
P(Nt+s = n + 1|Nt = n) + o(s)
s
= λb(n) + λd(n).
18.2 Random Sampling and Stochastic Inference
The transformations of inference are guided via the posterior in the sense that the algorithm for
changing the regular structures will correspond to the sample path of a Markov process. The
dynamics determining the transition measure of the random Markov process of changes will be
constructed to push towards the posterior distribution in which the information about the patterns
are stored. To illustrate, let us examine the ﬁnite state, discrete-time Markov chain case which is
more familiar. Suppose we have some probability {π(x), x ∈X} on ﬁnite state space X, and the goal
is to draw samples from π(·). Examine the following time honored approach. Create an aperiodic
Markov chain with stationary transition matrix Q(i, j), i, j ∈X (
j∈X Q(i, j) = 1) making the state
space X strongly connected, i.e. ∃n such that for all states i, j ∈X the transition matrix is strictly
positive: Qn > 0. Then the Perron–Frobenius theorem is in effect, implying Q(·, ·) has a unique left
eigenvector with eigenvalue 1. Call this eigenvector p(·). Of course, Qn = p1 + O(αn), with α < 1;
1 is the all 1’s row vector.
Now if we can choose the transition dynamics Q(·, ·) of the Markov chain properly so that
p(·) = π(·) the distribution to be sampled from, then we have a process whose transition law
converges exponentially quickly to π. It might be expected that averages generated by sampling
such a process will converge to their expectation, i.e. some sort of ergodic property should hold.
This will generally be the case. So then there are really only two pieces which are needed for the
inference to proceed: (i) to choose the Markov dynamics so that π(·) on X is invariant (stationary)
for the Markov process, and (ii) to show the dynamics makes π(·) on X the unique invariant
measure.

18.2 RANDOM SAMPLING AND STOCHASTIC INFERENCE
517
Since we are interested in parameter spaces that have connected components, gradients are
natural for searching within these components. Diffusions and stochastic differential equations in
the continuous-time Markov process formulation play a fundamental role. In such a setting, the
generator or backward Kolmogoroff operator provides the correct tool for verifying the invariant mea-
sure condition, and Harris recurrence and irreducibility provides the correct framework for proving
uniqueness of the invariant measure.
18.2.1 Stationary or Invariant Measures
This section follows [463] and deﬁnes stationarity and the generator.
Deﬁnition 18.6
For a stochastic process {X(t), t ≥0} having values in the state space X
and the transition probabilities Pt(x, dy), then π(dx) is invariant (stationary) if for all t > 0,

π(dx)Pt(x, dy) = π(dy).
Let X(t) be a Markov process with generator the linear operator A deﬁned by
Af(·)
sup–norm
=
lim
ǫ↓0

X Pǫ(·, dy)f(y) −f(·)
ǫ
,
f(·) ∈D,
(18.8)
D ⊂B the domain on which sup–norm convergence holds, B the supporting Banach space.
Theorem 18.7
Let X(t) be a Markov process with generator the linear operator A of
Eqn. 18.8.
Then π(·) is invariant if and only if for all f ∈D the domain of A,

Af(x)π(dx) = 0.
Proof
We ﬁrst consider the “only if” part. By the deﬁnition of invariance, since π(dx) is
invariant,
for
all t ≥0,

π(dx)Pt(x, dy) = π(dy).
Deﬁning Htf(·) =

X Pt(·, dy)f(y), gives
0 =
 
π(dx)Pt(x, dy)f(y) −

π(dy)f(y)
=

π(dx)Htf(x) −

π(dy)f(y)
(18.9)
=

π(dx)

Htf(x) −f(x)

.
This is true for all t ≥0, implying that for all f ∈D,
0 = lim
ǫ↓0
1
ǫ

π(dx)

Hǫf(x) −f(x)

=

π(dx)Af(x),
(18.10)

518
18 MARKOV PROCESSES AND RANDOM SAMPLING
where the sup-norm convergence (Hcf −f)/ǫ →Af is used in bringing the limit inside
the integral.
For the “if part” For this we follow [463]. Assume for all f ∈D,

Af(x)π(x)dx = 0.
Now it is the case (see Lemma 18.8 below) that for t > 0,
 t
0 Hsfds ∈D and
Htf −f = A
 t
0
Hsf ds,
∀f ∈B.
(18.11)
Denoting ft =
 t
0 Hsfds ∈D, then

Aft(x)π(dx) = 0,
(18.12)
since π is invariant. This implies

π(x)

Htf(x) −f(x)

= 0,
∀f ∈B.
(18.13)
Thus πHt = π, and we have established the sufﬁciency of the generator condition.
To complete the proof, we need the following lemma.
Lemma 18.8
If t > 0, then
 t
0 Hsf ds ∈D, ∀f ∈B and
Htf −f = A
 t
0
Hsf ds ,
∀f ∈B.
(18.14)
Proof of lemma:
1
h[Hh −I]
 t
0
Hsf ds = 1
h
 t
0
[Hs+hf −Hsf] ds
= 1
h
 t+h
h
Hsf ds −1
h
 t
0
Hsf ds
= 1
h
 t+h
h
Hsf ds −1
h
 h
0
Hsf ds,
(18.15)
whichbythefundamentaltheoremofcalculusconvergesas h ↓0toHtf−f.Apparently

Hsf ∈D and the Theorem proof is completed.
□
Example 18.9
Banach Spaces and Domains The generator A is a linear operator that
is not necessarily deﬁned on all of B, only a subspace D ⊆B. Examples of Banach
spaces and domains are as follows. For the jump processes studied the Banach space
B will be the bounded measurable functions, with the domain of the jump operator
the same, D = B. For Feller diffusions on X a compact state space, (e.g. a closed and
bounded subset of Rk for example), B = ¯C(X) bounded continuous functions, and the
domain is D = ¯C∞(X), the bounded, inﬁnitely differentiable functions. For diffusions
in Rk then B = ˆC(Rk), continuous functions vanishing at inﬁnity with D = ˆC∞(Rk)
the subset that are inﬁnitely differentiable.

18.2 RANDOM SAMPLING AND STOCHASTIC INFERENCE
519
18.2.2 Generator for Markov Jump Processes
Suppose we have a homogeneous Markov jump process {X(t), t ≥0} on state space X with the
transition law Pt(x, ·). The generator characterizes the process, and deﬁnes a necessary condition
for the distribution to be stationary.
Theorem 18.10
Consider the generator (or Kolmogoroff Backward Operator) for the jump
process with jump measures q(x, ·) : X →[0, ∞) deﬁned by q(x, A) = limǫ↓0((Pǫ(x, A) −
δx(A))/ǫ) (uniformly in x), A ⊂X, with bounded continuous intensity q(x) = q(x, X \ x),
q = supx∈X q(x) < ∞, and probabilities Q(x, ·) such that q(x)Q(x, dy) = q(x, dy), then
Af(x) = −q(x)f(x) +

X
q(x)Q(x, dy)f(y).
(18.16)
In particular, the transition law Pt(x, ·) of the jump process satisﬁes the differential equation
∂Pt
∂t (x, ·) = −q(x)Pt(x, ·) +

X
q(x)Q(x, dy)Pt(y, ·),
P0(x, ·) = δx(·).
(18.17)
Proof
Af(x) = lim
ǫ↓0

X Pǫ(x, dy)f(y) −f(x)
t
= lim
ǫ↓0

X (Pǫ(x, dy) −δx(dy) + δx(dy))f(y) −f(x)
ǫ
= lim
ǫ↓0

X (Pǫ(x, dy) −δx(dy))f(y) + f(x) −f(x)
ǫ
=

X
q(x, dy)f(y)
using uniform continuity in x, A
= −q(x)f(x) +

X
q(x)Q(x, dy)f(y)
= q(x)

X
Q(x, dy)(f(y) −f(x)).
The domain of the generator A is the Banach space of bounded-measurable
functions, since A is a bounded operator, implying that the transition law itself is in the
domain and satisﬁes the differential equation. This follows since ∥

Pt(x, dy)f(y) ∥≤
∥f ∥< ∞implying that the Banach space supporting the semi-group consists of
bounded measurable functions. Since q = supx∈X q(x) < ∞, then A is a bounded
operator, and for f ∈B a bounded measurable function, then Af ∈B since
∥Af ∥=∥q(x)

X
Q(x, dy)(f(y) −f(x)) ∥
≤∥q ∥∥f(y) −f(x) ∥≤K ∥f ∥,
implying Pt(x, ·) ∈D, giving the differential equation 18.17.
Example 18.11
Poisson process For a Poisson process with intensity λ, Theorem
18.10 dictates
∂Pt(n, n)
∂t
= −λPt(n, n) +

j̸=n
q(n, j)Pt(j, n)
= −λPt(n, n)

520
18 MARKOV PROCESSES AND RANDOM SAMPLING
implying the exponential waiting times Pt(n, n) = e−λt. The transition probabilities
satisfy
∂Pt(n, m)
∂t
= −λPt(n, m) +

j
q(n, n + j)Pt(n + j, m)
(18.18)
= −λPt(n, m) + λPt(n + 1, m).
18.2.3 Jump Process Simulation
There is a beautiful relationship between Markov processes and Markov chains transitioning on
Poisson times. This essentially demonstrates that the invariant distribution condition for the jump
Markov process (Eqn. 18.19 below) is not very different than the invariant distribution condition
for a Markov chain from classical Monte-Carlo theory corresponding to the left eigenvector having
eigenvalue 1. For this we subordinate the Markov process to a Markov chain jumping on Poisson
times (e.g. see Cinlar [75]).
Theorem 18.12
Let X(t) be a Markov process with bounded jump intensity q(x), x ∈X,
q = sup
x∈X
q(x) < ∞, and jump probabilities Q(x, ·). Then π(·) is invariant for a jump process
with jump measures q(x, ·), Q(x, ·), and jump rates q(x) iff for all f ∈B (bounded measurable
functions),

X

−q(x)f(x) + q(x)

Q(x, dy)f(y)

π(dx) = 0.
(18.19)
Deﬁne the normalized transition probabilities ˜Q(x, ·),
˜Q(x, ·) =

1 −q(x)
q

δx(·) + q(x)
q Q(x, ·),

X
˜Q(x, dy) = 1.
(18.20)
The Markov process is statistically equivalent to a Poisson process jumping on exponen-
tial times with intensity q, and with transition probability ˜Q. In particular, π(·) is the left
eigenvector of ˜Q(·, dy) having eigenvalue 1:
π(dx) =

π(dy) ˜Q(y, dx).
Proof
The invariant distribution condition 18.19 follows from Theorem 18.7 with

Afπ = 0. Using Theorem 18.10, Pt(x, ·) is in the domain of the generator so that
∂
∂tPt(x, ·) = lim
ǫ↓0
1
ǫ

X
Pǫ(x, dy)Pt(y, ·) −Pt(x, ·)

(18.21)
= APt(x, ·),
P0(x, ·) = δx(·).
(18.22)
Rewriting the generator A according to
Af(x) =

q(x)Q(x, dy)(f(y) −f(x))
= q

(f(y) −f(x)) ˜Q(x, dy)
= −qf(x) + q

˜Q(x, dy)f(y),
(18.23)

18.2 RANDOM SAMPLING AND STOCHASTIC INFERENCE
521
implies

X
Pt(x, dy)f(y) = etAf(x) = e−qtI+qt ˜Qf(x)
=
∞

k=0
e−qt (qt)k
k!

X
˜Qk(x, dy)f(y).
(18.24)
The transition law corresponds to a Markov chain jumping on exponential times at
rate q. The invariant distribution condition

Af(x)π(dx) = 0 gives

Af(x)π(dx) =
 
−qf(x) + q

˜Q(x, dy)f(y)

π(dx)
(18.25)
=

−f(x)qπ(dx) +

f(y)

q ˜Q(x, dy)π(dx)
(18.26)
changing variables of integration in the second term
= −

f(x)qπ(dx) +

f(x)

q ˜Q(y, dx)π(dy)
(18.27)
=

f(x)

−qπ(dx) +

q ˜Q(y, dx)π(dy)

= 0.
(18.28)
Since this is true for any bounded measurable function f(x),
π(dx) =

π(dy) ˜Q(y, dx),
implying that π(·) is the left eigenvector (with eigenvalue 1) of ˜Q(·, dy).
18.2.4 Metropolis–Hastings Algorithm
We now follow heavily the work of Lanterman [464] and examine the general framework for
inference based on the acceptance and rejection of proposals. The principal contribution will be
to choose a jumping scheme analogous to the Metropolis-Hastings sampling algorithm [465–467].
At exponentially distributed times, with a ﬁxed mean, a candidate xprop is drawn from a proposal
density r(xold, xprop); the acceptance probability is then computed:
α(xold, xprop) = min
C
π(xprop)r(xprop, xold)
π(xold)r(xold, xprop) , 1
F
.
(18.29)
The proposal is accepted with probability α(xold, xprop) and rejected with probability
1 −α(xold, xprop). A wide variety of proposal densities may be used. We will need a characteriza-
tion of the space of conﬁgurations that jump transitions can connect to, so that r(xold, xprop) > 0
for xprop ∈J 1(xold) and r(xold, xprop) = 0 for xprop ̸∈J 1(xold).
Deﬁnition 18.13
A process {Xt, t ∈T} is said to be weakly reversible if J (x) = J −1(x)
where J (x) is the set of states that can be reached given that the process started in state x and
J −1(x) is the set of states that can jump into state x:
J (x) = {y : y ∈support of Q(x, ·)}
(18.30)
J −1(x) = {y : x ∈support of Q(y, ·)}.
(18.31)

522
18 MARKOV PROCESSES AND RANDOM SAMPLING
Corollary 18.14
Assume r(·, ·) is chosen so the process is weakly reversible, i.e. J (x) =
J −1(x). The Metropolis-Hastings process for constructing the jump intensities becomes
q(x, dy) = min
π(y)r(y, x)
π(x)r(x, y), 1

r(x, y)dy,
(18.32)
and the detailed balance condition of Eqn. 18.19 for the jump process is satisﬁed.
Proof
To see that (18.32) satisﬁes Eqn. 18.19 of Theorem 18.12. We must satisfy the
detailed balance condition q(x)π(x)dx =

x q(y, dx)π(y)dy. First deﬁne
> =

y ∈J 1(x) : π(y)r(y, x)
π(x)r(x, y) > 1

(18.33)
< =

y ∈J 1(x) : π(y)r(y, x)
π(x)r(x, y) < 1

.
(18.34)
Substituting the deﬁnition of the jump intensity into the left-hand side (LHS) of the
detailed balance condition above gives
LHS =

J 1(x)
min
π(y)r(y, x)
π(x)r(x, y), 1

r(x, y)dy

π(x)dx
(18.35)
=

>
r(x, y)dy

π(x)dx +

<
π(y)r(y, x)
π(x)r(x, y)r(x, y)dy

π(x)dx.
(18.36)
=

>
r(x, y)dy

π(x)dx +

<
π(y)r(y, x)dy

dx.
(18.37)
Now deﬁne
−1
> =

y ∈J −1(x) : π(x)r(x, y)
π(y)r(y, x) > 1

(18.38)
−1
< =

y ∈J −1(x) : π(x)r(x, y)
π(y)r(y, x) < 1

.
(18.39)
Substituting 18.32, the choice for the jump intensity into the right-hand side (RHS) of
the detailed balance condition above yields
RHS =

J −1(x)

min
π(x)r(x, y)
π(y)r(y, x), 1

r(y, x)dx

π(y)dy.
(18.40)
=

−1
>

r(y, x)dx
	
π(y)dy +

−1
<
π(x)r(x, y)
π(y)r(y, x)r(y, x)dx

π(y)dy.
(18.41)
=

−1
>

r(y, x)dx
	
π(y)dy +

−1
<

π(x)r(x, y)dx
	
dy.
(18.42)
The weak reversibility of the jump moves J 1(x) = J −1(x) implies that −1
> =
< and −1
<
= >, which establishes the equality of the LHS and the RHS, giving
detailed balance.
Examples of the general Metropolis–Hastings jump-diffusion scheme described
in Section 18.2.4 have appeared throughout the literature.

18.2 RANDOM SAMPLING AND STOCHASTIC INFERENCE
523
18.2.4.1 Metropolis and Heat-Bath
Corollary 18.15 (Metropolis)
In the above, if r(xold, xprop) = r(rprop, rold), we have
α(xold, xprop) = min(π(rprop)/π(rold), 1), which corresponds to the traditional Metropolis
algorithm [468].
With π(x) = (e−E(x)/Z), x ∈X, then
q(x, dy) = e−(E(y)−E(x))+ dy, q(x) =

X \x
e−(E(y)−E(x))+ dy,
Q(x, dy) =
e−(E(y)−E(x))+ dy

X \x e−(E(y)−E(x))+ dy
with (x)+ deﬁned to be zero if x is negative or x otherwise, and π(·) is an invariant density.
Notice that if we had the means to sample directly from the conditional dis-
tributions, then the Metropolis–Hastings sampler would reduce to the Gibbs sampler or
Heat Bath.
Corollary 18.16 (Heat-Bath)
If r(x, y)dy = π(y)dy, then π(·) is an invariant density.
In the Bayesian framework, if an informative prior exists that is easy to sample from, then
it is beneﬁcial to choose xprop ∈T 1(xold), drawing candidates from the prior and accepting and
rejecting based on the likelihood. This is the approach of Corollary 2 of Theorems 1 and 2 of [137].
This approach is effective in the ground-to-air tracking study reviewed in a subsequent chapter
since drawing from a prior on motion dynamics is fast and effective.
18.3 Diffusion Processes for Simulation
For connected state spaces, we shall use diffusions for simulation.
Deﬁnition 18.17
A real-valued random process {X(t), t ∈T},T ⊆R is a diffusion
process if (i) X(t) is Markov, and (ii) X(t) is sample path continuous.
Dynkin provided a characterization of diffusions, that if X(t) is a real-valued random process
with the properties of
(i) right continuity ∀s ∈T : limt↓s X(t) = X(s);
(ii) ∀s ∈T : limt↑s X(t) exists; and
(iii) ∀t ∈T : ∀δ > 0 : P({ω : |X(t + h, ω) −X(t, ω)| > δ}|X(t) = x) = o(h). A sufﬁcient
condition for establishing a diffusion follows from the Dynkin property that if ∃p ≥3 :
limh↓0(1/h)E{|X(t + h) −X(t)|p|X(t) = x} = 0, then the process is a diffusion. This results
from the Markov inequality.
APoisson process, for example, fails to meet such conditions and it is not a diffusion process.
This is obviously true, since a Poisson process fails to be sample-path continuous.
The principal way in which we shall study and characterize diffusions is via their
inﬁnitesimal properties, particularly means and variances.
Deﬁnition 18.18
Let
X(t)
be
a
diffusion
process.
For
every
x
and
∀ǫ > 0,

|x−y| > ǫ P(t, x, s, dy) = o(s −t), and there exists functions a(t, x) and b(t, x) such that for
every x,
inﬁnitesimal mean a(x, t) = lim
h↓0
1
hE{X(t + h) −X(t) |X(t) = x}
(18.43)
inﬁnitesimal variance b(x, t) = lim
h↓0
1
hE{|X(t + h) −X(t)|2 |X(t) = x}.
(18.44)

524
18 MARKOV PROCESSES AND RANDOM SAMPLING
Example 18.19 (Brownian motion and the Dynkin condition)
Brownian
motion
satisﬁes the Dynkin condition with p = 4. To see this, condition on W(t) = x, then
the random variable W(t + h) is normally distributed with mean x and variance h,
implying
lim
h↓0
1
hE{|W(t + h) −W(t)|4|W(t) = x} = lim
h↓0
3h2
h
= 0 .
Example 18.20
(Inﬁnitesimal means and variances of Brownian motion and
diffusions) For a standard Wiener process, a(x, t) = 0 and b(x, t) = 1. To see this,
note E{W(t + h) −W(t) |W(t) = x} = 0, so the inﬁnitesimal mean is a(x, t) = 0. The
variance of an increment becomes E{|W(t + h) −W(t)|2 |W(t) = x} = h, implying
b(x, t) = 1.
Let X(t) be deﬁned by the equation
X(t) = Xa +
 t
a
m(X(s), s)ds +
 t
a
σ(X(s), s)dWs,
where W(t) is a Brownian motion process with smoothness properties on the coefﬁ-
cients m(·), σ(·). Then {X(t), t ∈T} is a diffusion (e.g. see [463]), with the inﬁnitesimal
mean and variance calculated as follows. Taking conditional expectation, the second
integral goes away because increments of X and W are independent and intervals of
Brownian motion have zero expected value. The ﬁrst integral is simpliﬁed using the
Lipschitz property. From the Lipschitz condition, |m(X(s), s) −m(x, s)| ≤K|X(s) −x|,
implying

 t+h
t
m(X(s), s)ds −
 t+h
t
m(x, t)ds
 ≤
 t+h
t
|m(X(s), s) −m(x, s)|dx
(18.45)
≤K
 t+h
t
|X(s) −x|dx
(18.46)
≤Ksups∈[t,t+h]|X(s) −x|h.
(18.47)
By a.s. continuity of the sample paths, sups∈[t,t+h]|X(s) −x| →0 as h →0, giving
a(x, t) = lim
h↓0
E{X(t + h) −X(t) |X(t) = x}
h
(18.48)
= lim
h↓0
E{
 t+h
t
m(X(s), s)ds}
h
= m(x, t).
For the inﬁnitesimal variance, ﬁrst compute:
(X(t + h) −X(t))2 =
 t+h
t
m(X(s), s)ds
2
+ 2
 t+h
t
σ(X(s), s)dW(s)
×
 t+h
t
m(X(s), s)ds +
 t+h
t
σ(X(s), s)dW(s)
2
.

18.3 DIFFUSION PROCESSES FOR SIMULATION
525
Taking conditional expectation, the middle term becomes zero obtaining
E{|X(t + h) −X(t)|2 |X(t) = x}
=
 t+h
t
m(x, s)ds
2
+ σ 2(x, t)E



 t+h
t
dW(s)
2


The last term above is derived by the independence of X(t) and W(t) at any
particular value of t. Dividing by h and taking limits as h →0+, the ﬁrst term goes to
zero, and properties of Brownian motion give the inﬁnitesimal variance,
b(x, t) = σ 2(x, t).
Example 18.21 (Langevin stochastic differential equation)
Langevin SDEs of the
form
dX(t) = −dE(X(t))
dx
dt + σdW(t),
are important with dE(x)/dx satisfying Lipschitz smoothness conditions of (see [213,
463] for examples). The inﬁnitesimal mean and variance just follow from above:
a(x, t) = −dE(x)
dx
,
b(x, t) = σ 2.
18.3.1 Generators of 1D Diffusions
Examine the diffusion {Xt, t ≥0} in R1. As discussed in [463], the appropriate setting for diffusions
is the Banach space of continuous functions vanishing at inﬁnity, B = ˆC(R), with the domain of
differentiable functions D = ˆC∞(R).
Theorem 18.22
For a real-valued diffusion on the real line T = R, with inﬁnitesimal means
and variances a(x), b(x) the generator has domain D = ˆC∞(R) ⊂B = ˆC(R). The generator
becomes, for all f ∈ˆC∞(R),
Af(x) = a(x)f′(x) + 1
2b(x)f′′(x),
With π(·) invariant iff for all f ∈D = ˆC∞,

X

a(x)f′(x) + 1
2b(x)f′′(x)

π(dx) = 0.
(18.49)
In
Rn,
D = ˆC∞(Rn)
with
inﬁnitesimal
means
and
variances
ai(x), bij(x),
i, j = 1, . . . , n and the generator given by
Af(x) =
n

i=1
ai(x) ∂
∂xi
f(x) + bij(x)
∂2
∂2xixj
f(x),
f ∈ˆC∞(Rn).
(18.50)
Proof
Since f ∈ˆC∞(R) vanishes at ∞, use the differentiability to express it in a
Taylor series around x:
f(y) = f(x) + f′(x)(y −x) + f′′(x)(y −x)2
2
+ (f′′(c) −f′′(x))(y −x)2
2
,

526
18 MARKOV PROCESSES AND RANDOM SAMPLING
for some c = αx + (1 −α)y, α ∈[0, 1]. (See Problem A.15 for further illustration).
Therefore, for all ǫ > 0,
Af(x) = lim
t→0
1
t

Pt(x, dy)(f(y) −f(x))
= lim
t→0
1
t

|y−x|≤ǫ
Pt(x, dy)(f(y) −f(x)) + o(t)

,
f is bounded, continuous
= lim
t→0
1
t

|y−x|≤ǫ
Pt(x, dy)

(y −x)f′(x) + 1
2 (y −x)2f′′(x)(1 + αǫ)

+ o(t)

= lim
t→0
1
t

f′(x)

|y−x|≤ǫ
(y −x)Pt(x, dy) + 1
2 f′′(x)(1 + αǫ)
× lim
t→0
1
t

|y−x|≤ǫ
(y −x)2Pt(x, dy) + 1
t o(t)

,
= a(x)f′(x) + 1
2 b(x)f′′(x),
where αǫ = sup|y−x|≤ǫ |f′′(y) −f′′(x)|, and since f ∈ˆC2, limǫ→0 αǫ = 0.
Example 18.23 (Diffusions and the Wiener process)
For a diffusion process, the
PDE from Theorem 18.22 is in terms of the inﬁnitesimal means and variances:
∂Pt
∂t (x, dz) = a(x)∂Pt
∂x (x, dz) + 1
2b(x)∂2Pt
∂2x (x, dz),
(18.51)
with P0(x, dz) = δx(dz).
For the Wiener process, the transition probability to the set s ⊂R is
Pt(x, S) =

S
1
√
2πt
e−(y−x)2/2tdy,
which as a function of initial state x is twice differentiable and vanishes at −∞, ∞and
is therefore in the domain D of the operator A = 1/2f′′(x).
Let us show that the transition law for a standard Wiener process satisﬁes the
above PDE Eqn. 18.51. For the standard Wiener process, a(x) = 0, b(x) = 1, and the
generator is APt(x, dz) = (1/2)(∂2/∂x2)Pt(x, dz) with the transition law for a standard
Wiener process being Pt(x, dy) =

1
√
2πt

e−(y−x)2/2t dy. Therefore, the derivatives
become
∂
∂tPt(x, dy) = −1
2
1
(2πt)3/2 2πe−(y−x)2/2t dy
+
1
√(2πt)e−(y−x)2/2t dy −(y −x)2
2
−1
t2

= e−(y−x)2/2t dy

−1
2t(2πt)1/2 +
(y −x)2
2t2(2πt)1/2

,
(18.52)
∂
∂xPt(x, dy) = (y −x)
t
1
√
2πt
e−(y−x)2/2t dy,
∂
∂x2 Pt(x, dy) = −1
t
1
√
2πt
e−(y−x)2/2t dy + (y −x)2
t2
e−(y−x)2/2t dy
= e−(y−x)2
2t
dy

−1
t(2πt)1/2 +
(y −x)2
t2(2πt)1/2

.
(18.53)
From Eqns. 18.52, 18.53 it follows that ∂
∂tPt(x, dz) = (1/2)(∂/∂2x)Pt(x, dz).

18.3 DIFFUSION PROCESSES FOR SIMULATION
527
Example 18.24
Deﬁne the S.D.E.
dX(t) = −dE(X(t))
dx
dt + σdW(t),
(18.54)
with the drift dE/dx satisfying the Lipschitz condition. This is a Markov process with
a(x) = dE(x)/dx, b(x) = σ 2 and with generator Af = −(dE(x)/dx)f′(x) + σ 2f′′(x)/2.
18.3.2 Diffusions and SDEs for Sampling
We now examine sampling in the continuum using diffusions. Assume the state-space is the
Euclidean space Rn.
Theorem 18.25
Let π(x) = e−E(x)/Z , x ∈Rn. If X(t) is a diffusion with state space
X = Rn with the properties that the diffusion X(t) on Rn satisﬁes the stochastic differential
equation
dX(t) = −1
2∇E(X(t))dt + dW(t)
(18.55)
with X(t), ∇E and W ∈Rn denoting the state, gradient and standard vector Brownian motion,
respectively, with the gradient ∇E satisfying Lipschitz continuity, then π(·) is an invariant
density.
Proof
Applying A to such f and integrating with respect to µ gives

X
Af(x)µ(dx) = 1
2

Rn

−
n

i=1
∂E(x)
∂xi
∂f(x)
∂xi
+
n

i=1
∂2f(x)
∂x2
i
e−E(x)
Z
dx
(a)
=

−1
2

Rn
n

i=1
∂E(x)
∂xi
∂f(x)
∂xi
+ 1
2

Rn
n

i=1
∂E(x)
∂xi
∂f(x)
∂xi
e−E(x)
Z
dx
(18.56)
+
n

i=1
e−E(x)
Z
∂f(x)
∂xi
|∞
−∞= 0,
(18.57)
where (a) follows from integration by parts of the second derivative term and using
the boundary term, which is zero since the derivative on the boundary is zero.
Example 18.26
Consider a diffusion process which follows the following stochastic
differential equation,
dX(t) = −1
2
∂E(X(t))
∂X
dt + σ dW(t),
(18.58)
with dE/dx satisfying Lipschitz and measurability conditions (e.g. see [213,463]). The
solution is a Markov process, with the generator given by
Af(x) = −1
2
∂E(x)
∂x
f′(x) + 1
2σ 2f′′(x),

528
18 MARKOV PROCESSES AND RANDOM SAMPLING
where a(x) = (−1/2)(∂E(x)/∂x) and b(x) = σ 2. Now clearly
π(dx) = e−E(x)/σ 2
Z
dx
is invariant for the Langevin Eqn. 18.58. The inﬁnitesimal means and variances satisfy
the Lipschitz conditions, therefore the S.D.E. is a Markov process, hence Theorem 18.7
is in effect:

Af(x)π(dx) =

R
−1
2
∂E(x)
∂x
f′(x) + 1
2σ 2f′′(x)
e−E(x)/σ 2
Z
dx
=

R
−1
2
∂E(X(t))
∂x
f′(x)π(dx) +

R
1
2f′(x)e−E(x)/σ 2
Z
∂E(x)
∂X
dx
+ 1
2f′(x)e−E(x)/σ 2
Z
|∞
−∞
=0.
Notice that f ∈ˆC(R) vanishing at inﬁnity is used in the boundary condition.
Example 18.27
The Gaussian distribution π(dx) = (1/
√
2πσ 2)e−x2/2σ 2dx is station-
ary for the Langevin equation
dX(t) = −1
2X(t) dt + σ dW(t).
(18.59)
To see this, the inﬁnitesimal mean and variance of (18.59) is −(1/2)x and σ 2,
respectively. The generator of the diffusion becomes
Af(x) = −1
2xf′(x) + 1
2σ 2f′′(x).
To show that the Gaussian density is invariant, examine f (with its derivative vanishing
at inﬁnity):
 ∞
−∞
Af(x)π(dx) =
 ∞
−∞
8
−1
2xf′(x) + 1
2σ 2f′′(x)
9
1
√
2πσ 2 e−x2/2σ 2 dx
=
 ∞
−∞
−xf′(x)
2
√
2πσ 2 e−x2/2σ 2 dx +
1
2
√
2πσ 2 f′(x)σ 2e−x2/2σ 2|∞
−∞
+
 ∞
−∞
−xf′(x)
2
√
2πσ 2 e−x2/2σ 2 dx
= 0.
18.4 Jump-Diffusion Inference on Countable Unions of Spaces
Let us now look at the situation where
X = ∪k X(k),
with prior probabilities pk attached to the subconﬁguration spaces X(k). The interpretation of this
could be, for example, that k represents the unknown number of objects in the scene or enumerates

18.4 JUMP-DIFFUSION INFERENCE ON COUNTABLE UNIONS OF SPACES
529
various pathological anatomies. We shall assume for simplicity that each subconﬁguration space
X(k) contains a conﬁguration of parameter x(k) ∈X(k). Having deﬁned the conﬁguration space
X = ∪kX(k) as the union of spaces over which the inference is to be performed, the crucial part
of the problem still remaining is the derivation of the inference algorithm for choosing the graphs
and their associated transformations. In other words, how can we carry out hypothesis formation? The
hypothesis space is a countable disconnected union of these connected parameter spaces, with the model order
(parametric dimension) a variable to be inferred as well. Since we take a Bayesian approach, a posterior
distribution is constructed sitting on this countable disconnected union of spaces. The parametric
representation of the target scene is selected to correspond to the conditional mean under this pos-
terior, requiring the generation of expectations over this complex parameter space. Conditional
expectations under the posterior are constructed via a Markov process with sample paths moving
through the parameter space, satisfying the ergodic property that the transition distribution of the
Markov process converges to the posterior distribution. This allows for the empirical generation
of conditional expectations under the posterior. To accommodate the connected and disconnected
nature of the state spaces, the Markov process is forced to satisfy jump-diffusion dynamics, i.e.
through the connected parts of the parameter space (Lie manifolds) the algorithm searches contin-
uously, with sample paths corresponding to solutions of standard diffusion equations; across the
disconnected parts of parameter space, the jump process determines the dynamics. The inﬁnitesi-
mal properties of these jump-diffusion processes are selected so that the Markov process is ergodic
in the sense that the transition distribution converges to the posterior.
It is the fundamental difference between diffusions (almost surely continuous sample paths)
and jump processes (which make large moves in parameter space in small time) that allows us to
explore the very different connected and non-connected nature of hypothesis space.
18.4.1 The Basic Problem
Scenes are assumed to contain a variable number of objects of one or several classes. The variability
within each class is described via a template with transformations which deform the template
into a large family. These transformations form Lie groups. The objects are described via their
associated transformations. Variable numbers of objects are described via ﬁnite products of the
transformations. The state space describing all possible objects becomes the disjoint union over
all ﬁnite powers of the transformation spaces. Inference in the Bayesian framework requires a
posterior distribution supported on this space.
The basic problem becomes as follows. Identify model k with parameter space X(k) a con-
nected space of dimension n(k) with prior on model k denoted Pk. Then given are a collection of
such spaces with the full hypothesis space X = ∪∞
k=0X(k), and posterior distribution µ supported
on X is of the Gibbs type:
µ(A) =
∞

k=0
Pk

A∩X (k)
e−Hk(x(k))
Zk
dx(k)
(18.60)
with normalizer Zk =

X (k) e−Hk(x(k)) dx(k). Given a measure µ on the countable union of spaces
X = ∪∞
k=0X(k), the crucial part of the problem is how shall we carry out hypothesis formation? We
choose to generate conditional expectations with respect to µ empirically, that is by generating a
sequence of samples X(t1), X(t2), . . . with the property that (1/n) 
n
i=1 f(X(ti)) →

X f(x)µ(dx).
This we do via the construction of a Markov process X(t) that satisﬁes jump-diffusion dynamics
through X in the sense that (i) on random exponential times the process jumps from one of the
countably inﬁnite set of spaces X(k), k = 0, 1, . . . to another, and (ii) between jumps it satisﬁes
diffusions of dimension appropriate for that space.

530
18 MARKOV PROCESSES AND RANDOM SAMPLING
The jump-diffusion is constructed from a set of independent and exponentially distributed
times w1, w2, . . . , with jump times t1, t2, . . . deﬁned according to ti = inf{t :
 t
ti−1 q(X(s))ds ≥wi},
and t0 = 0. The process X(t) within each space X(k) is a diffusion with inﬁnitesimal drifts a(x(k)) ∈
Rn(k) and inﬁnitesimal variance matrix B(x(k)) (an n(k) × n(k) matrix). The process moves from
one subspace to another on the ti’s with transition measures q(x, dy) deﬁned as above
q(x, dy) = lim
ǫ→0
1
ǫ

Pr{X(t + ǫ) ∈dy|X(t) = x} −δx(dy)

,
(18.61)
with q(x)
=

X \x q(x, dy), and transition probability measures Q(x, dy)
=
q(x, dy)/q(x),

X Q(x, dy) = 1.
For the technical details of the construction of the jump diffusion we rely on adaptation of
results from Ethier and Kurtz [463] for characterizing our diffusion processes in the countably
inﬁnite number of subspaces. First, we must establish that the addition of the jump and diffusion
processes results in a well-deﬁned Markov process. Let ˆC(X) be the Banach space of continuous
functions on X vanishing at inﬁnity and C∞
c (X) be the space of differentiable functions, compactly
supported on X.
Theorem 18.28
Let µ(dx)
=

Pk
k 1X (k)(x) e−Hk(x)
Zk
dx with the normalizer Zk
=

X (k) e−Hk(x) dx.
IF the jump diffusion process X(t) with state space X = ∪∞
k=0Rn(k) has the properties
that
(i) thediffusionX(t)withinanysubspaceRn(k) satisﬁesthestochasticdifferentialequation
dX(t) = −1
2∇Hk(X(t))dt + dWn(k)(t)
(18.62)
where X(t), ∇Hk and Wn(k) ∈Rn(k) are the state, gradient and standard vector
Brownian motion, respectively, with the gradient ∇Hk satisfying Lipschitz continuity,
and
(ii) the jump and transition probability measures q(x, dy), q(x), Q(x, dy) are bounded
continuous functions, with the jumps local (ﬁnite support) satisfying
q(x)µ(dx) =

X
q(y)Q(y, dx)µ(dy),
(18.63)
then the jump-diffusion process X(t) is a Markov process on X with unique invariant
measure µ = 
k pkµk.
Notice, if the Euclidean spaces are connected under the jumps, i.e. for all k, k′,
/bin/bash: w: command not found ﬁnite sequence of jumps carrying the process from
Rn(k) to Rn(k′), then this implies a recurrence asssuring that µ is the only invariant
measure.
Proof
The ﬁrst part of the proof of Theorem 18.28 relies on the fact that the generator
or backward Kolmogoroff operator A for the jump-diffusion process characterizes the
stationary measure. That is, µ is stationary for X(t) if and only if

Af(x)µ(dx) = 0
for all f in the core of A (p. 239 and the Echeverria theorem, p. 248 of [463]). Now
the generator is the superposition of the diffusion and jump generators A = Ad + Aj
(diffusion+jump). This follows from Ethier and Kurtz (p. 266, [463]). The core C2c(X)
is the set of functions f(x) = 
m
k=0 1Rn(k)(x)fk(x) , m ≥0, where fk(x) ∈C2c(Rn(k)) are

18.4 JUMP-DIFFUSION INFERENCE ON COUNTABLE UNIONS OF SPACES
531
twice continuously differentiable, compactly supported functions on Rn(k). Applying
A to such f and integrating with respect to µ gives

X
Af(x)µ(dx) =1
2
m

k=0

Rn(k)

−
n(k)

i=1
∂Hk(x(k))
∂xi
∂fk(x(k))
∂xi
(18.64)
+
n(k)

i=1
∂2fk(x(k))
∂x2
i
e−Hk(x(k))
Z
dx(k)
+

X
µ(dx)q(x)
 
X

f(y) −f(x)

Q(x, dy)

,
(18.65)
where the ﬁrst part the standard S.D.E. operator and the second the jump operator. To
show

Af(x)µ(dx) = 0, integrate by parts once the second derivative term in the ﬁrst
part of Eqn. 18.65 and use the second condition (b) of Eqn. 18.63. That completes the
ﬁrst part of the theorem.
Remark: In [471] we show for all x ∈X the associated chain X(i),  > 0, i =
0, 1 . . . for each x, X(i) = x, converges in total variation norm to µ.

19 J U M P D I F F U S I O N I N F E R E N C E I N C O M P L E X
S C E N E S
ABSTRACT
This chapter explores random sampling algorithms introduced in for generating
conditional expectations in hypothesis spaces in which there is a mixture of discrete, discon-
nected subsets. Random samples are generated via the direct simulation of a Markov process
whose state moves through the hypothesis space with the ergodic property that the transi-
tion distribution of the Markov process converges to the posterior distribution. This allows for
the empirical generation of conditional expectations under the posterior. To accommodate
the connected and disconnected nature of the state spaces, the Markov process is forced to
satisfy jump–diffusion dynamics. Through the connected parts of the parameter space (Lie
manifolds) the algorithm searches continuously, with sample paths corresponding to solu-
tions of standard diffusion equations. Across the disconnected parts of parameter space the
jump process determines the dynamics. The inﬁnitesimal properties of these jump–diffusion
processes are selected so that various sample statistics converge to their expectation under
the posterior.
We now examine the generation of conditional mean estimates of parameters in the rigid body
tracking and recognition scenario. Except under particular sets of assumptions, the posterior
distribution will be highly nonlinear in the parameters of the hypothesis space thus precluding
the direct closed-form analytic generation of conditional mean estimates. Towards this end, we
have taken advantage of the explosion which that occurred over the past years in the statistics
community with the introduction of random sampling methods for the empirical generation of
estimates from complicated distributions. See for example the reviews [469,470]. We now explore
random sampling algorithms introduced in [137,303,471,472] for generating conditional expecta-
tions in hypothesis spaces in which there are a mixture of discrete, disconnected subsets. Random
samples are generated via the direct simulation of a Markov process whose state moves through
the hypothesis space with the ergodic property that the transition distribution of the Markov pro-
cess converges to the posterior distribution. This allows for the empirical generation of conditional
expectations under the posterior. To accommodate the connected and disconnected nature of the
state spaces, the Markov process is forced to satisfy jump–diffusion dynamics, i.e. through the con-
nected parts of the parameter space (Lie manifolds) the algorithm searches continuously, with
sample paths corresponding to solutions of standard diffusion equations; across the disconnected
parts of parameter space the jump process determines the dynamics. The inﬁnitesimal properties
of these jump–diffusion processes are selected so that various sample statistics converge to their
expectation under the posterior.
The original motivation for introducing jump–diffusions in [137,471] is to accommodate the
very different continuous and discrete components of the object discovery process. Given a scene
of multiple objects, the problem is to identify the orientation, translation and scale parameters
accommodating the variability manifest in the viewing of each object type. For this, the parameter
space is sampled using diffusion search, in which the state vector winds continuously through the
similarities following gradients of the posterior. The second distinct part of the sampling process
corresponds to the target type and number deduction, during which the target types are being dis-
covered, with some subset of the scene only partially “recognized” at any particular time during
the process. Ajump in hypothesis space corresponds to (i) selecting between different object types,
or (ii) hypothesizing a new object in the scene or a “change of mind” via the deletion of an object
in the scene. The jump intensities are governed by the posterior density, with the process visiting
conﬁgurations of higher probability for longer exponential times, and the diffusion equation gov-
erning the dynamics between jumps. It is the fundamental difference between diffusions (almost
surely continuous sample paths) and jump processes (making large moves in parameter space in
small time) that allows us to explore the very different connected and non-connected nature of
hypothesis space.
532

19.1 RECOGNITION OF GROUND VEHICLES
533
19.1 Recognition of Ground Vehicles
Remote sensing of multiple ground vehicles has been extensively studied using several sensor
models including forward looking intra-red (FLIR) and laser radar (LADAR) sensors.
19.1.1 CAD Models and the Parameter Space
To create the basic building blocks of the hypothesized scenes deﬁne the primitive generating
elements G made up of disjoint classes G = (
α∈A Gα α ∈A, the generator indices. The generators
studied throughout are airframes and land vehicles. To accommodate variability, apply transfor-
mations to the generators from the similarity groups S, s : G ↔G. These do not change generator
type, α(sg) = α(g). Herein, G are 2-dimensional (2D) manifolds in R3 (surfaces), and their trans-
formation is the matrix group action. Figure 19.1 shows the basic CAD model used to represent
the target classes in several of the experiments to follow. For ground-based, rigid targets, we use
translation in the plane and rotation around the z = x3-axis. Then the similarity group S : G ↔G,
consists of the set of rigid motions of the type s = (O, p) : x →O(φ)x + p, where
(O(φ), p) :


x1
x2
x3

→


cos φ
sin φ
0
−sin φ
cos φ
0
0
0
1




x1
x2
x3

+


p1
p2
0

.
(19.1)
A single ground vehicle in the scene is parameterized by its orientation and position and
its class, an element of SO(2) × R2 × A. Identifying the similarity transformation of axis-ﬁxed
rotation with its Euler representation in the torus T , and translation in the plane R2, then an
m-object parameter vector becomes
x(m) = (φ1, p1), (φ2, p2), · · · ∈X(m) =

T × R2m
.
(19.2)
M60
M2
T62
M60
(1)
(3)
(2)
(4)
(5)
(6)
Figure 19.1 Top row: Panels 1 and 2 show the CAD models used for the simulations. Panel 3 shows
the top-down and perspective view of the M60, M2, and T62 tank scene. Bottom row: Panels
4,5, and 6 show the radiant intensities on the CAD models generated via the PRISM simulation
package.

534
19 JUMP DIFFUSION INFERENCE IN COMPLEX SCENES
2D Lattice
Projection
3D Volume
Imaging Space
With Noise
a (k)
p (k)
φ (k)
(1)
(2)
Figure 19.2 Panel 1 shows a single track of n(1) rotations and translations; panel 2 displays the
projection system for optical imaging.
The scene of m-targets is depicted in panel 3 of Figure 19.1.
For the tracking and recognition of aircraft, the transformations are rotations and translations
acting on R3. For airframes, the full orthogonal group SO(3) is needed (3 × 3 matrices with
determinant 1) of the form (O, p) : x →Ox + p, with the Euler angle representation providing
natural local coordinates, parameterizing O(φ) ∈SO(3) via the three angle vector φ = (φ1, φ2, φ3)
of the form
O(φ1, φ2, φ3) :


x1
x2
x3

→


1
0
0
0
cos φ1
sin φ1
0
−sin φ1
cos φ1




cos φ2
0
−sin φ2
0
1
0
sin φ2
0
cos φ2


×


cos φ3
sin φ3
0
−sin φ3
cos φ3
0
0
0
1




x1
x2
x3

.
(19.3)
A single aircraft in the scene is parameterized by its orientation, position and class, and
creates a track as it ﬂies through space. Identifying the rotations with the Euler representation,
then an m-track conﬁguration has parameter vector in the form
x(m) ∈X(m) =

T 3 × R3n(m)
.
(19.4)
An example of a track is shown in panel 1 of Figure 19.2.
19.1.2 The FLIR Sensor Model
The FLIR sensor has a wide ﬁeld of view relative to object packing dimensions demanding the
use of perspective projection, which maps according to (x1, x2, x3) →(x1/x3, x2/x3). This creates
the vanishing point effect in which objects that are further away from the sensor appear closer to
the center of the detector. Objects appear skewed depending on where they appear in the image
plane. For generating FLIR scenes, standard infra-red radiation models of the targets generated by
other groups such as Keweenaw Research Center, Michigan Technological University, are used.
The three panels of the bottom row of Figure 19.1 show the infra-red radiance simulated by the
PRISM code [299] for an M2, an M60, and a T62. For such modeling, target facets are assumed to
radiate known intensities determined from measured data [473] or simulation, or both [474].

19.1 RECOGNITION OF GROUND VEHICLES
535
For the ground scenario the scene consists of many varied objects, some of which are totally
or partially obscuring others, against a highly cluttered background of both natural and man-made
origin. Exemplar data are shown in the left column of Figure 19.3 showing images at 3–5 microns
(top) and 8–12 microns (bottom). For generating FLIR scenes, the CAD models have superimposed
standard infra-red radiation from CAD models of the targets generated using standard software
packages from the Keweenaw Research Center, Michigan Technological University. These models
are used for generating the 3D radiant scenes. The target facets are assumed to radiate known
intensities determined from via radiant model simulation. The resulting measurement is modeled
as Poisson with mean ﬁeld the projection with obscuration and occlusion represented via the
transformation T : I(x(m)) →TI(x(m)). If the measurements correspond to a CCD detector, the
Poisson model assumes the measured 2D image ﬁeld ID(·) is Poisson distributed [31,31,32] with
mean intensity the convolution of the mean ﬁeld TI, the projection on the detector plane [32,317]
with the point spread function of the camera. Let the optics in the detector plane have point-spread
density p(·). The measured counting process ID(yi) at location yi has mean given by the projection
of the temperature signature convolved with the point-spread
EID(·) = p ∗TI(·) ,
(19.5)
Hm(ID) = −⟨1, TI(x(m))⟩+
2
log(p ∗TI(x(m))), ID3
,
(19.6)
where ∗denotes the convolution operation.
Panel3ofFigure19.3depictsthemeanﬁeldTI oftheFLIRradianceprocessprojectedthrough
the projective geometry with point-spread optics in the detector plane. Panel 4 depicts the
(1)
(2)
(3)
(4)
Figure 19.3 Top row: Panels 1 and 2 show 3–5 and 8–12 microns infra-red image of a natural scene
obtained during the Grayling I Field Experiment, Smart Weapons Observability Enhancement Joint
Test & Evaluation Program. Courtesy Robert Guenther of the Physics Division, Army Research
Ofﬁce. Bottom row: Panel 3 shows an ideal infra-red scene TI through the projective geometry
and with Gaussian blur; panel 4 shows the corresponding range mean-ﬁeld for the LADAR.

536
19 JUMP DIFFUSION INFERENCE IN COMPLEX SCENES
19.2 Jump Diffusion for Sampling the Target Recognition Posterior
Random sampling is used to generate samples from the posterior distribution, which is supported
on multiple parameter spaces of varying dimensions (varying object number).
19.2.1 The Posterior distribution
The posterior is the product of the prior and data likelihood. The pose angles are taken as uniform
around the circle, and uniform in the ground frame over which the observations are made. The
prior on the number of objects is taken as Poisson with ﬁxed parameter λ. The posterior potential
Hm associated with subspace X(m) is the superposition of the data likelihood and prior potentials.
For an m-object conﬁguration x(m) ∈X(m) ⊂X deﬁne the potential Hm(x), x ∈X(m) consisting of
the superposition of the log-likelihood and the prior Pm Poisson in object number m with parameter
λ. Then, the posterior µ(·) on X = ∪mX(m) is taken to be of the form
µ(dx) =
∞

m=0
Pm1X (m)(x)e−Hm(x)
Zm
dx,
(19.7)
with the m-object potential
Hm(x) = Hm(ID)
x(m) ∈X(m) =

T × R2m
,
(19.8)
and the normalizer Zm =

X (m) e−Hm(x) dx, and dx the Lebesgue measure appropriate for the
space X(m). Let π(·|ID) be the associated density in each of the subspaces. Notice, π(·|ID) does
not integrate to one over any of the subspaces, so that

X (m)
π(x|ID) dx =

X (m) e−Hm(x) dx

m

X (m) e−Hm(x) dx.
Then, the posterior µ(·) for tracking on X = ∪mX(m) is similar to that above of the form
µ(dx) =
∞

m=0
Pm1X (m)(x)e−Hm(x)
Zm
dx,
(19.9)
with the m-track conﬁguration
X(m) =

T 3 × R3n(m)
× Am.
(19.10)
19.2.2 The Jump Diffusion Algorithms
A single subspace for one vehicle is X0 = R2 × T . The diffusion for the space of m ground vehicles
ﬂows through the manifold X m
0
= R2m × T m. Associate the ﬁrst 2m components of the state

19.2 JUMP DIFFUSION FOR SAMPLING
537
vector with the ﬂow through R2m, and the last m components with the ﬂow through T m. Deﬁning
Xt = [X(1)
t
, X(2)
t
], X(1)
t
∈R2m, X(2)
t
∈T m, then Xt drifts according to the drift vectors Am, Bm of
dimensions 2m, m satisfying the equations
X(1)
t
= X(1)
t0 −
 t
t0
Am(Xs) ds + W(1)
t
,
(19.11)
X(2)
t
=
A
X(2)
t0 −
 t
t0
Bm(Xs) ds + W(2)
t
B
mod 2π
,
(19.12)
where W(1), W(2) are standard vector Wiener processes of dimensions 2m, m, respectively, and
where [·] mod 2π is taken componentwise.
The jump process is controlled by the family of simple moves or graph changes J control-
ling the movement through the discrete non-connected subspaces. The set J controls the jump
dynamics by moving the process from one subspace to another on the jump times with transition
probabilities Q(x, dy) = q(x, dy)/q(x),

X Q(x, dy) = 1 and the measures q(x, dy) deﬁned in the
standard way [462]:
q(x, dy) = lim
ǫ→0
1
ǫ

Pr{Xt+ǫ ∈dy|Xt = x} −1dy(x)

,
q(x) =

J 1(x)
q(x, dy).
(19.13)
The jumps occur at exponentially distributed times governed by a jump intensity q(x, dy). Between
jumps, the process satisﬁes the stochastic differential Eqns. 19.11 and 19.12 appropriate for that
subspace. At each jump, the process moves from state x to a new state y according to the jump
transition-probability measure Q(x, dy) = q(x, dy)/q(x).
The algorithm must specify when the process should jump from space to space, and which
states within each of the spaces it should jump into, and in which directions it should drift. Proper
choice of jump parameters and drifts results in the jump–diffusion process having the special
property that µ is invariant for it allowing for empirical generation of conditional expectations
under the posterior distribution.
For technical considerations, the jump and diffusion process must satisfy certain regularity
conditions to result in a well-deﬁned Markov process (e.g. see [463]). This includes that (i) the
drifts be Lipschitz smooth so the diffusion is well deﬁned, and (ii) the jump intensities q(·) and
Q(x, d·) are bounded and continuous functions, with the jump transition probability measure
Q(x, d·) having ﬁnite support (local). To achieve the desired ergodic property, we choose the jump
intensities according to the following theorem.
Theorem 19.1
Assume the drifts and jump parameters satisfy the above regularity condi-
tions. Then,
1. if the gradient of the posterior follows the drift given by the gradient of the posterior
potential within any of the subspaces,
Am = 1
2∇1Hm ,
Bm = 1
2∇2Hm,
(19.14)
where ∇1, ∇2 are the gradients with respect to the 2m positions and m orientations,
respectively,
2. the jump intensities satisfy the detailed balance condition
q(x)π(x)dx =

J −1(x)
q(y, dx)π(y) dy,
(19.15)
3. and the family of jump moves is rich enough to permit any subspace to be reached from
any other disconnected subspace in a ﬁnite number of jump moves,
THEN, the posterior distribution µ(·) is the invariant distribution for the Markov process.

538
19 JUMP DIFFUSION INFERENCE IN COMPLEX SCENES
Proof
The proof boils down to showing (i) that µ is invariant for the process, and
(ii) verifying that the process is irreducible, and therefore µ is unique. Here, we verify
only (i) the stationarity for the jump diffusion Markov process. For the uniqueness
part (ii) of the proof, see [137,303,472].
The proof of invariance for the diffusion goes as follows. The generator (see
Chapter 18, Deﬁnition 18.6, Eqn. 18.8) or backward Kolmogoroff operator, for the
jump–diffusion process (denote it as A = Ad + Aj, diffusion+jump) characterizes the
stationary distribution in that µ is stationary for the jump–diffusion if and only if

Af(x)µ(dx) = 0
(19.16)
for all f in the domain D(A) of A. This is the Echeverria theorem [463]. Deﬁne the set
of functions forming the domain of the Euclidean part of the generator A as


f : f =
M

m=0
1R2mfm, fm ∈ˆC2(R2m), M ≥0


,
(19.17)
where ˆC2 is the set of twice continuously differentiable functions vanishing at ∞. The
generator is derived as in Section 18.4.1, Chapter 18. The inﬁnitesimal generator for
the diffusion Ad acting on such functions, f = 
M
m=0 1R2mfm, according to Eqn. 19.16
gives

X
Adf(x)π(x) dx =
M

m=0

−

R2m
1
2 < ∇1Hm(x), ∇fm(x) > e−Hm(x)
Z
dx
(19.18)
+

R2m
1
2


2m

i=1
∂2fm(x)
∂x2
i

e−Hm(x)
Z
dx

,
(19.19)
where < ·, · > stands for the vector dot-product and the gradients ∇Hm(x), ∇fm(x) are
with respect to the Euclidean positions (elements of R2m). Integration by parts of the
second term, with the fact that the functions fm vanish at the boundary, results in a
term that is negative of the ﬁrst term. Therefore, the posterior π is a stationary density
of the diffusion process.
Now extend the domain of the functions to include T m, so that X(m) = R2m ×
T m. Then the generator equation, which must be balanced is identical to that above,
just with gradients over all 2m + m components with the integral over all functions in
the domain
D(A) =


f : f =
M

m=0
1X (m)fm, fm ∈ˆC2(X(m)), M ≥0


,
(19.20)
with fm vanishing at inﬁnity for the R2m part of the domain, and periodic on the torus
T m (boundary identiﬁed for the torus). Integration by parts yields the same condition
as above. In passing to the torus, it must be shown that the inﬁnitesimal generator
is identical to the Euclidean case as given above, integrated over the full product
space of Euclidean and torul parameters. This follows from the fact that the torus is
a group with the form of the generator as derived in [303, 472], and the fact that the
usual identiﬁcation of the torus with the real line wrapped implies it has the same ﬂat
geometry (identical tangent space) as the reals.
The jump part of the generator Aj is given by
Ajf(x) = q(x)

X (J 1(x))
Q(x, dy)(f(y) −f(x)).

19.2 JUMP DIFFUSION FOR SAMPLING
539
It must satisfy

X Ajf(x)µ(dx) = 0 as well. Computing the adjoint of Eqn. 19.16 gives

X
Ajf(x)µ(dx) =

X
µ(dx)q(x)

X (J 1(x))
Q(x, dy)(f(y) −f(x))
=

X
µ(dx)q(x)

X (J 1(x))
Q(x, dy)f(y) −

X
µ(dx)q(x)f(x)
=

X
f(y)

X (J −1(y))
µ(dx)q(x)Q(x, dy) −

X
µ(dx)q(x)f(x).
(19.21)
Since this is true for all f in the domain of the generator 45, this implies
q(x)π(x)dx =

X (J −1(x))
q(y, dx)π(y) dy.
(19.22)
19.2.3 Jumps via Gibbs’ Sampling
The jump moves are transformations on the models that act by changing one model to another
with its resulting scene. For this, identify model m with a member of the countable set ℵ. The simple
moves are drawn probabilistically from the family J , and are applied discontinuously during the
dynamics of the algorithm, deﬁning transitions through ℵ, J : ℵ→ℵ. The family of transitions are
chosen large enough to act transitively in the sense that given any pair (m′, m′′) ∈ℵ2 it is possible
to ﬁnd a ﬁnite chain of transitions that leads from m′ to m′′. Deﬁne the set J 1(m) of models that
can be reached in one move, and J −1(m) which is the set that can reach m in one move. For the
desired ergodic properties of the algorithm, these need to satisfy a weak reversibility: J 1 = J −1.
The set of transformations add, delete, and change the identities of the objects. For ground vehicles
X0 = T × R2, we have
deletion
Jd
j : X m
0 × Am →X m−1
0
× Am−1,
j = 1, . . . , m,
(19.23)
addition
Jb : X m
0 × Am →X m+1
0
× Am+1,
(19.24)
identiﬁcation
Ja
j : X m
0 × Am →X m
0 × Am.
(19.25)
These are the only transformations of model type that are allowed. To carry the evolution of the
state forward from the diffusion the jump measures are delta-dirac measures (placing mass on
points in Rn) with respect to the Lebesgue measures in the respective subspaces that the jump
transformations move into, i.e. the part of the state that is not being added or deleted remains
unchanged after the jump transformation. Let x(m) ⊕j y(1) ∈X(m + 1) stand for the addition of
an object y(1) ∈X0 to the jth location in the list of x(m) ∈X(m). Let xdj(m) ∈X(m −1) denote
the deletion of the jth object in the list x(m) ∈X(m). Let xaj(m) ∈X(m) represent changing the
45 It must be shown that the domain is a separating set, i.e. it is sufﬁciently large such that it is distribution
determining. This is the case for examples we study here.

540
19 JUMP DIFFUSION INFERENCE IN COMPLEX SCENES
identity of the jth object in x(m) to a. Deﬁne the jump transitions measures to be
q(x(m), dy(m + 1)) =
m+1

j=1
qb(x(m), y(m + 1))δx(m)(dydj(m + 1))dy(1),
q(x(m), dy(m)) =
m

j=1

a∈A
qa(x(m), y(m))δx(m)(dyaj(m)),
q(x(m), dy(m −1)) =
m

j=1
qd(x(m), y(m −1))δxdj(m)(dy(m −1)),
(19.26)
with total jump intensity
q(x(m)) =

X (J 1(m))
q(x(m), dy).
(19.27)
Choosing the jump and diffusion parameters properly makes the invariant probability distribution
the posterior. A variety of jump–diffusion processes may be formulated which will result in the
desired invariant measure. The different schemes correspond to different choices for the jump
intensity q(x, dy). First examine the analog of a Gibbs sampler or heat bath.
Corollary 19.2
Let the jump intensities satisfy
q(x, dy) =

π(y)dy
if y ∈J 1(x)
0
otherwise
.
(19.28)
Then the detailed balance condition Eqn. 19.15 from Theorem 19.1 is satisﬁed.
Proof
The right-hand side (RHS) of Eqn. 19.22 becomes

y∈X (J −1(x))
q(y, dx)π(y)dy (a)
=

y∈X (J −1(x))
π(x) dxπ(y) dy
(19.29)
(b)
=

y∈X (J 1(x))
q(x, dy)π(x) dx = q(x)π(x) dx,
(19.30)
with (a,b) following from Eqn. 19.28 and weak reversibility, J 1 = J −1.
To prove the detailed balance condition Eqn. 19.22, assume that x ∈X(m) for
any m, and substitute the jump transitions. The left-hand side (LHS) of Eqn. 19.22
becomes
LHS = π(x) dx

X (J 1(x(m)))
q(x, dy)
(19.31)
= π(x) dx


m+1

j=1

X0
qb(x, x ⊕j y(1)) dy(1) +
m

j=1
qd(x, xdj) +

a∈A
m

j=1
qa(x, xaj)


(19.32)
= π(x) dx


m+1

j=1

X0
π(x ⊕j y(1)) dy(1) +
m

j=1
π(xdj) +

a∈A
m

j=1
π(xaj)

.
(19.33)

19.2 JUMP DIFFUSION FOR SAMPLING
541
Now examine the RHS of Eqn. 19.22:
RHS =
m+1

j=1

X (m+1)
π(y) dyqd(y, x)δydj (dx) +
m

j=1

X (m−1)
π(y) dyqb(y, x)δy(dxdj)
+
m

j=1

a∈A

X (m)
π(y) dyqa(y, x)δyaj (dx)
(19.34)
=
m+1

j=1

X0
π(x ⊕j y(1))qd(x ⊕j y(1), x) dy(1) dx
+
m

j=1
π(xdj)qb(xdj, x) dx +
m

j=1

a∈A
π(xaj)qa(xaj, x) dx
(19.35)
=
m+1

j=1

X0
π(x ⊕j y(1))π(x) dx dy(1) +
m

j=1
π(x)π(xdj) dx +
m

j=1

a∈A
π(xaj)π(x) dx.
Comparing with Eqn. 19.33 shows detailed balance.
The computational algorithm is as follows.
Algorithm 19.3
Set the jump number n = 1, simulation time t0 = 0, and X(0) = 0.
Construct a jump–diffusion process Xt, t ≥0 jumping on random times t1, t2, . . . according
to the following:
1. For t ≥tn−1, Xt follows the stochastic integral Eqns. 19.11 and 19.12 in the subspace
determined by Xtn−1 drifting as a diffusion according to Eqns. 19.11 and 19.12 associated
with the mth space under the drift vectors Am, Bm.
2. Generate a series of exponential random variables u1, u2, . . . with mean 1, determining
the nth jump time determined by the ﬂow and the jump parameters according to
tn = inf{τ :
 τ
tn−1

X (J (Xt))
q(Xt, dy) ≥un} ,
(19.36)
with Xt following the stochastic differential Eqns. 19.11 and 19.12, in [tn−1, tn).
3. At random time tn, jump from Xtn to an element in X(J 1(Xtn)) according to the
transition probability
Q(Xtn, dy) =
q(Xtn, dy)

X (J (Xtn)) q(Xtn, dy),
y ∈X(J 1(Xtn)).
(19.37)
4. n ←n + 1 and return to 1.
19.2.4 Jumps via Metropolis–Hastings Acceptance/Rejection
This section follows heavily the work of Lanterman [464]. To avoid the complexity of computing
the integral in Eqn. 19.27 we shall examine an alternative procedure based on the acceptance and
rejection of proposals. The principle contribution will be to choose a jumping scheme analogous to
the Metropolis–Hastings sampling algorithm [465–467]. At exponentially distributed times, with

542
19 JUMP DIFFUSION INFERENCE IN COMPLEX SCENES
ﬁxed mean, a candidate xprop is drawn from a proposal density r(xold, xprop); the acceptance probability
is then computed:
α(xold, xprop) = min
C
π(xprop)r(xprop, xold)
π(xold)r(xold, xprop) , 1
F
.
(19.38)
The proposal is accepted with probability α(xold, xprop) and rejected with probability 1 −
α(xold, xprop). A wide variety of proposal densities may be used; of course, r(xold, xprop) > 0
for xprop ∈J 1(xold) and r(xold, xprop) = 0 for xprop ̸∈J 1(xold).
Special cases of the general Metropolis–Hastings jump–diffusion scheme have appeared pre-
viously in the literature. If r(xold, xprop) = r(xprop, xold), we have α(xold, xprop) = min{π(xprop)/
π(xold), 1}, which corresponds to the traditional Metropolis algorithm [468] in which proposals
are drawn from the prior and accepted with the likelihood. When the prior distribution is highly
informative this is effective. We shall explore this in the subsequent section. For target detection
with a uniform prior on positions and orientations, the prior is not informative; it provides little
help in locating new targets or determining target orientation. The approach will be to propose
a birth, death, or identity–change move according to the prior on the number of targets, p(m), and
draw the proposal from the posterior density over the space which can be reached via the chosen
move type. By sampling from the posterior, the algorithm takes on a “Gibbs within Metropolis”
character.
Let m = #(x) and ξ = p(m + 1) + p(m −1)1>0(m) + p(m)1>0(m), where 1>0(m) = 1 if m > 0
and 0 otherwise. Deﬁne r(·, ·) according to r(x, y) = 0 for y ̸∈J 1(x), and
r(x, x ⊕j y(1)) = p(m + 1)
(m + 1)ξ
eH(x⊕jy(1))

X0 eH(x⊕jy′(1))dy′(1)
,
(19.39)
r(x, Ja
j x) = p(m)
ξ
eH(Ja
j x)

a′∈A eH(Ja′
j x) ,
(19.40)
r(x, Jd
j x) = p(m −1)
ξ
eH(Jd
j x)

m
j′=1 e
H(Jd
j′x) .
(19.41)
Algorithm 19.4
On each jump trial, this choice of r(·, ·) corresponds to the following
algorithm:
1. Draw one of three possible jump choices from the set {birth, death, identity −change}
according to the distribution {p(m + 1)/ξ, p(m −1)1>0(m)/ξ, p(m)1>0(m)/ξ},
2. If birth, then
(a) Draw j ∈1...m + 1 uniformly, and draw y(1) from the density
eH(xold⊕jy(1))

X0 eH(xold⊕jy′(1))dy′(1)
,
(19.42)
and accept xprop = xold ⊕j y(1) , with probability
min



(m + 1)

X0 eH(xold⊕jy′(1)) dy′(1)

m+1
j′=1 e
H(Jd
j′xprop)
, 1



.
(19.43)

19.2 JUMP DIFFUSION FOR SAMPLING
543
3. Else if identity–change, then
(a) Draw j ∈1...m uniformly, and draw identity a from the density
eH(Ja
j xold)

a′∈A eH(Ja′
j xold) ,
(19.44)
and accept xprop = Ja
j xold unconditionally.
4. Else if death, then
(a) Select xprop = Jd
j xold with probability
eH(Jd
j xold)

m
j′=1 e
H(Jd
j′xold) ,
(19.45)
and accept xprop with probability
min




m
j=1 eH(Jd
j xold)
(m + 1)

X0 eH(xprop⊕jy′(1)) dy′(1)
, 1



.
(19.46)
19.3 Experimental Results for FLIR and LADAR
19.3.1 Detection and Removal of Objects
To detect ground vehicles, the jump process in the jump diffusion solves the generalized hypothesis
test for detection by adding candidates according to their posterior probability. Target detection
ideally requires exhaustive evaluation of the posterior over the continuum of R2, corresponding
to the increase in dimension of translation in the plane. The jump parameters associated with a
birth of a new target, from x →(x ⊕y(1)), y(1) ∈R2 × T , requires the computation
π(x ⊕y(1))

R2×T π(x ⊕y(1)) dy(1),
y(1) ∈R2 × T .
(19.47)
This computation is performed by placing a lattice on the position-orientation space. Then π(x ⊕
y(1)), y(1) ∈L is computed over the lattice, with the jump process adding new detected targets at
those locations according to probabilistic selection under the distribution according to Eqn. 19.37.
At times, the algorithm proposes, but then rejects its choices via death moves that remove
hypothesized targets. To illustrate, columns 1 and 2 of Figure 19.4 show the death process for
removing objects depicting the state of the algorithm beginning with an initial condition in which
there are three extra targets. The vehicle in the front represents the most substantial mismatch to
the collected data. It is the ﬁrst target removed by the death process in iteration 2. Further jumps
remove the extraneous vehicles in the back.
19.3.2 Identiﬁcation
Figure 19.4 focuses on the identiﬁcation component of the algorithm. For a binary alphabet,
A = {M60, T62}, identiﬁcation corresponds to the transformation of the scene changing the jth

544
19 JUMP DIFFUSION INFERENCE IN COMPLEX SCENES
(1)
(2)
(3)
(4)
Figure 19.4 Columns 1 and 2 Death Process: Panel 1 shows an initial condition with extraneous
target hypotheses; 2, 3, and 4 show the state of the jump diffusion process at iterations 2, 41, and
65, respectively. Columns 3 and 4 Identiﬁcation Process: Column 3 shows the match of the M60 to
a data from a scene containing an M60 (top) and the match of the T62 to an M60 scene (bottom).
Column 4 shows the log-likelihood of the data on the pixel array for the two matches. Brightness
means higher relative log-probability. Results taken from Lanterman.
tank in the scene to type a ∈{M60, T62}, requiring the evaluations
π(xaj)
π(JM60
j
x) + π(JT62
j
x)
,
a ∈{M60, T62}.
(19.48)
This solves the generalized hypothesis test by choosing according to the relative probabilities.
Shown in column 3 of Figure 19.4 is the M60 (top) and T62 (bottom) superimposed over the data;
column 4 shows the difference in log-posterior probabilities (plotted in gray scale). Bright pixels
correspond to high evidence of object classiﬁcation. The relative probability corresponds to an
exponentiation of the integral of the brightness pictures. The top choice M60 is far more favorable
because of the bright log-probability values.
19.3.3 Pose and Identiﬁcation
Solving both the pose and identity problems essentially relies on the closeness of the synthesized
objects to the actual measured data. Examine airplanes in R3, with the orientation space of SO(3)
and translations in R3 according to (O, p) : x →Ox + p with
(O, p) :


x1
x2
x3

→


o11
o21
o31
o12
o22
o32
o13
o23
o33




x1
x2
x3

+


p1
p2
p3

,
(19.49)
with orthonormal columns for the O-matrix. The recognition of pose and identity is determined
by the closeness of the object to the measured data. Consider optical imagery assuming orthogonal
projection. The optical imaging measurements are Gaussian random ﬁelds whose mean ﬁelds are
taken to be perspective projection onto the camera. This is depicted in the top row of Figure 19.5
showing a set of imagery of an airplane at different poses. The middle row shows a sequence of
two renderings (panels 5 and 7) corresponding to estimated poses as a function of simulation time
(left to right). The associated data driving the rotations are shown in panels 6 and 8 via difference
images between the actual optical measurement data on the imaging lattice generated from the

19.3 EXPERIMENTAL RESULTS FOR FLIR AND LADAR
545
(1)
(2)
(3)
(4)
Figure 19.5 Top row shows examples of optical imagery for the airplane. Middle row shows two
poses of the target space; panels 5 and 7 show renderings of the target with panels 6 and 8
showing the difference between the optical data synthesized according to the Gaussian model
from the target at its estimated pose, subtracted from the true measured data. Bottom row shows
identiﬁcation showing two different identiﬁcations. Panels 9 and 11 show renderings of the two
different target types; panels 10 and 12 show the difference between the optical data generated
from the proposed target at its estimated pose, subtracted from the true measured data. The
bottom right panel 12 shows the correctly estimated identity and difference data.
target at its estimated pose subtracted from the true measured data. The rightmost panel shows
the most correct pose estimate. Notice the difference image is small and the relative probability
preference is proportional to the exponential of the squared differences. Pose is estimated by
computing stochastic gradients that follow these probability differences.
For identiﬁcation of airframes the algorithm jumps between target templates carrying the
pose and position parameters with it. The bottom row of Figure 19.5 shows two different target
types carrying its pose parameters as the algorithm tries the various target types. The target
identity shown in panel 11 is the correct one. Panels 9 and 11 show renderings of the two different
target types. Panels 10 and 12 show the difference between the optical data generated from the
proposed target at its estimated pose, subtracted from the true measured data. The target identity
shown in panel 11 is the correct one. Notice how panel 12 shows difference data, which is small.
The identiﬁcation part of the algorithm checks all the target types. In our experiment, there are
three types, A = {1, 2, 3}, and the algorithm computes the relative probability of the proposed
conﬁguration x →xaj, a ∈{1, 2, 3} choosing according to the probability
π(xaj)

a=1,2,3 π(xaj),
a ∈{1, 2, 3}.
(19.50)

546
19 JUMP DIFFUSION INFERENCE IN COMPLEX SCENES
Figure 19.6 Top row shows the target rendered at a sequence of four ﬂight times, time increasing
from left to right. Middle row shows the HRR data associated with the target imaged at these
orientations. Bottom row shows the estimates of the pose generated at the four ﬂight times.
Results taken from O’Sullivan [300].
19.3.4 Identiﬁcation and recognition via High Resolution Radar (HRR)
Figure 19.6 shows results of target orientation estimation using HRR data. The HRR mod-
els [308, 475] for target recognition of airframes assume illumination by an S-band radar with
a linear FM chirp for the transmitted signal with a center frequency of 3.2 GHz and a bandwidth
of 320 MHz. For performing the synthesis in the data likelihood function, the radar signature
prediction tool XPATCH is used to generate a library of range proﬁles. The data generation and
imaging model is described in Section 13.3.5 of Chapter 13; the likelihood model is speciﬁed via
the likelihood Eqn. 13.41. The top row shows the target rendered at a sequence of four ﬂight times,
time increasing from left to right. Shown in the middle row are the actual HRR data associated with
the target imaged at these orientations. The bottom row shows the estimated pose generated by the
algorithm as it diffuses through orientation space searching for the target proﬁle consistent with the
HRR data.
19.3.5 The Dynamics of Pose Estimation via the Jump–Diffusion Process
Figures 19.7 and 19.8 show results for FLIR imagery for ground-based targets. Panel 1 of Figure
19.7 shows the data modeled to accommodate the radiant intensity of the scene with superimposed
CCD array noise exhibiting photoconversion noise and dead (black) and saturated (white) pixels.
Panels 2–6 of Figure 19.7 show successive stages of the jump–diffusion process converging towards
the correct estimate of the number of targets and their positions and orientations. The hypothesized
targets are manifest as white wireframes superimposed over a noise-free rendering of the true
conﬁguration. By iteration 55 (panel 7 of Figure 19.7), the algorithm has discovered the number,
positions, and orientations of all of the targets.

19.3 EXPERIMENTAL RESULTS FOR FLIR AND LADAR
547
(1)
(2)
(3)
(4)
(5)
(6)
(7)
(8)
(9)
Figure 19.7 FLIR: Panel 1 shows the initial data from a conﬁguration of M2 APC’s observed by
the FLIR imager. Successive panels 2–9 show a sequence of states of the jump–diffusion process at
iterations 1, 10, 12, 25, 27, 34, 55, and 75. Results taken from Lanterman [32].
(1)
(2)
(3)
(4)
(5)
(6)
(7)
(8)
(9)
(10)
Figure 19.8 FLIR: Panel 1 shows the measured scene for the FLIR imager. Panels 2–10 show itera-
tions 1,3,24,32,34,68,87,88, and 117 of the jump–diffusion process. Results taken from Lanterman
[32].
These experiments suggest that operating in the conﬁguration space is exceptionally robust,
even with the level of noise seen in panel 2. Between iterations 1 and 10, the algorithm births
an object. Due to perspective projection, targets that are closer to the detector appear larger. In
iteration 25 (panel 5) the algorithm births a third target, and continues to discover all of the targets.
Interesting states of the sample path of a second FLIR experiment are shown in Figure 19.8.
Since the process was started with an empty conﬁguration, the algorithm tries a birth on the ﬁrst
iteration. The M60 on the right appears larger than the other targets since it is closer to the sensor.
The algorithm ﬁnds it ﬁrst since it can “explain” the largest amount of data pixels with it. In
iteration 3, it mistakes the T62 for an M60. It does this since it has not yet found the adjacent M2
and is trying to explain some of the M2’s pixels using the barrel of the M60. This demonstrates the
importance of moves that allow changes of type. In iteration 24, the algorithm ﬁnds the M2, but the
hypothesis is facing the wrong direction. While the diffusions may reﬁne orientation estimates,
they are impractical for making large orientation changes, suggesting the necessity of a jump
move for making such drastic changes in orientation. Notice the diffusions have found the correct
placement of the M60. The remaining M60 is found in iteration 32. The algorithm continues to
propose birth moves that are rejected since the data does not support a ﬁfth target.

548
19 JUMP DIFFUSION INFERENCE IN COMPLEX SCENES
In iteration 34, the barrel of the incorrectly-guessed M60 is no longer needed to explain
the M2 pixels, so a change in identity move swings the hypothesis around, but still incorrectly
supposes it to be an M60. It is ﬂipped back the other direction and incorrectly supposed to be an
M2 in iteration 68.
Between iterations 68 and 87, the diffusions pull the incorrectly-hypothesized M2 closer to
the correct position, so a change in identity move in iteration 88 correctly changes its type to a T62.
The correct orientation of the true M2 is found by iteration 117 with the conﬁguration of all of the
types, positions, and orientations correctly deduced. All throughout the inference, the algorithm
proposes death moves which are rejected.
19.3.6 LADAR Recognition
Similar results hold for the LADAR setting where a laser radar sensor is substituted in place of
the FLIR model. The LADAR observes the scene through the effects of obscuration and perspective
projection, in which a point (x, y, z) in 3D space is projected onto the 2D detector according to
(x, y, z) →(x/z, y/z) and discretized into a lattice of points. Following Section 13.3.3, Chapter 13,
Eqn. 13.36, the log-likelihood of the range data ID gives the true range image TI:
Hm(ID) =

i
log

[1 −PrA(yi)]e−1/2σ 2(yi)(ID(yi)+TI(x(m))(yi))2 + PrA(yi)
R

.
(19.51)
The parameters of the model are described in Section 13.3.3, Chapter 13. The idealized LADAR
range mean-ﬁeld is shown in panel 4 of Figure 19.3.
Figure 19.9 shows results for the LADAR similar to that depicted for the FLIR log-likelihood.
The jump–diffusion is virtually identical to the LADAR log-likelihood substituted and the Gibbs
within Metropolis approach taken for the jump diffusion. Interesting snapshots of the sample path
of the algorithm are depicted in the panels. The current hypothesis is shown as a white outline.
The algorithm ﬁrst ﬁnds the M2 on the right. Since it is closer to the detector, it takes up more
pixels, and the algorithm chooses it since it can “explain” a large portion of the data. By iteration
11 a birth move has discovered the M2 on the left, with the diffusions pulling the M2 on the right
to the correct position. In iteration 12, a change in identity move makes a large-scale orientation
change to correct the orientation of the M2 on the left. In iteration 24, a birth move ﬁnds the
(1)
(6)
(7)
(8)
(9)
(2)
(3)
(4)
(5)
Figure 19.9 Results from LADAR: Panel 1 shows the initial data from a conﬁguration of vehicles
observed by the LADAR imager. Panels 2–9 show iterations 1, 3, 11, 12, 24, 32, 38, and 130 of the
jump-diffusion process for the LADAR range data. Results taken from Lanterman [32].

19.3 EXPERIMENTAL RESULTS FOR FLIR AND LADAR
549
M2; also note that the diffusions have reﬁned the position of the leftmost M2. Iteration 32 shows
the algorithm birthing an M2 (facing the wrong direction) over the T62. The algorithm changes
its mind and switches this incorrect M2 to a T62 (now facing the correct direction) via a change
in identity move by iteration 130, and the diffusions have reﬁned the pose. Notice that, in this
preliminary experiment, the algorithm incorrectly estimates the orientation of the M2.
19.4 Powerful Prior Dynamics for Airplane Tracking
For airplanes, the basic parameter space is X0 = R3 × SO(3). Our examples employ ﬂight paths
which were generated using the ﬂight simulator software. The tracking data results from a nar-
rowband tracking array and a high resolution imaging sensor. The array geometry corresponds
to a 64-element cross-array of isotropic sensors located at half-wavelength spacing. The high res-
olution imaging data consists of Gaussian random ﬁelds on a 64 × 64 imaging lattice. The mean is
the projection of the rendered object.
For single objects, the possible jump transformations through parameter space are either
addition of a track segment y ∈X0, or deletion of a track segment, or changing the target type.
Shown in Figure 19.10 shows the evolution of the random sampling algorithm for estimating the
path of a single target. The top row shows the gray track representing the true airplane path
(1)
(2)
(3)
(4)
(5)
(6)
(7)
(8)
(9)
Figure 19.10 Top row: Panel 1 shows the actual track drawn in gray with the mesh representing
the ground supporting the observation system in the inertial frame of reference. Panels 2 and 3
display intermediate results from the single-track estimation with the estimates drawn in black.
Rows 2 and 3: Sequence of jump moves adding segments to the estimated state from left to right
with no diffusion. Bottom two rows: Sequence of panels showing diffusion towards the true track
mean.

550
19 JUMP DIFFUSION INFERENCE IN COMPLEX SCENES
consisting of 1500 track segments. The estimated track is shown overlapping in black at three
different times during the estimation.
The jump diffusion for m airplane tracks has n(m) segments of the diffusion ﬂowing through
the orientations and positions X n(m)
0
= T 3n(m) × R3n(m). The ﬁrst 3n(m) components of the state
vector ﬂow through R3n(m); the last ﬂow through T 3n(m) according to Xt = [X(1)
t
, X(2)
t
], X(1)
t
∈
R3n(m), X(2)
t
∈T 3n(m). The algorithm and its properties are essentially the same as in Theorem 19.2.
19.4.1 The Euler-Equations Inducing the Prior on Airplane Dynamics
Airplane dynamics are straightforwardly expressed using the velocities (v1, v2, v3) projected along
the body-frame axes, and the angular velocities (q1, q2, q3). Let O(·) : [t0, t] →SO(3) represent the
rotating coordinate system in the body frame satisfying the differential equations
˙Ot = OtQt,
O0 = id,
where Qt =


0
q1t
q2t
−q1t
0
q3t
−q2t
−q3t
0

,
(19.52)
where id is the 3 × 3 identity matrix. The matrix Qt determines the direction of the ﬂow of Ot ∈
SO(3) through the tangent space of the manifold SO(3). The linear momentum equation, written
in body-frame coordinates, becomes Otft = d/dt

Otvt

implying the familiar equation
ft = ˙vt + Qtvt.
(19.53)
Thepositionsininertialcoordinatesarerelatedtothevelocityinthebodyreferenceframeaccording
to pt =
 t
t0 Osvs ds + pt0.
To induce the prior, assume the forcing function f is a Gaussian white random process;
this induces a Gauss–Markov process on the velocities, conditioned on the sequence of Euler
angles. Assume the angular and linear velocities are piecewise constant over the time increments
vn = pn+1 −pn, On+1 = OneQn, where Qn =


0
q1n
q2n
−q1n
0
q3n
−q2n
−q3n
0

. The discretized equations
become fn = vn+1 −(id −Qn)vn. Choosing the Gaussian force process to have variance , the
velocities are ﬁrst-order Markov conditioned on the rotations, and the positions are second-order
Markov:
p(vn|On, On−1, vn−1) = det−1/2(2π)e−(vn−(id−Qn−1)vn−1)∗−1(vn−(id−Qn−1)vn−1).
(19.54)
We characterize rotational motion through their Euler angle representation O1(φ1), . . . , On(φn)
modeling the n Euler angle triples φ1, . . . , φn as a Von–Mises Markov density on the three torus
(see [476], for example):
p(φn|φn−1) = e

3
i=1 κi cos(φi,n−φi,n−1)
Z(1)
.
(19.55)
The jump operators for tracks correspond to deletion or removal of the jth track, and addition
and deletion of segments to existing tracks with the basic parameter space X0 = T 3 × R3. For
the jump process, let x(m) ⊕j y(1) represent an m + 1 plane track formed by adding y(1) ∈X0 to
x(m) at the jth location in the list, and x(m) ⊕j y signify the addition of a segment to the jth track
of x(m), y ∈X0. Let xdj(m) denote the deletion of the jth track in the list x(m) ∈X(m). Let xaj(m)
represent changing the identity of the jth track in x(m) to a.

19.4 POWERFUL PRIOR DYNAMICS FOR AIRPLANE TRACKING
551
As we have noted in the previous Section 19.2.4, various examples of the general Metropolis–
Hastings jump-diffusion scheme can be used depending upon the problem. For selecting model
order (object or track number), if an informative prior that is easy to sample from is used, it is
beneﬁcial to choose
r(xold, xprop) =
π(#(xprop), xprop)

J 1(xold) π(#(x′), x′) dx′
(19.56)
for xprop ∈J 1(xold), which corresponds to drawing candidates from the prior and accepting and
rejecting them based on the likelihood. This is the approach of Corollary 2 of Theorems 1 and
2 of [137]. This approach is effective in the ground-to-air tracking that we study in this section
since drawing from the prior on motion dynamics is fast and effective. For target detection with a
uniform prior on positions and orientations, the prior is not informative; it provides little help in
locating new targets or determining target orientation.
The jump parameters become
q(x(m), dy(m + 1)) =
m+1

j=1
qb
t (x(m), y(m + 1))δx(m)(dydtj (m + 1))dy(1),
(19.57)
q(x(m), dy(m)) =
m

j=1
qb
s(x(m), y(m))δx(m)(dydsj (m))dy +
m

j=1
qd
s (x(m), y(m))δ
x
dsj (m)(dy(m))
+
m

j=1

a∈A
qa(x(m), y(m))δx(m)(dyaj(m)),
(19.58)
q(x(m), dy(m −1)) =
m

j=1
qd
t (x(m), y(m −1))δ
x
dtj (m)(dy(m −1)).
(19.59)
Corollary 19.5 (Metropolis jump parameters)
With the parameters of the jump mea-
sures chosen as follows, then the detailed balance conditions for the jump process of Theorem
19.2 are satisﬁed:
qb
t (x(m), x(m) ⊕j y(1)) =
1
5(m + 1)e−[L(x(m)⊕jy(1))−L(x(m))]+πprior(y(1)), (19.60)
qb
s(x(m), x(m) ⊕j y) = 1
5me−[L(x(m)⊕jy)−L(x(m))]+πprior(y(1)),
(19.61)
qd
t (x(m), xdtj (m)) =
1
5m
e−[L(x
dtj (m))−L(x(m))]+
Z(1)
,
(19.62)
qd
s (x(m), xdsj (m)) =
1
5m
e−[L(x
dsj (m))−L(x(m))]+
Z(1)
,
(19.63)
qa(x(m), xaj(m)) =
1
5me−[L(xaj(m))−L(x(m))]+.
(19.64)
The proof follows the Metropolis/Hastings proof.
Figure 19.10 shows magniﬁed views of sections of tracks being estimated by the jump-
diffusion algorithm. The series shows successive guesses of the jump process which, continually
attempt to add and delete new track segments. On each addition, the new segment is drawn from
the prior on ﬂight dynamics, which is parameterized by the track up to that point in time. Hence,
the jump algorithm tends to infer track segments that are close to the true track.
The bottom two rows of Figure 19.10 show the diffusion only partly with no addition of track
segments. Notice how the state is brought into alignment following the addition of new segments.

552
19 JUMP DIFFUSION INFERENCE IN COMPLEX SCENES
(1)
(2)
(3)
(4)
Figure 19.11 Top row: Panel 1 shows the track estimates drawn overlapping in black at several
stages of the algorithm. Panel 2 depicts the posterior probability of a target at that position
proportional to the size of the sphere drawn. Panels 3 and 4 show candidates from the prior
distribution for target path estimation with the high prior probability candidates forming a cone
at the end of the track for the algorithm to sample from.
19.4.2 Detection of Airframes
To detect the existence of objects, the jump process queries the Euclidean space based on the track-
ing data. The jump part of the jump diffusion solves the generalized hypothesis test for detection
by adding candidates according to their posterior probability. Target detection ideally requires
exhaustive evaluation of the posterior density over the possible positions. The computation is
performed by placing a lattice on position space; during initial detection (discovery) there is no
orientation. Then π(x ⊕y(1)), y(1) ∈R3 is computed on the lattice sites, with the jump process
adding new detected targets at those orientations according to probabilistic selection under the
distribution according to
π(x ⊕y(1))

R3 π(x ⊕y(1)) dy(1).
(19.65)
The right panel in the top row of Figure 19.11 shows the conditional log-likelihood proﬁle in
the azimuth-elevation space of the tracking data vector conditioned on the current estimated
conﬁguration as a function of estimation (simulation) time. The algorithm selects points with high
probability, and if the data supports target birth, a track is initiated from that point. Shown in
the rightmost column are the most likely candidates y(1) on the lattice that have high posterior
probability; the probability is depicted via the size of the spherical shells.
19.4.3 Pruning via the Prior distribution
Inducing the prior distributions using the dynamical equations of motion is important in the
algorithm. The dynamical equations provide robustness to the estimation procedure and provide
an organized strategy for search and pruning of the high-dimensional state spaces. To illustrate

19.4 POWERFUL PRIOR DYNAMICS FOR AIRPLANE TRACKING
553
this, examine the generalized hypothesis test that the algorithm solves as it generates its fam-
ily of solutions. Every proposal of a new target during track growth corresponds to the move
x →x ⊕y(1), y(1) ∈R3 × SO(3). This requires sampling from the distribution according to the
calculation of
π(x ⊕y(1))

R3×SO(3) π(x ⊕y(1)) dy .
(19.66)
This calculation is performed by pruning the state space according to the dynamical equations
which determine the distribution.
The bottom row of Figure 19.11 shows the cone of support depicting the importance of the
dynamics-based prior. To show the support of the prior distribution in phase space, panels 3 and
4 show the 10 highest prior probability candidates placed at the track end for the algorithm to
choose from. Panels 3 and 4 correspond to a different time during the inference. The two panels
show that if the track vector is close to the true track, the cone of candidates predicts well the future
position.
19.5 Deformable Organelles: Mitochondria and Membranes
Now we turn to the most microscopic level of variability, that evident in the ultra-structural and
intracellular features of single cells. For this examine the structural features of single cells as studied
via electron microscopy. Panel 1 of Figure 19.12 is a sample micrograph illustrating the exquisite
variability manifested by the dark closed mitochondria shapes, surrounded by cytoplasm.
19.5.1 The Parameter Space for Contour Models
For representing biological shapes in electron micrographs, the generators become closed curves
as in Example 7.29 of Chapter 6. The transformations are scales and rotations

u1
u2
−u2
u1

=
(1)
(2)
Figure 19.12 Panel 1 shows the micrograph data at 30,000 times magniﬁcation with panel 2 show-
ing a hand segmentation into the disjoint partition ∪jIgj and with bounding contours gj of the
two region types, mitochondria (white) and background (black). Data taken from the laboratory
of Dr. Jeffrey Safﬁtz of Washington University.

554
19 JUMP DIFFUSION INFERENCE IN COMPLEX SCENES
ρ

cos θ
sin θ
−sin θ
cos θ

acting on the tangent elements of closed circular templates, gt, t ∈[0, 1], g0 =
g1. The ﬁnite-dimensional transformations of n-similarities arise by assuming piecewise constancy
of the transformations over intervals, with the template g0
l , l = 0, 1/n, . . . , 1 being a polygonal
approximation to the circle (see Figure 7.3 of Chapter 6). The n-similarities become the scales and
rotations applied to the chords, and adding global translation gives
gt= l
n =
 u00
u01

+ 2π

l
n
0

u1t
u2t
−u2t
u1t
  −sin 2πt
cos 2πt

dt
(19.67)
=
 u00
u01

+
nl

k=1

u1k
u2k
−u2k
u1k
  cos 2πk/n −cos 2π(k −1)/n
sin 2πk/n −sin 2π(k −1)/n

.
(19.68)
The similarity transformation consists of n-scale/rotations and one global translation. Identifying
the transformations with its parametric representation, the parameter space representing closed
contours is 2n + 2 −2 scale–rotation dimensions and global translation dimensions, since we lose
two dimensions for closure. A micrograph of m mitochondria is speciﬁed by the scale/rotations
and translations, having parameter vectors that are the concatenation of parameters associated
with each of the objects:
x(m) = (u(1), u(2), . . . u(m)) ∈X(m) = R2nm,
(19.69)
where u(i) = (u(i)
00, u(i)
01, u(i)
11, u(i)
21, . . . ).
The ideal scene is the set of interiors of the contours Ig1, Ig2, . . . with boundaries given by
the contours. This deﬁnes a disjoint partition of the background space Zn2 as the set of simply
connected regions ∪jIgj. Figure 19.12 is an illustration of the decomposition of the micrograph
image into the disjoint partition of simply connected regions. Panel 1 shows the micrograph data
with panel 2 showing a hand segmentation into the disjoint partition of connected interiors ∪jIgj
of the two region types, mitochondria (white) and background (black).
Gaussian Markov random ﬁeld models can be used for segmentation of the micro-
graphs; each mitochondria is a conditional Gaussian random ﬁeld conditioned on the random
conﬁguration of the contour.
19.5.2 Stationary Gaussian Contour Model
For the prior stationary processes for the contours are used as in Chapter 9, Example 9.28. Identify
the scale–rotation matrices with the 2 × 1 vectors ui =
 u1i
u2i

. The ui, i = 1, . . . , n are modeled as
a Gaussian cyclo-stationary, vector process with 2n×2n block Toeplitz, periodic covariance K(u(j))
with 2 × 2 blocks Kk,i(u(j)) = Kk−i,0(u(j)). The prior on m-contours becomes
π(x(m) = u(1), u(2), . . . ) =
m

j=1
det−1
2 2πK(u(j))e−1
2

kl u(j)∗
k
K(u(j))−1
kl u(j)
l ,
(19.70)
where K(u(j)) is the cyclo-stationary Toeplitz covariance of the jth contour. 46
46 The natural coordinates are the Fourier transform which diagonalizes the covariance into the block inde-
pendent complex variables ¯ui = 
n−1
k=0 uke−ji2πk/n, which are then a realization of an independent Gaussian

19.5 DEFORMABLE ORGANELLES: MITOCHONDRIA AND MEMBRANES
555
Figure 9.1 of Chapter 9 shows examples of eight closed shapes generated from the prior
distribution. The mean shape and the covariance spectrum is also represented, demonstrating the
typical structure and its range of variation. The covariance was estimated by empirical ﬁtting of
the hand-contoured micrographs to generate statistics on the scale–rotation parameters.
The constraint that two objects cannot be superimposed in electron-micrographs is imposed
via an intersection penalty with the potential energy with the intersection between adjacent con-
tours α 
m
i,j=1 |Igi ∩Igj|. Similar to the ground vehicles example, a Poisson prior on the number
of mitochondria can be used, with mean parameter chosen to reﬂect the micrograph data.
19.5.3 The Electron Micrograph Data Model: Conditional
Gaussian Random Fields
We model the micrograph imagery via two compartments, mitochondria interior and cytoplasm
exterior. The basic data model builds the electron micrograph imagery as a disjoint partition of
Gaussian random ﬁelds ID
gj , j = 1, . . . , m determined by the random contours g1, g2, . . . . Construct
the background space Zn2 as a disjoint partition of simply connected regions ∪jIgj, with Igj
denoting the interior of contour gj. Model ID
gj as a Gaussian random ﬁeld on the subgraph Igj ⊂
Zn2 associated with the jth mitochondria. Deﬁne the cytoplasm image ID
cyt on the complement:
ID
cyt on Zn2 \ ∪m
j=1Igj. Then, the measured micrograph images ID are modeled as realizations of
conditionally independent Gibbs random ﬁelds, conditioned on the subgraphs speciﬁed by the
boundaries.
Figure 19.12 shows an illustration of the decomposition of the image into a disjoint partition
of simply connected regions. Panel 1 shows the random image ID; panel 2 shows a partition of the
background space into a disjoint partition ∪jIgj depicting the contours in white; panel 3 shows
the partition into interiors Ig.
Gaussian Markov random ﬁeld models can be used for segmentation of the micrographs;
each mitochondria is a conditional Gaussian random ﬁeld conditioned on the random conﬁgu-
ration of the contour. Using the asymptotic partition results of Chapter 5 the partition functions
of the random contour can be calculated asymptotically and the Bayes posterior required for seg-
mentation can be calculated. Assume the mitochondria are random realizations of the stochastic
differential equation
L ID
i
= (−∇2 + a)ID
i
= Wi
i = (i1, i2) ∈Igj,
(19.72)
with ∇2ID
i1,i2 = ID
i1−1,i2 + ID
i1+1,i2 + ID
i1,i2−1 + ID
i1,i2+1 −4ID
i1,i2,
(19.73)
where a is the constant restoring force, with white Gaussian noise variance σ 2. The Laplacian
induces nearest-neighbor dependence between pixels on the discrete lattice of points. The three
parameters, a, µ and σ, completely specify the model. From Example 8.53 from Chapter 8,
process with 2 × 2 block covariances
i =
 λ11i
λ∗
21i
λ21i
λ22i

=
n−1

k=0
e−j2πik/nKk0.
(19.71)
Hermitian symmetry ¯un−i = ¯u∗
i implies there are only 2n degrees of freedom. The closure constraint on the
simplecurves(g0 = g1, seeEqn. 9.52)removestwomoredegreesoffreedomsincethehighestfrequencydiscrete
Fourier transform coefﬁcient satisﬁes ¯u1,n−1 = j¯u2,n−1. with Hermitian symmetry implying ¯u∗
11 = j¯u∗
21. Thus
the Fourier variables, with closure, require diffusion through R2n−2 for each closed curve.

556
19 JUMP DIFFUSION INFERENCE IN COMPLEX SCENES
Table 19.1 Table showing the maximum-likelihood estimates of the parameters µ, noise variance σ 2, and
restoring force a estimated from the mitochondria data using the algorithm from Example 5.26 of
Chapter 5.
µ Mean Gray Level
σ 2 Variance of White Noise
a Restoring Force
Mitochondria
58.7
847.3
0.62
Background
130.3
2809.9
0.15
the parameters were estimated using maximum-likelihood to be, for the mitochondria µ =
58.7, σ 2 = 847, a = 0.62, and for the background, µ = 130, σ 2 = 2810, a = 0.15.
From the asymptotic partition Theorem 5.17 of Chapter 5 (Example 5.26), the spectrum of
the 2D Laplacian gives the asymptotic version of the spectrum:
log det
1
2 KIgj =
|Igj|
4π2

[−π,π]2 log(2
2

k=1
(1 −cos ωk) + a) dω + O(|∂I(g)|);
(19.74)
the Gaussian random ﬁeld density becomes
p(ID|I) =
m

j=1
(2πσ 2)−|Igj|/2 det−1
2 KIgj e
−1/2σ 2∥LID
gj∥2
∝e−Hm(ID)
where
(19.75)
Hm(ID) =
m

j=1
|Igj|
2

1
4π2

[−π,π]2 log 2πσ 2(
2

k=1
(2 −2 cos ωk) + a) dω

+
m

j=1
1
2σ 2 ∥LID
gj ∥2.
Using the asymptotic maximum-likelihood parameter estimation of Chapter 5, Example 5.26 of
Theorem 5.17, parameters are estimated from sets of hand-labeled mitochondria and background
cytoplasm region types. The parameters are estimated from the micrograph data assuming the
Gaussian model with L = △−a having parameters of mean µ, noise power σ 2, and restoring force
a. The values estimated from the mitochondria data are shown in Table 19.1.
19.6 Jump–Diffusion for Mitochondria
The posterior potential Hm associated with the subspace X(m) = R2nm of m-mitochondria is the
superposition of the Gaussian random ﬁeld potential and the prior on the smoothness of the
contours:
Hm(x(m)) =
m

j=1
|Igj|
2

1
4π2

[−π,π]2 log(2πσ 2
2

k=1
(2 −2 cos ωk) + a) dω

+
m

j=1
1
2σ 2 ∥LID
gj ∥2
m

j=1

log det
1
2 2πK(u(j)) + 1
2

kl
u(j)∗
k
K(u(j))−1
kl u(j)
l

.
(19.76)
The posterior µ(·) is similar to the object recognition posterior, exceptitison X = ∪mR2nm.

19.6 JUMP–DIFFUSION FOR MITOCHONDRIA
557
19.6.1 The jump parameters
The jump process has the family of jump moves J determining which of the jump measures are
non-zero, as well as the properties of connectedness and reversibility. We deﬁne the graph changes
J ∈J to consist of the addition of one object at a time or deletion of one of the existing objects:
Jb
j : R2nm −→R2n(m+1),
Jd
j
: R2nm −→R2n(m−1),
with addition of an object y(1) ∈R2n into the jth location in the list denoted by x(m) ⊕j y(1), and
xdj(m) ∈R2n(m−1) denoting the jth object removed from the list. There are a total of j = 1, . . . , m+1
birth changes allowed for conﬁgurations in R2nm, and m possible death changes, implying that
the transition measures from space R2nm have mass on spaces R2n(m+1), R2n(m−1):
q(x(m), dy(m + 1)) =
m+1

j=1
qb(x(m), y(m + 1))δx(m)(dydj(m + 1))dy(1),
q(x(m), dy(m −1)) =
m

j=1
qd(x(m), y(m −1))δxdj(m)(dy(m −1)).
(19.77)
The total jump intensity is
q(x(m)) =
m+1

j=1

R2n qb(x(m), x(m) ⊕j y(1)) dy(1) +
m

j=1
qd(x(m), xdj(m)).
(19.78)
The diffusion process satisﬁes the stochastic differential Eqn. 19.11 with Xt, t ≥0 ﬂow-
ing through all the real subspaces, the mth one R2nm consisting of m sets of 2n × 1 vectors.
The drifts are variations of the Gibbs posterior energy with respect to the scale, rotation and
translation parameters of each of the m objects. To enforce closure, the stochastic differen-
tial equation is performed in the Fourier transform space along the drifts determined by the
derivatives (∂Hm)/(∂¯u1i), (∂Hm)/(∂¯u2i), i = 1, . . . , n, with closure satisﬁed according to Eqns.
9.52 and 9.53 of Chapter 9. The last two dimensions are constructed from the translation drifts
(∂Hm(x0))/(∂x0), (∂Hm(y0))/(∂y0).
19.6.2 Computing gradients for the drifts
The drifts are computed as in Chapter 6, Section 8.5 for 1-parameter variations of the deformable
models with respect to the translations and rotations. Assume the inside and outside potential
models
H(g(γ )) =

Ig(γ )
E1(x) dx+

I\Ig(γ )
E2(x) dx,
(19.79)
where E1 is the potential associated with the interior Ig(γ ) of the contour, and E2 is the potential
in the exterior I \ Ig(γ ). The gradients are given as in Theorem 8.42. For curves gt(γ ), t ∈[0, 1]

558
19 JUMP DIFFUSION INFERENCE IN COMPLEX SCENES
given by
gt(γ ) =
 u0
u1

+ 2π
 t
0
 u1l
−u2l
u2l
u1l
  −sin 2πl
cos 2πl

dl
(19.80)
with γ ∈{u1l, u2l, x0, y0} and continuous potentials E1, E2, then
∂E(g)
∂γ
=
 1
0
|Jl(γ )|[E1(gl) −E2(gl)] dl,
(19.81)
with Jacobian determinants given by
|Jl(u1t)| = (2π)2

−sin 2πt
∂gyl
∂l
−cos 2πt∂gxl
∂l

1≥t(l),
(19.82)
|Jl(u2t)| = (2π)2

−cos 2πt
∂gyl
∂l
+ sin 2πt∂gxl
∂l

1≥t(l),
(19.83)
|Jl(x0)| = (2π)2 ∂gyl
∂l
,
|Jl(y0)| = −(2π)2 ∂gxl
∂l
(19.84)
and the common bottom row of the Jacobian
∂gxl
∂l
= −sin(2πl)u1l −cos(2πl)u2l ,
∂gyl
∂l
= cos(2πl)u1l −sin(2πl)u2l.
(19.85)
For computation, there are n unique parameter vectors, {u1k, u2k}n
k=1 constant over intervals
l ∈((k −1)/n, k/n). The continuous curvi-linear integral given by the gradient is computed using
the standard discrete approximations of the trapezoid rule.
19.6.3 Jump Diffusion for Mitochondria Detection and Deformation
Figure 19.13 shows results in the electron micrograph data from the jump-diffusion process for
segmentation. Panel 1 depicts the underlying mechanism by which the jump algorithm dis-
covers and places new mitochondria candidates. Detection positions for the placement process
are shown with the amount of posterior probability depicted via brightness. Shown at every
one of the 16 locations depicted are the locations of candidate mitochondria chosen according
to the random selection algorithm choosing based on the relative ratios of the locations. The
brightness indicates the amount of posterior probability associated with that candidate. This
calculation corresponds to computing the posterior probability in conﬁgurations according to
(π(x ⊕y(1)))/(
16
i=1 π(x ⊕y(1))), where π(x ⊕y(1)) has log-probability given by the brightness
points shown in Figure 19.13.
Panel 2 depicts the representation of mitochondria resulting from the jump-diffusion algo-
rithm. Gaussian random ﬁeld models were estimated from the mitochondria and background. The
templates are placed in the scene and deformed to maximize the probability that the represented
regions in the data are realizations of the appropriate texture model. The vertices of the templates
are represented as white squares superimposed on the data.
Table 19.2 quantiﬁes the performance of the pixel-based optimal Bayesian hypothesis testing
(left column) compared with the segmentation via the global shape models. The true labels were
determined by a manual segmentation. Percentages indicate relative frequency of proper pixel
labeling for each model type. Figure 19.14 illustrates the progression of the algorithm through its

19.6 JUMP–DIFFUSION FOR MITOCHONDRIA
559
(1)
(2)
(3)
Figure 19.13 Panel 1 shows the jump detections scored via brightness indicating the amount
of posterior probability. Panel 2 depicts the representation of mitochondria resulting from the
jump-diffusion algorithm. Panel 3 shows a pixel-by-pixel segmentation of the image based on the
optimal Bayes hypothesis test for each pixel under the two models.
Table 19.2 Quantifying the performance of the pixel-based optimal Bayesian hypothesis testing (left
column) compared with the segmentation via the global shape models. The true labels were determined by
segmentation performed manually. Percentages indicate relative frequency of proper pixel labeling for
each model type.
Pixel Based Label (%)
Global Shape Label(%)
Mitochondria
88.9
93.2
Background
63.1
93.7
% of Total Pixels Mislabeled
26.2
6.5
Left
Middle
Right
Figure 19.14 Figure showing a birth of a mitochondria (left column), a death of a mitochondria
(middle column), and a merge of two mitochondria (right column).
jump moves showing an addition of a mitochondria (left column), a removal of a mitochondria
(middle column), and the merging of two mitochondria (right column). The top panel in each
column shows the state of the Markov process at the instant before the jump; the bottom panel
shows the new conﬁguration after the jump has occurred.

560
19 JUMP DIFFUSION INFERENCE IN COMPLEX SCENES
19.6.4 Pseudolikelihood for Deformation
For comparison, one can also examine Besag’s pseudolikelihood method for global segmentation.
This is a modiﬁed Bayes technique that offers an alternative to the purely asymptotic Gaus-
sian random ﬁeld approach outlined previously. Associate with each region type in the image
one of models type θ a MRF ﬁeld {Pθk(ID
i |ID
j , j ∈Ni)}, with its neighborhood system ∪iNi.
The asymptotic Bayes likelihood of Eqn. 19.75 in pseudolikelihood is simply the product of the
Left
Middle
Right
Right
(1)
(2)
(3)
(4)
Figure 19.15 Comparison of segmentations based on 4-gray level MRF pseudolikelihood models
(left two columns) and the Gaussian asymptotic partition function global Bayes model (right two
columns) for two sets of mitochondria (top and bottom rows). Rows 1 and 2 show manual place-
ment of mitochondria templates; rows 3 and 4 show automatic placement by the jump process.
Columns 1 and 3 show image data; columns 2 and 4 show pixel-by-pixel segmentation generated
by the likelihood ratio test (gray mitochondria, black cytoplasm).

19.6 JUMP–DIFFUSION FOR MITOCHONDRIA
561
conditional probabilities:
m

j=1

i∈Zn2
P(ID
i |ID
j , j ∈Ni).
(19.86)
The conditional probabilities on the assumed nearest-neighbor are estimated from the set
of training micrographs and are based on the local average gray-level feature on 256 gray levels
and a 4-gray level texture feature [127, 128]. Estimating the local conditional probabilities for
the organelles is straightforward. The Von–Mises estimator for the conditional probabilities are
computed by counting the relative frequency of occurrence of xi ∈{0, 1, 2, 3} given its neighboring
conﬁguration.47
Figure 19.15 shows a comparison of segmentations based on the 4-gray level pseudolike-
lihood model (left column) and global Bayesian model approximating the asymptotic partition
functions (right column) for two different sets of mitochondria (top and bottom rows). Rows 1 and
2 show results from manual placement of templates in the image; rows 3 and 4 show automatic
placement of templates. Panels 1 and 3 show the vertices of the templates superimposed on the
data image; panels 2 and 4 show the vertices superimposed on a pixel-by-pixel segmentation gen-
erated by the likelihood ratio test. Pixels that are labeled as mitochondria are colored gray, pixels
that are labeled as background are colored black.
Notice how accurately the deformable shapes capture the major mitochondria structures. It
also appears, that the extension to the continuum of gray levels along with the asymptotic partition
approximation (rather than pseudolikelihood) appears to be slightly more robust at the boundaries
of the shapes.
47 We note that the Gaussian potential could also be used with the pseudolikelihood model. However, the
Gaussian model affords the opportunity of exploiting the asymptotic partition approximation to the normalizer
(see Chapter 5, Theorem 5.17, Eqn. 5.75 on asymptotics of the partition function).

This page intentionally left blank 

REFERENCES
[1] U. Grenander. A uniﬁed approach to pattern analysis. Adv. Comput., 10:175–216, 1970.
[2] U. Grenander. Pattern Synthesis: Lectures in Pattern Theory, volume I. Springer-Verlag, New York, 1976.
[3] U. Grenander. Pattern Analysis: Lectures in Pattern Theory, volume II. Springer-Verlag, New York, 1978.
[4] U. Grenander. Regular Structures: Lectures in Pattern Theory, volume III. Springer-Verlag, New York, 1981.
[5] U. Grenander. General Pattern Theory. Oxford University Press, 1994.
[6] C. Shannon. The mathematical theory of communication. Bell Syst. Tech. J., 27:398–403, 1948.
[7] T. Cover and J. Thomas. Elements of Information Theory. John Wiley & Sons, New York, 1991.
[8] D. G. Luenberger. Optimization by Vector Space Methods. John Wiley and Sons, New York, 1969.
[9] U. Grenander. Abstract Inference. John Wiley & Sons, New York, 1981.
[10] H. J. Bierens. Topics in Advanced Econometrics: Estimation, Testing, and Speciﬁcation of Cross-Section and Time
Series Models. Cambridge University Press, 1994.
[11] Kjell A. Doksum Peter J. Bickel. Mathematical statistics: basic ideas and selected topics: Volume I. Prentice
Hall, Englewood Cliffs, New Jersey, 1977.
[12] A. Dempster, N. Laird, and D. Rubin. Maximum likelihood from incomplete data via the em algorithm.
J. R. Stat. Soc. B, 39(1):1–38, 1977.
[13] I. Csiszar and G. Tusnady. Information geometry and alternating minimization procedures. In Statistics
and Decisions, pages 205–237, R. Oldenbourg Berlag. Munchen 1984.
[14] B. R. Musicus. Iterative Algorithms for Optimal Signal Reconstruction and Parameter Identiﬁcation Given Noisy
and Incomplete Data. M.I.T. Thesis, Cambridge, MA, 1982.
[15] D. J. Thomson. Spectrum estimation and harmonic analysis. Proc. IEEE, 70, No. 9:1055–1096, 1982.
[16] B. Friedlander. Lattice methods for spectral estimation. Proc. IEEE, 70, No. 9:990–1017, 1982.
[17] R.KumaresanandD.Tufts. Estimatingtheanglesofarrivalofmultipleplanewaves. IEEETrans. Aerospace
Electr. Syst, AES-19, 1983.
[18] M. I. Miller and D. R. Fuhrmann. Maximum likelihood narrow-band direction ﬁnding and the EM
algorithm. IEEE Trans Acoust. Speech Signal Proces., 38, (9): 560–577, 1990.
[19] E. Becker. High Resolution NMR: Theory and Chemical Applications. Academic Press, New York, 1980.
[20] W. P. Aue, E. Bartholdi, and R. R. Ernst. Two-dimensional spectroscopy application to nuclear magnetic
resonance. J. Chem. Phys., 64:2229, 1976.
[21] A. Bax. Two-Dimensional Nuclear Magnetic Resonance in Liquids. Delft University Press, 1984.
[22] M.I.MillerandA.Greene. Maximum-likelihoodestimationfornuclearmagneticresonancespectroscopy.
J. Magn. Reson., 83:525–548, 1989.
[23] M. I. Miller, S. C. Chen, D. A. Kueﬂer, and D. A. D’Avignon. Maximum-likelihood estimation and the
em algorithm for 2-d nmr spectroscopy. J. Magn. Reson., Series A 104, 1993.
[24] S. C. Chen, T. J. Schaewe, R. S. Teichman, and M. I. Miller. Parallel algorithms for maximum likelihood
nuclear magnetic resonance spectroscopy. J. Magn. Reson., Series A,102:16–23, 1993.
[25] M.I.Miller, D.R.Fuhrmann, J.A.O’Sullivan, andD.L.Snyder. Maximum-likelihoodmethodsfortoeplitz
covariance estimation and radar imaging. In Simon Haykin, editor, Advances in Spectrum Estimation,
pages 145–172. Prentice-Hall, Englewood Cliffs, New Jersey, 1990.
[26] P. Moulin, J. A. O’Sullivan, and D. L. Snyder. Amethod of sieves for multiresolution spectrum estimation
and radar imaging. IEEE Trans. Inform. Theory, 1992.
[27] M. J. Turmon and M. I. Miller. Maximum-likelihood estimation of complex sinusoids and toeplitz
covariances. IEEE Trans. Acoust., Speech Signal Proces., 42(5), 1994.
[28] S. Haykin, V. Kezys, and E. Vertatschitsch. Maximum Likelihood for Angle-of-Arrival Estimation in Multipath,
Simon Haykin, editor pages 123–144. Prentice-Hall, Englewood cliffs, New Jersey 1990.
[29] S. Kay and L. Marple. Jr. Spectrum analysis- a modern perspective. Proc. IEEE, 69, No.11:1380–1418,
1981.
[30] S. Joshi and M. I. Miller. Maximum a Posteriori estimation with Good’s roughness for optical sectioning
microsopy. J. Opt. Soc. Am. A, 10(5):1078–1085, May 1993.
[31] D. L. Snyder, A. M. Hammoud, and R. L. White. Image recovery from data acquired with a charge-
coupled-device camera. J. Opt. Soc. of Am. A, 10(5):1014–1023, 1993.
563

564
REFERENCES
[32] A. D. Lanterman, M. I. Miller, and D. L. Snyder. Implementation of jump-diffusion processes for under-
standing ﬂir scenes. In F.A. Sadjadi, editor, Automatic Object Recognition V, vol. 2485, 309–320, SPIE,
Orlando, FL, 1995.
[33] S. Joshi. MAP intensity estimation with good’s roughness and global shape models for 3D optical sec-
tioning microscopy. Masters Thesis, Department of Electrical Engineering, Sever Institute of Technology,
Washington University, St. Louis, MO., 1993.
[34] M. I. Miller, K. B. Larson, J. E. Safﬁtz, D. L. Snyder, and Jr. L. J. Thomas. Maximum-likelihood estimation
applied to electron-microscope autoradiography. J. Electron Microsc. Tech., 2:611–636, 1985.
[35] D. L. Snyder, Jr. L. J. Thomas, and M. M. Ter-Pogossian. A mathematical model for positron emission
tomography systems having time-of-ﬂight measurements. IEEE Trans. Nucl. Sci., NS-28:3575–3583, 1981.
[36] L. A. Shepp and Y. Vardi. Maximum-likelihood reconstruction for emission tomography. IEEE Trans.
Med. Imaging, MI-1:113–121, 1982.
[37] L. A. Shepp, Y. Vardi, J. B. Ra, S. K. Hilal, and Z. H. Cho. Maximum-likelihood with real data. IEEE Trans.
Nucl. Sci., NS-31:910–913, 1984.
[38] E. Tanaka. Quantitative image reconstruction with weighted backprojection for single photon emission
computed tomography. J. Comput. Assist. Tomogr., 7:692–700, 1983.
[39] M. I. Miller, D. L. Snyder, and T. R. Miller. Maximum likelihood reconstruction for single photon emission
computed tomography. IEEE Trans. Nucl. Sci., NS-32, No. 1:769–778, 1985.
[40] S. Geman, K. M. Manbeck, and D. E. McClure. A comprehensive statistical model for single photon
emission tomography. In R. Challappa and A. Jain, editors, Markov Random Field: Theory and Applications.
Academic Press, 1991.
[41] M. I. Miller and D. L. Snyder. The role of likelihood and entropy in incomplete-data problems: Appli-
cations to estimating point-process intensities and toeplitz constrained covariances. Proc. IEEE, 75,
No.7:892–907, July 1987.
[42] D. L. Snyder and M. I. Miller. Random Point Processes in Time and Space. Springer-Verlag, 1991.
[43] Y. Vardi, L. A. Shepp, and L. Kaufman. A statistical model for positron emission tomograhy. J. Am. Stat.
Assoc., 80:8–35, 1985.
[44] K. Lange and R. Carson. Em reconstruction algorithms for emission and transmission tomography. J.
Comput. Assist. Tomogr., 8(2):306–316, 1984.
[45] D. L. Snyder and D. G. Politte. Image reconstruction from list-mode data in an emission tomography
system having time-of-ﬂight measurements. IEEE Trans. Nucl. Sci., NS-30:1843–1849, 1983.
[46] M.I.Miller, D.L.Snyder, andT.R.Miller. Maximum-likelihoodreconstructionforsingle-photonemission
computed-tomography. IEEE Trans. Nucl. Sci., NS-32:769–778, 1985.
[47] F. S. Gibson and F. Lanni. Diffraction by a circular aperture as a model for three-dimensional optical
microscopy. J. Opt. Soc. Am. A, 6(9):1357–1367, 1989.
[48] C. Preza, J. M. Ollinger, J. G. McNally, and Jr. L. J. Thomas. Point-spread sensitivity analysis for 3-D
ﬂuorescence microscopy. Proceedings of the SPIE/IS&T’s Electronic Imaging: Science and Technology, 1660,
1992.
[49] D. L. Collins, T. M. Peters, W. Dai, and A. C. Evans. Model-based segmentation of individual brain
structures from mri data. In Richard A. Robb, editor, Visualization in Biomedical Computing 1992, SPIE
1808, pages 10–23. 1992.
[50] I. Carlbom, D. Terzopoulos, and K. Harris. Computer-assisted registration, segmentation, and 3d
reconstruction from images of neuronal tissue sections. IEEE Trans. Med. Imaging, 13(2):351–362, 1994.
[51] W. M. Wells, E. I. Grimson, R. Kikinis, and F. A. Jolesz. Adptive segmentation of MRI data. IEEE Trans.
Med. Imaging, 15(4):429–442, 1996.
[52] W.E.L.Grimson, G.J.Ettinger, T.Kapur, M.E.Leventon, W.M.Wells, andR.Kikinis. Utilizingsegmented
MRI data in image-guided surgery. Int. J. Pattern Recogni. Artif. Intell., 11(8):1367–1397, 1997.
[53] M. Joshi, J. Cui, K. Doolittle, S. Joshi, D. C. van Essen, L. Wang, and M. I. Miller. Brain segmentation and
the generation of cortical surfaces. NeuroImage, 9:461–476, 1999.
[54] M. I. Miller, A. Massie, J. T. Ratnanather, K. N. Botteron, and J. G. Csernansky. Bayesian construction of
geometrically based cortical thickness metrics. NeuroImage, 12:676–687, 2000.
[55] Patrick C. Teo, Guillermo Sapiro, and Brain A. Wandell. Creating connected representations of cortical
gray matter for functional MRI visualization. IEEE Trans. Med. Imaging, 16(6):852–863, 1997.
[56] Chenyang Xu, Dzung L. Pham, and Jerry L. Prince. Finding the brain cortex using fuzzy segmentation,
isosurfaces and deformable surface models. In XVth Int. Conf. on Info Proc. in Medical Imaging, June 1997.
[57] A. Chakraborty and J. S. Duncan. Game-theoretic integration for image segmentation. IEEE Trans. Pattern
Anal. Mach. Intell., 21(1):12–30, 1999.
[58] G. Schwartz. Estimating the dimension of a model. Ann. Stat., 6:461–464, 1978.

REFERENCES
565
[59] J. Rissanen. A universal prior for integers and estimation by minimum description length. Ann. Stat.,
11:416–431, 1983.
[60] B. S. Clarke and A. R. Barron. Information-theoretic asymptotics of bayes method. IEEE Trans. Inform.
Theory, 36(3), 1990.
[61] G. Polya and G. Szego. Problems and Theorems in Analysis: Translated by D. Aeppli. Springer-Verlag, 1976.
[62] A. D. Lanterman. Schwarz, Wallace, and Rissanen: Intertwining themes in theories of model selection.
2000.
[63] E. T. Jaynes. Information theory and statistical mechanics. Phys. Rev., 106:620–630, 1957.
[64] E. T. Jaynes. On the rationale of maximum-entropy methods. Proc. IEEE, 70:939–952, 1982.
[65] D. L. Snyder. Random Point Processes. John Wiley Sons, New York, 1975.
[66] J. M. Van Campenhout and T. M. Cover. Maximum entropy and conditional probability. IEEE Trans.
Inform. Theory, IT-27, No. 4:483–489, 1981.
[67] A. P. Dawid. Conditional independence in statistical theory. J. R. Stat. Soc., Ser. B, (1):1–31, 1979.
[68] J. Pearl. From Bayesian Networks to Causal Networks. 1988.
[69] H. R. Keshavan, J. Barnett, D. Geiger, and T. Verma. Introduction to the special section on probabilistic
reasoning. IEEE Trans. Pattern Anal. Mach. Intell., 15, (3):193–195, 1993.
[70] R. G. Cowell, A. P. Dawid, and D. J. Spiegelhalter. Sequential model criticism in probabilistic expert
systems. IEEE Trans. Pattern Anal. Mach. Intell., PAMI-15, (3):209–219, 1993.
[71] A. J. Hartemink, D. K. Gifford, T. S. Jaakola, and R. A. Young. Using graphical models and genomic
expression data to statistically validate models of genetic regulatory networks. In Altman et al., editor,
Paciﬁc Symposium on Biocomputing 2001, volume 6, pages 422–433, World Scientiﬁc Publishing Company,
Singapore, 2001.
[72] T. E. Harris. The Theory of Branching Processes. Springer-Verlag, Berlin, 1963.
[73] K. B. Athreya and P. E. Ney. Branching Processes. Springer-Verlag, Berlin, 1972.
[74] F. R. Gantmacher. The Theory of Matrices. Chelsea Publishing Co., New York, 1959.
[75] Erhan Cinlar. Introduction to Stochastic Processes. Prentice-Hall, Englewood Cliffs, 1975.
[76] C. E. Shannon and W. Weaver. The Mathematical Theory of Communication. University of Illinois Press,
Urbana, IL, 1949.
[77] L. Breiman. Probability. Addison-Wesley Publishing Co., Reading, MA, 1968.
[78] U. Grenander. Probability Measures for Context-Free Languages. Brown University, Providence, R.I., 1967.
[79] M. I. Miller and J. A. O’Sullivan. Entropies and combinatorics of random branching processes and
context-free languagues. IEEE. Trans. Inform. Theory, 38, 4, 1992.
[80] Berger and Ye. Entropic aspects random ﬁelds. IEEE. Trans. Info. Theory, 1990.
[81] N. Chomsky and G. A. Miller. Finite state languages. Inform. Control, 1:91–112, 1958.
[82] N. Chomsky. Three models for the description of language. IRE Trans. Inform. Theory, IT-2:113–124,
1956.
[83] N. Chomsky. On certain formal properties of grammars. Inform. Control, 2:137–167, 1959.
[84] J. E. Hopcroft and J. D. Ullman. Introduction to Automata Theory, Languages, and Computation. Addison-
Wesley, Reading, MA, 1979.
[85] C. Shannon. Prediction and entropy of printed english. Bell Syst. Tech. J., 30, 1951.
[86] R. B. Banerji. Phrase structure languages, ﬁnite machines, and channel capacity. Inform. Control,
6:153–162, 1963.
[87] W. Kuich. On the entropy of context-free languages. Inform. Control, 16:173–200, 1970.
[88] R. Cameron. Source encoding using syntactic information source models. IEEE Trans. Inform. Theory, 34,
4:843–850, 1988.
[89] K. W. Church and R. L. Mercer. Introduction to the special issue on computational linguistics using large
corpora. Comput. Linguist., 19(1):1–24, 1993.
[90] K. Church. A stochastic parts program and noun phrase parser for unrestricted text. In ACL, editor,
Proceedings, Second Conference on Applied Natural Language Processing, pages 136–143, Austin, TX, ACL.
1988.
[91] J. Kupiec. Robust part-of-speech tagging using a hidden markov model. Comput. Speech Lang., 6:225–242,
1992.
[92] P. Brown, V. Della Pietra, P. deSouza, and R. Mercer. Class-based n-gram models of natural language.
Computat Linguistics, 18(4):467–479, 1992.
[93] K. Lari and S. Young. The estimation of stochastic context-free grammars using the inside-outside
algorithm. Comput Speech Language Process., 4:35–56, 1990.
[94] F. Jelinek, J. D. Lafferty, and R. L. Mercer. Basic methods of probabilistic context free grammars. Technical
Report RC 16374, IBM, Yorktown Heights, NY, 1990.

566
REFERENCES
[95] F. Jelinek and R. L. Mercer. Interpolated estimation of Markov source parameters from sparse data. In
Proceedings, Workshop on Pattern Recognition in Practice, pages 381–397, Amsterdam, The Netherlands,
1980.
[96] K. W. Church and W. A. Gale. A comparison of enhanced Good-Turing and deleted estimation methods
for estimating probabilities of English bigrams. Comput Speech Language, 5, 1991.
[97] L. R. Bahl, F. Jelinek, and R. L. Mercer. Amaximum likelihood approach to continuous speech recognition.
IEEE Trans. Pattern Anal Mach. Intell., PAMI-5(2):179–190, 1983.
[98] P. Brown, V. Della Pietra, P. V. deSouza J. Lai, and R. L. Mercer. Class-based n-gram models of natural
language. Comput. Linguist., 18(4):467–479, 1992.
[99] F. Jelinek. Markov source modeling of text generation. In J. K. Skwirzinski, editor, The Impact of Processing
Techniques on Communication. Nijhoff, Dordrecht, 1985.
[100] K. Mark, M. I. Miller, U. Grenander, and S. Abney. Parameter estimation for constrained context-free
language models. In 1992 DARPA Workshop on Speech and Natural Language, February 1992.
[101] K. Mark, M. I. Miller, and U. Grenander. Constrained stochastic language models. In Steven Levinson
and Larry Shepp, editors, Image Models (and their Speech Model Cousins), pages 131–140. Springer-Verlag,
1995.
[102] T. Cover and R. King. A convergent gambling estimate of the entropy of English. IEEE Trans. Inform.
Theory, 24(4):413–421, 1978.
[103] P. Brown, S. Della Pietra, V. Della Pietra, J. Lai, and R. Mercer. An estimate of an upper bound for the
entropy of English. Comput. Linguistic, 18(1):31–40, 1992.
[104] K. E. Mark, M. I. Miller, U. Grenander, and S. Abney. Parameter estimation for constrained context-free
language models. In DARPA Speech and Natural Language Workshop, Harriman, NY, February 1992.
[105] L. Rabiner. A tutorial on hidden Markov models and selected applications in speech recognition. Proc.
IEEE, (2):257–285, 1989.
[106] Andrew J. Viterbi. Error bounds for convolutional codes and an asymptotically optimum decoding
algorithm. IEEE Trans. Inform. Theory, IT-13:260–267, 1967.
[107] D. Forney. The viterbi algorithm. Proc. IEEE, 1973.
[108] L. Baum. An inequality and associated maximization technique in statistical estimation of probabilistic
functions of a Markov process. Inequalities, 3:1–8, 1972.
[109] Proceedings of the 1997 Large Vocabulary Continuous Speech Recognition Workshop, 1997. Available at
http://www.clsp.jhu.edu/ws97.
[110] J. J. Godfrey, E. C. Holliman, and J. McDaniel. Switchboard: Telephone Speech Corpus for Research and
Development. In IEEE Conference on Acoustics, Speech and Signal Processing, volume 1, pages 517–520,
SanFrancisco, CA, 1992.
[111] H. Hermansky. Perceptual linear predictive (PLP) analysis of speech. J. Acoust. Soc. Am., 87(4):1738–1752,
1990.
[112] V. I. Levenshtein. Binary codes capable of correcting deletions, insertions and reversals. Sov. Phys. Dokl.,
10(8):707–710, 1966.
[113] J. Baker. Trainable grammars for speech recognition. In Klatt and Wolf, editors, Speech Communication
Papers for the 97th Meeting of the Acoustical Society of America, pages 547–550, 1979.
[114] J. Kupiec. A trellis-based algorithm for estimating the parameters of a hidden stochastic context-free
grammar. In DARPA Speech and Natural Language Workshop, Asilomar, CA, February 1991.
[115] E. Ising. Beitrag sur theorie des ferromagnetismus. Z. Phys., 31:253–258, 1925.
[116] R. L. Dobrushin. Existence of a phase transition in two and three dimensional Ising models. Theor. Probab.
Appl., 10:193–213, 1965.
[117] R. L. Dobrushin. The gibbsian random ﬁelds for lattices systems with pairwise interactions. Funct. Anal.
Appl., 2:292–301, 1968.
[118] R. L. Dobrushin. The problem of uniqueness of a gibbsian random ﬁeld and the problem of phase
transitions. Funct. Anal. Appl, 2:302–312, 1968.
[119] R. L. Dobrushin. Prescribing a system of random variables by conditional distributions. Theoret. Probab.
Appl., 15:458–486, 1970.
[120] D. Ruelle. Thermodynamic Formalism, Encyclopedia of Mathematics and Its Applications, volume 5. Addison
Wesley, Reading MA, 1978.
[121] J. Besag. Spatial interaction and statistical analysis of lattice systems (with discussion). J. R. Stat. Soc., B,
36:192–326, 1974.
[122] S. Geman and D. Geman. Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of
images. IEEE Trans. Pattern Anal., 6:721–741, November 1984.
[123] R. Kinderman and J. L. Snell. Markov Random Fields and their Applications, volume 1. American
Mathematical Society- Contemporary Mathematics series, 1980.

REFERENCES
567
[124] D. Geman.
Random ﬁelds and inverse problems in imaging. Lect.
Notes Math,
1427:117–193,
1990.
[125] M. I. Miller, B. Roysam, K. Smith, and J. A. O’Sullivan. Representing and computing regular languages
on massively parallel networks. IEEE. Trans. Neural Networ, 2 (No.1):56–72, 1991.
[126] B. Roysam and M. I. Miller. Combining stochastic and syntactic processing with analog computation
methods. Digit. Signal Process.: Rev. J., 2 (2):48–64, 1992.
[127] K. R. Smith and M. I. Miller. Bayesian Inference of Regular Grammar and Markov Source Models, pages
388–398. Morgan-Kaufmann, Palo Alto, 1990.
[128] K. R. Smith and M. I. Miller.ABayesian approach incorporating Rissanen complexity for learning Markov
random ﬁeld texture models. Proceedings of the Intl. Conference on Acoustics, Speech, and Signal Processing,
4: (5) pages 2317–2320, April 1990.
[129] M. B. Averintsev. On a method of describing discrete parameter random ﬁelds. Problemy Peredachi
Informatsii, 6:100–109, 1970.
[130] G. R. Grimmett. A theorem about random ﬁelds. Bull. Lond. Math. Soc., 5:81–84, 1973.
[131] D. Griffeath. Ergodic theorems for graph interactions. Adv. Appl. Probab., 7:179–194, 1975.
[132] C. J. Preston. Gibbs States on Countable Sets. Cambridge University Press, Cambridge, 1974.
[133] D. Mumford. The hierarchical ising model. ??
[134] J. J. Clark and A. L. Yuille. Data Fusion for Sensory Information Processing Systems. Kluwer Academic
Publishers, Boston, 1990.
[135] D. Mumford. The Bayesian Rationale for Energy Functionals. Kluwer Academic, 1994.
[136] A. Blake and Al. Yuille, editors. Active Vision. MIT Press, 1992.
[137] U. Grenander and M. I. Miller. Representations of knowledge in complex systems. J. R. Stat. Soc. B,
56(3):549–603, 1994.
[138] Y. Amit and M. I. Miller. Large deviations for coding Markov chains and gibbs random ﬁelds. IEEE.
Trans. Inform. Theory, 39(1):109, 1993.
[139] Y. Amit and M. I. Miller. Large deviations for the asymptotitcs of ziv-lempel codes for 2-d gibbs ﬁelds.
IEEE Trans. Inform. Theory, 38 (4): 1992.
[140] S. C. Zhu, Y. N. Wu, and D. B. Mumford. Minimax entropy principle and its applications to texture
modeling. Neural Comput., 9(8):1627–1660, 1997.
[141] J. Daugman. Uncertainty relation for resolution in space, spatial frequency, and orientation optimized
by two-dimensional visual cortical ﬁlters. J. Opt. Soc. Am., 2(7), 1985.
[142] R. E. Peierls. On Ising’s ferromagnet model. Proc. Camb. Phil. Soc., 32:477–481, 1936.
[143] G. R. Grimmett and D. R. Stirzaker. Probability and Random Processes, 2nd Ed. Oxford Science Publications,
1995.
[144] In A. Possolo, editor, Spatial Statistics and Imaging, volume Lecture notes - monograph series, 20. Institute
of Mathematical Statistics, Haywood, California, 1991.
[145] M. Cannon and R. Passmore. Incorporation of background clutter effects into performance modeling
of infrared missile seekers. In Proc. of the 6th Annual Ground Target Modeling and Validation Conf., pages
144–151, Houghton, MI, August 1995. U.S. Army TACOM.
[146] Inequalities 2nd Ed. Cambridge University Press, Cambridge Mathematical Library, 1988.
[147] F. Klein. Gesammelte Abhandlungen. Springer, Berlin, 1971.
[148] W. M. Boothby. An Introduction to Differentiable Manifolds and Riemannian Geometry. Academic Press, New
York 1986.
[149] M. Artin. Algebra. Prentice Hall, Englewood Cliffs, NJ, 1991.
[150] U. Grenander, Y. Chow, and D. Keenan. HANDS: A Pattern Theoretic Study of Biological Shapes. Springer-
Verlag, New York, 1990.
[151] I. R. Porteous. Geometric Differentiation. Cambridge University Press, Cambridge, 1994.
[152] B. O’Neill. Elementary Differential Geometry. Academic Press, San Diego, 1966.
[153] N. Khaneja. Statistics and Geometry of Cortical Features. M.S. thesis, Department of Electri-
cal Engineering, Sever Institute of Technology, Washington University, St. Louis, MO, December
1996.
[154] N. Khaneja, U. Grenander, and M. I. Miller. Dynamic programming generation of curves on brain
surfaces. Pattern Anal. Machine Int., 20(10):1260–1264, 1998.
[155] D. J. Felleman and D. C. van Essen. Distributed hierarchical processing in the primate cerebral cortex.
Cereb. Cortex, 1(1):1–47, 1991.
[156] P. M. Thompson, C. Schwartz, R. T. Lin, A. A. Khan, and A. W. Toga. Three-dimensional statistical
analysis of sulcal variability in the human brain. J. Neurosci., 16(13):4261–4274, 1996.
[157] B. Hamann. Curvature approximation for triangulated surfaces. In Computing, pages 139–153. Springer-
Verlag, Berlin, 1993.

568
REFERENCES
[158] S. C. Joshi, J. Wang, M. I. Miller, D. C. van Essen, and U. Grenander. On the differential geometry of the
cortical surface. In R. A. Melter, A. Y. Wu, F. L. Bookstein, and W. D. Green, editors, Proc. SPIE, volume
2573, pages 304–311. San Diego, CA., 1995.
[159] W. E. Lorensen and H. E. Cline. Marching cubes: A high resolution 3D surface construction algorithm.
comput. Graph., 21(4):163–169, 1987.
[160] A. Gueziec and R. Hummel. Exploiting triangulated surface extraction using tetrahedral decomposition.
IEEE Trans Vis. Comput. Graph., 1(4):328–334, 1995.
[161] M. Claudio and S. Roberto. Using marching cubes on small machines. Graph. Model Im. Process., 56:182–3,
1994.
[162] J. W. Haller, A. Banerjee, G. E. Christensen, S. Joshi, M. I. Miller, M. W. Vannier, and J. C. Csernansky.
Three-dimensional hippocampal volumetry by high dimensional transformation of a neuroanatomical
atlas. Radiol., 202(2):504–510, 1997.
[163] J. T. Kent, K. V. Mardia, and J. M. West. Ridge curves and shape analysis. In Monograph, Department
of Statistics, University of Leeds., Leeds LS2 9JT, UK, May, 1996.
[164] J. Ph. Thirion and A. Goudon. Computing the differential characteristics of isointensity surfaces. Comp.
Vis. Image Und., 61(2):190–202, 1995.
[165] D. C. van Essen and J. H. R. Maunsell. Two-dimensional maps of the cerebral cortex. J. Comput. Neurol.,
191:255–281, 1980.
[166] D. C. van Essen and H. Drury. Structural and functional analyses of human cerebral cortex using a
surface-based atlas. J. Neurosci., 17:7079–7102, 1997.
[167] D. C. van Essen, H. Drury, S. Joshi, and M. I. Miller. Functional and structural mapping of human cerebral
cortex: Solutions are in the surfaces. Proc. Natl. Acad. Sci., 95:788–795, 1998.
[168] D. C. van Essen. Pulling strings to build a better brain: A tension-based theory of morphogenesis and
compact wiring in the central nervous sytem. Nature, 385:313–318, 1997.
[169] W. Welker. Why does cerebral cortex ﬁssure and fold. Cerebral Cortex, 83:3–136, 1990.
[170] J. Ph. Thirion and A. Goudon. The 3D marching lines algorithm. Graph. Models Image Process., 58(6):
503–509, 1996.
[171] D. B. Cooper, H. Elliott, F. Cohen, L. Reiss, and P. Symosek. Stochastic boundary estimation and
object recognition. In A. Rosenfeld, editor, Image Modeling, pages 63–94. Academic Press, New York,
1981.
[172] J. L. Elion, S. A. Geman, and K. M. Manbeck. Computer recognition of coronary arteries. J. Am. Coll.
Cardiol., 17(2): 1991.
[173] A. M. Dale, B. Fischl, and M. I. Sereno. Cortical surface-based analysis - i. segmentation and surface
reconstruction. NeuroImage, 9(2):179–194, 1999.
[174] B. Fischl, M. I. Sereno, and A. M. Dale. Cortical surface-based analysis - II: Inﬂation, ﬂattening, and a
surface-based coordinate system. NeuroImage, 9(2):195–207, 1999.
[175] H. A. Drury, D. C. van Essen, C. H. Anderson, C. H. Lee, T. A. Coogan, and J. W. Lewis. Computerized
mappings of the cerebral cortex. a multiresolution ﬂattening method and a surface-based coordinate
system. J. Cognitive Neurosci., 8:1–28, 1996.
[176] B. Wandell, S. Chial, and B. Backus. Visualization and measurement of the cortical surface. J. Cognitive
Neurosci., 12:739–752, 2000.
[177] S. Angenent, S. Haker, A. Tannenbaum, and R. Kikinis. On the Laplace-Beltrami operator and brain
surface ﬂattening. IEEE Trans. Med. Imaging, 18(8):700–711, 1999.
[178] M. K. Hurdal, P. L. Bowers, K. Stephenson, D. L. Sumners, K. Rehm, K. Schaper, and D. A. Rottenberg.
Quasi-conformally ﬂat mapping the human cerebellum. In C. Taylor and A. Colchester, editors, Lecture
Notes in Computer Science, pages 279–286. Springer-Verlag, Berlin, 1999.
[179] M. K. Hurdal, K. Stephenson, P. L. Bowers, D. L. Sumners, and D. A. Rottenberg. Cortical surface
ﬂattening: a quasi-conformal approach using circle packings. Submitted, 2002.
[180] C. R. Collins and K. Stephenson. A circle packing algorithm. Computational Geometry: Theory and
Application, to appear, 2002.
[181] S. G. Krantz. The Riemann mapping theorem. In Handbook of Complex Analysis, pages 86–87. Birkhäuser,
Boston, MA, 1999.
[182] M. K. Hurdal and K. Stephenson. Cortical cartography using the discrete conformal approach of circle
packing. NeuroImage, 2004.
[183] J. T. Ratnanather, P. E. Barta, N. A. Honeycutt, N. Lee, H. M. Morris, A. D. Dziorny, M. K. Hurdal,
G. D. Pearlson, and M. I. Miller. Dynamic programming generation of boundaries of local coordinatized
submanifolds in the neocortex: application to the planum temporale. NeuroImage, 2003.
[184] M. Kass, A. Witkin, and D. Terzopolous. Snakes: Active contour models. Int. J. Comput. Vision, 1(4):
321–331, 1988.

REFERENCES
569
[185] L. H. Staib and J. S. Duncan. Boundary ﬁnding with parametrically deformable models. IEEE Trans.
Pattern Anal. Machine Intell., 14:1061–1075, 1992.
[186] Y. Amit and A. Kong. Graphical templates for image matching. Technical Report: University of Chicago,
(373), 1994.
[187] C. Xu and J. L. Prince. Gradient vector ﬂow: A new external force for snakes. CVRP, November 1997.
[188] M. Vaillant and C. Davatzikos. Finding parametric representations of the cortical sulci using an active
contour model. Med. Image Anal., 1(4):295–315, 1997.
[189] N. Schultz and K. Conradsen. 2d vector-cycle deformable templates. Signal Process., 71(2):141–153, 1998.
[190] A. Garrido and N. P. De la Blanca. Physically-based active shape models: Initialization and optimization.
Pattern Recogn., 31(8):1003–1017, 1998.
[191] C. F. Westin, L. M. Lorigo, O. Faugeras, W. E. L. Grimson, S. Dawson, A. Norbash, and R. Kikinis.
Segmentation by adaptive geodesic active contours. In Medical Image Computing and Computer-Assisted
Intervention - Miccai 2000, volume 1935 of Lecture Notes in Computer Science, pages 266–275. Springer-
Verlag, Berlin, 2000.
[192] T. Chen and D. Metaxas. Image segmentation based on the integration of Markov random ﬁelds and
deformable models. In Medical Image Computing and Computer-Assisted Intervention - Miccai 2000, volume
1935 of Lecture Notes in Computer Science, pages 256–265. Springer-Verlag, Berlin, 2000.
[193] C. Xu, D. L. Pham, and J. L. Prince. Medical image segmentation using deformable models. In
J. M. Fitzpatrick and M. Sonka, editors, SPIE Handbook on Medical Imaging – Volume III: Medical
Image Analysis, pages 129–174. SPIE, Bellingham, WA, 2000.
[194] M. Mignotte and J. Meunier. A multiscale optimization approach for the dynamic contour- based
boundary detection issue. Comput. Med. Imag. Graph, 25(3):265–275, 2001.
[195] A. Yezzi, A. Tsai, and A. Willsky. A fully global approach to image segmentation via coupled curve
evolution equations. J. Vis. Commun. Image R., 13(1-2):195–216, 2002.
[196] D. Terzopoulos and D. Metaxas. Dynamic 3d models with local and global deformations: Deformable
superquadrics. IEEE Trans. Pattern. Anal. Mach. Intell., 13:703–714, 1991.
[197] I. Cohen, L. Cohen, and N. Ayache. Using deformable surfaces to segment 3-d images and infer
differential structures. Comput. Vis. Graph. Image Process., 56(2):242–263, 1992.
[198] M. I. Miller, S. Joshi, D. R. Mafﬁtt, J. G. McNally, and U. Grenander. Mitochondria, membranes and
amoebae: 1,2 and 3 dimensional shape models. In K. Mardia, editor, Statistics and Imaging, volume II.
Carfax Publishing, Abingdon, Oxon., 1994.
[199] T. F. Cootes, C. J. Taylor, D. H. Cooper, and J. Graham. Active shape models—their training and
application. Comput. Vis. Image Und., 61(1):38–59, 1995.
[200] L. H. Staib and J. S. Duncan. Model-based deformable surface ﬁnding for medical images. IEEE Trans.
Med. Imag., 15(5):1–13, 1996.
[201] J. Montagnat and H. Delingette. Volumetric medical images segmentation using shape constrained
deformable models. In Cvrmed-Mrcas’97, volume 1205 Lecture Notes in Computer Science, pages 13–22.
Springer-Verlag, Berlin, 1997.
[202] J. Montagnat, H. Delingette, and N. Ayache. A review of deformable surfaces: topology, geometry and
deformation. Imag. Vis. Comput., 19(14):1023–1040, 2001.
[203] S. Sclaroff and L. F. Liu. Deformable shape detection and description via model-based region grouping.
IEEE Trans. Pattern Anal. Mach. Intell., 23(5):475–489, 2001.
[204] F. Farassat. Introduction to generalized functions with applications in aerodynamics and aeroacoustics.
In NASA Technical Paper, number 3428, 1994.
[205] T. F. Chan and L. A. Vese. Active contours without edges. IEEE Trans. Image Proc., 10(2):266–277, 2001.
[206] G. Strang. Introduction to Applied Mathematics. Wellesley Cambridge Press, New York, 1986.
[207] M. C. Delfour and J.-P. Zolesio. Shapes and geometries: Analysis, differential calculus, and optimization.
Adv. Des. Control SIAM, 4: 2001.
[208] S. Osher and J. A. Sethian. Fronts propagating with curvature-dependent speed: Algorithms based on
Hamilton–Jacobi formulations. J. Comput. Phys., 79:12–49, 1988.
[209] J. N. Tsitsiklis. Efﬁcient algorithm for globally optimal trajectories. IEEE Trans. Automatic Control,
40(9):1528–1538, 1995.
[210] J. A. Sethian. A fast marching level set method for monotonically advancing fronts. Proc. Natl. Acad. Sci.
USA, 93:1591–1595, 1996.
[211] H. K. Zhao, T. Chan, B. Merriman, and S. Osher. A variational level set approach to multiphase motion.
J. Comput. Phys., 127:179–195, 1996.
[212] J. L. Doob. Stochastic Processes. Wiley, New York, 1953.
[213] E. Wong and B. Hajek. Stochastic Processes in Engineering Systems. Springer-Verlag, Berlin, 1984.
[214] I. Karatzas and S. E. Shreve. Brownian Motion and Stochastic Calculus. Springer-Verlag, Berlin, 1987.

570
REFERENCES
[215] M. Reed and B. Simon. Functional Analysis I: Revised and Enlarged Edition. Academic Press, New York,
1980.
[216] J. T. Kent, I. L. Dryden, and C. R. Anderson. Using circulant symmetry to model featureless objects.
Biometrika, 87(3):527–544, 2000.
[217] N. R. Goodman. Statistical analysis based on a certain multivariate complex gaussian distribution. Ann.
Math. Stat., 34(1):152–177, 1963.
[218] G. N. Watson. A Treatise on the Theory of Bessel Functions, 2nd ed. Cambridge University Press, Cambridge,
England, 1966.
[219] A. M. Yaglom. Theory of Stationary Random Processes. Prentice Hall, Englewood Cliffs, NJ 1962.
[220] G. E. Christensen, R. D. Rabbitt, and M. I. Miller. Deformable templates using large deformation
kinematics. IEEE Trans. Image Process., 5(10):1435–1447, 1996.
[221] G. E. Christensen, R. D. Rabbitt, and M. I. Miller. 3D brain mapping using a deformable neuroanatomy.
Phys. Med. Biol., 39:609–618, 1994.
[222] G. E. Christensen, R. D. Rabbitt, and M. I. Miller. Adeformable neuroanatomy textbook based on viscous
ﬂuidmechanics. Proc. 27thAnnualConf. onInformationSciencesandSystems, Baltimore, Maryland, TheJohns
Hopkins University, pages 211–216. March 1993.
[223] F. L. Bookstein. The Measurement of Biological Shape and Shape Change, volume 24. Lecture Notes in
Biomathematics, Springer-Verlag, New York, 1978.
[224] F. L. Bookstein. Biometrics, biomathematics and the morphometric synthesis. Bull. Math. Biol., 58(2):
313–365, 1996.
[225] Y. Amit, U. Grenander, and M. Piccioni. Structural image restoration through deformable templates.
J. Am. Stat. Assoc., 86(414):376–387, 1991.
[226] G. Christensen. Deformable shape models for anatomy. Ph.D. Dissertation, Department of Electrical
Engineering, Sever Institute of Technology, Washington University, St. Louis, MO, Aug 1994.
[227] M. Miller, Ayananshu Banerjee, Gary Christensen, Sarang Joshi, Navin Khaneja, U. Grenander, and
Larissa Matejic. Statistical methods in computational anatomy. Stat. Methods Med. Res., 6:267–299,
1997.
[228] Giovanni Sansone. Orthogonal functions. Interscience Publishers, New York, 1959.
[229] S. Joshi, M. I. Miller, and U. Grenander. On the geometry and shape of brain sub-manifolds. Int. J. Pattern
Recognition Artif. Intell., 11(8), 1997.
[230] R. Courant and D. Hilbert. Methods of Mathematical Physics, volume 1. Interscience Publishers, New York,
1953.
[231] Muge Bakircioglu, Sarang Joshi, and Michael Miller. Landmark matching on brain surfaces via large
deformation diffeomorphisms on the sphere. Proceedings SPIE Medical Imaging 1999: Image Processing,
July 1999.
[232] J. G. McNally, C. Preza, J. A. Conchello, and L. J. Thomas, Jr. Artifacts in computational optical sectioning
microscopy. J. Opt. Soc. Am. A, accepted.
[233] K. R. Castleman. Digital Image Processing. Prentice-Hall, Englewood Cliffe, NJ, 1979.
[234] D. A. Agard. Optical sectioning microscopy. Ann. Rev. Biophys. Bioeng., 13:191–219, 1984.
[235] C. Preza, M. I. Miller, Jr. L. J. Thomas, and J. G. McNally. A regularized linear method for reconstruction
of 3-D microscopic objects from optical sections. J. Opt. Soc., 9(2):219–228, 1992.
[236] A. Pentland and S. Sclaroff. Closed-form solutions for physically based shape modeling and recognition.
IEEE Trans. Pattern Anal. Mach. Intell., 13(7):715–729, 1991.
[237] M. N. O. Sadiku. Numerical Techniques in Electromagnetics. CRC Press, New York, 1992.
[238] P. P. Silvester and R. L. Ferrari. Finite Elements for Electrical Engineering. Cambridge University Press,
Cambridge, 1983.
[239] C. A. Brebbia and J. J. Connor. Fundamentals of Finite Element Technique. Butterworth, London, 1973.
[240] M. Reed and B. Simon. Fourier Analysis, Self-Adjointness. Academic Press, New York 1980.
[241] Y.Amit and M. Piccioni. Anonhomogenous Markov process for the estimation of gaussian random ﬁelds
with non-linear observations. Ann. Probab., 19:1664–1678, 1991.
[242] S. Joshi. Large Deformation Diffeomorphisms and Gaussian Random Fields for Statistical Characterization of
Brain SubManifolds. Sever Institute of Technology, Washington University, St. Louis, MO., 1997.
[243] D. L. Snyder and M. I. Miller. The use of sieves to stabilize images produced with the EM algorithms for
emission tomography. IEEE Trans. Nucl. Sci., NS-32:3864–3872, 1985.
[244] S. Geman and D. E. McClure. Bayesian image analysis: An application to single photon emission
tomography. Proceedings of the American Statistical Association, pages 12–18, 1985.
[245] D. L. Snyder, M. I. Miller, Jr. L. J. Thomas, and D. G. Politte. Noise and edge artifacts in maximum-
likelihood reconstruction for emission tomography. IEEE Trans. Med. Imaging, MI-6(3):228–237,
1987.

REFERENCES
571
[246] M. I. Miller and B. Roysam. Bayesian image reconstruction for emission tomography: Implementation
of the EM algorithm and Good’s roughness prior on massively parallel processors. Proc. of the Natl Acad.
Sci. USA, 88:3223–3227, April 1991.
[247] E. S. Chornoboy, C. J. Chen, M. I. Miller, T. R. Miller, and D. L. Snyder. An evaluation of maximum
likelihood reconstruction for SPECT. IEEE Trans. Med. Imaging, 9(1):99–110, 1990.
[248] T. Hebert and R. Leahy. A generalized EM algorithm for 3-d Bayesian reconstruction from poisson data
using Gibbs priors. IEEE Trans. Med. Imaging, MI-8(2):194–202, 1989.
[249] P. J. Green. On use of the EM algorithm for penalized likelihood estimation. J. R. Stat. Soc., B, 52(3):
443–452, 1990.
[250] K. Lange. Convergence of EM image reconstruction algorithms with Gibbs smoothing. IEEE Trans. Med.
Imaging, MI-9(4):439–446, 1990.
[251] A. W. McCarthy and M. I. Miller. Maximum likelihood SPECT in clinical computation times using
mesh-connected parallel computers. IEEE Trans. Med. Imaging, 10(3):426–436, 1991.
[252] G. T. Herman, A. R. De Pierro, and N. Gai. On methods for maximum a posteriori image reconstruction
with a normal prior. J. Visual Commun. Image Representation, 3(4):316–324, 1992.
[253] C. S. Butler and M. I. Miller. Maximum a posteriori estimation for SPECT using regularization techniques
on massively-parallel computers. IEEE Trans. Med. Imaging, 12(1):84–89, 1993.
[254] M. I. Miller A. D. Lanterman, D. L. Snyder. Extension of good’s roughness for maximum penalized-
likelihood estimation for emission tomography. IEEE Trans. Med. Imaging.
[255] R. A. Tapia and J. R. Thompson. NonParametric Probability Density Estimation. Johns Hopkins University
Press, Baltimore,Md., 1978.
[256] I. J. Good. A non-parametric roughness penalty for probability densities. Nature, London, 229:29–30,
1971.
[257] I. J. Good and R. A. Gaskins. Nonparametric roughness penalties for probability densities. Biometrika,
58(2):255–277, 1971.
[258] I. J. Good and R. A. Gaskins. Density estimation and bump-hunting by the penalized likelihood method
exempliﬁed by scattering and meteorite data. J. Am. Stat. Assoc., 75(369):42–73, 1980.
[259] U. Grenander. Abstract Inference. John Wiley and Sons, New York, 1981.
[260] T. Poggio, V. Torre, and C. Koch. Computational vision and regularization theory. Nature, 317:314–319,
1985.
[261] A. Yuille and N. M. Grzywach. A computational theory for the perception of coherent visual motion.
Nature, 33(5):71–74, 1988.
[262] E. Levitan and G. T. Herman. A maximum a posteriori probability expectation maximization algorithm
for image reconstruction in emission tomography. IEEE Trans. Med. Imaging, 6(3):185–192, 1987.
[263] H. Hart and Z. Liang. Bayesian image processing in two dimensions. IEEE Trans. Med. Imaging, 6(3):
201–208, 1987.
[264] G. T. Herman and D. Odhner. Performance evaluation of an iterative image reconstruction algorithm for
positron emission tomography. IEEE Trans. Med. Imaging, MI-10:336–346, 1991.
[265] L.S.JoyceandW.L.Root. Precisionboundsinsuperresolutionprocessing. J.Opt. Soc. Am. A,1(2):149–168,
1984.
[266] S. J. Lee, A. Rangarajan, and G. Gindi. Bayesian image reconstruction in spect using higher order
mechanical models as priors. IEEE Trans. Med. Imaging, 14(4):669–680, 1995.
[267] F. Guichard and J-M Morel. Image Analysis and Partial Differential Equations. 2001.
[268] Faisal Beg. Variational and computational methods for ﬂows of diffeomorphisms in image matching
and growth in computational anatomy. PhD thesis, The Johns Hopkins University, July 2003.
[269] P. Dupuis, U. Grenander, and M. I. Miller. Variational problems on ﬂows of diffeomorphisms for image
matching. Quart. Appl. Math., 56:587–600, 1998.
[270] A Trouvé. Inﬁnite dimensional group action and pattern recognition. Quar. Appl. Math., 1999.
[271] N. Aronszajn. Theory of reproducing kernels. Trans. Am. Math. Soc., 68(3):337–404, 1950.
[272] F. L. Bookstein. Morphometric Tools for Landmark Data: Geometry and Biology. Cambridge University Press,
1991.
[273] M. I. Miller, G. E. Christensen, Y. Amit, and U. Grenander. Mathematical textbook of deformable
neuroanatomies. Proc. Natl. Acad. Sci. USA, 90(24), 1993.
[274] J. C. Gee and R. K. Bajcsy. Elastic matching: Continuum mechanical and probabilistic analysis. In A. W.
Toga, editor, Brain Warping. Academic Press, New York, 1999.
[275] P. M. Thompson, J. N. Giedd, R. P. Woods, D. MacDonald, A. C. Evans, and A. W. Toga. Growth patterns
in the developing brain detected by using continuum mechanical tensor maps. Nature, 404:190–193, 2000.
[276] A. Trouvé. Action de groupe de dimension inﬁnie et reconnaissance de formes. C. R. Acad. Sci. Paris,
Sér. I, (321):1031–1034, 1995.

572
REFERENCES
[277] A. Trouvé. Diffeomorphisms groups and pattern matching in image analysis. Int. J. Comput. Vision,
28:213–221, 1998.
[278] M. I. Miller, A. Trouvé, and L. Younes. On the metrics and Euler-Lagrange equations of computational
anatomy. Ann. Rev. Biomed. Eng., 4:375–405, 2002.
[279] V. I. Arnold and B. A. Khesin. Topological methods in hydrodynamics. Ann. Rev. Fluid Mech., 24:145–166,
1992.
[280] D. Mumford. Pattern theory and vision. Questions Mathématiques En Traitement Du Signal et de L’Image,
chapter 3, pages 7–13. Institut Henri Poincaré, 1998.
[281] M. I. Miller, A. Trouvé, and L. Younes. Geodesic shooting for computational anatomy. J. Math. Imaging
Vision, 2004. accepted for publication, to appear.
[282] S. Joshi and M. I. Miller. Landmark matching via large deformation diffeomorphisms. IEEE Trans. Image
Process., 9(8):1357–1370, 2000.
[283] Grace Wahba. Spline Models for Observational Data, volume Regional Conference Series in Applied
Mathematics. SIAM, 1990.
[284] M. Vaillant, M. I. Miller, L. Younes, and A. Trouvé. Statistical analysis of diffeomorphisms via geodesic
shooting. NeuroImage, 2004.
[285] M. F. Beg, M. I. Miller, A. Trouvé, and L. Younes. Computing metrics via geodesics on ﬂows of
diffeomorphisms. Int. J. Comp. Vision, 61:139–157, 2004.
[286] M. I. Miller and L. Younes. Group actions, homeomorphisms, and matching: A general framework. Int.
J. Comput. Vision, 41(1/2):61–84, 2001.
[287] D. G. Kendall. Shape manifolds, procrustean metrics and complex projective spaces. Bull. Lond. Math.
Soc., 16:81–121, 1984.
[288] R. Bajcsy, R. Lieberson, and M. Reivich. A computerized system for the elastic matching of deformed
radiographic images to idealized atlas images. J. Comp. Assist. Tomog., 7(4):618–625, 1983.
[289] J.DenglerandM.Schmidt. Thedynamicpyramid-amodelformotionanalysiswithcontrolledcontinuity.
Int. J. Pattern Recognition Arti. Intell., 2(2):275–286, 1988.
[290] R. Dann, J. Hoford, S. Kovacic, M. Reivich, and R. Bajcsy. Evaluation of Elastic Matching Systems for
Anatomic (CT, MR) and Functional (PET) Cerebral Images. J. Comp. Assist. Tomog., 13(4):603–611, 1989.
[291] R. Bajcsy and S. Kovacic. Multiresolution Elastic Matching. Comput. Vision, Graphics, Image Process.,
46:1–21, 1989.
[292] J. Gee, L. L. Briquer, D. R. Haynor, and R. Bajcsy. Matching structural images of the human brain using
statistical and geometrical image features. Visualization in Biomedical Computing, volume SPIE 2359, pages
191–204, 1994.
[293] R. D. Rabbitt, J. A. Weiss, G. E. Christensen, and M. I. Miller. Mapping of hyperelastic deformable
templates using the ﬁnite element method. Presented at the International Symposium on Optical Science,
Engineering and Instrumentation, July 1995.
[294] C. Davatzikos. Spatial transformation and registration of brain images using elastically deformable
models. Comput. Vision Image Understanding, 66(2):207–222, 1997.
[295] G.E.Christensen. Consistentlinear-elastictransformationsforimagematching. InA.KubaandM.Samal,
editors, XVIth International Conference on Information Processing in Medical Imaging, Visegraàd, Hungary,
June 1999.
[296] G. E. Christensen, H. J. Johnson, J. W. Haller, M. W. Vannier, and J. L. Marsh. Synthesizing average 3D
anatomical shapes using deformable templates. In K.M. Hanson, editor, Medical Imaging 1999: Image
Processing, Proceedings of SPIE Vol. 3661, pages 574–582, Feb. 1999.
[297] A. W. Toga, P. M. Thompson, and B. A. Payne. Modeling morphometric changes of the brain during
development. In R. W. Thatcher, G. R. Lyon, and N. Krasnegor, editors, Developmental Neuroimaging:
Mapping the Development of the Brain and Behavior, Academic Press, New York, 1996.
[298] M. I. Miller D. Bitouk and L. Younes. Clutter invariant atr. IEEE Trans. PAMI, 27(5), 2005.
[299] Prism 3.1 User’s Manual. Keweenaw Research Center, Michigan Technological University, 1987.
[300] S. P. Jacobs, J. A. O’Sullivan, M. Faisal A. Srivastava, D. L. Snyder, and M. I. Miller. Automatic target
recognition using sequences of hrr radar reﬂections.
[301] Automatic target recognition using sequences of high resolution radar range-proﬁles. IEEE Trans.
Aerospace Electron. Syst., 2000.
[302] A. Srivastava and U. Grenander. Metrics for target recognition. Proceedings of the SPIE, Applications of
Artiﬁcial Neural Networks in Image Processing III, volume 3307, pages 29–37, January 1998.
[303] Anuj Srivastava, Michael Miller, and Ulf Grenander. Ergodic algorithms on special euclidean groups for
atr. Prog. Syst. Control: Syst. Control Twenty-First Century, 22:327–350, 1997.
[304] G. H. Golub and C. F. Van Loan. Matrix Computations. The Johns Hopkins University Press, Baltimore,
1983.

REFERENCES
573
[305] Cash J. Costello. Medical image registration using the Hilbert-Schmidt estimator. PhD Thesis, Depart-
ment of Biomedical Engineering, The Johns Hopkins University, 2000.
[306] A. Srivastava. Automated target tracking and recognition using jump diffusion processes. MS Thesis,
Washington University, St. Louis, Missouri, December 1993.
[307] M. Loizeaux, A. Srivastava, and M. I. Miller. Estimation of pose and location of ground targets
for atr. Proceedings of the SPIE, Signal Processing, Sensor Fusion and Target Recognition, volume 3720,
1999.
[308] J. A. O’Sullivan, M. D. DeVore, V. Kedia, and M. I. Miller. Automatic target recognition performance
for SAR imagery using conditionally Gaussian model. IEEE Trans. Aero. Electron. Syst., 37(1):91–108,
2001.
[309] M. D. DeVore. Recognition Performance from Synthetic Aperture Radar Imagery Subject to System
Resource Constraints. PhD. thesis, Washington University, 2001.
[310] Jr. T. J. Green and J. H. Shapiro. Target detection performance using 3d laser radar images. Proc. SPIE,
1471:328–341, 1991.
[311] Jr. T. J. Green and J. H. Shapiro. Maximum-likelihood laser radar range proﬁling with the expectation-
maximization algorithm. Opt. Eng., 31:2343–2354, 1992.
[312] D. Park, J. H. Shapiro, and R. W. Reinhold. Performance analyses for peak-detecting laser radars. Proc.
SPIE, 663: 38–56, 1986.
[313] J. Shapiro, B. A. Capron, and R. C. Harney. Imaging and target detection with a heterodyne-reception
optical radar. Appl. Opt., 20(19):3292–3313, 1981.
[314] Jr. T. J. Green and J. H. Shapiro. Detecting objects in 3d laser radar range images. Opt. Eng., 33:865–873,
1994.
[315] Jr. T. J. Green and J. H. Shapiro. Maximum-likelihood laser radar range proﬁling with the expectation-
maximization algorithm. Opt. Eng., 31:2343–2354, 1992.
[316] J. H. Shapiro. Target reﬂectivity theory for coherent laser radars. Appl. Opt., 21:3398–3407, 1982.
[317] A. D. Lanterman, M. I. Miller, D. L. Snyder, and W. J. Miceli. The uniﬁcation of detection, tracking,
and recognition for millimeter wave and infrared sensors. In W.J. Miceli, editor, Radar/Ladar Processing,
volume 2562, pages 150–161. San Diego, CA, 1995.
[318] S. M. Hannon. Detection Processing for Multidimensional Laser Radars. PhD. Thesis, Department of
Electrical Engineering and Computer Science, MIT, 1990.
[319] D. R. Wehner. High Resolution Radar. Artech House, Dedham, MA., 1987.
[320] H. L. Van Trees. Detection, Estimation and Modulation Theory, Part III. John Wiley and Sons, New York,
1971.
[321] D. Bitouk, U. Grenander, M. I. Miller, and P. Tyagi. Fisher information measures for atr in clutter. Proc.
SPIE, 2001.
[322] J. Kostakis, M. L. Cooper, T. J. Green, M. I. Miller, J. A. O’Sullivan, J. H. Shapiro, and D. L. Snyder.
Multispectral active-passive sensor fusion for ground-based target orientation estimation. Proceedings of
the SPIE Conference on Automatic Target Recognition VIII, 3371(3371):500–507, 1998.
[323] J. Kostakis, M. L. Cooper, T. J. Green, M. I. Miller, J. A. O’Sullivan, J. H. Shapiro, and D. L. Snyder.
Multispectral sensor fusion for ground-based ground-based target orientation estimation: Fl(ir, ladar,
hrr). Proceedings of the SPIE Conference on Automatic Target Recognition VIII, 1999.
[324] J. K. Bounds. The infrared airborne radar sensor suite. MIT Research Laboratory of Electronics Technical
Report 610, 1996.
[325] U. Grenander, A. Srivastava, and M. I. Miller. Asymptotic performance analysis of Bayesian target
recognition. IEEE Trans. Inform. Theory, 46(4):1658–1665, 2000.
[326] M. Lindenbaum. Bounds on shape recognition performance. IEEE Trans. Pattern Anal. Mach. Intell.,
17(7):666–680, 1995.
[327] Ben Yen. Target recognition for FLIR and Laser Radar Systems. M.S. Thesis, Department of Electrical
Engineering and Computer Science, MIT, Cambridge, MA, 2000.
[328] H. L. Van Trees. Detection, Estimation and Modulation Theory, Part I. John Wiley and Sons, New York, 1968.
[329] P. Belhumeur and J. P. Hespanha an D. J. Kriegman. Eigenfaces vs. ﬁsherfaces: Recognition using class
speciﬁc linear projection. IEEE Trans. PAMI, 45–58, 1997.
[330] R. Epstein, P. W. Hallinan, and A. L. Yuille. 5 ± eigenimages sufﬁce: An empirical investigation of low-
dimensional lighting models. Proceedings of IEEE Workshop on Physics-Based Modeling in Computer Vision,
to appear 1995.
[331] A. Yuille. Deformable templates for face recognition. J. of Cognitive Neurosci., 3, 1991.
[332] M. L. Cooper, A. D. Lanterman, S. C. Joshi, and M. I. Miller. Representing the variation of thermodynamic
state via principal components analysis. Proceedings of the Third Workshop On Conventional Weapon ATR,
481–490, 1996.

574
REFERENCES
[333] M. L. Cooper, U. Grenander, M. I. Miller, and A. Srivastava. Accommodating geometric and thermody-
namic variability for forward-looking infrared sensors. Proceedings of the SPIE, Algorithms for Synthetic
Aperture Radar IV, 3070, 1997.
[334] M. L. Cooper and M. I. Miller. Information measures for object recognition. Proceedings of the SPIE, 3370,
1998.
[335] M. Cooper. Information Theoretic Methods for Object Recognition and Image Understanding. PhD.
Thesis, Washington University, St. Louis, Missouri, 1999.
[336] A. D. Lanterman. Bayesian inference of thermodynamic state incorporating schwarz-rissanen complexity
for infrared target recognition. Opt. Eng., 39(5):1282–1292, 2000.
[337] Andrew Clinton. Splinetree, http://povplace.addr.com, 2000.
[338] J. Huang and D. Mumford. Statistics of natural images and models. 541–547, 1999.
[339] U. Grenander and A. Srivastava. Probability models for clutter in natural images. IEEE Trans. Pattern
Anal. Mach. Intell., 23(4):424–429, 2001.
[340] M. L. Cooper and M. I. Miller. Information measures for object recognition accommodating signature
variability. IEEE Trans. Inform. Theory, 46(5):1896–1907, 2000.
[341] M. I. Miller U. Grenander and A. Srivastava. Hilbert-schmidt lower bounds for estimators on matrix lie
groups for atr. IEEE Trans. on PAMI, 20(8):790–802, 1998.
[342] A. Srivastava. Inference on Transformation Groups Generating Patterns on Rigid Motions. PhD. Dis-
sertation, Department of Electrical Engineering, Sever Institute of Technology, Washington University,
St. Louis, MO, 1996.
[343] L. Shao, A. Hero, W. Rogers, and N. Clinthorne. The mutual information criterion for spect aperture
evaluation and design. IEEE Trans. Med. Imaging, 8(4):322–36, 1989.
[344] R. G. Gallager. Information Theory and Reliable Communication. John Wiley and Sons, Inc., New York, 1968.
[345] T. Berger. Rate Distortion Theory: A Mathematical Basis for Data Compressions. Prentice Hill, Inc, Englewood
Cliffs, N.J., 1971.
[346] R.E.Blahut. ComputationofChannelCapacityandRate-DistortionFunctions. IEEETrans. Inform. Theory,
IT-18(4):460–473, 1972.
[347] I. Csiszar. On the Computation of Rate-Distortion Functions. IEEE Trans. Information Theory, IT-20:
122–124, 1974.
[348] J. A. O’Sullivan.
Alternating minimization algorithms:
from blahut-arimoto to expectation-
maximization. In A. Vardy, editor, Codes,Curves, and Signals: Common Threads in Communications, pages
173–192. Kluwer Academic, Boston, 1998.
[349] U. Grenander and M. I. Miller. Computational anatomy: An emerging discipline. Quart. Appl. Math.,
56:617–694, 1998.
[350] J. Feldmar, N. Ayache, and F. Betting. 3D-2D projective registration of free-form curves and surfaces.
Comput Vision Image Und.: CVIU, 65(3):403–424, 1997.
[351] A. Bartesaghi and G. Sapiro. Asystem for the generation of curves on 3d brain images. Hum. Brain Mapp.,
14:1–15, 2001.
[352] L. M. Lorigo, O. D. Faugeras, W. E. Grimson, R. Keriven, R. Kikinis, A. Nabavi, and C. F. Westin. Curves:
curve evolution for vessel segmentation. Med. Image Anal., 5(3):195–206, 2001.
[353] M. E. Rettmann, X. Han, C. Xu, and J. L. Prince. Automated sulcal segmentation using watersheds on
the cortical surface. Neuroimage, 15(2):329–344, 2002.
[354] A. Cachia, J. F. Mangin, D. Riviere, F. Kherif, N. Boddaert, A. Andrade, D. Papadopoulos-Orfanos,
J. B. Poline, I. Bloch, M. Zilbovicius, P. Sonigo, F. Brunelle, and J. Regis. A primal sketch of the cortex
mean curvature: a morphogenesis based approach to study the variability of the folding patterns. IEEE
Trans. Med. Imaging, 22(6):754–765, 2003.
[355] A. Dale and M. Sereno. Improved localization of cortical activity by combining EEG and MEG with MRI
cortical surface reconstruction: A linear approach. J. Cogn. Neurosci., 5(2):162–176, 1993.
[356] T. McInerney and D. Terzopoulos. Adynamic ﬁnite element surface model for segmentation and tracking
in multidimensional medical images with application to cardiac 4d image analysis. Comp. Med. Imag.
Graph., 419(1):69–83, 1995.
[357] C. Xu, D. L. Pham, M. E. Rettmann, D. N. Yu, and J. L. Prince. Reconstruction of the human cerebral
cortex from magnetic resonance images. IEEE Trans. Med. Imaging, 18(6):467–480, 1999.
[358] D. L. Pham, C. Xu, and J. L. Prince. Current methods in medical image segmentation. Ann. Rev. Biomed.
Eng., 2:315–337, 2000.
[359] B. Wandell. Computational neuroimaging of human visual cortex. Ann. Rev. Neurosci., 22:145–179,
1999.
[360] X. Zeng, L. H. Staib, R. T. Schultz, and J. S. Duncan. Segmentation and measurement of the cortex from
3-d mr images using coupled-surfaces propagation. IEEE Trans. Med. Imaging, 18(10):927–937, 1999.

REFERENCES
575
[361] P. M. Thompson, R. P. Woods, M. S. Mega, and A. W. Toga. Mathematical/computational challenges in
creating deformable and probabilistic atlases of the human brain. Hum. Brain Mapp., 9(2):81–92, 2000.
[362] D. C. van Essen, J. W. Lewis, H. A. Drury, N. A. Hadjikhani, R. L. Tootell, M. Bakircioglu, and M. I. Miller.
Mapping visual cortex in monkeys and humans using surface-based atlases. Vision Res., 41:1359–1378,
2001.
[363] X. Gu, Y. Wang, T. F. Chan, P. M. Thompson, and S-T Yau. Genus zero surface conformal mapping and
its application to brain surface mapping. IEEE Trans. Med. Imaging, 23, 2004.
[364] D. Tosun, M. E. Rettmann, X. Han, X. Tao, C. Xu, S. M. Resnick, D. Pham, and J. L. Prince. Cortical surface
segmentation and mapping. NeuroImage, 2004.
[365] D. C. Van Essen. Surface-based approaches to spatial localization and registration in primate cerebral
cortex. NeuroImage, 2004.
[366] J-F. Mangin, D. Riviere, A. Cachia, E. Duchesnay, Y. Cointepas, D. Papadopoulus-Orfanos, T. Ochiai, and
J. Regis. A framework for studying cortical folding patterns. NeuroImage, 2004.
[367] D. L. Collins, C. J. Holmes, Terrence M. Peters, and A. C. Evans. Automatic 3-d model-based
neuroanatomical segmentation. Hum. Brain Mapp., 3:190–208, 1995.
[368] R. Kikinis, M. E. Shenton, D. V. Iosifescu, R. W. McCarley, P. Saiviroonporn, H. H. Hokama, A. Robatino,
D. Metcalf, C. Wible, C. M. Portas, R. M. Donnino, and F. A. Jolesz. A digital brain atlas for surgical
planning, model-driven segmentation, and teaching. IEEE Trans Visual Comput Graph, 2(3):232–241,
1996.
[369] P.Teo, G.Sapiro, andB.Wandell. Creatingconnectedrepresentationsofcorticalgraymatterforfunctional
mri visualization. IEEE Trans. Med. Imaging, 16:852–863, 1997.
[370] B. Crespo-Facorro, J. J. Kim, N. C. Andreasen, R. Spinks, D. S. O’Leary, H. J. Bockholt, G. Harris, and
V.A. Magnotta. Cerebral cortex: a topographic segmentation method using magnetic resonance imaging.
Psychiat. Res.: NeuroImaging, 100:97–126, 2000.
[371] J. T. Ratnanather, K. N. Botteron, T. Nishino, A. B. Massie, R. M. Lal, S. G. Patel, S. Peddi, R. D. Todd, and
M. I. Miller. Validating cortical surface analysis of medical prefrontal cortex. NeuroImage, 14:1058–1069,
2001.
[372] D. W. Shattuck, S. R. Sandor-Leahy, K.A. Schaper, D.A. Rottenberg, and R. M. Leahy. Magnetic resonance
image tissue classiﬁcation using a partial volume model. NeuroImage, 13:856–876, 2001.
[373] B. Fischl, D. H. Salat, E. Busa, M. Albert, M. Dieterich, C. Haselgrove, A. van der Kouwe, R. Killanay,
D. Kennedy, S. Klaveness, A. Montillo, N. Makris, B. Rosen, and A. M. Dale. Whole brain segmentation:
Automated labeling of neuroanatomical structures in the human brain. Neuron, 33:341–355, 2002.
[374] X. Han, C. Xu, U. Braga-Neto, and J. L. Prince. Topology correction in brain cortex segmentation using
a multiscale, graph-based algorithm. IEEE Trans. Med. Imaging, 21(2):109–121, 2002.
[375] S. O. Dumoulin, R. D. Hoge, Jr. C. L. Baker, R. F. Hess, R. L. Achtman, and A. C. Evans. Automatic
volumetric segmentation of human visual retinotopic cortex. NeuroImage, 18(3):576–587, 2003.
[376] B. Fischl, A. van der Kouwe, C. Destrieux, E. Halgren, F. Segonne, D. H. Salat, E. Busa, L. J. Seidman,
J. Goldstein, D. Kennedy, V. Caviness, N. Makris, B. Rosen, and A. M. Dale. Automatically parcellating
the human cerebral cortex. Cereb. Cortex, 14(1):11–22, 2004.
[377] B. Fischl, D. H. Salat, A. J. W. ven der Kouwe, F. Segonne, and A. M. Dale. Sequence-independent
segmentation of magnetic resonance images. NeuroImage, 2004.
[378] A. Pitiot, H. Delingette, P. M. Thompson, and N.Ayache. Expert knowledge guided segmentation system
for brain mri. NeuroImage, 2004.
[379] F. L. Bookstein. Landmark methods for forms without landmarks: morphometrics of group differences
in outline shape. Med. Image Anal., 1:225–243, 1996.
[380] F. Bookstein. Shape and the information in medical images: A decade of the morphometric synthesis.
Comput. Vision Image Und., 66:97118, 1997.
[381] P. R. Andresen, F. L. Bookstein, K. Conradsen, B. K. Ersbll, J. L. Marsh, and S. Kreiborg. Surface-bounded
growth modeling applied to human mandibles. IEEE Trans. Med. Imaging, 19:1053–1063, 2000.
[382] J. C. Gee. On matching brain volumes. Pattern Recogn., 32:99–111, 1999.
[383] J. C. Gee and D. R. Haynor. Numerical methods for high-dimensional warps. In A. W. Toga, editor, Brain
Warping, pages 101–113. Academic Press, San Diego, CA, 1999.
[384] B. Avants and J. C. Gee. Geodesic estimation for large deformation anatomical shape averaging and
interpolation. NeuroImage, 23(Supplement 1):S139–S150, 2004.
[385] A. C. Evans, W. Dai, L. Collins, P. Neelin, and S. Marret. Warping of a computerized 3-d atlas to match
brain image volumes for quantitative neuroanatomical and functional analysis. Image Process., 1445:
236–246, 1991.
[386] K. J. Friston, C. D. Frith, P. F. Liddle, and R. S. J. Frackowiak. Plastic transformation of pet images. J.
Comp. Assist. Tomog., 15:634–639, 1991.

576
REFERENCES
[387] A. W. Toga, P. K. Banerjee, and B. A. Payne. Brain warping and averaging. J. Cereb. Blood Flow Metab.,
11:S560, 1991.
[388] D. L. Collins, P. Neelin, T. M. Peters, and A. C. Evans. Automatic 3d intersubject registration of mr
volumetric data in standardized talairach space. J. Comp. Assist. Tomog., 192–205, 1994.
[389] K. J. Friston, J. Ashburner, C. D. Frith, J.-B. Poline, J. D. Heather, Liddle, and R. S. J. Frackowiak. Spatial
registration and normalization of images. Hum. Brain Mapp., 2:165–189, 1995.
[390] G. E. Christensen, A. A. Kane, J. L. Marsh, and M. W. Vannier. Synthesis of an individualized cranial
atlas with dysmorphic shape. IEEE Proc. Math. Methods Biomed. Image Anal., in press, 1996.
[391] C. Davatzikos. Spatial normalization of 3-d brain images using deformable models. J. Comp. Assist.
Tomog., 20(4):656,665, 1996.
[392] S. K. Kyriacou, C. Davatzikos, S. J. Zinreich, and R. N. Bryan. Modeling brain pathology and tis-
sue deformation using ﬁnite element based nonlinear elastic models. IEEE Trans. Med. Imaging, 1997.
submitted.
[393] D. V. Iosifescu, M. E. Shenton, S. K. Warﬁeld, R. Kikinis, J. Dengler, F. A. Jolesz, and R. W. McCarley. An
automated registration algorithm for measuring mri subcortical brain structures. NeuroImage, 6:13–25,
1997.
[394] P. M. Thompson and A. W. Toga. Anatomically driven strategies for high-dimensional brain image
warping and pathology detection. In A. W. Toga, editor, Brain Warping, pages 311–336. Academic Press,
San Diego, CA, 1999.
[395] B. Fischl, M. I. Sereno, R. B. H. Tootell, and A. M. Dale. High-resolution intersubject averaging and a
coordinate system for the cortical surface. Hum. Brain Mapp., 8(4):272–284, 1999.
[396] S. Warﬁeld, A. Robatino, J. Dengler, F. Jolesz, and R. Kikinis. Nonlinear registration and template-driven
segmentation. In Brain Warping, pages 67–84. Academic Press, New York 1999.
[397] N. Hata, A. Nabavi, W. M. Wells III, S. K. Warﬁeld, R. Kikinis, P. McL. Black, and F. A. Jolesz. Three-
dimensional optical ﬂow method for measurement of volumetric brain deformation from intraoperative
mr images. 24(4):531–538, 2000.
[398] A. Guimond, A. Roche, N. Ayache, and J. Meunier. Multimodal Brain Warping Using the Demons
Algorithm and Adaptative Intensity Corrections. IEEE Trans. Med. Imaging, 20(1):58–69, 2001.
[399] P. Cachier, J.-F. Mangin, X. Pennec, D. Rivière, D. Papadopoulos-Orfanos, J. Régis, and N. Ayache.
Multisubject Non-Rigid Registration of Brain MRI using Intensity and Geometric Features. In W. J.
Niessen and M. A. Viergever, editors, 4th Int. Conf. on Medical Image Computing and Computer-Assisted
Intervention (MICCAI’01), volume 2208 of LNCS, Utrecht, The Netherlands, pages 734–742, October
2001.
[400] A. Pitiot, A. W. Toga, and P. M. Thompson. Adaptive elastic segmentation of brain mri via shape-model-
guided evolutionary programming. IEEE Trans. Med. Imaging, 21(8):910–923, 2002.
[401] G. E. Christensen, S. C. Joshi, and M. I. Miller. Volumetric transformation of brain anatomy. IEEE Trans.
Med. Imaging, 16(6):864–877, 1997.
[402] L. Younes. Computable elastic distances between shapes. SIAM J. Appl. Math., 1998.
[403] J. Glaunes, A. Trouvé, and L. Younes. Diffeomorphic matching of distributions: A new approach for
unlabelled point-sets and sub-manifolds matching. Proceeding of CVPR, 2004.
[404] A. W. Toga, P. Thompson, and B. A. Payne. Development Neuroimaging: Mapping the Development of Brain
and Behavior, chapter Modeling Morphometric Changes of the Brain During Development. Academic Press,
1996.
[405] T. Paus, A. Zijdenbos, K. Worsley, D. L. Collins, J. Blumenthal, J. N. Giedd, J. L. Rapoport, and A. C.
Evans. Structural maturation of neural pathways in children and adolescents: in vivo study. Science,
283:1908–1911, 1999.
[406] E. R. Sowell, B. S. Peterson, P. M. Thompson, S. E. Welcome, A. L. Henkenius, and A. W. Toga. Mapping
cortical change across the human life span. Nat. Neurosci, 6(3):309–315, 2003.
[407] S. C. Joshi, B. Davis, M. Jomier, and G. Gerig. Unbiased diffeomorphic atlas construction for
computational anatomy. NeuroImage, 2004.
[408] J. G. Csernansky, L. Wang, S. Joshi, J. P. Miller, M. Gado, D. Kido, D. McKeel, J. C. Morris, and M. I. Miller.
Early dat is distinguished from aging by high dimensional mapping of the hippocampus. Neurology, 55,
2000.
[409] C. D. Good, I. S. Johnsrude, J. Ashburner, R. N. A. Henson, K. J. Friston, and R. S. J. Frackowiak. A
voxel-based morphometric study of ageing in 465 normal adult human brains. NeuroImage, 14:21–36,
2001.
[410] J. G. Csernansky, L. Wang, J. Swank, J. Philip Miller, M. Gado, D. Kido, D. McKeel, M. I. Miller, and
J. C. Morris. Hippocampal volume and shape predicts cognitive decline in the elderly. Am. J. Psychiat.,
2002.

REFERENCES
577
[411] C. D. Good, R. I. Scahill, N. C. Fox, J. Ashburner, K. J. Friston, D. Chan, W. R. Crum, M. N. Rossor, and
R. S. Frackowiak. Automatic differentiation of anatomical patterns in the human brain: validation with
studies of degenerative dementias. Neuroimage, 17(1):29–46, 2002.
[412] J. Gee, L. Ding, Z. Xie, M. Lin, C. DeVita, and M. Grossman. Alzheimer’s disease and frontotemporal
dementia exhibit distinct atrophy-behavior correlates: a computer-assisted imaging study. Acad. Radiol,
10(12):1392–1401, 2003.
[413] P. M. Thompson, K. M. Hayashi, G. de Zubicaray, A. L. Janke, S. E. Rose, J. Semple, D. Herman,
M. S. Hong, S. S. Dittmer, D. M. Doddrell, and A. W. Toga. Dynamics of gray matter loss in alzheimer’s
disease. J. Neurosci., 23(3):994–1005, 2003.
[414] Lei Wang, Jeffrey S. Swank, Irena E. Glick, Mokhtar H. Gado, Michael I. Miller, John C. Morris, and
John G. Csernansky. Changes in hippocampal volume and shape across time distinguish dementia of
the alzheimer type from healthy aging. NeuroImage, 2003. to appear.
[415] M. I. Miller, M. Hosakere, A. R. Barker, C. E. Priebe, N. Lee, J. T. Ratnanather, L. Wang, M. Gado, J. C.
Morris, and J. G. Csernansky. Labeled cortical mantle distance maps of the cingulate quantify differences
between dementia of the alzheimer type and healthy aging. Proc. Natl. Acad. Sci. USA, 100:15172–15177,
2003.
[416] M. Ballmaier, E. R. Sowell, P. M. Thompson,A. Kumar, K. L. Narr, H. Lavretsky, S. E. Welcome, H. DeLuca,
andA. W. Toga. Mapping brain size and cortical gray matter changes in elderly depression. Biol. Psychiat.,
55(4):382–389, 2004.
[417] M. Ballmaier, A. W. Toga, R. E. Blanton, E. R. Sowell, H. Lavretsky, J. Peterson, D. Pham, and A. Kumar.
Anterior cingulate, gyrus rectus, and orbitofrontal abnormalities in elderly depressed patients: an mri-
based parcellation of the prefrontal cortex. Am. J. Psychiat., 161(1):99–108, 2004.
[418] M. E. Shenton, R. Kikanis, F. A. Jolesz, S. D. Pollak, M. LeMay, C. G. Wible, H. Hokama, J. Martin,
D. Metcalf, M. Coleman, and R. W. McCarley. Abnormalities of the left temporal lobe and thought
disorder in schizophrenia. a qualitative magnetic resonance imaging study. New Engl. J. Med., 327(9):
604–612, 1992.
[419] J. L. Rapoport, J. N. Giedd, J. Blumenthal, S. Hamburger, N. Jeffries, T. Fernandez, R. Nicolson, J. Bedwell,
M. Lenane, A. Zijdenbos, T. Paus, and A. Evans. Progressive cortical change during adolescence in
childhood-onset schizophrenia. a longitudinal magnetic resonance imaging study. Arch. Gen. Psychiat.,
56:649–654, 1999.
[420] R. W. McCarley, M. E. Shenton, B. F. O’Donnell, S. F. Faux, R. Kikinis, P. G. Nestor, and F. A. Jolesz. Audi-
tory p300 abnormalities and left posterior superior temporal gyrus volume reduction in schizophrenia.
Arch. Gen. Psychiat., 50:190–197, 1993.
[421] P. G. Nestor, M. E. Shenton, R. W. McCarley, J. Haimsen, S. Smith, B. O’Donnell, M. Kimble, R. Kikinis,
and F. A. Jolesz. Neuropsychological correlates of mri temporal lobe abnormalities in schizophrenia.
Am. J. Psychiat., 150:1849–1855, 1993.
[422] John C. Csernansky, Sarang Joshi, Lei Wang, Mokhtar Gado, J. Philip Miller, Ulf Grenander, and Michael
I. Miller. Hippocampal morphometry in schizophrenia by high dimensional brain mapping. Proc. Natl
Acad. Sci., 95:11406–11411, 1998.
[423] Lei Wang, Sarang C. Joshi, Michael I. Miller, and John G. Csernansky. Statistical analysis of hippocampal
asymmetry in schizophrenia. NeuroImage, 14:531–545, 2001.
[424] K. L. Narr, P. M. Thompson, T. Sharma, J. Moussai, R. Blanton, B. Anvar, A. Edris, R. Krupp, J. Rayman,
M. Khaledy, and A. W. Toga. Three–dimensional mapping of temporo–limbic regions and the lateral
ventricles in schizophrenia: gender effects. Biol. Psychiat., 50:84–97, 2001.
[425] P. M. Thompson, C. Vidal, J. N. Giedd, P. Gochman, J. Blumenthal, R. Nicolson, A. W. Toga, and
J. L. Rapoport. Mapping adolescent brain change reveals dynamic wave of accelerated gray matter
loss in very early-onset schizophrenia. Proc. Natl Acad. Sci., 98:11650–11655, 2001.
[426] T. D. Cannon, P. M. Thompson, T. G. van Erp, A. W. Toga, V. P. Poutanen, M. Huttunen, J. Lonnqvist,
C. G. Standerskjold-Nordenstam, K. L. Narr, M. Khaledy, C. I. Zoumalan, R. Dail, and J. Kaprio. Cortex
mapping reveals regionally speciﬁc patterns of genetic and disease-speciﬁc gray-matter deﬁcits in twins
discordant for schizophrenia. Proc. Natl Acad. Sci. USA, 99(5):3228–3233, 2002.
[427] G. R. Kuperberg, M. R. Broome, P. K. McGuire, A. S. David, M. Eddy, F. Ozawa, D. Goff, W. C. West,
S. C. Williams, A. J. van der Kouwe, D. H. Salat, A. M. Dale, and B. Fischl. Regionally localized thinning
of the cerebral cortex in schizophrenia. Arch. Gen. Psychiat., 60(9):878–888, 2003.
[428] R.Tepest, L.Wang, M.I.Miller, P.Falkai, andJ.G.Csernansky. Hippocampaldeformitiesintheunaffected
siblings of schizophrenia subjects. Biol. Psychiat., 54(11):1234–1240, 2003.
[429] J. G. Csernansky, M. K. Schindler, N. R. Spliner, L. Wang, M. Gado, L. D. Selemon, D. Rastogi-Cruz,
P. A. Posener, and M. I. Miller. Abnormalities of thalamic volume and shape in schizophrenia. Am. J.
Psychiat., (161):896–902, 2004.

578
REFERENCES
[430] H. E. Hulshoff Pol, H. G. Schnack, R. C. Mandl, W. Cahn, D. L. Collins, A. C. Evans, and R. S. Kahn. Focal
white matter density changes in schizophrenia: reduced inter-hemispheric connectivity. Neuroimage,
21(1):27–35, 2004.
[431] K. L. Narr, R. M. Bilder, S. Kim, P. M. Thompson, P. Szeszko, D. Robinson, E. Luders, and A. W. Toga.
Abnormal gyral complexity in ﬁrst-episode schizophrenia. Biol. Psychiat., 55(8):859–867, 2004.
[432] K. L. Narr, P. M. Thompson, P. Szeszko, D. Robinson, S. Jang, R. P. Woods, S. Kim, K. M. Hayashi,
D. Asunction, A. W. Toga, and R. M. Bilder. Regional speciﬁcity of hippocampal volume reductions in
ﬁrst-episode schizophrenia. Neuroimage, 21(4):1563–1575, 2004.
[433] D. H. Salat, R. L. Buckner, A. Z. Snyder, D. N. Greve, R. S. Desikan, E. Busa, J. C. Morris, A. M. Dale, and
B. Fischl. Thinning of the cerebral cortex in aging. Cereb. Cortex, 2004.
[434] E. R. Sowell, P. M. Thompson, S. E. Welcome, A. L. Henkenius, A. W. Toga, and B. S. Peterson. Cor-
tical abnormalities in children and adolescents with attention-deﬁcit hyperactivity disorder. Lancet,
362(9397):1699–1707, 2003.
[435] J. G. Levitt, R. E. Blanton, S. Smalley, P. M. Thompson, D. Guthrie, J. T. McCracken, T. Sadoun,
L. Heinichen, and A. W. Toga. Cortical sulcal maps in autism. Cereb. Cortex, 13(7):728–735, 2003.
[436] C. H. Salmond, M. de Haan, K. J. Friston, D. G. Gadian, and F. Vargha-Khadem. Investigating individual
differences in brain abnormalities in autism. Phil. Trans. R. Soc. Lond. B Biol. Sci., 358(1430):405–413, 2003.
[437] J. A. Posener, L. Wang, J. L. Price, M. H. Gado, M. A. Province, M. I. Miller, C. M. Babb, and
J. G. Csernansky. High-dimensional mapping of the hippocampus in depression. Am. J. Psychiat.,
160(1):83–89, 2003.
[438] L. C. Wiegand, S. K. Warﬁeld, J. J. Levitt, Y. Hirayasu, D. F. Salisbury, S. Heckers, C. C. Dickey, R. Kikinis,
F. A. Jolesz, R. W. McCarley, and M. E. Shenton. Prefrontal cortical thickness in ﬁrst-episode psychosis:
a magnetic resonance imaging study. Biol. Psychiat., 55(2):131–140, 2004.
[439] E. R. Sowell, P. M. Thompson, S. N. Mattson, K. D. Tessner, T. L. Jernigan, E. P. Riley, and A. W. Toga.
Regional brain shape abnormalities persist into adolescence after heavy prenatal alcohol exposure. Cereb.
Cortex, 12(8):856–865, 2002.
[440] H. D. Rosas, A. K. Liu, S. Hersch, M. Glessner, R. J. Ferrante, D. H. Salat, A. van der Kouwe, B. G. Jenkins,
A. M. Dale, and B. Fischl. Regional and progressive thinning of the cortical ribbon in huntington’s disease.
Neurology, 58(5):695–701, 2002.
[441] J-P. Thirion, S. Prima, G. Subsol, and N. Roberts. Statistical analysis of normal and abnormal dissymmetry
in volumetric medical images. Med. Image Anal., 4:111–121, 2000.
[442] David Rey, Gérard Subsol, Hervé Delingette, and Nicholas Ayache. Automatic detection and segmen-
tation of evolving processes in 3D medical images: Application to multiple sclerosis. Med. Image Anal.,
2002. to appear.
[443] M. Sailer, B. Fischl, D. Salat, C. Tempelmann, M. A. Schonfeld, E. Busa, N. Bodammer, H. J. Heinze, and
A. Dale. Focal thinning of the cerebral cortex in multiple sclerosis. Brain, 126(8):1734–44, 2003.
[444] J. G. Csernansky, L. Wang, S. C. Joshi, J. T. Ratnanather, and M. I. Miller. Computational anatomy
and neuropsychiatric disease: Probabilistic assessment of variation and statistical inference of group
differences, hemispheric asymmetry, and time-dependent change. NeuroImage, 2004.
[445] G. E. Christensen, R. D. Rabbitt, M. I. Miller, S. C. Joshi, U. Grenander, and T. A. Coogan. Topological
properties of smooth anatomic maps. In Information Processing in Medical Imaging, pages 101–112. Kluwer
Academic Publishers, 1995.
[446] John W. Haller, Gary E. Christensen, Sarang Joshi, John W. Newcomer, Michael I. Miller, John
C. Csernansky, and Michael W. Vannier. Hippocampal MR imaging morphometry by means of general
pattern matching. Radiology, 199(3):787–791, 1996.
[447] M. Corbetta, E. Akbudak, T. E. Conturo, A. Z. Snyder, J. M. Ollinger, H. A. Drury, M. R. Linenweber,
S. E. Petersen, M. E. Raichle, D. C. Van Essen, and G. L. Shulman. Acommon network of functional areas
for attention and eye movements. Neuron, 21(4)(1):761–773, 1998.
[448] H. Drury, D. C. van Essen, and J. W. Lewis. Towards probabilistic surface-based atlases of primate
cerebral cortex. J. Neurosci., 1999.
[449] Y. Cao, M. I. Miller, R. L. Winslow, and L. Younes. Large deformation metric mapping of vector ﬁelds.
2004. in review.
[450] F. M. Beg, P. A. Helm, Elliot McVeigh, M. I. Miller, and R. Winslow. Computational cardiac anatomy
using magnetic resonance imaging. Magn. Reson. Med. (MR), 52(5):1167–1174, 2004.
[451] E. T. Ahrens, P. T. Narasimhan, T. Nakada, and R. E. Jacobs. Small animal neuroimaging using magnetic
resonance microscopy. Prog. Nucl. Magn. Reson. Spectrosc., 40:275–306, 2002.
[452] J. Zhang, L. J. Richards, P. Yarowsky, H. Huang, P. C. van Zijl, and S. Mori. Three-dimensional anatom-
ical characterization of the developing mouse brain by diffusion tensor microimaging. Neuroimage,
20(3):1639–1648, 1053-8119, 2003.

REFERENCES
579
[453] J. Zhang, L. Richards, P. Yarowsky, M. I. Miller, P. van Zijl, and S. Mori. High resolution diffusion tensor
imaging in study of postnatal mouse brain development. In ISMRM 11th Scientiﬁc Meeting, volume 1,
Toronto, Canada, 2003.
[454] H. Le and A. Kume. The fréchet mean shape and the shape of means. Adv. Appl. Probab. (SGSA), 32:
101–113, 2000.
[455] R. Bhattacharya and V. Patrangenaru. Nonparametric estimation of location and dispersion on
riemannian manifolds. J. Stat. Plan. Infer., 108:23–25, 2002.
[456] K.A. Gallivan, A. Srivastava, and Xiuwen L. Efﬁcient algorithms for inferences on grassmann manifolds.
Proceedings of IEEE Conference on Statistical Signal Processing, pages 315–318. IEEE, Piscataway NJ; 2003.
[457] P. T. Fletcher, C. Lu, and S. Joshi. Statistics of shape via principal geodesic analysis on lie groups.
Proceedings of CVPR, pages 95–101. IEEE, Piscataway NJ; 2003.
[458] F. L. Bookstein. Morphometric Tools for Landmark Data: Geometry and Biology. Cambridge University Press,
Cambridge, 1993.
[459] M. B. Stegmann. Analysis and segmentation of face images using point annotations and linear sub-
space techniques. Technical report, Informatics and Mathematical Modelling, Technical University of
Denmark, DTU, August 2002.
[460] V. Blanz and T. A. Vetter. Morphable model for the synthesis of 3D faces. SIGGRAPH ’99 Conference
Proceedings, pages 187–194, 1999.
[461] T. W. Anderson. An Introduction to Multivariate Statistical Analysis. Wiley, Newyork, 1958.
[462] I. I. Gihman and A. V. Skorohod. Introduction to the Theory of Random Processes. Saunders, Philadelphia,
1965.
[463] S. N. Ethier and T. G. Kurtz. Markov Processes. Wiley, Newyork, 1986.
[464] D. L. SnyderA. D. Lanterman, and M. I. Miller. General metropolis-hasting jump diffusions for automatic
target recognition in infrared scenes. Opt. Eng., 36(4):1123–1137, 2000.
[465] W. K. Hastings. Monte carlo sampling methods using markov chains, and their applications. Biometrika,
57 (2):97–109, 1970.
[466] P. J. Green. Monte carlo methods: An overview. In invited contribution to Proceedings of the IMA Conference
on Complex Stochastic Systems and Engineering, 1993.
[467] P. J. Green. Markov chain monte carlo in image analysis. In D. Spiegelhalter W. Gilks, S. Richardson,
editor, Practical Markov Chain Monte Carlo. Chapman and Hall, to appear 1995.
[468] N. Metropolis, A. Rosenbluth, M. Rosenbluth, A. Teller, and E. Teller. Equation of state calculations by
fast computing machines. J. Phys. Chem., 21:1087, 1953.
[469] B. Gidas. Metropolis type monte-carlo simulation algorithms and simulated annealing. Trends of
Contemporary Probability, 1993. to appear.
[470] J. Besag and P. J. Green. Spatial statistics and bayesian computation. J. R. Stat. Soc. B, 55(1):25–38, 1993.
[471] U. Grenander and M. I. Miller. Jump-diffusion processes for abduction and recognition of biological
shapes. Monograph of the Electronic Signals and Systems Research Laboratory, 1991.
[472] A. Srivastava, U. Grenander, G. R. Jensen, and M. I. Miller. Jump-diffusion markov processes on
orthogonal groups for object pose estimation. J. of Stat. Plan. Infer., 103:15–37, April 2002.
[473] C. E. Lucius. Targeting systems characterization facility. Thermal Imaging SPIE 636, pages 40–46, 1986.
[474] W. R. Owens. Data-based methodology for infrared signature projection. Thermal Imaging, SPIE 636,
pages 96–99, 1986.
[475] J. A. O’Sullivan, A. Srivastava, and V. Kedia. Performance analysis of atr from sar imagery. Proceedings
of the SPIE, Algorithms for Synthetic Aperture Radar Imagery V, April 1998.
[476] K. V. Mardia. Statistics of Directional Data. Academic Press, London, 1972.
[477] G. B. Folland. Real Analysis: Modern Techniques and Their Applications. John Wiley and Sons, Newyork,
1984.
[478] R. S. Strichartz. The Way of Analysis. Jones and Bartlett Publishers, 2000.
[479] R. L. Wheeden and A. Zygmund. Measure and Integral. Dekker, Newyork, 1977.
[480] A. Friedman. Partial Differential Equations of Parabolic Type. Prentice-Hall, Englewood cliffs, 1964.
[481] D. L. Snyder, A. M. Hammoud, and R. L. White. Image recovery from data acquired with a charge-
coupled-device camera. J. Opt. Soc. Am. A, 10:1014–1023, 1993.
[482] D. L. Snyder, M. Faisal, A. D. Lanterman, and R. L. White. Implementation of a modiﬁed richardson-
lucy method for image restoration on a massively parallel computer to compensate for space-variant
point-spread of a charge-coupled-device camera. J. Opt. Soc. Am. A, 12(12), 1995.
[483] D. L. Snyder, C. W. Helstrom, A. D. Lanterman, M. Faisal, and R. L. White. Compensation for readout
noise in ccd images. J. Opt. Soc. Am. A, 12:272–283, 1995.

This page intentionally left blank 

INDEX
Note: page numbers in bold refer to deﬁnitions and theorems, whilst those in italics refer to Figures
and Tables.
1-1 constraint languages 75–6
1D Gibbs random ﬁelds 119–20
1-single-type branching processes
extinction probability 63
probability generating function 60
1-type transition processes
extinction probability 71
Markhov chains 54, 57
mean 59
2-connection linear graphs 95
2D surface manifolds, eigenfunction models 419–21
2D surfaces 195
2-type reverse polish branching processes 58
Abelian groups 175
Abney, Steven, context-free grammar 83
acceptance probability 521, 542
actin-myosin shapes 146–7
anisotropic modeling 148
action on conﬁgurations 181
active closed contours 232–4
active contour models, level set methods 237–40
active deformable spheres 228–9, 236–7
active models 226
active shape models 494
Gaussian random ﬁeld models 240–3
general calculus 229–32
level set active contour models 237–40
active unclosed snakes and roads 234–6
adding the mean, least-squares estimation 21
addition theorem, spherical harmonics 287
adjoints, Gaussian processes 124–5
afﬁne group 178
action on background space 179
afﬁne motions 376–7
aging, hippocampal surface patterns 506–7, 508
aircraft tracking 157, 158, 534, 549–50
detection of airframes 552
Euler equations 550–1
pose and identiﬁcation 544–5, 546
pruning via prior distribution 552–3
all graphs 95, 96
almost-sure continuity 246
sample paths 299, 300–2
alternating minimization, EM sequence 31
Alzheimer’s disease, hippocampal surface patterns
506–7, 508, 509, 510
A-measurability, random processes 245, 246
Amit, Y. 275, 301
amoeba
active deformable surface spheres 290–3
COSM, EM algorithm reconstruction 33, 35
triangulated graph 157
anatomical manifolds
automated construction 468–9
comparison 469
anatomical source models 470–2
group actions 472–3
anatomical submanifolds, diffeomorphic study 469
anatomy see computational anatomy
animal fur
texture ﬁelds 114
texture synthesis 136
anisotropic textures 147–51
variable orientation 151–3
appendices, website address 4
arc-length of curves 191
arc-length parameterization 317
arithmetic expression grammar 79–80
Artin, M. 174
assumption of consistency 405
asymptotic approximations of information measures
452–6
asymptotic consistency 406–7
asymptotic covariance, stationary Gaussian random
ﬁelds 142
asymptotic equipartition (AEP) theorem 65–6
branching processes 72–3
asymptotic maximum likelihood estimators 144
asymptotic normality, maximum-likelihood estimator
(MLE) 28–9
asymptotics, Gaussian processes 138–42
a-typical trees 71
autocovariance function
cyclo-stationary Gaussian processes 135
stationary Gaussian processes 133
automated segmentation, cortex 482–3
automated target recognition, accommodation of clutter
438
automatic speech recognition see speech recognition
systems
automorphic pathologies 503
disease testing 503–10
auto-regressive image modeling 144–7
Gaussian processes 127–9
Green’s kernels 268
Aylward, E. 492
background space, action of afﬁne group 179
Bajcsy, R.K. 469
581

582
INDEX
Baker, J. 92
Bakircioglu, M. 289–90
Banach spaces 253, 519
generators 518
bandlimited generalized random processes 272
Barron, A.R. and Clark, B.S. 38
Barta, P. 209
Baum-Welch algorithm 89–90
Bayesian belief networks 52, 53
Bayesian fusion of information 402–5
Bayesian information criterion 40
Bayesian texture segmentation 110–12
Bayes integration, pose estimation 427–8
Bayes posterior distribution 5–8
Beg, F. 333
geodesic shooting 353, 354
landmark matching 342–3
LDDMM algorithm 476, 477, 479–80
scalar MRI mapping of heart 486–7
Belhumeur, P. 418, 419
belief networks 52, 53
Besag, J. 97, 110
pseudolikelihood method 560–1
bias 24
bigram language model 76, 81–2
entropy 86–7
generators and conﬁguration 160
bi-harmonic operator 279
binary order
adequacy 167
reduction of Markhov random ﬁelds 169–70
binary trees 79
binormal vector ﬁeld 191–2
Biomedical Informatics Research Network (BIRN) 502
birth-death processes 516
birth moves, Metropolis-Hastings jump-diffusion
scheme 542
birth transformations 162
Bitouk, D. 374, 402, 432, 437, 438
alternating minimization algorithm 443
clutter experiments 445
Blahut algorithm 457–9, 460–1, 462
bond function 154, 155
bonds of generators 154, 155
bond values, formal languages and grammars 158
Bookstein, F.L. 357, 469
approximate metric 361, 370
Boothby, W.M. 174, 221
Botteron, K. 204, 212, 213
boundary conditions, Gaussian ﬁelds in unit cube 274–5
boundary integral 234
level set function 238
bounded domains, periodic processes 258–62
brain
applications of computational anatomy 469
automated segmentation 482–3
caudate, mapping in Huntington’s disease 492–3
cortex
representation of curves 192–5, 208, 209, 210
cortical atlases, metric mapping 483–4
diffeomorphic studies of submanifolds 469–70, 481–2
ﬁtting curvatures 202–5
hippocampus
3D PCA 501–2
diffeomorphic mapping 481, 482
LDDMM algorithm 476, 478
landmark mapping 344
MRI-MPRAGE image 502, 503
surface patterns in Alzheimer’s and aging 506–7,
508, 509
surface patterns in depression 510
surface patterns in schizophrenia 506, 507
Hoffman phantom 307, 308
mouse brain development study 489–92
MRI registration 397–8, 399
multiple modality signal registration 429–31
planar mapping 211–13
representation of cortical curves 192–5
dynamic programming 208, 209, 210
small deformation vector ﬁelds models 280–3
sulcus, gyrus and geodesic curves on triangulated
graphs 205–7
superior temporal gyrus, use of dynamic
programming 208, 209, 210
symmetry 180
triangulated graphs 156–7
tumour growth 372, 373
whole brain landmark matching 363–5, 364
brain segmentation, Gaussian mixture modeling 35–8
branching processes
branching matrix 59–60
entropy 68–9
subcritical, critical and supercritical processes 70–1
extinction probability 63–4
moment-generating function 60–2
tree structure 69
see also multi-type branching processes
Brownian motion 252, 263–4
almost surely sample path continuity 302
Dynkin condition 524
inﬁnitesimal means and variances 524–5
Bucker, R. 501
Burger’s equation 338
Burg’s maximum-entropy principle, stationary
Gaussian processes 143
Butterworth ﬁlters 150, 151
CAD models 179, 180
3D to 2D projections in Gaussian noise 386
codebook construction 461–7
calculus, shape activation 229–32
canonical representation 166–7
directed acyclic graphs 167–9
Gaussian random ﬁelds 170–3
Markhov random ﬁelds 169–70
Cao, Y. 472, 484, 486, 487
capacity, language of strings 77
Cauchy-Schwartz inequality 18–19, 317
Center for Language and Speech Processing (CLSP),
HMM-based ASR system 91–2
Chan-Vese model 233
steepest descent direction 238
Chapman-Kolmogorov equations 55, 514

INDEX
583
characteristic function 11–12
characteristic functional 432, 433
Chomsky, N., formal languages 74, 158
Chomsky normal form (CNF) grammars
Inside/Outside algorithm 93
trellis-based algorithm 94
Christensen, G.E. 275–6, 332, 336, 481, 482
non-geodesic mapping algorithm 478–80
circle packing 210
circles
as immersions 221
local coordinate systems 217
as manifolds 215
normal deformation 236
Classical Projection Theorem 19
clique systems 95, 96
reduction to binary form 169–70
closed contours
active models 232–4
directed arcs as generators 185
transformations 183–4
closure of language 77–8
closure of set 104, 105
clutter
robust deformable templates 438–45
target detection/identiﬁcation 445–6
transported generator model 431–7
CNR (carrier-to-noise ratio) 404–5
LADAR 393
codebook construction 456–7
CAD models 460–5
output symbol distribution 465–7
Collins, C.R. and Stephenson, K. 211
combinatorics, typical trees 73–4
Commanche tank, FLIR imagery 454–6
communications model for image transmission 378–81
compact covariances 310–11
compact operators 254–5
complete data densities 30
complete-incomplete data problems 30–1
principle of maximum entropy 47
complete orthonormal (CON) basis 248, 255
ﬁnite element generation method 294–6
complexity of models 43
model order estimation 38–40
complexity of patterns 1
complex numbers 175
as Lie group 224
complex random variables 11
complex spherical harmonics 285–6
complex-valued random processes, measurability 245
computational anatomy (CA) 468–9
anatomical source model 470–2
group actions 472–3
Christensen non-geodesic mapping algorithm 478–80
data channel model 473–4
diffeomorphic study of anatomical submanifolds
469–70
disease testing of automorphic pathology 503–10
distribution free testing 510–11
DT-MRI mapping of heart 484–8
extrinsic mapping of surface and volume
submanifolds 480–4
Gaussian random ﬁelds 495–6
heteromorphic tumours 512–13
large deformation diffeomorphic metric mapping
(LDDMM) 474–8, 496–8
momentum of landmarked shape spaces 498–502
small deformation Gaussian ﬁelds on surface
submanifolds 502–3
small deformation setting 502
statistics analysis for shape ﬁelds 494–5
vector ﬁelds for growth 488–93
computational biology
graph transformations 165–6
PDAGs 52–4
small deformation vector ﬁelds models 280–3
computational optical-sectioning microscopy 290,
292–3
computational time variable, rotational motions 374
conditional distribution 10
conditional entropy 7, 8
directed acyclic graphs 65
Markhov chains 66–7
conditional expectation 24
conditionally Gaussian random ﬁeld models,
photometric variability 422–4
conditional mean 22–3
conditional mean minimum risk estimation 381–5
conditional probability, and maximum entropy 47–8
cone of support, aircraft tracking 552, 553
conﬁgurations, bijection with similarities 184
conﬁgurations of patterns 154–5, 156–8
congruence 174, 175
triangles 180
connected sets, diffeomorphisms 222
conservation of momentum
geodesics 326, 338–40
conservative jump processes 515
consistency 27, 405
content of conﬁgurations 155
context-free grammars 75, 83
generators, conﬁguration and regularity 160–2
Inside/Outside algorithm 93
regularity 159
rewriting rules 159
trellis-based algorithm 94
context-free language models 83–4
entropy 86, 87
context-sensitive grammars 75
generators and conﬁguration 162
regularity 159
re-writing rules 159
continuity 246
sample paths 300, 301–3
second order processes 249–50
continuous covariances 254
continuous functions, normed linear space 17
continuous mappings 219
continuum, representations on 244
contour models, mitochondria 553–6
contours, levels set active models 237–40

584
INDEX
convergence 246
maximum likelihood estimator 28–9
Cooper, M.L. 424–5, 427, 428
pose estimation 448–9, 450–2
coordinate charts, manifolds 214
coordinate frames 195, 321–3
Corbetta, M. et al. 483, 485
cortex
ﬁtting curvatures 202–5
representation of curves 192–5
dynamic programming 208, 209, 210
cortical atlases, metric mapping 483–4
cosets 175
COSM (computational optical sectioning microscopy)
33, 35
Costello, C.J. 397
covariance
Gaussian ﬁelds 123
Gaussian ﬁelds on sphere 288–9
Gaussian vector ﬁelds 262
second order processes 249, 250
transported generator model 436
covariance operators 417
covariance trace class 262–3
“cover” pathologies 513
Cover, T. and Thomas, J. 44, 47
Cowell, R.G. et al., belief networks 52, 53
Cramer decomposition 311
Cramer-Rao inequality 25–6
crest lines 205
brain tissue 206
critical branching processes 60
entropy 70–1
cross modality registration 429–31
Csernansky, J.C. 481, 502, 504, 506–9
Csiszar, I. and Tusnady, G., iterative algorithm
31, 47
cumulant generating function 433–4
CUP CAD model, codebook construction 461–7
curvatures 190
ﬁtting to surfaces 198–205
dynamic programming 207–10
curves 218
deﬁnitions 190–1
Frenet representation 191–5
ridge curves and crest lines 205
cycles 50, 52
cyclic graphs 95, 96
directed arcs as generators 185
generators, conﬁguration and regularity 156
cyclic shift invariant operators 273
cyclomatic number 189
cyclo-stationarity 259
cyclo-stationary Gaussian processes 134–7
orthogonal representation 310–11
cyclo-stationary operators 282–4
cylinder
principal curvatures 199
shape operator 198
cytoplasm, image analysis 98–9, 149, 151
data channel model, computational anatomy 473–4
Dawid, A.P. 51
death moves, Metropolis-Hastings jump-diffusion
scheme 543, 544
death transformations 162
deformable shapes, transport theorem 231–2
deformable templates 181, 182, 347, 357
deforming closed contours 226–7
deleted interpolation method, probabilities of m-grams
82
dementia, hippocampal surface patterns 506–7, 508,
509
Dempster, A., Laird, N. and Rubin, D. 47
iterative algorithm 31
density of random variables 9
depression, hippocampal surface patterns 510
derivative operator, canonical representation 171,
172
descendents of a site 50
detectability 408–9
detection 445–6
aircraft tracking 552
hypothesis testing 408
jump processes 543, 558–9
Metropolis-Hastings jump-diffusion scheme 541–3
detection theory 445
Devore, M.D. 391
Dictyostelium discoideum
3D motion studies 236
optical-sectioning microscopy 289
diffeomorphic landmark mapping 343–5
diffeomorphism metric
for image orbit 349
N-shapes 361
approximation for small deformation splines 361–5
diffeomorphisms 219, 221, 222–3, 332–3
anatomic submanifolds 469–70, 480–3
conditions on generation 333–5
matrix group actions 223–4
space of, metric 336–8
diffeomorphism splines, conservation of momentum
340–3
difference operators, Gaussian processes 124, 126–33,
132
differentiability, second order processes 250–1
differential entropy 8
differential of mapping 220
differential operators 335–6
induction of Gaussian processes 271–3
small deformation vector ﬁelds models 280
in unit cube 274–80
diffusion processes 523–5
generators of 1D diffusions 525–7
stochastic differential equations for sampling 527–8
see also jump-diffusion inference
diffusion tensor magnetic resonance imaging (DT-MRI)
472, 484, 485, 487–8
mouse brain development study 489–92
dimensional instability 304, 307
dimension of manifold 214
DIMER graph type 182
Dini’s Theorem 257

INDEX
585
directed acyclic graphs (DAGs) 50
canonical representation 167–9
entropies 64–5
graph transformations 165–6
Markhov chains 54–6
natural language modeling 81–7
probabilities on (PDAGs) 51–4
directed arcs, as generators of closed contours 185
directed graphs 49
splitting property 51
directional derivatives 197, 216–17
direct product groups 175
discrete graphs 95, 96
discrete integration, orthogonal group 385
discrete spaces, minimum risk hypothesis testing 12–16
disease testing, automorphic pathologies 503–10
distance between points on sphere 286
distance between sets 105
distortion (penalty) function 456–7
distribution free testing, computational anatomy (CA)
510–11
DNA 52
Dobrushin, Lanford, Ruelle equation 116
Dobrushin, R.L. 97
drift gradients, mitochondria 558–9
drift vectors 537
Drury, H. 483, 485
Dupuis, P. 334
Dijkstra’s algorithm 87
dynamic metric growth model 470, 471–2, 488
growth from landmarked shape spaces 488–93
dynamic programming 88
generation of curves 207–10
dynamics, incorporation into MMSE estimation 386–8
Dynkin properties, diffusions 523, 524
Echiverria theorem 538
Egevary’s representation, cyclo-stationary ﬁelds 259
eigenfunction generation, singular value decomposition
418
eigenfunction models
2D surface manifolds 419–21
human face 418–19
eigenfunctions of operators 253–4
eigen-spectrum 253
of Brownian motion 263–4
elasticity operator induced stationary priors 313–15
elasticity operators
small deformation elasticity operator 273, 274–5, 279
stationary Navier elasticity operator 278–9
elasticity-parameterized Gaussian vector ﬁelds,
maximum-likelihood estimation 280–2
elastic matching 370
electron-microscopic autoradiography 33, 35
elliptic operators 300
emission tomography 33, 34
image deconvolution 307–8
image reconstruction problems 303, 304
smoothing 304–5
empirical covariance, photometric variability 416–21
empirical maximum entropy texture coding 113–15
energy density of shapes 229–30
energy function 101
entropy 7–8, 453, 454
asymptotic equipartition (AEP) theorem 65–6
branching processes 68–9
subcritical, critical and supercritical processes 70–1
directed acyclic graphs (DAGs) 64–5
language models 85–7
Markhov chains 66–8
stationary Gaussian random ﬁelds 142–3
stationary Gibbs ﬁelds 121–2
equilibrium conditions 370
equivalence 405
equivalence classes 174, 175
equivalence relations 175–7
Erlangen program 174
error exponents 15–16
exponential error exponents theorem 410–11
error probabilities 407–13
errors, type I and type II 14
estimators, minimum risk estimation 6–7
Ethier, S.N. and Kurtz, T.G. 530
Euclid, congruence 174
Euclidean group 178
Euclidean metric 439
N-shapes 357–9
Euclidean motions, tracking 179
Euclidean space 17, 316
Euler angle representation 534
Euler equations 325–6
aircraft tracking 550–1
for photometric and geometric variation 369–70
even spherical harmonics 285
events 9
expectation-maximization (EM) algorithm 30, 31
Gaussian mixture modeling, brain segmentation 35–8
Gaussian noise in spectrum estimation 31–3
image restoration in point processes 33–5
ML estimation of HMM parameters 89–92
parameter estimation in natural language models
92–4
expected values 10
exponential error exponents 410–11, 413
exponential integral curve 321
exponential of matrices 320
exponential maximum entropy density 45
exponential spline smoothing via Good’s roughness
305–8
exterior boundary of set 104
external bonds of conﬁguration
formal grammars 159
patterns 155
extinction probability
branching processes 63–4, 72
ﬁnite-state Markhov chains 62–3
extrapolation 6
extrinsic understanding 6
eyelids experiment, Younes 373
fabric
texture ﬁelds 115
texture synthesis 136

586
INDEX
face
3D PCA 500–1
diffeomorphic mapping 481, 499–500
glasses and eyelids experiments 373
illumination variability 418–19
landmark mapping 344, 345
facial expressions 370
false alarm, probability of (type I error) 14
false alarm rate 408
family generation trees 57
family of graph transformations 162
fast marching method 237
ﬁeld of events 8–9
ﬁelds, continuity 299
ﬁgure eight, as immersion 221
ﬁltered point processes 432
ﬁltering functions, texture representation 113–15
ﬁlters, anisotropic 148, 150–1
ﬁnite element method, generation of CON 294–6
ﬁnite rank operators 254
ﬁnite-state grammars 74, 75
generators, conﬁguration and regularity 160
regularity 159
rewriting rules 159
ﬁnite-state graphs, generators, conﬁguration and
regularity 157–8
ﬁnite-state languages 76–7
ﬁnite-state Markhov chains 54–6
entropy 66–8
extinction probability 62–3
ﬁnite volume Gibbs distribution 105
ﬁrst order Markhov processes, maximum entropy
distribution 45, 46
Fisher information 25–6, 453–4
monotonicity 403
Fisher information matrix 399–402
Fisher’s method of randomization 510–11
“ﬂattening” of triangulated graphs 210–12
ﬂight path generation 549
FLIR (forward-looking infrared) imaging 33, 393
asymptotic error exponents 412, 413
Commanche database 454–6
empirical eigenfunction models 419–21
fusion of information 404–5
ground vehicle recognition 534–5
mutual information 451–2
photometric variability 415
Poisson projection model 394–5
pose estimation 425–7, 546–8
thermal variation 428
urban clutter 149, 151
forest graphs 95, 96, 190
formal grammars 74–5
generators 158–62
formal languages 74–80
Forney, D. 87
Forward/Backward algorithm 90–1
Fourier transform 313
Frenet representation of curves 191–2
in cortex 192–3
Frobenius (Hilbert-Schmidt) distortion 461, 462
Frobenius metric 329–30, 381, 382
fundamental loops see independent loops
fundus beds 206
fundus curve generation 208
fusion of information 402–5
Galton-Watson processes 56, 58
Gaussian curvature 199
Gaussian distribution 528
in generalized sense 271
Gaussian Markhov random ﬁeld models 440
image synthesis 444–5
Gaussian maximum entropy density 45, 46
Gaussian mixture modeling, brain segmentation 35–8
Gaussian noise, spectrum estimation 31–3
Gaussian noise models, deblurring and denoising 308–9
Gaussian priors
Good’s roughness 305–6
role in image reconstruction 303–4
and smoothness 304–5
Gaussian processes (ﬁelds) 123–4
adjoints 124–6
asymptotics 138–41
characteristic function 11–2
conditional mean 22–4
difference operators 124
induction via difference operators 126–33, 132
induction via linear differential operators 271–3
on sphere 289–93
stationary Gaussian random ﬁelds 133–4
asymptotic covariance and log-normalizer 142
entropy 142–3
in unit cube 274–8
maximum likelihood estimation 278–80
small deformation vector ﬁelds models 280–3
see also Gaussian random ﬁelds; Gaussian vector ﬁelds
Gaussian random ﬁelds
active shapes 240–3
canonical representation 170–3
computational anatomy 495–6
mitochondria 555–6
on smooth surfaces 293
Laplace-Beltrami operator with Neumann
boundary conditions 293–7
Gaussian vector ﬁelds
Brownian motion 263–4
mean and covariance 262–3
white noise 264
Gauss-Markhov representation, cortical curves 193–5
GEC algorithm 478–80
Gee, J.C. 469
Geman, S. and Geman, D. 97, 107, 108
generalized linear groups 178
generalized projective sensor transformation 379
generalized random processes 271
General Pattern Theory 1
generators 214
of 1D diffusions 525–7
of formal languages and grammars 158–62
group actions 177
Markhov jump processes 519–20
Markhov processes 517–18

INDEX
587
of patterns 154, 155–8
canonical representations 166–7
relationship to similarities 184
transported generator model 432
gene regulatory networks, Saccharomyces cerevisiae 165,
166
genetic regulatory networks, PDAGs 52–4
geodesic connection, normal momentum motion 350–4
geodesic distances, matrix groups 327–9
geodesic evolution equations, landmarks 498–9
geodesic generation 208, 209
geodesic metric 317–18
geodesics 206
conservation of momentum 326, 338–40
in matrix groups 324–6
geodesic shooting 353–4
Gibbs distribution 101–4
Markhov chains 120
partition function 110–12
splitting property 104–10
stationary Gibbs random ﬁelds 116–18
1D 119–20
entropy 121–2
Gibbs’ sampling, jump-diffusion processes 539–41
glasses experiment, Younes 373
Goel, V. 87
Good, I.J. 304
Good’s roughness 304, 305
exponential spline smoothing 306–9
as Gaussian prior 305–6
Good-Turing method, probabilities of m-grams 82
gradients
for drifts, jump-diffusion for mitochondria 558–9
for growth of landmarked shapes 489
grammars
arithmetic expressions 79–80
pseudolinear 78–9
grammatical rules 74–5
grammatical transformations 162
Gram-Schmidt orthogonalization 248
graphs
directed and undirected 49
representation of regularity 2
graph theory, deﬁnitions 189–90
graph transformations 162–3
directed acyclic graphs (DAGs) 165–6
multiple object recognition 163–4
parsing 164–5
Green’s formula 239
Green’s function 264–5, 267
Green’s kernels 267–71, 363
Green’s operator 271, 272, 277–8
ground vehicles
pose estimation 546–8
recognition 533–5, 539, 543–4
see also tanks
group actions 176, 320–1
anatomical source models 472–3
on multiple generators 181
on N-shapes 361
on set of images 346–7
on single generator 177
groups 174–5
equivalence relations 175–7
of geometric and photometric variation 415
matrix groups 177–81
transformations from products of groups 181–4
growth
equilibrium condition changes 370
large deformation diffeomorphic metric mapping
475–6
growth connection of dense imagery, normal
momentum motion 354–6
growth from landmarked shape spaces 488–93
gyri, curvature 205–7
Haller, J.W. 482
Hamman, B., curvature approximation algorithm 200–1
Hammersley Clifford result 169
Han, X. 240
Harris, T.E. 72
Hartemink, A.J. et al. 53–4
Hart, H. and Liang, Z. 304
heart
DT-MRI 484–8
mapping experiments 477
scalar MRI mapping 486–7
Heat Bath 523
Helm, P., scalar MRI mapping of heart 486–7
Herman, G.T. and Odhner, D. 304
Heschl’s gyrus 209, 210, 211, 212
heteromorphic pathologies 503, 512–13
hidden Markov models (HMM) 87–8
in automatic speech-recognition system 91–2, 99–100
ML estimation of parameters 89–91
hidden random branching processes, neighbourhood
structure 100–1
hidden state sequences, MAP estimators 88
hierarchical directed acylic graph language model 84–5
entropy 86, 87
hierarchical ﬁeld via line processes 108–10
hierarchy of graphs, multiple object recognition 163–4
high range-resolution radar (HRR) 395–7
fusion of information 404–5
target orientation estimation 546
Hilbert-Schmidt distortion 461, 462
output distribution 465, 466
Hilbert-Schmidt estimator 381, 382, 383, 453–4
computation for special Euclidean group 384–5
Hilbert-Schmidt metric 381, 382, 383
Hilbert-Schmidt theorem 255
Hilbert spaces 18–19, 20, 319–20
of photometric intensities 416–17
of photometric variability 440–1
random variables 247–8
reproducing kernel Hilbert spaces (RKHS) 265–6
hippocampus
3D PCA 501–2
diffeomorphic mapping 481, 482
LDDMM algorithm 476, 478
ﬁtting curvatures 202–3
landmark mapping 344
MRI-MPRAGE image 502, 503

588
INDEX
hippocampus (Contd.)
surface patterns in Alzheimer’s and aging 506–7, 508,
509
surface patterns in depression 510
surface patterns in schizophrenia 506, 507
Hoffman brain phantom 308
Holder inequality 17–18
homeomorphisms 219, 220
homogeneous coordinates 179
homogeneous Markhov jump processes 515
homogeneous Poisson counting processes, jump
measure 515
homogeneous spaces 176
Huang, J. and Mumford, D. 437
Huntington’s disease, mapping of caudate 492–3
hypothesis testing 407–12
automorphic pathology 505–6
m-ary multiple hypotheses 412–13
target detection 445–6
identiﬁability 408
identiﬁcation of objects
communications model for image transmission
378–81
conditional mean minimum risk estimation 381–5
hypothesis testing 408
jump-diffusion processes 543–6
identity-change moves, Metropolis-Hastings
jump-diffusion scheme 543
illumination variability, pose estimation 450–2
image deconvolution, time-of-ﬂight PET 307–8
image orbit, diffeomorphism metric 349
image processing
random Markhov ﬁelds 98–9
segmentation via Ising model 107–8
image recognition algorithms 1
image reconstruction, need for regularization 304
image reconstruction problems 303–4
image restoration in point processes, EM algorithm
33–5
image synthesis, GMRF model 444–5
immersions 221, 226–7
incomplete-data 30
independence, local coordinate frames 216
independent loops 189–90
independent processes, maximum entropy distribution
45
inexact matching 350–4
inference engine 5
inﬁnite dimensional geometric and photometric
variation 370–3
inﬁnitely extended trees 72–4
inﬁnitesimal mean, diffusion processes 523, 524
inﬁnitesimal variance, diffusion processes 523, 524–5
information, monotonicity 447–8
information loss, quantiﬁcation 449–52
information measures 7–8
asymptotic approximations 452–6
inhomogeneous Poisson counting processes, jump
measure 516
initial momentum 350
Inside/Outside algorithm 92, 93
integers 175
equivalence classes 176
integration, and quadratic mean continuity 251–2
interior boundary of set 104
interior of set 104, 105
internal bonds of conﬁguration
formal grammars 159
patterns 155
intrinsic mean 495
intrinsic understanding 6
invariance, Markov processes 517–18
see also stationarity
invariances 356
invariant metric distances 347–9
Inverse Function Theorem 221
inverse perturbation 350–1
Ising, E. 96
Ising model 103–4
1D Gibbs random ﬁelds 119–20
low temperature 117–18
segmentation 107–8
isosurface algorithm 202
isotropic Laplacian model, maximum-likelihood
estimation 144–7
isotropic processes on sphere 285
Jacobian matrices 220, 221
Jaynes, E.T., principle of maximum entropy 44, 112
JEEP CAD model, codebook construction 462–6
joint characteristic function 433
joint differential entropy 8
joint entropy 7
directed acyclic graphs 65
joint measurability 251–2
joint probability distribution 9
Joshi, S. 228, 236, 290, 292, 293, 503, 504
diffeomorphism splines 340–2
distribution free testing 510
large deformation landmark mapping 363, 364
strongly elliptic operators 301
Joyce, L.S. and Root, W.L. 303
jump-diffusion algorithms 536–9
jump-diffusion inference 532
aircraft tracking 549–53
on countable unions of spaces 528–31
detection and removal of objects 543
ground vehicle recognition 533–5
identiﬁcation of objects 543–6
mitochondria 556–61
pose estimation 546–8
posterior distribution 536
jump parameters, mitochondria 557
jump processes 515–16
generators 519–20
Gibbs’ sampling 539–41
Metropolis-Hastings algorithm 521–3, 541–3
jump process simulation 520–1
jump transition measures 540
Kalman ﬁlter 6, 21–2
Karatzas, I. and Shreve, S.E. 299

INDEX
589
Karhunen-Loeve Expansion 257–8, 259, 311, 417
Kendall, D.G., similitude invariant distance 359–60
kernels 265–6, 267
Green’s 267–70
Khaneja, N. 192, 207, 208, 364
Kinderman, R. and Snell, J.L. 97, 117
Klein, F. 174
Kolmogoroff backward operator 519, 538
Kolmogoroff complexity model 174, 184
Kraft inequality 42
Kreigman, D.J. 418, 419
Kuich, W. 78
Kullback Liebler divergence (relative entropy) 8, 15–6,
114
information loss 450
Kumar, S. 87
Kupiec, J., trellis-based algorithm 94
kurtosis 434–6
Lagrangian generation of diffeomorphisms 332
landmarked shape spaces, momentum 498–502
landmark mapping 343–5
landmark matching 342–3
landmark shapes see N-shapes
Lange, K. and Carson, R. 35
Langevin stochastic differential equations 525, 528
language 75
formal languages 74–80
language modeling
context-free models 82–4
EM algorithms for parameter estimation 92–4
entropy of models 85–7
hierarchical directed acylic graph model 84–5
m-gram models 81–2
language synthesis, graph transformations 164–5
Lanterman, D.L. 304, 425, 427, 521, 541, 547
Laplace-Beltrami operator
CON, smoothing arbitrary function on manifolds
297–9
Neumann boundary conditions 293–7
Laplacian circulant boundary 277
Laplacian operator 273, 275, 280
canonical representation 172–3
on discrete triangulated graph 298–9
Gaussian ﬁelds 130–1
on sphere 289–93
large deformation diffeomorphic metric mapping
(LDDMM) 474–8, 479–80, 496–8
face 481, 500–1
heart 486–8
mouse brain 491–2
large deformation landmark mapping, Joshi’s algorithm
363–5, 364
large deformation statistical model 497
laser radar (LADAR) imaging 392–3
asymptotic error exponents 412, 413
fusion of information 404–5
pose estimation 548–9
lattice graphs 95, 96
Ising model 103–4
limitations 244
magnetic spin models 155–6
Markhov random ﬁelds 108–10
leaf model 187–9
least squares estimation 20–2
left group action 320–1
left invariance 348–9
photometric transport metric 367–8
left invariant distance, similitudes 359
Legendre functions 285
length, in space of diffeomorphisms 336
length of curve 317
level set methods, active contour models 237–40
Levitan, E. Herman, G.T. 304
Lie algebra 495
Lie group action 223, 224
Lie groups 224–5
likelihood density 6
likelihood function, sensors 378
likelihood-ratio testing 14
limiting entropy rate, stationary Gaussian processes 142
linear differential operators, induction of Gaussian
processes 271–3
linear graphs 50, 95, 96
generators, conﬁguration and regularity 156
tracking of moving bodies 57
linear manifolds 215
linear membrane, road model 236
linear operator, null space 177
line processes, hierarchical ﬁelds 108–10
links 190
Lipschitz property 524
local coordinate frames 216
of circle and sphere 217
local coordinate patch construction 199
local diffeomorphisms 221
local optimization 323–4
local potential functions 166
local quadratic charts 200–1
local regularity 155
log-normalizer 110–12
stationary Gaussian random ﬁelds 142
lower neighbourhood 162
low temperature Ising model 117–18
M2 tanks, Poisson projection model 394
M60 tank, pose estimation 425–6
macaque cortex, mapping experiment 477
McNally, J.G. 228, 236
magnetic resonance imaging
cross modality registration 430–1
registration 397–8
magnetic spin models, generators, conﬁguration and
regularity 155–6
manifold of N-shapes 357
manifolds 214–15
parallelizable 218
smooth mappings 219–20
maple leaf model 187–9
MAP (maximum a posteriori) estimation 13–14
Gaussian case 41–2
hidden state sequences 88
multi-modality signature registration 430

590
INDEX
MAP (maximum a posteriori) estimation (Contd.)
pose 424–6
signature and pose, Commanche FLIR 454–6
mappings, deﬁnitions 219–20
“Marching Cubes” algorithm 201–2
marginal distributions 9
Markhov chains 54–6
1D Gibbs random ﬁelds 119–20
entropy 66–8
extinction probability 62–3
m-gram language models 81–2
neighbourhoods 97
transition dynamics 516
unique Gibbs distribution 120
Markhov jump processes 515–16, 537–8
generators 519–20
Markhov jump process simulation 520–1
Markhov processes 514
generators 517–18
jump-diffusion dynamics 529–31
stationarity (invariance) 517–18
Markhov random ﬁelds (MRF) 96–101, 97
canonical representation 169–70
Mark, K.
entropy of bigram mode 86
language model 84–5
matrix group actions, as diffeomorphisms 223–4
matrix groups 177–81
geodesics 324–6
metrics 327–9
maximum entropy
and conditional probability 47–8
principle of 44
maximum entropy models 45–6
texture representation 112–15
maximum-likelihood estimator (MLE) 26–7
anatomical maps 496
anisotropic textures 148
asymptotic normality 28–9
auto-regressive image modeling 144
consistency 27–8
Gaussian mixture modeling, brain segmentation 36–8
HMM parameters 89–92
image restoration 33–5
isotropic Laplacian model 144–7
sinusoids in Gaussian noise 31–3
mean
Gaussian ﬁelds 123
Gaussian vector ﬁelds 262
mean adjustment, least-squares estimation 20
mean branching matrix 59
mean curvature 199
measurability 245, 246
measurable events 9
measurable sets 9
medial prefrontal cortex
ﬁtting curvatures 204–5
planar maps 212, 213
medical diagnosis, Bayesian belief network 52, 53
medical imaging registration 397–8
Mercer’s theorem 255–7
messenger RNA 53
meteorological photometric variation 420, 421
method of moments estimation, transported generator
model 436–7
Method of Sieves 303
metrics 316
between orbits 356–7
between orbits of special Euclidean group 373–4
diffeomorphism metric for image orbit 349
Frobenius metric 329–30
images under rotational motions 374–6
induced via photometric and geometric ﬂow 366–8
in matrix groups 327–9
N-shapes 357–60
in SO(2, 3) 330–1
on space of diffeomorphisms 336–8
metric space norms for clutter 439–42
metric spaces 316
for photometric variability 365–6
of robust deformable templates 415–16
vector spaces 319–20
Metropolis-Hastings algorithm 521–3
jump processes 541–3, 551
m-gram language models 76, 81–2
Miller, M.I. 338
minimal geodesics 317, 318
Hilbert spaces 319
space of diffeomorphisms 338
minimum description length (MDL) principle 38, 42–3
minimum energy curves 317, 318
minimum-mean-squared error (MMSE) estimators 16,
381, 382–4, 502
conditional mean estimation 22–4
least-squares estimation 20–2
projective imagery models
3D to 1D projections 395–7
3D to 2D projections in Gaussian noise 385–9
LADAR imaging 392–3
medical imaging registration 397–8
Poisson projection model 393–5
synthetic aperture radar imaging 389–91
minimum risk estimation, Bayes paradigm 6–7
minimum risk estimator 7
minimum risk hypothesis testing, discrete spaces 12–16
Minkowski set difference and addition 126
miss, probability of (type II error) 14
missed detection rate 408
mitochondria 137, 476, 479
analysis using stationary periodic processes 259–62,
260
contour models 553–6
jump-diffusion inference 556–61
Markhov random ﬁeld model 98–9
partition imaging 229, 230
segmentation 241–3
texture parameters 146–7, 148, 149, 151
variability 553
m-memory Markhov chains 56
canonical representation 168–9
model order estimation 38–40
minimum description length principle 42–3
model uncertainty, quantiﬁcation of information loss
449–52

INDEX
591
moment-generating function, branching processes 60–2
moments
determination from characteristic function 11
nth 10
momentum
conservation along geodesics 326, 338–40, 474
conservation for diffeomorphism splines 340–3
of landmarked shape spaces 498–502
normal momentum motion
geodesic connection 350–4
temporal sequences 354–6
monotonic extension 163
monotonicity
Fisher information 403
sensor measurement information gain 447–8
Mori, S. 489
Morphable Faces database 500, 501
mouse brain development study 489–92
moving body systems 158
generators, conﬁguration and regularity 157
MRI-MPRAGE image, hippocampus 502, 503
MSE bounds, incorporation of dynamics 386–8
MSTAR database 389, 390
mud, texture ﬁelds 115
multiple extension 163
multiple generators, group actions 181
multiple modality signal registration 429–31
multiple object recognition, graph transformations
163–4
multiple sensor fusion 402–5
error exponent 411
multiple sensors 378
multiplicative numbers, as Lie group 224
multi-type branching processes 56–9
tree structure 69
see also branching processes
multivariate Gaussian processes, maximum entropy
density 45, 46
multivariate normal distribution 10–11
Mumford, D. 107, 108, 338, 418, 431
Musicus, B.R. 49
mutual information 8, 447–9
asymptotic approximations of information measures
452–6
quantiﬁcation of information loss 449–52
mutually independent random variables 9
Nadel, S. 397
N-buty alcohol analysis, use of EM algorithm 33
neighbourhood systems 95, 96
hidden Markov chains 99–100
hidden random branching processes 100–1
Markov chains 98
MRF representation of textured images 98–9
upper and lower neighbourhoods 162
nested family of graphs, multiple object recognition
163–4
Neymann-Pearson Lemma 14–15, 503, 504
noise, projective transformations 379–80
noise-equivalent differential temperature (NET) 395
noisy channels 5
non-compact operators 309–10
orthogonal scale representation 312–15
non-terminating trees 72–4
normal curvature 199, 200
normal deformable surfaces 227–9
normal deformation, circle 236
normal distribution, multivariate 10–11
normal form context-free grammars, Inside/Outside
algorithm 93
normalized entropy, branching processes 70
normal momentum motion
geodesic connection 350–4
temporal sequences 354–6
normal operators 253
normed linear vector space 17–18
N-shapes 340, 357
diffeomorphism metric 361
approximation for small deformation splines 361–5
Euclidean metric 357–9
group actions 361
Kendall’s similitude invariant distance 359–60
nth moment 10
nuisance integral 38, 39–40, 41–2, 408, 409
nuisance parameters 407–8
null space of linear operator 177
object recognition
communications model for image transmission
378–81
conditional mean minimum risk estimation 381–5
hypothesis testing 408
jump-diffusion processes 543–6
Oboukhov’s orthogonal representation 286, 288
occipital cortex, ﬁtting curvatures 203–4
occupancy vectors, branching processes 72
odd spherical harmonics 286
Onsager, L. 96
openGL software 425, 427, 450
open mappings 219, 220
operational photometric variation 420, 421
operator norm 253
operator symmetry 284
operator trace 139
optical ﬂow, smoothing 305
optical-sectioning microscopy 290, 292–3
optimal path computation, dynamic programming
207–8
orbits, metric distances 356–7
orbits of group 176
ordinary differential equations, landmark mapping
343–5
oriented textures 147–53
orthogonal expansions 257–8
orthogonal groups 178
discrete integration 385
MMSE estimator 382–3, 386
orthogonal processes 311
Cramer decomposition 311–2
orthogonal scale representation 312–15
orthonormal sequences 248
O’Sullivan, J.A. et al. 389, 390, 391
output symbol distribution 465–7

592
INDEX
parallelizable manifolds 218
parameter estimation 398
Fisher information 399–402
parents of a site 50
parity languages 75–6, 77
parsing, graph transformations 164–5
partial ordering 50
partition function 101
Gibbs distribution 110–12
partitioning image, conditional probability 229
partitioning of sets 175
part-of-speech tagging 82
paths 50
Pearl, J. 51
Pearlson, G. 209
Peierls, R.E. 117
penalized estimator 304
penalized likelihood methods 303, 305–8
penalty (distortion) function 456–7
Penn Treebank, bigram and trigram data support 82
Pentland, A. and Sclaroff, S. 293
periodic boundary conditions, Gaussian processes 129,
130–1
periodic processes on bounded domains 258–62
Perron-Frobenius theorem 62, 516
perspective projection 448, 449, 534
PET (positron emission tomography) 33, 35
phase transition, 2D Ising model 117
photometric and geometric ﬂow
computing inﬁnite dimensional variation 370–3
Euler equations 369–70
metrics 366–8
photometric space 416–17
photometric variability 365, 414, 415
conditionally Gaussian random ﬁeld models 422–4
empirical covariance 416–21
metric spaces 365–6
and pose estimation 424–8
phrase structure grammars, generators, conﬁguration
and regularity 160–2
piecewise linear roads 235–6
PIE phantom 308
planar mapping, surface manifolds 210–13
planum temporale
curvature proﬁles 298, 299
deﬁnition via dynamic programming 209, 210
planar maps 211, 212
surface harmonics 296–7
platelet of a point 206
Platonic solids 156
Poggio, T. et al. 304
point estimators 6
point processes, image restoration 33–5
Poisson counting processes
jump measures 515–16
maximum entropy density 45, 46
Poisson processes 519–20, 523
characteristic function 12
image restoration 33–4
Poisson projection model 393–5
polar decomposition 179
pose 378
pose estimation 424–7, 448–9, 544–9
Bayes integration 427–8
signature variability 450–2
Posener, J.A. et al. 510
POSET (partially ordered set) graphs 49, 50, 162
positively regularity, matrices 62, 64
posterior distribution 6, 536
and transformations of inference 516
posterior potential, mitochondria 556
power operators 139
power spectral density
cyclo-stationary Gaussian processes 135
stationary Gaussian processes 133
Precision Gauges 304
pre-whitening operators 278
Preza, C. et al. 304
principal curvatures 199
principal curves 205
principle components analysis (PCA) 494, 497–8
small deformation PCA versus large deformation
PCA 499–502
principle components of landmark momentum 499
principle of maximum entropy 44
prior distribution and density 6
prior induction, aircraft tracking 550–1
PRISM simulator software 37, 420, 425
probabilities on directed acyclic graphs (PDAGs) 51–4
probability distribution function 9
probability of extinction, Markhov chains 62–3
probability-generating functions, branching processes
60–1
probability measure 8–9
probability model-building 43
maximum entropy models 45–6
principle of maximum entropy 44
probability spaces 8–9
Procrustes mean 499
product groups 181–4
projection operators 309
Projection Theorem 19
projective imagery models
MMSE estimators
3D to 2D projections in Gaussian noise 385–9
high-resolution radar 395–7
LADAR imaging 392–3
medical imaging registration 397–8
Poisson projection model 393–5
synthetic aperture radar (SAR) imaging 389–91
photometric variation 422–3
projective transformations in noise 379–80
proofs, website address 4
proposal density 521, 542
pseudolikelihood 110
pseudolikelihood method, Besag 560–1
pseudolinear grammars 78–9
“push in” and “push out” pathologies 513
Qiu and Bitouk 293, 294–7, 298
quadratic mean continuity 246
and integration 251–2
quadratic patch 199

INDEX
593
Rabbitt, R.D. 478
radar cross section (RCS) 395
randomization, Fisher’s method 510–11
random processes
continuity 300
measurability 245
random variables 9
complex 11
Hilbert space 247–8
transformation 10
range of interaction 104
rank of mapping 220
rate-distortion function calculation, Blahut algorithm
457–9
rate-distortion theory 456–7
remote rate-distortion problem 459–65
Ratnanather, T. 209, 212
ray traced imagery 437
real numbers 175
real-valued random variables 9
recognition 6
recursive estimation, Kalman ﬁltering 21–2
Reed, M. and Simon, B. 253
reﬂexivity, as equivalence relation 175
regular conﬁgurations 155
regular curve 190
regular grammar see ﬁnite-state grammar
regularity
formal grammars 159
patterns 155–8
regularity constrained subset of similarities 182
regularization, role in image reconstruction 304
regular lattices, Ising model 103–4
relative entropy (Kullback Lieber divergence) 8
remote rate-distortion problem 459–65
reproducing kernel Hilbert spaces (RKHS) 265–6, 335–6
reproduction alphabet 456–7
restoration 6
Reverse Polish 58, 64
reversible transformations 163
re-writing rules, formal grammars 158–9
ridge curves 205
brain tissue 206
Riemannian manifold 317–18
Riemann Mapping Theorem 210
Riesz-Schauder theorem 255
right group action 320–1
risk functions 407
Rissanen, J., minimum description length principle
(MDL) 38
road contours, transformations 183
roads, active models 234–6
robust deformable templates 414–15
metric space 415–16
for natural clutter 438–45
robust multi-modality deformable template 429
Rodriquez formula 328
rotational motions
aircraft tracking 550
metric between images 374–6
rotation determination, variable orientation 152–3
rotation of grid 333
Ruelle, D. 97
run-length languages 75–6
Saccharomyces cerevisiae
gene regulatory networks 165, 166
genetic regulatory networks 53–4
Safﬁtz, J. 230, 243, 553
sample paths, almost-sure continuity 300, 301–3
sample space 8, 9
sampling, use of diffusions 527–8
scalar MRI mapping of heart 486–7
scale operator 310
scale representation, orthogonal 312–15
scale transform 314
scaling 376–7
schizophrenia, hippocampal surface patterns 506, 507
Schwarz, G. 38
second-order processes 247–8
properties 249–51
second-order processes stationary to scale 312
second-order quadratic charts 200–1
segmentation
of mitochondria 241–3
textured images 137–8
via Ising model 107–8
self-adjoint operators 253, 255
sensing models 379
sensor fusion 402–5
error exponent 411
sensor measurement information gain, monotonicity
447–8
sensors, likelihood function 378
sensor symmetry 406–7
sentence-language set 161
separability 245, 246–7
separability program 245
set of equivalence classes 176
set of equivalences through sensor map 405
set of images 346
sets, partitioning by equivalence classes 175
Shannon, C.E. 81
Shannon, C.E. and Weaver, W. 64
Shannon’s theory of communications 5
shape functions 293–4
shape operator 196, 197–8, 199
ridge curves 205
shape spaces, statistics analysis 494–5
Shapiro, J.H. 393, 404, 413
Shepp, L.A. and Vardi, Y. 34, 35
shift operator 105, 309
sphere 285
signalling models 2
signature variability, pose estimation 450–2
signed distance function 237
similarities, relationship to generators 184
similar triangles 180–1
similitude invariant distance, N-shapes 359–60
similitudes 178
geodesics 329
singular value decomposition (SVD) 418

594
INDEX
sinusoid estimation, generators, conﬁguration and
regularity 158
Slutsky’s theorem 29
small deformation elasticity operator 274, 275–6
small deformation Gaussian ﬁelds on surface
submanifolds 502–3
small deformation PCA 499–500
small deformations, template construction 502
small deformation splines, diffeomorphism metric
approximation 361–5
small deformation vector ﬁeld models 280–3
smooth coordinate patch 195
smoothing
exponential spline smoothing via Good’s roughness
306–9
role in image reconstruction 304
smoothing arbitrary function on manifolds 297–9
smooth manifolds 214
smooth mappings 219–20
smoothness, and Gaussian priors 304–5
smoothness conditions 38–9
generation of diffeomorphisms 334
smoothness properties 264–5
smooth surfaces, Gaussian random ﬁelds 293
smooth vector ﬁelds 217–18
snakes 226
active models 234–6
SNR (signal-to-noise ratio) 404–5
FLIR 395
high-resolution radar (HRR) 396
Snyder, D.L., Poisson noise models 34, 393
Snyder, D.L. and Miller, M.I. 306, 307, 432
Snyder, D.L. and Politte, D.G. 35
Sobolev spaces 266
solutions to problems, website address 4
source-channel view 447
source model 379
space of regular conﬁgurations 155
spanning trees 189
special Euclidean group (SE) 178
adding dynamical equations of motion 386–8
Hilbert Schmidt estimator 384–5
metrics between orbits 373–4
MMSE estimator 383–4
special linear groups 178
special orthogonal group 225
spectral density
cyclo-stationary Gaussian processes 135
stationary Gaussian processes 133–4
spectrum estimation, Gaussian noise 31–3
speech recognition systems
hidden Markov models 87, 91–2, 99–100
trigram model 82
speed of curves 190
SPET (single-photon emission tomography) 33, 35
sphere
coordinate frames 217
normal deformation 228–9, 236–7
shape operator 197–8
stationary processes 285–9
Laplacian operator induced Gaussian ﬁelds 289–93
spherical harmonics 285–7, 286
addition theorem 288
splitting property
directed graphs 51, 55
Gibbs distribution 104–10
squared error 7
squared error metric 439
square-integrable spaces (L2) 17–18
square-summable spaces (I2) 17–18
Srivastava, A. 386, 407, 409, 412
exponential error exponents 410–11, 413
MMSE estimator theorem 382–3
stabilizers 347–8
stabilizing subgroups 347
standard Brownian motion 263
standard Wiener process 263
static metric mapping model 470–1
stationarity 249
Markhov processes 517–18
on sphere 285
stationary Gaussian contour model, mitochondria
554–6
stationary Gaussian random ﬁelds 133–4
asymptotic covariance and log-normalizer 142
entropy 142–3
stationary Gibbs random ﬁelds 116–18
1D 119–20
entropy 121–2
stationary Navier elasticity operator 279–80
stationary periodic processes 258–9
analysis of mitochondria 259–62
stationary processes on sphere 285–9
Laplacian operator induced Gaussian ﬁelds 289–93
statistical communications 2
statistics analysis
disease testing of automorphic pathology 505–6
large deformation statistical model 497
shape spaces 494–5
steepest descent direction, Chan-Vese model 238
Stein’s Lemma 15–16
stochastic differential equations (SDEs) 525, 527–8
stochastic formal grammar 75
stochastic languages 75, 160
Stone’s representation 313
strongly elliptic operators 301
subcritical branching processes 60
entropy 70–1
subgroups 174
of diffeomorphisms 332
submanifolds 222
subspace topology 222
sulci, curvature 205–7
sulcus curves, representation 192–5
supercritical branching processes 60
entropy 70–1
superior temporal gyrus, use of dynamic programming
208, 209, 210
superposition norm-square 367–8
support set 104
surface harmonics 293
planum temporale 296–7
surface manifolds, mapping to planar coordinates
210–13

INDEX
595
surface normal 196
surfaces
deﬁnitions 195
ﬁtting curvatures 198–205
dynamic programming 207–10
shape operator 196, 197–8
symmetry
as equivalence relation 175
target and object 181
symmetry sets 406–7
synthesis 6
synthetic aperture radar (SAR) imaging 389–91
T62 tank CAD model, codebook construction 461–5
T-72 tank
MSTAR data analysis 389–91
pose estimation 426, 427
tangent spaces 195, 215–17, 218
tangent vectors 215
tangent vectors of surface 196
tanks
Commanche, FLIR imagery 454–6
M2 tanks, Poisson projection model 394
M60 tank, pose estimation 425–6
photometric variation 420–1
pose estimation 425–7, 546–9
T62 tank CAD model, codebook construction 461–5
T-72 tank
MSTAR data analysis 389–91
pose estimation 426, 427
thermal variation 428
transformations 182
tank/truck discrimination, likelihood ratio 412
Tapia, R.A. and Thompson, J.R. 304
target detection 445–6
aircraft tracking 552
hypothesis testing 407
jump processes 543, 558–9
Metropolis-Hastings jump-diffusion scheme 541–3
target and object symmetry 181
teacup and teapot data bases 419, 420
teapot, photometric variability 427–8
template construction, small deformations 502
templates 181, 182
see also deformable templates; robust deformable
templates
temporal sequences, normal momentum motion 354–6
textured images
random Markhov ﬁeld representation 98–9
segmentation 137–8
texture representation
cyclo-stationary Gaussian modeling 135–7
maximum entropy models 112–15
textures, anisotropic 147–53
theorems, website address 4
thermal variation, FLIR 428
Thompson, D’Arcy, “On Growth and Form” 468
thresholding function 7
time-invariance, Markhov chains 54
time-of-ﬂight PET 35
image deconvolution 307–8
Toeplitz covariance 135, 440
topology 214
torsion of curves 190
torus graphs 95, 96, 129
total jump intensity 540
trace-class property 251
tracking via Euclidean motions 179
transformation of random variables 10
transformations
application of 2
groups 174
translation 218
see also graph transformations
transition density 6
transition law 378
transition law of Markhov process 514
transitive group actions 176
transitivity as equivalence relation 175
transitivity of transformations 163
translation 218
transmission tomography 35
transported generator model 431–7, 432
transport theorem, deformable shapes 231–2
tree generators, kurtosis 435–6
tree graphs 50, 95
tree structure, multi-type branching processes 69
trellis-based algorithm, Kupiec 94
triangle inequality 17–18, 316
Riemannian manifold 318
triangles, similar and congruent 180
triangulated graphs
brain tissue 202–3, 206–7
dynamic programming 207–10
"ﬂattening" 210–12
generators, conﬁguration and regularity 156–7
similarities 186–7
triangulated surfaces, curvature approximation 200–1
trigram language model 76, 82
entropy 86
generators and conﬁguration 160
Trouvé, A. 334, 338
twigs 190
typical set 66, 68
typical trees 71–4
unbiased estimators 24
unclosed contours, vectors as generators 186
unconstrained conﬁgurations 155
understanding 6
undirected graphs 49, 95–6
uniform scale groups 178
Uniform Weak Law of Large Numbers 27
uniqueness, diffeomorphisms 334–5
unitary operators 309
unit cube, Gaussian ﬁelds 273–7
unit normal curvature vector ﬁeld 191–2
unit speed curves 191
unit tangent vector ﬁeld 191–2
univariate normal density 10
upper neighbourhood 162
urban clutter, image analysis 149, 151

596
INDEX
Vaillant, M. 344, 345, 499–500
Van Campenhout, J.M. and Cover, T.M. 48
Van Essen, D. 208, 209, 477, 482, 483, 484
Van Zijl, P. 489
variable orientation, anisotropic textures 151–3
vector ﬁelds 191–2, 217–18
vector ﬁelds for growth 488–93
vector Lie groups 224
vectors as generators of unclosed contours 186
vector spaces as metric spaces 319–20
vector-valued random processes
continuity 246
measurability 245
velocity of curve 317
viscous matching 370
Viterbi algorithm 87, 88–9
V-multi-type (vector) branching processes 57, 58–9
entropy 70–1
extinction probability 63–4, 71
Von-Mises Markhov density 550
vth probability generating function 60
waiting times, maximum entropy density 45, 46
weakly reversible processes 521
white Gaussian processes, on sphere 289
whitening model, maximum entropy 45, 46
white noise 264, 271–2
wide-sense stationarity 249
wide-sense-stationary uncorrelated-scatter (WSSUS)
model 396
Wiener process 263
almost surely sample path continuity 303
and diffusions 526–7
Green’s function 270–1
Winslow, R. 477, 484
Wong, E. and Hajek, B. 300
X-29, azimuth-elevation power spectrum 396
XPATCH simulator 379, 396
Yantis, S. 203–4
yeast (Saccharomyces cerevisiae), genetic regulatory
networks 53–4
Yen, B. 413
Younes, L. 359, 472
computing inﬁnite dimensional geometric and
photometric variation 370–3
DT-MRI 484, 486, 487
glasses and eyelids experiments 373
Yuille, A. 418
Yuille, A. and Grzywach, N.M. 304
zero level set function 237
Zhang, J. 489
Zhu, S.C. 112, 113, 114


Undergraduate Topics in Computer Science

Undergraduate Topics in Computer Science (UTiCS) delivers high-quality instructional content for un-
dergraduates studying in all areas of computing and information science. From core foundational and
theoretical material to ﬁnal-year topics and applications, UTiCS books take a fresh, concise, and modern
approach and are ideal for self-study or for a one- or two-semester course. The texts are all authored by
established experts in their ﬁelds, reviewed by an international advisory board, and contain numerous
examples and problems. Many include fully worked solutions.
For further volumes:
http://www.springer.com/series/7592

Maribel Fernández
Models
of Computation
An Introduction
to Computability Theory
123

Dr. Maribel Fernández
King’s College London
UK
Series editor
Ian Mackie, École Polytechnique, France and University of Sussex, UK
Advisory board
Samson Abramsky, University of Oxford, UK
Chris Hankin, Imperial College London, UK
Dexter Kozen, Cornell University, USA
Andrew Pitts, University of Cambridge, UK
Hanne Riis Nielson, Technical University of Denmark, Denmark
Steven Skiena, Stony Brook University, USA
Iain Stewart, University of Durham, UK
David Zhang, The Hong Kong Polytechnic University, Hong Kong
ISSN 1863-7310
ISBN 978-1-84882-433-1
e-ISBN 978-1-84882-434-8
DOI 10.1007/978-1-84882-434-8
Springer Dordrecht Heidelberg London New York
British Library Cataloguing in Publication Data
A catalogue record for this book is available from the British Library
Library of Congress Control Number: Applied for
c⃝Springer-Verlag London Limited 2009
Apart from any fair dealing for the purposes of research or private study, or criticism or review,
as permitted under the Copyright, Designs and Patents Act 1988, this publication may only
be reproduced, stored or transmitted, in any form or by any means, with the prior permission
in writing of the publishers, or in the case of reprographic reproduction in accordance with the
terms of licenses issued by the Copyright Licensing Agency. Enquiries concerning reproduction
outside those terms should be sent to the publishers.
The use of registered names, trademarks, etc., in this publication does not imply, even in
the absence of a speciﬁc statement, that such names are exempt from the relevant laws and
regulations and therefore free for general use.
The publisher makes no representation, express or implied, with regard to the accuracy of the
information contained in this book and cannot accept any legal responsibility or liability for
any errors or omissions that may be made.
Printed on acid-free paper
Springer is part of Springer Science+Business Media (www.springer.com)

Preface
Aim
The aim of this book is to provide an introduction to essential concepts in
computability, presenting and comparing alternative models of computation.
We deﬁne and analyse the most signiﬁcant models of computation and their
associated programming paradigms, from Turing machines to the emergent
computation models inspired by systems biology and quantum physics.
About this book
This book provides an introduction to computability using a series of abstract
models of computation.
After giving the historical context and the original challenges that motivated
the development of computability theory in the 1930s, we start reviewing the
traditional models of computation: Turing machines, Church’s Lambda calcu-
lus (or λ-calculus), and the theory of recursive functions of G¨odel and Kleene.
These three models of computation are equivalent in the sense that any compu-
tation procedure that can be expressed in one of them can also be expressed in
the others. Indeed, Church’s Thesis states that the set of computable functions
is exactly the set of functions that can be deﬁned in these models.
Each of the above-mentioned models of computation gave rise to a pro-
gramming paradigm: imperative, functional, or algebraic. We also include in
the ﬁrst part of the book a computation model based on deduction in a frag-
ment of ﬁrst-order logic, which gave rise to the logic programming paradigm,

vi
Preface
because the work by Herbrand in this area dates also from the late 1920s and
early 1930s.
As programming languages evolved and new programming techniques were
developed, other models of computation became available; for instance, based
on the concept of object or on a notion of interaction between agents. It is
possible, for example, to show that any computable function can be deﬁned
by using an abstract device where one can deﬁne objects, invoke their meth-
ods, and update them. In the second part of the book, we describe a calculus
of objects as a foundation for object-oriented programming and compare its
computational power with the traditional ones. We also describe a graphical,
interaction-based model of computation and a formalism for the speciﬁcation
of concurrent computations.
Recently, there has been a renewed interest in computability theory, with
the emergence of several models of computation inspired by biological and phys-
ical processes. In the last chapter of the book, we discuss biologically inspired
calculi and quantum computing.
This book is addressed to advanced undergraduate students, as a comple-
ment to programming languages or computability courses, and to postgraduate
students who are interested in the theory of computation. It was developed to
accompany lectures in a Master’s course on models of computation at King’s
College London. The book is for the most part self-contained; only some basic
knowledge of logic is assumed. Basic programming skills in one language are
useful, and knowledge of more programming languages will be helpful but is
not necessary.
Each chapter includes exercises that provide an opportunity to apply the
concepts and techniques presented. Answers to selected exercises are given at
the end of the book. Although some of the questions are just introductory,
most exercises are designed with the goal of testing the understanding of the
subject; for instance, by requiring the student to adapt a given technique to
diﬀerent contexts.
Organisation
The book is organised as follows. Chapter 1 gives an introduction to com-
putability and provides background material for the rest of the book, which is
organised into two parts.
In Part I, we present the traditional models of computation. We start with
the study of various classes of automata in Chapter 2. These are abstract
machines deﬁned by a collection of states and a transition function that con-

Preface
vii
trols the way the machine’s state changes. Depending on the type of memory
and the kind of response that the automaton can give to external signals, we
obtain machines with diﬀerent computation power. After giving an informal
description, we provide formal speciﬁcations and examples of ﬁnite automata,
push-down automata, and Turing machines. The chapter ends with a discus-
sion of the applications of these automata to programming language design and
implementation.
The next two chapters are dedicated to the study of computation models
inspired by the idea of “computation as functional transformation”. In Chap-
ter 3, we give an overview of the λ-calculus, with examples that demonstrate the
power of this formalism, highlighting the role of the λ-calculus as a foundation
for the functional programming paradigm. In Chapter 4, we deﬁne primitive
recursion and the general class of partial recursive functions.
The ﬁnal chapter in Part I describes a model of computation based on de-
duction in a fragment of ﬁrst-order logic. We introduce the Principle of Resolu-
tion and the notion of uniﬁcation. We then study the link between these results
and the development of logic programming languages based on SLD-resolution.
Part II studies three modern computation paradigms that can be seen
as the foundation of three well-known programming styles: object-oriented,
interaction-based, and concurrent programming, respectively. In addition, it
includes a short discussion on emergent models of computation inspired by bi-
ological and physical processes. More precisely, Part II is organised as follows.
In Chapter 6, we analyse the process of computation from an object-oriented
perspective: Computation is structured around objects that own a collection
of functions (methods in the object-oriented terminology). We describe object-
oriented computation models, providing examples and a comparison with tra-
ditional models of computation.
In Chapter 7, we study graphical models of computation, where computa-
tion is centred on the notion of interaction. Programs are collections of agents
that interact to produce a result. We show that some graphical models natu-
rally induce a notion of sequentiality, whereas others can be used to describe
parallel functions.
Chapter 8 describes a calculus of communicating processes that can be
used to specify concurrent computation systems, and gives a brief account of
an alternative view of concurrency inspired by a chemical metaphor.
Chapter 9 gives a short introduction to some of the emergent models of
computation: biologically inspired calculi and quantum computing.
The last chapter of the book (Chapter 10) contains answers to a selection
of exercises.
At the end of the book there is a bibliographical section with references to
articles and books where the interested reader can ﬁnd more information.

viii
Preface
Acknowledgements
The material presented in this book has been prepared using several diﬀerent
sources, including the references mentioned above and notes for my courses
in London, Paris, and Montevideo. I would like to thank the reviewers, the
editors, and the students in the Department of Computer Science at King’s
College London for their comments on previous versions of the book, and my
family for their continuous support.
Maribel Fern´andez
London, November 2008

Contents
1.
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.1
Models of computation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
1.2
Some non-computable functions . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4
1.3
Further reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
1.4
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
Part I. Traditional Models of Computation
2.
Automata and Turing Machines . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
2.1
Formal languages and automata . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
2.2
Finite automata . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
2.2.1
Deterministic and non-deterministic automata . . . . . . . . . 16
2.2.2
The power of ﬁnite automata . . . . . . . . . . . . . . . . . . . . . . . . 18
2.3
Push-down automata . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
2.4
Turing machines. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
2.4.1
Variants of Turing machines . . . . . . . . . . . . . . . . . . . . . . . . . 28
2.4.2
The universal Turing machine . . . . . . . . . . . . . . . . . . . . . . . . 29
2.5
Imperative programming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
2.6
Further reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
2.7
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
3.
The Lambda Calculus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
3.1
λ-calculus: Syntax . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
3.2
Computation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
3.2.1
Substitution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40

x
Contents
3.2.2
Normal forms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
3.2.3
Properties of reductions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
3.2.4
Reduction strategies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
3.3
Arithmetic functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
3.4
Booleans . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
3.5
Recursion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
3.6
Functional programming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
3.7
Further reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
3.8
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
4.
Recursive Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
4.1
Primitive recursive functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
4.2
Partial recursive functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
4.3
Programming with functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63
4.4
Further reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67
4.5
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67
5.
Logic-Based Models of Computation . . . . . . . . . . . . . . . . . . . . . . . . 69
5.1
The Herbrand universe . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
5.2
Logic programs. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
5.2.1
Answers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
5.3
Computing with logic programs . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76
5.3.1
Uniﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76
5.3.2
The Principle of Resolution . . . . . . . . . . . . . . . . . . . . . . . . . . 79
5.4
Prolog and the logic programming paradigm . . . . . . . . . . . . . . . . . 84
5.5
Further reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86
5.6
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86
Part II. Modern Models of Computation
6.
Computing with Objects. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93
6.1
Object calculus: Syntax . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94
6.2
Reduction rules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96
6.3
Computation power . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98
6.4
Object-oriented programming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99
6.5
Combining objects and functions . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
6.6
Further reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104
6.7
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104

Contents
xi
7.
Interaction-Based Models of Computation . . . . . . . . . . . . . . . . . . 107
7.1
The paradigm of interaction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107
7.2
Numbers and arithmetic operations . . . . . . . . . . . . . . . . . . . . . . . . . 111
7.3
Turing completeness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114
7.4
More examples: Lists . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115
7.5
Combinators for interaction nets . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
7.6
Textual languages and strategies for interaction nets . . . . . . . . . . 118
7.6.1
A textual interaction calculus . . . . . . . . . . . . . . . . . . . . . . . . 120
7.6.2
Properties of the calculus . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125
7.6.3
Normal forms and strategies . . . . . . . . . . . . . . . . . . . . . . . . . 126
7.7
Extensions to model non-determinism . . . . . . . . . . . . . . . . . . . . . . . 127
7.8
Further reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129
7.9
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130
8.
Concurrency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131
8.1
Specifying concurrent systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134
8.2
Simulation and bisimulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137
8.3
A language to write processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139
8.4
A language for communicating processes. . . . . . . . . . . . . . . . . . . . . 142
8.5
Another view of concurrency:
The chemical metaphor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147
8.6
Further reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147
8.7
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148
9.
Emergent Models of Computation . . . . . . . . . . . . . . . . . . . . . . . . . . 151
9.1
Bio-computing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152
9.1.1
Membrane calculi . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152
9.1.2
Protein interaction calculi . . . . . . . . . . . . . . . . . . . . . . . . . . . 153
9.2
Quantum computing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154
9.3
Further reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157
10. Answers to Selected Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159
Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183

1
Introduction
This book is concerned with abstract models of computation. Several new mod-
els of computation have emerged in the last few years (e.g., chemical machines,
bio-computing, quantum computing, etc.). Also, many developments in tradi-
tional computational models have been proposed with the aim of taking into
account the new demands of computer system users and the new capabilities
of computation engines. A new model of computation, or a new feature in a
traditional one, usually is reﬂected in a new family of programming languages
and new paradigms of software development. Thus, an understanding of the
traditional and emergent models of computation facilitates the use of modern
programming languages and software development tools, informs the choice of
the correct language for a given application, and is essential for the design of
new programming languages.
But what exactly is a “model of computation”? To understand what is
meant by a model of computation, we brieﬂy recall a little history. The notions
of computability and computable functions go back a long time. The ancient
Greeks and the Egyptians, for instance, had a good understanding of compu-
tation “methods”. The Persian scientist Al-Khwarizmi in 825 wrote a book
entitled “On the Calculation with Hindu Numerals”, which contained the de-
scription of several procedures that could now be called algorithms. His name
appears to be the origin of the word “algorithm”: When his book was trans-
lated into Latin, its title was changed to “Algoritmi de Numero Indorum”. The
word “algorithm” was later used to name the class of computation procedures
described in the book. Roughly, an algorithm is:

1
Introduction
This book is concerned with abstract models of computation. Several new mod-
els of computation have emerged in the last few years (e.g., chemical machines,
bio-computing, quantum computing, etc.). Also, many developments in tradi-
tional computational models have been proposed with the aim of taking into
account the new demands of computer system users and the new capabilities
of computation engines. A new model of computation, or a new feature in a
traditional one, usually is reﬂected in a new family of programming languages
and new paradigms of software development. Thus, an understanding of the
traditional and emergent models of computation facilitates the use of modern
programming languages and software development tools, informs the choice of
the correct language for a given application, and is essential for the design of
new programming languages.
But what exactly is a “model of computation”? To understand what is
meant by a model of computation, we brieﬂy recall a little history. The notions
of computability and computable functions go back a long time. The ancient
Greeks and the Egyptians, for instance, had a good understanding of compu-
tation “methods”. The Persian scientist Al-Khwarizmi in 825 wrote a book
entitled “On the Calculation with Hindu Numerals”, which contained the de-
scription of several procedures that could now be called algorithms. His name
appears to be the origin of the word “algorithm”: When his book was trans-
lated into Latin, its title was changed to “Algoritmi de Numero Indorum”. The
word “algorithm” was later used to name the class of computation procedures
described in the book. Roughly, an algorithm is:

2
Chapter 1. Introduction
– a ﬁnite description of a computation in terms of well-deﬁned elementary
operations (or instructions);
– a deterministic procedure: the next step is uniquely deﬁned, if there is one;
– a method that always produces a result, no matter what the input is (that
is, the computation described by an algorithm always terminates).
The modern computability theory has its roots in the work done at the
beginning of the twentieth century to formalise the concept of an “algorithm”
without referring to a speciﬁc programming language or physical computational
device. A computation model abstracts away from the material details of the
device we are using to make the calculations, be it an abacus, pen and paper,
or our favourite programming language and processor.
In the 1930s, logicians (in particular Alan Turing and Alonzo Church) stud-
ied the meaning of computation as an abstract mental process and started to
design theoretical devices to model the process of computation, which could be
used to express algorithms and also non-terminating computations.
The notion of a partial function generalises the notion of an algorithm
described above by considering computation processes that do not always lead
to a result. Indeed, some expressions do not have a value:
1. True + 4 is not deﬁned (we cannot add a number and a Boolean).
2. 10/0 is not deﬁned.
3. The expression factorial(−1) does not have a value if factorial is a recursive
function deﬁned as follows:
factorial(0)
=
1
factorial(n)
=
n ∗factorial(n −1)
The ﬁrst is a type error since addition is a function from numbers to num-
bers: For any pair of natural numbers, the result of the addition is deﬁned. We
say that addition is a total function on the natural numbers.
The second is a diﬀerent kind of problem: 10 and 0 are numbers, but division
by 0 is not deﬁned. We say that division is a partial function on the natural
numbers.
There is another case in which an expression may not have a value: The
computation may loop, as in the third example above. We will say that factorial
is a partial function on the integers.
The notion of a partial function is so essential in computability theory that
it deserves to be our ﬁrst deﬁnition.

Chapter 1. Introduction
3
Deﬁnition 1.1 (Partial function)
Let A and B be sets. We denote their Cartesian product by A × B; that is,
A × B denotes the set of all the pairs where the ﬁrst element is in A and the
second in B. We use the symbol ∈to denote membership; i.e., we write a ∈A
to indicate that the element a is in the set A.
A partial function f from A to B (abbreviated as f : A →B) is a subset
of A × B such that if (x, y) ∈f and (x, z) ∈f, then y = z. In other words,
a partial function from A to B associates to each element of A at most one
element of B.
If (x, y) ∈f, we write f(x) = y and say that y is the image of x. The
elements of A that have an image in B are in the domain of f.
In the study of computability, we are often interested only in functions
whose domain and co-domain are the set of integer numbers. In some cases,
this is even restricted to natural numbers; that is, integers that are positive or
zero.
The notion of a partial function is also important in modern programming
techniques. From an abstract point of view, we can say that each program
deﬁnes a partial function. In practice, we are interested in more than the func-
tion that the program computes; we also want to know how the function is
computed, how eﬃcient the computation is, how much memory space we will
need, etc. However, in this book we will concentrate on whether a problem has
a computable solution or not, and how the actual computation mechanism is
expressed, without trying to obtain the most eﬃcient computation.
1.1 Models of computation
Some mathematical functions are computable and some are not: There are
problems for which no computer program can provide a solution even assuming
that the amount of time and space available to carry out the computation is
inﬁnite. Complexity theory studies the “practical” aspects of computability;
that is, for a computable function, it answers the question: How much time
and space will be needed for the computation? We will not cover complexity
theory in this book but instead will concentrate on computability.
First we need to deﬁne precisely the notion of a computable function. This is
a diﬃcult task and is still the subject of research. We will ﬁrst give an intuitive
deﬁnition.

4
Chapter 1. Introduction
Deﬁnition 1.2 (Computable function)
All the functions on the natural numbers that can be eﬀectively computed in
an ideal world, where time and space are unlimited, are called partial recursive
functions or computable functions.
The deﬁnition of a computable function above does not say what our notion
of “eﬀective” computation is: Which programming language is used to deﬁne
the function? What kind of device is used to compute it? We need a model of
computation to abstract away from the material details of the programming
language and the processor we are using. In fact, computability was studied as
a branch of mathematical logic well before programming languages and com-
puters were built. Three well-studied abstract models of computation dating
from the 1930s are
– Turing machines, designed by Alan Turing to provide a formalisation of the
concept of an algorithm;
– the Lambda calculus, designed by Alonzo Church with the aim of providing
a foundation for mathematics based on the notion of a function; and
– the theory of recursive functions, ﬁrst outlined by Kurt G¨odel and further
developed by Stephen Kleene.
These three models of computation are equivalent in that they can all ex-
press the same class of functions. Indeed, Church’s Thesis says that they com-
pute all the so-called computable functions. More generally, Church’s Thesis
says that the same class of functions on the integers can be computed in any
sequential, universal model of computation that satisﬁes basic postulates about
determinism and the eﬀectiveness of elementary computation steps. This class
of computable functions is the set of partial recursive functions.
We say that a programming language is Turing complete if any computable
function can be written in this language. All general-purpose programming
languages available nowadays are complete in this sense. Turing completeness is
usually proved through an encoding in the programming language of a standard
universal computation model.
1.2 Some non-computable functions
Since the 1930s, it has been known that certain basic problems cannot be solved
by computation. The typical example is the Halting problem discussed below,

Chapter 1. Introduction
5
which was proved to be non-computable by Church and Turing. Other examples
of non-computable problems are:
– Hilbert’s 10th problem: solving Diophantine equations.
Diophantine equations are equations of the form
P(x1, . . . , xn) = Q(x1, . . . , xn)
where P and Q are polynomials with integer coeﬃcients. A polynomial is a
sum of monomials, each monomial being a product of variables with a coeﬃ-
cient. The coeﬃcients are constants; for example, x2 +2x+1 is a polynomial
on one variable, x.
The mathematician David Hilbert asked for an algorithm to solve Diophan-
tine equations; that is, an algorithm that takes a Diophantine equation as
input and determines whether this equation has integer solutions or not. This
problem was posed by Hilbert in 1900 in a list of open problems presented
at the International Congress of Mathematicians, and it became known as
Hilbert’s 10th problem. It is important to note that the coeﬃcients of the
polynomials are integers and the solution requested is an assignment of in-
teger numbers to the variables in the equation.
Hilbert’s 10th problem remained open until 1970, when it was shown to be
undecidable in general by Yuri Matijasevi˘c, Julia Robinson, Martin Davis,
and Hilary Putnam.
– Hilbert’s decision problem: the Entscheidungsproblem.
This problem was also posed by Hilbert in 1900. Brieﬂy, the problem requires
writing an algorithm to decide whether any given mathematical assertion in
the functional calculus is provable.
Hilbert thought that this problem was computable, but his conjecture was
proved wrong by Church and Turing, who showed that an algorithm to solve
this problem could also solve the Halting problem.
These are examples of undecidable problems. We end this introduction with
a description of the Halting problem.
The Halting problem.
Intuitively, to solve the Halting problem, we need an
algorithm that can check whether a given program will stop or not on a given
input. More precisely, the problem is formulated as follows:
Write an algorithm H such that given
– the description of an algorithm A (which requires one input) and

6
Chapter 1. Introduction
– an input I,
H will return 1 if A stops with the input I and 0 if A does not stop on I.
We can see the algorithm H as a function: H(A, I) = 1 if the program A
stops when the input I is provided, and H(A, I) = 0 otherwise.
In the quest for a solution to this problem, Turing and Church constructed
two abstract models of computation that later became the basis of the modern
theory of computing: Turing machines and the Lambda calculus.
In fact, Church and Turing proved that there is no algorithm H such that,
for any pair (A, I) as described above, H produces the required output. Its
proof, which follows, is short and elegant.
Proof
If there were such an H, we could use it to deﬁne the following program C:
C takes as input an algorithm A and computes H(A, A). If the result
is 0, then it answers 1 and stops; otherwise it loops forever.
Below we will use the notation A(I) ↑, where A is a program and I is its
input, to represent the fact that the program A does not stop on the input I.
Using the program C, for any program A, the following properties hold:
– If H(A, A) = 1, then C(A)↑and A(A) stops.
– If H(A, A) = 0, then C(A) stops and A(A)↑.
In other words, C(A) stops if and only if A(A) does not stop.
Since A is arbitrary, it could be C itself, and then we obtain a contradiction:
C(C) stops if and only if C(C) does not stop.
Therefore H cannot exist.
The proofs of undecidability of Hilbert’s decision problem or Diophantine
equations are more involved and we will not show them in this book, but it is
important to highlight that these results, obtained with the help of abstract
models of computation, still apply to current computers.
Since the class of computable functions is the same for all the traditional
computation models, we deduce that imperative or functional languages (which
are based on Turing machines and the Lambda calculus, respectively) can de-
scribe exactly the same class of computable functions. Several other models

Chapter 1. Introduction
7
of computation, or idealised computers, have been proposed, some of them in-
spired by advances in physics, chemistry, and biology. There is hope that some
of these new models might solve some outstanding non-feasible problems (i.e.,
problems that cannot be solved on a realistic timescale in traditional models).
1.3 Further reading
Readers interested in algorithms can ﬁnd more information in Harel and Feld-
man’s book [22]. Further information on partial functions and computability
in general can be found in [47, 49] and in the chapter on computability in
Mitchell’s book [36]. Additional references are provided in the following chap-
ters.
1.4 Exercises
1. Give more examples of total and partial functions on natural numbers.
2. To test whether a number is even or odd, a student has designed the fol-
lowing function:
test(x)
def
=
if x = 0 then "even"
else if x = 1 then "odd" else test(x-2)
Is this a total function on the set of integer numbers? Is it total on the
natural numbers?
3. Consider the following variant of the Halting problem:
Write an algorithm H such that, given the description of an al-
gorithm A that requires one input, H will return 1 if A stops for
any input I and H will return 0 if there is at least one input I for
which A does not stop.
In other words, the algorithm H should read the description of A and
decide whether it stops for all its possible inputs or there is at least one
input for which A does not stop.
Show that this version of the Halting problem is also undecidable.

Part I
Traditional Models of Computation

2
Automata and Turing Machines
In the 1930s, logicians (in particular Alan Turing and Alonzo Church) studied
the meaning of computation as an abstract mental process and started to design
theoretical devices to model it. As mentioned in the introduction, they needed
a precise, formal deﬁnition of an algorithm in order to show that some of
the problems posed by David Hilbert at the 1900 International Congress of
Mathematicians could not be solved algorithmically. This was a very important
step towards the construction of actual computers and, later, the design of
programming languages. Turing machines inﬂuenced the development of digital
computers, and the Lambda calculus is the basis of functional programming
languages. At the same time, computers give to the early computability studies
a practical application.
Turing deﬁned an algorithm as a process that an abstract machine, now
called a Turing machine, can perform. Church described his algorithms using
the Lambda calculus. Several other models of computation, or idealised com-
puters, have been proposed and studied since then. Depending on the features
of the idealised computer, some abstract models of computation can represent
all computable functions and others cannot. For instance, ﬁnite automata, one
of the classes of machines that we will deﬁne in this chapter, have less computa-
tion power than Turing machines but are very useful in text processing and in
the lexical analysis phase of compilers and interpreters. Another model of com-
putation, called a push-down automaton, is used in parsing (the second phase
of compilers and interpreters); it is a generalisation of the ﬁnite automaton that
includes memory in the form of a stack. These two models of computation are
not Turing complete; that is, they are not powerful enough to express all the

12
Chapter 2. Automata and Turing Machines
computable functions. The Turing machine is a more general automaton that
includes an unlimited memory.
In this chapter, we will deﬁne and compare these three kinds of automata
from the point of view of their power to represent algorithms. But ﬁrst we
will discuss another application of these machines: We can associate to each
automaton a formal language, which is simply the set of sequences of signals
that will take the machine to a speciﬁc state (a ﬁnal state). We call this set
of sequences the language recognised by the automaton. Each of the classes of
automata mentioned above can recognise a diﬀerent class of formal language.
2.1 Formal languages and automata
Formal languages are a particular kind of language that we distinguish from,
for instance, natural languages such as French, English, Spanish, etc. A for-
mal language is a set of words with a given syntax (the rules that govern the
construction of words) and a semantics that gives meaning to the words.
First, we need to specify what we mean by “word”. Formally, we start by
ﬁxing the alphabet of the language. This is just a ﬁnite set X of symbols.
Deﬁnition 2.1 (Language)
A formal language, or simply a language, with alphabet X is a set of words
over X.
A word over X is a sequence of symbols taken from X; that is, a chain or
string of elements in X. The chain could be empty, in which case we will write
it ǫ.
For example, a programming language such as Java or Haskell is a formal
language. It has well-deﬁned syntax rules; for instance, to build a conditional
expression, we use the string “if” followed by a condition, etc.
Once we have deﬁned the alphabet and the set of words in the language, two
questions arise: How do we check that a given word belongs to the language?
How do we generate the words in the language? To answer these questions, we
will build abstract machines — the automata mentioned above.
It is clear that the problem of deciding whether a word belongs to a cer-
tain language or not can be more or less diﬃcult depending on the form of
the language. In the 1950s, the linguist Noam Chomsky classiﬁed formal lan-
guages into four categories according to their expressive power. The ﬁrst two
classes of languages, called regular and context-free, respectively, are useful to

Chapter 2. Automata and Turing Machines
13
describe the syntax of programming languages. The fourth, most general class
of languages has all the expressive power of Turing machines. For each class of
language in Chomsky’s hierarchy, there is an associated class of automata. The
simplest kind, used to recognise regular languages, are called ﬁnite automata;
they are useful to describe the lower-level syntactic units of a programming lan-
guage, and for this reason we ﬁnd them in most compilers (lexical analysers are
speciﬁed as ﬁnite automata). To analyse the syntactic structure of a program,
we need more than a ﬁnite automaton: To recognise context-free languages, we
use push-down automata.
We study ﬁnite automata in the next section. We then go on to deﬁne
push-down automata before giving a description of Turing machines.
2.2 Finite automata
Automata can be seen as abstract machines, or abstract models of computa-
tion. Finite automata are the simplest kind of machines in this family, and the
computations that they can make are very restricted. However, they have im-
portant applications (for instance, in lexical analysis, as mentioned above), and
because of their simplicity they are a useful tool for the study of algorithms.
Finite automata are machines that can be in a ﬁnite number of diﬀerent
states and that respond to external signals by performing a transition; that is,
a change of state (they can also output a message).
Example 2.2
A lift can be modelled as a ﬁnite automaton. It can be in a ﬁnite number of
diﬀerent states (corresponding to its position, the direction in which it is going,
whether the door is closed or open, etc.) and reacts to external signals.
Another example is an automatic door (for instance, the doors at the en-
trance of an airport hall). They can be in a ﬁnite number of states (open or
closed) and react to signals (sent when somebody stands near the door).
A ﬁnite automaton with alphabet X is deﬁned by a ﬁnite number of states
and a set of transitions between states (one transition for each symbol in X).
There is a distinguished state, called the initial state, where the automaton
starts, and one or several ﬁnal states, or accepting states.
A ﬁnite automaton can be represented in diﬀerent ways; we will often use
a graphical representation, which emphasises the fact that automata are tran-
sition machines. However, before giving the graphical representation, we will

14
Chapter 2. Automata and Turing Machines
give a formal and concise deﬁnition of a ﬁnite automaton.
Deﬁnition 2.3 (Finite automaton)
A ﬁnite automaton is a tuple (X, Q, q0, F, δ), where
1. X is an alphabet (that is, a set of symbols);
2. Q is a ﬁnite set of states {q0, . . . , qn} for some n ≥0;
3. q0 ∈Q is the initial state;
4. F ⊆Q is the subset of ﬁnal states; and
5. δ is the transition function δ : Q × X →Q.
We can better visualise this deﬁnition if we give the transition function
as a diagram where the states are represented by points (or circles) and the
change of state (i.e., transition) is represented by an arrow. In other words,
the transition function is represented by edges between nodes in a graph. From
this point of view, a ﬁnite automaton is a directed graph, where the nodes
represent states and an edge from qi to qj labelled by x ∈X represents a
transition δ(qi, x) = qj.
Example 2.4
The automaton A = ({a}, {q0, q1}, q0, {q0}, δ), where δ(q0, a) = q1 and
δ(q1, a) = q0, is represented by
a
a
q0
q1
The arrow indicates the initial state (q0), and the double circle denotes a
ﬁnal state (in this case, the initial state is also ﬁnal).
Another image, depicted in Figure 2.1, can help us see automata as machines
with
– a control unit (states and transition function) and
– a tape with symbols from the alphabet.

Chapter 2. Automata and Turing Machines
15
a1 a2
a3
an−1 an
control
· · ·
Figure 2.1
A ﬁnite automaton depicted as a machine.
The machine is in the initial state at the beginning, with the reading head
in the ﬁrst position of the tape. It reads a symbol on the tape (or receives
an external signal), moves to the next symbol, and makes a state transition
according to the symbol just read. This cycle is repeated until we reach a ﬁnal
state or the end of the tape.
The language associated with a ﬁnite automaton — sometimes referred to
as the language recognised by the automaton — is simply the set of words (or
sequences of signals) that takes it to a ﬁnal state.
Deﬁnition 2.5
A word w over the alphabet X is recognised, or accepted, by a ﬁnite automaton
with alphabet X if the machine described above reaches a ﬁnal state when
started in the initial state on a tape that contains the word w and such that
the reading head is positioned on the ﬁrst symbol of w.
Using our graphical description of automata, we could reformulate the def-
inition of recognised words by saying that a word w over X is recognised by an
automaton with alphabet X if there is a path from the initial state to a ﬁnal
state in the graph that represents the automaton such that all the edges in the
path are labelled by the symbols in the word w (in the same order).
Example 2.6
All the words that contain an even number of symbols a (that is, all the words
of the form (a)2n, where n can also be zero) are recognised by the automaton
given in Example 2.4.

16
Chapter 2. Automata and Turing Machines
Deﬁnition 2.7
The language recognised by an automaton A is the set of words that the au-
tomaton accepts; we will denote it by L(A).
Lexical analysers are often speciﬁed using ﬁnite automata; we give a simple
example below.
Example 2.8
Consider a programming language where the syntax rules specify that iden-
tiﬁers must be ﬁnite sequences of letters or numbers (capitals, punctuation
symbols, and other characters are not allowed), starting with a letter. We can
use the following automaton to recognise strings satisfying the constraints (an
arrow labelled by a . . . z represents a set of arrows, each labelled with one of
the letters a to z, and similarly an arrow labelled with 0 . . . 9 is an abbreviation
for a set of arrows, each labelled with a digit).
a . . . z
a . . . z
0 . . . 9
q0
q1
2.2.1 Deterministic and non-deterministic automata
We deﬁned ﬁnite automata using a transition function δ : Q × X →Q. Thus,
given a state and an input, the next state is uniquely determined — we say
that the automaton is deterministic.
We can generalise this using a relation δ ⊆Q×X ×Q instead of a function.
This means that δ is now deﬁned as a set of triples, where the ﬁrst and third
elements are states and the second element is a symbol from the alphabet.
The idea is that we may have several triples with the same ﬁrst and second
elements. For instance, if δ contains (qi, x, q) and (qi, x, q′), then from the state
qi with input x, either q or q′ can be reached. Thus, given a state and an input,
the automaton can move to a number of diﬀerent states in a non-deterministic
way. Another way to represent this relation is as a function from Q and X to
the set of parts of Q:
δ : Q × X →P(Q)

Chapter 2. Automata and Turing Machines
17
In this way, we can write δ(q, x) to denote all the states that can be reached
from q when the machine reads the symbol x.
One may wonder what is the use of a machine that can make transitions
in a non-deterministic way. Does this kind of behaviour have a computational
meaning? Indeed, a non-deterministic automaton can be understood as a par-
allel machine: When there are several states that can be reached in a transition,
we can think of this as several threads proceeding in parallel.
However, deterministic and non-deterministic ﬁnite automata have the same
computation power. They are equivalent: They recognise exactly the same class
of languages (or implement the same class of functions). This is not the case
with more powerful automata. There are examples of machines for which the
non-deterministic versions are strictly more powerful than the deterministic
ones.
Non-deterministic ﬁnite automata can also be represented graphically. We
will deﬁne them using graphs where nodes correspond to states and edges
describe transitions, with the essential diﬀerence that now we can have several
edges coming out from the same state and labelled by the same symbol.
Example 2.9
The non-deterministic automaton
A = ({a, b, c}, {q0, q1, q2}, q0, {q1, q2}, δ)
where δ(q0, a) = {q1, q2}, δ(q1, b) = {q1}, and δ(q2, c) = {q2}, is represented
in Figure 2.2. We use the same conventions as above: The arrow points to the
initial state, and a double circle indicates a ﬁnal state; in this case we have two
ﬁnal states.
a
a
b
c
q0
q1
q2
Figure 2.2
Diagram of the ﬁnite automaton A

18
Chapter 2. Automata and Turing Machines
The language recognised by a non-deterministic automaton is deﬁned in the
same way as in the case of deterministic automata.
If A = (X, Q, q0, F, δ) is a non-deterministic automaton, the language L(A)
recognised by A is the set of words over X such that there is a path in the graph
representing the automaton A from the initial state to a ﬁnal state, where each
edge in the path is labelled by a symbol in the word.
The main diﬀerence with the previous deﬁnition is that we can now have
several diﬀerent paths labelled by the same word.
For example, the non-deterministic automaton in Figure 2.2 recognises all
the words in the alphabet {a, b, c} that consist of an a followed by either a
string of b or a string of c.
2.2.2 The power of ﬁnite automata
Only the languages in the most basic class in Chomsky’s hierarchy (that is,
regular languages) can be recognised by ﬁnite automata (deterministic or non-
deterministic). In practice, it would be useful to have a simple test to know,
given a language, whether there is some ﬁnite automaton that recognises it. In
this way we could know, for instance, how diﬃcult it would be to implement
an algorithm to recognise words in this language.
We can try to answer this question by studying the properties of the lan-
guages that can be recognised by ﬁnite automata. On the one hand, regular
languages are closed under simple set operations, such as union and intersec-
tion. On the other hand, it is possible to characterise the languages that cannot
be recognised by ﬁnite automata using the so-called Pumping Lemma. We can
use these two kinds of properties to decide whether a certain language can or
cannot be recognised by a ﬁnite automaton.
We will not study in detail the closure properties of regular languages;
instead we ﬁnish this section with the Pumping Lemma. Before giving its formal
statement, we will discuss the intuitive ideas behind this result.
Suppose that a certain language can be recognised by a ﬁnite, deterministic
automaton A with n states. If we consider an input w with more than n sym-
bols, it is obvious that, to recognise w, A will have to repeat at least one of its
states. In other words, there must be a loop in the path representing the tran-
sitions associated with w. Assume w = a1 . . . am (with m > n), and let ai, aj
be the symbols in the ﬁrst and last transitions in the loop (therefore i ≤j). As
a direct consequence of this observation, we can eliminate from w the symbols
ai . . . aj and still the word w′ = a1 . . . ai−1aj+1 . . . am will be recognised by A.
Similarly, we could traverse the loop in the path several times, repeating
this sequence of symbols. Let us write (ai . . . aj)∗to denote words containing 0

Chapter 2. Automata and Turing Machines
19
or more repetitions of the sequence of symbols ai . . . aj. Thus, all the words of
the form a1 . . . ai−1(ai . . . aj)∗aj+1 . . . am will also be recognised by A.
Therefore, if we have a suﬃciently long word w ∈L(A), where A is a ﬁnite
automaton, we can always identify a segment near the beginning of the word,
which we can repeat as many times as we want, and all the resulting words will
also belong to L(A). Using the same reasoning, if, given a language L and a
word w ∈L, there is no segment of w with the property described above, then
we can deduce that L is not a regular language.
Formally, the Pumping Lemma is stated as follows.
Proposition 2.10 (Pumping Lemma)
Let L be a regular language. There exists a constant n such that if z is any
given word in L with more than n symbols, then there are three words, u, v,
and w, such that z can be written as the concatenation uvw, where
1. the length of uv is less than or equal to n,
2. the length of v is greater than or equal to 1, and
3. for any i ≥0, uviw ∈L, where vi represents the word v repeated i times.
The Pumping Lemma indicates that ﬁnite automata have limited compu-
tation power. For instance, we can use the Pumping Lemma to show that the
language of well-balanced brackets (that is, words where each open bracket
has a corresponding closing bracket) cannot be recognised by a ﬁnite automa-
ton. This is indeed a corollary. Suppose, by contradiction, that the language
L of well-balanced brackets is regular, and take the word (n)n; that is, n open
brackets followed by n closed brackets, where n is the constant mentioned in
the Pumping Lemma. Now, using this lemma, we know that there are words u,
v, and w such that (n)n = uvw, the length of uv is less than or equal to n, and
v is not empty. Hence v is built out of open brackets only, and the Pumping
Lemma says that uviw ∈L for all i. Thus, L contains words that do not have
a well-balanced number of brackets, contradicting our assumptions.
Corollary 2.11
The language L containing all the words over the alphabet {(, )} where each
( has a corresponding ) is not regular.
This corollary shows that to check the syntax of programs that contain
arithmetic expressions, we need more than ﬁnite automata.

20
Chapter 2. Automata and Turing Machines
The Pumping Lemma can also be used to show that no ﬁnite automaton
can recognise the language L of strings built out of 0s and 1s such that each
word is formed by the concatenation of a string w and its reverse w:
L = {ww | w is a string of 0s and 1s and w is its mirror image}
No ﬁnite automaton can be used to check whether a given word belongs to
this language or not. To recognise this language, and also the language con-
sisting of words with well-balanced brackets, we need more powerful machines,
such as the push-down automata described in the next section.
2.3 Push-down automata
Push-down automata are a more general version of ﬁnite automata: They
have an additional component — a stack — that provides additional mem-
ory. Thanks to this memory, push-down automata (or PDAs for short) can
recognise some languages that ﬁnite automata cannot recognise; the class of
languages associated with PDAs contains strictly the set of regular languages.
Languages that can be recognised by push-down automata are called context-
free; they are one step up from regular languages in Chomsky’s hierarchy.
Before giving a precise deﬁnition of this class of abstract machines, we will
recall the main operations available in stacks. A stack is a sequence of elements
(possibly empty) where elements can be added on the top and also be taken
out from the top. Stacks are often associated with the acronym “LIFO”, which
stands for “last in, ﬁrst out”, referring to the fact that new elements are pushed
onto the top, and elements are read and removed also from the top.
A push-down automaton can read the top element of the stack (and only
the top element) and can put a new element at the top of the stack. The latter
operation is called push. It is also possible to remove the top element of the
stack. This operation is called pop. The elements in the stack, as well as the
input symbols for the push-down automaton, must belong to a given alphabet.
It is usually assumed that a push-down automaton can use diﬀerent alphabets
for the input and the stack.
The operational behaviour of a push-down automaton can be described sim-
ilarly to a non-deterministic ﬁnite automaton, but there is a crucial diﬀerence:
The transition function is now governed by the input symbol and the symbol
in the top of the stack. The push-down automaton starts on a distinguished
(initial) state with an empty stack. It reads an input symbol and the symbol
from the top of the stack (if the stack is not empty), and according to this

Chapter 2. Automata and Turing Machines
21
pair of values, a transition to a new state (or set of states) is deﬁned. We will
assume that each time a symbol is read from the stack, it is removed.
Thus, the current state, the input symbol, and the symbol at the top of the
stack determine a set of states to which the machine can move. If, at the end of
the input, the machine is in a ﬁnal state (also called an accepting state), then
the word containing the sequence of input symbols read is recognised.
Actually, it is possible to deﬁne transitions that ignore the input symbol
or the value in the top of the stack. These are called ǫ-transitions (recall that
ǫ represents the empty string) because we can always assume that there is an
empty string in front of the ﬁrst input symbol or on top of the element on the
top of the stack.
We are now ready to deﬁne PDA formally.
Deﬁnition 2.12 (Push-down automaton)
A push-down automaton is a tuple (X, Q, Γ, q0, F, δ) where
1. X is an alphabet;
2. Q is a ﬁnite set of states {q0, . . . , qn};
3. Γ is the alphabet of the stack;
4. q0 ∈Q is the initial state;
5. F ⊆Q is the subset of ﬁnal states; and
6. δ is the transition function from tuples containing a state, an input symbol
(or ǫ), and a stack symbol (or ǫ) to sets of pairs made up of a state and
stack:
δ : Q × (X ∪ǫ) × (Γ ∪ǫ) →P(Q × (Γ ∪ǫ))
Note that Γ was not part of the deﬁnition of ﬁnite automata (see Deﬁni-
tion 2.3). The transition function δ for a given state is now deﬁned on pairs
(input, stack-top) and produces as a result a set of pairs (state, stack-top).
Indeed, non-determinism is built into the deﬁnition (because δ returns a set
and because of the existence of ǫ-transitions). If we restrict ourselves to deter-
ministic PDAs, we obtain machines that have strictly less power. In this sense,
the properties of PDAs are diﬀerent from the properties of ﬁnite automata
since for ﬁnite automata the non-determinism of δ does not add any power;
deterministic and non-deterministic ﬁnite automata recognise the same class of
languages.
We can depict a push-down automaton as a machine in the same way as we
represented ﬁnite automata in Figure 2.1; we just need to add a stack, which
the control unit can consult and update.

22
Chapter 2. Automata and Turing Machines
In the previous section, we described two languages that are not recognisable
by ﬁnite automata (we used the Pumping Lemma for this):
1. {(n)n} for any number n (that is, the set of strings containing n opening
brackets followed by the same number of closing brackets);
2. {ww | w is a string of 0s and 1s and w is its mirror image }.
PDAs can recognise these languages because it is possible to use the stack
to memorise a string of symbols of any given length. For instance, to recognise
the ﬁrst language, a push-down automaton can push all the ‘(’ symbols in the
stack and start popping them when it reads a ‘)’ symbol. Then, the word is
accepted if at the end of the input string the stack is empty.
Formally, we deﬁne a push-down automaton recognising the language
{(n)n | n is a natural number}
as follows.
Let Q be the set {q1, q2, q3, q4}, where q1 is the initial state. The input
alphabet, X, contains the symbols ( and ). The stack’s alphabet, Γ, contains
just the symbol ( and a marker 0. The ﬁnal states are q1 and q4, and the
transition function contains the following moves:
δ(q1, ǫ, ǫ)
=
{(q2, 0)}
Starting from the initial state, and without reading
the input or the stack, the automaton moves to
state q2 and pushes 0 onto the stack.
δ(q2, (, ǫ)
=
{(q2, ()}
If in the state q2 the input symbol is (, without
reading the stack the automaton remains in q2
and pushes ( onto the stack.
δ(q2, ), ()
=
{(q3, ǫ)}
If in the state q2 the input symbol is ) and there is
a symbol ( on top of the stack, the automaton moves
to q3; the symbol ( is removed from the stack.
δ(q3, ), ()
=
{(q3, ǫ)}
If in the state q3 the input symbol is ) and there is
a ( on top of the stack, then the automaton remains
in q3; the symbol ( is removed from the stack.
δ(q3, ǫ, 0)
=
{(q4, ǫ)}
If in the state q3 the top of the stack is 0, then the
automaton moves to q4, which is a ﬁnal state.
As usual, the automaton starts in the initial state with an empty stack, but
in this case the ﬁrst transition will put a mark 0 in the stack and move to a
state q2 in which all the open brackets in the input word will be pushed onto
the stack. The presence of a closing bracket in the input word will trigger a
transition to state q3, and an open bracket is popped from the stack. Then, the

Chapter 2. Automata and Turing Machines
23
automaton remains in state q3 while there are closing brackets in the input word
and open brackets in the stack. If the word belongs to the language, the input
word will ﬁnish at the same time as we reach the 0 in the stack. However, if the
input word contains fewer closing brackets, then the automaton will be blocked
in q3, which is not an accepting state. Similarly, if the input word contains more
closing brackets than open brackets, the automaton will be blocked in q4.
Note that the word is recognised only if the automaton has reached a ﬁnal
state at the end of the input. While there are symbols in the input word, even
if the automaton reaches a ﬁnal state, the computation is not ﬁnished (the
automaton remains blocked if no transitions are deﬁned).
Finite automata and PDAs are useful tools for the implementation of com-
pilers and interpreters (typically, lexical analysers are speciﬁed as ﬁnite au-
tomata and parsers are deﬁned using PDAs). Although PDAs are strictly more
powerful than ﬁnite automata, their power is limited. In the previous section,
we used the Pumping Lemma to characterise the class of regular languages. It
is possible to prove a Pumping Lemma for context-free languages, but the char-
acterisation is more involved: Words in context-free languages can be divided
into ﬁve parts, such that the second and fourth parts can be repeated together
any number of times and the result is still a word in the same language.
We will not state the Pumping Lemma for PDAs here; instead we ﬁnish
this section with an example of a language that can be shown to be outside the
class of context-free languages: {anbncn | n ≥0}.
2.4 Turing machines
A Turing machine is a universal model of computation in the sense that all com-
putable functions can be deﬁned using a Turing machine. We can see a Turing
machine as an automaton, a generalisation of a push-down automaton where
we have an unlimited amount of memory and no restriction on the positions
that can be read from this memory (it is no longer a stack).
We can also think of a Turing machine as a speciﬁcation of a formal lan-
guage: The languages that can be recognised by Turing machines form the
topmost category in Chomsky’s hierarchy. However, it is not possible to de-
cide, in general, whether or not a word belongs to the language associated with
a Turing machine. This is a consequence of the undecidability of the Halting
problem: It is not possible to decide, given a Turing machine and an input
word, whether the machine will halt in an accepting state or not.
The memory of a Turing machine is usually represented by an inﬁnite tape
with a head that can read and write symbols and move in both directions on the

24
Chapter 2. Automata and Turing Machines
tape. In this way, the machine can store information on the tape and later move
back to read it. The other important component of the machine is its control
unit, represented by a set of states and a transition function that governs the
changes of state.
Similarly to the other automata discussed in this section, a Turing machine
has a distinguished state called the initial state. The machine always starts
from the initial state and with a tape containing only the input string (that is,
the tape is blank everywhere else). We also assume that the head is positioned
on the ﬁrst symbol of the input string when the machine starts.
The machine will make transitions depending on the symbol that the head
reads, and it can write on the tape and move the head one position to the left
or to the right (this will be indicated by the letters L and R). It continues
until a ﬁnal state is reached. If we think of the machine as recognising a certain
language, then it is useful to include two ﬁnal states qreject and qaccept. If an
input word belongs to the language, the machine will halt in the ﬁnal state
“accept” (qaccept); otherwise it will halt in the ﬁnal state “reject” (qreject) or
it could continue forever, never halting (recall the undecidability results for
languages associated with Turing machines discussed in Chapter 1).
At each point during the computation, the situation of the machine can be
described by giving the state in which the control is, the contents of the tape,
and the position of the head on the tape. These three data deﬁne the machine’s
conﬁguration. So the computation of a Turing machine can be described as a
sequence of conﬁgurations. The transition function indicates how to pass from
one conﬁguration to the next one. This sequence of conﬁgurations can of course
be inﬁnite. If it is ﬁnite, the state in the last conﬁguration must be a ﬁnal state.
Before giving the formal deﬁnition of a Turing machine, let us see an ex-
ample of a language that can be recognised with a Turing machine.
Example 2.13
Consider natural numbers written in unary notation; that is, each number
is represented by a string of 1s. The number 0 is represented by the empty
string, and any positive number n is represented by a sequence containing n
occurrences of the symbol 1. For instance, the number 3 is represented by the
string 111. The language
{12n | n ≥0}
(that is, the language of the strings that represent a power of 2 in unary no-
tation) can be recognised using a Turing machine. The informal description of
the machine is as follows.
Assume we start the machine with a number in unary notation written on
the tape (surrounded by blanks) and the reading head on the leftmost position

Chapter 2. Automata and Turing Machines
25
in this number.
1. If the head is on blank when we start, reject.
2. Otherwise, starting on the ﬁrst 1, move to the end of the string (i.e., the
ﬁrst blank symbol), changing every other 1 into •.
a) If the tape contained just one 1, accept.
b) If the number of 1s was odd, reject.
3. Return the head to the beginning of the input.
4. Repeat.
The idea is that each iteration changes half of the 1s into •. If the number
is a power of 2 in unary notation (that is, if the string contains a number of 1s
that is a power of 2), we will eventually end up with just one 1 and accept the
input.
We have given above a description of an algorithm to check whether the
length of a string of 1s is a power of 2. Below we specify this algorithm formally,
but ﬁrst we will formally deﬁne a Turing machine.
Deﬁnition 2.14 (Turing machine)
A Turing machine is a tuple (Q, X, Γ, δ, q0, F) where
1. Q is a ﬁnite set of states;
2. X is the input alphabet, which cannot contain the blank symbol;
3. Γ is the tape alphabet, containing the input alphabet and the blank symbol;
4. δ is the transition function,
δ : Q × Γ →Q × Γ × {L, R}
5. q0 ∈Q is a distinguished state, called the initial state; and
6. F ⊆Q is the set of ﬁnal states, and we assume that it contains two distin-
guished states, qreject and qaccept.
When the machine is started, the tape contains an input string surrounded
by blanks, the head is in the ﬁrst symbol of the input string, and the machine is
in the state q0. As the computation proceeds, the situation of the machine, or
conﬁguration, is described by a triple containing the current state, the contents
of the tape, and the position of the head on the tape.

26
Chapter 2. Automata and Turing Machines
If the machine reaches the state qaccept, it stops. We say that the machine
has accepted the input. If it reaches the state qreject, it also stops, and we say
that the machine has rejected the input. The machine can also loop forever.
Deﬁnition 2.15
A language L over the alphabet X is recognised by a Turing machine M if the
machine accepts every word in L and rejects every word over X that is not in
L. We will say that a language L is decidable if there is a Turing machine that
recognises L.
Example 2.16
We now give a formal description of the Turing machine that recognises the
language
{12n | n ≥0}
Let Q be the set of states {q1, q2, q3, q4, q5, qreject, qaccept}, where q1 is the
initial state and qreject, qaccept are the ﬁnal states. The input alphabet, X, con-
tains only the symbol 1. The tape alphabet, Γ, also contains 1 and additionally
the blank symbol ◦and a marker •. We give the transition function using a
diagram in Figure 2.3. In the diagram, states are represented as nodes in a
graph and transitions are represented by directed edges (arrows). Each arrow
is labelled by the symbol read, the symbol written, and the direction in which
the head moves. For instance, the arrow from q1 to q2 labelled by 1 ◦R in-
dicates that δ(q1, 1) = (q2, ◦, R). In other words, there is a transition from q1
to q2 when the symbol read is 1; the machine writes ◦and moves to the right.
The arrow pointing to q1 indicates that this is the initial state.
In this example, we have used the machine as a device to recognise the
words of the language; however, a Turing machine can also be seen as a device to
perform computations. In this case, the Turing machine represents an algorithm
that receives an input and computes an output that is written on the tape.
For instance, we can use a Turing machine to compute arithmetic functions
(addition, multiplication, etc.), as the following example shows.
Example 2.17
We describe informally a Turing machine that computes the double of a number
(its input) written in binary notation on the tape. The machine has an initial
state q0 and an accepting state qaccept (qreject is not used in this example). The
input alphabet is {0, 1}, and the tape alphabet is the same, with the addition

Chapter 2. Automata and Turing Machines
27
◦◦L
1 1R
1 1 L
• • L
• • R
• • R
• • R
• • R
◦◦R
◦◦R
◦◦R
◦◦R
1 ◦R
1 • R
1 • R
qreject
qaccept
q1
q2
q3
q4
q5
Figure 2.3
Transition function
of a blank symbol. The machine starts in the initial state, q0, with the head on
top of the ﬁrst binary digit. While there are digits in the input word (either 0
or 1), the machine moves to the right and remains in q0. Finally, when arriving
at a blank symbol, the machine replaces it by 0 and moves to the ﬁnal state
qaccept.
We will say that a Turing machine implements a partial function f from I
to O, where I and O are sets of words (I is the set of possible inputs and O
the set of outputs) if whenever the machine starts from a word w in I, with
the head in the ﬁrst symbol of w, the machine halts in the ﬁnal state qaccept
and leaves the word f(w) on the tape when f is deﬁned in w. If f(w) is not
deﬁned, then the machine may never halt. Thus, if the function f is total (that
is, it is deﬁned for all its possible inputs) and there is a Turing machine that
implements it, we can see the Turing machine as an algorithm to compute the
function (recall that an algorithm must always produce a result).

28
Chapter 2. Automata and Turing Machines
Deﬁnition 2.18 (Turing-computable functions)
A function that can be implemented by a Turing machine is called Turing
computable.
Thus, if a function is Turing computable, a Turing machine that implements
it is a representation of an algorithm to compute the function. The question
that arises is whether all computable functions can be deﬁned in terms of Turing
machines.
Church’s Thesis answers this question positively. It says that, given any
algorithm, there is some Turing machine that implements it. In other words,
if a function is computable, then it is Turing computable (that is, computable
by some Turing machine).
Although this thesis is formally stated, we cannot attempt to prove it be-
cause it is not precise enough; we have not given a formal deﬁnition of algorithm
that is independent from the deﬁnition of Turing machine.
It turns out that all other deﬁnitions of algorithm that have been proposed
(using diﬀerent models of computation, such as the Lambda calculus, recursive
functions, etc.) have turned out to be equivalent to Turing machines. This
provides evidence for Church’s Thesis (but does not prove it).
2.4.1 Variants of Turing machines
The deﬁnition of a Turing machine that we have given is one of many available
in the literature. Some deﬁnitions involve more elements than the one we gave
or impose more restrictions than we did.
For instance, there are non-deterministic Turing machines, which take their
name from the fact that the transition function is non-deterministic. In a non-
deterministic Turing machine, given a conﬁguration, there are several possible
moves, so the next conﬁguration is not uniquely determined.
Similarly to ﬁnite automata (and unlike PDAs), non-deterministic Turing
machines have the same computation power as deterministic ones in the sense
that they recognise the same languages or compute the same functions.
Other variants of Turing machines use several inﬁnite tapes instead of only
one tape, and again it is possible to show that these machines are equivalent
in power to the machines with only one tape.
Still another variant uses one tape but limited in one direction: The tape
has a starting point but has no bound to the right, so the machine can move
an inﬁnite number of steps towards the right. It is possible also to limit the

Chapter 2. Automata and Turing Machines
29
alphabet, for instance by reducing the tape alphabet to just two symbols. Again,
these variants have exactly the same power.
2.4.2 The universal Turing machine
It is possible to give a code for each Turing machine, so that from the code we
can retrieve the machine. An easy way of doing this is as follows.
Assume the machine has n states q0, . . . , qn−1, where each qi is a number,
q0 is the initial state, and the last m states are ﬁnal. Also assume that the
input alphabet is {1} and the tape alphabet is {0, 1} (where 0 plays the role of
a blank symbol). It is well known that a binary alphabet is suﬃcient to encode
any kind of data, so there is no loss of generality in making this assumption.
The transition function can be represented as a table, or equivalently a list of
5-tuples of the form (q, s, q′, s′, d), where q represents the current state, s the
symbol on the tape under the head, q′ the new state, s′ the symbol written
on the tape, and d the direction of movement, which we will write as 0, 1. The
order of the tuples is not important here. Thus, we can assume without loss of
generality that the transition function is represented by a list l of tuples. The
full description of the machine under these assumptions is given by the tuple
(n, m, l), where l is the list representing the transition function, n the number
of states, and m the number of ﬁnal states, as indicated above. We will say that
the tuple is the code for the machine since from it we can recover the original
machine. In fact, the code for the machine is not unique since we can reorder
the list l and still obtain an equivalent machine.
Now we can see the codes of Turing machines as words, and as such they
can be used as input for a Turing machine. It is then possible to deﬁne a Turing
machine U such that, when the code of a machine A is written on the tape,
together with an input word w for A, U decodes it and simulates the behaviour
of the machine A on w. The machine U is usually called the universal Turing
machine.
2.5 Imperative programming
The work done by Turing on abstract computation models had a deep inﬂuence
on the design of computers and later on the design of programming languages
for those computers. The main components of Turing abstract machines are
present in the von Neumann architecture of modern computers: a memory,
which can be thought of as inﬁnite if we consider not only the RAM but also
the storage available through disks and other peripherals; and a control unit

30
Chapter 2. Automata and Turing Machines
that governs the work of the machine. We can see modern computers as im-
plementations of Turing’s universal machine. The memory is one of the main
components of the computer, storing instructions and data, and the other im-
portant component is the processor.
The ﬁrst programming languages were designed to follow closely the phys-
ical design of the machine. The languages that evolved from them, usually
called imperative programming languages, are still inﬂuenced by the architec-
ture of the computer. Imperative languages are abstractions of the underlying
von Neumann machine in the sense that they retain the essential parts but
drop out complicating, superﬂuous details. Low-level languages provide a very
limited level of abstraction, whereas a high-level language can be seen as a
virtual machine where, in general, memory manipulation is transparent for the
programmer and input/output primitives are hardware independent.
Although the level of abstraction provided by imperative languages varies
greatly from assembly languages to sophisticated languages such as Java, there
are common features in the design of all the imperative languages that reﬂect
the underlying machine architecture. The memory and processor, the main
components of the machine, are abstracted in a high-level imperative language
by variables, representing memory space, together with assignment instructions
that modify their contents, and control structures that indicate the order of
execution of instructions in the processor.
The inﬂuence of Turing’s work is not limited to computer architecture and
the design of the ﬁrst imperative programming languages. Abstract machines
based on the notion of a state transition are used nowadays to give a precise
meaning to language constructs in use in imperative languages. Indeed, the
ﬁrst approach to giving a precise, formal description of the behaviour of pro-
gramming language constructs was in terms of an abstract machine, or more
precisely a transition system specifying an interpreter for the programming
language.
2.6 Further reading
For more information on automata theory, we refer the interested reader to [47,
24]. Further information on Church’s Thesis and ways to prove it can be found
in the recent article [12]. For more information on the use of abstract machines
as a tool to describe the semantics of imperative programming languages, we
refer to [15, 37, 53].

Chapter 2. Automata and Turing Machines
31
2.7 Exercises
1. Consider the alphabet {0, 1}. Describe (graphically or formally)
a) a ﬁnite automaton that recognises the language of the strings of 0s of
any length;
b) a ﬁnite automaton that recognises the language of the strings of 0s and
1s that contain a 1 in the second position;
c) a ﬁnite automaton that recognises the language of the strings of 0s and
1s that start and ﬁnish with 00 and do not contain 11.
2. Build ﬁnite automata with alphabet {0, 1} to recognise
a) the language of strings that have three consecutive 0s;
b) the language of strings that do not have three consecutive 1s.
3. Describe a ﬁnite automaton that recognises words over the alphabet
{a, b, c} with an odd number of symbols and such that they do not contain
aa or bb.
4. Let A be a ﬁnite automaton. Show that the set of subwords (that is, pre-
ﬁxes, suﬃxes, or any continuous segment) of the words in the language
L(A) can also be recognised by a ﬁnite automaton.
5. Use the Pumping Lemma to show that the language L containing all the
words of the form anbncn, for any n ≥0, cannot be recognised by a ﬁnite
automaton.
6. How can a push-down automaton recognise the language
{ww | w is a string of 0s and 1s and w is its mirror image}?
Give an informal description of such an automaton.
7. Show that the class of languages recognisable by push-down automata (i.e.,
the class of context-free languages) is closed under union and concatenation
but not under intersection.
8. Describe a Turing machine that recognises the language of the strings w•w,
where w is a string over an alphabet {0, 1}.
9. Deﬁne a Turing machine that, for any word w over the alphabet {0, 1},
outputs ww (that is, the machine starts with w and halts with a tape
containing ww).
10. Show that if a language L over the alphabet X can be recognised by a
Turing machine, then the following languages are also recognisable:

32
Chapter 2. Automata and Turing Machines
a) the complement of L (that is, the set of all the strings over X that are
not in L);
b) the union of L and another decidable language L′;
c) the concatenation of L and another decidable language L′ (that is, the
language consisting of all the words that can be formed by concatenat-
ing a word from L and a word from L′);
d) the intersection of L and another decidable language L′.
11. Deﬁne a Turing machine that accepts the words from the alphabet {a, b, c}
such that the number of occurrences of each character in the word is exactly
the same.

3
The Lambda Calculus
The Lambda calculus, or λ-calculus, is a model of computation based on the
idea that algorithms can be seen as mathematical functions mapping inputs
to outputs. It was introduced by Alonzo Church in the 1930s as a precise
notation for a theory of anonymous functions; its name is due to the use of the
Greek letter λ in functional expressions. Church remarked that when denoting
a function by an expression such as x + y, it was not always clear what the
intended function was. For instance, the expression x + y can be interpreted as
1. the number x + y, where x and y are some given numbers;
2. the function f : x →x + y that associates to a number x the number x + y
for some predetermined value y;
3. the function g : y →x + y that associates to an input y the number x + y
for some predetermined value x; or
4. the function h : x, y →x+y, which takes as arguments x and y and outputs
the value x + y.
This can be a source of ambiguity, and Church proposed a new notation for
functions that emphasises the distinction between variables used as arguments
and variables that stand for predeﬁned values. In this notation, a function with
an argument x is preceded by the symbol λ and the variable x. For instance,
the function f : x →x + y that associates to an input x the number x + y for
some predetermined value y is written λx.x + y. In particular, the functions
mentioned above can be easily distinguished using the λ-calculus notation:
– The number x + y is written just x + y.

34
Chapter 3. The Lambda Calculus
– The function f : x →x + y is written λx.x + y.
– The function g : y →x + y is written λy.x + y.
– The function h : x, y →x + y is written λxy.x + y.
The λ-calculus is a Turing-complete model of computation. It has exactly
the same computational power as Turing machines. Church’s work, like Tur-
ing’s, was motivated by the need to formalise the notion of an algorithm in order
to solve some of Hilbert’s open problems from the 1900 Congress of Mathemati-
cians. In addition to being a useful tool to analyse computability problems, in
recent years the λ-calculus has also been extremely useful to
– give a semantics to programming languages,
– study strategies and implementation techniques for functional languages,
– encode proofs in a variety of logic systems, and
– design automatic theorem provers and proof assistants.
In the rest of this chapter, we will describe the λ-calculus as an abstract
model of computation and also as the foundation for the functional program-
ming paradigm. We ﬁrst give the syntax of terms in the λ-calculus and then
associate computations with terms.
3.1 λ-calculus: Syntax
We assume that there is an inﬁnite, countable set of variables x, y, z, . . . , which
we will use to deﬁne by induction the set of λ-calculus terms (sometimes called
λ-terms, or simply terms if there is no ambiguity). There are three kinds of
terms in the λ-calculus: variables, abstractions, and applications. Below we give
the precise deﬁnition.
Deﬁnition 3.1 (λ-terms)
The set Λ of λ-terms is the smallest set such that:
– All the variables x, y, z, . . . are in Λ (that is, variables are λ-terms).
– If x is a variable and M is a λ-term, then (λx.M) is also a λ-term. Such
λ-terms are called abstractions.
– If M and N are λ-terms, then (M N), called an application, is also a λ-term.

Chapter 3. The Lambda Calculus
35
An abstraction (λx.M) can be seen as a function, where x is the argument and
M is the function body. We apply a function to a concrete argument by juxta-
posing the function and its argument; if M is a function and N its argument,
then the pair (M N) represents the application of M to N.
It is traditional to use some conventions to simplify the syntax, avoiding
writing too many brackets. In particular:
– We will omit the outermost brackets in abstractions and applications when
there is no ambiguity.
– Application associates to the left, so instead of writing ((MN)P) we will
simply write M N P (by default, if there are no brackets, the left association
determines the order of application).
– Abstraction associates to the right, so instead of writing λx.(λy.M) we simply
write λx.λy.M, or even shorter, λxy.M.
– We will assume the scope of a λ is as “big” as possible. In order to shorten
it, we will use brackets. For example, we write λx.y x instead of λx.(y x),
and we write (λx.y) x to limit the scope.
Example 3.2
The following are examples of λ-terms:
– x.
– λx.x — This term represents a function that takes an argument x and returns
just x. It is the identity function.
– λx.λy.x — This term represents a function that takes two arguments, x and
y, and returns the ﬁrst one.
– λx.λy.y — This term also represents a function with two arguments, but the
result is the second one.
– λx.λy.xy — Here the function has two arguments, and the result is obtained
by applying the ﬁrst one to the second one. Although we have not mentioned
types yet, it is clear that this term will make sense if the ﬁrst argument is
itself a function.
– λx.xx — This term is usually called self-application. It denotes a function
that takes an argument x and applies it to itself.
– λx.y — Here we have a function that takes an argument x but does not use
it at all. The result of the function is y.

36
Chapter 3. The Lambda Calculus
– λx.yx — In this case, x is used as an argument for the function y (a parameter
in this expression).
– λxyz.xz(yz) — This is an interesting term that takes three arguments. The
ﬁrst is applied to the third and to the result of the second applied to the third.
Here it is important to put the brackets in the expression (yz); otherwise,
according to the conventions, we would apply x to three arguments, z, y, z,
instead of two.
In a λ-term, it is important to distinguish between variables that are associ-
ated with a λ in an abstraction and variables that do not have a corresponding
λ. More precisely, in a λ-abstraction λx.M, the variable x is bound inside M.
The variables that are not bound by a λ are said to be free. For example, in
the term λx.yx, the variable x is bound, whereas y is free.
In fact, to be precise, we should talk about free and bound occurrences of
variables since the same variable may occur many times in a term and some
of the occurrences may be bound while others are free. For instance, in the
λ-term x(λx.x), the leftmost occurrence of x is free, but since we have on the
right a λ-abstraction for x, the occurrence of x in the body of the abstraction is
bound. Thus, each occurrence of a variable in a λ-term may be free or bound,
depending on whether it is under the scope of a corresponding λ or not.
The set of free variables of a λ-term M will be denoted FV (M). It is deﬁned
by induction below.
Deﬁnition 3.3 (Free variables)
We deﬁne the set of free variables of M, FV (M), as a recursive function. There
are three cases, depending on whether M is a variable, an abstraction, or an
application:
FV (x)
=
{x}
FV (λx.M)
=
FV (M) −{x}
FV (MN)
=
FV (M) ∪FV (N)
The deﬁnition above is an example of an inductive deﬁnition: We have
deﬁned the set of free variables of a term by induction on the structure of the
term. There is a case for each kind of λ-term. In the case of a variable, there is
no λ and therefore the variable is free. In an abstraction, the variable attached
to the λ is bound in the body, so it is not in the set of free variables. For
an application, we compute the set of free variables of the function and the
argument and take the union.
Terms without free variables are called closed terms. They are also called
combinators.

Chapter 3. The Lambda Calculus
37
Example 3.4
Using the deﬁnition above, we can easily see that the term λz.z is closed since
the only occurrence of z is bound by the leading λz. In the terms λx.z and
zλx.x, the variable z occurs free. The term λxyz.xz(yz) is closed since all the
variables are bound by a λ.
Similarly, we can deﬁne the set of bound variables of a term as follows.
Deﬁnition 3.5 (Bound variables)
The function BV computes the set of bound variables of a term:
BV (x)
=
∅
BV (λx.M)
=
{x} ∪BV (M)
BV (MN)
=
BV (M) ∪BV (N)
Note that, according to the previous deﬁnitions, FV (x (λx.x)) = {x} and
BV (x (λx.x)) = {x}; this is because, as explained above, the ﬁrst occurrence
of x is free but the second is bound by a λ.
In general, if we say that a variable is free in a term, it means that there is
at least one free occurrence of this variable.
Since an abstraction λx.M is the representation of a function that uses x
as a formal parameter, it is clear that we should obtain an equivalent function
if we chose a new variable z, changed x to z, and consistently renamed the
occurrences of x in M as z. In other words, the name of a bound variable is
not important. We can see the variable just as a placeholder, or a marker that
indicates the positions where the argument will be used. Since the name of a
bound variable is not important, λ-terms that diﬀer only in the names of their
bound variables will be equated. More precisely, we will take the quotient of the
set of λ-terms by an equivalence relation, called
α-equivalence, that equates
terms modulo renaming of variables. The renaming of x by y will be denoted
{x →y}.
The operation of renaming should be done in a consistent way to preserve
the meaning of the term. In particular, we should not capture variables during
the renaming process. We say that a variable has been captured if it was free
before renaming and it becomes bound after renaming. For instance, if we
rename x as y in the term λy.xy, the occurrence of x becomes y and therefore
becomes bound by the leading λ. This is a problem because the meaning of
the function has changed. Before renaming, we had a function with argument y
that applies some predeﬁned x to y, whereas after renaming we have a function
that takes an argument y and applies it to itself. We can solve the problem by

38
Chapter 3. The Lambda Calculus
ﬁrst changing y to a diﬀerent name, for instance z. More precisely,
(λy.xy){x →y} = (λz.xz){x →y} = (λz.yz)
Using renamings, we can now deﬁne the α-equivalence relation inductively.
Deﬁnition 3.6 (α-equivalence)
The α-equivalence relation on λ-terms, denoted by =α, is generated by the
following rules:
– M =α N if M and N are exactly the same variable: M = N = x.
– M =α N if M = M1M2, N = N1N2 and M1 =α N1, M2 =α N2.
– M =α N if M = λx.M1, N = λx.N1 and M1 =α N1.
– M =α N if M = λx.M1, N = λy.N1 and there is a fresh variable z such that
M1{x →z} =α N1{y →z}.
It is an equivalence relation (i.e., it is reﬂexive, symmetric, and transitive).
The following are concrete examples of α-equalities:
– λx.x =α λy.y.
– λx.λy.xy =α λz1.λz2.z1z2.
– (λx.x)z =α (λy.y)z.
In what follows, we will consider λ-terms as representatives of equivalence
classes for the α-equality relation. More precisely, λ-terms are deﬁned modulo
α-equivalence, so λx.x and λy.y are the same term. Indeed, we will see that
α-equivalent terms have the same computational behaviour.
3.2 Computation
Abstractions represent functions that can be applied to arguments. The main
computation rule, called
β-reduction, indicates how to ﬁnd the result of a
function (i.e., its output) when an argument (i.e., input) is provided.
A redex is a term of the form
(λx.M)N
It represents the application of the function λx.M to the argument N. To
obtain the result, the intuitive idea is that we need to perform the operations

Chapter 3. The Lambda Calculus
39
indicated in the body of the function using the concrete argument N instead
of the formal argument x. In other words, inside M (the body of the function
λx.M) we have to replace the formal argument x by the concrete argument N.
This is the main computation rule in the λ-calculus. Formally, we deﬁne it as
follows.
Deﬁnition 3.7 (β-reduction rule)
The reduction scheme
(λx.M)N →β M{x →N}
where (λx.M)N is a redex and M{x →N} represents the term obtained when
we substitute x by N in M is called the β-reduction rule.
We write M →β M ′ to indicate that M reduces to M ′ using the β-rule.
We will say that the redex (λx.M)N β-reduces, or simply reduces, to the
term M{x →N}, where {x →N} is a substitution. The notion of substitution
used here is subtle since we have to take into account the fact that λ-terms are
deﬁned modulo α-equivalence. We give the precise deﬁnition of substitution
below.
The β-reduction rule can be used to reduce a redex anywhere in a λ-term,
not necessarily at the top. In other words, we can reduce a subterm inside a
λ-term. We say that the rule generates a relation that is closed by context
(sometimes this is called a compatible relation). Closure by context can be
formally deﬁned as follows.
Deﬁnition 3.8 (β-reduction relation)
A context, denoted C[−], is a λ-term with one free occurrence of a distinguished
variable −. We write C[M] to denote the term obtained by replacing −with
M.
The β-reduction relation is a binary relation containing all the pairs
(λx.M)N →β M{x →N} generated by the β-reduction rule and in addition
all the pairs C[M] →β C[M ′] such that M →β M ′.
We write M →β M ′ to indicate that the pair of terms M and M ′ belongs
to the β-reduction relation, and we say that M reduces to M ′ in one step.
It is also useful to have a notation for terms that are related through a chain
of zero or more reduction steps. We write M →∗
β M ′ if there is a sequence of
terms M1, . . . , Mn (where n ≥1) such that M = M1 →β M2 →β . . . →β Mn =
M ′. Notice that, if n = 1, the sequence of reduction steps is empty and M ′ is
M itself. The relation →∗
β is the reﬂexive and transitive closure of →β.

40
Chapter 3. The Lambda Calculus
Before giving the formal deﬁnition of substitution, we show some simple
examples of reduction.
Example 3.9
– The redex (λx.x)y denotes the application of the identity function to the
argument y. The expected result is therefore y. We can see that β-reduction
computes exactly that. We have a reduction step (λx.x)y →β x{x →y},
where x{x →y} represents the term obtained by replacing x by y in x (that
is, the term y).
– More interestingly, the term (λx.λy.x)(λz.z)u has a β-redex on the left:
(λx.λy.x)(λz.z). This β-redex reduces to the term λy.λz.z. Since β-reduction
is closed by context, we have a step of reduction (λx.λy.x)(λz.z)u →β
(λy.λz.z)u. The latter still has a β-redex, and can be further reduced to
λz.z. Thus,
(λxλy.x)(λz.z)u →∗
β λz.z
– We also have a reduction sequence:
(λx.λy.xy)(λx.x) →β λy.(λx.x)y →β λy.y
Note that we use the word “reduce”, but this does not mean that the term
on the right is any simpler. For example, if the function is the self-application
term λx.xx and we apply it to the last term in Example 3.2, we have a reduction
step:
(λx.xx)(λxyz.xz(yz)) →β (λxyz.xz(yz))(λxyz.xz(yz))
3.2.1 Substitution
Substitution in the λ-calculus is a special kind of replacement. M{x →N}
means replace all free occurrences of x in M by the term N without capturing
free variables of N.
The reason why we only replace free occurrences of variables is clear: λ-
terms are deﬁned modulo α-equivalence; bound variables stand for unknown
arguments of functions.
The deﬁnition of substitution also takes into account the fact that in re-
placing x by N inside a λ-term M we should preserve the meaning of the term
N. In particular, if N contains free variables, they should remain free after the
replacement has been done. For instance, it would be wrong to replace y in
λz.yz by a term containing z free. Indeed, consider the substitution {y →z}

Chapter 3. The Lambda Calculus
41
and the term λz.yz. If we replace without taking into account binders, we ob-
tain (λz.zz) — the self-application. However, since λz.yz is a representative
of an equivalence class, we could have taken instead any other representative,
for instance λx.yx, which is α-equivalent. The replacement in this case would
produce λx.zx, which is not a self-application. In the ﬁrst case, we say that the
variable z was captured; this is something that should be avoided.
To avoid capturing variables, it is suﬃcient to rename the bound variables
appropriately. The operation of renaming boils down to choosing a diﬀerent
representative of an α-equivalence class, which is permitted since λ-terms are
deﬁned modulo α-equivalence.
We are now ready to deﬁne, by induction, the operation of substitution of
a variable x by a term N in M, avoiding capture.
Deﬁnition 3.10 (Substitution)
The result of M{x →N} is deﬁned by induction on the structure of M, with
cases for variable, application, and abstraction. If M is a variable, there are
two subcases, depending on whether M is x or a diﬀerent variable. The case
for abstraction is also divided into subcases:
x{x →N}
=
N
y{x →N}
=
y
(PQ){x →N}
=
(P{x →N})(Q{x →N})
(λx.P){x →N}
=
(λx.P)
(λy.P){x →N}
=
λy.(P{x →N})
if x ̸∈FV (P)
or y ̸∈FV (N)
(λy.P){x →N}
=
(λz.P{y →z}){x →N}
if x ∈FV (P)
and y ∈FV (N),
where z is fresh
In the last line, we have used a fresh variable z; that is, a variable that does
not occur in the expressions under consideration. This is to avoid capturing the
variable y that occurs free in the term N.
Example 3.11
Let us apply the deﬁnition above to compute (λz.yz){y →z}; in other words,
we will compute the result of the substitution {y →z} on the term λz.yz. In
this case, the term is an abstraction with bound variable z, and y is free in the
body of the abstraction. Also, the substitution will replace y by a term that
contains z free (the term to be substituted for y is precisely z). Therefore, we
are in the last case of the deﬁnition above, and before replacing y we need to

42
Chapter 3. The Lambda Calculus
rename the bound variable. Since x does not occur in any of the expressions in
our example, we can choose x as a fresh variable. Thus
(λz.yz){y →z} = (λx.(yz){z →x}){y →z}
According to the deﬁnition above,
(yz){z →x} = (y{z →x})(z{z →x}) = (yx)
Using this equality, we obtain
(λz.yz){y →z} = (λx.yx){y →z}
and now we can apply the second case for abstraction in Deﬁnition 3.10 together
with the cases for application and variables, obtaining
(λz.yz){y →z} = (λx.zx)
A useful property of substitution is the following, known as the Substitution
Lemma.
Property 3.12
If x ̸∈FV (P), (M{x →N}){y →P} = (M{y →P}){x →N{y →P}}.
3.2.2 Normal forms
Computation in the λ-calculus is a reduction process using the β-rule. A natural
question arises: When do we stop reducing? In other words, if computation is
reduction, we need to know when we have found the result.
There are several notions of “result” in the λ-calculus; we deﬁne two below.
– Normal form: A simple answer to the question above is: Stop reducing when
there are no redexes left to reduce.
A normal form is a term that does not contain any redexes.
A term that can be reduced to a term in normal form is said to be normalis-
able. Formally, M is normalisable if there exists a normal form N such that
M →∗
β N. For example,
λabc.((λx.a(λy.xy))b c) →β λabc.(a(λy.by)c)
and the latter is a normal form (recall that application associates to the left).

Chapter 3. The Lambda Calculus
43
– Weak head normal form: Another notion of result that is very useful in
functional programming requires reducing the β-redexes that are not under
an abstraction. In other words, we stop reducing when there are no redexes
left but without reducing under an abstraction. For example,
λabc.((λx.a(λy.xy))b c)
is a weak head normal form but not a normal form.
3.2.3 Properties of reductions
Since we can view a λ-term as a program and normal forms as a notion of
result, it is important to study the properties of the reduction relation that
will allow us to obtain the result associated with a program. The ﬁrst question
here is whether, given a program, there is a result at all. If there is a result, we
may wonder whether that result is unique. Indeed, each term has at most one
normal form in the λ-calculus, and some terms have none.
Some of the most interesting properties of reduction relations are stated
below.
– Conﬂuence: A reduction relation is conﬂuent if peaks of reductions (i.e., two
reduction sequences branching out of the same term) are always joinable.
More precisely, →is conﬂuent if, whenever M →∗M1 and M →∗M2, there
exists a term M3 such that M1 →∗M3 and M2 →∗M3.
The β-reduction relation in the λ-calculus is conﬂuent.
– Normalisation: A term is normalisable if there exists a sequence of reductions
that ends in a normal form.
Some λ-terms are not β-normalisable.
– Strong Normalisation (or Termination): A term M is strongly normalisable,
or terminating, if all reduction sequences starting from M are ﬁnite.
The λ-calculus is conﬂuent but not strongly normalising (or even normal-
ising), as witnessed by the term (λx.xx)(λx.xx); this term is usually called
Ω.
Each λ-term has at most one normal form: this unicity of normal forms is a
consequence of the conﬂuence property of β-reduction (the proof of this result
is left as an exercise; see Section 3.8).

44
Chapter 3. The Lambda Calculus
3.2.4 Reduction strategies
If a term has a normal form, there may be many diﬀerent reduction sequences
leading to that normal form (and the same can happen if we try to reduce to
a weak head normal form). For instance, we can build the following kinds of
reduction sequences:
– Leftmost-outermost reduction: If a term has several redexes, we ﬁrst reduce
the one at the leftmost-outermost position in the term; that is, the ﬁrst redex
starting from the left that is not contained in any other redex.
– Leftmost-innermost reduction: If a term has several redexes, we ﬁrst reduce
the one at the leftmost-innermost position in the term; that is, the ﬁrst redex,
starting from the left, that does not have any other redex inside.
A function that, given a term, selects a position to reduce is called a strategy.
Leftmost-outermost and leftmost-innermost are two examples of strategies. The
choice of strategy can make a huge diﬀerence in how many reduction steps are
needed and on whether we may ﬁnd a normal form at all (when one exists).
The leftmost-outermost strategy always ﬁnds the normal form of the term if
there is one. For this reason, it is usually called a normalising strategy. However,
it may be ineﬃcient (in the sense that there may be other strategies that ﬁnd
the normal form in fewer reduction steps). We show a simple example below.
Example 3.13
Consider the term λx.xxx and assume we apply it to the term (λy.y)z. We
give two reduction sequences below; the ﬁrst one follows the leftmost-outermost
strategy, whereas the second one is a leftmost-innermost reduction.
Leftmost-outermost reduction:
(λx.xxx)((λy.y)z)
→β
((λy.y)z)((λy.y)z)((λy.y)z)
→β
z((λy.y)z)((λy.y)z)
→β
zz((λy.y)z)
→β
zzz
Leftmost-innermost reduction:
(λx.xxx)((λy.y)z)
→β
(λx.xxx)z
→β
zzz
Most functional programming languages reduce terms (more precisely, pro-
grams) to weak head normal form (i.e., they do not reduce under abstractions).
This is because if the normal form of a program is an abstraction, that means

Chapter 3. The Lambda Calculus
45
the result is a function; reduction does not proceed until some arguments are
provided.
Although reduction to weak head normal form is standard in functional
languages, there is no consensus as to which is the best strategy of reduction
to implement. Several choices are possible:
1. Call-by-name (also called normal order of reduction): Arguments are eval-
uated each time they are needed. This corresponds to an outermost reduc-
tion.
2. Call-by-value (also called applicative order of reduction). Arguments are
evaluated ﬁrst and the reduced terms are then used in the substitution
(avoiding duplication of work). This corresponds to an innermost reduction.
3. Lazy evaluation: Arguments are evaluated only if needed, and at most once.
Lazy evaluation is similar to call-by-name in that arguments are evaluated
when they are needed, but it imposes a further restriction in that they are
only evaluated once to improve eﬃciency.
Most functional languages implement either a call-by-value or a lazy evalu-
ation strategy.
3.3 Arithmetic functions
The syntax of the λ-calculus is very simple: Terms can be variables, applica-
tions, or λ-abstractions. We have not given any syntax to represent numbers or
data structures. It turns out that no additional syntax is necessary for this. It
is possible to represent numbers, and general data structures, in the λ-calculus,
as we will see below.
Deﬁnition 3.14 (Church numerals)
We can deﬁne the natural numbers as follows:
0
=
λx.λy.y
1
=
λx.λy.x y
2
=
λx.λy.x(x y)
3
=
λx.λy.x(x(x y))
...
These are called Church integers or Church numerals. Below we write n to
denote the Church numeral representing the number n.

46
Chapter 3. The Lambda Calculus
Using this representation of numbers, we can deﬁne the arithmetic func-
tions. For example, the successor function that takes n and returns n + 1 is
deﬁned by the λ-term
S = λx.λy.λz.y(x y z)
To check it, we see that when we apply this function to the representation
of the number n, we obtain the representation of the number n + 1:
Sn
=
(λx.λy.λz.y((x y)z))(λx.λy.x . . . (x(x y)))
→β
λy.λz.y((λx.λy.x . . . (x(x y)) y)z)
→∗
β
λy.λz.y(y . . . (y(y z)) = n + 1
In the rest of the chapter, we will often call the Church numerals simply “num-
bers”. In general, to deﬁne an arithmetic function f that requires k arguments
f : Natk →Nat
we will use a λ-term λx1 . . . xk.M, which will be applied to k numbers:
(λx1 . . . xk.M)n1 . . . nk.
For example, the following term deﬁnes addition:
ADD = λx.λy.λa.λb.(x a)(y a b)
We can check that this term indeed behaves like the addition function by ap-
plying it to two arbitrary Church numerals m and n and computing the result
(which will be m + n). We leave this as an exercise; see Section 3.8.
3.4 Booleans
We can also represent the Boolean values True and False, as well as the Boolean
functions, using just variables, abstraction, and application.
Deﬁnition 3.15 (Booleans)
We deﬁne the constants True and False by the following terms:
False
=
λx.λy.y
True
=
λx.λy.x
Using these representations, we can now deﬁne Boolean functions such as
NOT, AND, OR, etc. For example, the function NOT is deﬁned by the λ-term
NOT = λx.x False True

Chapter 3. The Lambda Calculus
47
We can check that this deﬁnition is correct by applying it to the represen-
tation of the Boolean constants:
NOT False = (λx.x False True)False →β False False True →β True
and
NOT True = (λx.x False True)True →β True False True →β False
Using the same ideas, we can deﬁne a λ-term that behaves like a conditional
construct in a programming language. We will call it IF, and it is the λ-calculus
implementation of an if-then-else statement:
IF = λx.λy.λz.x y z
It is easy to see that
IF B E1 E2 →∗
β E1
if B = True
and
IF B E1 E2 →∗
β E2
if B = False
Instead of IF B E1 E2, we may write IF B THEN E1 ELSE E2.
Example 3.16
The function is-zero? can be deﬁned as
λn.n(True False)True
Then
is-zero? 0 →∗
β True
and
is-zero? n →∗
β False
if n > 0.
We can use IF and is-zero? to deﬁne the SIGN function:
SIGN = λn.IF (is-zero? n) THEN 0 ELSE 1

48
Chapter 3. The Lambda Calculus
3.5 Recursion
Assume we know how to compute multiplication, the predecessor, and a test for
zero, and we want to deﬁne the familiar factorial function on natural numbers
(that is, a function that associates with 0 the value 1 and for any number n > 0
it evaluates to the product of n and the factorial of n −1). Our goal will be to
deﬁne a λ-term FACT that when applied to a number produces as a result the
factorial of this number. In other words, the normal form of FACT n should be
the number representing the factorial of n.
As a ﬁrst attempt, we can write
FACT
=
λn.IF (is-zero? n)
THEN 1
ELSE (MULT n (FACT (PRED n))
However, this is not a well-deﬁned λ-term since we are using inside the
deﬁnition of FACT the term FACT that we are trying to deﬁne!
There is a solution to this problem via the so-called ﬁx point operators of
the λ-calculus. A ﬁx point operator is a λ-term that computes ﬁx points, or in
other words a λ-term Y such that for any term M
Y M =β M(Y M)
where =β is the reﬂexive, symmetric, and transitive closure of →β. In this case,
we say that Y computes the ﬁx point of M.
For instance, we can take
Y = λh.(λx.h(x x))(λx.h(x x))
or even better
YT = AA
where
A = λa.λf.f(aaf)
since using the latter we can compute ﬁx points by reduction:
YT F = AAF = (λa.λf.f(aaf))AF →β (λf.f(AAf))F →β F(AAF)=F(YT F)
The term YT is usually called Turing’s ﬁx point combinator and has the
property that, for all terms M, YT M →∗
β M(YT M), as shown above. Thanks
to this property, we can use YT to deﬁne recursive functions. For example,
consider the following deﬁnition of a term H:
H = λf.λn.IF (is-zero? n) THEN 1 ELSE (MULT n (f(PRED n)))
Now the factorial function can be deﬁned as the ﬁx point of H:
FACT = YT H

Chapter 3. The Lambda Calculus
49
We then have the reduction sequence
FACT n
= YT Hn →∗
β H(YT H)n →∗
β
IF (is-zero? n) THEN 1 ELSE (MULT n (YT H(PRED n)))
where YT H(PRED n) is, by our deﬁnition, FACT(PRED n), as required.
Thus, although functions in the λ-calculus are anonymous, we can simulate
a recursive “call” using a ﬁx point operator. This is a technique that we can
use to deﬁne recursive functions in general.
3.6 Functional programming
The work done by Church on abstract computation models based on the math-
ematical theory of functions has had a deep inﬂuence on the design of modern
functional programming languages. The λ-calculus can be seen as the abstract
model of computation underlying functional programming languages such as
LISP, Scheme, ML, and Haskell.
The main domains of application for functional languages up to now have
been in artiﬁcial intelligence (for the implementation of expert systems), text
processing (for instance, the UNIX editor emacs is implemented in LISP),
graphical interfaces, natural language, telephony, music composition, symbolic
mathematical systems, theorem provers, and proof assistants.
When developing software applications, properties such as low maintenance
cost, easy debugging, or formally provable correctness nowadays have a high
priority. For example, in safety-critical domains (such as medical applications,
telecommunications, or transport systems) it is important to develop programs
whose correctness can be certiﬁed (i.e., formally proved). Functional languages
are a good alternative to imperative languages in this case since functional
programs are in general shorter and are easier to debug and maintain than their
imperative counterparts. For these reasons, functional programming languages
are becoming increasingly popular.
LISP, introduced by John McCarthy in the 1950s, is considered to be the
ancestor of all functional programming languages. The syntax of LISP is based
on lists (as the name of the language suggests: LISt Processing), and the atomic
elements of lists are numbers and characters. Its conciseness and elegance have
made LISP a very popular language. Since data and programs are represented
as lists, it is easy in LISP to deﬁne higher-order functions (that is, functions
that take other functions as their argument or produce functions as their result).
This style of programming is one of the main features of functional languages.
Several versions of LISP are in use, including Scheme.

50
Chapter 3. The Lambda Calculus
Modern functional languages such as ML and Haskell have radically changed
the syntax and introduced sophisticated type systems with type inference ca-
pabilities. These modern functional languages are still based on the λ-calculus,
but their design is also inﬂuenced by the theory of recursive functions developed
by G¨odel and Kleene, which we discuss in the next chapter. We will come back
to the relationship between functional programming languages and abstract
computation models at the end of the next chapter.
3.7 Further reading
More information on the λ-calculus can be found in [21]. Barendregt’s book [4]
is a comprehensive reference. For an introduction to functional programming
we recommend the books by Bird [6] and by Cousineau and Mauny [10]. For
a description of the functional languages mentioned above, see the following
references: LISP [32], Scheme [50], ML [35], and Haskell [51].
3.8 Exercises
1. Compute the sets of free and bound variables for the terms in Example 3.2.
2. Write the result of the following substitutions.
a) x{y →M}, where M is an arbitrary λ-term
b) (λx.xy){y →(xx)}
c) (λy.xy){y →(xx)}
d) (xx){x →λy.y}
3. Compute the normal forms of the following terms
a) λy.(λx.x)y
b) λy.y(λx.x)
c) II
d) KI
e) KKK
where K = λxy.x and I = λx.x.

Chapter 3. The Lambda Calculus
51
4. Diﬀerent notions of normal form were discussed in this chapter, including
the full normal form (or simply normal form) and weak head normal form.
a) What is the diﬀerence between a term having a normal form and being
a normal form? Write down some example terms.
b) If a closed term is a weak head normal form, it has to be an abstraction
λx.M. Why?
c) Indicate whether the following λ-terms have a normal form:
– (λx.(λy.yx)z)v
– (λx.xxy)(λx.xxy)
d) Show that the term Ω = (λx.xx)(λx.xx) does not have a normal form.
Find a term diﬀerent from Ω that is not normalising (i.e., a term such
that every reduction sequence starting from it is inﬁnite).
5. Explain why if a reduction system is conﬂuent, then each term has at most
one normal form.
6. Show leftmost-outermost and leftmost-innermost reductions for the follow-
ing terms:
– G (F x) where
G
=
λx.xxx
F
=
λyz.yz
– ΘΘΘ
– Θ(ΘΘ)
where
Θ
=
λx.xKSK
S
=
λxyz.xz(yz)
K
=
λxy.x
7. In your view, which are the best and worst reduction strategies for a func-
tional programming language? Give examples to support your claims.
8. In this chapter, we have shown how to deﬁne arithmetic operations using
Church numerals.
a) Check that the term ADD = λxyab.(xa)(yab) behaves like the addi-
tion function; that is, show that when we apply ADD to two Church
numerals, we obtain the Church numeral representing their sum.
Hint: Reduce the term (λx.λy.λa.λb.(xa)(yab))n m.

52
Chapter 3. The Lambda Calculus
b) Show that the λ-term MULT = λx.λy.λz.x(yz) applied to two Church
numerals m and n computes their product m × n.
c) Which arithmetic operation does the term λn.λm.m (MULT n) 1 com-
pute?
9. Check that the following deﬁnitions are correct by applying them to the
Boolean constants:
AND = λx.λy.x y x
OR = λx.λy.x x y
10. Consider the model of computation deﬁned as the restriction of the λ-
calculus to the set of linear terms. Linear terms are inductively deﬁned as
follows:
– A variable is a linear term.
– If x occurs free in a linear term M, then λx.M is a linear term.
– If M and N are linear terms and the sets of free variables of M and N
are disjoint, then (M N) is a linear term.
In λx.M, the variable x is bound; terms are deﬁned modulo α-equivalence
as usual.
a) Show that λx.λy.xy is a linear term according to the deﬁnition above,
and give an example of a non-linear term.
b) The computation rule in the linear λ-calculus is the standard β-
reduction rule. Indicate whether each of the following statements is
true or false and why.
i. In the linear λ-calculus, we can ignore α-equivalence when we apply
the β-reduction rule.
ii. If we β-reduce a linear term, we obtain another linear term.
iii. The linear λ-calculus is conﬂuent.
iv. Every sequence of reductions in the linear λ-calculus is ﬁnite (in
other words, the linear λ-calculus is terminating).
v. The linear λ-calculus is a Turing-complete model of computation.
11. Combinatory logic (CL for short) is a universal model of computation.
Terms in the language of CL are built out of variables x, y, . . ., constants
S and K, and applications (M N). More precisely, terms are generated by
the grammar
M, N ::= x | S | K | (M N)

Chapter 3. The Lambda Calculus
53
The standard notational conventions are used to avoid brackets: Applica-
tions associate to the left, and we do not write the outermost brackets. For
instance, we write
K x y for the term ((K x) y).
There are two computation rules in combinatory logic:
K x y
→
x
S x y z
→
x z (y z)
a) Using the rules above, there is a sequence of reduction steps
SKKx →∗x
Show all the reduction steps in this sequence.
b) The term SKK can be seen as the implementation of the identity
function in this system since, for any argument x, the term SKKx
evaluates to x.
Show that SKM, where M is an arbitrary term, also deﬁnes the iden-
tity function.
c) Consider the system of combinatory logic without the second compu-
tation rule (that is, only the rule Kxy →x may be used). We call this
weaker system CL−.
We call CL+ the system of combinatory logic with an additional con-
stant I and rule Ix →x.
Indicate whether each of the following statements is true or false and
why.
i. In CL−, all the reduction sequences are ﬁnite.
ii. The system CL+ has the same computational power as the system
CL.
iii. The system CL−is Turing complete.

4
Recursive Functions
In the previous chapters, we discussed the notion of a computable function and
characterised this class of functions as the ones that can be deﬁned via Turing
machines or the λ-calculus. In this chapter, we give an alternative character-
isation of computable functions based on the notion of a recursive function.
Usually, we say that a function is recursive if it “calls itself”. Recursive func-
tions are functions for which the result for a certain argument depends on the
results obtained for other (smaller in some sense) arguments. Recursion is a
very useful tool in modern programming languages, in particular when dealing
with inductive data structures such as lists, trees, etc.
The theory of recursive functions was developed by Kurt G¨odel and Stephen
Kleene in the 1930s. In this chapter, we will deﬁne the general class of partial
recursive functions. These are functions on numbers, each one with a ﬁxed
arity; that is, with a speciﬁc number of arguments. In the deﬁnition of recursive
functions, we will identify some basic functions that serve as building blocks in
our characterisation of computability. We will also identify some mechanisms
that can be used to combine functions, so that starting from the basic initial
functions we can obtain a class of functions that is equivalent to the class of
functions that can be deﬁned, for instance, in the λ-calculus.
Primitive recursive functions play an important role in the formalisation of
computability. Intuitively speaking, partial recursive functions are those that
can be computed by Turing machines, whereas primitive recursive functions can
be computed by a speciﬁc class of Turing machines that always halt. Many of
the functions normally studied in arithmetic are primitive recursive. Addition,
subtraction, multiplication, division, factorial, and exponential are just some

56
Chapter 4. Recursive Functions
of the most familiar examples of primitive recursive functions. Ackermann’s
function, which we will deﬁne in Section 4.1, is a well-known example of a
non-primitive recursive function.
There are several alternative deﬁnitions of the class of primitive recursive
functions. There is no consensus as to what is the best set of basic initial
functions, and also the notion of recursion may vary; for instance, in some
cases, a notion of iteration is used instead of recursion. As with the variants of
Turing machines mentioned in Chapter 2, it can be shown that the alternative
deﬁnitions of primitive recursion available in the literature are all equivalent.
We start the chapter by deﬁning the class of primitive recursive functions as
the least set including the zero, successor, and projection functions and closed
under the operations of composition and primitive recursion. We then go on to
deﬁne more general recursive functions using a minimisation scheme. We ﬁnish
this chapter with a discussion of functional programming and partial recursive
functions.
4.1 Primitive recursive functions
In the deﬁnition of primitive recursive functions, we will use the natural num-
bers, together with some basic projection functions to erase, copy, and permute
arguments of functions. Starting from these basic functions, we will use two
mechanisms to deﬁne more interesting functions: composition and the primi-
tive recursive scheme. All the functions that we will deﬁne work on the set of
natural numbers, denoted Nat. Thus, a function of arity k will take k natural
numbers as arguments and produce a result of type Nat. This is abbreviated as
f : Natk →Nat.
Composition is a familiar operation. Given two functions f and g from Nat
to Nat, we can deﬁne a new function h so that the result of h on a number x
is obtained by applying f to the result of g on x; that is, h(x) = f(g(x)). The
composition operator used in the deﬁnition of primitive recursive functions is
more general than this, as we will see below.
Primitive recursion is possibly the easiest way to deﬁne recursive functions.
The idea is that, to deﬁne a function f, ﬁrst we give the value of f for 0,
and then for any other number n + 1 we deﬁne f(n + 1) in terms of f(n). For
example, the factorial function is usually deﬁned by the equations
0! = 1
and
(n + 1)! = (n + 1) ∗n!

Chapter 4. Recursive Functions
57
We see that, in the second equation, to compute the factorial of n + 1 we
use multiplication and the factorial of n. The primitive recursive scheme, de-
ﬁned below, generalises this technique. Before giving the deﬁnition of primitive
recursive functions, we introduce some notation.
Notation.
We use x1, . . . , y1, . . . to denote natural numbers, f, g, h to rep-
resent functions, and X1, X2, . . . to represent tuples or sequences of the form
x1, . . . , xn. We only have tuples on natural numbers; thus we will work modulo
associativity for simplicity: (X1, (x1, x2), X2) = (X1, x1, x2, X2).
Deﬁnition 4.1 (Primitive recursive functions)
A function f : Natk →Nat is primitive recursive if it can be deﬁned from a set
of initial functions using composition and the primitive recursive scheme. The
set of initial functions and the composition and recursive scheme are deﬁned
below.
– Initial functions: These can be either the zero and successor functions, used
to build natural numbers, or projections. More precisely:
1. The constant function zero, written 0, and the successor function S are
initial functions. Natural numbers can be built from these two functions
using composition. We write n or Sn(0) for S(. . . S(S



n
(0) . . .)).
2. Projection functions: These are functions that allow us to select an element
of a tuple. There are projection functions for tuples of any length. We will
denote by πn
i the function that selects the ith element of a tuple of length
n. More precisely,
πn
i (x1, . . . , xn) = xi
(1 ≤i ≤n)
We will omit the superindex, writing simply πi, when there is no ambiguity.
– Composition allows us to deﬁne a primitive recursive function h using aux-
iliary functions f, g1, . . . , gn, where n ≥0:
h(X) = f(g1(X), . . . , gn(X))
– The primitive recursive scheme allows us to deﬁne a recursive function h using
two auxiliary primitive recursive functions f, g. The function h is deﬁned as
follows:
h(X, 0)
=
f(X)
h(X, S(n))
=
g(X, h(X, n), n).

58
Chapter 4. Recursive Functions
There are two cases in the deﬁnition of h above, depending on whether the
last argument is 0 or not. If it is 0, then the value of h(X, 0) is obtained
by computing f(X). Otherwise, the second equation deﬁnes h by using the
auxiliary function g and the result of a recursive call to h.
According to Deﬁnition 4.1, any function that can be speciﬁed by using
initial functions and an arbitrary (ﬁnite) number of operations of composition
and primitive recursion is primitive recursive. We give examples of primitive
recursive functions below.
As we have already mentioned, there are alternative versions of the primitive
recursion scheme. For instance, the one above could be replaced by a more
restricted iteration scheme.
Deﬁnition 4.2
Let g be a primitive recursive function. The following scheme, deﬁning the
function h in terms of g, is called pure iteration:
h(X, 0)
=
X
h(X, S(n))
=
g(h(X, n))
The function h deﬁned by the pure iteration scheme, using the auxiliary func-
tion g, takes X and a number n and iterates n times the function g on X. For
this reason, we can abbreviate h(X, n) as gn(X).
We do not have constant functions of the form C(X) = n as initial functions
in Deﬁnition 4.1. However, we can see 0 as a constant function with no argu-
ments, and every other constant function can be built by composition using 0
and S, as shown in the following example.
Example 4.3
The constant function zero(x, y) = 0 is deﬁned as an instance of the com-
position scheme using the initial 0-ary function 0. The constant function
one(x, y) = S(zero(x, y)) is again an instance of the composition scheme.
Functions obtained from primitive recursive functions by introducing
“dummy” arguments, permuting arguments, or repeating them are also prim-
itive recursive. To keep our deﬁnitions simple, we will sometimes omit the
deﬁnition of those functions.

Chapter 4. Recursive Functions
59
Example 4.4
Consider the standard functions add and mul from Nat2 to Nat:
add(x, y) = x + y
mul(x, y) = x ∗y
The function add can be deﬁned by primitive recursion as
add(x, 0)
=
f(x)
add(x, S(n))
=
g(x, add(x, n), n)
where
f(x)
=
π1(x)
=
x
g(x1, x2, x3)
=
S(π2(x1, x2, x3))
=
S(x2)
The primitive recursive function mul is deﬁned by
mul(x, 0)
=
f(x)
mul(x, S(n))
=
g(x, mul(x, n), n)
where
f(x)
=
0
g(x1, x2, x3)
=
add(π1(x1, x2, x3), π2(x1, x2, x3))
=
add(x1, x2)
Similarly, we can deﬁne the function sub to subtract numbers.
sub(x, 0)
=
π1(x)
sub(x, S(n))
=
pred(x, sub(x, n), n)
where the function pred is deﬁned below using projections and the function
predecessor (deﬁned by primitive recursion).
pred(x, y, z)
=
predecessor(π2(x, y, z))
predecessor(0)
=
0
predecessor(S(n))
=
π2(predecessor(n), n)
Functions deﬁned by cases may be more diﬃcult to encode directly using
primitive recursion. In order to be able to express deﬁnitions by cases in a
convenient way, we introduce the notion of a recursive predicate.
Deﬁnition 4.5 (Primitive recursive predicates)
The condition P depending on X ∈Natn, such that P(X) is either true or
false, is called an n-ary predicate. An n-ary predicate P is primitive recursive
if its characteristic function XP : Natn →{0, 1} is primitive recursive. The
characteristic function of a predicate associates 1 with the tuples X for which
P(X) holds and 0 with the others.

60
Chapter 4. Recursive Functions
Example 4.6
The predicates eq (equality) and lt (less than) are primitive recursive with
characteristic functions
Xlt(x, y)
=
f(sub(y, x))
Xeq(x, y)
=
f(add(sub(x, y), sub(y, x)))
where f(0) = 0 and f(S(n)) = 1. The function f is primitive recursive (see the
exercises at the end of the chapter).
Deﬁnition 4.7 (Case construction)
If f1, . . . , fk are primitive recursive functions from Natn to Nat, P1, . . . , Pk are
primitive recursive n-ary predicates, and for every X ∈Natn exactly one of the
conditions P1(X), . . . , Pk(X) is true, then the function f : Natn →Nat deﬁned
below is primitive recursive.
f(X) =
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
f1(X)
if P1(X)
f2(X)
if P2(X)
. . .
fk(X)
if Pk(X)
We can easily understand how such a function can be built from primitive
recursive functions. Since exactly one of the conditions P1(X), . . . , Pk(X) is
true, then exactly one of the values of XPi(X) will be 1 and all the others will
be 0. Then one can obtain the function f by composition using P1, . . . , Pk, the
given functions f1, . . . , fk, addition, and multiplication (the latter two denoted
by + and ∗).
f(X) = f1(X) ∗XP1(X) + · · · + fk(X) ∗XPk(X)
Thus, f is a primitive recursive function.
For example, we can give a deﬁnition by cases for the operator of bounded
minimisation. This operator searches for the minimum number that satisﬁes
a given condition, in a given interval, where the condition is speciﬁed as a
primitive recursive predicate. To show that bounded minimisation is primitive
recursive, we can deﬁne it as follows.
Deﬁnition 4.8
Let P be an (n + 1)-ary primitive recursive predicate and X ∈Natn. The

Chapter 4. Recursive Functions
61
bounded minimisation of P is the primitive recursive function
mP (X, k) =

min{y | 0 ≤y ≤k and P(X, y)}
if the set is not empty
k + 1
otherwise
All the primitive recursive functions are total; that is, for any primitive
recursive function f : Natk →Nat, given k numbers n1, . . . , nk, the value
f(n1, . . . , nk) is well deﬁned. This can be proved as follows.
Proof
The initial functions are obviously total, as is the composition of two total
functions.
Assume h is deﬁned by primitive recursion using two total functions f and
g. We can prove by induction on n that h(X, n) is total for all n. First, note that
h(X, 0) is total (since f is). Next, assume that h(X, n) is well deﬁned (induction
hypothesis). Then, since g is total, h(X, S(n)) is also well-deﬁned.
Although most of the functions that we use are primitive recursive, the
set of computable functions also includes functions that are outside this class.
For instance, some computable functions are partial functions, and there are
also total computable functions that are not primitive recursive. Ackermann’s
function is a standard example of a total, non-primitive recursive function:
ack(0, n)
=
S(n)
ack(S(n), 0)
=
ack(n, S(0))
ack(S(n), S(m))
=
ack(n, ack(S(n), m))
In the next section, we deﬁne the class of partial recursive functions by
including an additional mechanism to build functions, called unbounded min-
imisation or just minimisation.
4.2 Partial recursive functions
We start by deﬁning the unbounded minimisation operator.
Deﬁnition 4.9 (Minimisation)
Let f be a total function from Natn+1 to Nat. The function g from Natn to Nat
that computes for each tuple X of numbers the minimum y such that f(X, y)

62
Chapter 4. Recursive Functions
is zero is called the minimisation of f. More precisely, the minimisation of f is
the function g deﬁned as follows:
g(X) = min{y | f(X, y) = 0}
We denote g as Mf.
Note that although the equality predicate used in the deﬁnition of minimi-
sation is total, the minimisation operation is not necessarily terminating. It
requires performing a search without an upper limit on the set of numbers to
be considered. For this reason, a function deﬁned by minimisation of a total
function may be partial.
The class of partial recursive functions includes the primitive recursive func-
tions and also functions deﬁned by minimisation. Despite its name, this class
also includes total functions. We will simply call the functions in this class
recursive functions.
Deﬁnition 4.10 (Recursive functions)
The set of recursive functions is deﬁned as the smallest set of functions con-
taining the natural numbers (built from 0 and the successor function S) and
the projection functions and closed by composition, primitive recursion, and
minimisation.
Closure by minimisation implies that, for every n ≥0 and every total
recursive function f : Natn+1 →Nat, the function Mf : Natn →Nat deﬁned by
Mf(X) = min{y | f(X, y) = 0}
is a (possibly partial) recursive function. In other words, a function is recursive
if it can be deﬁned using initial functions and a ﬁnite number of operations of
composition, primitive recursion, and minimisation.
In particular, every primitive recursive function is also recursive (since in
both deﬁnitions we use the same initial functions, composition, and primitive
recursive scheme). However, if minimisation is used in the deﬁnition of the
function, the result may not be primitive recursive, and it may fail to be total.
Kleene showed the following result, which indicates that only one minimi-
sation operation is suﬃcient to deﬁne recursive functions.
Theorem 4.11 (Kleene normal form)
Let h be a (possibly partial) recursive function on Natk. Then, a number n can

Chapter 4. Recursive Functions
63
be found such that
h(x1, . . . , xk) = f(Mg(n, x1, . . . , xk))
where f and g are primitive recursive functions.
Although all the functions we have deﬁned are functions from numbers to
numbers, the primitive recursion and minimisation mechanisms can also be
used to deﬁne functions on strings, lists, trees, etc. Indeed, using a technique
developed by G¨odel, known as G¨odel numbering, it is possible to associate
a number (i.e., a code) with each string, list, tree, etc., and then deﬁne the
functions on data structures as numeric functions acting on codes. Instead of
encoding the data, we can redeﬁne the initial functions, composition, recursive
schemes, and minimisation to work directly on the speciﬁc data structure of
interest.
We ﬁnish this section by stating, without a proof, that all the partial re-
cursive functions can be deﬁned in the λ-calculus. The converse is also true;
indeed, these two models of computation are equivalent (and also equivalent in
computational power to Turing machines).
Property 4.12
The set of recursive functions, the set of functions that can be deﬁned in the
λ-calculus, and the set of functions that can be computed by a Turing machine
coincide.
4.3 Programming with functions
The common feature of all functional programming languages is that programs
consist of functions (as in the mathematical notion of a function, which is the
basis of the λ-calculus and the theory of partial recursive functions; not to be
confused with the notion of a function used in imperative languages).
Most modern functional programming languages are strongly typed (that is,
equipped with a type system that guarantees that well-typed expressions will
not produce type errors at run time) and have built-in memory management.
ML and Haskell are examples of these. In the rest of this section, we give
examples of functional deﬁnitions using the syntax of Haskell.
A function in this sense is simply a mapping between elements of two sets,
for instance,
f :: α →β

64
Chapter 4. Recursive Functions
can be seen as a declaration of a function f that, applied to an argument x in
α, gives a result (f x) in β.1 With this approach, the focus is on what is to be
computed, not how it should be computed. In the example above, we say that
the function f has type α →β.
Some functional programming languages adopt a syntactic style that is
based on equational deﬁnitions similar to the deﬁnitions of primitive recursive
functions or, more generally, partial recursive functions. However, functional
languages also allow the programmer to deﬁne anonymous functions, as in the
λ-calculus. In general, a function in a functional programming language can
be deﬁned in terms of other functions previously deﬁned by the programmer,
taken from the libraries, or provided as language primitives. Composition of
functions and recursion play major roles in functional programming languages.
The composition operator, denoted by ·, as in the expression f · g, is itself
a function; it is predeﬁned in functional languages. In Haskell, it is deﬁned as
follows:
· :: ((β →γ), (α →β)) →(α →γ)
(f · g) x = f (g x)
The type of · indicates that we can only compose functions whose types are
compatible. In other words, the composition operator · expects two functions,
f and g, as arguments, such that the domain of f coincides with the co-domain
of g. The type of f is (β →γ) and the type of g is (α →β), where α, β, and γ
are type variables representing arbitrary types.
The result of composing two functions f and g of compatible types (β →γ)
and (α →β), respectively, is a function of type (α →γ). It accepts an argument
x of type α (which will be supplied to g) and produces f (g x), which is in
the co-domain of f and therefore is of type γ.
Example 4.13 (Composition)
Consider the function
square :: Integer →Integer
that computes the square of a number. It can be deﬁned by the equation
square x = x * x
where we have used a predeﬁned multiplication operator, written *.
1 Most functional languages adopt the λ-calculus notation (f x) for application.

Chapter 4. Recursive Functions
65
Using the function square and the composition operator, we can deﬁne a
function quad that computes the fourth power of a number as follows:
quad = square · square
Arithmetic operations are built-in functions used in inﬁx notation, as in the
expressions 3 + 4 or x * x. In Haskell, we can use them in preﬁx notation if
we enclose them in brackets; for example, (+) 3 4.
The functions (+) and + have diﬀerent types:
+ :: (Integer, Integer) →Integer
(+) :: Integer →Integer →Integer
The function (+) is the Curryﬁed version of +; that is, instead of working
on pairs of numbers (i.e., two numbers provided simultaneously), it expects
a number followed by another number. This might seem a small diﬀerence
at ﬁrst sight, but Curryﬁcation (the word derives from the name of the
mathematician Haskell Curry, after whom the programming language also is
named) provides great ﬂexibility to functional languages. For instance, we
can use (*), the Curryﬁed version of the multiplication operator, to deﬁne a
function double, which doubles its argument:
double :: Integer →Integer
double = (*) 2
As in the λ-calculus, there are some notational conventions to avoid writing
too many brackets in expressions; for example, it is understood that applica-
tion has priority over arithmetic operations. For example, square 1 + 4 * 2
should be read as (square 1) + (4 * 2).
The process of evaluating an expression is a simpliﬁcation process, also
called a reduction process. An evaluator for a functional programming language
implements the β-reduction rule of the λ-calculus. The goal is to obtain the
value or irreducible form (also called the normal form) associated with an
expression by a series of reduction steps. The meaning of an expression is its
value.
Functional programming languages inherit their main properties from the
λ-calculus. One of the main properties is the unicity of normal forms:
In (pure) functional languages, the value of an expression is uniquely
determined by its components.

66
Chapter 4. Recursive Functions
An obvious advantage of this property is improved readability of programs.
Not all the reduction sequences that start with a given expression lead to
a value. This is not in contradiction with the previous property. It is caused
by non-termination. Some reduction sequences for a given expression may be
inﬁnite, but all the sequences that terminate reach the same value. This is more
clearly seen with an example.
Example 4.14 (Non-termination)
Let us deﬁne the constant function fortytwo. This is a primitive recursive
function, and in a language like Haskell we can deﬁne it with an equation:
fortytwo x = 42
We can also deﬁne equationally
a non-primitive recursive function
infinity:
infinity = infinity + 1
It is clear that the evaluation of infinity never reaches a normal form.
The expression
fortytwo infinity
gives rise to some reduction sequences that do not terminate, but those that
terminate give the value 42 (unicity of normal forms).
The example above shows that, although the normal form is unique, the
order of reductions is important. As in the λ-calculus, functional programming
languages evaluate expressions by reduction and follow a given evaluation strat-
egy. Recall that a strategy of evaluation speciﬁes the order in which reductions
take place; in other words, it deﬁnes the reduction sequence that the language
implements.
The most popular strategies of evaluation for functional languages are:
1. Call-by-name (normal order): In the presence of a function application, ﬁrst
the deﬁnition of the function is used and then the arguments are evaluated
if needed.
2. Call-by-value (applicative order): In the presence of a function application,
ﬁrst the arguments are evaluated and then the deﬁnition of the function is
used to evaluate the application.
For example, using call-by-name, the expression fortytwo infinity is re-
duced in one step to the value 42 since this strategy speciﬁes that the deﬁnition

Chapter 4. Recursive Functions
67
of the function fortytwo is used, which does not require the argument (it is a
constant function). However, when using call-by-value, we must ﬁrst evaluate
the argument infinity, and, as we already mentioned, the reduction sequence
for this expression is inﬁnite; hence we will never reach a normal form. Call-
by-name guarantees that if an expression has a value, it will be reached.
As this example shows, diﬀerent strategies of evaluation require diﬀerent
numbers of reduction steps, and therefore the eﬃciency of a program (which
is related to the number of reduction steps) depends on the strategy used.
Some functional languages (for instance, ML) use call-by-value so that when
an argument is used several times in the deﬁnition of a function it is evaluated
only once. Haskell uses a strategy called lazy evaluation. It is based on call-by-
name, which guarantees that if an expression has a normal form, the evaluator
will ﬁnd it, but to avoid the potential lack of eﬃciency of a pure call-by-name
strategy, Haskell uses a sharing mechanism. When an argument is used many
times in a function deﬁnition, its evaluation is performed at most once, and the
value is shared between all its occurrences.
4.4 Further reading
There are many books and journal articles on recursive functions, for in-
stance [18, 19, 39, 46, 2]. Some of these references provide alternative deﬁnitions
of the classes of primitive recursive functions and general recursive functions.
Kleene’s book [27] is an interesting reference. To complement the information
on Haskell given in the previous section, we recommend [6, 51].
4.5 Exercises
1. Show that the factorial function is primitive recursive.
2. Show that the function f used in Example 4.6, deﬁned by f(0) = 0 and
f(S(n)) = 1, is primitive recursive.
3. Consider the functions Div and Mod such that Div(x, y) and Mod(x, y)
compute the quotient and remainder, respectively, of the division of x by y.
These are not total functions because division by 0 is not deﬁned, but we
can complete the deﬁnition by stating that Div(x, 0) = 0 and Mod(x, 0) =
x. Show that the extended functions Div and Mod are primitive recursive.
4. Show that the pure iteration scheme given in Deﬁnition 4.2 is equivalent

68
Chapter 4. Recursive Functions
to the primitive recursive scheme given in Deﬁnition 4.1.
5. Indicate whether the following statements are true or false:
a) All primitive recursive functions are total.
b) All total computable functions are primitive recursive.
c) All partial recursive functions are computable.
d) All total functions are computable.
6. Write functional programs deﬁning cube (the function that computes the
third power of a number) and double (the function that doubles its argu-
ment). Describe the reduction sequences for the expression
cube (double (3 + 1))
using call-by-name (normal order) and call-by-value (applicative order).
7. In functional languages, there is a primitive function if-then-else that
we can use to deﬁne a function by cases that depend on a Boolean condition
(see the case construction in Deﬁnition 4.7). Thus,
if x == 0 then 0 else x * y
will return 0 if the value of x is equal to 0 and will return the product of x
and y otherwise.
Assume the function mult on natural numbers is deﬁned by
mult x y
def
= if x == 0 then 0 else x * y
where == is the equality test. Assume that e1 == e2 is evaluated by reducing
e1 and e2 to normal form and then comparing the normal forms.
a) Is mult commutative over numbers; i.e., will mult m n and mult n m
compute the same result for all numbers m and n?
b) Let infinity be the function deﬁned by
infinity
def
= infinity + 1
What is the value of mult infinity 0?
What is the value of mult 0 infinity?

5
Logic-Based Models of Computation
During the late 1920s, Jacques Herbrand, a young mathematician, developed
a method to check the validity of a class of ﬁrst-order logic formulas. In his
thesis, published in 1931, Herbrand discussed what can be considered the ﬁrst
uniﬁcation procedure. Uniﬁcation is at the heart of modern implementations
of logic programming languages.
In this chapter, we will discuss the model of computation that serves as
a basis for the logic programming paradigm. Prolog, one of the most popular
logic programming languages, will be discussed in the ﬁnal part of the chapter.
5.1 The Herbrand universe
In logic programs, the domain of computation is the Herbrand universe, the
set of terms deﬁned over a universal alphabet of
– variables, such as X, Y , etc., and
– function symbols with ﬁxed arities (the arity of a symbol is as usual the
number of arguments associated with it).
Function symbols are usually denoted by f, g, h, . . ., or a, b, c, . . . if the arity
is 0 (i.e., a, b, c, . . . denote constants). In our examples, we will often use more
meaningful names for function symbols.

70
Chapter 5. Logic-Based Models of Computation
Deﬁnition 5.1 (Terms)
A term is either a variable or has the form f(t1, . . . , tn), where f is a function
symbol of arity n and t1, . . . , tn are terms. Notice that n may be 0, and in this
case we will just write f, omitting the brackets.
Example 5.2
If a is a constant, f a binary function, and g a unary function, then
f(f(X, g(a)), Y ) is a term, where X and Y are variables.
Function symbols in this framework correspond to data constructors; they
are used to give structure to the domain of computation. For example, if our
algorithm deals with arrays of three elements, a suitable data structure can
be deﬁned using a function symbol array of arity 3. The array containing the
elements 0, 1, 2 is then represented by the term array(0, 1, 2).
There is no deﬁnition associated with a function symbol (although in Prolog
implementations there are some built-in functions, such as arithmetic opera-
tions, that have a speciﬁc meaning).
We will not ﬁx the alphabet used to deﬁne the Herbrand universe. The
names of variables and function symbols needed to represent the problem do-
main can be freely chosen. In this chapter, names of variables start with capital
letters and names of functions start with lower case letters (we follow the con-
ventions used in Prolog’s syntax).
5.2 Logic programs
Once the domain of computation is established, a problem can be described by
means of logic formulas involving predicates.
Predicates represent properties of terms and are used to build basic formulas
that are then composed using operators such as and, not, and or, denoted by
∧, ¬, and ∨, respectively.
Deﬁnition 5.3
Let P be a set of predicate symbols, each with a ﬁxed arity. If p is a predicate
of arity n and t1, . . . , tn are terms, then p(t1, . . . , tn) is an atomic formula, or
simply an atom. Again, n may be 0, and in this case we omit the brackets. A
literal is an atomic formula or a negated atomic formula.

Chapter 5. Logic-Based Models of Computation
71
Example 5.4
The following are two literals (the second is a negated atom), where we use the
predicates value of arity 2 and raining of arity 0, a unary function symbol
number, and the constant 1:
value(number(1),1)
¬raining
We have followed another syntactic convention of Prolog in that names of
predicates start with a lower case letter.
We mentioned before that logic (or, more precisely, a subset of ﬁrst-order
logic) can be seen as an abstract model of computation. Logic formulas will be
used to express algorithms or, more generally, partial functions (since some of
the computations that we will model may not halt). We will call them logic
programs. As another piece of evidence to support Church’s Thesis, it can be
shown that logic programs can express exactly the same class of functions that
Turing machines can deﬁne. Logic programs are Turing complete.
Deﬁnition 5.5 (Logic programs)
Logic programs are sets of deﬁnite clauses, also called Horn clauses, that are
a restricted class of ﬁrst-order formulas. A deﬁnite clause is a disjunction of
literals with at most one positive literal.
We now introduce some notational conventions for clauses. We write P1,
P2, . . . to denote atoms. A deﬁnite clause P1 ∨¬P2 ∨. . . ∨¬Pn (where P1 is the
only positive literal) will be written
P1 :- P2, . . . , Pn.
and we read it as
“P1 if P2 and . . . and Pn.”
We call P1 the head of the clause and P2, . . . , Pn the body.
If the clause contains just P1 and no negative literals, then we write
P1.
Both kinds of clauses are called program clauses; the ﬁrst kind is called a
rule and the second kind is called a fact. If the clause contains only negative
literals, we call it a goal or query and write
:-P2, . . . , Pn.

72
Chapter 5. Logic-Based Models of Computation
Program clauses can be seen as deﬁning a database: Facts specify informa-
tion to be stored, and rules indicate how we can deduce more information from
the previously deﬁned data.
Goals are questions to be answered using the information about the problem
in the database. This can be better seen with some examples.
Example 5.6
In the following logic program, the ﬁrst four clauses are facts and the last one
is a rule.
based(prolog,logic).1
based(haskell,functions).
likes(claire,functions).
likes(max,logic).
likes(X,L) :- based(L,Y), likes(X,Y).
Here we have used two binary predicates, based and likes, constants
prolog, logic, haskell, functions, claire, and max, and variables X,
Y, and L.
The ﬁrst two clauses in the program can be read as “Prolog is based on logic
and Haskell on functions”. More precisely, these are facts about the predicate
based; they deﬁne a relation to be stored in the database.
The next three clauses deﬁne the predicate likes. There are two facts,
which can be read as “Claire likes functions and Max likes logic”, and a rule
that allows us to deduce more information about people’s tastes. We can read
this rule as “X likes L if L is based on Y and X likes Y”.
Once this information is speciﬁed in the program as shown above, we can
ask questions such as “Is there somebody (some Z) who likes Prolog?” which
corresponds to the goal
:- likes(Z,prolog).
With the information given in the program, we can deduce that Max likes
Prolog. We know that Max likes logic and Prolog is based on logic, and therefore
the last rule allows us to conclude that Max likes Prolog.
The precise deduction mechanism that we use to reach this conclusion can
be speciﬁed using an inference rule called resolution, which we describe below.
1 In some versions of Prolog, the word prolog is reserved; therefore, to run this
example, it might be necessary to replace prolog by myprolog, for instance.

Chapter 5. Logic-Based Models of Computation
73
5.2.1 Answers
Answers to goals will be represented by substitutions that associate values
with the unknowns (i.e., the variables) in the goal. Values are also terms in the
Herbrand universe (see Deﬁnition 5.1).
Deﬁnition 5.7 (Substitution)
A substitution is a partial mapping from variables to terms, with a ﬁnite do-
main. If the domain of the substitution σ is
dom(σ) = {X1, . . . , Xn}
we denote the substitution by {X1 →t1, . . . , Xn →tn}.
Substitutions are extended to terms and literals in the natural way: We
apply a substitution σ to a term t or a literal l by simultaneously replacing
each variable occurring in dom(σ) by the corresponding term. The resulting
term is denoted tσ.
Since substitutions are functions, composition of substitutions is simply
functional composition. For example, σ · ρ denotes the composition of the sub-
stitutions σ and ρ.
Example 5.8
The application of the substitution
σ = {X →g(Y ), Y →a}
to the term
f(f(X, g(a)), Y )
yields the term
f(f(g(Y ), g(a)), a)
Note the simultaneous replacement of X and Y in the term above.
Since logic programs are ﬁrst-order formulas, their meaning is precise. There
is a declarative interpretation in which the semantics of a program is deﬁned
with respect to a mathematical model (the Herbrand universe). There is also a
procedural interpretation of programs, which explains how the program is used
in computations. The latter deﬁnes a logic-based computation model.
The computation associated with a logic program is deﬁned through the
use of SLD-resolution, a speciﬁc version of the Principle of Resolution. Using
SLD-resolution, diﬀerent alternatives to ﬁnd a solution will be explored for a

74
Chapter 5. Logic-Based Models of Computation
given goal in the context of a program. These alternatives will be represented as
branches in a tree, called the SLD-resolution tree or simply SLD-tree. Some of
the branches in the SLD-tree may not produce a solution; we need to traverse
the whole tree (which can be inﬁnite) in order to ﬁnd all the solutions for a
goal. The traversal of the tree can be done in diﬀerent ways, and this will give
us models of computation with diﬀerent properties. Here we will consider a
strategy for the traversal that explores each branch in depth, from left to right.
Some branches may end with a failure (we will describe this notion below), and
we will have to backtrack to the nearest point in the tree where there are still
alternative branches to explore. We continue traversing the SLD-tree until all
the alternatives are exhausted.
To illustrate the idea, let us look at a logic program deﬁning the predicate
append for lists. The empty list is denoted by the constant [], and a non-empty
list is denoted as [X|L], where X represents the ﬁrst element of the list (also
called the head) and L is the rest of the list (also called the tail of the list).
Note that [ | ] is a binary function symbol, a constructor that is used to build
a list structure. We abbreviate [X|[]] as [X], [X|[Y|[]]] as [X,Y], and in
general [X1,...,Xn] denotes a list of n elements.
Example 5.9
The predicate append is deﬁned as a relation between three lists: the two lists
we want to concatenate and their concatenation. More precisely, the atomic
formula append(S,T,U) indicates that the result of appending the list T onto
the end of the list S is the list U. We can deﬁne the predicate append by giving
two program clauses (a fact and a rule):
append([],L,L).
append([X|L],Y,[X|Z]) :- append(L,Y,Z).
The predicate append deﬁnes a relation and can be used in diﬀerent ways.
For instance, a goal
:- append([0],[1,2],L).
will compute the answer substitution {L →[0, 1, 2]}, but with the same logic
program and the goal
:- append([0],U,[0,1,2]).
we will obtain the solution {U →[1, 2]}. In this case, the ﬁrst and third ar-

Chapter 5. Logic-Based Models of Computation
75
guments of the predicate are used as inputs and the second as output. All
combinations are possible.
Answers to goals (i.e., substitutions mapping variables to values) will be
automatically generated by the uniﬁcation algorithm, which is part of the pro-
cess of resolution. More precisely, to ﬁnd the answer for a goal, we need to
ﬁnd in the program the clauses that can be applied; during this process, some
equations between terms will be generated, and the uniﬁcation algorithm will
be called in order to solve these equations. If there is a solution, there is also
one that is the most general solution in the sense that all the others can be
derived from it. This is called the most general uniﬁer. We will formally deﬁne
uniﬁcation problems and give a uniﬁcation algorithm in the next section, but
we can already give an example.
Example 5.10
To solve the query
:- append([0],[1,2],U).
in the context of the logic program
append([],L,L).
append([X|L],Y,[X|Z]) :- append(L,Y,Z).
we will start by using the second program clause (the ﬁrst one cannot be applied
because in our goal the ﬁrst list is not empty). The substitution
{X →0, L →[], Y →[1,2], U →[0|Z]}
uniﬁes the head of the second program clause with the query; that is,
if we apply this substitution to the literals append([X|L],Y,[X|Z]) and
append([0],[1,2],U) we obtain exactly the same result:
append([0],[1,2],[0|Z]).
Since the second clause in the program says that append([X|L],Y,[X|Z])
holds if append(L,Y,Z) holds, all that remains to be proved is that
append([],[1,2],Z) holds for some Z.
Now we have an atom in which the ﬁrst list is empty, and we have a fact
append([],L,L) in the program. Applying the substitution
{Z →[1,2]}

76
Chapter 5. Logic-Based Models of Computation
to our atom, we obtain (an instance of) a fact.
Combining both substitutions we get
{U →[0,1,2]}
which solves the query. It is the most general answer substitution for the given
goal, and the process by which we derived this solution is an example of an
application of the Principle of Resolution.
Goals such as
:- append([0],[1,2],U)
:- append(X,[1,2],U)
:- append([1,2], U,[0])
can all be seen as questions to be answered using the deﬁnitions given in the
program. The ﬁrst one has only one solution:
{U →[0,1,2]}
The second has an inﬁnite number of solutions, and the third one has none.
5.3 Computing with logic programs
In this section, we will describe how logic programs are executed, or in other
words how computations are carried out in a model of computation where
algorithms are expressed as logic programs. We have already mentioned in the
previous section that the Principle of Resolution is the basis of this computation
model. We will start by deﬁning uniﬁcation, a key step in the Principle of
Resolution. Then we will deﬁne SLD-resolution, which uses a speciﬁc strategy
to search for solutions to goals.
5.3.1 Uniﬁcation
Although a process of uniﬁcation was sketched by Herbrand in his thesis in
the early 1930s, it was only in the 1960s, after Alan Robinson introduced the
Principle of Resolution and gave an algorithm to unify terms, that logic pro-
gramming became possible. Robinson’s uniﬁcation algorithm was the basis for
the implementation of the programming language Prolog. The version of the
uniﬁcation algorithm that we present is based on the work of Martelli and

Chapter 5. Logic-Based Models of Computation
77
Montanari, where uniﬁcation is described as a simpliﬁcation process to solve
equations between terms.
Deﬁnition 5.11 (Uniﬁer)
A uniﬁcation problem U is a set of equations between terms containing vari-
ables. We will use the notation
{s1 = t1, . . . , sn = tn}
A solution to U, also called a uniﬁer, is a substitution σ (see Deﬁnition 5.7)
such that when we apply σ to all the terms in the equations in U we obtain
syntactical identities: For each equation si = ti, the terms siσ and tiσ coincide.
A uniﬁer σ is said to be most general if any other uniﬁer for the problem
can be obtained as an instance of σ.
Although there may be many diﬀerent substitutions that are most general
uniﬁers, one can show that they are all equivalent modulo renaming of
variables. In other words, the most general uniﬁer is unique if we consider it
modulo renamings. The algorithm of Martelli and Montanari ﬁnds the most
general uniﬁer for a uniﬁcation problem if a solution exists; otherwise it fails,
indicating that there are no solutions. To ﬁnd the most general uniﬁer for
a uniﬁcation problem, the algorithm simpliﬁes (i.e., transforms) the set of
equations until a substitution is generated. The simpliﬁcation rules apply to
sets of equations and produce new sets of equations or a failure.
Uniﬁcation algorithm
Input: A ﬁnite set of equations between terms:
{s1 = t1, . . . , sn = tn}
Output: A substitution that is the most general uniﬁer (mgu) for these terms
or failure.
Transformation rules: The rules that are given below transform a uniﬁcation
problem into a simpler one or produce a failure. Below, E denotes an arbitrary
set of equations between terms.

78
Chapter 5. Logic-Based Models of Computation
(1)
f(s1, . . . , sn) = f(t1, . . . , tn), E
→
s1 = t1, . . . , sn = tn, E
(2)
f(s1, . . . , sn) = g(t1, . . . , tm), E
→
failure
(3)
X = X, E
→
E
(4)
t = X, E
→
X = t, E
if t is not a
variable
(5)
X = t, E
→
X = t, E{X →t}
if X is
not in t and X occurs in E
(6)
X = t, E
→
failure
if x occurs in t
and x ̸= t
The uniﬁcation algorithm applies the transformation rules in a non-
deterministic way until no rule can be applied or a failure arises. Note that
we are working with sets of equations, and therefore the order in which they
appear in the uniﬁcation problem is not important.
The test in case (6) is called occur-check; for example, X = f(X) fails.
This test is time-consuming, and for this reason in some systems it is not
implemented.
If the algorithm ﬁnishes without a failure, we obtain a substitution, which
is the most general uniﬁer of the initial set of equations.
Note that rules (1) and (2) apply also to constants (i.e., 0-ary functions):
In the ﬁrst case, the equation is deleted, and in the second there is a failure.
Example 5.12
1. We start with {f(a, a) = f(X, a)}.
a) Using rule (1), this problem is rewritten as {a = X, a = a}.
b) Using rule (4), we get {X = a, a = a}.
c) Using rule (1) again, we get {X = a}.
Now no rule can be applied, and therefore the algorithm terminates with
the most general uniﬁer {X →a}.
2. In Example 5.10, we solved the uniﬁcation problem
{[X|L] = [0], Y = [1,2], [X|Z] = U}

Chapter 5. Logic-Based Models of Computation
79
Recall that [ | ] is a binary function symbol (a list constructor; its argu-
ments are the head and the tail of the list, respectively). [0] is shorthand
for [0|[]], and [] is a constant (the empty list).
We apply the uniﬁcation algorithm, starting with the set of equations
above.
a) Using rule (1) in the ﬁrst equation, we get
{X = 0, L = [], Y = [1,2], [X|Z] = U}
b) Using rule (5) and the ﬁrst equation, we get
{X = 0, L = [], Y = [1,2], [0|Z] = U}
c) Using rule (4) and the last equation, we get
{X = 0, L = [], Y = [1,2], U = [0|Z]}
Then the algorithm stops. Therefore the most general uniﬁer is
{X →0, L →[],Y →[1,2], U →[0|Z]}
5.3.2 The Principle of Resolution
Resolution is based on refutation. In order to solve a query
:- A1,...,An
with respect to a set P of program clauses, resolution seeks to show that
P, ¬A1, . . . , ¬An
leads to a contradiction. That is, the negation of the literals in the goal is added
to the program P; if a contradiction arises, then we know that P did entail the
literals in the query.
Deﬁnition 5.13
A contradiction is obtained when a literal and its negation are stated at the
same time.
For example, A, ¬A is a contradiction. If a contradiction does not arise
directly from the program and the goal, new clauses will be derived by resolu-
tion, and the process will continue until a contradiction arises (the search may
continue forever). The derived clauses are called resolvents.

80
Chapter 5. Logic-Based Models of Computation
We will describe the generation of resolvents using a restriction of the Prin-
ciple of Resolution called SLD-resolution; Prolog is based on SLD-resolution.
5.3.2.1 SLD-resolution.
Let us consider ﬁrst a simple case where in the
query there is just one atom. If we have a goal
:- p(u1, . . . , un).
and a program clause (we rename the variables in the clause if necessary so
that all the variables are diﬀerent from those in the goal)
p(t1, . . . , tn) :- S1, . . . , Sm.
such that p(t1, . . . , tn) and p(u1, . . . , un) are uniﬁable with mgu σ, then we
obtain the resolvent
:- S1σ, . . . , Smσ.
In the general case, the query may have several literals. Prolog’s SLD-
resolution generates a resolvent using the ﬁrst literal in the goal.
Deﬁnition 5.14 (SLD-resolution)
If the query has several literals, for instance
:- A1, . . . , Ak.
the resolvent is computed between the ﬁrst atom in the goal (A1) and a
(possibly renamed) program clause. If there is a program clause
A′
1 :- S1, . . . , Sm.
such that A′
1 and A1 are uniﬁable with mgu σ, then we obtain a resolvent
:- S1σ, . . . , Smσ, A2σ, . . . , Akσ.
In other words, the resolvent is generated by replacing the ﬁrst atom in
the goal that uniﬁes with the head of a clause by the body of the clause and
applying the uniﬁer to all the atoms in the new goal. Note that when we
compute a resolvent using a fact (i.e., when m = 0), the atom disappears from

Chapter 5. Logic-Based Models of Computation
81
the query. An empty resolvent indicates a contradiction, which we will denote
by the symbol ♦.
We stress the fact that each resolution step computes a resolvent between
the ﬁrst atom of the last resolvent obtained and a clause in the program. This
is why this particular form of resolution is called SLD-resolution.
The ‘S’ stands for selection rule: A ﬁxed computation rule is applied in
order to select a particular atom to resolve upon in the goal. Prolog always
selects the leftmost literal in the goal.
The ‘D’ stands for deﬁnite: It indicates that all the program clauses are
deﬁnite.
The ‘L’ stands for linear, indicating that each resolution step uses the most
recent resolvent (to start with, it uses the given query) and a program clause.
Prolog uses the clauses in the program in the order they are written.
Given a logic program and a query, the idea is to continue generating re-
solvents until an empty one (a contradiction) is generated. When an empty
resolvent is generated, the composition of all the substitutions applied at each
resolution step leading to the contradiction is computed. This is also a substi-
tution (recall that substitutions are functions from terms to terms, and com-
position is simply functional composition; see Deﬁnition 5.7 for more details).
The restriction of this substitution to the variables that occur in the initial goal
is the answer to the initial query.
We represent each resolution step graphically as follows:
Query
| mgu
Resolvent
Since there might be several clauses in the program that can be used to
generate a resolvent for a given query, we obtain a branching structure called
an SLD-resolution tree.
Deﬁnition 5.15 (SLD-tree)
Every branch in the SLD-tree that leads to an empty resolvent produces an
answer. All the branches that produce an answer are called success branches.
If a ﬁnite branch does not lead to an empty resolvent, it is a failure.
An SLD-resolution tree may have several success branches, failure branches,
and also inﬁnite branches that arise when we can continue to generate resolvents
but never reach an empty one.

82
Chapter 5. Logic-Based Models of Computation
Example 5.16
Consider the program P
based(prolog,logic).
based(haskell,functions).
likes(max,logic).
likes(claire,functions).
likes(X,P) :- based(P,Y), likes(X,Y).
and the query
:- likes(Z,prolog).
Using the last clause and the mgu {X →Z, P →prolog}, we obtain the
resolvent
:- based(prolog,Y), likes(Z,Y).
Now using the ﬁrst clause and the mgu {Y →logic}, we obtain the new
resolvent
:- likes(Z,logic).
Finally, since we can unify this atom with the fact likes(max,logic) using
the substitution {Z →max}, we obtain an empty resolvent. This is therefore a
success branch in the SLD-tree for the initial query.
The composition of the substitutions used in this branch is
{X →max, P →prolog, Y →logic, Z →max}
Therefore, the answer to the initial query is {Z →max}.
There are other branches in the SLD-tree for this query, but this is the only
successful one. The SLD-resolution tree for this query is shown in Figure 5.1.
Note that in the branch that leads to failure we again use the last clause of
the program but rename its variables as X’, P’, Y’ to avoid confusion with
the previous use of this clause (see Deﬁnition 5.14).
Now consider the same program with an additional clause:
likes(claire,logic).

Chapter 5. Logic-Based Models of Computation
83
likes(Z,prolog)
| {X →Z, P →prolog}
based(prolog,Y), likes(Z,Y)
| {Y →logic}
likes(Z,logic).
{Z →max} /
\ {X’ →Z, P’ →logic}
♦
based(logic,Y’),likes(Z,Y’)
(Failure)
Figure 5.1
SLD-resolution tree for the query :- likes(Z,prolog). using
the program P.
The new program will be called P ′. The SLD-resolution tree for the same
query in the context of the program P ′ is shown in Figure 5.2.
likes(Z,prolog)
| {X →Z, P →prolog}
based(prolog,Y), likes(Z,Y)
| {Y →logic}
likes(Z,logic).
{Z →max}/ {Z →claire}|
\
♦
♦
based(logic,Y’),likes(Z,Y’)
(Failure)
Figure 5.2
SLD-tree for :- likes(Z,prolog). using the program P ′.
Finally, with the same program and a query
:- likes(Z,painting).

84
Chapter 5. Logic-Based Models of Computation
the SLD-tree is
likes(Z,painting)
| {X →Z, P →painting}
based(painting,Y), likes(Z,Y)
(Failure)
5.4 Prolog and the logic programming paradigm
We have seen how logic formulas can be used to express knowledge and describe
problems and how we can compute solutions to a problem using resolution as
the inference rule. In this section, we discuss the logic programming paradigm.
If we analyse the approaches to programming discussed in the previous
chapters (imperative and functional), we can single out one major diﬀerence:
Functional programs are concerned with what needs to be computed, whereas
imperative programs indicate how to compute it. Functional languages are
declarative. Logic programming languages also belong to the family of declar-
ative languages. Roughly speaking, programs in logic programming languages
specify a problem, and the execution of a program is a process of proof search-
ing during which solutions for the problem will be generated. Since programs
are just descriptions of problems, this is a knowledge-based programming style
that has many applications in artiﬁcial intelligence (for example, to build expert
systems).
The language of logic is a very powerful one. The same formalism can be
used to specify a problem, write a program, and prove properties of the pro-
gram. The same program can be used in many diﬀerent ways. Based on this
idea, several programming languages have been developed that diﬀer in the
kind of logic that is used for the description of the problem and the method
employed to ﬁnd proofs. The most well-known logic programming language is
Prolog, which is based on ﬁrst-order predicate calculus and uses the Principle
of Resolution. Actually, ﬁrst-order logic and the Principle of Resolution are too
general to be used directly as a model of computation, but in the 1970s Robert
Kowalski, Alain Colmerauer, and Philippe Roussel deﬁned and implemented a
suitable restriction based on the clausal fragment of classical ﬁrst-order logic,
as described in the previous sections. Their work resulted in the ﬁrst version
of Prolog.

Chapter 5. Logic-Based Models of Computation
85
Prolog builds the SLD-tree for a given query using the clauses in the pro-
gram in the order in which they occur, in a depth-ﬁrst manner: The leftmost
branch in the SLD-tree is generated ﬁrst. If this branch is inﬁnite, Prolog will
fail to ﬁnd an answer even if there are other successful branches. For this reason,
the order of the clauses in a Prolog program is very important.
If during the traversal of the tree Prolog arrives at a failure leaf, it will go
back (towards the root of the tree) to explore the remaining branches. This
process is called backtracking.
We could summarise Prolog’s computations as SLD-resolution with a depth-
ﬁrst search strategy and automatic backtracking.
Example 5.17
Consider the program P deﬁning the predicate append:
append([],L,L).
append([X|L],Y,[X|Z]) :- append(L,Y,Z).
The goal
:- append(X,[1,2],U).
produces the answer {X →[], U →[1, 2]}, but if we change the order of
the clauses in the program, the same goal leads to an inﬁnite computation. In
this case, there is no answer for the query, and eventually the interpreter will
give an error message (out of memory space because the leftmost branch of the
SLD-tree that Prolog is trying to build is inﬁnite).
SLD-resolution has interesting computational properties:
1. It is refutation-complete: Given a Prolog program and a goal, if a con-
tradiction can be derived, then SLD-resolution will eventually generate an
empty resolvent.
2. It is independent of the computation rule: If there is an answer for a goal,
SLD-resolution will ﬁnd it whichever selection rule is employed for choosing
the literals resolved upon.
However, the particular tree traversal strategy that Prolog uses is not com-
plete. In the example above, we see that if we change the order of the clauses
in the program, Prolog fails to ﬁnd an answer, even if an empty resolvent can
be generated by SLD-resolution. The problem is that this empty resolvent will
be generated in a branch of the SLD-tree that Prolog does not build.

86
Chapter 5. Logic-Based Models of Computation
There is an easy way to obtain a refutation-complete implementation of
SLD-resolution: using a breadth-ﬁrst search instead of a depth-ﬁrst search.
However, there is a price to pay. A breadth-ﬁrst search strategy will in general
take more time to ﬁnd the ﬁrst answer. For this reason, this strategy is not
used in practice.
Nowadays, several versions of Prolog exist. The basic framework has been
enriched to make it more eﬃcient and easier to use. Extensions include primi-
tive data types such as integers and real numbers, advanced optimisation tech-
niques, ﬁle-handling facilities, graphical interfaces, control mechanisms, and
others. Some of these features are non-declarative, and often programs that
use them are called impure because, to achieve eﬃciency in the program, the
problem description is mixed with implementation details (i.e., the what and
the how are mixed). Constraint logic programming languages, which were de-
veloped from Prolog, achieve eﬃciency by incorporating optimised proof search
methods for speciﬁc domains.
5.5 Further reading
We refer the reader to [23] for more examples of logic programs. Robinson’s
article [43] introduces the Principle of Resolution. More information on the
uniﬁcation algorithm presented in this chapter can be found in Martelli and
Montanari’s article [31]. The book [7] provides an introduction to logic pro-
gramming, and [44] is a reference document for Prolog.
5.6 Exercises
1. Assuming that A, B, C are atoms, which of the following clauses are Horn
clauses?
a) ¬A
b) A ∨B ∨¬C
c) A ∨¬A
d) A
2. Numbers and arithmetic operations are predeﬁned in Prolog. Assume we
deﬁne the predicate mean using the clause
mean(A,B,C) :- C is (A+B)/2.

Chapter 5. Logic-Based Models of Computation
87
What are the answers to the following goals?
:- mean(2,4,X).
:- mean(2,4,6).
3. Show that for the problem f(X) = f(Y) both {X →Y} and {Y →X} are
most general solutions. Can you ﬁnd a diﬀerent substitution that is also a
most general uniﬁer for these terms?
4. Give the most general uniﬁer (if it exists) of the following atoms (recall
that [1,2] is short for the list [1|[2|[]]]):
a) append([1,2],X,U), append([Y|L],Z,[Y|R])
b) append([1,2],X,[0,1]), append([Y|L],Z,[Y|R])
c) append([],X,[0,1]), append([Y|L],Z,[Y|R])
d) append([],X,[0]), append([],[X|L],[Y])
5. Lists are predeﬁned in Prolog; in particular, the predicate append is pre-
deﬁned, but in this exercise we will deﬁne a new append:
myappend([],Y,Y).
myappend([H|T],Y,[H|U]) :- myappend(T,Y,U).
What are the answers to the following goals?
:- myappend([1,2],[3,4,5],[1,2,3,4,5]).
:- myappend([1,2],[3,4,5],[1,2]).
:- myappend([1,2],[3,4,5],X).
:- myappend([1,2],X,[1,2,3,4,5]).
:- myappend(X,[3,4,5],[1,2,3,4,5]).
:- myappend(X,Y,[1,2,3,4,5]).
:- myappend(X,Y,Z).
Explain the answers.
6. Show that the resolvent of the clauses
P :- A1, . . . , An
and

88
Chapter 5. Logic-Based Models of Computation
:- Q1, . . . , Qm
is also a Horn clause.
7. Consider the program
nat(s(X)) :- nat(X).
nat(0).
and the query
:- nat(Y).
a) Describe the complete SLD-resolution tree for this query.
b) Explain why Prolog will not ﬁnd an answer for this query.
c) Change the program so that Prolog can ﬁnd an answer.
8. Write a logic program deﬁning a binary predicate member such that
member(a,l) is true if the element a is in the list l.
What are the answers to the following queries? Draw the SLD-resolution
tree for each one.
a) :- member(1,[2,1,3]).
b) :- member(1,[2,3,4]).
c) :- member(1,[]).
9. What is the purpose of the occur-check in the uniﬁcation algorithm?
10. Write a logic program for sorting a list of numbers (in ascending order)
using the insertion sort algorithm.
For this, you will need to deﬁne:
– a predicate sort such that sort(L,L’) holds if L’ is a list containing
the same elements as L but in ascending order; and
– a predicate insertion such that insertion(X,L,L’) holds if X is a
number, L is a sorted list (in ascending order), and L’ is the result of
inserting X in the corresponding place in the list L.
11. Consider the following program and queries:
Program:
even(0).
even(s(s(X))) :- even(X).

Chapter 5. Logic-Based Models of Computation
89
odd(s(0)).
odd(X) :- even(s(X)).
Queries:
:- odd(s(s(0))).
:- odd(s(0)).
Write an SLD-resolution tree for each query.
We now replace the fourth clause of the program by
odd(X) :- greater(X,s(0)), even(s(X)).
Write the clauses deﬁning the predicate greater such that greater(m,n)
holds when the number m is greater than n.
Give the SLD-tree for the query
:-odd(s(0)).
with the modiﬁed program.
12. A graph is a set V = {a, b, c, . . .} of vertices and a set E ⊆V × V of edges.
We use the binary predicate edge to represent the edges: edge(a,b) means
that there is an edge from a to b. In a directed graph, the edges have a
direction, so edge(a,b) is diﬀerent from edge(b,a). We say that there is
a path from a to b in a graph if there is a sequence of one or more edges
that allows us to go from a to b.
a) Write a logic program deﬁning the predicate path.
b) Write a query to compute all the directed paths starting from a in the
graph.
c) Write a query to compute all the directed paths in the graph.

Part II
Modern Models of Computation

6
Computing with Objects
Turing machines and the λ-calculus are two examples of universal (i.e., Turing-
complete) models of computation. We will now describe another universal
model, based on the use of objects, with method invocation and update as
main operations.
Many modern programming languages are based on the object model: Java,
Eiﬀel, C++, Smalltalk, Self, OCaml, OHaskell, etc. In deﬁning an object-based
model of computation, we will try to encapsulate the essential features of object-
oriented programming languages. These are:
– the ability to create objects, which are collections of ﬁelds and methods;
– the ability to use a method belonging to an object — this is usually called
method invocation, but sometimes the terminology “message passing” is used;
– the ability to modify a method in an object — this is usually called method
update or method override.
An object calculus is the analogue of the λ-calculus for objects rather than
functions. The object calculus that we will describe was introduced by Mart´ın
Abadi and Luca Cardelli in the 1990s. It has primitives to deﬁne methods (and
ﬁelds as a particular case), to call methods that have already been deﬁned,
and to update them. It can be seen as a minimal object-oriented programming
language or as the kernel of an object-oriented language. We will see that the
calculus of objects has the same computation power as the λ-calculus.
In the description of the object calculus in this chapter, we will follow the
same pattern as for the λ-calculus. We present ﬁrst the syntax of the object

94
Chapter 6. Computing with Objects
calculus, then the reduction rules we will use to compute with objects, and
ﬁnally we will discuss the properties of the calculus and its applications to the
design of object-oriented programming languages.
6.1 Object calculus: Syntax
We start by deﬁning the syntax of the terms that we will use to represent
objects and operations on them.
Objects will be represented as collections of methods, each method having
a diﬀerent label (its name) and body (the method’s deﬁnition). The body of
a method can refer to the whole object where the method is deﬁned; in other
words, objects can contain self-references. This is done by using a distinguished
variable, called self or this in object-oriented programming languages. In the
object calculus, we will simply use a bound variable. Thus a method will have
the form l = ς(x)b, where l is the name of the method, and the occurrences
of x in b will represent the object where the method is deﬁned. We say that
the variable x is bound in ς(x)b, and ς is a binder (like λ in the λ-calculus).
Because of the use of this symbol as a binder, the object calculus is sometimes
called ς-calculus.
Methods that do not use a self-reference in their deﬁnition are called ﬁelds;
in other words, a ﬁeld is a method of the form l = ς(x)b, where b does not
contain any occurrence of x. In this case, it can simply be written l = b.
We write objects by listing their methods between square brackets:
[l1 = ς(x1)b1, . . . , ln = ς(xn)bn]
We will sometimes use the notation [li = ς(xi)bi∈1...n
i
] as an abbreviation. Note
that the order in which we write the methods in an object is not important
(objects are sets of methods).
We assume that there is an inﬁnite, countable set X
of variables
x, y, z, . . . , x1, x2, . . ., and an inﬁnite, countable set L of labels l1, . . . , ln, . . .,
such that X and L are disjoint. The language of terms in the ς-calculus is
deﬁned by induction, with variables as a base case, as described below.
Deﬁnition 6.1
The set O of terms is the smallest set that contains
– all the variables in X;
– objects of the form [li = ς(xi)bi∈1...n
i
], where li ∈L, xi ∈X, and bi ∈O, for
all i ∈{1, . . . , n};

Chapter 6. Computing with Objects
95
– method invocations a.l, where a ∈O and l ∈L; and
– method updates a.l ⇐ς(x)b, where a, b ∈O, l ∈L, x ∈X.
An invocation a.l denotes a call to the method with label l in the object
a. Update operations will be used to modify method deﬁnitions (or ﬁelds as a
particular case). For instance, o.l ⇐ς(x)b will be used to replace (only) the
method l in o with ς(x)b. In the case of an update of a ﬁeld, we will simply
write a.l := b.
Since ς is a binder, we have an associated notion of free and bound variables.
Any occurrence of a variable x in b is bound in the term ς(x)b. Occurrences of
variables that are not in the scope of a binder are free. The sets of free and
bound variables of a term can be computed using the functions deﬁned below.
Deﬁnition 6.2 (Free and bound variables)
The set of free variables of o will be denoted by FV (o). This set can be com-
puted as follows:
FV (x)
=
{x}
FV (ς(x)b)
=
FV (b) −{x}
FV ([li = ς(xi)bi∈1...n
i
])
=
i∈1...n FV (ς(xi)bi)
FV (a.l)
=
FV (a)
FV (a.l ⇐ς(x)b)
=
FV (a) ∪FV (ς(x)b)
A term a is closed if it has no free variables; that is, FV (a) = ∅. The set of
bound variables of o is also deﬁned by induction:
BV (x)
=
∅
BV (ς(x)b)
=
BV (b) ∪{x}
BV ([li = ς(xi)bi∈1...n
i
])
=
{x1, . . . , xn} ∪i∈1...n BV (bi)
BV (a.l)
=
BV (a)
BV (a.l ⇐ς(x)b)
=
BV (a) ∪BV (ς(x)b)
As in the λ-calculus, terms represent α-equivalence classes: Two terms that
can be made equal by renaming their bound variables are considered equivalent.
Many of these ideas will become clearer after we give the computation rules
that deﬁne the dynamics of the calculus in the next section.

96
Chapter 6. Computing with Objects
6.2 Reduction rules
There are two computation rules in the ς-calculus: The invocation rule describes
the behaviour of a method invocation, and the update rule describes the eﬀect
of a method update. Object-oriented computation is described as a sequence of
reduction steps using these rules; in other words, a computation is a sequence
of method invocations and updates.
Consider an object o = [li = ς(xi)bi∈1...n
i
]. Intuitively, the invocation of the
method lj in o should trigger the evaluation of the body of this method (i.e.,
bj). Since bj may contain self-references (to the object o), before evaluating bj
all occurrences of xj in bj must be replaced by the object o. More precisely, the
invocation of the method lj in o triggers the evaluation of bj{xj →o}, where
we use the notation {xj →o} to represent the substitution of xj by o. This is
the essence of the invocation rule. The update rule simply replaces a method
with a new deﬁnition.
(invocation)
o.lj
−→
bj{xj →o}
(update)
o.lj ⇐ς(y)b
−→
[lj = ς(y)b, li = ς(xi)bi∈(1...n)−j
i
]
Substitution must be performed with care in the presence of bound vari-
ables. The notion of substitution used in the invocation rule is the same capture-
avoiding notion of substitution that we deﬁned for the λ-calculus in Chapter 3.
We recall it below.
In both rules above, we assume j ∈1 . . . n. We now give a simple example.
Example 6.3
Consider an object o with only one method, called l, whose body is just a
self-reference. In the ς-calculus, this object is deﬁned by the term
o = [l = ς(x)x]
Then, using the invocation rule, o.l −→o.
As usual, we denote a sequence of reduction steps from t to u by t −→∗u.
We deﬁne below the operation of substitution, taking into account the fact
that terms are deﬁned modulo α-equivalence: In substituting under a binder,
we must avoid the capture of free variables.

Chapter 6. Computing with Objects
97
Deﬁnition 6.4 (Substitution)
The substitution of x by c in a term o, written o{x →c}, is deﬁned as follows:
x{x →c}
=
c
y{x →c}
=
y
(ς(y)b){x →c}
=
ς(y′)(b{y →y′}{x →c})
(y′ fresh)
([li = ς(xi)bi∈1...n
i
]){x →c}
=
[li = (ς(xi)bi){x →c}i∈1...n]
(a.l){x →c}
=
a{x →c}.l
(a.l ⇐ς(y)b){x →c}
=
(a{x →c}).l ⇐(ς(y)b){x →c}
Note that in the third case above, when we apply the substitution {x →c} to
ς(y)b, the variable y is renamed to a fresh variable y′ to ensure that there are
no clashes with the variables in c.
Some examples of terms and computations in the object calculus follow.
Example 6.5
1. Empty object: It is possible to deﬁne an empty object, with no methods or
ﬁelds, by writing [ ]. The object o = [empty = [ ]] has just one ﬁeld, and
o.empty −→[ ].
2. Self: It is also possible to deﬁne an object with one method that returns the
object itself, as shown in Example 6.3. If o = [l = ς(x)x], then o.l −→o.
In some object-oriented programming languages, this can be achieved by
returning ‘self’ or ‘this’.
3. Non-termination: Due to the possibility of deﬁning recursive methods (that
is, methods that invoke themselves), objects in this calculus may generate
inﬁnite computation sequences. For instance, if we deﬁne o = [l = ς(x)x.l],
then the method invocation o.l produces a non-terminating computation
sequence:
o.l −→x.l{x →o} = o.l −→· · ·
The reduction relation generated by the invocation and update rules is
conﬂuent.
Property 6.6 (Conﬂuence)
The ς-calculus is conﬂuent: If a −→∗b and also a −→∗c, then there is some d
such that b −→∗d and c −→∗d.

98
Chapter 6. Computing with Objects
A term is in normal form if it is irreducible. Normal forms can be seen as
results; when a program is evaluated, it produces a normal form or an inﬁnite
computation.
The conﬂuence property implies the unicity of normal forms. Although some
terms may not produce a result, if a program gives a result, then this result is
uniquely determined.
6.3 Computation power
We mentioned at the beginning of the chapter that the object calculus is Turing
complete. This can be proved by deﬁning an encoding of the λ-calculus into the
object calculus. Below we show an encoding, deﬁned by Abadi and Cardelli,
that is based on the idea that a function can be represented as an object with
a ﬁeld arg to store the function’s argument and a ﬁeld val that deﬁnes the
function itself. Formally, the encoding is deﬁned inductively. The idea is to
give, for each class of λ-term, the corresponding ς-term. For this, we deﬁne a
transformation function from λ-terms to ς-terms and show that the translated
terms have the same behaviour. For the sake of uniformity, in this chapter we
write λ-abstractions as λ(n)t instead of λn.t.
Deﬁnition 6.7
Let T : λ →ς be a function from λ-terms to ς-terms deﬁned as follows:
T (x) = x
T (λx.M) = [arg = ς(x)x.arg, val = ς(x)T (M){x →x.arg}]
T (MN) = (T (M).arg := T (N)).val
According to the function T , the encoding of a λ-abstraction is an object,
where the body of the λ-abstraction is stored in the method val and any refer-
ence to its argument is replaced by a call to the method arg. Then, the encoding
of an application simply stores the actual argument in the ﬁeld arg and calls
val.
To see that this encoding actually works, we need to show that the reduc-
tions out of a λ-term can be simulated by reductions on the ς-term obtained by
the encoding. More precisely, we need to show that if t −→u in the λ-calculus,
then T (t) −→∗T (u) in the ς-calculus. For this it is suﬃcient to show that
T ((λx.M)N) →∗T (M{x →N}) = T (M){x →T (N)}

Chapter 6. Computing with Objects
99
which we can do as follows:
T ((λx.M)N)
=
(T (λx.M).arg := T (N)).val
=
[arg = T (N), val = ς(x)T (M){x →x.arg}].val
Let o be the object [arg = T (N), val = ς(x)T (M){x →x.arg}]. Then we can
write T ((λx.M)N) = o.val, and we have the following reduction steps:
o.val
−→
(T (M){x →x.arg}){x →o}
=
T (M){x →o.arg}
−→∗
T (M){x →T (N)}
Although we did not include numbers or other data structures in the syntax
of the ς-calculus, it should be clear that data structures can be encoded in this
calculus. For instance, this can be done via an encoding into the λ-calculus,
as shown in Chapter 3, which can itself be encoded in the ς-calculus as shown
above. In what follows, we freely use numbers in examples.
6.4 Object-oriented programming
The object-oriented paradigm is one of the most popular in industry. There are
two diﬀerent ﬂavours of object-oriented languages in use:
– Class-based object-oriented languages: These are the most widespread and
include languages such as C++, Java, and Smalltalk. In class-based languages,
there is a global hierarchy of object generators, called classes, organised by
the inheritance relation. Every object is generated by a single class, and
therefore the partial order between classes induces a categorisation and a
partial order on objects.
– Prototype-based object-oriented languages: In these languages, objects can be
deﬁned directly, without ﬁrst deﬁning a class. There is a cloning operation
that can be used to create additional copies of objects (i.e., objects are seen
as prototypes that can be cloned). Examples of prototype-based languages are
Self and Javascript. These languages, although less popular than class-based
ones, present remarkable features in terms of ﬂexibility, expressiveness, and
conceptual simplicity.
The object calculus is classiﬁed as prototype-based since there is no primitive
notion of class in the calculus. However, classes can be encoded in prototype-
based calculi. Before showing the encoding, let us give some simple examples
that relate the ς-calculus to class-based languages in the style of Java.
Consider the following program deﬁning the class Empty:

100
Chapter 6. Computing with Objects
class Empty {
}
class Test {
public static void main(String[] args){
Empty o = new Empty();
}
}
Objects in the class Empty do not contain any ﬁelds or methods. The object
o created with the command new Empty() corresponds to the object [ ] in the
ς-calculus.
Consider now the object
o = [l = 3]
in the ς-calculus, and the new object obtained by evaluating the expression
o.l ⇐4
which updates the value of the ﬁeld l. The same eﬀect can be obtained by
deﬁning the following classes in Java:
class Number {
int l = 3;
}
class Test {
public static void main(String[] args){
Number o = new Number();
o.l = 4;
}
}
The previous examples illustrate the creation of simple objects via the def-
inition of classes and the deﬁnition of the same objects in a direct way in the
ς-calculus. We can use this technique to simulate classes in the object calculus.
Since there is no primitive notion of class in the object calculus, we will use
objects to deﬁne classes. More precisely, a class will be an object with
– a method new for creating new objects and
– all the methods needed in the objects generated from the class.
For instance, to generate an object
o = [li = ς(xi)bi∈1...n
i
]

Chapter 6. Computing with Objects
101
we will use the class
c = [new = ς(z)[li = ς(x)z.li(x)i∈1...n], lj = λ(xj)bj∈1...n
j
]
It is easy to see that c.new = o since c.new −→[li = ς(x)c.li(x)i∈1...n], and
c.li(x) = bi.
We call the method new a generator. Each ﬁeld li is called a premethod.
As the attentive reader might have noticed, in this encoding of classes we
have also used the λ-calculus: We have deﬁned the methods lj in the class c as
λ-abstractions (i.e., functions) with formal argument xj and body bj. However,
it is also possible to encode classes using just the object calculus since we have
already shown that the λ-calculus can be encoded in the ς-calculus.
As this encoding shows, it is convenient to have access to both object prim-
itives and functions in the same calculus. We will deﬁne functional extensions
of the object calculus below.
6.5 Combining objects and functions
Although object-oriented languages are popular, languages that are based solely
on objects deprive users of a certain number of useful programming techniques
available in other paradigms (e.g., pattern matching on data, typical of mod-
ern functional programming languages). Ideally, we would like a programming
language to oﬀer the best features of each of the programming paradigms. How-
ever, combining object-oriented and functional programming styles in a single
multiparadigm language is not an easy task. The problem is ﬁnding a uniform
way of integrating both styles rather than glueing together a functional and an
object-oriented language. Several solutions have been proposed; for instance,
the programming languages OCaml, OHaskell, and Scala smoothly integrate
features of object-oriented languages and functional languages. Their under-
lying model of computation can be explained by extending the ς-calculus to
include other features, such as
– basic data types (e.g., numbers, Booleans),
– the λ-calculus, and
– more general reduction systems.
There are several motivations for using such combinations. For instance, it
is generally more eﬃcient to add “built-in” data structures rather than deﬁne
them in the basic calculus (numbers especially). Also, other calculi may provide
a more natural representation for certain features; typically, an input-output

102
Chapter 6. Computing with Objects
behaviour is more naturally captured as a function, and functions are more
naturally represented using the λ-calculus.
The addition of features such as numbers and functions allows the program-
mer to deﬁne in a concise way systems that would require heavy encodings in
the ς-calculus. However, the addition of these features does not oﬀer any ad-
ditional computation power. We have already seen that we can encode the
λ-calculus in the ς-calculus, and we can also encode numbers in these calculi.
Thus we can always replace the new features with the corresponding simulation
in the pure object calculus.
It is also possible to model imperative features in the object calculus; for
instance, memory locations (which can be directly manipulated in an impera-
tive language) can also be initialised and updated using the object calculus, as
the following example shows.
Example 6.8
We can model a memory cell in the ς-calculus by using an object with a ﬁeld
to store a value and a method set to change this value. The object loc deﬁned
below represents a memory location storing the value 0.
loc = [value = 0, set = ς(x)λ(n)x.value := n]
In this example, the ﬁeld value contains a number, and the method set allows
us to change this number.
The method invocation loc.value can be used to retrieve the value stored at
the location. Note that the method set is deﬁned using a function (represented
by a λ-abstraction) that takes as its argument the new value n to be stored at
the location.
Below we show a reduction sequence for the term loc.set(2):
loc.set(2)
−→
(λ(n)[value = 0, set = ς(x)λ(n)x.value := n].value := n)2
−→
[value = 0, set = ς(x)λ(n)x.value := n].value := 2
−→
[value = 2, set = ς(x)λ(n)x.value := n]
Thus loc.set(2).value −→∗2.
Functional and object-oriented computations ﬁt naturally in the example
above. The functional part in this example models the input of the value (in
this case the number 2), and the object-oriented part models the update of the
memory location with the input value.
We give below another example that shows the combined use of functions
and objects to model the behaviour of a pocket calculator.

Chapter 6. Computing with Objects
103
Example 6.9 (Calculator)
The calculator will be represented by an object calc, with an accumulator,
represented by the ﬁeld acc, and methods representing the arithmetic opera-
tions addition, subtraction, etc. The method equals behaves like the key = in
a pocket calculator.
calc
=
[arg = 0.0,
acc = 0.0,
enter = ς(s)λ(n)s.arg := n,
add = ς(s)(s.acc := s.equals).equals ⇐ς(s′)s′.acc + s′.arg,
sub = ς(s)(s.acc := s.equals).equals ⇐ς(s′)s′.acc −s′.arg,
· · ·
equals = ς(s)s.arg]
For example, the term calc.enter(5.0).equals reduces to the value 5.0, and
calc.enter(5.0).sub.enter(3.5).equals reduces to 1.5.
In the example above, we have used an extension of the ς-calculus that
includes numbers and basic functions to operate on them. The extension does
not increase the computation power of the calculus but makes it easier to use.
A more radical approach consists of adding general rewrite rules of the form
l →r
where l and r are terms. A rewrite rule can be thought of as an oriented equality,
or a simpliﬁcation step that can be used to compute the value of an expression.
For example, x + 0 →x is a rewrite rule with the intended meaning that to
compute the result of x + 0 for any arbitrary expression x we just need to
compute the value of x.
An extension of this kind, with arbitrary rewrite rules, can change the
computation properties of the calculus since it is not the case that all rewrite
rules can be encoded in the ς-calculus. The reason is simply that the addition
of arbitrary reduction rules can break the conﬂuence of the system. Moreover,
even if we restrict the extension to sets of conﬂuent rules, the resulting system
may be non-conﬂuent, as the following example shows.
Consider the rewrite system
f x x −→0
f x S(x) −→1
This system is conﬂuent, but the combination of the ς-calculus with such
rules leads to a non-conﬂuent system. Take o = [l = ς(x)S(x.l)], and examine
the possible results of f o.l o.l. Using the ﬁrst rule,
f o.l o.l →0

104
Chapter 6. Computing with Objects
Using the invocation rule, o.l −→S(o.l), and therefore
f o.l o.l →f o.l S(o.l) →1
However, if we extend the ς-calculus using only left-linear rules (that is,
rules such that on the left-hand side each variable occurs at most once), then
the extended calculus is conﬂuent. In particular, all the extensions mentioned
above (ς-calculus combined with numbers and arithmetic operations, ς-calculus
combined with λ-calculus, etc.) fall into this class and are therefore conﬂuent.
6.6 Further reading
For more details on object calculi, we refer the reader to Abadi and Cardelli’s
book [1]. The class-based object-oriented programming languages C++, Java,
and Smalltalk are described in [48],
[20], and
[25], respectively. For further
information about prototype languages, see the descriptions of Self [52] and
Javascript [13]. For more details on languages combining object-oriented and
functional features, see the descriptions of OCaml [30] and Scala [38]. Further
information on combinations of object calculi and rewrite rules can be found
in [9].
6.7 Exercises
1. What is the fundamental diﬀerence between a method deﬁned by l = ς(x)b
in an object o and a function with argument x deﬁned by the λ-term λ(x)b?
2. Describe at least two diﬀerent ways to encode numbers in the object cal-
culus.
3. Add a method get in the object loc deﬁned in Example 6.8 to represent a
memory location, so that the ﬁeld value is accessed by get.
4. In a calculus that combines objects, functions, numbers, and arithmetic
functions, we have deﬁned the following object:
loc
=
[value = 0,
set = ς(x)λ(n)x.value := n,
incr = ς(x)x.value := x.value + 1]
a) Describe in your own words the behaviour of the methods set and incr.

Chapter 6. Computing with Objects
105
b) Evaluate the terms (and show the reduction steps)
i. loc.set(1).set(3).value
ii. loc.incr.value
where loc is the object deﬁned above.
5. Show the reduction sequences for the following terms using the deﬁnition
of the calculator in Example 6.9:
calc.enter(5.0).equals
calc.enter(5.0).sub.enter(3.5).equals
calc.enter(5.0).add.add.equals
6. Recall the translation function T from the λ-calculus to the ς-calculus
deﬁned in this chapter:
T (x) = x
T (λx.M) = [arg = ς(x)x.arg, val = ς(x)T (M){x →x.arg}]
T (MN) = (T (M).arg := T (N)).val
a) Using this deﬁnition, write down the ς-terms obtained by the following
translations:
i. T (λx.x)
ii. T (λxy.x)
iii. T (λy.(λx.x)y)
iv. T ((λx.x)(λy.y))
b) Reduce T ((λx.x)(λy.y)) to normal form using the reduction rules of
the ς-calculus.
c) What are the advantages and disadvantages of a computation model
that combines the ς-calculus and additional rewriting rules? Compare
it with the pure ς-calculus.
7. Indicate whether each of the following statements about the ς-calculus is
true or false and why.
a) The ς-calculus is conﬂuent; therefore each expression has at most one
normal form in this calculus.
b) The ς-calculus does not have an operation to add methods to an object;
therefore it is not a Turing-complete model of computation.

7
Interaction-Based Models of Computation
In this chapter, we study interaction nets, a model of computation that can be
seen as a representative of a class of models based on the notion of “computation
as interaction”. Interaction nets are a graphical model of computation devised
by Yves Lafont in 1990 as a generalisation of the proof structures of linear logic.
It can be seen as an abstract formalism, used to deﬁne algorithms and analyse
their cost, or as a low-level language into which other programming languages
can be compiled. This is fruitful because interaction nets can be implemented
with reasonable eﬃciency.
An interaction net system is speciﬁed by a set of agents and a set of in-
teraction rules. One can think of agents as logical symbols (connectives) and
interaction rules as a speciﬁcation of their meaning. There is also an analogy
with electric circuits, where the agents are seen as gates and the edges as wires
connecting the gates. Or we can simply think of the agents as computation
entities, with interaction rules specifying their behaviour.
In the following sections, we give an overview of the interaction paradigm,
give examples of uses of interaction nets to express algorithms, and also show
how other computation models can be encoded in interaction nets.
7.1 The paradigm of interaction
Interaction net systems are speciﬁed by giving a set Σ of symbols used to build
nets and a set R of rewrite rules, called interaction rules, that must satisfy the

108
Chapter 7. Interaction-Based Models of Computation
set of conditions given below. Each symbol α ∈Σ has an associated (ﬁxed)
arity, a natural number. We assume that the function ar : Σ →Nat provides
the arity of each symbol in Σ.
Deﬁnition 7.1 (Net)
A net N built on Σ is a graph (not necessarily connected) where nodes are
labelled by symbols in Σ. A labelled node is called an agent, and an edge
between two agents is called a wire, so nets are graphs built out of agents and
wires.
The points of attachment of wires are called ports. If the arity of α is n,
then a node labelled with α must have n + 1 ports: a distinguished one called
the principal port, depicted by an arrow, and n auxiliary ports corresponding
to the arity of the symbol.
We index ports clockwise from the principal port, and hence the orientation
of an agent is not important. If ar(α) = n, then an agent α is represented
graphically in the following way:


α
?
@

· · ·
x1
xn
or equivalently


α
6

@
· · ·
xn
x1
Note that this agent has been rotated (not reﬂected) and the ports are
indexed in the same way. If ar(α) = 0, then the agent has no auxiliary ports,
but it will always have a principal port.
In an interaction net, edges connect agents together at the ports such that
there is at most one edge at each port (edges may connect two ports of the
same agent). The ports of an agent that are not connected to another agent
are called free ports. There are two special instances of a net that we should
point out. A net may contain only edges (no agents); this is called a wiring,
and the free extremities of the edges are also called ports. In this case, if there
are n edges, then there are 2n free ports in the net. If a net contains neither
edges nor agents, then it is the empty net. The interface of a net is its set of
free ports.
Deﬁnition 7.2 (Interaction rule)
A pair of agents (α, β) ∈Σ × Σ connected together on their principal ports is
called an active pair; this is the interaction net analogue of a redex, and it will
be denoted α ⊲⊳β.

Chapter 7. Interaction-Based Models of Computation
109
An interaction rule α ⊲⊳β =⇒N in R is composed of an active pair on
the left-hand side and a net N on the right-hand side. Rules must satisfy two
strong conditions:
1. In an interaction rule, the left- and right-hand sides have the same interface;
that is, all the free ports are preserved. The following diagram illustrates
the idea, where N is any net built from Σ.


α


β
-
@


@
...
...
xn
x1
ym
y1
=⇒
N
...
...
xn
x1
ym
y1
We remark that the net N may contain occurrences of the agents α and β.
N can be just a wiring (but only if the number of free ports in the active
pair is even), and if there are no free ports in the active pair, then the net
N may be (but is not necessarily) the empty net.
2. In a set R of interaction rules, there is at most one rule for each unordered
pair of agents (that is, only one rule for α ⊲⊳β, which is the same as the
rule for β ⊲⊳α).
Interaction rules generate a reduction relation on nets, as shown below.
Deﬁnition 7.3
A reduction step using the rule α ⊲⊳β =⇒N replaces an occurrence of the
active pair α ⊲⊳β by a net N. More precisely, we write W =⇒W ′ if there is
an active pair α ⊲⊳β in W and an interaction rule α ⊲⊳β =⇒N in R such
that W ′ is the net obtained by replacing α ⊲⊳β in W with N (since N has the
same interface as α ⊲⊳β, there are no dangling edges after the replacement).
We write =⇒for a single interaction step and =⇒∗for the transitive re-
ﬂexive closure of the relation =⇒. In other words, N =⇒N ′ indicates that we
can obtain N ′ from N by reducing one active pair, and N =⇒∗N ′ indicates
that there is a sequence of zero or more interaction steps that take us from N
to N ′.
We do not require a rule for each pair of agents, but if we create a net with
an active pair for which there is no interaction rule, then this pair will not be
reduced (it will be blocked).
It is important to note that the interface of the net is ordered. Adopting this
convention, we can avoid labelling the free edges of a net. To give an example,
we can write the rule

110
Chapter 7. Interaction-Based Models of Computation


α


β
-
@


@
x2
x1
y2
y1
=⇒
x2
x1
y2
y1
that connects x1 with y2 and x2 with y1 equivalently as the rule


α


β
-
@


@
x2
x1
y2
y1
=⇒
x2
x1
y1
y2
but in the latter the labelling is essential (the diﬀerence being that we have
changed the order of the free ports of the net). We will always make an eﬀort,
at the cost of making the rules look more complicated, to ensure that the order
of the edges is always preserved when we write a rule to avoid having to label
the edges (adopting the same convention for nets as we did for agents).
An interaction net is in full normal form (we will often just call it normal
form) if there are no active pairs. The notation N ⇓N ′ indicates that there
exists a ﬁnite sequence of interactions N =⇒∗N ′ such that N ′ is a net in
normal form. We say that a net N is normalisable if N ⇓N ′; N is strongly
normalisable if all sequences of interactions starting from N are ﬁnite.
As a direct consequence of the deﬁnition of interaction nets, in particular
of the constraints on the rewrite rules, reduction is (strongly) commutative in
the following sense: If two diﬀerent reductions are possible in a net N (that
is, N =⇒N1 and N =⇒N2), then there exists a net M such that N1 and
N2 both reduce in one step to M: N1 =⇒M and N2 =⇒M. This property
is stronger than conﬂuence (it is sometimes called strong conﬂuence or the
diamond property); it implies conﬂuence. Consequently, we have the following
result.
Proposition 7.4
Let N be a net in an interaction system (Σ, R). Then:
1. If N ⇓N ′, then all reduction sequences starting from N are terminating
(N is strongly normalisable).
2. Normal forms are unique: If N ⇓N ′ and N ⇓N ′′, then N ′ = N ′′.
Below we give an example of the implementation of two familiar operations
using interaction nets.

Chapter 7. Interaction-Based Models of Computation
111
Example 7.5
The following interaction rules deﬁne two ubiquitous agents, namely the erasing
agent (ǫ), which deletes everything it interacts with, and the duplicator (δ),
which copies everything.


ǫ


α
?
6

@
· · ·
=⇒


ǫ
?
· · ·


ǫ
?


α

@
· · ·


δ
@

6
?
=⇒


α


α


δ


δ
· · ·
?
?
6
6


@
@
@
In the diagrams representing the rules, α denotes any agent. Indeed, there is
one rule deﬁning the interaction between ǫ and each agent α in Σ and also one
rule for each pair δ ⊲⊳α.
According to the ﬁrst rule above, the interaction between α and ǫ deletes
the agent α and places erase agents on all the free edges of the agent. Note that
if the arity of α is 0, then the right-hand side of the rule is the empty net; in this
case the interaction marks the end of the erasing process. One particular case
of this is when α is an ǫ agent itself. These rules provide the garbage collection
mechanism for interaction nets.
In the second rule, we see that the α agent is copied, and the δ agents placed
on the free edges can now continue copying the rest of the net.
7.2 Numbers and arithmetic operations
Natural numbers can be represented using 0 and a successor function, as de-
scribed in previous chapters. For example, the number 3 is represented by
S(S(S(0))). Consider the following speciﬁcation of the standard addition op-
eration:
add(0, y)
=
y
add(S(x), y)
=
S(add(x, y))
which indicates that adding 0 to any number y gives y as a result, and to add
x + 1 to y, we need to compute x + y and add 1.

112
Chapter 7. Interaction-Based Models of Computation
To code this system into an interaction net program, we introduce three
agents, corresponding to add, S, and 0. These are drawn as follows.


0
6


S
6


add
@
@



Next we must specify the rules of interaction. In this case, we can mirror the
speciﬁcation of addition given above. The two rules that we need are as follows:


0


add


S


add


add


S
6

@


@





@
=⇒
=⇒
These rules trivially satisfy the requirements of preserving the interface for
an interaction.
Consider the net corresponding to the term add(S(0), S(0)):


S


0
6


0
6


add
@@





S
I
In this example, there is only one choice of reduction since at each step there
is only one possible interaction that can take place. The complete sequence of
reductions is shown below. The result is a net representing S(S(0)), as expected.


S


0
6


0
6


add
@@





S
I
=⇒


0


0
6


add


S
6
@@





S
I
=⇒


0
6


S
6


S
6

Chapter 7. Interaction-Based Models of Computation
113
This example is rather too simple though to bring out the essential features
of interaction nets. A more interesting example is the coding of the operation
of multiplication, speciﬁed by
mult(0, y)
=
0
mult(S(x), y)
=
add(mult(x, y), y)
To give an algorithm to multiply numbers using interaction nets, we need to
introduce a new agent, m, to represent the multiplication operator. The inter-
action rules for this agent are more involved than those for add due to the fact
that multiplication is not a linear operation (as was the case with addition).
To keep in line with the deﬁnition of an interaction rule, we must preserve the
interface. To illustrate this, here are the two rules for multiplication:


0


m


ǫ


0
@
=⇒



6
?


S


m



=⇒


add


m


δ
@
?




To preserve the interface, we have used the erasing and duplicating agents
(ǫ and δ), that were introduced in Example 7.5.
This example illustrates one of the most interesting aspects of interaction
nets. It is impossible to duplicate active pairs, and thus sharing of computation
is naturally captured. Indeed, to duplicate a net, δ must be able to interact with
all the agents in the net, but if α and β are connected on their principal ports,
they cannot interact with δ and therefore cannot be copied.
Below we give another example of an operation on numbers and its deﬁnition
using interaction nets.
Example 7.6
Consider the function that computes the maximum of two natural numbers:
max(0, y)
=
y
max(x, 0)
=
x
max(S(x), S(y))
=
S(max(x, y))
The problem with this speciﬁcation is that it is deﬁned by cases on both of the
arguments. If we follow the same ideas as in the previous examples, we would
need two principal ports for the agent max, but this is not possible in interaction
nets. However, we can transform the speciﬁcation of max, introducing a new

114
Chapter 7. Interaction-Based Models of Computation
function max′, to obtain an equivalent system where each operation is deﬁned
by cases on only one argument:
max(0, y)
=
y
max(S(x), y)
=
max′(x, y)
max′(x, 0)
=
S(x)
max′(x, S(y))
=
S(max(x, y))
The corresponding interaction rules are:


0


max
=⇒
@





mx′


0


S

@@
6
=⇒
R
I


S


max



@
=⇒


mx′

@
R


mx′


S


S


max
=⇒

@@


@
6
R
I
The deﬁnition of a system of interaction for computing the minimum of two
numbers is left as an exercise (see Section 7.9).
This example suggests a method of compiling functions on numbers into
interaction nets. Indeed, it is possible to compile all functional programs into
interaction nets. Interaction nets are in fact a universal programming language,
as we will see in the next section.
7.3 Turing completeness
To show that a model of computation is Turing complete, we have to prove
that any computable function can be represented. In the case of interaction
nets, this can be shown for instance by giving an encoding of combinatory
logic (CL). Combinatory logic was introduced in Chapter 3 (see Exercise 11)
as a system of combinators with constants, S and K, and two reduction rules
with the same power as the λ-calculus. Let us recall the reduction rules:
K x y
→
x
S x y z
→
x z (y z)

Chapter 7. Interaction-Based Models of Computation
115
To represent CL as a system of interaction nets, we require an agent @ corre-
sponding to application and several agents for the combinators. For example,
the K combinator is encoded by introducing two agents, K0 and K1, and two
interaction rules:


K0


@



@
=⇒


K1
6


K1


@



@
=⇒


ǫ
?
The combinator S can be deﬁned in a similar way using three agents and three
interaction rules; we leave it as an exercise.
Interaction nets have also been used to implement λ-calculus evaluators.
Indeed, the ﬁrst implementation of the optimal reduction strategy for the λ-
calculus (that is, the strategy that makes the minimum number of β-reduction
steps in order to normalise terms) used interaction nets. Interaction nets are
also used in other (non-optimal, but in some cases more eﬃcient) implementa-
tions of the λ-calculus.
Actually, if we restrict ourselves to linear λ-terms (see the deﬁnition of the
linear λ-calculus in Exercise 10 of Chapter 3), we only need an application
agent @ and an abstraction agent λ; variables can be encoded by wires. Then
the β-reduction rule is simply encoded as follows:


λ


@
6
?

@
@

=⇒
For general λ-terms, we have to introduce copying agents and erasing agents,
as well as auxiliary agents to keep track of the scope of abstractions.
7.4 More examples: Lists
We can represent lists in interaction nets in diﬀerent ways. For instance, we
can build a list by using a binary agent cons to link the ﬁrst element of the list
to the rest of the list. The empty list can be represented with an agent nil. This
representation of lists mimics the traditional speciﬁcation of the list data struc-
ture in functional languages using constructors cons and nil. For instance, in a

116
Chapter 7. Interaction-Based Models of Computation
functional language, we could deﬁne list concatenation (the append function)
as follows:
append(nil, l)
=
l
append(cons(x, l), l′)
=
cons(x, append(l, l′))
Using this representation of lists, the time required to concatenate two lists
is proportional to the length of the lists (more precisely, with the deﬁnition
above, it is proportional to the length of the ﬁrst list).
A trivial encoding of lists and the concatenation operator in interaction
nets, following the speciﬁcation above, uses three agents: cons, nil, and append.
However, if we use graphs instead of trees to represent lists, then we can obtain a
more eﬃcient implementation. The idea, to speed up the append function, is to
have direct access to the ﬁrst and last elements of the lists. Since interaction nets
are general graphs, not just trees, this can be achieved simply by representing
a list as a linked structure, using an agent Diﬀto hold pointers to the ﬁrst and
last elements of the list (the name comes from diﬀerence lists) and an agent
cons as usual to link the internal elements.
The empty list, nil, is then encoded by the net:


Diﬀ
6
The operation of concatenation is implemented in constant time with the
net


Diﬀ
6


Open


Open
?
?
using an additional interaction rule that allows us to access the lists:


Open


Diﬀ
6
?

@
@

=⇒

Chapter 7. Interaction-Based Models of Computation
117
For example, we have the following reduction:
Diff
Cons
Cons
Cons
Dappend
Diff
Diff
Cons
Cons
Diff
Diff
Cons
Diff
Diff
Open
Diff
Cons
Cons
Cons
=⇒
=⇒
C
C
C
O
O
O
M
M
M
7.5 Combinators for interaction nets
The combinators S and K of combinatory logic provide a complete characteri-
sation of computable functions; similarly, there is a universal set of combinators
for interaction nets that uses three agents called δ, γ, and ǫ. In Figure 7.1, we
give these three basic agents. The ﬁrst two provide multiplexing operations
(i.e., merging two wires into one), and the third is an erasing operation. All in-
teraction nets can be built from these agents by simply wiring agents together.


γ
@



δ
@



ǫ
?
?
?
Figure 7.1
Interaction combinators: γ, δ, and ǫ.
In Figure 7.2, we give the six interaction rules for this system. It is clear
that the ǫ agent behaves as an erasing operation in that it consumes everything
it interacts with. The multiplexing agents either annihilate each other (if they
are the same agent), giving a wiring, or they mutually copy each other (if they
are diﬀerent). Note that the right-hand side in the ﬁnal rule is the empty net.
This system of combinators is universal in the sense that any other inter-
action net system can be encoded using these combinators. There are other
universal systems of combinators for interaction nets.

118
Chapter 7. Interaction-Based Models of Computation


δ


δ
6
?
@


@
=⇒


ǫ


ǫ
6
?
=⇒


δ


γ
6
?
@


@
=⇒


δ


δ


γ


γ
6
6
?
?


@
@
@


γ


γ
6
?
@


@
=⇒


ǫ


γ
6
?
@

=⇒


ǫ


ǫ
6
6


ǫ


δ
6
?
@

=⇒


ǫ


ǫ
6
6
Figure 7.2
Interaction rules for the interaction combinators.
7.6 Textual languages and strategies for
interaction nets
The graphical language of interaction nets is very natural, and diagrams are
often easier to grasp than a textual description. However, a formal, textual
account of interaction nets has many advantages: It simpliﬁes the actual writing
of programs (graphical editors are not always available), and static properties
of nets, such as types, can be deﬁned in a more concise way. Indeed, several
textual notations for interaction nets have been devised. Below we describe
three notations through an example before developing one of the notations
into a full textual interaction calculus.
As a running example, consider the net given in Figure 7.3 and the inter-
action rule for β-reduction in the linear λ-calculus given in Section 7.3.
A natural textual notation for nets consists of listing all the agents, with
their ports, using some convention. For instance, we could list the ports clock-

Chapter 7. Interaction-Based Models of Computation
119


λ


@


λ
?
?
@

R
Figure 7.3
A λ-term represented as a net.
wise, starting from the principal port. Edges in the net can be represented by
using the same port name. Using these conventions, the example net above is
written
@(a, b, c), λ(a, d, d), λ(b, e, e)
since we have two λ agents and an application agent @. Note the repetition of
name ports to deﬁne edges; for instance, in λ(a, e, e), the repeated e indicates
that there is a wire linking the two auxiliary ports of this λ agent.
The same notation can be used to represent interaction rules. For example,
the interaction rule for linear β-reduction is written
@(a, b, c), λ(a, d, e) =⇒I(c, e), I(b, d)
where the symbol I is used to represent wirings (they are not attached to
agents). Note that exactly the same ports are used on the left- and right-hand
sides since interaction rules preserve the interface.
Another alternative is to use indices instead of names for ports, starting
with the index 0 for the principal port. For the example net above, we use a
set of agents:
Σ = {@1, λ1, λ2}
The linear β-rule is written
(λi, @j) −→(∅, {λi.1 ≡@j.1, λi.2 ≡@j.2})
and the example net is represented by
({@1, λ1, λ2}, {@1.0 ≡λ1.0, @1.1 ≡λ2.0, λ1.1 ≡λ1.2, λ2.1 ≡λ2.2})
A third alternative, which yields a more compact notation, is based on a
representation of active pairs as equations. In this case, our example net is
written
λ(a, a) = @(λ(b, b), c)
where the = sign represents the connection between the principal port of the
λ agent on the left-hand side and the principal port of the @ agent on the

120
Chapter 7. Interaction-Based Models of Computation
right-hand side (i.e., the equation encodes an active pair formed by a λ agent
and an @ agent). The left-hand side λ(a, a) of the equation indicates that both
auxiliary ports of this λ agent are connected and similarly for λ(b, b).
We follow the same approach to represent rules, but this time we use the
symbol ⊲⊳instead of =. For example, the linear β-reduction rule is written
@(x, y) ⊲⊳λ(x, y)
This notation, being more concise, is more suitable for the implementation
of interaction net systems. The textual calculus of interaction that we present
below is based on these ideas.
7.6.1 A textual interaction calculus
In this section, we describe a textual calculus for interaction nets that gives a
formal account of the reduction process.
Interaction nets are strongly conﬂuent, but as in all reduction systems, there
exist diﬀerent notions of strategies and normal forms (for instance, irreducible
nets, or weak normal forms associated with lazy reduction strategies). We will
see that these can be precisely deﬁned in the calculus. Such strategies have
applications for encodings of the λ-calculus, where interaction nets have had
the greatest impact, and where a notion of a strategy is required to avoid
non-termination.
We begin by describing the syntax of the interaction calculus.
Agents: Let Σ be a set of symbols α, β, . . ., each with a given arity (formally,
we assume that there is a function ar: Σ →Nat that deﬁnes the arity of
each symbol). An occurrence of a symbol will be called an agent. The arity
of a symbol corresponds precisely to its number of auxiliary ports.
Names: Let N be a set of names x, y, z, etc. N and Σ are assumed disjoint.
Terms: A term is built using agents in Σ and names in N. Terms are generated
by the grammar
t ::= x | α(t1, . . . , tn)
where x ∈N, α ∈Σ, and ar(α) = n, with the restriction that each name
can appear at most twice in a term. If n = 0, then we omit the parentheses.
If a name occurs twice in a term, we say that it is bound; otherwise it is
free. Since free names occur exactly once, we say that terms are linear.
We write ⃗t for a list of terms t1, . . . , tn.

Chapter 7. Interaction-Based Models of Computation
121
A term of the form α(⃗t) can be seen as a tree with edges between the leaves
if names are repeated; the principal port of α is at the root, and the terms
t1, . . . , tn are the subtrees connected to the auxiliary ports of α. Note that
all the principal ports have the same orientation, and therefore there are
no active pairs in such a tree.
Equations: If t and u are terms, then the (unordered) pair t = u is an equation.
Δ, Θ, . . . will be used to range over multisets of equations. Examples of
equations include x = α(⃗t), x = y, α(⃗t) = β(⃗u). Equations allow us to
represent nets with active pairs.
Rules: Rules are pairs of terms written as α(⃗t) ⊲⊳β(⃗u), where (α, β) ∈Σ × Σ
is the active pair of the rule (that is, the left-hand side of the graphical
interaction rule), and ⃗t, ⃗u are terms. All names occur exactly twice in a
rule, and there is at most one rule for each pair of agents.
Deﬁnition 7.7 (Names in terms)
The set N(t) of names of a term t is deﬁned in the following way, which extends
to multisets of equations and rules in the obvious way.
N(x)
=
{x}
N(α(t1, . . . , tn))
=
N(t1) ∪· · · ∪N(tn)
Given a term, we can replace its free names by new names, provided the lin-
earity restriction is preserved.
Deﬁnition 7.8 (Renaming)
The notation t{x →y} denotes a renaming that replaces the free occurrence
of x in t by a new name y. Note that since the name x occurs exactly once in
the term, this operation can be implemented directly as an assignment, as is
standard in the linear case. This notion extends to equations and multisets of
equations in the obvious way.
More generally, we consider substitutions that replace free names in a term
by other terms, always assuming that the linearity restriction is preserved.
Deﬁnition 7.9 (Substitution)
The notation t{x →u} denotes a substitution that replaces the free occurrence
of x by the term u in t. We only consider substitutions that preserve the linearity
of the terms.

122
Chapter 7. Interaction-Based Models of Computation
Note that renaming is a particular case of substitution. Substitutions have
the following commutation property.
Proposition 7.10
Assume that x ̸∈N(v).
If y ∈N(u), then t{x →u}{y →v} = t{x →u{y →v}}; otherwise
t{x →u}{y →v} = t{y →v}{x →u}.
We now have all the machinery that we need to deﬁne nets in this calculus.
Deﬁnition 7.11 (Conﬁgurations)
A conﬁguration is a pair c = (R, ⟨⃗t | Δ⟩), where R is a set of rules, ⃗t a sequence
t1, . . . , tn of terms, and Δ a multiset of equations. Each name occurs at most
twice in c. If a name occurs once in c, then it is free; otherwise it is bound.
For simplicity, we sometimes omit R when the set of rules used is clear from
the context. We use c, c′ to range over conﬁgurations. We call ⃗t the head or
observable interface of the conﬁguration.
Intuitively, ⟨⃗t | Δ⟩represents a net that we evaluate using R, and Δ repre-
sents the active pairs and the renamings of the net. It is a multiset (i.e., a set
where elements may be repeated since we may have several occurrences of the
same active pair). The roots of the terms in the head of the conﬁguration and
the free names correspond to ports in the interface of the net. We work modulo
α-equivalence for bound names as usual. Conﬁgurations that diﬀer only in the
names of the bound variables are equivalent since they represent the same net.
There is an obvious (although not unique) translation between the graphical
representation of interaction nets and the conﬁgurations that we are using.
Brieﬂy, to translate a net into a conﬁguration, we ﬁrst orient the net as a
collection of trees with all principal ports facing in the same direction. Each
pair of trees connected at their principal ports is translated as an equation, and
any tree whose root is free or any free port of the net goes in the head of the
conﬁguration. We give below a simple example to explain this translation.
Example 7.12
The usual encoding of the addition of natural numbers (see Section 7.2) uses
the agents Σ = {0, S, add}, where ar(0) = 0, ar(S) = 1, ar(add) = 2. The
diagrams below illustrate the net representing the addition 1+0 in the “usual”
orientation and also with all the principal ports facing up.

Chapter 7. Interaction-Based Models of Computation
123


0


S


add


0
6

@
@
I




0


S


0


add
6
6
6

@

x
We then obtain the conﬁguration ⟨x | S(0) = add(x, 0)⟩, where the only port
in the interface is x, which we put in the head of the conﬁguration.
The reverse translation simply requires that we draw the trees for the terms,
connect the common variables together, and connect the trees corresponding
to the members of an equation together on their principal ports.
Deﬁnition 7.13 (Computation rules)
The operational behaviour of the system is given by the following set of com-
putation rules:
Interaction: If (α(t′
1, . . . , t′
n) ⊲⊳β(u′
1, . . . , u′
m)) ∈R, then
⟨⃗t | α(t1, . . . , tn) = β(u1, . . . , um), Γ⟩−→
⟨⃗t | t1 = t′
1, . . . , tn = t′
n, u1 = u′
1, . . . , um = u′
m, Γ⟩
Indirection: If x ∈N(u), then
⟨⃗t | x = t, u = v, Γ⟩−→⟨⃗t | u{x →t} = v, Γ⟩
Collect: If x ∈N(⃗t), then
⟨⃗t | x = u, Δ⟩−→⟨⃗t{x →u} | Δ⟩
Multiset: If Θ ⇀
↽∗Θ′, ⟨⃗t1 | Θ′⟩−→⟨⃗t2 | Δ′⟩, Δ′ ⇀
↽∗Δ, then
⟨⃗t1 | Θ⟩−→⟨⃗t2 | Δ⟩
These rules generate a reduction relation −→on conﬁgurations. We denote
by −→∗the reﬂexive and transitive closure of −→.
The ﬁrst rule, Interaction, is the main computation rule. When using this
rule, we always apply an α-renaming to get a copy of the interaction rule with
all variables fresh. Indirection and Collect are administrative rules that we use
to obtain a more compact textual representation and to make explicit the active
pairs that may be created after applying the Interaction rule. The symbol ⇀
↽
above denotes an equivalence relation that states the irrelevance of the order
of equations in the multiset as well as the order of the members in an equation.

124
Chapter 7. Interaction-Based Models of Computation
The calculus makes evident the real cost of implementing an interaction
step, which involves generating an instance (i.e., a new copy) of the right-hand
side of the rule, plus renamings (rewirings). Of course this also has to be done
when working in the graphical framework, even though it is often seen as an
atomic step.
Example 7.14 (Natural numbers)
We show two diﬀerent encodings of natural numbers and addition using the
interaction calculus. The ﬁrst encoding is the standard one, and the second is
a more eﬃcient version that oﬀers a constant time addition operation.
1. Let Σ = {0, S, add} with ar(0) = 0, ar(S) = 1, ar(add) = 2, and R:
add(S(x), y)
⊲⊳
S(add(x, y))
add(x, x)
⊲⊳
0
As shown in Example 7.12, the net for 1+0 is given by the conﬁguration
(R, ⟨a | add(a, 0) = S(0)⟩). One possible sequence of reductions for this net
is the following:
⟨a | add(a, 0) = S(0)⟩
−→⟨a | a = S(x′), y′ = 0, 0 = add(x′, y′)⟩
−→∗⟨S(x′) | 0 = add(x′, 0)⟩
−→⟨S(x′) | x′′ = x′, x′′ = 0⟩
−→∗⟨S(0) |⟩
2. Let Σ = {S, N, N ∗}, ar(S) = 1, ar(N) = ar(N∗) = 2. Numbers are rep-
resented as a list of S agents, where N is a constructor holding a link to
the head and tail of the list. The number 0 is deﬁned by the conﬁgura-
tion ⟨N(x, x) |⟩, and in general n is represented by ⟨N(Sn(x), x) |⟩. The
operation of addition can then be encoded by the conﬁguration
⟨N(b, c), N ∗(a, b), N ∗(c, a)⟩
which simply appends two numbers. We only need one interaction rule
N(a, b) ⊲⊳N ∗(b, a)
which is clearly a constant time operation. To show how this works, we
give an example of the addition of 1+1:
⟨N(b, c) | N(S(x), x) = N ∗(a, b), N(S(y), y) = N ∗(c, a)⟩
−→∗⟨N(b, c) | b = S(a), a = S(c)⟩
−→∗⟨N(S(S(c)), c) |⟩

Chapter 7. Interaction-Based Models of Computation
125
The interaction calculus is a Turing-complete model of computation, and
therefore the halting problem (i.e., deciding whether a conﬁguration produces
an inﬁnite reduction sequence) is undecidable in general. The following example
shows that there are non-terminating conﬁgurations.
Example 7.15 (Non-termination)
Consider the net ⟨x, y | α(x) = β(α(y))⟩and the rule α(a) ⊲⊳β(β(α(a))). The
following non-terminating reduction sequence is possible:
⟨x, y | α(x) = β(α(y))⟩
−→
⟨x, y | x = a, β(α(a)) = α(y)⟩
−→
⟨a, y | β(α(a)) = α(y)⟩
−→
· · ·
There is an obvious question to ask about this language with respect to
the graphical formalism: Is it expressive enough to specify all interaction net
systems? Under some assumptions, the answer is yes. There are in fact two
restrictions. The ﬁrst one is that there is no way of writing a rule with an
active pair on the right-hand side. This is not a problem since it is possible
to show that the class of interaction net systems where interaction rules are
free of active pairs on the right-hand side has the same computation power
as the class of rules that may include active pairs on the right-hand side. The
second problem is the representation of interaction rules for active pairs without
interface. In the calculus, an active pair without interface can only rewrite to
the empty net. This is justiﬁed by the fact that disconnected nets can be ignored
in this model of computation (only global computation rules can distinguish
disconnected nets).
7.6.2 Properties of the calculus
This section is devoted to showing various properties of the reduction system
deﬁned by the rules Indirection, Interaction, Collect, and Multiset (see Def-
inition 7.13). We have already mentioned these properties for the graphical
formalism of interaction nets; they also hold for the calculus.
Proposition 7.16 (Conﬂuence)
The relation −→is strongly conﬂuent: If c −→d and c −→e, for two diﬀerent
conﬁgurations d and e, then there is a conﬁguration c′ such that d −→c′ and
e −→c′.

126
Chapter 7. Interaction-Based Models of Computation
We write c ⇓c′ if and only if c −→∗c′ ̸−→. In other words, c ⇓c′ if c′ is a
normal form of c. As an immediate consequence of the previous property, we
deduce that there is at most one normal form for each conﬁguration: c ⇓d and
c ⇓e implies d = e.
Although the calculus is non-terminating, as shown in Example 7.15, the
restriction to the “administrative” rules Indirection and Collect is indeed ter-
minating since applications of these rules reduce the number of equations in
a conﬁguration. Non-termination arises because of the Interaction rule (see
Example 7.15), as expected.
7.6.3 Normal forms and strategies
Although we have stressed the fact that systems of interaction are strongly
conﬂuent, there are clearly many ways of obtaining the normal form (if one
exists), and moreover there is scope for alternative kinds of normal forms, for
instance those induced by weak reduction.
It is easy to characterise conﬁgurations that are fully reduced — we will call
them full normal forms or simply normal forms. A conﬁguration (R, ⟨⃗t | Δ⟩) is
in full normal form if Δ is empty or all the equations in Δ have the form x = s
with x ∈s or x free in ⟨⃗t | Δ⟩.
We now deﬁne a weak notion of normal form, called the interface nor-
mal form, that is analogous to the notion of weak head normal form in the
λ-calculus. This is useful in the implementation of the λ-calculus and func-
tional programming languages to avoid non-terminating computations in dis-
connected nets.
Deﬁnition 7.17 (Interface normal form)
A conﬁguration (R, ⟨⃗t | Δ⟩) is in interface normal form (INF) if each ti in ⃗t is
of one of the following forms:
– α(⃗s). For example, ⟨S(x) | x = Z⟩.
– x, where x ∈N(tj), i ̸= j. This is called an open path. For example, ⟨x, x | Δ⟩.
– x, where x occurs in a cycle of principal ports in Δ. For example, the conﬁgu-
ration ⟨x | y = α(β(y), x), Δ⟩has a cycle of principal ports (see the diagrams
below).
Intuitively, an interaction net is in interface normal form when there are
agents with principal ports on all of the observable interface, or, if there are

Chapter 7. Interaction-Based Models of Computation
127
ports in the interface that are not principal, then they will never become prin-
cipal by reduction (since they are in an open path or a cycle).
The following diagrams illustrate the notion of an interface normal form.
The ﬁrst diagram, a subnet in the conﬁguration ⟨α(t1, . . . , tn) | Δ⟩, has an
agent α with a free principal port in the interface; the terms ti connected to
the auxiliary ports of α represent the rest of the net, and there may be active
pairs in this net if Δ is not empty. The second net contains an open path
(through the agent δ).
⟨α(t1, . . . , tn) | Δ⟩
⟨x, δ(x, 0) |⟩


α
6

@
t1
tn
· · ·


δ
6


0

@
@I
x
The following conﬁgurations are examples of nets with cycles of principal
ports.
⟨x | α(β(y), x) = y⟩
⟨x | δ(z, +(x, y)) = z, y = . . .⟩


α


β
6
6

x


δ


+

@
x
?
7.7 Extensions to model non-determinism
Interaction nets are a distributed model of computation in the sense that com-
putations in a net can take place in parallel at any point in the net (no syn-
chronisation is needed due to the strong conﬂuence property of reductions in
this model). However, interaction nets cannot model non-deterministic compu-
tations, which are a key ingredient of parallel programming languages.
To obtain an abstract model of computation capable of expressing non-
deterministic choice, several extensions of interaction nets have been proposed.
For instance, we could extend interaction net systems by
1. permitting the deﬁnition of several interaction rules for the same pair of

128
Chapter 7. Interaction-Based Models of Computation
agents, in which case one of the rules will then be chosen at random when
the two agents interact;
2. permitting edges that connect more than two ports; or
3. generalising the notion of an agent in order to permit interactions at several
ports (in other words, multiple principal ports are permitted in an agent).
The ﬁrst alternative is simple but not powerful enough to model a general
notion of non-determinism. The second and third alternatives are more pow-
erful. In fact, in the third case, it is suﬃcient to extend the interaction net
paradigm of computing with just one agent with two principal ports. This dis-
tinguished agent represents ambiguous choice and is usually called amb. It is
deﬁned by the following interaction rules.


α



@
· · ·


amb
@
m

a

@
R
b
=⇒


α
· · ·
m
6

@
b
a


α
· · ·

@
@I


amb
@

m
a


@
R
b
=⇒
b
a


α
· · ·
6
m

@
When an agent α has its principal port connected to a principal port of amb,
an interaction can take place and the agent α arrives at the main output port
of amb, which we called m in the diagram above. If in a net there are agents
with principal ports connected to both principal ports of amb, the choice of the
interaction rule to be applied is non-deterministic.
We illustrate the use of amb to program the Parallel-or function. This is
an interesting Boolean operation that applies to two Boolean expressions and
returns True if one of the arguments is True, independently of the computation
taking place in the other argument. In other words, if both arguments have
a Boolean value, this operation behaves exactly like an or, but even if one of
the arguments does not return a value, as long as the other one is True, the
Parallel-or function should return True. Since one of the arguments of Parallel-
or may involve a partially deﬁned Boolean function, the agent amb is crucial to
detect the presence of a value True in one of the arguments. Below we specify
this function using interaction nets extended with amb.
Example 7.18
The function Parallel-or must give a result True as soon as one of the arguments
is True, even if the other one is undeﬁned. Using an agent amb, we can easily
encode Parallel-or with the net

Chapter 7. Interaction-Based Models of Computation
129


or


@
@



amb


@
R
where the agent or represents the Boolean function or, deﬁned (in standard
interaction nets) by two interaction rules:


true




or
m

@
x
=⇒


true
m
6


ǫ
x ?


false




or
m

@
x
=⇒
x
m
The model of computation based on interaction nets extended with amb is
strictly more powerful than the interaction net model in the sense that it allows
us to deﬁne non-deterministic computations or non-sequential functions, such
as Parallel-or.
In order to deﬁne parallel processes explicitly and facilitate the analysis
of the behaviour of concurrent systems, in the next chapter we will present a
formalism based on a notion of communication between processes.
7.8 Further reading
Yves Lafont’s article [28] provides an introduction to interaction nets and many
examples of their use. For more information on interaction combinators, and
a proof of universality, we refer the reader to [29]. We refer to [42] for imple-
mentations of interaction nets. The compact textual notation for interaction
nets described in this chapter was suggested by Lafont in his introductory ar-
ticle [28]; the calculus based on this notation is developed in [16]. We also refer
the reader to [16] for more notions of normal forms and strategies of evaluation.

130
Chapter 7. Interaction-Based Models of Computation
7.9 Exercises
1. Using interaction nets, deﬁne the following functions on numbers repre-
sented with 0 and S (successor):
– is-zero, which produces a result True if the number is 0 and False other-
wise;
– min, which computes the minimum of two numbers;
– factorial, which computes the factorial of a number.
2. Specify an interaction system that generates inﬁnite computations (loops).
3. Complete the deﬁnition of the interaction system for combinatory logic
given in Section 7.3. More precisely, deﬁne the agents and rules needed to
deﬁne the S combinator (it can be deﬁned with three agents and three
rules).
4.
a) Give an interaction system to compute the Boolean function and.
b) Draw the interaction net representing the expression
(True and False) and True
How many reductions are needed to fully normalise this net?
c) Modify the system so that the result is True if and only if both argu-
ments have the same value (i.e., both True or both False).
5. Give a representation of lists in interaction nets, and use it to implement
a function that interleaves the elements of two given lists. More precisely,
deﬁne an interaction system that, given two lists l1 and l2, produces a new
list containing the elements of l1 interleaved with those of l2. For instance,
the result of interleaving [0, 2, 4] and [1, 3] is the list [0, 1, 2, 3, 4].
6. Textual rules deﬁning addition were given in Example 7.14. Can you write
the textual version of the rules for multiplication given in Section 7.2?
7. Explain why interaction nets are not suitable as a model for non-
deterministic computations.
8. Deﬁne the function Parallel-and using the agent amb. Parallel-and is a
binary Boolean operator returning the value False whenever one of the
arguments is False and True when both are True.

8
Concurrency
In the previous chapters, we described several models of computation that
reﬂect diﬀerent ways in which the process of computation can be understood.
All these diﬀerent abstract models of computation share one characteristic: The
goal is to express sequential algorithms. To describe the meaning of a sequential
program, we can use an operational approach in which we see an algorithm as a
black box transforming some given input data into the desired output. However,
in some contexts, for example when describing the behaviour of an operating
system, this input-output abstraction is not well suited. The ﬁnal result of the
algorithm might not be of interest, or the notion of “ﬁnal” might not even
apply. Indeed, an operating system does stop running in some cases, typically
when we shut down our computer, but then we are not expecting a “result”
from the computation.
Concurrent systems of computation diﬀer from sequential ones in three main
aspects:
– Non-termination:
Although sequential programs that do not terminate are usually uninter-
esting, in the concurrent case most interesting systems are actually non-
terminating. In this context, we need a more general notion of algorithm
that associates a computational meaning also to programs that do not stop.
– Non-determinism:
Sequential algorithms are usually deterministic, and each execution of the
same program with the same data in the same abstract machine produces

132
Chapter 8. Concurrency
the same results. However, in some cases non-determinism is useful (most
programming languages, even sequential ones, allow programmers to simulate
non-determinism by using, for instance, a random number generator).
– Interference:
In a concurrent system, the meaning of a program may depend on the be-
haviour of the other programs that are being executed concurrently, unlike
in sequential systems, where the meaning of a program is determined by the
program itself and the abstract machine for which it was written. For in-
stance, if several programs are running in parallel and they are all trying to
read and write the same record in a database, then, in order to guarantee
the consistency of the database, access to records must be controlled, as the
following example of interference shows.
Example 8.1
Suppose that a university stores the contact details of students in a ﬁle
containing a record for each student. Suppose the ﬁelds in each record include
the name and the address of the student, and we have a record containing
Name: "Claire", Address: "Belgravia"
Consider two processes, P1 and P2, running concurrently and executing the
following operations on Claire’s record, with the aim of adding more details
to the address (we use the symbol + to denote string concatenation).
Process P1:
Address := "Belgravia, London";
Print record
Process P2:
Address := Address + ", London";
Print record
Seen as sequential programs, P1 and P2 can be considered equivalent: they
both replace the contents of the address ﬁeld in this record by the string
“Belgravia, London”
and print it.

Chapter 8. Concurrency
133
However, in a concurrent system, if these processes are running in parallel,
the ﬁnal result depends on the order in which the instructions are executed.
We may have some unexpected results if, for instance, P1’s ﬁrst instruction
is followed by P2’s ﬁrst instruction. In this case, the execution of a printing
instruction for this record will show
Claire, Belgravia, London, London
which is not the intended result.
Concurrent programs should be carefully written to avoid interference.
Concurrent systems share a number of general characteristics, including the
following notions:
– Process: Any entity that describes computation, or that is capable of per-
forming computations, is usually called a process; the words agent, compo-
nent, or thread are sometimes used instead of the word process.
– Communication: Processes that are running in parallel can exchange data, or
information in general, by sending and receiving messages. In some cases, the
communication links are ﬁxed, whereas in other cases there is some degree of
ﬂexibility and the system can create new communication channels or change
the ones available.
– Interaction: As a result of the fact that processes are running concurrently,
their collective behaviour may depend on each other’s individual actions.
Processes can interact, either in a positive way to achieve a common goal or in
an unintended way as in the case of the interference described in Example 8.1.
There is another important aspect of concurrent systems: their observable
behaviour. This replaces the notion of “result” associated with a sequential
algorithm. Since in many cases there is no result associated with a concurrent
program, the important feature of such a system is its behaviour whilst running.
In other words, we should be able to observe changes during the execution of
the concurrent program, and we can compare concurrent systems by observing
their behaviour and comparing the observations.
Some examples may help to illustrate these ideas. We can identify the no-
tions described above in the concurrent systems that we encounter in everyday
situations. For example, the following are concurrent systems in the sense de-
scribed above:
– A vending machine for drinks and a person using the machine can be seen
as a concurrent system. Although in this chapter we will be focusing on
computer systems rather than physical systems, it is still interesting to see

134
Chapter 8. Concurrency
that the same notions apply. We can see the vending machine and the user
as two processes that communicate (albeit in a simple, not very ﬂexible way)
and interact in order to attain a common goal. The behaviour of the system
is easy to observe: Lights indicate whether diﬀerent drinks are available,
buttons can be used to select drinks, the machine accepts coins, the machine
delivers a drink, etc. And, of course, if two vending machines for drinks are
available, our choice is likely to be based on our observations.
– The World Wide Web is a good example of a concurrent system where the
notion of communication is very ﬂexible. New communication links can be
created, and existing links can be removed.
– Another example of a concurrent system where the communication channels
are not ﬁxed is an airport. More precisely, an airport’s control tower and
the collection of aircraft that at any time are under the control of the tower
can be seen as a concurrent system. In this case, an aircraft might establish
a communication link with the tower in order to land at the airport, and
after the landing has taken place, the channel may be destroyed. Interac-
tion between diﬀerent aircraft is possible, although not always expected, and
interaction between the tower and the aircraft is of course expected.
8.1 Specifying concurrent systems
In the previous section, we identiﬁed the main features that distinguish a con-
current system from a sequential one. In order to specify a concurrent system,
we need a formalism that allows us to deﬁne these diﬀerent features. In partic-
ular, since in the case of a concurrent system we are interested in the behaviour
of the system as opposed to its ﬁnal outputs, we need a formalism that allows
us to specify behavioural aspects of the system.
Transition diagrams are one of the tools used for the description of pro-
cesses in concurrent systems. We can see these diagrams as a particular kind of
automaton, where the transitions describe the possible actions of the machine.
There is also a textual view of these diagrams: In Chapter 2, we associated a
formal language with a ﬁnite automaton; similarly, it is possible to associate
an algebra of expressions with a transition diagram.
Speciﬁcally, in this chapter we will use labelled transition systems to specify
concurrent systems. These are graphs where nodes represent the state of the
system and edges correspond to transitions between states, labelled by actions.
Before giving the formal deﬁnition, we present some examples to illustrate the
idea.

Chapter 8. Concurrency
135
Example 8.2
Consider a simple version of a vending machine that can deliver coﬀee or tea.
Assume that, after introducing a coin, the machine allows us to select the drink
by pushing the tea or the coﬀee button. The machine will then produce the
required drink and deliver it, after which it is again ready to sell another drink.
The behaviour of this machine can be speciﬁed using a labelled transition
diagram, as depicted in Figure 8.1.
coin
tea
coffee
select tea
select coffee
1
2
3
4
Figure 8.1
Labelled transition diagram for a vending machine.
The diagram in Figure 8.1 is similar to the diagrams used in Chapter 2 to
recognise regular languages, and we have used the same notation to indicate
the initial state (a small arrow). However, in the case of ﬁnite automata, the goal
is to describe a formal language, whereas now we are describing the actions the
machine can do at each state. In this sense, the properties of labelled transition
diagrams are diﬀerent from the properties of ﬁnite automata. For instance, the
diagram in Figure 8.2, seen as a non-deterministic ﬁnite automaton, deﬁnes
the same language as the automaton in Figure 8.1, but in fact, as a description
of a vending machine, it speciﬁes a behaviour that is very diﬀerent from the
previous one. When in the initial state the user inserts a coin, the machine
will move in a non-deterministic way either to a state in which it can produce
a coﬀee or to a state in which it can produce a tea. In other words, whereas
in the machine speciﬁed in Figure 8.1 it is the user who chooses the drink, in
the second machine the choice is done internally. After we insert a coin, all we
know is that a drink will be delivered, but it could be either a coﬀee or a tea
and there is no way to know in advance which one the machine will produce!

136
Chapter 8. Concurrency
tea
coffee
select coffee
select tea
coin
coin
1′
2′
3′
4′
5′
Figure 8.2
Labelled transition diagram for a non-deterministic vending ma-
chine.
The question then naturally arises as to when two such systems are equiv-
alent. The fact that processes of interest may be non-terminating or non-
deterministic rules out a notion of equivalence based on the results obtained, as
is standard for sequential programs, where two functions f and g are equivalent
if they produce the same output for each given input, formally
f = g ⇐⇒∀x.f(x) = g(x).
Also, as the previous example shows, it is not useful to compare the automata
by comparing their associated languages. Moreover, it is easy to see that the
fact that two systems are deﬁned by diagrams with a similar shape does not
guarantee that they have the same behaviour. What then would be a reasonable
notion of equivalence?
Indeed, we should not forget that the speciﬁcation of a concurrent system
is mainly a description of its behaviour. It is then natural to say that two
systems are equivalent if they have the same behaviour. We mentioned in the
previous section that one of the main characteristics of this notion of behaviour
is that it is observable: We are interested in the observable behaviour of the
system. To give a concrete example, we could say that two vending machines
that oﬀer the same drinks at the same price (i.e., two machines for which all
the relevant observations coincide) are equivalent, even if internally they are
built in diﬀerent ways.
Below we will deﬁne this notion of behavioural equivalence formally using
a bisimilarity relation, but in order to do that, we need a formal deﬁnition of
labelled transition systems.

Chapter 8. Concurrency
137
Let Act be an alphabet; i.e., a denumerable (ﬁnite or inﬁnite) set of symbols,
called labels. The alphabets we will use in our examples are composed of two
kinds of labels, called actions and co-actions:
Act = N ∪N
For example, in the case of the vending machine, the set of actions is
N = {coin, coﬀee, tea, . . .}
and the co-actions are
N = {coin, coﬀee, tea, . . .}
Actions and co-actions represent two complementary views of an interaction
between two processes. This will be useful in systems composed of several
processes that need to work in a synchronised way.
We are now ready to deﬁne labelled transition systems.
Deﬁnition 8.3 (Labelled transition system)
A labelled transition system, with labels in Act, is a pair (Q, T), where
– Q is a set of states and
– T is a transition relation; that is, a ternary relation between a state, a label,
and another state: T ⊆(Q × Act × Q).
We write q −→a q′ if (q, a, q′) ∈T and say that in the state q the process
can perform the action a and move to state q′.
Each process in a concurrent system will be speciﬁed as a labelled transition
system. It is not necessary to deﬁne an initial state, and in general there are
no ﬁnal states in labelled transition systems. Indeed, in many cases there is
no distinguished starting state, every state can be considered as an initial one,
and every state can be a ﬁnal state.
We can now go back to the problem of deﬁning process equivalence.
8.2 Simulation and bisimulation
Given two labelled transition systems, we start by deﬁning a relation between
their states. In fact, it does not matter whether the states belong to the same

138
Chapter 8. Concurrency
automaton or to diﬀerent ones (in any case, an automaton can be composed of
several disconnected parts).
Intuitively, we would like to say that two states are equivalent if their ob-
servations coincide. More precisely, two states are equivalent if whenever an
action is possible in one of them it is also possible in the other, and after this
action takes place the resulting states are also equivalent. Formally, we deﬁne
the notion of strong simulation as follows.
Deﬁnition 8.4 (Strong simulation)
Let (Q, T) be a labelled transition system on Act. A binary relation S on Q is
a strong simulation if pSq implies that, for each a in Act such that p −→a p′,
there exists q′ in Q such that q −→a q′ and p′Sq′.
If pSq holds, we say that q simulates p.
The idea is that if pSq holds, any transition that can be done from the state
p can also be done from q, and the resulting states are still in the relation.
Example 8.5
Consider the diagrams in Figures 8.1 and 8.2. The relation
S = {(1′, 1), (2′, 2), (3′, 2), (4′, 3), (5′, 4)}
is a strong simulation. We can check that, for each pair (p, q) ∈S and for
each action a such that p −→a p′, there is a transition q −→a q′ such that
(p′, q′) ∈S. For instance, take (3′, 2). There is only one possible action at state
3′ in Figure 8.2, namely select coﬀee, with a transition 3′ −→select coﬀee 5′.
Similarly, in Figure 8.1, there is a transition 2 −→select coﬀee 4 and the pair
(5′, 4) is in S as required.
The fact that we can build a strong simulation as above indicates that the
deterministic vending machine can simulate the non-deterministic one. The re-
verse is not true: The non-deterministic machine cannot simulate the original
one. For this, it is suﬃcient to prove that there is no strong simulation contain-
ing the pair (2, i′) for any state i′ in the non-deterministic machine. In other
words, the behaviour of the original machine in state 2 is observably diﬀer-
ent from the behaviour of the non-deterministic machine, whichever state we
consider.
The strong simulation relation deﬁned above allows us to compare processes,
but it does not deﬁne an equivalence relation. To obtain an equivalence relation,

Chapter 8. Concurrency
139
we need simulations in both directions; this is the essence of the notion of strong
bisimulation.
Deﬁnition 8.6 (Strong bisimulation)
Let (Q, T) be a labelled transition system on Act and S a binary relation on
Q. We say that S is a strong bisimulation if S and S−1 are strong simulations,
where S−1 denotes the inverse of S (i.e., pS−1q if qSp).
We will write p ∼q if there is a strong bisimulation S such that (p, q) ∈S,
and in this case we will say that p and q are bisimilar. The relation ∼is called
strong bisimilarity.
According to the deﬁnition above, ∼is the union of all the strong bisimula-
tions — it contains all the pairs (p, q) such that pSq for some strong bisimulation
S.
The strong bisimilarity relation deﬁned above is also a strong bisimulation,
and it is an equivalence relation (indeed, it is the equivalence relation we were
looking for).
Proposition 8.7
1. The relation ∼is reﬂexive, symmetric, and transitive:
For all p, p ∼p.
For all p, q, if p ∼q, then q ∼p.
For all p, q, r, if p ∼q and q ∼r, then p ∼r.
2. The relation ∼is a strong bisimulation.
In the rest of the chapter, we deﬁne a simple programming language that
can be used to deﬁne individual processes (Section 8.3) and then show how
this language can be extended to model process communication and interaction
(Section 8.4). We brieﬂy describe an alternative view of concurrency, based on
the chemical metaphor, in Section 8.5.
8.3 A language to write processes
We can associate a process expression with a labelled transition diagram. A
language of expressions will be useful to program concurrent systems where each

140
Chapter 8. Concurrency
component can be seen as a process (i.e., an expression) and several processes
can be combined via suitable operators.
Consider a set of identiﬁers that will be used as names for processes
Id = {A, B, C, . . .}
and a set of labels
Act = N ∪N = {a, b, c, . . .} ∪{a, b, c, . . .}
We will also assume that there is a denumerable set of variables x1, . . . , xn, . . ..
Deﬁnition 8.8 (Process expression)
Process expressions are generated by the grammar
P ::= A⟨α1, . . . , αn⟩| Σi∈I αi.Pi
where the symbols αi range over variables or labels, A is a process identiﬁer, I
is a ﬁnite set of indices, and in αi.Pi we say that αi is a preﬁx.
The two kinds of process expressions generated by the grammar above are
called named processes and sums, respectively. A named process consists of a
process identiﬁer and a list of parameters, written A⟨α1, . . . , αn⟩, and a sum is a
ﬁnite set of expressions of the form αi.Pi, where each Pi is a process expression.
In the latter case, if I = ∅, then Σi∈Iαi.Pi is written 0, representing the inert
process that does not perform any computation.
To each process identiﬁer A we will associate an expression, using an equa-
tion that we call the deﬁnition of A,
A(x1, . . . , xn) = PA
where PA is a sum that can use the variables x1, . . . , xn.
The intuition behind a deﬁnition such as the one above is that each
time the process A is used in a program (that is, each time an expression
of the form A⟨a1, . . . , an⟩occurs), it can be replaced by the expression PA,
where each occurrence of xi is replaced by ai. The latter will be written
PA{x1 →a1, . . . , xn →an}, where the expression {x1 →a1, . . . , xn →an}
is a substitution.
For this reason, in the context of an equation A(x1, . . . , xn) = PA, the
expression A⟨a1, . . . , an⟩is equivalent to PA{x1 →a1, . . . , xn →an}, and, more
generally, two expressions P and Q are considered equivalent, written P ≡Q,
if the equality P = Q can be derived in the equational theory generated by the
equations.

Chapter 8. Concurrency
141
For example, in the context of the equation
A(x) = x.A⟨x⟩
we can derive the following equivalences:
A⟨a⟩≡a.A⟨a⟩≡a.a.A⟨a⟩
We now have all the ingredients to deﬁne processes.
Deﬁnition 8.9 (Process)
A process is deﬁned by a process expression (see Deﬁnition 8.8) together with
the equations that deﬁne the process identiﬁers occurring in the expression.
To each process we can associate a labelled transition system (Q, T) on Act
as follows:
– The set Q of states corresponds to the set of (sub)expressions in the process.
– There is a transition Σi∈I αi.Pi −→αj Pj for each j ∈I.
The example below illustrates the idea.
Example 8.10 (Buﬀer)
Consider a buﬀer of size two that we see as a container where we can store two
items. The actions associated with the buﬀer are only of two kinds. We can
either put an item in the buﬀer if there is space for it or take an item out if
the buﬀer is not empty. In the latter case, if the buﬀer is full, we assume that
any one of the items stored will be removed.
In this case, we can model the buﬀer using an alphabet that contains the
actions
N = {in, out}
We now have to deﬁne the associated process. For this, we will ﬁrst use
a labelled transition system to specify the behaviour of the buﬀer. The set of
states should include the empty buﬀer, the buﬀer that contains one value, and
a full buﬀer. Let us call the states B0 (empty buﬀer), B1 (buﬀer containing one
item), and B2 (full buﬀer).
We have the following transitions at each state:
B0 −→in B1
B1 −→in B2, B1 −→out B0
B2 −→out B1

142
Chapter 8. Concurrency
We can then associate the following deﬁnitions with the identiﬁers B0, B1,
and B2:
B0(in)
=
in.B1⟨in, out⟩
B1(in, out)
=
out.B0⟨in⟩+ in.B2⟨out⟩
B2(out)
=
out.B1⟨in, out⟩
The system is initialised by deﬁning the process Buﬀer = B0⟨in⟩.
8.4 A language for communicating processes
Using the language of expressions deﬁned in the previous section, we can specify
the behaviour of an individual process. In order to specify a system of concur-
rent processes, we need to extend the language, so that communication and
interaction between processes can be deﬁned. With this aim, we introduce two
new syntactic constructions:
– Parallel composition, written P1|P2, allows us to specify two processes, P1
and P2, and combine them by executing them in parallel. The binary operator
of parallel composition, denoted by |, is associative and commutative, so we
can compose several processes simply by writing P1|P2|P3| . . ..
– The restriction operator allows us to encapsulate a name to avoid name
clashes when several individual processes that could use the same alpha-
bet are combined together. We write νa.P to indicate that the name a is
private in P and will be distinguished from any other a used in the pro-
cesses composed with P. This is easy to achieve by considering ν as a binder
and deﬁning process expressions as equivalence classes modulo renamings of
bound names (for instance, in the same way as λ is a binder and λ-terms
are deﬁned modulo α-equivalence). We will sometimes abbreviate νa.νb.P as
νab.P.
Two processes that are running in parallel can interact by performing an
action and the associated co-action. For instance, in the example of the vending
machine, if we consider the machine as a process and a user as another one,
interaction can take place by an action performed by the user (e.g., introducing
a coin) with the associated co-action (accepting the coin) performed by the
machine. As mentioned previously, actions and co-actions are just two diﬀerent
views of the interaction, so it makes sense to synchronise parallel processes in
this way. More precisely, if P and Q are processes running in parallel, where P
performs a transition labelled by a and Q performs a transition labelled by a,
we will say that P and Q have interacted and as a result the system as a whole
has changed state.

Chapter 8. Concurrency
143
This interaction can be speciﬁed operationally
If P −→a P ′ and Q −→a Q′ then P|Q −→τ P ′|Q′
which states that the system formed by the concurrent processes P and Q
has changed state after the processes have performed complementary actions.
Notice that the transition out of P|Q has a special label, τ. Transitions labelled
by τ are called τ-transitions or silent transitions because if we consider the
system as a black box, we cannot observe the individual actions taking place
in P or Q. In other words, in a τ-transition there is no interaction with the
environment, the interaction is internal.
To take into account τ-transitions, we extend the set Act used in the deﬁ-
nition of individual processes by adding a distinguished label τ.
Summarising, the set of expressions in the extended language includes the
process expressions deﬁned previously (in Deﬁnition 8.8) and all the expressions
that can be obtained by parallel composition and restriction as indicated below.
Deﬁnition 8.11 (Extended process language)
The syntax of the expressions in the concurrent process language is deﬁned by
the grammar
P ::= A⟨α1, . . . , αn⟩| Σi∈Iαi.Pi | P1|P2 | νa.P
where the αis range over variables or labels from the alphabet Act:
Act = N ∪N ∪{τ} and N = {a, b, c, . . .}
We work modulo a congruence ≡on expressions (i.e., an equivalence relation
closed by context) generated by
– renaming (i.e., α-conversion) of restricted names;
– commutativity and associativity of the sum;
– commutativity and associativity of parallel composition;
– neutral element: P|0 ≡P;
– νa.(P|Q) ≡P|νa.Q if a is not free in P;
– νa.0 ≡0, νab.P ≡νba.P;
– A⟨b1, . . . , bn⟩≡PA{a1 →b1, . . . , an →bn} if A(a1, . . . , an) = PA.
The transition relation extends in the natural way to expressions that use
parallel composition and restriction operators. We deﬁne below the extended
relation, which we still call T since there is no ambiguity. In the deﬁnition of
transitions, we use α to denote any label in Act = N ∪N ∪{τ}.

144
Chapter 8. Concurrency
Deﬁnition 8.12 (Extended transition relation)
The transition relation is generated by the following rules:
(Sum)
M + α.P
−→α
P
(Def)
A⟨a1, . . . , an⟩
−→α
P ′
if A(x1, . . . , xn) = PA and
PA{x1 →a1, . . . , xn →an} −→α P ′
(Reaction)
P|Q
−→τ
P ′|Q′
if P −→α P ′ and Q −→α Q′
where α ̸= τ
(Par)
P|Q
−→α
P ′|Q
if P −→α P ′
(Restr)
νa.P
−→α
νa.P ′
if P −→α P ′ and α ̸= a, α ̸= a
Note that, since we are working on equivalence classes, if for a given expres-
sion P there is a transition P −→α P ′ according to the deﬁnition above, then
also Q −→α Q′ for any Q and Q′ such that P ≡Q and P ′ ≡Q′. In particular,
if Q −→a Q′, then P|Q −→a P|Q′ using the rule (Par).
We give an example below.
Example 8.13
Let P be the expression
νb.((a.b.P1 + b.P2 + c.0) | a.0) | (b.P3 + a.P4)
Using (Sum), (a.b.P1+b.P2+c.0) −→a b.P1, and also a.0 −→a 0. Therefore,
using (Reaction), we deduce
((a.b.P1 + b.P2 + c.0) | a.0) −→τ b.P1 | 0
Hence P −→τ νb.(b.P1|0) | (b.P3 + a.P4) using (Restr) and (Par), and the
latter expression is congruent with νb.(b.P1) | (b.P3 + a.P4), so
P −→τ νb.(b.P1) | (b.P3 + a.P4)
Now, although b and b are preﬁxes of parallel processes in this expression, the
reaction cannot take place because b is a private name in one of the components;
we could have written equivalently
P −→τ νc.(c.P1) | (b.P3 + a.P4)
Note that the transition relation is not conﬂuent: For a given expression,
there may be several diﬀerent irreducible forms. For instance, in the example
above, the initial expression P can be written as
νd.((a.d.P1 + d.P2 + c.0) | a.0) | (b.P3 + a.P4)

Chapter 8. Concurrency
145
which is congruent with
νd.((a.d.P1 + d.P2 + c.0) | (b.P3 + a.P4) | a.0)
and we have a transition P −→τ νd.(d.P1|P4|a.0).
Indeed, when several τ-transitions are possible, there is no pre-established
order for them; this non-deterministic aspect of the transition relation is one
of the features of concurrent systems.
Since the extended transition relation includes τ-transitions, which cannot
be observed, the deﬁnition of bisimulation is extended to ignore silent transi-
tions.
Let P ⇒α Q, where α ̸= τ, denote a sequence of transitions from P to Q con-
taining any number of τ steps and at least one α-transition. More precisely, we
write P ⇒α Q if there is a sequence of transitions P−→τ ∗P ′ −→α Q′−→τ ∗Q.
Bisimulation is deﬁned as in Deﬁnition 8.6 but using ⇒instead of −→; in this
way, bisimilarity equates processes that have the same behaviour when we do
not consider τ transitions.
The language of concurrent process expressions satisﬁes the following prop-
erties.
Proposition 8.14
– For each expression P, there are only a ﬁnite number of transitions P −→α P ′
available (i.e., the transition diagram is ﬁnitely branching).
– The structural congruence ≡is a strong bisimulation and thus is included in
the bisimilarity relation; that is, P ≡Q implies P ∼Q.
We ﬁnish this section with the speciﬁcation of a bidirectional channel using
the language of concurrent process expressions deﬁned above.
Example 8.15 (Bidirectional channel)
A channel can be seen as a buﬀer where the sender deposits a message on one
end and the receiver retrieves the message from the other end. In a bidirectional
channel, both ends can be senders or receivers.
We start by deﬁning a bidirectional channel of size 1. In this case, the
maximum number of messages in the channel at any time is 1. Let us call the
channel C and assume the points connected by the channel are called A and
B, as depicted in Figure 8.3.
In the process calculus, we will represent the channel C as a process and
will denote by a the action of sending a message from A and by a′ the action

146
Chapter 8. Concurrency
a
a′
b
b′
A
C
B
Figure 8.3
Bidirectional channel.
of receiving a message at A (similarly, b and b′ denote sending and receiving at
B). The process C can be deﬁned by the equation
C(a, a′, b, b′) = a.b.C + b′.a′.C
The transition diagram is given in Figure 8.4. It is easy to see in the diagram
that the following sequence of transitions is permitted, allowing messages to
pass from left to right through the channel:
C −→a b.C −→b C
Similarly, messages can be sent from right to left.
a
a′
b
b′
b.C
C
a′.C
Figure 8.4
Transition diagram for a bidirectional channel.
We can compose several channels; for instance, let D be another bidirec-
tional channel of size 1, connecting B with E. Assume D is deﬁned by the
equation
D(b, b′, e, e′) = C⟨b, b′, e, e′⟩
We can now deﬁne the channel CD connecting A with E by composing C and
D using the expression
CD(a, a′, e, e′) = νbb′.(C⟨a, a′, b, b′⟩|D⟨b, b′, e, e′⟩)
The channels C and D are now “joined” at B. The fact that B is no longer an
“open end” is represented by the restriction, which makes the names b and b′
private to C and D.

Chapter 8. Concurrency
147
8.5 Another view of concurrency:
The chemical metaphor
Around ﬁfteen years ago, Jean-Pierre Banˆatre and Daniel Le M´etayer intro-
duced the Γ language, which models computation as the global evolution of
a collection of values interacting freely. This idea can be explained intuitively
through the chemical reaction metaphor:
– Programs work on a data structure deﬁned as a multiset of atomic values that
can be thought of as a chemical solution. The values are molecules “ﬂoating”
freely in the solution.
– Programs specify chemical reactions through conditional reaction rules.
For example, to deﬁne the function that computes the maximum of a set
of numbers, we can consider the numbers to be molecules in the solution, and
the reaction is speciﬁed by the rule
(max)
x, y →x
if x ≥y
This rule indicates that a reaction can take place between the molecules x and
y, provided x ≥y, and as a result x and y are replaced by x.
In the solution, reactions can take place in parallel, provided the side con-
dition of the reaction rule is satisﬁed. Since reactions can occur simultaneously
in many points of the solution, this formalism allows us to model concurrent
computations in a simple and concise way. For example, after repeated, possi-
bly concurrent, applications of the rule (max) given above, there will be only
one molecule in the solution, which is the maximum of the numbers originally
in the solution. Note how compact this program is (just one line of code).
The Chemical abstract machine is an implementation of this paradigm of
concurrency; it deﬁnes a concurrent programming methodology that is free
from control management in the sense that the concurrent components (i.e.,
the molecules in this case) are freely moving in the solution and can commu-
nicate whenever they come in contact. It is possible to show that a calculus
of concurrent communicating processes such as the one deﬁned in the previous
sections can be implemented via the Chemical abstract machine.
8.6 Further reading
The process calculus deﬁned in this chapter is based on Robin Milner’s CCS
(a calculus of communicating systems) [33]. Several other calculi are available

148
Chapter 8. Concurrency
to specify and reason about concurrent processes; the references are numerous,
but we mention in particular the π-calculus, developed also by Robin Milner
as a direct generalisation of CCS that permits more ﬂexible communication
patterns [34]. For a detailed account of the theory of bisimulation, we refer to
David Park’s work [40]. An advanced treatment of the π-calculus theory can
be found in [45]. For more information on the Γ formalism and the Chemical
abstract machine, we recommend the articles [3, 5].
8.7 Exercises
1. Prove that the relation ∼introduced in Deﬁnition 8.6 is an equivalence
relation, as stated in Proposition 8.7.
2. Prove that if p ∼q, then p simulates q and q simulates p.
The reverse is not true. Can you give a counterexample?
3. Show that the following deﬁnition is equivalent to the deﬁnition of strong
bisimulation (see Deﬁnition 8.6):
R is a strong bisimulation if pRq implies
a) for all n and for all a1, . . . , an ∈Act,
p −→a1,...,an p′ ⇒∃q′ | q −→a1,...,an q′ and p′Rq′
b) for all n and for all a1, . . . , an ∈Act,
q −→a1,...,an q′ ⇒∃p′ | p −→a1,...,an p′ and p′Rq′
where p −→a1,...,an p′ denotes a sequence of transitions from p to p′ labelled
by the actions a1, . . . , an.
4. Consider a counter deﬁned as a device that can hold a natural number,
increment its value, or decrement it, but if the value of the counter is zero,
decrementing it does not change the value of the counter. Write a process
expression deﬁning such a counter.
5. In order to prove that P ≡Q implies P ∼Q as stated in the second part
of Proposition 8.14, it is suﬃcient to show that the structural congruence
≡is a strong bisimulation. Can you prove this fact?
6. Let P be the process deﬁned by the expression ν d e f .(K1|K2|K3), where
K1
=
f.a.d.K1
K2
=
d.b.e.K2
K3
=
f.e.c.K3

Chapter 8. Concurrency
149
and let H be the process deﬁned by the equation
H
=
a.b.c.H
a) Give labelled transition systems for P and for H.
b) Show that P ∼H.
7. Consider the process deﬁned by D(b, b′, e, e′) = C⟨b, b′, e, e′⟩, where C is
the bidirectional channel deﬁned in Example 8.15. Let the process CD be
deﬁned by
CD(a, a′, e, e′) = νbb′.(C⟨a, a′, b, b′⟩|D⟨b, b′, e, e′⟩)
Describe the transition diagram for CD, and show that CD can transmit
messages like a bidirectional channel but can also be in deadlock.

9
Emergent Models of Computation
In this chapter, we brieﬂy present two ﬁelds that have emerged in recent years:
natural computing and quantum computing.
Natural computing refers to computational techniques inspired in part by
systems occurring in nature. In particular, this includes models of computation
that take inspiration from the mechanisms that take place in living organisms.
The main observation here is that living organisms routinely perform complex
processes at the micro-level in a way that is hard to emulate with standard
computing technology. Several new models of computation have been proposed
in recent years based on advances in biology that have allowed us to understand
better how various processes take place. This family of computation models is
generally known as bio-computing.
Quantum computing refers to computation that uses quantum technology.
One of the motivations for the study of quantum computing stems from the
current trend to miniaturise computers. It has been observed that if this trend
continues, it will be necessary to replace the current technology with quantum
technology, as on the atomic scale matter obeys the rules of quantum mechanics,
which are very diﬀerent from the classical rules. Indeed, quantum technology
is already available (albeit not yet for general computer science applications),
and it can oﬀer much more than compact and fast computers: It can give rise
to a new kind of programming paradigm based on quantum principles.
The following sections give a short introduction to these two computing
paradigms.

152
Chapter 9. Emergent Models of Computation
9.1 Bio-computing
Biologically inspired models of computation make use of natural processes oc-
curring in living organisms as a basis for the development of programming
techniques. Since complex algorithms are eﬃciently performed at diﬀerent lev-
els in a living organism (in particular, at the cell level, at the gene level, and at
the protein level), the idea is to develop algorithms to solve complex computa-
tional problems by applying similar techniques. This research problem should
not be confused with the problem of developing software to simulate behaviours
or processes that occur in nature (sometimes called “executable biology”). Al-
though biological modelling is one of the applications of bio-computing, soft-
ware tools to represent and analyse biological processes have been written in a
variety of programming languages with diﬀerent underlying computation mod-
els. Bio-computing, in the sense deﬁned here, should also not be confused with
the branch of computer science that studies the use of biological materials in
hardware components.
The three main levels at which biological mechanisms occur correspond
to biochemical networks, performing all the mechanical and metabolic tasks,
formalised as protein interaction; gene regulatory networks, which control the
activities by regulating the concentrations of proteins; and transport networks,
deﬁned in terms of biochemical compartments (or membranes) where protein
interactions are conﬁned. Accordingly, biologically inspired models of compu-
tation can be classiﬁed by taking into account the level at which they work.
Without going into the details, we will describe here the main features of
two models of computation: membrane systems, introduced by Gheorghe Paun,
which take inspiration from the transport networks, and the protein-centric
interaction systems deﬁned by Vincent Danos and Cosimo Laneve. These two
computation models are representative of the two main classes of bio-computing
formalisms, but we should point out that several other calculi have been pro-
posed; this is an active research area and there is no “standard” model.
9.1.1 Membrane calculi
Membrane systems are a class of distributed, parallel computing devices in-
spired by the basic features of biological membranes, which are the essence of
the transport networks. Membranes play a fundamental role in the complex
reactions that take place in living organisms.
A membrane structure can be seen as a series of compartments where mul-
tisets of objects can be placed. Membranes are permeable, so objects can pass
through a membrane and move between compartments. The membranes can

Chapter 9. Emergent Models of Computation
153
change their permeability, and they can dissolve or divide, thus changing the ge-
ometrical conﬁguration of the membrane structure. This deﬁnition is based on
the observation that any biological system is a complex hierarchical structure
where the ﬂow of materials and information is essential to its function.
In membrane calculi, the objects inside the membranes evolve in accordance
with reaction rules associated with the compartments. Reaction rules can be
applied in a parallel, non-deterministic manner. In this way, computation can
be deﬁned as sequences of transitions between conﬁgurations of the system.
Programs in this formalism are called P systems. The objects (which are
usually numbers or strings) are the data structures on which the program
works. Many diﬀerent classes of P systems have already been investigated,
and, in particular, it has been shown that P systems are Turing complete: It
is possible to encode a Turing machine using a P system.
Since P systems can perform parallel computations, we could use P pro-
grams to model concurrent systems. In fact, the reverse is also possible, and
indeed process calculi have been used in the past to model biological mech-
anisms. For instance, variants of the π-calculus have been used to represent
cellular processes. However, the use of more abstract systems, such as mem-
brane calculi or protein-interaction calculi, has the advantage of permitting a
more clear and concise representation of biological systems.
9.1.2 Protein interaction calculi
The use of process calculi to represent biological systems has led to the design
of several diﬀerent calculi. Here we brieﬂy describe the κ-calculus, which is in-
spired by the mechanism of protein interaction. Our presentation will follow the
graphical intuitions behind the calculus; there is also an algebraic presentation.
The protein interaction calculus, as its name indicates, places a strong em-
phasis on the notion of interaction. In this sense, this formalism is closely related
to the interaction nets that were the subject of Chapter 7. One of the main dif-
ferences is that the notion of interaction here is more abstract in the sense that
interactions reﬂect protein behaviour and are therefore not restricted to binary
interactions involving principal ports of agents as in the case of interaction
nets.
Each protein has a structure that can be represented in an abstract way
as a set of switches and binding sites, not all of them accessible at a given
time. These components, generally called domains, determine possible bindings
between proteins and, as a consequence, possible interactions. Interactions can
result in changes in the protein folding, which in turn can aﬀect the interaction
capabilities of the protein. The notion of a site is used in the κ-calculus to

154
Chapter 9. Emergent Models of Computation
abstract over domains and folding states of a protein. Sites may be free or
bound, and the free sites can be visible or hidden.
In the graphical representation, proteins are nodes in the graph and sites
are ports, where edges can be attached, but only if the site is free and visible.
Bound sites correspond to sites that are already involved in a binding with
another protein (i.e., ports where an edge has been attached).
The graphs obtained by combining several nodes (i.e., proteins) and their
links are called protein complexes or simply complexes. In biological terms,
the complexes represent groups of proteins connected together by low-energy
bounds governed by protein-interaction rules. A collection of proteins and com-
plexes is called a solution; solutions evolve by means of reactions. Computa-
tionally, solutions are graphs where rewriting can take place, as deﬁned by
graph-rewriting rules associated with the biochemical reactions.
Biochemical reactions are either complexations or decomplexations: A com-
plexation is a reaction that creates a new complex out of proteins that can
interact, whereas a decomplexation breaks a complex into smaller components.
These reactions may occur in parallel and may involve activation or deactiva-
tion of sites. Causality does not allow simultaneous complexations and decom-
plexations at the same site. From a computational point of view, this means
that not all graph rewriting rules can be accepted as biochemical reactions.
For this reason, in the κ-calculus there are constraints on the kind of graph
rewriting rules that can be deﬁned as protein-interaction rules. For instance,
the left-hand sides should be connected, and a new edge can be attached to a
given site only if the site is free and visible.
Computationally, the κ-calculus is universal: Turing machines can be easily
simulated in this calculus by representing the contents of the tape as a ﬁnite
chain of nodes in the graph and using a system of graph rewriting rules to
represent the transitions between conﬁgurations of the machine. Compilations
of (the algebraic version of) the calculus into the π-calculus are also available.
9.2 Quantum computing
A quantum computer is a device that makes direct use of quantum mechanics
in order to perform computations. Roughly, if we think of computation as
the process of performing operations on data, the main diﬀerence between a
classical computer and a quantum one lies in the physical laws that govern the
medium used to store the data and the mechanisms used to manipulate the
data. In quantum computers, quantum properties are used to represent data
and perform operations on these data.

Chapter 9. Emergent Models of Computation
155
Before describing in more detail the principles behind quantum computing,
we need to recall a few notions from quantum physics.
Around 1900, physicists such as Max Planck, Niels Bohr, Erwin Schr¨odinger,
and others were working on a theory that became known as “quantum physics”.
The name derives from the word used by Planck when he announced that radi-
ant energy could only be propagated in tiny, indivisible bundles called quanta.
The word photon is used to refer to a quantum of light.
Quantum physics gives a description of the universe that is capable of ex-
plaining phenomena that cannot be explained by classical laws. One of the
best-known examples is a simple experiment using a beam splitter with equal
probability of reﬂecting or transmitting the photons (e.g., a half-silvered mir-
ror). When a photon source is directed towards the beam splitter, according to
the classical laws of physics, half of the photons should pass and half should
be reﬂected. Indeed, this is what the experiment with one beam splitter shows.
However, if instead of measuring the light that passes and the light that is
reﬂected we use two full mirrors to reﬂect it back to a second beam splitter, as
indicated in Figure 9.1, then the result does not agree with classical intuition.
Instead of seeing photons again split equally, we see that all the photons end
up in the same path (marked result in Figure 9.1).
photons
splitter
splitter
mirror
mirror
result
Figure 9.1
Two beam splitters.
The results of this experiment cannot be explained if we assume that after
the photon encounters the ﬁrst beam splitter it is either reﬂected or trans-
mitted with equal probability, as classical laws indicate. However, it can be
explained if we assume that the ﬁrst beam splitter has caused the photon to be
in a superposition of states (a combination of “reﬂected” and “transmitted”).
Then, the second beam splitter applies the same transformation, causing the
superposition to unfold into only one of the states.

156
Chapter 9. Emergent Models of Computation
The classical laws of physics are a good approximation of quantum mechan-
ics at the macroscopic scale, but on the quantum level the classical laws are
inaccurate, as this experiment shows, and the quantum laws should be used
instead.
The main idea behind quantum computation is then to replace the classical
circuits in traditional computers by quantum gates to obtain a computer whose
work is quantum-mechanical.
The notion of a quantum bit, or qubit, is fundamental in quantum comput-
ing. A qubit can be encoded in a two-level system such as a photon. Thus,
unlike classical bits, which represent binary information, in a quantum system
the state of the qubit is generally deﬁned by a vector in a two-dimensional
Hilbert space. This is a combination of the basis vectors, usually written as
|0⟩and |1⟩and corresponding to the classic binary values. In this way, it is
possible to represent states with superposition. In general, a qubit’s state can
be written as
α|0⟩+ β|1⟩
where α and β are complex numbers such that α2 + β2 = 1; states that diﬀer
only by a scalar factor with modulus 1 are considered indistinguishable.
One important diﬀerence between classic bits and qubits is the role of mea-
surement. If a qubit is in the state α|0⟩+ β|1⟩, it means that its value, if
measured, will be 0 with probability α2 and 1 with probability β2.
Another important diﬀerence is the entanglement phenomenon, which can
arise in systems with two or more qubits. This means that the states of the
qubits may be correlated in such a way that a measurement on one of the qubits
will determine the result of the measurement in the other (even if the qubits
are physically separated).
In a quantum circuit, logical qubits (quantum binary digits) are carried
along “wires” and quantum gates act on the qubits, changing their state.
The ﬁrst formal quantum circuit model was proposed by David Deutsch,
who also deﬁned a quantum Turing machine. Not only do these results show
that quantum mechanics can be used to design computers, but also it has been
shown that there are eﬃcient algorithms to solve problems for which no eﬃcient
solution is known on a standard or probabilistic Turing machine. Thus, if large-
scale quantum computers could be built, they would be able to solve certain
problems, such as integer factorisation, much faster than any current classical
computer. This has enormous implications in areas such as cryptography since
many encryption protocols would then be easy to break.
Quantum computing is still in its early stages, but experiments have been
carried out in which quantum computations were executed on a very small
number of qubits. Research continues, and we can expect that new results will
be available soon due to the important consequences of this research in areas
such as cryptanalysis.

Chapter 9. Emergent Models of Computation
157
9.3 Further reading
For further information on quantum computing, we refer the reader to the
introductory book by Kaye, Laﬂamme, and Mosca [26]; a survey on quantum
programming languages can be found in [17].
The use of process calculi to represent biological systems has led to the
design of several calculi. In addition to the membrane calculi and the κ-calculus
discussed in this chapter, for which we refer the reader to Paun’s work [41] and
Danos and Laneve’s article [11], respectively, we can mention the brane calculi
designed by Luca Cardelli [8] and the biochemical machine (BIOCHAM) [14],
amongst others.

10
Answers to Selected Exercises
Exercises in Chapter 1
1. Give more examples of total and partial functions on natural numbers.
Answer:
There are many examples of total functions. Addition, multiplication, and any
combination of these, as well as the well-known factorial function, are all total.
Subtraction is partial on natural numbers (but total on integers).
2. To test whether a number is even or odd, a student has designed the fol-
lowing function:
test(x)
def
=
if x = 0 then "even"
else if x = 1 then "odd" else test(x-2)
Is this a total function on the set of integer numbers? Is it total on the
natural numbers?
Answer:
The set of natural numbers contains 0 and all the positive integers. For these,
the test provided above gives a result: For 0 the result is “even”, for 1 the
result is “odd”, and for any number x greater than 1 the number x-2 is still
a natural number that can be tested again. Since each recursive call to the
function test carries a smaller argument, it is easy to see that eventually the
function will be called with either 0 or 1 and will produce a result. Therefore
the function is total on natural numbers.

160
Chapter 10. Answers to Selected Exercises
However, if the function is called with a negative number, for example
test(-1), then there is no result. Therefore the function is partial on integers.
3. Consider the following variant of the Halting problem:
Write an algorithm H such that, given the description of an al-
gorithm A that requires one input, H will return 1 if A stops for
any input I and H will return 0 if there is at least one input I for
which A does not stop.
In other words, the algorithm H should read the description of A and
decide whether it stops for all its possible inputs or there is at least one
input for which A does not stop.
Show that this version of the Halting problem is also undecidable.
Answer:
We adapt the proof of undecidability given for the Halting problem in Sec-
tion 1.2.
The speciﬁcation of H indicates that H(A) = 1 if A stops for all inputs and
H(A) = 0 if there is some I such that A(I)↑.
Assuming H exists, we can build an algorithm C such that C(A)↑if H(A) = 1
and C(A) = 0 otherwise. In other words, C(A) diverges if, for all inputs I,
A(I) stops; otherwise C(A) stops.
Now, if we run C with argument C, we have
C(C)↑if and only if, for all inputs I, C(I) stops.
This is a contradiction: If C stops with any input, this includes also the input
C, and therefore C(C) should stop.
Selected exercises from Chapter 2
2. Build ﬁnite automata with alphabet {0, 1} to recognise
a) the language of strings that have three consecutive 0s;
b) the language of strings that do not have three consecutive 1s.
Answer:
The diagrams below specify the required automata.

Chapter 10. Answers to Selected Exercises
161
q4
q3
q2
q1
0, 1
0, 1
0
0
0
q3
q1
q2
0
0
0
1
1
4. Let A be a ﬁnite automaton. Show that the set of subwords (that is, pre-
ﬁxes, suﬃxes, or any continuous segment) of the words in the language
L(A) can also be recognised by a ﬁnite automaton.
Answer:
To show that the language consisting of preﬁxes of words in L(A) is recog-
nisable by a ﬁnite automaton, we can simply build an automaton for it using
as a starting point the automaton A. Indeed, to recognise a preﬁx of a word
in L(A), it is suﬃcient to turn every state in A for which there is a path to a
ﬁnal state into a ﬁnal state. In this way, we have a ﬁnite automaton A′ with
the same alphabet as A and such that if a word is a preﬁx (i.e., the initial
segment) of a word in L(A), then A′ will reach a ﬁnal state.
Recognising suﬃxes is slightly more subtle, but again, starting from A we
can build an automaton with the required property by inserting ǫ transitions
between the initial state of A and all the other states for which there is a path
to a ﬁnal state. This gives a non-deterministic automaton A′′ that, for any
suﬃx (i.e., ﬁnal segment) of a word in L(A), reaches a ﬁnal state.
Finally, combining both techniques, we can obtain an automaton that recog-
nises any continuous segment of words in L(A).
5. Use the Pumping Lemma to show that the language L containing all the
words of the form anbncn, for any n ≥0, cannot be recognised by a ﬁnite
automaton.
Answer:
Similar to Corollary 2.11. We sketch the idea: If a word anbncn is in L, then
as a consequence of the Pumping Lemma there is a substring that can be

162
Chapter 10. Answers to Selected Exercises
repeated an arbitrary number of times. Therefore L contains strings where the
number of symbols a, b, or c is diﬀerent, which contradicts the assumptions.
6. How can a push-down automaton recognise the language
{ww | w is a string of 0s and 1s and w is its mirror image}?
Give an informal description of such an automaton.
Answer:
It is easy to build a non-deterministic automaton that recognises this language.
The idea is to deﬁne states that non-deterministically put in the stack the
symbols read and also start popping symbols in case we have already reached
the middle point in the word.
7. Show that the class of languages recognisable by push-down automata (i.e.,
the class of context-free languages) is closed under union and concatenation
but not under intersection.
Answer:
Union: Assume PDA1 and PDA2 recognise two context-free languages, L1
and L2. To build a PDA that recognises the union of L1 and L2, it is suﬃcient
to include all the states in PDA1 and PDA2 (without loss of generality, we
can assume that the sets of states are disjoint) but deﬁne a new initial state
q0 with ǫ transitions to the initial states of PDA1 and PDA2 (which are no
longer initial states in the new automaton).
Concatenation: Similarly, we can build a PDA that recognises all the words
formed by concatenation of a word from L1 and a word from L2 simply by
adding ǫ transitions from the ﬁnal states in PDA1 (which are no longer ﬁnal
in the new automaton) to the initial state in PDA2 (which is no longer an
initial state).
Intersection: Showing that context-free languages are not closed by intersection
is more diﬃcult. To show it, we rely on the fact that the language consisting
of all the strings of the form anbncn is not context-free (see Section 2.3 and
Exercise 5 of Chapter 2). This language is the intersection of two context-free
languages, L1 = {a∗bncn | n ≥0} and L2 = {anbnc∗| n ≥0}, where a∗
denotes a string with an arbitrary number (0 or more) of symbols a and c∗
denotes a string with an arbitrary number (0 or more) of symbols c. We leave
to the reader the proof that there are PDAs that recognise L1 and L2 — see
the PDA deﬁned in Section 2.3 to recognise the language
{(n)n | n is a natural number}

Chapter 10. Answers to Selected Exercises
163
8. Describe a Turing machine that recognises the language of the strings w•w,
where w is a string over an alphabet {0, 1}.
Answer:
The machine can be formally deﬁned as follows:
– The set Q of states contains q0, q1, q2, q3, q4, q5, q6, qaccept, qreject.
The initial state is q0, and the ﬁnal states are qaccept and qreject.
– The input alphabet is {0, 1}. The tape alphabet contains additionally the
blank symbol ◦.
– The transition function δ is deﬁned by
δ(q0, •)
=
(qaccept, •, R)
δ(q0, 0)
=
(q1, ◦, R)
δ(q0, 1)
=
(q4, ◦, R)
δ(q1, 0)
=
(q1, 0, R)
δ(q1, 1)
=
(q1, 1, R)
δ(q1, •)
=
(q2, •, R)
δ(q2, •)
=
(q2, •, R)
δ(q2, 0)
=
(q3, •, L)
δ(q3, •)
=
(q3, •, L)
δ(q3, 0)
=
(q3, 0, L)
δ(q3, 1)
=
(q3, 1, L)
δ(q3, ◦)
=
(q0, ◦, R)
δ(q4, 0)
=
(q4, 0, R)
δ(q4, 1)
=
(q4, 1, R)
δ(q4, •)
=
(q5, •, R)
δ(q5, •)
=
(q5, •, R)
δ(q5, 1)
=
(q6, •, L)
δ(q6, •)
=
(q6, •, L)
δ(q6, 0)
=
(q6, 0, L)
δ(q6, 1)
=
(q6, 1, L)
δ(q6, ◦)
=
(q0, ◦, R)
δ(qi, x)
=
(qreject, ◦, R)
for any (qi, x) not deﬁned above
The idea is that the machine, once started in the ﬁrst symbol of the word,
remembers whether it is a 0 or a 1 (by moving to q1 or q4) and replaces
the ﬁrst symbol by a blank. Then it jumps over all the remaining 0s and 1s
until it ﬁnds a •, and then it looks for the ﬁrst symbol diﬀerent from •. If
it is the required 0 or 1, then it replaces it by a • (otherwise, the word is
rejected). After replacing the symbol by •, the machine goes backwards to
the beginning of the word and repeats the cycle.
Selected exercises from Chapter 3
3. Compute the normal forms of the following terms
a) λy.(λx.x)y
b) λy.y(λx.x)

164
Chapter 10. Answers to Selected Exercises
c) II
d) KI
e) KKK
where K = λxy.x and I = λx.x.
Answer:
We have the following reductions to normal form:
a) λy.(λx.x)y →λy.y
b) λy.y(λx.x) (the term was already a normal form!)
c) II = (λx.x)(λx.x) →λx.x = I
d) KI = (λxy.x)(λx.x) →λyx.x
e) KKK = ((λxy.x)(λxy.x))(λxy.x) →∗λxy.x = K (recall that applica-
tion associates to the left).
4. Diﬀerent notions of normal form were discussed in Chapter 3, including
the full normal form (or simply normal form) and weak head normal form.
a) What is the diﬀerence between a term having a normal form and being
a normal form? Write down some example terms.
b) If a closed term is a weak head normal form, it has to be an abstraction
λx.M. Why?
c) Indicate whether the following λ-terms have a normal form:
– (λx.(λy.yx)z)v
– (λx.xxy)(λx.xxy)
d) Show that the term Ω = (λx.xx)(λx.xx) does not have a normal form.
Find a term diﬀerent from Ω that is not normalising (i.e., a term such
that every reduction sequence starting from it is inﬁnite).
Answer:
a) A term is in normal form if it is irreducible (i.e., it has no β-redex). It has
a normal form if it can be reduced to a term in normal form. For example,
the term (λx.x)(λx.x) has a normal form but is not a normal form.
b) A weak head normal form is a term where all β-redexes occur under an
abstraction. If a term is closed, it cannot be just a variable. It may be an
application or an abstraction. In the latter case, it is a weak head normal
form. We will now show that it cannot be an application. For this, we

Chapter 10. Answers to Selected Exercises
165
reason by contradiction. Suppose that the term is an application (MN).
Since it is closed and it is a weak head normal form, M must be an
application, M = (M1 . . . M2 . . . Mn), where M1 is either a variable or
an abstraction. The ﬁrst contradicts the closedness assumption, and the
latter contradicts the assumption that the term is a weak head normal
form.
c) The term (λx.(λy.yx)z)v has a normal form. It reduces to zv, which is in
normal form. The term (λx.xxy)(λx.xxy), on the other hand, does not
have a normal form.
d) The term Ω is reducible, but its only redex is Ω itself. If we reduce it, we
again obtain Ω. Therefore, the only reduction sequence out of Ω is the
inﬁnite sequence Ω →Ω →Ω . . ..
Another example was given above: (λx.xxy)(λx.xxy).
The term (λx.y)Ω is interesting. It is not strongly normalisable since there
is an inﬁnite reduction sequence that always reduces the Ω subterm; how-
ever, it has a normal form since it reduces in one step to y. This is an
example of a term that is normalisable but not strongly normalisable.
5. Explain why if a reduction system is conﬂuent, then each term has at most
one normal form.
Answer:
In a conﬂuent reduction system, for any term M such that M →∗M1 and
M →∗M2, there is some term M3 such that M1 →∗M3 and M2 →∗M3.
Now, let us assume, by contradiction, that in a conﬂuent system some term
M has two diﬀerent normal forms, N1 and N2. Since the system is conﬂuent,
there must exist a term N3 that joins N1 and N2. But then N1 and N2 are
not normal forms since they reduce to N3 (contradiction).
11. Combinatory logic (CL for short) is a universal model of computation.
Terms in the language of CL are built out of variables x, y, . . ., constants
S and K, and applications (M N). More precisely, terms are generated by
the grammar
M, N ::= x | S | K | (M N)
The standard notational conventions are used to avoid brackets: Applica-
tions associate to the left, and we do not write the outermost brackets. For
instance, we write
K x y for the term ((K x) y).
There are two computation rules in combinatory logic:
K x y
→
x
S x y z
→
x z (y z)

166
Chapter 10. Answers to Selected Exercises
a) Using the rules above, there is a sequence of reduction steps
SKKx →∗x
Show all the reduction steps in this sequence.
b) The term SKK can be seen as the implementation of the identity
function in this system since, for any argument x, the term SKKx
evaluates to x.
Show that SKM, where M is an arbitrary term, also deﬁnes the iden-
tity function.
c) Consider the system of combinatory logic without the second compu-
tation rule (that is, only the rule Kxy →x may be used). We call this
weaker system CL−.
We call CL+ the system of combinatory logic with an additional con-
stant I and rule Ix →x.
Indicate whether each of the following statements is true or false and
why.
i. In CL−, all the reduction sequences are ﬁnite.
ii. The system CL+ has the same computational power as the system
CL.
iii. The system CL−is Turing complete.
Answer:
a) The reduction sequence is SKKx →Kx(Kx) →x.
b) The reduction SKMx →Kx(Mx) →x justiﬁes the claim.
c) This question has three parts. In the ﬁrst part, the claim is that all re-
duction sequences are ﬁnite. This is true because each application of the
reduction rule decreases the size of the term, and therefore reductions
eventually terminate.
It is easy to see that CL+ has the same computational power as CL since
I can be implemented in CL as shown above.
On the other hand, CL−is strictly less powerful than CL: CL−is not
Turing complete. Several arguments can be used to justify this claim: the
fact that each reducible term is equivalent to one of its subterms, the fact
that there is no way to copy arguments, the termination of the reduction
relation, etc.

Chapter 10. Answers to Selected Exercises
167
Selected exercises from Chapter 4
1. Show that the factorial function is primitive recursive.
Answer:
The factorial function can be deﬁned using the primitive recursive scheme as
factorial(0)
=
S(0)
factorial(S(n))
=
g(factorial(n), n)
where the auxiliary function g multiplies the ﬁrst argument by the succes-
sor of the second. The function g can be deﬁned by the composition of the
multiplication and addition functions:
g(x, y) = add(π1(x, y), mul(x, y))
2. Show that the function f used in Example 4.6, deﬁned by f(0) = 0 and
f(S(n)) = 1, is primitive recursive.
Answer:
The function f can be deﬁned as
f(0)
=
0
f(S(n))
=
one(f(n), n)
where one(x, y) = S(zero(x, y)) and zero(x, y) = 0.
5. Indicate whether the following statements are true or false:
a) All primitive recursive functions are total.
b) All total computable functions are primitive recursive.
c) All partial recursive functions are computable.
d) All total functions are computable.
Answer:
The ﬁrst claim is true and was proved in Chapter 4.
The second claim is false since there are total functions that are not primitive
recursive. Ackermann’s function, given at the end of Section 4.1, is an example
of a total but not primitive recursive function.
The third claim is true. The class of partial recursive functions is equivalent to
the class of functions that can be computed by Turing machines.
The fourth claim is false. The function that solves the Halting problem is total,
but it is not computable.

168
Chapter 10. Answers to Selected Exercises
7. In functional languages, there is a primitive function if-then-else that
we can use to deﬁne a function by cases, depending on a Boolean condition
(see the case construction in Deﬁnition 4.7). Thus,
if x == 0 then 0 else x * y
will return 0 if the value of x is equal to 0 and will return the product of x
and y otherwise.
Assume the function mult on natural numbers is deﬁned by
mult x y
def
= if x == 0 then 0 else x * y
where == is the equality test. Assume that e1 == e2 is evaluated by reducing
e1 and e2 to normal form and then comparing the normal forms.
a) Is mult commutative over numbers; i.e., will mult m n and mult n m
compute the same result for all numbers m and n?
b) Let infinity be the function deﬁned by
infinity
def
= infinity + 1
What is the value of mult infinity 0?
What is the value of mult 0 infinity?
Answer:
If both arguments of mult are numbers, then the comparison with 0 always
produces a result, and therefore mult is commutative. If one of the arguments
is 0, the result is 0; otherwise it is the result of x ∗y.
However, for
mult infinity 0, the evaluation process does not terminate.
The value of infinity is undeﬁned, and therefore the comparison with 0 does
not return a result.
For
mult 0 infinity, the value is 0 and can be found with a strategy that
uses normal order.
Selected exercises from Chapter 5
1. Assuming that A, B, C are atoms, which of the following clauses are Horn
clauses?
a) ¬A
b) A ∨B ∨¬C

Chapter 10. Answers to Selected Exercises
169
c) A ∨¬A
d) A
Answer:
The only clause that is not a Horn clause is the second one (it has two positive
literals).
4. Give the most general uniﬁer (if it exists) of the following atoms (recall
that [1,2] is short for the list [1|[2|[]]]):
a) append([1,2],X,U), append([Y|L],Z,[Y|R])
b) append([1,2],X,[0,1]), append([Y|L],Z,[Y|R])
c) append([],X,[0,1]), append([Y|L],Z,[Y|R])
d) append([],X,[0]), append([],[X|L],[Y])
Answer:
a) The
most
general
uniﬁer
of
the
terms
append([1,2],X,U)
and
append([Y|L],Z,[Y|R]) is {Y →1, L →[2], Z →X, U →[1|R]}.
b) The terms append([1,2],X,[0,1]), append([Y|L],Z,[Y|R]) are not
uniﬁable since we need Y = 1 and Y = 0 at the same time.
c) The terms append([],X,[0,1]), append([Y|L],Z,[Y|R]) are not
uniﬁable since we cannot unify the empty list with a non-empty list.
d) The terms append([],X,[0]), append([],[X|L],[Y]) are not uniﬁ-
able since we cannot unify X with [X|L] (occur-check).
6. Show that the resolvent of the clauses
P :- A1, . . . , An
and
:- Q1, . . . , Qm
is also a Horn clause.
Answer:
By deﬁnition, each Horn clause contains zero or one positive literal. Resolving
eliminates one literal Qi and replaces it by A1, . . . , An, with a suitable sub-
stitution (which will only modify the terms inside the literals). Therefore the
resolvent is still a Horn clause.

170
Chapter 10. Answers to Selected Exercises
7. Consider the program
nat(s(X)) :- nat(X).
nat(0).
and the query
:- nat(Y).
a) Describe the complete SLD-resolution tree for this query.
b) Explain why Prolog will not ﬁnd an answer for this query.
c) Change the program so that Prolog can ﬁnd an answer.
Answer:
The complete SLD-tree is
nat(Y)
{Y →s(X1)}/ \{Y →0}
nat(X1)
♦
{X1 →s(X2)}/ \{X1 →0}
nat(X2)
♦
...
Prolog will not ﬁnd an answer because ﬁrst it explores the leftmost branch,
which is inﬁnite in this case. We need to change the order of the clauses:
nat(0).
nat(s(X)) :- nat(X).
12. A graph is a set V = {a, b, c, . . .} of vertices and a set E ⊆V × V of edges.
We use the binary predicate edge to represent the edges: edge(a,b) means
that there is an edge from a to b. In a directed graph, the edges have a
direction, so edge(a,b) is diﬀerent from edge(b,a). We say that there is
a path from a to b in a graph if there is a sequence of one or more edges
that allows us to go from a to b.
a) Write a logic program deﬁning the predicate path.
b) Write a query to compute all the directed paths starting from a in the
graph.
c) Write a query to compute all the directed paths in the graph.

Chapter 10. Answers to Selected Exercises
171
Answer:
The following program deﬁnes the predicate path:
path(X,Y) :- edge(X,Y).
path(X,Y) :- edge(X,Z), path(Z,Y).
The following query computes the paths starting from a:
:- path(a,X).
The following query computes all the paths:
:- path(X,Y).
Selected exercises from Chapter 6
1. What is the fundamental diﬀerence between a method deﬁned by l = ς(x)b
in an object o and a function with argument x deﬁned by the λ-term λ(x)b?
Answer:
In l = ς(x)b, x is the self variable. It refers to the whole object where the
method l is deﬁned.
We could simulate method invocation using function application. Assume we
deﬁne an object o = [li = λ(xi)bi∈1...n
i
]. Then we can replace the usual
invocation rule
o.lj −→bj{xj →o}
by
o.lj −→(λ(xj)bj)o
Observe that (λ(xj)bj)o →bj{xj →o} as required.
3. Add a method get in the object loc deﬁned in Example 6.8 to represent a
memory location, so that the ﬁeld value is accessed by get.
Answer:
loc
=
[value = 0,
set = ς(x)λ(n)x.value := n,
get = ς(x)x.value]

172
Chapter 10. Answers to Selected Exercises
4. In a calculus that combines objects, functions, numbers, and arithmetic
functions, we have deﬁned the following object:
loc
=
[value = 0,
set = ς(x)λ(n)x.value := n,
incr = ς(x)x.value := x.value + 1]
a) Describe in your own words the behaviour of the methods set and incr.
b) Evaluate the terms (and show the reduction steps)
i. loc.set(1).set(3).value
ii. loc.incr.value
where loc is the object deﬁned above.
Answer:
The method set stores a value in the ﬁeld value, and incr increases by one
the number stored in value.
The reductions are
loc.set(1).set(3).value
→
((λ(n)loc.value := n)1).set(3).value
→
(loc.value := 1).set(3).value
→
[value = 1, set = ς(x)λ(n)x.value := n,
incr = ς(x)x.value := x.value + 1].set(3).value
→
((λ(n)loc′.value := n)3).value
→
(loc′.value := 3).value
→
[value = 3, set = ς(x)λ(n)x.value := n,
incr = ς(x)x.value := x.value + 1].value
→
3
loc.incr.value
→
(loc.value := loc.value + 1).value
→
(loc.value := 0 + 1).value
→
[value = 0 + 1, set = ς(x)λ(n)x.value := n,
incr = ς(x)x.value := x.value + 1].value
→
0 + 1 →1
6. Recall the translation function T from the λ-calculus to the ς-calculus
deﬁned in Chapter 6:
T (x) = x
T (λx.M) = [arg = ς(x)x.arg, val = ς(x)T (M){x →x.arg}]
T (MN) = (T (M).arg := T (N)).val

Chapter 10. Answers to Selected Exercises
173
a) Using this deﬁnition, write down the ς-terms obtained by the following
translations:
i. T (λx.x)
ii. T (λxy.x)
iii. T (λy.(λx.x)y)
iv. T ((λx.x)(λy.y))
b) Reduce T ((λx.x)(λy.y)) to normal form using the reduction rules of
the ς-calculus.
c) What are the advantages and disadvantages of a computation model
that combines the ς-calculus and additional rewriting rules? Compare
it with the pure ς-calculus.
Answer:
T (λx.x) = [arg = ς(x)x.arg, val = ς(x)x.arg]
To compute T (λxy.x), we proceed as follows:
T (x)
=
x
T (λy.x)
=
[arg = ς(y)y.arg, val = ς(y)x]
T (λxy.x)
=
[arg = ς(x)x.arg,
val = ς(x)[arg = ς(y)y.arg, val = ς(y)x.arg]]
To compute T (λy.(λx.x)y), we proceed as follows:
T ((λx.x)y)
=
([arg = ς(x)x.arg, val = ς(x)x.arg].arg := y).val
T (λy.(λx.x)y)
=
[arg = ς(y)y.arg,
val = ς(y)([arg = ς(x)x.arg,
val = ς(x)x.arg].arg := y.arg).val]
Below we compute T ((λx.x)(λy.y)) and reduce it to normal form.
T ((λx.x)(λy.y))
=
([arg = ς(x)x.arg, val = ς(x)x.arg].arg :=
[arg = ς(y)y.arg, val = ς(y)y.arg]).val
−→
[arg = [arg = ς(y)y.arg, val = ς(y)y.arg],
val = ς(x)x.arg].val
−→
[arg = [arg = ς(y)y.arg, val = ς(y)y.arg],
val = ς(x)x.arg].arg
−→
[arg = ς(y)y.arg, val = ς(y)y.arg]
=
T (λy.y)

174
Chapter 10. Answers to Selected Exercises
The advantages of a model of computation combining the ς-calculus with
additional reduction rules include the fact that additional rules can make it
easier to write programs in speciﬁc domains; for instance, an extension with
the λ-calculus allows the natural representation of functional components of a
program, speciﬁcally input/output.
The disadvantage is that an extension may break some useful properties (e.g.,
conﬂuence).
7. Indicate whether each of the following statements about the ς-calculus is
true or false and why.
a) The ς-calculus is conﬂuent; therefore each expression has at most one
normal form in this calculus.
b) The ς-calculus does not have an operation to add methods to an object;
therefore it is not a Turing-complete model of computation.
Answer:
It is conﬂuent, and conﬂuence implies the unicity of normal forms.
It does not have an operation to extend objects with new methods, but it is
Turing complete. For instance, the λ-calculus can be encoded in the object
calculus, as shown in Chapter 6.
Selected exercises from Chapter 7
4.
a) Give an interaction system to compute the Boolean function and.
Answer:


F




and
m

@
x
=⇒


F
m
6


ǫ
x ?


T




and
m

@
x
=⇒
x
m
b) Draw the interaction net representing the expression
(True and False) and True
How many reductions are needed to fully normalise this net?

Chapter 10. Answers to Selected Exercises
175
Answer:
We omit the diagram; the net has an active pair T ⊲⊳and, which creates
another active pair F ⊲⊳and. This interaction produces an agent F and
creates an active pair T ⊲⊳ǫ. The ﬁnal result, after three interaction steps,
is False.
6. Textual rules deﬁning addition were given in Example 7.14. Can you write
the textual version of the rules for multiplication given in Section 7.2?
Answer:
m(0, ǫ)
⊲⊳
0
m(x, δ(y, z))
⊲⊳
S(m(add(x, z), y))
7. Explain why interaction nets are not suitable as a model for non-
deterministic computations.
Answer:
Interaction nets are intrinsically deterministic. They are strongly conﬂuent,
which means that all reduction sequences produce the same result.
Selected exercises from Chapter 8
1. Prove that the relation ∼introduced in Deﬁnition 8.6 is an equivalence
relation, as stated in Proposition 8.7.
Answer:
Recall that p ∼q if there is a strong bisimulation S such that (p, q) is in S. The
relation S is a strong bisimulation if both S and S−1 are strong simulations.
To prove that ∼is an equivalence relation, we need to show:
– Reﬂexivity: For all p ∈Q, p ∼p.
This can be proved using the relation S that contains all the pairs (p, p)
such that p is a state in Q.
The relations S and S−1 coincide in this case, and it is easy to see that S
is a strong simulation.
– Symmetry: For all p, q ∈Q, if p ∼q, then q ∼p.
Note that p ∼q implies that (p, q) is in a strong bisimulation S, and (q, p)
is in S−1, which is also a strong bisimulation by the deﬁnition of ∼. Hence
q ∼p.

176
Chapter 10. Answers to Selected Exercises
– Transitivity: For all p, q, r ∈Q, if p ∼q and q ∼r, then p ∼r.
To show transitivity, it is suﬃcient to prove that if S and S′ are strong
bisimulations, then so is S ◦S′, where
S ◦S′ = {(p, r) | (p, q) ∈S and (q, r) ∈S′ for some q}
2. Prove that if p ∼q, then p simulates q and q simulates p.
The reverse is not true. Can you give a counterexample?
Answer:
If p ∼q, then there is a strong bisimulation S such that pSq. By deﬁnition,
this means that S and S−1 are strong simulations and contain the pairs (p, q)
and (q, p), respectively. Therefore q simulates p and p simulates q.
The reverse implication is not true, as the following counterexample shows.
Consider two labelled transition systems, D1 = (Q1, T1) and D2 = (Q2, T2),
such that in D1 there is a state p with transitions p −→a p1 and also
p −→a p′
1; that is, from the state p, we can pass to the state p1 or p′
1
(non-deterministically) by a. Assume that a further transition is possible from
p1 to p2, labelled by b: p1 −→b p2.
Assume Q2 = {q, q1, q2} and T2 contains the transitions q −→a q1 and q1 −→b
q2.
We can show that q simulates p using the relation
S = {(p, q), (p1, q1), (p′
1, q1), (p2, q2)}
which is a strong simulation.
We can also show that p simulates q using the strong simulation
S′ = {(q, p), (q1, p1), (q2, p2)}
However, S and S′ are not inverses, and it is not possible to deﬁne a strong
simulation such that the inverse is also a strong simulation because p and q
are not observationally equivalent: There is a transition p −→a p′
1 after which
D1 is blocked (no further transitions are possible), whereas for D2 there is no
equivalent state.
4. Consider a counter deﬁned as a device that can hold a natural number,
increment its value, or decrement it, but if the value of the counter is zero,
decrementing it does not change the value of the counter. Write a process
expression deﬁning such a counter.

Chapter 10. Answers to Selected Exercises
177
Answer:
We can specify the counter using a transition system with a state for each
possible value of the counter (for instance, Q = Cn (n ≥0)) and transitions
labelled by incr or decr to increment or decrement the value of the counter.
The following equations can be used to deﬁne the states:
C0(inc, dec)
=
inc.C1⟨inc, dec⟩+ dec.C0⟨inc, dec⟩
Cn+1(inc, dec)
=
inc.Cn+2⟨inc, dec⟩+ dec.Cn⟨inc, dec⟩
We initialise the counter by deﬁning Counter = C0⟨inc, dec⟩.
5. In order to prove that P ≡Q implies P ∼Q as stated in the second part
of Proposition 8.14, it is suﬃcient to show that the structural congruence
≡is a strong bisimulation. Can you prove this fact?
Answer:
First, note that ≡and ≡−1 coincide, so we just need to show that ≡is a
strong simulation.
Assume P ≡Q. Then, if P −→α P ′, also Q −→α P ′, and P ′ ≡P ′ as
required.
6. Let P be the process deﬁned by the expression ν d e f .(K1|K2|K3), where
K1
=
f.a.d.K1
K2
=
d.b.e.K2
K3
=
f.e.c.K3
and let H be the process deﬁned by the equation
H
=
a.b.c.H
a) Give labelled transition systems for P and for H.
b) Show that P ∼H.
Answer:
a) The set of states is isomorphic to the set of subexpressions. The transitions
are deﬁned by
P
−→τ
P 1 = ν d e f .(ad.K1|dbe.K2|ec.K3)
−→a
P 2 = ν d e f .(d.K1|dbe.K2|ec.K3)
−→τ
P 3 = ν d e f .(K1|be.K2|ec.K3)
−→b
P 4 = ν d e f .(K1|e.K2|ec.K3)
−→τ
P 5 = ν d e f .(K1|K2|c.K3)
−→c
P

178
Chapter 10. Answers to Selected Exercises
and
H −→a H′ = bc.H −→b H′′ = c.H −→c H
To show that they are bisimilar, we can use the relation
S = {(H, P), (H, P 1), (H′, P 2), (H′, P 3), (H′′, P 4), (H′′, P 5)}.

Bibliography
[1] M. Abadi and L. Cardelli. A Theory of Objects. Monographs in Computer
Science. Springer, 1996.
[2] S. Alves, M. Fern´andez, M. Florido, and I. Mackie. Linear recursive func-
tions. In Rewriting, Computation and Proof: Essays Dedicated to Jean-
Pierre Jouannaud on the Occasion of His 60th Birthday, volume 4600 of
Lecture Notes in Computer Science, Festchrift. Springer, 2007.
[3] J.-P. Banˆatre and D. Le M´etayer. The GAMMA model and its discipline
of programming. Science of Computer Programming, 15:55–77, 1990.
[4] H. P. Barendregt. The Lambda Calculus: Its Syntax and Semantics. North-
Holland, 1984. Revised edition.
[5] G. Berry and G. Boudol. The Chemical Abstract Machine. In Proceedings,
17th ACM Symposium on Principles of Programming Languages, pages
81–94. ACM Press, 1990.
[6] R. Bird. Introduction to Functional Programming Using Haskell. Prentice-
Hall, 1998.
[7] P. Blackburn, J. Bos, and K. Striegnitz. Learn Prolog Now, volume 7 of
Texts in Computing. College Publications, 2007.
[8] L. Cardelli. Brane calculi — interactions of biological membranes. In Com-
putational Methods in Systems Biology: International Conference CMSB
2004, volume 3082 of Lecture Notes in Computer Science, pages 257–280.
Springer, 2005.
[9] A. Compagnoni and M. Fern´andez.
An object calculus with algebraic
rewriting. In Programming Languages: Implementations, Logics, and Pro-

180
Bibliography
grams. Proceedings of PLILP’97, volume 1292 of Lecture Notes in Com-
puter Science. Springer, 1997.
[10] G. Cousineau and M. Mauny. The Functional Approach to Programming.
Cambridge University Press, 1998.
[11] V. Danos and C. Laneve. Formal molecular biology. Theoretical Computer
Science, 325(1):69–110, 2004.
[12] N. Dershowitz and Y. Gurevich. A natural axiomatization of computability
and proof of Church’s thesis. Bulletin of Symbolic Logic, 14(3):299–350,
2008.
[13] ECMA.
ECMAScript language speciﬁcation, 1999.
Available from
http://www.ecma.ch/ecma1/stand/ecma-262.htm.
[14] F. Fages and S. Soliman. Formal cell biology in BIOCHAM (tutorial). In
8th International School on Formal Methods for the Design of Computer,
Communication and Software Systems: Computational Systems Biology.
In memory of Nadia Busi, volume 5016 of Lecture Notes in Computer
Science. Springer, 2008.
[15] M. Fern´andez. Programming Languages and Operational Semantics: An
Introduction, volume 1 of Texts in Computing. King’s College Publications,
2004.
[16] M. Fern´andez and I. Mackie. A calculus for interaction nets. In G. Na-
dathur, editor, Proceedings of the International Conference on Principles
and Practice of Declarative Programming (PPDP’99), volume 1702 of Lec-
ture Notes in Computer Science, pages 170–187. Springer-Verlag, 1999.
[17] S. J. Gay. Quantum programming languages: Survey and bibliography.
Mathematical Structures in Computer Science, 16(4):581–600, 2006.
[18] M. Gladstone. A reduction of the recursion scheme. Journal of Symbolic
Logic, 32:505–508, 1967.
[19] M. Gladstone. Simpliﬁcation of the recursion scheme. Journal of Symbolic
Logic, 36:653–665, 1971.
[20] J. Gosling, B. Joy, and G. Steele.
The Java Language Speciﬁcation.
Addison-Wesley, 1996.
[21] C. Hankin. An Introduction to Lambda Calculi for Computer Scientists,
volume 2 of Texts in Computing. King’s College Publications, 2004.
[22] D. Harel and Y. A. Feldman.
Algorithmics: The Spirit of Computing.
Pearson Education, 2004.

Bibliography
181
[23] C. J. Hogger. Introduction to Logic Programming. APIC Studies in Data
Processing. Academic Press, 1984.
[24] J. E. Hopcroft, R. Motwani, and J. D. Ullman. Introduction to Automata
Theory, Languages and Computability. Addison-Wesley, 2000.
[25] D. H. H. Ingalls. Design principles behind Smalltalk. BYTE Magazine,
1981.
[26] P. Kaye, R. Laﬂamme, and M. Mosca. An Introduction to Quantum Com-
puting. Oxford University Press, 2007.
[27] S. C. Kleene. Introduction to Metamathematics. North-Holland, 1952.
[28] Y. Lafont. Interaction nets. In Proceedings, 17th ACM Symposium on
Principles of Programming Languages, pages 95–108. ACM Press, 1990.
[29] Y. Lafont.
Interaction combinators.
Information and Computation,
137(1):69–101, 1997.
[30] X. Leroy, D. Doligez, J. Garrigue, and J. Vouillon. The Objective Caml
System. Technical report, INRIA.
[31] A. Martelli and U. Montanari. An eﬃcient uniﬁcation algorithm. Trans-
actions on Programming Languages and Systems, 4(2):258–282, 1982.
[32] J. McCarthy, P. Abrahams, D. Edwards, T. Hart, and M. Levin. LISP 1.5
Programmer’s Manual, second edition. MIT Press, 1965.
[33] R. Milner. Communication and Concurrency. Prentice-Hall, 1989.
[34] R. Milner. Communicating and Mobile Systems: The π-Calculus. Cam-
bridge University Press, 1999.
[35] R. Milner, M. Tofte, and R. Harper. The Deﬁnition of Standard ML. MIT
Press, 1990.
[36] J. C. Mitchell. Concepts in Programming Languages. Cambridge Univer-
sity Press, 2003.
[37] H. R. Nielson and F. Nielson. Semantics with Applications: An Appetizer.
Undergraduate Topics in Computer Science. Springer, 2007.
[38] M. Odersky. Programming in Scala, 2005. Available from
http://scala.epﬂ.ch/docu/.
[39] P. Odifreddi. Classical Recursion Theory. Elsevier Science, 1999.
[40] D. M. Park.
Concurrency on automata and inﬁnite sequences.
In
P. Deussen, editor, Conference on Theoretical Computer Science, volume
104 of Lecture Notes in Computer Science. Springer-Verlag, 1981.

182
Bibliography
[41] G. Paun. Computing with membranes. Journal of Computer and System
Sciences, 61:108–143, 2000.
[42] J. S. Pinto. Sequential and concurrent abstract machines for interaction
nets. In J. Tiuryn, editor, Proceedings of Foundations of Software Science
and Computation Structures (FOSSACS), volume 1784 of Lecture Notes
in Computer Science, pages 267–282. Springer-Verlag, 2000.
[43] J. A. Robinson. A machine-oriented logic based on the resolution principle.
Journal of the ACM, 12(1):23–41, 1965.
[44] P. Roussel. PROLOG: Manuel de r´ef´erence et d’utilisation, 1975. Research
Report, Artiﬁcial Intelligence Team, University of Aix-Marseille, France.
[45] D. Sangiorgi and D. Walker. The π-Calculus: A Theory of Mobile Pro-
cesses. Cambridge University Press, 2001.
[46] J. Shoenﬁeld. Recursion Theory. Springer-Verlag, 1993.
[47] M. Sipser. Introduction to the Theory of Computation. Course Technology
— Cengage Learning, 2006.
[48] B. Stroustrup. The C++ Programming Language. Addison-Wesley Long-
man, 1997.
[49] T. Sudkamp. Languages and Machines: An Introduction to the Theory of
Computer Science. Addison-Wesley, 2006.
[50] G. Sussman and G. Steele. Scheme: An interpreter for extended lambda
calculus, 1975. MIT AI Memo 349.
[51] S. Thompson. The Craft of Functional Programming. Addison-Wesley,
1999.
[52] D. Ungar and R. B. Smith. Self: The power of simplicity. In N. K. Mey-
rowitz, editor, Conference on Object-Oriented Programming Systems, Lan-
guages, and Applications (OOPSLA’87), 1987, Orlando, Florida, Proceed-
ings, SIGPLAN Notices 22(12), pages 227–242. ACM Press, 1987.
[53] G. Winskel. The Formal Semantics of Programming Languages. Founda-
tions of Computing. MIT Press, 1993.

Index
λ-calculus, 33
– reduction, 38
– strategies, 44
– substitution, 41
– terms, 34
algorithm, 1
answer, 73, 81
– substitution, 73
backtracking, 74, 85
bio-computing, 152
– membrane calculi, 152
– protein interaction calculus, 153
bisimulation, 139, 145
Chemical abstract machine, 147
chemical metaphor, 147
Church numerals, 45
clause, 71
– deﬁnite, 71
– fact, 71
– goal, 71
– Horn, 71
– program, 71
– query, 71
– rule, 71
computable function, 4
concurrency
– labelled transition system, 137
– parallel processes, 143
– process expression, 140
– strong bisimulation, 139
– strong simulation, 138
conﬂuence, 43, 97, 103, 110, 125
evaluation
– call-by-name, 45, 66
– call-by-value, 45, 66
– lazy, 45, 67
– strategy, 45, 66
ﬁnite automata, 13
– deterministic, 16
– non-deterministic, 16
– Pumping Lemma, 19
ﬁx point operators, 48
formal languages, 12
function, 63
– higher-order, 49
– partial recursive, 62
– primitive recursive, 56
goal, 71
Halting problem, 5
Herbrand universe, 69
interaction nets
– amb, 128
– textual calculus, 120
interaction nets, 107
– combinators, 117
– conﬁguration, 122
– interface, 108
– ports, 108

184
Index
– strong conﬂuence, 110
interaction rule, 109
labelled transition diagram, 134
literal, 70
method, 94
minimisation
– bounded, 60
– unbounded, 61
normal form, 65
normalisation, 43
object calculus
– conﬂuence, 98
– method, 94
– method invocation, 96
– method update, 96
– object, 94
object-oriented programming, 99
occur-check, 78
parallel composition, 142
partial function, 2
predicate, 70
primitive recursive scheme, 57
process, 141
programming languages
– functional object-oriented, 101
programming languages
– functional, 49, 63
– imperative, 30
– logic, 84
– object-oriented, 99
push-down automata, 20
quantum computing, 154
– entanglement, 156
– qubit, 156
– superposition, 156
qubit, 156
query, 71
resolution, 79
– failure, 81
– resolvent, 79
– search strategy, 85
– SLD-resolution, 73, 80, 85
– SLD-tree, 74, 81
– success, 81
restriction operator, 142
simulation, 138
strong normalisation, 43
termination, 43
Turing machine
– universal, 29
Turing machine, 23
– formal deﬁnition, 25
– variants, 28
types
– strong typing, 63
undecidable problems, 5
uniﬁcation, 75
– most general uniﬁer, 77
– uniﬁcation algorithm, 76
– uniﬁcation problem, 77
von Neumann machine, 30


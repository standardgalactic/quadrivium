Applied Mathematical Sciences
Daniela Calvetti
Erkki Somersalo
Bayesian 
Scientific 
Computing

Applied Mathematical Sciences
Founding Editors
F. John
J. P. LaSalle
L. Sirovich
Volume 215
Series Editors
Anthony Bloch, Department of Mathematics, University of Michigan, Ann Arbor,
MI, USA
C. L. Epstein, Department of Mathematics, University of Pennsylvania,
Philadelphia, PA, USA
Alain Goriely, Department of Mathematics, University of Oxford, Oxford, UK
Leslie Greengard, New York University, New York, NY, USA
Advisory Editors
J. Bell, Center for Computational Sciences and Engineering, Lawrence Berkeley
National Laboratory, Berkeley, CA, USA
P. Constantin, Department of Mathematics, Princeton University, Princeton, NJ,
USA
R. Durrett, Department of Mathematics, Duke University, Durham, CA, USA
R. Kohn, Courant Institute of Mathematical Sciences, New York University,
New York, NY, USA
R. Pego, Department of Mathematical Sciences, Carnegie Mellon University,
Pittsburgh, PA, USA
L. Ryzhik, Department of Mathematics, Stanford University, Stanford, CA, USA
A. Singer, Department of Mathematics, Princeton University, Princeton, NJ, USA
A. Stevens, Department of Applied Mathematics, University of Münster, Münster,
Germany
S. Wright, Computer Sciences Department, University of Wisconsin, Madison, WI,
USA

The mathematization of all sciences, the fading of traditional scientiﬁc boundaries,
the impact of computer technology, the growing importance of computer modeling
and the necessity of scientiﬁc planning all create the need both in education and
research for books that are introductory to and abreast of these developments. The
purpose of this series is to provide such books, suitable for the user of mathematics,
the mathematician interested in applications, and the student scientist. In particular,
this series will provide an outlet for topics of immediate interest because of the
novelty of its treatment of an application or of mathematics being applied or lying
close to applications. These books should be accessible to readers versed in
mathematics or science and engineering, and will feature a lively tutorial style, a
focus on topics of current interest, and present clear exposition of broad appeal.
A compliment to the Applied Mathematical Sciences series is the Texts in Applied
Mathematics series, which publishes textbooks suitable for advanced undergraduate
and beginning graduate courses.

Daniela Calvetti · Erkki Somersalo
Bayesian Scientiﬁc
Computing

Daniela Calvetti
Department of Mathematics, Applied
Mathematics, and Statistics
Case Western Reserve University
Cleveland, OH, USA
Erkki Somersalo
Department of Mathematics, Applied
Mathematics, and Statistics
Case Western Reserve University
Cleveland, OH, USA
ISSN 0066-5452
ISSN 2196-968X (electronic)
Applied Mathematical Sciences
ISBN 978-3-031-23823-9
ISBN 978-3-031-23824-6 (eBook)
https://doi.org/10.1007/978-3-031-23824-6
Mathematics Subject Classiﬁcation: 65C40, 65F22, 65F08, 62C10, 62F15
© The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature
Switzerland AG 2023
This work is subject to copyright. All rights are solely and exclusively licensed by the Publisher, whether
the whole or part of the material is concerned, speciﬁcally the rights of translation, reprinting, reuse
of illustrations, recitation, broadcasting, reproduction on microﬁlms or in any other physical way, and
transmission or information storage and retrieval, electronic adaptation, computer software, or by similar
or dissimilar methodology now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication
does not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant
protective laws and regulations and therefore free for general use.
The publisher, the authors, and the editors are safe to assume that the advice and information in this book
are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or
the editors give a warranty, expressed or implied, with respect to the material contained herein or for any
errors or omissions that may have been made. The publisher remains neutral with regard to jurisdictional
claims in published maps and institutional afﬁliations.
This Springer imprint is published by the registered company Springer Nature Switzerland AG
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland

To Oriana and Giovanni Madan

Preface
Fifteen years ago, when the idea of using probability to model unknown parameters
to be estimated computationally was a less commonly accepted idea than it is today,
writing a book to show how natural and rather straightforward it was to merge
Bayesian inference and scientiﬁc computing felt a brave and daring act. The natural
environment for such a symbiotic relation to develop was inverse problems, a well-
established research area where both Bayesian inference and computational methods
were establishing their own separate strongholds. One of the main reasons for the
separation is that probability was not part of the usual training of a numerical analyst,
just like scientiﬁc computing was not part of the typical background of a statistician
or a probabilist. Nonetheless, the concurrent similarity and complementarity of the
two ﬁelds suggested that if there could be a way to create some synergy between
them, it would probably lead to novel and unexpected ﬁndings.
We ourselves, coming from numerical analysis and Bayesian-physics-based
inverse problems, were very aware of some of the obstacles to be overcome, including
having to ﬁll some holes in the background without getting too deep into the intrica-
cies of these ﬁelds and feeling like we had ventured too far out of our own comfort
zone. The book Introduction to Bayesian Scientiﬁc Computing: Ten Lectures in
Subjective Computing [18] was a light-hearted gateway to what we called Bayesian
Scientiﬁc Computing, where we deliberately chose to not include theorems or proofs,
emphasizing instead the problem-solving power of the approach. The book was
written with the purpose of awaking the curiosity of unlikely readers, and in a style
that, we hoped, would make the reading experience more like following an unfolding
story rather than scientiﬁc reading. Likewise, when it came to compiling a list of refer-
ences, we went for a quite radical version of Occam’s razor. In hindsight, our ﬁrst
book on Bayesian scientiﬁc computing reﬂected our enthusiasm for the content and
our desire to make it easy for newcomers to look into it, preemptively answering
many of the questions that the reader may have had and would not dare to ask, in
fear of sounding naïve. Over the years, as the idea of combining classical numerical
analysis and Bayesian inference became more mainstream, the questions that people
asked became deeper, and we started to feel about our book the same way as we did
about the bell-bottom pants and the mullet haircuts of our younger years.
vii

viii
Preface
In the last 15 years, pairing Bayesian inference and scientiﬁc computing has
become more natural, also thanks to the tremendous growth of uncertainty quan-
tiﬁcation, an area of applied mathematics where Bayesian inference and scientiﬁc
computing ﬁt very naturally. Over the same period of time, Bayesian scientiﬁc
computing has advanced a lot, and we began to feel that it was time to update the 2007
book and write a more grown-up version that would include the new developments in
the ﬁeld. That would also give us a chance to revisit some of the topics that we, in our
initial enthusiasm, had sometimes treated in a very cavalier way. Among the topics
that were either missing, or not systematically developed in the 2007 monograph,
are hierarchical models and sparsity promotion priors. Sparse solutions of inverse
problems, and methods for computing them, have been the topic of active research
in the last 15 years, in particular, following the popularity of ℓ1 and total variation
regularization. Since sparsity is part of the a priori belief about the unknown, it is very
natural to express it in Bayesian terms. The systematic analysis of sparsity-promoting
priors carried out in the last 15 years within the larger family of hierarchical priors
has shed light on how to characterize sparsity in probabilistic terms. Hierarchical
priors and their computationally friendly formulations, examined at different levels
and from various points of view over several chapters, are prominently featured
in this monograph, and the connection between certain choices of hyperpriors and
Tikhonov regularization is also established. Another major topic that was completely
omitted from [18] is sequential Bayesian methods, which now occupy the last two
chapters of the book. Particle ﬁlters, Kalman ﬁlters and Ensemble Kalman ﬁlters
combine, in an elegant and organic way, many of the Bayesian scientiﬁc computing
topics introduced in the earlier chapters. The evolution and observation equations
can be interpreted as prior and likelihood, and the way in which the posterior is
updated from one time step to the next resembles the natural learning process. Like-
wise, sampling methods have a more prominent position, commensurate with their
key role in many Bayesian scientiﬁc computing tasks. The rational for Monte Carlo
integration in high dimensions is an excellent example of the need for sampling from
a distribution, and is used here as a gateway to some of the most popular sampling
schemes, including the intuitive importance sampling, the classical Gibbs sampler
and Metropolis–Hastings algorithms, all the way to the clever preconditioned Crank–
Nicolson sampler.
Other signiﬁcant ways in which this book differs from [18] are in the level of detail
and notations. During the years, we have experienced that some of our shorthand
notations in the previous book led to confusion and potential ambiguities, and in
order to avoid that, some notations in the new book are arguably a bit heavier but
less prone to misinterpretations. We have tried to retain some of the lightness of the
previous book, and we hope that the style manages to elucidate the origins of the
ideas and makes the book a rewarding reading experience, and not just a source of
reference.
One thing between the two book projects is invariant: Like the old book, the new
one is also the result of a dialogue with our students. The new book corresponds to
the curriculum of the graduate course Bayesian Scientiﬁc Computing taught over the
years at Case Western Reserve University, and much of the changes and additions are

Preface
ix
a result of the questions and comments of the smart and inquisitive CWRU students
from all over the campus. Moreover, we have had the opportunity to teach parts of this
material in several other universities and research institutes, including Instituto de
Matemática Pura e Aplicada (IMPA) in Rio de Janeiro, Brazil in 2011; University of
Copenhagen, Denmark in 2014; University of Naples “Federico II” in Naples, Italy
in 2015; Basque Center for Applied Mathematics (BCAM) in Bilbao, Spain in 2015;
UniversityofWarwickinWarwick,UKin2015;UniversityofRome“LaSapienza”in
Rome, Italy in 2015; MOX at Milan Polytechnic University in Milan, Italy in 2019;
Danish Technical University in Lyngby, Denmark in 2019, as well as Gran Sasso
Science Institute (GSSI) in L’Aquila, Italy in 2021. Our warmest thanks go to Jorge
Zubelli, Sami Brandt, Gerardo Toraldo, Salvatore Cuomo, Luca Gerardo Giorda,
Andrew Stuart, Francesca Pitolli, Barbara Vantaggi, Simona Perotto, Per Christian
Hansen and Nicola Guglielmi for making these visits possible, and especially to the
fantastic groups of graduate students, postdocs and local researchers participating in
those courses, whose enthusiasm convinced us that we should continue to develop
the material and that refreshing the book was the right thing to do. We are also very
grateful to Howard Elman, whose insightful comments on a near ﬁnal draft of this
book helped improve the presentation of some topics.
If many of the topic in this book have played a central role in our research projects,
our research interests have had a strong inﬂuence on the structure of the book. We
gratefully acknowledge partial support for the work of Daniela Calvetti by the grants
NSF-DMS 1522334, NSF-DMS 1951446, Simons Fellowship in Mathematics “The
inverse problem of magnetoencephalography,” and of Erkki Somersalo by the grants
NSF-DMS 1016183, NSF-DMS 1312424 and NSF-DMS 1714617.
Cleveland, USA
June 2022
Daniela Calvetti
Erkki Somersalo

Preface to the 2007 Book Introduction to
Bayesian Scientiﬁc Computing
The book of nature, according to Galilei, is written in the language of mathematics.
The nature of mathematics is being exact, and its exactness is underlined by the
formalism used by mathematicians to write it. This formalism, characterized by theo-
rems and proofs, and syncopated with occasional lemmas, remarks and corollaries,
is so deeply ingrained that mathematicians feel uncomfortable when the pattern is
broken, to the point of giving the impression that the attitude of mathematicians
towards the way mathematics should be written is almost moralistic. There is a
deﬁnition often quoted, “A mathematician is a person who proves theorems,” and a
similar, more alchemistic one, credited to Paul Erdös, but more likely going back to
Alfréd Rényi, stating that “A mathematician is a machine that transforms coffee into
theorems.”1 Therefore it seems to be the form, not the content, that characterizes
mathematics, similarly to what happens in any formal moralistic code wherein form
takes precedence over content.
This book is deliberately written in a very different manner, without a single
theorem or proof. Since morality has its subjective component, to paraphrase Manuel
Vasquez Montalban, we could call it Ten Immoral Mathematical Recipes.2 Does the
lack of theorems and proofs mean that the book is more inaccurate than traditional
books of mathematics? Or is it possibly just a sign of lack of coffee? This is our ﬁrst
open question.
Exactness is an interesting concept. Italo Calvino, in his Lezioni Americane,3
listed exactness as one of the values that he would have wanted to take along to the
twenty-ﬁrst century. Exactness, for Calvino, meant precise linguistic expression, but
in a particular sense. To explain what he meant by exactness, he used a surprising
example of exact expression: the poetry of Giacomo Leopardi, with all its ambiguities
and suggestive images. According to Calvino, when obsessed with a formal language
1 That said, academic mathematics departments should invest on high-quality coffee beans and
decent coffee makers, in hope of better theorems. As Paul Turán, a third Hungarian mathematician,
remarked, “weak coffee is ﬁt only to produce lemmas.”
2 M. V. Montalban: Ricette immorali (orig. Las recetas inmorales, 1981), Feltrinelli, 1992.
3 I. Calvino: Lezioni Americane, Oscar Mondadori, 1988.
xi

xii
Preface to the 2007 Book Introduction to Bayesian Scientiﬁc Computing
that is void of ambiguities, one loses the capability of expressing emotions exactly,
while by liberating the language and making it vague, one creates space for the most
exact of all expressions, poetry. Thus, the exactness of expression is beyond the
language. We feel the same way about mathematics.
Mathematics is a wonderful tool to express liberally such concepts as qualitative
subjective beliefs, but by trying to formalize too strictly how to express them, we may
end up creating beautiful mathematics that has a life of its own, in its own academic
environment, but which is completely estranged to what we initially set forth. The
goal of this book is to show how to solve problems instead of proving theorems. This
mischievous and somewhat provocative statement should be understood in the spirit
of Peter Lax’ comment in an interview given on the occasion of his receiving the
2005 Abel Prize4: “When a mathematician says he has solved the problem he means
he knows the solution exists, that it’s unique, but very often not much more.” Going
through mathematical proofs is a serious piece of work: we hope that reading this
book feels less like work and more like a thought-provoking experience.
The statistical interpretation, and in particular the Bayesian point of view, plays
a central role in this book. Why is it so important to emphasize the philosoph-
ical difference between statistical and non-statistical approaches to modeling and
problem-solving? There are two compelling reasons.
The ﬁrst one is very practical: admitting the lack of information by modeling the
unknown parameters as random variables and encoding the nature of uncertainty into
probability densities gives a great freedom to develop the models without having to
worry too much about whether solutions exist or are unique. The solution in Bayesian
statistics, in fact, is not a single value of the unknowns, but a probability distribution
of possible values that always exists. Moreover, there are often pieces of qualitative
information available that simply do not yield to classical methods, but which have
a natural interpretation in the Bayesian framework.
It is often claimed, in particular by mathematician in inverse problems working
with classical regularization methods, that the Bayesian approach is yet another way
of introducing regularization into problems where the data are insufﬁcient or of low
quality, and that every prior can be replaced by an appropriately chosen penalty.
Such statement may seem correct in particular cases when limited computational
resources and lack of time force one to use the Bayesian techniques for ﬁnding a
single value, typically the maximum a posteriori estimate, but in general the claim is
wrong. The Bayesian framework, as we shall reiterate over and again in this book,
can be used to produce particular estimators that coincide with classical regularized
solutions, but the framework itself does not reduce to these solutions, and claiming
so would be an abuse of syllogism.5
The second, more compelling reason for advocating the Bayesian approach,
has to do with the interpretation of mathematical models. It is well understood,
and generally accepted, that a computational model is always a simpliﬁcation. As
4 M. Raussen and C. Skau: Interview with Peter D. Lax. Notices of the AMS 53 (2006) 223–229.
5 A classic example of similar abuse of logic can be found in elementary books of logic: while it is
true that Aristotle is a Greek, it is not true that a Greek is Aristotle.

Preface to the 2007 Book Introduction to Bayesian Scientiﬁc Computing
xiii
George E. P. Box noted, “all models are wrong, some are useful.” As computa-
tional capabilities have grown, an urge to enrich existing models with new details
has emerged. This is particularly true in areas like computational systems biology,
where the new paradigm is to study the joint effect of huge number of details6 rather
than using the reductionist approach and seeking simpliﬁed lumped models whose
behavior would be well understood. As a consequence, the computational models
contain so many model parameters that hoping to determine them based on few
observations is simply impossible. In the old paradigm, one could say that there are
some values of the model parameters that correspond in an “optimal way” to what
can be observed. The identiﬁability of a model by idealized data is a classic topic of
research in applied mathematics. From the old paradigm, we have also inherited the
faith in the power of single outputs. Given a simplistic electrophysiological model
of the heart, a physician would want to see the simulated electrocardiogram. If the
model was simple, for example, two rotating dipoles, that output would be about all
the model could produce, and no big surprises were to be expected. Likewise, given a
model for millions of neurons, the physician would want to see a simulated cerebral
response to a stimulus. But here is the big difference: the complex model, unlike the
simple dipole model, can produce a continuum of outputs corresponding to ﬁctitious
data, never measured by anybody in the past or the future. The validity of the model is
assessed according to whether the simulated output corresponds to what the physician
expects. While when modeling the heart by a few dipoles, a single simulated output
could still make sense, in the second case the situation is much more complicated.
Since the model is overparametrized, the system cannot be identiﬁed by available or
even hypothetical data and it is possible to obtain completely different outputs simply
by adjusting the parameters. This observation can lead researchers to state, in frustra-
tion, “well, you can make your model do whatever you want, so what’s the point.”7
This sense of hopelessness is exactly what the Bayesian approach seeks to remove.
Suppose that the values of the parameters in the complex model have been set so that
the simulated output is completely in conﬂict with what the experts expect. The reac-
tion to such output would be to think that the current settings of the parameters “must”
be wrong, and there would usually be unanimous consensus about the incorrectness
of the model prediction. This situation clearly demonstrates that some combinations
of the parameter values have to be excluded, and the exclusion principle is based on
the observed data (likelihood), or in lack thereof, the subjective belief of an expert
(prior). Thanks to this exclusion, the model can no longer do whatever we want,
yet we have not reduced its complexity and thereby its capacity to capture complex,
unforeseen, but possible, phenomena. By following the principle of exclusion and
subjective learned opinions, we effectively narrow down the probability distributions
of the model parameters so that the model produced plausible results. This process
6 This principle is often referred to as emergence, as new unforeseen and qualitatively different
features emerge as a sum of its parts (cf. physics →chemistry →life →intelligence). Needless to
say, this holistic principle is old, and can be traced back to ancient philosophers.
7 We have actually heard this type of statement repeatedly from people who refuse to consider the
Bayesian approach to problem-solving.

xiv
Preface to the 2007 Book Introduction to Bayesian Scientiﬁc Computing
is cumulative: when new information arrives, old information is not rejected, as is
often the case in the infamous “model ﬁtting by parameter tweaking,” but included
as prior information. This mode of building models is not only Bayesian, but also
Popperian8 in the wide sense: data is used to falsify hypotheses thus leading to the
removal of impossible events or to assigning them as unlikely, rather than to verify
hypotheses, which is in itself a dubious project. As the classic philosophic argument
goes, producing one white swan, or, for that matter, three, does not prove the theory
that all swans are white. Unfortunately, deterministic models are often used in this
way: one, or three, successful reconstructions are shown as proof of a concept.
The statistical nature of parameters in complex models serves also another
purpose. When writing a complex model for the brain, for instance, we expect that
the model is, at least to some extent, generic and representative, and thus capable of
explaining not one but a whole population of brains. To our grace, or disgrace, not
all brains are equal. Therefore, even without a reference to the subjective nature of
information, a statistical model simply admits the diversity of those obscure objects
of our modeling desires.
This book, which is based on notes for courses that we taught at Case Western
Reserve University, Helsinki University of Technology and at the University of
Udine, is a tutorial rather than an in-depth treatise in Bayesian statistics, scientiﬁc
computing and inverse problems. When compiling the bibliography, we faced the
difﬁcult decision of what to include and what to leave out. Being at the crossroad of
three mature branches of research, statistics, numerical analysis and inverse prob-
lems,wewerefacedwiththreevasthorizons,astherewerethreetimesasmanypeople
whose contributions should have been acknowledged. Since compiling a comprehen-
sive bibliography seemed a Herculean task, in the end, the Occam’s razor won and
we opted to list only the books that were suggested to our brave students, whom
we thank for feedback and comments. We also want to thank Dario Fasino for his
great hospitality during our visit to Udine, Rebecca Calvetti, Rachael Hageman and
Rossana Occhipinti for help with proofreading. The ﬁnancial support of the Finnish
Cultural Foundation for Erkki Somersalo during the completion of the manuscript is
gratefully acknowledged.
Cleveland, USA
Helsinki, Finland
May 2007
Daniela Calvetti
Erkki Somersalo
8 See A. Tarantola: Inverse problems, Popper and Bayes, Nature Physics 2 492–494, (2006).

Contents
1
Bayesian Scientiﬁc Computing and Inverse Problems . . . . . . . . . . . . .
1
1.1
What Do We Talk About When We Talk About Random
Variables? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2
1.2
Through the Formal Theory, Lightly . . . . . . . . . . . . . . . . . . . . . . . .
5
1.2.1
Elementary Probabilities . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
1.2.2
Probability Distributions and Densities . . . . . . . . . . . . . . .
7
1.2.3
Expectation and Covariance . . . . . . . . . . . . . . . . . . . . . . . .
10
1.2.4
Change of Variables in Probability Densities . . . . . . . . . .
15
2
Linear Algebra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
19
2.1
Vectors and Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
19
2.1.1
The Singular Value Decomposition . . . . . . . . . . . . . . . . . .
23
2.1.2
The Four Fundamental Subspaces . . . . . . . . . . . . . . . . . . .
24
2.2
Solving Linear Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26
2.2.1
What Is a Solution? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
27
2.2.2
Direct Linear System Solvers . . . . . . . . . . . . . . . . . . . . . . .
28
3
Continuous and Discrete Multivariate Distributions . . . . . . . . . . . . . .
35
3.1
Covariance Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
35
3.2
Normal Distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
37
3.3
How Normal is it to be Normal? . . . . . . . . . . . . . . . . . . . . . . . . . . . .
40
3.4
Discrete Distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
41
3.4.1
Normal Approximation to the Poisson Distribution . . . .
46
4
Introduction to Sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
49
4.1
On Averaging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
49
4.2
Whitening and P–P Plots . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
53
4.3
Quadratures and Law of Large Numbers . . . . . . . . . . . . . . . . . . . . .
56
4.4
Drawing from Discrete Densities . . . . . . . . . . . . . . . . . . . . . . . . . . .
61
4.5
Sampling from a One-Dimensional Continuous Density . . . . . . . .
63
4.6
Sampling from Gaussian Distributions
. . . . . . . . . . . . . . . . . . . . . .
66
4.7
Some Useful Sampling Algorithms . . . . . . . . . . . . . . . . . . . . . . . . .
67
xv

xvi
Contents
4.7.1
Importance Sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
67
4.7.2
Drawing from Mixtures: SIR and Weighted
Bootstrap . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
70
4.8
Rejection Sampling: Prelude to Metropolis–Hastings . . . . . . . . . .
74
5
The Praise of Ignorance: Randomness as Lack of Certainty . . . . . . .
81
5.1
Construction of Likelihood . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
83
5.2
Noise Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
84
5.2.1
Additive Noise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
84
5.2.2
Multiplicative Noise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
86
5.2.3
Poisson Noise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
89
5.2.4
Composite Noise Models . . . . . . . . . . . . . . . . . . . . . . . . . .
92
6
Enter Subject: Construction of Priors . . . . . . . . . . . . . . . . . . . . . . . . . . .
95
6.1
Smoothness Priors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
97
6.1.1
Freeing the Boundary Values . . . . . . . . . . . . . . . . . . . . . . .
101
6.2
Generalization to Higher Dimensions . . . . . . . . . . . . . . . . . . . . . . .
103
6.3
Whittle–Matérn Priors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
106
6.4
Smoothness Priors with Structure . . . . . . . . . . . . . . . . . . . . . . . . . . .
107
6.5
Conditionally Gaussian Priors and Hierarchical Models . . . . . . . .
109
6.6
Sparsity-Promoting Priors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
113
6.7
Kernel-Based Priors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
116
6.8
Data-Driven Priors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
120
7
Posterior Densities, Ill-Conditioning, and Classical
Regularization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
125
7.1
Likelihood Densities and Ill-Posedness of Inverse Problems . . . .
125
7.2
Maximum a Posteriori Estimate and Regularization . . . . . . . . . . .
129
8
Conditional Gaussian Densities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
135
8.1
Gaussian Conditional Densities . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
135
8.2
Linear Inverse Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
139
8.3
Interpolation and Conditional Densities . . . . . . . . . . . . . . . . . . . . . .
141
8.4
Covariance or Precision? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
144
8.5
Some Computed Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
147
8.5.1
Bayesian Interpolation of Multi-Dimensional Data . . . . .
147
8.5.2
Posterior Density by Low-Rank Updating . . . . . . . . . . . .
149
9
Iterative Linear Solvers and Priorconditioners . . . . . . . . . . . . . . . . . . .
155
9.1
Iterative Methods in Linear Algebra . . . . . . . . . . . . . . . . . . . . . . . . .
155
9.2
Krylov Subspace Iterative Methods . . . . . . . . . . . . . . . . . . . . . . . . .
156
9.2.1
Conjugate Gradient Algorithm . . . . . . . . . . . . . . . . . . . . . .
157
9.2.2
Conjugate Gradient Method for Least Squares . . . . . . . .
159
9.3
Ill-Conditioning and Errors in the Data . . . . . . . . . . . . . . . . . . . . . .
162
9.4
Iterative Solvers in the Bayesian Framework . . . . . . . . . . . . . . . . .
167
9.4.1
Preconditioning and Tikhonov Regularization . . . . . . . . .
168

Contents
xvii
9.4.2
Priorconditioners: Specially Chosen
Preconditioners . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
174
9.4.3
Stopping Rule Revisited . . . . . . . . . . . . . . . . . . . . . . . . . . .
181
10
Hierarchical Models and Bayesian Sparsity . . . . . . . . . . . . . . . . . . . . . .
183
10.1
Posterior Densities with Conditionally Gaussian Priors . . . . . . . .
183
10.1.1
IAS, Sparsity, Sensitivity Weighting
and Exchangeability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
189
10.1.2
IAS with Priorconditioned CGLS . . . . . . . . . . . . . . . . . . .
194
10.2
More General Sparse Representations . . . . . . . . . . . . . . . . . . . . . . .
196
10.3
Some Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
200
11
Sampling: The Real Thing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
211
11.1
Preliminaries: Markov Chains and Random Walks . . . . . . . . . . . .
211
11.1.1
An Introductory Example . . . . . . . . . . . . . . . . . . . . . . . . . .
211
11.1.2
Random Walks in Rn . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
216
11.2
Metropolis–Hastings Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . .
219
11.2.1
Balance and Detailed Balance Equations . . . . . . . . . . . . .
220
11.2.2
Construction of the MH Transition . . . . . . . . . . . . . . . . . .
222
11.2.3
Metropolis–Hastings in Action . . . . . . . . . . . . . . . . . . . . .
224
11.3
Gibbs Sampler . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
232
11.4
Preconditioned Crank–Nicholson . . . . . . . . . . . . . . . . . . . . . . . . . . .
237
12
Dynamic Methods and Learning from the Past . . . . . . . . . . . . . . . . . . .
243
12.1
The Dog and the Hunter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
243
12.2
Sampling Importance Resampling (SIR) . . . . . . . . . . . . . . . . . . . . .
249
12.2.1
Survival of the Fittest . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
252
12.2.2
Estimation of Static Parameters . . . . . . . . . . . . . . . . . . . . .
254
13
Bayesian Filtering for Gaussian Densities . . . . . . . . . . . . . . . . . . . . . . . .
263
13.1
Kalman Filtering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
263
13.2
The Best of Two Worlds: Ensemble Kalman Filtering . . . . . . . . . .
266
13.2.1
Adding Unknown Parameters . . . . . . . . . . . . . . . . . . . . . . .
271
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
279
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
283

Chapter 1
Bayesian Scientiﬁc Computing
and Inverse Problems
Bayesian scientiﬁc computing, as understood in this text, is a ﬁeld of applied mathe-
matics that combines numerical analysis and traditional scientiﬁc computing to solve
problems in science and engineering with the philosophy and language of Bayesian
inference. An overarching goal is to apply these techniques to solve computational
inverse problems by using probabilistic methods with efﬁcient numerical tools.
The need for scientiﬁc computing to solve inverse problems has been recognized
for a long time. The need for subjective probability, surprisingly, has been recog-
nized even longer. One of the founding fathers of the Bayesian approach to inverse
problems, in addition to reverend Thomas Bayes1 himself, is Pierre-Simon Laplace2,
who used Bayesian techniques, among other things, to estimate the mass of the planet
Saturn from astronomical observations.
The design of predictive mathematical models usually follows the principle of
causality, meaning that the model predicts the consequences of known causes. The
models are often local in the sense that changes in a quantity are described by the
behavior of the quantity itself in a local neighborhood, the proper language, therefore,
being differential calculus. In inverse problems, the goal is to infer on unknown
causes when the consequences are observed. Inverse problems often lead to non-
local models, involving typically integral equations. The classical Second Law of
Thermodynamics states that in a closed system, entropy increases, which can be
1 ThomasBayes(1702–1761),inhisremarkableAnEssaytowardssolvingaProblemintheDoctrine
of Chances that was read in the Royal Society posthumously in 1763, ﬁrst analyzed the question of
inverse probability, that is, how to determine a probability of an event from observation of outcomes.
This has now become a core question of modern science.
2 Pierre-Simon Laplace (1749–1827): French mathematician and natural scientist, one of the devel-
opers of modern probability and statistics, among other achievements. His Mémoire sur la prob-
abilité des causes par les évènemens from 1774 shows the Bayes’ formula in action as we use it
today.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
D. Calvetti and E. Somersalo, Bayesian Scientiﬁc Computing, Applied Mathematical
Sciences 215, https://doi.org/10.1007/978-3-031-23824-6_1
1

2
1
Bayesian Scientiﬁc Computing and Inverse Problems
interpreted by saying that causal processes loose information. Therefore, inverse
problems ﬁght against the loss of information.
There is an almost perfect parallelism between inverse problems and Bayesian
methods. The famous formula of Bayes is a tool to reverse the roles of two mutually
dependent probabilistic events: If the probability of one is known assuming the other,
what is the probability of the latter assuming the former? Arranging the events as
causal ones, one arrives at the core question of inverse problems. However, there is
more: The Bayesian framework gives tools to import additional information about
the unknown complementary to what is included in the forward model, thus helping
in the process of ﬁghting the loss of information. Therefore, the Bayesian framework
provides the perfect language for inverse problems.
The interplay of Bayesian methods and scientiﬁc computing is not limited to
inverse problems. Numerical methods have long been used in statistics, in particular
in what is known as computational statistics. One of our goals is to promote and
nurture a ﬂow in the opposite direction, showing how Bayesian statistics can provide
a ﬂexible and versatile framework for computational methods, in particular those
used in the solution of inverse problems. This book addresses a collection of topics
that are usually taught in numerical analysis or Bayesian statistics courses, with the
common thread of looking at them from the point of view of retrieving information
from indirect observations by effective numerical methods.
1.1
What Do We Talk About When We Talk About
Random Variables?
Intuitively, random events, and the associated probabilities, have a well-deﬁned and
clear meaning3 and yet have been the subject of extensive philosophical and technical
discussions. Although it is not the purpose of this book to plunge too deeply into this
subject matter, it is useful to review some of the central philosophical concepts and
explain the point of view taken in the rest of the exposition.
A random event is often implicitly deﬁned as the complement of a deterministic
event, in the sense that if the outcome of a deterministic event is at least theoretically
completely predictable, the outcome of a random event is not fully predictable.4
Hence, according to this interpretation, randomness is tantamount to lack of certainty.
The degree of certainty, or more generally, our belief about the outcome of a random
event, is expressed in terms of probability. Random phenomena, however, should
not be confused with chaotic phenomena, whose outcomes are deterministic but so
sensitive to any imprecision in initial values to make them effectively unpredictable.
That said, the concept of probability in a philosophical sense has a subjective
component, since deciding what is reasonable to believe, and what previous experi-
3 As Laplace put it, “probability theory is nothing more but common sense reduced to calculation.”
4 Arguably, this deﬁnition is a little murky, similarly to how what is not conscious is labeled as
being part of the subconscious.

1.1 What Do We Talk About When We Talk About Random Variables?
3
ence or information the belief is based on, is a subjective matter. In this book, unless
otherwise speciﬁed, the term probability means subjective probability, which is also
known as Bayesian probability. To better understand this concept, and to be ready to
fend off the customary objections against it, let us consider some simple examples.
Example 1.1 Consider the random event of tossing a coin. In general, it is quite
natural to assume that the odds (i.e., relative probabilities) of the complementary
events of getting heads or tails are equal. This is formally written as
P{heads} = P{tails} = 1
2,
where P stands for the probability of the event to occur.5 A justiﬁcation for this choice,
or any other assignment of the odds, it is often argued, lies in the repeatability of the
random phenomenon. After tossing the coin over and over again, N times, we may
compute the relative frequency of heads,
f (N)
heads = # heads occurring
N
.
Letting N go to inﬁnity, tacitly assuming that a limit value exists, we obtain the
asymptotic value of the frequency
fheads = lim
N→∞f (N)
heads,
which can be argued to represent the probability of the event. If this number is 1/2,
the coin is said to be fair, meaning that both outcomes are equally likely to occur.
The repetition experiment can be seen as an empirical test of the hypothesis that the
coin is fair.
The previous example is in line with the frequentist deﬁnition of probability: the
probability of an event is identiﬁed with its relative frequency of occurrence in an
asymptotically inﬁnite series of repeated experiments. The statistics based on this
view is often referred to as frequentist statistics,6 and the most common argument in
support of this interpretation is its seemingly objective and empirical nature.
The interpretation of probability as a frequency works well for phenomena
described in terms of repeated mutually independent experiments. This underly-
ing assumption excludes a good part of the common use of probability. An exam-
ple is the probability of an outcome of a sports event: If a given sports team has
played several times against another team, the next game can hardly be viewed as
an independent realization of the same random event, even though the availabil-
ity of information about previous scores may help to set the odds. Assessing the
5 We use P with curly braces when we describe an event in words.
6 Frequentist statistics is also called Fisherian statistics, according to the English statistician and
evolutionary biologist Ronald Fisher (1890–1962).

4
1
Bayesian Scientiﬁc Computing and Inverse Problems
probabilities of different outcomes requires more information, and setting the odds
for betting, for instance, is a subjective, but usually not arbitrary, process.
The following example stresses the subjective nature of probability.
Example 1.2 There is a coin about to be tossed. After a series of tests, it is agreed
that the coin to be tossed can be considered fair. Consider now the following two
experiments:
1. You are asked to predict the outcome of the next coin toss. In this case, nobody
knows whether the result will be heads or tails.
2. You are asked to guess the outcome of a coin toss that has already occurred. In
this case, the person tossing the coin checks the outcome without showing it,
and therefore knows the outcome without revealing it to you.
The perceived fairness of the coin suggests that you may guess either one of the
possible outcomes as you wish, the probabilities being one half for both outcomes.
This consideration applies to both experiments. However, there is an important philo-
sophical difference between the two settings.
In the ﬁrst case, both subjects are equally ignorant about the outcome of the coin
toss, and need to use the concept of probability to predict it. In the second case,
the tosser knows the outcome and, unlike you, has no reason to use the concept of
probability. From the tosser’s point of view, you are assigning probabilities to a fully
deterministic, and known, outcome. In a betting context, for instance, the tosser’s
advantage in the second experiment is obvious,7 but the other person’s knowledge
of the outcome does not help you at all to set up the odds.
This example motivates the following important considerations about the nature
of probability.
1. Whether the concept of probability is needed or not depends on the subject (you
versus the coin tosser), not on the object (the coin). The probability can therefore
be argued to be a property of the subject, and not the object.8
2. Probability is used when the information is perceived as incomplete. Therefore,
we may deﬁne probability as an expression of subject’s lack of certainty, or
ignorance, arising from the lack of relevant information.
3. Whether the quantity that we consider is well-deﬁned and deterministic, such as
the face of a coin lying on the table, or a realization of a random process, such as
the coin toss, the probabilistic description depends only on what we know about
the quantity.
Subjective probability expresses a subject’s belief about the outcome of an obser-
vation. Subjectivity is not always appreciated in science, which strives towards objec-
tivity, in the sense of seeking to establish statements that can be shared unequivocally
by every intelligent being. However, the concept of subjective probability is not in
7 Of course, only a fool would be accepting to bet under those conditions!
8 The Italian mathematician Bruno De Finetti (1906–1985) expressed this observation strongly by
stating that “probability does not exist.”

1.2 Through the Formal Theory, Lightly
5
conﬂict with the scientiﬁc endeavor because subjective is not the same as arbitrary,
and shared evidence tends to lead to shared beliefs. Whenever we assign probabilities
to events, we should base it on the best judgement, such as previous experiments,
well-established models, or solid reasoning. For instance, in the previous example,
assigning equal probabilities to heads and tails was based on the presumably reliable
information, and possibly on extensive experimental evidence, that the coin is fair.
We conclude the discussion with the notion of objective chance. The current
interpretation of quantum mechanics implicitly assumes the existence of objective
chance, meaning that the probabilities of events are non-judgemental. For instance,
the square of the absolute value of the solution of the Schrödinger equation describing
a particle in a potential ﬁeld deﬁnes a probability density, predicting an objective
chance of ﬁnding a particle in a given domain. However, whether the uncertainty
about the value of a quantity is fundamental or ontological, as in quantum physics,
orcontingentorepistemological,asinthelackofagoodenoughmeasurementdevice,
is of no consequence from the point of view of certainty: to predict an outcome, or
to estimate an unknown, it is not important to know why we are ignorant.9
1.2
Through the Formal Theory, Lightly
Regardless of the interpretation, the theory of probability and randomness is widely
accepted and presented in a standard form. The formalization of the theory is
attributed to Kolmogorov.10 Since our emphasis is on something besides theoret-
ical considerations, we will present a light version of the fundamentals, spiced with
examples and intuitive reasoning. A rigorous treatment of the matter can be found
in standard textbooks on probability theory, see Notes and Comments at the end of
the chapter. We begin by presenting some deﬁnitions and basic results that will be
used extensively later. We will not worry too much here about the formal conditions
necessary for the quantities introduced to be well deﬁned. In particular, questions
concerning, e.g., measurability or completeness will be neglected.
9 The Danish physicist Niels Bohr (1885–1962), who contributed signiﬁcantly to the understanding
of randomness in quantum physics, wrote: “It is wrong to think that the task of physics is to ﬁnd
out how Nature is. Physics concerns what we say about Nature.” This statement, pointing out the
difference between physics and metaphysics, is in line with the subjective probability in the sense
that the source of uncertainty is of no consequence. This philosophical viewpoint has been recently
advanced in the theory of Quantum Bayesianism, of QBism.
10 Andrej Nikolaevich Kolmogorov (1903–1987): a Russian mathematician, often referred to as
one of the most prominent ﬁgures in the twentieth-century mathematics and the father of modern
probability theory.

6
1
Bayesian Scientiﬁc Computing and Inverse Problems
1.2.1
Elementary Probabilities
The standard theoretical treatment of probability starts by postulating a triplet
(Ω, B, P), called the probability space. Here, Ω is an abstract set called the sample
space. The set B is a collection of subsets of Ω, satisfying the following conditions:
(i)
Ω ∈B,
(ii)
If A ∈B, then Ω \ A ∈B,
(iii)
If A1, A2, . . . ∈B, then ∪∞
j=1 A j ∈B.
A σ-algebra is a collection of subsets satisfying the conditions above. We call the
set B the event space, and the individual sets in it are referred to as events. The third
element in the probability space triplet is the probability measure P, a mapping
P : B →R,
P(E) = probability of E, E ⊂Ω,
that must satisfy the following conditions:
(i)
0 ≤P(E) ≤1,
E ∈B,
(ii)
P(Ω) = 1,
(iii)
if A j ∈B, j = 1, 2, 3, . . . with A j ∩Ak = ∅whenever j ̸= k, we have
P
 ∞

j=1
A j

=
∞

j=1
P(A j). (σ-additivity)
It follows from the deﬁnition that
P(Ω \ A) = 1 −P(A),
which implies that P(∅) = 0. Moreover, if A1, A2 ∈B and A1 ⊂A2 ⊂Ω,
P(A1) ≤P(A2).
Two events, A and B, are independent, if
P(A ∩B) = P(A)P(B).
The conditional probability of A given B is the probability that A happens pro-
vided that B happens,
P(A | B) = P(A ∩B)
P(B)
,
assuming that P(B) > 0.
The probability of A conditioned on B is never smaller than the probability of A ∩B,
since P(B) ≤1, and it can be much larger if P(B) is very small, however, never larger
than 1, since

1.2 Through the Formal Theory, Lightly
7
P(A | B) = P(A ∩B)
P(B)
≤P(Ω ∩B)
P(B)
= P(B)
P(B) = 1.
This corresponds to the intuitive idea that the conditional probability is a probability
on the probability subspace where the event B has already occurred. It follows from
the deﬁnition of independent events that, if A and B are mutually independent, then
P(A | B) = P(A),
P(B | A) = P(B).
Vice versa, if one of the above equalities holds, then by the deﬁnition of conditional
probabilities A and B must be independent.
Consider two events A and B, both having a positive probability, and write the
conditional probability of A given B and of B given A,
P(A | B) = P(A ∩B)
P(B)
,
P(B | A) = P(A ∩B)
P(A)
.
By solving for the probability P(A ∩B) from the former and substituting it in the
latter, we obtain
P(B | A) = P(A | B)P(B)
P(A)
.
(1.1)
This is Bayes’ formula for elementary events, which is the very heart of most of the
topic discussed in this book.
1.2.2
Probability Distributions and Densities
Given a sample space Ω, a real valued random variable X is a mapping
X : Ω →R,
which assigns to each element of Ω a real value X(ω), such that for every open
set A ⊂R, X−1(A) ∈B. The latter condition is expressed by saying that X is a
measurable function. We call x = X(ω), ω ∈Ω, a realization of X. Therefore, a
random variable is a function, while the realizations of a real-valued random variable
are real numbers.
For each B ⊂R, we deﬁne
μX(B) = P(X−1(B)) = P

X(ω) ∈B

,
and call μX the probability distribution of X, i.e., μX(B) is the probability of the
event {ω ∈Ω : X(ω) ∈B}. The probability distribution μX(B) measures the size of
the subset of Ω mapped onto B by the random variable X.

8
1
Bayesian Scientiﬁc Computing and Inverse Problems
We restrict the discussion here mostly to probability distributions that are abso-
lutely continuous with respect to the Lebesgue measure over the reals,11 meaning
that there exists a function, the probability density πX of X, such that
μX(B) =

B
πX(x)dx.
(1.2)
A function is a probability density if it satisﬁes the following two conditions:
πX(x) ≥0,

R
πX(x)dx = 1.
(1.3)
Conversely, any function satisfying (1.3) can be viewed as a probability density of
some random variable.
The cumulative distribution function (cdf) of a real-valued random variable is
deﬁned as
ΦX(x) =
 x
−∞
πX(x′)dx′ = P{X ≤x}.
The cumulative distribution function will play a very important role in performing
random draws from a distribution. Observe that ΦX is non-decreasing, and it satisﬁes
lim
x→−∞ΦX(x) = 0,
lim
x→∞ΦX(x) = 1.
The deﬁnition of random variables can be generalized to cover multidimensional
state spaces. Given two real-valued random variables X and Y, the joint probability
distribution deﬁned over Cartesian products of sets is
μX,Y(A × B) = P

X ∈A, Y ∈B

= P

X−1(A) ∩Y −1(B)

= the probability of the event that X ∈A
and, at the same time, Y ∈B,
where A, B ⊂R. Assuming that the probability distribution can be written as an
integral of the form
μX,Y(A × B) =
 
A×B
πXY(x, y)dxdy,
(1.4)
the non-negative function πXY deﬁnes the joint probability density of the random
variables X and Y. We may deﬁne a two-dimensional random variable,
11 Concepts like absolute continuity of measures will play no role in these notes, and this comment
added here is just to avoid unnecessary “friendly ﬁre” from the camp of our fellow mathematicians.

1.2 Through the Formal Theory, Lightly
9
Z =
	 X
Y

,
and by approximating general two-dimensional sets by unions of rectangles, we may
write
P

Z ∈B

=
 
B
πXY(x, y)dxdy =

B
πZ(z)dz,
(1.5)
where we used the notation πXY(x, y) = πZ(z), and the integral with respect to z is
the two-dimensional integral, dz = dxdy. More generally, we deﬁne a multivariate
random variable as a measurable mapping
X =
⎡
⎢⎣
X1
...
Xn
⎤
⎥⎦: Ω →Rn,
where each component Xi is an R-valued random variable. The probability density
of X is the joint probability density
πX = πX1X2···Xn : Rn →R+
of its components, satisfying
P

X ∈B

= μX(B) =

B
πX(x)dx,
B ⊂Rn.
Consider two multivariate random variables X : Ω →Rn and Y : Ω →Rm. Their
joint probability density πXY is deﬁned in the space Rn+m similarly to (1.4), choosing
ﬁrst A ⊂Rn and B ⊂Rm, and then, more generally, extending the deﬁnition to all
sets in Rn+m as in (1.5). In the sequel, we will use the shorthand notation X ∈Rn to
indicate that X is an n-variate random variable, that is, its values are in Rn.
The random variables X ∈Rn and Y ∈Rm are independent if
πXY(x, y) = πX(x)πY(y),
(1.6)
in agreement with the deﬁnition of independent events. This formula gives us also a
way to calculate the joint probability density of two independent random variables.
Given two not necessarily independent random variables X ∈Rn and Y ∈Rm
with joint probability density πXY(x, y), the marginal density of X is the probability
of X when Y may take on any value,
πX(x) =

Rm πXY(x, y)dy.

10
1
Bayesian Scientiﬁc Computing and Inverse Problems
In other words, the marginal density of X is simply the probability density of X
without any thoughts about Y. The marginal of Y is deﬁned analogously by the
formula
πY(y) =

Rn πXY(x, y)dx.
(1.7)
Observe that, in general, (1.6) does not hold for marginal densities.
An important concept in Bayesian scientiﬁc computing is that of conditioning.
Consider formula (1.7), and assume that πY(y) ̸= 0. Dividing both sides by the scalar
πY(y) gives the identity

Rn
πXY(x, y)
πY(y)
dx = 1.
Since the integrand is a non-negative function, it deﬁnes a probability density for X,
for ﬁxed y. We deﬁne the conditional probability density of X given Y,
πX|Y(x | y) = πXY(x, y)
πY(y)
,
πY(y) ̸= 0.
With some caution, and in a rather cavalier way, one can interpret πX|Y as the prob-
ability density of X, assuming that the random variable Y takes on the value Y = y.
The conditional density of Y given X is deﬁned similarly as
πY|X(y | x) = πXY(x, y)
πX(x)
,
πX(x) ̸= 0.
Observe that the symmetric roles of X and Y imply that
πXY(x, y) = πX|Y(x | y)πY(y) = πY|X(y | x)πX(x),
(1.8)
leading to the important identity known as Bayes’ formula for probability densities,
πX|Y(x | y) = πY|X(y | x)πX(x)
πY(y)
.
(1.9)
This identity will play a central role in the rest of the book.
Graphical interpretation of the marginal and conditional densities are presented
in Fig. 1.1.
1.2.3
Expectation and Covariance
Given a random variable X ∈Rn with probability density πX, and a function f :
Rn →R, we deﬁne the expectation of f (X) as

1.2 Through the Formal Theory, Lightly
11
0
1
2
3
4
5
0
1
2
3
4
5
0
1
2
3
4
5
0
1
2
3
4
5
Fig. 1.1
Conditional density (left) and marginal density (right) visualized for two real-valued
random variables X and Y. The ellipsoids in the top panels are equiprobability curves of the joint
probability density πXY (x, y). The lower panels show 3D renditions of the conditional (left) and
marginal densities (right)
E

f (X)

=

Rn f (x)πX(x)dx,
assuming that the integral converges. Intuitively, the expectation can be thought of
as the value that one could expect the quantity f (X) to take based on the knowledge
of the probability density. In the following, we consider some important and useful
expectations.
Given a random variable X ∈R, its expected value, or mean value, is the center
of mass of the probability distribution deﬁned as
E

X

= x =

R
x πX(x)dx ∈R,
assuming that the integral is convergent. The variance of the random variable X is
the expectation of the squared deviation from the expectation,

12
1
Bayesian Scientiﬁc Computing and Inverse Problems
var

X

= E

(X −x)2
= σ 2
X =

R
(x −x)2 πX(x)dx,
(1.10)
provided that the integral is ﬁnite. It is the role of the variance to measure how distant
from the expectation the values taken on by the random variable are. A small variance
means that the random variable takes on mostly values close to the expectation, while
a large variance means the opposite. The square root σX of the variance is the standard
deviation of X.
The expectation and the variance are also known as the ﬁrst two moments of a
probability density function. The kth moment is deﬁned as
E

(X −x)k
=

R
(x −x)k πX(x)dx,
assuming that the integral converges. The third moment (k = 3) is related to the
skewness of the probability density, deﬁned as
skew

X

= E

(X −x)3
σ 3
X
,
and the fourth (k = 4) is related to the kurtosis,
kurt

X

= E

(X −x)4
σ 4
X
.
The mean and the variance lend themselves to an immediate generalization to mul-
tivariate random variables. Given X ∈Rn, the mean of X is the vector in Rn,
x =

Rn x πX(x)dx =
⎡
⎢⎣
x1
...
xn
⎤
⎥⎦∈Rn,
or, component-wise,
x j =

Rn x j πX(x)dx ∈R,
1 ≤j ≤n.
We may calculate the variance of each component in a straightforward manner, by
deﬁning
var

X j

=

Rn(x j −x j)2 πX(x)dx,
1 ≤j ≤n.
(1.11)
A more versatile quantity that encompasses the above deﬁnition while also account-
ing for the higher dimensionality of the random variable is the covariance matrix.
By deﬁnition, the covariance is an n × n matrix with elements

1.2 Through the Formal Theory, Lightly
13
cov(X)i j =

Rn(xi −xi)(x j −x j) πX(x)dx ∈R,
1 ≤i, j ≤n.
Alternatively, we can deﬁne the covariance using vector notation as
cov(X) =

Rn(x −x)(x −x)T πX(x)dx ∈Rn×n.
Observe that the variances of the components given by (1.11) are the diagonal entries
of the covariance matrix.
The expectation of functions depending on only one component of a multivariate
random variable can be calculated by using the corresponding marginal density.
Consider a function f (Xi) of the ith component of X. Denoting by x′
i ∈Rn−1 the
vector obtained by deleting the ith component from x,
E

f (Xi)

=

Rn f (xi) πX(x)dx
=

R
f (xi)
 
Rn−1 πX(xi, x′
i)dx′
i




=πXi (xi)
dxi
=

R
f (xi) πXi(xi)dxi.
An immediate consequence of this observation is that, by letting f (xi) = xi, we may
write the mean and the diagonal entries of the covariance matrix as
xi =

R
xi πXi(xi)dxi,
var

Xi

= cov

X

ii =

R
(xi −xi)2 πXi(xi)dxi.
The properties of the covariance matrix will be discussed in more detail in the next
chapter after the introduction of some tools from linear algebra.
The deﬁnition of conditional density leads naturally to the concept of conditional
expectation or conditional mean. Given two random variables X ∈Rn and Y ∈Rm,
we deﬁne
E

X | y

=

Rn x πX|Y(x | y)dx.
To compute the expectation of X via its conditional expectation, observe that
E

X

=

Rn x πX(x)dx =

Rn x
 
Rm πXY(x, y)dy

dx,

14
1
Bayesian Scientiﬁc Computing and Inverse Problems
and, substituting (1.8) into this expression, we obtain
E

X

=

Rn x
 
Rm πX|Y(x | y)πY(y)dy

dx
(1.12)
=

Rm
 
Rn xπX|Y(x | y)dx

πY(y)dy =

Rm E

X | y

πY(y)dy.
We conclude this section with some examples.
Example 1.3 The expectation of a random variable, in spite of its name, is not
always a value that we can expect a realization takes on. Whether this is the case or
not, it depends on the distribution of the random variable. To clarify what we mean,
consider two real-valued random variables X1 and X2 with probability densities
πX1(x) =
1, |x| < 1/2,
0, |x| ≥1/2,
(1.13)
and
πX2(x) =
⎧
⎨
⎩
1, |x −1| < 1/4,
1, |x + 1| < 1/4,
0, elsewhere.
It is straightforward to check that
E

X1

= E

X2

= 0,
and that, while the expected value is a possible value for X1, this is not the case
for any realization of X2. This simple consideration serves as a warning against the
danger of describing a random variable only through its expectation.
Example 1.4 Random variables waiting for the train: Assume that every day, except
on Sundays, a train for your destination leaves every S minutes from the station. On
Sundays, the interval between trains is 2S minutes. You arrive at the station with no
information about the trains’ timetable. What is your expected waiting time?
To address the problem, we deﬁne two integer-valued random variables: W is the
time variable measured in units of days, 0 ≤W ≤7, so that, for instance, the event
{0 ≤W < 1} is Monday. The second random variable T indicates the waiting time.
The conditional distribution of T given W is then
πT |W(t | w) = 1
S χS(t),
χS(t) =
1, 0 ≤t < S,
0 otherwise
if 0 ≤w < 6,
(1.14)
and
πT |W(t | w) = 1
2S χ2S(t),
if 6 ≤w < 7.
(1.15)

1.2 Through the Formal Theory, Lightly
15
If you are absolutely sure that it is not Sunday, you can express your belief about the
waiting time using the conditional density (1.14), leading to the expected waiting
time
E

T | w

=

R
t πT |W(t | w) dt = 1
S
 S
0
t dt = S
2 ,
0 ≤w < 6.
The expected waiting time with certainty that it is Sunday is
E

T | w

=

R
t πT |W(t | w) dt = 1
2S
 2S
0
t dt = S,
6 ≤w < 7.
If you have no idea which day of the week it is,12 you may give equal probability to
each weekday, deﬁning the probability density of W as
πW(w) = 1
7,
0 ≤w ≤7.
The expectation of T can be computed according to (1.12), yielding
E

T

=

R
E(T | w)πW(w)dw
= 1
7
 6
0
E(T | w)dw + 1
7
 7
6
E(T | w)dw
= 1
7
 6
0
S
2 dw + 1
7
 7
6
S dw = 4
7 S.
1.2.4
Change of Variables in Probability Densities
It is not uncommon that we are interested in the probability density of a random
variable that is a function of another random variable with known probability density.
Therefore, we need to perform a change of variables, keeping in mind that we are
working with probability densities. We start by solving the problem in the one-
dimensional case, then consider the general multivariate case.
Assume that we have two real-valued random variables X, Z that are related to
each other through a functional relation
X = φ(Z),
12 The probability of this to happen seems to be much larger for mathematicians than for other
individuals.

16
1
Bayesian Scientiﬁc Computing and Inverse Problems
where φ : R →R is a one-to-one mapping. For simplicity, assume that φ is strictly
increasing and differentiable, so that φ′(z) > 0. If the probability density function
πX of X is given, what is the corresponding density πZ of Z?
First, note that since φ is increasing, for any values a < b, we have
a < Z < b if and only if a′ = φ(a) < φ(Z) = X < φ(b) = b′,
therefore
P{a′ < X < b′} = P{a < Z < b}.
Equivalently, the probability density of Z satisﬁes
 b
a
πZ(z)dz =
 b′
a′ πX(x)dx.
Performing a change of variables in the integral on the right,
x = φ(z),
dx = dφ
dz (z)dz,
we obtain
 b
a
πZ(z)dz =
 b
a
πX(φ(z))dφ
dz (z)dz.
This holds for all a and b, and therefore we arrive at the conclusion that
πZ(z) = πX(φ(z))dφ
dz (z).
In the derivation above, we assumed that φ was increasing. If it is decreasing,
the derivative is negative. In general, since the density needs to be non-negative, we
write
πZ(z) = πX(φ(z))

dφ
dz (z)
 .
The above reasoning for one-dimensional random variables can be extended to
multivariate random variables as follows. Let X ∈Rn and Z ∈Rn be two random
variables such that
X = φ(Z),
where φ : Rn →Rn is a one-to-one differentiable mapping. Consider a set B ⊂Rn,
and let B′ = φ(B) ⊂Rn be its image in the mapping φ. Then we may write

B
πZ(z)dz =

B′ πX(x)dx.

1.2 Through the Formal Theory, Lightly
17
We perform the change of variables x = φ(z) in the latter integral, remembering that
dx =
det

Dφ(z)
 dz,
where Dφ(x) is the Jacobian of the mapping φ,
Dφ(z) =
⎡
⎢⎣
∂φ1
∂z1 · · · ∂φ1
∂zn
...
...
∂φn
∂z1 · · · ∂φn
∂zn
⎤
⎥⎦∈Rn×n,
and its determinant, the Jacobian determinant, expresses the local volume scaling
of the mapping φ. Occasionally, the Jacobian determinant is written in a suggestive
form to make it formally similar to the one-dimensional equivalent,
∂φ
∂z = det

Dφ(z)

.
With this notation,

B
πZ(z)dz =

B′ πX(x)dx =

B
πX(φ(z))

∂φ
∂z
 dz
for all B ⊂Rn, and we arrive at the conclusion that
πZ(z) = πX

φ(z)
 
∂φ
∂z
 .
This is the change of variables formula for probability densities.
Notes and Comments
The reference to reverend Thomas Bayes’ classical essay is [3]. The Welsh philoso-
pher and mathematician Richard Price had an important role in editing and discussing
Bayes’ work. The motivations of Thomas Bayes to get engaged in the kind of math-
ematics and statistics that led him to the concept of inverse probability are not fully
known, some authors believing that Bayes was interested in refuting arguments for
not believing in miracles advocated by philosophers like David Hume. An English
translation of Laplace’s Memoir mentioned in this chapter can be found in the refer-
ence [70].
For a groundbreaking article promoting the Bayesian methods in modern inverse
problems, see [75]. The books [18, 49, 74] develop the theory of computational
inverse problems further, discussing several of the themes of this book.

18
1
Bayesian Scientiﬁc Computing and Inverse Problems
An inspirational reference for the topic of subjective probability is [46]. The cita-
tion of de Finetti claiming that probability does not exist can be found in his own book
on probability [26]. We also encourage to read some of the discussion concerning
objectivity and Bayesian philosophy, see, e.g., [4] and the related commentaries.
A good standard reference for probability theory is [5]. For a more elementary but
thorough introduction to probability theory, we refer to [1]. An extensive discussion
concerning the intricacies of deﬁning conditional distributions can be found in the
monograph [36].

Chapter 2
Linear Algebra
There is no question that linear algebra is one of the cornerstones of scientiﬁc com-
puting. Moreover, linear algebra provides an effective language for dealing with
multidimensional phenomena, including multivariate statistics that without this lan-
guage would become awkward and cumbersome. Instead of collecting all the linear
algebra deﬁnitions and results that will be needed in a comprehensive primer, we
introduce them gradually throughout the exposition, so as not to overwhelm the
reader and to present examples of how these ideas are utilized. That said, for the
beneﬁts of readers with less traditional mathematical and statistical background, in
this chapter, we collect some of the most foundational linear algebra deﬁnitions and
results that will be used extensively in the remaining chapters.
2.1
Vectors and Matrices
In this book, we restrict the discussion of linear algebra to real vectors and matrices.
Given x in Rn with Cartesian coordinates (x1, x2, . . . , xn), we identify the point with
its position vector, interpreted as a column vector
x =
⎡
⎢⎣
x1
...
xn
⎤
⎥⎦.
The corresponding “row vector” is obtained by ordering the coordinates in a row, an
operation that is a special instance of the transposition to be formally deﬁned later,
xT =
 x1 · · · xn
	
.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
D. Calvetti and E. Somersalo, Bayesian Scientiﬁc Computing, Applied Mathematical
Sciences 215, https://doi.org/10.1007/978-3-031-23824-6_2
19

20
2
Linear Algebra
The addition of two vectors of same size1 is deﬁned component-wise, as is the
operation of scalar multiplication, where all components of a vector are multiplied
by the same scalar. A linear combination of a set of vectors is the sum of scalar
multiples of the vectors.
The inner products of two vectors x, y ∈Rn is the scalar
n

j=1
x j y j = xTy = yTx,
where
x =
⎡
⎢⎣
x1
...
xn
⎤
⎥⎦,
y =
⎡
⎢⎣
y1
...
yn
⎤
⎥⎦.
This deﬁnition of inner product is related to the Euclidian length, or ℓ2-norm of the
vector deﬁned through its square as
∥x∥2 =
n

j=1
x2
j = xTx.
Two vectors x, y ∈Rn are orthogonal with respect to the inner product deﬁned above
if and only if
xTy = 0,
a deﬁnition that generalizes the notion of orthogonality, or perpendicularity of vectors
in R2 and R3.
Matrices are two-dimensional arrays of real numbers organized in rows and
columns,
A =
⎡
⎢⎣
a11 · · · a1n
...
...
am1 · · · amn
⎤
⎥⎦∈Rm×n.
(2.1)
The matrix–vector product of a matrix A ∈Rm×n and a vector x ∈Rn is the vector
Ax ∈Rm
with entries
(Ax)k =
n

j=1
akjx j,
1 ≤k ≤m.
By regarding the matrix A as the collection of its column vectors,
A =
⎡
⎣
|
|
|
a1 a2 · · · an
|
|
|
⎤
⎦,
a j ∈Rm,
1 In this case, the term size refers to the number of components and to their ordering.

2.1 Vectors and Matrices
21
the matrix–vector product can be viewed as the linear combination of the columns
of the matrix, with the entries of the vector as coefﬁcients,
Ax =
n

j=1
x ja j.
(2.2)
The column-wise interpretation of a matrix allows us to generalize the matrix–
vector product to a matrix–matrix product. Given A ∈Rm×n, B ∈Rn×k, with
B =
 b1 b2 · · · bk
	
,
b j ∈Rn,
the product of the two matrices A and B is
C = AB =
 Ab1 Ab2 · · · Abk
	
∈Rm×k.
It can be veriﬁed in a straightforward way that this column-centric deﬁnition of
matrix–matrix product coincides with the traditional component-centric deﬁnition
in terms of the entries of the two matrices,
C = AB ∈Rm×k,
ci j =
n

ℓ=1
aiℓbℓj,
1 ≤i ≤m,
1 ≤j ≤k.
We can use this deﬁnition, together with the interpretation of vectors as special
matrices, to deﬁne the outer product of two vectors x ∈Rm, y ∈Rn, not necessarily
of the same size, as
xyT =
⎡
⎢⎣
x1
...
xm
⎤
⎥⎦
 y1 · · · yn
	
=
⎡
⎢⎣
x1y1 · · · x1yn
...
...
xm y1 · · · xm yn
⎤
⎥⎦∈Rm×n.
The transposition of a matrix is the operation that swaps the rows and columns of
the matrix. For example, if A ∈Rm×n is as in (2.1), the transpose of A is
AT =
⎡
⎢⎣
a11 · · · am1
...
...
a1n · · · amn
⎤
⎥⎦∈Rn×m.
An important property of the transpose which is straightforward to verify is that it is
distributive over the product, provided that we reverse the order of the factors,

AB
T = BTAT.

22
2
Linear Algebra
A central concept in linear algebra is that of linear independence. The k vectors
a1, a2, . . . , ak ∈Rn are linearly independent if and only if the only way to get the
zero vector as their linear combination is if all coefﬁcients are equal to zero,
c1a1 + c2a2 + . . . + ckak = 0 ⇔c1 = c2 = . . . = ck = 0.
(2.3)
If a set of vectors are linearly independent, it is not possible to express any one of
them as a linear combination of the others. The linear independence of a set of vectors
can be expressed concisely in matrix notation. Letting
A =
a1 a2 · · · ak
	
∈Rn×k,
c =
⎡
⎢⎣
c1
...
ck
⎤
⎥⎦,
condition (2.3) can be formulated as
Ac = 0n ⇔c = 0k,
where 0n (0k) is the vector in Rn (Rk) with all zero entries. In the sequel, the dimension
of a null vector is usually not indicated.
Given k vectors a1, . . . ak, the subspace spanned by these vectors is deﬁned as the
set of all their linear combinations,
H = span

a1, . . . , ak

=

x ∈Rn |x = c1a1 + . . . ckak
for some c1, . . . , ck ∈R

,
and the vectors a1, . . . , ak are called a spanning set of H. If the vectors a1, . . . , ak
are a spanning set of H and they are linearly independent, they form a basis of H.
If a vector space has a basis with a ﬁnite number of elements, every basis has the
same number of elements. The dimension of a subspace H ⊂Rn is the maximum
number of independent vectors that span the subspace H, or equivalently, the number
of elements in a basis.
The identity matrix of size n is an n × n matrix with zero off-diagonal entries and
ones in the diagonal,
In =
⎡
⎢⎣
1
...
1
⎤
⎥⎦.
A matrix A ∈Rn×n is invertible if there exists a matrix A−1 ∈Rn×n, called the inverse
of A, such that
A−1A = AA−1 = In =
⎡
⎢⎣
1
...
1
⎤
⎥⎦,

2.1 Vectors and Matrices
23
If the columns of an n × n matrix U are mutually orthogonal vectors and have
Euclidean length one, then U is invertible and U−1 = UT. In this case, we say that U
is an orthogonal matrix. The orthogonality of the matrix is often expressed through
the equations
UTU = UUT = In.
2.1.1
The Singular Value Decomposition
The singular value decomposition (SVD) is among the most powerful and versatile
matrix factorizations, and the method of choice to understand the properties of matri-
ces, whether considered on their own, or as operators that multiply vectors. Every
matrix A ∈Rm×n can be expressed as the product of three matrices,
A = UDVT,
where U ∈Rm×m and V ∈Rn×n are orthogonal matrices, that is, U−1 = UT, V−1 =
VT, and D ∈Rm×n is a diagonal matrix with nonnegative entries. In particular, when
n ̸= m,
D =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
d1
d2
...
dn
O(m−n)×n
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
, if m > n,
and
D =
⎡
⎢⎢⎢⎣
d1
d2
...
Om×(n−m)
dm
⎤
⎥⎥⎥⎦, if m < n,
where Ok×ℓis a null matrix of the indicated size. We write compactly,
D = diag

d1, d2, . . . , dmin(m,n)

,
d j ≥0,
when the dimensions of the matrix are understood by the context. By convention, the
diagonal entries d j ≥0, called the singular values of A, appear in decreasing order.
The ratio of the largest to the smallest positive singular value is the condition number
of the matrix.
The power of the SVD as an analytical tool comes from the fact that it makes it
possible to not only diagonalize every matrix, but to ensure that the diagonal entries
are all real and nonnegative, thus amenable to concrete interpretations. Moreover,

24
2
Linear Algebra
because of the orthogonality of the matrices U and V, many of the properties of the
rows and columns of A, regarded as vectors in the appropriate spaces, can be deduced
from the matrix D.
In the next subsection, we deﬁne four important spaces of vectors associated with
a matrix A, and we establish a relation between their dimensions and the singular
values of the matrix.
2.1.2
The Four Fundamental Subspaces
The analysis of the mapping properties of a matrix can be understood geometrically
by introducing certain associated subspaces.
Given a matrix A ∈Rm×n, we introduce the following two important subspaces:
(i)
The null space of A, denoted by N(A), deﬁned as
N(A) =

x ∈Rn | Ax = 0

.
(ii)
The range of A, denoted by R(A), deﬁned as
R(A) =

y ∈Rm | y = Ax for some x ∈Rn
.
The interpretation (2.2) of matrix–vector products provides another useful char-
acterization of R(A). If a1, . . . , an are the columns of A, then
R(A) = span

a1, . . . , an

= the set of all
linear combinations of columns of A.
It can be shown that if the matrix A has r nonzero singular values, then
R(A) = span

u1, . . . , ur

,
where u1, . . . , ur are the ﬁrst r columns of U, and
N(A) = span

vr+1, . . . , vn

,
where vr+1, . . . , vn are the last n −r columns of V.
The importance of these two subspaces becomes clear when we consider the
solvability of a linear system of the form
Ax = b,
A ∈Rm×n.
(2.4)

2.1 Vectors and Matrices
25
It follows from the deﬁnitions of null space and range that:
1. If N(A) ̸= {0}, the solution of (2.4), if it exists, is non-unique: If x∗∈Rn is a
solution and 0 ̸= x0 ∈N(A), then
A(x∗+ x0) = Ax∗+ Ax0 = Ax∗= b,
showing that x∗+ x0 is also a solution.
2. If R(A) ̸= Rm, the linear equation (2.4) does not always have a solution. In fact,
if b /∈R(A), then, by deﬁnition, there is no x ∈Rn such that (2.4) holds.
We are now ready to introduce two more subspaces associated with A:
(iii)
The null space of AT, denoted by N

AT
, deﬁned as
N

AT
=

x ∈Rm | ATx = 0

.
(iv)
The range of AT, denoted by R

AT
, deﬁned as
R

AT
=

y ∈Rn | y = ATx for some x ∈Rm
.
It can be shown that
R(AT) = span

v1, . . . , vr

,
where v1, . . . , vr are the ﬁrst r columns of V, and
N(AT) = span

ur+1, . . . , um

,
where ur+1, . . . , um are the last m −r columns of U.
To illustrate the signiﬁcance of these two subspaces, let us take a closer look at
the null space. If x ∈N(A), then, by deﬁnition, Ax = 0. Moreover, for any y ∈Rm,
we have
0 = yTAx =

yTA

x =

ATy
Tx for all y ∈Rm.
Observing that vectors of the form ATy represent all possible vectors in the sub-
space R

AT
⊂Rn, we conclude that every vector in the null space N(A) must be
orthogonal to every vector in R

AT
. It is customary to write
N(A) ⊥R

AT
.
Likewise, by interchanging the roles of A and AT above, we arrive at the conclusion
that
N

AT
⊥R(A).
In fact, it can be shown that every vector orthogonal to R(AT) is in N(A), which is
expressed by writing

26
2
Linear Algebra
N(A) = R(AT)⊥
=

x ∈Rn | x ⊥z for every z ∈R(AT)

= orthocomplement of R(AT).
The subspaces R(A), N(A), R(AT), N(AT) are commonly referred to as the
four fundamental subspaces of A. It is possible to decompose Rn in terms of two
orthogonal subspaces, so that every x ∈Rn admits a unique representation
x = u + v,
u ∈N(A),
v ∈R

AT
.
In this case, we say that Rn is the direct sum of N(A) and R

AT
, and we write
Rn = N(A) ⊕R

AT
.
(2.5)
Not surprisingly, an analogous result holds for Rm,
Rm = N

AT
⊕R(A).
(2.6)
The rank of a matrix is the maximum number of linearly independent columns of
A or, equivalently, the dimension of the range of A, equaling the number of nonzero
singular values of A.
While not immediately obvious from the deﬁnition of the four fundamental sub-
spaces, the connection with the SVD can be used to show that the maximum number
of independent columns in A and AT is the same, therefore
rank

AT
= dim

R

AT
= dim

R(A)

= rank(A).
This is often stated by saying that the column rank and the row rank of a matrix must
coincide. This important result, together with (2.5) and (2.6), implies that in order
for the dimensions to match, we must have
dim(N(A)) = n −rank(A),
dim(N

AT
) = m −rank

AT
.
These results will be very handy when analyzing certain linear systems arising in
the computations.
2.2
Solving Linear Systems
A core operation in scientiﬁc computing is the numerical solution of linear systems
of the form (2.4). There are two basically different approaches, direct methods that
depend on matrix factorizations, and iterative solvers that generate a sequence of
approximations converging towards the solution. The concept of a solution of a linear

2.2 Solving Linear Systems
27
system is not immediately clear; therefore, we begin by clarifying the meaning of a
solution.
2.2.1
What Is a Solution?
If the matrix A in (2.4) is square and invertible, then for any b ∈Rm, the solution
exists, is unique and we can write it formally as
x = A−1b.
(2.7)
As soon as the matrix A fails to be invertible, the question of what is meant with
solution becomes very pertinent, and the answer less categorical and obvious.
In general, if the linear system (2.4) is not square, formula (2.7) is meaningless,
and the word “solution” may have a different meaning depending on the values of m
and n, and on the properties of A and b. More speciﬁcally,
1. if n > m, the problem is underdetermined, i.e., there are more unknowns than
linear equations to determine them;
2. if n = m, the problem is formally determined, i.e., the number of unknowns equals
the number of equations;
3. if n < m, the problem is overdetermined, i.e., the number of equations exceeds
that of the unknowns.
The meaning of the word “solution” differs in each of these cases, and what that
implies is illustrated graphically in Fig. 2.1.
If m < n, the number of unknowns exceeds the number of equations. Since,
in general, we can expect to be able to determine at most as many unknowns as
linear equations, this means that at least n −m components will not be determined
withoutadditionalinformationaboutthesolution.Onewaytoovercometheproblems
Fig. 2.1
Meaning of a solution. Left: minimum norm solution of an underdetermined system
(n = 2, m = 1). Middle: exact solution of a non-singular system (n = m = 2). Right: least squares
solution of an overdetermined system (n = 2, m = 3)

28
2
Linear Algebra
is to assign the values of n −m of the unknowns, thus reducing the number of
the unknowns so as to match the number of linear equations, in which case the
problem is reduced to the solution of a formally determined linear system. This
approach will work best if there is a reasonable criterion to selected the values of the
unknowns. Another popular approach, that produces what is known as the minimum
norm solution, is to select, among all possible solutions, the one that is closest to the
origin in the Euclidean sense, i.e.,
xMN = arg min

∥x∥| Ax = b

.
If m > n, the number of linear equations exceeds the number of unknowns; there-
fore, we cannot expect to ﬁnd any x such that Ax = b. In this case, we deﬁne a least
squares solution xLS to be any vector that satisﬁes
∥b −AxLS∥2 = min ∥b −Ax∥2,
(2.8)
where ∥· ∥is the Euclidean vector norm. In general, the least squares solution may
be nonunique. In this case, the minimum norm solution can be generalized to mean
the least squares solution with minimum norm. The error in the ﬁt of the data, given
by
r = b −AxLS,
is called the residual error or discrepancy associated with xLS.
In the case m = n, when there are as many linear equations as unknowns, there
is guarantee that a unique solution exists for any choice of b only if the matrix A
is invertible; otherwise, the solution of the linear systems will have to address the
non-unicity and possibly non-existence of the solution. If some of the equations are
redundant, i.e., they can be obtained as linear combinations of the other equations,
they can be eliminated from the system, leading to an underdetermined system.
The choice of an algorithm for solving (2.4) often depends on several factors,
the size of the problem being important, but not the only consideration. Algorithms
for solving (2.4) can be broadly divided into the two classes of direct methods and
iterative methods. In this chapter, we only provide a brief review of some direct
methods.IterativesolversthatplayanimportantroleinBayesianscientiﬁccomputing
will be presented in Chap.9.
2.2.2
Direct Linear System Solvers
Directmethodssolve(2.4)byﬁrstfactorizingthematrixA intotheproductofmatrices
with special properties, then solving a sequence of simpler linear systems. Typically
the factorization of the matrix A requires

2.2 Solving Linear Systems
29
(a) the availability of the matrix in explicit form;
(b) memory allocation for the matrix A and its factors;
(c) a number of ﬂoating point operations proportional to the cube of the dimension
of the problem.
For these reasons, direct methods are most suitable for linear systems of small or
medium dimensions, or for matrices with sparse representation. More importantly,
for our scope, direct methods are of the “all or nothing” kind, in the sense that if
the solution process is not completed, no partial information about the solution is
retained. We are not going to present a systematic treatment of direct methods, a
topic that is addressed with the due attention in standard linear algebra textbooks,
but instead, we summarize the main algorithmic points through some examples.
Example 2.1 If (2.4) is a square system, i.e., m = n, a standard way to proceed to
is to ﬁrst factor the matrix into a product of a lower and an upper triangular matrix,
and then to solve two triangular linear systems by back and forward substitution. A
popular algorithm for performing the matrix factorization is Gaussian elimination,
producing the LU decomposition of A,
A = LU,
where L is a lower triangular matrix with ones on the diagonal, and U is an upper
triangular matrix,
L =
⎡
⎢⎢⎢⎣
1
∗1
...
...
∗· · · ∗1
⎤
⎥⎥⎥⎦,
U =
⎡
⎢⎢⎢⎣
∗∗· · · ∗
∗
∗
... ...
∗
⎤
⎥⎥⎥⎦,
the asterisks representing possibly non-vanishing elements. We point out that in
actual computations, row permutations of the matrix A may be needed to guarantee
that the algorithm does not break down, see Notes and Comments at the end of the
chapter. With the LU decomposition at hand, the original system is solved in two
phases,
Ly = b,
Ux = y.
The ﬁrst linear system is always solvable, while the solvability of the second one
depends on the diagonal elements of U. If the diagonal of U contains zeros, U is
singular and so is A. The LU decomposition is a built-in Matlab function, with the
following syntax:
[L,U] = lu(A);
A square matrices A ∈Rn×n is symmetric positive semi-deﬁnite if it is symmetric,
i.e., A = AT and

30
2
Linear Algebra
vTAv ≥0 for every v ∈Rn.
A symmetric matrix A that satisﬁes the stronger condition
vTAv > 0 for every v ∈Rn, v ̸= 0.
is
symmetric
positive
deﬁnite
(SPD).
Symmetric
positive
deﬁnite—and
semi-deﬁnite—matrices play an important role in statistics.
While the symmetry of a matrix is immediate to verify, checking for positive
deﬁniteness based on the deﬁnition above is all but straightforward.
Consider the special case, in which the symmetric matrix A admits a factorization
of the form
A = RTR,
R ∈Rn×n.
(2.9)
Then for any vector v ∈Rn,
vTAv =

vTRT
Rv

= (Rv)T(Rv) = ∥Rv∥2 ≥0,
proving that the matrix is automatically symmetric positive semi-deﬁnite. If, in addi-
tion,
vTAv = ∥Rv∥2 = 0 if and only if v = 0,
which is equivalent to having N(R) = {0}, and therefore R being invertible, the
matrix A is SPD.
We have seen that all symmetric square matrices that admit a factorization (2.9)
with R invertible are SPD. Conversely, it turns out that every SPD matrix admits a fac-
torization of the form (2.9) with R invertible. One can prove that a symmetric matrix
A ∈Rn×n is positive deﬁnite if and only if there exists a symmetric decomposition
(2.9), where the matrix R ∈Rn×n is upper triangular,
R =
⎡
⎢⎢⎢⎣
r11 r12 · · · r1n
r22
r2n
...
...
rnn
⎤
⎥⎥⎥⎦,
the empty slots indicating zero entries, and, moreover, r j j > 0 for all j, 1 ≤j ≤n.
The matrix decomposition (2.9) where R is an invertible upper triangular matrix is
the Cholesky factorization of A. Because of the easy availability of efﬁcient numerical
methods to compute the Cholesky factorization, often the most efﬁcient way of
ﬁguring out whether a symmetric matrix is positive deﬁnite amounts to testing if the
Cholesky factorization algorithm is successful.
The direct method of choice for solving a linear system of equations with an SPD
matrix A is to compute the Cholesky factor R, and then solve the two linear systems

2.2 Solving Linear Systems
31
RTy = b,
Rx = y.
This requires only one half of the memory allocation needed for LU factorization,
and the number of ﬂoating point operations is also reduced to about one half.
We conclude our discussion of symmetric positive deﬁnite matrices by looking
at yet another characterization. A real symmetric matrix A admits an eigenvalue
factorization of the form
A = QTQ,
(2.10)
where Q is an orthogonal matrix and  is a real diagonal matrix with diagonal entries
λ j, 1 ≤j ≤n. The columns of Q are mutually orthogonal eigenvectors of A, and the
entries λ j are the corresponding eigenvalues. The orthogonality of Q implies that
∥Qx∥2 = (Qx)T(Qx) = ∥x∥2.
By deﬁning y = Qx, we have
xTAx = (Qx)TQx = yTy =
n

j=1
λ j y2
j .
If λ j > 0, 1 ≤j ≤n, then for any x ̸= 0, xTAx > 0, hence A is SPD.
Conversely, if A is SPD, then for any x ̸= 0, xTAx > 0. Replacing A with its
eigenvalue factorization (2.10) and letting x = QTe j, 1 ≤j ≤n, it follows that
(QTe j)TA(QTe j) = eT
j QQTQQTe j = eT
j e j = λ j > 0,
1 ≤j ≤n,
proving that a matrix is SPD if and only if it is symmetric and all its eigenvalues are
real and positive.
Example 2.2 Consider an overdetermined linear system
Ax = b,
A =
⎡
⎢⎢⎢⎢⎢⎣
∗· · · ∗
∗· · · ∗
...
...
∗· · · ∗
∗· · · ∗
⎤
⎥⎥⎥⎥⎥⎦
∈Rm×n,
m > n,
where the solution is intended in the least squares sense.
Using the SVD of A, it can be shown that if the columns of A are linearly independent,
the null space of A contains only the zero vector, hence the minimizer of (2.8)
exists and is unique. To ﬁnd the minimizer, xLS, observe that the residual vector
r = b −AxLS must be orthogonal to the range of A, spanned by the column vectors
in A, that is
ATr = AT(b −AxLS) = 0.

32
2
Linear Algebra
Therefore, the least squares solution must satisfy the normal equations,
ATAx = ATb,
(2.11)
where the matrix ATA ∈Rn×n is symmetric and positive deﬁnite, since
vTATAv = (Av)TAv = ∥Av∥2 ≥0,
with equality holding if and only if Av = 0. But since N(A) = {0}, we have v = 0,
showing that ATA is SPD. The normal equations can be solved by using the Cholesky
factorization of ATA.
Passing to the normal equations is not necessarily the best way to solve a
least squares problem numerically, in particular when the coefﬁcient matrix is ill-
conditioned and the right-hand side is contaminated by noise. In fact, since the sin-
gular values of ATA are the squares of the nonzero singular values of A, the condition
number of the normal equations is the square of the condition number of the original
linear system, thus perturbations in the right hand side may become signiﬁcantly
emphasized. This issue is of particular concern when the condition number is large.
Rather than forming explicitly the normal equations, a standard way of solving
numerically least squares problems relies on the QR factorization of the matrix A,
A = QR,
where Q ∈Rm×m is an orthogonal matrix, that is, Q−1 = QT and R is upper trian-
gular,
R =
⎡
⎢⎢⎢⎢⎢⎣
∗· · · ∗
... ...
∗
⎤
⎥⎥⎥⎥⎥⎦
=

R1
O(m−n)×n

∈Rm×n.
Multiplying both sides of (2.4) from the left by QT yields the triangular system
Rx = QTb,
which can be partitioned as

R1x
O(m−n)×n

=
 
QTb

1

QTb

2

(n)
(m −n).
Whether the last m −n equations are satisﬁed or not is entirely dependent on the
right hand side b, while the ﬁrst n equations are satisﬁed provided that the vector x
is chosen so that
R1x = (QTb)1.

2.2 Solving Linear Systems
33
Such a vector x can be found by back-substitution, if the diagonal elements of R1
are nonzero, which is equivalent to the columns of A being linearly independent.
We remark that if the columns of A are linearly independent, the solution of the
normal equations is
xLS =

ATA
−1ATb = A†b.
The matrix A† =

ATA
−1AT ∈Rn×m is called the pseudoinverse of A. It is easy to
check that if A is square and invertible, the pseudoinverse coincides with the usual
inverse.
Often, the pseudoinverse is more of theoretical than computational interest, and it
can be helpful tool when analyzing the solution of linear systems with non-invertible
matrices.
Notes and Comments
There is a wealth of textbooks in linear algebra to choose from, see, e.g., [21, 45,
71]. For a more computational approach, we refer to [80].
We point out here that the discussion of the LU decomposition, as mentioned in
Example 2.1, is slightly streamlined, as it omits the often necessary row permutations
that constitute part of the decomposition, a process known as the partial pivoting in
Gaussian elimination. For details, we refer to the cited literature.

Chapter 3
Continuous and Discrete Multivariate
Distributions
After the generic introduction to probability and linear algebra, in this chapter, we
start to put together these concepts. In particular, we introduce the workhorse of
computational statistics, the normal distribution, using elements from matrix factor-
izations. Normal distributions play a role in computational statistics similar to that
of linear operators in analysis local linearizations of non-linear mappings, crucial
for designing efﬁcient computational algorithms, have the counterpart of normal
approximations in computational statistics. Furthermore, in anticipation of sampling
methods, we also discuss discrete distributions, and in particular, the Poisson distri-
bution that has a central role in modeling rare events.
3.1
Covariance Matrices
The importance of positive deﬁniteness in statistics is clear when considering covari-
ance matrices. Let X denote an n-variate random variable, X : Ω →Rn, and assume
that the mean and covariance of X,
x = E

X

,
C = E

(X −x)(X −x)T
,
are well-deﬁned. The variances of the components Xi, encoded in the diagonal entries
cii of the covariance matrix C, measure the variability of the random variable around
the mean along the coordinate directions. However, the coordinate directions need
not play any particular role among all directions, and we may be interested in the
variability in other directions. Given the vector v ̸= 0 in Rn, deﬁne the real-valued
random variable
Xv = vTX =
n

i=1
vi Xi,
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
D. Calvetti and E. Somersalo, Bayesian Scientiﬁc Computing, Applied Mathematical
Sciences 215, https://doi.org/10.1007/978-3-031-23824-6_3
35
https://avxhm.se/blogs/hill0

36
3
Continuous and Discrete Multivariate Distributions
whose mean and variance can be calculated as
E

Xv

=

Rn vTx πX(x)dx = vT

Rn x πX(x)dx

= vTx,
and
var

Xv

=

Rn(vTx −vTx)2 πX(x)dx
=

Rn

vT(x −x)
2πX(x)dx
=

Rn vT(x −x)(x −x)Tv πX(x)dx
= vT

Rn(x −x)(x −x)T πX(x)dx

v
= vTCv.
By deﬁnition, the variance of a real-valued random variable is non-negative, imply-
ing, in particular, that
vTCv ≥0.
Moreover, since the covariance matrix is symmetric, we conclude that covariance
matrices are always symmetric positive semideﬁnite. A legitimate question to ask
is whether we can expect, in general, covariance matrices to be SPD. The answer
is negative, as can be understood by a simple example: Let X1 : Ω →R be a real-
valued random variable with vanishing mean and ﬁnite variance σ 2 > 0, and deﬁne
X2 : Ω →R as X2(ω) = 0. The random variable
X =
 X1
X2
	
,
has covariance matrix
C =
σ 2
1 0
0 0
	
,
which is not SPD, since
0
1
	
C

 0 1 
= 0,
that is, the random variable X has no variability in the second coordinate direction.
In this case, we say that the random variable X is degenerate. Observe that the
second coordinate direction is aligned with the null space of the covariance matrix.
Therefore, by restricting the random variable X to the orthocomplement of the null
space, coinciding with R

CT
= R(C), the degeneracy disappears.
https://avxhm.se/blogs/hill0

3.2 Normal Distributions
37
3.2
Normal Distributions
The normal, or Gaussian distribution, is the workhorse of Bayesian scientiﬁc com-
puting and deserves to be discussed in detail. As usual, we start by addressing the
one-dimensional case, then consider the general multivariate case.
A random variable X ∈R is normally distributed, or Gaussian, indicated sym-
bolically by
X ∼N(μ, σ 2),
if its cumulative distribution is given by
P

X ≤t

=
1
√
2πσ 2
 t
−∞
exp

−1
2σ 2 (x −μ)2

dx.
Hence, the Gaussian probability density is
πX(x) =
1
√
2πσ 2 exp

−1
2σ 2 (x −μ)2

.
It is a straightforward integration exercise to show that
E

X

= μ,
var(X) = σ 2.
We are now ready to introduce multivariate normal distributions. Given n mutually
independent Gaussian random variables X j,
X j ∼N(μ j, σ 2
j ),
1 ≤j ≤n,
a multivariate Gaussian random variable X with components X j has the probability
density
πX(x) = πX1(x1) · · · πXn(xn)
=

1
(2π)nσ 2
1 · · · σ 2n
1/2
exp
⎛
⎝−1
2
n

j=1
(x j −μ j)2
σ 2
j
⎞
⎠.
(3.1)
This formula leads to a natural generalization of the multivariate Gaussian distri-
bution for non-independent components. Introduce the invertible diagonal matrix
D ∈Rn×n,
D =
⎡
⎢⎣
σ 2
1
...
σ 2
n
⎤
⎥⎦,
https://avxhm.se/blogs/hill0

38
3
Continuous and Discrete Multivariate Distributions
whose inverse is
D−1 =
⎡
⎢⎣
1/σ 2
1
...
1/σ 2
n
⎤
⎥⎦.
For any z ∈Rn, we have
zTD−1z =
n

j=1
z2
j
σ 2
j
,
det(D) = σ 2
1 σ 2
2 · · · σ 2
n ,
therefore, letting z = x −μ and substituting in (3.1), we get
πX(x) =

1
(2π)ndet(D)
1/2
exp

−1
2(x −μ)TD−1(x −μ)

,
where μ ∈Rn is the vector with components μ j, 1 ≤j ≤n.
To understand the role of independency of the components of X, observe that
since D is diagonal, the sets
(x −μ)TD−1(x −μ) = constant
represent ellipsoidal hypersurfaces in Rn with the principal axes parallel to the coor-
dinate axes. We may therefore ask what happens if we rotate the coordinate system
around the point μ so that the principal axes are no longer parallel to the coordinate
axes. Let U ∈Rn×n denote a rotation matrix in Rn. Deﬁne the new random variable
Z,
Z = μ + U(X −μ),
obtained by ﬁrst centering the random vector X at the origin by subtracting its mean,
rotating, and, ﬁnally, moving it back around the vector μ. Recalling that a rotation
matrix is orthogonal, hence its inverse coincides with its transpose, we obtain the
identity
X −μ = UT(Z −μ),
thus
X = μ + UT(Z −μ) = φ(Z).
Since Z is the result of a differentiable one-to-one coordinate transformation, we can
apply the procedure outlined in Chap. 1 to ﬁnd its probability density. First, notice
that the Jacobian matrix of φ is
Dzφ = UT,
https://avxhm.se/blogs/hill0

3.2 Normal Distributions
39
and since U is orthogonal, the determinant of the Jacobian is
∂φ
∂z = det(UT) = ±1.
Furthermore, the substitution x −μ = UT(z −μ) in the probability density function
of X yields
(x −μ)TD−1(x −μ) = (z −μ)TUD−1UT(z −μ) = (z −μ)T(UDUT)−1(z −μ),
where we used the fact that UT = U−1, hence
(UDUT)−1 = (UT)−1D−1U−1 = UD−1UT.
Introducing the matrix C ∈Rn×n,
C = UDUT,
(3.2)
we observe that
det(C) = det(UDUT) = det(U)det(D)det(UT)
=

det(U)
2det(D) = det(D).
Therefore, we conclude that the probability density of the rotated variable Z is given
by
πZ(z) =

1
(2π)ndet(C)
1/2
exp

−1
2(z −μ)TC−1(z −μ)

,
(3.3)
where C is a symmetric positive deﬁnite matrix. We can verify through an n-
dimensional integration argument that
E

Z

= μ,
cov(Z) = C.
This way, we have deﬁned the general Gaussian multivariate random variable, and
we indicate this by writing
Z ∼N(μ, C).
Later on, we write occasionally the probability density of a Gaussian, or normally
distributed random variable as
πZ(z) = N(z | μ, C).
A standard normal random variable is a Gaussian random variable with zero mean,
μ = 0, and covariance the n × n identity matrix, i.e., C = In.
https://avxhm.se/blogs/hill0

40
3
Continuous and Discrete Multivariate Distributions
3.3
How Normal is it to be Normal?
Normal densities are often used for convenience, as the two parameters, mean and
covariance, that completely characterize them, provide an intuitive interpretation of
the density. More generally, Gaussian random variables model well macroscopic
measurements that are sums of individual microscopic random effects, such as in
the case of pressure, temperature, electric current, and luminosity. The ubiquity of
Gaussian random variables can be understood and justiﬁed in the light of the Central
Limit Theorem, stated here for completeness. A proof can be found in standard
textbooks, see Notes and Comments at the end of the chapter.
Central Limit Theorem Assume that the real-valued random variables X1, X2, . . .
areindependentandidenticallydistributed(i.i.d.),thatis,theprobabilitydistributions
of X j are equal. Assume further that the common probability distribution has ﬁnite
mean and variance, denoted by μ ∈R and σ 2 > 0, respectively. Then the probability
distribution of the random variable
Zn =
1
σ√n

X1 + X2 + · · · + Xn −nμ

,
n = 1, 2, . . .
converges to the distribution of a standard normal random variable in the sense that
lim
n→∞P

Zn ≤x

= 1
2π
 x
−∞
e−t2/2dt.
Observe that
E

Zn

=
1
σ√n
⎛
⎝
n

j=1
E

X j

−nμ
⎞
⎠= 0,
and the scaling of the sum is chosen so that
Var

Zn) = E

Z2
n

=
1
nσ 2 E
⎛
⎝
n

j=1
(X j −μ)
2
⎞
⎠
=
1
nσ 2 E
⎛
⎝
n

j=1
(X j −μ)2 + 2

j>ℓ
(X j −μ)(Xℓ−μ)
⎞
⎠.
By the mutual independence of the zero mean random variables (X j −μ), the con-
tribution of the double sum vanishes, since
E
 
j>ℓ
(X j −μ)(Xℓ−μ)

=

j>ℓ
E(X j −μ)E(Xℓ−μ) = 0,
https://avxhm.se/blogs/hill0

3.4 Discrete Distributions
41
and therefore
Var

Zn) =
1
nσ 2
n

j=1
E(X j −μ)2 = 1,
indicating why one can expect the limit to have zero mean and unit variance.
Another way of thinking about the result of the Central Limit Theorem is the
following. Let Yn denote the average of n identically distributed independent random
variables with mean μ and variance σ 2,
Yn = 1
n
n

j=1
X j.
We observe that
E

Yn

= 1
n
n

j=1
E

X j

= μ,
and by writing
Yn −μ = 1
n
⎛
⎝
n

j=1
X j −nμ
⎞
⎠= σ
√n Zn,
we conclude that the variance of Yn is
Var(Yn) = E

Yn −μ
2
= σ 2
n .
The Central Limit Theorem states that, for n large, a good approximation for the
probability distribution of Yn is
Yn ∼N

μ, σ 2
n

,
thus conﬁrming the intuitive idea that by averaging random variables with common
mean, in the limit we arrive to that mean.
3.4
Discrete Distributions
In the discussion so far, we have assumed the existence of a probability density which
is a non-negative function. In the case where the random variable can take on only
discrete values, we need to introduce singular point masses to deﬁne its distribution.
In the following, we build a bridge between discrete and continuous densities that
https://avxhm.se/blogs/hill0

42
3
Continuous and Discrete Multivariate Distributions
later will be crucial to understand how probability distributions can be approximated
using sampling techniques.
A point mass in R is a measure that is concentrated at a single point. The concept of
point mass can be derived from normal distributions via a limiting process as follows.
We start with a random variable Y following the standard normal distribution:
Y ∼N(0, 1), or πY(y) =
1
√
2π
e−y2/2 = ψ(y).
For x0 ∈R and ε > 0, we deﬁne a new random variable X through an afﬁne trans-
formation
X = x0 + εY ⇔Y = X −x0
ε
= ϕ(X).
Using the change of variables formula for probability densities, we ﬁnd that the
density of X is
πX(x) = πY

ϕ(X)
dϕ
dx (x) =
1
√
2πε
e−(x−x0)2/2ε2 = 1
ε ψ
x −x0
ε

.
We observe that X is a Gaussian random variable centered at x0 and variance ε2. Next,
we investigate what happens when ε tends to zero. Intuitively, the probability mass
is more and more concentrated around x0. It is not difﬁcult to verify that, pointwise,
the probability density converges to zero at every point except for x0, where the
point value tends to inﬁnity. Since the pointwise limit is not particularly useful, we
formulate our question in a different way. Let f be any bounded continuous function.
We want to ﬁnd out what happens to the integral
Iε =

R
f (x)πX(x)dx = 1
ε

R
f (x)ψ
x −x0
ε

dx,
as ε →0+. To answer this question, we change back to the variable y, by making
the substitution
x = x0 + εy,
dx = ε dy,
and observe that, if
Iε =

R
f (x0 + εy)ψ(y)dy,
then
lim
ε→0+ Iε = f (x0)

R
ψ(y)dy = f (x0).
This calculation justiﬁes the following characterization. A point mass function at
x0 ∈R, denoted by δx0(x) is characterized by the property that for all bounded
continuous functions f ,
https://avxhm.se/blogs/hill0

3.4 Discrete Distributions
43

R
f (x)δx0(x)dx = f (x0),
or in other words, the point mass deﬁnes a point evaluation functional,
δx0 : f →f (x0).
In particular, for f (x) = 1 for all x, we have

R
δx0(x)dx = 1.
A point mass function is also referred to as Dirac’s delta function. Clearly, a point
mass is not a classical function but an instance of a generalized function.
The introduction of a point mass function allows us to deﬁne the large class of
random variables taking on values in a discrete set. Consider a random variable X
that assumes only non-negative integer values j = 0, 1, 2, . . ., and let
p j = P

X = j

.
Deﬁne the probability distribution of X as
πX(x) =
∞

j=0
p jδ j(x).
where the p js satisfy

R
πX(x)dx =
∞

j=0
p j = 1.
We then compute the expected value of X,
x = E

X

=

R
xπX(x)dx =
∞

j=1
p j

R
xδ j(x)dx =
∞

j=0
jp j,
and observe that there is no guarantee that it is an integer. In that sense, the term
expected value may be a misnomer, as we may not expect X to take on that value at
all. Similarly, the variance of X is given by
var(X) =

R
(x −x)2πX(x)dx =
∞

j=0
( j −x)2 p j.
A popular discrete distribution to model rare events is the Poisson distribution.
Before formally deﬁning it, we present the following motivating example.
https://avxhm.se/blogs/hill0

44
3
Continuous and Discrete Multivariate Distributions
Example 3.1 This example deals with modeling photon counts. A weak light source
emits photons that are counted with a charged coupled device (CCD). The counting
process
N(t) = {number of particles observed in [0, t]} ∈N
is an integer-valued random variable that depends on the parameter t > 0.
To set up a statistical model for N(t), we make the following assumptions:
1. Stationarity: Let Δ1 and Δ2 be any two time intervals of equal length, and let n
be a non-negative integer. Then
P

n photons arrive in Δ1

= P

n photons arrive in Δ2

.
2. Independent increments: Let Δ1, . . . , Δn be non-overlapping time intervals, let
k1, . . . , kn be non-negative integers, and let A j denote the event
A j =

k j photons arrive in the time interval Δ j

.
Then the events A1, · · · , An are mutually independent, i.e.,
P

A1 ∩· · · ∩An

= P

A1

· · · P

An

.
3. Negligible probability of coincidence: Assume that the probability of two or
more events occurring at the same time is negligible. More precisely, assume that
N(0) = 0 and
lim
h→0
P

N(h) > 1

h
= 0.
This condition can be interpreted by saying that the number of counts increases
at most linearly.
If these assumptions hold, then it can be shown (see Notes and Comments for a
reference) that N is a Poisson process and
P

N(t) = j

= (λt) j
j! e−λt,
λ > 0.
The previous example motivates us to deﬁne the random variable X with discrete
probability density,
πX(x) =
∞

j=0
p jδ j(x),
p j = θ j
j! e−θ,
(3.4)
where θ > 0 is the Poisson parameter. We write
X ∼Poisson(θ).
https://avxhm.se/blogs/hill0

3.4 Discrete Distributions
45
The calculation of the expectation of this Poisson random variable is rather straight-
forward:
E

X

= e−θ
∞

j=0
j θ j
j!
= e−θ
∞

j=1
θ j
( j −1)! = e−θ
∞

j=0
θ j+1
j!
= θ e−θ
∞

j=0
θ j
j!
  
=eθ
= θ.
To calculate the variance of a Poisson random variable, we ﬁrst observe that
E

(X −θ)2
= E

X2
−2θ E

X

  
=θ
+θ2
= E

X2
−θ2
=
∞

j=0
j2 p j −θ2.
Substituting the expression for p j from (3.4) yields
E

(X −θ)2
= e−θ
∞

j=0
j2 θ j
j! −θ2 = e−θ
∞

j=1
j
θ j
( j −1)! −θ2
= e−θ
∞

j=0
( j + 1)θ j+1
j!
−θ2
= θe−θ
⎧
⎨
⎩
∞

j=0
j θ j
j! +
∞

j=0
θ j
j!
⎫
⎬
⎭−θ2
= θe−θ
θeθ + eθ
−θ2
= θ.
Thus, the mean and the variance of the Poisson random variable X are equal to the
parameter characterizing the distribution.
https://avxhm.se/blogs/hill0

46
3
Continuous and Discrete Multivariate Distributions
3.4.1
Normal Approximation to the Poisson Distribution
We now return to the example of the CCD camera, and notice that the total photon
count can be thought of as a sum of random events, single photon arrivals, that
are independent and identically distributed. Therefore, it is reasonable to ask if the
Poisson distribution can be reliably approximated by a normal distribution when the
count number is high. In order to show that this is a reasonable approximation, we
need to introduce some auxiliary results.
Given two mutually independent random variables, both following a Poisson
distribution:
X j ∼Poisson(θ j),
j = 1, 2,
their sum is also Poisson distributed, and
X = X1 + X2 ∼Poisson(θ1 + θ2).
To prove the claim, we write
P

X = k

= P

(X1, X2) = (k, 0) or (X1, X2) = (k −1, 1) or
. . . or (X1, X2) = (0, k)

= P

(X1, X2) = (k, 0)

+ P

(X1, X2) = (k −1, 1)

+
. . . + P

(X1, X2) = (0, k)

.
Further, by the independency of X1 and X2,
P

(X1, X2) = (k −j, j)

= P

X1 = k −j

P

X2 = j

= e−(θ1+θ2)
θk−j
1
(k −j)!
θ j
2
j! ,
which implies that
P

X = k

= e−(θ1+θ2)
k

j=0
θk−j
1
(k −j)!
θ j
2
j! = 1
k!e−(θ1+θ2)
k

j=0
 k
j

θk−j
1
θk
2
= (θ1 + θ2)k
k!
e−(θ1+θ2)
by the binomial formula, thus completing the proof.
Consider now a Poisson distributed random variable, and assume, for simplicity,
that the Poisson parameter θ is an integer. We remark that this is not a necessary
requirement, but a way to simplify the discussion. We can represent X in an equivalent
form as
https://avxhm.se/blogs/hill0

3.4 Discrete Distributions
47
X =
θ

j=1
X j,
X j ∼Poisson(1),
where the variables X j are assumed to be mutually independent. By induction, the
reasoning above shows that the sum is indeed a Poisson-distributed random variable.
By the Central Limit Theorem, we conclude that, approximately,
X −θ
√
θ
∼N(0, 1),
and the approximation becomes more accurate when θ grows. In particular, for all
a, b, 0 ≤a < b, we have the approximation

a< j≤b
θ j
j! e−θ ≈
1
√
2πθ
 b
a
e−1
2θ (x−θ)2dx,
establishing the normal approximation.
It turns out that the above approximation of a Poisson random distribution
with a Gaussian can be slightly improved. If n is an integer, the probabilities
P

X ≤n + δ

coincide for all δ, 0 ≤δ < 1. Therefore, the normal approximation
becomes ambiguous, since

j≤n
θ j
j! e−θ =

j≤n+δ
θ j
j! e−θ ≈
1
√
2πθ
 n+δ
−∞
e−1
2θ (x−θ)2dx,
0 ≤δ < 1.
In practice, the value δ = 1/2 is used, with this shift of the upper bound referred to
as continuity correction.
Observe that if Y is the random variable corresponding to the normal density with
continuity correction, then Y has the probability density
πY(y) = d
dy

1
√
2πθ
 y+1/2
−∞
e−1
2θ (x−θ)2dx

=
1
√
2πθ
e−1
2θ (y−θ+1/2)2,
that is,
Y ∼N(θ −1/2, θ).
To get an idea of the quality of the approximation, we plot the Gaussian
N(θ −1/2, θ) versus the Poisson probabilities p j with parameter θ. Fig. 3.1 shows
the plots for different values of θ.
The Central Limit Theorem justiﬁes in many circumstances the use of Gaussian
approximations. As we shall see, normal distributions are computationally quite
convenient.
https://avxhm.se/blogs/hill0

48
3
Continuous and Discrete Multivariate Distributions
0
5
10
15
20
25
30
35
40
0
0.05
0.1
0.15
0.2
0
5
10
15
20
25
30
35
40
0
0.05
0.1
0.15
0.2
0
5
10
15
20
25
30
35
40
0
0.05
0.1
0.15
0.2
0
5
10
15
20
25
30
35
40
0
0.05
0.1
0.15
0.2
Fig. 3.1
Poisson distributions (dots) and their Gaussian approximations (solid curve) with various
values of the mean θ
Notes and Comments
For the proof and discussion of the Central Limit Theorem, we refer to [5]. The proof
concerning the statement about the Poisson distribution can be found, e.g., in [35].
The discovery of the normal distribution is often attributed to Abraham deMoivre,
while systematically developed by Carl Friedrich Gauss and Pierre-Simon Laplace. A
name worth mentioning is Adolphe Quetelet, the nineteenth-century Belgian statis-
tician who in his famous 1835 treatise Sur l’homme et le développement des ses
facultés, ou, Essay de physique sociale, introduced “l’homme moyen”, the average
man, bringing the Gaussian bell curve to anthropometric social sciences, and is con-
sidered as one of the founders of sociology, originally also known as social physics.
https://avxhm.se/blogs/hill0

Chapter 4
Introduction to Sampling
A large portion of statistical inference deals with statistical modeling and analysis,
which often include the description and understanding of the data collection pro-
cess. Therefore, statistical inference lies at the very core of scientiﬁc modeling and
empirical testing of theories. Statistical modeling may lead to the conclusion that the
underlying probability density can be described in a parametric form, such as a Gaus-
sian distribution, and the problem is reduced to one of estimating the parameters that
characterize it. Often, however, there are not enough reasons to assume a parametric
model, and one needs to resort to non-parametric models. In either case, it may be
useful to consider summary statistics of the sample, such as mean and variances, as
a ﬁrst step. As discussed in this section, these concepts of descriptive statistics lead
to more sophisticated ideas that turn out to be useful, for instance, when statistical
methods are used in computational inverse problems.
4.1
On Averaging
The most elementary way to summarize a large set of statistical data is to calculate
its average. Assuming that the data consist of a large sample of vectors,
S =

x1, x2, . . . , x N
,
that are realizations from an underlying probability density πX of a random variable
X, a natural approximation of the mean of this putative density is
E

X

=

x πX(x)dx ≈1
N
N

j=1
x j.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
D. Calvetti and E. Somersalo, Bayesian Scientiﬁc Computing, Applied Mathematical
Sciences 215, https://doi.org/10.1007/978-3-031-23824-6_4
49
https://avxhm.se/blogs/hill0

50
4
Introduction to Sampling
The rationale and theoretical justiﬁcation of this approximation is provided by the
Law of Large Numbers. This law has several variants, one of which is the following.
Law of Large Numbers: Assume that X1, X2, . . . are independent and identically
distributed random variables,
X j ∼πX,
with ﬁnite mean μ. Then
lim
N→∞
1
N

X1 + X2 + · · · + X N
= μ
almost certainly, that is,
P

lim
N→∞
1
N
N

j=1
X j = μ

= 1.
This formulation of the Law of Large Numbers is often referred to as the strong
law of large numbers. The expression almost certainly means, in particular, that, with
probability one, the averages of any realizations x1, x2, . . . of the random variables
X1, X2, . . . converge towards the mean. This is good news, since data sets are always
realizations. Observe the close connection with the Central Limit Theorem, which
states the convergence in terms of probability densities.
More generally, we may want to calculate estimators for the expectation of
a known function f (X) based on a sample of realizations. Observing that if
X1, . . . , X N are independent and identically distributed, then so are the random
variables f

X1
, . . . , f

X N
, and the Law of Large Numbers justiﬁes the approxi-
mation
E

f (X)

≈1
M
N

j=1
f

x j
,
(4.1)
assuming that the expectation on the left is ﬁnite.
The following example is meant to clarify the strength of a parametric model if
such model is available, and to what extent the simple summary statistics may be a
misleading way to describe a probability distribution.
Example 4.1 Given a sample of points in the plane,
S =

x1, x2, . . . , x N
,
x j =

x j
1
x j
2
	
∈R2,
we want to set up a parametric model for the data, by assuming that the points are
independent realizations of a normally distributed random variable,
X ∼N(μ, C),
(4.2)
https://avxhm.se/blogs/hill0

4.1 On Averaging
51
with unknown mean μ ∈R2 and covariance matrix C ∈R2×2. For future reference,
we express this by writing the probability density of X using the notation of condi-
tional densities,
πX(x | μ, C) =
1
2πdet(C)1/2 exp

−1
2(x −μ)TC−1(x −μ)

,
i.e., this is the form of the probability density provided that μ and C are given. The
inference problem is therefore to estimate the parameters μ and C from the sample S.
As the Law of Large Numbers suggests, we may hope that N is sufﬁciently large
to justify an approximation of the form
μ = E

X

≈1
N
N

j=1
x j = μ.
(4.3)
More precisely, above we have assumed that we have N independent copies X j,
1 ≤j ≤N, of identically distributed independent random variables with the same
distribution as X, and x j is a realization of X j.
To estimate the covariance matrix C = cov(X) = E

(X −μ)(X −μ)T
, we deﬁne
the matrix valued function
f : R2 →R2×2,
f (x) = (x −μ)(x −μ)T,
and, combining it with (4.1), arrive at the approximation
C ≈C = E

f (X)

≈1
N
N

j=1
(x j −μ)(x j −μ)T
(4.4)
The estimates μ and C of the mean and covariance, respectively, from formulas (4.3)
and (4.4), are often referred to as the empirical mean and covariance, or sample
mean and covariance, respectively.
Before illustrating these ideas with examples, a comment is in order. The approx-
imation (4.4) is often referred to as a biased estimator of the covariance matrix.
In the statistics literature, the preferred covariance estimate is a slightly different
approximation,
Cu =
1
N −1
N

j=1
(x j −μ)(x j −μ)T =
N
N −1
C,
(4.5)
known as the unbiased covariance estimator. The reason for the different scaling and
the terminology is explained in the Notes and Comments at the end of the chapter.
Clearly, for N large, the difference between the estimators becomes insigniﬁcant.
https://avxhm.se/blogs/hill0

52
4
Introduction to Sampling
We are now ready to present some computed examples. In the ﬁrst one, we assume
an underlying Gaussian parametric model that explains the sample, which was in fact
drawn from a two-dimensional Gaussian density. The left panel of Fig. 4.1 shows
the equiprobability curves of the original Gaussian probability density, which are
ellipses. A scatter plot of a sample of N = 500 points drawn from this density
is shown in the right panel of the same ﬁgure. We then compute the eigenvalue
decomposition of the empirical covariance matrix,
C = UDUT,
(4.6)
where U ∈R2×2 is an orthogonal matrix whose columns are the eigenvectors scaled
to have unit length, and D ∈R2×2 is the diagonal matrix with the diagonal entries
equal to the eigenvalues of C,
U =
v(1) v(2) 
,
D =
λ1
λ2

,
Cv( j) = λ jv( j),
j = 1, 2.
The right panel of Fig. 4.1 shows the eigenvectors of C scaled to have length twice
the square root of the corresponding eigenvalues, applied at the empirical mean. The
plot is in good agreement with our expectation.
To better understand the implications of adopting a parametric form for the under-
lying density, we now consider a sample from a density that is far from Gaussian. The
equiprobability curves of this non-Gaussian density are shown in the left panel of
Fig. 4.2. The right panel shows the scatter plot of a sample of N = 500 points drawn
from this density. We compute the empirical mean, the empirical covariance matrix,
and its eigenvalue decomposition. As in the Gaussian case, we plot the eigenvectors
scaled by a factor twice the square root of the corresponding eigenvalues, applied at
Fig. 4.1 The ellipses of the equiprobability curves of the original Gaussian distribution (left), and
on the right, the scatter plot of the random sample and the eigenvectors of the empirical covariance
matrix, scaled by the two times the square roots of the corresponding eigenvalues. These square
roots represent the standard deviations of the density in the eigendirections, so the lengths of the
drawn vectors equal two standard deviations
https://avxhm.se/blogs/hill0

4.2 Whitening and P–P Plots
53
Fig. 4.2 The equiprobability curves of the original non-Gaussian distribution (left) and the sample
drawn from it (right), together with the scaled eigenvectors of the empirical covariance matrix
applied at the empirical mean
the empirical mean. While the mean and the covariance estimated from the sample
are quite reasonable, a Gaussian model is clearly not in agreement with the sample.
Indeed, if we would generate a new sample from a Gaussian density with the calcu-
lated mean and covariance, the scatter plot would be very different from the scatter
plot of the sample. In this case, the mean and covariance of the sample provide a
poor summary description of the sample.
4.2
Whitening and P–P Plots
In the previous examples, the parametric Gaussian model was clearly not a good
choice for the second set of data. In one or two dimensions, it is relatively easy to
check whether a Gaussian model (4.2), or any other parametric model that we choose
to use, is reasonable for explaining the distribution of given data, simply by visually
inspecting scatter plots. Although in higher dimensions we can look at the scatter
plots of two-dimensional projections and try to assess if the normality assumption
is reasonable1, it is nonetheless preferable to develop more systematic methods to
investigate the goodness of the Gaussian approximation. One possible way is to
go through the cumulative distribution functions, comparing that of the theoretical
model with the one estimated from the sample, as explained below. To simplify the
discussion, we start by introducing the concept of whitening a random variable.
A standard normal Gaussian random variable W ∈Rn,
W ∼N(0, In),
1 As the legendary Yogi Berra summarized in one of his famous aphorisms, “you can observe a lot
by just watching.”
https://avxhm.se/blogs/hill0

54
4
Introduction to Sampling
where In is the n × n unit matrix, is also referred to as Gaussian white noise. In
this context, whitening is a transformation that maps an arbitrary Gaussian random
variable to Gaussian white noise. Consider a random variable
X ∼N(μ, C),
where μ ∈Rn, C ∈Rn×n is a symmetric positive deﬁnite matrix, and let
C = RTR
(4.7)
be its Cholesky factorization. Given a Gaussian white noise W, deﬁne the random
variable
Z = μ + RTW.
It is straightforward to verify that the mean and the covariance of this random variable
are
E

Z

= μ + RTE

W

= μ,
and
cov(Z) = E

(Z −μ)(Z −μ)T
= E

(RTW)(RTW)T
= RTE

W W T
R = RTR = C,
respectively.
Moreover, using the change of variables formula introduced in the previous
chapter, it can be shown that
Z ∼N(μ, C).
Therefore, X and Z can be identiﬁed as random variables, and we may write X in
terms of Gaussian white noise,
X = μ + RTW,
or, by solving for W, we can express the Gaussian white noise in terms of X,
W = R−T(X −μ),
(4.8)
where R−T = (RT)−1 = (R−1)T. Formula (4.8) deﬁnes a whitening transformation,
or Mahalanobis transformation, of the random variable X into Gaussian white noise.
As an immediate application of the whitening process, consider now the question
addressed earlier: Given a sample
S =

x1, x2, . . . , x N
,
x j ∈Rn,
https://avxhm.se/blogs/hill0

4.2 Whitening and P–P Plots
55
how close is it to a set of realizations from a Gaussian distribution? We perform the
analysis in two dimensions, n = 2, to keep the computations tractable; the exten-
sion of the technique to Rn is fairly straightforward, requiring only little more than
changing the notation. The general case is discussed in the Notes and Comments at
the end of this chapter. To simplify the notation, let μ and C denote the empirical
mean and covariance of the sample, and let (4.7) be the Cholesky decomposition of
the latter. Apply the whitening transformation deﬁned by R to the data, and consider
the whitened sample
w j = R−T(x j −μ),
1 ≤j ≤N.
If the points x j are realizations of a Gaussian random variable X with mean μ and
covariance C, then the points w j should be realizations of Gaussian white noise. It
is rather easy to test if this is indeed the case.
For any δ > 0, we can calculate the probability
P

∥W∥< δ

=

Dδ
πW(w)dw,
(4.9)
where Dδ is the ball in R2 with radius δ. Formula (4.9) deﬁnes the cumulative
distribution of the real-valued random variable ∥W∥. If W is indeed Gaussian white
noise, the value of the integral, computed by switching to polar coordinates (r, θ), is
P

∥W∥< δ

= 1
2π

Dδ
e−1
2 ∥w∥2dw = 1
2π
 δ
0
 2π
0
e−1
2 r2rdθdr
=
 δ
0
e−1
2 r2rdr = 1 −e−1
2 δ2.
(4.10)
This number is equal to the proportion of the mass of the Gaussian probability
density inside a disc of radius δ. Conversely, the radius δ of a disc centered at the
origin containing the fraction p of the mass in its interior, 0 ≤p ≤1, can be found
by solving
1 −e−1
2 δ2 = p,
or δ = δ(p) =

2 log

1
1 −p

.
Therefore, if the discrete sample is from an underlying Gaussian distribution, it is
reasonable to expect approximately the same proportion of the whitened sample w j
to be inside the disc Dδ. Introduce the function
ν(p) = 1
N #

w j ∈Dδ(p)

.
(4.11)
https://avxhm.se/blogs/hill0

56
4
Introduction to Sampling
Fig. 4.3 Plot of the
sample-based quantiles ν(p)
as a function of p. The solid
curve corresponds to a
sample arising from the
Gaussian density, and the
dashed curve to a
non-Gaussian sample.
Observe that in the
non-Gaussian case, the sets
Bα(p) with small p contain
little or no sample points,
and the slope is small
In the case when the points come from the Gaussian distribution, we should have
ν(p) = p, while large deviations of the values ν(p) from p suggest that the points
of S are not likely to be realizations of a random variable whose probability density
is normal. The plot of the curve p 	→ν(p) is a version of a multivariate P-P plot, or
probability–probability plot, or percent–percent plot.
Figure 4.3 shows the results of this test with the two samples of Example 4.1.
The solid curve is calculated by using the sample that indeed was generated by using
the Gaussian model: as expected, the P-P plot is following well the diagonal y = x.
The non-Gaussianity of the second sample, plotted as a dashed curve, is far from
the diagonal: We observe that for small p, the function ν(p), instead of increasing
with unit slope, remains close to zero, as there are no sample points near the mean,
but after a while, the accumulation becomes faster, and the slope exceeds the unity,
causing an overshoot of the curve.
4.3
Quadratures and Law of Large Numbers
So far, we have concentrated on characterizing a probability density underlying a dis-
crete sample. In the following, we reverse the approach, and we seek to approximate
a probability density by a discrete sample.
Consider ﬁrst a random variable X taking on values in R, and let πX denote its
probability density function. In order to calculate the expectation of a derived random
variable Y = f (X), where f is assumed to be a continuous and bounded function,
we need to evaluate the integral
E

f (X)

=

R
f (x)πX(x)dx.
Assuming, further, that we know that the possible values of X are restricted to a
ﬁnite interval which, for simplicity, we assume to be [−1, 1], we may approximate
https://avxhm.se/blogs/hill0

4.3 Quadratures and Law of Large Numbers
57
the integral using classical quadrature rules such as Gauss quadratures. Recall that if
we have a weight function w(x) deﬁned over the interval [−1, 1], a Gauss quadrature
rulecomprisesasetofnodes x j ∈[−1, 1]andcorrespondingweightsω j,1 ≤j ≤m,
such that the integral can be approximated by a ﬁnite sum,
 1
−1
f (x)w(x)dx ≈
m

j=1
ω j f (x j),
(4.12)
and the approximation is exact for all polynomials p of degree at most 2m −1,
 1
−1
p(x)w(x)dx =
m

j=1
ω j p(x j),
deg(p) ≤2m −1.
Different weight functions lead to different quadrature rules. For instance, the weight
function w(x) = 1 leads to Gauss-Legendre rules, where the nodes are zeros of the
Legendre polynomials, while the choice w(x) = (1 −x2)−1/2 leads to the Gauss-
Chebyshev rule. Assuming for simplicity that w(x) = 1, we may therefore write
E

f (X)

≈
m

j=1
ω jπX(x j) f (x j) =
m

j=1
w j f (x j),
w j = ω jπX(x j).
(4.13)
Observe that, by using the notion of point masses, we can write the approximation
(4.13) in the following form:
 1
−1
f (x)πX(x)dx ≈
 1
−1
m

j=1
w jδx j(x) f (x)dx.
with the probability density function πX replaced by a discrete point mass approxi-
mation,
πX(x) ≈
m

j=1
w jδx j(x).
The approximation is understood to hold in the sense that the discrete density inte-
grates polynomials up to the order 2m −1 exactly. To avoid the use of point masses,
this approximation can be written in terms of the cumulative distribution functions
as follows. Denoting the cdf of X by
ΦX(s) = P

X ≤s

=
 s
−∞
πX(x)dx,
https://avxhm.se/blogs/hill0

58
4
Introduction to Sampling
we have the piecewise constant approximation of ΦX,
ΦX(s) ≈
m

j=1
w j H(s −x j),
where H is the Heaviside step function,
H(x) =
1, if x ≥0,
0, if x < 0.
(4.14)
While Gauss quadratures and other classical quadrature rules are excellent tools
for approximating integrals in low-dimensional spaces, the curse of dimensionality
makes them less useful in higher dimensions. In fact, if the data are n-dimensional,
x j ∈Rn, a Gauss quadrature over a hypercube [−1, 1]n would require mn grid points,
anumberthatbecomesquicklyprohibitivelylarge.Whatmakesthematterevenworse
is the fact that at most of the grid points πX(x j) ≈0, as illustrated schematically in
Fig. 4.4. An additional difﬁculty arises from the fact that, in general, there is no
systematic way to determine a priori a hyper-rectangle encompassing any signiﬁcant
part of the probability mass of the distribution.
The above considerations are a great motivation to look at random sampling from
a different point of view. We start with a simple introductory example.
Example 4.2 Assume that on a rectangular sheet of paper with a known area A, an
irregular shape Ω is drawn, and the problem is to estimate the area of that shape
without any measuring tools such as a ruler or a compass. It is a rainy day, so we take
the paper sheet out and let the raindrops fall on it, counting the drops falling inside
the shape, as well as all drops falling on the paper sheet. We then estimate the area
|Ω| by writing
|Ω|
A ≈
# raindrops inside Ω
# all raindrops hitting the paper.
Fig. 4.4 A distribution in
[0, 1] × [0, 1] covered by a
regular grid of
21 × 21 = 441 nodes. The
nodes at which the density
takes on a value larger than
0.1% of its maximum are
indicated by dots,
representing about that 18%
of the grid points, indicating
that most of the function
evaluations at grid points are
numerically insigniﬁcant
https://avxhm.se/blogs/hill0

4.3 Quadratures and Law of Large Numbers
59
Here, we implicitly assume that the raindrops fall uniformly and independently. Thus,
if πX denotes the uniform distribution over the paper sheet,
πX(x) = 1
A,
and f (x) is the characteristic function of the shape Ω, we have
|Ω|
A = 1
A

f (x)dx =

f (x)πX(x)dx,
and if N is the total number of raindrops on the paper, and the position of the jth
raindrop is x j, we have
# raindrops inside Ω
# all raindrops hitting the paper = 1
N
N

j=1
f (x j).
The heuristics has therefore lead us to the stochastic quadrature formula,

f (x)πX(x)dx ≈1
N
N

j=1
f (x j),
a basic identity of Monte Carlo integration.
To demonstrate the algorithm, consider the domain Ω with a parametrized bound-
ary curve
x(s) = r (cos s + α(cos 2s −1)) ,
y(s) = rβ sin s,
0 ≤s ≤2π,
where the parameters are chosen as r = 0.5, α = 0.65, and β = 1.5. The domain is
shown in Fig. 4.5. Using Green’s theorem, we can compute the area of Ω exactly as
a trigonometric integral, to get
Fig. 4.5 The domain Ω inside the rectangle [−1, 1] × [−1, 1], and random points drawn from the
uniform distribution. The number of the random points is 100 (center) and 1000 (right)
https://avxhm.se/blogs/hill0

60
4
Introduction to Sampling
Fig. 4.6 Relative approximation error of the raindrop integrator as a function of the sample size N
(left) as well as as a function of its inverse 1/N (right). The dashed red line in the latter indicates
the power law ∼1/
√
N
|Ω| = πr2β ≈1.178.
Let the paper sheet have the area A = 4 as in the ﬁgure, and simulate the raindrops
by drawing points from the uniform distribution over the rectangle [−1, 1] × [−1, 1]
as shown in the ﬁgure.
Figure 4.6 shows the relative error as a function of the sample size N (left), as well
as a function of 1/N (right), plotted in logarithmic scales. The plots reveal that the
relative error decreases roughly at the rate of 1/
√
N, which is a very characteristic
rate for stochastic integration methods.
We discuss the idea now in more general terms. Assume that we have an ensemble
of points x j independently drawn from a distribution described by a probability
density πX, that is, x j is a realization of X j, and that the random variables X j
are independent and identically distributed, X j ∼πX. The Law of Large Numbers
guarantees that, for a continuous bounded function f ,
lim
N→∞
N

j=1
wN
j f (x j) =

f (x)πX(x)dx,
wN
j = 1
N ,
almost certainly. In other words,
N

j=1
wN
j δx j = πN
X →πX
almost certainly in the weak sense2, which means that, with probability one,

f (x)πN
X (x)dx →

f (x)πX(x)dx
for every bounded continuous f.
2 Strictly speaking, we should say “in the weak∗sense,” for reasons explained in Notes and
Comments at the end of the chapter.
https://avxhm.se/blogs/hill0

4.4 Drawing from Discrete Densities
61
Hence, we may view sampling from πX as a way to generate a stochastic quadrature
rule with uniform weights. In particular, as the points x j are sampled from the proba-
bility density πX, the method avoids wasting points in regions where the probability
density πX vanishes or is negligibly small. Moreover, the Central Limit Theorem
gives us an idea of the convergence rate of the approximation. Since the random
variables f (X j) are independent and identically distributed, we conclude that
√
N


f (x)πN
X (x)dx −

f (x)πX(x)dx

d→N(0, σ2),
where σ2 is the variance of the random variable f (X) and the convergence takes place
“in distribution” as deﬁned in Sec. 3.3. Thus, the theorem establishes a convergence
rate of 1/
√
N.
IntegrationbasedonrandomsamplingiscalledMonteCarlointegration.Although
it looks very attractive, it has indeed its share of difﬁculties; the proverbial free lunch
does not exist here, either. One of the problems is how to draw the sample. We start
examining this problem in this section, and return to it later with more sophisticated
tools at our disposal.
There are two elementary distributions that we take for granted in the sequel. We
assume that we have access to
• a random number generator drawing from the uniform distribution over the interval
[0, 1],
πX(x) = χ[0,1](x),
• a random number generator drawing from the standard normal distribution,
πX(x) =
1
√
2π
e−x2/2.
These (pseudo)random generators in Matlab are called with the commands rand
and randn, respectively.
4.4
Drawing from Discrete Densities
Considerarandomvariablewithaﬁnitesetofpossiblevalues{v1, v2, . . . , vn},having
therefore a probability density comprising point masses,
πX(x) =
n

j=1
p jδv j(x),
where the weights p j must satisfy
https://avxhm.se/blogs/hill0

62
4
Introduction to Sampling
p j ≥0,
n

j=1
p j = 1.
We start by dividing the unit interval [0, 1] into n disjoint intervals I j such that
length(I j) = p j,
1 ≤j ≤n.
Consider a random variable uniformly distributed in [0, 1], W ∼Uniform
([0, 1]). By deﬁnition of the uniform distribution, we conclude that
P

W ∈I j

= length(I j) = p j,
which suggests a simple algorithm for drawing randomly realizations of X:
1. Draw randomly the value w of W,
2. Find j such that w ∈I j, and set x = v j.
The implementation can be done effectively in terms of the cumulative distribution
function. Here is how to do it in Matlab:
Phi = 0;
j
= 0;
w
= rand;
while Phi < w
j
= j+1;
Phi = Phi + p(j);
end
In the algorithm, the variable Phi represents the cumulative distribution function.
The exit value of j is the output of the algorithm, as visualized in Fig. 4.7.
Fig. 4.7 Drawing from a discrete distribution: Divide the interval [0, 1] into subintervals I j of
length p j, then draw W from uniform distribution and check in which interval the value falls,
giving the discrete random variable
https://avxhm.se/blogs/hill0

4.5 Sampling from a One-Dimensional Continuous Density
63
If n is large, the above algorithm can be sped up by sorting ﬁrst the indices so
that the realizations with the highest probabilities appear ﬁrst, implying that the
cumulative distribution reaches the random number w in fewest possible steps. This
algorithm can be used to draw a sample from discrete random variables with an
inﬁnite set of possible values, such as the Poisson distribution.
4.5
Sampling from a One-Dimensional Continuous Density
We are now ready to generalize the idea of sampling from a discrete density to the
case of a random variable with a continuum of possible values. Hence, let πX denote
the probability density of a random variable X deﬁned on R. We start by recalling
the deﬁnition of the cumulative distribution function of X,
ΦX(s) = P

X ≤s

=
 s
−∞
πX(x)dx.
The function ΦX is non-decreasing since πX ≥0, and 0 ≤ΦX ≤1. To simplify the
discussion, we assume that πX is supported in some interval I ⊂R, and πX(x) > 0
except possibly at some isolated points in I. With these assumptions, the cumulative
distribution function is strictly increasing over I, and it attains all values in the open
interval (0, 1).
Let us deﬁne a new random variable,
T = ΦX(X).
(4.15)
We claim that T ∼Uniform([0, 1]). In general, a random variable Y follows the
uniform distribution over an interval [a, b] if
P

Y < s

= s −a
b −a ,
a ≤s ≤b.
To prove that T is uniformly distributed, observe ﬁrst that, due to the monotonicity
of ΦX,
P

T ≤s

= P

ΦX(X) ≤s

= P

X ≤Φ−1
X (s)

,
0 < s < 1.
On the other hand, by the deﬁnition of probability density,
P

X ≤Φ−1(s)

=
 Φ−1
X (s)
−∞
πX(x)dx =
 Φ−1
X (s)
−∞
Φ′
X(x)dx.
https://avxhm.se/blogs/hill0

64
4
Introduction to Sampling
Fig. 4.8 A random draw of x from the distribution πX(x), corresponding to the cumulative distri-
bution ΦX(x). Here, t ∼Uniform([0, 1])
After the change of variables
t = ΦX(x),
dt = Φ′
X(x)dx,
we arrive at
P

T ≤s

=
 Φ−1
X (s)
−∞
Φ′
X(x)dx =
 s
0
dt = s,
which is equivalent to saying that T is a random variable with uniform distribution
over [0, 1], thus proving the claim.
We are now ready to present an algorithm to draw from the distribution πX,
graphically illustrated in Fig. 4.8, comprising two simple steps:
1. Draw t ∼Uniform([0, 1]),
2. Calculate x = Φ−1
X (t).
This algorithm is sometimes referred to as the golden rule, or inverse cumulative
distribution rule. In spite of the simplicity of the idea, its implementation can present
some problems, as the following example shows.
Example 4.3 Consider a one-dimensional normal distribution with a bound con-
straint,
πX(x) ∝H(x −c)e−x2/2,
where “∝” means “proportional to” or equal to up to a normalizing constant, and H
is the Heaviside step function (4.14). The problem is to generate a sample drawing
independently from πX.
The cumulative distribution function is
ΦX(x) = C
 x
c
e−y2/2dy,
x ≥c,
https://avxhm.se/blogs/hill0

4.5 Sampling from a One-Dimensional Continuous Density
65
and ΦX(x) = 0 for x < c, where C > 0 is the normalizing constant
C =

 ∞
c
e−y2/2dy
−1
.
The function ΦX has to be calculated numerically. Fortunately, there is reliable soft-
ware available to do the calculation. In Matlab, the built-in error function, erf, is
deﬁned as
erf(s) =
2
√π
 s
0
e−z2dz.
We observe that
ΦX(x) = C

 x
0
−
 c
0

e−y2/2dy =
√
2C
 x/
√
2
0
−
 c/
√
2
0

e−z2dz
=
π
2 C

erf( x
√
2
) −α

,
α = erf( c
√
2
).
On the other hand, since
C =

π
2 (1 −α)
−1
,
we have
ΦX(x) = erf(x/
√
2) −α
1 −α
.
What about the inverse? Writing
ΦX(x) = t,
after some algebraic manipulations, we ﬁnd that
erf( x
√
2
) = t(1 −α) + α.
The double luck is that there are effective algorithms to calculate the inverse of the
error function, for example inverf in Matlab:
x = Φ−1
X (t) =
√
2 inverf

t

1 −α) + α

.
Hence, random generation in Matlab is very simple:
alpha = erf(c/sqrt(2));
t
= rand;
x
= sqrt(2)*erfinv(t*(1-alpha)+alpha);
https://avxhm.se/blogs/hill0

66
4
Introduction to Sampling
Here, a warning is in order: if the bound c is large, the above algorithm may fail,
because the error function saturates quickly to unity: When c is large, numerically
α = 1, and the above algorithm gives numerical noise as a value of x. To understand
what “large” means here, 1 −erf(3) ≈2.2 × 10−5, and 1 −erf(4) ≈1.5 × 10−8.
Therefore, a robust implementation must address the special case of c large. We do
not discuss the modiﬁcation that is needed.
4.6
Sampling from Gaussian Distributions
We turn now to the problem of generating samples from a general Gaussian probabil-
itydensity.Recallthatif X isamultivariateGaussianrandomvariable, X ∼N(μ, C),
where μ ∈Rn and C ∈Rn×n is a symmetric positive deﬁnite matrix, the probability
density function is
πX(x) =

1
(2π)ndet(C)
1/2
exp

−1
2(x −μ)TC−1(x −μ)

.
The main tool for generating a sample from πX is the whitening process, or Maha-
lanobis transformation, introduced in Sect.4.2. We compute the Cholesky factoriza-
tion of the covariance matrix,
C = RTR,
(4.16)
and represent X in terms of a standard normal random variable W as
X = μ + RTW,
W ∼N(0, In).
With this representation, random draws from πX can be done simply as follows:
1. Draw a realization w ∈Rn from the Gaussian standard normal density N(0, In),
2. Calculate the realization x by the formula
x = μ + RTw.
Notice that the decomposition (4.16) need not necessarily be the Cholesky factor-
ization, but, in fact, any symmetric factorization of C will do. This observation is
particularly useful for Gaussian densities that are given in terms of the precision
matrix, which, by deﬁnition, is the inverse of the covariance matrix, P = C−1 rather
than C itself. In those cases, it is natural to compute the Cholesky (or any other
symmetric) factorization of P,
P = LTL.
https://avxhm.se/blogs/hill0

4.7 Some Useful Sampling Algorithms
67
This factorization implicitly deﬁnes a symmetric factorization of the covariance
matrix,
C = P−1 = L−1L−T,
and we may express the factor R in terms of L as R = L−T. The above algorithm for
drawing samples x can be modiﬁed as follows:
1. Draw a realization w ∈Rn from the Gaussian standard normal density N(0, In),
2. Solve the linear system
w = Lz
for z, and deﬁne
x = μ + z.
The application of this alternative approach will be illustrated later.
4.7
Some Useful Sampling Algorithms
If the probability density of interest is not Gaussian, or one-dimensional, the methods
discussed so far cannot be employed, and the sampling may become more challeng-
ing. In this section, we review some methods that are of interest in those cases, and
that turn out to be particularly useful building blocks for the sequential Bayesian
methods that will be introduced later in the book.
4.7.1
Importance Sampling
Assume that two probability distributions π1 and π2 in Rn are given, and they are
related to each other through the formula:
π2(x) = C f (x)π1(x),
(4.17)
where f (x) ≥0, and C is a normalizing constant guaranteeing that

π2(x)dx = C

f (x)π1(x)dx = 1.
We address the following problem: Assuming that a sampling algorithm from the
distribution π1 is available, how can we use it to produce a sample from the distri-
bution π2? To that end, let ϕ be a test function, that we assume to be continuous and
bounded, and consider the integral
https://avxhm.se/blogs/hill0

68
4
Introduction to Sampling

ϕ(x)π2(x)dx = C

ϕ(x) f (x)π1(x)dx,
(4.18)
that we want to approximate by a ﬁnite sum. By assumption, we have a method of
sampling from the distribution π1. Let {(x j, w j)}N
j=1 be a sample from π1. Then

R
ϕ(x)π1(x)dx ≈
N

j=1
w jϕ(x j),
or, in terms of densities,
π1(x) ≈
N

j=1
w jδx j (x)
in the weak sense. This implies that we may approximate the integral (4.18) with

ϕ(x)π2(x)dx = C

ϕ(x) f (x) π1(x)dx ≈C
N

j=1
w j f (x j)ϕ(x j),
suggesting that
π2(x) ≈
N

j=1

Cw j f (x j)

δx j (x) =
N

j=1
w jδx j (x),
w j = Cw j f (x j),
that is, the desired sample W from π2 consists of the points in the sample from π1,
and only the weights need to be updated. Observe that although the normalizing
constant C may not be known, we need the new weights w j to deﬁne a probability
measure, so it is natural to assume that
N

j=1
w j = 1.
This leads to the following algorithm.
Importancesampling:Assumethattheprobabilitydensitiesπ1 andπ2 satisfy(4.17),
and that a sample {(x j, w j)}N
j=1 from π1 is available. Then the sample {(x j, w j)}N
j=1,
where the x js are the same and the weights w j are computed from the w j by
1. calculating w j = w j f (x j),
2. normalizing
w j →
w j
N
k=1 wk
,
approximates π2.
https://avxhm.se/blogs/hill0

4.7 Some Useful Sampling Algorithms
69
Fig. 4.9 Left: Uniformly distributed sample with equal weights over the square [−1.5, 1.5] ×
[−1.5, 1.5], and the density π2. Right: The sample obtained by importance sampling, the areas
of the dots being proportional to the corresponding weights. The crosshair indicates the estimated
mean of the density π2
In other words, if two distributions are related via a known functional form (4.17),
and one of them is approximated by a sample of ordered pairs of points and weights,
to approximate the other it sufﬁces to update the weights. This algorithm will turn
out to be very useful in the context of sequential updating.
Example 4.4 Consider the horseshoe distribution of Example 4.1, denoted here
by π2. We choose the distribution π1 to be a uniform distribution over the square
[−1, 5, 1.5] × [−1.5, 1.5], and draw a sample of N = 1 000 points from it, shown
on the left of Fig. 4.9. Using the importance sampling, we then adjust the weights
w j = 1/N. The sample points are shown on the right of Fig. 4.9, where the area of
each dot is proportional to the weight w j. The ﬁgure also shows the estimated mean
of π2, calculated by using the sample,
x =
N

j=1
w jx j.
Observe that while the algorithm is rather simple and easy to implement, it is of
limited use for high-dimensional integration, since it is open to the same criticism
as deterministic quadrature rules: Most of the evaluations to compute w j are giving
a negligible contribution, and the method requires a knowledge of a good bounding
box for generating the initial sample.
https://avxhm.se/blogs/hill0

70
4
Introduction to Sampling
4.7.2
Drawing from Mixtures: SIR and Weighted Bootstrap
Another way to approximate a probability density is in terms of a mixture of some
elementary probability densities πℓ,
πX(x) =
M

ℓ=1
αℓπℓ(x),
(4.19)
where the weights αℓare non-negative. The densities πℓare chosen so that drawing
from them is easy. If the densities πℓare scaled to have the integral equal to one, we
have

πX(x)dx =
M

ℓ=1
αℓ

πℓ(x)dx =
M

ℓ=1
αℓ,
and it is natural to assume that the weights αℓare normalized so that
M

ℓ=1
αℓ= 1.
In the commonly used Gaussian mixtures, each πℓis a Gaussian density,
πℓ(x) = N(x | μℓ, Cℓ),
with mean μℓand covariance matrix Cℓ.
In order to design a sampling algorithm from a mixture, we introduce the new
integer-valued random variable L that can take on the values {1, 2, . . . , M}, and
postulate that
πL(ℓ) = P{L = ℓ} = αℓ,
1 ≤ℓ≤M.
Furthermore, we deﬁne the conditional densities
πX|L(x | ℓ) = πℓ(x),
so that the mixture model (4.19) becomes simply a marginal density,
πX(x) =
M

ℓ=1
πX|L(x | ℓ)πL(ℓ).
The sampling strategy is to ﬁrst draw pairs (x j, ℓj) from the joint probability density
πX|L(x | ℓ)πL(ℓ),thenmarginalizebyprojectingthesampleontotheﬁrstcomponent.
https://avxhm.se/blogs/hill0

4.7 Some Useful Sampling Algorithms
71
In summary, we draw pairs (x, ℓ) from the joint density by repeating the following
steps:
1. Draw the realizations ℓj of the random variable L from the discrete distribution
with point masses πK(ℓ) = αℓ;
2. For each selected ℓj, draw x j from the conditional density πX|L(x | ℓj) = πℓj(x).
Finally, we marginalize by discarding the indices ℓj, keeping only the sample
{x1, x2, . . . , x M}.
Consider the special case when the Gaussian densities are deﬁned as
πℓ(x) = N(x | μℓ, h2In),
h > 0.
In the limit h →0+, when the distributions in the mixture become point masses,
πX|L(x | ℓ) = δμℓ(x),
we have a particular instance of the weighted bootstrap algorithm, and the probability
density is approximated as
πX(x) ≈
M

ℓ=1
αℓδμℓ(x).
(4.20)
One might wonder what the reason is for drawing samples from such an approxi-
mation, when the approximation itself is already a discrete approximation, and the
sample S =

(μ1, α1), . . . , (μM, αM)

is readily available. As will become evident
later, sometimes it is preferable to work with a sample where all sample points have
the same weights. Another instance when it is necessary to draw from an approxima-
tion is when one needs a sample of size different from M. In this case, drawing from
πX|L(x | ℓj) = δμℓj (x) can produce only one possible outcome, namely x j = μℓj.
Observe that if the weights αℓare of different magnitudes, the sampling is likely
to produce several copies of the same points with high weights, while some other
points with low weights are likely not to be represented at all. Thus, the importance of
points x j in the new sample is encoded in the possible repetition of particles having
originally a larger weight, called importance weight. For this reason, the resulting
algorithm is known as sampling-importance resampling (SIR).
In the discussion above, we assumed that a mixture model (4.19) was given, and
outlined a two-phase algorithm for drawing from it. Conversely, assume that a sample
{(ξ1, w1), . . . , (ξM, wM)} from the density πX is given, and we want to ﬁnd a mixture
model in order to draw new points from πX. If we use the approximation
πX(x) ≈
M

ℓ=1
wℓδξℓ(x),
(4.21)
https://avxhm.se/blogs/hill0

72
4
Introduction to Sampling
the SIR algorithm has the tendency to produce repeated examples of the same point,
often referred to as data thinning or sample impoverishment: A sample consisting
of mostly few repeated entries reﬂects poorly the variability of the underlying distri-
bution. To avoid this problem, we build a Gaussian mixture model using the known
sample.
To generate a Gaussian mixture, we ﬁrst estimate the mean and covariance of the
underlying density πX using the base sample,
μ =
M

j=1
w jξ j,
C =
M

j=1
w j(ξ j −μ)(ξ j −μ)T.
(4.22)
Next, we propose a Gaussian mixture model by deﬁning a small Gaussian density
around each base point, writing
πX(x) ≈πX(x) =
M

j=1
w jN(x | ξ j, h2C),
(4.23)
where h2 > 0 is a small parameter controlling the width of the Gaussians around
the base points. With this approximation, the weighted bootstrap produces a sample
similar to the one obtained with the SIR, except for the fact that the base points are
not repeated, but replaced by draws from small Gaussians centered at the base points.
Letting h2 →0, the algorithm approaches SIR.
While, at ﬁrst sight, all looks under control, it turns out that the algorithm, as
proposed, has a small problem. Ideally, the mean and the covariance of the approx-
imation (4.23) should be close to μ and C. This is indeed the case for the sample
mean,

xπX(x)dx =
M

j=1
w j

xN(x | ξ j, h2C)dx
=
M

j=1
w jξ j = μ.
Consider the covariance, writing it as

(x −μ)(x −μ)TπX(x)dx =
M

j=1
w j

(x −μ)(x −μ)TN(x | ξ j, h2C)dx,
(4.24)
and notice that each integral in the sum is
https://avxhm.se/blogs/hill0

4.7 Some Useful Sampling Algorithms
73

(x −μ)(x −μ)TN(x | ξ j, h2C)dx
=
 
x −ξ j + (ξ j −μ)

x −ξ j + (ξ j −μ)
TN(x | ξ j, h2C)dx
=
 
x −ξ j
x −ξ jTN(x | ξ j, h2C)dx
+
 
x −ξ j
ξ j −μ
TN(x | ξ j, h2C)dx
+
 
ξ j −μ

x −ξ jTN(x | ξ j, h2C)dx
+
 
ξ j −μ

ξ j −μ
TN(x | ξ j, h2C)dx
= h2C +

ξ j −μ

ξ j −μ
T,
(4.25)
since the second and third terms in (4.25) vanish. Substituting this expression into
(4.24) yields

(x −μ)(x −μ)TπX(x)dx = h2C +
M

j=1
w j

ξ j −μ

ξ j −μ
T
= (1 + h2)C,
Thus, we conclude that replacing the point masses by Gaussians automatically
increases the covariance by a factor of 1 + h2. This can be corrected by shifting
the base points ξ j slightly towards their common mean μ. To accomplish this, we
deﬁne
ξ
j = aξ j + (1 −a)μ,
a =

1 −h2,
(4.26)
and redeﬁne the Gaussian mixture density as
πX(x) =
M

j=1
w jN(x | ξ
j, h2C).
A similar computation shows that

xπX(x)dx =
M

j=1
w jξ j = aμ + (1 −a)μ = μ,
and since
ξ
j −μ = a(ξ j −μ),
https://avxhm.se/blogs/hill0

74
4
Introduction to Sampling
it follows that

(x −μ)(x −μ)TπX(x)dx =
M

j=1
w j

h2C +

ξ
j −μ

ξ
j −μ
T
= h2C + a2
M

j=1
w j

ξ j −μ

ξ j −μ
T
= C,
as desired.
We are now ready to summarize the procedure in the form of an algorithm.
Weighted bootstrap algorithm: To generate a sample

x1, . . . , x N
based on the
approximation (4.21),
1. Compute the empirical mean and covariance μ and C of the distribution from the
base sample using formulas (4.22).
2. Choose h > 0, and compute the shifted base sample {ξ
1, . . . , ξ
M
according to
formula (4.26).
3. For j = 1, 2, . . . , N repeat
• Draw an index ℓj using the relative weights w1, . . . , wM as probabilities,
• Draw x j from the Gaussian distribution N(ξ
ℓj
, h2C).
Figure 4.10 shows a graphical rendition of mixture model.
4.8
Rejection Sampling: Prelude to Metropolis–Hastings
A common strategy to produce samples from a given distribution is to make proposals
using a surrogate distribution, from which it is easy to draw samples, then either reject
or accept the proposal. We illustrate this with a simple example.
Example 4.5 Consider the Gaussian density with a bound constraint introduced in
Example 4.3,
πX(x) =

Ce−x2/2, when x > c,
0
otherwise,
where C > 0 is a normalizing constant. The most straightforward and intuitive
approach for generating a sample from this distribution is the following trial-and-
error algorithm:
1. Draw x from the normal distribution N(0, 1),
2. If x > c, accept, otherwise reject.
https://avxhm.se/blogs/hill0

4.8 Rejection Sampling: Prelude to Metropolis–Hastings
75
Fig. 4.10 Top left: A small sample drawn from a Gaussian, the ellipse of two standard deviations
shownintheﬁgure.Topright:Ellipsesoftwostandarddeviationsofthedistributions N(x | ξ j, h2C)
drawn around the points ξ j, with h = 0.3. Bottom left: The shifted points ξ
j are plotted by red
dots. Bottom right: Ellipses of two standard deviations of the distributions N(x | ξ
j, h2C) drawn
around the points ξ
j
Observe that this is a version of the importance sampling algorithm, as the distri-
bution can be written as
πX(x) = C H(x −c)N(x | 0, 1),
where H is the Heaviside step function, and the importance weights of points drawn
from the normal distribution are either zero or one, depending on which side of the
lower bound c the point falls.
While the algorithm works, it can be extremely slow. To understand why, consider
the acceptance rate for different values of c. If c = 0, in the average every other
proposal will be accepted, since the underlying normal distribution is symmetric. If
c < 0, the acceptance rate will be higher than 50%. On the other hand, if c > 0, it will
be smaller. In general, the probability of hitting the domain of acceptance {x > c} is
I (c) =
1
√
2π
 ∞
c
e−x2/2dx,
https://avxhm.se/blogs/hill0

76
4
Introduction to Sampling
and I (c) →0 superexponentially as c →∞. For instance, if c = 3, the acceptance
rate is less than 0.1%.
The situation degrades dramatically for multidimensional problems. Consider, for
example, a multivariate distribution in Rn,
π(x) ∝π+(x)e−∥x∥2/2,
x ∈Rn,
where
π+(x) =
1, if x j > 0 for all j,
0, else.
What is the acceptance rate using the trial-and-error algorithm? The normal distri-
bution is symmetric around the origin, so any sign combination is equally probable.
Since
P{x j > 0} = 1
2,
1 ≤j ≤n,
the rate of successful proposals is
P{x1 > 0, x2 > 0, . . . , xn > 0} =
n
j=1
P{x j > 0} =

1
2
n
,
and it is clear that even a simple positivity constraint puts us out of business if n is
large. The central issue therefore becomes how to modify the initial random draws,
step 1 above, to improve the acceptance rate.
Inspired by the trial-and-error drawing strategy of the previous example while
acknowledging its limitations, we propose an elementary sampling strategy that
works—at least in theory—also for multidimensional distributions. The proposed
algorithm is known as the rejection sampling algorithm.
Assume that the true probability density πX(x), from which we want to draw a
sample, is known up to a multiplicative constant, i.e., we have the function
p(x) = CπX(x),
with C > 0 unknown. In principle, if we had a way to integrate p over Rn, the
constant C could be determined, however, computing the integral of p may be hard,
and in fact, one aim of the whole sampling is to give a way to estimate the integral.
Assume that we have a proposal distribution, q(x) that is easy to use for random
draws—for instance, a Gaussian distribution. Furthermore, assume that the proposal
distribution satisﬁes the condition
p(x) ≤Mq(x) for some M > 0.
The following algorithm is a prototype for the Metropolis–Hastings algorithm that
will be introduced later:
https://avxhm.se/blogs/hill0

4.8 Rejection Sampling: Prelude to Metropolis–Hastings
77
1. Draw x from the proposal distribution q(x),
2. Calculate the acceptance ratio
α =
p(x)
Mq(x),
0 < α ≤1.
3. Flip the α-coin: Draw t ∼Uniform([0, 1]), and accept x if α > t, otherwise
reject.
The last step says simply that x is accepted with probability α, rejected with
probability 1 −α, thus the name α-coin.
Why does the algorithm work? What we need to show is that the distribution of
the accepted proposals x is πX. To this end, we introduce the binary random variable
A,
A =
1, if a draw from q is accepted,
0, if a draw from q is rejected.
The distribution of the accepted sample points drawn from q, i.e., the distribution of
the sample that we have just generated, is
πX|A(x | 1) = distribution of x’s, given that they are accepted.
To calculate this distribution, we use Bayes’ formula,
πX|A(x | 1) = πA|X(1 | x)q(x)
πA(1)
,
(4.27)
setting the prior distribution of x equal to q, since we draw x from q. One interpre-
tation of (4.27) is that before testing for acceptance, we believe that q is the correct
distribution of x, hence our prior. What are the other densities appearing above?
The probability of acceptance provided that x is given, is clearly the acceptance
ratio,
πA|X(1 | x) = α =
p(x)
Mq(x).
The marginal density of the acceptance is, in turn,
πA(1) =

πX,A(x, 1)dx =

πA|X(1 | x)q(x) dx
=

p(x)
Mq(x)q(x) dx = C
M

πX(x) dx
= C
M .
https://avxhm.se/blogs/hill0

78
4
Introduction to Sampling
Substituting the derived expressions into (4.27), we have
πX|A(x | 1) =
p(x)
Mq(x)q(x)
C
M
= p(x)
C
= πX(x).
exactly as we wanted.
We conclude this section verifying that the example of Gaussian distribution with
a bound constraint is a version of a rejection sampling algorithm. In that case, the
true distribution, up to a normalizing constant, is
p(x) = H(x −c)e−x2/2,
the proposal distribution is Gaussian,
q(x) =
1
√
2π
e−x2/2,
and the scaling constant M can be chosen as M =
√
2π so that
Mq(x) = e−x2/2 ≥p(x).
The acceptance rate is then
α =
p(x)
Mq(x) = H(x −c) =
1, if x > c,
0, if x ≤c.
Flipping the α-coin in this case says that we accept the proposal automatically if
x > c (α = 1), and reject otherwise (α = 0), which is exactly what our heuristic
algorithm was doing.
Notes and Comments
The factor 1/(N −1) in the unbiased estimator Cu, formula (4.5) follows from the
consideration that the estimated mean value itself is a realization of a random variable
N −1(X(1) + . . . + X(N)), where the variables X( j) are independent and identically
distributed, see, e.g, [1].
The P–P plots in this chapter were discussed only in R2 in order to keep the
integrals simple. The modiﬁcation for Rn is straightforward, requiring only that we
replace (4.10) by the integrals
https://avxhm.se/blogs/hill0

4.8 Rejection Sampling: Prelude to Metropolis–Hastings
79
P

∥W∥< δ

=

 1
2π
n/2 
Dδ
e−1
2 ∥w∥2dw
=

 1
2π
n/2  δ
0

Sn−1 e−1
2 t2tn−1dSdt
= |Sn−1|
(2π)n/2
 δ
0
e−1
2 t2tn−1dt,
where Sn−1 is the unit sphere in Rn and |Sn−1| is its surface area. By a change of
variables, the integral can be expressed in terms of the lower incomplete gamma
function,
γ(m, s) =
 s
0
e−ttm−1dt.
Recalling that the surface area of the n-sphere is
|Sn−1| =
2πn/2
Γ (n/2),
where Γ is the complete gamma function, we arrive at the formula
P

∥W∥< δ

= γ(n/2, δ2/2)
Γ (n/2)
,
which approaches 1 as δ approaches inﬁnity.
Estimates for the convergence rate of Gauss quadratures can be found in standard
books of numerical analysis, see. e.g., [72].
It was pointed out in a footnote that the convergence of the linear combinations of
point masses towards the distribution should be understood in the weak∗sense. The
reason for this nomenclature is that the dual space, C(I)∗, of bounded continuous
functions over the interval I, consists, by deﬁnition, of linear functionals C(I) →R,
and by the Riesz–Markov–Kakutani representation theorem, each functional has a
unique representation in terms of Radon measures over I [68]. Hence, the described
weak convergence takes place in the dual space, thus the notation.
The Gaussian mixture approximation using the shifted midpoints is discussed by
Mike West in [82].
https://avxhm.se/blogs/hill0

Chapter 5
The Praise of Ignorance: Randomness
as Lack of Certainty
“Quindi non avete una sola risposta alle vostre domande?”
“Adson, se l’avessi insegnerei teologia a Parigi.”
“A Parigi hanno sempre la risposta vera?”
“Mai,” disse Guglielmo, “ma sono molto sicuri dei loro errori.”
(Umberto Eco: Il Nome della Rosa)
“So you don’t have a unique answer to your questions?”
“Adson, if I had, I would teach theology in Paris.”
“Do they always have a right answer in Paris?”
“Never”, said William, “but there they are quite conﬁdent of their errors.”
(Umberto Eco: The Name of the Rose)
After the lengthy technical introduction of the previous chapters, we are now ready
to start estimating unknown quantities based on incomplete information and indirect
observations. We adopt here the Bayesian point of view: Any quantity that is not
known exactly, in the sense that a value can be attached to it with no uncertainty, is
modeled as a random variable. In this sense, randomness means lack of certainty.
The subjective part of this approach is clear: even if we believed that an underlying
parameter corresponds to an existing physical quantity that could, in principle, be
determined and therefore is conceptually deterministic, the lack of the subject’s
information about it justiﬁes modeling it as a random variable. The question of
whether a parameter will be modeled as a random variable is then answered according
to how much we know about the quantity or how strong our beliefs are. This general
guiding principle will be followed throughout the rest of the book, applied to various
degrees of rigor.1
When employing statistical techniques to solve inverse problems, the notion of
parameter needs to be extended and elaborated. In classical statistics, parameters are
often regarded as tools, like, for example, the mean or the variance, which may be
used to identify a probability density. It is not uncommon that even in that context,
1 After all, as tempting as it may sound, we don’t want to end up teaching theology in Paris.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
D. Calvetti and E. Somersalo, Bayesian Scientiﬁc Computing, Applied Mathematical
Sciences 215, https://doi.org/10.1007/978-3-031-23824-6_5
81
https://avxhm.se/blogs/hill0

82
5
The Praise of Ignorance: Randomness as Lack of Certainty
parametersmayhaveaphysical2 interpretation,yettheyaretreatedasabstractobjects.
In inverse problems, parameters are more often by deﬁnition physical quantities, but
they are regarded as statistical model parameters deﬁning probability densities. Dis-
quisitions about such differences in interpretation may seem unimportant, but these
very issues often complicate the dialogue between statisticians and “inversionists.”
Example 5.1 Consider the general inverse problem of estimating a quantity x ∈Rn
that cannot be observed directly, but for which indirect observations of a related
quantity y ∈Rm are available. We may, for example, be interested in the concentra-
tions of certain chemical species (variable x) in a gas sample, but, for some reason,
we cannot measure them directly; instead, we observe spectral absorption lines of
light that passes through the specimen (variable y). A mathematical model describing
light absorption by a mixture of different chemical compounds ties these quantities
together. The fact that the variables that we are interested in are concentration values
already carries a priori the information that they cannot take on negative values. In
addition, by knowing where the sample is taken from, regardless of the subsequent
measurement, may already give us a good idea of what to expect to be found in the gas
sample. For instance, a sample taken from the Earth’s atmosphere probably contains
signiﬁcant amounts of nitrogen and oxygen in approximately known concentrations.
In fact, the whole process of measuring may be performed to conﬁrm a hypothesis
about the concentrations.
In order to set up the statistical framework, we need to express the distribution
of y in terms of the parameter x. This is done by constructing the likelihood model.
The design of the prior model will take care of incorporating any available prior
information.
As the preliminary example above suggests, the statistical model for inverse prob-
lems comprises two separate parts:
1. The construction of the likelihood model;
2. The construction of the prior model,
both of which make extensive use of conditioning and marginalization. When several
random variables enter the construction of a model, conditioning is one way to take
into consideration one unknown at the time, assuming that the others are known.
This allows us to construct complicated models step by step. For example, the joint
density of x and y can be expressed in terms of the density of y assuming that x is
known, multiplied by the density of x. This is the essence of the formula,
πXY(x, y) = πY|X(y | x)πX(x).
If, on the other hand, some of the variables appearing in the model are of no interest,
we can eliminate them from the density by marginalizing them, that is by integrat-
ing them out. For example, if we have the joint density of three random variables
2 The word “physical” in this context may be misleading in the sense that it makes us think of physics
as a discipline. The use of the word here is more general, almost a synonym of “material” or “of
observable nature,” as opposed to something that is purely abstract.
https://avxhm.se/blogs/hill0

5.1 Construction of Likelihood
83
πXY V (x, y, v), but we are not interested in V , we can marginalize V out by observing
that
πXY(x, y) =

πXY V (x, y, v)dv.
The parameter v of no interest is often referred to as noise, or as a nuisance parameter.
To set up the big picture, let X be a random variable representing the quantity
of primary interest, and let Y denote a random variable representing the observable
quantity. In the Bayesian approach to inverse problems, rather than trying to ﬁnd a
formula mapping Y to X, we ask what can be said about the possible values of X
given that a realization y of Y is observed. The key to answer that question is Bayes’
formula,
πX|Y(x | y) = πX(x)πY|X(y | x)
πY(y)
,
y = observed value of Y.
To explain this formula in terms of the subjective probabilities discussed in Chap. 1,
πX encodes what we believe about X prior to observing Y, therefore we refer to it as
prior density. The term πY|X expresses the likelihood of a given observation Y = y,
explaining why this term is referred to as likelihood. Finally, πX|Y expresses what
we believe about X posterior to the observation, giving rise to the name posterior
density. The formula above thus constitutes an updating scheme from “before” to
“after.” Ignoring the marginal value πY(y) in the denominator, Bayes’ formula is
often summarized as
posterior ∝likelihood × prior.
Adhering to the principle that statistics and probability are “common sense reduced to
calculations,” there is no universal prescription for the design of priors or likelihoods,
althoughsomerecipesseemtobeusedmoreoftenthanothers.Thesewillbediscussed
next.
5.1
Construction of Likelihood
From the point of view of inverse problems, the likelihood encodes the mathematical
model connecting the unknown of primary interest to the observation, taking all
possible uncertainties into consideration. It is in that frame of mind that we can think
of the likelihood as answering the following question: If we knew the unknown x
and all other model parameters deﬁning the data, how would the measurements be
distributed?
Since the construction of the likelihood starts from the assumption that, if X = x
were known, the measurement Y would be a random variable, it is important to
understand the nature of its randomness. Randomness being synonymous of lack of
https://avxhm.se/blogs/hill0

84
5
The Praise of Ignorance: Randomness as Lack of Certainty
certainty, we need to analyze what would make the data deviate from the predictions
of our observation model. The most common sources of such deviations are
1. measurement noise in the data;
2. incompleteness of the observation model.
The probability density of the noise can, in turn, depend on unknown parameters, as
will be demonstrated later through some examples. The second source of randomness
is more complex, as it includes errors due to discretization, model reduction, and,
more generally, all the shortcomings of a computational model, that is the discrepancy
between the model and “reality”—in the heuristic sense of the word.3
5.2
Noise Models
We start by considering the construction of the likelihood based on different noise
models, assuming that the forward model is completely known and a faithful descrip-
tion of reality.
5.2.1
Additive Noise
In inverse problems, it is very common to use additive models to account for mea-
surement noise.
Assume that x ∈Rn is the unknown of primary interest, that the observable quan-
tity y ∈Rm is ideally related to x through a functional dependence
y = f (x),
f : Rn →Rm,
(5.1)
and that we are very certain of the validity of the model. The available measurement,
however, is corrupted by noise, which we attribute to external sources or to insta-
bilities in the measuring device, hence not dependent on x. Therefore, we write the
additive noise model
Y = f (X) + E,
(5.2)
where E : Ω →Rm is the random variable modeling the noise. Observe that since
X and Y are unknown, they are modeled as random variables—hence upper case
letters—and (5.2) can be thought of as a stochastic extension of the deterministic
model (5.1).
3 Writing a model for the discrepancy between the model and reality implicitly, and arrogantly,
assumes that we know the reality and we are able to tell how the model fails to describe it. Under
these assumptions, the word “reality” is used in quotes, and should be understood as the “most
comprehensive description available.”
https://avxhm.se/blogs/hill0

5.2 Noise Models
85
Let us denote the distribution of the error by
E ∼πE(e).
Since we assume that the noise does not depend on X, ﬁxing X = x does not affect
in any way the probability distribution of E. More precisely,
πE|X(e | x) = πE(e),
that is, the knowledge of X adds nothing to our knowledge of the noise. If, on the
other hand, X is ﬁxed, the only randomness in Y is due to E. Therefore,
πY|X(y | x) = πE(y −f (x)),
(5.3)
that is, the randomness of the noise is translated by f (x), as illustrated in Fig. 5.1.
In this setting, we assume that the distribution of the noise is known. Although this
is a common assumption, in practice the distribution of the noise is seldom known
exactly. More typically, the noise distribution itself may depend on parameters, col-
lected in a variable θ. Writing the parametric model as
πE(e) = πE(e | θ),
Equation (5.3) becomes
πY|X(y | x, θ) = πE(y −f (x) | θ).
In anticipation of addressing the case in which the parameter θ may be poorly known,
we include it explicitly in the formulas using the notation of conditional densities.
At this point, however, θ is just a parameter in the likelihood model.
Fig. 5.1
Additive noise: the
noise around the origin is
shifted to a neighborhood of
f (x) without otherwise
changing the distribution
https://avxhm.se/blogs/hill0

86
5
The Praise of Ignorance: Randomness as Lack of Certainty
To illustrate the notation, assume that the noise E is zero-mean Gaussian with
independent equally distributed components having variance σ 2, that is,
E ∼N(0, σ 2Im),
where Im ∈Rm×m is the identity matrix. The corresponding likelihood model is then
πY|X(y | x, σ 2) =
1
(2π)m/2σ m exp

−1
2σ 2 ∥y −f (x)∥2

,
with θ = σ 2. If the noise variance is assumed known, we usually do not write the
dependency explicitly, instead using the notation
πY|X(y | x) ∝exp

−1
2σ 2 ∥y −f (x)∥2

,
and ignoring the normalizing constant altogether. If the variance is unknown, the
dependency of the normalizing constant on it needs to be taken into account.
5.2.2
Multiplicative Noise
When discussing the additive noise, we formally solved for the noise E when X = x,
and obtained a formula
E = Y −f (x) = φ(Y),
that is, the two random variables E and Y are related through a simple one-to-one
mapping comprising a shift by f (x). The change of variable formula discussed in
Sect. 1.2.4 yields the likelihood. Observe that here, the Jacobian of φ is identity:
when the noise is multiplicative, this is no longer the case, as is illustrated in the
following example.
Example 5.2 Consider a noisy ampliﬁer that takes in a signal f (t) and sends it out
ampliﬁed by a constant factor α > 1. We assume that the output signal is observed,
and the input signal needs to be estimated.
If α is known and constant, the ideal model for the output signal is
g(t) = αf (t),
0 ≤t ≤T.
In practice, however, it may happen that the ampliﬁcation factor is not constant
but ﬂuctuates slightly around a mean value α0. To express that, we write a discrete
likelihood model for the output by ﬁrst discretizing the signal. Let
x j = f (t j),
y j = g(t j),
0 = t1 < t2 < · · · < tn = T.
https://avxhm.se/blogs/hill0

5.2 Noise Models
87
Assuming that the ampliﬁcation at t = t j is a j, we have a discrete model
y j = a jx j,
1 ≤j ≤n,
and, replacing the unknown quantities by random variables, we obtain the stochastic
extension
Y j = A j X j,
1 ≤j ≤n,
which we write in vector notation as
Y = A ⊙X,
(5.4)
the symbol ⊙denoting component-wise multiplication of the vectors A, X ∈Rn.
Assume that A as a random variable is independent of X, as is the case, for instance,
if the random ﬂuctuations in the ampliﬁcation are due to thermal phenomena. If A
has the probability density
A ∼πA(a),
to ﬁnd the probability density of Y, conditioned on X = x, we ﬁx X = x and write
A j = Y j
x j
,
1 ≤j ≤n.
Introduce the mapping φ : Rn →Rn,
φ(y) =
⎡
⎢⎣
y1
x1...
yn
xn
⎤
⎥⎦= y ⊘x,
where the symbol ⊘denotes componentwise division, whose Jacobian is
Dφ(y) =
⎡
⎢⎣
1
x1
...
1
xn
⎤
⎥⎦,
and conclude that
πY|X(y | x) = |det(Dφ(y))|πA(φ(y)) =
1
x1x2 · · · xn
πA (y ⊘x) .
(5.5)
Let us consider the case where all the variables are assumed to be positive, and
A is log-normally distributed, i.e., the logarithm of A is normally distributed. For
simplicity, we assume that the components of A are mutually independent, identically
distributed. We write a stochastic model for A j
A j = eμ+σ W j,
W j ∼N(0, 1),

88
5
The Praise of Ignorance: Randomness as Lack of Certainty
and solving for W j, we have
W j = 1
σ

log A j −μ

,
1 ≤j ≤n,
or, in vectorial form,
W = ψ(A),
ψ j(A) = 1
σ

log A j −μ

.
For the change of variables formula, we need the Jacobian of ψ which is
Dψ(a) = 1
σ
⎡
⎢⎣
1
a j
...
1
an
⎤
⎥⎦.
Noting that the distribution of W is
πW(w) =
 1
2π
n/2
exp
⎛
⎝−1
2
n

j=1
w2
j
⎞
⎠,
and writing μ = log a0, we obtain
πA(a) = |det

Dψ(a)

|πW(φ(a))
=
1
σ na1 · · · an
 1
2π
n/2
exp
⎛
⎝−1
2σ 2
n

j=1
(log a j −μ)2
⎞
⎠
=
1
a1 · · · an

1
2πσ 2
n/2
exp
⎛
⎝−1
2σ 2
n

j=1

log a j
a0
2
⎞
⎠.
Consequently, the likelihood (5.5) with this particular noise model is
πY|X(y | x) =

1
2πσ 2
n/2
1
x1 · · · xn
1
y1
x1 · · · yn
xn
exp
⎛
⎝−1
2σ 2
n

j=1

log
y j
a0x j
2
⎞
⎠
=

1
2πσ 2
n/2
1
y1 · · · yn
exp

−1
2σ 2 ∥log

y⊘(a0x)

∥2

,
where it is understood that the logarithm is applied component-wise.

5.2 Noise Models
89
5.2.3
Poisson Noise
In the previous examples, we started with an ideal deterministic model and corrupted
the model adding independent noise. There are important applications where such
error models are not appropriate, as in the following example where the forward
model itself is intrinsically probabilistic.
Example 5.3 Assume that our measuring device is a photon collector, depicted
schematically in Fig. 5.2 as a lens, and a counter of the photons emitted from N
discrete sources, with average photon emission per observation time equal to x j,
1 ≤j ≤N. Our goal is to estimate the total emission from each source over a ﬁxed
time interval.
We model the photon collector by assuming that the total photon count is the
weighted sum of the individual contributions. When the device is positioned above
the jth source, it collects the photons from its immediate neighborhood. If we denote
by ak the weights of the contributions from sources located k positions away from
the position of the device, the conditional expectation of the photons arriving to the
counter is
y j = E

Y j | x

=
L

k=−L
akx j−k.
The weights a j are determined by the device and the index L is related to the width
of the collector, as can be seen in Fig. 5.2. It is implicitly understood that x j = 0
whenever j < 1 or j > N.
Considering the ensemble of all source points at once, we can express the condi-
tional expectation in vector form
Fig. 5.2
The expected contributions from the different sources. The scalars ak are the weights
by which each source point x j±k contributes to the expected photon count when the center of the
photon collector is above the jth point source

90
5
The Praise of Ignorance: Randomness as Lack of Certainty
y = E

Y | x

= Ax,
where A ∈RN×N is the Toeplitz matrix
A =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
a0 a−1 · · · a−L
a1 a0
...
...
...
a−L
aL
...
...
...
a0 a−1
aL · · · a1 a0
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
The parameter L deﬁnes the bandwidth of the matrix.
If the sources are weak emitters, the observation model just described is a photon-
counting process. We may think that each Y j is a Poisson process with mean y j
Y j | x ∼Poisson

(Ax) j

,
that is,
πY j|X(y j | x) =
∞

k=0
δk(y j)
(Ax)k
j
k!
exp

−(Ax) j

.
Observe that, in general, there is no guarantee that the expectations (Ax) j are inte-
gers. If we assume that consecutive measurements are conditionally independent,
the random variable Y ∈RN has probability density
πY|X(y | x) =
N

j=1
πY j|X(y j | x),
that is, the probability of a given outcome y = (y1, y2, . . . , yN), given x, is
P

Y = (y1, y2, . . . , yN) | x

=
N

j=1
(Ax)
y j
j
y j!
exp

−(Ax) j

.
We express this relation simply by writing
Y | X ∼Poisson(Ax).
If we assume that the photon count is relatively large, the likelihood model can
be approximated using the Gaussian approximation of the Poisson density discussed
in Sect. 3.4.1, as

5.2 Noise Models
91
P

Y = y | x

≈
N

j=1

1
2π(Ax) j
1/2
exp

−
1
2(Ax) j

y j −(Ax) j
2

(5.6)
=

1
(2π)Ndet(C(x))
1/2
exp

−1
2(y −Ax)TC(x)−1(y −Ax)

,
where
C(x) = diag

Ax

.
We point out that, for simplicity, we did not include the continuity correction in the
formula above. The dependency on x is complicated here by the dependency of the
covariance C on x.
It is instructive to see what the Poisson noise looks like, and to see how Poisson
noise differs from Gaussian noise with constant variance. In Fig. 5.3, we have plotted
a piecewise linear function f : [0, 1] →R, and assume that f (t j) at discrete points
t j = j/N represent the mean values of mutually independent Poisson distributed
random variables
X j ∼Poisson( f (t j)),
0 ≤j ≤N.
For each j, we draw a random realization x j and plot x j versus t j. From the plot, it is
evident that the higher the mean, the higher the variance, in agreement with the fact
that the mean and the variance are equal. By visual inspection, Poisson noise could
be confused with multiplicative noise discussed in the previous section.
0
0.2
0.4
0.6
0.8
1
0
10
20
30
40
50
60
70
0
0.2
0.4
0.6
0.8
1
−15
−10
−5
0
5
10
15
20
Fig. 5.3
Left panel: the average piecewise linear signal and a realization of a Poisson process,
assuming that the values at each discretization point are mutually independent. Right panel: the
difference between the noisy signal and the average

92
5
The Praise of Ignorance: Randomness as Lack of Certainty
5.2.4
Composite Noise Models
Sometimes, the noise in the observation may comprise several different processes,
each one contributing to the uncertainty in its own particular way. As an example elu-
cidating this case, we assume that the photon counting convolution device discussed
previously adds a noise component to the collected data. More precisely, consider
an observation model of the form
Y = Z + E,
where Z ∼Poisson(Ax) and E ∼N(0, σ 2I).
To write the likelihood model, we begin by considering the conditional density of
the jth component of Y, Y j, assuming that X = x and Z j = z j are known, so that
the only uncertainty arises from the additive noise. Then,
πY j|Z j X(y j | z j, x) ∝exp

−1
2σ 2 (y j −z j)2

,
where we have left out the normalizing constant that depends on the known variance
σ 2. Observe that although x does not appear explicitly here, it is, in fact, a hidden
parameter that affects the distribution of Z j. It follows from the formula for the
conditional probability density that
πY j Z j|X(y j, z j | x) = πY j|Z j X(y j | z j, x)πZ j|X(z j | x),
where
πZ j|X(z j | x) =
∞

k=0
(Ax)k
j
k!
exp

−(Ax) j

δk(z j).
Since the value of z j is not of interest here, we can marginalize it out. We obtain
πY j|X(y j | x) =

πY j|Z j X(y j | z j, x)πZ j|X(z j | x)dz j
=
∞

k=0
 (Ax)k
j
k!
exp

−(Ax) j

πY j|Z j X(y j | z j, x)δk(z j)dz j
=
∞

k=0
(Ax)k
j
k!
exp

−(Ax) j

πY j|Z j X(y j | k, x)
∝
∞

k=0
(Ax)k
j
k!
exp

−(Ax) j

exp

−1
2σ 2 (y j −k)2

,

5.2 Noise Models
93
which gives us the likelihood as a function of the variable of interest x. The joint
likelihood πY|X(y | x) is constructed in the same manner, and we see that
πY|X(y | x) =
n
j=1
πY j|X(y j | x)
(5.7)
=

k∈Nn
(Ax)k1
1
k1!
· · · (Ax)kn
n
kn!
exp
⎛
⎝−
n

j=1
(Ax) j
⎞
⎠exp

−1
2σ 2 ∥y −k∥2

,
where the sum extends over all n-dimensional lattice points of non-negative integers.
Formula (5.7) states that the components Y j are conditionally independent, that is,
the components are mutually independent under the condition that X is known.
Notes and Comments
The likelihood plays the central role in the non-Bayesian statistical inference theory.
Given the likelihood πB|X(b | x), the maximum likelihood estimator (MLE) is a value
of x that maximizes the likelihood. In the non-Bayesian context, x is not a realization
of a random variable, but rather a parameter, and MLE is the value that makes the
observed value b most probable. Most importantly, the likelihood says nothing about
the probability of x, and in fact, in this context, references to probabilities of x are
meaningless. We don’t pursue this argument in this book, but refer to any standard
textbook on likelihood-based inference such as [65].

Chapter 6
Enter Subject: Construction of Priors
“The only relevant thing is uncertainty—the extent of our knowledge and ignorance. The
actual fact of whether or not the events considered are in some sense determined, or known
by other people, and so on, is of no consequence.”
(Bruno deFinetti)
The prior density expresses what we know1 about the unknown variable of interest
before taking the measurements into account. The introduction of the prior is the
reason why Bayesian inference is often characterized as being subjective. In reality,
there are very few situations in which we do not have absolutely any idea about the
unknown that we are trying to infer on. The level of a priori knowledge may vary a
lot depending on how much previous experience with similar types of problems we
have, and how deeply we are trusting our preconceived notions. For example, if the
unknowns in question are concentrations, we know that they cannot take on negative
values, and it would be foolish not to use that kind of knowledge to supplement the
information coming from data. The saying that a specialist is someone who has made
all possible mistakes in a ﬁeld2 is tantamount to saying that a specialist has acquired
a lot of a priori knowledge by integrating past experiences on a particular topic, and
uses the knowledge to avoid further mistakes. Willing or not, even in situations where
there is no reason to have any expectation about the outcome of an event, it is very
hard not to have a prior lurking in the background, which is why when the observed
event is not in agreement with our latent expectations, we are surprised. In the lack
of data, our inference is based solely on our prior. In the Bayesian framework, when
1 Actually, it is not necessary that we know, it is sufﬁcient that we believe in order to come up
with a prior, since the prior is by its very nature subjective. The difference is subtle, as pointed
out in Ludwig Wittgenstein’s On Certainty, note 291: “We know that the earth is round. We have
deﬁnitively ascertained that it is round. We shall stick to this opinion, unless our whole way of
seeing nature changes. “How do you know that?”—I believe it.”
2 The quote is attributed to Niels Bohr in the form “An expert is a man who has made all the mistakes
that can be made, in a narrow ﬁeld.”
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
D. Calvetti and E. Somersalo, Bayesian Scientiﬁc Computing, Applied Mathematical
Sciences 215, https://doi.org/10.1007/978-3-031-23824-6_6
95

96
6
Enter Subject: Construction of Priors
the data become available and are taken into account, the prior is corrected in the
light of them and the new description of the unknown is called the posterior. The
proverbial wise man who changes his mind is deﬁnitely a Bayesian. Philosophically,
declaring our a priori beliefs is a way to clarify what will be used to complement the
information that can be extracted from the data. How much weight the prior has in
guiding our estimation of the unknowns depends on the information content of the
data: if the data are scarce or heavily contaminated by noise, the missing information
will be provided by the prior which therefore may have signiﬁcant inﬂuence on the
estimate. If, on the other hand, the data are plentiful and highly informative, they
will play a big role in the estimation of the unknown, thus decreasing the role of the
prior. The next step is to address how to encode beliefs in the form of mathematical
functions, a task that is particularly challenging when the knowledge is vague and
of qualitative rather than quantitative type. Before addressing that question formally
and systematically, we consider an example that illustrates how sometimes priors are
used without being aware of them.
Example 6.1 Here is a banal example of hidden priors: assume that you want some-
body to pick “a number, any number.” If the somebody is a normal3 human being,
the answer may be 3, 7, 13—very popular choices in Judeo-Christian tradition—or
42—a favorite number in science ﬁction circles.4 You may think that you did not
have any prior on what to expect. Suppose now that the somebody you asked is
a space alien in disguise who, taking your question very seriously, starts to recite:
“40269167330954837...,” continuing over a week. Although eventually the stream
of digits will end, as the number is ﬁnite after all, you start to wonder if a lifetime
is enough to ﬁnish a round of the game. Quite soon, probably much before the end
of the week, you understand that this is not at all what you intended: the response is
deﬁnitely in conﬂict with your prior, even though you were not aware that you had
one. And in fact, the encounter with the alien made you realize that not only your
prior was there, but it was quite strong too: there are inﬁnitely many more numbers
that takes more than a week to say—or your life time, or the lifetime of the universe,
for that matter—than those you expected as an answer5!
Another, more pertinent example comes from the ﬁeld of inverse problems.
Example 6.2 Assume that your new orthopedic surgeon, planning an operation of
your knee that has caused problems, has asked you to bring at the next visit the MRI
slides of your knee. On the phone, you ask the doctor what she expects to be the
3 Preferably not from a math department, where people enjoy to come up with the most intricate
answers for the simplest of questions, a tendency which becomes exacerbated when the answer
involves numbers.
4 In the subculture of mathematicians, even more exotic answers are likely, such as the Hardy–
Ramanujan number, known also as the taxicab number, 1729. Essential here is to notice that to
arrive at a number not more complex than this, it takes already the best number-minded brains.
5 Jose Luis Borges has an elegant short story, The book of sand, about a book with an endless number
of pages. Opening the book and reading the page number would be tantamount to choosing “any”
number. Borges, possibly aware of the problem, and the intricacies related to the Axiom of Choice,
leaves the page numbering a mystery.

6.1 Smoothness Priors
97
problem, but being a new case, she says that she knows nothing yet about your case.
On your way out of the house, you accidentally take the envelope with the MRI
slides of the breast. As soon as the slides are taken out of the envelope, the doctor
knows that you have grabbed the wrong slides, in spite of having just told you not to
have any idea what to expect to see in the slides. Clearly, nobody is surprised of the
comment, as “having no idea” or “knowing nothing” in this context is just a way to
express the lack of information of some details.
The examples above clearly demonstrate how extremely difﬁcult it is to know
nothing.6
When an a priori belief is qualitative in nature and rather unreliable, the challenge
becomes to ﬁnd a quantitative formulation that is in agreement with its qualitative
property and conveys the appropriate degree of uncertainty. For example, how can
we express in a mathematical formula that we expect to see an MRI of the neck, or
how we describe quantitatively what radiologists mean by “habitus of a malignant
tumor”? Clearly, the problem is right at the edge between art and science. While
the prior expresses our a priori belief about the unknown, subjective is by no means
the same as arbitrary, as a prior position needs to be as defendable as any scientiﬁc
statement. When a prior is relatively non-committal, it may be the case that the same
data may support very different solutions, and excessive lack of commitment may
be catastrophic.
6.1
Smoothness Priors
A popular class of priors for distributed parameters expresses a priori expectations
about the smoothness properties of the solution.
We start by considering a one-dimensional signal f (t) over a given time interval,
whose values we do not know, and that we want to estimate from some indirect
observations that are not speciﬁed here. Assume for simplicity that the time has been
scaled so that the support of the signal is the interval [0, 1]. We divide the interval
into n equal subintervals by lattice points t j,
0 = t0 < t1 < . . . < tn = 1,
t j = j
n ,
and seek to estimate the discretized values x j = f (t j), 0 ≤j ≤n. Consider the
following descriptions of two different prior beliefs:
1. We know that x0 = 0, and believe that the absolute value of the slope of f should
be bounded by some m1 > 0.
2. We know that x0 = xn = 0 and believe that the absolute value of the second
derivative of f should be bounded by some m2 > 0.
6 Allegedly, Socrates said: “I am the wisest man alive, for I know one thing, and that is that I know
nothing.” Alas, the rest of us must admit that we know something.

98
6
Enter Subject: Construction of Priors
Consider the ﬁrst prior belief. By writing
f ′(t j) ≈x j −x j−1
h
,
h = 1
n ,
1 ≤j ≤n,
we may state the prior belief by saying that
x0 = 0,
|x j −x j−1| ≤h m1 with some uncertainty.
(6.1)
To express the uncertainty, we introduce the random variables X j, the values x j
representing their realizations. The boundary condition implies that X0 = 0 with
certainty, so we only need to write a probabilistic model for X j, 1 ≤j ≤n. We
write a stochastic version of (6.1) as
X j = X j−1 + γW j,
W j ∼N(0, 1),
γ = h m1,
assuming that the random variables W j are mutually independent. Collecting these
conditions into a system of linear equations,
X1 = X1 −X0 = γW1
X2 −X1 = γW2
...
...
Xn −Xn−1 = γWn,
we arrive at the matrix formulation of the prior as
L1X = γW,
W ∼N(0, In),
where
L1 =
⎡
⎢⎢⎢⎣
1
−1
1
... ...
−1 1
⎤
⎥⎥⎥⎦∈Rn×n,
X =
⎡
⎢⎢⎢⎣
X1
X2
...
Xn
⎤
⎥⎥⎥⎦,
W =
⎡
⎢⎢⎢⎣
W1
W2
...
Wn
⎤
⎥⎥⎥⎦.
The subindex of L1 here refers to the prior being a ﬁrst-order smoothness prior, related
to the ﬁrst-order derivative. This model is equivalent to specifying a Gaussian prior
probability density,
X ∼πX(x) ∝exp

−1
2γ2 ∥L1x∥2
	
.

6.1 Smoothness Priors
99
To ﬁnd the covariance of the variable X, we ﬁrst notice that since L1 is an invertible
matrix,
X = γL−1
1 W,
hence
cov(X) = E

X XT
= γ2L−1
1 E

W W T
L−T
1
= γ2L−1
1 L−T
1 .
The inverse of the covariance matrix, or the precision matrix, can be expressed in
terms of L1 as
cov(X)−1 = 1
γ2 LT
1 L1.
This prior is illustrated in the left panel of Fig. 6.1. Consider now the second prior
model. By writing the ﬁnite difference approximation for the second derivative at
the interior lattice points,
f ′′(t j) ≈x j−1 −2x j + x j+1
h2
,
1 ≤j ≤n −1,
the prior belief amounts to saying that
x0 = xn = 0,
|x j−1 −2x j + x j+1| ≤h2m2,
with some uncertainty.
Introducing again the random variables X j to model the signal at t = t j, and
assuming that the boundary conditions X0 = Xn = 0 hold with certainty, a proba-
bilistic model is needed only for the components X j, 1 ≤j ≤n −1. We may express
the prior belief as
X j = 1
2(X j−1 + X j+1) + 1
2γW j,
W j ∼N(0, 1),
γ = h2 m2,
or, equivalently, as a system of linear equations,
Fig. 6.1 Geometric interpretation of the ﬁrst-order smoothness prior (left) and the second-order
smoothness prior (right). The parameter γ expresses the standard deviation from the expected value
marked by a red square, which is the previous value in the ﬁrst-order prior model, and the average
of the neighboring values in the second-order model

100
6
Enter Subject: Construction of Priors
Fig. 6.2 Ten random draws from the priors corresponding to the ﬁrst-order (left) and the second-
order smoothness prior (right). The shaded areas indicate the credibility envelopes corresponding
to one marginal standard deviation (darker) or two marginal standard deviations (lighter). In this
example, we have set n = 50, m1 = 1 and m2 = 2
X2 −2X1 = X2 −2X1 + X0 = γW1
X3 −2X2 + X1 = γW2
...
...
−2Xn−1 −Xn−2 = Xn −2Xn−1 + Xn−2 = γWn−1.
The matrix form of this prior is
L2X = γW,
W ∼N(0, In−1),
where the subindex of L2 reminds us that the prior is related to the second derivative,
and
L2 =
⎡
⎢⎢⎢⎣
−2
1
1 −2
1
... ...
1 −2
⎤
⎥⎥⎥⎦∈R(n−1)×(n−1),
X =
⎡
⎢⎢⎢⎣
X1
X2
...
Xn−1
⎤
⎥⎥⎥⎦,
W =
⎡
⎢⎢⎢⎣
W1
W2
...
Wn−1
⎤
⎥⎥⎥⎦.
An illustration of this prior is shown in the right panel of Fig. 6.1. Having set
up a prior model, a natural question is whether it reﬂects adequately the a priori
assumptions that we had in mind. There are several ways to investigate the question.
A natural one is to generate random draws from the prior and, if possible, to look at
those realizations. The generation of random draws from a given generic distribution
will be studied in greater detail later on, but for the priors just introduced, generating
the samples is not particularly difﬁcult. Since both prior models can be expressed in
the form
L j X = γW,
W ∼N(0, I),
j = 1, 2,

6.1 Smoothness Priors
101
if we can generate a random sample from the standard white noise W,
W =

w(1), w(2), . . . , w(N)
,
we can obtain an ensemble of realizations of X according to the formula
x( j) = γL−1
j w( j),
1 ≤j ≤N.
The sample W can be obtained using standard (pseudo)random number generators.
Figure 6.2 shows 10 independent random draws from the probability densities cor-
responding to the two smoothness priors.
It is also useful to consider the uncertainties of each component x j. One measure
for the uncertainty is the marginal variance. Denoting by L either the matrix L1 or L2,
and recalling that variances of single components of a multivariate random variables
can be computed by using the joint probability density, we obtain
var

X j

=

x2
j πX(x)dx = cov(X) j j
= γ2eT
j L−1L−Te j
= γ2∥L−Te j∥2,
where e j is the canonical jth coordinate vector. In Fig. 6.2, the uncertainties are
visualized by shading the area between the values ±σ j and ±2σ j, where σ j is the
jth marginal standard deviation,
σ j =

var

X j

= γ∥L−Te j∥.
The envelopes are referred to as credibility envelopes.
6.1.1
Freeing the Boundary Values
The design of the smoothness prior above assumed that the boundary values of the
unknown function were known and ﬁxed at x0 = 0 and, in the second-order case,
also at xn = 0. It is natural to ask whether the rather limiting boundary condition
can be removed, in light of the fact that in many applications the boundary values
are not known. The Bayesian philosophy provides a natural answer: If something is
not known, it must be modeled as a random variable. For the sake of deﬁniteness,
considerthesecond-ordersmoothnessprior,andwritethediscreteapproximationsfor
the second derivatives without assuming vanishing boundary conditions, but instead
modeling the boundary values as independent Gaussian random variables. We have

102
6
Enter Subject: Construction of Priors
X0 = αW0
X0 −2X1 + X2 = γW1
X1 −2X2 + X3 = γW2
...
...
Xn−2 −2Xn−1 + Xn = γWn−1
Xn = αWn
or, in matrix notation,
LαX = γW,
W ∼N(0, In+1),
where
Lα =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
γ/α
1
−2
1
1 −2
...
1 −2
1
γ/α
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
∈R(n+1)×(n+1).
If we have some a priori belief about the range of the boundary values, we may
use that information to set the parameter α. If no speciﬁc information is available,
a possible way to set α is so that the marginal variances of the boundary values
are close to those of the interior values. This can be accomplished by the following
sequential adjustment strategy. First, assume that the boundary values are known to
be zero, and compute the marginal variances as in the previous subsection. Let
σmax = max{σ j} = max{γ∥L−Te j∥}.
We have seen that the maximum was attained in the middle of the interval: Assuming
for simplicity that n is even, we have
σmax = γ∥L−Ten/2∥.
Setting α = σmax so that the marginal variances at the boundaries coincide with that
in the center leads to
Lα =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
1/∥L−T
2 en/2∥
1
−2
1
1 −2
...
1 −2
1
1/∥L−T
2 en/2∥
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
.

6.2 Generalization to Higher Dimensions
103
Fig. 6.3 Ten random draws
from the prior corresponding
to the second-order
smoothness matrix Lα,
together with the credibility
envelopes corresponding to
one and two marginal
standard deviations
To test whether this procedure yields a prior reﬂecting our belief, we draw random
samples from the prior thus obtained and plot the credibility envelopes. Figure 6.3
shows the results.
6.2
Generalization to Higher Dimensions
The smoothness priors just introduced in the one-dimensional case can be generalized
to higher dimensions. Let f be a function deﬁned over the rectangle D = [0, 1] ×
[0, 1] to be estimated through some indirect observation. Assume that the domain D
is subdivided into n2 equal squares, denoting the lattice points by
r jk = (t j, sk),
t j = j
n ,
sk = k
n ,
0 ≤j, k ≤n,
and the function values at the lattice points by x jk = f (r jk). A priori we believe that:
1. At the boundaries of D, f vanishes for sure, i.e.,
f

∂D = 0.
2. In the interior points, the absolute value of the Laplacian of f ,
Δf = ∂2 f
∂t2 + ∂2 f
∂s2 ,
is bounded by some constant m > 0, with some uncertainty.
The next task is to express these prior beliefs in the form of a density for the
multivariate random variable X.
Consider a matrix x ∈R(n+1)×(n+1) containing the discrete values of f at the
lattice points. We encode the belief that f vanishes at the boundary by setting

104
6
Enter Subject: Construction of Priors
x0k = xnk = 0,
0 ≤k ≤n,
x j0 = x jn = 0,
0 ≤j ≤n.
The values of f in the interior points will be modeled as random variables that we
arrange into an (n −1) × (n −1) matrix,
X =
⎡
⎢⎣
X11
· · ·
X1,n−1
...
...
Xn−1,1 · · · Xn−1,n−1
⎤
⎥⎦.
We denote by X(k) the kth column of X, so that
X =
 X(1) · · · X(n−1) 
,
and the realizations of X(k) by x(k). Consider the second derivative of f with respect
to the variable t, and its ﬁnite difference approximation
∂2 f
∂t2 (x jk) ≈x j−1,k −2x j,k + x j+1,k
h2
,
h = 1
n .
(6.2)
To ﬁnd a matrix notation, observe that the second derivative with respect to t at x jk
depends only on the values of f in the column k. By taking the vanishing boundary
conditions into account, we may write the right-hand side of (6.2) using the matrix
L2 deﬁned earlier,
x j−1,k −2x j,k + x j+1,k
h2
= 1
h2

L2x(k)
j.
Consider now the second partial derivatives with respect to the variable s, and its
ﬁnite difference approximation,
∂2 f
∂s2 (r jk) ≈x j,k−1 −2x j,k + x j,k+1
h2
.
This time, we may express the right-hand side in terms of the k −1, k and k + 1
columns of X,
x j,k−1 −2x j,k + x j,k+1
h2
= 1
h2

x(k−1) −2x(k) + x(k+1)
j.
To organize the computations efﬁciently, we deﬁne the operator stacking the columns
of a matrix into a long column vector,

6.2 Generalization to Higher Dimensions
105
vec :R(n−1)×(n−1) →R(n−1)2,
vec(X) = X =
⎡
⎢⎢⎢⎣
X(1)
X(2)
...
X(n−1)
⎤
⎥⎥⎥⎦.
(6.3)
Furthermore, we deﬁne the Kronecker product of two matrices as follows: If A =
[ai j] ∈Rp1×q1 and B = [bi j] ∈Rp2×q2, their Kronecker product is the block matrix
A ⊗B =
⎡
⎢⎣
a11B · · · a1q1B
...
...
ap11B · · · ap1q1B
⎤
⎥⎦∈Rp1 p2×q1q2,
where the ( j, k)th block is a copy of the matrix B multiplied by the ( j, k)th ele-
ment of A. Kronecker products are very convenient for expressing ﬁnite difference
approximations of the second partial derivatives in a compact fashion. Indeed, the
vector containing the approximations of the second derivatives with respect to t can
be expressed as
⎡
⎢⎢⎢⎣
L2X(1)
L2X(2)
...
L2X(n−1)
⎤
⎥⎥⎥⎦=
⎡
⎢⎢⎢⎣
L2
L2
...
L2
⎤
⎥⎥⎥⎦
⎡
⎢⎢⎢⎣
X(1)
X(2)
...
X(n−1)
⎤
⎥⎥⎥⎦=

In−1 ⊗L2

X.
Similarly, the vector of the approximations of the second derivatives with respect to
s is written as
⎡
⎢⎢⎢⎢⎢⎣
−2X(1) + X(2)
X(1) −2X(2) + X(3)
...
X(n−3) −2X(n−2) + X(n−1)
X(n−2) −2X(n−1)
⎤
⎥⎥⎥⎥⎥⎦
=
⎡
⎢⎢⎢⎢⎢⎣
−2In−1
In−1
In−1 −2In−1
In−1
...
...
In−1 −2In−1
In−1
In−1 −2In−1
⎤
⎥⎥⎥⎥⎥⎦
⎡
⎢⎢⎢⎢⎢⎣
X(1)
X(2)
...
X(n−2)
X(n−1)
⎤
⎥⎥⎥⎥⎥⎦
=

L2 ⊗In−1

X.
The Kronecker matrix representations just introduced naturally yield the following
ﬁnite difference approximation of the Laplacian:
Δf ≈1
h2 (In−1 ⊗L2 + L2 ⊗In−1) X.
We are now ready to write the prior model conveying the belief that the Laplacian
of f is bounded by a quantity m with some uncertainty,

106
6
Enter Subject: Construction of Priors
(In−1 ⊗L2 + L2 ⊗In−1) X = γW,
where W is a standard normal random variable with independent components,
W ∼N(0, I(n−1)2),
γ = h2m.
Random draws from the second-order smoothness prior can be generated by draw-
ing the Gaussian random vectors w as realizations of W, and then solving the linear
equation
(In−1 ⊗L2 + L2 ⊗In−1) x = γw,
w ∼N(0, I(n−1)2).
Observe that while the ﬁnite difference matrix is rather large, it is also extremely
sparse.
6.3
Whittle–Matérn Priors
The smoothness priors introduced in the previous section can be modiﬁed to account
for a priori belief about the sizes of details in the unknown. We derive the corre-
sponding ﬂexible family of smoothness priors in the two-dimensional setting. Deﬁne
a family of symmetric positive deﬁnite matrices of size (n −1)2 × (n −1)2,
Wλ = 1
λ2 I(n−1)2 −Λ,
where Λ is the discrete Laplacian approximation derived in the previous section,
Λ = In−1 ⊗L2 + L2 ⊗In−1.
As λ increases, the matrix Wλ converges to the ﬁnite difference approximation of the
Laplacian. On the other hand, if λ tends to zero, the scaled identity matrix becomes
the dominant part, and in that case a random variable X satisfying the equation
WλX = γW,
W ∼N(0, I(n−1)2),
(6.4)
is increasingly similar to a scaled version of the white noise W. This observation
gives us a clue about the role of the parameter λ in controlling the granularity of the
unknown as expressed by the prior corresponding to the above model. For this reason,
λ is referred to as the correlation length parameter. We point out that the matrix Wλ is
always positive deﬁnite, so it indeed deﬁnes a proper covariance matrix. We illustrate
the effect of λ on the granularity in Fig. 6.4, where we show random draws computed
according to Eq. (6.4) with different choices of λ.

6.4 Smoothness Priors with Structure
107
Fig. 6.4 Three random draws from the Whittle–Matérn prior with varying correlation lengths with
n = 150. Denoting by h = 1/n, in the top row, λ = 5 h; in the middle row: λ = 10 h; and in the
bottom row, λ = 50 h. Observe that in each column, the right-hand side W in (6.4) was ﬁxed. The
dynamic ranges of the images can be controlled by the parameter γ
6.4
Smoothness Priors with Structure
The smoothness priors discussed in the previous sections can be enriched by adding
structure to them. We start by considering the one-dimensional case.
Suppose that we believe a priori that the slope of the function f over the unit
interval is bounded everywhere by a constant m except in a small interval around
a known point t∗∈(0, 1) where the absolute value of the slope can be larger by a
multiplicative factor
√
θ, θ > 1. The reason for using the square root of the parameter
as the multiplicative factor will become clear later, when we give an interpretation
for this parameter. To build the prior, we discretize the interval by subdividing it

108
6
Enter Subject: Construction of Priors
into n subintervals and use ﬁnite difference approximation for the derivatives. Let
t j∗= j∗/n be the point where the bound for the absolute value of the slope is scaled
up. As in the previous section, we write the conditions deﬁning the prior model in
the form of a linear system
X1 = X1 −X0 = γW1
X2 −X1 = γW2
...
...
X j∗−X j∗−1 = γ
√
θW j∗
(6.5)
...
...
Xn −Xn−1 = γWn,
and arrive at the matrix formulation
L1X = γD1/2
θ W,
W ∼N(0, In).
(6.6)
Here Dθ is a diagonal matrix,
Dθ = diag(1, . . . , θ, . . . , 1) ∈Rn×n,
with θ in the j∗th place on the diagonal, and the power 1/2 in Eq. (6.6) means
that a square root of the diagonal entries is taken. Similar construction can be made
with the second-order smoothness prior, allowing the second derivative to become
signiﬁcantly larger in absolute value at selected locations.
The effect of this change can be seen in Fig. 6.5, showing ten random draws with
n = 50 and
√
θ = 10 for both the ﬁrst-order and the second-order smoothness priors.
The random draws from ﬁrst order smoothness prior show that the realizations are
allowed larger variability at the special point indicated by the dashed line; however,
the prior does not force the realizations to jump: some of them pass the point with-
out being affected by the modiﬁcation. Similarly, in the case of the second-order
smoothness, the kink corresponding to the higher curvature is not forced, and some
of the realizations do not demonstrate any different behavior at the selected point.
After solving (6.6) for W,
W = 1
γ D−1/2
θ
L1X ∼N(0, In−1),
we conclude that the prior probability density is
πX(x) ∝exp

−1
2γ2 ∥D−1/2
θ
L1x∥2
	
,

6.5 Conditionally Gaussian Priors and Hierarchical Models
109
Fig. 6.5 Ten random draws from the modiﬁed ﬁrst-order smoothness prior (left) and the second-
order smoothness prior (right). The vertical line indicates the discretization point in which the
absolute value of the ﬁrst or the second derivative is allowed to take on a value ten times larger than
elsewhere
ignoring, for the time being, the normalizing constant that depends on θ.
In our derivation, we assumed that the slope was allowed to be exceptionally
different at one single location j∗. The procedure can be adapted to include as many
exceptional points as we wish, leading to a richer class of prior densities.
6.5
Conditionally Gaussian Priors and Hierarchical Models
The next class of priors that we consider are particularly well suited to describe
signals that are believed to be regular in a described sense almost everywhere, with a
few occasional exceptions in behavior at locations that, unlike in the previous section,
are unknown to us. In particular, not only we may not know where the anomalies
are, but we may lack the information about how many of them there are.
For the sake of deﬁniteness, let us consider the ﬁrst-order smoothness model. We
start, once again, with the smoothness prior model (6.5) and modify the standard
deviations at all locations. Folding the factor γ into the variables θ j, the new model
becomes
X1 = X1 −X0 =

θ1W1
X2 −X1 =

θ2W2
...
...
X j −X j−1 =

θ jW j
(6.7)
...
...
Xn −Xn−1 =

θnWn,

110
6
Enter Subject: Construction of Priors
or, in matrix form,
L1X = D1/2
θ W,
W ∼N(0, In),
(6.8)
where Dθ is a diagonal matrix parametrized by a vector θ ∈Rn with non-negative
entries,
Dθ = diag (θ1, . . . , θn) .
We solve (6.8) for W,
W = D−1/2
θ
L1X,
and use the change of variables formula to ﬁnd the probability density of X. In this
case, however, we will not ignore the normalizing factor that depends on θ. The
determinant of the Jacobian of the transformation above is
det

D−1/2
θ
L1

= det

D−1/2
θ

det

L1

.
Both determinants on the right hand side of the equal sign are easy to evaluate: The
matrix L1 is lower triangular with ones on the diagonal, and D−1/2
θ
is diagonal, so
det

D−1/2
θ

=
1
√θ1θ2 · · · θn
,
det

L1

= 1.
Therefore, we conclude that the prior density of X, given the parameter vector θ, is
πX (x | θ) =
1
(2π)n/2√θ1θ2 · · · θn
exp

−1
2∥D−1/2
θ
L1x∥2
	
.
So far, we have assumed that the parameter vector θ is known and speciﬁed. But
what happens if that is not the case, and θ is unknown? According to the Bayesian
paradigm, if a parameter is not known to us, we model it as a random variable, and
it must be estimated based on the data. Therefore, we introduce a random variable
Θ ∈Rn, and postulate that the prior just deﬁned is, in fact, a conditional density, that
is,
πX|Θ (x | θ) =
1
(2π)n/2√θ1θ2 · · · θn
exp

−1
2∥D−1/2
θ
L1x∥2
	
∝exp
⎛
⎝−1
2∥D−1/2
θ
L1x∥2 −1
2
n

j=1
log θ j
⎞
⎠.
(6.9)
The price for introducing the new random variable Θ is that we need to specify our
a priori belief about it in the form of a probability density πΘ. More precisely, since
now there are two random variables to estimate, we need to specify a prior for the
pair (X, Θ). This is done in terms of the conditional prior for X given Θ and the
prior for Θ, according to the formula

6.5 Conditionally Gaussian Priors and Hierarchical Models
111
πXΘ(x, θ) = πX|Θ(x | θ)πΘ(θ).
(6.10)
The model that we just derived is often referred to as hierarchical prior model, as
it follows a layered hierarchy, proceeding from the unknown quantity of primary
interest to a parameter deﬁning its prior. The next task is to express in terms of this
hierarchical prior model our belief about the primary unknown.
Let us get back to the original problem: We want to construct a prior for X
conveying our belief that
1. The function f represented by the discrete values X j varies little over most of
the interval [0, 1],
2. f may exhibit a few jumps at discrete points, although the number and locations
are unknown.
Translating the above information into properties of Θ j that control the jumps of
X j, we conclude that
a. The variables Θ j should be positive,
b. Most of the variables Θ j are presumably small,
c. A few of the components Θ j can be large.
A way to make it possible and easy for the solution to have sudden jumps is to look
for multivariate distributions with independent components, that is,
πΘ(θ) =
n
j=1
πΘ j(θ j).
Additionally, since there is no a priori preference for the location of the jumps, it is
reasonable to assume that all components Θ j are identically distributed. Moreover,
the ﬁrst observation requires that we look for a probability density that is supported
on the positive real axis. In summary, we have reduced the search to probability
distributions over the positive reals. The choice must now be guided by the remaining
points.
In classical statistics, there are many one-dimensional probability distributions
that favor small values while allowing outliers. Such distributions are often referred
to as fat-tailed densities or leptokurtic densities. Our selection criterion takes into
account computational convenience. Two distributions that stand out from that point
of view are the gamma distribution and the inverse gamma distribution.
A random variable Θ j follows the gamma distribution:
Θ j ∼Gamma (β, θ0) ,
β > 0, θ0 > 0,
if its probability density is given by
πΘ j

θ j

=
1
θβ
0 Γ (β)
θβ−1
j
exp

−θ j
θ0
	
,

112
6
Enter Subject: Construction of Priors
where Γ (β) is the gamma function. The parameters β and θ0, referred to as hyper-
parameters, are called the shape and scale parameters, respectively. Observe that
when β = 1, we have the Laplace distribution,
πΘ j(θ j) = 1
θ0
e−θ j/θ0.
To gain a better understanding of the role of the hyperparameters, we consider
the mean and the variance of the distribution. It can be shown that, for Θ j ∼
Gamma(β, θ0),
E

Θ j

= βθ0,
var

Θ j

= βθ2
0.
Similarly, the hyperprior follows an inverse gamma distribution:
Θ j ∼InvGamma (β, θ0) ,
β > 0, θ0 > 0,
if its probability density is
πΘ j

θ j

=
θβ
0
Γ (β)θ−β−1
j
exp

−θ0
θ j
	
.
The change of variables formula reveals that if the random variable T is gamma-
distributed, then S = θ2
0/T follows the inverse gamma distribution. The mean and
variance of the inverse gamma-distributed random variable are
E

Θ j

=
θ0
β −1, β > 1,
var

Θ j

=
θ2
0
(β −1)(β −2), β > 2.
Combining the conditionally Gaussian prior (6.9) and the gamma hyperprior
yields the joint prior of the pair (X, Θ),
πXΘ(x, θ) ∝exp
⎛
⎝−1
2∥D−1/2
θ
L1x∥2 −1
2
n

j=1
log θ j
⎞
⎠
n
j=1

θβ−1
j
exp

−θ j
θ0
		
= exp
⎛
⎝−1
2∥D−1/2
θ
L1x∥2 −1
2
n

j=1
log θ j + (β −1)
n

j=1
log θ j −
n

j=1
θ j
θ0
⎞
⎠
= exp
⎛
⎝−1
2∥D−1/2
θ
L1x∥2 +

β −3
2
	
n

j=1
log θ j −
n

j=1
θ j
θ0
⎞
⎠.
A similar expression can be derived for the joint prior when the hyperprior is an
inverse gamma distribution. We postpone any further discussion of hierarchical priors
to later, when we will use them in connection with speciﬁc problems.

6.6 Sparsity-Promoting Priors
113
6.6
Sparsity-Promoting Priors
It is not uncommon to have the a priori belief that the unknown vector we are looking
for has most of its components near zero, and that the data can be explained by only
a few components substantially different from zero. Such solutions are called sparse,
and we want to characterize priors that promote sparsity in the Bayesian framework.
It goes without saying that setting most of the components equal to zero would reduce
considerably the work burden, if it was not for the fact that usually we do not know a
priori how many of the components are nonzero, and where the nonzero components
are. As we will see later, the hierarchical priors introduced in the previous section
are ideally suited to promote sparse solutions. Before addressing Bayesian sparsity
in detail, we review some classic ways for promoting sparsity.
As a motivation, consider the following problem: Given a matrix A ∈Rm×n with
m < n, and a vector b ∈Rn, consider the equation
b = Ax.
There are two possibilities: If b /∈R(A), that is, b is not in the range of A, no solutions
can be found. On the other hand, if b ∈R(A), there is at least one x0 ∈Rn such that
the equation holds. However, since m < n, the null space of matrix A,
N(A) = {x ∈Rn | Ax = 0},
has dimension at least n −m, therefore the equation above is satisﬁed also by all x
such that
x ∈H(b, A) = x0 + N(A) = {x ∈Rn | x = x0 + z, z ∈N(A)}.
The set H(b, A) is an afﬁne subspace of Rn of dimension equal to that of the null
space of A. After acknowledging the non-uniqueness of the solution, we may ask if
there is a computationally efﬁcient way to select a sparse solution among the many.
Before proceeding any further, we introduce some notations. The support of a
vector x ∈Rn is the set of indices corresponding to its nonzero components,
supp(X) = { j | x j ̸= 0} ⊂{1, 2, . . . , n}.
The cardinality of the support, that is, the number of nonzero entries in x, is called
the “0-norm” of x,
∥x∥0 = card

supp(x)

∈{0, 1, . . . , n}.
A vector x ∈Rn is sparse, if it has signiﬁcantly fewer than n nonzero components,
∥x∥0 ≪n.

114
6
Enter Subject: Construction of Priors
Fig. 6.6 A minimum-norm solution xM N is the solution that lies on the afﬁne space Ax = b and
is closest to the origin in terms of the given norm
How much fewer, however, is not a particularly well-deﬁned measure. Usually, we
recognize a sparse vector when we see one.7
After this preamble, we are ready to consider the standard minimum-norm selec-
tion principle, selecting the solution of smallest ℓ2-norm,
xM N = argmin

∥x∥2 | x ∈H(b, A)

.
Geometrically, the minimum-norm solution corresponds to the point in H(b, A)
that lies closest to the origin in the Euclidean sense, see Fig. 6.6. It is natural to ask
what happens if, instead of the Euclidean norm, other norms are used. The family of
ℓp-norms constitute a natural generalization of the Euclidean norm,
∥x∥p =

n

j=1
|x j|p1/p,
1 ≤p < ∞,
(6.11)
augmented with the limiting case, the maximum norm, or ℓ∞-norm,
∥x∥∞= max{|x j|} = lim
p→∞∥x∥p.
To understand what is the effect of changing the norm, recall what the ℓp-spheres
look like, by plotting in R2 the unit spheres
Sp = {x ∈Rn | ∥x∥p = 1}.
Figure 6.6 shows how the minimum-norm estimate changes if the ℓ2-norm is replaced
by ℓ1 or ℓ∞. The closest point to the origin in the ℓ2-sense has both components x1
and x2 nonzero, while the point closest to the origin in the ℓ1-sense is of the form
(0, x2)withsome x2 ̸= 0.Inthetwo-dimensionalcase,thiscanbeconsideredasparse
7 This is like the famous statement “I know it when I see it” of the Supreme Court Justice Potter
Stewart discussing the threshold of obscenity.

6.6 Sparsity-Promoting Priors
115
solution. It is possible to imagine8 that the same will hold in higher dimensions: the
ℓ1-sphere reaches out most strongly along the coordinate axes, and therefore the ﬁrst
contact with the afﬁne subspace H(b, A) would take place at a point where several
components are zero.
The discussion above leads naturally to the following minimization problem:
minimize ∥x∥1 subject to b = Ax.
Observe that for some data b, there may indeed be no x satisfying the linear constraint.
However, we may always decompose b into two ℓ2-orthogonal components,
b = b1 + b2,
b1 ∈R(A),
b2 ⊥R(A),
so that
∥b −Ax∥2 = ∥b1 −Ax∥2 + ∥b2∥2 ≥∥b2∥2,
where the norm is the Euclidean ℓ2-norm, and the best we can do is to require that x
satisﬁes
Ax = b1, implying that ∥b −Ax∥2 = ∥b2∥2,
prompting us to consider instead the modiﬁed minimization problem,
minimize ∥x∥1 subject to ∥b −Ax∥2 = ∥b2∥2.
This can be recast in the form of unconstrained minimization problem using Lagrange
multipliers. In that formulation, we seek to ﬁnd
xλ = argmin

∥x∥1 + λ

∥b −Ax∥2 −∥b2∥2
,
where the Lagrange multiplier λ needs to be adjusted so that the constraint is satisﬁed.
It is customary, however, to consider the equivalent minimization problem,
xα = argmin

∥b −Ax∥2 + α∥x∥1

.
In the inverse problems literature, the solution is referred to as the ℓ1-penalized least
squares solution, while in the statistical literature it is called it the least absolute
shrinkage and selection operator, or LASSO solution. The signal processing com-
munity refers to this problem as basis pursuit denoising (BPDN) problem.
FromthepointofviewofBayesianinference,wenowhaveanexcellentmotivation
to look for a prior that is concentrated on vectors with small ℓ1-norm. The most
straightforward selection, which is also one of the most popular ones,
8 Well, it is possible, but not necessary easy, since our experience of the world is a three-dimensional
affair. If we add time, and possibly with the help of colors, we can imagine four dimensions, but
there is no way that we can stretch our imagination to envision ﬁve or higher dimensional objects.

116
6
Enter Subject: Construction of Priors
Fig. 6.7 Left: Equiprobability lines of the prior (6.12) with p = 2/3 and the afﬁne subspace
H(b, A). Right: The proﬁle of the posterior density along the afﬁne space H(b, A). Observe that
the non-convexity of the ℓp-spheres implies that the prior density along the afﬁne subspace H(b, A)
has more than one local maximum
πX(x) ∝exp (−α∥x∥1) = exp
⎛
⎝−α
n

j=1
|x j|
⎞
⎠,
is often referred to as ℓ1-prior, and it heralded as the prototype of sparsity-promoting
priors. Observe that the minimum ℓ1-norm solution of the problem Ax = b can be
thought of as a problem of maximizing πX(x) along the afﬁne space H(b, A).
In the discussion above, we limited the parameter p to the interval 1 ≤p ≤∞.
The discussion can be extended to the case p < 1, with the caveat that for p < 1,
formula (6.11) does not deﬁne a proper norm, as the triangle inequality is not valid.
Indeed, nothing prevents us from deﬁning a prior of the form
πX(x) ∝exp
⎛
⎝−α
n

j=1
|x j|p
⎞
⎠,
0 < p < 1.
(6.12)
This prior is even more sparsity-promoting than the ℓ1-prior, which can be understood
by considering the equiprobability contour lines corresponding to the ℓp-spheres,
see Fig. 6.7: The contour lines reach out along the axes even more greedily than
the corners of the ℓ1-ball. However, the non-convexity of ℓp-balls for p < 1 implies
that along the afﬁne space N(b, A), the prior πX has typically more than one local
maximum, posing a challenge for the optimization problem, see Fig. 6.7.
6.7
Kernel-Based Priors
Sometimes the unknown to be estimated represents a spatially distributed parameter,
for example, the density of a material, and the prior should convey information of
the size of the details we are expecting to see. To be more speciﬁc, assume that the

6.7 Kernel-Based Priors
117
unknown represents a physical property of subsurface structure of the Earth that we
believe is horizontally layered, so that it is natural to represent it as a function of
depth. Assume further that earlier geological investigations indicate that the stratiﬁed
layers have a typical thickness, and we want to include this information into our prior.
We may argue that values measured within a distance smaller than the typical layer
thickness should be correlated, while the correlation over long distances should be
minimal. Priors of this kind can be easily implemented using kernel methods.
Let f : Rd →R represent an unknown quantity to be estimated through an indi-
rect observation, and deﬁne a kernel function,
K : Rd × Rd →R,
which is symmetric, that is,
K(r1,r2) = K(r2,r1) for all r1,r2 ∈Rd.
Furthermore, we require that the kernel K satisﬁes the following property: For any
set of points r1,r2, . . . ,rN ∈Rd, with r j ̸= rk if j ̸= k, the matrix
K =
⎡
⎢⎣
K(r1,r1) · · · K(r1,rN)
...
...
K(rN,r1) · · · K(rN,rN)
⎤
⎥⎦∈RN×N
is symmetric positive semi-deﬁnite, that is, for any vector v ∈RN, v ̸= 0,
vTKv =
N

j,k=1
v jvk K(r j,rk) ≥0.
If K satisﬁes this property, it is said to be a symmetric positive deﬁnite kernel.9 Before
moving ahead, we introduce some explicitly symmetric positive deﬁnite kernels.
Consider ﬁrst a kernel that is separable,
K(r1,r2) = ϕ(r1)ϕ(r2),
for some function ϕ : Rd →R. The matrix K in this case is a rank-one matrix,
K =
⎡
⎢⎢⎣
ϕ(r1)
ϕ(r2)
. . .
ϕ(rN)
⎤
⎥⎥⎦
ϕ(r1) ϕ(r2) · · · ϕ(rN) 
= ΦΦT,
9 This nomenclature is confusing, as the matrices K are indeed required to be only positive semi-
deﬁnite.

118
6
Enter Subject: Construction of Priors
and, therefore, for any v ̸= 0,
vTKv = vTΦΦTv =

ΦTv
2 ≥0.
The matrix K is only positive semi-deﬁnite, instead of positive deﬁnite, as the above
quantity is zero for any vector v orthogonal to Φ. We may generalize the function
K, and deﬁne
K(r1,r2) =
L

ℓ=1
ϕℓ(r1)ϕℓ(r2).
If Φℓis a vector with entries ϕℓ(r j), then
vTKv =
L

ℓ=1

ΦT
ℓv)2 ≥0.
Moreover, if L ≥N and the vectors Φℓare linearly independent, we may conclude
that vTKv > 0 for v ̸= 0 or, equivalently, that the matrix is positive deﬁnite, while if
N > L the matrix is only positive semi-deﬁnite. Therefore, it is natural to consider
kernels of the type
K(r1,r2) =
∞

ℓ=1
ϕℓ(r1)ϕℓ(r2),
(6.13)
assuming that the sum converges, and hope that for any N and for all choices of
points r j, the matrices K are symmetric positive deﬁnite. It turns out that, according
to Mercer’s theorem (see Notes and Comments at the end of the chapter), every
positive deﬁnite kernel can be expressed as an inﬁnite sum of the form (6.13). We
shall not go any further in that direction, but instead present an example of kernel
functions.
Example 6.3 Assume that we want to estimate a function f deﬁned on a unit interval
[0, 1], which has been discretized by the nodes 0 = t0 < t1 < . . . < tn = 1 with
t j = j/n, where we set n = 100. We denote by x j = f (t j) the value of our unknown
at t j, and let the random variables X j represent the grid values. We take the positive
deﬁnite kernel function to be the Gaussian,
K(t, s) = exp

−1
2γ2 (t −s)2
	
,
where the parameter γ > 0 controls the correlation length. One can show that the
Gaussian kernel indeed is a positive deﬁnite kernel. It turns out that the covariance
matrix C computed by using this kernel is not numerically positive deﬁnite, so we
add to it a small multiple of the identity matrix,

6.7 Kernel-Based Priors
119
C =
⎡
⎢⎣
K(t0, t0) · · · K(t0, tn)
...
...
K(tn, t0) · · · K(tn, tn)
⎤
⎥⎦+ δIn+1 = K + δIn+1,
where δ > 0. The added scaled identity matrix guarantees that the matrix C is positive
deﬁnite, since it follows from the positive deﬁniteness of the kernel that
vTCv = vTKv + δ∥v∥2 ≥δ∥v∥2 > 0
for all v ̸= 0. We deﬁne a prior model for X, setting
X ∼N(0, C).
To test the effect of the correlation length parameter γ, we generate random draws
from the prior as follows: Let
C = RTR
be the Cholesky factorization of C. Then, we may write a whitened model for X as
Fig. 6.8 Five random draws using the Gaussian kernel-based covariance matrix with varying cor-
relation length parameter, γ = 0.5 (upper left), γ = 0.2, (upper right), γ = 0.1 (lower left) and
γ = 0.05 (lower right). The number of discretization points in each case is n = 101

120
6
Enter Subject: Construction of Priors
X = RTW,
W ∼N(0, In+1),
providing a straightforward algorithm to generate random draws. Figure 6.8 shows
ﬁve random draws from the prior using four different values for the correlation length
parameters. Theregularizationparameterδ was chosenas δ = 10−8. Theresults show
that γ controls well the sizes of the details in the signals.
Gaussian kernels are a popular choice also in higher dimensions,
K(r1,r2) = Cexp

−1
2γ2 ∥r1 −r2∥2
	
,
where the parameter γ > 0 controls the width of the kernel and C > 0 is a scaling
constant. Kernels depending on the distance of the points r1 and r2 are often referred
to as radial basis functions. It can be shown that K is a positive deﬁnite kernel in Rn.
6.8
Data-Driven Priors
Givenaprior,itspropertiescanbetestedbydrawingrandomsamplesfromthedensity
and, conversely, a representative sample of realizations can be used to construct a
probability density. Given a sample of typical solutions,
S = {x(1), x(2), . . . , x(p)},
x( j) ∈Rn,
we want to ﬁnd a Gaussian prior πX such that
(a) πX is concentrated in the afﬁne subspace
H = span{x(1), . . . , x(p)};
(b) πX, restricted to the subspace H , is distributed according to the sample.
If the subspace H is not the entire space Rn and the prior is supported on H , we
say that the prior is degenerate. This manifests itself as a covariance matrix that is
symmetric positive semi-deﬁnite, but not positive deﬁnite.
To construct a non-degenerate Gaussian prior, we ﬁrst arrange the data in a matrix,
X =
 x(1) x(2) · · · x(p) 
∈Rn×p.
We then compute the mean value of the data,
x = 1
p
p

j=1
x( j),

6.8 Data-Driven Priors
121
and deﬁne the centralized data matrix
Xc =

x(1)
c
x(2)
c
· · · x(p)
c

,
x( j)
c
= x( j) −x.
Using the centralized data matrix, we then compute the empirical covariance matrix,
C = 1
p
p

j=1
x( j)
c

x( j)
c
T = 1
p XcXT
c .
(6.14)
We observe that if C is not invertible, we cannot deﬁne the prior as a Gaussian as
before. One way to construct an invertible covariance is to add a small perturbation,
C →Cδ = C + δIn,
where δ > 0 is a small regularization parameter, and postulate a prior model X ∼
N(x, Cδ). We demonstrate the idea by a computed example.
Example 6.4 Consider a face recognition problem in which we assume a priori that
the unknown represents a photograph of a face. We construct a small database of pub-
licly available black and white head shots.10 After centering and cropping the faces,
each image is a 145 × 201 grayscale matrix, the pixel values scaled so that they are in
the range [0, 1]. From the database, we select the p = 120 representatives shown in
Fig. 6.9. We rearrange the image matrices by using the vec-operator (6.3), organizing
the data in a matrix X ∈R29 145×120, compute the mean and the covariance matrix.
Observe that the rank of the covariance matrix cannot exceed 120. Figure 6.10 shows
the singular values of the matrix X, indicating that despite the strong redundancy of
the data, almost all singular values are of signiﬁcant size.
To test how well the prior thus deﬁned represents the prior belief, we perform a
number of random draws from the prior. To avoid computing the Cholesky decom-
position of the huge covariance matrix C, instead we use the SVD of the centered
data matrix,
Xc = UDVT.
Observing that there are r ≤120 nonzero singular values, we may reduce the above
representation to comprise only the r ﬁrst singular vectors and singular values, and
write
Xc = UrDrVr,
10 http://cvc.cs.yale.edu/cvc/projects/yalefaces/yalefaces.html. The database is known as Yale
Faces, consisting of head shots of 15 individuals with different facial expressions, illumination,
or with and without eyeware.

122
6
Enter Subject: Construction of Priors
Fig. 6.9 Database consisting of 120 face images, originating from the Yale Face database, (http://
cvc.cs.yale.edu/cvc/projects/yalefaces/yalefaces.html)
Fig. 6.10 Singular values of the face data matrix X. Observe that only the very last values are of
negligible size
where Ur ∈Rn×r and Vr ∈Rp×r are obtained from U and V by retaining only the
r ﬁrst columns, and Dr ∈Rr×r is a diagonal matrix containing the non-vanishing
singular values. We deﬁne the Gaussian random variable

6.8 Data-Driven Priors
123
Z = x +
1
√p UrDrW,
W ∼N(0, Ir).
(6.15)
We observe that Z is a Gaussian random variable with mean x and covariance
E

(Z −x)(Z −x)T
= 1
p UrDrE

W W T
DT
r UT
r
= 1
p UrDrDT
r UT
r ,
and by using the fact that columns of Vr are mutually orthogonal and r ≤p, we have
VT
r Vr = Ir,
implying that
1
p UrDrDT
r UT
r = 1
p UrDrVT
r VrDT
r UT
r
= 1
p XcXT
c = C.
This calculation shows that we may use the random variable Z deﬁned in (6.15) to
generaterandomdrawsfromthedegenerateGaussiandistributionwiththecovariance
matrix C. If we are interested in generating samples corresponding to the positive
deﬁnite covariance matrix Cδ, we may deﬁne a random variable
Zδ = x +
1
√p UrDrW +
√
δW ′,
W ∼N(0, Ir),
W ′ ∼N(0, In),
where the variables W and W ′ are mutually independent. Finally, observe that by the
deﬁnition of Z, we do not need to compute the full singular value decomposition of
the matrix Xc which can be very time consuming and expensive. Instead, we only
need to compute the ﬁrst r singular vectors and singular values, which can be done
economically, e.g., using in Matlab the routine svds.
Figure 6.11 shows the mean x of the prior, and 28 random draws of the variable
Z, rearranged back into the form of 201 × 141 matrices and represented as grayscale
images. We observe that the random draws bear similarities with some of the faces,
and summarize rather well the original sample.
Notes and Comments
Smoothness priors have a long history, and they are rooted in classical regularization
theory of inverse problems [20]. An extension of the idea of freeing the boundary
values in the smoothness priors to dimensions higher than one can be found in

124
6
Enter Subject: Construction of Priors
Fig. 6.11 The mean face (left) and 28 random draws from the Gaussian prior density, represented
as images
[10, 16]. Whittle–Matèrn priors are popular in spatial statistics and geostatistics [54,
83], and they have recently been promoted in various applications to inverse problems
[66, 67]. Interestingly, Whittle–Matérn priors can be expressed in an equivalent form
intermsofkernel-basedpriors,wherethekernelfunctionsarecertainmodiﬁedBessel
functions. The advantage of deﬁning them by using differential operators is that in
this way, the matrices are sparse, which usually is not the case with kernel-based
deﬁnitions. Finally, we point out that while we used ﬁnite difference approximations
to discretize smoothness priors in rectangular domains, the methods are not restricted
to simple domains. For more complicated domains and with different boundary
values, ﬁnite element methods can be used.
Sparse inversion algorithms and sparsity-promoting priors have become particu-
larly important in the wake of the boom in compressed sensing in image and signal
processing [24, 27]. In this context, the ℓ1-based optimization algorithm has become
particularly popular [23]. A relatively recent review [37] contains a number of ref-
erences to computational solutions to this optimization challenge. The ℓ1-penalties
for sparse reconstruction, however, has a longer history in signal processing and
geophysics, as well as in statistics [76].
For a precise statement of Mercer’s theorem, and its proof we refer to classical
texts in functional analysis such as [62]. For more details about radial basis functions
that play a signiﬁcant role, e.g., in approximation theory, see, e.g., [7].

Chapter 7
Posterior Densities, Ill-Conditioning,
and Classical Regularization
Now that the basic principles guiding the design of likelihoods and priors have been
introduced, we are ready to welcome on the stage the main character in the Bayesian
play of inverse problems, the posterior distribution, and in particular, the posterior
density. Bayes’ formula is the way in which prior and likelihood combine into the
posterior density. In this chapter, we show through some examples how to explore
and analyze posterior distributions. In later chapters, particular attention will be given
to the design of numerical schemes of reduced complexity to deal with posteriors
for high-dimensional inverse problems. In this chapter, we will build connections
between posterior densities and classical regularization methods.
7.1
Likelihood Densities and Ill-Posedness of Inverse
Problems
We start by considering the most popular types of posterior densities, corresponding
to independent additive noise models. Let the observation model be of the form
b = f (x) + e,
where f : Rn →Rm is a known deterministic function, and e ∈Rm represents addi-
tive observation noise. According to the Bayesian paradigm, all unknowns are mod-
eled as random variables, leading to the stochastic extension,
B = f (X) + E.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
D. Calvetti and E. Somersalo, Bayesian Scientiﬁc Computing, Applied Mathematical
Sciences 215, https://doi.org/10.1007/978-3-031-23824-6_7
125

126
7
Posterior Densities, Ill-Conditioning, and Classical Regularization
If E and X are independent and have probability densities
E ∼πE,
X ∼πX,
according to Bayes’ formula, the posterior distribution is
πX|B(x | b) = πB|X(b | x)πX(x)
πB(b)
∝πE

b −f (x)

πX(x),
b = bobserved,
(7.1)
where the scaling factor in the denominator,
πB(b) =

πX,B(x, b)dx =

πB|X(b | x)πX(x)dx,
was assumed to be non-zero, and because it is simply a normalizing constant inde-
pendent of x, it was ignored.
Inverse problems are usually ill-posed, meaning that a solution may not exist, or
if it does, it may not be unique, and it tends to be highly sensitive to small errors
in the data. All these disturbing features are reﬂected in the likelihood density, and
can often be attenuated by using prior information, as the following simple example
demonstrates.
Example 7.1 Consider the problem of determining a pair (x1, x2) of positive real
numbers from noisy observations of their product, corresponding to an observation
model
b = f (x) + e,
f (x) = x1x2,
x1, x2 > 0.
The problem is clearly ill-posed in the sense described above, as the product of two
numbersdoesnotdeterminetheminauniqueway.Toseehowtheill-posednessaffects
the posterior, consider a plot of the likelihood density in the particular case in which e
is a realization of a random variable E that is assumed to be scaled white noise,
E ∼N(0, σ 2),
implying that
πB|X(b | x) =
1
√
2πσ 2 exp

−1
2σ 2 (b −x1x2)2

.
Figure 7.1 shows that the likelihood density is a ridge-like function concentrated
around the curve x1x2 = b, and all values along that curve are equally likely solutions.
The situation changes when we deﬁne an informative prior density, which in this
case was chosen to be a Gaussian distribution. The prior density is informative in
the sense that it contains complementary information about the unknown, and the
posterior density, shown in the right panel, is well localized and leads to a rather
good identiﬁcation of a credible solution.

7.1 Likelihood Densities and Ill-Posedness of Inverse Problems
127
Fig. 7.1 Left: Likelihood density corresponding to a noisy observation of the product of two non-
negative numbers, and an informative Gaussian prior. Right: Posterior density corresponding to this
model
The previous example, in all its simplicity, highlights the importance of informa-
tive prior densities and is a clear warning about the danger of relaying exclusively on
the likelihood. We elaborate further on this idea in connection with linear models.
Consider a linear observation model with additive noise,
b = Ax + e,
A ∈Rm×n,
and assume that the noise e is believed to be a realization of a scaled Gaussian white
noise E ∼N(0, σ 2Im). This leads to a likelihood model of the form
πB|X(b | x) ∝exp

−1
2σ 2 ∥b −Ax∥2

.
Based on this model alone, what would be a good estimate for x? In the classical
statistics, a frequently given answer is the maximum likelihood (ML) estimator,
xML = argmaxπB|X(b | x),
provided that such maximizer exists. For most inverse problems, the ML estimate is
of little use due to the intrinsic ill-posedness of the problem. The ML estimate is a
least squares solution,
xML = argmin∥b −Ax∥2,
and can be expressed in terms of the components of the singular value decomposition
A = UDVT
of the matrix A. We write the orthogonal matrices U and V in terms of their orthonor-
mal column vectors as

128
7
Posterior Densities, Ill-Conditioning, and Classical Regularization
U =
u1 · · · um

,
V =
v1 · · · vn

.
If A has r nonzero singular values d j > 0, 1 ≤j ≤r, we have
b −Ax =
m
	
j=1
u juT
j b −
r
	
j=1
u jd jvT
j x
=
r
	
j=1
u j

uT
j b −d jvT
j x

+
m
	
j=r+1
u juT
j b,
from which one can deduce that in order to minimize the norm of the residual, a least
squares solution must be of the form
xML =
r
	
j=1
uT
j b
d j
v j +
n
	
j=r+1
α jv j,
(7.2)
where the coefﬁcients α j can be chosen arbitrarily. The second term of the right-hand
side of (7.2) is a general vector in the null space of the matrix A, and has no effect
on the observation and, conversely, the observation does not contain any information
about it. We therefore observe that, if r < n, hence the rank of the matrix of A is less
than n, the ML estimate cannot be unique. Furthermore, it is usually very sensitive
to noise, because any noise component along the singular vector u j is divided by d j,
and the smaller d j, the larger the ampliﬁcation of the noise.
More generally, if the noise is Gaussian with SPD covariance matrix Σ, the like-
lihood density is of the form
πB|X(b | x) ∝exp

−1
2(b −Ax)TΣ−1(b −Ax)

,
and the maximum likelihood estimator is the minimizer of the quadratic form
E0(x, b) = 1
2(b −Ax)TΣ−1(b −Ax).
By the positive deﬁniteness of Σ, it admits a symmetric factorization , for example,
the Cholesky decomposition, or, the square root,
Σ = Σ1/2Σ1/2,
allowing us to write
E0(x, b) = 1
2∥b′ −A′x∥2,
b′ = Σ−1/2b,
A′ = Σ−1/2A.

7.2 Maximum a Posteriori Estimate and Regularization
129
The normal equations that a least squares solution needs to satisfy can be written as
(A′)T(A′)x = (A′)Tb′
or,
ATΣ−1Ax = ATΣ−1b.
This equation has a unique solution if and only if the matrix ATΣ−1A is invertible,
which usually is not guaranteed.
7.2
Maximum a Posteriori Estimate and Regularization
The classical deterministic approach for the solution of ill-posed problems is to
approximate them with a nearby well-posed problem, an approach generally called
regularization. Given an observation model with additive noise,
b = f (x) + e,
the idea behind regularization is to replace the functional,
E0(x, b) = ∥b −f (x)∥2,
whose minimizer is ill-deﬁned, with the functional
Eα(x, b) = ∥b −f (x)∥2 + αR(x),
(7.3)
where R is a suitably chosen regularization functional, and α > 0 is a regularization
parameter that determines the relative weights of the two terms. We denote the
minimizer of (7.3), if it exists, by xα.
The selection of the value of the regularization parameter for α is crucial for
the quality of the solution. One of the most popular selection criteria is based on
Morozov’s discrepancy principle that can be summarized as follows. If an estimate
for the norm of the additive noise is available,
∥e∥≈η,
the value of the parameter α is implicitly deﬁned by the condition
∥b −f (xα)∥= η.
The motivation behind Morozov discrepancy principle is that any vector satisfying
the above condition is acceptable and compatible with the noise level, while a residual
normsmallerthanthenormofthenoisewouldindicatethatthesolutionisﬁttingtothe

130
7
Posterior Densities, Ill-Conditioning, and Classical Regularization
noise, which is usually a bad idea, in particular, for ill-posed problems. The process
of ﬁnding the minimizer and adjusting the value of the regularization parameter is
called Tikhonov regularization.
The selection of the regularization functional is not a trivial problem, because R
implicitly determines the undesirable features of the candidate solutions that they
should be penalized for. The general principle for the design of the regularization
functional is that it will favor solutions for which R(x) is small. Popular regulariza-
tion functionals are of the form
R(x) = ∥Rx∥2,
where R ∈Rk×n is a properly chosen matrix. Often, the matrix R is chosen to be
invertible, although this is not a necessary condition. Observe that for any vector
in the null space of R, there is no penalty, indicating that the regularization favors
solutions that are in or near the null space of R.
To establish a connection between classical regularization methods and the
Bayesian approach, assume that the regularization functional is chosen so that the
expression
μX(x) ∝exp

−1
2θ R(x)

,
θ > 0,
(7.4)
deﬁnes a probability distribution, playing a role analogous to that of the prior. Under
the assumption that the additive error is scaled white noise, that is, e is a realization
of E ∼N(0, σ 2Im), the posterior distribution becomes
πX|B(x | b) ∝exp

−1
2σ 2 ∥b −f (x)∥2 −1
2θ R(x)

.
Similarly to the process leading to the maximum likelihood estimator, we deﬁne the
maximum a posteriori (MAP) estimate as
xMAP = argmax πX|B(x | b).
In the particular case that we are considering, computing the MAP estimate amounts
to ﬁnding a minimizer of the negative logarithm of the posterior,
xMAP = argmin

∥b −f (x)∥2 + σ 2
θ R(x)

.
Therefore, in this example, the Tikhonov regularized solution xα coincides with the
xMAP estimate, if we set the regularization parameter α = σ 2/θ.
A natural question is whether there is a Bayesian analogue of Morozov’s discrep-
ancy principle. We observe that setting the prior according to (7.4) assumes that θ is
known, and adjusting θ is tantamount to adjusting our prior belief, an operation that
should be independent of the observations, hence of the noise level. However, it is

7.2 Maximum a Posteriori Estimate and Regularization
131
quite common to not have a priori a set value for θ in mind. In that case, according
to the Bayesian paradigm, the parameter θ should be modeled as a random variable,
in which case we are in the case of the hierarchical priors discussed in the previous
chapter. Thus, we write the prior as a conditional prior,
μX|Θ(x | θ) = N(θ)exp

−1
2θ R(x)

,
(7.5)
where N(θ) is a normalizing factor that depends on the parameter, and then introduce
a hyperprior for Θ. Observe that the normalizing constant may, in general, be difﬁcult
to evaluate, as it depends on the functional form of R(x).
We close this chapter with an example that motivates the need for sampling meth-
ods and underlines the importance of ﬁnding effective samplers.
Example 7.2 Consider the simple inverse problem of estimating x ∈R2 from the
noisy observation
b = x3
1 −2x2 + e,
where e is a realization of a scaled white noise, E ∼N(0, σ 2) with σ = 0.1. For the
sake of deﬁniteness, assume that
b = bobserved = 0.1.
Furthermore, assume that the prior is, up to a normalizing constant,
πX(x) ∝e−∥x∥,
By Bayes’ formula, we have that
πX|B(x | b) ∝p(x) = exp (−∥x∥) exp

−1
2σ 2 (b −(x3
1 −2x2))2

.
To have an idea of what the posterior density looks like, we set up a rejection sampling
scheme. The plan is to
1. Draw randomly from the prior density;
2. Accept or reject the proposal based on the likelihood.
Therefore, we need to ﬁnd ﬁrst a way to sample from the prior. To this end, we
transform the prior density in polar coordinates,
x1 = ϕ1(r, θ) = r cos θ,
x2 = ϕ2(r, θ) = r sin θ.
The determinant of the Jacobian of this transformation is
det

Dϕ(r, θ)

= r,

132
7
Posterior Densities, Ill-Conditioning, and Classical Regularization
and therefore, we have
πRΘ(r, θ) = Ce−rr,
where C is a normalizing constant. To ﬁnd the value of this constant, we integrate
the expression over the whole space, getting
 2π
0
 ∞
0
πRΘ(r, θ)drdθ = C
 2π
0
 ∞
0
e−rrdrdθ = 2πC,
implying that
C = 1
2π .
In the rejection sampling algorithm, we set
q(x) = 1
2π e−∥x∥.
We then have
p(x) = exp (−∥x∥) exp

−1
2σ 2 (b −(x3
1 −x2))2

≤e−∥x∥= 2πq(x),
so the condition guaranteeing that the rejection sampling works is valid for M = 2π.
The acceptance ratio in this case becomes
α =
p(x)
2πq(x) = exp

−1
2σ 2 (b −(x3
1 −2x2))2

≤1.
We therefore need only to ﬁnd a way to draw from the prior. However, this is relatively
straightforward in polar coordinates,
πR,Θ(r, θ) = 1
2π e−rr,
allowing us to draw R and Θ separately. The angular variable is drawn from uniform
distribution
Θ ∼Uniform([0, 2π]).
To draw R, we write the cumulative distribution,
ΦR(r) =
 r
0
e−ttdt = 1 −(1 + r)e−r,
and use the algorithm discussed in Sect.4.5. Let t
be a realization of
T ∼Uniform([0, 1]). To obtain a random draw of R, we need to solve the equa-

7.2 Maximum a Posteriori Estimate and Regularization
133
Fig. 7.2 Left: A sample of 10000 points drawn from the prior density. Right: Rejection sampling
result, the accepted points plotted as black dots. In this example, only about 3% of the prior sample
was accepted
tion
ΦR(r) = t, or (1 + r)e−r = 1 −t.
This can be done numerically, e.g., by using the Newton algorithm for non-linear
equations.
The result of the rejection sampling is shown in Fig. 7.2. Observe that the likeli-
hood density is a narrow ridge around the curve x2 = 2x3
1 −b, and only few of the
prior points fall on this ridge, leading to acceptance. This example highlights the
need for more efﬁcient sampling methods than the rejection sampling.
Notes and Comments
The connections between Tikhonov regularization and Bayesian inverse problems
have been elaborated further in references [20, 49]. As pointed out here, the matrix
R deﬁning the penalty functional needs for not be invertible. A sufﬁcient condition
for making the regularized problem well-posed in the case of a linear forward model
is that N(A) ∩N(R) = {0}, thus leaving no ambiguity in the solution.
ItisnotuncommonthataregularizationtermR(x) = ∥x∥2 iscalledTikhonovreg-
ularization, and R(x) = ∥Rx∥2 generalized Tikhonov regularization. This nomen-
clature does not do justice to Tikhonov, who considered even more general penalty
functions. We refer here to [77–79] for further details.

Chapter 8
Conditional Gaussian Densities
Gaussian probability distributions are the workhorse in Bayesian scientiﬁc comput-
ing, providing a well understood subclass of distributions that often allow closed
form solutions to inference problems. Not only do the Gaussian distributions play a
role similar to that of linear operators in analysis and in inverse problems in particu-
lar, as pointed out earlier, but there is an even deeper connection between these two
classes, as seen in this chapter.
8.1
Gaussian Conditional Densities
We start by deriving the general form of conditional distributions for jointly Gaussian
random variables.
Let Z ∈RN be a Gaussian random variable, and assume that its ﬁrst n components
are unknown, 0 < n < N, while the remaining m = N −n ones are observed. From
the notion of independence, it is clear that if the two groups of components are
independent, observing the latter gives no information about the former, while in the
general case inference through correlation will occur, as shown below.
To write the density of the n ﬁrst components of Z conditioned on knowledge of
the last m ones, we start with a partitioning of Z of the form
Z =
 Z1
Z2
 ∈Rn
∈Rm ,
n + m = N,
and we express the probability density of Z as the joint density of Z1 and Z2,
πZ(z) = πZ1,Z2(z1, z2).
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
D. Calvetti and E. Somersalo, Bayesian Scientiﬁc Computing, Applied Mathematical
Sciences 215, https://doi.org/10.1007/978-3-031-23824-6_8
135

136
8
Conditional Gaussian Densities
The probability density of Z1, under the condition that Z2 is known, is given by
πZ1|Z2(z1 | z2) ∝πZ1,Z2(z1, z2),
z2 = z2,observed.
Observe that if we are investigating a Gaussian linear model with additive noise, and
the model is
Z1 = AZ2 + E,
the conditional density above corresponds to the likelihood, while reversing the roles
of Z1 and Z2 yields the posterior density.
In the following, we assume that Z ∈RN is a zero mean Gaussian random variable
with symmetric positive deﬁnite covariance matrix C ∈RN×N, the joint probability
density of Z1 ∈Rn and Z2 ∈Rm being
πZ1,Z2(z1, z2) ∝exp

−1
2zTC−1z

.
(8.1)
To investigate how this expression depends on z1 when z2 is given, we start by
partitioning the covariance matrix,
C =
C11 C12
C21 C22

∈RN×N,
(8.2)
where
C11 ∈Rn×n,
C22 ∈Rm×m,
and by the symmetry of C,
C12 = CT
21 ∈Rn×m.
We denote the precision matrix C−1 by B, and we partition it according to the
partition of C,
C−1 = B =
 B11 B12
B21 B22

∈RN×N.
(8.3)
We then write the quadratic form appearing in the exponent of (8.1) as
zTBz = zT
1B11z1 + 2zT
1B12z2 + zT
2B22z2
(8.4)
=

z1 + B−1
11 B12z2
TB11

z1 + B−1
11 B12z2

+ zT
2

B22 −B21B−1
11 B12

z2

	

independent of z1
.
This is the key equation when considering conditional densities. Observing that the
term in (8.4) independent on z1 contributes only to a multiplicative constant, we can
write

8.1 Gaussian Conditional Densities
137
πZ1,Z2(z1 | z2) ∝πZ1,Z2(z1, z2)
∝exp

−1
2

z1 + B−1
11 B12z2
TB11

z1 + B−1
11 B12z2

.
We therefore conclude that the conditional density is Gaussian,
πZ1|Z2(z1 | z2) ∝exp

−1
2(z1 −z1)TD−1(z1 −z1)

,
with mean
z1 = −B−1
11 B12z2
and covariance matrix
D = B−1
11 .
To express these quantities in terms of the covariance matrix C, we need to intro-
duce Schur complements. Let us consider the symmetric positive deﬁnite matrix
C ∈RN×N, partitioned according to (8.2). Since C is positive deﬁnite, so are C11
and C22. In fact, for any v1 ∈Rn, v1 ̸= 0,
vT
1 C11v1 =

vT
1 0
 C11 C12
C21 C22
  v1
0

> 0,
showing the positive deﬁniteness of C11. The proof that C22 is positive deﬁnite is
analogous.
To compute C−1 using the block partitioning, we solve the linear system
Cz = y
in block form using Gaussian elimination. We begin by partitioning z and y as
z =
 z1
z2
 ∈Rn
∈Rm ,
y =
 y1
y2
 ∈Rn
∈Rm ,
and perform block-wise multiplication to get
C11z1 + C12z2 = y1,
(8.5)
C21z1 + C22z2 = y2.
(8.6)
Solvingthesecondequationfor z2,whichcanbedonebecauseC22 ispositivedeﬁnite,
thus invertible, we have
z2 = C−1
22

y2 −C21z1

.

138
8
Conditional Gaussian Densities
Substituting this expression for z2 into the ﬁrst equation and rearranging the terms
yields

C11 −C12C−1
22 C21

z1 = y1 −C12C−1
22 y2.
We deﬁne the Schur complement of C22 to be
C22 = C11 −C12C−1
22 C21.
With this notation,
z1 = C−1
22 y1 −C−1
22 C12C−1
22 y2.
(8.7)
Similarly, solving (8.5) for z1 ﬁrst and plugging it into (8.6), we may express z2 as
z2 = C−1
11 y2 −C−1
11 C21C−1
11 y1
(8.8)
in terms of the Schur complement of C11,
C11 = C22 −C21C−1
11 C12.
Collecting (8.7) and (8.8) into a matrix expression as
 z1
z2

=

C−1
22
−C−1
22 C12C−1
22
−C−1
11 C21C−1
11
C−1
11
  y1
y2

,
we deduce that
C−1 =

C−1
22
−C−1
22 C12C−1
22
−C−1
11 C21C−1
11
C−1
11

=
 B11 B12
B21 B22

,
which is the formula that we were looking for.
In summary, we have derived the following result.
The conditional density πZ1|Z2(z1 | z2) is a Gaussian probability distribution
πZ1|Z2(z1 | z2) = N(z1 | z1, D),
with conditional mean (CM)
z1 = −B−1
11 B12z2 = C12C−1
22 z2,
and conditional covariance
D = B−1
11 = C22 = C11 −C12C−1
22 C21.
Before showing how these results can be used in some applications, the following
observation is in order. If Z1 and Z2 are uncorrelated, that is, C12 = 0, C21 = 0, the
conditional density of Z1 given Z2 reduces to the marginal density

8.2 Linear Inverse Problems
139
πZ1|Z2(z1 | z2) = πZ1(z1) = N(z1 | 0, C11).
In particular, this means that uncorrelated Gaussian random variables are also inde-
pendent. While vanishing correlation does not, in general, imply independency, in
the case of Gaussian random variables it does, and the two concepts are equivalent.
8.2
Linear Inverse Problems
Consider the linear inverse problem
b = Ax + e,
where x ∈Rn represents the unknown, b ∈Rm is the observed quantity, e ∈Rm is
the unknown additive noise, and A ∈Rm×n is the known forward mapping, and its
stochastic extension is
B = AX + E.
Assume that X and E are both zero mean Gaussian random variables, and for sim-
plicity, mutually independent,
X ∼N(0, Γ),
E ∼N(0, Σ),
where Γ ∈Rn×n and Σ ∈Rm×m are symmetric positive deﬁnite matrices. We com-
bine X and B into the composite random variable
Z =
 X
B

∈Rn+m.
In order to apply the formulas of the previous section, we compute the covariance
matric C of this random variable. Recalling that
E

X XT) = Γ,
and using the independency of X and E and the fact that they have zero means,
E

X BT
= E

X(AX + E)T
= E

X XT
AT + E

X

E

ET
= ΓAT
= E

BXTT,

140
8
Conditional Gaussian Densities
and, similarly,
E

BBT
= E

(AX + E)(AX + E)T
= AE

X XT
AT + E

E ET
= AΓAT + Σ.
The joint covariance matrix is therefore given by
C =
C11 C12
C21 C22

=
 Γ
ΓAT
AΓ AΓAT + Σ

.
The general result for conditional probability densities of Gaussian variables gives
us now the following result.
Given a linear observation model, and assuming a zero mean Gaussian prior and
zero mean independent Gaussian additive noise, the posterior density of X given
the observation B = b is
πX,B(x | b) = N(x | x, D),
where the posterior mean and posterior covariance are
x = ΓAT
AΓAT + Σ
−1b,
(8.9)
D = Γ −ΓAT
AΓAT + Σ
−1AΓ.
(8.10)
Before considering other applications, an observation about the posterior covari-
ance is in order. Given a direction vector v ∈Rn, ∥v∥= 1, the posterior variance of
X along v is given by vTDv. It follows from (8.10) that
vTDv = vTΓv −vTΓAT
AΓAT + Σ
−1AΓv ≤vTΓv,
that is, an indirect observation of a random variable can only decrease the uncertainty
about its value.
We remark further that if the prior is not zero mean but rather
X ∼N(μ, Γ),
the posterior mean needs to be modiﬁed as
x = μ + ΓAT
AΓAT + Σ
−1(b −Aμ),
(8.11)
while the posterior covariance remains unchanged.

8.3 Interpolation and Conditional Densities
141
8.3
Interpolation and Conditional Densities
A natural application of the computation of conditional densities for Gaussian dis-
tributions is a classic interpolation problem, known in geostatistics as kriging, and
in probability theory as the Wiener–Kolmogorov prediction.1 The problem that we
consider is stated as follows.
Estimate a smooth function f over a domain Ω ∈Rn based on few noisy obser-
vations of it,
b j = f (t j) + e j,
t j ∈Ω,
1 ≤j ≤m,
where e j represents the additive noise. In geostatistics, where n = 2, f could rep-
resent the ore distribution over an area, or underwater hydraulic head.
We start by considering the problem in one dimension over a ﬁnite interval,
Ω = [0, 1]. Let 0 < t1 < . . . < tm < 1, and assume at ﬁrst that the values of f at
the left and right endpoints are given,
f (0) = βL,
f (1) = βR.
A classic solution to this problem uses cubic splines, i.e., curves that between the
interpolation points are third-order polynomials, glued together at the observation
points so that the resulting piecewise deﬁned function and its derivatives, up to the
second order, are continuous. The Bayesian solution that we are proposing is based
on conditioning and, it turns out, can be orchestrated so that the realization with
highest posterior probability corresponds to a spline.
Since the statement of the problem does not include an explicit deﬁnition of
smoothness or of error bounds, we will provide a subjective interpretation. We start
by discretizing the problem. For simplicity, we introduce a uniform discretization
grid and we assume that the observation points coincide with some of the nodes of
the discretization. Let
xk = f (sk),
sk =
k
n + 1,
0 ≤k ≤n + 1,
for some n, and assume that t j = sk j for some k j, that is, the data are deﬁned as
b j = xk j + e j,
1 ≤j ≤m.
We may write the observation model in terms of matrices and vectors. Let x ∈Rn be
the vector containing the unknown function values at the interior points, x1, . . . , xn,
and write the observation model as
b = Ax + e,
1 The word kriging refers to the name of the South African geostatistician Danie Gerhardus Krige.
The other name of the method probably needs no explanations.

142
8
Conditional Gaussian Densities
where A ∈Rm×n is the sampling matrix with entries
a jk = 1 if and only if k = k j, 1 ≤j ≤m
a jk = 0 otherwise.
To account for the smoothness of f , we introduce the second-order smoothness prior
2X1 = βL + X2 + γW1
2X2 = X1 + X3 + γW2
...
...
2Xn = Xn−1 + βR + γWn,
where the random variables W j are the components of the random vector W ∼
N(0, In). In matrix form, the prior model is given by
LX = γW + β,
where the vector β accounts for the boundary values,
β =
⎡
⎢⎢⎢⎢⎢⎣
βL
0
...
0
βR
⎤
⎥⎥⎥⎥⎥⎦
∈Rn,
and L ∈Rn×n is the second-order ﬁnite difference matrix
L =
⎡
⎢⎢⎢⎣
2 −1
−1
2 −1
... ... ...
−1 2
⎤
⎥⎥⎥⎦∈Rn×n.
(8.12)
The matrix L is invertible and, moreover, it allows a factorization in terms of ﬁrst-
order ﬁnite difference matrices,
L = L1LT
1 ,
L1 =
⎡
⎢⎢⎢⎢⎢⎣
1 −1
1 −1
... ...
−1
1
⎤
⎥⎥⎥⎥⎥⎦
,

8.3 Interpolation and Conditional Densities
143
hence we may write
X = γL−1W + L−1β,
from which we conclude that X is Gaussian, and its prior density is
πX(x) = N(x | L−1β, γ2L−1L−T).
To model the observation error, we assume that the noise arises from a model
E ∼N(0, σIm),
that is, the additive error is scaled white noise. Therefore, we may write the posterior
mean as
x = L−1β + γ2L−1L−TAT
γ2AL−1L−TAT + σ2Im
−1(b −AL−1β)
= L−1β + L−1L−TAT
AL−1L−TAT + α2Im
−1(b −AL−1β),
where α = σ/γ, and the posterior covariance as
D = γ2L−1L−T −γ2L−1L−TAT
AL−1L−TAT + α2Im
−1AL−TL−1.
These rather cumbersome formulas can be slightly simpliﬁed by introducing the
matrix
M = L−TAT ∈Rn×m,
so that
x = L−1
β + M(MTM + α2Im)−1(b −MTβ)

,
D = γ2L−1
In −M(MTM + α2Im)−1MT
L−T.
Figure 8.1 shows the results of a computed example. Here, the endpoint values are
ﬁxed at βL = 1 and βR = 1.5, and four data points marked with a black dot are given
with error standard deviation σ = 0.1. The prior parameter is set to γ = 10/(n + 1),
where n = 50 is the number of discretization intervals. In the ﬁgure, we have plotted
the posterior mean x as well as the credible envelope with an upper and lower
boundary deﬁned by
c±
j = x j ±

D j j,
1 ≤j ≤n.
The interpolation approach can be generalized to several dimensions, however,
we discuss before some computational issues that simplify the implementation.

144
8
Conditional Gaussian Densities
Fig. 8.1
Smooth
interpolation of four data
points and ﬁxed endpoint
values. The solid line is the
posterior mean, surrounded
by the one standard deviation
credible envelope
8.4
Covariance or Precision?
In the example discussed in the previous section, the prior covariance matrix was
given by
Γ = γ2L−1L−T,
and its inverse, the precision matrix, is
Γ−1 = 1
γ2 LTL.
From the computational point of view, in this case the precision matrix is easier to
handle because it is a sparse matrix, while the covariance matrix is a full matrix.
In the example discussed above, the dimensionality of the problem was low enough
(n = 50) to guarantee that the fullness of the matrix was not a computational issue. In
higher spatial dimensions, however, the sparsity of the matrices used in computations
can make a big difference in terms of computational feasibility. Therefore, it is useful
to consider alternative formulas for the posterior, mean and covariance expressed in
terms of the precision matrix.
Consider the expressions for the probability densities of the Gaussian prior and
Gaussian likelihood,
πX(x) ∝exp

−1
2 xTΓ−1x

,
πB|X(b | x) ∝exp

−1
2(b −Ax)TΣ−1(b −Ax)

,
and combine them according to Bayes’ formula to obtain the probability density of
the posterior,

8.4 Covariance or Precision?
145
πX|B(x | b) ∝πX(x)πB|X(b | x)
∝exp

−1
2 xTΓ−1x −1
2(b −Ax)TΣ−1(b −Ax)

.
Consider the quadratic expression in the exponent of the posterior density,
Q(x) = (b −Ax)TΣ−1(b −Ax) + xTΓ−1x,
and collect the terms of the same order in x to get
Q(x) = xT
ATΣ−1A + Γ−1
x −2xTATΣ−1b + bTΣ−1b
= xTPx −2xTATΣ−1b + bTΣ−1b,
where
P = ATΣ−1A + Γ−1.
Completion of the square yields
Q(x) = (x −P−1ATΣ−1b)TP(x −P−1ATΣ−1b)
+ terms independent of x,
therefore we conclude that the posterior is also Gaussian, and its density is of the
form
πX|B(x | b) ∝exp

−1
2(x −P−1ATΣ−1b)TP(x −P−1ATΣ−1b)

.
Therefore the posterior mean and covariance are, respectively,
x =

ATΣ−1A + Γ−1−1ATΣ−1b,
(8.13)
D =

ATΣ−1A + Γ−1−1.
(8.14)
Formula (8.13) for x is also known as Wiener ﬁltered solution.
It is not immediately obvious that (8.9) and (8.13) are equal, although we know
this to be the case because the posterior is uniquely deﬁned, hence the two different
derivations of the mean and covariance must agree.
It is possible to prove directly the equality of the two expressions for the mean and
for the covariance using linear algebra. The proof, presented below, is based on the
Sherman–Morrison–Woodbury (SMW) identity, a useful tool to compute efﬁciently
the inverse of the sum of an invertible matrix plus a low rank perturbation. The
Sherman–Morrison–Woodbury identity can be formulated as follows.

146
8
Conditional Gaussian Densities
Let M ∈Rn×n be an invertible matrix, and U, V ∈Rn×k, k ≤n. Then the matrix
M+ = M + UVT
is invertible if the k × k matrix Ik + VTM−1U is invertible, and the inverse is given
by
M−1
+ = M−1 −M−1U

Ik + VTM−1U
−1VTM−1.
(8.15)
Before proving the SMW identity, we show how it can be used to prove the
equivalence of the expressions for the mean and the covariance. Consider formula
(8.14) for the posterior covariance,
D =

ATΣ−1A + Γ−1−1.
In the SMW formula, set
M = Γ−1,
U = V = ATΣ−1/2.
Then,
I + VTM−1U = I + Σ−1/2AΓATΣ−1/2
= Σ−1/2
Σ + AΓAT
Σ−1/2,
and, observing that M+ is the posterior precision, we ﬁnd that
D = M−1
+ = Γ −ΓAT
Σ + AΓAT−1AΓ,
as claimed.
We are now ready to prove the SMW identity.
Consider the linear equation
(M + UVT)u = v.
First multiply both sides from the left by M−1 to obtain
u + M−1UVTu = M−1v,
(8.16)
then multiply both sides from the left by VT to get
VTu + VTM−1UVTu = (Ik + VTM−1U)VTu
= VTM−1v.

8.5 Some Computed Examples
147
Solving the last equation for VTu, we obtain
VTu = (Ik + VTM−1U)−1VTM−1v.
(8.17)
On the other hand, from (8.16) we obtain the following expression for u,
u = M−1v −M−1UVTu,
and, substituting the right hand side of (8.17) for VTu, gives
u = M−1v −M−1U(Ik + VTM−1U)−1VTM−1v
=

M−1 −M−1U(Ik + VTM−1U)−1VTM−1
v,
which is the alternative formula (8.15) for M−1
+ .
8.5
Some Computed Examples
In this section, we consider some computed examples based on the results in this
chapter.
8.5.1
Bayesian Interpolation of Multi-Dimensional Data
We return the interpolation problem solved in the case of a one-dimensional signal
by kriging. In this case we want to ﬁnd the interpolant in terms of the alternative
formula for the posterior distribution that we have derived in this chapter.
Considertheproblemofestimatinganunknowntwo-dimensionalsmoothfunction
f : Ω = [0, 1] × [0, 1] →R given a few observed values. We assume here that at
the boundary ∂Ω, the function f vanishes. Let the measurements be the values of
the function at y j = (y j
1, y j
2) ∈Ω, 1 ≤j ≤m, and assume that the function values
are corrupted by additive scaled Gaussian white noise. We write the stochastic model
for the data,
B j = f (y j) + E j,
1 ≤j ≤m,
where E ∼N(0, σ2Im). In line with the belief that the underlying function is smooth,
we adopt a second-order smoothness prior to X, the random variable comprising the
unknown values of f in the n × n interior points of a regular discretization grid, i.e.,
X ∼N(0, Γ),
where the covariance matrix is of the form
Γ = γ2−1−T

148
8
Conditional Gaussian Densities
with
 = L2 ⊗In + In ⊗L2,
L2 ∈Rn×n being the second-order ﬁnite difference matrix, and the parameter γ pro-
viding a handle to control the variance of the interpolation. Observe that while the
matrix  is extremely sparse, thus easy to store and to work with numerically even
when the problem is of large dimensions, its inverse is a full matrix. In this case the
alternative formulas (8.13) and (8.14) for the posterior mean and covariance turn out
to be quite convenient. In fact, in this case the precision matrix, given by
Γ−1 = 1
γ2 T,
is the product of two sparse matrices, and we obtain the following expression for the
posterior mean:
x = 1
σ2
 1
σ2 ATA + 1
γ2 T
−1
ATb
=

ATA + η2T
−1 ATb,
η2 = σ2
γ2 .
The computation of the posterior mean requires the solution of the sparse linear
system

ATA + η2T

x = ATb,
which can be done efﬁciently by means of the iterative solvers described in the next
chapter even for very large values of n.
In the computed example, we deﬁne a discrete pixel map of size 150 × 150, and
assume that m = 12 randomly chosen pixel values are given, and the problem is
to smoothly interpolate between these values. The pixel values are assumed to be
Fig. 8.2
Surface plot (left) and contour plot (right) of the interpolating surface corresponding to
the posterior mean with second-order smoothness prior

8.5 Some Computed Examples
149
given with an uncertainty that is expressed in terms of a Gaussian additive error with
standard deviation σ = 0.05. The scaling parameter of the prior was set at γ = 15
The computed solution is shown in Fig. 8.2.
8.5.2
Posterior Density by Low-Rank Updating
In computational inverse problems, sometimes the data become available in smaller
batches, thus it is natural to compute the posterior density by sequential updating
steps. Updating strategies using a small subset of the data at a time have been used
in some classical inversion algorithms, see Notes and comments. Here, we employ
the Sherman–Morrison–Woodbury formula and the two alternative forms for the
Gaussian posterior distribution.
Consider a linear inverse problem, written in the stochastic framework as
B = AX + E,
A ∈Rm×n,
(8.18)
with Gaussian noise, E ∼N(0, Σ). If
Σ−1 = SST
is a symmetric factorization of the precision matrix of the noise, redeﬁning B = SB,
A = SA we can assume that the noise in (8.18) is white Gaussian noise. By assuming
further that X is a priori zero mean Gaussian, we write the model X ∼N(0, Γ).
We start by considering the particular case of the linear problem (8.18) in which
m = 1, that is, the action of the matrix A can be expressed as an inner product,
Ax = uTx,
u ∈Rn.
In this case, with Σ = 1, formulas (8.13) and (8.14) give for the posterior covariance
and mean the formulas
D =

uuT + Γ−1−1 ,
x =

uuT + Γ−1−1 ub = Dub.
We see that the posterior precision matrix is a rank-one update of the prior precision
matrix,
D−1 = uuT + Γ−1,
and the update of the covariance matrix can therefore be done using the Sherman–
Morrison–Woodbury formula, that is,

150
8
Conditional Gaussian Densities
D = Γ −(Γu)

1 + uT(Γu)
−1 uTΓ
= Γ −
vvT
1 + uTv ,
v = Γu,
which, too, is a rank-one update. This formula allows us to write the posterior mean
as
x = Γub −
vvT
1 + uTv ub
= b

1 −
uTv
1 + uTv

v
=
b
1 + uTv v.
Consider now the general case A ∈Rm×n. We write the matrix A in terms of its
row vectors as
A =
⎡
⎢⎣
uT
1...
uT
m
⎤
⎥⎦,
u j ∈Rn.
With this notation, the likelihood density, assuming that the noise has been whitened,
can be written as
πB|X(b | x) ∝exp
⎛
⎝−1
2
m

j=1
(b j −uT
j x)2
⎞
⎠
= exp

−1
2(b1 −uT
1 x)2

· · · exp

−1
2(bm −uT
mx)2

=
m

j=1
πB j|X(b j | x),
that is, after whitening, the observations b j are conditionally independent. We then
write the posterior density as
πX|B(x | b) ∝
m

j=1
πB j|X(b j | x)πX(x).

8.5 Some Computed Examples
151
For any k, 1 ≤k ≤m, we may reinterpret the formula by writing it as
πX|B(x | b) ∝
⎛
⎝
m

j=k+1
πB j|X(b j | x)
⎞
⎠
⎛
⎝
k
j=1
πB j|X(b j | x)πX(x)
⎞
⎠
=
⎛
⎝
m

j=k+1
πB j|X(b j | x)
⎞
⎠πX|B1,...,Bk(x | b1, . . . , bk).
This formula suggests a sequential updating of the posterior: Given the prior, we may
ﬁrst compute the posterior πX|B1(x | b1). Setting k = 1 in the above formula, we may
now consider this posterior as the prior for the remaining observations B2, . . . , Bm.
Inductively, we may then proceed and move one observation at a time from the
likelihood to the updated prior, until the full observation vector is processed.
We summarize the idea in algorithmic form.
1. Initialize: Set the current mean and covariance x = 0, Γ. Set the counter j = 0.
2. Iterate: While j < m,
(a) compute the vector v j = Γu j,
(b) update the mean and the covariance,
x+ = x +
b j −uT
j x
1 + uT
j v j
v j,
Γ+ = Γ −
v jvT
j
1 + uT
j v j
,
(c) replace the current values by the updated ones, x = x+, Γ = Γ+.
(d) Advance the counter, j = j + 1.
When completed, x and Γ contain the posterior mean and covariance.
We demonstrate the algorithm with a small computed example.
Consider the deconvolution problem of recovering a function f : [0, 1] →R from
few convolution data,
g(sk) =
 1
0
a(sk −t) f (t)dt + ek
≈1
n
n

j=1
a(sk −t j) f (t j) + ek,
t j = j
n ,
or in matrix notation, we can write the system as (8.18), where the components x j
correspond to the values of f at the interior points t j, 1 ≤j ≤n −1. We choose the
kernel a to be a Gaussian,
a(t) = 1
2e−t2/(2w2),
w = 0.08.

152
8
Conditional Gaussian Densities
Fig. 8.3
From top left in lexicographical order: The four row vectors of the matrix A corresponding
to the convolution kernels. Prior envelope of two marginal standard deviations, and the generative
function f ∗plotted in red. Updated posterior means (black curves) with the posterior marginal of
two standard deviation envelopes after one, two, three and four updates. The red arrow indicates
the center point of the convolution kernel at the updating round
We select the data points at s1 = 0.2, s2 = 0.4, s3 = 0.6 and s4 = 0.8, and the dis-
cretization level is set at n = 100. The four convolution kernels constituting the rows
of A are shown in the top left panel of Fig. 8.3. We generate the data using a generative
model
f ∗(t) = (0.5 sin 2πt −0.7 sin 4πt) e−15t2.
Additive independent Gaussian noise with standard deviation σ = 0.001 is subse-
quently added to the data, and the noise is whitened. For the prior, we use the second-
order smoothness prior with presumably known boundary values f (0) = f (1) = 0,
that is,
Γ−1 = γ2LTL,
L =
⎡
⎢⎢⎢⎢⎣
−2
1
1 −2 ...
... ...
1
1 −2
⎤
⎥⎥⎥⎥⎦
∈R(n−1)×(n−1).
The parameter γ is chosen so that the estimated marginal variances correspond to our
expectations of the size of the unknown function. In the current example, the value
γ = 1 was used. In Fig. 8.3, the second panel in the top row shows the generative
function f ∗, and the prior marginal envelope of two standard deviations, that is, the
shaded area lies between the values
μ j ± 2

Γ j j = ±2

Γ j j,
1 ≤j ≤n −1,
as the prior was assumed to be zero mean.

8.5 Some Computed Examples
153
We run the four updating steps, and after each update plot the posterior mean and
the marginal envelopes of two standard deviations. The progression of the iteration
is shown in Fig. 8.3. We observe that every time a new update is computed, the
posterior variance is reduced signiﬁcantly near the maximum of the corresponding
convolution kernel.
Notes and Comments
Sequential updating algorithms using a low-dimensional portion of the data at a
time were particularly popular in earlier times, when computers had rather restricted
memory capacity. One of such methods is known as Kaczmarz iteration, or Algebraic
Reconstruction Technique (ART) in computerized tomography, where the data are
entered one projection at a time [57]. Low-rank updates are also popular in opti-
mization by Newton-type methods, where one needs to update the Hessian as the
iterations proceed [32].
For the connection between the spline approximation and Bayesian models, we
refer to the article [51].

Chapter 9
Iterative Linear Solvers
and Priorconditioners
In this chapter we return to the solution of linear systems of equations, a task that in
scientiﬁc computing has a core role, either as a problem of interest per se or as part
of a larger computational endeavor. We begin by discussing iterative linear system
solvers and how they are used for the solution of ill-conditioned inverse problems
following a classical approach.
Traditionally, when solving an ill-conditioned linear system Ax = b, e.g., in the
least squares sense, the remedies for ill-posedness are selected based on the properties
of the matrix A rather than on the expectations about the sought solution x. In contrast,
in the Bayesian framework the beliefs about the solution play an active role. After
reviewing the classical use of iterative solvers for ill-conditioned linear systems,
we will adopt the Bayesian perspective and outline a strategy for importing a priori
beliefs in the solvers, taking advantage of the technical tools of numerical linear
algebra. In particular, statistically inspired preconditioners expressing the believed
properties of the solution are ideally positioned to bridge the deterministic and the
Bayesian paradigms for solving linear ill-posed problems.
9.1
Iterative Methods in Linear Algebra
The solution of a linear system by an iterative solver is inherently very different from
the solution by a direct solver, because the solution is formed gradually, through
a sequence of steps, starting from an initial approximate guess. At each step the
way in which the matrix of the coefﬁcients or its transpose enter is through their
multiplicative action, thus the matrix itself is not needed in explicit form, but only
a protocol for computing the needed matrix-vector products. Iterative methods that
do not require access to the matrix in explicit form are called matrix free.
In general, iterative methods for computing the solution of a system b = Ax, start
from an initial guess x0 and compute a sequence
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
D. Calvetti and E. Somersalo, Bayesian Scientiﬁc Computing, Applied Mathematical
Sciences 215, https://doi.org/10.1007/978-3-031-23824-6_9
155

156
9
Iterative Linear Solvers and Priorconditioners
x1, x2, . . . , xk, . . . ,
of improved approximate solutions. Since the matrix A does not need to be explicitly
formed, iterative linear solvers have long been the methods of choice when the matrix
A is either not explicitly known, or of very large dimensions relative to the computer
memory, while its action on a vector can be computed easily and effectively. The
matrix action x →Ax can be given in the form of an algorithm. Another instance
when iterative methods are preferable to direct linear solvers is when it sufﬁces to
ﬁnd an approximate solution of the linear system.
In general, even if an iterative solver is guaranteed to converge to the desired
solution, the process is terminated when either a maximum number of iterations
has been reached, or some convergence criterion has been satisﬁed. Iterative linear
solvers typically stop when the norm of the error in the approximation of the data,
rk = b −Axk,
referred to as the residual error in the linear algebra literature, or discrepancy in the
inverse problems literature, has been sufﬁciently reduced. The approximate solutions
determined during the iteration steps are called the iterates.
The availability of an approximate solution at any stage of the process is one of
the properties that differentiate iterative solvers most from direct methods, where the
entire process needs to run to completion before any information about the solution
is available. For that reason, direct methods are of the all or nothing kind, because
either the solution is returned at the end of the process, or all is lost if the process is
prematurely interrupted.
The literature on iterative linear solvers is rather vast and comprises different
families of iterative schemes. In this book we limit our discussion to a class of
Krylov subspace iterative solvers, introduced next, where the iterates belong to a
nested sequence of linear subspaces that are automatically determined in the iteration
steps. The subspaces are completely determined from the right-hand side vector and
the coefﬁcient matrix, thus the iterative schemes are effectively nonlinear solvers for
linear systems. This fact makes them extremely versatile and efﬁcient, but also more
difﬁcult to analyze.
9.2
Krylov Subspace Iterative Methods
We set up the stage for our iterative methods of choice by deﬁning the subspaces
that give them their name, and where the iterates live. We start by considering square
matrices. Given a matrix A ∈Rn×n and a vector b ∈Rn, the kth Krylov subspace
associated with the pair (A, b) is
Kk(A, b) = span{b, Ab, . . . , Ak−1b}.

9.2 Krylov Subspace Iterative Methods
157
The two iterative linear solvers that we will introduce below are characterized by
the fact that the kth iterate belongs to a suitable kth Krylov subspace, although the
matrix-vector pair determining the sequence of Krylov subspaces and the criterion
for determining the kth iterate will differ for the two schemes. Iterative schemes
whose subsequent iterates are in a nested sequence of Krylov subspaces are called
Krylov subspace iterative methods.
9.2.1
Conjugate Gradient Algorithm
The ﬁrst Krylov subspace iterative linear solver, originally proposed by Hestenes
and Stiefel in 1952 (see Notes and Comments at the end of the chapter) as a variation
on Gram–Schmidt orthogonalization for symmetric positive deﬁnite (SPD) matrices,
is the Conjugate Gradient (CG) method. The CG method was proposed originally
as an alternative to Gaussian elimination for the solution of the special class of
linear systems with SPD matrices, and the sequence of its iterates was guaranteed to
converge to the solution in at most as many steps as the dimension of the matrix. The
popularity of the CG method increased tremendously with the wider availability of
computers, and modiﬁcations of the algorithm for symmetric positive semi-deﬁnite
matrices were proposed in the literature.
One reason why the CG method is very popular for the solution of large-scale
problems is the fact that it requires only one matrix-vector product with the matrix A
per iteration, and that its memory allocation is very small and essentially independent
of the number of iterations. The kth iterate determined by the CG method implicitly
minimizes the A-norm of the error,
xk = arg
min
x∈Kk(A,b) ∥x −x∗∥2
A,
where the A-norm is deﬁned as
∥z∥2
A = zTAz,
and x∗= A−1b is the unknown exact solution that we want to compute. Fortunately,
the CG algorithm is a procedure for computing the minimizer xk without knowing
x∗. A detailed derivation of the CG algorithm is beyond the scope of this book and
will not be presented, instead we brieﬂy outline the ideas behind it.
At each step k ≥1, the CG algorithm searches for the scalar α which minimizes
the functional
α →∥xk−1 + αpk−1 −x∗∥2
A,
where xk−1 is the previous approximate solution and pk−1 is a vector, called search
direction because it is the direction of the correction to the previous iterate. The
minimizer, found by setting the derivative of the functional with respect to α equal
to zero, is

158
9
Iterative Linear Solvers and Priorconditioners
αk =
∥rk−1∥2
pT
k−1Apk−1
,
where rk−1 is the residual error of the previous approximation,
rk−1 = b −Axk−1,
and the new approximate solution is
xk = xk−1 + αk pk−1.
The selection of the search directions is crucial. In the ﬁrst iteration, the search
direction is determined by the residual error associated with the initial guess,
p0 = r0 = b −Ax0.
In each subsequent iteration, the new search direction pk is a vector A-conjugate to
all previous search directions, hence it must satisfy
pT
k Ap j = 0,
0 ≤j ≤k −1.
While at ﬁrst sight this requirement looks quite complicated and its numerical imple-
mentation time consuming, it turns out that each new search direction lies in the
subspace spanned by the previous search direction and the most recent residual error
vector, that is,
pk = rk + βk pk−1,
rk = b −Axk.
The parameter βk is chosen so that the A–conjugacy is satisﬁed. After some algebraic
manipulation it turns out that
βk =
∥rk∥2
∥rk−1∥2 .
We are now ready to outline how to organize the computations for the Conjugate
Gradient method. For simplicity, the initial guess is set to zero here.
The Conjugate Gradient (CG) algorithm: Given the right-hand side b and an
algorithm to multiply a vector by A:
1. Initialize:
x0 = 0;
r0 = b −Ax0;
p0 = r0;
Set the counter k = 1.

9.2 Krylov Subspace Iterative Methods
159
2. Iterate until a stopping criterion is met:
α =
∥rk−1∥2
pT
k−1Apk−1
;
xk = xk−1 + αpk−1;
rk = rk−1 −αApk−1;
β =
∥rk∥2
∥rk−1∥2 ;
pk = rk + βpk−1;
Advance the counter k →k + 1.
If the initial guess is the zero vector as we assume here, the ﬁrst k iterates deter-
mined by the CG algorithms live in the nested sequence of subspaces,
span{b} ⊂span{b, Ab} ⊂· · · ⊂span{b, Ab, . . . , Ak−1b},
as can be easily veriﬁed by the construction of the sequence.
9.2.2
Conjugate Gradient Method for Least Squares
In general, the conjugate gradient method will break down if the matrix A is not
symmetric positive deﬁnite. If the matrix A is not square and we want to compute
the least squares solution of the associated linear system, and if the columns of A are
linearly independent, the matrix ATA of the associated normal equations,
ATAx = ATb,
(9.1)
is SPD, and the CG methods can be used for computing the solution. The seminal
1952 paper by Hestenes and Stiefel described a variation of the CG algorithm which
can be used for computing the least squares solution of linear systems with general,
non-square matrix A. The algorithm, known as the Conjugate Gradient method for
Least Squares (CGLS) is mathematically equivalent to applying the CG method to
the normal equations (9.1) without ever forming the matrix ATA. The CGLS method
is computationally more expensive than the CG method, requiring two matrix-vector
products per iteration, one with A, one with AT, but its memory allocation is essen-
tially insensitive to the number of iterations. It can be shown that the kth CGLS
iterate solves the minimization problem
xk = arg
min
x∈Kk(ATA,ATb) ∥b −Ax∥2,

160
9
Iterative Linear Solvers and Priorconditioners
where the associated Krylov subspace is, by deﬁnition,
Kk(ATA, ATb) = span

ATb, (ATA)ATb, . . . , (ATA)k−1ATb

.
Equivalently, the kth iterate is characterized by
Φ(xk) =
min
x∈Kk(ATA,ATb) Φ(x),
where
Φ(x) = 1
2 xTATAx −xTATb.
The strategy for determining the minimizer is very similar to that of the Conjugate
Gradient method. The search directions,
p0, p1, . . . , pk−1, pk, . . . ,
determined in the iterations are ATA-conjugate directions, hence, by deﬁnition, the
vectors p j satisfy the condition
pT
j ATApk = 0,
0 ≤j ≤k −1..
The iterate xk is determined by updating xk−1 according to the formula
xk = xk−1 + αk pk−1,
where the coefﬁcient αk ∈R solves the minimization problem
αk = arg min
α∈R Φ(xk−1 + αpk−1).
Introducing the residual error of the normal equations for the CGLS method asso-
ciated with xk,
rk = ATb −ATAxk,
the passage from the current search direction to the next is given by
pk = rk + βk pk−1,
where the coefﬁcient βk is chosen so that pk is ATA-conjugate to the previous search
directions. It can be shown that
βk =
∥rk∥2
∥rk−1∥2 .

9.2 Krylov Subspace Iterative Methods
161
The quantity
dk = b −Axk
is called the discrepancy associated with xk. The discrepancy and the residual of the
normal equations are related via the equation
rk = ATdk.
It can be shown that the norms of the discrepancies form a non-increasing sequence,
while the norms of the solutions form a non-decreasing one,
∥dk+1∥≤∥dk∥,
∥xk+1∥≥∥xk∥.
We are now ready to describe how the calculations for the CGLS method should
be organized. For simplicity, the initial guess is set to zero.
The Conjugate Gradient for Least Squares (CGLS) algorithm: Given the right-
hand side b, and algorithms to multiply by A and AT.
1. Initialize:
x0 = 0;
d0 = b −Ax0;
r0 = ATd0;
p0 = r0;
y0 = Ap0;
Set the counter k = 1.
2. Iterate until the stopping criterion is met:
α = ∥rk−1∥2
∥yk−1∥2
xk = xk−1 + αpk−1;
dk = dk−1 −αyk−1;
rk = ATdk;
β =
∥rk∥2
∥rk−1∥2 ;
pk = rk + βpk−1;
yk = Apk;
Advance the counter k →k + 1.
Observe that when the columns of the matrix A ∈Rm×n are linearly independent,
the CGLS algorithm converges to the minimum norm solution of the least squares
problem in at most m iterations. Furthermore, if A is invertible, the minimum norm
solution is also the solution to the equation Ax = b.

162
9
Iterative Linear Solvers and Priorconditioners
The CGLS algorithm, combined with Bayesian prior models, turns out to be
an extremely useful tool for solving inverse problems. Observe that we have not
speciﬁed the stopping criterion for the CG or the CGLS algorithm, because, as it
turns out, the stopping criterion for inverse problems plays an important role that
will be discussed more in detail in the following section.
9.3
Ill-Conditioning and Errors in the Data
In real applications, the discretization of a linear relation linking the unknown to be
determinedandtheobserveddatagivesrisetolinearsystemswhichcanbemoderately
to severely ill-conditioned. The condition number of the matrix representation of the
discretized forward model is usually regarded as an indicator of how severely error
corrupting the data may be ampliﬁed in the process of solving the linear system. If
the linear system is solved by a direct method, there is no way to monitor the growth
of the ampliﬁed error, while when using iterative methods the contribution of the
error components accumulates with the number of steps. In fact, often at ﬁrst the
iterates seem to converge to a meaningful solution, then, as the iterations proceed,
they begin to diverge, a phenomenon known as semi-convergence.
Example 9.1 Consider the problem of numerical differentiation. We start with a
differentiable function f : [0, 1] →R, satisfying the boundary condition f (0) = 0,
and assume that discrete noisy observations of the function values constitute our
data,
b j = f (t j) + e j,
t j = j
n ,
1 ≤j ≤n.
The problem is to estimate the derivative of f at points t j, that is, the unknowns are
given by
x j = f ′(t j),
1 ≤j ≤n.
In our computed example, we consider a function
f (t) = 1 + erf

6

t −1
2

,
(9.2)
where erf is deﬁned as
erf(t) =
2
√π
 t
0
e−s2ds,
the sigmoidal error function, and we have
f ′(t) = 12
√π e−36(t−1/2)2.

9.3 Ill-Conditioning and Errors in the Data
163
Fig. 9.1 Left: Numerical differentiation data. The noiseless function is plotted with dashed curve.
Right: Numerical derivative by naïve ﬁnite differencing. The true solution is indicated by the dashed
curve
We set n = 100, and add scaled white noise to the discretized data, with a standard
deviation 3% of the maximum of the noiseless signal. The noiseless function f and
the noisy data are shown in Fig. 9.1.
A naïve and straightforward solution would try to use ﬁnite differences, writing
x j ≈f (t j) −f (t j−1)
h
,
h = 1
n ,
and simply to substitute the noisy approximations b j ≈f (t j) into this formula. The
computed solution is rather meaningless, as shown in Fig. 9.1. It is also rather easy
to understand why: Substitution of the noisy data into the ﬁnite difference formula
gives
b j −b j−1
h
= f (t j) −f (t j−1)
h
+ e j −e j−1
h
≈f ′(t j) + n(e j −e j−1),
that is, the ﬁnite difference formula for the data produces the correct approximation
with an additive error term ampliﬁed by a factor n which completely overwhelms
the approximation.
To set the problem in linear algebraic terms, we ﬁrst observe that, by the funda-
mental theorem of calculus,
f (t j) =
 t j
0
f ′(t)dt =
j
	
k=1
 tk
tk−1
f ′(t)dt,
which we approximate as
f (t j) ≈h
j
	
k=1
xk.

164
9
Iterative Linear Solvers and Priorconditioners
Thus, the problem allows a discrete approximate representation
b = Ax + e,
where A ∈Rn×n is given by
A = h
⎡
⎢⎢⎢⎣
1
1 1
...
...
1 1 · · · 1
⎤
⎥⎥⎥⎦∈Rn×n.
Observe that the CGLS algorithm does not require that we form the matrix A explic-
itly, as long as we have an algorithm to compute its action on a vector. In this case,
the matrix-vector product has components
(Ax) j = h
j
	
k=1
xk,
a cumulative sum of the entries of x, which in Matlab can be effectuated by the
command cumsum. Similarly, the transpose of A is an upper triangular matrix of
ones, and the algorithmic deﬁnition of its action is

ATy

j = h
n
	
k= j
yk.
We implement the CGLS algorithm for this problem and verify the semi-convergence
phenomenon. Figure 9.2 shows three plots, from left to right: The norm of the dis-
crepancy, ∥b −Ax j∥as a function of the iteration count j, the norm of the error of
the approximation x j with respect to the true value,
δ j = ∥x j −x∗∥,
where x∗is the point value of f given by (9.2), and ﬁnally, the norm ∥x j∥of the
approximation. Observe that in realistic problems, we don’t have the true solution,
so the quantity δ j is not available. As expected, the discrepancy decreases monotoni-
cally, while the norm of the solution keeps increasing. The norm of the error, however,
demonstrates the semi-convergence behavior: At the beginning, the approximation
error keeps decreasing, but at some point, the approximation starts to deteriorate,
indicating that the algorithm starts to ﬁt the approximation to the noise that soon
takes over.
Finally, in Fig. 9.3 we show the ﬁrst eight iterates computed by CGLS.
The previous example indicates that when using the CGLS method to solve a
linear discrete inverse problem with a noisy right-hand side, it is important to stop

9.3 Ill-Conditioning and Errors in the Data
165
Fig. 9.2 The norm of the discrepancy as a function of the iteration, ∥b −Ax j∥(left), the norm of
the error of the approximation, ∥x j −x∗∥(center), and the norm of the approximation, ∥x j∥(right)
Fig. 9.3 The approximate solutions computed by CGLS iteration, in lexicographical order from
iteration 1 to iteration 8. After the eighth iteration, the amplitude of the noise in the reconstruction
dominates
the iteration before the ampliﬁed error begins to dominate the computed solution. To
achieve this, the CGLS algorithm should be equipped with a stopping rule preventing
ampliﬁed noise components from being included in the solution, thus making the
solution less sensitive to noise in the data. The termination of the iterations before the
reduction of the residual error has been completed is effectively a way of regularizing
the problem, known as regularization by early stopping. The design of the stopping

166
9
Iterative Linear Solvers and Priorconditioners
Fig. 9.4 The residual norm
as a function of the iteration,
and the norm of the noise
rule for an iterative solver of linear discrete inverse problems is an essential part of
the approximate solver. We outline here how this problem can be approached in a
fully deterministic way, prior to revisiting the issue from a Bayesian perspective.
Assume for a moment that the data vector b represents a noisy observation of a
vector b∗that presumably corresponds to an underlying exact solution x∗∈Rn, that
is,
b = b∗+ e = Ax∗+ e,
and that, while the additive noise is unknown, an estimate of its size is available, i.e.,
∥e∥≈η,
η > 0 known.
Thus, Ax∗is a vector that lies roughly at the distance η from the data, but in an
unknown direction, suggesting that any vector x for which ∥Ax −b∥≤η should be
as good as any other approximation. To safeguard against underestimating the norm
of the noise, the condition for acceptable solutions is often stated as
∥Ax −b∥≤τη,
(9.3)
where the parameter τ > 1 is a safeguard factor whose value is close to one.
The idea of using (9.3) to decide how much a problem should be regularized is
usually referred to as the Morozov discrepancy principle. When applied to iterative
solvers, the Morozov discrepancy principle stipulates that the iterations should be
stopped at the iteration j∗satisfying
∥Ax j∗−1 −b∥> τη ≥∥Ax j∗−b∥.
Observe that since the norms of the discrepancies for CGLS form a non-increasing
sequence, the stopping criterion is unambiguous, assuming that the required value
for the norm of the discrepancy is eventually reached.

9.4 Iterative Solvers in the Bayesian Framework
167
In Fig. 9.4, we have plotted the residual norm of the previous example and indi-
cated the norm of the additive noise. The plot indicates that Morozov’s discrepancy
principle with τ = 1 would suggest the fourth iteration to be a suitable solution,
which in the light of the semi-convergence analysis is also the best solution. Observe,
however, that in this synthetic case, we were able to evaluate the norm of the noise
vector, while in real applications, the norm can be at best a rough estimate based on
the statistical properties of the noise, as we shall see when moving to the statistical
version of the principle.
9.4
Iterative Solvers in the Bayesian Framework
The solution of a linear system by Krylov subspace iterative solvers is sought in a
subspace that depends on the data and the matrix deﬁning the system. If we want to
incorporate prior information about the solution into the subspaces, we need to mod-
ify the linear system using the prior information. How to do this in a computationally
efﬁcient manner is the topic of this section.
We start by brieﬂy reviewing some of the basic facts and results about precondi-
tioners which we will be using later. Given a linear system of equations
Ax = b,
A ∈Rn×n,
(9.4)
with A invertible, and a nonsingular matrix M ∈Rn×n, it is immediate to verify that
the linear system
M−1Ax = M−1b
(9.5)
has the same solution as (9.4). Likewise, if R ∈Rn×n is a nonsingular matrix, the
linear system
AR−1w = b,
Rx = w,
(9.6)
has also the same solution x as (9.4).
The convergence rate of an iterative method for the solution of a linear system
depends typically on the spectral properties of the matrix A. Thus, if we replace
the linear system (9.4) with (9.5) or (9.6), the rate of convergence will depend on
the spectral properties of M−1A or AR−1, respectively, instead of on the spectral
properties of A. It is therefore clear that if M or R are suitably chosen, the convergence
of an iterative method for (9.5) or (9.6), respectively, may be much faster than for
(9.4).
The matrix M (R, respectively) is called a left (right) preconditioner and the linear
system (9.5) (or (9.6)) is referred to as the left (right) preconditioned linear system.

168
9
Iterative Linear Solvers and Priorconditioners
Naturally, it is possible to combine both right and left preconditioners and consider
the system
M−1AR−1w = M−1b,
Rx = w.
In the statistical discussion to ensue, the left and right preconditioners will have dif-
ferent roles: one will be related to the likelihood, the other to the prior. Moreover, we
are less interested in the convergence properties than in the qualities of the solution,
as expressed implicitly by the prior.
9.4.1
Preconditioning and Tikhonov Regularization
In general, the closer the matrix A is to the identity, the easier it becomes for a
Krylov subspace iterative method to sufﬁciently reduce the norm of the residual
error. Thus, in the traditional linear algebraic framework, for the construction of
good preconditioners it is desirable to ﬁnd a matrix M or R such that AR−1 or M−1A
is close to the identity.
Since the application of an iterative method to a preconditioned linear system will
require the computation of matrix-vector products of the form M−1Ax or AR−1w,
followed by the solution of linear systems of the form Rx = w in the case of right
preconditioners, it is also important that these computations can be done efﬁciently.
In the general case, the question of whether we should use left or right precon-
ditioning depends only on the iterative method that we are using, and on whether
or not there are reasons to keep the right-hand side of the original linear system
unchanged. Furthermore, the best general purpose preconditioners are those which
yield the fastest rate of convergence for the least amount of work.
The situation becomes a little different when dealing with linear systems of equa-
tions with an ill-conditioned matrix and a right-hand side contaminated by noise. We
have seen in the previous sections that as the iteration number increases, ampliﬁed
noise components start to corrupt the computed solution. Ideally, a good precondi-
tioner speeds up the convergence of the iterations without speeding up the rate at
which the ampliﬁed noise takes over the solution. The sensitivity of a linear system
to noise is related to the smallest eigenvalues of the matrix, or more generally, its
smallest singular values. In an effort to accelerate the rate of convergence of the
iterative methods, while keeping the noise in the right-hand side from overtaking the
computed solution, preconditioners that enhance the convergence in the subspace
corresponding to singular vectors associated with large singular values of A have
been proposed in the literature. This separation may be difﬁcult if there is no obvi-
ous gap in the singular values of A. Furthermore, the separation should also depend
on the noise level. Moreover, ﬁnding the spectral information of the matrix may be
computationally challenging and costly.
In the Bayesian context, the main interest is in the properties of the unknown, some
of which we may have expressed in the form of a priori belief, and the preconditioner
is the Trojan horse carrying this belief into the algorithm. Thus, instead of getting

9.4 Iterative Solvers in the Bayesian Framework
169
our inspiration about the selection of preconditioners from the general theory about
iterative system solvers, we examine them in the light of linear Gaussian models that
are closely related also to Tikhonov regularization.
The design of regularizing preconditioners starts traditionally from Tikhonov
regularization, which is a bridge between statistical and deterministic solution of
inverse problems. Recall that instead of seeking a solution to the ill-conditioned
linearsystemAx = b,thestrategyinTikhonovregularizationistoreplacetheoriginal
problem by a nearby minimization problem,
xα = arg min

∥Ax −b∥2 + α∥Rx∥2
,
(9.7)
where the penalty term is selected in such a way that for the desired solution, the
norm of Rx is not excessively large, and the regularization parameter α is chosen,
e.g., by the Morozov discrepancy principle.
The simplest version of Tikhonov regularization method—sometimes, erro-
neously, called the Tikhonov regularization method, while calling (9.7) a gener-
alized Tikhonov regularization (see Notes and Comments of Chap. 7)1—is to choose
R = In, the identity matrix, leading to the minimization problem
xα = arg min

∥Ax −b∥2 + α∥x∥2
.
(9.8)
Recalling the argument in favor of early termination of iterative solvers to avoid the
norm of the solution to become excessively large, and observing that avoiding such
increase is exactly the reason for introducing the penalty term in (9.8), we conclude
that a suitable alternative to Tikhonov regularization (9.8) is to use iterative solvers
with an early stopping rule. In particular, when the CGLS method is employed, the
norms of the iterates form a non-decreasing sequence, yielding a straightforward
way of monitoring the growth of the penalty term ∥x∥.
So what could be an alternative to the more general regularization strategy (9.7)?
It is obvious that if R is a nonsingular square matrix, the regularization strategy (9.7)
is equivalent to
wα = arg min

∥AR−1w −b∥2 + α∥w∥2
,
Rxα = wα,
and the natural candidate for an alternative strategy to Tikhonov regularization would
be a iterative solver with early stopping and right preconditioner R.
Tounderstandtheeffectsofchoosingpreconditionersinthisfashion,wegobackto
the Bayesian theory, which was our starting point to arrive at Tikhonov regularization.
We consider the linear additive model
B = AX + E,
E ∼N(0, σ 2I),
X ∼N(0, Γ),
1 Andrei Nikolaevich Tikhonov (1906–1993), who had a great impact on various areas of mathe-
matics, was originally a topologist, and, loyal to his background, thought of regularization in terms
of compact embeddings.

170
9
Iterative Linear Solvers and Priorconditioners
leading to the posterior probability density
πX|B(x | b) ∝exp

−1
2σ 2 ∥b −Ax∥2 −1
2 xTΓ−1x

.
Consider the prior covariance in terms of its eigenvalues and eigenvectors. Let
Γ = WΛWT
betheeigenvaluedecompositionofthecovariancematrixΓ,whereW isanorthogonal
matrix whose columns w j ∈Rn are the eigenvectors of Γ, and the diagonal matrix
Λ = diag

λ1, λ2, . . . , λn

,
λ1 ≥λ2 ≥· · · ≥λn > 0,
contains the eigenvalues in decreasing order. After introducing the symmetric fac-
torization
Γ−1 = WΛ−1/2 Λ−1/2WT



=R
= RTR,
we can write the posterior density in the form
πX|B(x | b) ∝exp

−1
2σ 2

∥b −Ax∥2 + σ 2∥Rx∥2
,
from which it can be easily seen that the maximum a posteriori (MAP) estimate is
the Tikhonov regularized solution with α = σ 2, and
R = Λ−1/2WT.
(9.9)
This formulation lends itself naturally to a geometric interpretation of the Tikhonov
penalty. Since
Rx =
n
	
j=1
wT
j x
λ j
e j,
where e j is the jth canonical basis vector of Rn, by penalizing the solution for the
growth of those components wT
j x that are divided by small numbers λ j, the penalty
term effectively pushes the solution toward the eigenspace corresponding to largest
eigenvalues of the covariance matrix.
Example 9.2 We illustrate the iterative solver approach by considering a small
example with a forward map A ∈R2×2. The singular value decomposition of the
forward map A is assumed to be given by
A = UDVT,
(9.10)

9.4 Iterative Solvers in the Bayesian Framework
171
where
U =
 u1 u2

=
 1 0
0 1

,
V =
u1 u2

=
 cos(π/6) −sin(π/6)
sin(π/6)
cos(π/6)

,
and
D =
λ1
λ2

=
0.5
0.1

.
We choose a point x∗∈R2 and let b∗denote the noiseless data
b∗= Ax∗,
x∗=
 1
1.5

.
To visualize the mapping properties of A, in the top row of Fig. 9.5, we show a
unit disc around x∗and the singular vectors v j, and the image of the disc under the
matrix A, which is an ellipse around b∗with semiaxes parallel to u j. Conversely, if we
consider a disc around b∗, its preimage is an ellipse around x∗with semiaxes parallel
to singular vectors v j, but this time the longer semiaxis corresponds to the smaller
singular value. To better understand the effect of the different singular values, we pick
two points b1 and b2 which can be interpreted as two noisy realizations of b with the
same noise level, and map them to the x-space. We observe that, while the noise level
is the same, the effect of the noise on the retrieved x-values is different, as the mapping
A is more sensitive to noise in the u2-direction than in the u1-direction. Therefore,
ideally, an inversion algorithm should treat the singular directions differently. It turns
out that CGLS does exactly that, as the following simulation shows.
We start by generating a cloud of N = 50 perturbed data vectors around the
generative model b∗,
b( j) = b∗+ r w( j),
w( j) ∼N(0, I2),
r = 1/2.
In the ﬁrst simulation, we perform two steps of CGLS without preconditioning, thus
generating vectors

x(1)
1 , x(2)
1 , . . . , x(N)
1

(one CGLS iteration),

x(1)
2 , x(2)
2 , . . . , x(N)
2

(two CGLS iterations).
Observe that since A is invertible, CGLS converges in two steps and we have x( j)
2
=
A−1b( j). In Fig. 9.6, top row, we have plotted the original data points (left panel), and
the clouds of the ﬁrst and second iterations (middle and right panels, respectively).
To understand the result, recall that small perturbations of x in the direction v1
have the largest effect on the data, as the top row of Fig. 9.5 indicates. Therefore,
in order to minimize the discrepancy in the ﬁrst iteration step, the CGLS algorithm
selects a search direction in which the forward map is most sensitive to perturbations

172
9
Iterative Linear Solvers and Priorconditioners
Fig. 9.5 Top row: Unit disc around the point x∗with the right singular vectors (left), and the image
of it under the mapping A (right), with the left singular vectors. Bottom row: A disc of radius r = 0.5
around the point b∗with two points obtained by moving into the directions of the singular vectors
(left), and the preimage of it by the mapping A (right)
of x. Such direction is near the singular vector v1, and hence it is not a surprise that
the ﬁrst iteration cloud lies near the line from the origin (initial point of the iteration)
in the direction of the ﬁrst singular vector as shown in the top panel in the middle.
The second iteration step resolves the rest of the discrepancy, and we arrive at the
ﬁgure in the top right panel.
Next, we add a prior. Let the covariance matrix of the Gaussian prior be given in
terms of its eigenvalue decomposition,
Γ = WΛWT,
where
W =
w1 w2

=
cos θ −sin θ
sin θ
cos θ

,

9.4 Iterative Solvers in the Bayesian Framework
173
Fig. 9.6 Three runs of CGLS starting with the same data (left column). Panels in the middle column
show the results after one iterations, and those in the right column after two iterations. In the top
row, no preconditioner was included, while in the middle and in the bottom row, the preconditioner
corresponds to the prior density plotted in the ﬁgure
and Λ is a diagonal matrix of prior variances,
Λ =
 λ2
1
λ2
2

.
We choose ﬁrst the values as
θ = π
4 ,
λ1 = 1.5,
λ2 = 0.2,

174
9
Iterative Linear Solvers and Priorconditioners
meaning that we expect the variable X to have relatively large variance in the direction
w1 pointing from the origin into the positive quadrant, and relatively small variance
in the perpendicular direction w2. We run again the CGLS algorithm, this time with
preconditioning. The center row of Fig. 9.6 shows the result. The data on the left
are the same as in the previous example, and in the center and right panels, the ﬁrst
and the second iteration clouds are shown. The panels show also the contour plot
of the prior probability density. As expected, the second iterations with and without
preconditioning coincide, as the matrix A is invertible. However, the ﬁrst iterations
differ: In the presence of the preconditioner, the iteration cloud lies almost perfectly
on the main principal axis of the prior equiprobability ellipse corresponding to the
larger eigenvalue.
To further elaborate on the role of preconditioning, we swap the roles of the
prior variances, which is tantamount to rotating the prior density by an angle of
π, or, equivalently, setting θ = −π/4 in the deﬁnition of the prior. Running the
preconditioned CGLS with this choice leads to the results shown in the bottom row
of Fig. 9.6. Again, after the ﬁrst iteration, the points lie near the principal axis of the
prior equiprobability ellipse, however, the likelihood pushes the points slightly off
so that a compromise between the prior belief and the data is reached.
The previous example raises a general question concerning the compatibility of
prior and likelihood. In the second choice of preconditioning, in particular, the prior
seems to be in conﬂict with what the data are suggesting, and one could come to the
conclusion that the second prior is in some sense wrong. However, it is important to
remember what the prior and the likelihood mean. We may have good reasons, based
on previous observations or theoretical reasoning to set the prior as it was given in the
example. Furthermore, in the example, we generated the data corresponding to the
forward model that was used in the inverse problem. Such procedure is sometimes
referred to as an inverse crime. In real-world applications, the data do not come
from a model and can be in conﬂict with the model we decide to use. The mismatch
between the model and data is called model discrepancy, and it is not a trivial matter
to try to take it into account in computations.
The examples above serve as an intuitive basis for the more systematic develop-
ment of the methodology.
9.4.2
Priorconditioners: Specially Chosen Preconditioners
After the preliminary considerations with low-dimensional problems of the previous
section, we now develop further the idea of preconditioning with Krylov subspace
solvers based on Bayesian approach. We start by summarizing the basic idea as
follows.
(a) Given a linear forward model
B = AX + E,
A ∈Rm×n,

9.4 Iterative Solvers in the Bayesian Framework
175
and assuming that
X ∼N(0, Γ),
E ∼N(0, ),
where Γ ∈Rn×n and  ∈Rm×m are symmetric positive deﬁne matrices, the goal
is to approximate the MAP estimate.
(b) Using the symmetric factorization of the precision matrices,
Γ−1 = LTL,
−1 = STS,
(9.11)
write the objective functional as
E(x, b) = 1
2(b −Ax)T−1(b −Ax) + 1
2 xTΓ−1x
= 1
2

∥S(b −Ax)∥2 + ∥Lx∥2
= 1
2

∥y −SAL−1w∥2 + ∥w∥2
,
w = Lx,
y = Sb.
The minimization of this expression with respect to w is a standard form
Tikhonov regularized problem.
(c) Replace the Tikhonov regularized problem with an approximate CGLS solution
to the linear problem
y = SAL−1w,
w = Lx,
regularizing the problem with an early stopping rule.
Since the preconditioners have been constructed on the basis of a Bayesian model,
the matrices S and L are referred to as the left and the right priorconditioners,
respectively.
The approach outlined above produces a computationally efﬁcient approximation
of the MAP estimate, and we refer to this solution as the quasi-MAP (qMAP) estimate.
The functionality of the above approach will be analyzed more in detail below
and illustrated via computed examples, but before that, the stopping rule needs to be
set.
Let us start considering plain CGLS without priorconditioners to ﬁnd an approx-
imate solution to the problem
b = Ax + e,
where we assume that the noise is whitened, that is, e is a realization of Gaussian
white noise E ∼N(0, Im). The CGLS iterate x j approximating the least squares
solutions satisﬁes
x j = argmin

∥b −Ax∥| x ∈K j(ATb, ATA)

,
where the jth Krylov subspace associated with the method is

176
9
Iterative Linear Solvers and Priorconditioners
K j(ATb, ATA) = span

ATb, (ATA)ATb, . . . , (ATA) j−1ATb

.
Since K j(ATb, ATA) ⊂K j+1(ATb, ATA) for all j, the norms of the discrepancies
r j = b −Ax j form a non-increasing sequence,
∥b −Ax j∥≥∥b −Ax j+1∥for all j.
To ﬁnd a feasible stopping criterion similar to Morozov’s discrepancy principle,
observe that from the assumption about the noise,
E

∥E∥2
= E

ETE

= trace

E

E ET 
= trace(Im) = m,
that is, the expected size of the squared norm of the error in the data is of the order
m. This suggests that the iterations should be stopped as soon as the approximate
solution x j satisﬁes
∥b −Ax j∥2 < m.
Typically, the stopping condition is satisﬁed in a number of iterations much smaller
than m, making the computations much less costly than, e.g., using Tikhonov regu-
larization.
To understand the limitations of the iterative regularization, consider the orthog-
onal decomposition based on the four fundamental subspaces,
Rn = N(A) ⊕R(AT).
By the deﬁnition of the Krylov subspspaces,
K j(ATb, ATA) ⊂R(AT),
which implies that
x j ⊥N(A) for all j,
that is, the iterates x j never contain any information about the part of the solution
that is in the null space of A. This feature may have a dramatic effect on the quality
of the approximate solutions, as will be demonstrated by computed examples.
Consider now the case in which the noise and the prior covariances are not neces-
sarily identity matrices. Our main interest here is on the effect of the prior, therefore,
we assume that  = Im. If this is not the case, we can always reduce the problem to
this case by whitening, that is, by replacing
b →Sb,
A →SA,
where S ∈Rm×m is a symmetric factor of the noise precision matrix −1. Introducing
the notation A = AL−1, we deﬁne the qMAP estimate x j by

9.4 Iterative Solvers in the Bayesian Framework
177
w j = argmin

∥b −Aw∥| x ∈K j(ATb,ATA)

,
w j = Lx j,
and we stop the iterations as soon as j satisﬁes the stopping condition
∥b −Aw j∥2 < m.
By deﬁnition of the Krylov subspaces, it follows that
x j ∈L−1K j(ATb,ATA)
= span

L−1ATA
ℓAb | 0 ≤ℓ≤j −1

.
Observe that, from (9.11),
L−1AT = L−1L−TAT
=

LTL)−1AT
= ΓAT,
and inductively,
L−1ATA
ℓAT = L−1ATA
ATA
ℓ−1AT
= ΓATAL−1ATA
ℓ−1AT
=

ΓATA
ℓΓAT.
Therefore, priorconditioning by L modiﬁes the subspace of the approximate solutions
x j, so that
x j ∈span

ΓATA
ℓΓATb | 0 ≤ℓ≤j −1

.
In particular, we observe that
x j ∈Γ

R(AT)

= Γ

N(A)⊥
,
and the qMAP solution is not necessarily orthogonal to the null space, but may have
a signiﬁcant component in that space. In particular, by writing A in terms of its row
vectors as
A =
⎡
⎢⎣
aT
(1)
...
aT
(m)
⎤
⎥⎦,
a( j) ∈Rn,
we have
R(AT) = span

a(1), . . . , a(m)

,
while

178
9
Iterative Linear Solvers and Priorconditioners
Γ

R(AT)

= span

Γa(1), . . . , Γa(m)

.
Visualizing the basis vectors of the subspaces of the priorconditioned CGLS iterates
is often an efﬁcient way to understand the effect of priorconditioning.
The role of the null spaces is clearly highlighted in the next example. Before
the example, however, we point out an important detail of the priorconditioning
that is not immediately obvious. Assume that the prior covariance matrix, or its
inverse is known only up to a multiplicative constant. This could be the case, e.g.,
when smoothness priors are used, but no other information than the order of the
smoothness is available. When scaling the matrix L by a constant, L →μL, the
priorconditioned CGLS algorithm computes a solution that is scaled by that same
constant, w j →μw j. However, when we return to the original variable x j, we write
x j =

μL
−1(μw j) = L−1w j,
thatis,theeffectofthescalingvanishes!Therefore,thepriorcovariance,oritsinverse,
needs to be speciﬁed only up to a multiplicative constant, making the approach easier
to apply in practice.
Example 9.3 In this example, we consider an underdetermined one-dimensional
deconvolution problem with a Gaussian convolution kernel. We divide the unit inter-
val in n −1 intervals by n equidistant points, sℓ= ℓ/(n −1), 0 ≤ℓ≤n −1, and
approximate the convolution with the kernel a by a ﬁnite sum,
g(t) =
 1
0
a(t −s) f (s)ds ≈
1
n −1
n−1
	
ℓ=0
a(t −sℓ) f (sℓ),
where the convolution kernel is
a(t) = e−t2/(2γ 2),
γ = 0.02.
The data are assumed to consist of m noisy measurements at points t j, 1 ≤j ≤m,
b j = g(t j) + e j,
1 ≤j ≤m,
or, by denoting the unknown grid values of the function f by x j = f (s j−1), 1 ≤j ≤
n,
b = Ax + e,
A ∈Rm×n,
a jk =
1
n −1a(t j −sk−1).
We assume that the noise vector e is a realization of Gaussian scaled white noise E,
E ∼N(0, σ 2Im), and we set σ = 10−3. In our computed example, we set n = 150,
and m = 6, choosing the observation points as t j = j/(m + 1), 1 ≤j ≤m. This
way, the problem is strongly underdetermined, and the matrix A has a null space of
dimension n −m = 144. We generate the data by using a smooth function,

9.4 Iterative Solvers in the Bayesian Framework
179
f (s) = 1
2

1 −cos 3π
2 s

.
(9.12)
We demonstrate the effect of priorconditioning by choosing the prior precision matrix
to correspond to a second-order smoothness with free end points,
Γ−1 = μ
⎡
⎢⎢⎢⎢⎢⎣
α
−1 2 −1
... ... ...
−1 2 −1
α
⎤
⎥⎥⎥⎥⎥⎦
∈Rn×n,
where μ > 0 is an arbitrary scaling, and α is selected so that the marginal variances
of the endpoints equal the marginal variance of the values x j near s = 1/2.
We ﬁrst run the plain CGLS algorithm with no priorconditioning, stopping the
iterations at the discrepancy. It turns out that the norm of the discrepancy r j =
Ax j −b with whitened noise reaches the level √m in three iterations. The iterates
x j are plotted in Fig. 9.7 together with the generative model (9.12).
The estimated solution does not correspond well to the generative model mainly
because of the large null space of the forward model. Indeed, the range of the matrix
AT consists of six vectors v( j), the rows of A, given by
v( j)
k
=
1
n −1e−(t j−sk−1)2/(2γ 2),
1 ≤j ≤6,
1 ≤k ≤n,
(9.13)
which are the narrow Gaussians plotted in Fig. 9.8, and the plain CGLS reconstruction
isalinearcombinationofthesevectors,whicharenotcapableofrepresentingaslowly
varying smooth function like the generative model.
Fig. 9.7 The three ﬁrst
iterations of the CGLS
algorithm with no
priorconditioning. The third
iteration corresponds to the
solution that satisﬁes the
discrepancy criterion

180
9
Iterative Linear Solvers and Priorconditioners
Fig. 9.8 The basis vectors v( j) given by (9.13), dotted line, and the basis vectorsv ( j) corresponding
to the priorconditioned problem (9.14), solid line
Fig. 9.9 The six ﬁrst
iterations of the CGLS
algorithm with the
priorconditioned forward
map, the last iteration being
the proposed solution
We then switch to the priorconditioned CGLS. Figure 9.9 shows the iterates, the
last one satisfying the stopping criterion. We see that in order to satisfy the stopping
criterion, twice as many iterations than for the plain CGLS were needed, however,
the solution is quite close to the generative model. The solution in this case is a linear
combination of the basis vectors
v ( j) = Γv( j),
1 ≤j ≤m,
(9.14)
plotted in Fig. 9.8.

9.4 Iterative Solvers in the Bayesian Framework
181
9.4.3
Stopping Rule Revisited
We ﬁnish this section by revisiting the stopping criterion in the presence of prior-
conditioners. Recall that in the presence of a priorconditioner L, the least squares
problem
y = SAL−1w,
w = Lx,
y = Sb,
solved approximately using CGLS with early stopping is a proxy for the original
problem of minimizing the objective function
E(x, b) = ∥SAx −y∥2 + ∥Lx∥2.
When we generate the sequence of iterates
x j = L−1w j,
w j ∈K j(ATb,ATA),
it may happen that the objective function E(x j, b) stops decreasing before the stop-
ping criterion
∥Sb −SAx j∥2 < m
(9.15)
is satisﬁed. In other words, the stopping criterion following the Morozov discrepancy
principle may be optimal for the proxy problem, while an earlier stopping would be
consistent with the original problem. This consideration justiﬁes a slight modiﬁcation
of the stopping criterion, whereas the CGLS iteration is stopped as soon as either the
criterion (9.15) or
E(x j+1, b) > E(x j, b)
is met. This way, the balance between the prior and the likelihood is better respected.
Notes and Comments
The Conjugate Gradient algorithm was ﬁrst introduced in the paper of Hestenes
and Stiefel from 1952 [44], where the reason for the name of the algorithm is clearly
explained. We refer to the book [69] for a more modern and comprehensive discussion
of Krylov subspace methods.
The idea of using right preconditioners that guide the CGLS iterations to favor at
ﬁrst solutions with desirable features has been discussed in the deterministic context
in, e.g., [42]. The connection between right preconditioners and Bayesian methods,
and the term priorconditioner was ﬁrst introduced in [8, 17].
The properties of the priorconditioned CGLS iterations can be analyzed by using
Generalized Singular Value Decompositions (GSVD) introduced in [81]. We refer
to the review article [13] for further details.

Chapter 10
Hierarchical Models and Bayesian
Sparsity
Algorithms for ﬁnding sparse solutions are of importance in several application areas,
and the design of penalty functionals that promote sparsity continues to be the topic of
active research. In the Bayesian framework, searching for a sparse solution implicitly
states that there is an a priori belief that only a few components of the unknown are
nonzero. Therefore sparsity is a trait that should be described by the prior, although it
is not immediately clear how that can be done in a way that leads to a computationally
convenient posterior. Sparsity is more a qualitative than a quantitative trait, and in
general there are no set guidelines for deciding when a solution is sparse. The concept
ofsparsityisdimension-dependent:Intwodimensions,forexample,avectorwithone
vanishing component may be regarded as sparse, while 50% vanishing components in
an unknown with a million of components may not qualify it as sparse. Moreover, the
concept is complicated by the fact that sparsity is not a property of the unknown alone,
but of its representation: Sparse coding is a process of representing the unknown in
a suitable basis or frame in terms of few nonzero coefﬁcients. To elucidate the latter
statement, a discretized sinusoidal arc is not a sparse vector, however, its spectrum
consists of just one component and in the spectral basis it is indeed sparse.
In this chapter, we propose a family of conditionally Gaussian hierarchical priors
that areparticularlywell suitedtoexpress sparsitybelief, andthat incombinationwith
a Gaussian likelihood yield posteriors whose maximizers can be computed efﬁciently
even for high-dimensional problems using Krylov subspace linear solvers.
10.1
Posterior Densities with Conditionally Gaussian Priors
Consider linear inverse problems with additive Gaussian noise,
b = Ax + e,
A ∈Rm×n,
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
D. Calvetti and E. Somersalo, Bayesian Scientiﬁc Computing, Applied Mathematical
Sciences 215, https://doi.org/10.1007/978-3-031-23824-6_10
183

184
10
Hierarchical Models and Bayesian Sparsity
where the noise e ∈Rm is a realization of a Gaussian random variable E ∼N(0, Σ),
with a symmetric positive deﬁnite matrix Σ ∈Rm×m. If
Σ−1 = STS
is a symmetric factorization of the precision matrix of the noise, a multiplication of
the stochastic extension of the problem from the left by S,
SB = SAX + SE,
whitens the noise, since SE ∼N(0, Im). Therefore, without of loss of generality,
we restrict our attention to the whitened problem
B = AX + E,
E ∼N(0, Im).
We want to design a prior that favors a solution X with most of its entries zero
or close to zero. We recall the discussion about the hierarchical prior models from
Chap. 6, and develop further the ideas here.
As in the earlier discussion, we consider an uncorrelated conditionally Gaussian
prior model for X,
πX|Θ(x | θ) =
n
j=1
1
2πθ j
exp

−1
2
x2
j
θ j

=
 1
2π
n/2
exp
⎛
⎝−1
2
n

j=1
x2
j
θ j
−1
2
n

j=1
log θ j
⎞
⎠,
that depends on the vector θ of the variances of the components. If the vector θ is
not known, we model it as a random variable Θ with independent components, and
we express our a priori belief about Θ j in terms of the gamma distribution,
Θ j ∼Gamma(β, ϑ j),
or
πΘ j(θ j) =
θβ−1
j
Γ (β)ϑβ
j
e−θ j/ϑ j,
where β > 0 is the shape parameter, and ϑ j is a scale parameter. We assume that
β > 0 is the same for every component, while we allow the values of the scale
parameters to be different. This ﬂexibility with the values of the scale parameters
will turn out to be quite convenient, as we will see later. Assuming that the variances
Θ j are mutually independent, we arrive at the hyperprior model
πΘ(θ) =
1
Γ (β)n
1
(ϑ1 · · · ϑn)β exp
⎛
⎝−
n

j=1
θ j
ϑ j
+ (β −1)
n

j=1
log θ j
⎞
⎠.

10.1 Posterior Densities with Conditionally Gaussian Priors
185
Fig. 10.1 Gamma distributions (left) with two different choices of the shape parameter. In the
middle, realizations of Θ ∈R100 with independent components distributed according to the gamma
distribution, and on the right, corresponding realizations of X ∈R100 with independent components
X j ∼N(0, θ j)
The joint prior model for the pair (X, Θ) can now be written as
πX,Θ(x, θ) = πX|Θ(x | θ)πΘ(θ)
= C(β, ϑ)exp
⎛
⎝−1
2
n

j=1
x2
j
θ j
+ (β −3
2)
n

j=1
log θ j −
n

j=1
θ j
ϑ j
⎞
⎠,
where the normalizing constant is given by
C(β, ϑ) =
1
(
√
2πΓ (β))n
1
(ϑ1 · · · ϑn)β .
(10.1)
Before considering the posterior distribution, we demonstrate the properties of
the hierarchical prior model by showing random draws from it. In Fig. 10.1, the
panels in the left column show the gamma distributions with two different shape
parameter values, β = 1.1 and β = 1.5. In the middle column, we show realizations θ
of the variance vector Θ ∈R100 with independent components following the gamma
distribution, and ﬁnally, on the right, draws of the vector X ∈R100 with independent
components X j ∼N(0, θ j) are shown. The draws of Θ indicate that the relatively
fat tail of the gamma distribution allows occasional outliers of the variances, leading
to realizations of X in which small values are favored, while occasional relatively
large values are possible.

186
10
Hierarchical Models and Bayesian Sparsity
Now, we combine the prior with the likelihood density,
πB|X(b | x) ∝exp

−1
2∥b −Ax∥2

,
according to Bayes’ formula to get the posterior probability density
πX,Θ|B(x, θ | b) ∝exp
⎛
⎝−1
2∥b −Ax∥2 −1
2
n

j=1
x2
j
θ j
+ η
n

j=1
log θ j −
n

j=1
θ j
ϑ j
⎞
⎠,
where η = β −3/2. The expression of the neglected normalizing constant in terms
of the parameters β and ϑ is as in (10.1). In the following discussion, it is assumed
that β > 3/2, thus guaranteeing that η > 0.
The ﬁrst problem that we address is the following. Given the observation b ∈Rm,
ﬁnd the maximum a posteriori (MAP) estimate for the pair (X, Θ),
(x, θ)MAP = argmax πX,θ|B(x, θ | b).
In the search for a maximizer, we take advantage of the fact that a maximizer
minimizes the negative logarithm of the posterior density, or Gibbs energy functional,
E (x, θ) =
(a)



1
2∥b −Ax∥2 + 1
2
n

j=1
x2
j
θ j
−η
n

j=1
log θ j +
n

j=1
θ j
ϑ j



(b)
.
(10.2)
The structure of the Gibbs energy functional suggests a minimization strategy based
on the following sequential alternating scheme:
(a) Fix θ, and minimize the x-dependent part (a) of E (x, θ) with respect to x;
(b) Fix x to the updated value, and minimize the θ-dependent part (b) of E (x, θ)
with respect to θ.
To implement this algorithm, consider the two minimization problems separately.
Introducing a notation similar to that for conditional probability, we write the expres-
sions (a) and (b) as
Eθ(x) = 1
2∥b −Ax∥2 + 1
2
n

j=1
x2
j
θ j
,
Ex(θ) = 1
2
n

j=1
x2
j
θ j
−η
n

j=1
log θ j +
n

j=1
θ j
ϑ j
.
To minimize Eθ(x) with θ ﬁxed, observe that this is a quadratic expression in x and
can be written as

10.1 Posterior Densities with Conditionally Gaussian Priors
187
Eθ(x) = 1
2∥b −Ax∥2 + 1
2∥D−1/2
θ
x∥2 = 1
2


A
D−1/2
θ

x −
b
0

2
,
(10.3)
where Dθ is the diagonal matrix
Dθ =
⎡
⎢⎣
θ1
...
θn
⎤
⎥⎦∈Rn×n.
Therefore,solvingtheminimizationproblemamountstosolvingalinearleastsquares
problem.
The expression Ex(θ), on the other hand, is a sum of terms each depending only on
a single component θ j, therefore the minimization can be carried out independently
for each component. Writing the ﬁrst-order optimality condition with respect to each
θ j,
∂
∂θ j
Ex(θ) = −1
2
x2
j
θ2
j
−η 1
θ j
+ 1
ϑ j
= 0,
and multiplying it by θ2
j /ϑ j yields the second-order equation
 θ j
ϑ j
2
−η θ j
ϑ j
−1
2
x2
j
ϑ j
= 0,
with the unique positive solution
θ j = ϑ j
⎛
⎝η
2 +

η2
4 +
x2
j
2ϑ j
⎞
⎠= f (x j, ϑ j, η).
(10.4)
Therefore the update of θ can be done in a component-wise manner by a closed form
formula evaluation.
To put the above idea on more solid footing, we prove that the function E (x, θ)
has a global unique minimizer by proving that the function is strictly convex. This
is done by showing that the Hessian matrix is symmetric positive deﬁnite. We begin
by partitioning the Hessian of E (x, θ) as
H =
 ∇x∇xE (x, θ) ∇x∇θE (x, θ)
∇θ∇xE (x, θ) ∇θ∇θE (x, θ)

∈R2n×2n,
and compute the four blocks separately.
It follows from (10.3) that
∇x∇xE (x, θ) = ATA + D−1
θ .

188
10
Hierarchical Models and Bayesian Sparsity
Likewise, a simple computation shows that
∇θ∇xE (x, θ) = −
⎡
⎢⎣
x1/θ2
1
...
xn/θ2
n
⎤
⎥⎦,
while
∇θ∇θE (x, θ) =
⎡
⎢⎣
x2
1/θ3
1 + η/θ2
1
...
x2
n/θ3
n + η/θ2
n
⎤
⎥⎦.
To show the positive deﬁniteness of H, let
q =
 u
v

∈R2n,
and evaluate the quadratic form,
qTHq = uT
ATA + D−1
θ

u −2uT
⎡
⎢⎣
x1/θ2
1
...
xn/θ2
n
⎤
⎥⎦v
+ vT
⎡
⎢⎣
x2
1/θ3
1 + η/θ2
1
...
x2
n/θ3
n + η/θ2
n
⎤
⎥⎦v
= ∥Au∥2 +
n

j=1

u2
j
θ j
−2u jx jv j
θ2
j
+
v2
j x2
j
θ3
j
+ η
v2
j
θ j

= ∥Au∥2 +
n

j=1
1
θ j

u j −v jx j
θ j
2
+ ηv2
j

≥0,
with equality holding only if u = v = 0, that is, q = 0. Therefore, since H is positive
deﬁnite, E (x, θ) is convex. Moreover, one can check that the Gibbs energy functional
tends to inﬁnity at the limits θ j →0+ or θ →∞, as well as when ∥x∥→∞. We
therefore conclude that its minimizer exists and is unique.
It follows from the convexity of the objective function, that the following algo-
rithm converges to the unique global minimizer of the objective function E (x, θ),
or, equivalently, to the unique MAP estimate of the posterior density.
Iterative Alternating Sequential (IAS) algorithm for MAP estimation: Given
the shape and scale parameters β > 3/2, ϑ ∈Rn
+ and data b ∈Rm,
1. Initialize: Set θ = ϑ.

10.1 Posterior Densities with Conditionally Gaussian Priors
189
2. Iterate Until a convergence criterion is satisﬁed:
(a) Update x, solving the least squares problem
x = argmin


A
D−1/2
θ

x −
b
0

2
.
(b) Update θ, setting
θ j = ϑ j
⎛
⎝η
2 +

η2
4 +
x2
j
2ϑ j
⎞
⎠,
1 ≤j ≤n.
(10.5)
While the IAS algorithm itself is rather simple, the objective function being mini-
mized depends on the values of the shape and scale parameters. Moreover, a stopping
rule terminating the iterations must be speciﬁed, and in order to use it for large-scale
problems, a computationally efﬁcient solution of the least squares problem is needed.
Furthermore, the claim that the hierarchical model favors sparse solutions was rather
heuristic and requires a more rigorous justiﬁcation. The latter question is also related
to the choice of the shape and scaling parameters. We address these points in detail
in the next subsection.
10.1.1
IAS, Sparsity, Sensitivity Weighting
and Exchangeability
To provide a strategy for setting the values of the parameters of the hyperprior, we
consider their effect on the MAP estimate. To understand the role of β, consider the
objective function E (x, θ) when (x, θ) is on the manifold
Mη =

(x, θ) | θ = f (x, ϑ, η)

⊂R2n,
where f : Rn →Rn is deﬁned component-wise by the formula (10.4). Note that each
iteration round of the IAS algorithm determines an iterate that lies on this manifold.
In particular, we are interested in the behavior of the objective function as η →0+.
Therefore, consider each term in the objective function E (x, θ) restricted to Mη.
The ﬁrst term is
1
2
x2
j
θ j
= 1
2
x2
j
f (x j, ϑ j, η) = 1
2
x2
j
ϑ j

η
2 +

η2
4 +
x2
j
2ϑ j
,

190
10
Hierarchical Models and Bayesian Sparsity
which, in the limit as η →0+, converges to
lim
η→0+
1
2
x2
j
f (x j, ϑ j, η) =
1
√
2
|x j|
ϑ j
.
Similarly,
lim
η→0
θ j
ϑ j
= lim
η→0
f (x j, ϑ j, η)
ϑ j
= lim
η→0
⎛
⎝η
2 +

η2
4 +
x2
j
2ϑ j
⎞
⎠=
1
√
2
|x j|
ϑ j
,
and
lim
η→0+ η log θ j = lim
η→0+ η log f (x j, ϑ j, η) = 0.
Therefore, we conclude that
lim
η→0+ E

x, f (x, ϑ, η)

= 1
2∥b −Ax∥2 +
√
2
n

j=1
|x j|
ϑ j
,
(10.6)
which is equivalent to saying that, for small η > 0, the objective function, restricted to
the manifold Mη, approaches the mean square error term augmented with a weighted
ℓ1-penalty term. The fact that penalizing growth in the ℓ1-norm favors solutions with
small support suggests that when η is small, the hierarchical model with gamma
hyperprior favors sparse solutions. Moreover, it can be shown that the solution of
the IAS algorithm converges to the minimizer of (10.6). We omit the proof here;
references to the proof and further analysis of the algorithm can be found in Notes
and Comments at the end of the chapter. We conclude that the value of β should be
chosen to reﬂect the level of sparsity that we expect a priori.
To select the values of the scale parameters, consider ﬁrst a reduced model in
which ϑ j = ϑ0, that is, all components are equal. To understand what the parameter
ϑ0 represents, we consider the updating formula (10.5). If x j = 0, we get the lower
bound for θ j,
θ j = ηϑ j = ηϑ0,
therefore the product ηϑ0 represents the variance of the components outside the
support of x, thus if a clean background is desired, its value should be small. On the
other hand, making ϑ0 too small may have an adverse effect on the dynamic range
of the solution, as θ j represents the prior variance of x j. These considerations will
be tested with computed examples at the end of this chapter.
In the search for an automatic way of setting the value of the scale parameters
ϑ, we recall that in inverse problems, the selection of the value of the Tikhonov
regularization parameter depends on the noise level. Therefore it is not too surprising

10.1 Posterior Densities with Conditionally Gaussian Priors
191
that the noise in the data plays a role also in the Bayesian approach. In preparation
for the discussion of the role of ϑ, we start by deﬁning what we mean by the signal-
to-noise ratio (SNR) . For simplicity, we limit our discussion to a linear observation
model of the form
B = AX + E.
The SNR is deﬁned as
SN R = E

∥B∥2
E

∥E∥2,
or the ratio of the signal power and the noise power. In engineering literature, the
SNR is often given in decibels,
SN RdB = 10 log10 SN R.
To relate the SNR to the prior and likelihood models, we observe that
E

∥E∥2
=
m

j=1
E

E2
j

= trace

E

E ET
= trace(Σ).
Similarly,
E

∥B∥2
= trace

E

BBT
= trace

E

(AX + E)(AX + E)T
= trace

E

AX(AX)T
+ trace

E

E ET
= trace

AE

X XT
AT
+ trace(Σ).
To calculate the expectation above, we write it in terms of the probability densities,
E

X XT
=
  
xxTπX,Θ(x, θ)dxdθ
=
  
xxTπX|Θ(x | θ)dx

πΘ(θ)dθ
=
 
DθπΘ(θ)dθ
= βDϑ,
since the expectation of Θ j ∼Gamma(β, ϑ j) is βϑ j, which is easy to check using
the properties of gamma functions: By a change of variables, with t = θ j/ϑ j,

192
10
Hierarchical Models and Bayesian Sparsity
E

Θ j

= 1
ϑβ
j
 ∞
0
θ jθβ−1
j
e−θ j/ϑ jdθ j
=
ϑ j
Γ (β)
 ∞
0
tβe−tdt
=
ϑ j
Γ (β)Γ (β + 1)
= βϑ j.
By expressing the matrix A in terms of its columns,
A =
!a1 · · · an
"
,
a j ∈Rm,
we have
ADϑAT =
n

j=1
ϑ ja jaT
j .
Taking the trace of both sides,
trace

ADϑAT
=
n

j=1
ϑ jtrace

a jaT
j

=
n

j=1
ϑ j∥a j∥2,
and substituting this expression in the formula for the SNR, we obtain
SN R =
β #n
j=1 ϑ j∥a j∥2
trace(Σ)
+ 1.
So far, sparsity has not played any role. To introduce the sparsity, suppose, for the
moment, that we know the support S of X, deﬁned as a subset of the index set
{1, 2, . . . , n} by
j ∈S = supp(X) if and only if X j ̸= 0,
where X j ̸= 0 means that X j is not a zero random variable, although some of its
realizations may assume the value zero. If S is known, we can deﬁne the SNR
conditioned on the support of the signal as
SN RS =
β #
j∈S ϑ j∥a j∥2
trace(Σ)
+ 1.
(10.7)
According to this deﬁnition, there is no contribution to the signal from components
outside the support, which are known to vanish.
Considernowacaseinwhichthesupportisnotknown,butweknowitscardinality,
∥X∥0 = #supp(X) = number of non-vanishing components of X.

10.1 Posterior Densities with Conditionally Gaussian Priors
193
We introduce now a concept of SN R −exchangeability. We recall that in proba-
bility theory, exchangeability of a multivariate random variable means that the prob-
ability density is invariant under permutations of the coordinates. In analogy with
that, we assume that, if the cardinality of the support is known, the SNR is invariant
with respect to permutations of the coordinates, that is
SN RS = SN RS′ whenever #S = #S′.
In light of the formula (10.7), this is tantamount to requiring that

j∈S
ϑ j∥a j∥2 =

j∈S′
ϑ j∥a j∥2 whenever #S = #S′.
Therefore, a way to express our belief that any group of #S nonzero entries should
have the same possibility of yielding the given SNR is to set
ϑ j =
C
∥a j∥2 ,
1 ≤j ≤n.
To determine the value of the scalar C, observe that with #S = k, we have now
SN RS =
β #
j∈S ϑ j∥a j∥2
trace(Σ)
+ 1 =
βCk
trace(Σ) + 1 = SN Rk,
where the notation emphasizes the fact that the SNR depends only on the cardinality.
Therefore, we conclude that
ϑ j | {∥X∥0 = k} = trace(Σ)(SN R −1)
kβ∥a j∥2
,
thatis,iftheSNRandthecardinalityofthesupportaregiven,thescalingiscompletely
determined. Since we don’t know the cardinality, we should treat it as a random
variable and assign some reasonable prior probabilities to it. If we deﬁne
pk = P

∥X∥0 = k

,
n

k=1
pk = 1,
the marginal of the scale parameters are
ϑ j =
n

k=1
pkϑ j | {∥X∥0 = k} = trace(Σ)(SN R −1)
β∥a j∥2
n

k=1
1
k pk.
(10.8)
Hence, we have found a reasonable expression for the scaling variables. To give a
further interpretation to the formula, let us write (10.8) as

194
10
Hierarchical Models and Bayesian Sparsity
ϑ j =
α2
∥a j∥2 ,
α = constant,
and substitute the expression in the formula (10.6), to get
E

x, f (x, ϑ, 0+)

= 1
2∥b −Ax∥2 +
√
2
α
n

j=1
∥a j∥|x j|.
This suggests that in the penalty term, the components should be weighted by the
norms of the columns of the forward map A. This type of weighting has been used for
a long time in geophysics and in biomedical imaging, and it is known as sensitivity
weighting. Indeed, in classical signal analysis, the sensitivity of a forward map x →
f (x) to the jth component of x around the value x = x0 is deﬁned as
s j(x0) =

∂f
∂x j
(x0)
 .
In the case of a linear forward map, the sensitivity is the same at every point x0, and
is given by
s j = ∥a j∥.
The heuristic justiﬁcation for sensitivity weighting is that to avoid favoring compo-
nents to which the data are highly sensitive, the penalty should be proportional to
the sensitivity. The above argument therefore provides a Bayesian justiﬁcation for
sensitivity weighting in addition to giving a criterion for setting the values of the
scale parameters.
10.1.2
IAS with Priorconditioned CGLS
We now turn to the question of how to terminate the iterations. A popular stopping
criterion for iterative schemes is to terminate the iteration when the relative change
in two consecutive solutions falls below a certain level. In the IAS algorithm, we use
this criterion for the hyperparameter θ.
Before incorporating the stopping criterion in the IAS algorithm, consider the
updating step of the variable x. The solution of the least squares problem can be
approximated in a computationally efﬁcient way by using one of the iterative linear
solverspresentedinthepreviouschapter,namely,thepriorconditionedCGLSmethod
equipped with a suitable termination rule. We now augment the IAS algorithm with
these features.

10.1 Posterior Densities with Conditionally Gaussian Priors
195
Iterative Alternating Sequential (IAS) algorithm with priorconditioned
CGLS: Given the shape parameter β > 3/2, an estimate of the SNR, stopping tol-
erance τ > 0 and maximum number of iterations tmax, and data b ∈Rm,
1. Initialize: Assign the values of the scale parameters ϑ according to (10.8), then
set θ = ϑ, t = 0, Δθ = ∞.
2. Iterate: While Δθ > τ and t < tmax,
(a) Update x using priorconditioned CGLS: Set Aθ = AD1/2
θ , and compute with
CGLS iterates
wℓ= argmin{∥Aθw −b∥| x ∈Kℓ(Aθb, AT
θ Aθ)}.
Terminate the CGLS iteration at the step k, where k is the ﬁrst index such
that either
∥Aθwk+1 −b∥2 < m
or
∥b −Aθwk+1∥2 + ∥wk+1∥2 > ∥b −Aθwk∥2 + ∥wk∥2.
(10.9)
Set x = D1/2
θ wk.
(b) Set θold = θ.
(c) Update θ, setting
θ j = ϑ j
⎛
⎝η
2 +

η2
4 +
x2
j
2ϑ j
⎞
⎠,
1 ≤j ≤n.
(d) Update
Δθ = ∥θ −θold∥
∥θold∥
,
and advance the counter, t →t + 1.
Observe that as discussed at the end of the previous chapter, we augmented the
stopping criterion of the inner CGLS iteration based on the Morozov discrepancy
principle by the condition (10.9) based on the rationale that the CGLS iteration, in
fact, seeks to minimize the quadratic expression


A
D−1/2
θ

x −
b
0

2
= ∥b −Aθw∥2 + ∥w∥2,
w = D−1/2
θ
x,
so it is natural to continue the iterations only as long as this quantity keeps decreasing.

196
10
Hierarchical Models and Bayesian Sparsity
10.2
More General Sparse Representations
So far, we have assumed that the unknown vector x ∈Rn itself is a realization of a
random variable that is believed to be sparse, or, at least close to sparse. In general,
however, we may require that instead of X, a derived quantity,
Y = LX,
L ∈Rk×n,
is the sparse signal to be estimated. If k = n and the matrix L is invertible, the inverse
problem can be stated in terms of the new variable, but in general, this is not the case.
Forexample,considerapixelimage,modeledasaquadrilateralmeshwherethenodes
represent the pixel values. Adjacent pixels in the vertical and horizontal directions
are connected by edges. For simplicity, we assume here that the pixel values at the
boundary nodes vanish, while in the interior nodes the values need to be estimated
from some indirect noisy data. If the number of interior nodes is nv, and the number
of edges between the nodes is ne, we deﬁne the increments over the edges as the
difference of the nodal values at the endpoints of the edges. The vector of increments
y is related to the vector x of interior nodal values through the equation
y = Lx,
L ∈Rne×nv,
the matrix L being sparse and containing at most a pair (+1, −1) of nonzero values
in each row. Considering only the pixel values at the interior nodes, we see that the
null space of L is trivial,
N(L) = {0}.
(10.10)
Indeed, if all increments vanish so that Lx = 0, connecting any interior node to a
boundary node by a chain of edges shows that the value at that node must vanish,
and therefore, we may conclude that x = 0.
To characterize the range of L, consider the circulation around any elementary loop
Tk in the network, see Fig. 10.2. For consistency, the signed sum of the increments
Fig. 10.2 The compatibility
condition expresses the fact
that the circulation around
each closed loop in the
network must vanish. At the
boundary, since the function
is zero at boundary nodes,
the loop comprises only the
free edges, that is, edges with
at least one end node that is
not a boundary node

10.2 More General Sparse Representations
197
over the edges forming the loop must vanish. Therefore, if nt is the number of
elementary loops, there is a matrix M ∈Rnt×ne whose elements are zeros and ±1s,
such that
My = 0.
(10.11)
It is not difﬁcult to deduce that condition (10.11) holds exactly for increment vectors
corresponding to some nodal values y = Lx, that is,
R(L) = N(M).
(10.12)
The conditions (10.10) and (10.12) can be stated by saying that L and M form a short
exact chain, and expressed formally as
{0} −→Rnv
L
−→Rne
M
−→Rnt −→{0}.
Condition (10.11) is a compatibility condition that the vector y needs to satisfy.
Assume now that we believe, a priori, that the image has a sparse representation,
in the sense that there are only a few nonzero increments, or at least, few incre-
ments above some small threshold value. We write the corresponding conditionally
Gaussian prior for the random variable Y as
πY|Θ(y | θ) ∝exp
⎛
⎝−1
2
ne

j=1
y2
j
θ j
−1
2
ne

j=1
log θ j
⎞
⎠.
However, since the data, hence the likelihood, is expressed in terms of X, we would
like to express the prior in terms of X rather than Y. Formally, the dimensionality
of Y is much higher than that of X, however, the compatibility condition (10.11)
forces Y into a lower dimensional subspace, the null space of M, which is of the
same dimension as X. The problem that arises is how to restrict the prior density to
the subspace N(M). Fortunately, numerical linear algebra comes to our rescue here.
Rather than building the connection through the matrix M, we show how to ﬁnd a
computationally feasible strategy to modify the problem while solving it.
We start by introducing the auxiliary variable
z = D−1/2
θ
y,
Dθ =
⎡
⎢⎣
θ1
...
θne
⎤
⎥⎦
and express the compatibility condition y ∈N(M) = R(L) as
z ∈R(Lθ),
Lθ = D−1/2
θ
L.
(10.13)

198
10
Hierarchical Models and Bayesian Sparsity
The matrix Lθ has more rows than columns, therefore its QR-decomposition is of
the form
Lθ = QR =
!Q1 Q2
" R1
O

,
where the matrix R1 ∈Rnv×nv is upper triangular with nonzero diagonal entries,
because from (10.10) it follows that L and Lθ have rank nv. Condition (10.13) guar-
antees that there is x ∈Rnv such that
z = Lθx = QRx.
Multiplying both sides by QT, and using the fact that QTQ = I, it follows that
QTz =
QT
1 z
QT
2 z

= Rx =
R1x
0

.
(10.14)
Therefore, any vector z in the range of Lθ satisﬁes the compatibility condition
QT
2 z = 0,
or
z ∈H = N(QT
2).
We include this condition as part of the prior for the random variable Z = D−1/2
θ
Y,
conditioned on Θ = θ by setting
πZ|Θ(z | θ) ∝exp
⎛
⎝−1
2
ne

j=1
z2
j −1
2
ne

j=1
log θ j
⎞
⎠δH (z),
where δH (z) is a density that accumulates all the probability mass on the subspace
H . Moreover, we can use (10.14) to write the likelihood in terms of z as
πB|Z(b | z) ∝exp

−1
2∥b −AR−1
1 QT
1 z∥2

.
According to Bayes’ formula, the posterior density for the pair (Z, Θ) is
πZ,Θ|B(z, θ)
∝exp
⎛
⎝−1
2∥b −AR−1
1 QT
1 z∥2 −1
2
ne

j=1
z2
j −1
2
ne

j=1
log θ j
⎞
⎠δH (z)πΘ(θ),
and the MAP estimate can be computed with the IAS algorithm, where the updating
of z is done by solving the least squares problem
z = argmin


AR−1
1 QT
1
Ine

z −
 b
0

2
.

10.2 More General Sparse Representations
199
The last point that needs to be addressed is how to ensure that the IAS algorithm
enforces the compatibility condition z ∈H . Fortunately, the compatibility condition
is automatically obeyed by the IAS iterates. To verify it, write
z = z1 + z2,
z1 ∈H ,
z2 ∈H ⊥,
and observe that from
QT
1 z2 = 0,
it follows that


AR−1
1 QT
1
Ine

z −
b
0

2
= ∥b −AR−1
1 QT
1 z∥2 + ∥z∥2
= ∥b −AR−1
1 QT
1 z1∥2 + ∥z1∥2 + ∥z2∥2.
Therefore the least squares solution always satisﬁes z2 = 0, because any nonzero
component of z2 would make the sum of the squares larger.
We conclude by pointing out that since
L†
θ = R−1
1 QT
1,
we may organize the steps of the IAS algorithm as follows.
IAS algorithm with priorconditioned CGLS, generalized sparsity condition:
Given the shape parameter β > 3/2, an estimate of the SNR, stopping tolerance
τ > 0, maximum number of iterations tmax, and data b ∈Rm,
1. Initialize: Computethescaleparametersϑ by(10.8).Setθ = ϑ,t = 0,Δθ = ∞.
2. Iterate: While Δθ > τ and t < tmax,
(a) Updatex usingpriorconditionedCGLS:SetAθ = AL†
θ,with Lθ = D−1
θ L,and
compute the CGLS iterates
zℓ= argmin{∥Aθz −b∥| x ∈Kℓ(Aθb, AT
θ Aθ)}.
Terminate the CGLS iteration at the step k, where k is the ﬁrst index such that
either
∥Aθzk −b∥2 < m
or
∥b −Aθzk+1∥2 + ∥zk+1∥2 > ∥b −Aθzk∥2 + ∥zk∥2.
Set x = L†
θzk.
(b) Set θold = θ.

200
10
Hierarchical Models and Bayesian Sparsity
(c) Given the updated z, update θ, evaluating
θ j = ϑ j
⎛
⎝η
2 +

η2
4 +
z2
j
2ϑ j
⎞
⎠,
1 ≤j ≤ne.
(d) Update
Δθ = ∥θ −θold∥
∥θold∥
,
and advance the counter, t →t + 1.
Before presenting some computed examples, a comment on the implementation
of the CGLS in this setting is in order. The action of the matrix AL†
θ on a given vector
u can be computed in two steps: First, solve in the least squares sense the problem
Lθv = u,
then multiply v by A. To compute the product of a vector with the transpose of AL†
θ,
recall that

AL†
θ
T = Lθ

LT
θ Lθ
−1AT.
To take advantage of the sparsity of the matrix LT
θ Lθ, ﬁrst solve the linear system

LT
θ Lθ

w = ATv,
for w, then multiply w by Lθ to get
Lθw = Lθ

LT
θ Lθ
−1ATv =

AL†
θ
Tv.
We close this chapter with computed examples demonstrating the viability of the
method.
10.3
Some Examples
The following three examples elucidate the different aspects of the IAS algorithm.
We start with a rather elementary one-dimensional inverse problem and then move
to two-dimensional problems where the sensitivity and dimensionality of the data
play a role.
Example 10.1 In this example, we consider the problem of estimating a function
u : [0, 1] →R from noisy observations of a convolution of it,

10.3 Some Examples
201
g(t) =
 1
0
a(t −s)u(s)ds + noise,
where a is a given convolution kernel. We discretize the problem as follows: Divide
the interval [0, 1] into n subintervals of length h = 1/n by discretization points
s j = j
n ,
0 ≤j ≤n,
and write an approximation
 1
0
a(t −s)u(s)ds ≈h
n

j=1
a(t −s j)u(s j) = h
n

j=1
a(t −s j)z j.
The data are assumed to be observed at some of the discretization points tk,
bk = g(tk) = h
n

j=1
a(tk −s j)z j + ek,
or, in matrix form,
b = Az + e,
where we assume that e is a realization of a Gaussian random variable. We assume
that while the signal z itself is not sparse, the vector x of its increments is, where
x j = z j −z j−1 = jth increment of z.
Hence, we write the sparsity promoting prior for the increment vector. To this end,
we need to express the likelihood, too, in terms of the increments. Assuming that
z0 = 0, we have the telescoping sum representation,
zk = zk −zk−1 + (zk−1 −zk−2) + . . . + (z1 −z0) =
k

j=1
x j,
(10.15)
which allows to write the matrix transformations between x and z as
x = Lz,
L =
⎡
⎢⎢⎢⎣
1
−1
1
... ...
−1
1
⎤
⎥⎥⎥⎦,
and

202
10
Hierarchical Models and Bayesian Sparsity
z = L−1x,
L−1 =
⎡
⎢⎢⎢⎣
1
1
1
...
...
1
1 . . .
1
⎤
⎥⎥⎥⎦,
The form of the inverse matrix can be deduced from formula (10.15). In terms of x,
the forward model is
b = AL−1x + e = ALx + e,
Lz = x,
where
AL = AL−1.
We can now write the Gibbs energy for the increment vector using the hierarchical
prior, assuming that the Gaussian noise has been whitened,
E (x, θ) = 1
2∥b −ALx∥2 + 1
2
n

j=1
x2
j
θ j
−η
n

j=1
log θ j +
n

j=1
θ j
ϑ j
.
To generate the data, the unit interval is divided into 128 equal subintervals, and we
deﬁne the generative model as a piecewise constant function with ﬁve discontinuities,
Thus,∥x∥0 = 5.Thekernela ischosentobeaGaussian,suchthatafterdiscretization,
a jk = 0.05 e−|s j−tk|2/2λ2,
λ = 0.02.
To demonstrate the effect of the parameters, we start by choosing the shape and
scaling parameters as
η = β −3
2 = 0.1,
ϑ j = ϑ0 = 0.1.
The stopping tolerance for the outer iterations is set at τ = 10−3, and in the inner
iteration, the least squares problem for updating x is solved by using the standard
system solver (mldivide, or “backslash”), and the algorithm converged in 60 outer
iterations. Figure 10.3, ﬁrst row, shows the outcome. We observe that η is not small
enough to promote the sparsity of the increment, and the estimated z is more like
a smooth function than a piecewise constant one as we hope. We observe that the
dynamical range of the estimate, controlled mostly by the parameter ϑ0, corresponds
well to that of the generative model. To improve the quality of the estimate, we
decrease signiﬁcantly the parameter η that controls the sparsity, choosing
η = β −3
2 = 10−3,
ϑ j = ϑ0 = 0.1.

10.3 Some Examples
203
Fig. 10.3 Results of the IAS algorithm, with parameter values from top to bottom as follows:
(η, ϑ0) = (0.1, 0.1) (top row), (η, ϑ0) = (10−3, 0.1) (middle row), and (η, ϑ0) = (10−3, 10−2)
(bottom row). On the left, the generative model is plotted as a dashed red curve. On the right, the
value ϑ0 is indicated by the horizontal red line. The results corroborate the theoretical argument
that η controls the sparsity, and ϑ0 the dynamical range, and together with η, the cleanliness of the
estimate outside the singularities

204
10
Hierarchical Models and Bayesian Sparsity
The results are shown in the second row of Fig. 10.3. This time, the solution is close
to a piecewise constant function, and θ is in general small, with a few signiﬁcant
outliers at and around the points of discontinuities that match well with the ones of
the generative model. The iterations converged in 165 outer iteration rounds.
Finally, to test the effect of the scaling parameter, we run the algorithm with
parameter values
η = β −3
2 = 10−3,
ϑ j = ϑ0 = 10−2.
The IAS algorithm converged after 153 outer iterations, and the results are shown
in the third row of Fig. 10.3. We observe two things: First, the size of the estimated
θ is signiﬁcantly smaller, due to the decreased value of ϑ0, which in turn translates
into an insufﬁcient dynamical range of the vector z. Moreover, since the product ϑ0η
controls the size of θ j at points outside the support of the increment vector, as this
product is smaller by a factor of ten, the estimate is ﬂatter between the discontinuities
corresponding to the ones of the generative model.
The following example demonstrates the use of the sensitivity scaling and the
CGLS in the IAS algorithm.
Example 10.2 We consider a linear inverse source problem in two dimensions, hav-
ing certain similarities with the inverse problems of electroencephalography (EEG)
and magnetoencephalography (MEG). In our example, consider the unit disc Ω in
R2 containing a distribution ρ(r), r ∈Ω of sources that generate a ﬁeld b(r′) outside
Ω, such that
b(r′) =
 
Ω
ρ(r)
|r −r′|2 dr,
r′ ∈R2 \ Ω.
We assume that the ﬁeld b is measured at points r′
j, 1 ≤j ≤m. To set up the discrete
forward model, we approximate the integral by a quadrature,
 
Ω
ρ(r)
|r −r′|2 dr ≈
n

j=1
w j
ρ(r j)
|r j −r′|2 ,
wherew jsarethequadratureweightsandr jsarethecorrespondingquadraturenodes.
We denote the unknowns by q j = w jρ(r j), and write the observation model as
bk =
n

j=1
q j
|r j −r′
k|2 + ek,
1 ≤k ≤m,
where ek is the additive noise. The discretization and the measurement geometry
are shown on the left of Fig. 10.4. On the right, the generative model with three-
point sources is shown, together with the data, contaminated by additive Gaussian
independent error with standard deviation σ = 0.05, corresponding to 1.1% of the
maximum amplitude of the data, or 2.5% of the mean amplitude.

10.3 Some Examples
205
Fig. 10.4 Left: Quadrature points used to discretize the forward model (n = 1 301, the green
squares indicating the measurement points (m = 20). Right: The generative model with three-point
sources in Ω (shaded region), and visualization of the data. Positive sources are marked by red and
negative by blue. The same color coding applies to the data
Fig. 10.5 Computed values of ϑ j using the formula (10.8), linearly interpolated between the nodal
values. The lowest sensitivity at the center of the disc leads to the highest value of ϑ j
To run the IAS algorithm, we initialize the process by computing the scaling
parameter ϑ using the sensitivity argument and formula (10.8). We assume a priori
that the source consists of at most 10 active sources, with no preference for any
cardinality below that limit, thus setting
pk = 1
10 for 1 ≤k ≤10, and pk = 0 for k > 10,
implying that
n

k=1
1
k pk = 1
10
10

k=1
1
k ≈0.293.

206
10
Hierarchical Models and Bayesian Sparsity
Fig. 10.6 The result of the IAS algorithm using the SNR-exchangeability-based sensitivity scaling
(top row) and the corresponding result obtained by setting ϑ j = ϑ0 = constant. In the former case,
the algorithm converged in 101 outer iterations, while the latter case required 38 iterations. The
positions of the sources of the generative model are indicated by red dots in the upper row
Wevisualizethedistributionofthevaluesofϑ j byplottingapiecewiselinearsurface
over Ω interpolating between the values ϑ j at the nodal points r j, shown in Fig. 10.5.
Asexpected,thesensitivityinthecenterofthediscislowest,leadingtothelargestvalue
of ϑ j, and at nodal points closest to the observation points r′
k it is the highest.
We run the IAS algorithm using the priorconditioned CGLS updating strategy,
setting the stopping criterion for the outer iteration at τ = 10−3. The stopping crite-
rion was reached after 108 iterations. Figure 10.6 shows the result in the top row. For
comparison, we run the algorithm without the sensitivity weighting, setting ϑ j = ϑ0,
a constant value throughout. The constant was chosen so that it corresponds to ϑ j at
the center of the disc when sensitivity is taken into account, ϑ ≈1.38. The results in
the bottom row of Fig. 10.6 show a complete lack of depth resolution: The algorithm
explains the data by adjusting the source at the discretization points nearest to the
receivers, having the highest sensitivity.
Finally, we point out that the dynamic range of the solution does not correspond
to the source values of the generative models, which is due to the fact that the

10.3 Some Examples
207
reconstruction distributes the source to several source locations as well as to the fact
that the quadrature weights are lumped with the source density. For an analysis of
the source strengths, an integral of the sources should be computed to compare with
the generative values.
The third example demonstrates how the IAS algorithm works when the map-
ping from the primary unknown and its sparse representation is not one-to-one. We
consider a classical X-ray tomography problem with limited projection data.
Example 10.3 Let Ω ⊂R2 denote the object that is illuminated by X-ray sources.
The goal is to estimate the density distribution inside Ω from the transmission attenu-
ation of the X-rays through it. Denoting by ρ(r) the density, we assume that the X-ray
attenuation of the initial intensity I0 is described by the Beer–Lambert law stating
that the attenuation of the intensity between the source at rsource and the receiver at
rreceiver along a line ℓof length L and parametrized by the arclength,
ℓ: r(s) = (1 −s/L)rsource + s/L rreceiver,
0 ≤s ≤L,
is given by
I = I0 exp

−
 L
0
ρ

r(s)

ds

.
Assuming that the object is illuminated from different directions, the attenuation
being measured over m lines ℓ(k), 1 ≤k ≤m the noiseless data can be described by
the linear model
bk =
 Lk
0
ρ

r(k)(s)

ds,
1 ≤k ≤m,
(10.16)
wherer(k)(s) is the parametrization of the kth ray, and Lk is its length. In this example,
we assume that Ω is a unit disc, and for discretization, it is approximated by a polygon
divided in small, approximately equilateral triangles. Denoting by r j the jth vertex
in the triangular mesh, we discretize ρ for the forward problem by denoting
x j = ρ(r j),
and write
ρ(r) =
n

j=1
x jψ j(r),
where ψ j is a piecewise linear Lagrange basis function such that
ψ j(rℓ) = 1, if j = ℓ, and 0 otherwise.
Then,

208
10
Hierarchical Models and Bayesian Sparsity
Fig. 10.7 The X-ray tomography arrangement with three views (left), and the target used to generate
the test data. Each view angle corresponds to 400 lines of integration across the target
 Lk
0
ρ

r(k)(s)

ds ≈
n

j=1
x j
 Lk
0
ψ j

r(k)(s)

ds =
n

j=1
akjx j.
Taking into account the observation error and modeling errors, we arrive at the linear
model
b = Ax + ε,
A ∈Rm×n.
We will model the error ε as Gaussian noise, which is more a choice of convenience
than a result of analysis of the sources of error.
In the computed example, we assume that the target Ω is illuminated from few
directions by a fan beam source, and the attenuation is measured on the opposite side
of the object along a line segment, see Fig. 10.7. Moreover, the prior belief is that
the target density is a piecewise constant function, implying that the increments over
the edges joining the nodes in the triangular mesh comprise a sparse vector. Thus, if
ne is the number of edges, and the edges are deﬁned in terms of the end point nodes,
eℓ= {riℓ,r jℓ}, we deﬁne a matrix L ∈Rne×n such that

Lx

ℓ= xiℓ−x jℓ.
The edges outnumber the nodes as Euler’s formula for planar graphs implies, so the
matrix L has more rows than columns and cannot be invertible. Therefore, we need
to apply the ideas outlined in Sect. 10.2.
We start by generating the data, considering two data sets; in one we use ﬁve
illumination angles, and in the other, 15 illumination angles are used, in both cases,
uniformly distributed around the circle. Each illumination angle corresponds to 400
lines of integration over the opening angle with a full view of Ω as indicated in the
ﬁgure. The generative model used for the data generation is shown in Fig. 10.7. We
add Gaussian independent and equally distributed noise to the noiseless data, with

10.3 Some Examples
209
Fig. 10.8 Top row: estimate of the density based on 15 uniformly distributed projections. The
estimate on the left is computed by the CGLS algorithm with early stopping at the discrepancy,
and the one on the right with the IAS iteration. Bottom row: The data consist of ﬁve projection
directions
standard deviation corresponding to 0.1% of the maximum attenuation over all lines.
The area Ω is divided into triangles with a diameter of the order 0.02, computed
with a standard mesh generator (see Notes and Comments) yielding a tessellation of
9062 nodes, 26874 edges, and 17813 triangles.
ToinitializetheIASalgorithm,wesetϑ j = ϑ0,thatis,wedonotusethesensitivity
scaling, and choose the constant ϑ0 = 0.1, large enough to allow the variances θ j
to allow jumps of the order of magnitude ∼1. To keep the background clean, we
choose η so that the product ηϑ0 is close to zero, setting η = 10−5. This choice is also
favoring a sparse solution. The maximum allowed number of outer IAS iterations is
limited to 50, with the tolerance τ = 10−2.
Consider ﬁrst the case with more complete data, assuming 15 projection direc-
tions. In this case, the dimension of the data is m = 15 × 400 = 6 000. The requested
tolerance τ is reached in 21 IAS iteration rounds. It is of interest to see how many
inner iterations of the priorconditioned CGLS are required . The numbers are shown
in Fig. 10.9, indicating the efﬁciency of the algorithm: The ﬁrst few outer iterations
require more work, and as the algorithm proceeds, only a few inner iterations are
needed. In Fig. 10.8, the left column shows the reconstruction using plain CGLS

210
10
Hierarchical Models and Bayesian Sparsity
Fig. 10.9 Number of priorconditioned CGLS iterations in each outer iteration of the IAS algorithm.
The left panel corresponds to 15 projections, and the right to 5 projections
algorithm with an early stopping at the discrepancy as a regularization, and on the
right, the outcome of the IAS algorithm. The former results contain the characteristic
streak artifacts typical for reconstructions from a small number of projections, while
in the IAS reconstruction, the background as well as the constant density objects are
free of artifacts. For comparison, we run the same algorithms with only 5 projections.
The corresponding results are shown in the second row of Fig. 10.8, and the numbers
of inner iterations are shown in the right panel of Fig. 10.9.
Notes and Comments
The IAS algorithm as presented here has been developed and discussed in a series
of articles [9, 11, 12, 19, 22], with a particular application to functional brain imag-
ing by magnetoencephalography. The hypermodel for the variance Θ need not be
a gamma distribution, and in fact, hypermodels from a wider class of generalized
gamma distributions were considered, however, the convexity of the objective func-
tion guaranteeing uniqueness is no longer warranted. The ideas of using non-convex
objective functions are developed further in the references [14, 15, 61].
The triangulation of the computational domain in the example 9.3 was done by
using the mesh generator DistMesh, described in detail in the article [60].

Chapter 11
Sampling: The Real Thing
In Chap. 4, where we ﬁrst introduced random sampling, we pointed out that sampling
is used primarily to explore a given probability distribution and to calculate estimates
of integrals via Monte Carlo integration. It was also indicated that sampling from a
non-Gaussian probability density may be a challenging task. In this section we further
develop the topic and introduce Markov chain Monte Carlo (MCMC) sampling.
11.1
Preliminaries: Markov Chains and Random Walks
In this section, we introduce the basic concepts of random walks, transition matrices
and kernels, Markov chains, and their connections to sampling. We start with a
discrete model example.
11.1.1
An Introductory Example
As an introduction to sampling by using Markov chains, we start with a motivational
example in discrete state space. Let S be a ﬁnite state space,
S = {1, 2, 3, 4, 5},
and let X be a random variable with possible values in S, that is, X :  →S. We
denote the probabilities of different values by
p j = P

X = j

,
1 ≤j ≤5,
5

j=1
p j = 1,
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
D. Calvetti and E. Somersalo, Bayesian Scientiﬁc Computing, Applied Mathematical
Sciences 215, https://doi.org/10.1007/978-3-031-23824-6_11
211

212
11
Sampling: The Real Thing
and introduce the corresponding probability vector
πX =
⎡
⎢⎣
p1
...
p5
⎤
⎥⎦.
In Sect. 4.4, we constructed an algorithm for generating independently drawn real-
izations from a discrete probability distribution. With that algorithm, we can compute
a sample of size T of independently drawn realizations,
S =

x1, x2, . . . , xT 
,
xt ∈S,
with the property that asymptotically, as T increases, the relative frequencies of
possible output values converge to the probability of the value, that is,
νT
j = #{xt = j}
T
→p j.
When the state space is small as in the present example, the independent sampling
approach is effective and easy to implement, but as seen in Chap. 4, when the state
space dimension increases, alternative ways are needed. In particular, we are inter-
ested in a sequential sampling approach, in which the next realization xt+1 can be
computed from the current realization xt alone.
In order to deﬁne a sequential algorithm, let xt denote the current realization,
interpreted as the position of a random walker at time t in the state space, and the
next realization xt+1 is obtained by choosing a transition xt →xt+1 in a stochastic
manner. To that end, we deﬁne a transition matrix P ∈R5×5 with entries
p jk = probability of moving from node k to node j.
In particular, P is deﬁned so that the matrix entries are all non-negative, and the sum
of the entries in each column is one. To give a concrete example of a transition matrix,
consider the directed network in Fig. 11.1, where the nodes correspond to the state
space S and the directed links correspond to possible transitions. In this example, we
assume that if at time t the random walker is in node k, a move to another node j
is allowed if and only if there is a directed link k →j. Furthermore, for the sake of
simplicity, we postulate that all possible moves from node k have equal probability.
In the case of the network of Fig. 11.1, this leads to a transition matrix given by
P =
⎡
⎢⎢⎢⎢⎣
0 1 0 1/3 0
1/2 0 0 1/3 0
0 0 0
0 1/2
1/2 0 1/2 0 1/2
0 0 1/2 1/3 0
⎤
⎥⎥⎥⎥⎦
,
(11.1)

11.1 Preliminaries: Markov Chains and Random Walks
213
Fig. 11.1 A simple directed
network with ﬁve nodes. The
outlinks from node 4 are
shown in red, and the moves
out of node 4 have all the
same probability 1/3
where the probabilities of the moves from the current position are listed in the cor-
responding column of the transition matrix. Having the transition matrix P, we now
deﬁne a random walk over the state space as follows:
1. Initialize by choosing one node, e.g., x1 = 1. Set t = 1, and deﬁne T as the
maximum number of steps.
2. While t < T , repeat:
(a) Draw a node ℓ, 1 ≤ℓ≤5 using probabilities in the column xt of the matrix
P.
(b) Set xt+1 = ℓ, and advance the counter t →t + 1.
The process above generates a random sample {x1, x2, . . . , xT } in a sequential
manner, however, in order for the sample to indeed represent realizations of the
random variable X that we are interested in, the matrix P must be carefully tailored.
To see how the generated sample values are distributed when P is given, and under
which conditions this distribution coincides with that of X, we introduce some tools
to analyze the sample, using the matrix P of (11.1) as an example.
We deﬁne a sequence of random variables Xt, t = 1, 2, . . ., with values in the
state space S such that Xt+1 indicates the position of the random walk after t steps.
If we start the random walk from node 1, the probability vector of X2, whose entries
are the probabilities of possible outcomes of the move, is given by the ﬁrst column
of P, that is,
πX2|X1=1 = Pe1 =
⎡
⎢⎢⎢⎢⎣
0
1/2
0
1/2
0
⎤
⎥⎥⎥⎥⎦
.
(11.2)
Observe that in this case the vector e1 represents the probability vector of X1 when
we know for sure that the walk starts from node 1. What is the probability vector of
X3, given that the walk started at node 1? Since by (11.2), X2 has two equiprobable
realizations, x2 = 2 and x2 = 4, we conclude that

214
11
Sampling: The Real Thing
πX3|X1=1 = 1
2Pe2 + 1
2Pe4 = P
⎡
⎢⎢⎢⎢⎣
0
1/2
0
1/2
0
⎤
⎥⎥⎥⎥⎦
= P2e1.
Inductively, the probability vector after t steps is
πXt+1|X1=1 = PπXt|X1=1 = P2πXt−1|X1=1 = · · · = Pte1.
So far, we assumed that the random walk started from node 1. More generally, we
may ask what is the probability vector of Xt+1 if the initial state is chosen randomly.
If we assign equal probability 1/5 to every initial state, then the marginal probability
vector is
πXt+1 =
5

j=1
πXt+1|X1= jP{X1 = j} = Ptu,
u = 1
5
⎡
⎢⎢⎢⎢⎣
1
1
1
1
1
⎤
⎥⎥⎥⎥⎦
.
(11.3)
Here, u is the probability vector of the uniform distribution.
A remarkable property of the stochastic process outlined above is that it forgets
the past in the following sense: Suppose that you know the state Xt, that is, Xt = xt
for some xt ∈{1, 2, 3, 4, 5}. The probability vector of the next state, regardless of
how one arrived at the current node, is always
πXt+1|Xt=xt = Pe j,
xt = j for some j,
that is, if the path to Xt = xt is X1 = x1, X2 = x2, …, Xt−1 = xt−1, we have
πXt+1|X1=x1,...,Xt=xt = πXt+1|Xt=xt.
(11.4)
We say that a discrete time stochastic process

X1, X2, . . .

is a Markov process if it
has the property (11.4). This condition is often expressed by saying that “tomorrow
depends on the past only through today.”
We have now enough tools to analyze the relation between the original ques-
tion of generating a sample from a given distribution and generating a sample by
using a transition matrix. Assume that a transition matrix P, not necessarily the one
of our example, is given, deﬁning a Markov chain {X1, X2, . . .}. The probability
distributions of consecutive random variables Xt and Xt+1 are related through
πXt+1 = PπXt.

11.1 Preliminaries: Markov Chains and Random Walks
215
Fig. 11.2 The visiting
histogram with different
values of T. The horizontal
black lines indicate the sizes
of the components of the
eigenvector of the transition
matrix P corresponding to
the eigenvalue λ = 1,
normalized so that the sum
of the entries is one
To guarantee that the resulting sample represents draws from some distribution with
probability vector πX, we require that every Xt is distributed according to πX, which
implies, in particular, that
πX = PπX.
(11.5)
This is equivalent to saying that πX is an eigenvector of the transition matrix P
with corresponding eigenvalue λ = 1. The above condition is often expressed by
saying that πX is an invariant probability vector of the transition matrix P. Let us
explore empirically what the probability vector πX would look like for the matrix
(11.1). Given P, we generate a sample {x1, x2, . . . , xT } and compute the relative
visiting frequencies of each node. In Fig. 11.2, the relative frequencies of the visits in
different nodes with sample sizes T = 500, T = 5 000 and T = 15 000, respectively,
are plotted as histograms. On the other hand, it turns out that the matrix P has indeed
λ = 1 as an eigenvalue, and that the corresponding eigenvector can be scaled so that
its components are all positive and sum up to one, hence can be interpreted as a
probability vector. The sizes of the components of this eigenvector are also indicated
in Fig. 11.2, conﬁrming that the sampling-based relative frequencies indeed converge
toward these values.
We close the discussion of this example with some comments about the underlying
theory. It can be shown that a matrix with non-negative entries and column sums equal
to one has λ = 1 as an eigenvalue. Moreover, if the entries are strictly positive, the
Frobenius–Perron Theorem (see Notes and Comments for reference) ensures that
λ = 1 is the eigenvalue of largest modulus with geometric multiplicity one, and that
a corresponding eigenvector can be scaled so that it is a probability vector. Finally,
one can show that regardless of the initial state, the probability vectors
πXt+1 = PtπX1

216
11
Sampling: The Real Thing
converge to this unique eigenvector, which is the core idea of the power method to
compute the eigenvector. Therefore, regardless of what the random draws of xt are
at the beginning, asymptotically their probability vectors are closer and closer to the
limit vector. This is the core idea of Markov chain Monte Carlo (MCMC) sampling.
In the discussion above, we assumed that the transition matrix P was given,
and found the associated invariant probability vector πX as an eigenvector of P.
Conversely, suppose that πX is known instead and that we want to generate a sample
distributed according to πX. The problem therefore is how to ﬁnd a transition matrix
P such that condition (11.5) is satisﬁed. Answering this question is at the core of the
MCMC method and will be discussed next in the general setting of continuous state
space.
11.1.2
Random Walks in Rn
Since, in general, we are interested in problems in which the random variables take on
values in Rn rather than in a ﬁnite state space as in the previous example, we are now
going to set the stage for Markov chain Monte Carlo with non-discrete state space.
We start by deﬁning the concept of random walk in Rn which, as the name suggests,
is a process of moving around by taking random steps. The most elementary random
walk can be deﬁned as follows:
1. Start at an arbitrary point x0 ∈Rn.
2. Draw a random vector w1 ∼N(0, In) and set x1 = x0 + σw1.
3. Repeat the process: Set xk+1 = xk + σwk+1, wk+1 ∼N(0, In).
Using the random variables notation, the location of the random walk at time k is
a realization of the random variable Xk, and we have an evolution model
Xk+1 = Xk + σW k+1,
W k+1 ∼N(0, In).
The conditional density of Xk+1, given Xk = xk, is
πXk+1|Xk(xk+1 | xk) =
1
(2πσ2)n/2 exp

−1
2σ2 ∥xk −xk+1∥2

= qk(xk, xk+1).
The function qk : Rn × Rn →R+ is called the transition kernel and it is the con-
tinuous equivalent of the transition matrix P in the discrete state space example. To
establish this correspondence, consider the joint probability density of Xk and Xk+1,
given by
πXk,Xk+1(xk, xk+1) = πXk+1|Xk(xk+1 | xk)πXk(xk)
= qk(xk, xk+1)πXk(xk).

11.1 Preliminaries: Markov Chains and Random Walks
217
By marginalizing the variable xk, it follows that
πXk+1(xk+1) =

qk(xk, xk+1)πXk(xk)dxk,
that is, the transition kernel deﬁnes a linear mapping P that propagates the probability
densities,
P : πXk →πXk+1,
through the integral equation above. Since
qk(x, y) = q(x, y) =
1
(2πσ2)n/2 exp

−1
2σ2 ∥x −y∥2

for all k = 0, 1, 2, . . .,
i.e., the step is always equally distributed independently of the value of k, the kernel
q is called time invariant.
The process above deﬁnes a chain

Xk, k = 0, 1, · · · } of random variables, each
with its own probability. The chain is a discrete time stochastic process with values
in Rn and has the particular feature that the probability distribution of each variable
Xk+1 depends on the past only through the previous element Xk of the chain. This
can be expressed in terms of conditional densities as
πXk+1|X0,...,Xk(xk+1 | x0, x1, . . . , xk) = πXk+1|Xk(xk+1 | xk).
(11.6)
The condition (11.6) is the continuous version of the discrete condition (11.4), and it
is referred to as the Markov property. As in the discrete case of the previous example,
a stochastic process with this property is called a Markov chain.
Example 11.1 To understand the role of the transition kernel, consider a Markov
chain deﬁned by a random walk model in R2,
Xk+1 = Xk + W k+1,
W k+1 ∼N(0, C),
(11.7)
where C ∈R2×2 is a symmetric positive deﬁnite matrix, with eigenvalue decompo-
sition
C = UDUT,
(11.8)
where U ∈R2×2 is an orthogonal matrix, and D ∈R2×2 is diagonal with positive
diagonal entries. The inverse of C can be decomposed as
C−1 = UD−1UT =

UD−1/2 
D−1/2UT



=L
= LTL,
and the transition kernel can be written as

218
11
Sampling: The Real Thing
q(xk | xk+1) = πXk+1|Xk(xk+1 | xk) ∝exp

−1
2∥L(xk −xk+1)∥2

.
Alternatively, we may write the random walk model (11.7) as
Xk+1 = Xk + L−1W k+1,
W k+1 ∼N(0, I2),
(11.9)
where the random step is whitened.
To illustrate the role of the covariance matrix, let
U =
cos θ −sin θ
sin θ
cos θ

,
θ = π
6 ,
and
D = diag(s2
1, s2
2),
s1 = 0.5, s2 = 0.1.
In the random walk model (11.9), the standard deviation in the direction of the ﬁrst
eigenvector u1 is ﬁve time the standard deviation in the orthogonal direction of u2,
hence we assume that the random steps have a component about ﬁve times larger in
the direction of the ﬁrst eigenvector u1 than along the second eigenvector u2, where
u1 =
 cos θ
sin θ

,
u2 =
−sin θ
cos θ

.
The left panel of Fig. 11.3 shows a random walk realization with the covariance
matrix C = σ2I2 starting from the origin of R2 and choosing the step size σ = 0.1. In
the right panel, the covariance matrix is chosen as above. Obviously, by judiciously
choosing the transition kernel, we may guide the random walk quite effectively.
Fig. 11.3 Realizations of random walks. Left: The covariance matrix of the random step is σ2I2,
with the standard deviation σ = 0.1, and the number of steps is N = 1000. Right: The covariance
is C given in (11.8). The eigenvectors of the covariance matrix are indicated in the plot, scaled
proportionally to the respective standard deviations

11.2 Metropolis–Hastings Algorithm
219
Consider now an arbitrary transition kernel q : Rn × Rn →R+, normalized so
that

q(x, y)dy = 1.
Assume that X is a random variable with known probability density πX(x) = p(x).
Suppose that we generate a new random variable Y by using the kernel q(x, y), that
is,
πY|X(y | x) = q(x, y).
The probability density of this new variable Y can be found by marginalization,
πY(y) =

πY|X(y | x)πX(x)dx =

q(x, y)p(x)dx.
If the probability density of Y is equal to the probability density of X, i.e.,

q(x, y)p(x)dx = p(y),
(11.10)
we say that p is an invariant density of the transition kernel q. The classical problem
in the theory of Markov chains can then be stated as follows: Given a transition
kernel q, ﬁnd the corresponding invariant density p that satisﬁes Eq. (11.10).
When using Markov chains to sample from a given density, we are actually consid-
ering the inverse problem: Given a probability density p = p(x), generate a sample
that is distributed according to it. If we had a transition kernel q with invariant density
p, generating such sample would be easy: starting from x0, draw x1 from q(x0, x1)
considered as a probability density with respect to x1 and x0 ﬁxed, and continue
inductively. In general, given xk, draw xk+1 from q(xk, xk+1). After a while, the
xk’s generated in this manner are distributed more and more according to p. This
was the strategy for generating the sample in the discrete state space problem of
Subsect. 11.1.1.
Therefore the problem we are facing now is: Given a probability density p, ﬁnd
a kernel q such that p is its invariant density.
Probably the most popular technique for constructing such transition kernel is the
Metropolis–Hastings algorithm.
11.2
Metropolis–Hastings Algorithm
We are now ready to derive, starting from the concept of invariant densities, one of the
bread-and-butter Markov chain Monte Carlo (MCMC) algorithms, the Metropolis–
Hastings (MH) algorithm.

220
11
Sampling: The Real Thing
11.2.1
Balance and Detailed Balance Equations
Assume that a probability density p : Rn →R+ is given and that the goal is to ﬁnd
a transition kernel such that p is its invariant probability density. To deﬁne a Markov
chain, consider the following algorithm: Starting from the current point x ∈Rn,
either
1. Stay put at x with probability r(x), 0 ≤r(x) < 1, or
2. Move away from x using a kernel K(x, y) ≥0, x, y ∈Rn.
The above alternatives can be summarized by deﬁning a transition kernel consisting
of a point mass and a distributed part,
q(x, y) = r(x)δx(y) + K(x, y).
In order for q to deﬁne a transition kernel, it must satisfy

q(x, y)dy = r(x)

δx(y)dy +

K(x, y)dy
= r(x) +

K(x, y)dy = 1,
implying that
1 −r(x) =

K(x, y)dy.
(11.11)
If the conditional density of the random variable Y, given X = x, is deﬁned by using
this transition kernel, we have, for any set B ⊂Rn,
P{Y ∈B | X = x} =

B
πY|X(y | x)dy
=

B
q(x, y)dy
= r(x)

B
δx(y)dy +

B
K(x, y)dy.
We observe that the ﬁrst integral equals one if and only if x ∈B, otherwise the
integral vanishes. Therefore,

B
πY|X(y | x)dy = r(x)χB(x) +

B
K(x, y)dy,
(11.12)
where χB is the characteristic function (or indicator function) of the set B. Multi-
plying the left side of the identity (11.12) by p(x), integrating over Rn and recalling
that

11.2 Metropolis–Hastings Algorithm
221
πY(y) =

πY|X(y | x)πX(x)dx =

πY|X(y | x)p(x)dx,
the left hand side yields

p(x)

B
πY|X(y | x)dy =

B

πY|X(y | x)p(x)dx

dy
=

B
πY(y)dy.
(11.13)
On the other hand, the right hand side gives

p(x)

r(x)χB(x) +

B
K(x, y)dy

dx
(11.14)
=

B

p(y)r(y) +

p(x)K(x, y)dx

dy.
Since the identity of the right hand sides (11.13) and (11.14) holds for every B, we
conclude that
πY(y) = p(y)r(y) +

p(x)K(x, y)dx.
Since our goal is to ﬁnd a transition scheme for which p is an invariant probability
density, we substitute πY(y) = p(y) in the expression above to get the necessary and
sufﬁcient condition

1 −r(y)

p(y) =

p(x)K(x, y)dx.
Substituting the right-hand side of (11.11) with the roles of x and y interchanged in
the left-hand side of the above identity, we get
p(y)

K(y, x)dx =

p(y)K(y, x)dx =

p(x)K(x, y)dx.
(11.15)
This equation, known as the balance equation, is the necessary and sufﬁcient con-
dition for p to be the invariant density for the proposed transition scheme. This
condition is satisﬁed, in particular, if the integrands are equal,
p(y)K(y, x) = p(x)K(x, y),
(11.16)
yielding a stronger, sufﬁcient condition to guarantee the balance condition, known
as the detailed balance equation. The Metropolis–Hastings algorithm provides a
technique for ﬁnding a kernel K that satisﬁes (11.16).

222
11
Sampling: The Real Thing
11.2.2
Construction of the MH Transition
The Metropolis–Hastings algorithm starts by selecting a proposal distribution, or
candidate generating kernel R(x, y) ≥0, usually chosen so that generating a Markov
chain with it is easy. It is mainly for this reason that Gaussian kernels are a very
popular choice. Assume that R is a transition kernel, that is,

R(x, y)dy = 1.
If R satisﬁes the detailed balance equation,
p(y)R(y, x) = p(x)R(x, y),
we let r(x) = 0, hence q(x, y) = K(x, y) = R(x, y), and we are done, since the
previous analysis shows that p is an invariant density for this kernel. If, as is more
likely to happen, the detailed balance equation is not satisﬁed, the left-hand side is
either larger or smaller than the right-hand side. Assume, for the sake of deﬁniteness,
that
p(y)R(y, x) < p(x)R(x, y).
(11.17)
To enforce the detailed balance equation we modify the kernel K to
K(x, y) = α(x, y)R(x, y),
where the correcting factor α is chosen so that
p(y)α(y, x)R(y, x) = p(x)α(x, y)R(x, y).
(11.18)
Since the kernel α need not be symmetric, we can choose
α(y, x) = 1,
and let the other correcting factor be determined from (11.18):
α(x, y) = p(y)R(y, x)
p(x)R(x, y) < 1.
(11.19)
Observe that if the direction of inequality (11.17) is reversed, we simply interchange
the roles of x and y, letting instead α(x, y) = 1. In summary, we deﬁne K as
K(x, y) = α(x, y)R(x, y),
α(x, y) = min

1, p(y)R(y, x)
p(x)R(x, y)

.

11.2 Metropolis–Hastings Algorithm
223
This expression for K looks rather complicated, and it may seem that generating
random draws should be all but simple. However, drawing from this kernel can be
performed easily in two phases, as in the case of rejection sampling, according to
the following algorithm.
Metropolis–Hastings algorithm: Given a proposal transition kernel R(x, y), the
probability density p, and the target size N of the sample:
1. Initialize: Choose x0, set the counter k = 1.
2. Iterate: While k < N,
(a) Given xk−1, draw y using the transition kernel R(xk−1, y).
(b) Calculate the acceptance ratio,
α(xk−1, y) =
p(y)R(y, xk−1)
p(xk−1)R(xk−1, y).
(c) Flip the α–coin: draw t ∼Uniform([0, 1]); if α > t, accept y, and set
xk = y, otherwise stay put, setting xk = xk−1.
(d) Advance the counter k →k + 1.
Observe that rejection of the proposal leads to acceptance of the old point in the
sample. Therefore, typically the sample contains multiple copies of some points. It is
important to not discard the repetitions, as they reﬂect the importance of such points
in representing the density. Alternatively, one can think that the repetitions are a way
to give a larger importance weight to the sample point.
A careful reader will notice that the derivation given above is not a complete proof
of the validity of the Metropolis–Hastings algorithm, because when the next point
y was generated from the previous one, we tacitly assumed that the previous one,
x, was sampled from the underlying p. We are omitting the proof that the generated
distribution indeed converges toward p, because it requires more advanced technical
tools that have not been introduced in this discussion. For a reference to the complete
proof, see Notes and Comments.
Before presenting some computed examples, a few remarks are in order. Often,
the proposal distribution R is chosen to be symmetric,
R(x, y) = R(y, x).
(11.20)
If this is the case, the acceptance ratio α simpliﬁes to
α(x, y) = p(y)R(y, x)
p(x)R(x, y) = p(y)
p(x),
(11.21)
that is, α compares only the values of the density at the two points. Intuitively, the
idea is that the move is always accepted if the proposed point is more probable than
the old one, otherwise the move is accepted with a certain probability. This also

224
11
Sampling: The Real Thing
explains the role of repeated sample points: a multiply repeated point is one with
high probability, as it is hard to move away from it, and consequently, it deserves to
have a larger weight.
11.2.3
Metropolis–Hastings in Action
We will now highlight some convenient features of the algorithm while following it
into action, starting with an example.
Example 11.2 Consider the probability density in R2,
p(x) ∝exp

−1
2σ2 ((x2
1 + x2
2)1/2 −1)2 −1
2δ2 (x2 −1)2

,
(11.22)
where
σ = 0.1,
δ = 1,
whose equiprobability curves are shown in Fig. 11.4.
We explore this density with a random walk sampler. For that purpose, consider
the scaled white noise random walk proposal,
R(x, y) =
1

2πγ2 exp

−1
2γ2 ∥x −y∥2

.
Since the transition kernel is symmetric, satisfying condition (11.20), the acceptance
ratio simpliﬁes as in (11.21).
We start the Markov chain from the origin, x0 = (0, 0), and to illustrate how
it progresses, we adopt the plotting convention that each new accepted point is
Fig. 11.4 The
equiprobability curves of the
original density (11.22)

11.2 Metropolis–Hastings Algorithm
225
represented as a dot. If a proposal is rejected and we remain in the current posi-
tion, the size of the dot increases, so that the area of the dot is proportional to the
number of rejections. A Matlab code that generates the sample can be written as
follows. For later use, we keep also track of how often a move is accepted.
nsample = 500;
% Size of the sample
Sample = zeros(2,nsample);
count = 1;
x = [0;0];
% Initial point
Sample(:,1) = x;
lprop_old = logpdf(x);
% logpdf = log of the prob.density
acc_rate = 0;
while count < nsample
% draw candidate
y = x + step*randn(2,1);
% check for acceptance
lprop =
logpdf(y);
if lprop - lprop_old > log(rand);
% accept
acc_rate = acc_rate + 1;
x = y;
lprop_old = lprop;
end
count = count+1;
Sample(:,count+1) = x;
end
We remark that in actual computations, the acceptance ratio is calculated in log-
arithmic form: we accept the move x →y if
log p(y) −log p(x) > log t,
t ∼Uniform([0, 1]).
The reason for proceeding in this manner is to avoid numerical problems due to
underﬂow in the computation of the ratio of p(y) and p(x). In fact, it may happen
that the numbers p(y) and p(x) end up being smaller than the computing precision,
leading to an ill-deﬁned ratio, while the logarithmic form avoids such problems.
In our ﬁrst exploration of the density, we decide to move rather conservatively
and take very small steps by setting γ = 0.02. A plot of the ﬁrst 500 points generated
by the MH algorithm is shown in Fig. 11.5. The ﬁgure also indicates how often the
proposal was accepted.
It is clear from the plot that, after 500 draws, the sampler has not even started to
explore the density. In fact, almost the entire sample has been used to move from the
initial point to the numerical support of the density. This initial tail, which has nothing
to do with the actual probability density, is usually referred to as the burn-in of the

226
11
Sampling: The Real Thing
Fig. 11.5 The
Metropolis–Hastings sample
with the step size γ = 0.02,
and 93% of the proposals are
accepted. The computed
sample mean is marked by
the cross hair
sample. It is normal procedure in MCMC sampling methods to discard the beginning
of the sample to avoid that the burn-in affects the estimates that are subsequently
calculated from the sample. In general, it is not easy to decide a priori how many
points should be discarded.
The second observation is that the acceptance rate is rather high: approximately
nine out of every ten proposed moves are accepted. A high acceptance rate usually
indicates that the chain is moving too conservatively, and in that case longer steps
should be used to get better coverage of the distribution.
Motivated by the observed high acceptance rate, we increase the step by a factor
of hundred, choosing γ = 2. The results of this modiﬁcation can be seen in Fig. 11.6.
Now the acceptance rate is only 4%, meaning that most of the time the chain does not
move and more than 90% of the proposals are rejected. Notice that the big dots in the
ﬁgure indicate points from which the chain does not want to move away. In this case
the burn-in effect is practically absent, and the estimated conditional mean is much
closer to what one could expect. The result seems to suggest that low acceptance rate
is better than too high.
By playing with the steplength, we may be able to tune the proposal distribution
so as to achieve an acceptance rate between 20% and 30%, which is often advocated
as optimal.
Figure 11.7 shows the results obtained with γ = 0.5, yielding an acceptance rate
of approximately 24%. We see that the points are rather well distributed over the
support of the probability density. The estimated mean, however, is not centered,
probably because that the size of the sample is too small.
The previous example shows that the choice of the proposal distribution has an
effect on the quality of the sample thus generated. While in the two-dimensional case
it is fairly easy to assess the quality of the sampling strategy simply by looking at the
scatter plot of the sample, in higher dimensions this approach becomes impossible,
and more systematic means to analyze the sample are needed. There is the deﬁnitive

11.2 Metropolis–Hastings Algorithm
227
Fig. 11.6 The
Metropolis–Hastings sample
with the step size γ = 2. The
computed sample mean is
marked by the cross hair
Fig. 11.7 The
Metropolis–Hastings sample
with the step size γ = 0.5
way to measure the quality of a sample, and we only touch on this rather complex
topic here.
The Central Limit Theorem suggests a way to measure the quality of the sample.
According to the Central Limit Theorem, the asymptotic convergence rate of a sum
of N independently sampled, identically distributed random variables is 1/
√
N.
While the goal of Markov chain Monte Carlo sampling is to produce a sample that is
asymptotically drawn from the limit distribution, focusing mostly on the identically
distributed aspect, the independence of the computed sample is more problematic.
Clearly, since the sample is a realization of a Markov chain, complete independence
of the sample points cannot be expected: every draw depends on at least the previous
elementinthechain.ThisdependencyhasrepercussionsontheconvergenceofMonte
Carlo integrals. Suppose that, on the average, only every kth sample point can be
considered independent. Then, by the asymptotic law of the Central Limit Theorem,
we may expect a convergence rate of the order of √k/N, a rate that is painfully slow
if k is large. Therefore, when designing Metropolis–Hastings strategies, we should

228
11
Sampling: The Real Thing
0
1
2
3
4
5
6
7
8
9
10
x 10
4
−5
0
5
Fig. 11.8 A realization of a Gaussian white noise signal of length 100 000
aim at choosing the step length in the proposal distribution so that the correlation
length is small.
Let us begin with a visual inspection of the samples. Suppose that we have a
sampling problem in n spatial dimensions. While we cannot inspect a scatter plot
when n is large, we may always look at the sample histories of individual compo-
nents, plotting the individual components as a function of the sample index. The ﬁrst
question is then what a good sample history looks like.
A typical example of a sample with completely uncorrelated elements is a white
noise signal: at each discrete time instant, the sample is drawn independently. A
Gaussian white noise signal of length n is, in fact, a realization of a Gaussian mul-
tivariate vector with covariance In. Figure 11.8 shows a realization of a white noise
signal. It looks like a “fuzzy worm.” This gives a visual description to an MCMCer
of what a good sample history should look like. This visual description, although
rather vague, is in fact, quite useful to quickly assess the quality of a sample.
A more quantitative measure for assessing the quality of the sample can be
obtained by looking at its correlation structure. The autocorrelation of a signal is a
useful tool to analyze the independency of realizations. Let z j, 1 ≤j ≤N denote a
discrete time ﬁnite segment of a real-valued signal. Assume, for simplicity, that the
signal has zero mean. After augmenting the signal with trailing zeros to an inﬁnite
signal, 1 ≤j < ∞, consider the discrete convolution,
hk =
∞

j=1
z j+kz j,
k = 0, 1, . . .
When k = 0, the above formula returns the total energy of the signal,
h0 =
N

j=1
z2
j = ∥z∥2.

11.2 Metropolis–Hastings Algorithm
229
If z is a white noise signal and k > 0, the random positive and negative contribu-
tions cancel out, and hk ≈0. This observation gives a natural tool to analyze the
independency of the components in a sample: plot
k →hk =
1
∥z∥2 hk,
k = 0, 1, . . .
and estimate the correlation length from the rate of decay of this sequence. The
quantity hk is the autocorrelation of z with lag k.
The following example demonstrates how to use this idea.
Example 11.3 Consider the horseshoe distribution of the previous example, and the
Metropolis–Hastings algorithm with a scaled white noise proposal distribution. We
consider three different step sizes: γ = 0.1, γ = 1, and γ = 5. We generate a sample

x1, x2, . . . , x N
of size N = 50 000, where, for a good measure, we have discarded
the 500 ﬁrst sample points as they may not represent the distribution. We calculate
the mean,
x = 1
N
N

j=1
x j,
and the lagged autocorrelations of the centered components,
hi,k =
1
∥z∥2
N−k

j=1
z j+kz j,
z j = (x j −x)i,
i = 1, 2.
Figure 11.9 shows the sample history of both components x j
1 and x j
2 with the
different step sizes. Visually, the most reasonable step size γ = 1 produces a sample
history that is similar to the white noise sample, while with the smaller step size
there is a visible low frequency component that tells us about the slow walk around
the density. The larger step size, on the other hand, causes the sampler to stay put for
long periods of times with an adverse effect on the independency of the samples. The
computed autocorrelation functions agree with the visual assessment, the step size
γ = 1 producing the most rapidly decreasing autocorrelation function. Based on the
autocorrelation function, one can conclude that roughly every 50th sample point can
be safely considered to represent an independent sample from the distribution.
We close this section with a computed example of a parameter estimation problem
and its solution using the Metropolis–Hastings algorithm.
Example 11.4 Consider a system of chemical reactions involving substances A, B,
C, and D,
A
k1⇌
k2 B
k3⇌
k4 C + D,
where k j, 1 ≤j ≤4 are the reaction rates. Assuming that the reactions satisfy the
mass action model, the concentrations u1 = [A], u2 = [B], u3 = [C] and u4 = [D]

230
11
Sampling: The Real Thing
Fig. 11.9 Sample histories of the components x1 (odd rows) and x2 (even rows) with different
proposal step parameters γ, and the corresponding autocorrelation functions. The red horizontal
lines indicate the estimated posterior means

11.2 Metropolis–Hastings Algorithm
231
of the substances satisfy the system of differential equations
du1
dt
= −k1u1 + k2u2,
du2
dt
=
k1u1 −k2u2 −k3u2 + k4u3u4,
du3
dt
=
k3u2 −k4u3u4,
du3
dt
=
k3u2 −k4u3u4.
The problem considered here is to estimate the reaction rates k j from noisy observa-
tions of the concentrations, given that the initial concentrations are known. Denoting
by k ∈R4 the vector containing the reaction rates and by u(t) ∈R4 the vector of the
concentrations at time t, the data are deﬁned as
b j = u(t j) + e j,
1 ≤j ≤m,
u(0) = u0 given,
where 0 < t1 < . . . < tm. For simplicity, we assume that e j is a realization of a
random variable E j ∼N(0, σ2I4), and the variables E j are mutually independent.
A priori we only assume that the reaction rates are positive. To write a proper
prior, however, we need to impose an upper bound constraint, k j < κ j for some
large value κ j, leading to a prior model
πK(k) ∝
4

j=1
χ[0,κ j](k j),
which is the product of the characteristic functions of the respective intervals.
We generate the data with the above generative model, using parameter values
k∗
1 = 0.2,
k∗
2 = 0.5,
k∗
3 = 5,
k∗
4 = 15,
and initial values
u0,1 = 0.2,
u0,2 = 1,
u0,3 = 0.2,
u0,4 = 0.05.
The data are collected at times
t j = 0.01 + ( j −1)0.03,
1 ≤j ≤17,
and to each computed concentration, normally distributed error with standard devi-
ation σ = 0.01 is added. The data are shown in Fig. 11.10.
The likelihood model in this case is given by

232
11
Sampling: The Real Thing
Fig. 11.10 The solid curves
correspond to the noiseless
concentrations, and the data
are obtained by adding
normal error of standard
deviation σ = 0.01 to the
sampled values at times t j
πB|K(b | k) ∝exp
⎛
⎝−1
2σ2
17

j=1
∥b j −u(t j; k)∥2
⎞
⎠;
for practical computations, u(t j) can be computed with any standard ODE solver
such as ode45 in Matlab.
To estimate the parameters, we run the standard Metropolis–Hastings algorithm
with Gaussian random walk proposal, step size γ = 0.1, starting the chain at k0 =
[1; 1; 10; 10], and we generate a chain of length N = 200 000. The acceptance rate
with this step size turns out to be 5.8%. Reducing the step size to γ = 0.05 leads
to an acceptance rate of 17.2%. Based on visual inspection of the time traces, we
discard 500 sample points from the beginning of the chain.
In Fig. 11.11 the results are presented graphically as a scatter matrix, showing
the histograms of each component in the diagonal panels, and pairwise scatter plots
in the off-diagonal panels.
The scatter plots of the components k1 versus k2, and k3 versus k4, reveal a strong
correlation between them, indicating that the data contain information about the ratios
of the corresponding rate constants. We point out that the ratios k1/k2 and k3/k4 are
related to the equilibrium conditions that the system asymptotically approaches,
indicating that the concentrations at large times convey information about those
ratios. In our example, the system was still relatively far from the equilibrium.
11.3
Gibbs Sampler
Another classical MCMC algorithm is Gibbs sampler. The intuitive idea behind
the algorithm is rather simple, and it carries a certain similarity with the classical

11.3 Gibbs Sampler
233
Fig. 11.11 A scatter matrix representation of the Metropolis–Hastings run with proposal step size
γ = 0.1. In the panels along the diagonal, the histograms of each component is shown, with the
sample mean marked by a solid black line. For comparison, the generative value is marked by a
dashed red line. The off-diagonal panels show the pairwise scatter plots of the components. The
results with the proposal steps size γ = 0.05 are essentially similar, the sample means coinciding
up to the third decimal
Gauss–Seidel algorithm for solving linear systems: The random draws are done
component-wise, updating one coordinate at the time.
Before presenting the details of the algorithm, we establish the notation. Given a
probability density in Rn, p(x) = p(x1, x2, . . . , xn), we use the expression
p(x j | x1, . . . , x j−1, x j+1, . . . , xn)
to denote the density of the component x j conditioned on the other components,
obtained by
1. Freezing all components xℓ, except for ℓ= j;
2. Normalizing the resulting density to have integral with respect to x j equal to
one.
More precisely, denoting by p(x j) the marginal density,

234
11
Sampling: The Real Thing
p(x j) =

Rn−1 p(x1, . . . , xn)dx1 · · · dx j−1dx j+1 · · · dxn,
we deﬁne
p(x j | x1, . . . , x j−1, x j+1, . . . , xn) = p(x1, . . . , xn)
p(x j)
,
or, equivalently,
p(x) = p(x j | x1, . . . , x j−1, x j+1, . . . , xn)p(x j).
Unlike in the Metropolis–Hastings algorithm where the sampling was done by using
a proposal distribution, in Gibbs sampling we draw directly from the target density
p(x), x ∈Rn. Consequently, there is no need to deﬁne an acceptance probability, as
every proposal will be accepted.
Formally, we deﬁne a transition kernel
q(x, y) =
n
i=1
p(yi | y1, . . . , yi−1, xi+1, . . . , xm),
(11.23)
which leads to the conceptually simple updating algorithm presented below. This
transition kernel does not, in general, satisfy the detailed balance equation (11.16),
however, it satisﬁes the weaker balance equation,

Rn p(y)q(y, x)dx =

Rn p(x)q(x, y)dx,
(11.24)
which has been shown to be the necessary and sufﬁcient condition for p to be an
invariant distribution of the transition rule deﬁned by the kernel q. The proof is
straightforward but tedious. In order to avoid that the discussion becomes overly
technical, we present it for the case n = 2, with the understanding that similar ideas
apply to the general case.
For n = 2, the transition kernel is given by
q(x, y) = p(y1 | x2)p(y2 | y1),
and therefore
q(y, x) = p(x1 | y2)p(x2 | x1).
We start by considering the left-hand side of the identity (11.24). Integrating q(y, x)
with respect to x yields

11.3 Gibbs Sampler
235

R2 q(y, x)dx =

R2 p(x1 | y2)p(x2 | x1)dx1dx2
=

R

p(x1 | y2)

R
p(x2 | x1)dx2



=1

dx1
=

R
p(x1 | y2)dx1 = 1,
hence

R2 p(y)q(y, x)dx = p(y).
Next write
p(x)q(x, y) = p(x)p(y1 | x2)p(y2 | y1) = p(x1, x2)p(y1 | x2)p(y2 | y1),
and integrate with respect to x1 to obtain

R
p(x)q(x, y)dx1 = p(y1 | x2)p(y2 | y1)

R
p(x1, x2)dx1



=p(x2)
= p(y1 | x2)p(x2)



=p(y1,x2)
p(y2 | y1)
= p(y1, x2)p(y2 | y1).
Integrating this expression with respect to x2, we obtain

R
p(y1, x2)p(y2 | y1)dx2 = p(y2 | y1)

R
p(y1, x2)dx2



p(y1)
= p(y2 | y1)p(y1)
= p(y1, y2) = p(y),
thus completing the proof.
As intimidating as formula (11.23) may look, in practice it is quite easy to imple-
ment it, by treating the factors in the product sequentially, until every component
has been updated, see Fig. 11.12. This is summarized in the following sampling
algorithm.
Gibbs Sampler algorithm: Given the probability density p and the target size
N of the sample of its realizations:
1. Initialize: Choose x0, set the counter k = 1.
2. Iterate: While k < N,

236
11
Sampling: The Real Thing
Fig. 11.12 Gibbs Sampler
generates the sample by
drawing the coordinates one
at a time from the marginal
density, keeping the
remaining coordinates ﬁxed
(1) Draw xk
1 from t →p(t, xk−1
2
, xk−1
3
, · · · , xk−1
n
),
(2) Draw xk
2 from t →p(xk
1, t, x(k−1)
3
, · · · , x(k−1)
n
),
...
(n) Draw xk
n from t →p(xk
1xk
2, · · · , xk
n−1, t).
Increase k →k + 1.
A major appeal of the Gibbs sampler algorithm is that there is no need for tuning
parameters affecting the acceptance as in the Metropolis–Hastings algorithm. The
drawback, however, is that the drawing from one-dimensional marginals, while con-
ceptually straightforward, may be tedious to implement and time consuming. This is
particularly true if the marginals are multi-modal, or when the support of the density
is poorly known. Moreover, it is easy to imagine situations in which Gibbs sampler
fails to explore the density, or when the process becomes very slow. An example of
the former situation is given by a bimodal distribution in R2,
p(x) ∝χB1(x) + χB2(x),
where B1, B2 ⊂R2 are two sets in the plane so that
B1 ⊂{(x1, x2) | x1, x2 < 0},
B2 ⊂{(x1, x2) | x1, x2 > 0}.
see the left panel of Fig. 11.13. In this case, it is impossible to reach B2 starting from
B1 and vice versa by the coordinate update scheme. An example of the latter case
is given by the distribution shown in the right panel of the same ﬁgure, where the
updating step size in the coordinate directions is very small, making the progress
of the exploration prohibitively slow. Here a simple and educated rotation of the
coordinate axes would resolve the problem, however, it may be hard to ﬁnd, in
particular when n is large. Another option is to perform the updates in random

11.4 Preconditioned Crank–Nicholson
237
Fig. 11.13 Two examples of probability densities that Gibbs sampler has problems to explore. On
the left, the sampler cannot jump from one disc to the other along paths with segments parallel to
coordinate axes. On the right, the components x1 and x2 are strongly correlated, and the distribution
allows only short steps parallel to the coordinate axes, therefore rendering the sampler inefﬁcient
directions rather than along coordinate directions. The ensuing algorithm is referred
to as Hit and Run algorithm.
11.4
Preconditioned Crank–Nicholson
In the Metropolis–Hastings algorithm, the acceptance rate can be controlled by the
step size in the proposal distribution, with smaller steps leading to higher accep-
tance rate. The tuning process is known to be sensitive to the dimensionality of the
underlying space and, in high dimensions, targeting a given acceptance rate leads
typically to a step size so small that the algorithm becomes impractical, requiring
sample sizes that are not realistically achievable. A solution proposed in the literature
for a particular class of problems is to modify the proposal, and its tuning parameter,
leading to a sampling scheme referred to as preconditioned Crank–Nicholson (pCN)
algorithm.
Following the original presentation of the pCN algorithm, we assume that the
probability density p to be explored is of the form
p(x) ∝e−Φ(x) p0(x),
(11.25)
where p0 is a Gaussian probability distribution with zero mean and symmetric pos-
itive deﬁnite covariance C ∈Rn×n,
p0(x) = N(x | 0, C),
and Φ is a real-valued potential function. In this case, we say that the Gaussian p0
is a dominant distribution. Observe that the formulation is particularly natural for

238
11
Sampling: The Real Thing
posterior distributions with a Gaussian prior p0, the exponential representing in this
case the likelihood density.
We start by observing that since p0 is Gaussian, drawing samples from p0 is
straightforward by using the Mahalanobis transformation. Therefore, one could use
rejection sampling with p0 as a proposal. This, however, is usually not a good idea,
since rejection sampling may lead to extremely low acceptance rates, in particular,
when the dimensionality of the problem is high, or when the data are of good quality.
An advantage of MCMC algorithms is that, unlike rejection sampling, they learn the
effective support of the density, and it is possible to control the step size so that the
proposals do not fall too far from of the previously accepted sample points.
A natural way to take advantage of this idea is to modify the rejection sampling
by introducing a control on the step size and to consider a Metropolis–Hastings
algorithm with the dominant Gaussian distribution as the proposal density. Deﬁne
the random walk proposal
y = xk−1 + βwk,
wk ∼N(0, C)
(11.26)
with some β > 0 controlling the step size. To get more insight into the problem,
assume ﬁrst that Φ(x) = 0 and p(x) = p0(x), that is, we are simply seeking to
generate a sample from the density p0 itself. Assuming that X ∼p0, consider a
random variable corresponding to the proposal (11.26),
Y = X + βW,
X, W ∼N(0, C),
(11.27)
assuming that X and W are mutually independent. Then, Y is a zero mean Gaussian
random variable, with covariance
E(YY T) = E(X XT) + β2E(W W T) = (1 + β2)C,
or, equivalently,
Y ∼N(0, (1 + β2)C).
Therefore, the proposal (11.26) is drawn from the dominant Gaussian distribution
with covariance inﬂated by a factor (1 + β2), and the acceptance of the proposal is
not automatic. In fact, when the dimensionality of the problem increases, the accep-
tance rate drops quickly, and one needs to decrease the step size β, which makes the
algorithm inefﬁcient. However, we can use the same remedy for the covariance inﬂa-
tion that we used in Sect. 4.7.2 when discussing the weighted bootstrap algorithm.
More precisely, we modify formula (11.27) to
Y =

1 −β2X + βW,
by moving the point X slightly closer to the mean of p0. Now, the covariance of Y is

11.4 Preconditioned Crank–Nicholson
239
E(YY T) = (1 −β2)E(X XT) + β2E(W W T) = C,
that is, the proposal is drawn from p0 and is automatically accepted!
After these preliminary considerations, consider the Metropolis–Hastings accep-
tance ratio corresponding to the proposal (11.26). The move to y is accepted, and
xk = y, if
p(y)
p(xk−1) > t,
t ∼Uniform([0, 1]),
otherwise we set xk = xk−1. Taking into account the particular form of the probability
density, the acceptance condition can be written in logarithmic form as
1
2∥xk−1∥2
C + Φ(xk−1) −
1
2∥y∥2
C + Φ(y)

> log t.
Here, the notation zTC−1z = ∥z∥2
C is used, which differs from the similar notation
used in Chap. 9. Another way of expressing the proposal is to write the conditional
density of the random variable Y given the realization of Xk−1,
πY|Xk−1
y | xk−1
∼N(xk−1, β2C).
Modiﬁcation of the proposal distribution, with a given β, 0 < β < 1, yields
πY|Xk−1
y | xk−1
∼N(

1 −β2xk−1, β2C),
and write the proposal as
y =

1 −β2xk−1 + βwk, wk ∼N(0, C).
Observe that since this proposal kernel is not symmetric, the acceptance ratio
needs to be calculated using the formula (11.19). Writing ﬁrst the transition kernel
as
πY|Xk−1
y | xk−1
= R(x, y) ∝exp

−1
2β2 ∥y −

1 −β2x∥2
C

,
we see that the ratio of the proposal kernels is

240
11
Sampling: The Real Thing
R(y, x)
R(x, y) = exp

−1
2β2 ∥x −

1 −β2y∥2
C +
1
2β2 ∥y −

1 −β2x∥2
C

= exp

−
1
2β2

∥x∥2
C −2

1 −β2xTC−1y + (1 −β2)∥y∥2
C
−∥y∥2
C + 2

1 −β2yTC−1y −(1 −β2)∥x∥2
C

= exp

−1
2

∥x∥2
C −∥y∥2
C

.
Likewise, the ratio of the densities is
p(y)
p(x) = exp

−1
2∥y∥2
C −Φ(y) + 1
2∥x∥2
C + Φ(x)

,
so that the acceptance ratio is reduced to
α(x, y) = exp (−Φ(y) + Φ(x)) ,
which depends solely on the likelihood! The fact that the Gaussian portion is not
playing a role in the acceptance ratio is a major reason for the popularity of the pCN
algorithm for high-dimensional problems.
We summarize the pCN steps in the following algorithm.
Preconditioned Crank–Nicholson algorithm: Given a probability density p of
the form (11.25), and the target size N of the sample:
1. Initialize: Choose x0, set the counter k = 1.
2. Iterate: While k < N,
(a) Given xk−1, deﬁne
y =

1 −β2xk−1 + βwk,
w ∼N(0, C).
(b) Calculate the acceptance ratio,
α(xk−1, y) = exp

−Φ(y) + Φ(xk−1)

.
(c) Flip the α–coin: draw t ∼Uniform([0, 1]); if α > t, accept y, and set
xk = y, otherwise stay put, setting xk = xk−1.
(d) Advance the counter k →k + 1.
Notice that the parameter β controls the acceptance rate: In the limit, as β →0+,
the proposal y converges to the previous sample vector, and therefore the acceptance
ratio converges to one. As a rule of thumb, the acceptance rates advocated in the
literature for pCN should be below or around 50%.

11.4 Preconditioned Crank–Nicholson
241
Notes and Comments
For a reference to Frobenius–Perron Theorem and related topics in linear algebra,
we refer to the book [56]. There is a vast literature on Markov chain Monte Carlo
methods, addressing both theoretical and practical aspects. For good collections
of articles addressing several aspects of MCMC, see [6, 38]. The origins of the
Metropolis–Hastings algorithm go back to the works of Nikolas Metropolis and
colleagues on simulated annealing, [55], the current version being introduced by
Hastings in [43]. For the origins of Gibbs sampler, we refer to [33, 34]. In the
discussion of the MCMC algorithm, we have left out a number of important details
needed to guarantee that the algorithm indeed explores the underlying distribution.
For a concise but comprehensive discussion, we refer to [59].
The basic Metropolis–Hastings algorithm can be made more efﬁcient by using
proposal distributions with size and spatial orientation adjusted to the underlying
density that is explored. Adapted Metropolis (AM) algorithm [39] is dynamically
updating the Gaussian proposal distribution by recalculating the covariance matrix
empirically from the previously accepted sample points, leading usually to higher
acceptance of the proposals and therefore improved computational efﬁciency.
A different idea to improve the proposal distribution is to guide the proposals
toward the local maxima of the underlying density. Metropolis-adjusted Langevin
algorithm (MALA) [64] uses a discretized Langevin diffusion process as a pro-
posal, where the random step drawn from the Gaussian distribution is biased toward
the direction of the gradient of the negative logarithm of the density. The proposal
therefore is closely related to the stochastic gradient descent algorithm for ﬁnding a
minimum of a differentiable function. The Langevin algorithm is related to Hybrid
Monte Carlo (HMC) algorithm having its roots in molecular dynamics simulations
[30]. The proposal is a discrete approximation of Hamiltonian dynamics [58], for
which HMC is also an abbreviation for Hamiltonian Monte Carlo. As comprehensive
references to various MCMC algorithms we refer to [6, 52].
For a detailed introduction to the Preconditioned Crank–Nicholson algorithm, we
refer to the articles [25, 40].

Chapter 12
Dynamic Methods and Learning from
the Past
Richard Price, a Welsh philosopher and mathematician who has signiﬁcantly con-
tributed to the early development of Bayesian theory, published in 1764 an essay on
Bayes’ work, in which he asked how to assign a subjective probability to the sunrise,
given that the sun had been observed to rise a given number of times before. Price’s
idea is that we learn from earlier experiences, and update our expectations based on
them. The question was revisited by Pierre-Simon Laplace in his 1774 essay, and
again in 1777 by the French scientist and mathematician George-Louis Leclerc de
Buffon. While this interest in the question of sunrise may seem rather academic, its
real value is the scientist’s probing of the human thought process, and the difﬁculty
of translating qualitative beliefs into numbers, a major challenge of Bayesian science
even today. This quest also serves as an introduction to sequential Bayesian methods:
Today’s posterior will be tomorrow’s prior while waiting for the new observation to
arrive.
12.1
The Dog and the Hunter
We start with an introductory example to clarify the concept of Bayesian ﬁltering
and particle methods in general.1
Consider the following problem: A hunter, in full mimetic gear, leaves the house
with his loyal dog to take a walk in the forest. We want to reconstruct the trajectory
of the hunter, who cannot be observed, based on occasional sightings of the dog.
What we know is that
1. At t = 0, the hunter left from his house.
2. At times 0 < t1 < t2 < . . . < tn, the dog was seen at positions q1, q2, . . . , qn.
1 We owe this example to our colleague Giulio d’Agostini.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
D. Calvetti and E. Somersalo, Bayesian Scientiﬁc Computing, Applied Mathematical
Sciences 215, https://doi.org/10.1007/978-3-031-23824-6_12
243

244
12
Dynamic Methods and Learning from the Past
3. The speed of the hunter is likely to be no greater than a reasonable walking speed
vmax > 0.
4. The dog usually does not venture further than a distance dmax > 0 from its master.
We solve the problem by generating, at each time step, a large cloud of possible
positions of the hunter, propagate them according to the information we have, and,
at each sighting of the dog, assess the viability of each particle.
We start by ﬁrst treating the problem heuristically, and afterwards we proceed
to put our solution on more solid ground. Let N denote the number of particles,
which are realizations of the R2-valued random variable Xt modeling the position
of the hunter at the time instance t. Since at t = 0 we know for sure that the hunter
is at home, positioned at the coordinate origin of R2, we initialize the problem by
choosing N particles
x1
0 = x2
0 = . . . = x N
0 = 0,
all drawn from the prior probability density
πX0(x) = δ0(x).
Since the particles are identical, we assign them equal weights,
w1
0 = w2
0 = . . . = wN
0 = 1
N .
Next, we start to propagate. Denoting by xℓ
1 the position of the ℓth particle at time
t = t1, we set
xℓ
1 = xℓ
0 + vℓ
1,
1 ≤ℓ≤N,
(12.1)
where vℓ
1 is a random step, reﬂecting the fact that we do not know which way the
hunter moves. If we interpret vmax as the maximum speed of the hunter, we should
have
∥vℓ
1∥≤vmax(t1 −t0) = γ1.
If there is no information on preferred direction, the direction of the move is arbitrary,
and we may deﬁne vℓ
1 as a realization of a random variable V1 deﬁned as
V1 = γ1S
cos Θ
sin Θ

,
S ∼Uniform

[0, 1]

,
Θ ∼Uniform

[0, 2π]

.
This way, the particles (12.1) deﬁne a predictive sample,

x1
1, . . . ,x N
1

, which is
based solely on the model, without using any data. The predictive sample can be
thought of as our a priori information of the position of the hunter at t = t1.
At time t = t1, the ﬁrst observation of the dog’s position arrives. Therefore, we
need to deﬁne a likelihood expressing the distribution of the position of the dog,
assuming that we know where the hunter is. If the only information that we have is

12.1 The Dog and the Hunter
245
that the dog is never more than dmax away from the master, a natural candidate for
the likelihood is
πQ1|X1(q1 | x1) =
1
πd2max
χB(x1,dmax)(q1) =
	1/πd2
max, if ∥d1 −x1∥} < dmax,
0
otherwise,
(12.2)
the characteristic function of the disk centered at x1 with radius dmax. Heuristically,
it looks reasonable to associate to each particle xℓ
1 the weight wℓ
1, with
wℓ
1 = πQ1|X1(q1 | xℓ
1),
q1 = the observed value of q1,
(12.3)
and then normalize the weights so that they sum to one,
wℓ
1 →
wℓ
1

ℓ′ wℓ′
1
.
With the likelihood model (12.2), the weights (12.3) before normalization are either
1/πd2
max if the point is inside the disk, and zero otherwise.
Next we perform importance sampling: Select N indices ℓ1, ℓ2, · · · , ℓN, drawn
with replacement from the set {1, 2, . . . , N}, where each integer ℓhas probability
wℓ
1 of being selected, and deﬁne the new sample as
xk
1 = xℓk
1 ,
1 ≤k ≤N.
Observe that this way, the particles with zero weight that we deemed to be impossible
positions for the hunter are never chosen, while positions more likely to explain the
data may be selected multiple times. The weights of the new particles are all equal,
wℓ
2 = 1/N, because importance is now expressed in the form of representation, that
is, more probable positions may have been picked multiple times. The steps are
explained in a graphical form in Fig. 12.1. The process can now be repeated for the
next time step.
Before discussing the details of the proposed algorithm, let us consider a small
computed example illustrating the merits and ﬂaws of the approach.
Example 12.1 In our example, we generate the data assuming that the velocity of
the hunter is given by
v(t) = cos 3πt
 cos πt
sin πt

,
0 ≤t ≤1,
satisfying
∥v(t)∥≤1 = vmax.

246
12
Dynamic Methods and Learning from the Past
Fig. 12.1 Left: The initial sample consists of ﬁve particles indicating the hunter’s initial position.
Each particle is propagated by adding a random step, resulting in proposal particles x( j)
1 . Middle:
The dog shows up (red square). The blue disk indicates all possible locations no more than dmax
away from the dog. Only two proposal particles qualify. Right: New sample of hunter’s position
is drawn. Since the proposal particles outside the disk have zero weight, all ﬁve points must be
replicates of the two possible positions
The trajectory of the hunter is shown in Fig. 12.2. We assume that at times t j = j/30,
1 ≤j ≤30, the dog is observed at distance at most dmax = 6 from the hunter. The
red dots in the ﬁgure indicate the observed positions of the dog, joined with a line
segment to the corresponding position of the hunter.
To highlight the limitations of the proposed model, we ﬁrst run the algorithm
with a small number of particles, setting N = 50. In Fig. 12.3, a few snapshots of
the progression are shown: In the ﬁgure, the predictive particles xℓ
k are plotted with
the color coding indicating if the weight wℓ
k is zero or positive. The snapshots clearly
demonstrate the impoverishment of the sample over time; sometimes, only one of
the particles has a non-vanishing weight, and at the ﬁnal step, none of the particles
survive, leading to a failure of the algorithm.
Fig. 12.2
Generative trajectory of the hunter with the positions at which the dog (right) is observed
at times t j = j/30, 1 ≤j ≤30. The starting point is indicated by the green square

12.1 The Dog and the Hunter
247
Fig. 12.3 Few snapshots of the algorithm. The red dot indicates the position of the dog, and the
predictive particles are marked either by an empty dot if the likelihood weight vanishes or by a blue
dot if the weight is positive. The time, in lexicographical order, is k = 1, 2, 8, 9, 10, 11. The data
thinning, or sample impoverishment, is clearly seen here as often only one predictive particle has a
positive weight. Observe that in the last step, none of the predictive particles has a positive weight,
indicating that the algorithm fails
To compensate for the loss of particles, we increase the particle number to N =
5 000. This time, the particle cloud survives, and we are able to estimate the position
of the hunter by averaging over the particle positions,
xk = 1
N
N

ℓ=1
xℓ
k.
Figure 12.4 shows the estimated position of the hunter. Moreover, we can use the
particle sample to compute the covariance matrix of the position,
Ck =
1
N −1
N

ℓ=1
(xℓ
k −xk)(xℓ
k −xk)T.
At each shown time step, around the mean position, we plot the ellipsoid corre-
sponding to two standard deviations. This is done by ﬁrst computing the symmetric
singular value decomposition of the covariance matrix,
Ck = UkDkUT
k ,

248
12
Dynamic Methods and Learning from the Past
Fig. 12.4
Estimated
position of the hunter (green
squares) and the uncertainty
ellipses, corresponding to
two standard deviations of
the posterior covariance
when using N = 5000
particles. Observe that
occasionally, the ellipses
collapse to a point due to
loss of particles
and plotting an ellipse parametrized by the angle θ,
sk(θ) = xk + 2UkD1/2
k
cos θ
sin θ

,
0 ≤θ ≤2π.
While most of the ellipses are of reasonable size, we observe that there are at least
two time instances when the ellipses collapse to a single point, indicating that the
underlying particle sample is imploded into one or few particles that are replicated.
The results highlight some of the shortcomings of the model. First, as we saw,
the number of predicted particles that will be effectively discarded from the sample
by being assigned zero weight can be signiﬁcant, leading to an impoverishment of
the sample. In fact, it may even happen that all predicted particles are discarded,
causing the algorithm to stop. To avoid such catastrophic situation, we may write a
less stringent likelihood model accounting for the fact that, with small probability,
the dog could end up further than dmax from its master.2 A likelihood conveying that
sort of belief is a Gaussian model of the form,
πQ1|X1(q1 | x1) ∝exp

−
1
2(τdmax)2 ∥x1 −q1∥2

,
where τ > 0 is a parameter expressing how strongly the belief about the distance
of the dog from the master is held. Observe that, with this model, the weights wℓ
1
deﬁned by (12.3) never vanish, and the possibility of losing all particles is avoided.
Another assumption that we may question is whether the hunter never walks at
a speed higher than vmax, and replace the step-limited innovation with a Gaussian
random walk model of the form
X1 = X0 + V1,
V1 ∼N(0, (ργ1)2),
where ρ > 0 is a parameter controlling the expected length of the random step.
2 After all, even the most trustworthy unleashed dog may occasionally wander further away than
usual.

12.2 Sampling Importance Resampling (SIR)
249
Fig. 12.5 Estimated position
of the hunter (green squares)
at discrete time instances and
the associated uncertainty
ellipses, corresponding to
two standard deviations of
the posterior covariance,
using a Gaussian likelihood
and Gaussian innovation
We run again a short simulation with the new model, showing the difference
between the two models, using the scaling parameter τ = ρ = 0.4. The number of
particles is set to N = 10 000. We run the algorithm with the same data as before,
computing the particle mean and posterior covariances. The results are shown in
Fig. 12.5. Notably, the covariance ellipses do not collapse, indicating that this model
accounts better for the uncertainty in the estimate.
12.2
Sampling Importance Resampling (SIR)
Havingoutlinedthegeneralnotionofparticleﬁlteringwiththehelpofthepreliminary
example, we are ready to approach the problem in a more general setting. Before
doing so, we establish the notation to be used. We begin by omitting, whenever
unambiguous, the subindices specifying the random variables in probability density
functions, with the understanding that the realizations will convey the information.
Thus, instead of πX|Y(x | y), we write simply π(x | y). Furthermore, we use the
shorthand notation b1:t to indicate an accumulation of data {b1, b2, . . . , bt} up to
time t.
For the sake of deﬁniteness, let us consider an evolution-observation model of
random variables,
Xt = F(Xt−1) + Vt,
Bt = G(Xt) + Wt,
(12.4)
for t = 1, 2, . . .. Here, the state vectors Xt describe a discrete time stochastic process,
and the random variable V is referred to as state noise or innovation process at time
t. The random variable Bt deﬁnes the observation process, and the variable Wt is
referred to as observation noise process at time t. The ﬁrst equation in (12.4) is the
evolution model, the second one is the observation model.
In our model, we assume that F and G are known deterministic functions. Fur-
thermore, we assume the random variables X j, 1 ≤j ≤t −1 and Vt in the evolution

250
12
Dynamic Methods and Learning from the Past
model to be mutually independent, and, likewise, Xt and Wt in the observation model
to be mutually independent. The evolution model, together with the independence
of the innovation Vt from the past history of X, imply that the stochastic process Xt
satisﬁes the Markov property,
π(xt | x0:t−1) = π(xt | xt−1),
(12.5)
that is, the current state Xt depends on the past X0, X1, . . . , Xt−1 only through the
previous state. Likewise, the current observation at time t depends on the states only
through the current state,
π(bt | x0:t) = π(bt | xt).
(12.6)
The Bayesian ﬁltering algorithm comprises a sequence of two-step iterations, the
prediction step (P) and the correction step (C), also known as analysis step:
π(xt−1 | b1:t−1)
P
−→π(xt | b1:t−1)
C
−→π(xt | b1:t),
where it is understood that for t = 1, π(x0 | b1:0) = π(x0), that is, the density of
X0 before any observation becomes available is simply a prior distribution. The
prediction step is based on the evolution model, while the correction step follows
from the observation model.
To derive the pertinent formulas, consider ﬁrst the joint probability density of Xt−1
and Xt, conditioned on the past history of observations, B1:t−1. Using the deﬁnition
of conditional densities, we write
π(xt−1, xt | b1:t−1) = π(xt | xt−1, b1:t−1)π(xt−1 | b1:t−1).
Next, we use the Markov properties of our model to simplify this formula. On the
right-hand side, Xt depends on the past only through Xt−1; therefore, knowing the
past observation history does not add information about Xt, implying that
π(xt | xt−1, b1:t−1) = π(xt | xt−1).
In the propagation step, we are interested in how Xt depends on the past observations
regardless of the value of Xt−1. To that end, we calculate the marginal density,
π(xt | b1:t−1) =

π(xt−1, xt | b1:t−1)dxt−1
=

π(xt | xt−1)π(xt−1 | b1:t−1)dxt−1.
(12.7)
This relation, known as the Chapman–Kolmogorov formula, states that the predictive
distributionisobtainedfromthepreviousdistributionbyapplyinganintegraloperator
with a kernel π(xt | xt−1) that depends on the evolution model only.

12.2 Sampling Importance Resampling (SIR)
251
The correction step is the result of Bayes’ formula conditioned on the past obser-
vations, using the predictive distribution as the prior distribution. More precisely, we
write
π(xt | b1:t) = π(xt | bt, b1:t−1)
∝π(bt | xt, b1:t−1)π(xt | b1:t−1)
= π(bt | xt)π(xt | b1:t−1),
(12.8)
the last equality following from the Markov property that the observation Bt depends
on the past only through Xt, since the observation noise was assumed to be indepen-
dent of the past. We may now combine the prediction step and the correction step
into one single updating formula,
π(xt | b1:t) ∝π(bt | xt)

π(xt | xt−1)π(xt−1 | b1:t−1)dxt−1.
(12.9)
This equation is the starting point for all Bayesian ﬁltering algorithms.
We consider ﬁrst the algorithm similar to what was used in the hunter-and-dog
example, but with the more general evolution-observation model (12.4). Assume that
at time t −1, a sample drawn from the density π(xt−1 | b1:t−1) with weights,
St−1 =

(x1
t−1, w1
t−1), (x2
t−1, w2
t−1), . . . , (x N
t−1, wN
t−1)

,
is given. Using the idea of approximating a continuous density by a point mass
density, we write
π(xt−1 | b1:t−1) ≈
N

j=1
w j
t−1δx j
t−1(xt−1).
Substituting this approximation in (12.9) yields an approximation
π(xt | b1:t) ∝
∼π(bt | xt)
N

j=1
w j
t−1π(xt | x j
t−1),
(12.10)
where the symbol ∝
∼stands for approximately proportional. Next we use some of the
ideas introduced in Sect. 4.7.1: For every particle x j
t−1, generate a proposal particle
x j
t drawing from π(xt | x j
t−1), which is tantamount to using the propagation model,
x j
t = F(x j
t−1) + v j
t ,
where the innovation v j
t is drawn independently from the probability density of
Vt. In this manner, we have generated a sample of candidate particles that we then
score according to how probable they make the new observation bt. To this end, we
calculate the scores and normalize them,

252
12
Dynamic Methods and Learning from the Past
w j
t = wt
t−1π(bt | x j
t ),
w j
t →
w j
t

N
k=1 wk
t
,
then resample the particles x j
t by drawing with replacement from the set {x1
t , . . . ,x N
t }
using the weights w j
t as probabilities. In this manner, the weights of the particles in
the new sample are all equal,
w1
t = . . . wN
t = 1
N .
We are now ready to organize the different steps in the form of an algorithm.
Particle Filter Sampling Importance Resampling (PF-SIR): Assume an
evolution-observation model (12.4) with a given prior density π(x0) of X0.
1. Initialize: Draw N independent samples x1
0, . . . , x N
0 from π0. Set the counter
t = 1.
2. Repeat: while t ≤T :
(a) For each particle x j
t−1, calculate x j
t = F(x j
t−1) + v j
t , 1 ≤j ≤N.
(b) Compute and normalize the scores,
w j
t = π(bt | x j
t ),
w j
t →
w j
t

N
k=1 wk
t
.
(c) Generate a new sample x1
t , . . . , x N
t by drawing with replacement from the
set {x1
t , . . . ,x N
t } with probabilities w j
t .
(d) Advance the counter by one, t →t + 1.
The PF-SIR algorithm was used in the hunter-and-dog example, with F(x) = x,
that is, the propagation was purely a random step added to the previous position.
Observe that since the weights are all equal before and after the updating in Step
2, they may be left out of the algorithm. In the algorithm, every particle x j
t−1 is
propagated, producing one proposal particle x j
t . This process is sometimes called
layered sampling. It turns out that often it is better to select ﬁrst the particles to be
propagated, as will be shown in the next section.
12.2.1
Survival of the Fittest
The passage from the current sample at time instance t −1 to the next generation
sample at time instance t may be seen as an evolutionary process, with the scoring
done by having the likelihood act as Darwinian selection. In the layered sampling

12.2 Sampling Importance Resampling (SIR)
253
algorithm, each parent particle generated exactly one offspring, and the offsprings
were subsequently scored according to their ﬁtness. However, mimicking the process
of natural selection, one can design an algorithm in which more ﬁt parent particles
produce more offsprings than the less ﬁt ones, and then the offsprings will be scored.
The assumption is that a genetically ﬁt parent produces offsprings that are also likely
to be ﬁt, following the popular wisdom that the apple doesn’t fall far from the tree.
Our outline of the Darwinian variant of the particle ﬁlter algorithm starts with
the approximate formula (12.10). Now, however, instead of layered sampling, we
compute auxiliary particles without innovation,
x j
t = F(x j
t−1),
1 ≤j ≤N.
To check how well these particles explain the next data installment, we compute and
normalize their ﬁtness scores,
w j
t = w j
t−1π(bt | x j
t ),
w j
t →
w j
t

N
k=1 wk
t
.
With the notation just introduced, we can write formula (12.10) as
π(xt | b1:t)∝
∼π(bt | xt)
N

j=1
w j
t−1π(xt | x j
t−1)
∝
N

j=1
w j
t
π(bt | xt)
π(bt | x j
t )
π(xt | x j
t−1).
(12.11)
Next we select N parent particles from the auxiliary particles x j
t for proliferation,
drawingthemwithreplacementbyusingtheﬁtnessscoresw j
t asprobabilities.Denote
the selected particles by x
ℓj
t , 1 ≤j ≤N. Observe that it is likely to have several
repetitions in this set. We then deﬁne the new particles by setting
x j
t = x
ℓj
t + v j
t ,
where v j
t is an independently drawn random realization of the innovation Vt. Finally,
the ﬁtness of each offspring needs to be assessed. It follows from formula (12.11)
that the natural way to assign their weights is
w j
t = π(bt | x j
t )
π(bt | x
ℓj
t )
.
The sequence of operation in each survival of the ﬁttest algorithm’s iteration is
illustrated schematically in Fig. 12.6.

254
12
Dynamic Methods and Learning from the Past
Fig. 12.6 Schematic explanation of the survival of the ﬁttest algorithm. Left: Particles x j
t−1 are
propagated using the deterministic forward map, x j
t = F(x j
t−1). Middle: The propagated particles
are scored according to how well they explain the data in terms of the likelihood. Right: The particles
generate offsprings according to their ﬁtness score. The offsprings are then weighted according to
their ﬁtness
Before organizing the different steps in the form of an algorithm, let us examine
the difference between the layered sampling algorithm and the proposed one in the
light of the hunter-and-dog example. Figure 12.7 demonstrates schematically the
difference. In the layered sampling algorithm (top row), each particle produces one
offspring through the addition of a random step, regardless of how well, or badly,
the particle is capable of explaining the next installment. As a result, several of the
new offsprings may turn out to have a low weight, ending up being discarded at the
following step. In the survival of the ﬁttest algorithm, on the other hand, the particles
are ﬁrst scored according to their ﬁtness, then the proliferation takes place according
to the ﬁtness score. Consequently, the new particles are typically better positioned to
explain the data, leading to a higher weight and lower discarding rate. We postpone
the algorithm to the next section, where a generalized version of it is derived.
12.2.2
Estimation of Static Parameters
In some cases, the deterministic forward map F depends on parameters that may be
poorly known and therefore should be estimated based on the data as well. We denote
by θ ∈Rk the vector of k real parameters, and write the evolution-observation model
as
Xt = F(Xt−1, Θ) + Vt,
Bt = G(Xt) + Wt,
t = 1, 2, . . .
(12.12)
where the parameter vector is deﬁned as a random variable Θ with realizations θ.
To extend the PF algorithm to a parametric problem, we formally treat the random
variable Θ as if it were time dependent, and use the notation Θt to highlight the time
dependency. Observe that doing so does not mean that we believe that the parameter
is changing in time, but rather that, as time marches on, our certainty about its value

12.2 Sampling Importance Resampling (SIR)
255
Fig. 12.7 In the left column, the old particles are denoted by empty circles, while the new particles
are denoted by squares. In the top row, the layered sampling approach adds innovation to each old
particle to generate new particles (blue squares). Particles are ranked according to how well they
explain the data. The green squares in the right column correspond to new particles, the size of the
square indicating how many times the particle is repeated in the new sample. In the bottom row, the
old particles are ﬁrst ranked according to their ﬁtness to explain the data, and the more ﬁt particles
generate more offsprings
may change. As randomness in the Bayesian context represents uncertainty, each
Θt is therefore different as a random variable. A crucial point is the choice of the
innovation model for the parameter. While a random walk model is a possibility, it
is not in line with the assumption that the parameter is static. A better choice is to
resort to a Gaussian mixture model, introduced in Sect. 4.7.2.
We start by writing the formula (12.9) for the pair (xt, θt),
π(xt, θt | b1:t) ∝π(bt | xt, θt)
×

π(xt, θt | xt−1, θt−1)π(xt−1, θt−1 | b1:t−1)dxt−1dθt−1.
Assuming that we have the current particle cloud,
St−1 =

(x1
t−1, θ1
t−1, w1
t−1), . . . , (x N
t−1, θN
t−1, wN
t−1)

,
using the point mass approximation for the current density, we obtain

256
12
Dynamic Methods and Learning from the Past
π(xt, θt | b1:t) ∝π(bt | xt, θt)
N

n=1
wn
t−1π(xt, θt | xn
t−1, θn
t−1).
Next we need to write a propagation model for both xt and θt. For xt, we use the
model (12.12), which deﬁnes a probability density
π(xt | xt−1, θt−1) = πVt

xt −F(xt−1, θt−1)

.
To deﬁne a propagation model for θt, recalling that xt and θt are assumed to be
conditionally independent, and using the Gaussian mixture approximation, we write
π(xt, θt | b1:t) ∝π(bt | xt, θt)
N

n=1
wn
t−1π(xt | xn
t−1, θn
t−1)N(θt | θ
n
t−1, h2Ct−1).
The auxiliary particles θ
n
t−1 were obtained by shifting the particles θn
t−1 towards the
ensemble mean,
θ
n
t−1 = aθn
t−1 + (1 −a)θt−1,
0 < a < 1,
and
Ct−1 =
1
N −1
N

n=1
(θn
t−1 −θt−1)(θn
t−1 −θt−1)T,
θt−1 = 1
N
N

n=1
θn
t−1,
where the parameters a and h are related by
a2 + h2 = 1.
Observe that in the limit as a →1, the Gaussian densities converge to point measures,
i.e., there is no innovation in the parameters.
We now organize the steps of the particle ﬁlter in the form of an algorithm.
ParticleFilterSurvivaloftheFittest(PF-SOF):Assumeanevolution-observation
model (12.12) with the prior densities, π(x0) and π(θ0), of X0 and Θ0 given.
1. Initialize: Draw N independent samples x1
0, . . . , x N
0 from π0, and θ1
0, . . . , θN
0
from π(θ0), and set the counter t = 1.
2. Repeat while t ≤T :
(a) Compute the ensemble mean θt−1 and covariance Ct−1 of the parameter
vectors θn
t−1.
(b) For each particle (x j
t−1, θ j
t−1), calculate x j
t = F(x j
t−1, θ j
t−1), and θ
j
t−1 =
aθ j
t−1 + (1 −a)θt−1, 1 ≤j ≤N.

12.2 Sampling Importance Resampling (SIR)
257
(c) Compute and normalize the ﬁtness scores,
g j
t = w j
t−1π(bt | x j
t , θ
j
t−1),
g j
t →
g j
t

N
k=1 gk
t
.
(d) Draw N indices ℓj with replacement from {1, 2, . . . , N}, using the scores
g j
t as probabilities.
(e) Proliferate: Generate the new samples (x1
t , θ1
t ), . . . , (x N
t , θN
t ),
x j
t = x
ℓj
t + v j
t ,
θ j
t ∼N(θt | θ
ℓj
t , h2Ct−1),
where v j
t is a realization of the innovation Vt.
(f) Calculate the weights of the particles,
w j
t = π(bt | x j
t , θ j
t )
π(bt | x j
t , θ
j
t )
,
w j
t →
w j
t

N
k=1 wk
t
.
(g) Advance the counter by one, t →t + 1.
To conclude the chapter, we consider a classic tracking problem, a ﬁeld in which
Bayesian ﬁltering methods have been very successful.
Example 12.2 We consider the problem of tracking an object such as a meteorite or
a space module entering the atmosphere. The object’s altitude is measured at given
time instances by using a simple pulse radar not capable of giving an estimate of the
velocity. To obtain information about the size and shape of the object, it is also of
interest to assess the drag force encoded in the ballistic constant of the object.
To set up the model, we deﬁne the dynamic state variables,
h = h(t) = altitude of the object, h > 0,
v = v(t) = downwards speed, v > 0.
The variables are related through the basic equation of motion,
dh
dt = −v.
(12.13)
The acceleration of the object depends on two factors, the Earth’s gravitational ﬁeld
and the drag of the atmosphere. Thus, the acceleration of the object assumes the form
dv
dt = g −g ρ(h) v2
2β
,
(12.14)

258
12
Dynamic Methods and Learning from the Past
where the parameters in the equation are
g = constant acceleration due to the gravity = 9.81 m/s2,
ρ(h) = altitude-dependent density of the air,
β = ballistic constant, β > 0.
The unknown ballistic constant depends on the shape, mass and cross-sectional area
of the object, all of which are of interest but unknown. Here, we believe that the
parameter β remains constant in time. Instead of including it in the ﬁltering problem
as a model parameter, we assume here a more straightforward approach, modeling
it as part of the state vector, and express the belief that its value does not change in
time by writing
dβ
dt = 0.
(12.15)
For the atmospheric density, we assume that it has a known dependency on h, given
by
ρ(h) = γe−ηh,
(12.16)
where the parameters are assumed to be known, with values γ = 1.754 kg/m3 and
η = 1.39 × 10−4 m−1.
We deﬁne the state vector of unknowns as
x(t) =
⎡
⎣
h(t)
v(t)
β
⎤
⎦.
Equations (12.13)–(12.16) deﬁne a non-linear system
dx
dt = f (x) =
⎡
⎢⎢⎣
−v
g −g ρ(h) v2
2β
0
⎤
⎥⎥⎦=
⎡
⎢⎢⎣
−x2
g

1 −ρ(x1) x2
2
2x3

0
⎤
⎥⎥⎦
(12.17)
that constitutes the basis of the propagation model.
Consider now the observation model. We assume that at times t j = jΔt, j =
0, 1, 2, . . ., the variable h is observed with some uncertainty. We write the model as
b j = qTx(t j) + ε j,
q =
⎡
⎣
1
0
0
⎤
⎦,
and we assume that the observation errors ε j can be reasonably modeled as realiza-
tions of independent identically distributed random variables E j ∼N(0, σ2).
To write a discrete propagation model, marching from one observation time to
the next, we use for simplicity a naïve forward Euler approximation,

12.2 Sampling Importance Resampling (SIR)
259
x(t j) ≈x(t j−1) + Δt f (x(t j−1)).
In the Bayesian framework, taking into account the discretization error and possible
shortcomings in the dynamic model, we deﬁne the random variables X j that satisfy
the propagation model given by
X j = X j−1 + Δt f (X j−1) + Vj = F

X j−1) + Vj,
where
F(x) = x + Δt f (x),
and the innovations Vj are modeled for simplicity as independent identically dis-
tributed Gaussian random variables,
Vj ∼N(0, C),
where the covariance matrix C ∈R3×3 is assumed to be diagonal. We write the
observation model in terms of random variables as
B j = qTX j + E j.
For the computed example, we start by generating the data using the system
(12.17), with initial values,
h0 = h∗(0) = 61 000 m
v0 = v∗(0) = 3 048 m/s,
β∗= 19 161 kg/ms2.
The data are generated by using a standard numerical ODE solver such as ode45
of Matlab. We assume that the altitude is measured with a frequency of 10 Hz for
a period of half a minute, that is, Δt = 0.1 s and 1 ≤j ≤300. The resolution of
the radar is assumed to be some hundreds of meters, which is included in the data
generation by setting the standard deviation of the noise to σ = 500 m. The time
series of the data,
b0, b1, . . . , bT ,
T = 300,
is shown in Fig. 12.8.
To estimate the state vector, we use the PF-SOF algorithm. The ﬁrst step is to
generate the initial particle cloud. As initial guess for the altitude, in lack of anything
better, we use the ﬁrst radar reading,
hinit = b0.

260
12
Dynamic Methods and Learning from the Past
Fig. 12.8 Simulated altitude
measurements
As initial guess for the velocity and ballistic coefﬁcient, we use the values
vinit = 3 000 m/s,
βinit = 20 000 kg/ms2.
The number of particles is set to N = 5 000. To express our uncertainty about the
initial values, the initial sample is generated by setting
xℓ(t0) = xℓ
0 =
⎡
⎣
hinit
vinit
βinit
⎤
⎦+ C1/2ηℓ,
ηℓ∼N(0, I3),
where 1 ≤ℓ≤N, and the prior covariance matrix C is assumed to be diagonal. The
prior standard deviations for h, v, β, or diagonal entries of C1/2, are set to
σh = 500 m,
σv = 200 m/s,
σβ = 1 500 kg/ms2.
Furthermore, we assign equal weights wℓ
0 = 1/N to all particles.
Next we outline the details of the algorithm. At each time instance t j, j =
1, . . . , T ,
1. Propagate each particle xℓ
j−1 using the propagation model F,
x ℓ
j = F(xℓ
j−1),
1 ≤ℓ≤N.
2. For every candidate particle, compute the ﬁtness weight,
w ℓ
j = wℓ
j−1 exp

−1
2σ2 |qTx ℓ
j −b j|2

,
and normalize them so as to have the sum equal to one.

12.2 Sampling Importance Resampling (SIR)
261
3. Using the ﬁtness weights as probabilities, draw with replacement N indices
kℓ∈{1, 2, . . . , n}, and generate the new particles by adding innovation,
xℓ
j = x kℓ+ D1/2ξℓ,
ξℓ∼N(0, I3),
where D ∈R3×3 is the covariance matrix of the innovation. We assume that
the innovation covariance matrix is diagonal, and the standard deviations of the
innovation, or the diagonal entries of D1/2, are
σh = 100 m,
σv = 100 m/s,
σβ = 5 kg/ms2.
4. Update the weights, by computing
wℓ
j = exp

−1
2σ2

|(qTxℓ
j −b j|2 −|qTx kℓ
j −b j|2
,
followed by a normalization step.
We run the described algorithm, and for each time step, estimate the particle mean
and standard deviation. Figure 12.9 shows the results, plotted against the generative
target values. We see that the algorithm is capable of identifying the velocity proﬁle
rather well. The ballistic constant estimate shows some drift towards the end of
the estimation period, possibly due to the diminishing sensitivity of the data to the
parameter towards the end of the period, when the velocity decreases.
Fig. 12.9 The average estimated velocity (left panel) and ballistic constant (right panel) are the
black curve, while the red curves are the values in the generative model. The envelopes correspond
to one standard deviation uncertainty

262
12
Dynamic Methods and Learning from the Past
Notes and Comments
As in the case of Markov chain Monte Carlo, the literature on particle ﬁltering
methods is rather extensive. We refer to [28] for a nice overview, and to the article
[53] in the collection [29] which contains a number of further relevant contributions.
The tracking problem at the end of the chapter originates from the book [63].

Chapter 13
Bayesian Filtering for Gaussian Densities
In the previous chapter, we considered dynamic inverse problems where the
posterior density is updated sequentially as new observations become available. The
particle ﬁlter approach is fully general and does not assume anything particular about
the probability densities, as they were approximated by particle-based point mass
distributions. However, if parametric forms of the distributions are known, or if
the distributions can be approximated by parametric ones, computationally efﬁcient
methods for Bayesian ﬁltering are available. In this section, we review methods that
make use of Gaussian densities which, being completely characterized by the mean
and the covariance, allow a natural parametric representation. In particular, we derive
the classical Kalman ﬁltering algorithm, and discuss some generalizations of it.
13.1
Kalman Filtering
Consider an evolution-observation model where we have a stochastic process
X0, X1, . . . taking on values in Rn, and an observation process B1, B2, . . ., taking on
values in Rm, such that
Xt = FXt−1 + Vt,
(13.1)
Bt = AXt + Wt,
(13.2)
where t = 1, 2, . . ., and F ∈Rn×n and A ∈Rm×n. Moreover, we assume that
1. X0 ∼N(x0, D0);
2. Vt ∼N(0, Γt), Wt ∼N(0, Σt);
3. The random variables X0, Vt and Wt are mutually independent.
As in the case of particle ﬁltering, our goal is to compute the conditional proba-
bility densities πXt|B1:t(xt | b1:t), where we use the shorthand notation
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
D. Calvetti and E. Somersalo, Bayesian Scientiﬁc Computing, Applied Mathematical
Sciences 215, https://doi.org/10.1007/978-3-031-23824-6_13
263

264
13
Bayesian Filtering for Gaussian Densities
B1:t = {B1, B2, . . . , Bt},
to indicate the accumulation of the observations up to the current time t.
A well-known result concerning Gaussian random variables states that any linear
combination of Gaussian random variables is also a Gaussian random variable.
This implies that if a matrix is applied to a Gaussian random vector, the outcome is
also Gaussian, and the sum of two Gaussian random vectors is a Gaussian random
vector. The proof of these statements is not difﬁcult, and is left as an exercise. In
particular, it follows from the form of the system (13.1)–(13.2) and the assumptions
about the stochastic processes that the vectors Xt and Bt are normally distributed for
all t.
In the derivation of the updating formulas for the conditional densities, we observe
that since a Gaussian density is completely speciﬁed by its mean and its covariance
matrix, it sufﬁces to ﬁnd an updating formula for the mean and the covariance matri-
ces. As in the case of the particle ﬁlter, we examine the prediction step and the
correction step separately.
We start with the prediction step and the evolution Eq. (13.1). Assume that the
Gaussian densities,
Xt−1 | Bt−1 ∼N(xt−1, Dt−1),
Vt ∼N(0, Γt),
are given, and that Vt is independent of the past history. Based on this information,
we can calculate the mean Xt, conditioned on B1:t−1,
E

Xt | B1:t−1

= FE

Xt−1 | B1:t−1

+ E

Vt | B1:t−1

= Fxt−1.
To calculate the covariance, observe that from the evolution equation,
Xt −E

Xt | B1:t−1

= F(Xt−1 −xt−1) + Vt,
hence, by the independency of Xt−1 and Vt,
E

Xt −E

Xt

Xt −E

Xt
T | B1:t−1

= FE

(Xt−1 −xt−1)(Xt−1 −xt−1)T | B1:t−1

FT + E

VtV T
t

= FDt−1FT + Γt.
We have therefore established that, based on the evolution model alone,
πXt|B1:t−1(xt | b1:t−1) = N(xt | xt, Dt),
(13.3)
where

13.1 Kalman Filtering
265
xt = Fxt−1,
(13.4)
Dt = FDt−1FT + Γt.
(13.5)
Observe that the above formulas do not give any indication how the Gaussian density
depends on the earlier observations, because this information is implicitly contained
in the mean and covariance at the previous time step.
We now consider the correction step, as the new observation Bt = bt arrives.
According to Bayes’ formula,
πXt|Bt(xt | bt) ∝πXt(xt)πBt|Xt(bt | xt).
To condition the density of Xt also on all previous information B1:t−1, observe that the
likelihood of Bt depends only on Xt. Therefore, as in the particle ﬁltering problem,
we can write
πXt|B1:t(xt | b1:t) = πXt|Bt,B1:t−1(xt | bt, b1:t−1)
∝πXt|B1:t−1(xt | b1:t−1)πBt|Xt,B1:t−1(bt | xt, b1:t−1)
= πXt|B1:t−1(xt | b1:t−1)πBt|Xt(bt | xt).
In other words, we consider the conditional density (13.3) as a prior, and derive the
posterior density given the new observation. Since everything is Gaussian, we can
use the explicit expressions for the posterior mean and covariance derived in Chap. 8.
In particular, from (8.11) and (8.10), we have
πXt|B1:t(xt | b1:t) = N(xt | xt, Dt),
where
xt = xt + DtAT
ADtAT + Σt
−1(bt −Axt),
Dt = Dt −DtAT
ADtAT + Σt
−1ADt.
This completes the correction step.
In the classical literature, the vector
Δt = bt −Axt,
expressing how much xt fails to predict the next observation is referred to as mea-
surement residual, and the matrix
Kt = DtAT
ADtAT + Σt
−1
is referred to as the Kalman gain matrix.

266
13
Bayesian Filtering for Gaussian Densities
We are ready now to present the sequential update process in the form of an
algorithm.
Kalman Filtering algorithm: Given x0, the covariance matrices D0, Γt, Σt; the
matrices F and A; and the data vectors b1, . . . , bT .
1. Initialize: Set the counter t = 1.
2. Iterate: While t < T ,
(a) Prediction: Update the predicted mean and covariance,
xt = Fxt−1,
(13.6)
Dt = FDt−1FT + Γt.
(13.7)
(b) Correction: On arrival of new data bt, update the posterior mean and covari-
ance,
xt = xt + DtAT
ADtAT + Σt
−1(bt −Axt),
Dt = Dt −DtAT
ADtAT + Σt
−1ADt.
(c) Advance the counter t →t + 1.
13.2
The Best of Two Worlds: Ensemble Kalman Filtering
The appeal of the Kalman ﬁltering algorithm is its straightforward linear algebraic
nature, and its drawback is that the assumptions about the model are quite restrictive
and not justiﬁed in many real-world problems. Particle methods, on the other hand,
are very intuitive as far as the particle propagation is concerned, but the correction
or analysis step can be quite challenging, and making a particle ﬁltering algorithms
work can require a lot of tuning and testing.
Another issue with particle ﬁltering algorithms is the sample size. In complex
high-dimensional problems, the number of particles needed for a reliable uncertainty
analysis tends to be large, which may be a real bottleneck in applications such as
weather prediction, where the real-time nature of the process poses strict constraints
on computing times.
The idea behind the algorithm that we present in this section is intuitive: Because
of the conceptual clarity of the particle propagation, the prediction step is done as
in particle ﬁltering. The key component of the correction step, on the other hand, is
Bayes’ formula, stating that the posterior is the product of the prior and the likelihood,
hence there are two sources for the uncertainty in the posterior density, namely, the
uncertainty in the prior and the uncertainty in the likelihood. In the case of Gaussian
densities, we can analyze the posterior uncertainty by perturbing the mean of the
prior and the data separately, and analyzing the joint effect of these perturbations on

13.2 The Best of Two Worlds: Ensemble Kalman Filtering
267
the posterior. The technique can be extended approximately to densities not too far
from Gaussians. In this manner, we arrive at an algorithm combining the best of the
particle and the parametric worlds.
Inthefollowing,weformalizetheideasoutlinedabove,startingwiththeprediction
step. Consider the evolution-observation model
Xt = F(Xt−1) + Vt,
Bt = G(Xt) + Wt,
t = 1, 2, . . .
(13.8)
where
Vt ∼N(0, Γt),
Wt ∼N(0, Σt).
(13.9)
Consider a particle sample,
St−1 =

x1
t−1, x2
t−1, . . . , x N
t−1

drawn from the conditional density πXt−1|B1:t−1, where the particles have all equal
weight 1/N. To update the sample according to the evolution model, we perform
the prediction step as in the particle ﬁltering algorithm. For simplicity, we follow the
layered sampling algorithm, and generate the prediction sample

St =

x 1
t ,x 2
t , . . . ,x N
t

,
where
x j
t = F(x j
t−1) + v j
t ,
v j
t ∼N(0, Γt),
1 ≤j ≤N.
Next we calculate the predictive mean and covariance,
xt = 1
N
N

j=1
x j
t ,
Dt =
1
N −1
N

j=1
(x j
t −xt)(x j
t −xt)T.
At this point, if the predictive sample represents a Gaussian distribution, and if
the observation model is linear, we can continue as in Kalman ﬁltering, using the
updating formulas to get a posterior distribution. In the case where at least one of
these conditions is not satisﬁed, we introduce the following modiﬁcation, inspired
by the solution of linear inverse problems.
Consider a linear inverse problem with a Gaussian prior and Gaussian additive
noise,
B = AX + W,
W ∼N(0, Σ),
x ∼N(x, D).
(13.10)
The posterior density, which is proportional to the product of the prior and the
likelihood, is of the form

268
13
Bayesian Filtering for Gaussian Densities
πX|B(x | b) ∝exp

−1
2E (x, b)
	
,
where the Gibbs energy is given by
E (x, b) = ∥Ax −b∥2
Σ + ∥x −x∥2
D.
Consider now the following sampling scheme, called Randomize, then optimize
(RTO) algorithm, to explore the posterior density.
1. For 1 ≤j ≤N, draw two mutually independent realizations of Gaussian random
variables,
η j ∼N(0, Σ),
ν j ∼N(0, D),
(13.11)
and perturb the data and prior mean by deﬁning
b j = b + η j,
x j = x + ν j.
(13.12)
2. Deﬁne
x j = argmin

∥Ax −b j∥2
Σ + ∥x −x j∥2
D

.
(13.13)
It turns out that the algorithm above produces a sample
S = {x1, . . . , x N},
which is distributed according to the posterior density. To prove it, consider the
symmetric decompositions of the precision matrices,
Σ−1 = STS,
D−1 = LTL,
and write the Gibbs energy as
E (x, b) = ∥SAx −Sb∥2 + ∥Lx −Lx∥2 =




SA
L

x −
Sb
Lx




2
.
The minimizer of the Gibbs energy is the least squares solution of the linear system
SA
L

x =
Sb
Lx

,
and also the solution of the normal equations,

(SA)TSA + LTL

x = (SA)TSb + LTLx,
which can be expressed in terms of the covariance matrices as

13.2 The Best of Two Worlds: Ensemble Kalman Filtering
269

ATΣ−1A + D−1
x = ATΣ−1b + D−1x.
Not surprisingly, the solution is the posterior mean estimate,
x =

ATΣ−1A + D−1−1
ATΣ−1b + D−1x

.
(13.14)
Now, instead of solving for the posterior mean, we ﬁrst perturb b and x according to
(13.12) and (13.11): Let ν and η be independent Gaussian random variables,
η ∼N(0, Σ),
ν ∼N(0, D),
and consider the normal equations,

ATΣ−1A + D−1
X = ATΣ−1(b + η) + D−1(x + ν).
(13.15)
The solution X to (13.15) is a random variable, with mean
E

X

= x,
as can be readily veriﬁed by computing the expectation on both sides of Eq. (13.15),
yielding (13.14). Furthermore, by subtracting Eq. (13.14) from (13.15) side by side,
we ﬁnd that

ATΣ−1A + D−1
(X −x) = ATΣ−1η + D−1ν.
To ﬁnd the covariance of X, we ﬁrst solve the above equation for X −x, obtaining
X −x =

ATΣ−1A + D−1−1
ATΣ−1η + D−1ν

= D

ATΣ−1η + D−1ν

,
where
D =

ATΣ−1A + D−1−1.
Then, because of the mutual independency of η and ν, the covariance matrix of X
turns out to be
E

(X −x)(X −x)T
= DE

ATΣ−1η + D−1ν

ATΣ−1η + D−1ν
T
DT
= D

ATΣ−1E

ηηT
Σ−1A + D−1E

ννTD−1
D
= D

ATΣ−1ΣΣ−1A + D−1DD−1
D
= D

ATΣ−1A + D−1
D
= D.

270
13
Bayesian Filtering for Gaussian Densities
In conclusion,
X ∼N(x, D),
which is exactly the posterior distribution corresponding to the problem (13.10), as
claimed.
This result motivates an algorithm where the correction step of the particle ﬁltering
algorithm is replaced by an RTO step, which, in the non-linear and non-Gaussian
case, gives a sample approximation of the posterior, while in the linear Gaussian case
produces an exact sample from the posterior.
We summarize the procedure in the following Ensemble Kalman Filtering (EnKF)
algorithm.
Ensemble Kalman Filtering algorithm: Given the prior density πX0, the
evolution-observation model (13.8) with covariance matrices (13.9), and the data
vectors b1, . . . , bT .
1. Initialize: Draw a sample {x1
0, . . . , x N
0 } from the prior density πX0 by indepen-
dent sampling. Set the counter t = 1.
2. Iterate: While t < T ,
(a) Prediction: Propagate the sample to get the predictive sample
x j
t = F(x j
t−1) + v j
t ,
v j
t ∼N(0, Γt).
(b) Compute an estimate of the predictive covariance matrix,
Dt =
1
N −1
N

j=1
(x j
t −xt)(x j
t −xt)T + αIn,
where
xt = 1
N
N

j=1
x j
t ,
and α > 0 is a small variance inﬂation parameter.
(c) Correction: On arrival of new data, perturb the data, generating an artiﬁcial
sample of data
b j
t = bt + η j
t ,
η j
t ∼N(0, Σt),
1 ≤j ≤N.
(d) For every j, solve the minimization problem
x j
t = argmin

∥b j
t −G(xt)∥2
Σt + ∥xt −x j
t ∥2
Dt

.
(e) Advance the counter t →t + 1.

13.2 The Best of Two Worlds: Ensemble Kalman Filtering
271
The algorithm deserves a few comments. First, observe that in step (b), a small
multiple of the identity matrix is added to the sample-based estimate of the covari-
ance matrix to ensure that the resulting covariance is positive deﬁnite, which is not
guaranteed when the sample is used. This is often the case for high-dimensional
problems, where modest sample sizes yield only a positive semi-deﬁnite matrix.
A second comment is that we add no perturbation to the propagated particles,
since we tacitly assume that they are already distributed around the ensemble mean
according to the empirical covariance.
The ﬁnal comment concerns step (d). If the observation function G is non-linear,
a non-linear optimizer must be used to compute the minimizer, while if the mapping
G is linear, G(x) = Ax for some matrix A, the problem reduces to a least squares
problem, and the formula for the minimizer is essentially that of the posterior mean.
In fact, in this special case,
bt = Axt + wt,
and the analysis step reduces to a least squares problem with the explicit solution,
x j
t = x j
t + Kt

b j
t −Ax j
t

,
j = 1, 2, . . . , N,
(13.16)
where Kt is the Kalman gain matrix
Kt = DtAT 
ADtAT + Σt
−1 .
13.2.1
Adding Unknown Parameters
As in the particle ﬁltering algorithm, the forward propagation model F may depend
on unknown parameters that need to be estimated simultaneously with the state.
Typically, we may have a propagation model
xt = F(xt−1, θ) + Vt,
where θ ∈Rk is a poorly known parameter vector. Even if we assume that the param-
eter θ is static, we model it as a the time-dependent random variable Θt, the time
dependency reﬂecting only the change in our information about its value, and intro-
duce a propagation model for an extended variable Zt,
Zt =
 Xt
Θt

=
 F(Xt−1, Θt−1)
Θt−1

= F(Xt−1).
To understand how the EnKF algorithm learns about the parameter values, consider
the special case in which the data consist of direct observation of some components
of the state vector Xt, as is sometimes the case in chemical kinetics, where the

272
13
Bayesian Filtering for Gaussian Densities
concentrations of one or two substances that participate in the reaction are measured.
Thus, the observation model is given by
Bt = PXt + Wt =
P Om×k

Zt + Wt,
where P is a projection matrix onto the observed components, obtained by deleting
all rows that correspond to non-observed components from the identity matrix In.
By partitioning the predictive covariance matrix Dt ∈R(n+k)×(n+k) of the extended
variable Zt as
Dt =
Dxx Dxθ
Dθx Dθθ

∈R(n+k)×(n+k),
the Kalman gain matrix becomes
Kt =
Dxx
Dθx

PT(PDxxPT + Σt)−1.
The block partitioning of the Kalman gain reveals that the algorithm learns about the
parameter vector θ through the correlation between the variables Xt and Θt, since
the updating of the components corresponding to Θt is nonzero only if the parameter
is correlated with the state vector.
We close this chapter with a computed example elucidating the ideas of the EnKF
algorithm.
Example 13.1 Consider a simple example of a reversible chemical reaction,
A
k1⇌
k2 B,
where k1 and k2 are reaction rates. Assuming that the reactions satisfy the mass action
model, the concentrations c1 and c2 of the substances A and B, respectively, satisfy
the system
dc1
dt = −k1c1 + k2c2,
dc2
dt = −k1c1 −k2c2.
Given the initial values
c1(0) = cA,
c2(0) = cB,
we may write the solution to the initial value problem explicitly as
c1(t; k1, k2) = α + βe−t/τ,
c2(t; k1, k2) = αδ −βe−t/τ,

13.2 The Best of Two Worlds: Ensemble Kalman Filtering
273
where the time constant τ is given by
τ =
1
k1 + k2
,
and the coefﬁcients α and β are given by
α = cA + cB
1 + δ ,
β = δcA −cB
1 + δ
,
δ = k1
k2
.
We consider the following problem: Assuming that the initial values cA and cB are
measured with a given accuracy, we want to estimate the parameters k1 and k2 from
noisy observations of the concentration of the substance A,
b j = c1(t j; k1, k2) + e j,
1 ≤j ≤m,
where 0 < t1 < . . . < tm. For simplicity, we will assume that e j is a realization of a
random variable E j ∼N(0, σ2), and that the variables E j are mutually independent.
At ﬁrst, we assume that both concentrations cA and cB are measured with the
same precision as the data. Furthermore, we assume that the parameters satisfy the
a priori bound constraints,
k j,min ≤k j ≤k j,max,
j = 1, 2.
We introduce the combined state and parameters vector
x =
⎡
⎢⎢⎣
x1
x2
x3
x4
⎤
⎥⎥⎦=
⎡
⎢⎢⎣
c1
c2
k1
k2
⎤
⎥⎥⎦,
and deﬁne the propagation model
dx
dt = F(x) =
⎡
⎢⎢⎣
−x3x1 + x4x2
x3x1 −x4x2
0
0
⎤
⎥⎥⎦.
(13.17)
The observation model in terms of x is
b j = Ax(t j) + e j,
A =
1 0 0 0 
.
We generate the data using the generative values
c∗
A = 2,
c∗
B = 1,
k∗
1 = 2,
k∗
2 = 0.5

274
13
Bayesian Filtering for Gaussian Densities
Fig. 13.1 The concentration
time courses of the two
substances corresponding to
the generative model. The
simulated data are indicated
in the ﬁgure by the dots. The
noise level, given in terms of
the standard deviation of the
normal distribution, is
σ = 0.05
at m = 12 time instances, and corrupted by additive Gaussian scaled white noise
with σ = 0.05. The noiseless curves c1(t) and c2(t) with the noisy data are shown
in Fig. 13.1.
To start the EnKF algorithm, we choose the ensemble size to be N = 5 000, and
generate the initial particle ensemble as follows: Assume that the measured initial
values are
cA = 2.1,
cB = 0.9.
We deﬁne the initial particles xℓ
0 component-wise,
xℓ
0,1 = cA + σwℓ
1,
xℓ
0,2 = cB + σwℓ
2,
where
wℓ
1, wℓ
2 ∼N(0, 1),
reﬂecting the fact that the measurement precision is the same as in the process of
collecting data. Further, we set
xℓ
0,3 = k1,min +

k1,max −k1,min

wℓ
3,
xℓ
0,4 = k2,min +

k2,max −k2,min

wℓ
4,
where
wℓ
3, wℓ
4 ∼Uniform

[0, 1]

,
and assume that
k1,min = 0.5,
k1,max = 3,
k2,min = 0.1,
k2,max = 1.5.
ConsidernowtheEnKFiteration.Inordertokeepthenotationsaslightaspossible,
weadoptthefollowingconvention:Inthepropagationstep, xℓreferstotheℓthparticle
at time t j−1, and x ℓto the prediction at time t j. Likewise, in the correction step, we

13.2 The Best of Two Worlds: Ensemble Kalman Filtering
275
write for brevity x ℓfor x ℓ
j , and the new particles after the correction step overwrite
the old xℓ.
With this notation, we denote the current particle ensemble at time t j−1 by

x1, x2, . . . , x N
,
xℓ∈R4.
For the prediction step, we propagate the ensemble by solving the system (13.17)
with initial values x(ti−1) = xℓ, either by using a numerical integrator or the analytic
expressions,
x ℓ
1 = αℓ+ βℓe−(ti−ti−1)/τ ℓ,
x ℓ
2 = α jδℓ−βℓe−(ti−ti−1)/τ ℓ,
x ℓ
3 = xℓ
3,
x ℓ
4 = xℓ
4,
where
τ ℓ=
1
xℓ
3 + xℓ
4
,
δℓ= xℓ
3
xℓ
4
,
and
αℓ= xℓ
1 + xℓ
2
1 + δℓ,
βℓ= δℓxℓ
1 −xℓ
2
1 + δℓ
.
This way, we obtain the predictive ensemble

x 1,x 2, . . . ,x N
,
x ℓ∈R4.
(13.18)
Observe that here we trust the forward model, and therefore do not add any random
innovation.
At t = t j, the new data b = b j arrives, and the predictions need to be updated. To
this end, we compute the ensemble meanx and covariance D ∈R4×4 of the predictive
ensemble (13.18). We generate the artiﬁcial data cloud,
bℓ= b + σvℓ,
vℓ∼N(0, 1),
and update the predictive cloud. Since our observation model is linear, the updating
can be made by using the formula (13.16). In the present case, the observation is a
scalar, and therefore

ATDA + Σ)−1 =
1
d11 + σ2 ,
DAT = d1,
where d1 ∈R4 is the ﬁrst column of the matrix D and d11 is its ﬁrst component.
Therefore, the updating simpliﬁes to

276
13
Bayesian Filtering for Gaussian Densities
xℓ= x ℓ+ bℓ−x ℓ
1
d11 + σ2 d1.
In Fig. 13.2, the results have been summarized by plotting the ensemble predictive
envelopes: At each observation time t j, we compute the ensemble mean, and for a
given percentage p, we ﬁnd an interval containing p% of the particle values, simply
by discarding (100 −p)/2 largest and smallest particle values. For visualization
purposes, the upper and lower bounds of the intervals are connected to form an
envelope. The ﬁgure shows the envelopes of p = 90 and p = 75.
The results show that the EnKF algorithm manages to identify the concentration
of B with an accuracy in line with the observation error. Furthermore, we notice that
after the system reaches near equilibrium, the parameter estimates do not improve,
and, in fact, their estimation error seems to increase. One explanation for this may be
that after the system reaches equilibrium, the solution c1(t) loses exponentially fast
its dependency on the time constant τ, depending only on the ratio δ of the reaction
rates. A disturbing feature in the results is the negativity of some of the particle
components corresponding to the parameter k2. The negative values are the result
of applying the Kalman ﬁltering formula in the correction step. One way to avoid
negative values for a one that is known to be positive is to deﬁne the state vector
using a logarithmic transformation, and deﬁne
x =
⎡
⎢⎢⎣
x1
x2
x3
x4
⎤
⎥⎥⎦=
⎡
⎢⎢⎣
c1
c2
log k1
log k2
⎤
⎥⎥⎦.
Figure 13.3 shows the results with this modiﬁcation. The negative particle values are
eliminated, and the estimated values are closer to the generative values.
Finally, recall that in the correction step of the EnKF algorithm, the unobserved
components are updated from the data on the basis of the correlation between the
observed and unobserved quantities. Therefore, it is instructive to look at the cor-
relation between these variables by using the particle sample. Figure 13.4 displays
scatter plots of the three unobserved quantities against the observed one. These plots
give a good idea how uncertainties in the observations propagate to uncertainties in
the unobserved quantities.
Notes and Comments
The idea of Kalman ﬁltering is usually attributed to Rudolf Kálmán [50] and Ruslan
Stratonovich [73], although the full history is much richer. The word “ﬁltering”
refers to the original use of the methodology for signal denoising. The engineering
literature on Kalman ﬁltering is vast due to its extensive use in signal processing.

13.2 The Best of Two Worlds: Ensemble Kalman Filtering
277
Fig. 13.2 Left panel: 75% (darker) and 90% (lighter) credibility envelopes of the concentrations.
The values between observation points are plotted by linear interpolation of the credibility intervals.
Right panel: The corresponding envelopes of the parameters k1 and k2. The generative values are
indicated by the dashed curves
Fig. 13.3 This ﬁgure shows the same quantities as Fig. 13.2, when the correction step is computed
by using logarithms of k1 and k2 as particle components to avoid negative values
Fig. 13.4 Scatter plot of xℓ
1 versus xℓ
k with k = 2 (left), k = 3 (center) and k = 4 (right), corre-
sponding to the last time instance. The parameter values xk, 2 ≤k ≤4, are based on the correlation
with the observed variable x1

278
13
Bayesian Filtering for Gaussian Densities
Extensions of Kalman ﬁltering include the Extended Kalman ﬁltering (EKF), where
non-linearities are locally linearized, and the Kalman ﬁltering is used iteratively.
The methodology is extensively used in navigation and GPS. Ensemble Kalman
ﬁltering was developed in the framework of data assimilation, with applications to
meteorology and oceanography where the forward models tend to be so complex
that large particle ensembles are impossible to use [31]. The concept of Randomize,
then optimize (RTO) that we used as a motivation for the analysis step in EnKF goes
back to the article [2]. An alternative approach combining ideas from linear ﬁltering
and particle ﬁltering to reduce the ensemble size is the Unscented Kalman ﬁltering
(UKF), where the covariance in the analysis step is approximated by propagating
a small ensemble of cleverly chosen particles that in the Gaussian case would be
sufﬁcient for the full recovery of the posterior covariance matrix [47, 48].

References
1. Anderson DF, Seppäläinen T and Valkó B (2017) Introduction to probability. Cambridge Uni-
versity Press
2. Bardsley JM, Solonen A, Haario H and Laine M (2014) Randomize-then-optimize: A method
for sampling from posterior distributions in nonlinear inverse problems. SIAM Journal on
Scientiﬁc Computing 36: A1895–A1910
3. Bayes T (1763) An Essay towards solving a Problem in the Doctrine of Chances. Philosophical
Transactions of the Royal Society of London 53: 370–418
4. Berger J (2006) The case for objective Bayesian analysis. Bayesian Analysis 1: 385–402
5. Billingsley P (2012) Probability and measure. Anniversary edition, John Wiley & Sons
6. Brooks S, Gelman A, Jones G and Meng XL (eds.) (2011) Handbook of Markov chain Monte
Carlo. CRC press
7. Buhmann MD (2003) Radial basis functions: theory and implementations. Cambridge Univer-
sity Press
8. Calvetti D (2007) Preconditioned iterative methods for linear discrete ill-posed problems from
a Bayesian inversion perspective. Journal of Computational and Applied Mathematics 198:
378–395
9. Calvetti D, Hakula H, Pursiainen S and Somersalo E (2009) Conditionally Gaussian hyper-
models for cerebral source localization. SIAM Journal on Imaging Science 2: 879–909
10. Calvetti D, Kaipio JP and Somersalo E (2006) Aristotelian prior boundary conditions. Inter-
national Journal of Mathematics and Computer Science 1: 63–81
11. Calvetti D, Pascarella A, Pitolli F, Somersalo E and Vantaggi B (2015) A hierarchical Krylov-
Bayes iterative inverse solver for MEG with physiological preconditioning. Inverse Problems
31: 125005
12. Calvetti, D, Pascarella A, Pitolli F, Somersalo E. and Vantaggi B (2019) Brain Activity Mapping
fromMEGDataviaaHierarchicalBayesianAlgorithmwithAutomaticDepthWeighting.Brain
Topography 32: 363–393
13. Calvetti D, Pitolli F, Somersalo E and Vantaggi B (2018) Bayes meets Krylov: Statistically
inspired preconditioners for CGLS. SIAM Review 60: 429–461
14. Calvetti D, Pragiola M, Somersalo E and Strang A (2020). Sparse reconstructions from few
noisy data: analysis of hierarchical Bayesian models with generalized gamma hyperpriors.
Inverse Problems 36: 025010
15. Calvetti D, Pragliola M and Somersalo E (2020) Sparsity promoting hybrid solvers for hierar-
chical Bayesian inverse problems. SIAM Journal on Scientiﬁc Computing 42: A3761–A3784
© The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer
Nature Switzerland AG 2023
D. Calvetti and E. Somersalo, Bayesian Scientiﬁc Computing, Applied Mathematical
Sciences 215, https://doi.org/10.1007/978-3-031-23824-6
279

280
References
16. Calvetti D and Somersalo E (2005) Statistical elimination of boundary artefacts in image
deblurring. Inverse Problems 21: 1697
17. Calvetti D and Somersalo E (2005) Priorconditioners for linear systems. Inverse problems 21:
1397
18. Calvetti D and Somersalo E (2007) Introduction to Bayesian Scientiﬁc Computing – Ten Lec-
tures on Subjective Computing. Springer Verlag, New York
19. Calvetti D and Somersalo E (2008) Hypermodels in the Bayesian imaging framework. Inverse
Problems 24: 034013
20. Calvetti D and Somersalo E (2018) Inverse problems: From regularization to Bayesian infer-
ence. Wiley Interdisciplinary Reviews: Computational Statistics 10: e1427
21. Calvetti D and Somersalo E (2022) The less is more linear algebra of vector spaces and
matrices. SIAM, Philadelphia.
22. Calvetti D, Somersalo E and Strang A (2019) Hierarchical Bayesian models and sparsity:
ℓ2-magic. Inverse Problems 35: 035003 (26pp)
23. Candès EJ, Romberg J and Tao T (2006) Robust uncertainty principles: Exact signal recon-
struction from highly incomplete frequency information. IEEE Transactions on Information
Theory 52: 489–509
24. Chen SS, Donoho DL and Saunders MA (2001) Atomic decomposition by basis pursuit. SIAM
Rev 43: 129–159
25. Cotter SL, Roberts GO, Stuart AM and White D (2013) MCMC methods for functions: modi-
fying old algorithms to make them faster. Statistical Science 28: 424–446
26. De Finetti B (2017) Theory of probability: A critical introductory treatment (Vol. 6). John
Wiley & Sons
27. Donoho DL (2006) Compressed sensing. IEEE Transactions on Information Theory 52: 1289–
1306
28. Doucet A and Johansen AM (2009) A tutorial on particle ﬁltering and smoothing: Fifteen years
later. Handbook of nonlinear ﬁltering, 12: 656–704
29. Doucet A, De Freitas N and Gordon NJ (2001) Sequential Monte Carlo methods in practice
(Vol. 1, No. 2). Springer, New York
30. Duane S, Kennedy AD, Pendleton BJ and Roweth D (1987) Hybrid Monte Carlo. Phys letters
B 195: 216–222
31. Evensen G (2009) Data assimilation: the ensemble Kalman ﬁlter (Vol. 2) Springer, Berlin
32. Fletcher R (1987) Practical methods of optimization (2nd ed.) John Wiley & Sons, New York
33. Gelfand AE and Smith AFM (1990) Sampling based approaches to calculating marginal den-
sities. Journal of the American Statistical Association 85:398–409
34. Geman S and Geman D (1984) Stochastic relaxation, Gibbs distributions and the Bayesian
restoration of images. IEEE Transactions on Pattern Analalysis and Machine Intelligence
6:721–741
35. Ghahramani S (1996) Fundamentals of Probability. Prentice Hall
36. Gikhman II and Skorokhod AV (2004 The theory of stochastic processes (Reprint of the 1974
edition). Springer Science & Business Media.
37. Marques EC, Maciel N, Naviner L, Cai H and Yang J (2018) A review of sparse recovery
algorithms. IEEE access 7: 1300–1322
38. Gilks WR, Richardson S and Spiegelhalter DJ (1996) Markov Chain Monte Carlo in Practice.
Chapmann & Hall
39. Haario H, Saksman E and Tamminen J (2001) An adaptive Metropolis algorithm. Bernoulli 7
223–242
40. Hairer M, Stuart AM and Vollmer SJ (2014) Spectral gaps for a Metropolis-Hastings algorithm
in inﬁnite dimensions. The Annals of Applied Probability 24: 2455–2490
41. Hanke M and Hansen PC (1993) Regularization methods for large-scale problems. Surveys in
Mathematics in Industry 3: 253–315
42. Hansen PC (2013) Oblique projections and standard-form transformations for discrete inverse
problems. Numerical Linear Algebra with Applications 20: 250–258

References
281
43. Hastings WK (1970) Monte Carlo sampling methods using Markov chains and their applica-
tions. Biometrika 57: 97–109
44. Hestenes MR and Stiefel E (1952) Methods of Conjugate Gradients for Solving. Journal of
research of the National Bureau of Standards 49: 409–436
45. Hoffmann K and Kunze RA (1971) Linear algebra. Prentice-Hall, New Jersey
46. Jeffrey R (2004) Subjective probability: The real thing. Cambridge University Press
47. Julier SJ and Uhlmann JK (1997) New extension of the Kalman ﬁlter to nonlinear systems.
Signal processing, sensor fusion, and target recognition VI 3068: 182–193
48. Julier SJ and Uhlmann JK (2004) Unscented ﬁltering and nonlinear estimation. Proceedings
of the IEEE 92: 401–422
49. Kaipio J and Somersalo E (2004) Statistical and Computational Inverse Problems. Springer
Verlag, New York
50. Kalman RE (1960) A new approach to linear ﬁltering and prediction problems. ASME Journal
of Basic Engineering 82D: 35–45
51. Kimeldorf GS and Wahba G (1970) A correspondence between Bayesian estimation on stochas-
tic processes and smoothing by splines. The Annals of Mathematical Statistics 41: 495–502
52. Liu JS (2003) Monte Carlo strategies in scientiﬁc computing. Springer NewYork Berlin Hei-
delberg
53. LiuJandWestM(2001)Combinedparameterandstateestimationinsimulation-basedﬁltering.
In: Sequential Monte Carlo methods in practice (pp. 197–223), Springer, New York
54. Matérn B (1960) Spatial variation. Meddelanden frøan Statens Skogsforsknigsinstitut 49: 1–
144
55. Metropolis N, Rosenbluth AW, Rosenbluth MN, Teller AH and Teller E (1953) Equations of
state calculations by fast computing machine. Journal of Chemical Physics 21: 1087–1091
56. Meyer CD (2000) Matrix analysis and applied linear algebra. SIAM, Philadelphia
57. Natterer F (2001) The mathematics of computerized tomography Society for Industrial and
Applied Mathematics, Philadelphia
58. Neal RM (2011) MCMC using Hamiltonian dynamics. In: Brooks et al. Handbook of Markov
chain Monte Carlo
59. Nummelin E (2002) MC’s for MCMC’ists. International Statistical Review 70: 215–240
60. Persson PO and Strang G (2004) A simple mesh generator in MATLAB. SIAM Review 46:
329–345
61. Pragliola M, Calvetti D and Somersalo E (2022) Overcomplete representation in a hierarchical
Bayesian framework. Inverse Problems and Imaging 16: 19–38
62. Riesz F and Sz.-Nagy B (1990) Functional analysis. Dover, New York
63. Ristic B, Arulampalam S and Gordon N (2004) Beyond the Kalman ﬁlter. Artech House,
Boston-London
64. Roberts GO and Tweedie RL (1996) Exponential convergence of Langevin distributions and
their discrete approximations. Bernoulli 2: 341–363
65. Rossi RJ (2018) Mathematical Statistics : An Introduction to Likelihood Based Inference. John
Wiley & Sons, New York
66. Roininen L, Huttunen J M and Lasanen S (2014). Whittle-Matérn priors for Bayesian statistical
inversion with applications in electrical impedance tomography. Inverse Problems Imaging 8:
561–586
67. Roininen L, Lehtinen M, Lasanen S, Orispää M and Markkanen M (2011) Correlation priors.
Inverse Problems and Imaging 5: 167–184
68. Rudin W (1987) Real and complex analysis (3rd edition). McGraw-Hill, New York
69. Saad Y (2003) Iterative methods for sparse linear systems. SIAM, Philadelphia
70. Stigler SM (1986) Laplace’s 1774 Memoir on Inverse Probability, Statistical Science 1: 359–
363
71. Strang G (2006) Linear algebra and its applications Thomson, Brooks/ColemBelmont, CA
Strang G (1993) Introduction to linear algebra . Wellesley-Cambridge Press, Wellesley, MA
72. Bulirsch R and Stoer, J (2002) Introduction to numerical analysis (Vol. 3). Springer Verlag,
Heidelberg

282
References
73. Stratonovich RL (1960) Application of the Markov processes theory to optimal ﬁltering. Radio
Engineering and Electronic Physics 5: 1–19
74. Tarantola A (2005) Inverse problem theory and methods for model parameter estimation.
SIAM, Philadelphia
75. Tarantola A and Valette B (1982) Inverse problems = quest for information, Journal of Geo-
physics 50: 159–170
76. Tibshirani R (1996) Regression shrinkage and selection via the lasso. Journal of the Royal
Statistical Society: Series B (Methodological) 58 267–288
77. Tihonov AN (1963) On the solution of ill-posed problems and the method of regularization,
Doklady Akademii Nauk SSSR 151: 501–504 (Russian)
78. Tihonov AN (1963) On the regularization of ill-posed problems. Doklady Akademii Nauk
SSSR textbf153: 49–52 (Russian)
79. Tikhonov AN and Arsenin VY (1977) Solutions of ill-posed problems. VH Winston & Sons
80. Trefethen LN and Bau III D (1997) Numerical linear algebra. SIAM, Philadelphia
81. Van Loan CF (1976) Generalizing the singular value decomposition, SIAM Journal on Numer-
ical Analysis 13: 76–83
82. West M (1993) Approximating posterior distributions by mixtures. Journal of the Royal Sta-
tistical Society 55: 409–422
83. Whittle P (1954) On stationary processes in the plane. Biometrika 41: 434–449

Index
A
ATA-conjugate direction, 160
Absolutely continuous, 8
Acceptance rate, 75, 226
Acceptance ratio, 77, 223
A-conjugate direction, 158
α–coin, 77, 223, 240
Approximate solution, 156
Autocorrelation, 228
Averaging, 49
B
Balance equation, 221
Bandwidth, 90
Basis, 22
Bayes’ formula, 7
for distributions, 10
Bayesian ﬁltering, 250
Bayesian inference, 1
Bayesian probability, 3
Beer–Lambert law, 207
Burn-in, 225
C
Candidate generating kernel, 222
Cardinality of support of a vector, 113
Cardinality of the support, 192
Central Limit Theorem, 40, 227
CG algorithm, 158
CGLS algorithm, 161
Chapman–Kolmogorov formula, 250
Charged Coupled Device (CCD), 44
Cholesky factorization, 30
Conditional covariance, 138
Conditional expectation„ 13
Conditional mean, 13, 138
Conditional probability, 6
Conditional probability density, 10, 92
Conditioning, 10, 82
Condition number, 23, 162
Conjugate Gradient method, 157
Conjugate
Gradient
method
for
Least
Squares, 159
Continuity correction, 47
Convergence criterion, 156
Convergence rate, 167
Correlation length, 106, 228
Counting process, 44
Covariance matrix, 12
Credibility envelopes, 101
Cumulative distribution function, 8
Curse of dimensionality, 58
D
Data thinning, 72
Decibel, 191
Degenerate random variable, 36
Detailed balance equation, 221
Dimension, 22
Dirac’s delta function, 43
Directed link, 212
Direct sum, 26
Discrepancy, 28, 156, 161
Discrete time stochastic process, 214
© The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer
Nature Switzerland AG 2023
D. Calvetti and E. Somersalo, Bayesian Scientiﬁc Computing, Applied Mathematical
Sciences 215, https://doi.org/10.1007/978-3-031-23824-6
283

284
Index
E
Eigenvalue decomposition, 52, 170
Eigenvalue factorization, 31
Empirical covariance, 51
Empirical mean, 51
Equiprobability curves, 52, 53
Error function, 65
Error function, inverse of, 65
Event, 6
Event space, 6
Evolution-observation model, 249
Expectation, 10
Expectation, Poisson distribution, 45
Expected value, 11
F
Fan beam tomography, 208
Fat-tailed density, 111
Fitness score, 253
Formally determined linear system, 27
Frequentist statistics, 3
Fundamental subspaces, 26
G
Gamma distribution, 111
Gaussian approximation, 90
Gaussian distribution, 37
Gaussian mixture, 70
Gaussian model, 53
Gaussian white noise, 54
Gibbs energy, 186, 268
Golden rule, 64
H
Heaviside function, 58
Hierarchical prior model, 111
Hit and Run algorithm, 237
Hyperparameter, 112
I
Identity matrix, 22
Ill-conditioning, 162
Importance weight, 71
Independent events, 6
Independent, identically distributed (i.i.d.),
40, 87
Independent random variables, 9
Inner product, 20
Innovation, 249, 265
Interpolation, 141, 147
Invariant density, 219
Invariant probability vector, 215
Inverse crime, 174
Inverse cumulative distribution rule, 64
Inverse gamma distribution, 111
Inverse of a matrix, 22
Inverse problem, 84
Iterates, 156
Iterative solver, 155
J
Jacobian, 17
Jacobian determinant, 17
Joint probability density, 8, 82
K
Kalman gain, 265
Kriging, 141
Krylov subspace, 156
Krylov subspace iterative methods, 157, 168
Kurtosis, 12
L
Laplace distribution, 112
Law of Large Numbers, 50, 51
Layered sampling, 252
Least squares solution, 28
Leptokurtic density, 111
Likelihood, 83
Linear independency, 22
Linear system, 137, 167
Log-normal distribution, 87
LU decomposition, 29
M
Mahalanobis transformation, 54
Marginal density, 9
Marginalization, 82
Marginal variance, 101
Markov chain, 211, 217
Markov chain Monte Carlo, 211
Markov process, 214
Markov property, 217, 250
Matlab code, 65, 225
Matrix, 20
Matrix free methods, 155
Matrix-matrix product, 21
Matrix-vector product, 20
Maximum A Posteriori (MAP) estimate, 130

Index
285
Maximum Likelihood (ML) estimator, 93,
127
Mean value, 11
Measurable function, 7
Measurement residual, 265
Mercer’s theorem, 118
Metropolis-Hastings, 74
Minimization problem, 157, 159, 160, 169
Minimum norm solution, 28
Model discrepancy, 174
Moment, kth, 12
Monte Carlo integration, 61
Morozov’s discrepancy principle, 129, 166
Multivariate random variable, 9
N
Node of a network, 212
Noise, additive, 84
Noise, multiplicative, 91
Noise, Poisson, 91
Normal distribution, 37
Normal equations, 32, 159
Nuisance parameter, 83
O
Observation, 249
Observation process, 249
Orthogonal matrix, 23
Orthogonal vectors, 20
Outer product, 21
Overdetermined linear system, 27
P
Paris, 81
Penalty term, 170
Point mass function, 42
Poisson density, 90
Poisson parameter, 44
Poisson process, 44, 90
Posterior covariance, 140
Posterior distribution, 125
Posterior mean, 140
P-P plot, 56
Precision matrix, 66, 99, 136, 144
Preconditioner, left (right), 167
Predictive envelope, 276
Priorconditioner, 174, 175
Prior information, 82
Probability density, 8
Probability distribution, 7
Probability measure, 6
Probability vector, 213
Proposal distribution, 76, 222
Pseudoinverse, 33
Q
QR factorization, 32
Quasi-MAP (qMAP) estimate, 175
R
Radial basis functions, 120
Randomize, Then Optimize (RTO), 268
Random variable, 2, 7
Random walk, 216
Rank of a matrix, 26
Reality, 84
Realization of a random variable, 7
Regularization, 129
Regularization by early stopping, 165
Regularization functional, 129
Regularization parameter, 129
Rejection sampling algorithm, 76
Residual error, 28, 156
Residual error of the normal equations, 160
S
Sample covariance, 51
Sample history, 228
Sample impoverishment, 72
Sample mean, 51
Sample space, 6
Scale parameter, 112
Scatter matrix, 232
Schur complement, 137, 138
Scientiﬁc computing, 1
Search direction, 158
Semiconvergence, 162
Sensitivity weighting, 194
Shape parameter, 112
Sherman–Morrison–Woodbury identity, 145
Short exact chain, 197
Signal-to-Noise Ratio (SNR), 191
Singular value decomposition, 23
Singular values, 23
Skewness, 12
SNR-exchangeability, 193
Spanning vectors, 22
Sparse coding, 183
Sparsity, 113
Standard deviation, 12
Standard normal random variable or distri-
bution, 39

286
Index
Stochastic process, 217, 249
Stopping rule, 166
Subjective probability, 3
Support of a vector, 113
Symmetric positive deﬁnite, 30
Symmetric positive deﬁnite kernel, 117
Symmetric positive semi-deﬁnite, 29
T
Tikhonov regularization, 130, 169
Time invariant kernel, 217
Toeplitz matrix, 90
Transition kernel, 216
Transition matrix, 212
Transpose of a matrix, 21
Transpose of a vector, 19
U
Unbiased covariance estimator, 51, 52
Underdetermined linear system, 27
Uniform distribution, 63
V
Variance, 11
Variance, Poisson distribution, 45
W
Whitening, 53
Whitening transformation, 54
Whittle–Matérn prior, 106
X
X-ray tomography, 207


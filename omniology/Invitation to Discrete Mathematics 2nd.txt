
Invitation to Discrete Mathematics

“Only mathematicians could appreciate this work . . .”
Illustration by G.Roux from the Czech edition of Sans dessus dessous by
Jules Verne, published by J.R. Vil´ımek, Prague, 1931 (English title: The
purchase of the North Pole).

Invitation to Discrete Mathematics
Jiˇr´ı Matouˇsek
Jaroslav Neˇsetˇril
2nd edition.
1

3
Great Clarendon Street, Oxford OX2 6DP
Oxford University Press is a department of the University of Oxford.
It furthers the University’s objective of excellence in research, scholarship,
and education by publishing worldwide in
Oxford New York
Auckland Cape Town Dar es Salaam Hong Kong Karachi
Kuala Lumpur Madrid Melbourne Mexico City Nairobi
New Delhi Shanghai Taipei Toronto
With oﬃces in
Argentina Austria Brazil Chile Czech Republic France Greece
Guatemala Hungary Italy Japan Poland Portugal Singapore
South Korea Switzerland Thailand Turkey Ukraine Vietnam
Oxford is a registered trade mark of Oxford University Press
in the UK and in certain other countries
Published in the United States
by Oxford University Press Inc., New York
c
⃝Jiˇr´ı Matouˇsek and Jaroslav Neˇsetˇril 2008
The moral rights of the authors have been asserted
Database right Oxford University Press (maker)
First Published 2008
All rights reserved. No part of this publication may be reproduced,
stored in a retrieval system, or transmitted, in any form or by any means,
without the prior permission in writing of Oxford University Press,
or as expressly permitted by law, or under terms agreed with the appropriate
reprographics rights organization. Enquiries concerning reproduction
outside the scope of the above should be sent to the Rights Department,
Oxford University Press, at the address above
You must not circulate this book in any other binding or cover
and you must impose the same condition on any acquirer
British Library Cataloguing in Publication Data
Data available
Library of Congress Cataloging in Publication Data
Data available
Typeset by Newgen Imaging Systems (P) Ltd., Chennai, India
Printed in Great Britain
on acid-free paper by
Biddles Ltd., Kings Lynn, Norfolk
ISBN 978–0–19–857043–1
ISBN 978–0–19–857042–4 (pbk)
1 3 5 7 9 10 8 6 4 2

Preface to the second edition
This is the second edition of Invitation to Discrete Mathematics.
Compared to the ﬁrst edition we have added Chapter 2 on partially
ordered sets, Section 4.7 on Tur´an’s theorem, several proofs of the
Cauchy–Schwarz inequality in Section 7.3, a new proof of Cayley’s
formula in Section 8.6, another proof of the determinant formula for
counting spanning trees in Section 8.5, a geometric interpretation of
the construction of the real projective plane in Section 9.2, and the
short Chapter 11 on Ramsey’s theorem. We have also made a number
of smaller modiﬁcations and we have corrected a number of errors
kindly pointed out by readers (some of the errors were corrected in
the second and third printings of the ﬁrst edition). So readers who
decide to buy the second edition instead of hunting for a used ﬁrst
edition at bargain price should rest assured that they are getting
something extra. . .
Prague
J. M.
November 2006
J. N.

This page intentionally left blank 

Preface
Why should an introductory textbook on discrete mathematics have
such a long preface, and what do we want to say in it? There are many
ways of presenting discrete mathematics, and ﬁrst we list some of the
guidelines we tried to follow in our writing; the reader may judge
later how we succeeded. Then we add some more technical remarks
concerning a possible course based on the book, the exercises, the
existing literature, and so on.
So, here are some features which may perhaps distinguish this
book from some others with a similar title and subject:
• Developing mathematical thinking.
Our primary aim, besides
teaching some factual knowledge, and perhaps more importantly
than that, is to lead the student to understand and appreciate
mathematical notions, deﬁnitions, and proofs, to solve problems
requiring more than just standard recipes, and to express math-
ematical thoughts precisely and rigorously. Mathematical habits
may give great advantages in many human activities, say in pro-
gramming or in designing complicated systems.1 It seems that
many private (and well-paying) companies are aware of this.
They are not really interested in whether you know mathemat-
ical induction by heart, but they may be interested in whether
you have been trained to think about and absorb complicated
concepts quickly—and mathematical theorems seem to provide
a good workout for such a training. The choice of speciﬁc mat-
erial for this preparation is probably not essential—if you’re en-
chanted by algebra, we certainly won’t try to convert you to
combinatorics! But we believe that discrete mathematics is esp-
ecially suitable for such a ﬁrst immersion into mathematics, since
the initial problems and notions are more elementary than in
analysis, for instance, which starts with quite deep ideas at the
outset.
1On the other hand, one should keep in mind that in many other human
activities, mathematical habits should better be suppressed.

viii
Preface
• Methods, techniques, principles. In contemporary university cur-
ricula, discrete mathematics usually means the mathematics of
ﬁnite sets, often including diverse topics like logic, ﬁnite aut-
omata, linear programming, or computer architecture. Our text
has a narrower scope; the book is essentially an introduction to
combinatorics and graph theory. We concentrate on relatively
few basic methods and principles, aiming to display the rich var-
iety of mathematical techniques even at this basic level, and the
choice of material is subordinated to this.
• Joy.
The book is written for a reader who, every now and
then, enjoys mathematics, and our boldest hope is that our
text might help some readers to develop some positive feelings
towards mathematics that might have remained latent so far.
In our opinion, this is a key prerequisite: an aesthetic pleasure
from an elegant mathematical idea, sometimes mixed with a tri-
umphant feeling when the idea was diﬃcult to understand or
to discover. Not all people seem to have this gift, just as not
everyone can enjoy music, but without it, we imagine, studying
mathematics could be a most boring thing.
• All cards on the table. We try to present arguments in full and
to be mathematically honest with the reader. When we say that
something is easy to see, we really mean it, and if the reader
can’t see it then something is probably wrong—we may have
misjudged the situation, but it may also indicate a reader’s prob-
lem in following and understanding the preceding text. When-
ever possible, we make everything self-contained (sometimes we
indicate proofs of auxiliary results in exercises with hints), and
if a proof of some result cannot be presented rigorously and
in full (as is the case for some results about planar graphs,
say), we emphasize this and indicate the steps that aren’t fully
justiﬁed.
• CS. A large number of discrete mathematics students nowadays
are those specializing in computer science. Still, we believe that
even people who know nothing about computers and computing,
or ﬁnd these subjects repulsive, should have free access to dis-
crete mathematics knowledge, so we have intentionally avoided
overburdening the text with computer science terminology and
examples. However, we have not forgotten computer scientists
and have included several passages on eﬃcient algorithms and

Preface
ix
their analysis plus a number of exercises concerning algorithms
(see below).
• Other voices, other rooms. In the material covered, there are sev-
eral opportunities to demonstrate concepts from other branches
of mathematics in action, and while we intentionally restrict the
factual scope of the book, we want to emphasize these connec-
tions. Our experience tells us that students like such applica-
tions, provided that they are done thoroughly enough and not
just by hand-waving.
Prerequisites and readership. In most of the book, we do not
assume much previous mathematical knowledge beyond a standard
high-school course. Several more abstract notions that are very com-
mon in all mathematics but go beyond the usual high-school level are
explained in the ﬁrst chapter. In several places, we need some con-
cepts from undergraduate-level algebra, and these are summarized
in an appendix. There are also a few excursions into calculus (enc-
ountering notions such as limit, derivative, continuity, and so on),
but we believe that a basic calculus knowledge should be generally
available to almost any student taking a course related to our book.
The readership can include early undergraduate students of math-
ematics or computer science with a standard mathematical prepa-
ration from high school (as is usual in most of Europe, say), and
more senior undergraduate or early graduate students (in the United
States, for instance). Also nonspecialist graduates, such as biologists
or chemists, might ﬁnd the text a useful source. For mathematically
more advanced readers, the book could serve as a fast introduction
to combinatorics.
Teaching it. This book is based on an undergraduate course we
have been teaching for a long time to students of mathematics and
computer science at the Charles University in Prague. The second
author also taught parts of it at the University of Chicago, at the
University of Bonn, and at Simon Fraser University in Vancouver.
Our one-semester course in Prague (13 weeks, with one 90-minute
lecture and one 90-minute tutorial per week) typically included mat-
erial from Chapters 1–9, with many sections covered only partially
and some others omitted (such as 3.6, 4.5 4.5, 5.5, 8.3–8.5, 9.2).
While the book sometimes proves one result in several ways, we
only presented one proof in a lecture, and alternative proofs were

x
Preface
occasionally explained in the tutorials. Sometimes we inserted two
lectures on generating functions (Sections 12.1–12.3) or a lecture on
the cycle space of a graph (13.4).
To our basic course outline, we have added a lot of additional (and
sometimes more advanced) material in the book, hoping that the
reader might also read a few other things besides the sections that are
necessary for an exam. Some chapters, too, can serve as introductions
to more specialized courses (on the probabilistic method or on the
linear algebra method).
This type of smaller print is used for “second-level” material, namely
things which we consider interesting enough to include but less essential.
These are additional clariﬁcations, comments, and examples, sometimes
on a more advanced level than the basic text. The main text should
mostly make sense even if this smaller-sized text is skipped.
We also tried to sneak a lot of further related information into the
exercises. So even those who don’t intend to solve the exercises may
want to read them.
On the exercises. At the end of most of the sections, the reader will
ﬁnd a smaller or larger collection of exercises. Some of them are only
loosely related to the theme covered and are included for fun and
for general mathematical education. Solving at least some exercises
is an essential part of studying this book, although we know that
the pace of modern life and human nature hardly allow the reader to
invest the time and eﬀort to solve the majority of the 478 exercises
oﬀered (although this might ultimately be the fastest way to master
the material covered).
Mostly we haven’t included completely routine exercises requiring
only an application of some given “recipe”, such as “Apply the al-
gorithm just explained to this speciﬁc graph”. We believe that most
readers can check their understanding by themselves.
We classify the exercises into three groups of diﬃculty (no star,
one star, and two stars). We imagine that a good student who has
understood the material of a given section should be able to solve
most of the no-star exercises, although not necessarily eﬀortlessly.
One-star exercises usually need some clever idea or some slightly
more advanced mathematical knowledge (from calculus, say), and
ﬁnally two-star exercises probably require quite a bright idea. Almost
all the exercises have short solutions; as far as we know, long and
tedious computations can always be avoided. Our classiﬁcation of
diﬃculty is subjective, and an exercise which looks easy to some

Preface
xi
may be insurmountable for others. So if you can’t solve some no-star
exercises don’t get desperate.
Some of the exercises are also marked by CS, a shorthand for
computer science. These are usually problems in the design of eﬃ-
cient algorithms, sometimes requiring an elementary knowledge of
data structures. The designed algorithms can also be programmed
and tested, thus providing material for an advanced programming
course. Some of the CS exercises with stars may serve (and have
served) as project suggestions, since they usually require a combi-
nation of a certain mathematical ingenuity, algorithmic tricks, and
programming skills.
Hints to many of the exercises are given in a separate chapter
of the book. They are really hints, not complete solutions, and al-
though looking up a hint spoils the pleasure of solving a problem,
writing down a detailed and complete solution might still be quite
challenging for many students.
On the literature. In the citations, we do not refer to all sources
of the ideas and results collected in this book. Here we would like
to emphasize, and recommend, one of the sources, namely a large
collection of solved combinatorial problems by Lov´asz [8]. This book
is excellent for an advanced study of combinatorics, and also as an
encyclopedia of many results and methods. It seems impossible to
ignore when writing a new book on combinatorics, and, for exam-
ple, a signiﬁcant number of our more diﬃcult exercises are selected
from, or inspired by, Lov´asz’ (less advanced) problems. Biggs [1] is a
nice introductory textbook with a somewhat diﬀerent scope to ours.
Slightly more advanced ones (suitable as a continuation of our text,
say) are by Van Lint and Wilson [7] and Cameron [3]. The beautiful
introductory text in graph theory by Bollob´as [2] was probably writ-
ten with somewhat similar goals as our own book, but it proceeds
at a less leisurely pace and covers much more on graphs. A very rec-
ent textbook on graph theory at graduate level is by Diestel [4]. The
art of combinatorial counting and asymptotic analysis is wonderfully
explained in a popular book by Graham, Knuth, and Patashnik [6]
(and also in Knuth’s monograph [41]). Another, extensive and mod-
ern book on this subject by Flajolet and Sedgewick [5] should go to
print soon. If you’re looking for something speciﬁc in combinatorics
and don’t know where to start, we suggest the Handbook of Combi-
natorics [38]. Other recommendations to the literature are scattered

xii
Preface
throughout the text. The number of textbooks in discrete mathe-
matics is vast, and we only mention some of our favorite titles.
On the index. For most of the mathematical terms, especially those
of general signiﬁcance (such as relation, graph), the index only refers
to their deﬁnition. Mathematical symbols composed of Latin letters
(such as Cn) are placed at the beginning of the appropriate letter’s
section. Notation including special symbols (such as X \ Y , G ∼= H)
and Greek letters are listed at the beginning of the index.
Acknowledgments. A preliminary Czech version of this book was
developed gradually during our teaching in Prague. We thank our
colleagues in the Department of Applied Mathematics of the Charles
University, our teaching assistants, and our students for a stimulating
environment and helpful comments on the text and exercises. In
particular, Pavel Socha, Eva Matouˇskov´a, Tom´aˇs Holan, and Robert
Babilon discovered a number of errors in the Czech version. Martin
Klazar and Jiˇr´ı Otta compiled a list of a few dozen problems and
exercises; this list was a starting point of our collection of exercises.
Our colleague Jan Kratochv´ıl provided invaluable remarks based on
his experience in teaching the same course. We thank Tom´aˇs Kaiser
for substantial help in translating one chapter into English. Adam
Dingle and Tim Childers helped us with some comments on the
English at early stages of the translation. Jan Nekov´aˇr was so kind
as to leave the peaks of number theory for a moment and provide
pointers to a suitable proof of Fact 12.7.1.
Several people read parts of the English version at various stages
and provided insights that would probably never have occurred to
us. Special thanks go to JeﬀStopple for visiting us in Prague, care-
fully reading the whole manuscript, and sharing some of his teaching
wisdom with us. We are much indebted to Mari Inaba and Helena
Neˇsetˇrilov´a for comments that were very useful and diﬀerent from
those made by most of other people. Also opinions in several rep-
orts obtained by Oxford University Press from anonymous referees
were truly helpful. Most of the ﬁnishing and polishing work on the
book was done by the ﬁrst author during a visit to the ETH Zurich.
Emo Welzl and the members of his group provided a very pleasant
and friendly environment, even after they were each asked to read
through a chapter, and so the help of Hans-Martin Will, Beat Tra-
chsler, Bernhard von Stengel, Lutz Kettner, Joachim Giesen, Bernd

Preface
xiii
G¨artner, Johannes Bl¨omer, and Artur Andrzejak is gratefully ack-
nowledged. We also thank Hee-Kap Ahn for reading a chapter.
Many readers have contributed to correcting errors from the ﬁrst
printing. A full list can be found at the web page with errata men-
tioned below; here we just mention Mel Hausner, Emo Welzl, Hans
Mielke, and Bernd Bischl as particularly signiﬁcant contributors to
this eﬀort.
Next, we would like to thank Karel Hor´ak for several expert sug-
gestions helping the ﬁrst author in his struggle with the layout of
the book (unfortunately, the times when books used to be typeset
by professional typographers seem to be over), and Jana Chleb´ıkov´a
for a long list of minor typographic corrections.
Almost all the ﬁgures were drawn by the ﬁrst author using the
graphic editor Ipe 5.0. In the name of humankind, we thank Otfried
Cheong (formerly Schwarzkopf) for its creation.
Finally, we should not forget to mention that S¨onke Adlung has
been extremely nice to us and very helpful during the editorial pro-
cess, and that it was a pleasure to work with Julia Tompson in the
ﬁnal stages of the book preparation.
A final appeal. A long mathematical text usually contains a sub-
stantial number of mistakes. We have already corrected a large num-
ber of them, but certainly some still remain. So we plead with readers
who discover errors, bad formulations, wrong hints to exercises, etc.,
to let us know about them.2
2Please send emails concerning this book to matousek@kam.mff.cuni.cz. An
Internet home page of the book with a list of known mistakes can currently be
accessed from http://kam.mff.cuni.cz/˜matousek/.

This page intentionally left blank 

Contents
1
Introduction and basic concepts
1
1.1
An assortment of problems
2
1.2
Numbers and sets: notation
7
1.3
Mathematical induction and other proofs
16
1.4
Functions
25
1.5
Relations
32
1.6
Equivalences and other special types
of relations
36
2
Orderings
43
2.1
Orderings and how they can be depicted
43
2.2
Orderings and linear orderings
48
2.3
Ordering by inclusion
52
2.4
Large implies tall or wide
55
3
Combinatorial counting
59
3.1
Functions and subsets
59
3.2
Permutations and factorials
64
3.3
Binomial coeﬃcients
67
3.4
Estimates: an introduction
78
3.5
Estimates: the factorial function
85
3.6
Estimates: binomial coeﬃcients
93
3.7
Inclusion–exclusion principle
98
3.8
The hatcheck lady & co.
103
4
Graphs: an introduction
109
4.1
The notion of a graph; isomorphism
109
4.2
Subgraphs, components, adjacency matrix
118
4.3
Graph score
125
4.4
Eulerian graphs
130
4.5
Eulerian directed graphs
138
4.6
2-connectivity
143
4.7
Triangle-free graphs: an extremal problem
148

xvi
Contents
5
Trees
153
5.1
Deﬁnition and characterizations of trees
153
5.2
Isomorphism of trees
159
5.3
Spanning trees of a graph
166
5.4
The minimum spanning tree problem
170
5.5
Jarn´ık’s algorithm and Bor˚uvka’s algorithm
176
6
Drawing graphs in the plane
182
6.1
Drawing in the plane and on other surfaces
182
6.2
Cycles in planar graphs
190
6.3
Euler’s formula
196
6.4
Coloring maps: the four-color problem
206
7
Double-counting
217
7.1
Parity arguments
217
7.2
Sperner’s theorem on independent systems
226
7.3
An extremal problem: forbidden four-cycles
233
8
The number of spanning trees
239
8.1
The result
239
8.2
A proof via score
240
8.3
A proof with vertebrates
242
8.4
A proof using the Pr¨ufer code
245
8.5
Proofs working with determinants
247
8.6
The simplest proof?
258
9
Finite projective planes
261
9.1
Deﬁnition and basic properties
261
9.2
Existence of ﬁnite projective planes
271
9.3
Orthogonal Latin squares
277
9.4
Combinatorial applications
281
10 Probability and probabilistic proofs
284
10.1 Proofs by counting
284
10.2 Finite probability spaces
291
10.3 Random variables and their expectation
301
10.4 Several applications
307
11 Order from disorder: Ramsey’s theorem
317
11.1 A party of six
318
11.2 Ramsey’s theorem for graphs
319
11.3 A lower bound for the Ramsey numbers
321

Contents
xvii
12 Generating functions
325
12.1 Combinatorial applications of polynomials
325
12.2 Calculation with power series
329
12.3 Fibonacci numbers and the golden section
340
12.4 Binary trees
348
12.5 On rolling the dice
353
12.6 Random walk
354
12.7 Integer partitions
357
13 Applications of linear algebra
364
13.1 Block designs
364
13.2 Fisher’s inequality
369
13.3 Covering by complete bipartite graphs
373
13.4 Cycle space of a graph
376
13.5 Circulations and cuts: cycle space revisited
380
13.6 Probabilistic checking
384
Appendix: Prerequisites from algebra
395
Bibliography
402
Hints to selected exercises
407
Index
433

1
Introduction and basic
concepts
In this introductory chapter, we ﬁrst give a sample of the problems
and questions to be treated in the book. Then we explain some basic
notions and techniques, mostly fundamental and simple ones com-
mon to most branches of mathematics. We assume that the reader
is already familiar with many of them or has at least heard of them.
Thus, we will mostly review the notions, give precise formal deﬁni-
tions, and point out various ways of capturing the meaning of these
concepts by diagrams and pictures. A reader preferring a more det-
ailed and thorough introduction to these concepts may refer to the
book by Stewart and Tall [9], for instance.
Section 1.1 presents several problems to be studied later on in
the book and some thoughts on the importance of mathematical
problems and similar things.
Section 1.2 is a review of notation. It introduces some common
symbols for operations with sets and numbers, such as ∪for set
union or  for summation of a sequence of numbers. Most of the
symbols are standard, and the reader should be able to go through
this section fairly quickly, relying on the index to refresh memory
later on.
In Section 1.3, we discuss mathematical induction, an important
method for proving statements in discrete mathematics. Here it is
suﬃcient to understand the basic principle; there will be many oppo-
rtunities to see and practice various applications of induction in sub-
sequent chapters. We will also say a few words about mathematical
proofs in general.
Section 1.4 recalls the notion of a function and deﬁnes special
types of functions: injective functions, surjective functions, and bije-
ctions. These terms will be used quite frequently in the text.

2
Introduction and basic concepts
Sections 1.5 and 1.6 deal with relations and with special types of
relations, namely equivalences and orderings. These again belong to
the truly essential phrases in the vocabulary of mathematics. How-
ever, since they are simple general concepts which we have not yet
ﬂeshed out by many interesting particular examples, some readers
may ﬁnd them “too abstract”—a polite phrase for “boring”—on ﬁrst
reading. Such readers may want to skim through these sections and
return to them later. (When learning a new language, say, it is not
very thrilling to memorize the grammatical forms of the verb “to
be”, but after some time you may ﬁnd it diﬃcult to speak ﬂuently
knowing only “I am” and “he is”. Well, this is what we have to do in
this chapter: we must review some of the language of mathematics.)
1.1
An assortment of problems
Let us look at some of the problems we are going to consider in this
book. Here we are going to present them in a popular form, so you
may well know some of them as puzzles in recreational mathematics.
A well-known problem concerns three houses and three wells.
Once upon a time, three fair white houses stood in a meadow in
a distant kingdom, and there were three wells nearby, their water
clean and fresh. All was well, until one day a seed of hatred was sown,
ﬁghts started among the three households and would not cease, and
no reconciliation was in sight. The people in each house insisted that
they have three pathways leading from their gate to each well, three
pathways which should not cross any of their neighbors’ paths. Can
they ever ﬁnd paths that will satisfy everyone and let peace set in?
A solution would be possible if there were only two wells:
But with three wells, there is no hope (unless these proud men and
women would be willing to use tunnels or bridges, which sounds quite

1.1 An assortment of problems
3
unlikely). Can you state this as a mathematical problem and prove
that it has no solution?
Essentially, this is a problem about drawing in the plane. Many
other problems to be studied in this book can also be formulated in
terms of drawing. Can one draw the following picture without lifting
the pencil from the paper, drawing each line only once?
And what about this one?
If not, why not? Is there a simple way to distinguish pictures that
can be drawn in this way from those that cannot? (And, can you
ﬁnd nice accompanying stories to this problem and the ones below?)
For the subsequent set of problems, draw 8 dots in the plane in
such a way that no 3 of them lie on a common line. (The number 8 is
quite arbitrary; in general we could consider n such dots.) Connect
some pairs of these points by segments, obtaining a picture like the
following:
What is the maximum number of segments that can be drawn so that
no triangle with vertices at the dots arises? The following picture has

4
Introduction and basic concepts
13 segments:
Can you draw more segments for 8 dots with no triangle? Probably
you can. But can you prove your result is already the best possible?
Next, suppose that we want to draw some segments so that any
two dots can be connected by a path consisting of the drawn seg-
ments. The path is not allowed to make turns at the crossings of the
segments, only at the dots, so the left picture below gives a valid
solution while the right one doesn’t:
What is the minimum number of segments we must draw? How many
diﬀerent solutions with this minimum number of segments are there?
And how can we ﬁnd a solution for which the total length of all the
drawn segments is the smallest possible?
All these problems are popular versions of simple basic questions
in graph theory, which is one of main subjects of this book (treated
in Chapters 4, 5, and 6). For the above problems with 8 dots in the
plane, it is easily seen that the way of drawing the dots is immaterial;
all that matters is which pairs of dots are connected by a segment
and which are not. Most branches of graph theory deal with problems
which can be pictured geometrically but in which geometry doesn’t
really play a role. On the other hand, the problem about wells and
houses belongs to a “truly” geometric part of graph theory. It is
important that the paths should be built in the plane. If the houses
and wells were on a tiny planet shaped like a tire-tube then the
required paths would exist:

1.1 An assortment of problems
5
Another important theme of this book is combinatorial counting,
treated in Chapters 3 and 12. The problems there usually begin with
“How many ways are there. . . ” or something similar. One question
of this type was mentioned in our “8 dots” series (and it is a nice
question—the whole of Chapter 8 is devoted to it). The reader has
probably seen lots of such problems; let us add one more. How many
ways are there to divide n identical coins into groups? For instance,
4 coins can be divided in 5 ways: 1 + 1 + 1 + 1 (4 groups of 1 coin
each), 1 + 1 + 2, 1 + 3, 2 + 2, and 4 (all in one group, which is not
really a “division” in the sense most people understand it, but what
do you expect from mathematicians!). For this problem, we will not
be able to give an exact formula; such a formula does exist but its
derivation is far beyond the scope of this book. Nonetheless, we will
at least derive estimates for the number in question. This number is
a function of n, and the estimates will allow us to say “how fast” this
function grows, compared to simple and well-known functions like n2
or 2n. Such a comparison of complicated functions to simple ones is
the subject of the so-called asymptotic analysis, which will also be
touched on below and which is important in many areas, for instance
for comparing several algorithms which solve the same problem.
Although the problems presented may look like puzzles, each of
them can be regarded as the starting point of a theory with numerous
applications, both in mathematics and in practice.
In fact, distinguishing a good mathematical problem from a bad one
is one of the most diﬃcult things in mathematics, and the “quality” of
a problem can often be judged only in hindsight, after the problem has
been solved and the consequences of its solution mapped. What is a good

6
Introduction and basic concepts
problem? It is one whose solution will lead to new insights, methods,
or even a whole new fruitful theory. Many problems in recreational
mathematics are not good in this sense, although their solution may
require considerable skill or ingenuity.
A pragmatically minded reader might also object that the problems
shown above are useless from a practical point of view. Why take a
whole course about them, a skeptic might say, when I have to learn so
many practically important things to prepare for my future career? Ob-
jections of this sort are quite frequent and cannot be simply dismissed, if
only because the people controlling the funding are often pragmatically
minded.
One possible answer is that for each of these puzzle-like problems,
we can exhibit an eminently practical problem that is its cousin. For
instance, the postal delivery service in a district must deliver mail to all
houses, which means passing through each street at least once. What is
the shortest route to take? Can it be found in a reasonable time using a
supercomputer? Or with a personal computer? In order to understand
this postal delivery problem, one should be familiar with simple results
about drawing pictures without lifting a pencil from the paper.
Or, given some placement of components of a circuit on a board, is
it possible to interconnect them in such a way that the connections go
along the surface of the board and do not cross each other? What is
the most economical placement of components and connections (using
the smallest area of the board, say)? Such questions are typical of VLSI
design (designing computer chips and similar things). Having learned
about the three-wells problem and its relatives (or, scientiﬁcally speak-
ing, about planar graphs) it is much easier to grasp ways of designing
the layout of integrated circuits.
These “practical” problems also belong to graph theory, or to a
mixture of graph theory and the design of eﬃcient algorithms. This
book doesn’t provide a solution to them, but in order to comprehend
a solution in some other book, or even to come up with a new good
solution, one should master the basic concepts ﬁrst.
We would also like to stress that the most valuable mathematical
research was very seldom directly motivated by practical goals. Some
great mathematical ideas of the past have only found applications quite
recently. Mathematics does have impressive applications (it might be
easier to list those human activities where it is not applied than those
where it is), but anyone trying to restrict mathematical research to the
directly applicable parts would be left with a lifeless fragment with most
of the creative power gone.
Exercises are unnecessary in this section. Can you solve some of
the problems sketched here, or perhaps all of them? Even if you try
and get only partial results or fail completely, it will still be of great

1.2 Numbers and sets: notation
7
help in reading further.
So what is this discrete mathematics they’re talking about, the
reader may (rightfully) ask? The adjective “discrete” here is an oppo-
site of “continuous”. Roughly speaking, objects in discrete mathematics,
such as the natural numbers, are clearly separated and distinguishable
from each other and we can perceive them individually (like trees in
a forest which surrounds us). In contrast, for a typical “continuous”
object, such as the set of all points on a line segment, the points are
indiscernible (like the trees in a forest seen from a high-ﬂying airplane).
We can focus our attention on some individual points of the segment
and see them clearly, but there are always many more points nearby
that remain indistinguishable and form the totality of the segment.
According to this explanation, parts of mathematics such as algebra
or set theory might also be considered “discrete”. But in the common
usage of the term, discrete mathematics is most often understood as
mathematics dealing with ﬁnite sets. In many current university curric-
ula, a course on discrete mathematics has quite a wide range, including
some combinatorics, counting, graph theory, but also elements of math-
ematical logic, some set theory, basics from the theory of computing
(ﬁnite automata, formal languages, elements of computer architecture),
and other things. We prefer a more narrowly focussed scope, so perhaps
a more descriptive title for this book would be “Invitation to combina-
torics and graph theory”, covering most of the contents. But the name
of the course we have been teaching happened to be “Discrete mathe-
matics” and we decided to stick to it.
1.2
Numbers and sets: notation
Number domains. For the set of all natural numbers, i.e. the set
{1, 2, 3, . . .}, we reserve the symbol N. The letters n, m, k, i, j, p and
possibly some others usually represent natural numbers.
Using the natural numbers, we may construct other well-known
number domains: the integers, the rationals, and the reals (and also
the complex numbers, but we will seldom hear about them here).
The integer numbers or simply integers arise from the natural
numbers by adding the negative integer numbers and 0. The set of
all integers is denoted by Z.
The rational numbers are fractions with integer numerator and
denominator. This set is usually denoted by Q but we need not
introduce any symbol for it in this book. The construction of the
set R of all real numbers is more complicated, and it is treated in
introductory courses of mathematical analysis. Famous examples of
real numbers which are not rational are numbers such as
√
2, some

8
Introduction and basic concepts
important constants like π, and generally numbers whose decimal
notation has an inﬁnite and aperiodic sequence of digits following
the decimal point, such as 0.12112111211112 . . ..
The closed interval from a to b on the real axis is denoted by [a, b],
and the open interval with the same endpoints is written as (a, b).
Operations with numbers.
Most symbols for operations with
numbers, such as + for addition,
√
for square root, and so on, are
generally well known. We write division either as a fraction, or some-
times with a slash, i.e. either in the form a
b or as a/b.
We introduce two less common functions. For a real number x,
the symbol ⌊x⌋is called1 the lower integer part of x (or the ﬂoor
function of x), and its value is the largest integer smaller than or
equal to x. Similarly ⌈x⌉, the upper integer part of x (or the ceiling
function), denotes the smallest integer greater than or equal to x.
For instance, ⌊0.999⌋= 0, ⌊−0.1⌋= −1, ⌈0.01⌉= 1, ⌈17
3 ⌉= 6,
⌊
√
2⌋= 1.
Later on, we will introduce some more operations and functions
for numbers, which have an important combinatorial meaning and
which we will investigate in more detail. Examples are n! and
n
k

.
Sums and products. If a1, a2, . . . , an are real numbers, their sum
a1 + a2 + · · · + an can also be written using the summation sign ,
in the form
n

i=1
ai.
This notation somewhat resembles the FOR loop in many program-
ming languages. Here are a few more examples:
5

j=2
1
2j = 1
4 + 1
6 + 1
8 + 1
10
5

i=2
1
2j = 1
2j + 1
2j + 1
2j + 1
2j = 2
j
1In the older literature, one often ﬁnds [x] used for the same function.

1.2 Numbers and sets: notation
9
n

i=1
n

j=1
(i + j) =
n

i=1
((i + 1) + (i + 2) + · · · + (i + n))
=
n

i=1
(ni + (1 + 2 + · · · + n))
= n

n

i=1
i

+ n(1 + 2 + · · · + n)
= 2n(1 + 2 + · · · + n).
Similarly as sums are written using  (which is the capital Greek
letter “sigma”, from the word sum), products may be expressed using
the sign  (capital Greek “pi”). For example,
n

i=1
i + 1
i
= 2
1 · 3
2 · . . . · n + 1
n
= n + 1.
Sets. Another basic notion we will use is that of a set. Most likely
you have already encountered sets in high school (and, thanks to
the permanent modernization of the school system, maybe even in
elementary school). Sets are usually denoted by capital letters:
A, B, . . . , X, Y, . . . , M, N, . . .
and so on, and the elements of sets are mostly denoted by lowercase
letters: a, b, . . ., x, y, . . ., m, n, . . ..
The fact that a set X contains an element x is traditionally writ-
ten using the symbol ∈, which is a somewhat stylized Greek letter
ε—“epsilon”. The notation x ∈X is read “x is an element of X”,
“x belongs to X”, “x is in X”, and so on.
Let us remark that the concept of a set and the symbol ∈are
so-called primitive notions. This means that we do not deﬁne them
using other “simpler” notions (unlike the rational numbers, say, which
are deﬁned in terms of the integers). To understand the concept of a set,
we rely on intuition (supported by numerous examples) in this book. It
turned out at the beginning of the 20th century that if such an intuitive
notion of a set is used completely freely, various strange situations, the
so-called paradoxes, may arise.2 In order to exclude such paradoxes, the
2The most famous one is probably Russell’s paradox. One possible formulation
is about an army barber. An army barber is supposed to shave all soldiers who
do not shave themselves—should he, as one of the soldiers, shave himself or not?
This paradox can be translated into a rigorous mathematical language and it
implies the inconsistency of notions like “the set of all sets”.

10
Introduction and basic concepts
theory of sets has been rebuilt on a formalized basis, where all proper-
ties of sets are derived formally from several precisely formulated basic
assumptions (axioms). For the sets used in this text, which are mostly
ﬁnite, we need not be afraid of any paradoxes, and so we can keep
relying on the intuitive concept of a set.
The set with elements 1, 37, and 55 is written as {1, 37, 55}. This,
and also the notations {37, 1, 55} and {1, 37, 1, 55, 55, 1}, express the
same thing. Thus, a multiple occurrence of the same element is ign-
ored: the same element cannot be contained twice in the same set!
Three dots (an ellipsis) in {2, 4, 6, 8, . . .} mean “and further similarly,
using the same pattern”, i.e. this notation means the set of all even
natural numbers. The appropriate pattern should be apparent at
ﬁrst sight. For instance, {21, 22, 23, . . .} is easily understandable as
the set of all powers of 2, while {2, 4, 8, . . .} may be less clear.
Ordered and unordered pairs.
The symbol {x, y} denotes the
set containing exactly the elements x and y, as we already know. In
this particular case, the set {x, y} is sometimes called the unordered
pair of x and y. Let us recall that {x, y} is the same as {y, x}, and
if x = y, then {x, y} is a 1-element set.
We also introduce the notation (x, y) for the ordered pair of
x and y. For this construct, the order of the elements x and y is
important. We thus assume the following:
(x, y) = (z, t) if and only if x = z and y = t.
(1.1)
Interestingly, the ordered pair can be deﬁned using the notion of
unordered pair, as follows:
(x, y) = {{x}, {x, y}} .
Verify that ordered pairs deﬁned in this way satisfy the condition (1.1).
However, in this text it will be simpler for us to consider (x, y) as another
primitive notion.
Similarly, we write (x1, x2, . . . , xn) for the ordered n-tuple consist-
ing of elements x1, x2, . . . , xn. A particular case of this convention is
writing a point in the plane with coordinates x and y as (x, y), and
similarly for points or vectors in higher-dimensional spaces.
Defining sets. More complicated and interesting sets are usually
created from known sets using some rule. The sets of all squares of
natural numbers can be written
{i2 : i ∈N}

1.2 Numbers and sets: notation
11
or also
{n ∈N:
there exists k ∈N such that k2 = n}
or using the symbol ∃for “there exists”:
{n ∈N: ∃k ∈N (k2 = n)}.
Another example is a formal deﬁnition of the open interval (a, b)
introduced earlier:
(a, b) = {x ∈R: a < x < b}.
Note that the symbol (a, b) may mean either the open interval, or
also the ordered pair consisting of a and b. These two meanings must
(and usually can) be distinguished by the context. This is not at all
uncommon in mathematics: many symbols, like parentheses in this case,
are used in several diﬀerent ways. For instance, (a, b) also frequently
denotes the greatest common divisor of natural numbers a and b (but
we avoid this meaning in this book).
With modern typesetting systems, it is no problem to use any kind
of alphabets and symbols including hieroglyphs, so one might think of
changing the notation in such cases. But mathematics tends to be rather
conservative and the existing literature is vast, and so such notational
inventions are usually short-lived.
The empty set. An important set is the one containing no element
at all. There is just one such set, and it is customarily denoted by ∅
and called the empty set. Let us remark that the empty set can be
an element of another set. For example, {∅} is the set containing the
empty set as an element, and so it is not the same set as ∅!
Set systems. In mathematics, we often deal with sets whose ele-
ments are other sets. For instance, we can deﬁne the set
M = {{1, 2}, {1, 2, 3}, {2, 3, 4}, {4}},
whose elements are 4 sets of natural numbers, more exactly 4 subsets
of the set {1, 2, 3, 4}. One meets such sets in discrete mathematics
quite frequently. To avoid saying a “set of sets”, we use the notions
set system or family of sets. We could thus say that M is a system of
sets on the set {1, 2, 3, 4}. Such set systems are sometimes denoted
by calligraphic capital letters, such as M.
However, it is clear that such a distinction using various types of let-
ters cannot always be quite consistent—what do we do if we encounter
a set of sets of sets?

12
Introduction and basic concepts
The system consisting of all possible subsets of some set X is
denoted by the symbol3 2X and called the power set of X. Another
notation for the power set common in the literature is P(X).
Set size. A large part of this book is devoted to counting various
kinds of objects. Hence a very important notation for us is that for
the number of elements of a ﬁnite set X. We write it using the same
symbol as for the absolute value of a number: |X|.
A more general notation for sums and products. Sometimes it
is advantageous to use a more general way to write down a sum than
using the pattern n
i=1 ai. For instance,

i∈{1,3,5,7}
i2
means the sum 12 + 32 + 52 + 72. Under the summation sign, we
ﬁrst write the summation variable and then we write out the set of
values over which the summation is to be performed. We have a lot
of freedom in denoting this set of values. Sometimes it can in part
be described by words, as in the following:

i: 1≤i≤10
i a prime
i = 2 + 3 + 5 + 7.
Should the set of values for the summation be empty, we deﬁne the
value of the sum as 0, no matter what appears after the summation
sign. For example:
0

i=1
(i + 10) = 0,

i∈{2,4,6,8}
i odd
i4 = 0.
A similar “set notation” can also be employed for products. An
empty product, such as 
j : 2≤j<1 2j, is always deﬁned as 1 (not
0 as for an empty sum).
Operations with sets. Using the primitive notion of set member-
ship, ∈, we can deﬁne further relations among sets and operations
3This notation may look strange, but it is traditional and has its reasons.
For instance, it helps to remember that an n-element set has 2n subsets; see
Proposition 3.1.2.

1.2 Numbers and sets: notation
13
with sets. For example, two sets X and Y are considered identical
(equal) if they have the same elements. In this case we write X = Y .
Other relations among sets can be deﬁned similarly. If X, Y are
sets, X ⊆Y (in words: “X is a subset of Y ”) means that each
element of X also belongs to Y .
The notation X ⊂Y sometimes denotes that X is a subset of Y
but X is not equal to Y . This distinction between ⊆and ⊂is not quite
uniﬁed in the literature, and some authors may use ⊂synonymously
with our ⊆.
The notations X ∪Y (the union of X and Y ) and X ∩Y (the
intersection of X and Y ) can be deﬁned as follows:
X ∪Y = {z : z ∈X or z ∈Y },
X ∩Y = {z : z ∈X and z ∈Y }.
If we want to express that the sets X and Y in the considered union
are disjoint, we write the union as X ˙∪Y . The expression X \Y is the
diﬀerence of the sets X and Y , i.e. the set of all elements belonging
to X but not to Y .
Enlarged symbols ∪and ∩may be used in the same way as the
symbols  and . So, if X1, X2, . . . , Xn are sets, their union can be
written
n	
i=1
Xi
(1.2)
and similarly for intersection.
Note that this notation is possible (or correct) only because the
operations of union and intersection are associative; that is, we have
X ∩(Y ∩Z) = (X ∩Y ) ∩Z
and
X ∪(Y ∪Z) = (X ∪Y ) ∪Z
for any triple X, Y, Z of sets. As a consequence, the way of “parenthe-
sizing” the union of any 3, and generally of any n, sets is immaterial,
and the common value can be denoted as in (1.2). The operations ∪
and ∩are also commutative, in other words they satisfy the relations
X ∩Y = Y ∩X,
X ∪Y = Y ∪X.
The commutativity and the associativity of the operations ∪and ∩are
complemented
by
their
distributivity.
For
any
sets
X, Y, Z

14
Introduction and basic concepts
we have
X ∩(Y ∪Z) = (X ∩Y ) ∪(X ∩Z),
X ∪(Y ∩Z) = (X ∪Y ) ∩(X ∪Z).
The validity of these relations can be checked by proving that any
element belongs to the left-hand side if and only if it belongs to the right-
hand side. The relations can be generalized for an arbitrary number of
sets as well. For instance,
A ∩

n
	
i=1
Xi

=
n
	
i=1
(A ∩Xi);
A ∪

n

i=1
Xi

=
n

i=1
(A ∪Xi).
Such relations can be proved by induction; see Section 1.3 below. Other
popular relations for sets are
X \(A∪B) = (X \A)∩(X \B)
and
X \(A∩B) = (X \A)∪(X \B)
(the so-called de Morgan laws), and their generalizations
X \

n
	
i=1
Ai

=
n

i=1
(X \ Ai)
X \

n

i=1
Ai

=
n
	
i=1
(X \ Ai).
The last operation to be introduced here is the Cartesian product,
denoted by X ×Y , of two sets X and Y . The Cartesian product of X
and Y is the set of all ordered pairs of the form (x, y), where x ∈X
and y ∈Y . Written formally,
X × Y = {(x, y): x ∈X, y ∈Y } .
Note that generally X×Y is not the same as Y ×X, i.e. the operation
is not commutative.
The name “Cartesian product” comes from a geometric interpreta-
tion. If, for instance, X = Y = R, then X × Y can be interpreted as all
points of the plane, since a point in the plane is uniquely described by
an ordered pair of real numbers, namely its Cartesian coordinates4—
the x-coordinate and the y-coordinate (Fig. 1.1a). This geometric view
can also be useful for Cartesian products of sets whose elements are not
numbers (Fig. 1.1b).
4These are named after their inventor, Ren´e Descartes.

1.2 Numbers and sets: notation
15
a
b
(a, b)
a
b
c
d
1
2
3
(a, 3) (b, 3)
(c, 2)
X
Y
X × Y
(a)
(b)
Fig. 1.1 Illustrating the Cartesian product: (a) R×R; (b) X ×Y for ﬁnite
sets X, Y .
The Cartesian product of a set X with itself, i.e. X × X, may
also be denoted by X2.
Exercises
1. Which of the following formulas are correct?
(a) ⌊(n+1)2
2
⌋= ⌊n2
2 ⌋+ n,
(b) ⌊n+k
2 ⌋= ⌊n
2 ⌋+ ⌊k
2⌋,
(c) ⌈(⌊x⌋)⌉= ⌈x⌉(for a real number x),
(d) ⌈(⌊x⌋+ ⌊y⌋)⌉= ⌊x⌋+ ⌊y⌋.
2. ∗Prove that the equality ⌊√x⌋= ⌊

⌊x⌋⌋holds for any positive real
number x.
3. (a) Deﬁne a “parenthesizing” of a union of n sets n
i=1 Xi. Similarly,
deﬁne a “parenthesizing” of a sum of n numbers n
i=1 ai.
(b) Prove that any two parenthesizings of the intersection n
i=1 Xi
yield the same result.
(c) How many ways are there to parenthesize the union of 4 sets
A ∪B ∪C ∪D?
(d) ∗∗Try to derive a formula or some other way to count the number
of ways to parenthesize the union of n sets n
i=1 Xi.
4. True or false? If 2X = 2Y holds for two sets X and Y , then X = Y .
5. Is a “cancellation” possible for the Cartesian product? That is, if
X × Y = X × Z holds for some sets X, Y, Z, does it necessarily fol-
low that Y = Z?
6. Prove that for any two sets A, B we have
(A \ B) ∪(B \ A) = (A ∪B) \ (A ∩B).

16
Introduction and basic concepts
7. ∗Consider the numbers 1, 2, . . . , 1000. Show that among any 501 of
them, two numbers exist such that one divides the other one.
8. In this problem, you can test your ability to discover simple but “hid-
den” solutions. Divide the following ﬁgure into 7 parts, all of them con-
gruent (they only diﬀer by translation, rotation, and possibly by a mir-
ror reﬂection). All the bounding segments in the ﬁgure have length 1,
and the angles are 90, 120, and 150 degrees.
1.3
Mathematical induction and other proofs
Let us imagine that we want to calculate, say, the sum 1 + 2 + 22 +
23 + · · · + 2n = n
i=0 2i (and that we can’t remember a formula
for the sum of a geometric progression). We suspect that one can
express this sum by a nice general formula valid for all the n. By
calculating numerical values for several small values of n, we can
guess that the desired formula will most likely be 2n+1 −1. But even
if we verify this for a million speciﬁc values of n with a computer, this
is still no proof. The million-and-ﬁrst number might, in principle, be
a counterexample. The correctness of the guessed formula for all n
can be proved by so-called mathematical induction. In our case, we
can proceed as follows:
1. The formula n
i=0 2i = 2n+1 −1 holds for n = 1, as one can
check directly.
2. Let us suppose that the formula holds for some value n = n0.
We prove that it also holds for n = n0 + 1. Indeed, we have
n0+1

i=0
2i =
 n0

i=0
2i

+ 2n0+1.
The sum in parentheses equals 2n0+1 −1 by our assumption (the
validity for n = n0). Hence
n0+1

i=0
2i = 2n0+1 −1 + 2n0+1 = 2 · 2n0+1 −1 = 2n0+2 −1.
This is the required formula for n = n0 + 1.

1.3 Mathematical induction and other proofs
17
This establishes the validity of the formula for an arbitrary n: by
step 1, the formula is true for n = 1, by step 2 we may thus infer
it is also true for n = 2 (using step 2 with n0 = 1), then, again by
step 2, the formula holds for n = 3. . . , and in this way we can reach
any natural number. Note that this argument only works because
the value of n0 in step 2 was quite arbitrary. We have made the
step from n0 to n0 +1, where any natural number could equally well
appear as n0.
Step 2 in this type of proof is called the inductive step. The ass-
umption that the statement being proved is already valid for some
value n = n0 is called the inductive hypothesis.
One possible general formulation of the principle of mathematical
induction is the following:
1.3.1 Proposition. Let X be a set of natural numbers with the
following properties:
(i) The number 1 belongs to X.
(ii) If some natural number n is an element of X, then the number
n + 1 belongs to X as well.
Then X is the set of all natural numbers (X = N).
In applications of this scheme, X would be the set of all numbers
n such that the statement being proved, S(n), is valid for n.
The scheme of a proof by mathematical induction has many vari-
ations. For instance, if we need to prove some statement for all n ≥2,
the ﬁrst step of the proof will be to check the validity of the state-
ment for n = 2. As an inductive hypothesis, we can sometimes use
the validity of the statement being proved not only for n = n0, but
for all n ≤n0, and so on; these modiﬁcations are best mastered by
examples.
Mathematical induction can either be regarded as a basic property of
natural numbers (an axiom, i.e. something we take for granted without
a proof), or be derived from the following other basic property (axiom):
Any nonempty subset of natural numbers possesses a smallest element.
This is expressed by saying that the usual ordering of natural numbers
by magnitude is a well-ordering. In fact, the principle of mathematical
induction and the well-ordering property are equivalent to each other,5
and either one can be taken as a basic axiom for building the theory of
natural numbers.
5Assuming that each natural number n > 1 has a unique predecessor n −1.

18
Introduction and basic concepts
Proof of Proposition 1.3.1 from the well-ordering property. For
contradiction, let us assume that a set X satisﬁes both (i) and (ii), but
it doesn’t contain all natural numbers. Among all natural numbers n
not lying in X, let us choose the smallest one and denote it by n0.
By condition (i) we know that n0 > 1, and since n0 was the smallest
possible, the number n0 −1 is an element of X. However, using (ii) we
get that n0 is an element of X, which is a contradiction.
2
Let us remark that this type of argument (saying “Let n0 be the
smallest number violating the statement we want to prove” and deriv-
ing a contradiction, namely that a yet smaller violating number must
exist) sometimes replaces mathematical induction. Both ways, this one
and induction, essentially do the same thing, and it depends on the
circumstances or personal preferences which one is actually used.
We will use mathematical induction quite often. It is one of our
basic proof methods, and the reader can thus ﬁnd many examples
and exercises on induction in subsequent chapters.
Mathematical proofs and not-quite proofs. Mathematical proof
is an amazing invention. It allows one to establish the truth of a
statement beyond any reasonable doubt, even when the statement
deals with a situation so complicated that its truth is inaccessible to
direct evidence. Hardly anyone can see directly that no two natural
numbers m, n exist such that m
n =
√
2 and yet we can trust this
fact completely, because it can be proved by a chain of simple logical
steps.
Students often don’t like proofs, even students of mathematics.
One reason might be that they have never experienced satisfaction
from understanding an elegant and clever proof or from making a
nice proof by themselves. One of our main goals is to help the reader
to acquire the skill of rigorously proving simple mathematical state-
ments.
A possible objection is that most students will never need such
proofs in their future jobs. We believe that learning how to prove math-
ematical theorems helps to develop useful habits in thinking, such as
working with clear and precise notions, exactly formulating thoughts
and statements, and not overlooking less obvious possibilities. For ins-
tance, such habits are invaluable for writing software that doesn’t crash
every time the circumstances become slightly non-standard.
The art of ﬁnding and writing proofs is mostly taught by exam-
ples,6 by showing many (hopefully) correct and “good” proofs to the
6We will not even try to say what a proof is and how to do one!

1.3 Mathematical induction and other proofs
19
student and by pointing out errors in the student’s own proofs. The
latter “negative” examples are very important, and since a book is
a one-way communication device, we decided to include also a few
negative examples in this book, i.e. students’ attempts at proofs with
mistakes which are, according to our experience, typical. These int-
entionally wrong proofs are presented in a special font like this. In the
rest of this section, we discuss some common sources of errors. (We
hasten to add that types of errors in proofs are as numerous as grains
of sand, and by no means do we want to attempt any classiﬁcation.)
One quite frequent situation is where the student doesn’t under-
stand the problem correctly. There may be subtleties in the problem’s
formulation which are easy to overlook, and sometimes a misunder-
standing isn’t the student’s fault at all, since the author of the prob-
lem might very well have failed to see some double meaning. The only
defense against this kind of misunderstanding is to pay the utmost
attention to reading and understanding a problem before trying to
solve it. Do a preliminary check: does the problem make sense in the
way you understand it? Does it have a suspiciously trivial solution?
Could there be another meaning?
With the current abundance of calculators and computers, errors are
sometimes caused by the uncritical use of such equipment. Asked how
many zeros does the decimal notation of the number 50! = 50·49·48·. . .·1
end with, a student answered 60, because a pocket calculator with an
8-digit display shows that 50! = 3.04140·1064. Well, a more sophisticated
calculator or computer programmed to calculate with integers with ar-
bitrarily many digits would solve this problem correctly and calculate
that
50!=30414093201713378043612608166064768844377641568960512000000000000
with 12 trailing zeros. Several software systems can even routinely solve
such problems as ﬁnding a formula for the sum 12·21+22·22+32·23+· · ·+
n22n, or for the number of binary trees on n vertices (see Section 12.4).
But even programmers of such systems can make mistakes and so it’s
better to double-check such results. Moreover, the capabilities of these
systems are very limited; artiﬁcial intelligence researchers will have to
make enormous progress before they can produce computers that can
discover and prove a formula for the number of trailing zeros of n!, or
solve a signiﬁcant proportion of the exercises in this book, say.
Next, we consider the situation where a proof has been written
down but it has a ﬂaw, although its author believes it to be satisfac-
tory.

20
Introduction and basic concepts
In principle, proofs can be written down in such detail and in such
a formal manner that they can be checked automatically by a com-
puter. If such a completely detailed and formalized proof is wrong,
some step has to be clearly false, but the catch is that formalizing
proofs completely is very laborious and impractical. All textbook
proofs and problem solutions are presented somewhat informally.
While some informality may be necessary for a reasonable pre-
sentation of a proof, it may also help to hide errors. Nevertheless,
a good rule for writing and checking proofs is that every statement
in a correct proof should be literally true. Errors can often be det-
ected by isolating a speciﬁc false statement in the proof, a mistake
in calculation, or a statement that makes no sense (“Let ℓ1, ℓ2 be two
arbitrary lines in the 3-dimensional space, and let ρ be a plane contain-
ing both of them. . . ” etc.). Once detected and brought out into the
light, such errors become obvious to (almost) everyone. Still, they
are frequent. If, while trying to come up with a proof, one discovers
an idea seemingly leading to a solution and shouts “This must be
IT!”, caution is usually swept aside and one is willing to write down
the most blatant untruths. (Unfortunately, the ﬁrst idea that comes
to mind is often nonsense, rather than “it”, at least as far as the
authors’ own experience with problem solving goes.)
A particularly frequent mistake, common perhaps to all mathe-
maticians of the world, is a case omission. The proof works for some
objects it should deal with, but it fails in some cases the author over-
looked. Such a case analysis is mostly problem speciﬁc, but one keeps
encountering variations on favorite themes. Dividing an equation by
x −y is only allowed for x ̸= y, and the x = y case must be treated
separately. An intersection of two lines in the plane can only be used
in a proof if the lines are not parallel. Deducing a2 > b2 from a > b
may be invalid if we know nothing about the sign of a and b, and so
on and so on.
Many proofs created by beginners are wrong because of a confused
application of theorems. Something seems to follow from a theorem
presented in class or in a textbook, say, but in reality the theorem
says something slightly diﬀerent, or some of its assumptions don’t
hold. Since we have covered no theorems worth mentioning so far, let
us give an artiﬁcial geometric example: “Since ABC is an isosceles
triangle with the sides adjacent to A having equal length, we have
|AB|2 + |AC|2 = |BC|2 by the theorem of Pythagoras.” Well, wasn’t
there something about a right angle in Pythagoras’ theorem?

1.3 Mathematical induction and other proofs
21
A rich source of errors and misunderstandings is relying on unp-
roved statements.
Many proofs, including correct and even textbook ones, contain un-
proved statements intentionally, marked by clauses like “obviously. . . ”.
In an honest proof, the meaning of such clauses should ideally be “I,
the author of this proof, can see how to prove this rigorously, and since
I consider this simple enough, I trust that you, my reader, can also ﬁll
in all the details without too much eﬀort”. Of course, in many mathe-
matical papers, the reader’s impression about the author’s thinking is
more in the spirit of “I can see it somehow since I’ve been working on
this problem for years, and if you can’t it’s your problem”. Hence omit-
ting parts of proofs that are “clear” is a highly delicate social task, and
one should always be very careful with it. Also, students shouldn’t be
surprised if their teacher insists that such an “obvious” part be proved
in detail. After all, what would be a better hiding place for errors in a
proof than in the parts that are missing?
A more serious problem concerns parts of a proof that are omitted
unconsciously. Most often, the statement whose proof is missing is
not even formulated explicitly.7 For a teacher, it may be a very chal-
lenging task to convince the proof’s author that something is wrong
with the proof, especially when the unproved statement is actually
true.
One particular type of incomplete proof, fairly typical of students’
proofs in discrete mathematics, could be labeled as mistaking the par-
ticular for the general. To give an example, let us consider the following
Mathematical Olympiad problem:
1.3.2 Problem. Let n > 1 be an integer. Let M be a set of closed
intervals. Suppose that the endpoints u, v of each interval [u, v] ∈M
are natural numbers satisfying 1 ≤u < v ≤n, and, moreover, for any
two distinct intervals I, I′ ∈M, one of the following possibilities occurs:
I ∩I′ = ∅, or I ⊂I′, or I′ ⊂I (i.e. two intervals must not partially
overlap). Prove that |M| ≤n −1.
An insufficient proof attempt.
In order to construct an M as large
as possible, we ﬁrst insert as many unit-length intervals as possible, as in
the following picture:
1
2
13
. . .
7Even proofs by the greatest mathematicians of the past suﬀer from such
incompleteness, partly because the notion of a proof has been developing over
the ages (towards more rigor, that is).

22
Introduction and basic concepts
These ⌊n/2⌋intervals are all disjoint. Now any other interval in M must
contain at least two of these unit intervals (or, for n odd, possibly the
last unit interval plus the point that remains). Hence, to get the maximum
number of intervals, we put in the next “layer” of shortest possible intervals,
as illustrated below:
1
2
13
. . .
We continue in this manner, adding one layer after another, until we ﬁnally
add the last layer consisting of the whole interval [1, n]:
1
2
13
. . .
It remains to show that the set M created in this way has at most n −1
intervals. We note that every interval I in the kth layer contains a point of
the form i+ 1
2, 1 ≤i ≤n−1, that was not contained in any interval of the
previous layers, because the space between the two intervals in the previous
layer was not covered before adding the kth layer. Therefore, |M| ≤n −1
as claimed.
2
This “proof” looks quite clever (after all, the way of counting the
intervals in the particular M constructed in the proof is quite elegant).
So what’s wrong with it? Well, we have shown that one particular M
satisﬁes |M| ≤n −1. The argument tries to make the impression of
showing that this particular M is the worst possible case, i.e. that no
other M may have more intervals, but in reality it doesn’t prove any-
thing like that! For instance, the ﬁrst step seems to argue that an M
with the maximum possible number of intervals should contain ⌊n/2⌋
unit-length intervals. But this is not true, as is witnessed by M = {[1, 2],
[1, 3], [1, 4], . . . , [1, n]}. Saving the “proof” above by justifying its various
steps seems more diﬃcult than ﬁnding another, correct, proof. Although
the demonstrated “proof” contains some useful hints (the counting idea
at the end of the proof can in fact be made to work for any M), it’s
still quite far from a valid solution.
The basic scheme of this “proof”, apparently a very tempting one,
says “this object X must be the worst one”, and then proves that this
particular X is OK. But the claim that nothing can be worse than X is
not substantiated (although it usually looks plausible that by construct-
ing this X, we “do the worst possible thing” concerning the statement
being proved).
Another variation of “mistaking the particular for the general” often
appears in proofs by induction, and is shown in several examples in
Sections 5.1 and 6.3.

1.3 Mathematical induction and other proofs
23
Exercises
1. Prove the following formulas by mathematical induction:
(a) 1 + 2 + 3 + · · · + n = n(n + 1)/2
(b) n
i=1 i · 2i = (n −1)2n+1 + 2.
2. The numbers F0, F1, F2, . . . are deﬁned as follows (this is a deﬁnition
by mathematical induction, by the way): F0 = 0, F1 = 1, Fn+2 =
Fn+1 + Fn for n = 0, 1, 2, . . . Prove that for any n ≥0 we have Fn ≤
((1 +
√
5)/2)n−1 (see also Section 12.3).
3. (a) Let us draw n lines in the plane in such a way that no two are
parallel and no three intersect in a common point. Prove that the
plane is divided into exactly n(n + 1)/2 + 1 parts by the lines.
(b) ∗Similarly, consider n planes in the 3-dimensional space in gen-
eral position (no two are parallel, any three have exactly one point in
common, and no four have a common point). What is the number of
regions into which these planes partition the space?
4. Prove de Moivre’s theorem by induction: (cos α+i sin α)n = cos(nα)+
i sin(nα). Here i is the imaginary unit.
5. In ancient Egypt, fractions were written as sums of fractions with nu-
merator 1. For instance, 3
5 = 1
2 + 1
10. Consider the following algorithm
for writing a fraction m
n in this form (1 ≤m < n): write the fraction
1
⌈n/m⌉, calculate the fraction m
n −
1
⌈n/m⌉, and if it is nonzero repeat the
same step. Prove that this algorithm always ﬁnishes in a ﬁnite number
of steps.
6. ∗Consider a 2n × 2n chessboard with one (arbitrarily chosen) square
removed, as in the following picture (for n = 3):
Prove that any such chessboard can be tiled without gaps or overlaps
by L-shapes consisting of 3 squares each.
7. Let n ≥2 be a natural number. We consider the following game. Two
players write a sequence of 0s and 1s. They start with an empty line
and alternate their moves. In each move, a player writes 0 or 1 to
the end of the current sequence. A player loses if his digit completes
a block of n consecutive digits that repeats itself in the sequence for

24
Introduction and basic concepts
the second time (the two occurrences of the block may overlap). For
instance, for n = 4, a sequence produced by such a game may look
as follows: 00100001101011110011 (the second player lost by the last
move because 0011 is repeated).
(a) Prove that the game always ﬁnishes after ﬁnitely many steps.
(b) ∗Suppose that n is odd. Prove that the second player (the one who
makes the second move) has a winning strategy.
(c) ∗Show that for n = 4, the ﬁrst player has a winning strategy.
Unsolved question: Can you determine who has a winning strategy for
some even n > 4?
8. ∗On an inﬁnite sheet of white graph paper (a paper with a square
grid), n squares are colored black. At moments t = 1, 2, . . ., squares
are recolored according to the following rule: each square gets the color
occurring at least twice in the triple formed by this square, its top
neighbor, and its right neighbor. Prove that after the moment t = n,
all squares are white.
9. At time 0, a particle resides at the point 0 on the real line. Within 1
second, it divides into 2 particles that ﬂy in opposite directions and
stop at distance 1 from the original particle. Within the next second,
each of these particles again divides into 2 particles ﬂying in opposite
directions and stopping at distance 1 from the point of division, and so
on. Whenever particles meet they annihilate (leaving nothing behind).
How many particles will there be at time 211 + 1?
10. ∗Let M ⊆R be a set of real numbers, such that any nonempty subset
of M has a smallest number and also a largest number. Prove that M
is necessarily ﬁnite.
11. We will prove the following statement by mathematical induction: Let
ℓ1, ℓ2, . . . , ℓn be n ≥2 distinct lines in the plane, no two of which are
parallel. Then all these lines have a point in common.
1. For n = 2 the statement is true, since any 2 nonparallel lines intersect.
2.
Let the statement hold for n = n0, and let us have n = n0 + 1
lines ℓ1, . . . , ℓn as in the statement. By the inductive hypothesis, all these
lines but the last one (i.e. the lines ℓ1, ℓ2, . . . , ℓn−1) have some point
in common; let us denote this point by x. Similarly the n −1 lines
ℓ1, ℓ2, . . . , ℓn−2, ℓn have a point in common; let us denote it by y. The
line ℓ1 lies in both groups, so it contains both x and y. The same is true
for the line ℓn−2. Now ℓ1 and ℓn−2 intersect at a single point only, and
so we must have x = y. Therefore all the lines ℓ1, . . . , ℓn have a point in
common, namely the point x.
Something must be wrong. What is it?

1.4 Functions
25
12. Let n1, n2, . . . , nk be natural numbers, each of them at least 1, and let
n1+n2+· · ·+nk = n. Prove that n2
1+n2
2+· · ·+n2
k ≤(n−k+1)2+k−1.
“Solution”: In order to make k
i=1 n2
i as large as possible, we must set
all the ni but one to 1. The remaining one is therefore n −k + 1, and in
this case the sum of squares is (n −k + 1)2 + k −1.
Why isn’t this a valid proof? ∗Give a correct proof.
13. ∗Give a correct proof for Problem 1.3.2.
14. ∗Let n > 1 and k be given natural numbers. Let I1, I2, . . . , Im be
closed intervals (not necessarily all distinct), such that for each interval
Ij = [uj, vj], uj and vj are natural numbers with 1 ≤uj < vj ≤n,
and, moreover, no number is contained in more than k of the intervals
I1, . . . , Im. What is the largest possible value of m?
1.4
Functions
The notion of a function is a basic one in mathematics. It took a long
time for today’s view of functions to emerge. For instance, around
the time when diﬀerential calculus was invented, only real or com-
plex functions were considered, and an “honest” function had to be
expressed by some formula, such as f(x) = x2 + 4, f(x) =

sin(x/π),
f(x) =
 x
0 (sin t/t)dt, f(x) = ∞
n=0(xn/n!), and so on. By today’s stan-
dards, a real function may assign to each real number an arbitrary real
number without any restrictions whatsoever, but this is a relatively
recent invention.
Let X and Y be some quite arbitrary sets. Intuitively, a function f is
“something” assigning a unique element of Y to each element of X.
To depict a function, we can draw the sets X and Y , and draw an
arrow from each element x ∈X to the element y ∈Y assigned to it:
α
β
γ
δ
7
15
8
X
Y
Note that each element of X must have exactly one outgoing
arrow, while the elements of Y may have none, one, or several ingoing
arrows.
Instead of saying that a function is “something”, it is better to
deﬁne it using objects we already know, namely sets and ordered
pairs.

26
Introduction and basic concepts
1.4.1 Definition. A function f from a set X into a set Y is a set
of ordered pairs (x, y) with x ∈X and y ∈Y (in other words, a
subset of the Cartesian product X × Y ), such that for any x ∈X, f
contains exactly one pair with ﬁrst component x.
Of course, an ordered pair (x, y) being in f means just that the
element x is assigned the element y. We then write y = f(x), and
we also say that f maps x to y or that y is the image of x.
For instance, the function depicted in the above picture consists
of the ordered pairs (α, 8), (β, 8), (γ, 15) and (δ, 8).
A function, as a subset of the Cartesian product X × Y , is also
drawn using a graph. We depict the Cartesian product as in Fig. 1.1,
and then we mark the ordered pairs belonging to the function. This
is perhaps the most usual way used in high school or in calculus. The
following picture shows a graph of the function f : R →R given by
f(x) = x3 −x + 1:
-1
1
1
2
The fact that f is a function from a set X into a set Y is written
as follows:
f : X →Y .
And the fact that the function f assigns some element y to an ele-
ment x can also be written
f : x →y.
We could simply write y = f(x) instead. So why this new notation?
The symbol →is advantageous when we want to speak about some
function without naming it. (Those who have programmed in LISP,
Mathematica, or a few other programming languages might recall the
existence of unnamed functions in these languages.) For instance, it is
not really correct to say “consider the function x2”, since we do not say
what the variable is. In this particular case, one can be reasonably sure
that we mean the function assigning x2 to each real number x, but if we
say “consider the function zy2 + 5z3y”, it is not clear whether we mean
the dependence on y, on z, or on both. By writing y →zy2 + 5z3y, we
indicate that we want to study the dependence on y, treating z as some
parameter.

1.4 Functions
27
Instead of “function”, the words “mapping” or “map” are used with
the same meaning.8
Sometimes we also write f(X) for the set {f(x): x ∈X} (the
set of those elements of Y that are images of something). Also other
terms are usually introduced for functions. For example, X is called
the domain and Y is the range, etc., but here we try to keep the
terminology and formalism to a minimum.
We deﬁnitely need to mention that functions can be composed.
1.4.2 Definition (Composition of functions). If f : X →Y and
g: Y →Z are functions, we can deﬁne a new function h: X →Z by
h(x) = g(f(x))
for each x ∈X. In words, to ﬁnd the value of h(x), we ﬁrst apply f
to x and then we apply g to the result.
The function h (check that h is indeed a function) is called the
composition of the functions g and f and it is denoted by g ◦f. We
thus have

g ◦f

(x) = g

f(x)

for each x ∈X.
The composition of functions is associative but not commutative.
For example, if g ◦f is well deﬁned, f ◦g need not be. In order that
two functions can be composed, the “middle set” must be the same.
Composing functions can get quite exciting. For example, consider
the mapping f : R2 →R2 (i.e. mapping the plane into itself) given by
f : (x, y) →

sin(ax) + b sin(ay), sin(cx) + d sin(cy)

with a = 2.879879, b = 0.765145, c = −0.966918, d = 0.744728. Except
for the rather hairy constants, this doesn’t look like a very complicated
function. But if one takes the initial point p = (0.1, 0.1) and plots
the ﬁrst several hundred thousand or million points of the sequence p,
f(p), f(f(p)), f(f(f(p))),. . . , a picture like Fig. 1.2 emerges.9 This is
8In some branches of mathematics, the word “function” is reserved for func-
tion into the set of real or complex numbers, and the word mapping is used for
functions into arbitrary sets. For us, the words “function” and “mapping” will
be synonymous.
9To be quite honest, the way such pictures are generated by a computer is
actually by iterating an approximation to the mapping given by the formula,
because of limited numerical precision.

28
Introduction and basic concepts
Fig. 1.2 The “King’s Dream” fractal (formula taken from the book by C.
Pickover: Chaos in Wonderland, St Martin’s Press, New York 1994).
one of the innumerable species of the so-called fractals. There seems
to be no universally accepted mathematical deﬁnition of a fractal, but
fractals are generally understood as complicated point sets deﬁned by
iterations of relatively simple mappings. The reader can ﬁnd colorful
and more sophisticated pictures of various fractals in many books on
the subject or download them from the Internet. Fractals can be not
only pleasant to the eye (and suitable for killing an unlimited amount of
time by playing with them on a personal computer) but also important
for describing various phenomena in nature.
After this detour, let us return to the basic deﬁnitions concerning
functions.
1.4.3 Definition (Important special types of functions).
A
function f : X →Y is called
• a one-to-one function if x ̸= y implies f(x) ̸= f(y),
• a function onto if for every y ∈Y there exists x ∈X satisfying
f(x) = y, and
• a bijective function, or bijection, if f is one-to-one and onto.
A one-to-one function is also called an injective function or an
injection, and a function onto is also called a surjective function or
a surjection.

1.4 Functions
29
In a pictorial representation of functions by arrows, these types
of functions can be recognized as follows:
• for a one-to-one function, each point y ∈Y has at most one
ingoing arrow,
• for a function onto, each point y ∈Y has at least one ingoing
arrow, and
• for a bijection, each point y ∈Y has exactly one ingoing arrow.
The fact that a function f : X →Y is one-to-one is sometimes
expressed by the notation
f : X	→Y.
The 	→symbol is a combination of the inclusion sign ⊂with the map-
ping arrow →. Why? If f : X	→Y is an injective mapping, then the set
Z = f(X) can be regarded as a “copy” of the set X within Y (since f
considered as a map X →Z is a bijection), and so an injective map-
ping f : X	→Y can be thought of as a “generalized inclusion” of X in
Y . This point can probably be best appreciated in more abstract and
more advanced parts of mathematics like topology or algebra.
There are also symbols for functions onto and for bijections, but
these are still much less standard in the literature than the symbol for
an injective function, so we do not introduce them.
Since we will be interested in counting objects, bijections will be
especially signiﬁcant for us, for the following reason: if X and Y are sets
and there exists a bijection f : X →Y , then X and Y have the same
number of elements. Let us give a simple example of using a bijection
for counting (more sophisticated ones come later).
1.4.4 Example. How many 8-digit sequences consisting of digits 0
through 9 are there? How many of them contain an even number of
odd digits?
Solution.
The answer to the ﬁrst question is 108. One easy way of
seeing this is to note that each eight-digit sequence can be read as the
decimal notation of an integer number between 0 and 108 −1, and
conversely, each such integer can be written in decimal notation and, if
necessary, padded with zeros on the left to produce an 8-digit sequence.
This deﬁnes a bijection between the set {0, 1, . . . , 108 −1} and the set
of all 8-digit sequences.
Well, this bijection was perhaps too simple (or, rather, too custom-
ary) to impress anyone. What about the 8-digit sequences with an even
number of odd digits? Let E be the set of all these sequences (E for
“even”), and let O be the remaining ones, i.e. those with an odd num-
ber of odd digits. Consider any sequence s ∈E, and deﬁne another
sequence, f(s), by changing the ﬁrst digit of s: 0 is changed to 1, 1 to
2,. . . , 8 to 9, and 9 to 0. It is easy to check that the modiﬁed sequence

30
Introduction and basic concepts
f(s) has an odd number of odd digits and hence f is a mapping from
E to O. From two diﬀerent sequences s, s′ ∈E, we cannot get the same
sequence by the described modiﬁcation, so f is one-to-one. And any
sequence t ∈O is obtained as f(s) for some s ∈E, i.e. s arises from
t by changing the ﬁrst digit “back”, by replacing 1 by 0, 2 by 1,. . . ,
9 by 8, and 0 by 9. Therefore, f is a bijection and |E| = |O|. Since
|E| + |O| = 108, we ﬁnally have |E| = 5 · 107.
2
In the following proposition, we prove some simple properties of
functions.
Proposition.
Let f : X →Y and g: Y →Z be functions. Then
(i) If f, g are one-to-one, then g ◦f is also a one-to-one function.
(ii) If f, g are functions onto, then g ◦f is also a function onto.
(iii) If f, g are bijective functions, then g ◦f is a bijection as well.
(iv) For any function f : X →Y there exist a set Z, a one-to-one func-
tion h: Z	→Y , and a function onto g: X →Z, such that f = h ◦g.
(So any function can be written as a composition of a one-to-one
function and a function onto.)
Proof. Parts (i), (ii), (iii) are obtained by direct veriﬁcation from the
deﬁnition. As an example, let us prove (ii).
We choose z ∈Z, and we are looking for an x ∈X satisfying
(g ◦f)(x) = z. Since g is a function onto, there exists a y ∈Y such that
g(y) = z. And since f is a function onto, there exists an x ∈X with
f(x) = y. Such an x is the desired element satisfying (g ◦f)(x) = z.
The most interesting part is (iv). Let Z = f(X) (so Z ⊆Y ). We
deﬁne mappings g: X →Z and h: Z →Y as follows:
g(x) = f(x)
for x ∈X
h(z) = z
for z ∈Z.
Clearly g is a function onto, h is one-to-one, and f = h ◦g.
2
Finishing the remaining parts of the proof may be a good exercise
for understanding the notions covered in this section.
Inverse function.
If f : X →Y is a bijection, we can deﬁne a
function g: Y →X by setting g(y) = x if x is the unique element
of X with y = f(x). This g is called the inverse function of f, and
it is commonly denoted by f−1. Pictorially, the inverse function is
obtained by reversing all the arrows. Another equivalent deﬁnition
of the inverse function is given in Exercise 4. It may look more com-
plicated, but from a certain “higher” mathematical point of view it
is better.

1.4 Functions
31
Exercises
1. Show that if X is a ﬁnite set, then a function f : X →X is one-to-one
if and only if it is onto.
2. Find an example of:
(a) A one-to-one function f : N	→N which is not onto.
(b) A function f : N →N which is onto but not one-to-one.
3. Decide which of the following functions Z →Z are injective and which
are surjective: x →1 + x, x →1 + x2, x →1 + x3, x →1 + x2 + x3.
Does anything in the answer change if we consider them as functions
R →R? (You may want to sketch their graphs and/or use some
elementary calculus methods.)
4. For a set X, let idX : X →X denote the function deﬁned by idX(x) =
x for all x ∈X (the identity function). Let f : X →Y be some func-
tion. Prove:
(a) A function g: Y →X such that g ◦f = idX exists if and only if f
is one-to-one.
(b) A function g: Y →X such that f ◦g = idY exists if and only if f
is onto.
(c) A function g: Y →X such that both f ◦g = idY and g ◦f = idX
exist if and only if f is a bijection.
(d) If f : X →Y is a bijection, then the following three conditions are
equivalent for a function g: Y →X:
(i) g = f −1,
(ii) g ◦f = idX, and
(iii) f ◦g = idY .
5. (a) If g ◦f is an onto function, does g have to be onto? Does f have
to be onto?
(b) If g◦f is a one-to-one function, does g have to be one-to-one? Does
f have to be one-to-one?
6. Prove that the following two statements about a function f : X →Y
are equivalent (X and Y are some arbitrary sets):
(i) f is one-to-one.
(ii) For any set Z and any two distinct functions g1 : Z →X and
g2 : Z →X, the composed functions f ◦g1 and f ◦g2 are also distinct.
(First, make sure you understand what it means that two functions
are equal and what it means that they are distinct.)
7. In everyday mathematics, the number of elements of a set is under-
stood in an intuitive sense and no deﬁnition is usually given. In the
logical foundations of mathematics, however, the number of elements

32
Introduction and basic concepts
is deﬁned via bijections: |X| = n means that there exists a bijection
from X to the set {1, 2, . . . , n}. (Also other, alternative deﬁnitions of
set size exist but we will consider only this one here.)
(a) Prove that if X and Y have the same size according to this deﬁni-
tion, then there exists a bijection from X to Y .
(b) Prove that if X has size n according to this deﬁnition, and there
exists a bijection from X to Y , then Y has size n too.
(c) ∗Prove that a set cannot have two diﬀerent sizes m and n, m ̸= n,
according to this deﬁnition. Be careful not to use the intuitive notion
of “size” but only the deﬁnition via bijections. Proceed by induction.
1.5
Relations
It is remarkable how many mathematical notions can be expressed using
sets and various set-theoretic constructions. It is not only remarkable
but also surprising, since set theory, and even the notion of a set itself,
are notions which appeared in mathematics relatively recently, and some
100 years ago, set theory was rejected even by some prominent mathe-
maticians. Today, set theory has entered the mathematical vocabulary
and it has become the language of all mathematics (and mathemati-
cians), a language which helps us to understand mathematics, with all
its diversity, as a whole with common foundations.
We will show how more complicated mathematical notions can
be built using the simplest set-theoretical tools. The key notion of a
relation,10 which we now introduce, is a common generalization of such
diverse notions as equivalence, function, and ordering.
1.5.1 Definition. A relation is a set of ordered pairs.11 If X and
Y are sets, any subset of the Cartesian product X × Y is called a
relation between X and Y . The most important case is X = Y ;
then we speak of a relation on X, which is thus an arbitrary subset
R ⊆X × X.
If an ordered pair (x, y) belongs to a relation R, i.e. (x, y) ∈R,
we say that x and y are related by R, and we also write xRy.
We have already seen an object which was a subset of a Cartesian
product, namely a function. Indeed, a function is a special type of
relation, where we require that any x ∈X is related to precisely one
10As a mathematical object; you know “relation” as a word in common lan-
guage.
11In more detail, we could say a binary relation (since pairs of elements are
being related). Sometimes also n-ary relations are considered for n ̸= 2.

1.5 Relations
33
1
2
3
4
1
2
3
4
Fig. 1.3 A graphic presentation of the relation R = {(1, 2), (2, 4), (3, 2),
(4, 2), (4, 4)} on the set {1, 2, 3, 4}.
y ∈Y . In a general relation, an x ∈X can be related to several
elements of Y , or also to none.
Many symbols well known to the reader can be interpreted as rela-
tions in this sense. For instance, = (equality) and ≥(nonstrict inequal-
ity) are both relations on the set N of all natural numbers. The ﬁrst
one consists of the pairs (1, 1), (2, 2), (3, 3), . . ., the second one of the
pairs (1, 1), (2, 1), (2, 2), (3, 1), (3, 2), (3, 3), (4, 1), . . . We could thus
also write (5, 2) ∈≥instead of the usual 5 ≥2, which we usually don’t
do, however. Note that we had to specify the set on which the relation
≥, say, is considered: as a relation on R it would be a quite diﬀerent
set of ordered pairs.
Many interesting “real life” examples of relations come from various
kinds of relationships among people, e.g. “to be the mother of”, “to be
the father of”, “to be a cousin of” are relations on the set of all people,
usually well deﬁned although not always easy to determine.
A relation R on a set X can be captured pictorially in (at least)
two quite diﬀerent ways. The ﬁrst way is illustrated in Fig. 1.3. The
little squares correspond to ordered pairs in the Cartesian product,
and for pairs belonging to the relation we have shaded the corre-
sponding squares. This kind of picture emphasizes the deﬁnition of
a relation on X and it captures its “overall shape”.
This picture is also very close in spirit to an alternative way of
describing a relation on a set X using the notion of a matrix.12 If R
is a relation on some n-element set X = {x1, x2, . . . , xn} then R is
12An n × m matrix is a rectangular table of numbers with n rows and m
columns. Any reader who hasn’t met matrices yet can consult the Appendix for
the deﬁnitions and basic facts, or, preferably, take a course of linear algebra or
refer to a good textbook.

34
Introduction and basic concepts
completely described by an n × n matrix A = (aij), where
aij = 1
if (xi, xj) ∈R
aij = 0
if (xi, xj) ̸∈R.
The matrix A is called the adjacency matrix of the relation R. For
instance, for the relation in Fig. 1.3, the corresponding adjacency matrix
would be




0
1
0
0
0
0
0
1
0
1
0
0
0
1
0
1



.
Note that this matrix is turned by 90 degrees compared to Fig. 1.3. This
is because, for a matrix element, the ﬁrst index is the number of a row
and the second index is the number of a column, while for Cartesian
coordinates it is customary for the ﬁrst coordinate to determine the
horizontal position and the second coordinate the vertical position.
The adjacency matrix is one possible computer representation of a
relation on a ﬁnite set.
Another picture of the same relation as in Fig. 1.3 is shown below:
1
2
3
4
Here the dots correspond to elements of the set X. The fact that
a given ordered pair (x, y) belongs to the relation R is marked by
drawing an arrow from x to y:
x
y
and, in the case x = y, by a loop:
x
A relation between X and Y can be depicted in a similar way:
r
r
r
r




X
r
r
r




Y
R
-
QQQQQQQ
s
-

1

3

1.5 Relations
35
This way was suggested for drawing functions in Section 1.4.
Composition of relations. Let X, Y, Z be sets, let R ⊆X × Y be a
relation between X and Y , and let S ⊆Y × Z be a relation between
Y and Z. The composition of the relations R and S is the relation
T ⊆X × Z deﬁned as follows: for given x ∈X and z ∈Z, xTz holds
if and only if there exists some y ∈Y such that xRy and ySz. The
composition of relations R and S is usually denoted by R ◦S.
The composition of relations can be nicely illustrated using a draw-
ing with arrows. In the following picture,
r
r
r
r




X
r
r
r




Y
R
-
QQQQQQQ
s
-

1

3
r
r
r
r
r
S
Z

1
-

1
PPPPPPP
q

3




a pair (x, z) is in the relation R ◦S whenever one can get from x to z
along the arrows (i.e. via some y ∈Y ).
Have you noticed? Relations are composed in the same way as func-
tions, but the notation is unfortunately diﬀerent! For relations it is cus-
tomary to write the composition “from left to right”, and for functions
it is usually written “from right to left”. So if f : X →Y and g: Y →Z
are functions, their composition is written g ◦f, but if we understood
them as relations, we would write f ◦g for the same thing! Both ways of
notation have their reasons, such a notation has been established his-
torically, and probably there is no point in trying to change it. In this
text, we will talk almost exclusively about composing functions.
Similarly as for functions, the composition is not deﬁned for arbi-
trary two relations. In order to compose relations, they must have the
“middle” set in common (which was denoted by Y in the deﬁnition).
In particular, it may happen that R ◦S is deﬁned while S ◦R makes
no sense! However, if both R and S are relations on the same set X,
their composition is always well deﬁned. But also in this case the result
of composing relations depends on the order, and R ◦S is in general
diﬀerent from S ◦R—see Exercise 2.
Exercises
1. Describe the relation R ◦R, if R stands for
(a) the equality relation “=” on the set N of all natural numbers,
(b) the relation “less than or equal” (“≤”) on N,

36
Introduction and basic concepts
(c) the relation “strictly less” (“<”) on N,
(d) the relation “strictly less” (“<”) on the set R of all real numbers.
2. Find relations R, S on some set X such that R ◦S ̸= S ◦R.
3. For a relation R on a set X we deﬁne the symbol Rn by induction:
R1 = R, Rn+1 = R ◦Rn.
(a) Prove that if X is ﬁnite and R is a relation on it, then there exist
r, s ∈N, r < s, such that Rr = Rs.
(b) Find a relation R on a ﬁnite set such that Rn ̸= Rn+1 for every
n ∈N.
(c) Show that if X is inﬁnite, the claim (a) need not hold (i.e. a relation
R may exist such that all the relations Rn, n ∈N, are distinct).
4. (a) Let X = {x1, x2, . . . , xn} and Y = {y1, y2, . . . , ym} be ﬁnite sets,
and let R ⊆X × Y be a relation. Generalize the deﬁnition of the
adjacency matrix of a relation to this case.
(b) ∗Let X, Y, Z be ﬁnite sets, let R ⊆X × Y and S ⊆Y × Z be rel-
ations, and let AR and AS be their adjacency matrices, respectively.
If you have deﬁned the adjacency matrix in (a) properly, the matrix
product ARAS should be well deﬁned. Discover and describe the con-
nection of the composed relation R ◦S to the matrix product ARAS.
5. Prove the associativity of composing relations: if R, S, T are relations
such that (R◦S)◦T is well deﬁned, then R◦(S ◦T) is also well deﬁned
and equals (R ◦S) ◦T.
1.6
Equivalences and other special
types of relations
Each language has its peculiarities. Some languages favor wovels, others
love consonants. Some have a simple grammar, others have an easy
pronunciation. The situation in mathematics is similar. The language
of mathematics in itself has its essential properties (it is exact and
details matter very much in it, perhaps even too much), but the various
branches of mathematics diﬀer by style and language. For example, a
typical feature of algebra seems to be a large number of deﬁnitions
and notions, which usually appear at the beginning, before anything
begins to “really happen”. However, many of these algebraic notions
show up in other subﬁelds as well and they belong to the vocabulary
of mathematics in general. Here we present an example of how such
notions can be introduced. This section essentially belongs to algebra
and it concerns various special kinds of relations.
We recall that functions are regarded as relations of a special
kind. Now we are going to deﬁne four properties that a relation may
or may not have. They are so useful that each deserves a name, and

1.6 Equivalences and other special types of relations
37
they will in turn be used for deﬁning equivalences and orderings,
which together with functions are arguably the most important spe-
cializations of the general concept of relation.
1.6.1 Definition. We say that a relation R on a set X is
• reﬂexive if xRx for every x ∈X;
• symmetric if xRy implies yRx, for all x, y ∈X;
• antisymmetric13 if, for every x, y ∈X, xRy and yRx never hold
simultaneously unless x = y;
• transitive if xRy and yRz imply xRz, for all x, y, z ∈X.
In a drawing like that in Fig. 1.3, a reﬂexive relation is one con-
taining all squares on the diagonal (drawn by a dotted line). In draw-
ing using arrows, a reﬂexive relation has loops at all points.
For a symmetric relation, a picture of the type in Fig. 1.3 has
the diagonal as an axis of symmetry. In a picture using arrows, the
arrows between two points always go in both directions:
x
y
In contrast, this situation is prohibited in an antisymmetric
relation:
x
y
The condition of transitivity can be well explained using arrows.
If there are arrows x →y and y →z, then the x →z arrow is present
as well:
x
y
z
The pictures for reﬂexivity, symmetry, antisymmetry, and transi-
tivity using arrows emphasize the fact that these properties are easy
to verify (in principle), since they are deﬁned using two-element and
three-element subsets.
13Sometimes this is called weakly antisymmetric, while for a strongly antisym-
metric relation xRy and yRx never happen at the same time, i.e. xRx is also
excluded.

38
Introduction and basic concepts
These properties can also be described using the operation of
composing relations (see Section 1.5) plus the following two notions:
The inverse relation R−1 to a given relation R is given by R−1 =
{(y, x): (x, y) ∈R}. It arises by “reversing arrows” in R.
The symbol ∆X denotes the smallest reﬂexive relation on a set X:
∆X = {(x, x): x ∈X}.
The relation ∆X is called the diagonal (on the set X). The name
is motivated by the matrix-like picture of a relation discussed in
Section 1.5.
With these tools, Deﬁnition 1.6.1 can be concisely reformulated
as follows:
(1) R is reﬂexive if ∆X ⊆R.
(2) R is symmetric if R = R−1.
(3) R is antisymmetric if R ∩R−1 ⊆∆X.
(4) R is transitive if R ◦R ⊆R.
Now we can deﬁne equivalences, orderings, and linear orderings.
1.6.2 Definition.
• A relation R on a set X is called an equivalence on X (or some-
times an equivalence relation) if it is reﬂexive, symmetric, and
transitive.
• A relation R on a set X is called an ordering on X if it is reﬂexive,
antisymmetric, and transitive.
• A relation R on a set X is called a linear ordering on X if it is
an ordering and moreover, R∪R−1 = X ×X (or in other words,
for every two elements x, y ∈X we have xRy or yRx).
It may seem at ﬁrst sight that the diﬀerences between the three
notions just deﬁned are minor and insigniﬁcant. This impression is
quite misleading, though, and it illustrates a general rule: One has
to watch every word in a mathematical deﬁnition. The notions of
equivalence and of ordering are in fact quite remote from each other,
so remote that they are usually studied separately. This is how we
will also proceed: Equivalences will be discussed in the rest of the
present section, while orderings will be covered later, in Chapter 2.
For a quick illustration we present initial examples; some of them
will be discussed later in more detail. We consider three relations on

1.6 Equivalences and other special types of relations
39
the set N of all natural numbers:
• The relation R deﬁned by xRy if x −y is an even integer is an
equivalence (but not an ordering).
• The relation | given by x|y if x divides y, i.e. if there exists a
natural number q such that qx = y, is an ordering (but neither
an equivalence nor a linear ordering).
• The relation ≤, where x ≤y has the usual meaning, i.e. that the
number x is less or equal to y, is a linear ordering (and thus an
ordering as well, but not an equivalence).
The reader is now welcome to check that the just deﬁned relations
satisfy the appropriate conditions in Deﬁnition 1.6.2.
Equivalences. Informally, an equivalence on a set X is a relation
describing which pairs of elements of X are “of the same type” in
some sense. The notion of equivalence is a common generalization of
notions expressing identity, isomorphism, similarity, etc. Equivalence
relations are often denoted by symbols like =, ≡, ≃, ≈, ∼=, and so
on. The reader may want to contemplate for a while why the prop-
erties deﬁning an equivalence in general (reﬂexivity, symmetry, and
transitivity) are natural for a relation that should express something
like “being of the same type”.
Let us consider some geometric examples. Let X be the set of
all triangles in the plane. By saying that two triangles are related if
and only if they are congruent (i.e. one can be transformed into the
other by translation and rotation), we have deﬁned one equivalence
on X. Another equivalence is deﬁned by relating all pairs of similar
triangles (two triangles are similar if one can be obtained from the
other one by translation, rotation, and scaling; in other words, if
their corresponding angles are the same). And a third equivalence
arises by saying that each triangle is only related to itself.
Although an equivalence R on a set X is a special type of relation
and we can thus depict it by either of the two methods considered
above for relations in general, more often a picture similar to the one
below is used:
!!!
```
E
E
E
E
EE





@
@
@
'
&
$
%

40
Introduction and basic concepts
The key to this type of drawing is the following notion of equiv-
alence class. Let R be an equivalence on a set X and let x be an
element of X. By the symbol R[x], we denote the set of all elements
y ∈X that are equivalent to x; in symbols, R[x] = {y ∈X : xRy}.
R[x] is called the equivalence class of R determined by x.
1.6.3 Proposition. For any equivalence R on X, we have
(i) R[x] is nonempty for every x ∈X.
(ii) For any two elements x, y ∈X, either R[x] = R[y] or R[x] ∩
R[y] = ∅.
(iii) The equivalence classes determine the relation R uniquely.
Before we start proving this, we should explain the meaning of
(iii). It means the following: if R and S are two equivalences on X
and if the equality R[x] = S[x] holds for every element x ∈X, then
R = S.
Proof.
The proof is simple using the three requirements in the
deﬁnition of equivalence.
(i) The set R[x] always contains x since R is a reﬂexive relation.
(ii) Let x, y be two elements. We distinguish two cases:
(a) If xRy, then we prove R[x] ⊆R[y] ﬁrst. Indeed, if z ∈R[x],
then we also know that zRx (by symmetry of R) and there-
fore zRy (by transitivity of R). Thus also z ∈R[y]. By using
symmetry again, we get that xRy implies R[x] = R[y].
(b) Suppose that xRy doesn’t hold. We show that R[x]∩R[y] =
∅. We proceed by contradiction. Suppose that there exists
z ∈R[x] ∩R[y]. Then xRz and zRy (by symmetry of R),
and so xRy (by transitivity of R), which is a contradiction.
(iii) This part is obvious, since the equivalence classes determine R
as follows:
xRy if and only if {x, y} ⊆R[x].
2
The above proposition explains the preceding picture. It guaran-
tees that the equivalence classes form a partition of the set X; that
is, they are disjoint subsets of X whose union is the whole X. Con-
versely, any partition of X determines exactly one equivalence on X.
That is, there exists a bijective mapping of the set of all equivalences
on X onto the set of all partitions of X.

1.6 Equivalences and other special types of relations
41
Exercises
1. Formulate the conditions for reﬂexivity of a relation, for symmetry of
a relation, and for its transitivity using the adjacency matrix of the
relation.
2. ∗Prove that a relation R on a set X satisﬁes R◦R−1 = ∆X if and only
if R is reﬂexive and antisymmetric.
3. Prove that a relation R is transitive if and only if R ◦R ⊆R.
4. (a) Prove that for any relation R, the relation T = R ∪R ◦R ∪R ◦R ◦
R ∪. . . (the union of all multiple compositions of R) is transitive.
(b) Prove that any transitive relation containing R as a subset also
contains T.
(c) Prove that if |X| = n, then T = R ∪R ◦R ∪· · · ∪R ◦R ◦· · · ◦R



(n−1)×
.
Remark. The relation T as in (a), (b) is the smallest transitive relation
containing R, and it is called the transitive closure of R.
5. Let R and S be arbitrary equivalences on a set X. Decide which of
the following relations are necessarily also equivalences (if yes, prove;
if not, give a counterexample).
(a) R ∩S
(b) R ∪S
(c) R \ S
(d) R ◦S.
6. Describe all relations on a set X that are equivalences and orderings
at the same time.
7. Let R and S be arbitrary orderings on a set X. Decide which of the
following relations are necessarily orderings:
(a) R ∩S
(b) R ∪S
(c) R \ S
(d) R ◦S.
8. (a) Suppose that R is a transitive relation on the set Z of all integers,
and we know that for any two integers a, b ∈Z, if |a −b| = 2 then
aRb. Is every R satisfying these conditions necessarily an equivalence?
(Note that a pair of elements can perhaps be in R even if it is not
enforced by the given conditions!)
(b) Suppose that R is a transitive relation on Z, and we know that for
any two integers a, b ∈Z, if |a −b| ∈{3, 4} then aRb. Is R necessarily
an equivalence?

42
Introduction and basic concepts
9. Call an equivalence ∼on the set Z (the integers) a congruence if the
following condition holds for all a, x, y ∈Z: if x ∼y then also a + x ∼
a + y.
(a) Let q be a nonzero integer. Deﬁne a relation ≡q on Z by letting
x ≡q y if and only if q divides x −y. Check that ≡q is a congruence
according to the above deﬁnition.
(b) ∗Prove that any congruence on Z is either of the form ≡q for some
q or the diagonal relation ∆Z.
(c) Suppose we replaced the condition “a+x ∼a+y” in the deﬁnition
of a congruence by “ax ∼ay”. Would the claim in (a) remain true for
this kind of “multiplicative congruence”? ∗And how about the claim
in (b)?

2
Orderings
The reader will certainly be familiar with the ordering of natural
numbers and of other number domains by magnitude (the “usual”
ordering of numbers). In mathematics, such an ordering is considered
as a special type of a relation, i.e. a set of pairs of numbers. In the
case just mentioned, this relation is usually denoted by the symbol
“≤” (“less than or equal”). Various orderings can be deﬁned on other
sets too, such as the set of all words in a language, and one set can
be ordered in many diﬀerent and perhaps exotic ways.
The general notion of ordering has already been introduced in
Deﬁnition 1.6.1: A relation R is called an ordering if it is reﬂexive,
antisymmetric, and transitive. Let us also add that if X is a set and
R is an ordering on X, the pair (X, R) is called an ordered set.
Ordered sets have numerous interesting properties. We mention
some of them in this chapter, and we will encounter a few others
later on, most notably in Chapter 7.
2.1
Orderings and how they can be depicted
We begin with several remarks concerning the notion of ordering.
Orderings are commonly denoted by the symbols ⪯or ≤. The ﬁrst
of them is useful, e.g. when we want to speak of some ordering of the
set of natural numbers other than the usual ordering by magnitude,
or if we consider some arbitrary ordering on a general set.
If we have some ordering ⪯, we deﬁne a derived relation of “strict
inequality”, ≺, as follows: a ≺b if and only if a ⪯b and a ̸= b.
Further, we can introduce the “reverse inequality” ⪰, by letting a ⪰b
if and only if b ⪯a.
Linear orderings and partial orderings.
Let us recall that a
relation R is called a linear ordering if it is an ordering, and moreover,
for every x, y we have xRy or yRx.

44
Orderings
In order to emphasize that we speak of an ordering that is not
necessarily linear, we sometimes use the longer term partial ordering.
A partial ordering thus means exactly the same as ordering (without
further adjectives), and so a partial ordering may also happen to be
linear. Similarly, instead of an ordered set, one sometimes speaks of
a partially ordered set. To abbreviate this long term, the artiﬁcial
word poset is frequently used.
Examples. We have already mentioned several examples of ordered
sets—these were (N, ≤), (R, ≤), and similar ones, where ≤denotes
the usual ordering, formally understood as a relation.
As is easy to check, if R is an ordering on a set X, and Y ⊆X is
some subset of X, the relation R ∩Y 2 (the restriction of R on Y ) is
an ordering on Y . Intuitively, we order the elements of Y in the same
way as before but we forget the others. This yields further examples
of ordered sets, namely various subsets of real numbers with the usual
ordering. This turns out to be a rather general example of a linearly
ordered set; see Exercise 2.3.6.
The idea of alphabetic order of words in a dictionary is formally
captured by the notion of lexicographic ordering . Let us ﬁrst consider
a particular case: Let X = N × N be the Cartesian product of two
copies of the natural numbers, that is, the set of all ordered pairs
(a1, a2), where a1 and a2 are natural numbers. We deﬁne the relation
≤lex of lexicographic ordering on X as follows: (a1, a2) ≤lex (b1, b2) if
either a1 < a2, or a1 = a2 and a2 ≤b2. More generally, if (X1, ≤1),
(X2, ≤2),. . . , (Xn, ≤n) are arbitrary linearly ordered sets, we deﬁne
the relation ≤lex of lexicographic ordering of the Cartesian product
X1 × X2 × · · · × Xn in the following way:
(a1, a2, . . . , an) ≤lex (b1, b2, . . . , bn)
holds if (a1, a2, . . . , an) = (b1, b2, . . . , bn), or if there exists an index
i ∈{1, 2, . . . , n} such that aj = bj for all j < i and ai <i bi. It is easy
to see that the alphabetic ordering of words is an example of a lexico-
graphic ordering in our sense, although at a closer look, we discover
various complications: for example, words have various lengths, not
speaking of ﬁne points such as “van Beethoven” occurring under B
in encyclopedias.
Examples of partially ordered sets.
What do orderings that
are not linear look like? One example is the relation ∆X = {(x, x):x

2.1 Orderings and how they can be depicted
45
∈X} (the diagonal). It satisﬁes the deﬁnition of an ordering, but for
|X| > 1 it is not a linear ordering.
Let us describe more interesting examples of partially ordered
sets.
2.1.1 Example. Let us imagine we intend to buy a refrigerator. We
simplify the complicated real situation by a mathematical abstrac-
tion, and we suppose that we only look at three numerical parameters
of refrigerators: their cost, electricity consumption, and the volume
of the inner space. If we consider two types of refrigerators, and if
the ﬁrst type is more expensive, consumes more power, and a smaller
amount of food ﬁts into it, then the second type can be considered
a better one—a large majority of buyers of refrigerators would agree
with that. On the other hand, someone may prefer a smaller and
cheaper refrigerator, another may prefer a larger refrigerator even if
it costs more, and someone expecting a sharp rise of electricity costs
may even buy an expensive refrigerator if it saves power.
The relation “to be clearly worse” (denote it by ⪯) in this sense
is thus a partial ordering on refrigerators or, mathematically re-
formulated, on the set of triples (c, p, v) of real numbers (c stands for
cost, p for power consumption, and v for volume), deﬁned as follows:
(c1, p1, v1) ⪯(c2, p2, v2) if and only if
c1 ≥c2, p1 ≥p2, and v1 ≤v2.
(2.1)
The following example has already been mentioned, but let us
recall it here:
2.1.2 Example. For natural numbers a, b, the symbol a|b means “a
divides b”. In other words, there exists a natural number c such that
b = ac. The relation “|” is a partial ordering on N (as the reader has
already been invited to verify).
2.1.3 Example. Let X be a set. Recall that the symbol 2X denotes
the system of all subsets of the set X. The relation “⊆” (to be a
subset) deﬁnes a partial ordering on 2X.
Drawing partially ordered sets. Orderings of ﬁnite sets can be
drawn using arrows, as with any other relations. Typically, such
drawings will contains lots of arrows. For instance, for a 10-element
linearly ordered set we would have to draw 10 + 9 + · · · + 1 = 55
arrows and loops. A number of arrows can be reconstructed from

46
Orderings
transitivity, however: if we know that x ⪯y and y ⪯z, then also
x ⪯z, so we may leave out the arrow from x to z. Similarly, we
need not draw the loops, since we know they are always there. For
ﬁnite ordered sets, all the information is captured by the relation of
“immediate predecessor”, which we are now going to deﬁne.
Let (X, ⪯) be an ordered set. We say that an element x ∈X is
an immediate predecessor of an element y ∈X if
• x ≺y, and
• there is no t ∈X such that x ≺t ≺y.
Let us denote the just-deﬁned relation of immediate predecessor
by .
The claim that the ordering ⪯can be reconstructed from the relation
 may be formulated precisely as follows:
2.1.4 Proposition. Let (X, ⪯) be a ﬁnite ordered set, and let  be
the corresponding immediate predecessor relation. Then for any two
elements x, y ∈X, x ≺y holds if and only if there exist elements
x1, x2, . . . , xk ∈X such that x  x1  · · ·  xk  y (possibly with k = 0,
i.e. we may also have x  y).
Proof. One implication is easy to see: if we have x  x1  · · ·  xk  y,
then also x ⪯x1 ⪯· · · ⪯xk ⪯y (since the immediate predecessor
relation is contained in the ordering relation), and by the transitivity
of ⪯, we have x ⪯y.
The reverse implication is not diﬃcult either, and we prove it by
induction. We prove the following statement:
Lemma.
Let x, y ∈X, x ≺y, be two elements such that there exist
at most n elements t ∈X satisfying x ≺t ≺y (i.e. “between” x and y).
Then there exist x1, x2, . . . , xk ∈X such that x  x1  · · ·  xk  y.
For n = 0, the assumption of this lemma asserts that there exists no t
with x ≺t ≺y, and hence xy, which means that the statement holds
(we choose k = 0).
Let the lemma hold for all n up to some n0, and let us have x ≺y
such that the set Mxy = {t ∈X : x ≺t ≺y} has n = n0 + 1 elements.
Let us choose an element u ∈Mxy, and consider the sets Mxu = {t ∈
X : x ≺t ≺u} and Muy deﬁned similarly. By the transitivity of ≺it
follows that Mxu ⊂Mxy and Muy ⊂Mxy. Both Mxu and Muy have at
least one element less than Mxy (since u ̸∈Mxu, u ̸∈Muy), and by the
inductive hypothesis, we ﬁnd elements x1, . . . , xk and y1, . . . , yℓin such
a way that x  x1  · · ·  xk  u and u  y1  · · ·  yℓ y. By combining
these two “chains” we obtain the desired sequence connecting x and y.
2

2.1 Orderings and how they can be depicted
47
By the above proposition, it is enough to draw the relation of
immediate predecessor by arrows. If we accept the convention that
all arrows in the drawing will be directed upwards (this means that
if x ≺y then y is drawn higher than x), we need not even draw the
direction of the arrows—it is enough to draw segments connecting
the points. Such a picture of a partially ordered set is called its Hasse
diagram. The following ﬁgure shows a 7-element linearly ordered set,
such as ({1, 2, . . . , 7}, ≤):
The next drawing depicts the set {1, 2, . . . , 10} ordered by the divis-
ibility relation (see Example 2.1.2):
1
2
4
8
10
7
5
6
3
9
The following ﬁgure is a Hasse diagram of the set {1, 2, 3} × {1, 2, 3}
with ordering ⪯given by the rule (a1, b1) ⪯(a2, b2) if and only if
a1 ≤a2 and b1 ≤b2:
(1, 1)
(1, 2)
(1, 3)
(2, 3)
(3, 3)
(3, 2)
(3, 1)
(2, 1)
(2, 2)

48
Orderings
Finally, here is a Hasse diagram of the set of all subsets of {1, 2, 3}
ordered by inclusion:
∅
{1}
{2}
{3}
{2, 3}
{1, 2, 3}
{1, 2}
{1, 3}
Exercises
1. Verify that the relation (2.1) in Example 2.1.1 indeed deﬁnes a partial
ordering.
2. ∗Let R be a relation on a set X such that there is no ﬁnite sequence
of elements x1, x2, . . . , xk of X satisfying x1Rx2, x2Rx3,. . . , xk−1Rxk,
xkRx1 (we say that such an R is acyclic). Prove that there exists an
ordering ⪯on X such that R ⊆⪯. You may assume that X is ﬁnite if
this helps.
3. Show that Proposition 2.1.4 does not hold for inﬁnite sets.
4. Let (X, ≤), (Y, ⪯) be ordered sets. We say that they are isomorphic
(meaning that they “look the same” from the point of view of ordering)
if there exists a bijection f : X →Y such that for every x, y ∈X, we
have x ≤y if and only if f(x) ⪯f(y).
(a) Draw Hasse diagrams for all nonisomorphic 3-element posets.
(b) Prove that any two n-element linearly ordered sets are isomorphic.
(c)∗Find two nonisomorphic linear orderings of the set of all natural
numbers.
(d)∗∗Can you ﬁnd inﬁnitely many nonisomorphic linear orderings
of N? Uncountably many (for readers knowing something about the
cardinalities of inﬁnite sets)?
2.2
Orderings and linear orderings
Each linear ordering is also a (partial) ordering. The converse state-
ment (“each partial ordering is linear”) is obviously false, as we have
seen in several examples. On the other hand, the following important
theorem holds:
2.2.1 Theorem. Let (X, ⪯) be a ﬁnite partially ordered set. Then
there exists a linear ordering ≤on X such that x ⪯y implies x ≤y.

2.2 Orderings and linear orderings
49
Each partial ordering can thus be extended to a linear ordering.
The latter is called a linear extension of the former.
Before the proof of Theorem 2.2.1 we introduce yet another imp-
ortant notion.
2.2.2 Definition. Let (X, ⪯) be an ordered set. An element a ∈X
is called a minimal element of (X, ⪯) if there is no x ∈X such that
x ≺a. A maximal element a is deﬁned analogously (there exists no
x ≻a).
The following holds:
2.2.3 Theorem. Every ﬁnite partially ordered set (X, ⪯) has at
least one minimal element.
Proof. Let us choose x0 ∈X arbitrarily. If x0 is minimal, we are
done. If, on the other hand, x0 is not minimal in (X, ⪯), then there
exist some x1 ≺x0. If x1 is minimal, we are done now, and else, we
ﬁnd some x2 ≺x1, and so on. After ﬁnitely many steps we arrive
at a minimal element, for otherwise, X would have inﬁnitely many
diﬀerent elements x0, x1, x2, . . . .
2
Let us remark that Theorem 2.2.3 is not valid for inﬁnite sets.
For instance, the set (Z, ≤) of integers with the natural ordering has
no minimal element.
The reader may ﬁnd the (algorithmically motivated) proof of The-
orem 2.2.3 suspicious. Instead of explaining it in more detail, we add
another, more usual version of the proof.
Second proof.
Let us consider the ordered set (X, ⪯), and let
us choose an x ∈X such that the set Lx = {y: y ⪯x} has the
smallest possible number of elements. If |Lx| = 1, then we are done,
since x is necessarily a minimal element: We have |Lx| = {x}. We
prove that |Lx| > 1 is impossible. Namely, in this case there exists a
y ∈Lx, y ̸= x, and then we have |Ly| < |Lx|, which contradicts the
choice of the element x.
2
We apply the existence of a minimal element (Theorem 2.2.3) in
the next proof.
Proof of Theorem 2.2.1 (existence of linear extensions). We
proceed by induction on |X|. For |X| = 1 there is nothing to prove:
there is only one ordering of X and it is already linear. Thus, let
us consider an ordered set (X, ⪯) with |X| > 1. Let x0 ∈X be a

50
Orderings
minimal element in (X, ⪯). We set X′ = X \ {x0}, and we let ⪯′
be the relation ⪯restricted to the set X′. We already know that
(X′, ⪯′) is an ordered set, and hence by the inductive hypothesis,
there exists a linear ordering ≤′ of X′ such that x ⪯′ y implies
x ≤′ y for all x, y ∈X′. We deﬁne a relation ≤on the set X as
follows:
x0 ≤y
for each
y ∈X;
x ≤y
whenever
x ≤′ y.
Obviously, x ⪯y implies x ≤y. The reader is invited to verify that
≤is indeed a linear ordering.
2
The existence of linear extensions is important and useful in many
applications. In computer science one often needs to compute a linear
extension of a given partial ordering. This algorithmic problem known
as topological sorting.
Theorem 2.2.1, the existence of linear extensions, is also valid for
inﬁnite sets. But it cannot be proved that easily. Actually, in a sense, it
cannot be proved at all, since it can be regarded as one of the axioms
of set theory (similar to the so-called axiom of choice, to which it is
closely related).
Let us conclude this section with a linguistic warning. A notion
seemingly very similar to minimal element is a smallest element
(sometimes also called a minimum element). But this similarity is
only in the language and the notion itself is quite diﬀerent, as the
following deﬁnition shows.
2.2.4 Definition. Let (X, ⪯) be an ordered set. An element a ∈X
is called a smallest element of (X, ⪯) if for every x ∈X we have
a ⪯x. A largest element (sometimes also called a maximum element)
is deﬁned analogously.
A smallest element is obviously minimal as well. For example, in
the set of all natural numbers ordered by the divisibility relation, i.e.
(N, |), 1 is both a smallest element and a minimal element. But a
minimal element need not always be a smallest element: For exam-
ple, if we consider any set X with at least two elements, and no
two distinct elements are comparable (thus, we deal with the poset
(X, ∆X)), then every element is minimal but there is no smallest
element. Another example is (N \ {1}, |), that is, the natural num-
bers greater than 1 ordered by the relation of divisibility. It has no
smallest element, while there are inﬁnitely many minimal elements;
see Exercise 7.

2.2 Orderings and linear orderings
51
Exercises
1. (a) Show that a largest element is always maximal.
(b) Find an example of a poset with a maximal element but no largest
element.
(c) Find a poset having no smallest element and no minimal element
either, but possessing a largest element.
2. (a) Consider the set {1, 2, . . . , n} ordered by the divisibility relation
| (see Example 2.1.2). What is the maximum possible number of ele-
ments of a set X ⊆{1, 2, . . . , n} that is ordered linearly by the relation
| (such a set X is called a chain)?
(b) Solve the same question for the set 2{1,2,...,n} ordered by the rela-
tion ⊆(see Example 2.1.3).
3. Let le(X, ⪯) denote the number of linear extensions of a partially ord-
ered set (X, ⪯). Prove:
(a) le(X, ⪯) = 1 if and only if ⪯is a linear ordering;
(b) le(X, ⪯) ≤n!, where n = |x| (you may want to read Chapter 3
ﬁrst).
4. Prove that a smallest element, if it exists, is determined uniquely.
5. Prove that for a linearly ordered set, a minimal element is also the
smallest element.
6. Prove or disprove: If a partially ordered set (X, ⪯) has a single minimal
element, then it is a smallest element as well.
7. (a) Prove that the partially ordered set (N\{1}, |) has inﬁnitely many
minimal elements. What are they usually called?
(b) How many minimal elements are there in the ordered set (X, |),
for X = {4k + 2: k ≥2}? Let us remark that the analogous question
for the set {4k +1: k ≥2} ordered by divisibility is considerably more
diﬃcult.
8. Let (X, R) be a partially ordered set. Prove:
(a) Then (X, R−1) is also a partially ordered set.
(b) An element x ∈X is maximal in (X, R) if and only if x is minimal
in (X, R−1).
(c) An element x ∈X is the largest element of (X, R) if and only if x
is the smallest element of (X, R−1).
Since (R−1)−1 = R, we can see that notions for ordered sets come
in symmetric pairs, such as minimal element and maximal element,
smallest element and largest element, etc. We can thus often consider
deﬁnitions and proofs for only one notion in such a pair. For example,

52
Orderings
it suﬃces to prove Theorem 2.2.3 (existence of a minimal element), and
the analogous theorem about the existence of a maximal element fol-
lows automatically—the proof can easily be “translated”. This method
is called, somewhat imprecisely, the duality principle for ordered sets.
9. Let (X, ⪯) be a poset and let A ⊆X be a subset. An element s ∈X
is called a supremum of the set A if the following holds:
• a ⪯s for each a ∈A,
• if a ⪯s′ holds for all a ∈A, where s′ is some element of X, then
s ⪯s′.
The inﬁmum of a subset A ⊆X is deﬁned analogously, but with all
inequalities going in the opposite direction.
(a) Check that any subset A ⊆X has at most one supremum and
at most one inﬁmum. (The supremum of A, if it exists, is denoted by
sup A. Similarly inf A denotes the inﬁmum.)
(b) What element is the supremum of the empty set (according to the
deﬁnition just given)?
(c) Find an example of a poset in which every nonempty subset has
an inﬁmum, but there are nonempty subsets having no supremum.
(d) ∗Let (X, ⪯) be a poset in which every subset (including the empty
one) has a supremum. Show that then every subset has an inﬁmum as
well.
10. Consider the poset (N, |) (ordering by divisibility).
(a) Decide whether each nonempty subset of N has a supremum.
(b) Decide whether each nonempty ﬁnite subset of N has a supremum.
(c) Decide whether each nonempty subset has an inﬁmum.
2.3
Ordering by inclusion
In Section 1.4 we have shown how an equivalence on a set X can
be described by a partition of the set. This correspondence is one-
to-one; we can also say that partitions constitute just a diﬀerent
representation of equivalences. Does anything similar exist for partial
orderings?
The answer seems to be no. The notion of ordering appears to be
much richer and more complex than that of equivalence, and this is
also why we have devoted a special chapter to orderings. However,
in this section we show that we can imagine every partial ordering
as being deﬁned by inclusion, i.e., in a (seemingly) simple way. This
result will be formulated using the following notion.

2.3 Ordering by inclusion
53
2.3.1 Definition. Let (X, ⪯) and (X′, ⪯′) be ordered sets. A
mapping f : X →X′ is called an embedding of (X, ⪯) into (X′, ⪯′)
if the following conditions hold:
(i) f is an injective mapping;
(ii) f(x) ⪯′ f(y) if and only if x ⪯y.
Let us add a few remarks. If f is an embedding that is also surjec-
tive (onto), then it is an isomorphism, which we have already consid-
ered
in
Exercise
2.1.4.
While
isomorphism
of
ordered
sets
expresses the fact that they “look the same”, an embedding of (X, ⪯)
into (X′, ⪯′) means that some part of (X′, ⪯′), namely, the part
determined by the set {f(x): x ∈X}, “looks the same” as (X, ⪯).
The next drawing illustrates two posets:
1
2
3
P1
P2
t
v
u
w
x
y
z
Examples of embeddings of P1 into P2 are the mappings f : 1 →v,
2 →x, 3 →y and f′ : 1 →t, 2 →x, 3 →w, while, for example,
neither g: 1 →t, 2 →v, 3 →y nor g′ : 1 →x, 2 →w, 3 →u are
embeddings.
2.3.2 Theorem. For every ordered set (X, ⪯) there exists an embed-
dings into the ordered set (2X, ⊆).
Proof. We show that, moreover, the embedding as in the theorem
is very easy to ﬁnd. We deﬁne a mapping f : X →2X by f(x) =
{y ∈X : y ⪯x}. We verify that this is indeed an embedding.
1. We check that f is injective. Let us assume that f(x) = f(y).
Since x ∈f(x) and y ∈f(y), the deﬁnition of f yields x ⪯y as
well as y ⪯x, and hence x = y (by the antisymmetry of ⪯).
2. We show that if x ⪯y, then f(x) ⊆f(y). If z ∈f(x), then
z ⪯x, and transitivity of ⪯yields z ⪯y. The last expression
means that z ∈f(y).
3. Finally, we show that if f(x) ⊆f(y), then x ⪯y. If f(x) ⊆f(y),
then x ∈f(y), and hence x ⪯y.
2

54
Orderings
Actually, we have already met the deﬁnition of the mapping f in
the proof of Theorem 2.2.3, where the set f(x) was denoted by Lx.
It is no problem that we have denoted the same thing diﬀerently in a
diﬀerent context. The construction of the sets Lx is quite frequent in
mathematics and it appears under various names (e.g., a lower ideal or
a down-set).
Note that the theorem above holds for inﬁnite sets as well.
The ordered sets (2X, ⊆) are thus universal in the sense that
they contain a copy of every ordered set. No wonder that they have
been studied very intensively and that there are special notions and
notation deﬁned for them. In particular, for X = {1, 2, . . . , n} the
set (2X, ⊆) is often denoted by Bn. Hasse diagrams of B1, B2, and
B3 are drawn below:
B1
B2
B3
When additional properties of the ordered set Bn are consid-
ered, then one speaks of a Boolean algebra, a Boolean lattice, an
n-dimensional cube, etc.
Exercises
1. How many linear extensions of B2 are there, and what about B3?
2. Modify the proof of Theorem 2.3.2 using up-sets, that is, sets of the
form Ux = {y: xRy}.
3. Find an example of an ordered set that can be embedded into Bn for
some n < |X|.
4. ∗Prove that every ﬁnite poset can be embedded into (N, |).
5. ∗Prove that not every ﬁnite poset admits an embedding into the ord-
ered set of triples of real numbers as in Example 2.1.1.
6. (a) Describe an embedding of the set {1, 2}×N with the lexicographic
ordering into the ordered set (Q, ≤), where Q denotes the set of all
rational numbers and ≤is the usual ordering.
(b) Solve the analog of (a) with the set N × N (ordered lexicographi-
cally) instead of {1, 2} × N.

2.4 Large implies tall or wide
55
(c)∗Prove that every countable linearly ordered set admits an
embedding into (Q, ≤).
7. ∗Prove that every subset of the poset Bn has both supremum and
inﬁmum (see Exercise 2.2.9 for the deﬁnitions).
8. Count the number of embeddings of P1 into P2, where P1 and P2 are
the partially ordered sets in the picture above Theorem 2.3.2.
2.4
Large implies tall or wide
Let (X, ⪯) be a ﬁnite partially ordered set. For brevity, let us denote
it by the letter P. In most of this section we will consider only one
(but arbitrary) ordered set. The notions that we will explore are
introduced by the following deﬁnitions.
2.4.1 Definition. A set A ⊆X is called independent in P if we
never have x ⪯y for two distinct elements x, y ∈A.
An independent set is also referred to as an antichain.
The deﬁnition can be rephrased using the following terminology.
Let us say that two distinct elements x and y are incomparable if
neither x ⪯y, nor y ⪯x. So a set is independent if every two of its
elements are incomparable.
Let α(P) denote the maximum size of an independent set in P.
In symbols, this can be written
α(P) = max{|A|: A independent in P}.
2.4.2 Example. For the following ordered sets P1 and P2
P1
P2
we have α(P1) = 3, α(P2) = 4.
2.4.3 Observation. The set of all minimal elements in P is ind-
ependent.
2.4.4 Definition. A set A ⊆X is called a chain in P if every two
of its elements are comparable (in P).

56
Orderings
Equivalently, the elements of A form a linearly ordered subset of
P. Let ω(P) denote the maximum number of elements of a chain
in P. For the ordered sets P1 and P2 above we have ω(P1) = 3,
ω(P2) = 2.
It is easy to check that ω(Bn) = n+1. Determining α(Bn) is con-
siderably more complicated; we answer this question in Chapter 7.
The above examples indicate that the number α(P) can be thou-
ght of as a kind of abstract “width” of the ordered set P, while ω(P)
corresponds to its “height”.
The following theorem, with an innocent-looking proof, actually
has quite powerful consequences.
2.4.5 Theorem. For every ﬁnite ordered set P = (X, ⪯) we have
α(P) · ω(P) ≥|X|.
(The reader may ﬁrst want to check that the examples above
fulﬁll the conclusion of the theorem.)
Proof. We deﬁne sets X1, X2, . . . , Xt inductively: Let X1 be the set
of all minimal elements of the ordered set P. In an inductive step,
let X1, . . . , Xℓbe already deﬁned, and let X′
ℓ= X \ ℓ
i=1 Xi denote
the set of all elements belonging to none of the sets X1, . . . , Xℓ. If X′
ℓ
is the empty set, then we put t = ℓand the construction is ﬁnished.
Otherwise, for X′
ℓ̸= ∅, we let ⪯′ stand for the ordering ⪯restricted
to the set X′
ℓ, and we deﬁne Xℓ+1 as the set of all minimal elements in
(X′
ℓ, ⪯′). The proof will be ﬁnished as soon as we verify the following
claims:
(1) The sets X1, . . . , Xt form a partition of X.
(2) Each Xi is an independent set in P.
(3) ω(P) ≥t.
Claims (1) and (2) follow immediately from the deﬁnition of the
sets X1, X2, . . . , Xt and from Observation 2.4.3. Thus, it suﬃces to
prove (3).
By backward induction, for k = t, t −1, . . . , 2, 1, we ﬁnd elements
xi ∈X such that the set {x1, x2, . . . , xt} constitutes a chain. Let us
choose xt ∈Xt arbitrarily. Since xt /∈Xt−1, there necessarily exists
xt−1 ∈Xt−1 so that xt−1 ≺xt. This argument is a basis of the
whole proof: Having already constructed elements xt ∈Xt, xt−1 ∈
Xt−1, . . . , xk+1 ∈Xk+1, then xk+1 /∈Xk, and hence there exists
xk ∈Xk with xk ≺xk+1.

2.4 Large implies tall or wide
57
The set {x1, . . . , xt} thus constructed is a chain. Therefore, ω(P)
≥t. (Actually we have ω(P) = t; we do not need this and we leave
it as a simple exercise.)
2
Theorem 2.4.5 has a number of nice connections, as is illustrated
by the following celebrated application:
2.4.6 Theorem (Erd˝os–Szekeres lemma). An arbitrary sequence
(x1, . . . , xn2+1) of real numbers contains a monotone subsequence of
length n + 1.
Let us ﬁrst deﬁne the notions in this theorem explicitly. A subse-
quence of length m is determined by indices i1, . . . , im, i1 < i2 < · · ·
< im, and it has the form (xi1, xi2, . . . , xim). Such a sequence is
monotone if we have either xi1 ≤xi2 ≤· · · ≤xim, or xi1 ≥xi2 ≥
· · · ≥xim. For example, the sequence (3, 5, 6, 2, 8, 1, 4, 7) contains the
monotone subsequence (3, 5, 6, 8) (with i1 = 1, i2 = 2, i3 = 3 and
i4 = 5), or the monotone subsequence (6, 2, 1) (with i1 = 3, i2 = 4,
i3 = 6), as well as many other monotone subsequences.
Proof of Theorem 2.4.6. Let a sequence (x1, . . . , xn2+1) of n2 + 1
real numbers be given. Let us put X = {1, 2, . . . , n2 + 1}, and let us
deﬁne a relation ⪯on X by
i ⪯j if and only if both i ≤j and xi ≤xj.
It is not diﬃcult to verify that the relation ⪯is a (partial) order-
ing of the set X. So we have α(X, ⪯) · ω(X, ⪯) ≥n2 + 1, and hence
α(X, ⪯) > n or ω(X, ⪯) > n. Now it is easily checked that a chain
i1 ≺i2 ≺· · · ≺im in the ordering ⪯corresponds to a nondecreasing
subsequence xi1 ≤xi2 ≤· · · ≤xim (note that i1 < i2 < · · · < im),
while an independent set {i1, i2, . . . , im} corresponds to a decreasing
subsequence (if we choose the notation so that i1 < i2 < · · · < im,
then we get xi1 > xi2 > · · · > xim, since for example, xi1 ≤xi2 and
i1 < i2 would mean i1 ≺i2).
2
Exercises
1. (a) Let ⪯i, i = 1, . . . , k, be orderings on some set X. Prove that
k
i=1 ⪯i is again an ordering on X (recall that ⪯i is a relation, and
thus a subset of X × X).
(b) ∗Prove that every partial ordering ⪯on a set X can be expressed
as the intersection of linear orderings of X.

58
Orderings
2. Prove that ω(Bn) = n + 1.
3. Find a sequence of real numbers of length 17 that contains no mono-
tone subsequence of length 5.
4. Prove the following strengthening of Theorem 2.4.6: Let k, ℓbe nat-
ural numbers. Then every sequence of real numbers of length kℓ+ 1
contains an nondecreasing subsequence of length k + 1 or a decreasing
subsequence of length ℓ+ 1.
5. (a) Prove that Theorem 2.4.5, as well as the preceding exercise, are
optimal in the following sense: For every k and ℓthere exists a partially
ordered P with n elements such that n = kℓ, α(P) = k, and ω(P) = ℓ.
(b) ∗Given k, ℓ≥1, construct a sequence of real numbers of length kℓ
with no nondecreasing subsequence of length k + 1 and no decreasing
subsequence of length ℓ+ 1.
6. (a) Let us consider two sequences a = (a1, . . . , an) and b = (b1, . . . , bn)
of distinct real numbers. Show that indices i1, . . . , ik, 1 ≤i1 < · · · <
ik ≤n, always exist with k = ⌈n1/4⌉such that the subsequences
determined by them in both a and b are increasing or decreasing (all
4 combinations are allowed, e.g. “increasing in a, decreasing in b”,
“decreasing in a, decreasing in b”, etc.).
(b) ∗Show that the bound for k in (c) cannot be improved in general.
7. ∗∗(Dilworth’s theorem) Let (X, ⪯) be a ﬁnite partially ordered set.
Show that X can be expressed as a (disjoint) union of at most α =
α(X, ⪯) chains.

3
Combinatorial counting
In this chapter, we are going to consider problems concerning the
number of various conﬁgurations, such as “How many ways are there
to send n distinct postcards to n friends?”, “How many mappings
of an n-element set to an m-element set are there?”, and so on. We
begin with simple examples that can usually be solved with common
sense (plus, maybe, some cleverness) and require no special know-
ledge. Later on, we will come to somewhat more advanced techniques.
3.1
Functions and subsets
As promised, we begin with a simple problem with postcards.
Problem. Professor X. (no real person meant), having completed
a successful short-term visit at the School of Mathematical Contem-
plation and Machine Cleverness in the city of Y., strolls around one
sunny day and decides to send a postcard to each of his friends Alice,
Bob, Arthur, Merlin, and HAL-9000. A street vendor nearby sells 26
kinds of postcards with great sights of Y.’s historical center. How
many possibilities does Professor X. have for sending postcards to
his 5 friends?
Since the postcard for each friend can be picked in 26 ways, and
the 5 selections are independent (making some of them doesn’t in-
ﬂuence the remaining ones), the answer to this problem is 265. In a
more abstract language, we have thereby counted the number of all
mappings of a 5-element set (Prof. X.’s friends) to a 26-element set
(the postcard types). Here is another closely related problem:
Problem. How many distinct 5-letter words are there (using
the 26-letter English alphabet, and including meaningless words
such as ywizp1)?
1Who can tell which words are meaningless? It might mean something in
Tralfamadorian, say.

60
Combinatorial counting
Since each of the 5 letters can be picked independently in 26 ways, it
is not hard to see that the answer is again 265. And, indeed, a 5-letter
word can be understood as a mapping of the set {1, 2, . . . , 5} to the
set {a, b, . . . , z} of letters: for each of the 5 positions in the word,
numbered 1, 2, . . . , 5, we specify the letter in that position. Finding
such simple transformations of counting problems is one of the basic
skills of the art of counting.
In the next proposition, we count mappings of an n-element set
to an m-element set. The idea is exactly the same as for counting
the ways of sending the postcards, but we use this opportunity to
practice more rigorous mathematical proofs in a simple situation.
3.1.1 Proposition. Let N be an n-element set (it may also be
empty, i.e. we admit n = 0, 1, 2, . . .) and let M be an m-element
set, m ≥1. Then the number of all possible mappings f : N →M
is mn.
Proof. We can proceed by induction on n. What does the proposi-
tion say for n = 0? In this case, we consider all mappings f of the
set N = ∅to some set M. The deﬁnition of a mapping tells us that
such an f should be a set of ordered pairs (x, y) with x ∈N = ∅
and y ∈M. Since the empty set has no elements, f cannot possibly
contain any such ordered pairs, and hence the only possibility is that
f is the empty set (no ordered pairs). On the other hand, f = ∅does
indeed satisfy the deﬁnition of a mapping in this case: the deﬁnition
says that for each x ∈N something should be true, but there are no
x ∈N. Therefore, exactly 1 mapping f : ∅→M exists. This agrees
with the formula, because m0 = 1 for any m ≥1. We have veriﬁed
the n = 0 case as a basis for the induction.
Many would object that a mapping of the empty set makes no sense
and so it is useless to consider it, and we could really start the induction
with n = 1 without any diﬃculty. But, in mathematical considerations,
it often pays to clarify such “borderline” cases, to ﬁnd out what exactly
the general deﬁnition says about them. This allows us to avoid various
exceptions and special cases later on, or missing cases and mistakes in
proofs. It is similar to the usefulness of deﬁning an empty sum (with no
addends) as 0, etc.
Next, suppose that the proposition has been proved for all n ≤n0
and for all m ≥1. We set n = n0+1 and we consider an n-element set
N and an m-element set M. Let us ﬁx an arbitrary element a ∈N.
To specify a mapping f : N →M is the same as specifying the value

3.1 Functions and subsets
61
f(a) ∈M plus giving a mapping f′ : N \ {a} →M. The value f(a)
can be chosen in m ways, and for the choice of f′ we have mn−1 ways
by the inductive hypothesis. Each choice of f(a) can be combined
with any choice of f′, and so the total number of possibilities for
f equals m · mn−1 = mn. Here is a picture for the more visually
oriented reader:
N
M
a
m possibilities
to map a
the rest can
be mapped in
mn−1 ways
2
3.1.2 Proposition. Any n-element set X has exactly 2n subsets
(n ≥0).
This is another simple and well-known counting result. Let us
give two proofs.
First proof (by induction).
For X = ∅, there exists a single
subset, namely ∅, and this agrees with the formula 20 = 1.
Having an (n + 1)-element set X, let us ﬁx one element a ∈X,
and divide the subsets of X into two classes: those not containing a
and those containing it. The ﬁrst class are exactly all the subsets of
the n-element set X \ {a}, and their number is 2n by the inductive
hypothesis. For each subset A of the second class, let us consider the
set A′ = A \ {a}. This is a subset of X \ {a}. Clearly, each subset
A′ ⊆X \ {a} is obtained from exactly one set A of the second class,
namely from A′ ∪{a}. In other words, there is a bijection between
all subsets of the ﬁrst class and all subsets of the second class. Hence
the number of subsets of the second class is 2n as well, and altogether
we have 2n + 2n = 2n+1 subsets of the (n + 1)-element set X as it
should be.
2
Second proof (reduction to a known result). Consider an ar-
bitrary subset A of a given n-element set X, and deﬁne a mapping
fA : X →{0, 1}. For an element x ∈X we put
fA(x) =
 1
if x ∈A
0
if x ̸∈A.

62
Combinatorial counting
(This mapping is often encountered in mathematics; it is called the
characteristic function of the set A.) Schematically,
X
A
fA
0
1
0
0
1
1
0
Distinct sets A have distinct mappings fA, and conversely, any given
mapping f : X →{0, 1} determines a subset A ⊆X with f = fA.
Hence the number of subsets of X is the same as the number of all
mappings X →{0, 1}, and this is 2n by Proposition 3.1.1.
2
Now, a somewhat more diﬃcult result:
3.1.3 Proposition. Let n ≥1. Each n-element set has exactly 2n−1
subsets of an odd size and exactly 2n−1 subsets of an even size.
Proof.
We make use of Proposition 3.1.2. Let us ﬁx an element
a ∈X. Any subset A ⊆X \ {a} can be completed to a subset
A′ ⊆X with an odd number of elements, by the following rule: if
|A| is odd, we put A′ = A, and for |A| even, we put A′ = A ∪{a}.
It is easy to check that this deﬁnes a bijection between the system
of all subsets of X \ {a} and the system of all odd-size subsets of X.
Therefore, the number of subsets of X of odd cardinality is 2n−1.
For subsets of an even size, we can proceed similarly, or we can
simply say that their number must be 2n minus the number of odd-
size subsets, i.e. 2n −2n−1 = 2n−1.
2
Later on, we will examine several more proofs.
Injective mappings.
Problem. Professor X., having spent some time contemplating the
approximately 12 million possibilities of sending his postcards,
returned to the street vendor and wanted to buy his selection. But
the vendor had already sold all the postcards and was about to close.
After some discussion, he admitted he still had one sample of each of
the 26 postcards, and was willing to sell 5 of them to Professor X. for
$5 apiece. In this situation, Professor X. has to make a new decision
about which postcard is best for whom. How many possibilities does
he have now?
As Professor X. (and probably the reader too) recognized, one has
to count one-to-one mappings from a 5-element set to a 26-element

3.1 Functions and subsets
63
set. This is the same as counting 5-letter words with all letters dis-
tinct.
3.1.4 Proposition. For given numbers n, m ≥0, there exist exactly
m(m −1) . . . (m −n + 1) =
n−1

i=0
(m −i)
one-to-one mappings of a given n-element set to a given m-element
set.
Proof. We again proceed by induction on n (and more concisely this
time). For n = 0, the empty mapping is one-to-one, and so exactly
1 one-to-one mapping exists, and this agrees with the fact that the
value of an empty product has been deﬁned as 1. So the formula
holds for n = 0.
Next, we note that no one-to-one mapping exists for n > m,
and this again agrees with the formula (since one factor equal to 0
appears in the product).
Let us now consider an n-element set N, n ≥1, and an m-element
set M, m ≥n. Fix an element a ∈N and choose the value f(a) ∈M
arbitrarily, in one of m possible ways. It remains to choose a one-to-
one mapping of the set N\{a} to the set M\{f(a)}. By the inductive
assumption, there are (m −1)(m −2) . . . (m −n + 1) possibilities for
the latter choice. Altogether we have m(m−1)(m−2) . . . (m−n+1)
one-to-one mappings f : N →M. (Where is the picture? Well, these
days, you can’t expect to have everything in a book in this price
category.)
2
As we have noted for the postcards and 5-letter words, choosing a
one-to-one mapping of an n-element set to an m-element set can also be
viewed as selecting n objects from m distinct objects, where the order of
the selected objects is important (i.e. we construct an ordered n-tuple).
Such selections are sometimes called variations of n elements from m
elements without repetition.
Exercises
1. Let X = {x1, x2, . . . , xn} be an n-element set. Describe how each sub-
set of X can be encoded by an n-letter word consisting of the letters
a and b. Infer that the number of subsets of X is 2n. (This is very
similar to the second proof of Proposition 3.1.2.)

64
Combinatorial counting
2. Determine the number of ordered pairs (A, B), where A ⊆B ⊆
{1, 2, . . . , n}.
3. Let N be an n-element set and M an m-element set. Deﬁne a bijection
between the set of all mappings f : N →M and the n-fold Cartesian
product M n.
4. Among the numbers 1, 2, . . . , 1010, are there more of those containing
the digit 9 in their decimal notation, or of those with no 9?
5. (a) How many n × n matrices are there with entries chosen from the
numbers 0, 1, . . . , q −1?
(b) ∗Let q be a prime. How many matrices as in (a) have a determinant
that is not divisible by q? (In other words, how many nonsingular
matrices over the q-element ﬁeld are there?)
6. ∗Show that a natural number n ≥1 has an odd number of divisors
(including 1 and itself) if and only if √n is an integer.
3.2
Permutations and factorials
A bijective mapping of a ﬁnite set X to itself is called a permutation
of the set X.
If the elements of X are arranged in some order, we can also imag-
ine a permutation as rearranging the elements in a diﬀerent order.
For instance, one possible permutation p of the set X = {a, b, c, d}
is given by p(a) = b, p(b) = d, p(c) = c, and p(d) = a. This can also
be written in a two-row form:
 a
b
c
d
b
d
c
a

.
In the ﬁrst row, we have listed the elements of X, and under each
element x ∈X in the ﬁrst row, we have written the element p(x) into
the second row. Most often one works with permutations of the set
{1, 2, . . . , n}. If we use the convention that the ﬁrst row always lists
the numbers 1, 2, . . . , n in the natural order, then it suﬃces to write
the second row only. For example, (2 4 3 1) denotes the permutation
p with values p(1) = 2, p(2) = 4, p(3) = 3, and p(4) = 1.
In the literature, permutations of a set X are sometimes understood
as arrangements of the elements of X in some order, i.e. as linear order-
ings on the set X. This may be a quite useful point of view, but we will
mostly regard permutations as mappings. This has some formal advan-
tages. For instance, permutations can be composed (as mappings).

3.2 Permutations and factorials
65
Yet another way of writing permutations is to use their so-called
cycles. Cycles are perhaps easiest to deﬁne if we depict a permuta-
tion using arrows, in the way we depicted relations using arrows in
Section 1.5. In the case of a permutation p: X →X, we draw the ele-
ments of the set X as dots, and we draw an arrow from each dot x to
the dot p(x). For example, for the permutation p = (4 8 3 5 2 9 6 1 7)
(this is the one-line notation introduced above!), such a picture looks
as follows:
1
4
5
2
8
6
7
9
3
Each point has exactly one outgoing arrow and exactly one ingo-
ing arrow. It is easy to see that the picture of a permutation con-
sists of several disjoint groups of dots, where the dots in each group
are connected by arrows into a cycle. One can walk around such a
cycle in one direction following the arrows. The groups of elements
connected together by these cycles are called the cycles of the con-
sidered permutation. (Any reader who is not satisﬁed with this pic-
torial deﬁnition can ﬁnd a formal deﬁnition of a cycle in Exercise 2.)
Using the cycles, the depicted permutation p can also be written
p = ((1, 4, 5, 2, 8)(3)(6, 9, 7)). In each of the inner parentheses, the
elements of one cycle are listed in the order along the arrows, start-
ing with the smallest element in that cycle.
What can permutations be good for? They are studied, for instance,
in the design and analysis of various sorting algorithms. Certain eﬃcient
algorithms for problems with graphs, or with geometric objects, start
by rearranging the input objects into a random order, i.e. by performing
a random permutation with them. In the so-called group theory, which
is extremely important in almost all mathematics and also in many
areas of modern physics, groups of permutations (with composition as
the group operation) are one of the basic objects of study. An ultimate
reason for the impossibility of a general algebraic solution of algebraic
equations of degree 5 is in the properties of the group of all permutations
on the 5-element set. Rubik’s Cube, a toy which used to be extremely
popular at the beginning of the 1980s, provides a pretty example of
a complicated permutation group. Surprisingly involved properties of

66
Combinatorial counting
permutations are applied in a mathematical analysis of card shuﬄing.
This is just a small sample of areas where permutations play a role.
According to Proposition 3.1.4, the number of permutations of
an n-element set is n(n −1) · . . . · 2 · 1. This number, regarded as a
function of n, is denoted by n! and is called n factorial. Hence we
have
n! = n(n −1) · . . . · 2 · 1 =
n−1

j=0
(n −j) =
n

i=1
i.
In particular, for n = 0 we have 0! = 1 (because 0! is deﬁned as the
empty product).
Exercises
1. How many permutations of {1, 2, . . . , n} have a single cycle?
2. For a permutation p: X →X, let pk denote the permutation arising
by a k-fold composition of p, i.e. p1 = p and pk = p ◦pk−1. Deﬁne a
relation ≈on the set X as follows: i ≈j if and only if there exists a
k ≥1 such that pk(i) = j. Prove that ≈is an equivalence relation on
X, and that its classes are the cycles of p.
3. Let p be a permutation, and let pk be deﬁned as in Exercise 2. By
the order of the permutation p we mean the smallest natural number
k ≥1 such that pk = id, where id denotes the identity permutation
(mapping each element onto itself).
(a) Determine the order of the permutation (2 3 1 5 4 7 8 9 6).
(b) Show that each permutation p of a ﬁnite set has a well-deﬁned
ﬁnite order, and ∗show how to compute the order using the lengths of
the cycles of p.
4. CS Write a program that lists all permutations of the set {1, 2, . . . , n},
each of them exactly once. Use a reasonable amount of memory even
if n!, the number of permutations, is astronomically large. ∗Can you
make the total number of operations of the algorithm proportional to
n!, if the operations needed for the output (printing the permutations,
say) are not counted?
5. (This is an exercise for those who are getting bored by the easy material
covered in the ﬁrst two sections of this chapter.) Let p be a permuta-
tion of the set {1, 2, . . . , n}. Let us write it in the one-line notation, and
let us mark the increasing segments in the resulting sequence of num-
bers, for example, (4 5 7 2 6 8 3 1). Let f(n, k) denote the number of
permutations of an n-element set with exactly k increasing segments.

3.3 Binomial coeﬃcients
67
(a) ∗Prove that f(n, k) = f(n, n + 1 −k), and derive that the aver-
age number of increasing segments of a permutation is (n + 1)/2 (the
average is taken over all permutations of {1, 2, . . . , n}).
(b) ∗Derive the following recurrent formula:
f(n, k) = k · f(n −1, k) + (n + 1 −k)f(n −1, k −1).
(c) Using (b), determine the number of permutations of {1, 2, . . . , n}
with 2 increasing segments, with 3 increasing segments, and ∗with k
increasing segments.
(d) ∗For a randomly chosen permutation π of the set {1, 2, . . . , n}, cal-
culate the probability that the ﬁrst increasing segment has length k.
Show that for n large, the average length of the ﬁrst increasing seg-
ments approaches the number e −1.
Remark. These and similar questions have been studied in the analysis
of various algorithms for sorting.
6. Let π be a permutation of the set {1, 2, . . . , n}. We say that an ordered
pair (i, j) ∈{1, 2, . . . , n} × {1, 2, . . . , n} is an inversion of π if i < j
and π(i) > π(j).
(a) Prove that the set I(π) of all inversions, regarded as a relation on
{1, 2, . . . , n}, is transitive.
(b) Prove that the complement of I(π) is transitive too.
(c) CS Consider some sorting algorithm which rearranges n input num-
bers into a nondecreasing order, and in each step, it is only allowed to
exchange two neighboring numbers (in the current order). Prove that
there are input sequences whose sorting requires at least cn2 steps of
this algorithm, where c > 0 is some suitable constant.
(d) ∗,CS Can you design an algorithm that calculates the number of
inversions of a given permutation of {1, 2, . . . , n} using substantially
less than n2 steps? (See e.g. Knuth [41] for several solutions.)
7. (a) ∗Find out what is the largest power of 10 dividing the number 70!
(i.e. the number of trailing zeros in the decimal notation for 70!).
(b) ∗Find a general formula for the highest power k such that n! is
divisible by pk, where p is a given prime number.
8. Show that for every k, n ≥1, (k!)n divides (kn)!.
3.3
Binomial coeﬃcients
Let n ≥k be nonnegative integers. The binomial coeﬃcient
n
k

(read
“n choose k”) is a function of the variables n, k deﬁned by the formula
n
k

= n(n −1)(n −2) . . . (n −k + 1)
k(k −1) · . . . · 2 · 1
=
k−1
i=0 (n −i)
k!
.
(3.1)

68
Combinatorial counting
The reader might know another formula, namely
n
k

=
n!
k!(n −k)!.
(3.2)
In our situation, this is equivalent to (3.1). Among these two possible
deﬁnitions, the ﬁrst one, (3.1), has some advantages. The numerical
value of
n
k

is more easily computed from it, and one also gets smaller
intermediate results in the calculation. Moreover, (3.1) makes sense for
an arbitrary real number n (more about this in Chapter 12), and, in
particular, it deﬁnes the value of
n
k

also for a natural number n < k;
in such cases,
n
k

= 0.
The basic combinatorial meaning of the binomial coeﬃcient
n
k

is the number of all k-element subsets of an n-element set. We prove
this in a moment, but ﬁrst we introduce some notation.
3.3.1 Definition. Let X be a set and let k be a nonnegative integer.
By the symbol
X
k

we denote the set of all k-element subsets of the set X.
For example,
{a,b,c}
2

= {{a, b}, {a, c}, {b, c}}. The symbol
x
k

has
two meanings now, depending on whether x is a number or a set.
The following propositions put them into a close connection:
3.3.2 Proposition. For any ﬁnite set X, the number of all k-
element subsets equals
|X|
k

.
In symbols, this statement can be written

X
k
 =
|X|
k

.
Proof. Put n = |X|. We will count all ordered k-tuples of elements of
X (without repetitions of elements) in two ways. On the one hand, we
know that the number of the ordered k-tuples is n(n−1) . . . (n−k+1)
by Proposition 3.1.4 (see the remark following its proof). On the
other hand, from one k-element subset M ∈
X
k

, we can create k!
distinct ordered k-tuples, and each ordered k-tuple is obtained from
exactly one k-element subset M in this way. Hence
n(n −1) . . . (n −k + 1) = k!

X
k
 .
2

3.3 Binomial coeﬃcients
69
Another basic problem leading to binomial coefficients. How
many ways are there to write a nonnegative integer m as a sum of r
nonnegative integer addends, where the order of addends is impor-
tant? For example, for n = 3 and r = 2, we have the possibilities
3 = 0+3, 3 = 1+2, 3 = 2+1, and 3 = 0+3. In other words, we want
to ﬁnd out how many ordered r-tuples (i1, i2, . . . , ir) of nonnegative
integers there are satisfying the equation
i1 + i2 + · · · + ir = m.
(3.3)
The answer is the binomial coeﬃcient
m+r−1
r−1

. This can be proved
in various ways. Here we describe a proof almost in the style of a
magician’s trick.
Imagine that each of the variables i1, i2, . . . , ir corresponds to
one of r boxes. We have m indistinguishable balls, and we want to
distribute them into these boxes in some way (we assume that each
box can hold all the m balls if needed). Each possible distribution
encodes one solution of Eq. (3.3). For example, for m = 7 and r = 6,
the solution 0+1+0+3+1+2 = 7 corresponds to the distribution
r
r
r
r
r
r r
So we are interested in the number of distributions of the balls into
boxes. We now let the bottoms and the leftmost and rightmost walls
of the boxes disappear, so that only m balls and r−1 walls separating
the boxes remain:
r
r r r
r
r r
(we have also moved the balls and walls for a better aesthetic impres-
sion). This situation still contains full information about the distri-
bution of the balls into boxes. Hence, choosing a distribution of the
balls means selecting the position of the internal walls among the
balls. In other words, we have m + r −1 objects, balls and internal
walls, arranged in a row, and we determine which positions will be
occupied by balls and which ones by walls. This corresponds to a
selection of a subset of r −1 positions from m + r −1 positions, and
this can be done in
m+r−1
r−1

ways.
2
Simple properties of binomial coefficients. One well-known for-
mula is
n
k

=

n
n −k

.
(3.4)

70
Combinatorial counting
Its correctness (for n ≥k ≥0) can immediately be seen from the
already-mentioned formula
n
k

=
n!
k!(n−k)!. Combinatorially, Eq. (3.4)
means that the number of k-element subsets of an n-element set is
the same as the number of subsets with n −k elements. This can be
veriﬁed directly without referring to binomial coeﬃcients—it suﬃces
to assign to each k-element subset its complement.
Here is another important formula, attributed to Pascal:
n −1
k −1

+
n −1
k

=
n
k

.
(3.5)
One elegant proof is based on a combinatorial interpretation of both
sides of Eq. (3.5). The right-hand side is the number of k-element
subsets of some n-element set X. Let us ﬁx one element a ∈X
and divide all k-element subsets of X into two groups depending
on whether they contain a or not. The subsets not containing a are
exactly all k-element subsets of X\{a}, and so their number is
n−1
k

.
If A is some k-element subset of X containing a, then we can assign
the (k −1)-element set A′ = A \ {a} to A. It is easy to check that
this assignment is a bijection between all k-element subsets of X
containing the element a and all (k −1)-element subsets of X \ {a}.
The number of the latter is
n−1
k−1

. Altogether, the number of all
k-element subsets of X equals
n−1
k

+
n−1
k−1

.
2
The identity (3.5) is closely related to the so-called Pascal
triangle:
1
1
1
1
2
1
1
3
3
1
1
4
6
4
1
1
5
10
10
5
1
...
...
Every successive row in this scheme is produced as follows: under each
pair of consecutive numbers in the preceding row, write their sum, and
complement the new row by 1s on both sides. An induction using (3.5)
shows that the (n + 1)-st row contains the binomial coeﬃcients
n
0

,
n
1

,. . . ,
n
n

.

3.3 Binomial coeﬃcients
71
Binomial theorem.
Equation (3.5) can be used for a proof of
another well-known statement involving binomial coeﬃcients: the
binomial theorem.
3.3.3 Theorem (Binomial theorem).
For any nonnegative
integer n, we have
(1 + x)n =
n

k=0
n
k

xk
(3.6)
(this is an equality of two polynomials in the variable x, so in par-
ticular it holds for any speciﬁc real number x).
From the binomial theorem, we can infer various relations among
binomial coeﬃcients. Perhaps the simplest one arises by substituting
x = 1, and it reads
n
0

+
n
1

+
n
2

+ · · · +
n
n

= 2n.
(3.7)
Combinatorially, this is nothing else than counting all subsets of an
n-element set. On the left-hand side, they are divided into groups
according to their size.
Second proof of Proposition 3.1.3 (about the number of odd-
size subsets). By substituting x = −1 into the binomial theorem,
we arrive at
n
0

−
n
1

+
n
2

−
n
3

+ . . . =
n

k=0
(−1)k
n
k

= 0.
(3.8)
Adding this equation to Eq. (3.7) leads to
2
n
0

+
n
2

+
n
4

+ . . .

= 2n.
The brackets on the left-hand side contain the total number of even-
size subsets of an n-element set. Therefore, the number of even-
size subsets equals 2n−1. The odd-size subsets can be counted as a
complement to 2n.
2
Further identities with binomial coefficients. Literally thou-
sands of formulas and identities with binomial coeﬃcients are known
and whole books are devoted to them. Here we present one more for-
mula with a nice combinatorial proof. More formulas and methods on
how to derive them will be given in the exercises and in Chapter 12.

72
Combinatorial counting
3.3.4 Proposition.
n

i=0
n
i
2
=
2n
n

.
Proof. The ﬁrst trick is to rewrite the sum using the symmetry of
binomial coeﬃcients, (3.4), as
n

i=0
n
i
 n
n −i

.
Now we show that this sum expresses the number of n-element sub-
sets of a 2n-element set (and so it equals the right-hand side in the
formula being proved). Consider a 2n-element set X, and color n of
its elements red and the remaining n elements blue. To choose an
n-element subset of X now means choosing an i-element subset of
the red elements plus an (n−i)-element subset of the blue elements,
where i ∈{0, 1, . . . , n}:
n red
n blue
i
n −i
X
For a given i, there are
n
i

possibilities to choose the red sub-
set and, independently,
 n
n−i

possibilities for the blue subset. Al-
together, an n-element subset of X can be selected in n
i=0
n
i
 n
n−i

ways.
2
Multinomial coefficients and the multinomial theorem. Here is
one of the favorite problems of American textbooks: how many distinct
words, including nonsense ones, can be produced using all the letters
of the word MISSISSIPPI? In other words, how many distinct ways are
there to rearrange these letters? First, imagine that the letters in the
name are distinguished somehow, so that we have 4 diﬀerent Ss, etc. In
our text, we distinguish them by indices: M1I1S1S2I2S3S4I3P1P2I4. So
we have 11 distinct letters, and these can be permuted in 11! distinct
ways. Now consider one (arbitrary) word produced from a “nonindexed”
MISSISSIPPI, such as SIPISMSIPIS. From how many “indexed” words
do we get this word by deleting the indices? The indices of the 4 letters
S can be placed in 4! ways, the indices of the 4 letters I can be arranged

3.3 Binomial coeﬃcients
73
(independently) in 4! ways, for the 2 letters P we have 2! possibilities,
and ﬁnally for the single M we have 1 (or 1!) possibility. Thus, the word
SIPISMSIPIS, and also any other word created from MISSISSIPPI, can
be indexed in 4!4!2!1! ways. The number of nonindexed words, which is
the answer to the problem, is 11!/(4!4!2!1!).
The same argument leads to the following general result: if we have
objects of m kinds, ki indistinguishable objects of the ith kind, where
k1 + k2 + · · · + km = n, then the number of distinct arrangements of
these objects in a row is given by the expression
n!
k1!k2! . . . km!.
This expression is usually written

n
k1, k2, . . . , km

and is called a multinomial coeﬃcient. In particular, for m = 2 we get a
binomial coeﬃcient, i.e.

n
k,n−k

denotes the same thing as
n
k

. Why the
name “multinomial coeﬃcient”? It comes from the following theorem:
3.3.5 Theorem (Multinomial
theorem).
For
arbitrary
real
numbers x1, x2, . . . , xm and any natural number n ≥1, the following
equality holds:
(x1 + x2 + · · · + xm)n =

k1+···+km=n
k1,...,km≥0

n
k1, k2, . . . , km

xk1
1 xk2
2 . . . xkm
m .
The right-hand side of this formula usually has fairly many terms
(we sum over all possible ways of writing n as a sum of m nonnega-
tive integers). But the theorem is most often applied to determine the
coeﬃcient of some particular term. For example, it tells us that the
coeﬃcient of x2y3z5 in (x + y + z)10 is
 10
2,3,5

= 2520.
The multinomial theorem can be proved by induction on n (see
Exercise 26). A more natural proof can be given by the methods we
discuss in Chapter 12.
Exercises
1. Formulate the problem of counting all k-element subsets of an n-
element set as a problem with sending or buying postcards.
2. Prove the addition formula (3.5) by using the deﬁnition (3.1) of bino-
mial coeﬃcients and by manipulating expressions.

74
Combinatorial counting
3. (a) Prove the formula
r
r

+
r + 1
r

+
r + 2
r

+ · · · +
n
r

=
n + 1
r + 1

(3.9)
by induction on n (for r arbitrary but ﬁxed). Note what the formula
says for r = 1.
(b) ∗Prove the same formula combinatorially.
4. ∗For natural numbers m ≤n calculate (i.e. express by a simple formula
not containing a sum) n
k=m
 k
m
n
k

.
5. Calculate (i.e. express by a simple formula not containing a sum)
(a) n
k=1
 k
m
 1
k,
(b) ∗n
k=0
 k
m

k.
6. ∗∗Prove that
m

k=0
m
k
n + k
m

=
m

k=0
n
k
m
k

2k.
7. ∗How many functions f : {1, 2, . . . , n} →{1, 2, . . . , n} are there that
are monotone; that is, for i < j we have f(i) ≤f(j)?
8. How many terms are there in the sum on the right-hand side of the
formula for (x1 + · · · + xm)n in the multinomial theorem?
9. ∗How many k-element subsets of {1, 2, . . . , n} exist containing no two
consecutive numbers?
10. (a) Using formula (3.9) for r = 2, calculate the sums n
i=2 i(i−1) and
n
i=1 i2.
(b) Using (a) and (3.9) for r = 3, calculate n
i=1 i3.
(c) ∗Derive the result of (b) using Fig. 3.1 (the ﬁgure is drawn for the
case n = 4).
11. Prove the binomial theorem by induction on n.
12. For a real number x and a natural number n, let the symbol xn denote
x(x −1)(x −2) . . . (x −n + 1) (the so-called nth factorial power of x).
Prove the following analog of the binomial theorem:
(x + y)n =
n

i=0
n
i

xiyn−i.
Proceed by induction on n.

3.3 Binomial coeﬃcients
75
Fig. 3.1 A graphical derivation of the formula for 13 + 23 + · · · + n3.
13. Prove the so-called Leibniz formula for the diﬀerentiation of a product.
Let u, v be real functions of a single real variable, and let f (k) denote
the kth derivative of a function f. Then
(uv)(n) =
n

k=0
n
k

u(k)v(n−k)
(supposing that all the derivatives in the formula exist). The case n = 1
is the formula for diﬀerentiating a product, (uv)′ = u′v + uv′, which
you may assume as being known.
14. CS Write a computer program that lists all k-element subsets of the
set {1, 2, . . . , n}, each of them exactly once. Use a reasonable amount
of memory even if
n
k

, the number of such subsets, is very large. ∗Can
you make the total number of operations of the algorithm proportional
to
n
k

, if the operations needed for the output are not counted?
15. Let p be a prime and let n, k be natural numbers.
(a) Prove that for k < p,
p
k

is divisible by p.
(b) Prove that
n
p

is divisible by p if and only if ⌊n/p⌋is divisible
by p.
16. (a) ∗Using the binomial theorem, derive a formula for the number of
subsets of cardinality divisible by 4 of an n-element set.
(b) ∗Count the subsets of size divisible by 3 of an n-element set.
17. We have n kinds of objects, and we want to determine the number of
ways in which a k-tuple of objects can be selected. We consider vari-
ants: we may be interested in selecting ordered or unordered k-tuples,

76
Combinatorial counting
and we may have either just 1 object of each kind or an unlimited
supply of indistinguishable objects of each kind. Fill out the formulas
in the following table:
Only 1 object
Arbitrarily many
of each kind
objects of each kind
Ordered k-tuples
Unordered k-tuples
18. We have k balls, and we distribute them into n (numbered) bins. Fill
out the formulas for the number of distributions for various variants
of the problem in the following table:
At most 1 ball Any number of balls
into each bin
into each bin
Balls are distinguishable
(have distinct colors)
Balls are
indistinguishable
19. ∗How many ways are there to arrange 7 elves and 5 goblins in a row
in such a way that no goblins stand next to each other?
20. A table is set with 13 large plates. We have 5 lobsters (indistinguishable
ones) and 8 stuﬀed snails (also indistinguishable). We are interested
in the number of ways to serve the snails and lobsters on the plates.
The order of serving is important. Imagine we were writing a script
for a movie: “Put a snail on plate no. 3, then serve a lobster on plate
no. 11. . . ”. Only one item is served at a time. How many ways are
there if
(a) if there are no restrictions, everything can come on the same plate,
say, and
(b) if at least 1 item should come on each plate?
21. Draw a triangle ABC. Draw n points lying on the side AB (but dif-
ferent from A and B) and connect all of them by segments to the
vertex C. Similarly, draw n points on the side AC and connect them
to B.
(a) How many intersections of the drawn segments are there? Into how
many regions is the triangle ABC partitioned by the drawn segments?
(b) ∗Draw n points also on the side BC and connect them to A. Assume
that no 3 of the drawn segments intersect at a single point. How many
intersections are there now?
(c) ∗How many regions are there in the situation of (b)?

3.3 Binomial coeﬃcients
77
22. Consider a convex n-gon such that no 3 diagonals intersect at a single
point. Draw all the diagonals (i.e. connect every pair of vertices by a
segment).
(a) ∗How many intersections do the diagonals determine?
(b) ∗Into how many parts is the polygon divided by the diagonals?
23. (Cayley’s problem) ∗Consider a regular convex n-gon P with vertices
A1, A2, . . . , An. How many ways are there to select k of these n vertices,
in such a way that no two of the selected vertices are consecutive
(in other words, if we draw the polygon determined by the selected
vertices, it has no side in common with P)? Hint: First, calculate the
number of such selections including A1.
24. ∗Consider a regular n-gon. We divide it by nonintersecting diagonals
into triangles (i.e. we triangulate it), in such a way that each of the
resulting triangles has at least one side in common with the original
n-gon.
(a) How many diagonals must we draw? How many triangles do we
get?
(b) ∗How many such triangulations are there?
25. (a) What is the coeﬃcient of x2y3z in the polynomial (2x−y2 + 3z)6?
What about the coeﬃcient of x2y2z?
(b) Find the coeﬃcient of x2y8z in (2x + y2 −5z)7.
(c) What is the coeﬃcient of u2v3z3 in (3uv −2z + u + v)7?
26. (a) Prove the equality

n
k1, k2, . . . , km

=

n −1
k1 −1, k2, k3, . . . , km

+

n −1
k1, k2 −1, k3, . . . , km

+ · · · +

n −1
k1, k2, . . . , km−1, km −1

(n ≥1, k1 + · · · + km = n, ki ≥1).
(b) Prove the multinomial theorem by induction on n.
27. Count the number of linear extensions for the following partial order-
ings:
(a) X is a disjoint union of sets X1, X2, . . . , Xk of sizes r1, r2, . . . , rk,
respectively. Each Xi is linearly ordered by ⪯, and no two elements
from the diﬀerent Xi are comparable.
(b) ∗The Hasse diagram of (X, ⪯) is a tree as in the following picture.
The root has k sons, the ith son has ri leaves.

78
Combinatorial counting
. . .
3.4
Estimates: an introduction
If we are interested in some quantity and we ask the question “How
much?”, the most satisfactory answer seems to be one determining the
quantity exactly. A millionairess may ﬁnd some fascination in knowing
that her account balance is 107,343,726.12 doublezons2 at the moment.
In mathematics, an answer to a counting problem is usually considered
most satisfactory if it is given by an exact formula. But quite often we
do not really need an exact result; for many applications it is enough to
know a quantity approximately. For instance, many people may ﬁnd it
suﬃcient, although perhaps not comforting, to learn that their account
balance is between 4000 and 4100 doublezons. Often even a one-sided
inequality suﬃces: if we estimate that a program for ﬁnding an optimal
project schedule by trying all possibilities would run for at least 1010
days, we probably need not put further eﬀort into determining whether
it would actually run for more than 1012 days or less than that.
Exact results may be diﬃcult to ﬁnd. Sometimes computing an exact
result may be possible but laborious, and sometimes it is beyond
our capabilities no matter how hard we try. Hence, heading for an
estimate instead of the exact result may save us lots of work and
considerably enlarge the range of problems we are able to cope with.
Another issue is that an exact answer may be diﬃcult to grasp
and relate to other quantities. Of course, if the answer is a single
number, it is easy to compare it to other numbers, but the situation
is more delicate if we have a formula depending on one or several
variables. Such a formula deﬁnes a function, say a function of n, and
we would like to understand “how big” this function is. The usual
approach is to compare the considered function to some simple and
well-known functions. Let us give a nontrivial example ﬁrst.
3.4.1 Example (Estimating the harmonic numbers). The fol-
lowing sum appears quite often in mathematics and in computer
science:
Hn = 1 + 1
2 + 1
3 + · · · + 1
n =
n

i=1
1
i .
2Doublezon is a currency unit taken from the book L’´Ecume des jours (English
translation: Froth on the Daydream) by Boris Vian.

3.4 Estimates: an introduction
79
1
1
2
9
G1
G2
G3
G4
i
Fig. 3.2 Partitioning the sequence ( 1
1, 1
2, 1
3, . . . ) into groups.
This Hn is called the nth harmonic number. It turns out that there
is no way to simplify this sum (it has no “closed form”). We want
to get some idea about the behavior of Hn for n growing to ∞. In
particular, we want to decide whether Hn →∞for n →∞.
A simple estimate. The idea is to divide the terms of the sequence
1
1, 1
2, 1
3, . . . into groups, each group consisting of numbers that are
roughly of the same magnitude. That is, we let the kth group Gk
consist of the numbers 1
i with
1
2k < 1
i ≤
1
2k−1
(see Fig. 3.2).
Hence Gk contains the 2k−1 numbers
1
2k−1 ,
1
2k−1 + 1,
1
2k−1 + 2, . . . ,
1
2k −1.
Therefore, the sum of the terms in each Gk satisﬁes

x∈Gk
x ≤|Gk| max Gk = 2k−1
1
2k−1 = 1,
and similarly

x∈Gk
x ≥|Gk| min Gk > 2k−1 1
2k = 1
2.

80
Combinatorial counting
A given term 1
i belongs to the group Gk with 2k−1 ≤i < 2k, i.e.
with k = ⌊log2 i⌋+ 1. Therefore, Hn is no bigger than the sum of
numbers in the ﬁrst ⌊log2 n⌋+ 1 groups, and we get
Hn =
n

i=1
1
i ≤
⌊log2 n⌋+1

k=1
1 ≤log2 n + 1.
Similarly we can derive a lower bound
Hn >
⌊log2 n⌋

k=1
1
2 ≥1
2⌊log2 n⌋.
We may conclude that Hn does grow to inﬁnity but quite slowly,
about as slowly as the logarithm function. Even for very large values
of n, we can estimate the value of Hn by computing the logarithm.
If n is large, the ratio of the upper and lower bounds is close to 2.
For somewhat more sophisticated and more precise estimates of
Hn, see Exercise 3.5.13.
In this example, we seem to have been lucky. We could approxi-
mate the considered function Hn quite closely by suitable multiples of
the logarithm function. But experience shows, and certain theoretical
results conﬁrm, that this is not exceptional luck, and that functions
(of a single variable n) occurring in natural problems can usually be
estimated fairly accurately by everyday functions like n, n2, n35/13, 2n,
3.26n, 3n2/2, ln n,
3
11n(ln n)2, etc. But ﬁnding such estimates may often
be quite tricky. In the subsequent sections, we will demonstrate several
techniques which may be helpful in such an eﬀort.
Asymptotic comparison of functions.
In the above example,
we have shown that the function Hn is “smaller” than the function
log2 n + 1, meaning that the inequality Hn ≤log2 n + 1 holds for all
n ∈N. But if we consider the functions f(n) = 5n and g(n) = n2,
then neither is smaller than the other, strictly speaking, since, for
example, f(1) = 5 > g(1) = 1 but f(6) = 30 < g(6) = 36, so
neither of the inequalities f(n) ≤g(n) and f(n) ≥g(n) is correct
for all n. Yet we feel that g “grows much faster” than f: after some
initial hesitation for small values of n, g(n) exceeds f(n) and remains
above it for all the larger n.
In mathematics and in theoretical computer science, functions
deﬁned on the natural numbers are usually compared according to
their behavior as n tends to inﬁnity, while their behavior for small

3.4 Estimates: an introduction
81
values n are ignored. This approach is usually called the asymptotic
analysis of the considered functions. We also speak of the asymptotic
behavior or asymptotics of some function, meaning its comparison to
some simple functions for n →∞.
If f and g are real functions of a single variable n, we may introduce
the symbol f ⪯g, meaning that there exists some number n0 such that
the inequality f(n) ≤g(n) holds for all n ≥n0; that is, “g ultimately
outgrows f”. So, for the example in the preceding paragraph, we can
write 5n ⪯n2.
It is useful to think a bit about the relation ⪯just introduced. It
can be viewed as a “soft” inequality between the considered functions.
If f ⪯g holds, we are sure that g outgrows f for large enough n but
we generally do not know how large n must be. The notation ⪯thus
suppresses some information. This often makes it much easier to de-
rive the ⪯inequality between two functions than to prove the “hard”
inequality ≤(which should hold for all n). But it may also make the ⪯
inequality treacherous for the “end-user”. Suppose that someone sells
us a black box that, for each input number n, computes and displays
some value f(n). We also get a guarantee that f(n) ⪯n. We can never
really prove that the guarantee is invalid. No matter how many of the
n we ﬁnd with f(n) > n, the seller can always claim that the number
n0 implicit in the guarantee is still much bigger than our examples.
The notation f ⪯g is not common in the literature (although we
believe it has some didactical value for understanding the other nota-
tions to come). Instead, several other notations are used that suppress
still somewhat more information, and thus may make the estimates yet
more convenient to derive.
The ‘‘big-Oh’’ notation.
The following notation is used quite
often; for instance, it appears frequently in the analysis of algorithms.
3.4.2 Definition. Let f, g be real functions of a single variable
deﬁned on the natural numbers (most often we assume that the
values attained by both f and g are nonnegative). The notation
f(n) = O(g(n))
means that there exist constants n0 and C such that for all n ≥n0,
the inequality |f(n)| ≤C · g(n) holds. If one has to read f(n) =
O(g(n)) aloud one usually says “f is big-Oh of g”.
Here the information suppressed by the notation f(n) = O(g(n))
is the value of the constant C. It may be 0.1, 10, or 1010—we only
learn that some constant C exists. The notation f(n) = O(g(n))
can intuitively be understood as saying that the function f doesn’t

82
Combinatorial counting
grow much faster than g, i.e. that f(n)/g(n) doesn’t grow to inﬁn-
ity. Instead of f(n) and g(n), speciﬁc formulas may appear in this
notation. For example, we may write 10n2 + 5n = O(n2).
We should warn that f(n) = O(g(n)) says that f(n) is not too
big, but it does not say anything about f(n) not being very small.
For example, n+5 = O(n2) is a true statement, although perhaps not
as helpful as n + 5 = O(n). Let us also emphasize that although the
notation contains the equality sign “=”, it is asymmetric (essentially,
it is an inequality); one shouldn’t write O(f(n)) = g(n)!
The O( ) notation often allows us to simplify complicated expres-
sions wonderfully. For example, we have
(7n2 + 6n + 2)(n3 −3n + 28) = O(n5).
(3.10)
Why? We note the following two simple rules concerning the O( )
notation: if we have f1(n) = O(g1(n)) and f2(n) = O(g2(n)) then
f1(n) + f2(n) = O(g1(n) + g2(n)), and similarly for multiplication,
f1(n)f2(n) = O(g1(n)g2(n)) (Exercise 6). Since obviously n = O(n2)
and 1 = O(n2), by a repeated application of the addition and multi-
plication rules we get 7n2 + 6n + 2 = O(n2 + n2 + n2) = O(n2), and
similarly n3 −3n+28 = O(n3). A ﬁnal application of the multiplica-
tion rule gives Eq. (3.10). A nice thing in this derivation is that we
didn’t need to multiply out the parentheses ﬁrst!
After some practice, one can write estimates/simpliﬁcations like
(3.10) right away without too much thinking, by quickly spotting the
“main term” in an expression (the one that grows fastest) and letting
all others disappear in the O( ) notation. Such insight is usually based
on a (maybe subconscious) use of the following simple rules:
3.4.3 Fact (Useful asymptotic inequalities). In the following,
let C, a, α, β > 0 be some ﬁxed real numbers independent of n. We
have
(i) nα = O(nβ) whenever α ≤β (“a bigger power swallows a smaller
one”),
(ii) nC = O(an) for any a > 1 (“an exponential swallows a power”),
(iii) (ln n)C = O(nα) for any α > 0 (“a power swallows a logarithm”).
(In fact, in all the inequalities above, we can write the ⪯symbol
instead of the O( ) notation.)
Part (i) is trivial, proving part (ii) is a simple exercise in calculus,
and part (iii) can be easily derived from (ii) by taking logarithms.

3.4 Estimates: an introduction
83
Using the symbol O( ), we can also write a more exact comparison
of functions. For example, the notation f(n) = g(n) + O(√n) means
that the function f is the same as g up to an “error” of the order
√n, i.e. that f(n) −g(n) = O(√n). A simple concrete example is
n
2

= n(n −1)/2 = 1
2n2 + O(n).
The next example shows how to estimate a relatively complicated
sum.
3.4.4 Example. Let us put f(n) = 13 + 23 + 33 + · · · + n3. We want
to ﬁnd good asymptotic estimates for f(n).
In this case, it is possible to ﬁnd an exact formula for f(n) (see
Exercise 3.3.10), but it is quite laborious.3 But we can get reasonable
asymptotic estimates for f(n) in a less painful way. First, we may note
that f(n) ≤n · n3 = n4. On the other hand, at least n
2 addends in
the sum deﬁning f(n) are bigger than (n/2)3, and so f(n) ≥(n/2)4 =
n4/16. As a ﬁrst approximation, we thus see that f(n) behaves like the
function n4, up to small multiplicative factors.
To get a more precise estimate, we can employ the summation for-
mula (3.9) (Exercise 3.3.3) with r = 3:
3
3

+
4
3

+
5
3

+ · · · +
n
3

=
n + 1
4

.
Set g(k) =
k
3

. We ﬁnd that g(k) = k(k−1)(k−2)
3!
= k3
6 + O(k2). Hence
we have
f(n) =
n

k=1
k3 =
n

k=1
6g(k) +
n

k=1

k3 −6g(k)
 
= 6
n + 1
4

+ O

n

k=1
k2

= n4
4 + O(n3).
In this derivation, we have used the following fact: if f, g are some
functions such that f(n) = O(g(n)), then n
k=1 f(k) = O (n
k=1 g(k)).
It is a simple but instructive exercise to prove it.
A few more remarks. A similar “big-Oh” notation is also used for
functions of several variables. For instance, f(m, n) = O(g(m, n)) means
that for some constants m0, n0, C and for all m ≥m0 and all n ≥n0,
we have |f(n, m)| ≤C · g(m, n).
In the literature, one frequently encounters several other symbols for
expressing “inequality” between the order of magnitude of functions.
They can be quite useful since once one gets used to them, they provide
a convenient replacement for complicated phrases (such as “there exists
3At least by hand; many computer algebra systems can do it automatically.

84
Combinatorial counting
a constant c > 0 such that for all n ∈N we have . . . ” etc.). We will
not discuss them in detail, but we will at least list the deﬁnitions of the
most common symbols in the table below.
Notation
Deﬁnition
Meaning
f(n) = o(g(n))
limn→∞
f(n)
g(n) = 0
f grows much more slowly
than g
f(n) = Ω(g(n))
g(n) = O(f(n))
f grows at least as fast as g
f(n) = Θ(g(n))
f(n) = O(g(n)) and
f(n) = Ω(g(n))
f and g have about the
same order of magnitude
f(n) ∼g(n)
limn→∞
f(n)
g(n) = 1
f(n) and g(n) are almost
the same
So, ﬁnally, it is natural to ask—what is a bound f(n) = O(g(n))
good for? Since it doesn’t say anything about the hidden constant,
we cannot deduce an estimate of f(n) for any speciﬁc n from it!
There are several answers to this question. In some mathematical
considerations, we do not really care about any particular n, and it
is enough to know that some function doesn’t grow much faster than
another one. This can be used, for example, to prove the existence of
some object without actually constructing it (see Chapter 10 for such
a proof method). A more practically oriented answer is that in most
situations, the constant hidden in the O( ) notation can actually be
ﬁgured out if needed. One just has to go through a computation done
with the O( ) notation very carefully once more and track the con-
stants used in all the estimates. This is usually tedious but possible.
As a general rule of thumb (with many many exceptions), one can
say that if a simple proof leads to an O( ) estimate then the hidden
constant is usually not too large, and so if we ﬁnd that f(n) = O(n)
and g(n) = Ω(n2) then typically f(n) will be smaller than g(n) even
for moderate n. We add more remarks concerning the O( ) notation
in connection with algorithms in Section 5.3.
Exercises
1. Check that the relation ⪯introduced in the text is a transitive relation
on the set of all functions f : N →R. Find an example of functions f
and g such that neither f ⪯g nor g ⪯f.

3.5 Estimates: the factorial function
85
2. Find positive and nondecreasing functions f(n), g(n) deﬁned for all
natural numbers such that neither f(n) = O(g(n)) nor g(n) = O(f(n))
holds.
3. Explain what the following notations mean, and decide which of them
are true.
(a) n2 = O(n2 ln n)
(b) n2 = o(n2 ln n)
(c) n2 + 5n ln n = n2
1 + o(1)

∼n2
(d) n2 + 5n ln n = n2 + O(n)
(e) n
i=1 i8 = Θ(n9)
(f) n
i=1
√
i = Θ(n3/2).
4. What is the meaning of the following notations: f(n) = O(1), g(n) =
Ω(1), h(n) = nO(1)? How can they be expressed brieﬂy in words?
5. ∗Order the following functions according to their growth rate, and
express this ordering using the asymptotic notation introduced in this
section: n ln n, (ln ln n)ln n, (ln n)ln ln n, n · e
√
ln n, (ln n)ln n, n · 2ln ln n,
n1+1/(ln ln n), n1+1/ ln n, n2.
6. Check that if we have f1(n) = O(g1(n)) and f2(n) = O(g2(n)) then
f1(n) + f2(n) = O(g1(n) + g2(n)) and f1(n)f2(n) = O(g1(n)g2(n)).
3.5
Estimates: the factorial function
In this section, we are going to consider estimates of the function
n! (n factorial). At the ﬁrst sight, it might seem that the deﬁnition
of the factorial itself, i.e. the formula n! = n(n −1) · . . . · 2 · 1, tells
us everything we may ever need to know. For small values of n, n!
can be very quickly evaluated by a computer, and for larger n, one
might think that the values of the factorial are too large to have any
signiﬁcance in the “real world”. For example, 70! > 10100, as many
owners of pocket calculators with the n! button may know. But in
various mathematical considerations, we often need to compare the
order of magnitude of the function n! to other functions, even for
very large values of n. For this purpose, the deﬁnition itself is not
very suitable, and also an evaluation of n! by a computer sometimes
won’t be of much help. What we need are good estimates that bound
n! by some “simpler” functions.
When approaching a problem, it is usually a good strategy to start
looking for very simple solutions, and only try something complicated

86
Combinatorial counting
if simple things fail. For estimating n!, a very simple thing to try is
the inequality
n! =
n

i=1
i ≤
n

i=1
n = nn.
As for a very simple lower bound, we can write
n! =
n

i=2
i ≥
n

i=2
2 = 2n−1.
Hence, n! is somewhere between the functions 2n−1 and nn. In many
problems, this may be all we need to know about n!. But in other
problems, such as Example 3.5.1 below, we may start asking more
sophisticated questions. Is n! “closer” to nn or to 2n−1? Does the
function nn
n! grow to inﬁnity, and if so, how rapidly?
To some extent, this can be answered by still quite simple consid-
erations (similar to the ﬁrst part of the solution to Example 3.4.4). If
n is even, then n
2 of the numbers in the set {1, 2, . . . , n} are at most
n
2 , and n
2 of them are larger than n
2 . Hence, for n even, we have, on
the one hand,
n! ≥
n

i=n/2+1
i >
n

i=n/2+1
n
2 =
n
2
n/2
=
!n
2
n
,
(3.11)
and on the other hand,
n! ≤
 n/2

i=1
n
2

n

i=n/2+1
n

= nn
2n/2 .
(3.12)
For n odd, one has to be slightly more careful, but it turns out that
both the formulas n! >

n/2
n
and n! ≤nn/2n/2 can be derived
for all odd n ≥3 as well (Exercise 1). So, from Eq. (3.11) we see
that n! grows considerably faster than 2n; in fact, sooner or later it
outgrows any function Cn with a ﬁxed number C. Eq. (3.12) tells us
that nn grows still faster than n!.
Here is a simple example where the question of comparing nn and
n! arises naturally.
3.5.1 Example. Each of n people draws one card at random from a
deck of n cards, remembers the card, and returns it back to the deck.
What is the probability that no two of the people draw the same card?

3.5 Estimates: the factorial function
87
Is there some “reasonable chance”, or is it a very rare event? Mathe-
matically speaking, what is the probability that a mapping of the set
{1, 2, . . . , n} to itself chosen at random is a permutation?
The number of all mappings is nn, the number of permutations is n!,
and so the required probability is n!/nn. From the upper bound (3.12),
we calculate
n!
nn ≤nn/2n/2
nn
= 2−n/2.
Therefore, the probability is no more than 2−n/2, and for n not too
small, the considered event is extremely unlikely. From more precise
estimates for n! derived later on, we will see that the probability in
question behaves roughly as the function e−n.
A simple estimate according to Gauss. We show an elegant way of
deriving estimates similar to (3.11) and (3.12) but a bit stronger. This
proof is of some historical interest, since it was invented by the great
mathematician Gauss (or, written in the German way, Gauß), and we
also learn an important and generally useful inequality.
3.5.2 Theorem. For every n ≥1,
nn/2 ≤n! ≤
n + 1
2
n
.
We begin the proof with an inequality between the arithmetic and
geometric mean of two numbers. For positive real numbers a, b, we deﬁne
the arithmetic mean of a and b as a+b
2 , and the geometric mean4 of a
and b as
√
ab.
3.5.3 Lemma (Arithmetic–geometric mean inequality). For any
pair of positive real numbers a, b, the geometric mean is no bigger than
the arithmetic mean.
Proof.
The square of any real number is always positive, and so
(√a −
√
b)2 ≥0. By expanding the left-hand side we have a−2
√
ab+b ≥
0, and by adding 2
√
ab to both sides of this inequality and dividing by
2 we get
√
ab ≤a+b
2 . This is the desired inequality.
2
Proof of Theorem 3.5.2.
The idea is to pair up each number i ∈
{1, 2, . . . , n} with its “cousin” n+1−i and estimate each of the products
i(n + 1 −i) from above and from below. If i runs through the values 1,
2, . . . , n then n + 1 −i runs through n, n −1, . . . , 1. The product
4If g denotes the geometric mean of a and b then the ratio a : g is the same
as g : b. From the point of view of the ancient Greeks, g is thus the appropriate
segment “in the middle” between a segment of length a and a segment of length
b, and that’s probably why this mean is called geometric.

88
Combinatorial counting
n

i=1
i(n + 1 −i)
thus contains each factor j ∈{1, 2, . . . , n} exactly twice, and so it equals
(n!)2. Therefore we have
n! =
n

i=1

i(n + 1 −i).
(3.13)
If we choose a = i and b = n + 1 −i in the arithmetic–geometric mean
inequality, we get

i(n + 1 −i) ≤i + n + 1 −i
2
= n + 1
2
,
and by (3.13)
n! =
n

i=1

i(n + 1 −i) ≤
n

i=1
n + 1
2
=
n + 1
2
n
,
which proves the upper bound in Theorem 3.5.2.
In order to prove the lower bound for n!, it suﬃces to show that
i(n + 1 −i) ≥n for all i = 1, 2, . . . , n. For i = 1 and i = n we directly
calculate that i(n+1−i) = n. For 2 ≤i ≤n−1, we have a product of two
numbers, the larger one being at least n
2 and the smaller one at least 2,
and hence i(n + 1 −i) ≥n holds for all i. Therefore, n! ≥
√nn = nn/2
as was to be proved.
2
Of course, not everyone can invent such tricks as easily as Gauss
did, but at least the arithmetic–geometric mean inequality is worth
remembering.
Having learned some estimates of n!, we may keep asking more
and more penetrating questions, such as whether
 n+1
2
n/n! grows
to inﬁnity, and if so how fast, etc. We will now skip some stages of
this natural evolution and prove bounds that estimate n! up to a
multiplicative factor of only n (note that in the preceding estimates,
our uncertainty was still at least an exponential function of n). In
these more sophisticated estimates, we encounter the so-called Euler
number e = 2.718281828 . . ., the basis of the natural logarithms.
The reader may learn much more about this remarkable constant in
calculus. Here we need the following:
3.5.4 Fact. For every real number x,
1 + x ≤ex
holds (see Fig. 3.3).

3.5 Estimates: the factorial function
89
-1.5
-1
-0.5
0.5
1
1.5
1
2
3
4
Fig. 3.3 The functions y = 1 + x and y = ex in the vicinity of the origin.
This fact is something which should be marked in large bold-faced
ﬂuorescent letters in the notebook of every apprentice in asymptotic
estimates. Here we use it to prove
3.5.5 Theorem. For every n ≥1, we have
e
n
e
n
≤n! ≤en
n
e
n
.
First proof (by induction). We only prove the upper bound n! ≤
en(n/e)n, leaving the lower bound as Exercise 9. For n = 1, the right-
hand side becomes 1, and so the inequality holds. So we assume that
the inequality has already been proved for n −1, and we verify it for
n. We have
n! = n · (n −1)! ≤n · e(n −1)
n −1
e
n−1
by the inductive assumption. We further transform the right-hand
side to
"
en
n
e
n#
·
n −1
n
n
e.
In the brackets, we have the upper bound for n! we want to prove. So
it suﬃces to show that the remaining part of the expression cannot
exceed 1. By an algebraic manipulation and by using Fact 3.5.4 with
x = −1
n, we obtain
e
n −1
n
n
= e

1 −1
n
n
≤e

e−1/nn
= e · e−1 = 1.
2

90
Combinatorial counting
y = ln x
1
0
2
6
3
4
5
y = ln⌊x⌋
Fig. 3.4 Estimating the area below the step function by integration.
Let us note that Fact 3.5.4 is the only property of the number e
that was used in the proof; for example, the numerical value of e hasn’t
played any role. It so happens that e is characterized by Fact 3.5.4: If a is
a real number such that 1+x ≤ax for all x ∈R, then necessarily a = e.
The existence and uniqueness of a real number e with this property has
to be established by the means of mathematical analysis (a task which
we don’t consider here).
Second proof of Theorem 3.5.5 (using an integral).
We again
do the upper bound only. We begin with a formula for the factorial,
n! = 1 · 2 · . . . · n, and we take natural logarithms on both sides. In this
way, we get
ln n! = ln 1 + ln 2 + · · · + ln n
(the function ln is the logarithm with base e). The expression on the
right-hand side can be thought of as the area enclosed between the
x-axis and the step function x →ln ⌊x⌋on the interval [1, n + 1]; see
Fig. 3.4.
Since ln ⌊x⌋≤ln x on this interval, the area in question is no bigger
than the area below the graph of the function x →ln x on the interval
[1, n + 1]. We express this latter area as an integral:
ln n! ≤
$ n+1
1
ln x dx = (n + 1) ln(n + 1) −n,
as one can calculate as a simple exercise in integration. This estimate
can be further manipulated into
n! ≤e(n+1) ln(n+1)−n = (n + 1)n+1
en
.
This is not yet the expression we want. But we can use this inequality
for n −1 instead of n, and this gives the formula in the theorem:

3.5 Estimates: the factorial function
91
n! = n · (n −1)! ≤n · nn
en−1 = en
n
e
n
.
2
A curious reader might want to discover how the number e enters
the second proof. It might seem that we inserted it artiﬁcially, since
we started with taking the natural logarithm of n!, that is, logarithm
base e. However, it turns out that if we start with logarithm with any
other base, e appears in the ﬁnal bound as well, only the calculation
becomes more complicated.
For the reader’s interest, let us mention a considerably more precise
estimate for n!, known by the name Stirling’s formula: If we deﬁne the
function
f(n) =
√
2πn
n
e
n
,
where π = 3.1415926535 . . . is the area of the unit disk, we have
f(n) ∼n!. Recall that this means
lim
n→∞
f(n)
n!
= 1.
So if we estimate n! by f(n) then the relative error of this estimate
tends to 0 for n tending to inﬁnity. For example, for n = 8, the error
is about 1%. Let us note that Stirling’s formula is approximately “in
the middle” of the estimates from Theorem 3.5.5 (see also Exercise 10).
Proving Stirling’s formula requires somewhat more advanced tools from
calculus and it doesn’t quite ﬁt into this book, so we omit it (see Knuth
[41] for a proof).
Exercises
1. (a) Check that the formula n! >

n/2
n
is valid for all odd n ≥1,
by a consideration similar to Eq. (3.11).
(b) Check that also n! ≤nn/2n/2 holds for all odd n ≥3.
2. Using Fact 3.5.4, prove that
(a) (1 + 1
n)n ≤e for all n ≥1, and
(b) ∗(1 + 1
n)n+1 ≥e for all n ≥1.
(c) Using (a) and (b), conclude that limn→∞(1 + 1
n)n = e.
(d) Prove (1 −1
n)n ≤1e ≤(1 −1
n)n−1.
3. (Calculus required) ∗Prove Fact 3.5.4.
4. Show that
n√n tends to 1 for n →∞, and ∗use Fact 3.5.4 to prove
that
n√n −1 ≥ln n
n
for all n ≥1.

92
Combinatorial counting
5. Decide which of the following statements are true:
(a) n! ∼((n + 1)/2)n
(b) n! ∼ne(n/e)n
(c) n! = O((n/e)n)
(d) ln(n!) = Ω(n ln n)
(e) ln(n!) ∼n ln n.
6. (a) For which pairs (a, b), a, b > 0, does the equality
√
ab = (a + b)/2
hold?
(b) The harmonic mean of positive real numbers a, b is deﬁned by the
expression 2ab/(a + b). Based on examples, suggest a hypothesis for
the relation (inequality) of the harmonic mean to the arithmetic and
geometric means, and ∗prove it.
7. Let x1, x2, . . . , xn be positive reals. Their arithmetic mean equals (x1+
x2 + · · · + xn)/n, and their geometric mean is deﬁned as
n√x1x2 . . . xn.
Let AG(n) denote the statement “for any n-tuple of positive reals
x1, x2, . . . , xn, the geometric mean is less than or equal to the arith-
metic mean”. Prove the validity of AG(n) for every n by the following
strange induction:
(a) Prove that AG(n) implies AG(2n), for each n.
(b) ∗Prove that AG(n) implies AG(n −1), for each n > 1.
(c) Explain why proving (a) and (b) is enough to prove the validity of
AG(n) for all n.
8. (Computation of the number π) ∗Deﬁne sequences {a0, a1, a2, . . .} and
{b0, b1, b2, . . .} as follows: a0 = 2, b0 = 4, an+1 = √anbn, bn+1 =
2an+1bn/(an+1 + bn). Prove that both sequences converge to π. Hint:
Find a relation of the sequences to regular polygons with 2n sides
inscribed in and circumscribed to the unit circle.
Remark. This method (of Archimedes) of calculation of π is not very
eﬃcient. Here is an example of a much faster algorithm: x1 = 2−3/4 +
2−5/4, y1 = 21/4, π0 = 2 +
√
2, πn = πn−1(xn + 1)/(yn + 1), yn+1 =
(yn√xn + 1/√xn)/(yn + 1), xn+1 = (√xn + 1/√xn)/2. Then the πn
converge to π extremely fast. This and other such algorithms, as well
as the remarkable underlying theory, can be found in Borwein and
Borwein [16].
9. Prove the lower bound n! ≥e(n/e)n in Theorem 3.5.5
(a) by induction (use Fact 3.5.4 cleverly),
(b) via integration.
10. (Calculus required) ∗Prove the following upper bound for the fac-
torial function (which is already quite close to Stirling’s formula):

3.6 Estimates: binomial coeﬃcients
93
n! ≤e√n (n/e)n. Use the second proof of Theorem 3.5.5 as a starting
point, but from the area below the curve y = ln x, subtract the areas
of suitable triangles.
11. Prove Bernoulli’s inequality: for each natural number n and for every
real x ≥−1, we have (1 + x)n ≥1 + nx.
12. Prove that for n = 1, 2, . . ., we have
2
√
n + 1 −2 < 1 + 1
√
2 + 1
√
3 + · · · +
1
√n ≤2√n −1.
13. Let Hn be as in Example 3.4.1: Hn = n
i=1
1
i .
(a) ∗Prove the inequalities ln n < Hn ≤ln n+1 by induction on n (use
Fact 3.5.4).
(b) Solve (a) using integrals.
3.6
Estimates: binomial coeﬃcients
Similar to the way we have been investigating the behavior of the
function n!, we will now consider the function
n
k

= n(n −1) . . . (n −k + 1)
k(k −1) · . . . · 2 · 1
=
k−1

i=0
n −i
k −i.
(3.14)
From the deﬁnition of
n
k

, we immediately get
n
k

≤nk,
and for many applications, this simple estimate is suﬃcient. For
k > n
2 , one should ﬁrst use the equality
n
k

=
 n
n−k

.
In order to derive some lower bound for
n
k

, we look at the def-
inition of the binomial coeﬃcient written as a product of fractions,
as in (3.14). For n ≥k > i ≥0 we have n−i
k−i ≥n
k , and hence
n
k

≥
n
k
k
.
Quite good upper and lower bounds for
n
k

can be obtained from
Stirling’s formula, using the equality
n
k

=
n!
k!(n−k)!. These bounds are
somewhat cumbersome for calculation, however, and also we haven’t
proved Stirling’s formula. We do prove good but less accurate estimates
by diﬀerent methods (the main goal is to demonstrate these methods,
of course).

94
Combinatorial counting
3.6.1 Theorem. For every n ≥1 and for every k, 1 ≤k ≤n, we have
n
k

≤
en
k
k
.
Proof. We in fact prove a stronger inequality:
n
0

+
n
1

+
n
2

+ · · · +
n
k

≤
en
k
k
.
We start from the binomial theorem, which claims that
n
0

+
n
1

x +
n
2

x2 + · · · +
n
n

xn = (1 + x)n
for an arbitrary real number x. Let us now assume 0 < x < 1. Then by
omitting some of the addends on the left-hand side, we get
n
0

+
n
1

x + · · · +
n
k

xk ≤(1 + x)n,
and dividing this by xk leads to
1
xk
n
0

+
1
xk−1
n
1

+ · · · +
n
k

≤(1 + x)n
xk
.
Each of the binomial coeﬃcients on the left-hand side is multiplied by
a coeﬃcient that is at least 1 (since we assume x < 1), and so if we
replace these coeﬃcients by 1s the left-hand side cannot increase. We
obtain
n
0

+
n
1

+ · · · +
n
k

≤(1 + x)n
xk
.
The number x ∈(0, 1) can still be chosen at will, and we do it in such
a way that the right-hand side becomes as small as possible. A suitable
value, which can be discovered using some elementary calculus, is x = k
n.
By substituting this value into the right-hand side, we ﬁnd
n
0

+
n
1

+ · · · +
n
k

≤

1 + k
n
n n
k
k
.
Finally, by using Fact 3.5.4 we arrive at

1 + k
n
n
≤

ek/nn
= ek,
and the inequality in Theorem 3.6.1 follows.
2
The trick used in this proof is a small glimpse into the realm of
perhaps the most powerful known techniques for asymptotic estimates,

3.6 Estimates: binomial coeﬃcients
95
using the so-called generating functions. We will learn something about
generating functions in Chapter 12, but to see the full strength of this
approach for asymptotic bounds, one needs to be familiar with the
theory of functions of a complex variable.
The binomial coefficient

n
⌊n/2⌋

.
From the deﬁnition of the
binomial coeﬃcients, we can easily get the following formula:
n
k

= n −k + 1
k

n
k −1

.
Therefore, for k ≤n/2 we have
n
k

>
 n
k−1

, and conversely, for
k ≥n/2 we obtain
n
k

>
 n
k+1

. Hence for a given n, the largest
among the binomial coeﬃcients
n
k

are the middle ones: for n even,
 n
n/2

is bigger than all the others, and for n odd, the two largest
binomial coeﬃcients are

n
⌊n/2⌋

and

n
⌈n/2⌉

.
The behavior of the binomial coeﬃcient
n
k

as a function of k, with
n ﬁxed as some large number and for k close to n/2, is illustrated in
Fig. 3.5(a). The graph of the function
n
k

isn’t really a continuous curve
(since
n
k

is only deﬁned for an integer k), but if n is very large, there
are so many points that they visually blend into a curve. The “height”
of this bell-shaped curve is exactly

n
⌊n/2⌋

, and the “width” of the bell
shape approximately in the middle of its height is about 1.5√n. The
scales on the vertical and horizontal axes are thus considerably diﬀerent:
the horizontal axis shows a range of k of length 3√n, while the vertical
range is

n
⌊n/2⌋

(which is nearly 2n as we will soon see).
If you plot the function x →e−x2/2, you get a curve which looks
exactly the same as the one we have plotted for binomial coeﬃcients,
up to a possibly diﬀerent scaling of the axes. This is because the e−x2/2
curve, called the Gauss curve, is a limit of the curves for binomial co-
eﬃcients for n →∞(in a suitably deﬁned precise sense). The Gauss
curve is very important in probability theory, statistics, and other ar-
eas. For example, it describes a typical distribution of errors in physical
measurements, the percentage of days with a given maximal tempera-
ture within a long time period, and so on. In statistics, the distribution
given by the Gauss curve is called the normal distribution. The Gauss
curve is one of the “ubiquitous” mathematical objects arising in many
often unexpected contexts (another such omnipresent object is the Eu-
ler number e, and we will meet some others later in this book). You
can learn more about the Gauss curve and related things in a prob-
ability theory textbook (Grimmett and Stirzaker [20] can be highly
recommended).

96
Combinatorial counting
n
2
n
2 −√n
n
2 + √n
(a)
(b)
k
Fig. 3.5 A graph of
n
k

as a function of k in the vicinity of
n
2 (a), or
perhaps a hat, or maybe a gigantic boa constrictor which has swallowed an
elephant (b) (see [28]).
How large is the largest binomial coeﬃcient

n
⌊n/2⌋

? A simple but
often accurate enough estimate is
2n
n + 1 ≤

n
⌊n/2⌋

≤2n.
The upper bound is obvious from the equality n
k=0
n
k

= 2n, and
the lower bound follows from it as well, because

n
⌊n/2⌋

is largest
among the n + 1 binomial coeﬃcients
n
k

whose sum is 2n.
We prove a considerably more precise estimate. For convenient
notation, we will only work with even values of n, and so we write
n = 2m.
3.6.2 Proposition. For all m ≥1 we have
22m
2√m ≤
2m
m

≤22m
√
2m.
Proof.
Both inequalities are proved similarly. Let us consider the
number
P = 1 · 3 · 5 · . . . · (2m −1)
2 · 4 · 6 · . . . · 2m
(the whole idea of the proof is hidden in this step). Since
P = 1 · 3 · 5 · . . . · (2m −1)
2 · 4 · 6 · . . . · 2m
· 2 · 4 · . . . · (2m)
2 · 4 · . . . · (2m) =
(2m)!
22m(m!)2 ,
we get
P =
1
22m
2m
m

.
Thus, we want to prove
1
2√m ≤P ≤
1
√
2m.

3.6 Estimates: binomial coeﬃcients
97
For the upper bound, consider the product

1 −1
22
 
1 −1
42

. . .

1 −
1
(2m)2

,
which can be rewritten as
1 · 3
22
 3 · 5
42

. . .
(2m −1)(2m + 1)
(2m)2

= (2m + 1)P 2.
Since the value of the product is less than 1, we get (2m + 1)P 2 < 1,
and hence P ≤1/
√
2m.
For the lower bound we consider the product

1 −1
32
 
1 −1
52

. . .

1 −
1
(2m −1)2

,
and we express it in the form
2 · 4
32
 4 · 6
52

. . .
(2m −2)(2m)
(2m −1)2

=
1
2 · (2m) · P 2 ,
which gives P ≥1/2√m.
2
Let us remark that by approximating both (2m)! and m! using
Stirling’s formula, we get a more precise result
2m
m

∼22m
√πm.
Such estimates have interesting relations with number theory, for
example. One of the most famous mathematical theorems is the follow-
ing statement about the density of primes:
3.6.3 Theorem (Prime number theorem).
Let π(n) denote the
number of primes not exceeding the number n. Then
π(n) ∼
n
ln n
(i.e. limn→∞π(n) ln n/n = 1).
Several proofs of this theorem are known, all of them quite diﬃcult
(and a quest for interesting variations and simpliﬁcations still contin-
ues). Within the 19th century, Tschebyshev found a simple proof of the
following weaker result:
π(n) = Θ
 n
ln n

,
i.e. c1n/ ln n ≤π(n) ≤c2n/ ln n holds for all n and for certain constants
c2 ≥c1 > 0. Part of the proof is based on the estimates
22m
2m+1 ≤
2m
m

≤

98
Combinatorial counting
22m (see Exercise 2). Tschebyshev also proved the so-called Bertrand
postulate: For every n ≥1, there exists a prime p with n < p ≤2n.
Perhaps the simplest known proof uses, among others, the estimates in
Proposition 3.6.2. The reader can learn about these nice connections in
Chandrasekhar [35], for example.
Exercises
1. (a) Prove the estimate
n
k

≤(en/k)k by induction on k.
(b) Prove the estimate in (a) directly from Theorem 3.5.5.
2. (Tschebyshev estimate of π(n))
(a) Show that the product of all primes p with m < p ≤2m is at most
2m
m

.
(b) ∗Using (a), prove the estimate π(n) = O(n/ ln n), where π(n) is as
in the prime number theorem 3.6.3.
(c) ∗Let p be a prime, and let m, k be natural numbers. Prove that if
pk divides
2m
m

then pk ≤2m.
(d) Using (c), prove π(n) = Ω(n/ ln n).
3.7
Inclusion–exclusion principle
We begin with a simple motivating example. As many authors of exam-
ples with ﬁnite sets have already done, we resort to a formulation with
clubs in a small town.
3.7.1 Example. The town of N. has 3 clubs. The lawn-tennis club has
20 members, the chandelier collectors club 15 members, and the mem-
bership of the Egyptology club numbers 8. There are 2 tennis players
and 3 chandelier collectors among the Egyptologists, 6 people both play
tennis and collect chandeliers, and there is even one especially eager per-
son participating in all three clubs. How many people are engaged in
the club life in N.?
As a warm-up, let us count the combined membership of tennis and
Egyptology. We see that we have to add the number of tennis players
and the Egyptology fans and subtract those persons who are in both
these clubs, since they are accounted for twice in the sum. Written in
symbols, we have |T ∪E| = |T| + |E| −|T ∩E| = 20 + 8 −2 = 26. The
reader who isn’t discouraged by the apparent silliness of the whole prob-
lem5 can probably ﬁnd, with similar but more complicated considera-
tions, that the answer for the 3 clubs is 33. To ﬁnd the answer, it may
be helpful to draw a picture:
5Which may indicate mathematical inclinations.

3.7 Inclusion–exclusion principle
99
C
T
E
The inclusion–exclusion principle mentioned in the section’s title is
a formula which allows us to solve problems of a similar type for an
arbitrary number of clubs. It is used in situations where we want to
compute the size of the union of several sets, and we know the sizes
of all possible intersections of these sets. For 2 sets, T and E, such
a formula has been given above, and for 3 sets C, T, E it reads
|C ∪T ∪E| = |C|+|T|+|E|−|C ∩T|−|C ∩E|−|T ∩E|+|C ∩T ∩E|.
Expressed in words: in order to get the size of the union, we ﬁrst add
up the sizes of all the sets, then we subtract the sizes of all pairwise
intersections, and ﬁnally we add the size of the intersection of all
the 3 sets. As will be shown in a moment, such a method also works
for an arbitrary number n of ﬁnite sets A1, A2, . . . , An. The size of
their union, i.e. |A1 ∪A2 ∪· · · ∪An|, is obtained as follows: we add
up the sizes of all the sets, then we subtract the sizes of all pairwise
intersections, add the sizes of all triple intersections, subtract the
sizes of all 4-wise intersections, etc.; as the last step, we either add
(for n odd) or subtract (for n even) the size of the intersection of all
the n sets.
How do we write this in a formula? One attempt might be
|A1 ∪A2 ∪· · · ∪An| = |A1| + |A2| + · · · + |An|
−|A1 ∩A2|−|A1 ∩A3|−· · ·−|A1 ∩An|−|A2 ∩A3|−· · ·−|An−1 ∩An|
+|A1 ∩A2 ∩A3| + |A1 ∩A2 ∩A4|
+ · · · + (−1)n−1|A1 ∩A2 ∩· · · ∩An|.
This is a cumbersome and not very clear way of expressing such a
simple rule. Slightly better is a notation using sums:
|A1 ∪A2 ∪· · · ∪An| =
n

i=1
|Ai| −

1≤i1<i2≤n
|Ai1 ∩Ai2|

100
Combinatorial counting
+

1≤i1<i2<i3≤n
|Ai1 ∩Ai2 ∩Ai3|
−· · · + (−1)n−1|A1 ∩A2 ∩· · · ∩An|.
If we recall the notation
X
k

for the set of all k-element subsets of
a set X, and if we use a notation similar to  also for multiple
intersections and unions, we can write the same formula still more
elegantly:
3.7.2 Theorem (Inclusion–exclusion principle). For any collec-
tion A1, A2, . . . , An of ﬁnite sets, we have

n	
i=1
Ai
 =
n

k=1
(−1)k−1

I∈({1,2,...,n}
k
)


i∈I
Ai
.
(3.15)
In case you cannot see why this formula expresses the rule we
have formulated in words, you may want to devote some time to it
and work out the case n = 3 in detail. Many students have problems
with this notation (or any mathematical notation) for the inclusion–
exclusion principle, confusing numbers with sets and vice versa, and
this makes a clean solution of more complicated problems very hard.
Finally, the shortest and almost devilish way of writing the inclusion–
exclusion principle is

n
	
i=1
Ai
 =

∅̸=I⊆{1,2,...,n}
(−1)|I|−1


i∈I
Ai
.
(3.16)
First proof of the inclusion–exclusion principle: by induction.
The induction is on n, the number of sets. There is a small subtlety
here: for the inductive step, we need the formula for the case n = 2,
and so we use n = 2 as the basis for induction. For 2 sets, as we
know, the formula holds. Assume its validity for arbitrary n−1 sets.
We have

n	
i=1
Ai
 =

 n−1
	
i=1
Ai

∪An
 =

n−1
	
i=1
Ai
 + |An| −

 n−1
	
i=1
Ai

∩An

(here we used inclusion-exclusion for 2 sets, i.e. the equality
|A ∪B| = |A| + |B| −|A ∩B| with A = A1 ∪· · · ∪An−1, B = An)
=

n−1
	
i=1
Ai
 + |An| −

n−1
	
i=1
(Ai ∩An)


3.7 Inclusion–exclusion principle
101
(distributivity of the intersection: X ∩(Y ∪Z) = (X ∩Y ) ∪(X ∩Z);
now we use the inductive hypothesis twice, once for |A1 ∪· · ·∪An−1|
and once for |A′
1 ∪· · · ∪A′
n−1|, where A′
i = Ai ∩An)
=
 n−1

k=1
(−1)k−1

I∈({1,2,...,n−1}
k
)


i∈I
Ai


+ |An|
−
 n−1

k=1
(−1)k−1

I∈({1,2,...,n−1}
k
)


i∈I∪{n}
Ai


.
We are nearly done. In the ﬁrst sum, we add, with the proper signs,
the sizes of all intersections not involving the set An. In the second
sum, the sizes of all the intersections involving An appear, and the
intersection of k + 1 sets (i.e. some k sets among A1, . . . , An−1 plus
An) has the sign −(−1)k−1 = (−1)k. The second sum doesn’t inc-
lude the term |An|, but this appears separately between both sums.
Altogether, the size of the intersection of any k-tuple of sets among
A1, . . . , An appears exactly once in the expression, with the sign
(−1)k−1. This agrees with Eq. (3.15), and the proof by induction is
ﬁnished. Without a reasonable notation, we would easily get lost in
this proof.
2
Second proof of the inclusion–exclusion principle: by count-
ing.
Let us consider an arbitrary element x ∈A1 ∪· · · ∪An. It
contributes exactly 1 to the size of the union on the left-hand side
of (3.15). Let us look at how much x contributes to the various in-
tersection sizes on the right-hand side. Let j be the number of sets
among the Ai that contain x. We can rename the sets so that x is
contained in A1, A2, . . . , Aj.
The element x now appears in the intersection of every k-tuple
of sets among A1, A2, . . . , Aj and in no other intersections. Since
there are
j
k

k-element subsets of a j-element set, x appears in
j
k

intersections of k-tuples of sets. The sizes of k-wise intersections are
counted with the sign (−1)k−1, and so x contributes the quantity
j −
j
2

+
j
3

−· · · + (−1)j−1
j
j

to the right-hand side of the inclusion–exclusion formula (3.15). By
the formula (3.8) for the sum of binomial coeﬃcients with alternat-
ing signs, the above expression equals 1. The contribution of each

102
Combinatorial counting
element x to both sides of the inclusion–exclusion formula (3.15) is
thus 1, and the formula is proved.
2
And one more proof. If one looks at the inclusion–exclusion prin-
ciple in a proper way, it is a consequence of the following formula for
expanding a product:
(1 + x1)(1 + x2) . . . (1 + xn) =

I⊆{1,2,...,n}
 
i∈I
xi

.
(3.17)
Contemplate what this formula says (write it out for n = 1, 2, 3, say)
and why it holds.
In order to prove the inclusion–exclusion principle, let us denote
A = A1 ∪A2 ∪· · · ∪An, and let fi : A →{0, 1} be the characteristic
function of the set Ai, which means that fi(a) = 1 for a ∈Ai and
fi(a) = 0 otherwise. For every a ∈A, we have n
i=1(1 −fi(a)) = 0
(don’t we?), and using (3.17) with xi = −fi(a) we get

I⊆{1,2,...,n}
(−1)|I| 
i∈I
fi(a) = 0.
By adding all these equalities together for all a ∈A, and then by inter-
changing the summation order, we arrive at
0 =

a∈A


I⊆{1,2,...,n}
(−1)|I| 
i∈I
fi(a)

=

I⊆{1,2,...,n}
(−1)|I|
 
a∈A

i∈I
fi(a)

.
(3.18)
Now it suﬃces to note that the 
i∈I fi(a) is the characteristic func-
tion of the set 
i∈I Ai, and therefore 
a∈A

i∈I fi(a) =

i∈I Ai
. In
particular, for I = ∅, 
i∈∅fi(a) is the empty product, with value 1
by deﬁnition, and so 
a∈A

i∈∅fi(a) = 
a∈A 1 = |A|. Hence (3.18)
means
|A| +

∅̸=I⊆{1,2,...,n}
(−1)|I|


i∈I
Ai
 = 0,
and this is exactly the inclusion–exclusion principle. An expert in algebra
can thus regard the inclusion–exclusion principle with mild contempt:
a triviality, she might say.
2
Bonferroni inequalities. Sometimes we can have the situation where
we know the sizes of all the intersections up to m-fold ones, but we
do not know the sizes of intersections of more sets than m. Then we
cannot calculate the size of the union of all sets exactly. The so-called

3.8 The hatcheck lady & co.
103
Bonferroni inequalities tell us that if we leave out all terms with k > m
on the right-hand side of the inclusion–exclusion principle (3.15) then
the error that we make in this way in the calculation of the size of the
union has the same sign as the ﬁrst omitted term. Written as a formula,
for every even q we have
q

k=1
(−1)k−1

I∈(
{1,2,...,n}
k
)


i∈I
Ai
 ≤

n
	
i=1
Ai

(3.19)
and for every odd q we have
q

k=1
(−1)k−1

I∈(
{1,2,...,n}
k
)


i∈I
Ai
 ≥

n
	
i=1
Ai
.
(3.20)
This means, for instance, that if we didn’t know how many diligent
persons are simultaneously in all the three clubs in Example 3.7.1, we
could still estimate that the total number of members in all the clubs
is at least 32. We do not prove the Bonferroni inequalities here.
Exercises
1. Explain why the formulas (3.15) and (3.16) express the same equality.
2. ∗Prove the Bonferroni inequalities. If you cannot handle the general
case try at least the cases q = 1 and q = 2.
3. (Sieve of Eratosthenes) How many numbers are left in the set
{1, 2, . . . , 1000} after all multiples of 2, 3, 5, and 7 are crossed out?
4. How many numbers n < 100 are not divisible by a square of any integer
greater than 1?
5. ∗How many orderings of the letters A, B, C, D, E, F, G, H, I, J, K,
L, M, N, O, P are there such that we cannot obtain any of the words
BAD, DEAF, APE by crossing out some letters? What if we also forbid
LEADING?
6. How many ways are there to arrange 4 Americans, 3 Russians, and 5
Chinese into a queue, in such a way that no nationality forms a single
consecutive block?
3.8
The hatcheck lady & co.
3.8.1 Problem (Hatcheck lady problem). Honorable gentlemen,
n in number, arrive at an assembly, all of them wearing hats, and
they deposit their hats in a cloak-room. Upon their departure, the

104
Combinatorial counting
hatcheck lady, maybe quite absent-minded that day, maybe even al-
most blind after many years of service in the poorly lit cloak-room,
issues one hat to each gentleman at random. What is the probability
than none of the gentlemen receives his own hat?
As stated, this is a toy problem, but mathematically it is quite
remarkable, and a few hundred years back, it occupied some of the
best mathematical minds of their times. First we reformulate the
problem using permutations. If we number the gentlemen (our apolo-
gies) 1, 2, . . . , n, and their hats too, then the activity of the hatcheck
lady results in a random permutation π of the set {1, 2, . . . , n},
where π(i) is the number of the hat returned to the ith gentle-
man. The question is, what is the probability of π(i) ̸= i holding
for all i ∈{1, 2, . . . , n}? Call an index i with π(i) = i a ﬁxed point
of the permutation π. So we ask: what is the probability that a ran-
domly chosen permutation has no ﬁxed point? Each of the n! possible
permutations is, according to the description of the hatcheck lady’s
method of working, equally probable, and so if we denote by D(n)
the number of permutations with no ﬁxed point6 on an n-element
set, the required probability equals D(n)/n!.
Using the inclusion–exclusion principle, we derive a formula for
D(n). We will actually count the “bad” permutations, i.e. those
with at least one ﬁxed point. Let Sn denote the set of all permu-
tations of {1, 2, . . . , n}, and for i = 1, 2, . . . , n, we deﬁne Ai = {π ∈
Sn : π(i) = i}. The bad permutations are exactly those in the union
of all the Ai.
Here we suggest that the reader contemplate the deﬁnition of the
sets Ai carefully—it is a frequent source of misunderstandings (their
elements are permutations, not numbers).
In order to apply the inclusion–exclusion principle, we have to
express the size of the k-fold intersections of the sets Ai. It is easy
to see that |Ai| = (n−1)!, because if π(i) = i is ﬁxed, we can choose
an arbitrary permutation of the remaining n −1 numbers. Which
permutations lie in A1 ∩A2? Just those with both 1 and 2 as ﬁxed
points (and the remaining numbers can be permuted arbitrarily), and
so |A1∩A2| = (n−2)!. More generally, for arbitrary i1 < i2 < · · · < ik
we have |Ai1 ∩Ai2 ∩· · · ∩Aik| = (n −k)!, and substituting this into
the inclusion–exclusion formula yields
6Such permutations are sometimes called derangements.

3.8 The hatcheck lady & co.
105
|A1 ∪· · · ∪An| =
n

k=1
(−1)k−1
n
k

(n −k)! =
n

k=1
(−1)k−1 n!
k!.
We recall that we have computed the number of bad permutations
(with at least one ﬁxed point), and so
D(n) = n! −|A1 ∪· · · ∪An| = n! −n!
1! + n!
2! −· · · + (−1)n n!
n! ,
which can still be rewritten as
D(n) = n!

1 −1
1! + 1
2! −· · · + (−1)n 1
n!

.
(3.21)
As is taught in calculus, the series in parentheses converges to e−1
for n →∞(where e is the Euler number), and it does so very
fast. So we have the approximate relation D(n) ≈n!/e, and the
probability in the hatcheck lady problem converges to the constant
e−1 = 0.36787 . . .. This is what also makes the problem remarkable:
the answer almost doesn’t depend on the number of gentlemen!
The Euler function ϕ. A function denoted usually by ϕ and named
after Leonhard Euler plays an important role in number theory. For a
natural number n, the value of ϕ(n) is deﬁned as the number of natural
numbers m ≤n that are relatively prime to n; formally
ϕ(n) = |{m ∈{1, 2, . . . , n}: gcd(n, m) = 1}| .
Here gcd(n, m) denotes the greatest common divisor of n and m; that is,
the largest natural number that divides both n and m. As an example
of application of the inclusion–exclusion principle, we ﬁnd a formula
which allows us to calculate ϕ(n) quickly provided that we know the
factorization of n into prime factors.
The simplest case is when n = p is a prime. Then every m < p is
relatively prime to p, and so ϕ(p) = p −1.
The next step towards the general solution is the case when n = pα
(α ∈N) is a prime power. Then the numbers not relatively prime to
pα are multiples of p, i.e. p, 2p, 3p, . . . , pα−1p, and there are pα−1 such
multiples not exceeding pα (in general, if d is an any divisor of some
number n, then the number of multiples of d not exceeding n is n/d).
Hence, there are ϕ(pα) = pα −pα−1 = pα(1 −1/p) remaining numbers
that are relatively prime to pα.
An arbitrary n can be written in the form
n = pα1
1 pα2
2 . . . pαr
r ,
where p1, p2, . . . , pr are distinct primes and αi ∈N. The “bad” m ≤n,
i.e. those not contributing to ϕ(n), are all multiples of some of the

106
Combinatorial counting
primes pi. Let us denote by Ai = {m ∈{1, 2, . . . , n}: pi|m} the set
of all multiples of pi. We have ϕ(n) = n −|A1 ∪A2 ∪· · · ∪Ar|. The
inclusion–exclusion principle commands that we ﬁnd the sizes of the
intersections of the sets Ai. For example, the intersection A1 ∩A2 con-
tains the numbers divisible by both p1 and p2, which are exactly the
multiples of p1p2, and hence |A1 ∩A2| = n/(p1p2). The same argument
gives
|Ai1 ∩Ai2 ∩· · · ∩Aik| =
n
pi1pi2 . . . pik
.
Let us look at the particular cases r = 2 and r = 3 ﬁrst. For n = pα1
1 pα2
2
we have
ϕ(n) = n −|A1 ∪A2| = n −|A1| −|A2| + |A1 ∩A2|
= n −n
p1
−n
p2
+
n
p1p2
= n

1 −1
p1
 
1 −1
p2

.
Similarly, for n = pα1
1 pα2
2 pα3
3
we get
ϕ(n) = n −n
p1
−n
p2
−n
p3
+
n
p1p2
+
n
p1p3
+
n
p2p3
−
n
p1p2p3
= n

1 −1
p1
 
1 −1
p2
 
1 −1
p3

.
This may raise a suspicion concerning the general formula.
3.8.2 Theorem. For n = pα1
1 pα2
2 . . . pαr
r , we have
ϕ(n) = n

1 −1
p1
 
1 −1
p2

. . .

1 −1
pr

.
(3.22)
Proof. For an arbitrary r, the inclusion–exclusion principle (we use,
to our advantage, the short formula (3.16)) gives
ϕ(n) = n −

∅̸=I⊆{1,2,...,r}
(−1)|I|−1
n

i∈I pi
= n ·

I⊆{1,2,...,r}
(−1)|I|

i∈I pi
.
We claim that this frightening formula equals the right-hand side of
Eq. (3.22). This follows from the formula (3.17) for expanding the prod-
uct (1+x1)(1+x2)(1+x3) . . . by substituting xi = −1/pi, i = 1, 2, . . . , r.
2
Exercises
1. There are n married couples attending a dance. How many ways are
there to form n pairs for dancing if no wife should dance with her
husband?

3.8 The hatcheck lady & co.
107
2. (a) Determine the number of permutations with exactly one ﬁxed
point.
(b) Count the permutations with exactly k ﬁxed points.
3. What is wrong with the following inductive “proof” that D(n) =
(n −1)! for all n ≥2? Can you ﬁnd a false step in it?
For n = 2,
the formula holds, so assume n ≥3. Let π be a permutation of
{1, 2, . . . , n −1} with no ﬁxed point. We want to extend it to a per-
mutation π′ of {1, 2, . . . , n} with no ﬁxed point. We choose a number
i ∈{1, 2, . . . , n −1}, and we deﬁne π′(n) = π(i), π′(i) = n, and π′(j) =
π(j) for j ̸= i, n. This deﬁnes a permutation of {1, 2, . . . , n}, and it is easy
to check that it has no ﬁxed point. For each of the D(n −1) = (n −2)!
possible choices of π, the index i can be chosen in n −1 ways. Therefore,
D(n) = (n −2)! · (n −1) = (n −1)!.
4. ∗Prove the equation
D(n) = n! −nD(n −1) −
n
2

D(n −2) −· · · −

n
n −1

D(1) −1.
5. (a) ∗Prove the recurrent formula D(n) = (n−1)[D(n−1)+D(n−2)].
Prove the formula (3.21) for D(n) by induction.
(b) ∗Calculate the formula for D(n) directly from the relation derived
in (a). Use an auxiliary sequence given by an = D(n) −n D(n −1).
6. How many permutations of the numbers 1, 2, . . . , 10 exist that map no
even number to itself?
7. (Number of mappings onto) Now is the time to calculate the number
of mappings of an n-element set onto an m-element set (note that we
have avoided it so far). Calculate them
(a) for m = 2
(b) for m = 3.
(c) ∗Write a formula for a general m; check the result for m = n = 10
(what is the result for n = m?). Warning: The resulting formula is a
sum, not a “nice” formula like a binomial coeﬃcient.
(d) ∗Show, preferably without using part (c), that the number of map-
pings onto an m-element set is divisible by m!.
8. (a) ∗How many ways are there to divide n people into k groups (or:
how many equivalences with k classes are there on an n-element set)?
Try solving this problem for k = 2, 3 and k = n −1, n −2 ﬁrst. For a
general k, the answer is a sum.
(b) What is the total number of equivalences on an n-element set?
(Here the result is a double sum.)

108
Combinatorial counting
(c) ∗If we denote the result of (b) by Bn (the nth Bell number), prove
the following (surprising) formula:
Bn = 1
e
∞

i=0
in
i! .
9. ∗Prove the formula (3.22) for the Euler function in a diﬀerent way.
Suppose it holds for n = pα (a prime power). Prove the following
auxiliary claim: if m, n are relatively prime, then ϕ(mn) = ϕ(m)ϕ(n).
10. ∗For an arbitrary natural number n, prove that 
d|n ϕ(d) = n (the
sum is over all natural numbers d dividing n).
11. (a) How many divisors does the number n = pα1
1 pα2
2 . . . pαr
r
have
(p1, p2, . . . , pr are distinct primes)?
(b) Show that the sum of all divisors of such a number n equals
r

i=1
pαi+1 −1
pi −1
.
(c) ∗∗Call a number n perfect if it equals the sum of all its divisors
(excluding itself). For example, 6 = 1 + 2 + 3 is perfect. Prove that
every even perfect number has the form 2q(2q+1 −1), where q ≥1 is
a natural number and 2q+1 −1 is a prime.
Remark. No odd perfect numbers are known but no one can show that
they don’t exist.
12. (a) ∗For a given natural number N, determine the probability that
two numbers m, n ∈{1, 2, . . . , N} chosen independently at random
are relatively prime.
(b) ∗Prove that the limit of the probability in (a) for N →∞equals
the inﬁnite product 
p(1 −1/p2), where p runs over all primes. (Let
us remark that the value of this product is 6/π2; this can be proved
from Fact 12.7.1.)
13. (a) Determine the number of graphs with no vertices of degree 0 on a
given n-element vertex set V (see Sections 4.1 and 4.3 for the relevant
deﬁnitions).
(b) Determine the number of all graphs with at least 2 vertices of
degree 0 on V , and with exactly 2 vertices of degree 0.
14. ∗How many ways are there to seat n married couples at a round table
with 2n chairs in such a way that the couples never sit next to each
other?

4
Graphs: an introduction
4.1
The notion of a graph; isomorphism
Many situations in various practically motivated problems and also
in mathematics and theoretical computer science can be captured by
a scheme consisting of two things:
• a (ﬁnite) set of points, and
• lines joining some pairs of the points.
For example, the points may represent participants at a birthday
party and the joins correspond to pairs of participants who know
each other. Or the points can represent street crossings in a city and
the joins the streets. Also a municipal transport network or a railway
network is usually displayed as a scheme of this type (see Fig. 4.1),
and electrotechnical schemes often have a similar character as well.
In such cases, the points are commonly called vertices (or also nodes)
and the joins are called edges.1
If we disregard the length, shape, and other properties of the
joins and we only pay attention to which pairs of points are joined
and which are not, we arrive at the mathematical notion of a graph.
Although very simple, a graph is one of the key concepts in discrete
mathematics. This is also illustrated in the subsequent sections.
4.1.1 Definition. A graph2 G is an ordered pair (V, E), where V is
some set and E is a set of 2-point subsets of V . The elements of the
set V are called vertices of the graph G and the elements of E edges
of G.
1The origins of this terminology are mentioned in Section 6.3.
2What we simply call a graph here is sometimes more verbosely called a simple
undirected graph, in order to distinguish it from other, related notions. Some of
them will be mentioned later.

110
Graphs: an introduction
Praha
Vran´e
n. Vlt.
Dobˇr´ıˇs
Hostivice
Zdice
Lochovice
Sedlˇcany
Olbramovice
ˇCerˇcany
Ledeˇcko
Kladno
Rakovn´ık
Krup´a
ˇZatec
Kouˇrim
Beˇcv´ary
Boˇsice
Peˇcky Velk´y Osek
Kol´ın
Kutn´a Hora
ˇC´aslav
Chlumec
Ostromˇeˇr
Kˇrinec
Kopidlno
Poˇr´ıˇcany
Mˇeln´ık
Luˇzec
n. Vlt.
Vˇsetaty
Neratovice
Mochov
ˇCel´akovice
Milovice
Vele-
liby
Mlad´a
Boleslav
Bakov n. Jizerou
Doln´ı
Bousov
Jiˇc´ın
Libuˇn
Star´a
Paka
Zlonice
Libochovice
Straˇskov
Louny
Louny-
pˇredmˇest´ı
Podleˇs´ın
Kralupy
n. Vlt.
Vra-
ˇnany
Kole-
ˇsovice
Tˇremoˇs-
nice
n. Cidl.
Roudnice
n. L.
Vel-
vary
Lys´a n.L.
Zadn´ı
Tˇrebaˇn
Beroun-
Z´avod´ı
Beroun
Luˇzn´a
u Rakovn´ıka
Zruˇc
n. S´az.
Svˇetl´a
n. S´az.
Beneˇsov
u Prahy
Pˇr´ıbram
Vlaˇsim
ˇCesk´y
Brod
Slan´y
Nymburk-
hl. n.
Nymburk-
mˇesto
Trhov´y
ˇStˇep´anov
Posto-
loprty
Rudn´a
u Prahy
Fig. 4.1 A scheme of part of the Czech railway network in a region of about
150 × 100 km around Prague.
In this book, we almost always consider graphs with ﬁnite vertex
sets. The few cases where we deal with inﬁnite graphs too will be
mentioned explicitly.
If we want to point out that some graph G has V as the vertex
set and E as the edge set we write G = (V, E). If we talk about some
known graph G and we want to refer to its vertex set, we denote it
by V (G). Similarly we write E(G) for the edge set of G. A useful
notation is also
V
2

for the set of all 2-element subsets of V (see
Section 3.3.1 for a motivation of this symbol). We can brieﬂy say
that a graph is a pair (V, E), where E ⊆
V
2

.
The following terminology is fairly self-explanatory: if {u, v} is
an edge of some graph G, we say that the vertices u and v are
adjacent in G or that u is a neighbor of v (and v is a neighbor
of u).
Graphs are usually depicted by drawings in the plane. The ver-
tices of a graph are assigned points in the plane (drawn as dots,
bullets, little circles, etc.) and the edges are expressed by connect-
ing the corresponding pairs of points by straight or variously curved
lines (these lines are called arcs in this context). In this way, we get
pictures like this:

4.1 The notion of a graph; isomorphism
111
The word graph itself perhaps comes from the possibility of such
a drawing. (We should emphasize that the word graph is used here
with a diﬀerent meaning than in “graph of a function”—we sincerely
hope that the reader has noticed this by now.)
The role of the drawing of a graph is auxiliary, however. A graph
can also be represented in many other ways, and, for instance, in a
computer memory it is certainly not stored as a picture. One graph
can be drawn in many diﬀerent ways. For example, the ﬁrst two of
the above pictures show the same graph with vertex set {1, 2, 3, 4, 5}
and edges {1, 2}, {2, 3}, {3, 4}, {4, 5}, {5, 1}.
In a visually well-arranged drawing of a graph, the arcs should
“cross” as little as possible. Crossings could possibly be mistaken
for vertices, and also in some schemes of electronic circuits and in a
number of other situations crossings are inadmissible. This leads to
the study of an important class of the so-called planar graphs (see
Chapter 6).
Drawing graphs is an important aid in the theory of graphs. Draw
pictures for yourself whenever possible! Many notions are motivated
“pictorially” and drawings can make such notions much more intu-
itive.
The railway scheme in Fig. 4.1 also illustrates that the notion of
a graph is a nontrivial abstraction and simpliﬁcation of real life situa-
tions. The arcane and historically developed railway network shown in
the ﬁgure resembles a graph drawing, but, for example, there are few
places where tracks branch outside of railway stations. Also, there are
plenty of other types of information one might want to associate with
a railway network scheme (track quality and number for each connec-
tion, which tracks go straight through a station and which form a side
branch, station distances, train schedules, and so on). If one should make
a mathematical model of a railway network and didn’t know the notion
of a graph, one would probably come up with something much more
complex. In applications, graphs are indeed often augmented by further
information. But the notion of a graph is very useful as a “skeleton”
of such mathematical models, and once we use a graph as a signiﬁcant
part of the model, we immediately have a well-developed mathematical
theory at our disposal, suggesting further notions, properties, and eﬃ-
cient algorithms to consider.

112
Graphs: an introduction
Important graphs. We introduce several types of speciﬁc graphs,
which are quite often encountered in graph theory and for which
standard notation and terminology have become customary.
The complete graph Kn:
V = {1, 2, . . . , n}, E =
V
2

.
K3
K4
K5
K6
Mainly for aesthetic reasons, we also include a drawing of K23 with
all of its 253 edges:
The cycle Cn:
V = {1, 2, . . . , n}, E =
%
{i, i + 1}: i = 1, 2, . . . , n −1
&
∪
%
{1, n}
&
.

4.1 The notion of a graph; isomorphism
113
C3
C4
C5
C6
The path Pn:
V = {0, 1, . . . , n}, E =
%
{i −1, i}: i = 1, 2, . . . , n
&
.
P7
The complete bipartite graph Kn,m:
V = {u1, . . . , un} ∪{v1, . . . , vm},
E =
%
{ui, vj}: i = 1, 2, . . . , n, j = 1, 2, . . . , m
&
.
K1,1
K1,2
K1,3
K2,3
K3,3
A small explanation seems appropriate here. The word “bipartite”
means “consisting of two parts”. In general, a graph G is called
bipartite if the set V (G) can be divided into two disjoint sets V1 and
V2 in such a way that each edge of G connects a vertex from V1 to
a vertex from V2. Written in symbols, E(G) ⊆{{v, v′}: v ∈V1, v′ ∈
V2}. Such sets V1 and V2 are sometimes called the classes of G, but
not “partites”, however tempting this neologism may be.
Graph isomorphism. Two graphs G and G′ are considered iden-
tical (or equal) if they have the same set of vertices and the same
set of edges, i.e. G = G′ means V (G) = V (G′) and E(G) = E(G′).
But many graphs diﬀer “only” by the names of their vertices and
edges and have the same “structure”. This is captured by the notion
of isomorphism.
4.1.2 Definition. Two graphs G = (V, E) and G′ = (V ′, E′) are
called isomorphic if a bijection f : V →V ′ exists such that
{x, y} ∈E
if and only if
{f(x), f(y)} ∈E′
holds for all x, y ∈V , x ̸= y. Such an f is called an isomorphism
of the graphs G and G′. The fact that G and G′ are isomorphic is
written G ∼= G′.

114
Graphs: an introduction
An isomorphism is thus required to map adjacent vertices to adj-
acent vertices and nonadjacent vertices to nonadjacent vertices, and
it can be thought of as “renaming the vertices” of a graph. The
relation ∼= (“to be isomorphic”) is an equivalence, on any set of
graphs (see Exercise 7).
Problem. The following three pictures show isomorphic graphs.
Show this by ﬁnding suitable isomorphisms!
a
b
c
d
e
f
1
2
3
4
5
6
∗
⊕
⊗
△
×
⊙
Solution. All the three graphs are isomorphic to K3,3. An isomor-
phism of the ﬁrst graph to the second graph: for instance, 1 →a,
2 →d, 3 →b, 4 →e, 5 →c, 6 →f (several other possibilities exist!).
The others are left to the reader.
Warning. The deﬁnition of isomorphism looks easy but many students
tend to mess it up. They often think that an isomorphism of G to G′ is
any bijection between E and E′, etc. As a precaution, it is best to try
writing the deﬁnition down without looking in the textbook.
Testing isomorphism. For small pictures, it is usually not too diﬃ-
cult to ﬁnd out whether they correspond to isomorphic graphs or not
(although the preceding problem indicates that the pictures need not
look the same at all). But the problem of deciding whether two given
graphs are isomorphic or not is diﬃcult in general, and no eﬃcient alg-
orithm is known for it (i.e. one working fast in all cases). It is even
suspected that no such eﬃcient algorithm exists. Roughly speaking, the
diﬃculty lies in showing that two given graphs on n vertices are not iso-
morphic. To check this according to the deﬁnition, we must verify that
none of the possible n! bijections of the vertex sets is an isomorphism.
Of course, often we can use a shortcut and exclude the possibility of an
isomorphism right away. For instance, if the numbers of edges diﬀer, the
graphs cannot be isomorphic because isomorphism preserves the num-
ber of edges. More generally, if we can assign some number, vector, etc.,
to a graph in such a way that isomorphic graphs are always assigned
the same value, we can sometimes use this to distinguish nonisomorphic
graphs (examples will be discussed later). But so far no fast method has
been found that would always succeed in distinguishing nonisomorphic
graphs.

4.1 The notion of a graph; isomorphism
115
Number of nonisomorphic graphs. Let V be the set {1, 2, . . . , n}.
To choose a graph with vertex set V means choosing an arbitrary subset
E ⊆
V
2

. The set
V
2

has
n
2

elements, and thus the number of diﬀerent
graphs on V is exactly 2(
n
2). However, there are considerably fewer than
2(
n
2) pairwise nonisomorphic graphs with n vertices. For example, for
V = {1, 2, 3} we get the following 8 = 2(
3
2) distinct graphs:
1
2
3
1
2
3
1
2
3
1
2
3
1
2
3
1
2
3
1
2
3
1
2
3
Among these 8 possibilities, only 4 nonisomorphic ones can be found:
How many pairwise nonisomorphic graphs on n vertices exist for a
general n? In other words, how many classes of the equivalence rela-
tion ∼= on the set of all graphs with vertex set V = {1, 2, . . . , n} are
there? Determining this number exactly is not easy (see, e.g. Harary
and Palmer [21]), but we can at least get a reasonable estimate by
a simple (but clever!) trick. On the one hand, the number of noniso-
morphic graphs on n vertices is certainly not larger than the number of
all distinct graphs on the set V , i.e. 2(
n
2). On the other hand, consider
a particular graph G with vertex set V . How many distinct graphs G′
on V are isomorphic to it? For instance, if G is the graph
1
2
3
on the vertex set {1, 2, 3}, there are 3 such isomorphic graphs. By def-
inition, if G′ is such a graph isomorphic to G, there exists a bijection
f : V →V that is an isomorphism of G and G′. The number of all pos-
sible bijections f : V →V is n!, and hence G is isomorphic to at most n!
distinct graphs on the set V . (We may be overcounting! For our speciﬁc
example for n = 3, we had 3! = 6 bijections but only 3 distinct graphs
isomorphic to G—can you explain why?) In other words, each class of
the equivalence ∼= on the set of all graphs with vertex set V consists of
no more than n! graphs, and therefore the number of equivalence classes
is at least
2(
n
2)
n! .
Consequently, there is a collection of at least this many pairwise non-
isomorphic graphs on n vertices.

116
Graphs: an introduction
We claim that this function of n doesn’t grow much more slowly
than 2(
n
2). To see this, we take the logarithms of both functions and
manipulate the resulting expressions somewhat. We use the obvious
estimate n! ≤nn:
log2
"
2(
n
2)#
=
n
2

= n2
2

1 −1
n

,
log2
2(
n
2)
n!
=
n
2

−log2 n! ≥1
2n2 −1
2n −n log2 n
= n2
2

1 −1
n −2 log2 n
n

.
We see that for large n, the logarithms of both functions behave “roughly
as” the function 1
2n2: the relative error we would make by replacing their
logarithms by 1
2n2 goes to 0 for n →∞. (Section 3.4 says more about
estimating the growth of functions.) In particular, if n is suﬃciently
large, the number of nonisomorphic graphs on n vertices is much much
larger than 2n, say.
In the consideration just made, we have only shown that many non-
isomorphic graphs exist, but, remarkably, we have constructed no spe-
ciﬁc collection of such graphs. Similar methods will be discussed more
systematically in Chapter 10. Constructing many nonisomorphic graphs
explicitly is not so easy—see Exercise 8.
Exercises
1. (a) Find an isomorphism of the following graphs:
(b) ∗Show that both the graphs above are isomorphic to the following
graph: the vertex set is
{1,2,...,5}
2

(unordered pairs of numbers), and
two vertices {i, j} and {k, ℓ} (i, j, k, ℓ∈{1, 2, . . . , 5}) form an edge if
and only if {i, j} ∩{k, ℓ} = ∅.
Remark.
This graph is called the Petersen graph and it is one of
the most remarkable small graphs (being a counterexample to numer-
ous conjectures, an exceptional case in many theorems, etc.). It is the

4.1 The notion of a graph; isomorphism
117
smallest nontrivial member of a family of the so-called Kneser graphs,
which supplies many more examples of graphs with interesting proper-
ties. The vertex set of a Kneser graph is
{1,2,...,n}
k

for natural numbers
n > k ≥1, and edges correspond to empty intersections.
2. Which of the following statements about graphs G and H are true?
Substantiate your answers!
(i) G and H are isomorphic if and only if for every map f : V (G) →
V (H) and for any two vertices u, v ∈V (G), we have {u, v} ∈
E(G) ⇔{f(u, f(v)} ∈V (H).
(ii) G and H are isomorphic if and only if there exists a bijection
f : E(G) →E(H).
(iii) If there exists a bijection f : V (G) →V (H) such that every ver-
tex u ∈V (G) has the same degree as f(u), then G and H are
isomorphic.
(iv) If G and H are isomorphic, then there exists a bijection f : V (G) →
V (H) such that every vertex u ∈V (G) has the same degree as
f(u).
(v) If G and H are isomorphic, then there exists a bijection f : E(G) →
E(H).
(vi) G and H are isomorphic if and only if there exists a map f : -
V (G) →V (H) such that for any two vertices u, v ∈V (G), we
have {u, v} ∈E(G) ⇔{f(u), f(v)} ∈E(H).
(vii) Every graph on n vertices is isomorphic to some graph on the
vertex set {1, 2, . . . , n}.
(viii) Every graph on n ≥1 vertices is isomorphic to inﬁnitely many
graphs.
3. An automorphism of a graph G = (V, E) is any isomorphism of G
and G, i.e. any bijection f : V →V such that {u, v} ∈E if and only
if {f(u), f(v)} ∈E. A graph is called asymmetric if its only automor-
phism is the identity mapping (each vertex is mapped to itself).
(a) Find an example of an asymmetric graph with at least 2 vertices.
(b) Show that no asymmetric graph G exists with 1 < |V (G)| ≤5.
4. Show that a graph G with n vertices is asymmetric (see Exercise 3) if
and only if n! distinct graphs on the set V (G) are isomorphic to G.
5. Call a graph G = (V, E) vertex-transitive if for any two vertices v, v′ ∈
V an automorphism f : V →V of G exists (see Exercise 3) with f(v) =
v′. Similarly, G is edge-transitive if for any two edges e, e′ ∈E an
automorphism f : V →V exists with f(e) = e′ (if e = {u, v} then the
notation f(e) stands for the set {f(u), f(v)}).
(a) Prove that the graph in Exercise 1 is vertex-transitive.
(b) Decide whether each vertex-transitive graph is edge-transitive as
well.

118
Graphs: an introduction
(c) Find a graph that is edge-transitive but not vertex-transitive.
(d) ∗Show that any graph as in (c) is necessarily bipartite.
6. How many graphs on the vertex set {1, 2, . . . , 2n} are isomorphic to
the graph consisting of n vertex-disjoint edges (i.e. with edge set
{{1, 2}, {3, 4}, . . . , {2n −1, 2n}}?
7. ∗Let V be a ﬁnite set. Let G denote the set of all possible graphs with
vertex set V . Verify that ∼= (“to be isomorphic”) is an equivalence
relation on G.
8. ∗Construct as many pairwise nonisomorphic graphs with vertex set
{1, 2, . . . , n} as possible (suppose that n is a very large number). Can
you ﬁnd more than n2 of them? At least 2n/10, or even substantially
more?
9. (a) CS Plot the logarithms of the functions 2(
n
2) and 2(
n
2)/n! in a suit-
able range.
(b) ∗,CS Write a computer program for calculating the number of non-
isomorphic graphs on n vertices for a given n. (Warning: Unless you
devise a clever method, you will only be able to deal with very small
values of n!). For the values of n you can handle, draw the actual
numbers on the plot made in (a).
(c) ∗If you were able to solve (b) cleverly, the numbers should indicate
that the lower bound 2(
n
2)/n! for the number of nonisomorphic graphs
is much closer to the truth than the upper bound 2(
n
2). The upper
bound was gained by a quite trivial method anyway; can you improve
it?
4.2
Subgraphs, components, adjacency matrix
The next deﬁnition captures the intuitive notion of “one graph being
contained in another graph”. It turns out there are at least two ways
of making this precise.
4.2.1 Definition. Let G and G′ be graphs. We say that G is a
subgraph of G′ if V (G) ⊆V (G′) and E(G) ⊆E(G′).
We say that G is an induced subgraph of G′ if V (G) ⊆V (G′) and
E(G) = E(G′) ∩
V (G)
2

.
This deﬁnition can also be rephrased as follows: an induced sub-
graph of a graph G′ arises by deleting some vertices of G′ and all
edges containing a deleted vertex. To get a subgraph, we can also
delete some more edges although none of their end-vertices has been
deleted. Figure 4.2(a) shows a graph and its subgraph isomorphic
to the path P4 drawn by a thick line. This subgraph is not induced

4.2 Subgraphs, components, adjacency matrix
119
a
b
(a)
(b)
Fig. 4.2 An example of a subgraph (a), and of an induced subgraph (b).
(because of the edge {a, b}). Figure 4.2(b) shows an induced sub-
graph, isomorphic to the cycle C5; this is a subgraph as well, of
course.
Paths and cycles. A subgraph of a graph G isomorphic to some
path Pt is called a path in the graph G; see Fig. 4.2(a). A path in a
graph G can also be understood as a sequence
(v0, e1, v1, . . . , et, vt),
where v0, v1, . . . , vt are mutually distinct vertices of the graph G,
and for each i = 1, 2, . . . , t we have ei = {vi−1, vi} ∈E(G). This is
like the log of a traveler who followed the path from one end to the
other end and recorded the visited vertices and edges.3 We also say
that the path (v0, e1, v1, . . . , et, vt) is a path from v0 to vn of length t.
Let us remark that we also allow t = 0, i.e. a path of zero length
consisting of a single vertex.
Similarly, a subgraph of G isomorphic to some cycle Ct (t ≥3) is
called a cycle in the graph G; see Fig. 4.2(b). (An alternative name
used in the literature is a circuit.) A cycle in a graph G can also be
understood as a sequence
(v0, e1, v1, e2, . . . , et−1, vt−1, et, v0)
(the initial and ﬁnal points coincide), where v0, v1, . . . , vt−1 are pair-
wise distinct vertices of the graph G, and ei = {vi−1, vi} ∈E(G)
for i = 1, 2, . . . , t −1, and also et = {vt−1, v0} ∈E(G). The number
t ≥3 is the length of the cycle.
3Since we only consider simple graphs, recording the edges is not really nec-
essary, as they can be reconstructed from the sequence of vertices. The deﬁnition
we use is advantageous if multiple edges connecting two vertices are allowed.

120
Graphs: an introduction
Connectedness, components. We say that a graph G is connected
if for any two vertices x, y ∈V (G), G contains a path from x to y.
Diagram (a) shows an example of a connected graph,
(a)
(b)
while (b) is a drawing of a disconnected graph.
The notion of connectedness can also be deﬁned slightly diﬀer-
ently. First we deﬁne a notion similar to a path in a graph. Let
G = (V, E) be a graph. A sequence (v0, e1, v1, e2, . . . , et, vt) is
called a walk in G (more verbosely, a walk of length t from v0 to vt)
if we have ei = {vi−1, vi} ∈E for all i = 1, . . . , t. In a walk some
edges and vertices may be repeated, while for a path this was for-
bidden. A walk is the log of a leisurely traveler who doesn’t mind
visiting edges or vertices several times.
Next, we deﬁne a relation ∼on the set V (G) by letting x ∼y
if and only if there exists a walk from x to y in G. It is a fairly
easy exercise to check that ∼is an equivalence relation. Let V =
V1 ˙∪V2 ˙∪· · · ˙∪Vk be a partition of the vertex set V into classes of the
equivalence ∼. The subgraphs of G induced by the sets Vi are called
the components of the graph G. The following observation relates the
deﬁnition of components to the previous deﬁnition of a connected
graph.
4.2.2 Observation. Each component of any graph is connected,
and a graph is connected if and only if it has a single component.
Proof. Clearly, a connected graph has a single component. On the
other hand, any two vertices x, y in the same component of a graph
G can be connected by a walk. Any walk from x to y of the shortest
possible length must be a path.
2
Why did we choose the somewhat roundabout deﬁnition of compo-
nents using walks, rather than using paths? We could deﬁne the com-
ponents using the relation ∼′, where x ∼′ y if a path from x to y exists.
The above considerations show that ∼′ is in fact the same relation as
∼. But showing directly that ∼′ is an equivalence is a bit messy, and
the approach via walks seems cleaner.

4.2 Subgraphs, components, adjacency matrix
121
It is relatively easy to decide whether a given graph is connected,
or to ﬁnd the components. We aren’t going to describe such algorithms
here; they can be found in almost any textbook on algorithms. They
are usually presented as algorithms for searching a graph, or a maze.
One such algorithm is the so-called depth-ﬁrst search.
Distance in graphs.
Let G = (V, E) be a connected graph. We
deﬁne the distance of two vertices v, v′ ∈V (G), denoted by dG(v, v′),
as the length of a shortest path from v to v′ in G.
Hence dG is a function, dG : V ×V →R, and it is called the distance
function or the metric of the graph G. The metric of G has the following
properties:
1. dG(v, v′) ≥0, and dG(v, v′) = 0 if and only if v = v′;
2. (symmetry) dG(v, v′) = dG(v′, v) for any pair of vertices v, v′;
3. (triangle inequality) dG(v, v′′) ≤dG(v, v′)+dG(v′, v′′) for any three
vertices v, v′, v′′ ∈V (G).
The validity of these statements can be readily checked from the deﬁ-
nition of the distance function dG(v, v′). Each mapping d: V × V →R
with properties 1–3 is called a metric on the set V , and the set V to-
gether with such a mapping d is called a metric space. The distance
function dG of a graph has, moreover, the following special properties:
4. dG(v, v′) is a nonnegative integer for any two vertices v, v′;
5. if dG(v, v′′) > 1 then there exists a vertex v′, v ̸= v′ ̸= v′′, such
that dG(v, v′) + dG(v′, v′′) = dG(v, v′′).
Conditions 1–5 already characterize functions arising as distance
functions of graphs with vertex set V (see Exercise 7).
Graph representations. We have seen representations of graphs
by drawings, and also by writing out a list of vertices and edges.
Graphs can also be represented in many other ways. Some of them
become particularly important if we want to store and manipulate
graphs in a computer. A very basic and very common representation
is by an adjacency matrix:
4.2.3 Definition. Let G = (V, E) be a graph with n vertices. Denote
the vertices by v1, v2, . . . , vn (in some arbitrary order). The adjacency
matrix of G, with respect to the chosen vertex numbering, is an n×n
matrix AG = (aij)n
i,j=1 deﬁned by the following rule:
aij =
 1
if {vi, vj} ∈E
0
otherwise.
This is very similar to the adjacency matrix of a relation deﬁned
in Section 1.5. The adjacency matrix of a graph is always a symmetric

122
Graphs: an introduction
square matrix with entries 0 and 1, with 0s on the main diagonal.
Conversely, each matrix with these properties is the adjacency matrix
of some graph.
Example. The graph G =
1
2
3
4
5
6
has the adjacency matrix
AG =








0
1
1
1
0
0
1
0
1
0
1
0
1
1
0
1
1
1
1
0
1
0
0
1
0
1
1
0
0
1
0
0
1
1
1
0








.
Let us emphasize that the adjacency matrix also depends on the
chosen numbering of the vertices of a graph!
It might seem that we gain nothing new by viewing a graph as
a matrix since the graph and the adjacency matrix both encode the
same information. To illustrate that we can proﬁt from this alterna-
tive representation, let us show a simple connection between matrix
multiplication and the graph metric.
4.2.4 Proposition. Let G = (V, E) be a graph with vertex set V =
{v1, v2, . . . , vn} and let A = AG be its adjacency matrix. Let Ak denote
the kth power of the adjacency matrix (the matrices are multiplied
as is usual in linear algebra, i.e. if we put B = A2, we have bij =
n
k=1 aikakj). Let a(k)
ij
denote the element of the matrix Ak at position
(i, j). Then a(k)
ij
is the number of walks of length exactly k from the
vertex vi to the vertex vj in the graph G.
Proof.
This is easy but very instructive. We proceed by induction
on k. A walk of length 1 between two vertices means exactly that these
vertices are connected by an edge, and hence for k = 1 the proposition
just reformulates the deﬁnition of the adjacency matrix.
Next, let k > 1, and let vi, vj be two arbitrary vertices (possibly
identical). Any walk of length k from vi to vj consists of an edge from
vi to some neighbor vℓof vi and a walk of length k −1 from vℓto vj:

4.2 Subgraphs, components, adjacency matrix
123
vi
vℓ
vj
By the inductive hypothesis, the number of walks of length k −1 from
vℓto vj is a(k−1)
ℓj
. Hence the number of walks of length k from vi to vj
is

{vi,vℓ}∈E(G)
a(k−1)
ℓj
=
n

ℓ=1
aiℓa(k−1)
ℓj
.
But this is exactly the element at position (i, j) in the product of the
matrices A and Ak−1, i.e. a(k)
ij .
2
4.2.5 Corollary. The distance of any two vertices vi, vj satisﬁes
dG(vi, vj) = min{k ≥0: a(k)
ij ̸= 0}.
This result has surprising applications. For instance, if one wants
to ﬁnd the distance for all pairs of vertices in a given graph, one can
apply sophisticated algorithms for matrix multiplication (some of them
are described in Aho, Hopcroft, and Ullman [11]) and some other ideas
and get unexpectedly fast methods for computing the function dG. Ex-
ercises 10 and 11 indicate other algorithmic applications.
Let us remark that the adjacency matrix is not always the best
computer representation of a graph. Especially if a graph has few
edges (much fewer than
n
2

), it is usually better to store the list
of neighbors for every vertex. For a fast implementation of certain
algorithms, other, more complicated representations are used as well.
Exercises
1. Prove that the complement of a disconnected graph G is connected.
(The complement of a graph G = (V, E) is the graph (V,
V
2

\ E).)
2. What is the maximum possible number of edges of a graph with n
vertices and k components?
3. CS Design an algorithm that ﬁnds the decomposition of a given graph
G into its components. (Try to get an algorithm which needs at most
O(n + m) steps for a graph with n vertices and m edges.)
4. ∗Prove that a graph is bipartite if and only if it contains no cycle of
odd length.
5. ∗Describe all graphs containing no path (not necessarily induced!) of
length 3.
6. ∗Having solved the preceding exercise, describe all graphs containing
no path of length 4.

124
Graphs: an introduction
7. Show that if a function d: V × V →{0, 1, 2, . . .} satisﬁes conditions
1–5 above then a graph G = (V, E) exists such that dG(v, v′) = d(v, v′)
for any pair of elements of V .
8. Deﬁne the “diameter” and “radius” of a graph (in analogy with the
intuitive meaning of these notions).
9. (a) Find a connected graph of n vertices for which each of the powers
A1
G, A2
G, . . . of the adjacency matrix contains some zero elements.
(b) Let G be a graph on n vertices, A = AG its adjacency matrix, and
Inthe n×n identity matrix (with 1s on the diagonal and 0s elsewhere).
Prove that G is connected if and only if the matrix (In + A)n−1 has
no 0s.
(c) Where are the 0s in the matrix (In + A)n−1 if the graph G is not
connected?
10. Show that a graph G contains a triangle (i.e. a K3) if and only if there
exist indices i and j such that both the matrices AG and A2
G have the
entry (i, j) nonzero, where AG is the adjacency matrix of G.
Remark. In connection with algorithms for fast matrix multiplication,
this observation gives the fastest known method for deciding whether
a given graph contains a triangle, substantially faster than the obvious
O(n3) algorithm.
11. ∗Let G be a graph. Prove the following formula for the number of
cycles of length 4 in G (not necessarily induced):
1
8

trace

A4
G

−2|E(G)| −4

v∈V (G)
degG(v)
2

.
Here A4
G is the 4th power of the adjacency matrix, and trace

A4
G

denotes the sum of the elements on the main diagonal of A4
G. For the
deﬁnition of degG(v), see the next section. Note that this gives an
O(n3) algorithm for counting the number of cycles of length 4, or even
a faster algorithm using algorithms for fast matrix multiplication.
12. Prove that G and G′ are isomorphic if and only if a permutation matrix
P exists such that
AG′ = PAGP T .
Here AG is the adjacency matrix of G and P T denotes the transposed
matrix P. A matrix P is called a permutation matrix if its entries are
0 and 1 and each row and each column contain precisely one 1.

4.3 Graph score
125
4.3
Graph score
Let G be a graph, and let v be a vertex of G. The number of edges
of G containing the vertex v is denoted by the symbol degG(v). The
number degG(v) is called the degree of v in the graph G.
Let us denote the vertices of G by v1, v2, . . . , vn (in some arbi-
trarily chosen order). The sequence

degG(v1), degG(v2), . . . , degG(vn)

is called a degree sequence of the graph G, or a score of G. By choosing
diﬀerent numberings of the vertices of the same graph, we usually
obtain several diﬀerent sequences of numbers diﬀering by the order
of their terms. Thus, we will not distinguish two scores if one of
them can be obtained from the other by rearranging the order of the
numbers. We will usually write scores in nondecreasing order, with
the smallest degree coming ﬁrst.
It is easy to see that two isomorphic graphs have the same scores,
and thus two graphs with diﬀerent scores are necessarily noniso-
morphic. On the other hand, graphs with the same score need not
be isomorphic! For example, the graphs
both have score (2, 2, 2, 2, 2, 2), but they cannot be isomorphic since
one of them is connected while the other one is not. All the three
graphs in Fig. 4.3 have score (3, 3, 3, 3, 3, 3, 3, 3, 3, 3) but no two of
them are isomorphic (to prove this is a bit harder; see Exercise 1).
In spite of these negative examples, the score is an important and
easily computable characteristic of a graph, and it can often help to
distinguish nonisomorphic graphs in practice.
Fig. 4.3 Three connected nonisomorphic graphs with the same score.

126
Graphs: an introduction
A problem studied in graph theory, although not really one of
the most important ones, is to characterize sequences of numbers
that can appear as scores of graphs. One such basic and often quite
signiﬁcant property is a consequence of the following observation:
4.3.1 Proposition. For each graph G = (V, E) we have

v∈V
degG(v) = 2|E|.
Proof. The degree of a vertex v is the number of edges containing v.
Each edge contains 2 vertices, and hence by summing up all degrees
we get twice the number of edges.
2
4.3.2 Corollary (Handshake lemma).
The number of odd-
de-gree vertices is even, in any graph. (Or: the number of partici-
pants at a birthday party who shook hands with an odd number of
other participants is always even—for any ﬁnite party.)
Let us remark that the handshake lemma is not true for inﬁnite
parties. A one-sided inﬁnite path has a single odd-degree vertex:
. . .
The handshake lemma (Corollary 4.3.2), and some other simple
necessary conditions, are not suﬃcient to characterize sequences that
can show up as graph scores (see Exercise 2). A full characterization
of scores is not quite simple, and it is related to so-called network
ﬂows which are not treated in this book. Here we explain a simple
algorithm for deciding whether a given sequence of integers is a graph
score or not. The algorithm is an easy consequence of the following
result.
4.3.3 Theorem (Score theorem). Let D = (d1, d2, . . . , dn) be a
sequence of natural numbers, n > 1. Suppose that d1 ≤d2 ≤· · · ≤
dn, and let the symbol D′ denote the sequence (d′
1, . . . , d′
n−1), where
d′
i =
 di
for i < n −dn
di −1
for i ≥n −dn.
For example, for D = (1, 1, 2, 2, 2, 3, 3), we have D′ = (1, 1, 2, 1, 1, 2).
Then D is a graph score if and only if D′ is a graph score.

4.3 Graph score
127
Proof.
One implication is easy. Suppose that D′ is a score of a
graph G′ = (V ′, E′), where V ′ = {v1, v2, . . . , vn−1} and degG(vi) = d′
i
for all i = 1, 2, . . . , n −1. Fix a new vertex vn distinct from all of
v1, . . . , vn−1, and deﬁne a new graph G = (V, E) where
V = V ′ ∪{vn}
E = E′ ∪{{vi, vn}: i = n −dn, n −dn + 1, . . . , n −1}.
Expressed less formally, the new vertex vn is connected to the dn
last vertices of the graph G′. Clearly G has score D. (Remembering
this construction is the best way to remember the statement of the
theorem.)
It is more diﬃcult to prove the reverse implication, i.e. if D is a
score then D′ is a score. So assume that D is a score of some graph.
The trouble is that in general, we cannot reverse the construction
by which we passed from D′ to D, i.e. to tear oﬀa largest-degree
vertex, since such a vertex can be connected to other vertices than
we would need. An example is shown in the following picture:
v1
v2
v3 v4 v5
v6
v7
We thus consider the set G of all graphs on the vertex set
{v1, . . . , vn} in which the degree of each vertex vi equals di, i =
1, 2, . . . , n. We prove the following:
Claim. The set G contains a graph G0 in which the vertex vn is
adjacent exactly to the vertices vn−dn, vn−dn+1, . . . , vn−1, i.e. to the
last dn vertices.
Having a graph G0 as in the claim, it is already clear that the
graph G′ = ({v1, . . . , vn−1}, E′), where E′ = {e ∈E(G0): vn ̸∈e},
has score D′ (i.e. we can remove the vertex vn in G0), and this proves
the score theorem. It remains to establish the claim.
If dn = n −1, i.e. vn is connected to all other vertices, then
any graph from G satisﬁes the claim. So suppose dn < n −1 and
deﬁne, for a graph G ∈G, a number j(G) as the largest index j ∈
{1, 2, . . . , n −1} such that {vj, vn} ̸∈E(G). Let G0 ∈G be a graph
with the smallest possible value of j(G). We prove that j(G0) =
n −dn −1, and from this one can already see that G0 satisﬁes the
claim.

128
Graphs: an introduction
v1
v2
. . .
. . .
. . .
. . .
vk
vi
vj
vn
the last vertex not connected to vn
Fig. 4.4 Illustration for the proof of the score theorem.
For contradiction, let us suppose that j = j(G0) > n−dn−1. The
vertex vn has to be adjacent to dn vertices, and at most dn−1 of them
can have a larger index than vj. Hence there exists some index i < j
such that vi is adjacent to vn, and so we have {vj, vn} ̸∈E(G0),
{vi, vn} ∈E(G0) (refer to Fig. 4.4). Since degG0(vi) ≤degG0(vj),
there exists a vertex vk adjacent to vj but not to vi. In this situation,
we consider a new graph G′ = (V, E′), where
E′ =

E(G0) \
%
{vi, vn}, {vj, vk}
&
∪
%
{vj, vn}, {vi, vk}
&
.
It is easy to check that the graph G′ has score D too, and at the
same time j(G′) ≤j(G0) −1, which contradicts the choice of G0.
This proves the claim, and hence also Theorem 4.3.3 is proved.
2
As we have said, the theorem just proved gives an easy method
for deciding whether a given sequence is a graph score. We illustrate
the procedure with a concrete example.
Problem. Decide whether there exists a graph with score (1, 1, 1,
2, 2, 3, 4, 5, 5).
Solution.
We reduce the given sequence by a repeated use of the
score theorem 4.3.3:
(1, 1, 1, 2, 2, 3, 4, 5, 5)
(1, 1, 1, 1, 1, 2, 3, 4)
(1, 1, 1, 0, 0, 1, 2); rearranged nondecreasingly (0, 0, 1, 1, 1, 1, 2)
(0, 0, 1, 1, 0, 0); rearranged (0, 0, 0, 0, 1, 1)
(0, 0, 0, 0, 0).
Since the last sequence is a graph score (of the graph with 5
vertices and no edges), we get that the original sequence is a score
of some graph as well. Construct an example of such a graph!

4.3 Graph score
129
Exercises
1. Prove that the three graphs in Fig. 4.3 are pairwise nonisomorphic.
2. Construct an example of a sequence of length n in which each term is
some of the numbers 1, 2, . . . , n −1 and which has an even number of
odd terms, and yet the sequence is not a graph score.
3. Where was the assumption d1 ≤d2 ≤· · · ≤dn used in the proof of
the score theorem? Show that the statement is not true if we omit this
assumption.
4. Find a smallest possible example (with the smallest number of vertices)
of two connected nonisomorphic graphs with the same score.
5. Draw all nonisomorphic graphs with score (6, 3, 3, 3, 3, 3, 3). Prove that
none was left out!
6. Find an example, as small as possible, of a graph with 6 vertices of
degree 3, other vertices of degree ≤2, and with 12 edges.
7. Let G be a graph with 9 vertices, each of degree 5 or 6. Prove that it
has at least 5 vertices of degree 6 or at least 6 vertices of degree 5.
8. (a) Decide for which n ≥2 there exists a graph whose score consists
of n distinct numbers.
(b) ∗For which n does there exist a graph on n vertices whose score has
n −1 distinct numbers (i.e. exactly 2 vertices have the same degree)?
9. Let G be a graph in which all vertices have degree at least d. Prove
that G contains a path of length d (not necessarily an induced one).
10. ∗Let G be a graph with maximum degree 3. Prove that its vertices can
be colored by 2 colors (each vertex gets one color) in such a way that
there is no path of length two whose 3 vertices all have the same color.
11. ∗∗Let G be a graph with all vertices of degree at least 3. Show that G
contains a cycle which is not induced (i.e. it has a “diagonal”).
12. A graph G is called k-regular if all its vertices have degree exactly k.
Determine all pairs (k, n) such that there exists a k-regular graph on
n vertices.
13. Draw all nonisomorphic 3-regular graphs on 6 vertices.
14. Find a 3-regular asymmetric graph (see Exercise 4.1.3).
15. (a) Show that the following graph and the graph in Fig. 9.3 are
isomorphic:

130
Graphs: an introduction
(b) ∗Prove that any 3-regular graph with 14 vertices containing no
cycle of length 5 or smaller is isomorphic to the graph in (a) (this
graph is called the Heawood graph). Reading Chapter 9 ﬁrst might
help.
16. ∗Let G be a connected graph in which any two distinct vertices u, v
have either 0 or 5 common neighbors. Prove that G is k-regular for
some k.
17. ∗∗Prove that each graph with an even number of vertices has two
vertices with an even number of common neighbors.
4.4
Eulerian graphs
Here is one of the oldest problems concerning graph drawing.
Problem. Draw a given graph G = (V, E) with a single closed line,
without lifting the pencil from the paper (and drawing each edge only
once).
Mathematically, this can be formalized as follows: ﬁnd a closed
walk (v0, e1, v1, . . . , em−1, vm−1, em, v0) containing all the vertices and
all the edges, each edge exactly once (while vertices can be repeated).
(Note that the ﬁrst vertex and the last vertex coincide.) Such a walk
is called a closed Eulerian tour in G, and a graph possessing a closed
Eulerian tour is called Eulerian.
Here is an example of drawing a graph with a single line:
:

4.4 Eulerian graphs
131
and here is another one:
It turns out that Eulerian graphs can be characterized nicely by
a “local” condition, using vertex degrees.
4.4.1 Theorem (Characterization of Eulerian graphs). A
graph G = (V, E) is Eulerian if and only if it is connected and each
vertex has an even degree.
First proof. It is rather easy to show that the condition is necessary
for G to be Eulerian. Clearly, an Eulerian graph must be connected.
The reason for each degree being even is that whenever a closed
Eulerian tour enters a vertex it must also leave it. In more detail: if we
ﬁx some direction of traversing the closed Eulerian tour and consider
some vertex v ∈V (G), the edges incident to v can be classiﬁed as
either ingoing or outgoing, and the tour deﬁnes a bijection between
the set of the ingoing edges and the set of the outgoing edges.
Proving that a connected graph with all degrees even has a closed
Eulerian tour is a bit more demanding. For brevity, deﬁne a tour in
G as a walk in which no edge is repeated (vertices can be repeated,
though). Consider a tour T = (v0, e1, v1, . . . , em, vm) in G of the
largest possible length, m. We prove that
(i) v0 = vm, and
(ii) {ei : i = 1, 2, . . . , m} = E.
Ad (i). If v0 ̸= vm then the vertex v0 is incident to an odd number
of edges of the tour T. But since the degree degG(v0) is even, there
exists an edge e ∈E(G) not contained in T. Hence we could extend
T by this edge—a contradiction.

132
Graphs: an introduction
Ad (ii). So, assume that v0 = vm. Write V (T) for the set of vertices
occurring in T and E(T) for the set of edges in T. First assume
V (T) ̸= V . Thanks to the connectedness of G, an edge exists of the
form e = {vk, v′} ∈E(G), where vk ∈V (T) and v′ ̸∈V (T). In this
case, the tour
(v′, e, vk, ek+1, vk+1, . . . , vm−1, em, v0, e1, v1, . . . , ek, vk)
has length m + 1 and thus leads to a contradiction. Pictorially:
vk
v′
T
;
vk
v′
If V (T) = V and E(T) ̸= E, consider an edge e ∈E \ E(T), and
write e = {vk, vℓ}. Analogously to the previous case, a new tour
(vk, ek+1, vk+1, . . . , vm−1, em, v0, e1, v1, . . . , ek, vk, e, vℓ)
leads to a contradiction. Pictorially:
vk
vℓ
;
vk
vℓ
This proves Theorem 4.4.1.
2
Note that the main trick in the proof is to look at the longest
possible tour; the rest is more or less routine. This trick is worth
remembering, since similar twists occur in numerous graph theory
proofs (in this book see e.g. Exercise 4.3.11 or Lemma 5.1.3).
Second proof of Theorem 4.4.1. We prove only that every con-
nected graph with all degrees even has a closed Eulerian tour; the
reverse implication is easy and one proof suﬃces for it. The ﬁrst step
is the following lemma:
4.4.2 Lemma. If a graph G = (V, E) has all degrees even, then the
edge set E can be partitioned into disjoint subsets E1, E2, . . . , Ek so
that each Ei is the edge set of a cycle.
Proof of the lemma. We proceed by induction on |E|. For E = ∅
the lemma holds. In the induction step it suﬃces to show that G
contains at least one cycle. Indeed, if we denote the edge set of such

4.4 Eulerian graphs
133
a cycle by E1, then the graph (V, E \ E1) has again all degrees even,
and so its edge set can be partitioned into cycles by the inductive
hypothesis.
Thus, we look for a cycle in G assuming E ̸= ∅. We are going
to construct a path in G. We pick an arbitrary vertex v0 of nonzero
degree, we choose e1 as one of the edges having v0 as an endpoint, and
we take (v0, e1, v1) as the initial path. Having already constructed a
path (v0, e1, . . . , vi−1, ei, vi), we look whether vi is adjacent to any of
the vertices vj of the path constructed so far, 0 ≤j ≤i −2. If yes,
then the edge {vi, vj} together with the segment of the constructed
path between vj and vi constitutes the desired cycle. If vi is not
adjacent to any vj, 0 ≤j ≤i −2, then we can extend the path by
an edge ei+1 = {vi, vi+1}, because vi−1 is not the only neighbor of vi
(for otherwise, vi would have degree 1, which is not an even number).
In a ﬁnite graph a path cannot have inﬁnite length, and therefore,
the described procedure sooner or later ﬁnds a cycle.
2
Now we ﬁnish the proof of Theorem 4.4.1. Given a connected
graph G = (V, E) with all degrees even, we decompose E into disjoint
edge sets of cycles E1, E2, . . . , Ek as in the lemma just proved. We
are going to connect these cycles into a closed Eulerian tour, adding
one at a time. More precisely, we prove the following: Let G = (V, E)
be a connected graph, and let E be expressed as a disjoint union of
sets E1, E2, . . . , Eℓ, ℓ> 1, where each Ei can be traversed by a single
closed tour. Then there exists an index i ̸= 1 such that E1 ∪Ei can
also be traversed by a single closed tour.
By using this statement repeatedly, we can convert the initial k
cycles into a closed Eulerian tour. In order to prove the statement,
let us denote by Vi the set of all vertices that are contained in at
least one edge of Ei. It suﬃces to verify that there exists i ̸= 1 such
that V1 ∩Vi ̸= ∅(then the closed tours traversing E1 and Ei can be
glued together into a single closed tour as in the next picture).
v
E1
Ei
So let us assume the contrary: V1 ∩(V2 ∪V3 ∪· · · ∪Vk) = ∅. But
this means that there is no edge connecting V1 to V2 ∪V3 ∪· · · ∪Vk

134
Graphs: an introduction
(we recall that each edge of G belongs to some Ei), and so G is not
connected—a contradiction.
2
Remark about multiple edges. So far we have deﬁned an edge of
a graph as a 2-point set of vertices, and we will stick to this deﬁnition
in most of the rest of this book. This means, among other things,
that two vertices can be connected by at most one edge. In some
applications, it is natural to admit two vertices to be connected by
several distinct edges. We thus get graphs with multiple edges, also
called multigraphs. Mathematically, this notion can be formalized in
several diﬀerent ways, some of them being more handy than others.
For instance, we could assign a nonnegative integer m(u, v) to each
pair {u, v} of vertices. This m(u, v) would be the multiplicity of the edge
{u, v}. Hence m(u, v) = 0 would mean that the edge is not present in
the graph, m(u, v) = 1 would denote an “ordinary” (simple) edge, and
m(u, v) > 1 would say that the graph contains m(u, v) “copies” of the
edge {u, v}. A multigraph would then be an ordered pair (V, m), where
m:
V
2

→{0, 1, 2, . . .}.
Another common way of introducing multiple edges, a more ele-
gant one from a certain point of view, is to consider edges as “abs-
tract” objects, i.e. to take E as some ﬁnite set disjoint from the
vertex set V . For each edge e ∈E, we then determine the pair of
end-vertices of e. The same pair of vertices can occur for several
edges. Formally, a graph with multiple edges is an ordered triple
(V, E, ε), where V and E are disjoint sets and ε: E →
V
2

is a map-
ping determining the end-vertices of edges. Imagine that we have a
graph-building kit with a supply of vertices and edges; then the map-
ping ε tells us how to assemble a particular graph from the vertices
and edges:

4.4 Eulerian graphs
135
Sometimes it is useful also to admit loops in a graph, i.e. edges
beginning and ending in the same vertex. Formally, loops can again
be introduced in a number of ways. The simplest way is to represent
a loop attached to a vertex v as the 1-element set {v} in the edge
set E (while the ordinary edges are 2-element sets). If multiple edges
are introduced using the mapping ε, loops can be added by letting
ε be a mapping into the set
V
2

∪V , and a loop e is mapped by ε
to its single end-vertex.
Yet another modiﬁcation of the notion of a graph, the so-called
directed graphs, will be discussed in Section 4.5.
For simple applications, the formal way of introducing multiple
edges and loops doesn’t really matter a great deal, provided that we
choose a single way and keep to it consistently, and if this chosen
way is not too clumsy.
Exercises
1. The following sketch of a city plan depicts 7 bridges:
(a) Show that one cannot start walking from some place, cross each
of the bridges exactly once, and come back to the starting place (no
swimming please). Can one cross each bridge exactly once if it is not
required to return to the starting position?
This is a historical motivation for the notion of the Eulerian graphs.
The scheme (loosely) corresponds to a part of the city of K¨onigsberg,
Kr´alovec, Kr´olewiec, or Kaliningrad—that’s what it was variously
called during its colorful history—and the problem was solved by Euler
in 1736. Can you ﬁnd the city on a modern map?
(b) How many bridges need to be added (and where) so that a closed
tour exists?
Remark. Many people, not armed with the notion of a graph, might
try to solve the K¨onigsberg bridges practically, by actually walking

136
Graphs: an introduction
through the city. If you have ever tried to ﬁnd your way in a foreign
city, you will probably agree that the chance of ﬁnding the negative
solution in this way is negligible. From this point of view, one can
appreciate Euler’s genius and the simplicity of the graph model of the
situation.
2. Characterize graphs that have a tour, not necessarily a closed one,
covering all edges.
3. Draw the following graphs with a single line:
4. (a) Formulate an algorithm for ﬁnding a closed Eulerian tour in a given
graph, based on the ﬁrst proof of Theorem 4.4.1.
(b)CS How fast can you implement it (i.e. how many steps are needed
for a graph with n vertices and m edges in the worst case)?
(c)CS Solve the analogy of (a) and (b) for the second proof. Which of
the resulting bounds is better?
5. Check that Theorem 4.4.1 also holds for graphs with loops and multiple
edges (what is the correct way to deﬁne the degree of a vertex for such
graphs?).
6. Characterize the sequences of nonnegative integers that can appear
as scores of graphs possibly with loops and multiple edges. (A loop
increases the vertex degree by 2.)
7. A Hamiltonian cycle in a graph G is a cycle containing all vertices of G.
This notion may seem quite similar to an Eulerian tour but it turns out
that it is much more diﬃcult to handle. For instance, no eﬃcient algo-
rithm is known for deciding whether a graph has a Hamiltonian cycle
or not. This and the next two exercises are a microscopic introduction
to this notion (another nice result is mentioned in Exercise 5.3.3).
(a) Decide which of the graphs drawn in Fig. 6.1 has a Hamiltonian
cycle. Try to prove your claims!
(b) Construct two connected graphs with the same score, one with and
one without a Hamiltonian cycle.
8. For a graph G, let L(G) denote the so-called line graph of G, given by
L(G) = (E,
%
{e, e′}: e, e′ ∈E(G), e ∩e′ ̸= ∅
&
). Decide whether the

4.4 Eulerian graphs
137
following is true for every graph G:
(a) G is connected if and only if L(G) is connected.
(b) G is Eulerian if and only if L(G) has a Hamiltonian cycle (see
Exercise 7 for a deﬁnition).
9. (a) ∗Prove that every graph G with n vertices and with all vertices
having degree at least n
2 has a Hamiltonian cycle (see Exercise 7 for a
deﬁnition).
(b) ∗Is it suﬃcient to assume degrees at least ⌊n/2⌋in (a)?
10. We say that a graph G = (V, E) is randomly Eulerian from a vertex v0
if every maximal tour starting at v0 is already a closed Eulerian tour
in G. That is, if we start at v0 and draw edges one by one, choosing
a continuation arbitrarily among the yet unused edges, we can never
get stuck. (It would be nice if art galleries or zoos were randomly
Eulerian, but unfortunately they seldom are. The result in part (c)
below indicates why.)
(a) Prove that the following graphs are randomly Eulerian:
v0
v0
(b) Show that the graphs below are not randomly Eulerian:
v0
v0
(c) ∗Prove the following characterization of randomly Eulerian graphs.
A connected graph G = (V, E) all of whose vertices have even deg-
ree is randomly Eulerian from a vertex v0 if and only if the graph
(V \ {v0}, {e ∈E : v0 /∈e}) contains no cycle.

138
Graphs: an introduction
4.5
Eulerian directed graphs
All graphs considered so far were “undirected”—their edges were
unordered pairs of vertices. Many situations involve one-way streets
and schemes similar to the following ones:
To reﬂect such situations, one introduces directed graphs, where
every edge has a direction.
4.5.1 Definition. A directed graph G is a pair (V, E), where E is a
subset of the Cartesian product V ×V . The ordered pairs (x, y) ∈E
are called directed edges. We say that a directed edge e = (x, y) has
head y and tail x, or4 that e is an edge from x to y.
Further we could introduce directed graphs with multiple edges,
and also for each notion or problem for undirected graphs, we could
look at its directed analogy. Sometimes the results for directed graphs
are simple modiﬁcations of the results for undirected graphs. In some
other problems, the directed and undirected cases diﬀer substan-
tially, and as a rule, the directed version is then more diﬃcult to
handle. In this book, we deal almost exclusively with undirected
graphs. Let us make an exception here and introduce Eulerian di-
rected graphs and describe one of their cute applications.
An attentive reader might have noticed that a directed graph G =
(V, E) is the same object as a relation on the set V . Nevertheless, both
these notions are introduced, since directed graphs are investigated in
diﬀerent contexts than relations.
4Here is some alternative terminology. The artiﬁcial word digraph is often used
for a directed graph. A directed edge is sometimes called an arrow, a (directed)
arc, etc. An oriented graph is a special type of directed graph, where we do not
admit directed edges (x, y) and (y, x) simultaneously.

4.5 Eulerian directed graphs
139
It is quite natural to deﬁne a directed tour in a directed graph
G = (V, E) as a sequence
(v0, e1, v1, e2, . . . , em, vm)
such that ei = (vi−1, vi) ∈E for each i = 1, 2, . . . , m and, moreover,
ei ̸= ej whenever i ̸= j. Similarly we can deﬁne a directed walk,
directed path, and directed cycle.5
We say that a directed graph (V, E) is Eulerian if it has a closed
directed tour containing all vertices and passing each directed edge
exactly once. Eulerian directed graphs can again be characterized
nicely. Before stating the theorem, we should add a few more notions.
For a given vertex v in a directed graph G = (V, E), let us denote
the number of directed edges ending in v (i.e. having v as the head)
by deg+
G(v). Similarly, deg−
G(v) stands for the number of directed
edges originating in v. The number deg+
G(v) is called the indegree of
v, and deg−
G(v) is the outdegree of v.
Each directed graph G = (V, E) can be assigned an undirected
graph sym(G) = (V, ¯E), where
¯E =
%
{x, y}: (x, y) ∈E or (y, x) ∈E
&
.
The graph sym (G) is called the symmetrization of the graph G.
Now we can formulate the promised characterization of Eulerian
directed graphs.
4.5.2 Proposition. A directed graph is Eulerian if and only if its
symmetrization is connected6 and deg+
G(v) = deg−
G(v) holds for each
vertex v ∈V (G).
A proof of this proposition is very similar to the proof of Theo-
rem 4.4.1, and we leave it as an exercise.
An application. A wheel has a sequence of n digits 0 and 1 written
along its circumference. We can read k consecutive digits through a
slot:
5A directed cycle is sometimes simply called a cycle in the literature, or the
neologism dicycle is also occasionally used, which may sound more like a name
for some obscure vehicle.
6A directed graph whose symmetrization is connected is called weakly connec-
ted (a policeman who can ignore one-way street signs can get from any vertex to
any other one). On the other hand, in a strongly connected directed graph, any
two vertices can be connected by a directed path, in both directions.

140
Graphs: an introduction
The sequence of n digits should be such that the position of the wheel
can always be detected unambiguously from the k digits in the slot,
no matter how the wheel is rotated. (Imagine a device for controlling
the angular position of a radar or something else your fantasy can
envisage.) For a given k, we want to manufacture a wheel with n
as large as possible (so that the angular position can be controlled
fairly precisely). A mathematical formulation of the problem is the
following:
Problem. Find a cyclic sequence of digits 0 and 1, as long as pos-
sible, such that no two k-tuples of consecutive digits coincide (here
a cyclic sequence means positioning the digits on the circumference
of a circle).
Let ℓ(k) denote the maximum possible number of digits in such
a sequence for a given k. We prove the following surprising result:
Proposition. For each k ≥1 we have ℓ(k) = 2k.
Proof. Since the number of distinct k-digit sequences made of digits
0 and 1 is 2k, the length of the cyclic sequence cannot be longer than
2k. It remains to construct a cyclic sequence of length 2k with the
required property. The case k = 1 is easy, so let us assume k ≥2.
Deﬁne a graph G = (V, E) in the following manner:
• V is the set of all sequences of 0s and 1s of length k −1 (so
|V | = 2k−1).
• The directed edges are all pairs of (k −1)-digit sequences of the
form

(a1, . . . , ak−1), (a2, . . . , ak)

.
Directed edges are in a bijective correspondence with k-digit
sequences
(a1, a2, . . . , ak),
and hence |E| = 2k. For brevity, let us denote the directed edge

(a1, . . . , ak−1), (a2, . . . , ak)

by (a1, a2, . . . , ak). No confusion should
arise.

4.5 Eulerian directed graphs
141
0
1
10
01
00
11
00
11
10
01
000
111
110
100
001
011
010
101
k = 2
k = 3
Fig. 4.5 The directed graphs in the wheel problem.
The reader will probably agree that deg−
G(v) = deg+
G(v) = 2 for
each vertex v ∈V . The symmetrization of G is connected, because
by repeatedly omitting the last term of a (k −1)-digit sequence and
adding 0s to its beginning we can convert any sequence to the seq-
uence of 0s. Hence G is an Eulerian directed graph. Examples for
k = 2 and k = 3 are shown in Fig. 4.5.
Set |E| = 2k = K, and let (e1, . . . , eK) be the sequence of edges
in some directed Eulerian tour in G. Each edge ei has the form
ei = (ai
1, . . . , ai
k). The desired cyclic sequence of digits 0 and 1 of
length K for our wheel can be deﬁned as (a1
1, a2
1, . . . , aK
1 ). That is, we
take the ﬁrst element from each ei. Each subsequence of k consecutive
digits in this sequence corresponds to traversing one directed edge
of the Eulerian tour, and since no directed edge is repeated in the
tour, no two k-digit segments coincide. This proves ℓ(k) = 2k.
For example, for k = 2, from the graph in Fig 4.5 we can ﬁnd a
tour 00, 01, 11, 10 and the corresponding cyclic sequence 0011, and
for k = 3 we get a tour 000, 001, 011, 111, 110, 101, 010, 100 and
the corresponding cyclic sequence 00011101.
2
Let us remark that the noteworthy graphs constructed in the pre-
ceding proof are called the De Bruijn graphs. Although they are exp-
onentially large in k, the neighbors of a given vertex can be found
quickly. They are sometimes used as interconnecting networks in parallel
computing. Other graphs with similar properties are the k-dimensional
cubes: the vertex set is again all sequences of 0s and 1s of length k,
and two sequences are adjacent if and only if they diﬀer in exactly one
coordinate.

142
Graphs: an introduction
Exercises
1. Prove Proposition 4.5.2.
2. Design an algorithm for ﬁnding an Eulerian directed tour in a directed
graph.
3. When can a directed graph be drawn with a single line (not necessarily
a closed one)? Each directed edge must be drawn exactly once and in
the direction from its tail to its head.
4. Let G = (V, E) be a graph. An orientation of G is any oriented graph
G′ = (V, E′) arising by replacing each edge {u, v} ∈E either by the
directed edge (u, v) or by the directed edge (v, u).
(a) Prove that if all degrees of G are even then an orientation H of G
exists with deg+
H(v) = deg−
H(v) for all vertices v ∈V (G).
(b) Prove that a directed graph G satisfying deg+
G(v) = deg−
G(v) for all
vertices v is strongly connected if and only if it is weakly connected.
5. ∗Let G = (V, E) be a directed graph, and let w: E →R be a function
assigning a real number to each edge. A function p: V →R deﬁned
on vertices is called a potential for w if w(e) = p(v) −p(u) holds for
every directed edge e = (u, v). Prove that a potential for w exists if
and only if the sum of the values of w over the edges of any directed
cycle in G is 0.
6. ∗Prove that the following two conditions for a strongly connected dir-
ected graph G are equivalent:
(i) G contains a directed cycle of an even length.
(ii) The vertices of G can be colored by 2 colors (each vertex receives
one color) in such a way that for each vertex u there exists a directed
edge (u, v) with v having the color diﬀerent from the color of u.
7. ∗∗Knights from two enemy castles are sitting at a round table and
negotiating for peace. The number of knights with an enemy sitting
on their right-hand side is the same as the number of knights with an
ally on their right-hand side. Prove that the total number of knights
is divisible by 4.
8. ∗A tournament is a directed graph such that for any two distinct ver-
tices u, v, exactly one of the directed edges (u, v) and (v, u) is present
in the graph. Prove that each tournament has a directed path passing
through all vertices (such a path is called Hamiltonian).
9. ∗Prove that in any tournament (see Exercise 8 for a deﬁnition), there
exists a vertex v that can be reached from any other vertex by a
directed path of length at most 2.

4.6 2-connectivity
143
4.6
2-connectivity
A graph G is called k-vertex-connected if it has at least k + 1 vertices
and it remains connected after removing any k−1 vertices. A graph G is
called k-edge-connected if we obtain a connected graph by deleting any
k −1 edges of G. The maximum k such that G is k-vertex-connected is
called the vertex connectivity of G, and similarly for edge connectivity.
If a graph is a scheme of a city public transport network, a railway
network, telephone cables, etc., its higher connectivity gives hope for a
reasonable functioning of the network even in critical conditions, when
one or several nodes or connections of the network fail. The notions of
vertex connectivity and edge connectivity are theoretically and prac-
tically quite important in graph theory. They are related to so-called
network ﬂows, which are not treated in this book. Here we restrict our
attention to 2-vertex-connectivity, which will be needed in a chapter
on planar graphs, and which will also serve as an illustration for some
proof methods and constructions.
Instead of 2-vertex-connectivity we will brieﬂy say 2-connectivity.
To be on the safe side, let us give the deﬁnition once more:
4.6.1 Definition (2-connectivity). A graph G is called 2-connec-
ted if it has at least 3 vertices, and by deleting any single vertex we
obtain a connected graph.
It is easy to check that a 2-connected graph is also connected
(here we need the assumption that a 2-connected graph has at least
3 vertices—we recommend the reader to think this over). In this
section we give alternative descriptions of 2-connected graphs. Before
we begin with this, we introduce the notation for several graph-
theoretic operations. It simpliﬁes formulas considerably and will also
be useful later on.
4.6.2 Definition (Some graph operations). Let G = (V, E) be a
graph. We deﬁne various new graphs created from G:
• (Edge deletion)
G −e = (V, E \ {e}),
where e ∈E is an edge of G;
• (Adding a new edge)
G + ¯e = (V, E ∪{¯e}),
where ¯e ∈
V
2

\ E is a pair of vertices that is not an edge of G;

144
Graphs: an introduction
e
G
e′
v
G −e
G −v
G%e
G + e′
Fig. 4.6 Examples of graph operations.
• (Vertex deletion)
G −v =

V \ {v}, {e ∈E : v ̸∈e}

,
where v ∈V (we delete the vertex v and all edges having v as
an endpoint);
• (Edge subdivision)
G%e =

V ∪{z}, (E \
%
{x, y}
&
) ∪
%
{x, z}, {z, y}
&
,
where e = {x, y} ∈E is an edge, and z ̸∈V is a new vertex (we
“draw a new vertex z” on the edge {x, y}).
We say that a graph G′ is a subdivision of the graph G if G′ is
isomorphic to a graph created from G by successive operations of
edge subdivision.
Examples of the operations just deﬁned are shown in Fig. 4.6.
Let us go back to 2-connectivity. Here is the ﬁrst remarkable
equivalent characterization:
4.6.3 Theorem. A graph G is 2-connected if and only if there exi-
sts, for any two vertices of G, a cycle in G containing these two
vertices.
Let us remark that this theorem is a particular case of a very imp-
ortant result called Menger’s theorem, which says the following. If x

4.6 2-connectivity
145
and y are two vertices in a k-vertex-connected graph, then there exist
k paths from x to y that are mutually disjoint except for sharing the
vertices x and y.
Proof.
The given condition is, no doubt, suﬃcient, since if two
vertices v, v′ lie on a common cycle then there exist two paths con-
necting them having no common vertices except for the end-vertices,
and so v and v′ can never fall into distinct components by removing
a single vertex.
We now prove the reverse implication. The existence of a com-
mon cycle for v, v′ will be established by induction on dG(v, v′), the
distance of the vertices v and v′.
First let dG(v, v′) = 1. This means that {v, v′} = e ∈E(G).
By 2-connectivity of G, the graph G −e is connected (if it were
disconnected, at least one of the graphs G −v, G −v′ would also be
disconnected). Therefore there exists a path from v to v′ in the graph
G−e, and this path together with the edge e forms the required cycle
containing both v and v′.
Next, suppose that any pair of vertices at distance less than k lies
on a common cycle, for some k ≥2. Consider two vertices v, v′ ∈V
at distance k. Let P = (v = v0, e1, v1, . . . , ek, vk = v′) be a shortest
path from v to v′. Since dG(v, vk−1) = k−1, a cycle exists containing
both v and vk−1. This cycle consists of two paths, P1 and P2, from
v to vk−1. Now consider the graph G −vk−1. It is connected, and
hence it has a path ˇP from v to v′. This path thus doesn’t contain
vk−1. Let us look at the last vertex on the path ˇP (when going from
v to v′) belonging to one of the paths P1, P2, and denote this vertex
by w, as in the illustration:
v
vk−1
v′
w
P1
P2
ˇP
Without loss of generality, suppose that w is a vertex of P1. Then
the desired cycle containing v and v′ is formed by the path P2, by
the portion of the path P1 between v and w, and by the portion of
the path ˇP between w and v′ (drawn by a thick line).
2

146
Graphs: an introduction
4.6.4 Observation. A graph G is 2-connected if and only if any
subdivision of G is 2-connected.
Proof.
It is enough to show that, for any edge e ∈E(G), G is
2-connected if and only if G%e is 2-connected. If v ∈V (G) is a
vertex of G, it is easy to see that G −v is connected if and only if
(G%e)−v is connected. Therefore, if G%e is 2-connected then so is G.
For the reverse implication, it remains to show that for a 2-connected
G, the graph (G%e) −z is connected, where z is the newly added
vertex. This follows from the fact (observed in the previous proof)
that G −e is connected for a 2-connected G.
2
The next characterization of 2-connected graphs is particularly
suitable for proofs. We show how 2-connected graphs are built from
simpler graphs.
4.6.5 Theorem (2-connected graph synthesis). A graph G is
2-connected if and only if it can be created from a triangle (i.e. from
K3) by a sequence of edge subdivisions and edge additions.
Such a synthesis is illustrated below:
G
%
%
+
%
+
+
Proof. Every graph that can be produced from K3 by the above-
mentioned operations is obviously 2-connected. So, we need to prove
that we can construct each 2-connected graph.

4.6 2-connectivity
147
Actually, we show the possibility of creating any 2-connected
graph by a somewhat diﬀerent construction. We start with a cy-
cle G0, and if a graph Gi−1 has already been built, a graph Gi arises
by adding a path Pi connecting two vertices of the graph Gi−1. The
path Pi only shares its end-vertices with Gi−1, while all edges and
all inner vertices are new. As illustrated in the following drawing,
G0
P1
P2
P3
we successively glue “ears” to the graph G (and, indeed, the decom-
position is commonly called an ear decomposition).
Since adding a path can be simulated by an edge addition and
edge subdivisions,7 it suﬃces to show that every 2-connected graph
G can be produced by a repeated ear addition.
Let us pick a cycle G0 in the graph G arbitrarily. Suppose that
graphs Gj = (Ej, Vj) for j ≤i have already been deﬁned, with
properties as described above. If Gi = G the proof is over, so let us
assume that Ei ̸= E(G). Since G is connected, there exists an edge
e ∈E(G) \ Ei such that e ∩Vi ̸= ∅.
If both vertices of e lie in Vi then we put Gi+1 = Gi + e. In the
other case, let e = {v, v′}, where v ∈Vi, v′ ̸∈Vi:
v
v′
v′′
G0
G1
Consider the graph G−v. This is connected (since G is 2-connected),
and therefore a path P exists connecting the vertex v′ to some vertex
v′′ ∈Vi, where v′′ is the only vertex of the path P belonging to Vi
7One has to be careful here: if v, v′ ∈V (Gi−1) are already connected by an
edge and if we want to connect them by a new path, we cannot start by adding
the edge {v, v′} (at least if we do not allow multiple edges). We have to subdivide
the edge {v, v′} ﬁrst, then again add the edge {v, v′}, and then continue extending
the path by subdivisions if needed.

148
Graphs: an introduction
(to this end, take the shortest path joining v′ to Vi in the graph
G−v). Then we can deﬁne the graph Gi+1 by adding the edge e and
the path P to Gi, i.e. Vi+1 = Vi ∪V (P), Ei+1 = Ei ∪{e}∪E(P).
2
Exercises
1. Prove that for any two edges of a 2-connected graph, a cycle exists
containing both of them.
2. Let G be a critical 2-connected graph; this means that G is 2-connected
but no graph G −e for e ∈E(G) is 2-connected.
(a) Prove that at least one vertex of G has degree 2.
(b) For each n, ﬁnd an example of a critical 2-connected graph with a
vertex of degree at least n.
(c) ∗For each n, give an example of a critical 2-connected graph with a
vertex of degree ≥n, which is at distance at least n from each vertex
of degree 2.
3. (a) Is it true that any critical 2-connected graph (see Exercise 2) can be
obtained from a cycle by successive gluing of “ears” (paths) of length
at least 2?
(b) Is it true that any critical 2-connected graph can be obtained from
a cycle by a successive gluing of “ears” in such a way that each of the
intermediate graphs created along the way is also critical 2-connected?
4. Prove that any 2-connected graph has a strongly connected orientation
(see Section 4.5 for these notions).
5. ∗Determine the vertex connectivity of the k-dimensional cube. The
k-dimensional cube was deﬁned at the end of Section 4.5.
6. ∗Let d ≥3 be an integer, and let G be a d-regular graph (every vertex
has degree d) which is d-edge-connected. Prove that such a G is tough,
meaning that removing any k vertices disconnects G into at most k
connected components (for all k ≥1).
7. (Mader’s theorem) ∗∗Let G be a graph on n vertices such that |E(G)| ≥
(2k −3)(n −k + 1) + 1, where k is natural number with 2k −1 ≤n.
By induction on n, prove that G has a k-vertex-connected subgraph.
4.7
Triangle-free graphs: an extremal problem
Let us consider a graph G with n vertices. What can be said about
the number of its edges? This is an easy question and we already
know the answer: the number of edges can be any integer between
0 and
n
2

. The maximum (“extremal”) number of edges of a graph

4.7 Triangle-free graphs: an extremal problem
149
with n vertices thus equals
n
2

, and any graph with this number of
edges is isomorphic to the complete graph Kn.
In this section we are going to solve a more challenging, “real”
extremal problem. How many edges can a graph with n vertices have
if we know that it doesn’t contain a triangle (that is, a subgraph iso-
morphic to the graph K3)? Again we are interested in the maximum
possible number of edges of such a graph. Let us denote this number
by T(n).
Clearly T(1) = 0, T(2) = 1, and T(3) = 2. It is also easy to check
that T(4) = 4: the inequality T(4) ≥4 is witnessed by the cycle C4
of length 4, and the inequality T(4) < 5 can be veriﬁed, for example,
by showing that a graph with 4 vertices and 5 edges is determined
uniquely up to isomorphism and it contains a triangle.
What is T(5)? The cycle C5 of length 5 shows that T(5) ≥5.
This graph might seem to be the best possible, because adding any
edge to it creates a triangle. But appearances can be deceptive: we
actually have T(5) ≥6, as is witnessed by the following graph:
As we will see, this graph really is the best possible. In the next
theorem we show that T(5) = 6, and we even determine T(n) exactly
for all n.
4.7.1 Theorem. For every natural number n we have T(n) = ⌊n2
4 ⌋.
We recall that ⌊x⌋denotes the largest integer smaller or equal to
a real number x.
Proof. First we establish the (considerably easier) inequality T(n) ≥
⌊n2
4 ⌋. For this it suﬃces to ﬁnd suitable triangle-free graphs. We will
formulate the description slightly more generally than necessary; this
will pay oﬀlater. For disjoint sets X and Y , let KX,Y denote the
graph with vertex set X ∪Y and edge set {{x, y}: x ∈X, y ∈Y }.
The graph KX,Y is called a complete bipartite graph and if we set
a = |X| and b = |Y |, then KX,Y is isomorphic to Ka,b as introduced
in Section 4.1. A complete bipartite graph contains no triangle, and
the graph Ka,b has ab edges. Let us note that the graph in the above
picture witnessing T(5) ≥6 is isomorphic to K2,3.

150
Graphs: an introduction
For our proof it suﬃces to ﬁnd values a, b so that a + b = n and
a · b = ⌊n2
4 ⌋. We check that a = ⌊n
2 ⌋and b = n −a will do. We
distinguish two cases, n odd and n even. First, let n be odd; i.e.,
n = 2k + 1. In this case a = k, b = k + 1, and ab = k2 + k. On
the other hand, ⌊n2
4 ⌋= ⌊(2k+1)2
4
⌋= ⌊k2 + k + 1
4⌋= k2 + k = ab.
The second, easier case of even n is left to the reader. We have thus
shown T(n) ≥⌊n2
4 ⌋.
Now we prove the harder inequality T(n) ≤⌊n2
4 ⌋. Since T(n)
is integral, it suﬃces to show T(n) ≤
n2
4 . We will establish this
statement by a somewhat unusual induction on n. We already know
that the statement holds for n ≤4. In the inductive step we prove
the following implication:
T(n) ≤n2
4
⇒
T(n + 2) ≤(n + 2)2
4
.
So let G = (V, E) be any graph with n + 2 vertices containing no
triangle. We aim at showing |E| ≤(n+2)2
4
. Let us choose an edge
e0 = {x, y} ∈E arbitrarily, let us set V ′ = V \ {x, y}, and let
G′ = (V ′, E′) be the subgraph of G induced by the set V ′. The
graph G′ is triangle-free, and so by the inductive hypothesis we have
|E′| ≤n2
4 .
Let Ex be the set of edges of G that are incident with the vertex
x; formally Ex = {e ∈E : x ∈e ∈E, e ̸= e0}. We deﬁne the set
Ey analogously for y. We thus have E = E′ ∪(Ex ∪Ey) ∪{e0} and
|E| = |E′| + |Ex ∪Ey| + 1. A key step in the proof is the inequality
|Ex ∪Ey| ≤n, which follows from the fact that no edge of Ex has a
common vertex with any edge of Ey.
x
y
Ex
Ey
G
e0
Altogether we thus get |E| ≤n2
4 + n + 1 = (n+2)2
4
.
2
We can extract even more from the above proof. We introduce
the following notion: we say that a graph G = (V, E) with n vertices
is extremal if it contains no triangle and has ⌊n2
4 ⌋edges. We already
know that K⌊n/2⌋,n−⌊n/2⌋is extremal.

4.7 Triangle-free graphs: an extremal problem
151
4.7.2 Theorem. For every n each extremal graph is isomorphic to
the graph Ka,b with a = ⌊n
2 ⌋, b = n −⌊n
2 ⌋.
So the appropriate complete bipartite graph is the only extremal
graph. This is a remarkable fact, which was one of the main inspira-
tions of an entire theory—the extremal theory. Other examples will
be mentioned in Section 7.3. Sperner’s theorem discussed in Sec-
tion 7.2 can also be considered as a result of extremal theory.
Proof of Theorem 4.7.2.
We proceed as in the proof of Theo-
rem 4.7.1, and since we’re more or less copying that proof, we only
stress the extra arguments needed to show the uniqueness of the ext-
remal graph. The statement again holds for n = 1, 2, 3, since we can
easily consider all extremal graphs. In the inductive step we assume
the uniqueness of the extremal graph with n vertices, and our goal
is to show uniqueness for n + 2 vertices.
So let G = (V, E) be a triangle-free graph with n+2 vertices and
⌊(n+2)2
4
⌋edges. We choose an edge e0 = {x, y} arbitrarily and we
consider the graph G′ = (V ′, E′) with V ′ = V \ {x, y} and E′ = E ∩
V ′
2

. Since |E| = ⌊n2
4 ⌋+n+1 and since |E′| ≤⌊n2
4 ⌋and |Ex∪Ey| ≤n
(all according to the proof of Theorem 4.7.1), we obtain that |E′|
must be even equal to ⌊n2
4 ⌋. So G′ is extremal and thus isomorphic
to Ka,b, where a = ⌊n
2 ⌋and b = n −a. Hence G′ = KX,Y for a
suitable partition of V ′ into two sets X and Y , |X| = a, |Y | = b.
We note that x cannot be connected to both X and Y , and similarly
for y. But since we also have |Ex| + |Ey| = n, we see that one of
the following two possibilities has to occur: either x is connected
to all vertices of X and y is connected to all vertices of Y , or x
is connected to all of Y and y is connected to all of X. These two
possibilities may look diﬀerent but actually they yield isomorphic
graphs.
2
We conclude this section with a short presentation of another proof
of Theorem 4.7.1. Actually, we prove a somewhat diﬀerent theorem:
4.7.3 Theorem. Let G = (V, E) be a triangle-free graph. Then there
exists a partition of V into two subsets X and Y such that for every
vertex x ∈V we have dG(x) ≤dKX,Y (x).
First we show how Theorem 4.7.3 implies Theorem 4.7.1. It is easy.
From the inequality in Theorem 4.7.3 we get that the number of edges
of G is no larger than the number of edges of KX,Y (since the number
of edges of a graph is twice the sum of all degrees), and so it is enough

152
Graphs: an introduction
to verify that the maximum possible number of edges of a graph Ka,b
with a + b = n equals ⌊n2
4 ⌋– see Exercise 1.
Proof of Theorem 4.7.3. Let G = (V, E) be a triangle-free graph.
We pick a vertex x0 ∈V whose degree in G is maximum. We set Y =
{y: {x0, y} ∈E} and X = V \ Y . Clearly x0 ∈X, and for every x ∈X
we have dKX,Y (x) = |Y | = dG(x0) ≥dG(x), and hence the inequality
in the theorem holds for all x ∈X. Next, we note that no two vertices
of Y are adjacent, because all vertices of Y are neighbors of x0 and G
contains no triangles. So all neighbors of each y ∈Y lie in X and we
have dG(y) ≤|X| ≤dKX,Y (y).
2
Exercises
1. Determine natural numbers a and b with a + b = n for which the
expression a · b is maximized.
2. For natural numbers k and n, determine all values of natural numbers
a1, . . . , ak satisfying k
i=1 ai = n and such that the product a1 · a2 ·
. . . · ak is maximized.
3. A complete k-partitne graph K(V1, V2, . . . , Vk) on a vertex set V is
determined by a partition V1, . . . , Vk of the set V , and its edges are
the pairs {x, y} of vertices such that x and y lie in diﬀerent classes
of the partition. Formally K(V1, . . . , Vk) = (V, E), where {x, y} ∈E
exactly if x ̸= y and |{x, y} ∩Vi| ≤1 for all i = 1, . . . , k. Prove
(using Exercise 2) that the maximum number of edges of a complete
k-partite graph on a given vertex set corresponds to a partition with
almost equal parts, i.e. one with
|Vi| −|Vj|
 ≤1 for all i, j. What is
the number of edges of such a graph K(V1, . . . , Vk)?
4. ∗Prove that for every k ≥1 and for every graph G = (V, E) containing
no Kk+1 as a subgraph there exists a partition V1, . . . , Vk of V such
that the degree of every vertex x in G is no larger than the degree of
x in K(V1, . . . , Vk) (this is an analogy of Theorem 4.7.3). In this way,
one can obtain a generalization of Theorem 4.7.1,replacing the triangle
K3 by Kk for arbitrary k, which is the celebrated Tur´an theorem.

5
Trees
5.1
Deﬁnition and characterizations of trees
Even very abstract and elusive concepts in mathematics are often
given names from common language. Similarly as mathematical def-
initions are not at all arbitrary, the name for a notion may be
quite important. Sometimes the name helps to communicate, on
an intuitive level, some key property of the object which is not
easy to notice behind the formal deﬁnition. Other names may sound
quite illogical in today’s context without knowing their often convo-
luted history. For example, ﬁelds in mathematics are neither green
ﬁelds nor strawberry ﬁelds nor any other kind of ﬁelds most peo-
ple may be used to, and it is hard to imagine why something in
the plane should be called a vertex (see Chapter 6 for the origin
of this name). And, an unﬁtting name may lead one completely
astray. In this chapter, the reader can judge how well mathemati-
cians succeeded in choosing a name for a simple and fundamental
graph-theoretic concept—a tree. While trees in mathematics perhaps
cannot match those in nature for beauty or diversity, they still form
quite a rich area in graph theory, and closely related concepts of trees
appear in other branches of mathematics and of computer science
as well.
In graph theory, a tree is a graph similar to the ones in the
following drawings:

154
Trees
From these nature-inspired pictures, one may get quite a good
idea of what is meant by a tree in graph theory. But how do we deﬁne
this notion rigorously? Try it by yourself before reading further!
Perhaps the most usual deﬁnition is the following:
5.1.1 Definition. A tree is a connected graph containing no cycle.
Don’t worry if your deﬁnition doesn’t match ours. A tree can be
deﬁned in several rather diﬀerent ways. Soon we will give even four
more equivalent deﬁnitions.
Why should one deﬁne a tree in several diﬀerent ways? First of all,
the deﬁnition just given is somewhat unsuitable from several points of
view. For instance, it is not clear from it how we can check whether
a given graph is a tree or not. The connectedness can be veriﬁed by a
simple algorithm, but deciding the existence of a cycle seems to be a
problem. The alternative descriptions given below give very straight-
forward algorithmic ways of recognizing trees, and they tell us several
interesting properties of trees which may be useful in applications.
Second, while a proof of the equivalence of the various deﬁnitions
is probably not very exciting reading, it provides good exercise mate-
rial for students’ own proofs (according to our experience). The various
implications in the proof are not formidably diﬃcult, but they are not
completely easy either, and a student trying to prove one such implica-
tion has plenty of opportunities to make and discover errors or gaps in
the proof. The proof given below also illustrates how one can proceed
in proving the equivalence of several statements.
Third, the equivalent characterizations of trees below are simple
but they show samples of “good” characterizations of a mathematical
object one should look for; diﬃcult major theorems in several areas have
a formally similar pattern.
Here are the promised equivalent deﬁnitions of a tree. The most
remarkable of them is perhaps the one saying that among connected
graphs, a tree can be recognized simply by counting its edges and
vertices.
5.1.2 Theorem (Tree characterizations). The following condi-
tions are all equivalent for a graph G = (V, E):
(i) G is a tree.
(ii) (Path uniqueness)
For every two vertices x, y ∈V , there exists exactly one path
from x to y.
(iii) (Minimal connected graph)
The graph G is connected, and deleting any of its edges gives
rise to a disconnected graph.

5.1 Deﬁnition and characterizations of trees
155
(iv) (Maximal graph without cycles)
The graph G contains no cycle, and any graph arising from G by
adding an edge (i.e. a graph of the form G+e, where e ∈
V
2

\E)
already contains a cycle.
(v) (Euler’s formula)
G is connected and |V | = |E| + 1.
Note that this theorem not only describes various properties of
trees, such as “any tree on n vertices has n −1 edges”, but also lists
properties equivalent to Deﬁnition 5.1.1, so for instance it says “A
graph on n vertices is a tree if and only if it is connected and has
n −1 edges”.
Proving equivalences of various pairs of statements in Theorem 5.1.2
seems far from trivial for beginners, and all sorts of shortcomings ap-
pear in attempts at such proofs. Here is a (hypothetical) example. Con-
sider the following implication: “If G is a tree, then any two vertices
of G can be connected by exactly one path.” Most people quickly no-
tice that since G is connected, any two vertices can be connected by
at least one path. Then, if the proof is being created from the inter-
action of a student with the teacher, dialog of the following sort often
develops:
S.: “We proceed by contradiction. If u and v are two vertices in G that
can be connected by two diﬀerent paths, then G contains a cycle.
Hence the implication holds.”
T.: “But why must there be a cycle in G if u and v are connected by
two distinct paths?”
S.: “Well, the two paths together contain a cycle.”
For the teacher, this is not at all easy to argue with, in particular
because it’s true and, moreover, “obvious from a picture”.
T: “But why? Can you prove it rigorously? In your picture, there is
indeed a cycle, and I can’t show you a graph with no cycle and with
two paths between u and v either, but isn’t it possible that some
extraterrestrians, much more clever than me and you and all other
people together, can construct such a graph?”
(Suggestions of didactically more convincing ways of arguing are wel-
come.) In fact, a rigorous proof is not entirely trivial, but it seems that
the message most diﬃcult to get through is that there really is some-
thing to prove. As you will see, in our proof of Theorem 5.1.2 below we
preferred to avoid this direct argument.
We now begin with the proof of Theorem 5.1.2. Since there are
many implications to prove, it is important to organize the proof
suitably. The basic idea in all steps is to proceed by induction on
the number of vertices of the considered graph, and to “tear oﬀ”

156
Trees
a vertex of degree 1 in the inductive step. In a few simple lemmas
below, we prepare the ground for this method.
Let us call a vertex of degree 1 in a graph G an end-vertex of G or
a leaf of G. We begin with the following almost obvious observation:
5.1.3 Lemma (End-vertex lemma). Each tree with at least 2 ver-
tices contains at least 2 end-vertices.
Proof.
Let P = (v0, e1, v1, . . . , et, vt) be a path of the maximum
possible length in a tree T = (V, E). Clearly the length of the path
P is at least 1, and so in particular v0 ̸= vt. We claim that both v0
and vt are end-vertices. This can be shown by contradiction: if, for
example, v0 is not an end-vertex, then there exists an edge e = {v0, v}
containing the vertex v0 and diﬀerent from the ﬁrst edge e1 = {v0, v1}
of the path P. Then either v is one of the vertices of the path P, i.e.
v = vi, i ≥2 (in this case the edge e together with the portion of
the path P from v0 to vi form a cycle), or v ̸∈{v0, . . . , vt}—in that
case we could extend P by adding the edge e. In both cases we thus
get a contradiction.
2
Let us remark that the end-vertex lemma does not hold for inﬁnite
trees (the proof just given fails because a path of the maximum length
need not exist). For instance, the “one-sided inﬁnite path” has only one
end-vertex
. . .
and the “two-sided inﬁnite path” has none:
. . .
. . .
We only consider ﬁnite graphs here, however.
Next, we recall a notation from Section 4.6: if G = (V, E) is a
graph and v is a vertex of G, then G−v stands for the graph arising
from G by deleting the vertex v and all edges containing it. In case
v is an end-vertex of a tree T, the graph T −v arises by deleting the
vertex v and the single edge containing v.
5.1.4 Lemma (Tree-growing lemma). The following two state-
ments are equivalent for a graph G and its end-vertex v:
(i) G is a tree
(ii) G −v is a tree.

5.1 Deﬁnition and characterizations of trees
157
Proof. This is also quite easy. First we prove the implication (i) ⇒(ii).
The graph G is a tree, and we want to prove that G −v is a tree as
well. Consider two vertices x, y of G−v. Since G is connected, x and
y are connected by a path in G. This path cannot contain a vertex
of degree 1 diﬀerent from both x and y, and so it doesn’t contain v.
Therefore it is completely contained in G −v, and we conclude that
G −v is connected. Since G has no cycle, obviously G −v cannot
contain a cycle, and thus it is a tree.
It remains to prove the implication (ii) ⇒(i). Let G−v be a tree.
By adding the end-vertex v back to it, no cycle can be created. We
must also check the connectedness of G, but this is obvious too: any
two vertices distinct from v were connected already in G −v, and
a path to v from any other vertex x is obtained by considering the
(single) neighbor v′ of v in G, connecting it to x by a path in G −v,
and extending this path by the edge {v′, v}.
2
This lemma allows us to reduce a given tree to smaller and smaller
trees by removing end-vertices successively. Now we are going to
apply this device.
Proof of Theorem 5.1.2. We prove that each of the statements
(ii)–(v) is equivalent to (i). This, of course, proves the mutual equiva-
lence of all the statements. The proofs go by induction on the number
of vertices of G using the tree-growing lemma 5.1.4. As for the in-
duction basis, we note that all the statements are valid for the graph
with a single vertex.
First let us see that (i) implies all of (ii)–(v). To this end, let G
be a tree with at least 2 vertices, let v be one of its end-vertices, and
let v′ be the single neighbor of v in G. Suppose that the graph G−v
already satisﬁes (ii)–(v); this is our inductive hypothesis.
In this situation, the validity of (ii), (iii), and (v) for G can be
considered obvious (we leave a detailed argument to the reader).
As for (iv), we do not even need the inductive hypothesis for G−v.
Since G is connected, any two vertices x, y ∈V (G) can be connected
by a path, and if {x, y} ̸∈E(G) then the edge {x, y} together with
the just-mentioned path creates a cycle. This proves the implication
(i) ⇒(iv).
Let us now prove that each of the conditions (ii)–(v) implies (i).
In (ii) and (iii) we already assume connectedness. Moreover, a graph
satisfying (ii) or (iii) cannot contain a cycle. For (ii), this is because
two vertices in a cycle can be connected by two distinct paths, and

158
Trees
for (iii), the reason is that by omitting an edge in a cycle we obtain
a connected graph. Thus we have already proved the equivalence of
(i)–(iii).
In order to verify the implication (iv)⇒(i), it suﬃces to check
that G is connected. For this, we can use the argument by which
we have proved (i)⇒(iv) turned upside down. If x, y ∈V (G) are two
vertices, either they are connected by an edge, or the graph G+{x, y}
contains a cycle, and removing the edge {x, y} from this cycle gives
a path from x to y in G.
Finally the implication (v)⇒(i) is again proved by induction on
the number of vertices. Let us consider a connected graph G satisfy-
ing |V | = |E| + 1 ≥2. The sum of the degrees of all vertices is thus
2|V | −2 (why?). This means that not all vertices can have degree 2
or larger, and since all the degrees are at least 1 (by connectedness!),
there exists a vertex v of degree exactly 1, i.e. an end-vertex of the
graph G. The graph G′ = G −v is again connected and it satisﬁes
|V (G′)| = |E(G′)|+1. Hence it is a tree by the inductive hypothesis,
and thus G is a tree as well.
2
Exercises
1. Draw all trees with vertex set {1, 2, 3, 4}, and all pairwise noniso-
morphic trees on 6 vertices.
2. Prove that any graph G = (V, E) having no cycles and satisfying
|V | = |E| + 1 is a tree.
3. ∗Let n ≥3. Prove that a graph G on n vertices is a tree if and only if
it is not isomorphic to Kn and adding any edge (on the same vertex
set) not present in G creates exactly one cycle.
4. Prove that a graph on n vertices with c components has at least n −c
edges.
5. Suppose that a tree contains a vertex of degree k. Show that it has at
least k end-vertices.
6. Let T be a tree with n vertices, n ≥2. For a positive integer i, let pi
be the number of vertices of T of degree i. Prove
p1 −p3 −2p4 −· · · −(n −3)pn−1 = 2.
(This provides an alternative proof of the end-vertex lemma.)
7. King Uxamhwiashurh had 4 sons, 10 of his male descendants had 3
sons each, 15 had 2 sons, and all others died childless. How many male
descendants did King Uxamhwiashurh have?

5.2 Isomorphism of trees
159
8. Consider the following two conditions for a sequence (d1, d2, . . . , dn) of
(strictly) positive integers (where n ≥1):
(i) There exists a tree with score (d1, d2, . . . , dn).
(ii) n
i=1 di = 2n −2.
The goal is to show that these conditions are equivalent (and so there
is a very simple way to tell sequences that are scores of trees from
those that aren’t).
(a) Why does (i) imply (ii)?
(b) Why is the following “proof” of the implication (ii)⇒(i) insuﬃcient
(or, rather, makes no sense)? We proceed by induction on n. The base
case n = 1 is easy to check, so let us assume that the implication holds
for some n ≥1. We want to prove it for n + 1. If D = (d1, d2, . . . , dn) is
a sequence of positive integers with n
i=1 di = 2n −2, then we already
know that there exists a tree T on n vertices with D as a score. Add
another vertex v to T and connect it to any vertex of T by an edge,
obtaining a tree T ′ on n+1 vertices. Let D′ be the score of T ′. We know
that the number of vertices increased by 1, and the sum of degrees of
vertices increased by 2 (the new vertex has degree 1 and the degree of
one old vertex increased by 1). Hence the sequence D′ satisﬁes condition
(ii) and it is a score of a tree, namely of T ′. This ﬁnishes the inductive
step.
(c) ∗Prove (ii)⇒(i).
9. Suppose we want to prove that any connected graph G = (V, E) with
|V | = |E| + 1 is a tree, i.e. the implication (v)⇒(i) in Theorem 5.1.2.
What is wrong with the following proof?
We already assume that the considered graph is connected, so all we need
to prove is that it has no cycle. We proceed by induction on the number
of vertices. For |V | = 1, we have a single vertex and no edge, and the
statement holds. So assume the implication holds for any graph G =
(V, E) on n vertices. We want to prove it also for a graph G′ = (V ′, E′)
arising from G by adding a new vertex. In order that the assumption
|V ′| = |E′|+1 holds for G′, we must also add one new edge, and because
we assume G′ is connected, this new edge must connect the new vertex
to some vertex in V . Hence the new vertex has degree 1 and so it cannot
be contained in a cycle. And because G has no cycle (by the inductive
hypothesis), we get that neither does G′ have a cycle, which ﬁnishes the
induction step.
5.2
Isomorphism of trees
As we have mentioned in Section 4.1, no fast algorithm is known for de-
ciding whether two given graphs are isomorphic or not. For some special
classes of graphs eﬃcient algorithms exist, however. One of these classes

160
Trees
is trees. In fact, many (perhaps most) algorithmic problems which are
intractable for general graphs can be solved relatively easily for trees.
In this section we demonstrate a fast and simple algorithm for test-
ing the isomorphism of two given trees T and T ′. For a given tree
T, the algorithm computes a sequence of 0s and 1s of length 2n,
called the code of the tree T. Isomorphic trees yield identical se-
quences, while nonisomorphic ones receive distinct sequences. In this
way, the testing for isomorphism is reduced to a simple sequence
comparison.
Next, we introduce a number of specialized notions, each with a
particular name. This is quite usual in algorithmic graph theory, where
many notions arose from various applications and where the terminology
is quite diverse.
A rooted tree is a pair (T, r), where T is a tree and r ∈V (T) is a
distinguished vertex of T called the root. If {x, y} ∈E(T) is an edge
and the vertex x lies on the unique path from y to the root, we say
that x is the father of y (in the considered rooted tree) and y is a
son of x.
A planted tree is a rooted tree (T, r) plus a drawing of T in the
plane. In this drawing, the root is marked by an arrow pointing
downwards, and the sons of each vertex lie above that vertex.
For those who don’t like this deﬁnition, note that a planar drawing
of a tree is fully described, up to a suitable continuous deformation of
the plane, by the left-to-right order of the sons of each vertex. A planted
tree is thus a rooted tree in which every vertex v is assigned a linear
ordering of its sons. Thus, we can formally write a planted tree as a
triple (T, r, ν), where ν is a collection of linear orderings, one linear
ordering for the set of sons of each vertex.
For each of the above-deﬁned types of trees, an isomorphism is
deﬁned in a slightly diﬀerent way. Let us recall that a mapping
f : V (T) →V (T ′) is an isomorphism of trees T and T ′ if f is a
bijection (i.e. it is one-to-one and onto) satisfying {x, y} ∈E(T)
if and only if {f(x), f(y)} ∈E(T ′). The existence of such an iso-
morphism is written T ∼= T ′. An isomorphism of rooted trees (T, r)
and (T ′, r′) is an isomorphism f of the trees T and T ′ for which we
have, moreover, f(r) = r′. This is denoted by (T, r) ∼=′ (T ′, r′). An
isomorphism of planted trees is an isomorphism of rooted trees that
preserves the left-to-right ordering of the sons of each vertex. The
fact that two planted trees are isomorphic in this sense is denoted
by (T, r, ν) ∼=′′ (T ′, r′, ν′).

5.2 Isomorphism of trees
161
The deﬁnitions of ∼=, ∼=′, and ∼=′′ are successively stronger and
stronger. This can be best understood in the following small
examples:
∼=
̸∼=′
but
but
∼=′
̸∼=′′
The deﬁnition of the isomorphism of planted trees is most re-
strictive, and thus the coding of these trees is easiest. The following
method assigns a certain code to each vertex of a planted tree. The
code of the whole tree is then deﬁned as the code of the root.
K1. Each end-vertex distinct from the root is assigned 01.
K2. Let v be a vertex with sons v1, v2, . . . , vt (written in the left-to-
right order). If Ai is the code of the son vi, then the vertex v
receives the code 0A1A2 . . . At1.
The process of a successive building of a code is illustrated below:
01 01
01
01
01
01 01
01
01
01
001011
01 01
01
01
01
001011
000101101011
01 01
01
01
01
001011
000101101011
0000101101011011
Clearly, isomorphic planted trees have been assigned the same codes,
because in the code construction, we only used properties of a planted
tree preserved by the isomorphism of planted trees.

162
Trees
Now we show how the original planted tree can be reconstructed
from the resulting code. In this way, we prove that nonisomorphic
planted trees are assigned distinct codes. We proceed by induction
on the length of the code.
The shortest possible code, 01, corresponds to the planted tree
with a single vertex. In an induction step, suppose that we are given
a code k of length 2(n + 1). This code has the form 0 A 1, where
A = A1A2 . . . At is a concatenation of codes of several planted trees.
The part A1 can be identiﬁed as the shortest initial segment of the
sequence A containing the same number of 0s and 1s. Similarly, A2
is the next shortest segment with the number of 0s and 1s balanced,
and so on. By the inductive hypothesis, each Ai corresponds to a
unique planted tree. The planted tree coded by the code k has a
single root r, and this root has as sons the roots r1, r2, . . . , rt of the
planted trees coded by A1, A2, . . . , At, respectively (in the left-to-
right order). Hence the code uniquely determines a planted tree.
Decoding by the arrow method. We present an intuitive (pictorial)
procedure for reconstructing a planted tree from its code.
In a given code sequence, we replace every 0 by the arrow “↑” and
every 1 by the arrow “↓”. Next, we take this sequence of arrows as
instructions for drawing a tree. When encountering an “↑”, we draw
an edge from the current point upwards (and to the right of the parts
already drawn from that point). For a “↓”, we follow an already drawn
edge downwards. The whole procedure is illustrated in the following
picture:
0000101101011011
↓
−→
−→
↑↑↑↑↓↑↓↓↑↓↑↓↓↑↓↓
(The procedure draws the root’s arrow as well, since a one-point tree
has the code 01.)
We have studied the isomorphism of planted trees in some detail,
since the algorithm for planted trees easily leads to an algorithm for
rooted trees. For a rooted tree (T, r), we build its code in a similar
way to the method for planted trees, but the rule K2 is replaced by
the following modiﬁcation:

5.2 Isomorphism of trees
163
K2′. Suppose that each son w of a vertex v has already been
assigned a code A(w). Let the sons of v be denoted by
w1, w2, . . . , wt, in such a way that A(w1) ≤A(w2) ≤· · · ≤
A(wt). The vertex v receives the code 0 A1 A2 . . . At 1, where
Ai = A(wi).
Here A ≤B means that the sequence A is less than or equal to
the sequence B in some ﬁxed linear ordering of all ﬁnite sequences
of 0s and 1s. For deﬁniteness, we can use the so-called lexicographic
ordering. Two (distinct) sequences A = (a1, a2, . . . , an) and B =
(b1, b2, . . . , bm) are compared as follows:
• If A is an initial segment of B then A < B. If B is an initial
segment of A then B < A. (For example, we have 0010 < 00100
and 0 < 0111.)
• Otherwise, let j be the smallest index with aj ̸= bj. Then if
aj < bj we let A < B, and if aj > bj we let A > B. (For
example, we have 011 < 1 and 10011 < 10110.)
We have to check that two rooted trees are isomorphic if and only
if they have the same codes. This can be done quite similarly as for
planted trees, and we leave it to the reader.
We now turn our attention to coding trees without a root.1 Our
task would be greatly simpliﬁed if we could identify a vertex which
would play the role of the root, and which would be preserved by
any isomorphism. For trees, such a distinguished vertex can indeed
be found (well, not always, but the exceptional cases can be char-
acterized and they can be handled slightly diﬀerently). The relevant
deﬁnitions can be useful also in other contexts, so we formulate them
for general graphs rather than just for trees.
For a vertex v of a graph G, let the symbol exG(v) denote the
maximum of the distances of v from other vertices of G. The number
exG(v) is called the excentricity of the vertex v in the graph G. We
can imagine the vertices with a large excentricity as lying on the
periphery of G.
Now let C(G) denote the set of all vertices of G with the minimum
excentricity. The set C(G) is called the center2 of G. The example
1In the literature, the term free tree is sometimes used if one speaks about a
tree and wants to emphasize that it is not considered as a rooted tree.
2The excentricity of the vertices of the center is called the radius of the
graph G.

164
Trees
of a cycle (and many other graphs) shows that sometimes the center
may coincide with the whole vertex set. However, for trees, we have
Proposition. For any tree T, C(T) has at most 2 vertices. If C(T)
consists of 2 vertices x and y then {x, y} is an edge.
Proof. We describe a procedure for ﬁnding the center of a tree. Let
T = (V, E) be a given tree. If T has at most 2 vertices, then its center
coincides with the vertex set and the proposition holds. Otherwise
let T ′ = (V ′, E′) be the tree arising from T by removing all leaves.
Explicitly,
V ′ = {x ∈V : degT (x) > 1},
E′ =
%
{x, y} ∈E : degT (x) > 1 and degT (y) > 1
&
.
We clearly have V (T ′) ̸= ∅, since not all vertices of T can be leaves.
Further, for any vertex v, the vertices most distant from v are nec-
essarily leaves, and hence for each v ∈V ′ we get
exT (v) = exT ′(v) + 1.
In particular, we have C(T ′) = C(T). If T ′ has at least 3 vertices we
repeat the construction just described, otherwise we have found the
center of T.
2
We can now specify the coding of a tree T.
• If the center of T is a single vertex, v, then we deﬁne the code
of T to be the code of the rooted tree (T, v).
• If the center of T consists of an edge e = {x1, x2}, we con-
sider the graph T −e. This graph has exactly 2 components T1
and T2; the notation is chosen in such a way that xi ∈V (Ti).
Let the letter A denote the code of the rooted tree (T1, x1)
and the letter B the code of the rooted tree (T2, x2). If A ≤B
in the lexicographic ordering, the tree T is coded by the code of
the rooted tree (T, x1), and for A ≥B it is coded by the code of
(T, x2).
This ﬁnishes the coding procedure for trees.
The decoding is done in exactly the same way as for planted trees
(we then obtain a “canonical” drawing of the considered tree). Since
an isomorphism maps a center to a center and since we have already
seen that the coding works properly for rooted trees, it is easy to see
that two trees have the same code if and only if they are isomorphic.

5.2 Isomorphism of trees
165
The algorithms for the isomorphism of various types of trees exp-
lained in this section can be implemented in such a way that the number
of elementary steps (computer instructions, say) is bounded by a linear
function of the total number of vertices of the input trees.
Several more classes of graphs are known for which the isomorphism
problem can be solved eﬃciently. Perhaps the most important examples
are the class of all planar graphs and the class of all graphs with maxi-
mum degree bounded by a small constant. Here the known algorithms
for isomorphism testing are fairly complicated.
Exercises
1. (a) Find an asymmetric tree, i.e. a tree with a single (identical) auto-
morphism (see Exercise 4.1.3), with at least 2 vertices.
(b) Find the smallest possible number of vertices a tree as in (a) can
have (i.e. prove that no smaller tree can be asymmetric).
2. Find two nonisomorphic trees with the same score.
3. A rooted tree is called binary if each nonleaf vertex has exactly 2 sons.
(a) Draw all nonisomorphic binary trees with 9 vertices.
(b) Characterize the codes of binary trees.
4. Prove in detail that isomorphic trees (not rooted) receive the same
code by the explained procedure, and nonisomorphic trees get distinct
codes.
5. ∗,CS Let A1, . . . , At be sequences of 0s and 1s (possibly of diﬀerent
lengths). Let n denote the sum of their lengths. Devise an algorithm
that sorts these sequences lexicographically in O(n) steps. One step
may only access one term of some Ai; it is not possible to manipulate
a whole sequence of 0s and 1s at once.
6. Prove that there exist at most 4n pairwise nonisomorphic trees on n
vertices.
7. Let T = (V, E) be a tree and v some vertex of T. Put
τ(v) = max(|V (T1)|, |V (T2)|, . . . , |V (Tk)|),
where T1, . . . , Tk are all the components of the graph T −v. The cen-
troid of the tree T is the set of all vertices v ∈V with the minimum
value of τ(v).
(a) ∗Prove that the centroid of any tree is either a single vertex or 2
vertices connected by an edge.
(b) Is the centroid always identical to the center?
(c) Prove that if v is a vertex in the centroid then τ(v) ≤2
3|V (T)|.

166
Trees
5.3
Spanning trees of a graph
A spanning tree is one of the basic graph constructions:
5.3.1 Definition. Let G = (V, E) be a graph. An arbitrary tree
of the form (V, E′), where E′ ⊆E, is called a spanning tree of the
graph G. So a spanning tree is a subgraph of G that is a tree and
contains all vertices of G.
Obviously, a spanning tree may only exist for a connected graph G.
It is not diﬃcult to show that every connected graph has a spanning
tree. We prove it by giving two (fast) algorithms for ﬁnding a span-
ning tree of a given connected graph. In the subsequent sections we
will need variants of these algorithms, so let us study them carefully.
5.3.2 Algorithm (Spanning tree). Let G = (V, E) be a graph
with n vertices and m edges. We order the edges of G arbitrarily into
a sequence (e1, e2, . . . em). The algorithm successively constructs sets
of edges E0, E1, . . . ⊆E.
We let E0 = ∅. If the set Ei−1 has already been found, the set Ei
is computed as follows:
Ei =
 Ei−1 ∪{ei}
if the graph (V, Ei−1 ∪{ei}) has no cycle
Ei−1
otherwise.
The algorithm stops either if Ei already has n −1 edges or if i = m,
i.e. all edges of the graph G have been considered. Let Et denote
the set for which the algorithm has stopped, and let T be the graph
(V, Et).
5.3.3 Proposition (Correctness of Algorithm 5.3.2). If Algo-
rithm 5.3.2 produces a graph T with n−1 edges then T is a spanning
tree of G. If T has k < n −1 edges then G is a disconnected graph
with n −k components.
Proof. According to the way the sets Ei are created, the graph G
contains no cycle. If k = |E(T)| = n −1 then T is a tree according
to Exercise 5.1.2, and hence it is a spanning tree of the graph G. If
k < n −1, then T is a disconnected graph whose every component
is a tree (such a graph is called a forest). It is easy to see that it has
n −k components.
We prove that the vertex sets of the components of the graph T
coincide with the vertex sets of the components of the graph G. For
contradiction, suppose this is not true, and let x and y be vertices

5.3 Spanning trees of a graph
167
lying in the same component of G but in distinct components of T.
Let C denote the component of T containing the vertex x, and con-
sider some path (x = x0, e1, x1, e2, . . . , eℓ, xℓ= y) from x to y in the
graph G, as in the following picture:
x
y
xi
e
C
Let i be the last index for which xi is contained in the component C.
Obviously i < ℓ, and hence xi+1 ̸∈C. The edge e = {xi, xi+1} thus
does not belong to the graph T, and so it had to form a cycle with
some edges already selected into T at some stage of the algorithm.
Therefore the graph T +e also contains a cycle, but this is impossible
as e connects two distinct components of T. This provides the desired
contradiction.
2
Complexity of the algorithm. We have just shown that Algorithm
5.3.2 always computes what it is supposed to compute, i.e. a spanning
tree of the input graph. But if we really needed to ﬁnd spanning trees
for some large graphs, should we choose this algorithm and spend our
time programming it, or our money by buying some existing code?
To answer such a question is no simple matter, and algorithms are
compared according to diﬀerent, and often contradictory, criteria. For
instance, it is important to consider the clarity and simplicity of the
algorithm (a complicated or obscure algorithm easily leads to program-
ming errors), the robustness (how do rounding errors or small changes
in the input data inﬂuence the correctness of the output?), memory
requirements, and so on. Perhaps the most common measure of com-
plexity of an algorithm is its time complexity, which means the number
of elementary operations (such as additions, multiplications, compar-
isons of two numbers, etc.) the algorithm needs for solving the input
problem. Most often the worst-case complexity is considered, i.e. the
number of operations needed to solve the worst possible problem, one
expressly chosen to make the algorithm slow, for a given size of input.
For computing a spanning tree, the input size can be measured as the
number of vertices plus the number of edges of the input graph. Instead
of “worst-case time complexity” we will speak brieﬂy of “complexity”,
since we do not discuss other types of complexity.
The complexity of an algorithm can seldom be determined precisely.
In order that we could even think of doing it, we would have to deter-
mine exactly what the allowed primitive operations are (so, in principle,
we would restrict ourselves to a speciﬁc computer), and also we would

168
Trees
have to describe the algorithm in the smallest details including various
routine steps; that is, essentially look at a concrete program. Even if
we did both these things, determining the precise complexity is quite
laborious even for very simple algorithms. For these reasons, the com-
plexity of algorithms is only analyzed asymptotically in most cases. We
could thus say that some algorithm has complexity O(n3/2), another
one O(n log n), and so on (here n is a parameter measuring the size of
the input).
For a real assessment of algorithms, it is usually necessary to com-
plement such a theoretical analysis by testing the algorithm for various
input data on a particular computer. For example, if the asymptotic
analysis yields complexity O(n2) for one algorithm and O(n log4 n) for
another then the second algorithm looks clearly better at ﬁrst sight
because the function n log4 n grows much more slowly than n2. But if
the exact complexity of the ﬁrst algorithm were, say, n2 −5n and of the
second one 20n(log2 n)4, the superiority of the second algorithm will
only show for n > 5 · 106, and such a superiority is quite illusory from
a practical point of view.
Let us try to estimate the asymptotic complexity of Algorithm 5.3.2.
We have described the algorithm on a “high level”, however. This
doesn’t refer to a prestigious social position but to the fact that we
have used, for instance, a test of whether a given set of edges contains
a cycle, which cannot be considered an elementary operation even with
a very liberal approach. The complexity of the algorithm will thus dep-
end on our ability to realize such a complex operation by elementary
operations.
For our Algorithm 5.3.2, we may note that it is not necessary to store
all the edge sets Ei, and that all of them can be represented by a single
variable (say, a list of edges) which successively takes values E0, E1, . . ..
The only signiﬁcant question is how to test eﬃciently whether adding
a new edge ei creates a cycle or not. Here is a crucial observation: a
cycle arises if and only if the vertices of the edge ei belong to the same
connected component of the graph (V, Ei−1). Hence we need to solve
the following problem:
5.3.4 Problem (UNION–FIND problem). Let V = {1, 2, . . . , n}
be a set of vertices. Initially, the set V is partitioned into 1-element
equivalence classes; that is, no distinct vertices are considered equiva-
lent. Design an algorithm which maintains an equivalence relation on
V (in other words, a partition of V into classes) in a suitable data
structure, in such a way that the following two types of operations can
be executed eﬃciently:
(i) (UNION) Make two given nonequivalent vertices i, j ∈V equiva-
lent, i.e. replace the two classes containing them by their union.

5.3 Spanning trees of a graph
169
(ii) (Equivalence testing—FIND) Given two vertices i, j ∈V , decide
whether they are currently equivalent.
A new request for an operation is input to the algorithm only after it
has executed the previous operation.
Our Algorithm 5.3.2 for ﬁnding a spanning tree needs at most n −1
operations UNION and at most m operations FIND.
We describe a quite simple solution of Problem 5.3.4. In the beg-
inning, we assign distinct marks to the vertices of V , say the marks
1, 2, . . . , n. During the computation, the marks will always be assigned
so that two vertices are equivalent if and only if they have the same
mark. Thus, equivalence testing (FIND) is a trivial comparison of marks.
For replacing two classes by their union, we have to change the marks
for the elements of one of the classes. So, if the elements of each class are
also stored in a list, the time needed for the mark-changing operation
is proportional to the size of the class whose marks are changed.
For a very rough estimate of the running time, we can say that no
class has more than n elements, so a single UNION operations never
needs more than O(n) time. For n −1 UNION operations and m FIND
operations we thus get the bound O(n2 + m). One inconspicuous imp-
rovement is to maintain also the size of each class and to change marks
always for the smaller class. For such an algorithm, one can show a much
better total bound: O(n log n+m) (Exercise 1). The best known solution
of Problem 5.3.4, due to Tarjan, needs time at most O(nα(n) + m) for
m FIND and n −1 UNION operations (see e.g. Aho, Hopcroft, and
Ullman [11]), where α(n) is a certain function of n. We do not give the
deﬁnition of α(n) here; we only remark that α(n) does grow to inﬁnity
with n →∞but extremely slowly, much more slowly than functions like
log log n, log log log n, etc. For practical purposes, the solution described
above (with re-marking the smaller class) may be fully satisfactory.
Let us present one more algorithm for spanning trees, perhaps
even a simpler one.
5.3.5 Algorithm (Growing a spanning tree). Let a given graph
G = (V, E) have n vertices and m edges. We will successively con-
struct sets V0, V1, V2, . . . ⊆V of vertices and sets E0, E1, E2, . . . ⊆E
of edges. We let E0 = ∅and V0 = {v}, where v is an arbitrary vertex.
Having already constructed Vi−1 and Ei−1, we ﬁnd an edge ei =
{xi, yi} ∈E(G) such that xi ∈Vi−1 and yi ∈V \ Vi−1, and we
set Vi = Vi−1 ∪{yi}, Ei = Ei−1 ∪{ei}. If no such edge exists, the
algorithm ﬁnishes and outputs the graph constructed so far, T =
(Vt, Et).
5.3.6 Proposition (Correctness of Algorithm 5.3.5). If the
algorithm ﬁnishes with a graph T with n vertices, then T is a spanning

170
Trees
tree of G. Otherwise G is a disconnected graph and T is a spanning
tree of the component of G containing the initial vertex v.
Proof. The graph T is a tree because it is connected and has the
right number of edges and vertices. If T has n vertices, it is a spanning
tree, so let us assume that T has ¯n < n vertices. It remains to show
that V (T) is the vertex set of a component of G.
Let us suppose the contrary: let there be an x ∈V (T) and a
y ̸∈V (T) connected by a path in the graph G. As in the proof of
Proposition 5.3.3, we ﬁnd an edge e = {xj, yj} ∈E(G) on this path
such that xj ∈V (T) and yj ∈V \ V (T). The algorithm could thus
have added the edge e and the vertex yj to the tree, and should
not have ﬁnished with the tree T. This contradiction concludes the
proof.
2
Remark. The details of the algorithm just considered can be designed
in such a way that the running time is O(n + m) (see Exercise 2).
Exercises
1. Prove that if Problem 5.3.4 is solved by the described method (always
changing the marks for the smaller class), then the total complexity of
n −1 UNION operations is at most O(n log n).
2. ∗,CS Design the details of Algorithm 5.3.5 is such a way that the run-
ning time is O(n + m) in the worst case. (This may require some
knowledge of simple list-like data structures.)
3. From Exercise 4.4.7, we recall that a Hamiltonian cycle in a graph G
is a cycle containing all vertices of G. For a graph G and a natural
number k ≥1, deﬁne the graph G(k) as the graph with vertex set V (G)
and two (distinct) vertices connected by an edge if and only if their
distance in G is at most k.
(a) ∗Prove that for each tree T, the graph T (3) has a Hamiltonian
cycle.
(b) Using (a), conclude that G(3) has a Hamiltonian cycle for any
connected graph G.
(c) Find a connected graph G such that G(2) has no Hamiltonian cycle.
5.4
The minimum spanning tree problem
Imagine a map of your favorite region of countryside with some 30–40
villages. Some pairs of villages are connected by gravel roads, in such
a way that each village can be reached from any other along these

5.4 The minimum spanning tree problem
171
roads. The county’s council decides to modernize some of these roads
to highways suitable for fast car driving, but it wants to invest as
little money as possible under the condition that it will be possible
to travel between any two villages along a highway. In this way, we
arrive at a fundamental problem called the minimum spanning tree
problem. This section is devoted to its solution.
Of course, you may object that your favorite part of the countryside
was full of ﬁrst-class expressways a long time ago. So you may consider
some less advanced country, or ﬁnd another natural formulation of the
underlying mathematical problem.
We also assume that the existing roads have no branchings outside
the villages, and that the new roads can only be built along the old
ones (because of proprietary rights, say). Otherwise, it may be cheaper
for instance to connect four places like this
instead of like this
If we allowed the former kind of connection, we would arrive at a diﬀer-
ent algorithmic problem (called the Steiner tree problem), which turns
out to be much less tractable than the minimum spanning tree problem.
A mathematical formulation of the minimum spanning tree prob-
lem requires that the notion of graph is enriched a bit: we will con-
sider graphs with weighted edges. This means that for every edge
e ∈E we are given a number w(e), called the weight of the edge e.
The weight of an edge is usually a nonnegative integer or real num-
ber. A graph G = (V, E) together with a weight function w on its
edges, w: E →R, is sometimes called a network.
Let us formulate the above road-building problem in graph-
theoretic terms:
Problem. Given a connected graph G = (V, E) with a nonnegative
weight function w on the edges, ﬁnd a spanning connected subgraph
(V, E′) such that the sum

172
Trees
w(E′) =

e∈E′
w(e)
(5.1)
has the minimum possible value.
It is easy to see that there is always at least one spanning tree
of G among the solutions of this problem. If the weights are strictly
positive, each solution must be a spanning tree. For instance, if all
edges have weight 1, then the solutions of the problem are exactly the
spanning trees of the graph, and the expression (5.1) has minimum
value |V | −1.
Hence, it suﬃces to deal with the following problem:
5.4.1 Problem (Minimum spanning tree problem). For a con-
nected graph G = (V, E) with a weight function w on the edges,
ﬁnd a spanning tree T = (V, E′) of the graph G with the smallest
possible value of w(E′).
An attentive reader might have observed that here we do not assume
nonnegativity of the weights. Indeed, the algorithms we are going to dis-
cuss solve even this more general problem with arbitrary weights. There
is more to this remark than meets the eye: many graph problems that
are easy for nonnegative weights turn into algorithmically intractable
problems if we admit weights with arbitrary signs. An example of such
a problem is ﬁnding the shortest path in a network, where the length
of a path is measured as the sum of the edge weights.
A given graph may have very many spanning trees (see Chapter 8)
and it may seem diﬃcult to ﬁnd the best one. It is not really so
diﬃcult, and nowadays it can be done by an easy modiﬁcation of the
algorithms from the previous section. We present several algorithms.
A simple and very popular one is the following:
5.4.2 Algorithm (Kruskal’s or the ‘‘greedy’’ algorithm). The
input is a connected graph G = (V, E) with edge weight function w.
Let us denote the edges by e1, e2, . . . , em in such a way that
w(e1) ≤w(e2) ≤· · · ≤w(en).
For this ordering of edges, execute Algorithm 5.3.2.
Before proving the correctness of Kruskal’s algorithm, which is
not completely easy, let us illustrate the algorithm with a small
example.

5.4 The minimum spanning tree problem
173
Example. Let us apply Kruskal’s algorithm for the following
network:
1
1
3
2
2
4
3
4
One possible execution of the algorithm is shown in the next diagram:
5.4.3 Proposition (Correctness of Kruskal’s algorithm). Alg-
orithm 5.4.2 solves the minimum spanning tree problem.
Proof. This proof is not really deep but it seems to require concen-
trated attention, for otherwise it is very easy to make a mistake in
it (both the authors have a rich experience of it by presenting it in
lectures).
Let T be the spanning tree found by the algorithm, and let ˇT
be any other spanning tree of the graph G = (V, E). We need to
show that w(E(T)) ≤w(E( ˇT)). Let us denote the edges of T by
e′
1, e′
2, . . . , e′
n−1 in such a way that w(e′
1) ≤w(e′
2) ≤· · · ≤w(e′
n−1)
(the edge e′
i has been denoted by some ej in the algorithm, so that
it now has two names!). Similarly let ˇe1, . . . , ˇen−1 be the edges of ˇT
ordered increasingly by weights.
We show that for i = 1, . . . , n −1, we even have
w(e′
i) ≤w(ˇei).
(5.2)
This of course shows that T is a minimum spanning tree. For contra-
diction, let us assume that (5.2) is not true, and let i be the smallest
index for which it is violated, i.e. w(e′
i) > w(ˇei). We consider the sets
E′ = {e′
1, . . . , e′
i−1},
ˇE = {ˇe1, . . . , ˇei}.
The graphs (V, E′) and (V, ˇE) contain no cycles and, moreover,
|E′| = i −1, | ˇE| = i.

174
Trees
For the desired contradiction it suﬃces to show that there exists
an edge e ∈ˇE for which the graph (V, E′ ∪{e}) contains no cycle.
Then we obtain w(e) ≤w(ˇei) < w(e′
i) and this means that at the
moment the edge e was considered in the algorithm we made a mis-
take. There was no reason to reject e, and we should have selected
it instead of the edge e′
i or earlier.
Therefore it is enough to show the following: If E′, ˇE ⊆
V
2

are
two sets of edges, such that the graph (V, ˇE) has no cycle and |E′| <
| ˇE|, then some edge e ∈ˇE connects vertices of distinct components of
the graph (V, E′). This can be done by a simple counting argument.
Let V1, . . . , Vs be the vertex sets of the components of the graph
(V, E′). We have
E′ ∩
Vj
2
 ≥|Vj| −1,
and by summing these inequalities over j we get |E′| ≥n −s. On
the other hand, since ˇE has no cycles, we get
 ˇE ∩
Vj
2
 ≤|Vj| −1,
and therefore at most n −s edges of ˇE are contained in some of the
components Vj. But since we assumed | ˇE| > |E′|, there is an edge
e ∈ˇE going between two distinct components.
2
Kruskal’s algorithm is a prototype of the so-called greedy algorithm.
At every step, it selects the cheapest possible edge among those allowed
by the restrictions of the problem (here “the graph should contain no
cycle”). In general, “greedy algorithm” is a term for a strategy trying
to gain as much as possible at any given moment, never contemplating
the possible disadvantages such a choice may bring in the future.3 For
numerous problems, this short-sighted strategy may fail completely. For
instance, a greedy strategy applied for a chess game would mean (in
the simplest form) that a player would always take a piece whenever
possible, and always the most valuable one. And by following such a
naive play he would lose very soon.
3In the minimum spanning tree problem, a “parsimonious algorithm” would
perhaps be a more appropriate name, since we always take the cheapest possible
edge. But “greedy algorithm” is a universally accepted name. It is derived from
situations where one tries to maximize something by grabbing as much as possible
in each step. Since the minimizing and maximizing problems aren’t conceptually
very diﬀerent, it seems better to stick to the single term “greedy algorithm” in
both situations.

5.4 The minimum spanning tree problem
175
In this context, it seems somewhat surprising that the greedy alg-
orithm ﬁnds a minimum spanning tree correctly. The greedy strategy
may be useful also for many other problems (especially if we have no
better idea). Often it at least yields a good approximate solution. Prob-
lems for which the greedy algorithm always ﬁnds an optimal solution
are studied in the so-called matroid theory. The reader can learn about
it in Oxley [26], for instance.
Exercises
1. Analogously to the minimum spanning tree problem, deﬁne the max-
imum spanning tree problem. Formulate a greedy algorithm for this
problem and show that it always ﬁnds an optimal solution.
2. Prove that if T = (V, E′) is a spanning tree of a graph G = (V, E)
then the graph T + e, where e is an arbitrary edge of E \ E′, contains
exactly one cycle.
3. Prove that if T is a spanning tree of a graph G then for every e ∈
E(G) \ E(T) there exists an e′ ∈E(T) such that (T −e′) + e is a
spanning tree of G again.
4. Let G be a connected graph with a weight function w on the edges,
and assume that w is injective. Prove that the minimum spanning tree
of G is determined uniquely.
5. ∗Let G be a connected graph with a weight function w on the edges.
Prove that for each minimum spanning tree T of G, there is an initial
ordering of the edges in Kruskal’s algorithm such that the algorithm
outputs the tree T.
6. Let w and w′ be two weight functions on the edges of a graph G =
(V, E). Suppose that w(e1) < w(e2) holds if and only if w′(e1) <
w′(e2), for any two edges e1, e2 ∈E. Prove that (V, E′) is a minimum
spanning tree of G for the weight function w if and only if (V, E′) is a
minimum spanning tree of G for the weight function w′. (This means:
the solution to the minimum spanning tree problem only depends on
the ordering of edge weights.)
7. CS Using the discussion of Algorithm 5.3.2 in the preceding section,
design the details of Kruskal’s algorithm in such a way that its time
complexity is O((n + m) log n).
8. Consider an n-point set V in the plane. We deﬁne a weight function on
the edge set of the complete graph on V : the weight of an edge {x, y}
is the distance of the points x and y.
(a) ∗Show that no minimum spanning tree for this network has a vertex
of degree 7 or higher.

176
Trees
(b) ∗Show that there exists a minimum spanning tree whose edges do
not cross.
9. ∗Let V be a set of n > 1 points in the unit square in the plane. Let T be
a minimum spanning tree for V (i.e. for the complete graph with edge
weights given by distances as in Exercise 8). Show that the total length
of the edges of T is at most 10√n. (The constant 10 can be improved
signiﬁcantly; the best known estimate is about 1.4√n + O(1).)
10. Let G = (V, E) be a graph and let w be a nonnegative weight function
on its edges.
(a) ∗Each set E′ ⊆E of pairwise disjoint edges (i.e. sharing no vertices)
is called a matching in the graph G. Let νw(G) denote the maximum
possible value of w(E′) for a matching E′ ⊆E. A greedy algorithm
for ﬁnding a maximum matching works similar to Kruskal’s algorithm
for a maximum spanning tree, i.e. it considers edges one by one in the
order of decreasing weights, and it selects an edge if it has no common
vertex with the previously selected edges. Show that this algorithm
always ﬁnds a matching with weight at least 1
2νw(G).
(b) Show that the bound in (a) cannot be improved; that is, for any
constant α > 1
2 there exists an input for which the greedy algorithm
ﬁnds a matching with weight smaller than α νw(G).
11. A set C ⊆E in a graph G = (V, E) is called an edge cover if each
vertex v ∈V is contained in at least one edge e ∈C. Let us look for a
small edge cover by a greedy algorithm: if there is an edge containing 2
uncovered vertices take an arbitrary such edge, otherwise take any edge
covering some yet uncovered vertex, and repeat until all is covered.
Show that the number of edges in a cover thus found is
(a) at most twice the size of the smallest possible cover,
(b) ∗∗and (even) at most 3
2 of the size of the smallest possible cover.
12. ∗A set D ⊆V in a graph G = (V, E) is called a dominating set if

e∈E : e∩D̸=∅e = V . Let us look for a small dominating set by a greedy
algorithm: we always select a vertex connected to the maximum pos-
sible number of yet uncovered vertices. Show that for any number C
there exists a graph for which |DG| ≥C|DM|, where DG is a domi-
nating set selected by the greedy algorithm and DM is a dominating
set of the smallest possible size. (Start by ﬁnding examples for small
speciﬁc values of C.)
5.5
Jarn´ık’s algorithm and Bor˚uvka’s algorithm
What we call “Jarn´ık’s algorithm” is mostly known under the name
“Prim’s algorithm”. However, since Prim’s paper is dated 1957 while

5.5 Jarn´ık’s algorithm and Bor˚uvka’s algorithm
177
Jarn´ık4 already described the same algorithm in an elegant and precise
way in 1930 (continuing the work of Bor˚uvka who published the ﬁrst
documented algorithm for the minimum spanning tree problem in 1928),
we believe it is appropriate to use the name of the ﬁrst inventor.
Nowadays, Jarn´ık’s algorithm can be viewed as a simple extension
of Algorithm 5.3.5.
5.5.1 Algorithm. [Jarn´ık’s algorithm] Proceed according to Alg-
orithm 5.3.5, and always choose the newly added edge ei as an edge
of the smallest possible weight from the set {{x, y} ∈E(G): x ∈
Vi−1, y ̸∈Vi−1}.
5.5.2 Proposition (Correctness
of
Jarn´ık’s
algorithm).
Jarn´ık’s algorithm ﬁnds a minimum spanning tree for every
connected network.
Proof. Let T = (V, E′) be the spanning tree resulting from Jarn´ık’s
algorithm, and suppose that the edges of E′ are numbered e1 through
en−1 in the order they were added to T. For contradiction, suppose
that T is not a minimum spanning tree.
Let T ′ be some minimum spanning tree. Let k(T ′) denote the
index k for which all the edges e1, e2, . . . , ek belong to E(T ′) but
ek+1 ̸∈E(T ′). Among all minimum spanning trees, select one which
has the maximum possible value of k and denote it by ˇT = (V, ˇE).
Write k = k( ˇT).
Now consider the moment in the algorithm’s execution when the
edge ek+1 has been added to T. Let Tk = (Vk, Ek) be the tree formed
by the edges e1, . . . , ek. Then ek+1 has the form {x, y}, where x ∈
V (Tk) and y ̸∈V (Tk). Consider the graph ˇT + ek+1. This graph
contains some cycle C (since it is connected and has more than n−1
edges), and such a C necessarily contains the edge ek+1 (see also
Exercise 5.4.2).
The cycle C consists of the edge ek+1 = {x, y} plus a path P
connecting the vertices x and y in the spanning tree ˇT. At least one
edge of the path P has one vertex in the set Vk and the other vertex
outside Vk. Let e be some such edge. Obviously e ̸= ek+1, and further
we know that e ∈ˇE and ek+1 ̸∈ˇE; see Fig. 5.1. Both the edges e
and ek+1 connect a vertex of Vk with a vertex not lying in Vk, and
by the edge selection rule in the algorithm we get w(ek+1) ≤w(e).
4An approximate pronunciation is YAR-neekh, and for Bor˚uvka it is BOH-
roof-kah.

178
Trees
ek+1
x
y
Vk
e
P
ˇT
Fig. 5.1 Illustration for the correctness proof of Jarn´ık’s algorithm.
Now consider the graph T ′ = ( ˇT +ek+1)−e. This graph has n−1
edges and, as is easy to check, it is connected; hence it is a spanning
tree. We have w(E(T ′)) = w( ˇE) −w(e) + w(ek+1) ≤w( ˇE), and thus
T ′ is a minimum spanning tree as well, but with k(T ′) > k( ˇT). This
contradiction to the choice of ˇT proves Proposition 5.5.2.
Warning. This is another proof of a slippery nature: make one step
slightly diﬀerently and the whole thing falls apart.
2
Bor˚uvka’s algorithm. In conclusion, let us mention the historically
ﬁrst algorithm for minimum spanning tree computation due to Bor˚uvka.
As is usual in science, the ﬁrst method discovered was not the simplest—
both Kruskal’s and (in particular) Jarn´ık’s algorithm are conceptually
simpler. But yet it was Bor˚uvka’s algorithm that recently became a
starting point for the theoretically fastest known algorithm for the min-
imum spanning tree problem (Karger, Klein, and Tarjan [40]). This lat-
ter algorithm is fairly complicated and uses a number of other ideas
(which we will not pursue here) to make the computation fast.
5.5.3 Algorithm (Bor˚uvka’s algorithm). The input is a graph G =
(V, E) with edge weight function w. We need to assume, moreover,
that distinct edges get distinct weights, i.e. that the weight function is
one-to-one. This assumption is not particularly restrictive. Each weight
function can be converted into a one-to-one function by arbitrarily small
changes of the weights, which changes the weight of a minimum span-
ning tree by an arbitrarily small amount. (Alternatively, the algorithm
can be modiﬁed to work with arbitrary weight functions, by adding a
simple tie-breaking rule outlined in Exercise 6 below.)
The algorithm successively constructs sets E0, E1, . . . ⊆E of edges,
beginning with E0 = ∅.
Suppose that the set Ei−1 has already been computed, and let
(V1, . . . , Vt) be the partition of the vertex set according to the

5.5 Jarn´ık’s algorithm and Bor˚uvka’s algorithm
179
components of the graph (V, Ei−1). Strictly speaking, this partition
should also have the index i since it is diﬀerent in each step, but we
omit this index in order to make the notation simpler. For each set
Vj of this partition we ﬁnd the edge ej = {xj, yj} (where xj ∈Vj,
yj ̸∈Vj) whose weight is minimum among all edges of the form {x, y},
x ∈Vj, y ∈V \ Vj (it may happen that ej = ej′ for j ̸= j′). We put
Ei = Ei−1 ∪{e1, . . . , et}. The algorithm ﬁnishes when the graph (V, Ei)
has a single component.
The algorithm could also be called a “bubbles algorithm”. The graph
G is covered by a collection of “bubbles”. In each step, we merge each
bubble with its nearest neighboring bubble.
We will not prove the correctness of this algorithm. We only show
that the constructed graph has no cycle (which, unlike the previous
algorithms, is not quite obvious). So suppose for contradiction that a
cycle arose in some step i. This means that there are pairwise distinct
indices j(1), j(2), . . . , j(k) for which
xj(1) ∈Vj(1),
yj(1) ∈Vj(2)
xj(2) ∈Vj(2),
yj(2) ∈Vj(3)
...
xj(k−1) ∈Vj(k−1),
yj(k−1) ∈Vj(k)
xj(k) ∈Vj(k),
yj(k) ∈Vj(1).
Here is an illustration:
Vj(k)
Vj(1)
Vj(2)
Vj(3)
xj(1)
yj(1)
xj(2)
yj(2), xj(3)
yj(k)
xj(k)
yj(k−1)
Since distinct edges have distinct weights, the edge ej(ℓ) always has the
smallest weight among the edges leaving the component Vj(ℓ), and in
particular, we get
w(ej(1)) < w(ej(2)) < · · · < w(ej(k)) < w(ej(1)).
This chain of strict inequalities cannot hold and so Bor˚uvka’s algorithm
ﬁnds some spanning tree of G. With some more eﬀort, it can be proved
that it ﬁnds the minimum spanning tree.
2
Example.
Consider the following network (the weights are given as
labels of the edges):

180
Trees
12
9
8
2
16
1
15
11
7
14
13
5
17
3
4
10
6
Jarn´ık’s algorithm started in the upper left corner proceeds as follows:
Kruskal’s algorithm needs 17 steps (but only in 10 of them is a new
edge added). Bor˚uvka’s algorithm, on the other hand, is quite short:
But in each step we have to do much more work.
Exercises
1. (General spanning tree algorithm) Consider the following algorithm for
the minimum spanning tree problem. The input is a connected graph
G = (V, E) with weight function w. We put E0 = ∅. Suppose that
Ei−1 has already been deﬁned. Choose an arbitrary component Vi of
the graph (V, Ei−1), select an edge ei of the minimum weight among
the edges with one vertex in Vi and the other vertex not in Vi, and set
Ei = Ei−1 ∪{ei}. Prove that (V, En−1) is a minimum spanning tree.
(Imitate the correctness proof for Jarn´ık’s algorithm.)
Check that this proves the correctness of both Kruskal’s and Jarn´ık’s
algorithm.
2. (“Inverse” greedy algorithm) Consider the following algorithm for the
minimum spanning tree problem. The input is a connected graph G =
(V, E) with weight function w. Label the edges e1, . . . , em in such a
way that w(e1) ≥· · · ≥w(em). Put E0 = E, and
Ei =
 Ei−1 \ {ei}
if the graph (V, Ei−1 \ {ei}) is connected
Ei
otherwise.
Prove that (V, Em) is a minimum spanning tree of G.

5.5 Jarn´ık’s algorithm and Bor˚uvka’s algorithm
181
3. ∗Prove the correctness of Bor˚uvka’s algorithm.
4. CS Design the details of Jarn´ık’s algorithm in such a way that its
time complexity is O((m+n) log n) (this probably requires some basic
knowledge of data structures).
5. (a) Prove that Bor˚uvka’s algorithm has at most O(log n) phases, i.e.
the graph (V, Ei) is connected already for some i = O(log n).
(b) CS Design the details of Bor˚uvka’s algorithm in such a way that
its time complexity is at most O((m + n) log n).
6. (Tie-breaking in Bor˚uvka’s algorithm) Given an arbitrary weight func-
tion on edges of a graph G = (V, E), we choose an arbitrary ordering
e1, e2, . . . , em of the edges once and for all, and we deﬁne ei  ej if
either w(ei) < w(ej) or both w(ei) = w(ej) and i ≤j.
(a) Formulate Bor˚uvka’s algorithm with edges ordered by the linear
ordering  (instead of the usual ordering ≤on the weights).
(b) Check that the algorithm in (a) computes a spanning tree of G.
(c) Prove that the algorithm in (a) computes a minimum spanning
tree of G with respect to the weight function w (modify the proof in
Exercise 3).

6
Drawing graphs in the plane
6.1
Drawing in the plane and on other surfaces
Often it is advantageous to draw graphs. As you can see, most of
the graphs in this book are speciﬁed by a picture (instead of a list of
vertices and edges, say). But so far we have been studying properties
of graphs not related to their drawings, and the role of drawings was
purely auxiliary. In this chapter the subject of analysis will be the
drawing of graphs itself and we will mainly investigate graphs that
can be drawn in the plane without edge crossings. Such graphs are
called planar.
From the numerous pictures shown so far and from the informal
deﬁnition given in Section 4.1, the reader might have gained a quite
good intuition about what is meant by a drawing of a graph. Such an
intuition is usually suﬃcient if we want to show, say, that some graph
is planar—we can simply draw a suitable picture of the graph with no
edge crossings. However, if we want to prove, in a strictly logical way,
that some graph is not planar, then we cannot do without a math-
ematical deﬁnition of the notion of a drawing, based on other exact
mathematical notions. Today’s mathematics is completely built from a
few primitive notions and axioms of set theory—or at least the majority
of mathematicians try to ensure it is. So, for instance, the notion of a
“plane” is being modeled as the Cartesian product R × R. Each real
number is deﬁned as a certain subset of the rationals, the rational num-
bers are created from natural numbers, and ﬁnally the natural numbers
are deﬁned as certain special sets produced from the empty set. (This
is seldom apparent in everyday mathematics, but if you look at a book
on the foundations of mathematics you can ﬁnd it in there.)
In order to introduce the notion of a drawing formally, we deﬁne an
arc ﬁrst: this is a subset α of the plane of the form α = γ([0, 1]) =
{γ(x): x ∈[0, 1]}, where γ : [0, 1] →R2 is an injective continuous
map of the closed interval [0, 1] into the plane. The points γ(0) and
γ(1) are called the endpoints of the arc α.

6.1 Drawing in the plane and on other surfaces
183
This deﬁnition, frightening as it may look, is very close to the int-
uitive notion of drawing. The interval [0, 1] can be thought of as a time
interval during which we draw a line from the point γ(0) to the point
γ(1). Then γ(t) is the position of the pencil’s tip at time t. The con-
tinuity of the mapping γ means a continuous motion on the paper’s
surface in time, and the injectivity says that the line being drawn never
intersects itself.
6.1.1 Definition. By a drawing of a graph G = (V, E) we mean
an assignment as follows: to every vertex v of the graph G, assign a
point b(v) of the plane, and to every edge e = {v, v′} ∈E, assign an
arc α(e) in the plane with endpoints b(v) and b(v′). We assume that
the mapping b is injective (diﬀerent vertices are assigned distinct
points in the plane), and no point of the form b(v) lies on any of the
arcs α(e) unless it is an endpoint of that arc. A graph together with
some drawing is called a topological graph.1
A drawing of a graph G in which any two arcs corresponding to
distinct edges either have no intersection or only share an endpoint
is called a planar drawing. A graph G is planar if it has at least one
planar drawing.
We have given the above formal deﬁnition of a graph drawing a bit
“for show”, in order to illustrate that the notion of a drawing can be
included in the logical construction of mathematics. We will not con-
tinue building the subsequent theory of planar graphs in a strictly logical
way, though. We would have to use notions and results concerning pla-
nar curves. These belong to a branch of mathematics called topology.
Only very little from topology is usually covered in introductory math-
ematical courses, and we would have to introduce quite complicated
machinery in order to do everything rigorously. Moreover, proofs of cer-
tain “intuitively obvious” statements are surprisingly diﬃcult. For these
reasons, we will sometimes rely on the reader’s intuition in the subse-
quent text, and we will ask the reader to believe in some (valid!) state-
ments without a proof. A rigorous treatment can be found, for example,
in the recent book by Mohar and Thomassen [24]. Fortunately, in the
theory of graph drawing, the basic intuition about drawing seldom leads
one astray.
A planar drawing is advantageous for a visualization of a graph
(edge crossings in a nonplanar drawing could be mistaken for ver-
tices), and in some applications where the drawing has a physical
1A planar graph with a given planar drawing, i.e. a topological planar graph,
is sometimes called a plane graph.

184
Drawing graphs in the plane
meaning, edge crossings can be inadmissible (for instance, in the
design of single-layer integrated circuits).
Faces of a graph drawing. Let G = (V, E) be a topological planar
graph, i.e. a planar graph together with a given planar drawing.
Consider the set of all points in the plane that lie on none of the arcs
of the drawing. This set consists of ﬁnitely many connected regions
(imagine that we cut the plane along the edges of the drawing):
F1
F2
F3
(We say that a set A ⊆R2 is connected if for any two points x, y ∈A
there exists an arc α ⊆A with endpoints x and y. “Being con-
nected”2 is an example of a topological notion.) These regions will
be called the faces of the considered topological planar graph. The
region spreading out to inﬁnity, such as F1 in the picture, is called
the outer face (or the unbounded face) of the drawing, and all the
other faces are called inner faces (or bounded faces).
Let us stress that faces are deﬁned for a given planar drawing.
Faces are usually not deﬁned for a nonplanar drawing, and also we
should not speak about faces for a planar graph without having a
speciﬁc drawing in mind.
Drawing on other surfaces.
A graph can also be drawn on other
surfaces than the plane. Let us list some examples of interesting surfaces.
Everyone knows the sphere (i.e. the surface of a ball). The surface
of a tire-tube is scientiﬁcally called the torus:3
2What we called a “connected set” is usually called an arc-connected set in
topology. A connected set is deﬁned as follows. A set A ⊆R2 is connected
if no two disjoint open sets A1, A2 ⊆R2 exist such that A ⊆A1 ∪A2 and
A1 ∩A ̸= ∅̸= A2 ∩A. For sets considered in this chapter, such as faces of a
graph, both these notions of being connected coincide, and so we use the shorter
name.
3People with a US-centered worldview might want to speak about the surface
of a doughnut, but since doughnuts in other countries (Australia, United King-
dom) are often spherical, we don’t consider this name 100% politically correct.

6.1 Drawing in the plane and on other surfaces
185
If we take a long strip of paper, turn one of its ends by 180 degrees,
and glue it to the other end, we obtain an interesting surface called the
M¨obius band:
Other examples are a sphere with two handles:
(after a suitable deformation, this is also the surface of a “fat ﬁgure 8”),
or the so-called Klein bottle:
Each of these surfaces can be created from a planar polygon by
“gluing” and a suitable deformation. In the above examples, with the
exception of the sphere with two handles, we would always start from
a planar rectangle, and we would identify (glue) some its edges in a
suitable way. We have already introduced the M¨obius band using such
a procedure, and this method is also a basis for a rigorous deﬁnition of

186
Drawing graphs in the plane
such surfaces (which we do not present here). For example, the torus
can be made as follows:
a
b
c
d
That is, we identify the opposite edges of the rectangle abcd, in such a
way that the edge ab is glued to the edge dc and the edge ad is glued
to the edge bc. The orientation of edges for gluing is usually marked by
arrows. The arrows in the following picture mean that when gluing the
edge ad to the edge bc, the point a goes to the point b and the point d
goes to c.
a
c
d
b
The following picture once again indicates how to manufacture the
M¨obius band:
(we only identify the two edges marked by arrows in such a way that
the arrow directions are the same on the glued edge). The Klein bottle
is produced by following the instructions of the next picture:
In fact, these are not quite honest directions for producing your own
Klein bottle, since the Klein bottle cannot be realized in the 3-
dimensional Euclidean space. The indicated gluing cannot be done in

6.1 Drawing in the plane and on other surfaces
187
R3 unless the rectangle intersects itself (as it does in the picture show-
ing the Klein bottle). Nevertheless, the deﬁnition of the Klein bottle
by gluing makes good sense mathematically, and the surface can be
realized in R4, say.
There is a general theorem saying that any closed surface (having no
“boundary points” and not “running to inﬁnity” anywhere; a scholarly
term is “a compact 2-manifold without boundary”) can be created by a
suitable gluing and deformation from a regular convex polygon. More-
over, if the resulting surface is two-sided (note that both the M¨obius
band and the Klein bottle only have one side!) then it can be continu-
ously deformed into a sphere with ﬁnitely many handles. The basics of
this theory and a number of related topics are beautifully explained in
the book by Stillwell [29].
Graphs can be classiﬁed according to the surfaces they can be drawn
on. As will be shown in the next section, neither the graph K5, the
complete graph on 5 vertices, nor K3,3, the complete bipartite graph on
3+3 vertices, is planar. But K5 can be drawn on the torus, for instance:
and K3,3 on the M¨obius band:
As we said above, these surfaces can be obtained by a suitable gluing
of the edges of a rectangle. In order that our spatial imagination is not
overstrained, we can convert drawing on surfaces into a modiﬁed planar
drawing, where edges can “jump” among the glued rectangle edges. The
drawings just shown can thus be recast as follows:

188
Drawing graphs in the plane
Let us remark that even the graph K4,4 can be drawn on the torus:
1
2
3
4
a
b
c
d
a
b
c
d
a
b
c
d
1
2
1
2
3
4
(K7 can be so drawn as well; see Exercise 2). In general, we have
6.1.2 Proposition. Any graph can be drawn without edge crossings
on a sphere with suﬃciently many handles.
(This proposition must be taken informally, since we gave no exact
deﬁnition of a sphere with handles.)
Informal proof. Let us draw the given graph G = (V, E) on the sphere,
possibly with edge crossings. Let e1, e2, . . . , en be all edges having a
crossing with another edge. For each edge ei, add a handle serving as
a “bridge” for that edge to avoid the other edges, in such a way that
the handles are disjoint and the edges drawn on handles do not cross
anymore:
Since we deal with ﬁnitely many edges only, it is easy to ﬁnd such
handles.
2
Hence the following deﬁnition makes sense.
6.1.3 Definition. The smallest number of handles that must be
added to the sphere so that a graph G can be drawn on the
resulting surface without edge crossings is called the genus4 of the
graph G.
4The notion of genus is primarily used for surfaces. For example, the genus
of a sphere with handles is the number of handles. Interestingly, 2-dimensional
surfaces were ﬁrst systematically studied in connection with algebraic equations!
The set of all complex solutions of a polynomial equation in 2 variables is typically
a 2-dimensional surface, and its genus is crucial for various properties of the
equation. The (diﬃcult) area of mathematics studying algebraic equations in

6.1 Drawing in the plane and on other surfaces
189
In conclusion of this section, let us show that planar graphs are
exactly graphs of genus 0, i.e. ones that can be drawn on the sphere. This
becomes quite obvious if we use the stereographic projection. We place
the sphere in the 3-dimensional space in such a way that it touches the
considered plane ρ. Let o denote the point of the sphere lying farthest
from ρ (the “north pole”):
o
x
x′
ρ
Then the stereographic projection maps each point x ̸= o of the sphere
to a point x′ in the plane, where x′ is the intersection of the line ox
with the plane ρ. (For the point o, the projection is undeﬁned.) This
deﬁnes a bijection between the plane and the sphere without the point
o. Given a drawing of a graph G on the sphere without edge crossings,
where the point o lies on no arc of the drawing (which we may assume
by a suitable choice of o), the stereographic projection yields a planar
drawing of G. Conversely, from a planar drawing we get a drawing on
the sphere by the inverse projection.
Exercises
1. Find
(a) a planar graph all of whose vertices have degree 5,
(b) ∗connected graphs as in (a) with arbitrarily many vertices.
2. (a) Check that the picture shown in the text indeed gives a drawing
of K4,4 on the torus.
(b) Find a drawing of K6 on the torus.
(c) ∗Draw K7 on the torus.
3. ∗Let G be a planar Eulerian graph. Consider some planar drawing of
G. Show that there exists a closed Eulerian tour that never crosses
itself in the considered drawing (it may touch itself at vertices but it
never “crosses over to the other side”).
4. Try to invent some suitable deﬁnition of a closed 2-dimensional surface.
Then look up what the usual deﬁnition is in a book on topology.
this spirit is called algebraic geometry (Cox, Little, and O’Shea [18] is a nice
introduction to the subject).

190
Drawing graphs in the plane
6.2
Cycles in planar graphs
We will investigate various combinatorial properties of planar graphs.
Among others, it turns out that the notion of a planar graph itself
can be equivalently deﬁned by purely combinatorial means, without
using topological properties of the plane or intuition about graph
drawing.
Should the geometric deﬁnition of planar graphs be converted into
a combinatorial deﬁnition, we have to use some property of the plane
connecting geometry to combinatorics. Such a property is expressed
by the Jordan curve theorem below. First, a deﬁnition: a Jordan
curve5 is a closed curve without self-intersections. More formally, a
Jordan curve is deﬁned as an arc whose endpoints coincide, i.e. a
continuous image of the interval [0, 1] under a mapping f that is
one-to-one except for the equality f(0) = f(1).
6.2.1 Theorem (Jordan curve theorem). Any Jordan curve k
divides the plane into exactly two connected parts, the “interior”
and the “exterior” of k, and k is the boundary of both the interior
and the exterior. (Both the interior and exterior will be called the
regions of k.) This means that if we deﬁne a relation ≈on the set
R2 \ k by setting x ≈y if and only if x and y can be connected by
an arc disjoint from k, then ≈is an equivalence with 2 classes, one
of them being a bounded set and the other one an unbounded set.
This theorem is intuitively obvious, but its proof is by no means
simple, although signiﬁcant simpliﬁcations have been found recently by
Thomassen [47]. For some Jordan curves in the plane, the statement is
very evident,
5Another common name for this object is a simple closed curve.

6.2 Cycles in planar graphs
191
but for others it is perhaps less obvious (try ﬁnding an arc connecting
the points ◦and • and not intersecting the curve):
◦
In order to illustrate that intuition is not always reliable for such
“obvious” statements, let us mention a related theorem. An extension of
the Jordan curve theorem, the Jordan–Sch¨onﬂies theorem, tells us that
for any Jordan curve, the interior can be continuously deformed onto
the interior of the (usual geometric) circle. More precisely, there exists
a continuous mapping whose inverse mapping is continuous as well, a
so-called homeomorphism, between the (closed) region bounded by any
Jordan curve and the ordinary circular disk. Similarly, one would expect
that if we deﬁne a “topological sphere” as the image of the usual geo-
metric sphere by an injective continuous map, such a thing will bound
a region that can be continuously deformed onto the ordinary ball. But
this is false—a counterexample is known under the name “Alexander’s
horned sphere” (see e.g. the excellent but somewhat more advanced
book by Bredon [17]).
Let us remark that the diﬃculties with proving the Jordan curve the-
orem mainly stem from the considerable generality of the notion of an
arc. We admit an arbitrary injective continuous mapping of the unit in-
terval in the deﬁnition of an arc, and such mappings can be quite “wild”.
A simpler way to build a logically precise theory of planar graphs is to
permit only arcs consisting of a ﬁnite number of straight segments—let
us call them polygonal arcs. We can thus call a graph polygonally planar
if it can be drawn without edge crossings using polygonal arcs. To prove
the Jordan curve theorem for polygonal arcs only is reasonably easy (see
Exercise 7). And it is not too diﬃcult to verify that any planar graph is
polygonally planar too;6 for this one needs some topology but very little.
6Even a much stronger statement holds: any planar graph can be drawn with-
out edge crossings in such a way that every edge is a straight segment! But this
is not an easy theorem.

192
Drawing graphs in the plane
Therefore, allowing for general nonpolygonal arcs achieves nothing new
in graph drawing—only complications with the Jordan curve theorem.
As was announced earlier, we prefer to rely on intuition at some
points in deriving results about planar graphs. At the price of longer
and more complicated proofs, such imperfections could be removed,
and everything could be derived from the Jordan curve theorem and
its variations.
Let us begin with a proof of nonplanarity of the graph K5. Later
on, we will prove it again by other means.
6.2.2 Proposition. K5 is not planar.
Proof. Proceed by contradiction. Let b1, b2, b3, b4, b5 be the points
corresponding to the vertices of K5 in some planar drawing. The arc
connecting the points bi and bj will be denoted by α(i, j).
Since b1, b2, and b3 are vertices of a cycle in the graph K5, the
arcs α(1, 2), α(2, 3), and α(3, 1) form a Jordan curve k, and hence
the points b4 and b5 lie either both inside or both outside k, for
otherwise the arc α(4, 5) would cross k. First suppose that b4 lies
inside k, as in the following picture:
b1
b2
b3
b4
Then b5 lies inside the Jordan curve formed by the arcs α(1, 4),
α(2, 4), and α(1, 2), or in the Jordan curve made up by α(2, 3),
α(3, 4), and α(2, 4), or inside the Jordan curve consisting of α(1, 3),
α(3, 4), and α(1, 4).7 In the ﬁrst of these cases, the arc α(3, 5) has
to intersect the Jordan curve formed by the arcs α(1, 4), α(2, 4), and
α(1, 2), however, and similarly in the remaining two cases.
If the points b4 and b5 lie both outside k, we proceed quite anal-
ogously.
2
Faces and cycles in 2-connected graphs. If e1, . . . , en are the
edges of a cycle in a topological planar graph G, then the arcs
α(e1), . . . , α(en) form a Jordan curve. By the Jordan curve theorem,
7We haven’t proved that the interiors of these Jordan curves together cover
the interior of k, and so this is one of the points where we rely on intuition
somewhat.

6.2 Cycles in planar graphs
193
we get that each face of G lies either inside or outside this Jordan
curve. For brevity, let us call this Jordan curve a cycle of G too (so
a cycle of G may now mean either a cycle in the graph-theoretic
sense, i.e. a subgraph of G, or the Jordan curve corresponding to a
graph-theoretic cycle of G in some drawing of G).
For some topological planar graphs, each face is the interior or
the exterior of some cycle of G. But it need not always be so. For ins-
tance, a planar drawing of a tree has only one face. Another example
might look as follows:
F3
F4
F1
F2
It turns out that the bad examples are exactly the graphs that are
not 2-connected.
6.2.3 Proposition. Let G be a 2-vertex-connected planar graph.
Then every face in any planar drawing of G is a region of some cycle
of G.
Proof. We proceed by induction, making use of Proposition 4.6.5
(characterization of 2-connected graphs). If the graph G is a triangle,
the statement we are proving follows from the Jordan curve theorem.
Let G = (V, E) be a connected topological planar graph with
at least 4 vertices. By Proposition 4.6.5, either there exists an edge
e ∈E such that the graph G′ = G −e is 2-connected, or there are
a 2-connected graph G′ = (V ′, E′) and an edge e ∈E′ such that
G = G′%e, where % denotes the operation of edge subdivision.
Since G is a topological planar graph, G′ is a topological planar
graph as well, in both cases. Since G′ is 2-connected, we can use the
inductive hypothesis. Each face of the topological graph G′ is thus a
region of some cycle of G′.
Let us consider the ﬁrst case, where G′ = G −e, e = {v, v′}. The
vertices v and v′ are connected by the arc α(e) corresponding to the
edge e, and hence they both lie on the boundary of a face F of G′.
Let kF be the cycle bounding the face F. As the following picture
indicates, the arc α(e) divides F into two new faces F ′ and F ′′:
v
v′
F ′
F ′′
α(e)
α1
α2

194
Drawing graphs in the plane
(This is one of the points where we rely on pictorial intuition and
omit a rigorous proof, but see Exercise 7.) The faces F ′ and F ′′ are
regions of the cycles α1 ∪α(e) and α2 ∪α(e), where α1 and α2 are
the two arcs connecting v and v′ and together forming the cycle kF .
Hence the faces of G are all bounded by cycles as claimed. This
ﬁnishes the inductive step in the ﬁrst case, where G = G′ + e.
The remaining second case is easier: if G = G′%e and each face
of G′ is a region of some cycle of G′, then G has the same property,
as follows immediately from the deﬁnition of edge subdivision. This
concludes the proof of the proposition.
2
Proposition 6.2.3 shows that 2-connected planar graphs behave, in
some sense, more nicely than arbitrary planar graphs. But it turns out
that a still much nicer behavior results by requiring that the consid-
ered planar graphs be 3-vertex-connected. Such graphs have an essen-
tially unique drawing on the sphere (up to a continuous deformation of
the sphere and mirror reﬂection), and in many ways they are easier to
work with (see Section 6.3). When proving theorems or designing alg-
orithms concerning planar graphs, it is usually advisable to deal with
3-connected planar graphs ﬁrst, and then try to handle the general case
by decomposing a given graph into 3-connected pieces. Here we will not
pursue this matter any further.
A combinatorial characterization of planar graphs.
Let us
remark that the following clearly holds: A graph G is planar if and
only if each subdivision of G is planar. This property can be used for a
combinatorial characterization of planar graphs—a characterization
purely in graph-theoretic notions, using no geometric notions at all.
It is the following celebrated result:
6.2.4 Theorem (Kuratowski’s theorem). A graph G is planar
if and only if it has no subgraph isomorphic to a subdivision of K3,3
or to a subdivision of K5.
It is easy to prove one of the implications in this theorem (if G is
planar then it cannot contain a subdivision of a nonplanar graph),
but the reverse implication is more demanding and we will not prove
it in this book.
This theorem shows that the nonplanarity of any nonplanar graph
can be certiﬁed by ﬁnding a subdivision of K3,3 or K5 in it. From a
computational point of view, i.e. if we want to really test graph pla-
narity by a computer and perhaps also look for a planar drawing, this
method is not very eﬃcient. Algorithms are known for testing whether

6.2 Cycles in planar graphs
195
a given graph contains a subdivision of a ﬁxed (small) graph, but these
algorithms are fairly complicated and impractical. For planarity test-
ing and ﬁnding “nice” planar drawings, a number of fast (although
also complicated) methods have been invented. Such methods can, for
instance, test planarity of a given graph on n vertices in time O(n).
Concerning “nice” drawings, it is known that, for example, any pla-
nar graph on n vertices can be drawn in such a way that the vertices
have integer coordinatesbetween 1 and n and the edges are straight seg-
ments. Recent results in this direction, as well as further references, can
be found in Kant [39]. There are also many interesting problems related
to graph drawing which remain open. For instance, it is not known (at
the time of writing this book) whether every planar graph has a pla-
nar drawing where all edges are straight segments with integer lengths.
(This is an example of how easily one can sometimes formulate even
diﬃcult problems in discrete mathematics.)
Kuratowski’s theorem characterizes planar graphs by specifying 2
“obstacles to planarity”, namely the presence of a subdivision of K5
or of a subdivision of K3,3. Recently, many theorems in a similar spirit
have been found, characterizing various classes of graphs by ﬁnite sets of
“obstacles”. The obstacles are usually not forbidden subdivisions of cer-
tain graphs but rather so-called forbidden minors (see Exercise 6.4.11).
Many outstanding problems in graph theory, including numerous ques-
tions about eﬃcient algorithms, have been solved by this approach and
related ideas, and currently this area (called the structural graph the-
ory) constitutes one of the most dynamic and successful parts of mod-
ern graph theory. A sample of recent progress in this area is Robertson,
Seymour, and Thomas [44], where a long-standing open problem has
been resolved by related methods.
Exercises
1. Show that the graph K3,3 is not planar, in a manner similar to the
proof of nonplanarity of K5 given in the text.
2. (a) Find a subdivision of either K3,3 or K5 in the graph on the left in
Fig. 4.3.
(b) Is the graph in the middle in Fig. 4.3 planar?
(c) Is the graph in Fig. 9.3 planar?
3. The complete k-partite graph Kn1,n2,...,nk
has vertex set V
=
V1 ˙∪V2 ˙∪· · · ˙∪Vk, where V1, . . . , Vk are disjoint sets with |Vi| = ni, and
each vertex v ∈Vi is connected to all vertices of V \ Vi, i = 1, 2, . . . , k.
Describe all k-tuples (n1, n2, . . . , nk) of natural numbers, k = 1, 2, . . .,
such that Kn1,n2,...,nk is a planar graph.

196
Drawing graphs in the plane
4. In the proof of Proposition 6.2.3, we proceeded by induction, but ind-
uction on what? (There was no explicitly mentioned natural number
as an induction parameter.)
5. Prove that if each face of a topological planar graph G is a region of
some cycle of G then G is 2-connected.
6. ∗Consider an arbitrary drawing (not necessarily planar) of the com-
plete graph Kn. Prove that at least 1
5
n
4

pairs of edges have to cross.
(Use the nonplanarity of K5.)
7. The goal of this exercise is to give a rigorous proof, without relying on
geometric intuition.
(a) ∗Let k be a Jordan curve consisting of ﬁnitely many segments (i.e.
a polygon). Deﬁne two points of R2 \ k to be equivalent if they can
be connected by a polygonal arc not intersecting k. Prove that this
equivalence has at most two classes.
(b) ∗Show that in the situation as in (a) there are at least two classes.
Hint: deﬁne an “interior point” as one for which a vertical semiline
emanating upwards from it has an odd number of intersections with k.
(c) Let k be a polygonal Jordan curve as in (a), let p, q be two distinct
points of k, let k1, k2 be the two polygonal arcs into which k is divided
by the points p and q, and let r ∈k1, s ∈k2 be points inside these arcs.
Let ℓbe a polygonal arc connecting p and q and lying completely in
the interior of k (except for its endpoints). Prove that any polygonal
arc connecting r to s and lying in the interior of k (except for the
endpoints) must intersect ℓ.
6.3
Euler’s formula
There exists essentially only one basic quantitative formula for planar
graphs. One can say that all other results use this formula to some
extent. At the same time, it is the oldest formula. It was known to Euler
in 1752, and sometimes it is asserted that it was known to Descartes in
1640 as well; the original statement was about convex polytopes rather
than about planar graphs.
6.3.1 Proposition (Euler’s formula). Let G = (V, E) be a con-
nected planar graph, and let f be the number of faces of some planar
drawing of G. Then we have
|V | −|E| + f = 2.
In particular, the number of faces does not depend on the particular
way of drawing.

6.3 Euler’s formula
197
Proof.
We proceed by induction on the number of edges of the
graph G. If E = ∅then |V | = 1 and f = 1, and the formula holds.
So let |E| ≥1. We distinguish two cases:
1. The graph G contains no cycle. Then G is a tree and hence
|V | = |E| + 1; at the same time we have f = 1 since a planar
drawing of a tree has only one (unbounded) face.
2. Some edge e ∈E is contained in a cycle. In this case the graph
G −e is connected. Hence by the inductive hypothesis, Euler’s
formula holds for it (we consider the drawing arising from the
given drawing of G by removing the edge e). The edge e in
the considered drawing of G is adjacent to two distinct faces
F and F ′, by the Jordan curve theorem. These faces become a
single face after deleting e. Hence both the number of faces and
edges increases by 1 by adding e back to the drawing, and the
number of vertices is unchanged; hence Euler’s formula is true
for G too.
2
Application: Platonic solids. A Greek school of thinkers associated
with Plato’s name used to attribute a particular signiﬁcance to highly
regular geometric solids, the so-called regular polytopes, looking for them
even in the foundations of the structure of the universe. (Besides, Kepler
also regarded as one of his most important discoveries a theory, most
likely a mistaken one, according to which the spacing among the plan-
ets’ orbits is determined by the geometry of the regular polytopes.) A
regular polytope is a 3-dimensional convex body8 bounded by a ﬁnite
number of faces. All faces should be congruent copies of the same regu-
lar convex polygon, and the same number of faces should meet at each
vertex of the body. One reason for the above-mentioned great interest
in these objects is most likely their exceptionality. There are only 5
types of regular polytopes: the regular tetrahedron, the cube, the regu-
lar octahedron, the regular dodecahedron, and the regular icosahedron
(surely the reader will know which is which):
8Convexity means that whenever x and y are two points of the considered
body, then the whole segment xy is a part of the body, i.e. the surface has no
“dips” in it.

198
Drawing graphs in the plane
d = k = 3
d = 3, k = 4
d = 4, k = 3
d = 3, k = 5
d = 5, k = 3
Fig. 6.1 Graphs of the Platonic solids.
This fact was already known to the ancient Greeks. Let us remark
that if we relax the conditions on regularity a little (we do not insist
on convexity, or we allow for two types of faces, etc.), or if we go to
higher dimensions, we can meet many more interesting and beautiful
geometric shapes. Research in this area is still quite active.
Using Euler’s formula, we will show that no other regular poly-
topes than the 5 Platonic ones exist. (The actual existence of these reg-
ular polytopes must be checked geometrically, which we do not consider
here.) The ﬁrst step in proving the non-existence of other regular poly-
topes is converting a convex polytope into a planar graph. We place
the considered polytope inside a sphere, in such a way that the center
of the sphere lies inside the polytope. Then we project the polytope
onto the sphere (imagine that the edges of the polytope are made from
wire and we place a tiny lamp in the center). This yields a graph drawn
on the sphere without edge crossings, and as we know from Section 6.1,
such a drawing can be metamorphosed into a planar drawing using the
stereographic projection. The vertices, edges, and faces of the polytope
become the vertices, edges, and faces of this planar drawing, respec-
tively. This is where the terms “vertex” and “edge” for graphs and
“face” for planar graphs come from. For the 5 regular solids, we thus
obtain the graphs in Fig. 6.1.

6.3 Euler’s formula
199
For a regular polytope, the resulting topological planar graph has
the same degree, d, of each vertex (where d ≥3), and each face has the
same number, k ≥3, of vertices on its boundary. The nonexistence of
any other regular polytopes is thus a consequence of the following:
6.3.2 Proposition. Let G be a topological planar graph in which each
vertex has degree d and each face is adjacent to k vertices, for some
integers d ≥3 and k ≥3. Then G is isomorphic to one of the graphs in
Fig. 6.1.
Proof. Let us denote the number of vertices of the considered graph
G = (V, E) by n, the number of edges by m, and the number of
faces by f. First we use the equation 
v∈V degG(v) = 2|E| (Propos-
ition 4.3.1), which in our case specializes to
dn = 2m.
Similarly we obtain the equality
2m = kf.
We double-count the number of ordered pairs (e, F), where F is a face
of G and e ∈E is an edge lying on the boundary of F. Each edge
contributes 2 such pairs (as each face is bounded by a cycle), and each
face k pairs.
Next, we express both n and f in terms of m using the just derived
equations, and we substitute the results into Euler’s formula:
2 = n −m + f = 2m
d −m + 2m
k .
By adding m and dividing by 2m, we obtain
1
d + 1
k = 1
2 + 1
m.
Hence if both d and k are known, the other parameters n, m, and f
are already determined uniquely. Obviously min(d, k) = 3, for otherwise
1
d + 1
k ≤1
2 . For d = 3 we get 1
k −1
6 = 1
m > 0, and therefore k ∈{3, 4, 5}.
Similarly for k = 3 we derive d ∈{3, 4, 5}. Hence one of the following
possibilities must occur:
d
k
n
m
f
3
3
4
6
4
3
4
8
12
6
3
5
20
30
12
4
3
6
12
8
5
3
12
30
20
Now it is easy to check that in each of these cases the graph is completely
determined by the values d, k, n, m, f, and it is isomorphic to one of the
graphs in Fig. 6.1.
2

200
Drawing graphs in the plane
Let us remark that the connection between planar graphs and
3-dimensional convex polytopes is closer than it might seem. As we have
seen, we obtain a planar graph from each convex polytope. A quite diﬃ-
cult theorem, due to Steinitz, asserts that any vertex 3-connected planar
graph (i.e. a planar graph that remains connected even after deleting
any 2 vertices) is the graph of some 3-dimensional convex polytope. A
delightful account of the theory of convex polytopes is Ziegler [31].
A very important property of planar graphs is that they can only
have relatively few edges: a planar graph on n vertices has O(n)
edges. Here is a precise formulation of this property:
6.3.3 Proposition (Planar graph has O(n) edges).
(i) Let G = (V, E) be a planar graph with at least 3 vertices. Then
|E| ≤3|V |−6. Moreover, equality holds for any maximal planar
graph; that is, a planar graph such that adding any new edge
(while preserving the same vertex set) makes it nonplanar.
(ii) If, moreover, the considered planar graph contains no triangle
(i.e. K3 as a subgraph) and has at least 3 vertices, then |E| ≤
2|V | −4.
(We do not admit graphs with multiple edges in this proposition,
of course!)
Proof of (i). If the graph G is not maximal planar we keep adding
edges until it becomes maximal planar. Hence part (i) will be proved
as soon as we show that |E| = 3|V | −6 is true for any maximal
planar graph with at least 3 vertices.
As a ﬁrst step, we want to show that each face (including the
outer one) of a maximal planar graph with at least 3 vertices is a
triangle,9 i.e. it is bounded by a cycle of length 3.
If G is disconnected, we can clearly connect its two distinct com-
ponents by a new edge. If G is connected but not 2-connected, it
has some vertex v whose removal disconnects the graph, creating
components V1, V2, . . . , Vk, k ≥2 (note that we’re using the assump-
tion of G having at least 3 vertices at this moment!). Choose two
edges e and e′ connecting v to two distinct components Vi, Vj such
that e and e′ are drawn next to each other. Their endpoints can be
connected by a new edge ¯e without destroying planarity:
9For this reason, a maximal topological planar graph is also called a triangul-
ation.

6.3 Euler’s formula
201
Vi
Vj
¯e
e
e′
Hence a maximal planar graph with at least 3 vertices is necessarily
2-connected, and by Proposition 6.2.3, each face is bounded by a
cycle of the graph. For contradiction, assume that the bounding cycle
of some face F contains t ≥4 vertices v1, . . . , vt. If the vertex v1 is not
connected by an edge with the vertex v3 then we can draw the edge
{v1, v3} inside the face F. On the other hand, if {v1, v3} ∈E(G),
this edge must be drawn outside the face F, and therefore {v2, v4}
cannot be an edge, for otherwise {v1, v3} and {v2, v4} would have to
cross:
v1
v2
v3
v4
F
Hence we can safely draw the edge {v2, v4} inside the face F.
Thus, each face of a maximal planar graph is a triangle as was
asserted above. From this we get, similar to the proof of Proposi-
tion 6.3.2, the equality 3f = 2|E|, where f is the number of faces.
Expressing f from Euler’s formula and substituting into the just
derived equation, we obtain
|V | −|E| + 2
3|E| = 2.
The desired equality |E| = 3|V | −6 follows by a simple algebraic
manipulation. This proves part (i).
Proof of (ii). We proceed likewise. After adding some edges if neces-
sary, we may suppose that our graph is an edge-maximal triangle-free
planar graph, meaning that by adding any new edge we create a tri-
angle or make the graph nonplanar (or both). We can again assume
that the graph is connected.
If G is not (vertex) 2-connected then it has a vertex v whose
deletion splits G into components V1, . . . , Vk, k ≥2. Certainly we can
add some edge going between distinct components in such a way that
G remains planar, but for some edges we could introduce a triangle

202
Drawing graphs in the plane
(this happens in case we connect two vertices both adjacent to v),
and so we have to proceed more carefully. If each of the components
Vi consists of a single vertex then G is a tree and the formula being
proved holds for it. So let us suppose |V1| ≥2, and consider a face
F having both a vertex of V1 and a vertex of some other Vi on its
boundary, as in the picture:
v
v2
v1
Vi
V1
The component V1 must have at least one edge, {v1, v2}, on the
boundary of F, and since G has no triangle, it is not possible that
both v1 and v2 are adjacent to v. Hence v1 or v2 can be connected
to a vertex of Vi within F without possibly creating a triangle.
Therefore, we may assume that G is 2-connected. In this case,
each face is bounded by a cycle in G. Each such cycle has length at
least 4, and by double-counting we get 2|E| ≥4f this time. Using
Euler’s formula we ﬁnally arrive at |E| ≤2|V | −4.
2
A typical wrong proof. Suppose we want to prove that any topolog-
ical planar graph with n ≥3 vertices such that each face is bounded by
3 edges has 3n−6 edges. Students sometimes give an (incorrect) answer
of the following type. We proceed by induction on n. For n = 3 we have
a single triangle with 3 edges and 3 vertices, so the claim holds. Next, let
us assume the statement holds for any topological planar graph G with n
vertices. For any such G, we add a vertex into some face and connect it to
the 3 vertices of that face as in the following picture:
;
This yields a graph G′ with n + 1 vertices. The number of edges of G
is 3n −6 by the inductive hypothesis, and we have added 3 new edges.
Hence G′ has 3(n + 1) −6 edges and the statement holds for graphs
with n + 1 vertices as well.
Well, the problem is that not all possible
topological planar graphs G′ on n + 1 vertices with triangular faces can
arise from some G by the operation described. For instance, the graph
of the regular octahedron (drawn in Fig. 6.1) has all degrees 4, while

6.3 Euler’s formula
203
any graph produced from a smaller graph by the operation has at least
one vertex of degree 3.
Part (i) of Proposition 6.3.3 has an important and often applied
consequence, namely that every planar graph has some vertex of
degree at most 5. Similarly part (ii) guarantees that a planar graph
without triangles contains a vertex of degree no more than 3.
Part (i) also shows that K5 is not planar, because it has 10 edges,
while a planar graph on 5 vertices has at most 9 edges. Similarly
(ii) implies the nonplanarity of K3,3, since a triangle-free graph on 6
vertices has at most 8 edges.
We prove yet another proposition, giving us more information about
possible scores of planar graphs.
6.3.4 Proposition. Let G = (V, E) be a 2-connected planar graph
with at least 3 vertices. Let ni be the number of its vertices of deg-
ree i, and let fi be the number of faces (in some ﬁxed planar drawing)
bounded by cycles of length i. Then we have

i≥1
(6 −i)ni = 12 + 2

j≥3
(j −3)fj,
or, rewritten diﬀerently,
5n1 +4n2 +3n3 +2n4 +n5 −n7 −2n8 −· · · = 12+2f4 +4f5 +6f6 +· · · .
Hence 5n1 +4n2 +3n3 +2n4 +n5 ≥12, and so every planar graph with
at least 3 vertices contains at least 3 vertices of degree no larger than 5.
Proof. Clearly |V | = 
i ni, f = 
i fi. By substituting for |V | and f
from these equations into Euler’s formula we have
2|E| = 2(|V | + f −2) =

i
2ni +

j
2fj −4.
(6.1)
By a double-counting similar to that in previous proofs we obtain
further relations: 
i ini = 2|E| = 
j jfj. By expressing 2|E| using
(6.1), these equalities are transformed into

j
(j −2)fj + 4 =

i
2ni

j
2fj =

i
(i −2)ni + 4.
We multiply the ﬁrst of these equalities by 2 and we subtract the second
one from it. The result is

i
(6 −i)ni −4 = 2

j
(j −3)fj + 8.
This already gives the proposition.
2

204
Drawing graphs in the plane
Exercises
1. Prove that the bound |E| ≤2|V | −4 for triangle-free planar graphs is
the best possible in general. That is, for inﬁnitely many n construct
examples of triangle-free planar graphs with n vertices and 2n−4 edges.
2. (a) Show that a topological planar graph with n ≥3 vertices has at
most 2n −4 faces.
(b) Show that a topological planar graph without triangles has at most
n −2 faces.
3. Prove that a planar graph in which each vertex has degree at least 5
must have at least 12 vertices.
4. For which values of k can you prove the following statement? There
exists an n0 such that any planar graph on at least n0 vertices contains
at least k vertices of degree at most 5. Could it hold for every k?
5. ∗Consider a maximal triangle-free planar graph G = (V, E), i.e. a
triangle-free planar graph such that any graph of the form G + e,
where e ∈
V
2

\E, contains a triangle or is nonplanar. Prove that each
face in any drawing of such a graph is a quadrilateral or a pentagon.
6. Find an example, other than the Platonic solids, of a convex polytope
in the 3-dimensional space such that all faces are congruent copies of
the same regular convex polygon. ∗Can you list all possible examples?
7. (Game “Sprouts”) The following game has been invented by J. H.
Conway and M. S. Paterson. Initially, n dots are drawn on a sheet
of paper (the game is already interesting for small n, say for n = 5).
The players alternate their moves, and the player with no legal move
left loses. In each move, a player connects two dots with an arc and
draws a new dot somewhere on the newly drawn arc. A dot can be
used as an endpoint of a new arc only if there are at most 2 other arc
ends leading into that dot, and a new arc must not cross any other
arcs already drawn. (So at each moment we have a planar drawing of
a graph with maximum degree at most 3; the dot on the newly added
arc initially has degree 2.) An example:
0
1
2
3
4
(a) Prove that a game with n initial dots lasts no more than 3n −1
moves (for any strategy of the players).
(b) ∗Prove that a game with n initial dots lasts at least 2n moves (for
any strategy of the players).

6.3 Euler’s formula
205
(c) ∗(“Brussels sprouts”) We modify the game as follows. Instead of
dots we draw little crosses, and the ends of new arcs are connected to
the arms of the crosses (thus, the vertices can have maximum degree
4 this time). On each new arc, a new cross is drawn by crossing the
arc with a short segment, as in the following example:
0
1
2
3
4
5
6
7
8
Prove that this game always has exactly 5n −2 moves (and so it is
easy to determine who wins).
8. Consider a set L consisting of n lines in the plane. No two of them
are parallel but many can pass through a single point. By drawing
these lines we create vertices (intersection points of the lines), edges
(parts of the lines between intersections and semiinﬁnite rays extend-
ing from the ﬁrst and last intersections on a line to inﬁnity), and faces
(connected parts of the plane after removing the lines from it).
(a) ∗Express the number of edges in terms of the number of vertices
and faces.
(b) Prove that unless all the lines pass through a single point, there
exist at most n faces bounded by 2 (semiinﬁnite) edges.
(c) ∗Prove that unless all lines pass through a single point, there exists
at least one intersection with only 2 lines passing through it. (This is
the famous Sylvester’s problem.)
9. ∗∗Consider an arbitrary topological planar graph. Suppose that every
edge has one of the two colors red, blue. Show that there exists a vertex
of the following type:
blue edges
red edges
(red edges form a contiguous segment when going around the vertex,
and similarly for blue edges; one of the two groups can be empty).

206
Drawing graphs in the plane
Fig. 6.2 An example of a map for the four-color problem.
6.4
Coloring maps: the four-color problem
Consider a political map showing the boundaries of states as in
Fig. 6.2. Suppose that each state is a connected region (that’s why we
haven’t drawn islands like Britain, Ireland, Sardinia, Sicily, Corsica,
etc., on our schematic map, and we also had to leave out Russia—a
disconnected state in AD 1997!). We consider two regions neighbors
if they have at least a small piece of border in common, so it is not
suﬃcient if the boundaries touch in one or several points (but such
situations are very rare on maps anyway). We want to color each
state in such a map by some color, and no two neighboring states
should get the same color, as is usual on political maps. What is the
minimum necessary number of colors? For the map shown, 4 colors
are suﬃcient (try ﬁnding such a coloring!).
Here is one of most celebrated combinatorial problems:
6.4.1 Problem (Four-color problem). Can each planar map be
colored by at most 4 colors?

6.4 Coloring maps: the four-color problem
207
Four is certainly the minimum number of colors coming into con-
sideration for coloring all planar maps. This is illustrated by the
map shown above (consider Austria or Luxembourg, say) or by the
following examples:
Here we prove that any planar map can be colored by 5 colors. Al-
though this result has been known for more than 100 years, the four-
color problem was only solved (positively) in the late 1970s. Known
proofs of the fact that any planar map can be colored by 4 colors are
diﬃcult and they substantially depend on analyzing a large number
of cases by computer. So far nobody has managed to do the proof
by hand, although the original proof was greatly simpliﬁed in 1995
(Robertson et al. [43]). In 2004 Gonthier [37] managed to create a
proof fully veriﬁable by computer. He expressed the theorem in a
certain formal language, and in the same language he generated a
proof, whose correctness can be checked fully automatically, using
a simple relatively general-purpose program. Only the fact that the
formal statement truly expresses the desired theorem still has to be
checked by humans.
Some of the basic ideas of these complicated proofs appear in two
proofs of the “ﬁve-color theorem” given below.
The four-color problem looks like a geometric question, but it can
be reformulated in a purely combinatorial manner. Coloring regions
of a map can be translated to coloring vertices of a planar graph.
First we deﬁne the notion of coloring for an arbitrary graph.
6.4.2 Definition (Chromatic number of a graph).
Let G =
(V, E) be a graph, and let k be a natural number. A mapping c: V →
{1, 2, . . . , k} is called a coloring of the graph10 G if c(x) ̸= c(y) holds
for every edge {x, y} ∈E. The chromatic number of G, denoted by
10Sometimes this is called a proper coloring of G.

208
Drawing graphs in the plane
χ(G), is the minimum k such that there exists a coloring c: V (G) →
{1, 2, . . . , k}.
The chromatic number of a graph belongs among the most imp-
ortant combinatorial notions. However, in this book we mention it
only in this section.
Mathematically, a map can be regarded as a drawing of a planar
graph. The states on the map are faces of the graph, the vertices of
the graph are the points lying on borders of 3 or more states, and the
edges of the graph are the portions of the borders between vertices:
Note that multiple edges may arise in the graph corresponding to
a map.
In order to convert the map-coloring problem (i.e. coloring the
faces of a topological planar graph) to a problem of coloring graph
vertices in the sense of the deﬁnition just given, we introduce the
notion of the so-called dual graph. Imagine that we mark the capital
city of each state in the considered map by a dot, and that the
capitals of each two neighboring states are connected by a highway
lying in the territory of these two states and crossing their common
boundary. The vertices of the dual graph are the capitals and the
edges are the highways.
For deﬁning the dual graph formally, we need also multiple edges
and loops, so let us recall one possible way of introducing them. A
graph with multiple edges and loops can be represented as a triple
(V, E, ε), where V and E are disjoint sets and ε: E →
V
2

∪V is a
mapping assigning to each edge its two endpoints and to each loop
its single endpoint (see Section 4.4 for a more detailed discussion).
Now we can give a mathematical deﬁnition of the dual graph:

6.4 Coloring maps: the four-color problem
209
6.4.3 Definition (Dual graph).
Let G be a topological planar
graph, i.e. a planar graph (V, E) with a ﬁxed planar drawing. Let
F denote the set of faces of G. We deﬁne a graph, possibly with
loops and multiple edges, of the form (F, E, ε), where ε is deﬁned by
ε(e) = {Fi, Fj} whenever the edge e is a common boundary of the
faces Fi and Fj (we also permit Fi = Fj, in the case when the same
face lies on both sides of a given edge). This graph (F, E, ε) is called
the (geometric) dual of G, and it is denoted by G∗.
An example:
G
G∗
The dual graph G∗can be drawn together with the drawing of the
graph G, as was already suggested in the above informal explanation
with capitals and highways. We choose a point bF inside each face
F of G, and for each edge e of G we draw an arc crossing e and
connecting the points bF and bF ′, where F and F ′ are the faces
adjacent to the edge e. This arc lies completely in the faces F and F ′.
In this way, we obtain a planar drawing of G∗:
This way of drawing the dual graph witnesses its planarity. Further
examples of dual graphs can be found in Fig. 6.1. The graphs of the
cube and of the regular octahedron are dual to each other, similarly
the graphs of the regular dodecahedron and icosahedron are mutu-
ally dual, and ﬁnally the graph of the tetrahedron (i.e. K4) is dual
to itself.

210
Drawing graphs in the plane
Consider a planar map, and regard it as a drawing of a planar
graph G in the manner indicated above. The colorability of this map
by k colors is then equivalent to the colorability of the vertices of
the dual graph G∗by k colors.
On the other hand, any planar graph can be obtained as a sub-
graph of a suitable dual graph. We indicate a proof by a picture only:
for example, the graph
is contained in the dual of
(Alternatively, one can argue that any topological planar graph is
isomorphic to the dual of its dual.) Hence problems of map colorabil-
ity can be reformulated as problems concerning the colorability of
planar graphs. In particular, we can rephrase:
6.4.4 Problem (Four-color problem again). Does χ(G) ≤4 hold
for every planar graph G?
We prove a weaker result:
6.4.5 Proposition (Five-color theorem). Any planar graph G
satisﬁes χ(G) ≤5.
First proof. We proceed by induction on the number of vertices of
the graph G = (V, E). For |V | ≤5, the statement holds trivially.
By the results of Section 6.3 we know that any planar graph has
a vertex v of degree at most 5. If we even have degG(v) < 5 then
consider the graph G −v, and apply the inductive hypothesis on it.

6.4 Coloring maps: the four-color problem
211
Assuming that the graph G−v is colored by colors 1, 2, . . . , 5, then
we color the vertex v by some color i ∈{1, 2, . . . , 5} not occurring
among the (at most 4) colors used on the neighbors of v. In this way,
we get a coloring of G by 5 colors.
A very similar argument already shows that the chromatic number
of every planar graph is at most 6, so in the rest of the proof we work
on improving this 6 to 5.
Also, it may be instructive to formulate an algorithm for coloring
every planar graph by at most 6 colors (or, with the improvement below,
by at most 5 colors) based on this proof. Note that such an algorithm
colors the low-degree vertex v last, after all other vertices have been
colored!
It remains to investigate the case when degG(v) = 5. Let us con-
sider the graph G with some ﬁxed planar drawing, and let t, u, x,
z, y be the vertices connected to v by an edge, listed in the order
the corresponding edges emanate from the vertex v (in the clockwise
direction, say).
x
u
t
y
z
Let us again consider a coloring c: V (G −v) →{1, 2, . . . , 5} of
the graph G−v by 5 colors guaranteed by the inductive assumption.
If at most 4 colors occur at the neighbors of v then the vertex v
can be assigned a color distinct from the colors of all its neighbors.
Thus, we suppose that the neighbors of v have all the 5 distinct
colors. Consider the vertices x and y, and deﬁne Vx,y as the set of
all vertices of the graph G −v having color c(x) or c(y). Clearly x,
y ∈Vx,y. It may be that there exists a path from x to y in the graph
G −v using only the vertices of the set Vx,y, or such a path need not
exist. We distinguish these two cases.
1. No such path exists. Let V ′
x,y be the set of all the vertices s ∈
V (G −v) that can be reached from x by a path using only the
vertices from Vx,y. In particular, we have y ̸∈V ′
x,y. We deﬁne a
new coloring c′ of the graph G −v:

212
Drawing graphs in the plane
c′(s) =





c(s)
if s /∈V ′
x,y
c(y)
if s ∈V ′
x,y and c(s) = c(x)
c(x)
if s ∈V ′
x,y and c(s) = c(y)
(this means that we exchange the colors on the set V ′
x,y). It
is easy to see that c′ is a coloring of G −v again, and since
c′(x) = c′(y) = c(y), we can set c′(v) = c(x), obtaining a valid
coloring of G by 5 colors in this way.
2. Next, suppose that there exists a path P from x to y with all
vertices in Vx,y. In this case, we consider the pair of vertices t
and z and we deﬁne a set Vt,z as the set of vertices of G −v
colored by the colors c(t) and c(z). The sets Vx,y and Vt,z are
disjoint. The drawing of the path P forms, together with the
edges {v, x} and {v, y}, a cycle:
One of the points z, t lies inside this cycle and the other one
outside, and hence any path from z to t has to use some vertex
of the cycle. Hence there is no path from z to t only using vertices
of the set Vz,t, and so we can construct a coloring of G by 5 colors
in the same way as in case 1, only with z and t in the role of x
and y.
This ﬁnishes the ﬁrst proof of the ﬁve-color theorem.
2
Edge contraction. Before commencing a second proof, let us int-
roduce one more important graph operation. Let G = (V, E) be a
graph (not necessarily planar at this moment) and let e ∈E be one
of its edges. The contraction of e means that we “glue together” both
vertices of e into a single new vertex, and then we remove multiple
edges that may have arisen by this gluing. The resulting graph is
denoted by G.e, and formally it is deﬁned as follows:
G.e = (V ′, E′),

6.4 Coloring maps: the four-color problem
213
where e = {x, y} and
V ′ = (V \ {x, y}) ∪{z}
E′ = {e ∈E : e ∩{x, y} = ∅}
∪{{z, t}: t ∈V \ {x, y}, {t, x} ∈E or {t, y} ∈E};
here z ̸∈V denotes a new vertex.
Lemma. If G is a planar graph and e ∈E(G) is an edge then the
graph G.e is planar as well.
Informal proof. By a picture:
−→
e
x
y
z
2
Second proof of Proposition 6.4.5. We again work by induction
on the number of vertices of the considered planar graph G = (V, E).
We begin as in the ﬁrst proof. So we may assume that G = (V, E) is a
planar graph with at least 6 vertices in which each vertex has degree
at least 5. Let us choose a vertex v of degree 5. Since the graph G
is planar, it contains no K5 as a subgraph, and hence there exists
a pair of neighbors of v not connected by an edge. Let us denote
the vertices in some such pair by x and y, and let t, u, and z be
the remaining 3 neighbors. We look at the graph G′ produced from
G by contracting the edges {x, v} and {y, v} (i.e. the triple x, y, v
of vertices is replaced by a single new vertex w; this generalizes the
deﬁnition of contraction of a single edge in an obvious manner). This
graph is planar and has fewer vertices than G. Hence some coloring c′
of G′ by 5 colors exists by the inductive hypothesis. In this situation,
we deﬁne a coloring c of the graph G as follows:
c(s) =



c′(s)
if s ̸∈{x, y, v}
c′(w)
if s = x or s = y
i ∈{1, . . . , 5} \ {c′(w), c′(u), c′(t), c′(z)}
if s = v.

214
Drawing graphs in the plane
The picture below illustrates this deﬁnition:
w
1
2
5
3
−→
w
1
2
5
3
3
4
situation in G′
situation in G
It is not diﬃcult to check that the c thus deﬁned is a valid coloring
of the graph G.
2
Yet another proof is outlined in Exercise 13.
Let us remark that the question of colorability can also be studied
for graphs that can be drawn on other types of surfaces. This question
is completely solved by now: the maximum possible chromatic number
of a graph of genus k equals
+7 +
√
1 + 48k
2
,
.
It is remarkable that the case of genus 0, i.e. of planar graphs, is by far
the most diﬃcult one. For larger genus, this result (Heawood’s formula)
was proved much earlier than the four-color theorem.
Exercises
1. Prove that χ(G) ≤1 + max{degG(x): x ∈V } holds for every (ﬁnite)
graph G = (V, E).
2. For a graph G, put δ(G) = min{degG(v): v ∈V } (the minimum
degree of G). Prove χ(G) ≤1 + max{δ(G′): G′ ⊆G}, where G′ ⊆G
means that G′ is a subgraph of G.
3. ∗Call a graph G outerplanar if a drawing of G exists in which the
boundary of one of the faces contains all the vertices of G (we can
always assume the outer face has this property). Prove that every
outerplanar graph has chromatic number at most 3.
4. Let G be a planar graph containing no K3 as a subgraph. Prove
χ(G) ≤4. (A diﬃcult theorem due to Gr¨otsch asserts that, actually,
χ(G) ≤3 holds for all planar triangle-free graphs.)
5. CS Based on one of the proofs of the ﬁve-color theorem, design an
algorithm for coloring a given planar graph by at most 5 colors. Assume
that a planar drawing of the graph is given in the following form: for
each vertex of G, we have a circular list of the neighbors of v, listed in
the order of the corresponding outgoing edges in the drawing.

6.4 Coloring maps: the four-color problem
215
6. CS
(a) Consider a greedy algorithm for graph coloring. Pick one of the yet
uncolored vertices arbitrarily and color it with the smallest color (the
colors are natural numbers 1, 2, . . .) not used on any of its already col-
ored neighbors. For each number K, ﬁnd a graph G having a coloring
by 2 colors (i.e. bipartite) but such that the algorithm just described
sometimes colors G by at least K colors.
(b) Can you ﬁnd a planar G as the example in (a)?
(c) The algorithm can be made more sophisticated as follows: among
the yet uncolored vertices, pick one with the largest degree, and color
it as in (a). Show that graphs G as in (a) still exist.
(d) This can be continued as a game. Participants propose versions of
the greedy algorithm for graph coloring, and others try to ﬁnd graphs
for which the algorithm performs badly.
Remark. It is known that no polynomial-time algorithm can do appr-
oximate coloring very well, for instance to color all 3-colorable graphs
on n vertices by at most K colors for some constant K, unless poly-
nomial-time algorithms for exact coloring and many other diﬃcult
problems exist, which is considered very unlikely.
7. Consider a map M where each state has at most k connected regions
(a more realistic model of the world situation). Using Exercise 2, show
that the chromatic number of every such map is no larger than 6k (for
each state, we insist that all its regions be colored by the same color).
8. The following picture is an example of a map where each state has 2
regions:
11
2
4
6
10
8
2
12
5
7
10
11
1
3
12
3
6
5
8
1
4
7 9
9
Prove that the chromatic number of this map is 12.
9. Prove that each graph G has at least
χ(G)
2

edges.

216
Drawing graphs in the plane
10. (a) ∗Consider a planar graph G with all degrees even. Prove that the
map arising from (any) planar drawing of G can be colored by 2 colors.
(b) ∗Using (a), prove that there exists no topological planar graph
with all degrees even such that all the inner faces are triangles (i.e.
bounded by cycles of length 3) and the outer face is a pentagon.
11. ∗Let us say that a graph H is a minor of a graph G if a graph iso-
morphic to H can be obtained from G by a repeated application of
the following operations: deleting an edge, deleting an isolated vertex,
contracting an edge. Deduce the following result from Kuratowski’s
theorem: A graph G is planar if and only if neither K5 nor K3,3 is a
minor of G.
12. (List chromatic number) Let G = (V, E) be a graph, and suppose that
a ﬁnite list L(v) of natural numbers is given for each vertex v ∈V . A
mapping c: V →N is called a list coloring (with respect to the given
lists L(v), v ∈V ) if c(v) ∈L(v) holds for all v ∈V and c(v) ̸= c(v′)
whenever {v, v′} ∈E. The list chromatic number of G, denoted by
χℓ(G), is the smallest number k such that G has a list coloring for any
collection of lists L(v) with |L(v)| ≥k for all v ∈V .
(a) Find an example of a graph with χℓ(G) > χ(G).
(b) ∗For each integer k, construct a bipartite graph G with χℓ(G) > k.
Remark. There exists a planar graph G with χℓ(G) = 5.
13. ∗The notion of list coloring from the previous exercise is used in a short
and remarkable proof of the ﬁve-color theorem 6.4.5 due to Thomassen.
To reconstruct that proof, establish the following statement by induc-
tion on the number of vertices:
Let G = (V, E) be a topological planar graph and let L(v), v ∈V , be
lists with the following properties:
• The boundary of the unbounded face is a cycle C with vertices
v1, v2, . . . , vk (numbered along the circumference).
• All bounded faces are triangles.
• L(v1) = {1}, L(v2) = {2}.
• |L(vi)| = 3 for i = 3, 4, . . . , k.
• |L(v)| = 5 for all vertices distinct from the vi (i.e. lying in the
interior of the bounding cycle C).
Then G has a list coloring with respect to the lists L(v).
This proof uses neither Euler’s formula nor the existence of a low-
degree vertex, and it is an example of mathematical induction par
excellence.

7
Double-counting
In pre-computer ages, double-counting was used by accountants.
When adding up the numbers in a table, they ﬁrst found the sum of
the row totals and then they compared it to the sum of the column
totals. If their computation was correct, both results were the same.
Put mathematically, if A is an n × m matrix then
n

i=1
m

j=1
aij =
m

j=1
n

i=1
aij.
Pictorially,
=
In other words, the order of summation in a double sum like this may
be changed. This simple idea underlies many mathematical tricks
and proofs; in more complex cases it is accompanied by other ideas,
of course. The main diﬃculty is usually in ﬁguring out what exactly
should be double-counted.
7.1
Parity arguments
In Section 4.3, we encountered the handshake lemma: Any graph
has an even number of odd-degree vertices. The proof was in fact a
typical double-counting (we double-counted “ends of edges”). Using
this claim, we were able to exclude some vectors as possible graph
scores—for instance, the vector (3, 3, 3, 3, 3). But there are much
more interesting ways to use the handshake lemma. For instance,
we can sometimes prove the existence of a certain object. To this
end, we reformulate the claim slightly: If we know that a graph G
has at least one vertex of an odd degree, then it must have at least
two such vertices. Next, we demonstrate a nice application.

218
Double-counting
Let us draw a big triangle in the plane with vertices A1, A2, A3.
We divide it arbitrarily into a ﬁnite number of smaller triangles, as
in the following picture:
A3
A1
A2
3
3
2
2
2
2
2
3
1
1
1
1
1
2
1
2
1
No triangle may have a vertex inside an edge of any other small tri-
angle, so that if we consider the resulting picture as a drawing of a
planar graph, all inner faces are triangular. Let us assign the labels
1, 2, 3 to the vertices of the big and the small triangles, under the
following rules: the vertex Ai gets the label i, i = 1, 2, 3, and all ver-
tices lying on the edge AiAj of the big triangle may be assigned only
the label i or j. Otherwise the assignment is completely arbitrary.
7.1.1 Proposition (Sperner’s lemma—a planar version).
In
the situation described above, a small triangle always exists whose
vertices are assigned all the three labels 1, 2, 3.
Proof.
We deﬁne an auxiliary graph G; see Fig. 7.1. Its vertices
are the faces of our triangulation, i.e. all small triangles plus the
outer face. In the ﬁgure, the vertices are depicted as little black
triangles inside the corresponding faces. The vertex for the outer face
is denoted by v. Two vertices, i.e. faces of the original drawing, are
joined by an edge in G if they are neighboring faces and the endpoints
of their common edge have labels 1 and 2. This also concerns the
outer face vertex v: it is connected to all small triangles adjacent to
the circumference of the big triangle by a side labeled 12.
A small triangle can be connected to some of its neighbors in
this graph G only if one of its vertices is labeled by 1 and another
by 2. If the remaining vertex is labeled 1 or 2, the considered small

7.1 Parity arguments
219
A3
A1
A2
3
3
2
2
2
2
2
3
1
1
1
1
1
2
1
2
v
1
Fig. 7.1 Illustration for the proof of Sperner’s lemma.
triangle is adjacent to exactly two of its neighbors. If the remaining
vertex has label 3, then the considered triangle is adjacent to exactly
one neighbor, and this is the only case where the degree of a small
triangle in the graph G is odd. We now show that the vertex v (the
outer face) has an odd degree in G. Then, by the handshake lemma,
there exists at least one other vertex of odd degree in G, and this is
the desired small triangle labeled 1, 2, 3.
The edges of the graph G incident to v can obviously only cross
the side A1A2 of the big triangle. By the rules of the labeling, this
side only contains vertices labeled by 1 or 2. Let us write down
the sequence of these labels, starting at A1 and ending at A2. The
number of neighbors of v is just the number of alterations between 1
and 2 in this sequence (the number of times a segment of 1s ends and
a segment of 2s starts or the other way round). Since the sequence
begins with 1 and ends with 2, the number of such alterations must
be odd. Hence v has an odd degree in G.
2
Sperner’s lemma is not a toy problem only; it is a crucial step in the
proof of a famous theorem. Before we state this theorem, let us mention
a simpler theorem of a similar kind as a warm-up.
7.1.2 Proposition (One-dimensional fixed point theorem). For
any continuous function f : [0, 1] →[0, 1], there exists a point x ∈[0, 1]
such that f(x) = x.
Such an x is called a ﬁxed point of the function f. The proposition
can be proved by considering the function g(x) = f(x) −x. This is a

220
Double-counting
continuous function with g(0) ≥0 and g(1) ≤0. Intuitively it is quite
clear that the graph of such a continuous function cannot jump across
the x-axis and therefore it has to intersect it, and hence g is 0 at some
point of [0, 1]. Proving the existence of such a point rigorously requires
quite some work. In analysis, this result appears under the heading
“Darboux theorem”.
Fixed point theorems generally state that, under certain circum-
stances, some function f must have a ﬁxed point, i.e. there exists
an x such that f(x) = x. Such theorems belong to the key results
in many areas of mathematics. They often serve as a tool for prov-
ing the existence of solutions to equations of various types (diﬀeren-
tial equations, integral equations, etc.). They even play a role in the
theory of the meaning of computer programs, the so-called program
semantics.
In Brouwer’s ﬁxed point theorem, the 1-dimensional interval from
Proposition 7.1.2 is replaced by a triangle in the plane, or by a tetrahe-
dron in the 3-dimensional space, or by their analogs in higher dimensions
(simplices). Here we prove only the 2-dimensional version since we have
only proved Sperner’s lemma in 2 dimensions (but see Exercise 5). The
proof belongs more to mathematical analysis. However, we will try to
present it using a minimum of facts and notions from analysis, and we
will recall the necessary facts as we go along.
Let ∆denote a triangle in the plane. For simplicity, let us take the
triangle with vertices A1 = (1, 0), A2 = (0, 1), and A3 = (0, 0):
A3 = (0, 0)
A1 = (1, 0)
A2 = (0, 1)
A function f : ∆→∆is called continuous if for every a ∈∆and for
every ε > 0 there exists δ > 0 such that if b ∈∆is a point at distance
at most δ from a then the distance of f(a) and f(b) is at most ε. Brieﬂy,
f maps close points to close points.
7.1.3 Theorem (Planar Brouwer’s fixed point theorem).
Every
continuous function f : ∆→∆has a ﬁxed point.
Proof. We deﬁne three auxiliary real-valued functions β1, β2, and β3
on the triangle ∆. For a point a ∈∆with coordinates (x, y), we set
β1(a) = x,
β2(a) = y,
β3(a) = 1 −x −y.
Geometrically, the βi are as in the following illustration:

7.1 Parity arguments
221
A3
A1
A2
a
1
√
2β3(a)
β2(a)
β1(a)
The key properties of these functions are: βi(a) ≥0 and β1(a)+β2(a)+
β3(a) = 1 for all a ∈∆.
Further we deﬁne sets M1, M2, M3 ⊆∆:
Mi = {a ∈∆: βi(a) ≥βi(f(a))},
i = 1, 2, 3. Thus, Mi consists of the points that are not moved farther
apart from the side opposite to Ai by the function f.
Note that every point p ∈M1 ∩M2 ∩M3 is a ﬁxed point of the
function f, for if p is not ﬁxed then f must move it away from some of
the sides. In more detail, if p ∈M1 ∩M2 ∩M3 then we have βi(p) ≥
βi(f(p)) for all i = 1, 2, 3, and since 
i βi(p) = 
i βi(f(p)) = 1 we get
βi(p) = βi(f(p)) for all i, which implies p = f(p). Our goal now is to
ﬁnd a point in the intersection M1 ∩M2 ∩M3.
Consider a sequence of successively reﬁning triangulations of the
triangle ∆:
. . .
In each of these triangulations, we label all vertices of the triangles by
1, 2, or 3. We require that a vertex labeled i belongs to the set Mi, and,
moreover, that the assignment satisﬁes the rules of Sperner’s lemma.
We have to make sure this can always be arranged.
The vertex A1 has the largest possible distance from its opposite
side; hence this distance cannot increase under f. Therefore A1 ∈M1
and we can label A1 by 1; similarly for A2 and A3. A point a lying on
the side A1A2 has β1(a) + β2(a) = 1, which implies that f(a) cannot
satisfy both β1(f(a)) > β1(a) and β2(f(a)) > β2(a). Thus a ∈M1∪M2,
and so we can label all vertices on the side A1A2 only by 1s and 2s.
A similar argument works for the other sides. Finally, each point of ∆
belongs to at least one of the sets Mi since it cannot be moved farther
from all three sides at once. (This time we leave a detailed veriﬁcation
to the reader.)

222
Double-counting
Now Sperner’s lemma 7.1.1 implies that each of the successively
reﬁning triangulations has a triangle labeled 1, 2, 3. Let us denote the
vertices of some such triangle in the jth triangulation by aj,1, aj,2, and
aj,3 in such a way that aj,i belongs to Mi for i = 1, 2, 3.
Consider the inﬁnite sequence of points (a1,1, a2,1, a3,1, . . .). We need
to choose an inﬁnite convergent subsequence from it. This is always pos-
sible; in fact, any inﬁnite sequence of points inside the triangle contains a
convergent inﬁnite subsequence. (This property of the triangle—shared,
for instance, by any closed and bounded subset of the plane—is called
the compactness.) So suppose that we have chosen a convergent subse-
quence (aj1,1, aj2,1, aj3,1, . . .), j1 < j2 < j3 < . . ., and let us denote its
limit point by p.
We claim that p ∈M1. Indeed, by the deﬁnition of M1, we have
β1(ajk,1) ≥β1(f(ajk,1)) for all jk, and taking a limit on both sides yields
β1(p) ≥β1(f(p)), because taking a limit preserves nonstrict inequalities
between continuous functions.
Since the diameter of triangles in the successive triangulations tends
to 0, the sequences of the other vertices, i.e. (aj1,2, aj2,2, aj3,2, . . .) and
(aj1,3, aj2,3, aj3,3, . . .), also converge to the point p. This implies that
p ∈M2 and p ∈M3 as well. Thus p is the desired ﬁxed point of the
function f.
2
A number of more complicated results similar to Brouwer’s ﬁxed
point theorem are known. For example, under any continuous mapping
of the surface of a 3-dimensional ball to the plane, some two points at
opposite ends of a diameter of the ball are mapped to the same point
(the so-called Borsuk–Ulam theorem). Such theorems are proved in a
branch of mathematics called algebraic topology. The proofs typically
employ fairly complex techniques, but deep down they are often based
on parity arguments similar to Sperner’s lemma.
Let us present another example illustrating the use of the hand-
shake lemma. We will analyze a game similar to the game called
HEX. It takes place on a board like the one shown in Fig. 7.2 (but
the triangulation inside the outer square may be arbitrary). The
players take turns. Each player in turn marks an unmarked node
with her symbol. For example, the ﬁrst player (Alice) paints nodes
gray and the second player (Betty) black (boring colors but we can
illustrate them in a black-and-white book). In the starting position,
Alice has the nodes a and c marked, and Betty has b and d. Alice
wins if she manages to mark all nodes of a path from a to c, and
Betty’s goal is a path from b to d. If a player is supposed to make a
move and has no more nodes to mark, the game ends in a draw.

7.1 Parity arguments
223
a
b
c
d
Fig. 7.2 A game board.
7.1.4 Proposition. On a board of the given type (the outer face is
a square, all inner faces are triangles), a draw is impossible.
Proof. Assume for contradiction that a draw has occurred. Let A
be the set of nodes marked by Alice and let B be the set of nodes
marked by Betty.
Let us assign labels 1, 2, 3 to nodes according to the following
rules. A node in A is labeled by 1 if it can be connected to a by a
path with all vertices belonging to A. Similarly, nodes in B connected
to b by a path lying entirely in B are labeled by 2. The remaining
nodes get label 3. By the hypothesis, both c and d are labeled by 3,
otherwise one of the players would have won.

224
Double-counting
We will show that there is an inner triangular face T labeled 1,
2, 3. This leads to a contradiction, since the node of T labeled 3 (call
it x) can belong neither to A nor to B. Indeed, if x belongs to A,
we consider the node y of the triangle T labeled by 1. By deﬁnition
of the labeling, there is a path from a to y using only nodes of A,
and this path could be extended to x, since y is adjacent to x. For
a similar reason x does not belong to B, which is a contradiction.
This reasoning is illustrated by the following picture:
a
b
?
1
1
1
1
2
2
2
2
3
How do we prove the existence of a triangle labeled 1, 2, 3? Exa-
ctly like Sperner’s lemma. We leave it to the reader. Another very
similar proposition is presented in Exercise 1.
2
Let us remark that this proof makes substantial use of the fact
that all inner faces of the board are triangular and the outer face has
just the 4 vertices a, b, c, d. If we allow, for instance, quadrilateral
inner faces, the game might possibly end in a draw.
Various games from a mathematical point of view are investigated
in the interesting book by Berlekamp, Conway, and Guy [14].
Exercises
1. Consider a drawing of a planar graph all of whose faces, including
the outer one, are triangular (i.e. have 3 vertices). To each vertex we
assign, quite arbitrarily, one of the labels 1, 2, 3. Prove that there are
an even number of faces whose vertices get all 3 labels.
2. A well-known problem about a tourist climbing a mountain also rel-
ates to ﬁxed points. A tourist starts climbing a mountain at 6 in the
morning. He reaches the summit at 6pm and spends the night there
(in a shelter built there especially for that purpose). At 6 the next
morning he starts descending along the same trail. He often pauses to
contemplate the view, and so he reaches the starting point at 6 in the
evening again. Prove that there is a place on the trail that he passed
through at the same time on both days.
3. A building engineer is standing in a freshly ﬁnished apartment and
holding a ﬂoor plan of the same apartment. Prove that some point in

7.1 Parity arguments
225
the plan is positioned exactly above the point on the apartment’s ﬂoor
it corresponds to.
4. Decide for which of the various sets X listed below is an analog of
Brouwer’s ﬁxed point theorem 7.1.3 valid (i.e. every continuous func-
tion f : X →X has a ﬁxed point). If it is valid, derive it from Theo-
rem 7.1.3, and if not, describe a function f witnessing it.
(a) X is a circle in the plane (we mean the curve, not the disk bounded
by it);
(b) X is a circular disk in the plane;
(c) X is a triangle in the plane with one interior point removed;
(d) X is a sphere in the 3-dimensional space (a surface);
(e) X is a sphere in the 3-dimensional space with a small circular hole
punctured in it;
(f) X is the torus (see Section 6.1);
(g) X is the Klein bottle (see Section 6.1).
5. (Sperner’s lemma in dimension 3)
(a) ∗Consider a tetrahedron T = A1A2A3A4 in the 3-dimensional space
and some subdivision of T into small tetrahedra, such that each face
of each small tetrahedron either lies on a face of the big tetrahedron or
is also a face of another small tetrahedron. Let us label the vertices of
the small tetrahedra by labels 1, 2, 3, 4, in such a way that the vertex
Ai gets i, the edge AiAj only contains vertices labeled i and j, and
the face AiAjAk has only labels i, j, and k. Prove that there exists a
small tetrahedron labeled 1, 2, 3, 4.
(b) Formulate and prove a 3-dimensional version of Brouwer’s ﬁxed
point theorem (about continuous mappings of a tetrahedron into
itself).
6. Consider a game as in Proposition 7.1.4.
(a) Prove that on any board meeting the condition of Proposition 7.1.4,
either Alice or Betty has a winning strategy (i.e. if she does not make
a mistake she wins, regardless of how the other player plays).
(b) Find an example of a game board (meeting the condition again)
such that Betty has a winning strategy.
(c) ∗Show that if the board is symmetric with respect to rotation by
90 degrees about the center (i.e. if it looks the same from both players’
points of view), then Alice always has a winning strategy.

226
Double-counting
7. The real game HEX, as the reader might know, was invented by Piet
Hein and it is played on a board like this:
Alice’s side
Betty’s side
Alice’s side
Betty’s side
Alice’s goal is to connect her two sides by a contiguous chain of ﬁelds
occupied by her pieces, and Betty wants to connect her sides.
(a) Discover and explain the connection of this game to the version
discussed in the text.
(b) Having no HEX game board at hand, Alice and Betty started
playing HEX on an ordinary chessboard. Alice tried to connect two
opposite sides and Betty the other two sides, with two squares consid-
ered adjacent if they share a side. Soon they found the game pretty
boring. Can you guess why?
7.2
Sperner’s theorem on independent systems
Yes, this is the second time that we meet the name Sperner in this
chapter, but Sperner’s theorem, which we consider next, deals with
something quite diﬀerent than Sperner’s lemma of the previous sec-
tion. It is about an n-element set X and a system M of its subsets.
We call the system M independent if it contains no two diﬀerent sets
A, B such that A ⊂B. Before reading further, you may want to try
ﬁnding an independent set system on a 4-element set with as many
sets as possible. How many sets can you get?
7.2.1 Theorem (Sperner’s theorem). Any independent system
of subsets of an n-element set contains at most

n
⌊n/2⌋

sets.
This is in fact a theorem on partially ordered sets (posets). Con-
sider the set system 2X consisting of all subsets of the set X. The
relation ⊆, “to be a subset of”, is a partial ordering on 2X (it is
even one of the most important examples of a partial ordering; see
Chapter 2). An independent system of sets is exactly a set of pair-
wise incomparable elements in the poset (2X, ⊆). A set of pairwise

7.2 Sperner’s theorem on independent systems
227
incomparable elements in a poset is commonly called an antichain,
and so Sperner’s theorem gives an upper bound on the size of any
antichain in (2X, ⊆).
Before we start proving Sperner’s theorem, we should remark
that the upper bound in that theorem is certainly the best possible,
because all subsets of X of size ⌊n/2⌋constitute an independent
system of size exactly

n
⌊n/2⌋

.
Proof of Theorem 7.2.1.
We ﬁrst say what a chain of subsets
of X is: it is any set {A1, A2, . . . , Ak} of subsets of X such that
A1 ⊂A2 ⊂· · · ⊂Ak. In the language of ordered sets, it is simply a
linearly ordered subset of the poset (2X, ⊆).
The key observation is that any chain has at most one element in
common with any antichain. For example, if we succeeded in proving
that the whole poset in question can be expressed as a union of at
most r chains, then no antichain would have more than r elements.
Our proof uses this simple observation in a more sophisticated way,
however.
We consider the maximal chains in (2X, ⊆), where a maximal
chain is a chain such that if we add to it any other set of 2X, the
result is no longer a chain. It is easy to see what the maximal chains
look like: they contain one subset of X of each of the possible sizes;
that is, they have the form
∅⊂{x1} ⊂{x1, x2} ⊂{x1, x2, x3} ⊂· · · ⊂{x1, x2, . . . , xn},
(7.1)
where x1, x2, . . . , xn are all elements of X written out in some arbi-
trary order. Every maximal chain therefore induces a linear order-
ing of elements of X, and, on the other hand, every linear ordering
yields exactly one maximal chain. As a result, the number of maxi-
mal chains equals the number of permutations of X, i.e. n!.
Let M be an antichain (an independent system of subsets). Form
all ordered pairs (R, M), where M ∈M is a set and R is a maximal
chain containing M. We count such pairs in two ways.
First, by the observation mentioned above, every chain contains
at most one M ∈M (because M is an antichain), so the number of
pairs (R, M) is less than or equal to the number of maximal chains,
which is n!.
On the other hand, we can take a set M ∈M and ask how
many maximal chains contain it. A maximal chain of the form (7.1)
contains M if and only if {x1, x2, . . . , xk} = M, where k = |M|.

228
Double-counting
Hence we ask how many linear orderings of X there are such that
the ﬁrst k elements are just the elements of M. We can still order
the elements of M in k! ways, thus determining the ﬁrst k sets of the
chain, and the elements outside M can be ordered in (n −k)! ways,
which determines the rest of the chain. Altogether M is contained in
k!(n −k)! maximal chains. So the number of ordered pairs (R, M)
is equal to

M∈M
|M|!(n −|M|)!,
while according to the ﬁrst way of counting, it is at most n!. Dividing
the resulting inequality by n!, we obtain

M∈M
|M|!(n −|M|)!
n!
=

M∈M
1
 n
|M|
 ≤1.
(7.2)
We use the fact that

n
⌊n/2⌋

is at least as large as any binomial
coeﬃcient of the form
n
k

, k = 0, 1, . . . , n. Therefore
1 ≥

M∈M
1
 n
|M|
 ≥|M|
1

n
⌊n/2⌋
,
and hence |M| ≤

n
⌊n/2⌋

.
2
The remarkable inequality (7.2) is called the LYM inequality, after
its (independent) discoverers Lubell, Meshalkin, and Yamamoto.
Another proof of Sperner’s theorem. As with many other impor-
tant theorems, Sperner’s theorem can be proved in several essentially
diﬀerent ways. From each of the various proofs one can learn something
new, or just enjoy the beautiful ideas. We will describe two more proof
methods. The ﬁrst one cleverly covers 2X with chains of a special type.
Let us consider a chain in the poset (2X, ⊆), i.e. a sequence of sets
in inclusion: M1 ⊂M2 ⊂· · · ⊂Mt. Call such a chain symmetric if it
contains one set of size k, one set of size k + 1, one set of size k + 2, . . .,
one set of size n−k, and no other sets (for some number k). For example,
for n = 3, the chain consisting of the sets {2} and {2, 3} is symmetric,
as well as the chain {∅, {3}, {2, 3}, {1, 2, 3}}, but the chains {{1}} and
{∅, {1, 2, 3}} are not symmetric in this sense. A partition into symmetric
chains is a way of expressing 2X as a union of several disjoint symmetric
chains.
Any partition into symmetric chains (if it exists at all) has to con-
sist of exactly

n
⌊n/2⌋

symmetric chains, because each symmetric chain

7.2 Sperner’s theorem on independent systems
229
contains exactly one set of size ⌊n/2⌋. Each chain has at most one set in
common with any independent set system (this was the basic observa-
tion in the ﬁrst proof of Sperner’s theorem). Hence Sperner’s theorem
is a consequence of the following:
Claim. For any ﬁnite set X, the system 2X has a partition into sym-
metric chains.
Proof of the claim. We may assume that X = {1, 2, . . . , n}. The proof
is based on the following construction:
To each set M ⊆X, we assign a sequence “m1m2 . . . mn” consisting
of left and right parentheses by the rule
mi =

“(”
if i ∈M
“)”
if i ̸∈M.
For example, for n = 7 and the set M = {2, 6}, we get the sequence
“m1m2 . . . m7”= “)()))()”. The resulting sequence of parentheses is
quite general, and certainly it need not be a “correct” parenthesizing,
i.e. the left and right parentheses need not match. We can get a “partial
pairing of parentheses” from it, however. First, we pair up all pairs “()”
of adjacent parentheses. Then we ignore these already paired parenthe-
ses, and we continue pairing up the remaining parentheses according to
the same rule. Here are two examples:
) ( )
 ) ) ( )

) ) ) ( ( ( )
 ( )
 )



(
After ﬁnishing this pairing procedure, some parentheses may remain
unmatched. But the rule of the pairing implies that the sequence of the
remaining unmatched parentheses has only closing parentheses at the
beginning and then, from some position on, only opening parentheses.
We say that two sequences of parentheses have the same partial
pairing if the paired parentheses are the same in both sequences (also
in the same positions). This is the case for the sequences corresponding
to the 3 sets below (the sets are regarded as subsets of {1, 2, . . . , 11}):
M1 = {4, 5, 6, 8, 11}
. . .
) ) ) ( ( ( ) ( ) ) (
M2 = {5, 6, 8, 11}
. . .
) ) ) ) ( ( ) ( ) ) (
M3 = {5, 6, 8}
. . .
) ) ) ) ( ( ) ( ) ) )
The only way two sequences with the same partial pairing may diﬀer
is that one has either more unmatched right parentheses on the left,
or more unmatched left parentheses on the right, than the other. From
this it is easy to see that two sets with the same partial pairing of the

230
Double-counting
corresponding sequences of parentheses have to be in inclusion (one is
a subset of the other).
We now deﬁne an equivalence ∼on the set 2X, by letting M ∼M ′
hold if and only if both M and M ′ have the same partial pairing of their
sequences. We claim that each class of this equivalence is a symmetric
chain. We leave the veriﬁcation as an easy exercise. So we have proved
Sperner’s theorem once more.
2
Finally we demonstrate one more proof of Sperner’s theorem. It is
remarkably diﬀerent from the previous two proofs, and it uses the highly
symmetric structure of the poset (2X, ⊆).
We begin with a general deﬁnition (which has already been men-
tioned in Exercise 2.1.4). Let (X, ≤) and (Y, ⪯) be some posets. A
mapping f : X →Y is called an isomorphism of posets if f is a bi-
jection and for any two elements x, y ∈X, x ≤y holds if and only
if f(x) ⪯f(y). An isomorphism of a poset (X, ≤) onto itself is called
an automorphism of (X, ≤). An automorphism preserves all properties
which can be deﬁned in terms of the ordering relation ≤. For example,
x is the largest element of some subset A ⊆X if and only if f(x) is the
largest element of the set f(A), and so on.
Third proof of Sperner’s theorem. Let X be a given n-element set.
Each permutation f : X →X induces a mapping f # : 2X →2X (i.e.
sending subsets of X to subsets of X) given by f #(A) = {f(x): x ∈A}.
It is clear that f # is a bijection1 2X →2X, and even an automorphism
of the poset (2X, ⊆).
Let us now consider a system M of subsets of the set X. For each
permutation f of the set X, we get the set system {f #(M): M ∈M};
that is, the system of images of the sets from M under the mapping
f #. In this way, we have deﬁned a new mapping
f ## : 22X →22X
(assigning set systems to set systems) by the formula
f ##(M) = {f #(M): M ∈M}.
The mapping f ## is again a bijection.
1In Section 1.4, we adopted a convention according to which one can write
the image of a set under some mapping in the same way as for the image of
an element. That is, in our case we could write the set {f(x): x ∈A} simply
as f(A). In the present proof, however, it is better to distinguish more exactly
between the image of an element and the image of a set. That is why we have
introduced a diﬀerent symbol, f #, for the mapping of sets.

7.2 Sperner’s theorem on independent systems
231
We introduce a relation  on the set of all set systems on X (i.e. on
the set 22X):
M  N
⇔
for each M ∈M there exists an N ∈N with M ⊆N.
Note that the relation  is something diﬀerent than the inclusion be-
tween set systems. It is a possibly larger relation than inclusion (hence
M ⊆N implies M  N). The reader is invited to check that the rela-
tion  is reﬂexive and transitive, but that it need not be antisymmetric
(ﬁnd an example on a 3-element set X).
Let the letter Ξ stand for the set of all independent set systems on
the set X (so Ξ ⊂22X). We claim that the relation  restricted to Ξ
is already antisymmetric, and consequently it is a partial ordering on
Ξ. Indeed, if M and N are independent systems of sets such that both
MN and N M, we consider an arbitrary set M ∈M. The system N
has to contain some set M ′ ⊇M, and then M contains some M ′′ ⊇M ′
too. Thus we get M, M ′′ ∈M with M ⊆M ′′, and by the independence
of M it follows that M = M ′′ = M ′, and hence M ∈N. This shows
that M ⊆N, and symmetrically we obtain N ⊆M, whence M = N.
Thus (Ξ, ) is an ordered set.
Further we claim that for any permutation f, the mapping f ## is
an automorphism of the poset (Ξ, )—we leave the veriﬁcation to the
reader (this is a good way to a real understanding of the notions like Ξ
and f ##).
The proof of Sperner’s theorem is based on the following lemma:
Lemma.
Let Ξ0 ⊆Ξ denote the set of the independent set systems
with the largest possible number of sets. The set Ξ0 has a largest element
N0 with respect to the ordering by . This means M  N0 for all
M ∈Ξ0.
Proof of the lemma. Since there are only ﬁnitely many set systems on
X, it suﬃces to prove that for any two set systems M, M′ ∈Ξ0, there
exists a set system N ∈Ξ0 that is larger than both M and M′, i.e.
M  N and M′  N.
So we consider some M, M′ ∈Ξ0, and we form a new set system
¯
M = M ∪M′. Since both M and M′ are independent, the longest
chain in
¯
M, with respect to the ordering of
¯
M by inclusion, has at
most two sets. Next, let
¯
Mmin be the system of all sets from
¯
M for
which
¯
M contains no proper subset, or in other words the system of
all sets of
¯
M minimal with respect to inclusion. Similarly we introduce
the system ¯
Mmax as the system of all inclusion maximal sets of ¯
M. We
want to check that the system N =
¯
Mmax belongs to Ξ0 and satisﬁes
both M  ¯
Mmax and M′  ¯
Mmax.
Both the systems
¯
Mmin and
¯
Mmax are independent, and we have
¯
M =
¯
Mmin ∪¯
Mmax. Clearly also
¯
Mmax  M and
¯
Mmax  M′. It

232
Double-counting
remains to verify that
¯
Mmax has the largest possible number of sets,
i.e. | ¯
Mmax| = |M|. Let us note that by the independence of M and M′,
we get M ∩M′ ⊆¯
Mmin ∩¯
Mmax (check!). Hence | ¯
Mmin| + | ¯
Mmax| =
| ¯
Mmin∪¯
Mmax|+| ¯
Mmin∩¯
Mmax| ≥|M∪M′|+|M∩M′| = |M|+|M′|,
and so if we had | ¯
Mmax| < |M| = |M′| then we would get | ¯
Mmin| >
|M| and the systems M and M′ would not have the maximum possible
size. This proves the lemma.
It remains to ﬁnish the third proof of Sperner’s theorem. Let us con-
sider the largest element N0 of the set (Ξ0, ). For each permutation
f of the set X, the corresponding induced automorphism f ## maps
the set Ξ0 (independent systems of maximum size) onto itself, and so
it must map its unique largest element N0 onto itself: f ##(N0) = N0.
This implies, however, that if N0 contains at least one k-element set
then it already contains all k-element sets! In other words,
X
k

⊆
N0. It is impossible to add any set to the system
X
k

so that it re-
mains independent, and thus N0 =
X
k

. The maximality of the bi-
nomial coeﬃcients

n
⌊n/2⌋

and

n
⌈n/2⌉

then implies that N0
=

X
⌈n/2⌉

.
2
Exercises
1. Let us call a system N of subsets of X semiindependent if it contains
no three sets A, B, C such that A ⊂B ⊂C.
(a) Show by a method similar to the ﬁrst proof of Sperner’s theorem
that |N| ≤2

n
⌊n/2⌋

, where n = |X|.
(b) Show that for odd n, the estimate from (a) cannot be improved.
2. (a) Determine the number of maximal chains in the set {1, 2, . . . , 10!}
ordered by the divisibility relation.
(b) Count the number of maximal antichains in the set {1, 2, . . . , 5!}
ordered by the divisibility relation.
3. ∗Show that the set systems

X
⌊n/2⌋

and

X
⌈n/2⌉

are the only indepen-
dent set systems on an n-element set X with the largest possible num-
ber of sets.
4. Determine the number of automorphisms of the poset (2X, ⊆).
5. By modifying the third proof of Sperner’s theorem, show that for any
ﬁnite poset (P, ≤) there exists an antichain of maximum possible size
that is mapped to itself by all automorphisms of (P, ≤) (i.e. it is a
“ﬁxed point” of all automorphisms).

7.3 An extremal problem: forbidden four-cycles
233
6. Let a1, a2, . . . , an be real numbers with |ai| ≥1. Let p(a1, . . . , an) be
the number of vectors (ε1, ε2, . . . , εn), where εi = ±1, such that
−1 <
n

i=1
εiai < 1.
(a) ∗Prove that for any a1, a2, . . . , an we have p(a1, . . . , an) ≤

n
⌊n/2⌋

.
(This is one of the ﬁrst applications of Sperner’s theorem—the so-
called Littlewood–Oﬀord problem.)
(b) Find a1, a2, . . . , an with p(a1, . . . , an) =

n
⌊n/2⌋

.
7. Let n be a natural number that is not divisible by the square of any
integer greater than 1. Determine the maximum possible size of a set
of divisors of n such that no divisor in this set divides another (i.e.
max |M|, where x ∈M ⇒x|n and x, y ∈M, x ̸= y ⇒x doesn’t divide
y).
7.3
An extremal problem: forbidden four-cycles
In Section 4.7 we investigated the maximum possible number of edges of
a graph on n vertices that contains no triangle as a subgraph. Here we
are going to inverstigate a rather similar-looking question: what is the
maximum possible number of edges of an n-vertex graph that contains
no subgraph isomorphic to K2,2 (in other words, a cycle of length 4)?
A subgraph is considered in the non-induced sense here, and so for
example, the complete graph K4 on 4 vertices does contain a K2,2.
One might expect that forbidding a K2,2 will have an eﬀect similar to
forbiding a K3, but surprisingly, the answers are principially diﬀerent. A
graph containing no triangle can have as many as ⌊n2/4⌋edges, which
is approximately half of
n
2

(this is the maximum possible number
of edges of a graph on n vertices with no restrictions whatsoever). In
contrast to this, forbidding K2,2 makes the maximum number of edges
much smaller—roughly n3/2, as we will see. This number, for large n,
is negligible compared to
n
2

(or to ⌊n2/4⌋).
Both of these problems belong to extremal graph theory, which in
general studies the maximum possible number of edges of a graph on n
vertices that doesn’t contain a given forbidden graph as a subgraph, or
more generally, doesn’t contain a subgraph from a given set of forbidden
graphs. We also met a (simple) problem of this kind in the chapter on
trees: we saw that an n-vertex graph containing no cycle (that is, a
forest) has no more than n −1 edges.
Now we proceed with the main result of this section.
7.3.1 Theorem. If a graph G on n vertices contains no subgraph
isomorphic to K2,2 then it has at most 1
2

n3/2 + n

edges.

234
Double-counting
As we will see later (Theorem 9.4.1), the bound in Theorem 7.3.1
is nearly the best possible for large n.
Proof. Let us write V = V (G). We will double-count the size of
the set M of all pairs ({u, u′}, v), where v ∈V , {u, u′} ∈
V
2

, and v
is connected by an edge to both u and u′. In other words, we count
(noninduced) subgraphs of the form
v
u
u′
For a ﬁxed pair {u, u′}, only one vertex v ∈V may exist joined to
both u and u′. If there were two such vertices, v and v′, they would
together with u and u′ form a subgraph isomorphic to K2,2. Hence
|M| ≤
n
2

.
Now let us see how many elements of the form ({u, u′}, v) are con-
tributed to the set M by a ﬁxed vertex v ∈V . For each pair {u, u′}
of its neighbors, v contributes one element of M, so if v has degree d
it contributes
d
2

elements. Therefore, if we denote by d1, d2, . . . , dn
the degrees of the vertices of V , we obtain |M| = n
i=1
di
2

.
Combining this with the previous estimate, we get
n

i=1
di
2

≤
n
2

.
(7.3)
At the same time, the number of edges of the considered graph is
1
2
n
i=1 di. The rest of the proof is just a manipulation of inequalities
which can be done in many diﬀerent ways.
Here we show one proof, and another is sketched in Exercise 5
(the latter proof is a bit more complicated but it can be generalized
more easily). First, we need a famous and generally useful inequality:
7.3.2 Proposition (Cauchy–Schwarz inequality).
For arbitr-
ary real numbers x1, x2, . . . , xn and y1, y2, . . . , yn we have
n

i=1
xiyi ≤
-
.
.
/
n

i=1
x2
i
-
.
.
/
n

i=1
y2
i .
The Cauchy–Schwarz inequality has a nice geometric meaning. If
we interpret x = (x1, . . . , xn) and y = (y1, . . . , yn) as vectors in the
n-dimensional Euclidean space, the left-hand side of the inequality is

7.3 An extremal problem: forbidden four-cycles
235
the scalar product of x and y, while the right-hand side is the product
of the lengths of the vectors (check this for n = 2, i.e. in the plane,
provided you know at least a little about vectors). A well-known
formula for vectors says that the cosine of the angle of two vectors
equals their scalar product divided by the product of their lengths.
The Cauchy–Schwarz inequality thus amounts to saying that this
cosine is never greater than 1!
The Cauchy–Schwarz inequality is usually treated in courses of cal-
culus, but why shouldn’t we present two elegant proofs?
First proof: from the AG inequality. Let us write X = n
i=1 x2
i
and Y = n
i=1 y2
i . If X = 0, then all xi are 0 and the inequality holds,
and similarly for Y = 0. Let us now assume X > 0 and Y > 0, and let
us deﬁne ai = x2
i /X, bi = y2
i /Y . The inequality between the arithmetic
mean and the geometric mean, see the proof of Theorem 3.5.2, tells us
that √aibi ≤(ai +bi)/2. By adding these inequalities for i = 1, 2, . . . , n
together we obtain on the left-hand side
n

i=1

aibi =
n

i=1
|xiyi|
√
XY
,
while the right-hand side yields
1
2
 1
X
n

i=1
x2
i + 1
Y
n

i=1
y2
i

= 1
2(1 + 1) = 1.
Hence n
i=1
|xiyi|
√
XY ≤1, and after multiplying by the expression
√
XY
we arrive at the Cauchy–Schwarz inequality.
2
Second proof: magic with a discriminant. We ﬁx the numbers xi
and yi, and we deﬁne a function p of a new variable t by
p(t) =
n

i=1
(xi + tyi)2.
By multiplying out the brackets and collecting terms that contain the
same power of t, we see that p(t) is a quadratic function of the form
p(t) = at2+bt+c, where a = n
i=1 y2
i , b = n
i=1 2xiyi, and c = n
i=1 x2
i .
On the other hand, p(t) is a sum of squares, and therefore nonnegative
for all real numbers t.
If the quadratic equation at2 + bt + c = 0 had a positive discrimi-
nant, then it would have two real solutions, and thus the function p(t)
would take negative values on a part of the real axis. Therefore, the
discriminant is nonpositive, i.e., b2 −4ac ≤0. By substituting a, b,
and c by their expressions in terms of xi and yi, we obtain exactly the
Cauchy–Schwarz inequality.
2

236
Double-counting
By
far
this
doesn’t
exhaust
proofs
of
the
Cauchy–Schwarz
inequality—some others are indicated in Exercise 4, for instance.
Finishing the proof of Theorem 7.3.1. Obviously, we can assume
that our graph has no isolated vertices, and hence di ≥1 for all i.
Then we have
di
2

≥1
2(di −1)2, and so we obtain from (7.3) that
n

i=1
(di −1)2 ≤n2.
We now apply the Cauchy–Schwarz inequality with xi = di −1,
yi = 1. We get
n

i=1
(di −1) ≤
-
.
.
/
n

i=1
(di −1)2√n ≤
√
n2√n = n3/2,
so |E(G)| = 1
2
n
i=1 di ≤1
2(n3/2 + n).
2
The intuition. In the proof, we have derived an upper bound for
the sum n
i=1
di
2

, and we ask how large the sum n
i=1 di can be. For
di not too small,
di
2

behaves roughly like 1
2d2
i . Given that  d2
i is
about n2, the way to make  di largest is to choose all the di equal,
which in our case means each should be √n. Then  di is about
n3/2. Of course, this is not a proof, but such a rough calculation can
often give us a good idea about what is going on. The above proof
with the Cauchy–Schwarz inequality is a polished version of the same
idea.
Exercises
1. Prove that for any t ≥2, the maximum number of edges of a graph
on n vertices containing no K2,t as a subgraph is at most
1
2
√
t −1 n3/2 + n

.
2. Let X be an n-element set, and let S1, S2, . . . , Sn be subsets of X such
that |Si ∩Sj| ≤1 whenever 1 ≤i < j ≤n. Prove that at least one
of the sets Si has size at most C√n, for some absolute constant C
(independent of n).
3. Let G be a bipartite graph with vertex classes of size n and m. Suppose
that G contains no K2,2 as a subgraph. Prove that G has at most

7.3 An extremal problem: forbidden four-cycles
237
O(m√n+n) edges. (Note that for n much larger than m, this is better
than the bound from Theorem 7.3.1.)
4. (a) Prove the Cauchy–Schwarz inequality by induction on n (square
both sides ﬁrst).
(b) Prove the Cauchy–Schwarz inequality directly, starting from the
inequality n
i,j=1(xiyj −xjyi)2 ≥0.
5. (a) Let f : R →R be a convex function, i.e. for any x, y ∈R and
λ ∈[0, 1] we have f(λx+(1−λ)y) ≤λf(x)+(1−λ)f(y). Geometrically,
this means that if we connect any two points on the graph of f by a
segment, then no part of this segment reaches below the graph of f:
x
y
y = f(x)
λx + (1 −λ)y
Prove (by induction) that for f convex, the inequality
f
 1
nx1 + 1
nx2 + · · · + 1
nxn

≤1
nf(x1) + 1
nf(x2) + · · · + 1
nf(xn) (7.4)
holds for any real numbers x1, x2, . . . , xn (this is sometimes called
Jensen’s inequality).
(b) Deﬁne a function f by the formula
f(x) =

0
for x ≤1
x(x −1)/2
for x > 1.
Prove that f is convex.
(c) Prove Theorem 7.3.1 from (7.3) using (a) and (b). First derive
n · f
 m
2n

≤
n
2

, where m = |E(G)|.
6. ∗In a way similar to the method in Exercise 5, deduce that if a graph
on n vertices does not contain K3,3 as a subgraph then it has O(n5/3)
edges.
7. (a) ∗Let L be a set of n (distinct) lines in the plane and P a set of n
(distinct) points in the plane. Prove that the number of pairs (p, ℓ),
where p ∈P, ℓ∈L, and p lies on ℓ, is bounded by O(n3/2).
Remark. It is known that the right bound is O(n4/3); see Pach and
Agarwal [27].

238
Double-counting
(b) ∗Show that the bound in (a) remains valid if lines are replaced by
circles of unit radius. Use Exercise 1.
Remark.
Here the best known upper bound is O(n4/3) as well, but
it is suspected that the number of incidences is actually bounded by
O(n1+ε) for an arbitrarily small constant ε > 0 (with the constant
hidden in the O(.) notation depending on ε). Proving or disproving
this is a very challenging open problem.

8
The number of spanning
trees
8.1
The result
For a given graph G, let T(G) denote the number of all spanning
trees of G. For example, we have T(K3) = 3 (so we really count
all possible spanning trees, not just nonisomorphic ones). In this
chapter, we present several proofs of the following result:
8.1.1 Theorem (Cayley’s formula). For each n ≥2, the number
T(Kn), i.e. the number of trees on given n vertices, equals nn−2.
Although T(Kn) can be expressed in such a nice and simple way,
no completely straightforward method is known for deriving the for-
mula. In the course of time, many proofs have been discovered. Their
basic ideas diﬀer substantially, but each of them involves a clever
trick or follows from a nontrivial theory. We have collected several
of these proofs into this chapter (by far not all known ones!), mainly
as an illustration of the richness of mathematical thought.
The proof in Section 8.2 counts the number of spanning trees with
a given score by a clever induction. Section 8.3 presents an elegant
argument constructing a bijection between the set of all spanning
trees of Kn with two marked vertices and the set of all mappings
{1, 2, . . . , n} →{1, 2, . . . , n}. Section 8.4 is a classical proof with
encoding trees by sequences of length n −2. Finally the method of
Section 8.5 is the most advanced one and it gives perhaps the best
insight into the problem. It expresses the number of spanning trees
of an arbitrary graph as a certain determinant. Yet another proof is
sketched in Exercise 8.2.2.
By no means do we want to claim that Theorem 8.1.1 belongs among
the most fundamental results in mathematics. On the other hand, the
number of spanning trees of a graph (and the related theory) has many

240
The number of spanning trees
theoretical and practical applications, and it was ﬁrst studied in con-
nection with electrical circuits.
As an illustration, let us mention without a proof an “electrotechni-
cal” meaning of the number of spanning trees of a graph. Imagine that
a given graph G is an electrical circuit, where each edge is a wire of
unit resistance and the vertices are simply points where the wires are
connected together. If x and y are vertices connected by an edge, then
the resistance we would measure between x and y in this circuit equals
the number of spanning trees of G containing the edge {x, y} divided
by the total number of spanning trees, T(G). This result is not too
useful directly for applications because the resistances are seldom all
identical, but a generalization exists for graphs with weights on edges.
More about the subject of this chapter can be found in Lov´asz [8] or
Biggs [15].
In the whole chapter, we assume that the vertex set V of the
considered graph G is the set {1, 2, . . . , n}.
Exercises
1. Prove that the number of nonisomorphic trees on n vertices is at least
en−1/n3 (see also Exercise 5.2.6!).
2. ∗Assume the validity of Theorem 8.1.1, and determine the number of
spanning trees of the complete graph on n vertices minus one edge.
3. Put Tn = T(Kn). Prove the recurrent formula
(n −1)Tn =
n−1

k=1
k(n −k)
n −1
k −1

TkTn−k.
Remark. Theorem 8.1.1 can be derived from this recurrence too, but
it’s not so easy.
4. ∗Let G be a connected topological planar graph, and let G∗denote its
dual (as in Deﬁnition 6.4.3). Prove that T(G) = T(G∗). If convenient,
you may assume that G is such that G∗has no loops and no multiple
edges.
8.2
A proof via score
First we count the number of trees with a given score:
8.2.1 Proposition. Let d1, d2, . . . , dn be positive integers summing
up to 2n −2. Then the number of spanning trees of the graph Kn in
which the vertex i has degree exactly di for all i = 1, 2, . . . , n equals
(n −2)!
(d1 −1)!(d2 −1)! · · · (dn −1)!.

8.2 A proof via score
241
Proof.
By induction on n. For n = 1, 2, the proposition holds
trivially, so let n > 2. Since the sum of the di is smaller than 2n,
there exists an i with di = 1. We now do the proof assuming that
dn = 1. This is just for notational convenience; exactly the same
argument works for any other di = 1 (or, put diﬀerently, the number
of the spanning trees of the required type obviously doesn’t change
by exchanging the values of dn and di so we may assume dn = 1
without loss of generality).
Let T be the set of all spanning trees of Kn with the given degrees
(i.e. in which each vertex i has degree di). Classify the trees of T into
n−1 groups T1, . . . , Tn−1: the set Tj consists of all trees of T in which
the vertex n is connected to the vertex j. Next, we consider a tree
from Tj, and we delete the vertex n together with its (single) edge.
We obtain a spanning tree of Kn−1, whose degree at the vertex i is di
for i ̸= j and dj −1 for i = j. It is easy to see that in this way, we get
a bijection between the set Tj and the set T ′
j of all spanning trees
of Kn−1 with degrees d1, d2, . . . , dj−1, dj −1, dj+1, . . . , dn−1 (since
distinct trees of Tj give rise to distinct trees of T ′
j , and from each
tree of T ′
j we can get a tree of Tj by adding the vertex n back and
connecting it to the vertex j).
By the inductive hypothesis, we have
|Tj| = |T ′
j | =
(n−3)!
(d1−1)! · · · (dj−1−1)!(dj−2)!(dj+1−1)! · · · (dn−1−1)!
=
(n −3)!(dj −1)
(d1 −1)!(d2 −1)! · · · (dn−1 −1)!.
This formula also holds when dj = 1—then it gives 0 which agrees
with the fact that no spanning tree with degree dj −1 = 0 at the
vertex j exists.
Therefore, the total number of spanning trees on n vertices with
degrees d1, d2, . . . , dn, where dn = 1, is
|T | =
n

j=1
|Tj| =
n−1

j=1
(n −3)!(dj −1)
(d1 −1)!(d2 −1)! · · · (dn−1 −1)!
=
 n−1

j=1
(dj −1)

(n −3)!
(d1 −1)!(d2 −1)! · · · (dn−1 −1)!
=
(n −2)(n −3)!
(d1 −1)!(d2 −1)! · · · (dn−1 −1)!.

242
The number of spanning trees
Since dn = 1, we can multiply the denominator by the factor
(dn −1)! = 0! = 1 with no harm, and this ﬁnishes the inductive
step.
2
Next, we prove Theorem 8.1.1. We will sum over all possible scores
of spanning trees, and we will use the multinomial theorem 3.3.5:
T(Kn) =

d1,d2,...,dn≥1
d1+d2+···+dn=2n−2
(n −2)!
(d1 −1)!(d2 −1)! · · · (dn −1)!
=

k1+k2+···+kn=n−2
k1,...,kn≥0
(n −2)!
k1!k2! · · · kn!
= (1 + 1 + · · · + 1



n×
)n−2 = nn−2.
2
Exercises
1. (a) ∗Find the number of trees (on given n vertices) in which all vertices
have degree 1 or 3.
(b) ∗What if we allow degrees 1, 2, or 3?
2. (Yet another proof of Theorem 8.1.1) Let Nk denote the number
of spanning trees of Kn in which the vertex n has degree k, k =
1, 2, . . . , n −1 (recall that we assume V (Kn) = {1, 2, . . . , n}).
(a) ∗Prove that (n −1 −k)Nk = k(n −1)Nk+1.
(b) Using (a), derive Nk =
n−2
k−1

(n −1)n−1−k.
(c) Prove Theorem 8.1.1 from (b).
8.3
A proof with vertebrates
Consider a spanning tree of the complete graph Kn. Mark one of its
vertices by a circle, and one vertex by a square, as in Fig. 8.1(a). We
do not exclude the case that the same vertex is marked by both a
circle and a square. Each object that can arise in this way from some
spanning tree on Kn is called a vertebrate. Let V denote the set of
all vertebrates (for the considered value of n).
From each given spanning tree, we can create n2 vertebrates.
Therefore the number of all spanning trees equals |V|/n2. We now
show

8.3 A proof with vertebrates
243
8
4 14 9
3
7 15
1
2
5
6
10
11
12
13
16
17
18
19
11
10
13
12
19
7
17 16
1
18
6
5
2
14
15
8
3
9
4
(a)
(b)
Fig. 8.1 (a) A vertebrate on 19 vertices; (b) the corresponding mapping.
Lemma. There exists a bijection F between the set V of all verte-
brates and the set of all mappings of the vertex set V to itself.
Since the number of all mappings of an n-element set to itself
is nn, the number of vertebrates is the same by the lemma, and
therefore the number of spanning trees is nn−2.
Proof of the lemma. We demonstrate the deﬁnition of the bijection
F on the example in Fig. 8.1. We start from the vertebrate W drawn
in Fig. 8.1(a). The marked vertices 2 and ⃝are connected by a
unique path, which we call the chord. Let us write out the numbers
of vertices of the chord ordered by magnitude. Then, we write these
numbers again on the next line, in the order as they appear along
the chord from ⃝to 2:
3
4
7
8
9
14
15
8
4
14
9
3
7
15
We deﬁne an auxiliary directed graph P: the vertex set consists of
the vertices of the chord, and we make an arrow (directed edge)
from each vertex written in the ﬁrst line to the vertex below it in the
second line. Since there is exactly one arrow going from each vertex
and also exactly one arrow entering it, the graph P is a disjoint union
of directed cycles (including possibly also isolated vertices with a
directed loop). We can also say that the chord deﬁnes a permutation
of its vertices, and P consists of the cycles of this permutation (see
Section 3.2). In our example, these cycles are (3, 8, 9), (4), (7, 14),
and (15).
We now look back at the whole vertebrate W. If we remove the
edges of the chord from it, it splits into components (which are trees
again). We direct the edges of the components so that they point to

244
The number of spanning trees
the (single) vertex of the chord contained in that component. This
gives rise to one more set of directed edges on the set V . We now
deﬁne a directed graph with vertex set V : its edges are all directed
edges of the components, plus the edges of the graph P. In the ﬁgure,
this is very intuitive. We draw the cycles of the graph P, and then we
draw to each vertex (originally coming from the chord) the tree that
was hanging by that vertex on the chord of the considered vertebrate;
see Fig. 8.1(b).
We claim that the resulting directed graph, G, is a graph of a
mapping, i.e. there is exactly one arrow going from each vertex. For
the vertices of the chord, this has already been mentioned. For the
other vertices, the reason is that there is a unique path to the chord
from each vertex in the vertebrate W. Using the graph G, we can
ﬁnally deﬁne a mapping f : {1, 2, . . . , n} →{1, 2, . . . , n}: for each
i ∈V , we set f(i) = j, where j is the vertex of G that the arrow
emanating from i ends in. In our speciﬁc example, we get the map-
ping 1 →7, 2 →15, 3 →8, 4 →4, 5 →2, 6 →5, 7 →14, 8 →9,
9 →3, 10 →4, 11 →10, 12 →4, 13 →12, 14 →7, 15 →15,
16 →7, 17 →16, 18 →1, and 19 →8. In this way, each vertebrate
W determines a mapping F(W).
It remains to prove that the original vertebrate W can be re-
constructed from the mapping f produced as above, and that every
mapping can be obtained from some vertebrate. This is left as an
exercise.
2
Exercises
1. For a mapping f : V →V , where V is a ﬁnite set, we deﬁne the
(directed) graph of f as the directed graph with vertex set V and edge
set {(i, f(i)): i ∈V } (such a graph was used in the proof above).
Prove that each (weakly connected) component of such a graph is a
directed cycle, possibly with some trees hanging at the vertices of the
cycle, with edges directed towards the cycle.
2. Given a mapping f : {1, 2, . . . , n} →{1, 2, . . . , n} of the form F(W)
for some vertebrate W, describe how the vertebrate W can be recon-
structed from the knowledge of f. Prove that any mapping f can be
obtained as F(W) for some vertebrate W (use Exercise 1).
3. ∗,CS Let f : {1, 2, . . . , n} →{1, 2, . . . , n} be a mapping. For each i ∈
{1, 2, . . . , n}, the sequence (i, f(i), f(f(i)), . . .) must be eventually
periodic. Design an algorithm that ﬁnds the shortest period of this
sequence for a given i. That is, in the language of the directed graph

8.4 A proof using the Pr¨ufer code
245
of the mapping f, we want to ﬁnd the length of the directed cycle in
the component containing the vertex i. The algorithm should use only
an amount of memory bounded by some constant independent of n. It
has a subroutine (black box) at its disposal for evaluating the function
f for any given i.
4. CS (a) Design the details of an algorithm for producing a vertebrate
from a mapping. How many steps does it need in the worst case? ∗Can
you make it run in O(n) time?
(b) ∗Program the algorithm from (a), and use it to generate random
spanning trees of the complete graph from randomly generated map-
pings. Using this program, experimentally estimate the average (expec-
tation) of the maximum degree and the diameter of a random spanning
tree on a given number of vertices (104, say).
8.4
A proof using the Pr¨ufer code
We show how each spanning tree of the complete graph Kn can be
encoded by an (n −2)-term sequence such that each term is one of
the numbers 1, 2, . . . , n. This coding will deﬁne a bijection between
all spanning trees and all sequences of the type just described. Since
the number of such sequences is obviously nn−2, this will establish
Theorem 8.1.1.
Consider a spanning tree T; our running example on 8 vertices
is drawn in Fig. 8.2(a). We explain how to construct a sequence
p = P(T) = (p1, p2, . . . , pn−2), the so-called Pr¨ufer code of the tree T.
The basic idea1 is to tear oﬀthe leaves of T one by one until the
tree is reduced to a single edge. We will thus construct an auxiliary
sequence T0 = T, T1, T2, . . . , Tn−2 = K2 of trees, and produce the
sequence p simultaneously. Suppose that the tree Ti−1 has already
been constructed for some i (initially we have T0 = T). As we know,
this Ti−1 has at least one leaf (i.e. a vertex of degree 1). We take
the smallest of the leaves of Ti−1 (recall that the vertices of T are
the numbers 1, 2, . . . , n), and we form Ti by removing this leaf from
Ti−1 together with the edge incident to it. At the same time, we also
deﬁne the ith term, pi, of the constructed sequence as the neighbor
of the leaf just torn oﬀfrom Ti−1. This is the main trick: we do
not record the leaf but its neighbor! By doing this successively for
i = 1, 2, . . . , n −2, we have deﬁned the whole sequence p = P(T).
Now we derive how to reconstruct the original tree T = P −1(p)
from the sequence p = (p1, p2, . . . , pn−2). More exactly, we give a rule
1A somewhat vandalic one.

246
The number of spanning trees
5
2
3
1
6
4
7
8
5
2
3
1
6
4
7
step 1
step 2
step 3
step 4
step 5
(a)
(b)
Fig. 8.2 (a) A spanning tree with code (5, 1, 1, 4, 5, 1); (b) the procedure
of its reconstruction from the code.
creating a spanning tree from each given sequence p, and we proceed
in such a way that if p arose from some spanning tree T then our rule
gives us back the tree T. This will show that the coding of spanning
trees by sequences indeed deﬁnes a bijection as we claimed.
So let us suppose that a given sequence p arose by the above-
described construction from some (as yet unknown) spanning tree T.
Let ℓ1 denote the leaf of T that was removed ﬁrst. How can we
tell ℓ1 from the sequence p? This ℓ1 cannot occur anywhere in the
sequence p (since we wrote only vertices still present in the current
tree into the sequence p). Further, any vertex not contained in the
set {p1, p2, . . . , pn−2} has to be a leaf of the tree T0, for otherwise we
would sooner or later tear oﬀa leaf hanging from it, and it would
appear in the sequence p at that moment. According to the rule for
removing leaves, ℓ1 must thus be the minimum element of the set
{1, 2, . . . , n} \ {p1, p2, . . . , pn−2}. Now we can draw the vertices ℓ1
and p1 and connect them by an edge (see Fig. 8.2b).
Further, we proceed similarly. Having found the leaves ℓ1, ℓ2, . . . ,
ℓi−1 removed from T in the steps 1 through i −1, we determine the
leaf ℓi. It cannot be any of the vertices pi, pi+1, . . . , pn−2, and neither
any one of ℓ1, . . . , ℓi−1, of course—therefore ℓi is the minimum of the
set {1, 2, . . . , n}\{pi, pi+1, . . . , pn−2, ℓ1, ℓ2, . . . , ℓi−1}. We connect this
ℓi by an edge to the vertex pi; if ℓi has not been drawn yet, we draw
it as well, and similarly for pi. The ﬁrst 5 steps of this construction
are depicted in Fig. 8.2(b). In the 6th step, we would draw the edge
{1, 5}.
After n−2 steps, we have drawn n−2 edges of the spanning tree
T, namely all the edges that have been removed in the construction

8.5 Proofs working with determinants
247
of p. It remains to deduce which edge was the last remaining one.
One of its endpoints must be pn−2, i.e. the neighbor of the last leaf
torn oﬀ, and the other endpoint is the vertex not occurring among
the removed leaves ℓ1, . . . , ℓn−2 and distinct from pn−2. In Fig. 8.2,
this edge is {1, 8}.
This ﬁnishes the description of the reconstruction procedure, but
the proof is not ﬁnished yet. It is easy to check that for any input
sequence p, the algorithm executes properly and returns some sub-
graph G of Kn with n −1 edges, and we also know that if p was
obtained by encoding some spanning tree T, then G = T. It remains
to prove that (1) G is always a tree, and (2) that the encoding pro-
cedure applied to G always yields the original p.
Let Gi = ({1, 2, . . . , n}, {ei, ei+1, . . . , en−1}), where e1, e2, . . . , en−1
are the edges of G in the order as they are generated by the decoding
algorithm. One can check that none of the edges ei+1, . . . , en−1 can
be incident to the vertex ℓi, and therefore ℓi is a leaf in Gi. Thus
G is a tree by the tree-growing lemma (Lemma 5.1.4), and more
generally, Gi is a tree plus i + 1 isolated vertices.
As for (2), it suﬃces to verify that ℓi is always the smallest leaf in
Gi, 1 ≤i ≤n −2. By the deﬁnition of ℓi, a smaller leaf could occur
only among ℓ1, . . . , ℓi−1 or among {pi, . . . , pn−2}. The ﬁrst group is
out of question, since ℓ1, . . . , ℓi−1 are isolated in Gi. Let us consider
a vertex pk, i ≤k ≤n −2. In the graph Gk, pk is a neighbor of
the leaf ℓk, and it has yet another neighbor since it lies in the single
connected component of Gk+1 with at least 2 vertices.
2
Exercises
1. Let T be a spanning tree of Kn, and let p = P(T) be its Pr¨ufer code.
Let mi be the number of times the vertex i appears in the sequence p,
i = 1, 2, . . . , n. Prove that degT (i) = mi + 1 holds for all i.
2. CS (a) Design the details of an algorithm for producing a spanning
tree of Kn from its Pr¨ufer code. How many steps does it need in the
worst case? ∗Can you make it run in O(n log n) time, or even faster?
(b) ∗Program the algorithm from (a). Use it as in part (b) of Exer-
cise 8.3.4. Which of the two algorithms runs faster?
8.5
Proofs working with determinants
Two proofs of Theorem 8.1.1 given in this section are based on
linear algebra, and they illustrates a nice combinatorial meaning of

248
The number of spanning trees
the determinant (the deﬁnition and facts about determinants we
need can be found in the Appendix). They are somewhat more dif-
ﬁcult than the previous proofs and require basic results concerning
determinants, but they provide a formula for the number of spanning
trees of an arbitrary graph.
Let G be an arbitrary graph with vertices 1, 2, . . . , n, n ≥2, and
with edges e1, e2, . . . , em. We introduce an n×n matrix Q, called the
Laplace matrix of the graph G, whose elements qij are determined
by the following formula:
qii
=
degG(i)
i = 1, 2, . . . , n
qij
=
 −1
for {i, j} ∈E(G)
0
otherwise
i, j = 1, 2, . . . , n, i ̸= j.
Soon we will ﬁnd useful the observation that the sum of the rows of
the Laplace matrix is the zero vector.
Further, let Qij denote the (n −1) × (n −1) matrix arising from
the matrix Q by deleting the ith row and the jth column.
The following remarkable theorem holds:
8.5.1 Theorem. For every graph G, we have T(G) = det Q11.
Let us remark that also T(G) = | det Qij| holds for any two indices
i, j ∈{1, 2, . . . , n}. We do not prove this here; a proof is indicated in
Exercise 1.
Before we start proving Theorem 8.5.1, we calculate the num-
ber of spanning trees of a complete graph (Theorem 8.1.1) from
it. For G = Kn, the Laplace matrix has the number n −1 every-
where on the diagonal, and −1 everywhere else. If we delete the ﬁrst
row and the ﬁrst column, we obtain an (n −1) × (n −1) matrix of
the form





n −1
−1
−1
. . .
−1
−1
n −1
−1
. . .
−1
...
...
...
. . .
...
−1
−1
−1
. . .
n −1




.
We calculate the determinant by suitable row and column operations.
We subtract the ﬁrst row from all rows except for the ﬁrst one, and

8.5 Proofs working with determinants
249
then we replace the ﬁrst column by the sum of all columns. We get
a matrix having the numbers 1, n, n, n, . . . , n on the main diagonal
and zeros everywhere below the main diagonal. The determinant is
then the product of all diagonal elements, i.e. nn−2.
We prove Theorem 8.5.1 in two ways. The ﬁrst one is shorter and
more graph-theoretic. The second one uses more of linear algebra,
and probably it better explains why the theorem is true.
First proof of Theorem 8.5.1. We proceed by induction, and to
make it work, we strengthen the inductive hypothesis and show that
the theorem also holds for multigraphs, i.e., for graphs with multiple
edges. These were already mentioned in Section 4.4: Any two ver-
tices may be joined by an arbitrary number of edges (none, one, or
several). If two vertices u and v are joined by several edges, then we
count each spanning tree in which u and v are adjacent the corre-
sponding number of times, in other words, we distinguish between
spanning trees that use diﬀerent edges. The following multigraph,
for instance, has 6 spanning trees:
We do not allow our graphs to have loops, since these have no
eﬀect on the number of spanning trees. What does the Laplacian of a
multigraph look like? If two vertices u and w are joined by m edges,
then quv = −m. The diagonal entry quu is the degree of the vertex
u, where the incident edges are counted with multiplicity (e.g. the
middle vertex in the preceding ﬁgure has degree 5).
Further, we will rely on the formula
T(G) = T(G −e) + T(G : e),
(8.1)
where e is an arbitrary edge of the graph G, G −e denotes the
graph obtained by deleting that edge, and G : e the one obtained by
contracting the edge. The latter means that we remove the edge e
from the graph G and its endvertices are merged into one. By doing
this, we may introduce new multiple edges—this is the diﬀerence to
the kind of edge contraction considered in Section 6.4. However, if
the endvertices of e are joined in G by edges other than e, then these
edges will be deleted (they could become loops at the vertex created

250
The number of spanning trees
by the merging, but we do not consider loops here). The following
drawing shows an example of a contraction:
e
G
G : e
;
1
2
3
4
5
In order to see why Eq. (8.1) holds, we divide the spanning trees
of G into two classes. The spanning trees of the ﬁrst class are those
that do not contain the edge e. But these are exactly the spanning
trees of the graph G−e, and there are T(G−e) of these. The spanning
trees of the second class are those that do contain the edge e, and
these are in one-to-one correspondence with the spanning trees of
G : e, as indicated in the following picture
e
;
and as the reader can conclude at his own leisure. Thus, there are
T(G : e) trees of this kind.
Now we still have analyze how the operations of edge deletion and
edge contraction aﬀect the Laplacian. More precisely, assume that
the edge e has endvertices 1 and 2 and let us consider how the matrix
Q11 changes. For edge deletion, this is very simple: If we denote by
Q′ the Laplacian of G−e, then Q′
11 arises from Q11 by subtracting 1
from the element in the upper left corner (because deleting the edge
e only aﬀects the elements in the positions (1, 1), (1, 2), (2, 1), and
(2, 2) in the Laplacian matrix, and we are interested in the Laplacian
with the ﬁrst row and ﬁrst column deleted).
When contracting the edge e, the vertices 1 and 2 disappear, and
a new vertex, which arises from their merging, appears in their stead.
Let us number the vertices of G : e in such a way that the new vertex
gets the label 1 and an old vertex that used to have the number i ≥3
now gets the label i−1, and let Q′′ denote the Laplacian of G : e with
this numbering of the vertices. It is easy to see that Q′′
11 = Q11,22,
which is the matrix obtained from Q by deleting both the ﬁrst and

8.5 Proofs working with determinants
251
second rows and the ﬁrst and second columns. For instance, for the
graph in the two pictures above, we have
Q11 =




5
0
−1
−1
0
2
−1
0
−1
−1
3
0
−1
0
0
1



,
Q′′
11 =


2
−1
0
−1
3
0
0
0
1

.
Now we are ready to proceed with the actual inductive proof. We
will show by induction on m that T(G) = det Q11 holds for every
multigraph G with at most m edges.
If in a multigraph G the vertex number 1 is not incident to any
edge, then we have T(G) = 0. The ﬁrst row of the Laplacian matrix
consists only of zeros, and since the rows of a Laplacian always sum
up to zero, the sum of the rows of Q11 is also zero. Thus, det Q11 = 0,
i.e, the statement we want to show holds true. In particular, we have
established the validity of our claim for the base case m = 0.
The second case is more interesting: If the vertex number 1 is
incident to at least one edge, then we ﬁx one such edge, let us call
it e, and we choose a numbering of the vertices such that the other
endvertex of e is labeled by 2. If we denote by Q, Q′, and Q′′ the
Laplacian matrices of the graphs G, G−e, and G : e, respectively, as
above, then by Eq. (8.1) and by the inductive assumption, we have
T(G) = T(G−e)+T(G : e) = det Q′
11+det Q′′
11 = det Q′
11+det Q11,22.
As we know, the determinant of a matrix is a linear function of
each row, and the matrix Q11 arises from Q′
11 by adding the vector
e1 = (1, 0, 0, . . . , 0) to the ﬁrst row. Thus, det Q11 = det Q′
11 +det R,
where R has the vector e1 as its ﬁrst row and agrees with Q11 in
the remaining rows. By expanding the determinant of along the ﬁrst
row, we see that det R = det Q11,22, and hence det Q′
11 +det Q11,22 =
det Q′
11 + det R = det Q11. This completes the inductive step and
hence the ﬁrst proof of Theorem 8.5.1. We conclude by remarking
that we only needed a very special case of the row expansion formula
for determinants, where the row in question contains a single 1 and
otherwise only zeros, and this case is immediate from the deﬁnition
of the determinant.
2
Second proof of Theorem 8.5.1.
First we ﬁx some arbitrarily
chosen orientation ⃗G of the graph G, i.e. for each edge ek, we select

252
The number of spanning trees
one of its endpoints as the head and the other one becomes the tail
(see Section 4.5 for the terminology of directed graphs). This directed
edge will be denoted by ⃗ek. Interestingly, we need some orientation
for the proof, although the conclusion is independent of the particular
orientation, and only depends on G! We deﬁne an auxiliary matrix
D = D ⃗G, called the incidence matrix for the chosen orientation ⃗G.
This matrix has n rows, corresponding to the vertices of ⃗G, and m
columns, corresponding to the edges of ⃗G, and it is deﬁned as follows:
dik =



−1
if i is the tail of ⃗ek
1
if i is the head of ⃗ek
0
otherwise
(recall that the edges are numbered e1, . . . , em). Note that the matrix
D has exactly one entry 1 and one entry −1 in each column, the other
entries are 0, and the sum of all rows is the zero vector.
Let us recall that if A is a matrix, the symbol AT denotes the
transposed matrix A; that is, AT has the element aji at position
(i, j). Next, let ¯D denote the matrix arising from D by deleting the
ﬁrst row. Here is a connection between the matrix D and the Laplace
matrix of G.
8.5.2 Lemma. For any orientation ⃗G of the graph G, the equalities
DDT = Q and ¯D ¯DT = Q11 hold, where D = D ⃗G.
Proof. By the deﬁnition of matrix multiplication, the element at
position (i, j) in the product DDT equals m
k=1 dikdjk. For i = j,
the product dikdjk = d2
ik is 1 if i is the head or the tail of the edge
⃗ek, and it is 0 otherwise, and therefore the considered sum is just the
degree of the vertex i in G. For i ̸= j, the product dikdjk is nonzero
only if ⃗ek = (i, j) or ⃗ek = (j, i), and in this case it equals −1. By
comparing this with the deﬁnition of the Laplace matrix, we see that
DDT = Q. The second equality claimed in the lemma is a simple
consequence of the deﬁnition of matrix multiplication.
2
The following key lemma connects spanning trees to determi-
nants:
8.5.3 Lemma. Let T be a graph on the vertex set {1, 2, . . . , n} with
n −1 edges (n ≥2), and let ⃗T be an orientation of T. Let C = D⃗T
be the incidence matrix of the directed graph ⃗T, and let ¯C denote
the square matrix obtained from C by deleting its ﬁrst row. Then

8.5 Proofs working with determinants
253
det ¯C has one of the values 0, 1, −1, and it is nonzero if and only if
T is a tree (this means it is a spanning tree of the complete graph
on vertex set {1, 2, . . . , n}).
Proof.
We proceed by induction on n. For n = 2, the situation
is simple: T has one edge, so it is a spanning tree, and the single
element of the matrix ¯C is either 1 or −1.
Let us consider an arbitrary n > 2, and let us distinguish two
cases, depending on whether any of the vertices 2, 3, . . . , n has de-
gree 1 in T.
First, suppose that such a vertex of degree 1 exists among the
vertices 2, 3, . . . , n. Without loss of generality, we may assume that
it is the vertex n (should it be another vertex, we simply renumber
the vertices). The vertex n belongs to a single edge, ⃗ek. This means
that the matrix ¯C has a single nonzero element in the last row (equal
to 1 or −1), namely in the kth column.
We expand the determinant of the matrix ¯C according to the row
corresponding to the vertex n (this is the (n −1)st row of ¯C):
det ¯C =
n−1

j=1
(−1)n−1+j¯cn−1,j det ¯Cn−1,j,
where ¯Cij denotes the matrix ¯C after deleting the ith row and the jth
column. Since the (n−1)st row has only one nonzero element, namely
¯cn−1,k, we get det ¯C = (−1)n−1+k¯cn−1,k det ¯Cn−1,k, and therefore
| det ¯C| = | det ¯Cn−1,k|.
Let ⃗T ′ be the directed graph obtained from ⃗T by deleting the
vertex n and the edge ⃗ek. The matrix ¯C′ arising from C′ = D⃗T ′ by
deleting the ﬁrst row is just ¯Cn−1,k. By the inductive assumption,
we thus know that | det ¯C′| is 1 or 0 depending on whether T ′ (the
undirected version of ⃗T ′) is a spanning tree on its vertex set or not.
Since we have removed a degree 1 vertex from T, T is a spanning tree
if and only if T ′ is a spanning tree. This concludes the inductive step
for the case when at least one of the vertices 2, 3, . . . , n has degree
1 in T.
Let us discuss the second case, when none of the vertices 2, 3, . . . , n
has degree 1 in T. First we observe that T has an isolated vertex in
this case (if it were not so, the vertex 1 would have degree at least
1 and the other vertices degrees at least 2, and hence the sum of
degrees would be greater than 2|E(T)| = 2(n−1)—a contradiction).

254
The number of spanning trees
Because of an isolated vertex, T is disconnected, and hence it
is not a spanning tree. To ﬁnish the proof we need to show that
det ¯C = 0. If there is an isolated vertex among 2, 3, . . . , n then it
corresponds to a zero row in the matrix ¯C. If the vertex 1 is isolated
then the sum of all rows of ¯C is the zero vector because the incidence
matrix D⃗T has zero row sum. In both cases we have det ¯C = 0.
2
By the lemma just proved, we know that the number of spanning
trees of the graph G equals the number of square (n −1) × (n −1)
submatrices with nonzero determinant of the matrix ¯D. For ﬁnish-
ing the proof of Theorem 8.5.1, we use an algebraic result about
determinants.
8.5.4 Theorem (Binet–Cauchy theorem). Let A be an arbi-
trary matrix with n rows and m columns. Then
det(AAT ) =

I
det(AI)2,
where the sum is over all n-element subsets I ∈
{1,2,...,m}
n

, and where
AI denotes the matrix obtained from A by deleting all columns whose
indices do not lie in I.
We give a proof of this theorem for completeness, but ﬁrst let us
look at how the Binet–Cauchy theorem implies Theorem 8.5.1. With
the preparation we have made, this is actually quite straightforward.
By Lemma 8.5.2 and then by Theorem 8.5.4 we get
det Q11 = det( ¯D ¯DT ) =

I∈({1,2,...,m}
n−1
)
det( ¯DI)2,
and by Lemma 8.5.3, we see that the last expression is exactly the
number of spanning trees of G.
2
Proof of the Binet–Cauchy theorem 8.5.4. Let us denote M =
AAT . We expand the determinant of M according to the deﬁnition
of a determinant, i.e.
det M =

π∈Sn
sgn(π)
n

i=1
mi,π(i),
where the sum is over all permutations π of the set {1, 2, . . . , n},
and where sgn(π) stands for the sign of the permutation π (for any

8.5 Proofs working with determinants
255
permutation, the sign is +1 or −1). We will not need the deﬁnition of
the sign of a permutation directly, and so we only recall the following:
8.5.5 Fact. For any permutation π of the set {1, 2, . . . , n} and for
indices i, j, 1 ≤i < j ≤n, let the symbol πi↔j denote the per-
mutation whose value at i is π(j), the value at j is π(i), and the
values for all other numbers agree with those of π. Then we have
sgn(π) = −sgn(πi↔j).
Now we substitute the values of the elements mij of the matrix M,
namely mij = m
k=1 aikajk, into the above expansion of the deter-
minant of M. Then we multiply out each of the products. This leads
to
det M =

π
sgn(π)
n

i=1
 m

k=1
aikaπ(i)k

=

π
sgn(π)
m

k1,k2,...,kn=1
n

i=1
ai,kiaπ(i),ki.
Let us change the notation in this last formula to a more suit-
able one. The choice of the n-tuple k1, . . . , kn of the summation
indices in the inner sum can be understood as a choice of a map-
ping f : {1, 2, . . . , n} →{1, 2, . . . , m} deﬁned by f(i) = ki. With this
new fancy notation, we thus have
det M =

π
sgn(π)

f : {1,2,...,n}→{1,2,...,m}
n

i=1
ai,f(i)aπ(i),f(i).
In this sum, we exchange the order of summation: we sum according
to the permutation π (inner sum) and then according to the function
f (outer sum). For a yet more convenient notation, we introduce the
symbols
P(f, π) =
n

i=1
ai,f(i)aπ(i),f(i),
S(f) =

π
sgn(π)P(f, π),
and then we have
det M =

f : {1,2,...,n}→{1,2,...,m}
S(f).

256
The number of spanning trees
A key lemma for the whole proof is the following one:
8.5.6 Lemma. If a function f : {1, 2, . . . , n} →{1, 2, . . . , m} is not
injective, then we have S(f) = 0 (for any choice of the matrix A).
Proof of the lemma. Let i, j be indices such that f(i) = f(j). Then
for any permutation π, the products P(f, π) and P(f, πi↔j) are the
same (the notation is as in Fact 8.5.5). If π runs through all permu-
tations, then also πi↔j runs through all permutations (although in
a somewhat diﬀerent order), and therefore we have
S(f) =

π
sgn(πi↔j)P(f, πi↔j) =

π
−sgn(π)P(f, π) = −S(f);
hence S(f) = 0 as the lemma claims.
By the lemma, we can write
det(AAT ) =

f : {1,2,...,n}→{1,2,...,m}
S(f),
where the summation is over all injective functions {1, 2, . . . , n}	→
{1, 2, . . . , m}.
Our goal now is to show that the right-hand side of the last equa-
tion equals 
I det(AI)2, where the sum runs over I ∈
{1,2,...,m}
n

.
Let us choose some I ∈
{1,2,...,m}
n

, and calculate
det(AI)2 = det(AI) det(AT
I ) = det(AIAT
I ).
This determinant can be expanded in exactly the same manner as
we did for the determinant of AAT . This time we get
det(AIAT
I ) =

f : {1,2,...,n}→I
S(f).
We thus have

I∈({1,2,...,m}
n
)
det(AI)2 =

I∈({1,2,...,m}
n
)

f : {1,2,...,n}→I
S(f)
=

f : {1,2,...,n}→{1,2,...,m}
S(f) = det(AAT );
the second equality follows from the fact that any injective function
f : {1, 2, . . . , n}	→{1, 2, . . . , m} uniquely determines the n-element
set I of its values. This proves the Binet–Cauchy theorem 8.5.4.
2

8.5 Proofs working with determinants
257
P
Pxz
Pxy
Pyz
z
x
y
area(P)2 = area(Pxy)2
+ area(Pxz)2+ area(Pyz)2
Fig. 8.3 Illustration for the geometric meaning of the Binet–Cauchy theo-
rem.
Let us mention a geometric interpretation of the Binet–Cauchy the-
orem in terms of volumes. We omit all proofs, since these, although
elementary, would lead us farther into geometry than we want to go in
this book. Let us consider an n×m matrix A as in the theorem, and let
us interpret its n rows as n vectors a1, a2, . . . , an in the m-dimensional
space Rm. These n vectors span an n-dimensional parallelotope P in
Rm; Fig. 8.3 illustrates this for m = 3 and n = 2 (about the only
nontrivial case where a picture can be drawn). It can be shown that
| det(AAT )| is the square of the n-dimensional volume of P (in the ﬁg-
ure, it is simply the squared area of P). Choosing an n×n submatrix B
corresponds to projecting the vectors a1, a2, . . . , an to a n-dimensional
subspace of Rm spanned by some n coordinate axes. In the ﬁgure, there
are 3 possible 2 × 2 submatrices corresponding to the projections into
the 3 coordinate planes. The quantity | det(BBT )| is the squared volume
of the corresponding projection of P, and the Binet–Cauchy theorem
asserts that the squared volume of P equals the sum of the squared
volumes of the n-dimensional projections of P. Actually, for n = 1 and
m = 2, the reader may want to check that this is just the theorem of
Pythagoras!
Exercises
1. In this exercise, G is a graph on n vertices, Q is its Laplace matrix,
and Q∗denotes the matrix whose element at position (i, j) equals
(−1)i+j det Qij.
(a) Prove that det Q = 0.

258
The number of spanning trees
(b) Prove that if the graph G is connected then its Laplace matrix has
rank n −1.
(c) ∗Prove that the rank of the Laplace matrix of a disconnected graph
is at most n −2. Derive that for a disconnected G, Q∗is the zero
matrix.
(d) Show that if G is connected and x ∈Rn is an arbitrary vector, then
Qx = 0 holds if and only if x is a multiple of the vector (1, 1, . . . , 1).
(e) ∗Prove that the product QQ∗is a zero matrix. Using (d), infer that
all the elements of the matrix Q∗are identical.
2. Solve Exercise 8.1.2 using Theorem 8.5.1.
3. ∗Calculate T(Kn,m) (the number of spanning trees of the complete
bipartite graph).
4. ∗Let G be an (undirected) graph and M its incidence matrix; if G has
n vertices v1, v2, . . . , vn and m edges e1, e2, . . . , em then M is an n×m
matrix with
mik =

1
if vi ∈ek
0
otherwise.
Prove that the following two conditions are equivalent:
(i) G is bipartite.
(ii) Any square submatrix of M (arising by deleting some rows and
columns) has determinant 0, 1, or −1. A matrix M with this property
is called totally unimodular.
8.6
The simplest proof?
Which proof of Cayley’s formula is the simplest? This depends on
personal preferences, of course. However, the following one, recently
found by Jim Pitman, a probabilist from the University of California
at Berkeley, is a good candidate. It illustrates vividly that there is
always a chance to discover something new even in areas of mathe-
matics that are considered well-understood.
This proof is a sophisticated example of double counting (which
was discussed in Chapter 7). We will not count just trees (or verte-
brates and similar species), but rather PARTs, or Plans of Assembly
of a Rooted Tree. What is a PART? Formally it is a triple (T, r, ℓ),
where T is a tree with vertex set V = {1, 2, . . . , n} (n is ﬁxed pos-
itive integer), r ∈V is a root, and ℓis a labeling of the edges of T

8.6 The simplest proof?
259
by the numbers 1, 2, . . . , n −1. That is, ℓis a bijection ℓ: E(T) →
{1, 2, . . . , n −1}. Here is an example of a PART:
5
2
3
1
6
4
7
8
root
1
2
3
4
5
6
7
We can imagine that we start with the empty graph on the vertex
set V and we make a rooted tree by adding edges one by one. The
labeling ℓencodes the order in which the edges are added.
Given a tree T, we can select the root in n ways, and then we can
choose the labeling ℓin (n −1)! ways. Hence the number of PARTs
equals n(n −1)! κ(Kn).
We now count the number of PARTs in a diﬀerent way. To this
end, we ﬁrst interpret a rooted tree as an oriented tree in which all
edges are directed towards the root (such a tree is sometimes called
an in-branching).
root
We note that the root is the unique vertex that is not the tail of any
arrow (directed edge); in other words, the unique vertex of outdegree
0. Moreover, any orientation of a tree with exactly one vertex of
outdegree 0 corresponds to a (unique) rooted tree in this way.
Now we consider PARTs in this oriented interpretation and we
want to count the number of PARTs that can be obtained from
empty graph on V by successive addition of n −1 arrows.
The ﬁrst arrow has to join two diﬀerent vertices, and thus it can
be chosen in n(n−1) ways. The second arrow has to be diﬀerent from
the ﬁrst, and it has to satisfy an extra condition: Since all arrows
point towards the root, every vertex has outdegree at most 1, and
hence the tail of the second arrow has to be diﬀerent from the tail of
the ﬁrst one. The head of the second arrow can be chosen arbitrarily,
and so we have n(n −2) possibilities for the second arrow.

260
The number of spanning trees
What are the restrictions for selecting the kth arrow? We have to
obey the following two rules:
(A) We must not create a cycle (in the corresponding undirected
graph; i.e. disregarding the orientation of the edges). Thus, each
newly selected arrow has to connect two diﬀerent components
of the graph created so far (by components we again mean the
components of the underlying undirected graph).
(B) At the end of this process every vertex, with the single exception
of the root, has to be the tail of exactly one of the selected arrows.
Since we have only n −1 arrows at disposal, we must not waste
any of them and thus no two arrows may have the same tail.
Here is a key observation: Every component of the current graph at
every step has exactly one vertex of outdegree 0. This is because a
component with m vertices has exactly m−1 arrows (since it is a tree
by (A)), and because each vertex always has outdegree at most 1.
It follows that if we select k arrows respecting rules (A) and (B),
then the current graph has n −k components (we leave the veriﬁca-
tion to the reader). The next picture illustrates the situation after
selecting the ﬁrst k = 4 arrows of the PART from the previous pic-
ture:
1
2
3
4
The next arrow, with label k + 1, can terminate in an arbitrary
vertex. The tail has to be chosen as a root of one of the components,
and this component must be diﬀerent from the one containing the
head. Hence we have n(n −k −1) possibilities for choosing the kth
arrow.
After n −1 steps of this procedure we arrive at a PART, and
every PART is obtained exactly once in this way. Thus the number
of PARTs is
n−2

k=0
n(n −k −1) = (n −1)! nn−1.
Comparing this with the expression n(n −1)! κ(n) derived earlier,
we arrive at Cayley’s formula: κ(Kn) = nn−2.

9
Finite projective planes
Mathematicians are often interested in objects that are highly reg-
ular in some sense. A good example is the regular Platonic solids
we have seen in Section 6.3. They are scarce, beautiful, and also
very useful. For instance, symmetry groups related to them play an
important role in physics.
In this chapter, we will consider certain very regular families
of ﬁnite sets, the so-called ﬁnite projective planes. As the name
suggests, this notion has a geometric inspiration. Finite projective
planes are highly symmetric and also somewhat rare. Appreciating
their mathematical usefulness, and maybe also their beauty, requires
learning something about them ﬁrst. Let us remark that the study
of regular conﬁgurations of sets with a ﬂavor somewhat similar to
ﬁnite projective planes is an extensive branch of combinatorial math-
ematics (we will say a little more about such objects in Chapter 13).
Suitable further reading for the present chapter is Van Lint and
Wilson [7].
9.1
Deﬁnition and basic properties
A ﬁnite projective plane is a system of subsets of a ﬁnite set with
certain properties.
9.1.1 Definition (Finite projective plane). Let X be a ﬁnite set,
and let L be a system of subsets of X. The pair (X, L) is called a
ﬁnite projective plane if it satisﬁes the following axioms:
(P0) There exists a 4-element set F ⊆X such that |L ∩F| ≤2
holds for each set L ∈L.
(P1) Any two distinct sets L1, L2 ∈L intersect in exactly one
element, i.e. |L1 ∩L2| = 1.
(P2) For any two distinct elements x1, x2 ∈X, there exists exactly
one set L ∈L such that x1 ∈L and x2 ∈L.

262
Finite projective planes
If (X, L) is a ﬁnite projective plane, we call the elements of X
points and the sets of L lines. If x ∈X is a point and L ∈L a line
and x ∈L, we say that “the point x lies on the line L” or also that
“the line L passes through the point x” and similarly.
If we express the axioms (P0)–(P2) in this new language, they
start resembling familiar geometric statements. Axiom (P1) says that
any two distinct lines meet at exactly one point (of course, in the
usual planar geometry, this is not quite true—there is the exception
of parallel lines!). Axiom (P2) tells us that there is exactly one line
passing through any two distinct points. Axiom (P0) then requires
the existence of 4 points such that no 3 of them are collinear. This
axiom is of a somewhat auxiliary nature and it serves just for ex-
cluding a few “degenerate” types of set systems that satisfy (P1) and
(P2) but are rather uninteresting.
If a, b ∈X are two distinct points of a ﬁnite projective plane,
the unique line L ∈L containing both a and b will be denoted by
the symbol ab. If L, L′ ∈L are distinct lines, the unique point of
L1 ∩L2 is called their intersection (although, strictly speaking, the
intersection of L1 and L2 in the usual sense is a one-point set).
Finite projective planes are a ﬁnite analogy of the so-called pro-
jective plane (more exactly, real projective plane) studied in geometry.
The terminology introduced above (“points”, “lines”, etc.) follows this
analogy. Thus, we make a short detour and mention what a real projec-
tive plane is. First, we should perhaps remark that the adjective “real”
indicates that the real projective plane is constructed from the set of
real numbers, and not that the other kinds of projective planes would
be somehow faked.
In the usual (Euclidean) plane, any two lines intersect at a single
point, but with an exception: parallel lines do not intersect at all. In
many geometric considerations, such an exception is unpleasant, since it
may require treating many special cases, both in proofs and in analytic
calculations. The real projective plane is a suitable extension of the
Euclidean plane by a set of additional points called the points at inﬁnity.
Roughly speaking, each direction of lines in the plane corresponds
to one point at inﬁnity, and all the lines parallel to that direction are
deﬁned to intersect at this point. All the points at inﬁnity lie on a single
line at inﬁnity. In this way, one achieves that now every two distinct
lines intersect at a single point (which can lie at inﬁnity, of course), and
hence all the axioms (P1), (P2), and (P0) hold in the real projective
plane.
The points at inﬁnity are no philosophical mystery here. The pro-
jective plane is a mathematical construction of a similar kind as, for

9.1 Deﬁnition and basic properties
263
example, producing rational numbers from the integers or real num-
bers from the rationals, i.e. a kind of completion. Readers interested
in a detailed construction of the real projective plane will ﬁnd it in
Section 9.2.
We still owe the reader an explanation of the adjective “projective”
in the term projective plane. First, we should indicate what is a pro-
jective transform. Consider two planes ρ and σ in the 3-dimensional
Euclidean space and a point c lying neither on ρ nor on σ, and project
each point of ρ from the point c into the plane σ. This deﬁnes a map-
ping, called a projective transform, of ρ into σ. Or does it? Well, not
quite: if x ∈ρ is a point such that the segment cx is parallel to the plane
σ then, in the usual Euclidean geometry, the image of x is undeﬁned.
But if we complement both ρ and σ by lines at inﬁnity so that they
become projective planes, then this projective transform is a bijection
between these two projective planes. If both ρ and σ are considered
as copies of the same plane, we can regard this mapping as a bijec-
tive mapping of the projective plane onto itself. The projective plane
is thus an appropriate domain for doing projective geometry, a branch
of geometry concerned with properties of geometric objects and con-
ﬁgurations that are preserved by projective transforms. For example,
projective transforms map conic sections (circles, ellipses, hyperbolas,
and parabolas) to conic sections, but an ellipse can be transformed to a
hyperbola, etc., and an elegant uniﬁed theory of conic sections can be
built in the projective geometry.
The analogy of the ﬁnite projective planes with the real projec-
tive plane is useful as a motivation for various notions, and often also
for intuition (we can draw geometric pictures). Geometric considera-
tions made in the real projective plane and using axioms (P0), (P1),
and (P2) only can be carried out in ﬁnite projective planes as well. It
should not be forgotten, though, that a ﬁnite projective plane is only
a system of ﬁnite sets with properties (P0)–(P2) and nothing else, and
hence other geometric notions cannot be transferred to it automatically.
For instance, there is no good notion of distance in a ﬁnite projective
plane, and hence it is not clear what a “circle” should mean. Another
important diﬀerence is that in the “usual” geometric plane, the points
of each line are naturally ordered “along” that line, but no such ordering
can be reasonably introduced in ﬁnite projective planes.
As was remarked above, ﬁnite projective planes are rare creatures,
and, given only the deﬁnition, it is not easy to discover any example
at all (try it if you don’t believe). Even the smallest example is
interesting.
9.1.2 Example. The smallest possible example of a ﬁnite projective
plane has 7 points and 7 lines, each line with 3 points, and it is called

264
Finite projective planes
1
2
3
4
5
6
7
a
b
c
d
e
f
g
Fig. 9.1 The Fano plane.
the Fano plane. It is shown in Fig. 9.1; the points are marked by
dots labeled 1–7, and the three points of each line are connected by
a segment, in one case by a circle arc.1 These connecting lines are
labeled by a–g in the ﬁgure.
The Fano plane, although small, is a useful mathematical object (see
Section 9.4 for one application), and it can also appear as a solution to
various puzzles or even totally serious problems, such as the following
one. Seven policemen were transferred to the 87th precinct from various
districts. A good opportunity for them to get acquainted with each
other is to watch the Plano Bar on Southwest C Drive, which otherwise
is a simple and somewhat boring duty as the bar’s patrons are mostly
computer criminals, digital money counterfeiters, and the like. A shift
of three men is required for that service, seven days a week. How can a
weekly schedule of shifts be arranged in such a way that every two of
the seven men have a common shift? The Fano plane provides a good
solution (points correspond to policemen and the shifts are the lines,
arranged in some order); see Fig. 9.2. Everyone has the same number
of shifts, no one has more than two consecutive days, but on the other
hand, every shift has one man who was also present the previous day
and knows what was going on, etc.2 We are not aware of such a schedule
actually being used by police forces, but, for instance, some motorcycle
races are organized according to a scheme based on an aﬃne plane of
order four (an aﬃne plane is a concept closely related to projective
planes; see Exercise 10 for a deﬁnition).
1It can be shown that 7 points cannot be drawn in the Euclidean plane in
such a way that each triple corresponding to a line in the Fano plane lies on a
Euclidean straight line—see Exercise 11.
2A quiz for aﬁcionados of classical detective stories: can you recall where the
seven policemen in Fig. 9.2 come from, and the names of their more famous rivals
or partners, the great detectives?

9.1 Deﬁnition and basic properties
265
Weekly schedule for location: Plano Bar
Mon
Cramer
Hoong
Japp
Tue
Cramer
Holcomb
Lestrade
Wed
Holcomb
Hoong
Janvier
Thu
Cramer
Janvier
Parker
Fri
Holcomb
Japp
Parker
Sat
Janvier
Japp
Lestrade
Sun
Hoong
Lestrade
Parker
Oﬃcer’s signature:
Fig. 9.2 Allocating 7 persons to 7 shifts by 3.
We now prove several propositions showing that in a construction
of a ﬁnite projective plane, our freedom is much more restricted than
might appear at ﬁrst sight.
9.1.3 Proposition. Let (X, L) be a ﬁnite projective plane. Then all
its lines have the same number of points; that is, |L| = |L′| for any
two lines L, L′ ∈L.
Proof.
Choose two lines L, L′ ∈L arbitrarily. First we prove an
auxiliary claim: There exists a point z ∈X lying neither on L nor
on L′.
Proof of the auxiliary claim. Consider a set F ⊆X as in the axiom
(P0). We have |L ∩F| ≤2 and |L′ ∩F| ≤2. If F is not contained in
L∪L′ we are done. The only remaining possibility is that L intersects
F at 2 points (call them a, b) and L′ intersects F at the 2 remaining
points (denote them by c, d). Then we consider the lines L1 = ac
and L2 = bd. Let z be the intersection of L1 and L2.
The following geometric picture illustrates the situation:
a
b
L
L′
c
d
z
L1
L2

266
Finite projective planes
Of course, we have to be very careful to use only conditions (P0)–
(P2), rather than some extra geometric information from the picture.
In many respects, ﬁnite projective planes “look” quite diﬀerent from
the geometric (Euclidean) plane.
We assert that z ̸∈L∪L′. The lines L and L1 intersect at a single
point, namely at a, and so if z ∈L, we would get z = a. But this is
impossible since then the line L2 would contain the points z = a, b,
and d, which are 3 points of F. This is forbidden by the condition
(P0). Therefore z ̸∈L, and analogously one can show that z ̸∈L′.
This ﬁnishes the proof of the auxiliary claim.
Now we show that the lines L and L′ have the same size. To this
end, we deﬁne a mapping ϕ: L →L′; it will turn out that it is a
bijection. We ﬁx a point z ̸∈L ∪L′ and deﬁne the image ϕ(x) of
a point x ∈L as the intersection of the lines zx and L′, as in the
following picture:
L
L′
z
x
ϕ(x)
By axioms (P1) and (P2), the point ϕ(x) is well deﬁned. Next, we
check that ϕ is a bijection. If y ∈L′ is an arbitrary point, we consider
the line zy, and let x be its intersection with the line L. Then the
lines zy and zx coincide, and hence we have y = ϕ(x). The mapping
ϕ is a bijection as claimed and so |L| = |L′|.
2
9.1.4 Definition (Order of a projective plane). The order of a
ﬁnite projective plane (X, L) is the number |L| −1, where L ∈L is
a line (according to the proposition just proved, the order doesn’t
depend on the choice of the line L).
For example, the Fano plane has order 2 (the lines have 3 points),
and it can be shown that it is the only projective plane of order 2 (up
to renaming of the points, i.e. up to an isomorphism). Subtracting 1
from the line size in the deﬁnition of the order may seem odd, but
this deﬁnition of order is very natural in other connections, e.g. for
aﬃne planes (Exercise 10) or for Latin squares (Section 9.3).
We continue proving properties of ﬁnite projective planes.

9.1 Deﬁnition and basic properties
267
9.1.5 Proposition. Let (X, L) be a projective plane of order n.
Then we have
(i) Exactly n + 1 lines pass through each point of X.
(ii) |X| = n2 + n + 1.
(iii) |L| = n2 + n + 1.
Proof of (i). Consider an arbitrary point x ∈X. First we observe
that there exists a line L that doesn’t contain x. That is, if F is the
4-point conﬁguration as in (P0) and a, b, c ∈F are points distinct
from x, then at least one of the lines ab and ac doesn’t contain x, as
is very easy to check.
Fix such a line L, x ̸∈L. For each point y ∈L, we consider the
line xy; these are n + 1 lines passing through x. On the other hand,
any line containing x intersects L at some point y ∈L and hence it is
counted among the above-mentioned n + 1 lines. Therefore, exactly
n + 1 lines pass through x.
Proof of (ii). We choose some L = {x0, x1, x2, . . . , xn} ∈L and a
point a ̸∈L, as in the following picture:
a
xn
x2
x1
x0
...
L0
L1
L2
Ln
L
Let Li denote the line axi, i = 0, 1, . . . , n. According to (P1), any two
of these lines, Li and Lj, intersect at a single point, and this is the
point a. The lines L0, L1, . . . , Ln each have n more points besides a,
and hence they together contain (n + 1)n + 1 = n2 + n + 1 distinct
points. It remains to show that any point x ∈X \ {a} already lies
on some of the lines Li. By (P1), the line ax intersects the line L at
some point, xi, and by (P2), the line ax must thus coincide with L.
This proves part (ii).
We omit the proof of part (iii) for now. In the sequel, we will
learn an important principle, and from it we will see that (iii) follows
immediately from what we have already proved.
2
Duality. The meaning of duality in projective planes is “exchanging
the roles of points and lines”. In order to formulate this exactly, we
ﬁrst introduce the so-called incidence graph of a ﬁnite projective

268
Finite projective planes
1
2
3
4
5
6
7
a
b
c
d
e
f
g
1
2
3
4
5
6
7
a
b
c
d
e
f
g
Fig. 9.3 The Fano plane and its incidence graph.
plane. In fact, the incidence graph can be deﬁned for an arbitrary
system S of subsets of a set X. The incidence graph is a bipartite
graph with vertex set X ∪S, where each set S ∈S is connected
by an edge to all points x ∈S. Consequently, each point x ∈X is
connected to all the sets containing it. Brieﬂy, we can say that the
edges of the incidence graph correspond to the membership relation
“∈”. The incidence graph of the Fano plane is shown in Fig. 9.3; the
vertices are labeled by the labels of the corresponding points and
lines. (By the way, the resulting graph is a pretty and important
graph, although the drawing in our ﬁgure is rather ugly, and it even
has a name: the Heawood graph.)
Given a ﬁnite projective plane (X, L), the dual of (X, L) is
obtained by taking the incidence graph of (X, L) and interpreting
as lines the vertices that were understood as points, and conversely,
vertices that used to be sets start playing the role of points. In
Fig. 9.3, we could just ﬂip the graph upside down. Hence L is now
thought of as a point set, and for each point x ∈X, the set of lines
{L ∈L: x ∈L} is interpreted as a line. For the Fano plane example,
the points of the dual are thus {a, b, . . . , g}, and the lines of the dual
are {a, c, e} (for the point 1 in the Fano plane), {a, d, g} (for the
point 2), and so on.
Proposition. The dual of a ﬁnite projective plane is itself a ﬁnite
projective plane.
Proof. Let (X, L) be a ﬁnite projective plane. The dual of (X, L) is a
pair (L, Λ), where Λ is a system of subsets of L, each of these subsets
corresponding to some point of X. (Note that distinct points always
yield
distinct
subsets
of
L,
since
two
points
share
only
one line.)

9.1 Deﬁnition and basic properties
269
We have to verify conditions (P0)–(P2) for (L, Λ). We begin with
the condition (P0). If this condition is translated into the language
of the original set system (X, L), it means that we should ﬁnd 4 lines
L1, L2, L3, L4 ∈L such that no 3 of them have a point in common. To
this end, let us consider a 4-point conﬁguration F = {a, b, c, d} ⊆X
as in the condition (P0), and deﬁne L1 = ab, L2 = cd, L3 = ad,
L4 = bc. If we look at any 3 of these 4 lines, any two of them share
one point of F, and this point is not contained in the third one.
Hence any 3 of the lines L1, . . . , L4 have an empty intersection, and
we have conﬁrmed the validity of the condition (P0) for the dual set
system.
The condition (P1) formulated for the dual (L, Λ) requires the fol-
lowing: if x, x′ ∈X are two distinct points, then there exists exactly
one line L ∈L containing both x and x′. This is exactly condition
(P2) for (X, L)! Similarly, we ﬁnd that (P2) for the dual is a conse-
quence of (P1) for the original projective plane (X, L).
2
Now we can call the dual of a ﬁnite projective plane the dual
projective plane. Proposition 9.1.5(i) implies that the dual projective
plane has the same order as the projective plane we started with.
Also, one can see that parts (ii) and (iii) of Proposition 9.1.5 are
dual to each other, and if we prove one of them, the other one must
be valid too.
In general, if we have some valid statement about ﬁnite projective
planes of order n, and if we interchange the words “point” and “line”
everywhere in it, we get a valid statement again. To get a meaningful
sentence, we may have to rephrase other parts as well. For instance,
if the original statement said “lines L1, L2 intersect at the point x”,
we should say “points x1, x2 are connected by the line L” in the
dual statement, etc. Hence, we have a “recipe for producing new
theorems” which gives us about half of the theorems in projective
geometry for free! It is sometimes called the duality principle, and
it was noted by geometers studying the real projective plane a long
time ago.
Exercises
1. Prove that the Fano plane is the only projective plane of order 2 (i.e.
any projective plane of order 2 is isomorphic to it—deﬁne an isomor-
phism of set systems ﬁrst).

270
Finite projective planes
2. ∗Construct a projective plane of order 3 (before reading the next sec-
tion!).
3. (a) Find an example of a set system (X, L) on a nonempty ﬁnite set
X that satisﬁes conditions (P1) and (P2) but doesn’t satisfy (P0).
(b) Find an X and L as in (a) such that |X| ≥10, |L| ≥10, and each
L ∈L has at least 2 points.
(c) ∗Describe all set systems (X, L) as in (a).
4. Let X be a ﬁnite set and let L be a system of subsets of X satisfying
conditions (P1), (P2), and the following condition (P0′):
There exist at least distinct lines L1, L2 ∈L having at least 3
points each.
Prove that any such (X, L) is a ﬁnite projective plane.
5. Prove part (iii) of Proposition 9.1.5 directly, without using duality.
6. Show that the number of sets in a set system consisting of 3-element
sets on a 9-point set, such that no two sets share more than one point,
is at most 12. ∗Find an example with 12 sets.
7. ∗Is it possible to arrange 8 bus routes in a city so that
(i) if any single route is removed (doesn’t operate, say) then any stop
can still be reached from any other stop, with at most one change, and
(ii) if any two routes are removed, then the network becomes discon-
nected?
8. ∗Let X be a set with n2 +n+1 elements, n ≥2, and let L be a system
consisting of n2 + n + 1 subsets of X of size n + 1 each. Suppose that
any two distinct sets of L intersect in at most one point. The goal is to
prove that (X, L) is a ﬁnite projective plane of order n. The following
sequence of auxiliary statements give one possible way of arranging
the proof.
(a) Prove that each pair or points of X is contained in exactly one set
of L (use double-counting).
(b) Prove that at most n + 1 sets contain any given point.
(c) Prove that each point is contained in exactly n + 1 sets.
(d) Prove that any two sets of L intersect.
(e) Verify that (X, L) is a projective plane of order n.
9. ∗Let (X, L) be a projective plane of order n, and let A ⊆X be a set
with no 3 points lying on a common line. Prove that |A| ≤n + 2 (for
n odd, it can even be shown that |A| ≤n + 1).

9.2 Existence of ﬁnite projective planes
271
10. (Aﬃne planes) Let us deﬁne an aﬃne plane as a pair (X, A), where X
is a set and A is a collection of subsets of X (called lines) satisfying
the following axioms: there exist 3 points not contained in a common
line, any two distinct points are contained in exactly one line, and for
any point p and line A ∈A with p ̸∈A, there exists exactly one line
A′ with A′ ∩A = ∅and p ∈A′.
(a) Check that the axioms of an aﬃne plane are valid for the usual
Euclidean plane. Find out how to construct a ﬁnite aﬃne plane from
a ﬁnite projective plane.
(b) Deﬁne relation ∥on A by letting A1 ∥A2 if and only if A1 = A2
or A1 ∩A2 = ∅. Prove that ∥is an equivalence.
(c) ∗Analogously to our treatment of projective planes, prove that all
lines in a ﬁnite aﬃne plane have the same cardinality n, and that such
an aﬃne plane has n2 + n lines and n2 points, with n + 1 lines passing
through each point.
(d) ∗Show that from any aﬃne plane of order n, one can construct a
projective plane of order n.
11. Show that the Fano plane cannot be embedded into the Euclidean
plane. That is, there exist no 7 points and 7 lines in the Euclidean
plane such that each pair of the points lies on one of the lines and each
pair of the lines intersects at one of the points. Use Exercise 6.3.8.
9.2
Existence of ﬁnite projective planes
Projective planes of orders 2, 3, 4, and 5 exist. But there is no projec-
tive plane of order 6! (Proving this is not easy; see e.g. Van Lint and
Wilson [7].) Projective planes of orders 7, 8, 9 again exist, but none
of order 10. Is there any regularity in this? It turns out that a projec-
tive plane of order n exists whenever a ﬁeld with n elements exists.
Here a ﬁeld is meant in the algebraic sense; that is, it is a set with
operations of addition, subtraction, multiplication, and division that
satisfy certain axioms—check the Appendix if you are not sure about
the deﬁnition. As algebra teaches us, an n-element ﬁeld exists if and
only if n is a power of a prime number. This means, in particular,
that projective planes of arbitrarily large orders exist.
For an n divisible by at least two distinct primes, no n-element
ﬁeld exists, but nevertheless it is not known whether a projective
plane of order n may exist in some such cases or not. There are some
partial negative results; for instance, if the number n divided by 4
gives remainder 1 or 2 and it cannot be written as a sum of two
squares of integers then no projective plane of order n exists (this
is not an easy theorem). This rules out the existence of projective

272
Finite projective planes
planes of orders 6, 14, and many others, but it doesn’t by far cover
all possible orders. For instance, it says nothing about orders n = 10
or 12.
The existence of a projective plane of order 10 has also been
excluded. These results have an interesting history, For order 6, a
proof was already attempted by Euler, but only Tarry gave a convinc-
ing argument around 1900. For order 10, a proof was done recently
using enormous computer calculations. For the next higher order,
12, the existence of a projective plane remains an open problem. It
is clear that this problem can be solved by checking a ﬁnite number
of conﬁgurations, but the number of conﬁgurations seems to be too
gigantic for contemporary computing technology.
An algebraic construction of a projective plane. For readers with
an interest in the subject and with a little background in introductory
algebra, we explain how does one construct a projective plane from a
ﬁeld. We are particularly interested in ﬁnite projective planes, but the
construction works in exactly the same way for the real projective plane
(and, in general, for any ﬁeld).
may look slightly complicated, since the points of the resulting pro-
jective plane are certain equivalence classes on a set of ordered triples.
However, once one gets used to the deﬁnitions, verifying the axioms of
a projective plane is straightforward. In the case where the ﬁeld is R,
the real numbers, the construction has a quite intuitive geometric inter-
pretation, which we present in the end. For some people the geometric
interpretation may help in orientation in the formal construction, but
for others it might perhaps cause even more confusion, so we leave it to
the reader’s taste how much emphasis should be put on the geometric
intuition.
The construction starts with some ﬁeld K. For the real projective
plane (i.e. a suitable extension of the usual Euclidean plane by points
at inﬁnity), we take the ﬁeld R of all real numbers for K. If we choose
an n-element ﬁeld for K, the construction results in a ﬁnite projective
plane of order n. Our running example is the 3-element ﬁeld K, i.e. the
set {0, 1, 2} with arithmetic operations modulo 3.
First we consider the set T = K3 \ {(0, 0, 0)}; that is, the set of all
ordered triples (x, y, t) where x, y, t ∈K and x, y, t are not all simulta-
neously equal to 0. On this T, we deﬁne an equivalence relation ≈as
follows: (x1, y1, t1) ≈(x2, y2, t2) if and only if a nonzero λ ∈K exists
such that x2 = λx1, y2 = λy1, and t2 = λt1 (it is not at all diﬃcult
to check that this is indeed an equivalence). Points of the constructed
projective plane are the classes of this equivalence. The projective plane

9.2 Existence of ﬁnite projective planes
273
thus produced3 is usually denoted by PK2 in the literature, where one
may write the speciﬁc ﬁeld being used instead of K. For instance, the
real projective plane is usually denoted by PR2.
In order to gain a better intuition about the projective plane, we
select one representative triple from each class of the equivalence ≈. For
these representatives, we choose the triples whose last nonzero compo-
nent equals 1. Hence, the representatives are triples of the types (x, y, 1),
(x, 1, 0) (for some x, y ∈K), and the triple (1, 0, 0). It is easy to
convince oneself that any other triple is equivalent to a triple of the
above-mentioned form, and also that no two of the triples chosen as
representatives are equivalent under ≈.
It would be cumbersome to keep speaking about equivalence classes
all the time. Thus, we will say “a point (x, y, t)” in the sequel, meaning
the whole equivalence class containing (x, y, t).
If K is an n-element ﬁeld, we can now count how many points we ob-
tain. The number of points of the form (x, y, 1) is n2, there are n points
of the form (x, 1, 0), and, moreover, we have the one point (1, 0, 0)—
altogether n2 + n + 1 as it should be. For n = 3, all the points with
their labels are drawn in the following diagram:
0
1
2
010 110 210 100
0
1
2
The points with the last coordinate equal to 1 are arranged into a
natural 3 × 3 grid pattern.
Now it is time to deﬁne the lines. For each triple (a, b, c) ∈K3 \
{(0, 0, 0)}, we deﬁne a line L(a, b, c) as the set of all points (x, y, t) of
our projective plane satisfying the equation
ax + by + ct = 0.
(9.1)
Obviously, two equivalent triples (x, y, t) and (λx, λy, λt) either both
satisfy this equation or none does, and hence we have indeed deﬁned a
certain set of points of the projective plane. Also, one can see that for
all nonzero λ ∈K, the triples (λa, λb, λc) deﬁne the same line as the
triple (a, b, c). Hence, on the triples that deﬁne lines we have exactly
3For a ﬁnite ﬁeld, we haven’t yet proved that we obtain a ﬁnite projective
plane in the sense of Deﬁnition 9.1.1, and we haven’t even deﬁned what the lines
are, so strictly speaking, we shouldn’t yet call the object being constructed a
projective plane. But, being (hopefully) among friends, there is no reason for
such a total strictness, is there?

274
Finite projective planes
the same equivalence relation as on the triples that deﬁne points. We
can select the same representative triples as we did for the points, i.e.
triples whose last nonzero component is 1. In the following picture, we
have drawn all the lines passing through the point (0, 0, 1) labeled by
their representative triples. We have omitted the labels of most of the
points (they are as in the preceding diagram):
001
100
010
110
210
In order to show that for an n-element ﬁeld, we have really constructed
a ﬁnite projective plane of order n, we must check conditions (P0)–(P2).
We begin with the condition (P1) (any two distinct lines intersect at a
single point). So let (a1, b1, c1) and (a2, b2, c2) be two triples that are
not equivalent, i.e. one is not a multiple of the other.
We could now directly calculate the point of intersection of the two
considered lines (this is a solution of a small system of linear equations)
and verify its uniqueness. We give another proof relying on some basic
results of linear algebra. A reader who tries to do the proof by a direct
calculation may gain a certain new appreciation of linear algebra.
Let us regard the triples (a1, b1, c1) and (a2, b2, c2) as 3-dimensional
vectors over the ﬁeld K. Both are nonzero vectors, and the fact that one
is not a multiple of the other means that they are linearly independent.
Hence, the matrix

a1
b1
c1
a2
b2
c2

has rank 2 (linear independence and rank are over the ﬁeld K). Let
us now view the columns of this matrix as 2-dimensional vectors. We
know that 3 vectors in a 2-dimensional space must necessarily be linearly
dependent, which means that there exist 3 numbers x, y, t ∈K, not all
of them simultaneously 0, and such that
x(a1, a2) + y(b1, b2) + t(c1, c2) = (0, 0).
(9.2)
If we rewrite this for each coordinate separately, we obtain that the
point (x, y, t) lies on both the considered lines.
On the other hand, since the rank of the considered matrix is 2, two
linearly independent columns have to exist. Suppose, for instance, that
they are (a1, a2) and (b1, b2). This means that for any vector (u, v), the

9.2 Existence of ﬁnite projective planes
275
Fig. 9.4 The real projective plane in moonlight.
equation x(a1, a2) + y(b1, b2) = (u, v) has a unique solution. In other
words, if we prescribe the value of t in Eq. (9.2), the values of x and y
are already determined uniquely, and so all solutions to this equation
are multiples of a single vector. This means that the two considered
lines intersect at a single point.
This argument can be expressed in a somewhat more learned and
more concise way. The linear mapping sending a vector (x, y, t) ∈K3
to the vector x(a1, a2) + y(b1, b2) + t(c1, c2) ∈K2 has rank 2, hence it
is onto and its kernel is 1-dimensional.
We have proved (P1). The condition (P2) could be proved in a simi-
lar way, or we can say right away that the roles of the triples (a, b, c) and
(x, y, z) in Eq. (9.1) are completely symmetric, and so we have (again)
a duality between lines and points. Finally, veriﬁcation of the condition
(P0) is left to the reader.
2
Geometric interpretation.
As we mentioned in the beginning of this
chapter, the basic idea of the construction of the real projective plane
is to extend the usual Euclidean plane by points at inﬁnity, so that for
each direction of lines in the Euclidean plane there is one corresponding
point at inﬁnity (not two!), where all of these parallel lines intersect.
So points at inﬁnity can be imagined as points “on the horizon” in a
drawing of the Euclidean plane in perspective; see Fig. 9.4, where circles
correspond to points with integer coordinates.
In order to make this kind of picture more formal, we consider
the three-dimensional space with Cartesian coordinate system (x, y, t),
where the t-axis is vertical. The Euclidean plane is placed in this

276
Finite projective planes
three-dimensional space as the horizontal plane with equation t = 1,
shown gray in the next picture:
t = 1
t = 0
0
ℓ1
ℓ2
ℓ3
a1
a2
Each point a of this plane corresponds to a line ℓ= 0a passing through
the origin of the three-dimensional space. For example, the point a1 in
the picture is assigned the line ℓ1. Conversely, each line through the
origin of the three-dimensional space corresponds to exactly one point
of the gray plane, except, of course, for horizontal lines, such as ℓ3.
We note that when we start tilting the line ℓ1 towards the position ℓ2
and further towards the horizontal position ℓ3, the corresponding point
in the gray plane recedes to inﬁnity along the dashed line. So it is not
unnatural to think that horizontal lines such as ℓ3 correspond to points
at inﬁnity, to those which we would like to add to the Euclidean plane.
And here we use one of the typical tricks of modern mathematics—
instead of explaining what are the points at inﬁnity that correspond to
the horizontal lines, we forget about the original Euclidean plane, and
we say that the points of the projective plane are lines in the three-
dimensional space passing through the origin. We haven’t quite said so
in the formal construction of PK2 described above, but a point of the
projective plane was a set of the form {(λx, λy, λt): λ ∈K\{0}}, which
geometrically is exactly a line in K3 passing through the origin, except
that we have removed the origin for technical convenience.
Lines of the projective plane correspond to planes in the three-
dimensional space passing through the origin; this we leave to the reader
as food for thought.
Declaring that lines are points might sound strange, but readers who
have gone through a proper course of mathematical analysis probably
got used to oddities of this kind, for example, when told that a real
number is really a set of rational numbers, or that a rational number is
an equivalence class on the set of all pairs of integers, and so on. See,
for example, Gowers [19] for an excellent explanation the merits of this
approach.
Remark. The construction presented above might give the impression
that some points of the projective plane, those at inﬁnity, are some-
what special, diﬀerent from the others. This is not so; no points have
any special signiﬁcance, and the “inﬁnity” can in some sense be imag-
ined wherever convenient—the projective plane looks “locally every-
where the same”. This should be intuitively clear from the geometric

9.3 Orthogonal Latin squares
277
1
2
3
2
3
1
3
1
2
1
2
3
3
1
2
2
3
1
Fig. 9.5 Two orthogonal Latin squares of order 3.
interpretation above, where we identify the real projective plane with
the set of all lines through the origin in the three-dimensional space.
Then the horizontal lines, corresponding to the points at inﬁnity, have
no privileged position among the other lines.
Exercises
1. ∗∗,CS Prove the nonexistence of a projective plane of order 6. Write a
computer program for checking and excluding all conﬁgurations com-
ing into consideration. One has to proceed cleverly, since searching all
systems of 43 7-tuples on 43 points would take way too long. (Clever
proofs are known that do not need any case analysis but these are not
easy to discover.)
9.3
Orthogonal Latin squares
A Latin square of order n is a square table with n rows and n
columns. Each entry is a number from the set {1, 2, . . . , n}, and each
number in this set occurs exactly once in each row and also in each
column. Two 3 × 3 Latin squares are depicted in Fig. 9.5.
Now we say what it means when two Latin squares of the same
order are orthogonal. Imagine that one of the squares is printed on a
transparency and that we lay it over the other square in such a way
that the corresponding entries lie above one another. For example,
the squares from Fig. 9.5 overlaid produce
1 1
2 2
3 3
2 3
3 1
1 2
3 2
1 3
2 1
In this way, we get n2 ordered pairs, each pair being formed by an
entry of the square on the transparency and the corresponding entry
of the underlying square. The considered squares are orthogonal if no

278
Finite projective planes
ordered pair appears twice. Since the number of all possible ordered
pairs of numbers from 1 to n is n2 too, each pair has to appear
exactly once.
9.3.1 Theorem. Let M be a set of Latin squares of order n, such
that each two of them are orthogonal. Then |M| ≤n −1.
Proof. We begin with the following observation. Let A and B be
orthogonal Latin squares of order n, and let π be some permutation
of the numbers 1, 2, . . . , n. Let us make a new Latin square A′, whose
entry at position (i, j) is the number π(aij), where aij is the entry at
position (i, j) of the square A. By the deﬁnition of orthogonality, it is
not hard to see that A′ and B are also orthogonal Latin squares. This
observation can be summarized by the phrase “the orthogonality of
Latin squares doesn’t change by renaming the symbols in one of
them”.4
To prove the theorem, imagine we have Latin squares A1, A2,. . . ,
At, each two of them being orthogonal. For each Ai, permute its
symbols (i.e. the numbers 1, 2, . . . , n) in such a way that the ﬁrst
row of the resulting Latin square A′
i is (1, 2, . . . , n). By the above
observation, the Latin squares A′
1, . . . , A′
t are still pairwise orthogo-
nal. Let us look at which numbers can occupy the position (2, 1) of
the square A′
i. First of all, this entry must not be 1, because the ﬁrst
column already has a 1 in the ﬁrst row. Further, no two squares A′
i
and A′
j may have the same numbers at position (2, 1): if they did, by
overlaying A′
i and A′
j we would get a pair of identical numbers, say
(k, k), at the position (2, 1), but this pair has already appeared at
the kth entry of the ﬁrst row! Hence each of the numbers 2, 3, . . . , n
can only stand at the position (2, 1) in one of the A′
i, and therefore
t ≤n −1.
2
A reader who has been wondering why we, suddenly, started talk-
ing about Latin squares in a chapter on projective planes may per-
haps be satisﬁed by the next theorem:
4In fact, one is not really obliged to ﬁll Latin squares only with numbers
1, 2, . . . , n. Equally well, one can use n distinct letters, n diﬀerent kinds of Cognac
glasses, and so on. This is also perhaps the appropriate place to address the
somewhat puzzling question: why are Latin squares called Latin? It seems that
in some traditional problems, the symbols written in the considered square tables
used to be Latin letters (while some other squares were ﬁlled with Greek letters
and called—what else?—Greek squares).

9.3 Orthogonal Latin squares
279
9.3.2 Theorem. For any n ≥2, a projective plane of order n exists
if and only if there exists a collection of n −1 mutually orthogonal
Latin squares of order n.
Proof of Theorem 9.3.2. We will not do it in detail. We only describe
how to construct a projective plane from orthogonal Latin squares and
vice versa. Given n−1 orthogonal Latin squares S1, . . . , Sn−1 of order n,
we will produce a projective plane of order n.
First we deﬁne the point set X of the constructed plane. It has n+1
points “at inﬁnity” denoted by r, c, and s1, s2, . . . , sn−1, and n2 points
(i, j), i, j = 1, 2, . . . , n. Next, we introduce the lines in several steps.
One line B = {r, c, s1, . . . , sn−1} consists of the points “at inﬁnity”.
Then we have the n lines R1, R2, . . . , Rn, where
Ri = {r, (i, 1), (i, 2), . . . , (i, n)},
and the n lines
Cj = {c, (1, j), (2, j), . . . , (n, j)}.
These are all drawn in the following picture (for n = 3):
r



@
@
@@
@
@
@
@



r
        







B
r
s1
r
s2
c
R1
r
r
r
r
r
r
R2
R3
r
r
r
r
C1
C2
C3
The points and lines of the projective plane we have drawn and
labeled so far must look exactly the same in any projective plane of
order n (we also haven’t yet used any information from the given n −1
orthogonal Latin squares). The squares will now specify the lines of the
projective plane passing through the points s1, s2, . . . , sn−1 (besides the
line B). As the chosen notation suggests, the Latin square Sk determines
the lines passing through the point sk. If (Sk)ij denotes the entry of Sk
in the ith row and jth column, we deﬁne the lines
Lkm = {sk} ∪{(i, j): (Sk)ij = m}
for m = 1, 2, . . . , n and k = 1, 2, . . . , n −1. For instance, if S1 were the
Latin square in Fig. 9.5 on the left, then the line L11 corresponding to
the number 1 in the square would be L11 = {s1, (1, 1), (2, 3), (3, 2)}.

280
Finite projective planes
This ﬁnishes the description of the ﬁnite projective plane corre-
sponding to the collection of n−1 orthogonal Latin squares. It remains
to verify the axioms of the projective plane. It is easy to calculate that
the total number of both lines and points is n2+n+1. By Exercise 9.1.8,
it is now suﬃcient to check that any two lines intersect in at most one
point. For this, one has to use both the facts that each Sk is a Latin
square and that any two of the Latin squares are orthogonal. We leave
this as an exercise.
To prove the equivalence in Theorem 9.3.2, we should also show how
to construct n −1 orthogonal Latin squares from a projective plane of
order n. This construction follows the same scheme as the converse one.
In the projective plane, we choose two distinct points r and c arbitrarily,
and we ﬁx the notation further as in the above construction. Then the
kth Latin square Sk is ﬁlled out according to the lines passing through
the point sk. So much for the proof of Theorem 9.3.2.
2
Remark. The proof actually becomes more natural when it is done with
ﬁnite aﬃne planes (see Exercise 9.1.10).
Exercises
1. Go through the construction in the proof of Theorem 9.3.2 for n = 2
(when only one Latin square exists, up to a permutation of the num-
bers). Check that we obtain the Fano plane in this way.
2. Verify that any two lines constructed in the proof of Theorem 9.3.2
intersect in at most one point (do not forget the lines Ri and Cj!).
Distinguish where one uses the deﬁnition of a Latin square and where
the orthogonality.
3. Show that the construction sketched at the end of the proof of Theo-
rem 9.3.2 indeed produces n −1 orthogonal Latin squares.
4. Deﬁne a liberated square of order n as an n × n table with entries
belonging to the set {1, 2, . . . , n}. Orthogonality of liberated squares
is deﬁned in the same way as for Latin squares. For a given number t,
consider the following two conditions:
(i) There exist t mutually orthogonal Latin squares of order n.
(ii) There exist t+2 mutually orthogonal liberated squares of order n.
(a) Prove that (i) implies (ii).
(b) ∗Prove that (ii) implies (i).
5. Let T be a ﬁnite ﬁeld with n elements. Denote its elements by t0,
t1, . . . , tn−1, where t0 = 0 and t1 = 1. For k = 1, 2, . . . , n −1, deﬁne
an n × n matrix S(k), where the entry of the matrix S(k) at position
(i, j) equals titk + tj (the multiplication and addition in this formula
are in the ﬁeld T). Prove that S(1), S(2), . . . , S(n−1) is a collection of

9.4 Combinatorial applications
281
mutually orthogonal Latin squares of order n. (Using Theorem 9.3.2,
this gives an alternative construction of a projective plane of order n.)
6. For natural numbers m ≤n, we deﬁne a Latin m × n rectangle as a
rectangular table with m rows and n columns with entries chosen from
the set {1, 2, . . . , n} and such that no row or column contains the same
number twice. Count the number of all possible Latin 2×n rectangles.
9.4
Combinatorial applications
In combinatorial mathematics, ﬁnite projective planes often serve as
examples of set systems with various remarkable properties. One can
say that if we have some hypothesis about ﬁnite set systems and look
for a counterexample to it, or if we want an example of a set system
with some prescribed properties, then it may be a good idea to try
a ﬁnite projective plane among the ﬁrst candidates.
Trying to document the usefulness of projective planes in math-
ematics and its applications resembles explaining the usefulness of
noodles, say, in the kitchen. Certainly one can do lots of great cooking
without them. Wonderful foods can also be prepared from noodles,
but it’s a question of a good recipe (and cook) and of adding various
subtle ingredients, spices, etc. Noodles by themselves probably seem
exciting to specialists only. Here we haven’t accumulated enough
mathematical ingredients and spices to prepare a nice but compli-
cated recipe–example, and so we will oﬀer two simple combinatorial
dishes only.
Just at the time this section was being written, newspapers reported
on the ﬁrst Swiss bank to introduce on-line banking over the Internet
(in the US, it had been around for more than a year). Of course, safety
of the information being interchanged over the public network is of ut-
most importance in such a case. The supposedly indecipherable codes
by which the banks expect to achieve safety5 are based on the so-called
elliptic curves over ﬁnite ﬁelds. The theory of elliptic curves has been
developed for a long time in number theory and algebraic geometry, tra-
ditionally considered absolutely pure mathematics without any applica-
bility whatsoever. And these elliptic curves are inhabitants (or subsets
if you prefer) of ﬁnite projective planes. So it goes.
The book by Koblitz [22] can serve both as an introduction to num-
ber theory and a gate to the world of mathematical cryptography. But
let us return to our combinatorial applications.
5Software products using some of these codes are also subject to US export
restrictions, just like high-tech weapons.

282
Finite projective planes
Coloring set systems by two colors. Let X be a ﬁnite set, and
let M be a system of subsets of X. We say that the set system M
is 2-colorable if it is possible to color each of the points of X by one
of two given colors, say red or white, in such a way that each set
M ∈M contains points of both colors (2-colorablility is often called
property B in the literature.)
For example, if X = {1, 2, 3} and M = {{1, 2}, {1, 3}, {2, 3}} then
M is not 2-colorable. More generally, if all the sets of M have exactly
2 elements, then we can regard (X, M) as a graph, and here being
2-colorable means exactly the same thing as being bipartite. What if
the sets of M have more than 2 points each? For instance, what can
be said if all the sets of M have exactly 3 points? It turns out that
the situation with 2-colorability becomes much more complicated
than for graphs. For instance, the question of whether a given M is
2-colorable or not becomes algorithmically diﬃcult.
Let us consider the following natural question: what is the small-
est number of sets of a set system M consisting of 3-point sets that is
not 2-colorable? It turns out that the answer is 7, and the Fano plane
provides one-half of this answer: it has 7 sets consisting of 3 points
each, and it is not 2-colorable (we leave a proof as Exercise 1). And,
in fact, it is the only set system with 7 triples that is not 2-colorable!
We will deal with the second half of the answer, i.e. methods for
showing that all systems with 6 or fewer triples are 2-colorable, in
Section 10.1 (Theorem 10.1.5).
One could talk much longer about 2-colorability, as it is a quite
important concept in combinatorics, but here we only wanted to
point out a little problem in this area where the projective planes,
surprisingly, come into play.
More on graphs without K2,2 with many edges. Theorem 7.3.1
tells us that if G is a graph on m vertices containing no K2,2 as
a subgraph, then G has at most 1
2

m3/2 + m

edges. Using ﬁnite
projective planes, we show that this bound is nearly the best possible
in general:
9.4.1 Theorem. For inﬁnitely many values of m, there exists a
K2,2-free graph on m vertices with at least 0.35 m3/2 edges.
Proof. Take a projective plane of order n, and consider its incidence
graph (as in the part concerning duality in Section 9.1). The number
of vertices of this graph is m = 2(n2 + n + 1). Each of the n2 + n + 1

9.4 Combinatorial applications
283
lines has n+1 points, and this means that the total number of edges
is (n2 + n + 1)(n + 1) ≥(n2 + n + 1)3/2 =
 m
2
3/2 ≈0.35m3/2.
What would it mean if the incidence graph contained a K2,2 as a
subgraph? Well, in the language of the projective plane, it would say
that there exist two points x, x′ and two lines L, L′ such that x ∈L,
x′ ∈L, x ∈L′, and x′ ∈L′. This cannot happen in a projective
plane.
2
Remark. The constant 0.35 in Theorem 9.4.1 can still be improved
somewhat. The optimal value is 0.5; see Exercise 2.
Exercises
1. Prove that the Fano plane is not 2-colorable.
2. (Better K2,2-free graphs) Let n be a prime power, and let K be an
n-element ﬁeld. Consider the equivalence classes of the equivalence
relation ≈on the set of triples K3 \ {(0, 0, 0)} (introduced in
Section 9.2). Let these classes be vertices of a graph G, and two
vertices (a, b, c) and (x, y, z) are connected by an edge if and only
if ax + by + cz = 0. Prove that
(a) the edges are well deﬁned,
(b) ∗the graph G contains no K2,2 as a subgraph,
(c) each vertex has degree at least n, and
(d) if m = n2 +n+ 1 denotes the number of vertices, then the number
of edges is at least 1
2m3/2 −m.
3. Let G be a bipartite graph with both vertex classes of sizes n and
containing no K2,2 as a subgraph.
(a) ∗By the method of Section 7.3, prove that G has at most
1
2n(1 +
√
4n −3)
edges.
(b) ∗Prove that such a G with precisely this number of edges exists if
and only if a projective plane of order q exists with n = q2 + q + 1.

10
Probability and probabilistic
proofs
The reader will most likely know various problems about determining
the probability of some event (several such problems are also scat-
tered in the other chapters). Textbooks often contain such problems
taken “from real life”, or at least pretending to be real life problems.
They speak about shuﬄing and drawing cards, tossing coins or even
needles, or also about defunct lightbulbs, defunct phone lines, or
decaying radioactive atoms—depending on the author’s inclinations
and fantasy. In this chapter we want to show a remarkable mathe-
matical application of probability, namely how mathematical state-
ments can be proved using elementary probability theory, although
they don’t mention any probability or randomness.
Alon and Spencer [12] is an excellent book for studying proba-
bilistic methods in combinatorics in more depth, and Grimmett and
Stirzaker [20] can be recommended as a probability theory textbook.
10.1
Proofs by counting
In two introductory examples, probability will not be mentioned; we
use a simple counting instead.
10.1.1 Example. Consider a new deck of 52 cards (in such a deck,
cards go in a certain ﬁxed order). We will shuﬄe the cards by so-
called dovetail shuﬄing: we divide the deck into two parts of equal
size, and we interleave the cards from one part with the cards from
the other part, in such a way that the order of cards from each
part is unchanged (see Fig. 10.1). We prove that if this procedure is
repeated at most 4 times, we cannot get all possible orderings of the
cards, and hence 4 rounds of dovetail shuﬄing certainly won’t yield
a random order of the cards.

10.1 Proofs by counting
285
Fig. 10.1 Dovetail shuﬄing.
Proof. There are 52! possible orderings of the cards. We count how
many diﬀerent orderings can be obtained by the described shuﬄing
procedure. How many ways are there to intermix the two separate
parts of the deck (let us refer to them as the left part and the right
part)? If we know the ordering in both the left and the right parts,
and if we specify which cards in the intermixed deck come from the
left part and which from the right part, we can already reconstruct
the ordering of the deck after the parts are intermixed. Hence, after
the ﬁrst dividing and interleaving there are
52
26

possible orderings,
and by 4 repetitions of this procedure we can thus obtain at most
52
26
4 orderings. A pocket calculator, or the estimates in Chapter 3,
can tell us that this number is smaller than 52!, and hence there
exists some ordering that cannot be obtained by 4 rounds of dovetail
shuﬄing.
2
The result just proved doesn’t say that we could get all possible
orderings by 5 rounds. The question of how many random dovetail
shuﬄes are needed to obtain an ordering reasonably close to a ran-
dom one is considerably more diﬃcult. A very sophisticated paper
on the subject is Bayer and Diaconis [33].
Difficult Boolean functions. A Boolean function of n variables is a
mapping f : {0, 1}n →{0, 1}, i.e. assigning either 0 or 1 to each possible
combination of n 0s and 1s. (Here 1 represents the logical value “true”
and 0 the logical value “false”.) A Boolean function can be speciﬁed by
a table of values, but also in many other ways. For example, a computer
program that reads n bits of input and computes a YES/NO answer
deﬁnes a certain Boolean function of n variables. Similarly, an inte-
grated circuit with n input wires and one output wire deﬁnes a Boolean
function of n variables, assuming that the inputs and the output each
have only two possible states.

286
Probability and probabilistic proofs
We want to show that there exist Boolean functions that require a
very large description, i.e. a very long program, or a circuit with a huge
number of components, etc. Technically, this is probably simplest to do
for yet another way of describing Boolean functions, namely by logical
formulas.
The reader may know logical formulas from predicate calculus, say.
A logical formula in n variables is a string made up of symbols x1,
x2, . . . , xn for the variables (each of them can be repeated several
times), parentheses, and the following symbols for the logical connec-
tives: ∧(conjunction), ∨(disjunction), ⇒(implication), ⇔(equiva-
lence), and ¬ (negation). Not every sequence of these symbols is a
logical formula, of course; a formula has to satisfy simple syntactical
rules, such as proper parenthesizing, etc. The details of these rules are
not important for us here. One possible formula in 3 variables is, for
example, (x1 ∧x2) ∨(x3 ∧¬x1). Each logical formula in n variables
deﬁnes a Boolean function of n variables: for given values of the variables
x1, x2, . . . , xn, substitute these values into the formula and evaluate the
truth value of the formula by the rules for the various connectives. For
example, we have 0 ∧0 = 0 ∧1 = 1 ∧0 = 0 and 1 ∧1 = 1, and
so on. It is not too diﬃcult to show that any Boolean function can be
deﬁned by a formula. The question now is: how long does such a formula
have to be?
We show
10.1.2 Proposition. There exists a Boolean function of n variables
that cannot be deﬁned by any formula with fewer than 2n/ log2(n + 8)
symbols. For example, for 23 variables we may already need a formula
with more than a million symbols.
Proof. The number of all Boolean functions of n variables is 22n, while
the number of formulas in n variables written by at most m symbols is
no more than (n + 8)m, because each of the m positions in the formula
can be ﬁlled in by one of the n + 7 possible symbols, or maybe by
a space. In this way, we have also counted lots of nonsense strings of
symbols, but a rough upper bound is fully suﬃcient. If 22n > (n + 8)m,
there exists a Boolean function that cannot be expressed by a formula
with at most m symbols. By taking logarithms in the inequality, we get
m ≥2n/ log2(n + 8).
2
Similarly, one may consider Boolean functions deﬁned by computer
programs in some ﬁxed programming language, or by integrated circuits
consisting of some given set of components, and so on. In each such case,
a counting similar to the above proof shows the existence of functions of
n variables that cannot be deﬁned by a program or circuit of size smaller
than roughly 2n. (We did the proof for formulas since their deﬁnition
and counting seems simplest.)

10.1 Proofs by counting
287
The two examples just given have a common scheme. We have a
set of objects, and we want to show that there exists some “good”
object among them (with some required property; in Example 10.1.1,
good objects were card orderings unattainable by 4 dovetail shuﬄes).
We count how many objects there are in total, and we upper-bound
the number of bad objects. If we can show that the number of bad
objects is smaller than the number of all objects, this means that
some good object has to exist. Typically, we even show that most of
the objects must be good.
A remarkable feature of this method is that we do not construct any
particular good object, and we don’t even learn anything about what it
might look like—we only know that it exists. In Example 10.1.1, we have
shown that some ordering of cards cannot be obtained, but we have not
exhibited any speciﬁc such ordering. In the proof of Proposition 10.1.2
we have not found out how to get a “diﬃcult” Boolean function (with
no short formula). This situation is quite usual for proofs of this type.
It might look fairly paradoxical, since we are usually in the situation of
searching for a piece of hay in a haystack with at most a few needles
in it, i.e. we can prove that a large majority of the objects are good.
But avoiding the needles proves enormously hard in many situations.
For many interesting combinatorial objects, there are relatively easy
existence proofs but none or only very diﬃcult explicit constructions
are known.
The argument about good and bad objects can be reformulated
in the language of probability. Imagine that we choose an object
from the considered set at random. If we show that with a nonzero
probability, we choose a good object, this means that at least one
good object must exist. In more complicated problems, the language
of probability becomes simpler than counting the objects, and one
can apply various more advanced results of probability theory whose
formulation in terms of object counting would be immensely cum-
bersome. We will use the language of probability in the subsequent
example. Any reader who should feel uneasy about some of the
notions from probability theory can ﬁrst read the next section.
Two-coloring revisited. Let X be a ﬁnite set and let M be a
system of subsets of X. From Section 9.4, we recall the deﬁnition of
2-colorability: we say that M is 2-colorable if each of the points of X
can be colored either red or white in such a way that no set of M has
all points red or all points white. Here we will discuss the following
problem (a particular case has been considered in Section 9.4).

288
Probability and probabilistic proofs
10.1.3 Problem. Suppose that each set of M has exactly k ele-
ments. What is the smallest number, m(k), of sets in a system M
that is not 2-colorable?
It is easy to ﬁnd out that m(2) = 3, since we need 3 edges to make
a graph that is not bipartite. But the question for k = 3 is already
much more diﬃcult. In Section 9.4, we met a system of 7 triples that
is not 2-colorable, namely the Fano plane, and so m(3) ≤7. In fact,
m(3) = 7; to prove this, we have to show that all systems with 6 or
fewer triples can be 2-colored. We begin with a general statement
which gives a weaker bound for k = 3. Then, with some more eﬀort,
we will improve the result for the particular case k = 3.
10.1.4 Theorem. We have m(k) ≥2k−1, i.e. any system consisting
of fewer than 2k−1 sets of size k admits a 2-coloring.
Proof. Let M be a system of k-element subsets of some set X, and
let |M| = m. We color each point of X red or white by the following
random procedure. For each point x ∈X, we toss a fair coin. If we
get heads we color x white and if we get tails we color x red.
Let M ∈M be one of the k-tuples in the considered system.
What is the probability that all the points of M get the same color
in a random coloring? The probability that all the k points are si-
multaneously white is obviously 2−k, and also the probability that
all the points of M turn out red is 2−k. Altogether, the probability of
M ending up monochromatic is 2·2−k = 21−k. Hence the probability
that at least one of the m sets in M is monochromatic is at most
m 21−k. If this number is strictly smaller than 1, i.e. if m < 2k−1,
then our random coloring is a 2-coloring for the system M with a
nonzero probability. Hence, at least one 2-coloring exists. Deﬁnitely
and certainly, no probability is involved anymore! Theorem 10.1.4 is
proved.
2
How good is the bound on m(k) in the theorem just proved? It is
known that for large k, the function m(k) grows roughly as 2k (more
exactly, we have m(k) = Ω(2kk1/3) and m(k) = O(2kk2); see [12]),
and so the theorem gives quite a good idea about the behavior of
m(k). On the other hand, for k = 3 we only get the estimate m(k) ≥
4, which is still quite far from the correct value 7. We improve the
bound with two more tricks.
10.1.5 Theorem. m(3) ≥7.

10.1 Proofs by counting
289
We have to show that any system of 6 triples on a ﬁnite set X
is 2-colorable. We distinguish two cases depending on the number of
points of X: |X| ≤6 and |X| > 6. Only the ﬁrst case will be handled
by a probabilistic argument.
Lemma. Let X be a set with at most 6 elements, and let M be a
system of at most 6 triples on X. Then M is 2-colorable.
Proof. If needed, we add more points to X so that it has exactly
6 points. We choose 3 of these 6 points at random and color them
white, and the remaining 3 points are colored red. Hence, we have
6
3

= 20 possibilities for choosing such a coloring. If M is any triple
from M, there are only 2 among the possible colorings that leave M
monochromatic: either M is colored red and the remaining 3 points
are white, or M is white and the other points are red. Hence the
probability that M is monochromatic is
1
10. The probability that
some of the 6 triples of M become monochromatic is thus no more
than
6
10 < 1, and hence a 2-coloring exists.
2
The same proof shows that also any 9 triples on 6 points can be
2-colored.
For the second step, we need the following deﬁnition. Let (X, M) be
a system of sets, and let x, y be two elements of X. We say that x and
y are connected if there exists a set M ∈M containing both x and y. If
x and y are points that are not connected, we deﬁne a new set system
(X′, M′) arising by “gluing” x and y together. The points x and y are
replaced by a single point z, and we put z into all sets that previously
contained either x or y. Written formally, X′ = (X \ {x, y}) ∪{z},
M′ = {M ∈M: M ∩{x, y} = ∅} ∪{(M \ {x, y}) ∪{z}: M ∈M,
M ∩{x, y} ̸= ∅}.
Let us note that if points x and y are not connected and M is a
system of triples, then (X′, M′) is again a system of triples, and the
set X′ has one point fewer than X. Further we claim that if (X′, M′)
is 2-colorable then (X, M) is 2-colorable too. To see this, consider a
2-coloring of the set X′, and color X in the same way, where both x
and y receive the color the “glued” point z had. It is easy to see that no
monochromatic set can arise in this way. Hence, for ﬁnishing the proof
of Theorem 10.1.5, it suﬃces to prove the following:
Lemma.
Let (X, M) be a system of 6 triples with |X| ≥7. Then X
contains two points that are not connected in M.
Proof. One triple M ∈M makes 3 pairs of points connected. Hence
six triples yield at most 3·6 = 18 connected pairs. But the total number

290
Probability and probabilistic proofs
of pairs of points on a 7-element set is
7
2

= 21, and hence some pair is
not connected (even at least 3).
2
Let us remark that the exact value of m(4) is already unknown (and
similarly for all the larger k). It is also easy to see that m(4) can in
principle be computed by considering ﬁnitely many conﬁgurations (sys-
tems of 4-tuples). But the number of conﬁgurations appears suﬃcient
to resist the power of all kinds of supercomputers, at least if that power
is not accompanied by enough human ingenuity.
Exercises
1. (a) Prove that any Boolean function of n variables can be expressed
by a logical formula.
(b) Show that the formula in (a) can be made of length at most Cn2n
for a suitable constant C. ∗Can you improve the order of magnitude
of this bound, say to O(2n) or even better?
2. (a) Prove that m(4) ≥15, i.e. that any system of 14 4-tuples can
be 2-colored. Proceed similarly as in the proof of Theorem 10.1.5,
distinguishing two cases according to the total number of points.
(b) ∗Give as good an upper bound on m(4) as you can! Can you get
below 50? Below 30?
3. We have 27 fair coins and one counterfeit coin, which looks like a fair
coin but is a bit heavier. Show that one needs at least 4 weighings to
determine the counterfeit coin. We have no calibrated weights, and in
one weighing we can only ﬁnd out which of two groups of some k coins
each is heavier, assuming that if both groups consist of fair coins only
the result is an equilibrium.
4. ∗In the following diagram, a train with n cars is standing on the rail
track labeled A. The cars are being moved to the track B. Each car
may or may not be shifted to some of the side tracks I–III but it should
visit each side track at most once and it should go through tracks C
and D only once.
A
B
I
II
III
C
D
Prove that if n is large enough then there is some order of the cars
that cannot be achieved on track B.

10.2 Finite probability spaces
291
10.2
Finite probability spaces
Now it is time to talk about the basic notions of mathematical prob-
ability theory. We will restrict ourselves to things we will need for
our examples. By no means do we intend to provide a substitute for
a proper course in probability theory. Anyone educated in mathe-
matics or in theoretical computer science should know considerably
more about probability than what is said here.
Probability is a notion that can appear both in mathematics and
outside it: “in real life”, “in practice”, or whatever else one can call
this. Deﬁning the “real” probability is diﬃcult, and it is a philosoph-
ical problem. Mathematics avoids its solution, though: it constructs a
certain model of the “real” probability, and this model is a purely math-
ematical object. At its fundamental level, several simple properties of
probability derived from knowledge about the real world are built in
as axioms, such as the fact that the probability of some event occur-
ring plus the probability of its not occurring add up to 1. But once
the axioms are accepted, one works with the mathematical notion of
probability as with any other mathematical object, and all its proper-
ties and all rules for calculating with probabilities are logically derived
from the axioms. This model is very useful and its predictions agree
with the behavior of probability in practice, but this doesn’t mean that
the mathematical probability and the “real” probability are the same
notions. In the sequel, we will speak of probability in the mathemat-
ical sense, but we will employ examples with “real” probability as a
motivation of the notions and axioms.
A basic notion in probability theory is a probability space. Here
we restrict ourselves to ﬁnite probability spaces.
10.2.1 Definition. By a ﬁnite probability space we understand a
pair (Ω, P), where Ωis a ﬁnite set and P : 2Ω→[0, 1] is a function
assigning a number from the interval [0, 1] to every subset of Ω, such
that
(i) P(∅) = 0,
(ii) P(Ω) = 1, and
(iii) P(A ∪B) = P(A) + P(B) for any two disjoint sets A, B ⊆Ω.
The set Ωcan be thought of as the set of all possible outcomes
of some random experiment. Its elements are called the elemen-
tary events. For instance, if the experiment is rolling one fair die,
the elementary events would be “1 was rolled”, “2 was rolled”,. . . ,
“6 was rolled”. For brevity, we can denote these elementary events
by ω1, ω2, . . . , ω6. Subsets of Ωare called events. An example of an

292
Probability and probabilistic proofs
event is “an even number was rolled”, in other words {ω2, ω4, ω6}.
Beware: An elementary event is not an event, since events are sets
of elementary events!
Probability theory has its names for various set-theoretic notions
and operations with events. For instance, “ω ∈A” can be read as
“the event A occurred”, “ω ∈A ∩B” can be interpreted as “both
the events A and B occurred”, “A∩B = ∅” can be expressed as “the
events A and B are incompatible”, and so on.
If A ⊆Ωis an event, the number P(A) is called the probability of
the event A. Axioms (i)–(iii) express properties which we naturally
expect from probability. From condition (iii), it is easy to see that
it suﬃces to specify the values of the function P on all one-element
events (sets), since the probability of any event equals the sum of
probabilities of all its elementary events (more precisely, the sum of
probabilities of all its one-element subsets, but we allow ourselves
the abbreviated formulation).
Concerning axiom (iii) in the deﬁnition of a ﬁnite probability
space, let us remark that for any two events A, B ⊆Ω, not necessarily
disjoint ones, we have the inequality P(A ∪B) ≤P(A) + P(B)
(Exercise 1).
The simplest ﬁnite probability space, and perhaps also the most
important one, is that in which all the elementary events have the
same probability, i.e. the function P is given by
P(A) = |A|
|Ω|
for all events A.
Such a probability space reﬂects the so-called classical deﬁnition of
probability. In this deﬁnition, formulated by Laplace, one assumes that
all possible outcomes of some random experiment are equally likely
(such an assumption can be based on some symmetry and/or homo-
geneity in the experiment). If the number of all outcomes (elementary
events) of the experiment is n, and, among these, there are m outcomes
favorable for some event A, then the probability of the event A is deﬁned
as m/n. This is sometimes expressed by the phrase that probability is
the number of favorable outcomes divided by the number of possible
outcomes. For deﬁning what probability is, the classical deﬁnition is
not really satisfactory (the catch is in the term “equally likely”) and,
moreover, it doesn’t include inﬁnite probability spaces, but in many
speciﬁc cases it is at least a useful hint for probability calculation.

10.2 Finite probability spaces
293
On infinite probability spaces.
By restricting ourselves to ﬁnite
probability spaces we have simpliﬁed the situation considerably from a
mathematical point of view. (A true probability theorist would probably
say that we have also excluded everything interesting.) But for mod-
eling many interesting events, it is more natural to work with inﬁnite
probability spaces, whose deﬁnition is technically more complicated. For
example, if we choose 5 points at random in the interval [0, 1], what is
the probability that some two of them lie at distance at most
1
10? We
can ask thousands of questions of a similar type. We would need to
deﬁne what a “random point from the interval [0, 1]” means in the ﬁrst
place. The elementary events should naturally be all the points in [0, 1].
And the probability of all individual points should be the same, at least
if we want the points to be “uniformly distributed”, and because there
are inﬁnitely many points, the probability of each point–elementary
event must be 0. Therefore, it is not possible to specify the probability
function P by deﬁning it for the one-element events only (as in the
ﬁnite case). The probability of an event A has to be a “measure” of A
in a suitable sense. This is a more complicated notion, which is closely
related to integration and other questions of mathematical analysis.
Other examples of inﬁnite probability spaces are used, without calling
them so, in Sections 12.5 and 12.6.
Next, we list several important species of ﬁnite probability spaces.
10.2.2 Definition (A random sequence of n 0s and 1s). The
elementary events of this probability space are all n-term sequences
of 0s and 1s, i.e. elements of the set {0, 1}n, and all elementary events
have the same probability. Since the number of elementary events
is 2n, the probability of any event A equals |A|/2n. We denote this
probability space by Cn.
This probability space models a sequence of n coin tosses, for
instance (assuming the coin is symmetric and heads and tails occur
with the same probability). If the ith toss yields heads we write 1 in
the ith position in the sequence, and for tails we write 0. An example
of an event is A =“Heads appear exactly 10×”, whose probability is
 n
10

/2n.
10.2.3 Definition (A random permutation). Elementary events
of this probability space are all permutations of the set {1, 2, . . . , n},
and the probability of an event A equals |A|/n!. The set of all
permutations of the set {1, 2, . . . , n} is traditionally denoted by Sn,
and we denote the probability space by Sn.

294
Probability and probabilistic proofs
This space is a model for arranging some n distinct elements in
a random order, e.g. for a well-shuﬄed card deck.
Problem. What is the probability that in a random ordering of a
bridge card deck, the ace of spades precedes the king of hearts?
Reformulated in our probability model, what is the probability of
the event A = {π ∈S52 : π(1) < π(2)}? We could, of course, honestly
count the permutations with π(1) < π(2), but we can also determine
the probability in question by a simple consideration: by symmetry, it
is impossible that one of the events “π(1) < π(2)” and “π(1) > π(2)”
would be more likely than the other, and so the required probability
is 1
2. More precisely, such a reasoning should perhaps be formulated
as follows. We can construct a bijection between the set A and the
set A′ = {π ∈S52 : π(1) > π(2)}. To a permutation π ∈A, we assign
the permutation π′ with π′(1) = π(2), π′(2) = π(1), and π′(i) = π(i)
for i > 2. Hence |A| = |A′|, and since A and A′ are disjoint and
together cover the whole probability space, we have P(A) = 1
2.
To revive the reader’s attention, we now include a problem with a
surprising solution.
Problem.
We play the following game. Our rival has 100 blank cards,
and on each of them he writes some quite arbitrary number at will.
Then he shuﬄes the cards (or, a neutral third party should better do
it), and the shuﬄed card deck is laid on a table face down so that the
numbers are not visible. We start removing cards from the top one by
one and look at their numbers. After turning any of the cards, we can
end the game. We win if the last card we turned has the largest number
among all the cards (those already turned but also those still lying on
the table). If we win we take 40 doublezons, and if we lose we pay 10
doublezons. Can we expect to gain on this game?
At ﬁrst sight it may seem that this game is not advantageous for
us at all. Let us keep the following strategy, though: turn the ﬁrst 50
cards no matter what, and remember the largest number we saw there;
let it be M. Then turn the remaining cards, and ﬁnish as soon as we
turn a card with a number greater than or equal to M. If we encounter
no such card we ﬁnish with the last card.
We claim that the probability of winning is greater than 1
4 for this
strategy. Therefore, we can expect to win at least 1 game out of 4 on
the average, and so our expected gain in a long series of games will be
positive, at least about 1
4 · 40 −3
4 · 10 = 2.50 doublezons per game—no
fortune but enough for a beer. For simplicity, let us suppose that all the
numbers on the cards are distinct (an interested reader can consider the

10.2 Finite probability spaces
295
modiﬁcation for the case of arbitrary numbers). The strategy described
surely leads to winning if
• the largest number is among the last 50 cards, and
• the second largest number is among the ﬁrst 50 cards.
Obviously, for our strategy, the game depends solely on the ordering
of the numbers and not on their actual values, and so we can imagine
that the cards contain numbers 1, 2, . . . , 100, and their random ordering
is thus an elementary event from the probability space S100. We are
interested in the event A = {π ∈S100 : π(100) > 50 and π(99) ≤50}.
Here it is useful to think of a permutation as a linear ordering. The
position of the number 100 can be chosen in 50 ways, the position of 99
can be chosen in 50 ways independently of the placement of 100, and
the remaining numbers can be arranged in 98! ways. Hence
P(A) = 50 · 50 · 98!
100!
= 50 · 50
99 · 100 ˙= 0.2525 > 1
4 .
Let us ﬁnish this example with a few remarks. The event A is not the
only situation in which our strategy wins, and hence the probability of
A is only a lower bound for the probability of winning. The number
50 used as a threshold in our strategy optimizes the probability of the
event A (meaning the event that 100 comes after the threshold and 99
before it). But if we also take other possibilities of winning into account,
a somewhat better winning chance is obtained for a diﬀerent threshold.
We will not continue a detailed analysis of the game here; we leave it
as a challenge for the reader.
Next, we are going to consider the notion of a “random graph”.
As we will see later, there are several ways a random graph can be
reasonably deﬁned. Here we consider the simplest way. A simple,
undirected graph on the vertex set V = {1, 2, . . . , n} is speciﬁed by
deciding, for each pair {i, j} ∈
V
2

, whether this pair is an edge or
not. Hence there are 2(n
2) graphs (many of them are isomorphic, of
course, but this doesn’t concern us here). If we select a random graph
G on the vertex set V , in such a way that all possible graphs have
the same probability, we can view this as
n
2

symmetric coin tosses.
That is, for each pair of vertices we toss a coin to decide whether it
becomes an edge or not. This is reﬂected in the following deﬁnition.
10.2.4 Definition (A random graph). This probability space, de-
noted by Gn, has all the possible graphs on vertex set {1, 2, . . . , n} as
elementary events, and all of them have the same probability, equal
to 2−(n
2).

296
Probability and probabilistic proofs
As examples of events in the probability space Gn we can study all
sorts of natural graph properties, such as S = “the graph G is connec-
ted”, B = “the graph G is bipartite”, and so on. Computing the prob-
ability of such events exactly is often very diﬃcult, but we are usually
interested in a very rough estimate. For the two mentioned examples of
events, it turns that for n →∞, P(S) rapidly tends to 1 while P(B)
approaches 0 very quickly. This is sometimes expressed by saying that
“a random graph is almost surely connected, and almost surely it is not
bipartite”. We prove the second of these two claims, by an approach
typical also for many other assertions of this type.
10.2.5 Proposition. A random graph almost surely isn’t bipartite, i.e.
limn→∞P(B) = 0.
Proof.
As we know, the vertex set V of a bipartite graph can be
partitioned into two parts, U and W, in such a way that all edges go
between U and W only. For a given subset U ⊆V , let BU denote the
event that all edges of the random graph G go between vertices of U
and vertices of W = V \ U only. If k = |U|, we have k(n −k) pairs
{u, w} with u ∈U and w ∈V \U, and so the event (set) BU consists of
2k(n−k) graphs. Therefore P(BU) = 2k(n−k)−(
n
2). It is not hard to check
that the function k →k(n−k) attains its maximum for k = n
2 , and the
value of this maximum is n2/4; hence k(n −k) ≤n2/4 for all k. So we
have, for any U,
P(BU) ≤2n2/4−(
n
2) = 2−n(n−2)/4.
Each bipartite graph belongs to some BU (for a suitable choice of the
set U). For diﬀerent choices of U, the events BU need not be disjoint,
but in any case, the probability of a union of events is always at most
the sum of their probabilities, and so
P(B) ≤

U⊆V
P(BU) ≤2n · 2−n(n−2)/4 = 2−n(n−6)/4 →0.
2
In this problem, we were interested in a certain qualitative property
of a “big” random graph. This somewhat resembles the situation in var-
ious areas of physics (such as thermodynamics or solid-state physics)
where macroscopic properties of a large collection of microscopic parti-
cles are studied. It is assumed that the individual particles behave ran-
domly in a suitable sense, and the macroscopic properties are a result
of their random interactions. Also mathematical methods in the study
of ferromagnetism and other properties of the solid-state matter are
similar to methods for random graphs. Analogous approaches are also
applied in social sciences, for instance in modeling epidemics, etc.

10.2 Finite probability spaces
297
In Deﬁnition 10.2.4, we suppose that all graphs have the same prob-
ability, or in other words, that the edge probability is 1
2. In interesting
problems and applications, the edge probability is usually chosen as a
parameter p generally distinct from 1
2. This means that a random graph
is constructed by
n
2

coin tosses, one toss for each pair of vertices (heads
means an edge and tails no edge), but the coin has the probability of
heads equal to p and the probability of tails equal to 1−p. The proper-
ties of the random graph are often investigated in dependence on this p.
For instance, if p grows from 0 to 1, for which value does the random
graph typically start to be connected? This is again conceptually not too
far from physics questions like: at what temperature does this crystal
start to melt?
Independent events. We have to cover one more key notion. Two
events A, B in a probability space (Ω, P) are called independent if
we have
P(A ∩B) = P(A)P(B).
Independence means that if Ωis divided into two parts, A and its
complement, the event B “cuts” both these parts in the same ratio.
In other words, if an elementary event ω were chosen at random
not in the whole Ωbut among the elementary events from A, the
probability of ω ∈B would be exactly equal to P(B) (assuming
P(A) ̸= 0).
Independence does not mean that A∩B = ∅as some might perhaps
think.
Most often, we encounter independent events in the following sit-
uation. The elements of Ω, i.e. the elementary events, can be viewed
as ordered pairs, so Ωmodels the possible results of a “compound”
experiment consisting of two experiments. Let us assume that the
course and result of the ﬁrst of these two experiments cannot possi-
bly inﬂuence the result of the second experiment and vice versa (the
experiments are separated by a thick enough wall or something). If
A ⊆Ωis an event depending on the outcome of the ﬁrst experiment
only (i.e if we know this outcome we can already decide whether A
occurred or not), and similarly if B only depends on the outcome of
the second experiment, then the events A and B are independent.
The space Cn (a random n-term sequence of 0s and 1s) is a typi-
cal source of such situations. Here we have a compound experiment
consisting of n subsequent coin tosses, and we assume that these do
not inﬂuence one another in any way; for instance, the coin is not
deformed or stolen during the ﬁrst experiment. So, for example, if

298
Probability and probabilistic proofs
an event A only depends on the ﬁrst 5 tosses (“heads appeared at
least 3 times in the ﬁrst 5 tosses”) and an event B only depends on
the 6th and subsequent tosses (“heads appeared an odd number of
times in tosses number 6 through 10”), such events are independent.
Similarly, in the probability space Gn (a random graph), the edges
are independent, and so for example the events “the graph G has
at least one triangle on the vertices 1, 2, . . . , 10” and “the graph G
contains an odd-length cycle with vertices among 11, 12, . . . , 20” are
independent.
A subtler situation can be demonstrated in the probability space
Sn (a random permutation). The events A = {π(1) = 1} and B =
{π(2) = 1} are clearly not independent because P(A) > 0 and
P(B) > 0, but A ∩B = ∅and thus P(A ∩B) = 0. If we deﬁne
another event C = {π(2) = 2}, it is equally easy to see that B and
C aren’t independent either. But maybe it is not so obvious anymore
that even A and C are not independent: we have P(A) = P(C) = 1
n
but P(A ∩C) =
1
n(n−1) ̸= P(A)P(C). Intuitively, if we know that
A occurred, i.e. that π(1) = 1, we have also excluded one of the
n possibilities for the value of π(2), and hence π(2) has a slightly
bigger chance of being equal to 2. On the other hand, the events A
and D = {π(2) < π(3)} are independent, as one can check by com-
puting the relevant probabilities. One has to be careful about such
subtleties. (Perhaps the most frequent source of error in probabilistic
proofs is that some events are assumed to be independent although
in reality they are not.)
The notion of independence can also be extended to several events
A1, A2, . . . , An.
10.2.6 Definition. Events A1, A2, . . . , An ⊆Ωare called indepen-
dent if we have, for each set of indices I ⊆{1, 2, . . . , n},
P
 
i∈I
Ai

=

i∈I
P(Ai).
In particular, this deﬁnition requires that each two of these events
be independent, but we have to warn that the independence of each
pair doesn’t in general imply the independence of all events!
In the probability spaces Cn (random 0/1 sequence) and Gn (ran-
dom graph) we have typical situations with many independent events.
Let us deﬁne an event Ai in the probability space Cn, consisting of all

10.2 Finite probability spaces
299
sequences with 1 in the ith position. Then the events A1, A2, . . . , An
are independent (we omit a proof—it is not diﬃcult).
In various probabilistic calculations and proofs, the probability
spaces are usually not described explicitly—one only works with
them without saying. Nevertheless, it is important to clarify these
basic notions.
We conclude this section with a nice probability proof (histori-
cally, it is one of the ﬁrst proofs by this method).
Let us consider a tournament of n players, say a tennis tour-
nament, in which each player plays against everyone else and each
match has a winner. If there are big diﬀerences among the play-
ers’ strength, we may expect that the best player beats everyone,
the second best beats all but the ﬁrst one, etc., so that the tour-
nament determines the order of players convincingly. A tournament
with a more leveled strength of players can give a more complicated
outcome. Of course, it can happen that every player is beaten by
someone. And, mathematicians wouldn’t be mathematicians if they
didn’t ask the following generalization:
Problem. Is it possible that in some tournaments, every two play-
ers are both beaten by some other player? And, more generally, for
which numbers k can there be a tournament in which for every k
players there exists another player who has beaten them all?
For k = 2, one can construct such a tournament “by hand”.
But for larger values of k, the construction of such tournaments
seemed diﬃcult, and a solution has been sought in vain for quite
some time. But the method using probability shows the existence of
such tournaments (with many players) quite easily. For simplicity, we
show the solution for k = 3 only, so we want a tournament outcome
in which every 3 players x, y, z are all beaten by some other player, w.
Let us consider a random tournament, where we imagine that the
result of each match is determined by lot, say by tossing a fair coin.
Let us look at some ﬁxed triple {x, y, z} of players. The probability
that some other player, w, wins against all of them is 2−3 =
1
8.
Hence the probability that w loses against at least one of x, y, z is
7
8. What is the probability that each of the n −3 players who can
appear in the role of w loses against at least one among x, y, z?
For distinct players w, the results of their matches with x, y, z are

300
Probability and probabilistic proofs
mutually independent, and so the desired probability is (7
8)n−3. The
triple {x, y, z} can be selected in
n
3

ways, and hence the probability
that for at least one of the triples {x, y, z}, no player beats x, y, and
z simultaneously, is at most
n
3

( 7
8)n−3. Using a pocket calculator
we can check that for n ≥91, this probability is smaller than 1.
Therefore, there exists at least one result of a tournament of 91
players in which any 3 players are simultaneously beaten by some
other player. This is the desired property.
2
Exercises
1. Prove that for any two events A, B in a (ﬁnite) probability space, we
have P(A∪B) ≤P(A)+P(B). Generalize this to the case of n events.
2. (Probabilistic formulation of inclusion–exclusion)
(a) Formulate the inclusion–exclusion principle (Theorem 3.7.2) in the
language of probability theory. Let A1, A2, . . . , An be events in some
ﬁnite probability space. Assuming that all elementary events in this
probability space have the same probability, express the probability
P(A1 ∪· · · ∪An) using the probabilities of various intersections of
the Ai.
(b) Show that the formula as in (a) holds for events in an arbitrary
ﬁnite probability space. (The ﬁniteness is not really needed here.)
3. Prove that a random graph in the sense of Deﬁnition 10.2.4 almost
surely contains a triangle (this provides another proof for Proposi-
tion 10.2.5).
4. ∗Show that a random graph is almost surely connected.
5. Find an example of 3 events in some probability space, such that each
2 of them are independent but all 3 are not independent.
6. Show that if A, B are independent events then also their complements,
Ω\ A and Ω\ B, are independent.
7. Let (Ω, P) be a ﬁnite probability space in which all elementary events
have the same probability. Show that if |Ω| is a prime number then no
two nontrivial events (distinct from ∅and Ω) can be independent.
8. (a) Show that the events A1, A2, . . . , An in the probability space Cn
deﬁned in the text following Deﬁnition 10.2.6 are really independent.
(b) ∗Let (Ω, P) be a ﬁnite probability space. Suppose that n indepen-
dent events A1, A2, . . . , An ⊆Ωexist such that 0 < P(Ai) < 1 for
each i. Show that then |Ω| ≥2n.

10.3 Random variables and their expectation
301
9. For simplicity, assume that the probabilities of the birth of a boy and
of a girl are the same (which is not quite so in reality). For a certain
family, we know that they have exactly two children, and that at least
of them is a boy. What is the probability that they have two boys?
10. (a) CS Write a program to generate a random graph with a given
edge probability p and to ﬁnd its connected components. For a given
number n of vertices, determine experimentally at which value of p the
random graph starts to be connected, and at which value of p it starts
to have a “giant component” (a component with at least n
2 vertices,
say).
(b) ∗∗Can you ﬁnd theoretical explanations for the ﬁndings in (a)? You
may want to consult the book [12].
10.3
Random variables and their expectation
10.3.1 Definition. Let (Ω, P) be a ﬁnite probability space. By a
random variable on Ω, we mean any mapping f : Ω→R.
A random variable f thus assigns some real number f(ω) to each
elementary event ω ∈Ω. Let us give several examples of random
variables.
10.3.2 Example (Number of 1s). If Cn is the probability space of
all n-term sequences of 0s and 1s, we can deﬁne a random variable
f1 as follows: for a sequence s, f1(s) is the number of 1s in s.
10.3.3 Example (Number of surviving rabbits). Each of n hunt-
ers selects a rabbit at random from a group of n rabbits, aims a gun
at it, and then all the hunters shoot at once. (We feel sorry for
the rabbits but this is what really happens sometimes.) A random
variable f2 is the number of rabbits that survive (assuming that no
hunter misses). Formally, the probability space here is the set of all
mappings α: {1, 2, . . . , n} →{1, 2, . . . , n}, each of them having the
probability n−n, and f2(α) = |{1, 2, . . . , n} \ α({1, 2, . . . , n})|.

302
Probability and probabilistic proofs
10.3.4 Example (Number of left maxima). On the probability
space Sn of all permutations of the set {1, 2, . . . , n}, we deﬁne a
random variable f3: f3(π) is the number of left maxima of a permu-
tation π, i.e. the number of the i such that π(i) > π(j) for all j < i.
Imagine a long-jump contest, and assume for simplicity that each
competitor has a very stable performance, i.e. always jumps the same
distance, and these distances are diﬀerent for diﬀerent competitors
(these, admittedly unrealistic, assumptions can be relaxed signiﬁ-
cantly). In the ﬁrst series of jumps, n competitors jump in a random
order. Then f3 means the number of times the current longest jump
changes during the series.
10.3.5 Example (Sorting algorithm complexity). This random
variable is somewhat more complicated. Let A be some sorting algo-
rithm, meaning that the input of A is an n-tuple (x1, x2, . . . , xn) of
numbers, and the output is the same numbers in a sorted order. Sup-
pose that the number of steps made by algorithm A only depends on
the ordering of the input numbers (so that we can imagine that the
input is some permutation π of the set {1, 2, . . . , n}). This condition
is satisﬁed by many algorithms that only use pairwise comparisons
of the input numbers for sorting; some of them are frequently used
in practice. We deﬁne a random variable f4 on the probability space
Sn: we let f4(π) be the number of steps made by algorithm A for the
input sequence (π(1), π(2), . . . , π(n)).
10.3.6 Definition. Let (Ω, P) be a ﬁnite probability space, and let
f be a random variable on it. The expectation of f is a real number
denoted by E [f] and deﬁned by the formula
E [f] =

ω∈Ω
P({ω})f(ω).
In particular, if all the elementary events ω ∈Ωhave the same
probability (as is the case in almost all of our examples), then the
expectation of f is simply the arithmetic average of the values of f
over all elements of Ω:
E [f] = 1
|Ω|

ω∈Ω
f(ω).
The expectation can be thought of as follows: if we repeat a random
choice of an elementary event ω from Ωmany times, then the average
of f over these random choices will approach E [f].

10.3 Random variables and their expectation
303
Example 10.3.2 (Number of 1s) continued. For an illustration,
we compute the expectation of the random variable f1, the num-
ber of 1s in an n-term random sequence of 0s and 1s, according to
the deﬁnition. The random variable f1 attains a value 0 for a sin-
gle sequence (all 0s), value 1 for n sequences, . . . , value k for
n
k

sequences from Cn. Hence
E [f1] = 1
2n

s∈{0,1}n
f1(s)
= 1
2n
n

k=0
n
k

k.
As we will calculate in Example 12.1.1, the ﬁnal sum equals n2n−1,
and so E [f1] =
n
2 . Since we expect that for n coin tosses, heads
should occur about n
2 times, the result agrees with intuition.
The value of E [f1] can be determined in a simpler way, by the
following trick. For each sequence s ∈Cn we consider the sequence ¯s
arising from s by exchanging all 0s for 1s and all 1s for 0s. We have
f1(s) + f1(¯s) = n, and so
E [f1] = 1
2n

s∈{0,1}n
f1(s) =
1
2n · 2

s∈{0,1}n
(f1(s) + f1(¯s))
= 2−n−12nn = n
2 .
We now describe a method that often allows us to compute the
expectation in a surprisingly simple manner (we saw that the calcu-
lation according to the deﬁnition can be quite laborious even in very
simple
cases).
We
need
a
deﬁnition
and
a
simple
theorem.
10.3.7 Definition. Let A ⊆Ωbe an event in a probability space
(Ω, P). By the indicator of the event A we understand the random
variable IA : Ω→{0, 1} deﬁned in the following way:
IA(ω) =
 1
for ω ∈A
0
for ω ̸∈A.
(So the indicator is just another name for the characteristic function
of A.)

304
Probability and probabilistic proofs
10.3.8 Observation. For any event A, we have E [IA] = P(A).
Proof. By the deﬁnition of expectation we get
E [IA] =

ω∈Ω
IA(ω)P({ω}) =

ω∈A
P({ω}) = P(A).
2
The following result almost doesn’t deserve to be called a theorem
since its proof from the deﬁnition is immediate (and we leave it to
the reader). But we will ﬁnd this statement extremely useful in the
sequel.
10.3.9 Theorem (Linearity of expectation). Let f, g be arbi-
trary random variables on a ﬁnite probability space (Ω, P), and let
α be a real number. Then we have E [αf] = αE [f] and E [f + g] =
E [f] + E [g].
2
Let us emphasize that f and g can be totally arbitrary, and need
not be independent in any sense or anything like that. (On the other
hand, this nice behavior of expectation only applies to adding ran-
dom variables and multiplying them by a constant. For instance, it
is not true in general that E [fg] = E [f] E [g]!) Let us continue with
a few examples of how 10.3.7–10.3.9 can be utilized.
Example 10.3.2 (Number of 1s) continued again. We calculate
E [f1], the average number of 1s, in perhaps the most elegant way.
Let the event Ai be “the ith coin toss gives heads”, so Ai is the
set of all n-term sequences with a 1 in the ith position. Obviously,
P(Ai) = 1
2 for all i. We note that for each sequence s ∈{0, 1}n we
have f1(s) = IA1(s) + IA2(s) + · · · + IAn(s) (this is just a rather
complicated way to write down a trivial statement). By linearity of
expectation and then using Observation 10.3.8 we obtain
E [f1] = E [IA1] + E [IA2] + · · · + E [IAn]
= P(A1) + P(A2) + · · · + P(An) = n
2 .
2
Example 10.3.3 (Number of surviving rabbits) continued.
We will compute E [f2], the expected number of surviving rabbits.

10.3 Random variables and their expectation
305
π(i), π(i −1),. . . , π(1) are still in
. . .
. . .
1
2
i
i + 1
n −1
n
π(i + 1)
π(n)
π(n −1)
Fig. 10.2 A procedure for selecting a random permutation.
This time, let Ai be the event “the ith rabbit survives”; formally, Ai
is the set of all mappings α that map no element to i. The proba-
bility that the jth hunter shoots the ith rabbit is 1
n, and since the
hunters select rabbits independently, we have P(Ai) = (1 −1/n)n.
The remaining calculation is as in the preceding example:
E [f2] =
n

i=1
E [IAi] =
n

i=1
P(Ai) =

1 −1
n
n
n ≈n
e
(since (1 −1/n)n converges to e−1 for n →∞; see Exercise 3.5.2).
About 37% of the rabbits survive on the average.
2
Example 10.3.4 (Number of left maxima) continued. Now we
will calculate the expected number of left maxima of a random per-
mutation, E [f3]. Let us deﬁne Ai as the event “i is a left max-
imum of π”, meaning that Ai = {π ∈Sn : π(i) > π(j) for j =
1, 2, . . . , i−1}. We claim that P(Ai) = 1
i . Perhaps the most intuitive
way of deriving this is to imagine that the random permutation π is
produced by the following method. We start with a bag containing
the numbers 1, 2, . . . , n. We draw a number from the bag at random
and declare it to be π(n). Then we draw another random number
from the bag which becomes π(n −1) etc., as in Fig. 10.2. The value
of π(i) is selected at the moment the bag contains exactly i numbers.
The probability that we choose the largest one of these i numbers
for π(i) (which is exactly the event Ai) thus equals 1
i . The rest is
again the same as in previous examples:
E [f3] =
n

i=1
E [IAi] =
n

i=1
P(Ai) = 1 + 1
2 + 1
3 + · · · + 1
n .
The value of the sum of reciprocals on the right-hand side is roughly
ln n; see Section 3.4.
2

306
Probability and probabilistic proofs
Exercises
1. Show with examples that if f and g are arbitrary random variables
then none of the following equalities must necessarily hold: E [fg] =
E [f] E [g], E

f2 
= E [f]2, E [1/f] = 1/E [f].
2. Prove that E

f 2 
≥E [f]2 holds for any random variable f.
3. Let f(π) be the number of ﬁxed points of a permutation π (see Sec-
tion 3.8). Compute E [f] for a random permutation π in the space Sn.
4. Let π be a random permutation of the set {1, 2, . . . , n}.
(a) ∗Determine the expected length of the cycle of π containing the
number 1 (see Section 3.2 for the deﬁnition of a cycle).
(b) ∗Determine the expected number of cycles of π.
5. A bus route connects downtown Old Holstein with the local university
campus. Mr. X., a student at the university, takes the bus from down-
town every weekday after he wakes up, which happens at a random
time of the day (24 hours). According to his records, he has to wait
for the bus for 30 minutes on the average. At the same time, the bus
company claims that the average interval between two buses during
the day (over the period of 24 hours) is 15 minutes. Can you construct
a schedule such that both Mr. X. and the bus company are right?
6. ∗We toss a fair coin n times. What is the expected number of “runs”?
Runs are consecutive tosses with the same result. For instance, the
toss sequence HHHTTHTH has 5 runs.
7. (Markov inequality) Let X be a random variable on some probability
space attaining nonnegative values only. Let µ = E [X] be its expecta-
tion, and let t ≥1 be a real number. Prove that the probability that
X attains a value ≥tµ is at most 1
t ; in sympols,
P ({ω ∈Ω: X(ω) ≥tµ}) ≤1
t .
(This is a simple but quite important inequality. It is often used if we
want to show that the probability of some quantity getting too big is
small.)
8. (a) What is the expected number of surviving rabbits in Example 10.3.3
if there are m rabbits and n hunters?
(b) ∗Using Exercise 7 and suitable estimates, show that if we have
n > m(ln m + 5) then with probability at least 0.99, no rabbit survives.
(In other words, most of the mappings from an n-element set into an
m-element set are onto.)
(c) ∗Solve part (b) diﬀerently: use the derivation of the formula for the
number of mappings onto via inclusion–exclusion (Exercise 3.8.7) and
the Bonferroni inequality (3.20) with q = 1.

10.4 Several applications
307
10.4
Several applications
In this section, we have collected examples of using the probabilistic
method and the linearity of expectation in particular. They are not
routine examples but rather small mathematical gems.
Existence of large bipartite subgraphs.
Given a graph G =
(V, E), we would like to partition the vertex set into two parts in
such a way that as many edges as possible go between these parts.
Moreover, we often need that the parts have an approximately equal
size. The following theorem shows that we can always make at least
half of the edges go between the parts, and, moreover, that the parts
can be chosen with an equal size (if the number of vertices is even).
10.4.1 Theorem. Let G be a graph with an even number, 2n, of
vertices and with m > 0 edges. Then the set V = V (G) can be
divided into two disjoint n-element subsets A and B in such a way
that more than m
2 edges go between A and B.
Proof. Choose A as a random n-element subset of V , all the
2n
n

n-element subsets having the same probability, and let us put B =
V \ A. Let X denote the number of edges of G going “across”, i.e.
edges {a, b} with a ∈A and b ∈B. We calculate the expectation
E [X] of the random variable X. For each edge e = {u, v} ∈E(G),
we deﬁne the event Ce that occurs whenever the edge e goes between
A and B; formally, Ce = {A ∈
V
n

: |A ∩e| = 1}. Then we have
X = 
e∈E(G) ICe, and hence E [X] = 
e∈E(G) P(Ce). So we need
to determine the probability P(Ce).
Altogether, there are
2n
n

possible choices of A. If we require that
u ∈A and v ̸∈A, the remaining n −1 elements of A can be selected
in
2n−2
n−1

ways. Similar reasoning works for the symmetric situation
u ̸∈A, v ∈A. Thus
P(Ce) =
2
2n−2
n−1

2n
n

=
n
2n −1 > 1
2 .
From this we get E [X] = 
e∈E(G) P(Ce) > m
2 . The expectation of
X is the arithmetic average of the values of X over all choices of the
set A. An average cannot be greater than the maximum of all these
values, and therefore a choice of A exists with more than half of the
edges going across.
2

308
Probability and probabilistic proofs
Independent sets. In Section 4.7 we investigated the maximum
possible number of edges of an n-vertex graph that contains no tri-
angle. More generally, for given k ≥3, we can ask for the maximum
possible number of edges of an n-vertex graph with no subgraph iso-
morphic to Kk, that is, the complete graph on k vertices. This ques-
tion is answered by Tur´an’s theorem, one of the celebrated results of
extremal graph theory. This theorem can be formulated in various
ways. The strongest version, which we proved for k = 3 in Section 4.7
(for arbitrary k see Exercise 4.7.4), describes exactly how a graph
with the maximum possible number of edges looks. Here we demon-
strate a very cute probabilistic proof. We only give the bound on
the maximum number of edges. With some more work, one can also
derive the structure of the graph with the maximum possible number
of edges, but we omit that part.
Tur´an’s theorem is most often applied in a “reverse” form: if a
graph on n vertices has more than a certain number of edges then it
has to contain a Kk. If we consider the complement of the graph G,
i.e. edges in the new graph are exactly at the positions where G has
no edges, Tur´an’s theorem says if a graph on n vertices has fewer
than a certain number of edges then it contains an independent set
of size at least k (an independent set is a set of vertices such that
no two of them are connected by an edge). This is perhaps the most
useful version for applications, and it is also the one we state and
prove here.
10.4.2 Theorem (Tur´an’s theorem). For any graph G on n ver-
tices, we have
α(G) ≥
n2
2|E(G)| + n ,
where α(G) denotes the size of the largest independent set of vertices
in the graph G.
The probabilistic method is used in the proof of the next lemma:
Lemma. For any graph G, we have
α(G) ≥

v∈V (G)
1
degG(v) + 1
(where degG(v) denotes the degree of a vertex v in the graph G).

10.4 Several applications
309
Proof.
Suppose that the vertices of G are numbered 1, 2, . . . , n,
and let us pick a random permutation π of the vertices. We deﬁne
a set M = M(π) ⊆V (G) consisting of all vertices v such that all
neighbors u of v satisfy π(u) > π(v); that is, the vertex v precedes
all neighbors in the ordering given by the permutation π. Note that
the set M(π) is an independent set in G, and so |M(π)| ≤α(G) for
any permutation π. Hence also E [|M|] ≤α(G). We now calculate
the expected size of M in a diﬀerent way.
For a vertex v, let Av be the event “v ∈M(π)”. If Nv denotes
the set of all neighbors of the vertex v, then all the orderings of the
set Nv ∪{v} by the permutation π have the same probability, and
so the probability of v being the smallest element of this set equals
1/(|Nv| + 1) = 1/(degG(v) + 1). Therefore P(Av) = 1/(degG(v) + 1),
and we can calculate as we already did several times before:
α(G) ≥E [|M|] =

v∈V (G)
E [IAv]
=

v∈V (G)
P(Av) =

v∈V (G)
1
degG(v) + 1 .
2
Proof of Theorem 10.4.2.
This is now a mere calculation with
inequalities. The number of edges e = |E(G)| is half of the sum of the
vertex degrees. So we have the following situation: for nonnegative
real numbers d1, d2, . . . , dn, we know that n
i=1 di = 2e, and we ask
what the smallest possible value is of the sum
n

i=1
1
di + 1 .
It can be shown that this sum is minimized for d1 = d2 = · · · = dn =
2e/n (we leave this as an exercise), and in such a case, its value is
n2/(2e + n) as in the theorem.
2
Number of intersections of level ≤k. This is a geometric prob-
lem arising in the analysis of certain geometric algorithms. Let us
consider a set L consisting of n lines in the plane, such that no three
lines of L meet at a common point and no two of them are parallel.
Let o be a point lying on none of the lines of L. We will consider
the pairwise intersections of the lines of L. Each pair of lines has one

310
Probability and probabilistic proofs
























l
l
l
l
l
l
l
l
l
l
l
l
ll




























t
t
t
t
t
t
o
Fig. 10.3 Intersections of level 1 for a set of lines.
intersection, so there are
n
2

intersections altogether. Let us say that
an intersection v has level k if the segment ov intersects, in addition
to the two lines deﬁning the intersection v, exactly k more lines of
L (Fig. 10.3 depicts all intersections of level 1). What is the maxi-
mum possible number of intersections of level at most k, for a given
n and k? The following theorem gives an upper bound which is the
best possible up to the value of the constant of proportionality:
10.4.3 Theorem. For any set of n lines, there exist at most 3(k +
1)n intersections of level at most k.
Proof. First we look at the particular case k = 0. We ﬁnd that the
intersections of level 0 are exactly the vertices of the convex polygon
containing the point o. Since each line contributes at most one side
of this polygon, the number of its sides is at most n, and hence also
the number of intersections of level 0 is at most n. We will use this
in the proof for an arbitrary k, which we do next.
Let p denote a certain suitable number in the interval (0, 1), whose
value will be determined at the end of the proof. Let us imagine the
following random experiment. We choose a subset R ⊆L at random,
by choosing each line ℓ∈L with probability p, these choices being
independent for distinct lines ℓ.
Here we should perhaps say a little more about the underlying prob-
ability space. It is a generalization of the space Cn from Example 10.2.2.
That example models tosses of a symmetric coin, where both heads and

10.4 Several applications
311
tails have the same probability, namely 1
2. In the case considered here,
we would use an asymmetric coin of some kind, where heads have prob-
ability p and tails probability 1 −p. For each line ℓ∈L, we toss this
asymmetric coin once, and we include ℓinto our sample R if the toss
yields heads. Formally, the probability space is the set of all subsets
of L, and the probability of an r-element set R ⊆L is pr(1 −p)n−r,
where r = |R| (in order to get exactly the set R by the coin tosses,
certain r tosses must give heads and the remaining n −r tosses tails).
This is also an example of a probability space where the elementary
events need not have all the same probability.
Let us return to our geometric situation. We have thus chosen
a random set R ⊆L of lines. Let us imagine that only the lines of
R are drawn in the plane. We deﬁne a random variable f = f(R)
as the number of intersections that have level 0 with respect to the
lines of R; that is, intersections of lines of R such that no line of
R obscures their view of the point o. We are going to estimate the
expectation E [f] in two ways. On one hand, by the remark at the
beginning of the proof, we have f(R) ≤|R| for any speciﬁc set R,
and hence E [f] ≤E [|R|]. It is not diﬃcult to calculate (we leave
this to Exercise 8) that E [|R|] = pn.
Now we will count E [f] in a diﬀerent way. For each intersection
v of the lines of L, we deﬁne an event Av that occurs if and only if
v is one of the intersections of level 0 with respect to the lines of R,
i.e. it contributes 1 to the value of f(R). The event A occurs if and
only if the following two conditions are satisﬁed:
• Both the lines determining the intersection v lie in R.
• None of the lines intersecting the segment ov at an interior point
(and thus obscuring the view from the point v to the point o)
falls into R.
From this, we deduce that P(Av) = p2(1−p)ℓ(v), where ℓ(v) denotes
the level of the intersection v.
Let M denote the set of all intersections of the lines of L, and let
Mk ⊆M be the set of intersections at level at most k. We have
E [f] =

v∈M
E [IAv] =

v∈M
P(Av) ≥

v∈Mk
P(Av)
=

v∈Mk
p2(1 −p)ℓ(v) ≥

v∈Mk
p2(1 −p)k = |Mk|p2(1 −p)k.

312
Probability and probabilistic proofs
Altogether we have derived np ≥E [f] ≥|Mk|p2(1 −p)k, in other
words
|Mk| ≤
n
p(1 −p)k .
Let us now choose the number p in such a way that the value of
the right-hand side is as small as possible. A suitable choice is, for
instance, p = 1/(k + 1). By Exercise 3.5.2, we have (1 −
1
k+1)k ≥
e−1 >
1
3 for any k ≥1. This leads to |Mk| ≤3(k + 1)n as the
theorem claims.
2
Let us remark that the problem of estimating the maximum pos-
sible number of intersections of level exactly k is much more diﬃcult
and still unsolved. The branch of mathematics studying problems of a
similar nature, i.e. combinatorial questions about geometric conﬁgura-
tions, is called combinatorial geometry. A highly recommendable book
for studying this subject is Pach and Agarwal [27]. A more specialized
book considering problems closely related to estimating the number of
intersections of level k is Sharir and Agarwal [45].
Average number of comparisons in QUICKSORT. Algorithm
QUICKSORT, having received a sequence (x1, x2, . . . , xn) of num-
bers as input, proceeds as follows. The numbers x2, x3, . . . , xn are
compared to x1 and divided into two groups: those smaller than
x1 and those at least as large as x1. In both groups, the order of
numbers remains the same as in the input sequence. Each group
is then sorted by a recursive invocation of the same method. The
recursion terminates with trivially small groups (at most one-element
ones, say). For example, the input sequence (4, 3, 6, 1, 5, 2, 7) would
be sorted as follows:
(4, 3, 6, 1, 5, 2, 7)
(3, 1, 2)
(6, 5, 7)
(1, 2)
∅
(5)
(7)
(2)
∅
This algorithm may need about n2 steps in the worst case (the
worst thing we can do to the algorithm is to confront it with an
already sorted sequence). In practice, however, QUICKSORT is very

10.4 Several applications
313
popular, and it is considered one of the fastest sorting algorithms.
It behaves very well “on average”. This is partially expressed in the
following theorem.
10.4.4 Theorem. Let x1 < x2 < · · · < xn be a sequence of real
numbers in an increasing order. Let π be a permutation of the
set {1, 2, . . . , n}, and let T(π) be the number of comparisons (of
pairs of elements) made by QUICKSORT for the input sequence
(xπ(1), xπ(2), . . . , xπ(n)). Then the expectation of T(π) for a random
permutation π is at most 2n ln n.
Let us remark that any algorithm that sorts every n-tuple of distinct
real numbers and uses only pairwise comparisons of these numbers has
to make at least log2 n! comparisons in the worst case. This is because
the algorithm must select one of the n! possible permutations according
to the outcomes of the comparisons, and k comparisons only have 2k
diﬀerent outcomes. As we know from Section 3.5, we have n! ≥
 n
e
n,
and so log2 n! = (log2 e) ln n! ≥(log2 e)(n −1) ln n ≈1.443(n −1) ln n.
Therefore, the average behavior of QUICKSORT guaranteed by Theo-
rem 10.4.4 is quite good.
Reasons for the O(n log n) average behavior of the QUICKSORT
algorithm are not diﬃcult to see. When the input elements are ran-
domly ordered, we expect that the ﬁrst element usually divides the
remaining elements into two groups of roughly comparable size—it is
unlikely that one group is much smaller than the other. If this hap-
pens in most cases, the recursion in the algorithm will have roughly
log n levels, and at each level of recursion we need O(n) comparisons
in total.
This may sound quite convincing, but certainly it is not a proof.
Next, we give a rigorous proof, based on a rather diﬀerent idea.
Proof of Theorem 10.4.4. Let Ti = Ti(π) be the number of el-
ements compared to the element xπ(i) at the moment xπ(i) is the
dividing element. For instance, we always have T1 = n −1, since
xπ(1) is the ﬁrst element in the input sequence, and all the other
elements are compared to it. If π(2) < π(1), T2 is π(1) −2, and
for π(2) > π(1) we have T2 = n −π(1) −1. In general, Ti can be
interpreted according to the following diagram:
d
d
t
d
t
t
d
d
d
t
d
d
d
t
h
xπ(i)
↓
↓
↓
↓

314
Probability and probabilistic proofs
The small circles in this diagram depict the elements x1, x2, . . . , xn
in the sorted order. The full circles are the elements with indices
π(1), π(2), . . . , π(i −1), i.e. the ﬁrst i −1 elements in the input se-
quence. The element xπ(i) is marked by a double circle, and the
remaining elements have empty circles. It is not diﬃcult to see that
Ti is exactly the number of empty circles “seen” by the element xπ(i),
if the full circles are considered opaque.
We will investigate the expectation of Ti. The key idea is to think
of the algorithm running backwards in time, as if we were watching
a movie going backwards. Imagine that we consider the indices i in
the backward order n, n−1, . . . , 1, and we look at the corresponding
picture with full and empty circles. Initially, all the circles are full.
The random permutation π is not yet deﬁned, and we create it by
random choices as we go along. First we choose one full circle at
random and make it empty. The index of this circle becomes π(n).
Then we randomly select another full circle and make it empty, which
determines π(n −1), and so on; this is the same way of generating
a random permutation as in Fig. 10.2. At the moment when i full
circles remain, we choose one of them at random, which corresponds
to xπ(i). The quantity Ti is the number of empty circles seen by xπ(i)
at this moment.
To estimate the expected value of Ti, we use double-counting.
Each of the n −i empty circles sees at most two full circles. Hence
the total number of pairs (full circle,empty circle) whose members
see each other is at most 2(n −i). On the average, one of the i full
circles sees at most 2(n−i)
i
empty circles, and hence E [Ti] ≤2(n−i)
i
.
Since T(π) = n
i=1 Ti(π), we get
E [T] =
n

i=1
E [Ti] ≤
n

i=1
2(n −i)
i
= 2n
n

i=1
1
i −2n ≤2n ln n.
2
Remark. It can be shown that the probability of the running time
of QUICKSORT deviating from the expectation by a signiﬁcant
amount is quite small (exponentially small), i.e. bad permutations
are quite rare. We will not consider this here.

10.4 Several applications
315
Exercises
1. From Theorem 10.4.2, prove that any graph on n vertices with no
triangles has at most n2/4 edges; this yields yet another proof of
Theorem 4.7.1.
2. ∗Show that if d1, . . . , dn are nonnegative real numbers adding up to 1,
then the expression n
i=1 1/(di + 1) is minimal for d1 = d2 = · · · =
dn = 1/n.
3. Consider the following algorithm for ﬁnding an independent set in a
graph G. Keep deleting a vertex of the maximum degree from the cur-
rent graph, together with edges containing it, until the current graph
has no edges. Show that this algorithm always ﬁnds an independent
set of size at least F(G) = 
v∈V (G) 1/(1 + degG(v)). Hint: Check
that if H arises from G by deleting a maximum degree vertex then
F(H) ≥F(G).
4. ∗Prove the following version of Tur´an’s theorem. Let r ≥3 be an inte-
ger, and let n be divisible by r −1. Then any graph on n vertices con-
taining no Kr as a subgraph has at most n2
2

1 −
1
r−1

edges. Proceed
by induction on the number
n
r−1.
5. Let G be a graph. Consider the following algorithm. Divide V (G) into
two parts A and B arbitrarily, and repeat the following step while
applicable: if there is a vertex with more neighbors in the part con-
taining it than in the other part, select some such vertex and move it
to the other part (than the one currently containing it). Show that this
algorithm always ﬁnishes (and give as good a bound on the running
time as possible), and that at least half of the edges of G go between
the resulting parts A, B.
6. (a) Modify the above algorithm QUICKSORT as follows. Let the cur-
rent input sequence be (x1, x2, . . . , xn). Choose a random number i
among 1, 2, . . . , n (all values have probability
1
n), and use xi as the
dividing element. Show that for any ﬁxed n-tuple of input numbers
(x1, x2, . . . , xn), assuming that no two of them are equal, the expected
number of comparisons is the same and is bounded by 2n ln n as in
Theorem 10.4.4.
(b) ∗Consider another modiﬁcation of the algorithm: given an input
sequence (x1, x2, . . . , xn), choose two independent random numbers
i1, i2 from {1, 2, . . . , n}, and use max(xi1, xi2) as the dividing element.
Suppose that (x1, x2, . . . , xn) are all distinct. Show that the expected
number of comparisons for a given input sequence is the same as in (a).
We do not count the comparisons needed to determine max(xi1, xi2)!
7. ∗Consider n empty circles in a row. An observer sits in one of the n−1
gaps between them. A random subset of r circles is colored black. Show

316
Probability and probabilistic proofs
that the expected number of empty circles the observer sees is at most
2(n −r)/(r + 1). (The observer can see through empty circles but not
through black ones.) Hint: Instead of coloring r circles color r + 1 of
them black ﬁrst and then make a random black circle empty again.
8. Let the subset R ⊆L be chosen at random as in the proof of Theo-
rem 10.4.3. Prove that E [|R|] = pn.
9. ∗Consider n lines as in Theorem 10.4.3, assuming, moreover, that none
of them is vertical. We say that an intersection v is a peak if one of
the lines deﬁning it has a positive slope and the other one a negative
slope. Prove that at most 6(k + 1)2 peaks of level at most k exist.

11
Order from disorder:
Ramsey’s theorem
Paul Erd˝os (1913–1996), a famous 20th century mathematician
and arguably one of the main creators of contemporary discrete
mathematics, liked to tell the following story. X-rays were discov-
ered by Wilhelm Conrad R¨ontgen in 1895. But the British physicist
Sir William Crookes observed a similar eﬀect earlier: photographic
plates became black mysteriously when stored near a tube nowadays
called by Crookes’ name. He noticed this and issued a directive to the
technicians that henceforth the plates were to be stored elsewhere.
The moral of this story is twofold. First, Fortune favors explorers
who are prepared for the discovery (and Erd˝os used to emphasize
this point). And second, key discoveries often have very modest and
seemingly triﬂing origins. Great theories often begin with eﬀects that
are almost imperceptible. But we have to be ready.
Mathematics and computer science also have their discoveries,
which often ﬁrst manifest themselves inconspicuously, as seemingly
irrelevant curiosities. In this chapter we discuss one such peculiarity,
concerning graphs with a mere 6 vertices.
We begin with the following popular form of the result. Six people
meet at a party. Some of them know each other, some of them don’t,
perhaps because they see one another for the ﬁrst time. The party
may look according to one of the following schemes, for example:
party 50 years
after graduation
lonely hearts
party
party of
admirers
meeting of two
mafia bosses

318
Order from disorder: Ramsey’s theorem
(we have represented the participants by dots, we have connected
the pairs of acquaintances by straight lines, and behold!—we have
graphs on 6 vertices). Or the party can also look in many other
ways, which wouldn’t even all ﬁt here. But whatever the party may
be, there will always be a triple of participants who mutually know
one another (three friends, say) or a triple of mutual strangers. We
can assert with certainty that one of these possibilities must always
occur, although there is no way of telling in advance which one.
This modest statement is a rudiment of a whole theory—Ramsey
theory, from which we will see a small part in this chapter. In the
next section we will prove the claim about parties with 6 partici-
pants, which is not as easy as it might perhaps seem, and in Sec-
tion 11.2 we prove Ramsey’s theorem for graphs, which shows that
the peculiarity for 6 vertices is not such a peculiarity after all. We
will meet another surprise concealed in Ramsey’s theorem in Sec-
tion 11.3: We will demonstrate that Ramsey’s theorem holds only
for sets that are truly gigantic. As a combinatorial principle it is
thus a rather ineﬀective and diﬃcult tool. We will employ the ap-
proach from Chapter 10—the probabilistic method.
11.1
A party of six
We now establish the claim for parties with 6 participants. A party
can of course be imagined as a graph, where vertices correspond to
the participants and the edges correspond to pairs of participants
who know each other. We thus assume that the “being acquainted
with” is a symmetric relation.
Let G be a graph. Similar to Section 2.4, we introduce the fol-
lowing notation: ω(G) denotes the maximum number of vertices of a
complete subgraph of G, and α(G) is the maximum number of ver-
tices of an independent set in G, that is, of a set with no two vertices
connected by an edge.
We prove the following theorem.
11.1.1 Theorem. Let G be a graph with at least 6 vertices. Then
α(G) ≥3 or ω(G) ≥3.
Proof. Let us choose one of the vertices of G arbitrarily and let us
denote it by u. Let A ⊆V (G)\{u} be the set of vertices that are not
connected to u; formally A = {v ∈V (G): v ̸= u, {u, v} ̸∈E(G)}.
Let B = V (G) \ A \ {u} consist of the vertices connected to u. Since

11.2 Ramsey’s theorem for graphs
319
|V (G)| ≥6, one of the sets A, B necessarily has at least 3 elements.
We now distinguish two cases.
1. Case |A| ≥3. Let A contain vertices x, y, z. If every two of them
form an edge of G, then {x, y, z} determines a triangle, i.e., a
complete subgraph showing that ω(G) ≥3. Otherwise, if, e.g.
{x, y} ̸∈E(G), then {u, x, y} is an independent set in G, and
thus α(G) ≥3.
2. Case |B| ≥3. We proceed analogously: either no pair of vertices
of B is an edge of G, and then α(G) ≥3, or else there exist
vertices x, y ∈B forming an edge G. In the latter case the set
{u, x, y} induces a complete subgraph of G, and hence ω(G) ≥3.
2
Note how many cases we had to distinguish in order to prove the
theorem dealing with graphs on 6 vertices. With all this eﬀort, the
result may look like a triﬂe. However, it is a special case of more
general theorems. Let us note that in the proof of Theorem 11.1.1
we have used a particular case, with t = 2, n1 = n2 = 3, of the
following well-known theorem:
11.1.2 Theorem (Pigeonhole principle). Let n1, n2, . . . , nt be
natural numbers, let X be a set with at least 1 + t
i=1(ni −1)
elements, and let X1, X2, . . . , Xt be disjoint sets forming a partition
of X. Then there exists i such that Xi has at least ni elements.
11.2
Ramsey’s theorem for graphs
Here is the promised generalization of Theorem 11.1.1 for “larger
parties”.
11.2.1 Theorem (Ramsey’s theorem for graphs). Let a graph
G have at least
k+ℓ−2
k−1

vertices. Then ω(G) ≥k or α(G) ≥ℓ.
Let us note that the condition in the theorem is symmetric with
respect to k and ℓ, since
k+ℓ−2
k−1

=
k+ℓ−2
ℓ−1

.
If we substitute k = ℓ= 3 in the theorem, we obtain that every
graph G with
4
2

= 6 vertices satisﬁes ω(G) ≥3 or α(G) ≥3, which
is exactly Theorem 11.1.1. An attentive reader will notice how similar
the following proof is to the proof of Theorem 11.1.1.
Proof. We proceed by induction on k + ℓ. For k = 1 or ℓ= 1 it
is easily seen that the statement holds. (The validity of the theorem
for k = 2 or ℓ= 2 is also easy to see, but we need not deal with this
case separately.)

320
Order from disorder: Ramsey’s theorem
Now let us assume that k, ℓ≥2 and that the statement holds for
k, ℓ−1 as well as for k −1, ℓ. We put n =
k+ℓ−2
k−1

, n1 =
k+ℓ−3
k−1

,
and n2 =
k+ℓ−3
k−2

. By the Pascal formula (3.5) for adding binomial
coeﬃcients we have n = n1 + n2, and we rewrite this in the form
n = 1 + ((n1 −1) + (n2 −1) + 1), which suggests the scheme of the
proof.
Let G = (V, E) be an arbitrary graph with n vertices. We want
to show that ω(G) ≥k or α(G) ≥ℓ. Let us choose a vertex u ∈V
arbitrarily. We divide the remaining vertices into two sets A and B
depending on whether they are connected to u; see the picture:
u
A
B
V \ {u}
Formally,
A = {v ∈V \ {u}: {u, v} ̸∈E}
B = {v ∈V \ {u}: {u, v} ∈E}.
By the pigeonhole principle we have |A| ≥n1 or |B| ≥n2. If |A| ≥
n1, we use the inductive assumption for the pair (k, ℓ−1). We get
that the graph GA induced by the set A satisﬁes ω(GA) ≥k or
α(GA) ≥ℓ−1. In the former case we have ω(G) ≥k. In the latter
case any independent set I ⊂A together with the vertex u constitute
an independent set I ∪{u} in G, and so α(G) ≥α(GA) + 1 ≥ℓ.
In the case |B| ≥n2 we proceed analogously and we again obtain
that ω(G) ≥k or α(G) ≥ℓ. This ﬁnishes the induction step.
2
The theorem just proved allows us to introduce the following
deﬁnition. Let r(k, ℓ) denote the minimum natural number n such
that every graph G with n vertices satisﬁes ω(G) ≥k or α(G) ≥ℓ.
The number r(k, ℓ) is called a Ramsey number. Ramsey’s theorem
for graphs guarantees that r(k, ℓ) exists for every k ≥1 and ℓ≥1.
Let us summarize the values of the Ramsey numbers that we have
derived or that are easily checked:
r(1, ℓ)
=
1,
r(k, 1)
=
1
r(2, ℓ)
=
ℓ,
r(k, 2)
=
k
r(3, 3)
=
6.
Only a handful of other values are known in addition to those listed
above. For example, it is known that r(4, 4) = 18, but already the

11.3 A lower bound for the Ramsey numbers
321
value of r(5, 5) remains unknown, in spite of considerable eﬀort of
men and computers.
In the next section we present a lower bound for r(k, k).
Exercises
1. (a) Derive the following statement from Theorem 11.2.1: For every n
there exists N such that if X is an N-element set and the set
X
2

is
partitioned into two subsets E1 and E2, then at least one of the graphs
(X, E1) and (X, E2) contains a complete subgraph on n vertices.
(b) ∗Generalize the statement in (a) for a partition of
X
2

into r parts,
and prove this generalization. (Hint: proceed by induction on r.)
2. For a graph G let us put f(G) = α(G)·ω(G), and let us deﬁne f(n) =
min f(G), where the minimum is over all graphs with n vertices.
(a) Prove that for n ∈{1, 2, 3, 4, 6} we have f(n) ≥n.
(b) Prove that f(5) < 5.
(c)∗For natural numbers n, k, 1 < k ≤n/2 we deﬁne a graph Cn,k:
We begin with Cn, i.e., a cycle of length n, and then we connect by
edges all pairs of vertices that have distance at most k in Cn. Use these
graphs (with a judicious choice of k) to prove that f(n) < n for all
n ≥7.
3. Show that the function f(n) as in Exercise 2 is nondecreasing and that
it is not bounded from above.
4. Prove that k ≤k′ and ℓ≤ℓ′ imply r(k, ℓ) ≤r(k′, ℓ′).
5. ∗∗Try to understand the following generalization of Ramsey’s theorem
and to prove it. For any three natural numbers p, r, n there exists a
number N with the following property: If X is an arbitrary set with
at least N elements and if
X
p

= A1 ∪A2 ∪· · · ∪Ar is an arbitrary
partition of the set of all p-tuples of elements of X, then there exists
an n-element set Y ⊆X such that
Y
p

is completely contained in a
single class Ai. (Hint: proceed by induction on p.)
11.3
A lower bound for the Ramsey numbers
In this section we derive one of the basic properties of the Ramsey
numbers, which is essential for their investigation and for applica-
tions of Ramsey’s theorem. We show that the number r(k, k), which
for the sake of brevity we denote simply by r(n), grows exponentially
fast. (We note that for some generalizations of Ramsey’s theorem,
which will not be discussed here, one obtains functions that grow
even much faster.) Before formulating and proving the result, let us
recall what proving a lower bound for r(k) means: If we claim that

322
Order from disorder: Ramsey’s theorem
r(k) > n, we assert the existence of a certain graph G on n vertices,
namely, G satisfying ω(G) < k and α(G) < k. (In particular, it is
not enough to check that n <
2k−2
k−1

, because the expression
2k−2
k−1

from Theorem 11.2.1 is only an upper bound for r(k), and r(k) itself
might be much smaller!)
For example, in the preceding section we proved r(3) > 5 using
the graph C5. We also mentioned without proof that r(4) = 18. The
inequality r(4) > 17 can be proved by means of the following graph:
Interestingly, both for r(3) > 5 and for r(4) > 17 the respective
graphs are determined uniquely up to isomorphism. This indicates
the inherent diﬃculty of bounding the Ramsey numbers from below.
Here we prove a lower bound for r(k) by the probabilistic method,
whose basics are explained in Chapter 10. It was one of the ﬁrst
proofs by the probabilistic method in combinatorics, and actually, in
mathematics in general.
11.3.1 Theorem. Let k and n be natural numbers satisfying
n
k

·
21−(k
2) < 1. Then r(k) > n.
What numbers k and n satisfy the condition in the theorem? As
is usual in the probabilistic method, we are going to use quite rough
and simple estimates:
n
k

≤nk
k! <
nk
2k/2+1

11.3 A lower bound for the Ramsey numbers
323
(the second inequality holds only for k ≥3). Let us remark that we
wouldn’t get a much better result by using the more precise estimates
from Chapter 3. So we bound the expression from Theorem 11.3.1
as follows:
n
k

· 21−(k
2) <
nk
2k/2+1 · 21−k(k−1)/2 =
 n
2k/2
k
.
So the condition in the theorem is satisﬁed for n = 2k/2, and we
arrive at the next corollary:
11.3.2 Corollary. For all k ≥3 we have r(k) > 2k/2.
This inequality is valid for k = 2 as well, but it is not a conse-
quence of the estimates above.
Proof of Theorem 11.3.1. As in Chapter 10 we consider a random
graph G on the vertex set V = {1, . . . , n}, where each of the
n
2

potential edges appears with probability 1
2, independent of all others.
Let K be an arbitrary k-element subset of V . Let AK denote
the event “the vertices of K induce a complete subgraph of G”. The
probability P(AK) of AK equals 2−(k
2). Similarly, if we denote by BK
the event “the vertices of K constitute an independent set in G”, then
P(BK) = 2−(k
2) as well. The probability of the even CK = AK ∪BK
(i.e. K induces either a complete subgraph or an independent set)
equals 2·2−(k
2) = 21−(k
2). Let p denote the probability that there exists
a k-element K ⊆V for which CK occurs. This probability is diﬃcult
to determine, since the events CK are in general not independent.
But for our purposes it suﬃces to estimate p from above by the sum
of the probabilities of all CK:
p ≤

K⊆V, |K|=k
P(CK)
(the sum is over all k-element subsets of the n-element set V ). We
thus obtain p ≤
n
k

· 21−(k
2), and therefore, the assumption in the
theorem guarantees that p < 1. But this means that there has to
exist at least one graph on V for which both α and ω are smaller
than k.
2
We note that does proof not provide any method for constructing
the desired graph witnessing the inequality r(k) > 2
k
2 . Not only does

324
Order from disorder: Ramsey’s theorem
this proof not provide a method: although this celebrated proof, due
to Erd˝os, was discovered in 1947, no construction of such graphs has
been found up until now, and this lower bound for Ramsey numbers
remains one of the most convincing pieces of evidence for the power
of the probabilistic method.
Exercises
1. Prove that
√
2 ≤
k
r(k) ≤4 (for all k ≥2).
2. Construct a graph G showing that r(k) ≥(k −1)2.
3. The graph witnessing r(4) > 17 may look complicated, but actually,
it is easy to remember. For example, it is enough to remember this:
17; 1, 2, 4, 8. Or this: quadratic residues modulo 17. Can you explain
these two somewhat cryptic memory aids?
4. Prove r(4) = 18 by considering the example given in the text and by
“improving” the proof of Theorem 11.2.1.

12
Generating functions
In this chapter, we are going to present a useful calculation technique.
The basic idea, quite a surprising one, is to consider an inﬁnite seq-
uence of real numbers and associate a certain continuous function
with it, the so-called generating function of the sequence. Problems
about sequences can then be solved by calculations with functions.
In this introductory text, we will only get to simpler examples, whose
solution can mostly be found also without generating functions. In some
problems, various tricks can reach the goal even quicker than the method
of generating functions (but it may not be easy to discover such tricks).
This should not discourage the reader from learning about generating
functions, since these are a powerful tool also for more advanced prob-
lems where other methods fail or become too complicated.
In some solutions, alternative methods are mentioned, for others
they are indicated in the exercises. Sometimes the reader will perhaps
ﬁnd even simpler solutions.
12.1
Combinatorial applications of polynomials
How do we multiply the polynomials p(x) = x + x2 + x3 + x4 and
q(x) = x + x3 + x4? Here is a simple rule: multiply each term of p(x)
by each term of q(x) and add all these products together. Adding the
products is simple, since all the products have coeﬃcient 1. In this
way, we calculate that p(x)q(x) = x8+2x7+2x6+3x5+2x4+x3+x2.
Let us now ask a diﬀerent question. We pick some power of x,
say x5, and we want to know its coeﬃcient in p(x)q(x), without
calculating the whole product. In our case, x5 appears by multiplying
the term x in p(x) by x4 in q(x), and also by multiplying x2 in p(x)
by x3 in q(x), and ﬁnally by multiplying x4 in p(x) by x in q(x).
Each of these possibilities adds 1 to the resulting coeﬃcient, and
hence the coeﬃcient of x5 in the product p(x)q(x) is 3.
This is as if we had 4 silver coins with values 1, 2, 3, and 4
doublezons (these are the exponents of x in the polynomial p(x)) and

326
Generating functions
3 golden coins with values 1, 3, and 4 doublezons (corresponding to
the exponents of x in q(x)), and asked how may ways there are to
pay 5 doublezons by one silver and one golden coin. Mathematically
speaking, the coeﬃcient of x5 is the number of ordered pairs (i, j),
where i + j = 5, i ∈{1, 2, 3, 4}, and j ∈{1, 3, 4}.
Let us express the consideration just made for the two particular
polynomials somewhat more generally. Let I and J be ﬁnite sets of
natural numbers. Let us form the polynomials p(x) = 
i∈I xi and
q(x) = 
j∈J xj (note that the coeﬃcients in such polynomials are
0s and 1s). Then, for any natural number r, the number of solutions
(i, j) of the equation
i + j = r
with i ∈I and j ∈J equals the coeﬃcient of xr in the product
p(x)q(x).
A further, more interesting generalization of this observation deals
with a product of 3 or more polynomials. Let us illustrate it on a
particular example ﬁrst.
Problem. How many ways are there to pay the amount of 21 dou-
blezons if we have 6 one-doublezon coins, 5 two-doublezon coins, and
4 ﬁve-doublezon coins?
Solution. The required number equals the number of solutions of
the equation
i1 + i2 + i3 = 21
with
i1 ∈{0, 1, 2, 3, 4, 5, 6}, i2 ∈{0, 2, 4, 6, 8, 10}, i3 ∈{0, 5, 10, 15, 20}.
Here i1 is the amount paid by coins of value 1 doublezon, i2 the
amount paid by 2-doublezon coins, and i3 the amount paid by
5-doublezon coins.
This time we claim that the number of solutions of this equation
equals the coeﬃcient of x21 in the product

1 + x + x2 + x3 + · · · + x6 
1 + x2 + x4 + x6 + x8 + x10
×

1 + x5 + x10 + x15 + x20
(after multiplying out the parentheses and combining the terms with
the same power of x, of course). Indeed, a term with x21 is obtained

12.1 Combinatorial applications of polynomials
327
by taking some term xi1 from the ﬁrst parentheses, some term xi2
from the second, and xi3 from the third, in such a way that i1 + i2 +
i3 = 21. Each such possible selection of i1, i2, and i3 contributes 1
to the considered coeﬃcient of x21 in the product.
How does this help us in solving the problem? From a purely prac-
tical point of view, this allows us to get the answer easily by computer,
provided that we have a program for polynomial multiplication at our
disposal. In this way, the authors have also found the result: 9. Since we
only deal with relatively few coins, the solution can also be obtained by
listing all possibilities, but it could easily happen that we forget some.
However, the method explained above is most signiﬁcant as a prelude
to handling more complicated situations.
A combinatorial meaning of the binomial theorem.
The
binomial theorem asserts that
(1 + x)n =
n
0

+
n
1

x +
n
2

x2 + · · · +
n
n

xn.
(12.1)
On the left-hand side, we have a product of n polynomials, each of
them being 1+x. Analogously to the above considerations with coins,
the coeﬃcient of xr after multiplying out the parentheses equals the
number of solutions of the equation
i1 + i2 + · · · + in = r,
with i1, i2, . . . , in ∈{0, 1}. But each such solution to this equation
means selecting r variables among i1, i2, . . . , in that equal 1—the
n −r remaining ones must be 0. The number of such selections is
the same as the number of r-element subsets of an n-element set,
i.e.
n
r

. This means that the coeﬃcient of xr in the product (1+x)n
is
n
r

. We have just proved the binomial theorem combinatorially!
If we play with the polynomial (1+x)n and similar ones skillfully,
we can derive various identities and formulas with binomial coeﬃ-
cients. We have already seen simple examples in Section 3.3, namely
the formulas n
k=0
n
k

= 2n and n
k=0(−1)kn
k

= 0 obtained by
substituting x = 1 and x = −1 into (12.1), respectively.
For the next example, the reader should be familiar with the
notion of a derivative (of a polynomial).
12.1.1 Example. For all n ≥1, we have
n

k=0
k
n
k

= n2n−1.

328
Generating functions
Proof.
This equality can be proved by diﬀerentiating both sides
of the formula (12.1) as a function of the variable x. On both sides,
we must obtain the same polynomial. By diﬀerentiating the left-hand
side we get n(1+x)n−1, and diﬀerentiating the right-hand side yields
n
k=0 k
n
k

xk−1. By setting x = 1 we get the desired identity.
2
An example of a diﬀerent type is based on the equality of coeﬃ-
cients in two diﬀerent expressions for the same polynomial.
Another proof of Proposition 3.3.4. We want to prove
n

i=0
n
i
2
=
2n
n

.
Consider the identity
(1 + x)n(1 + x)n = (1 + x)2n.
The coeﬃcient of xn on the right-hand side is
2n
n

by the binomial
theorem. On the left-hand side, we can expand both the powers
(1 + x)n according to the binomial theorem, and then we multiply
the two resulting polynomials. The coeﬃcient of xn in their product
can be expressed as
n
0
n
n

+
n
1
 n
n−1

+
n
2
 n
n−2

+ · · · +
n
n
n
0

,
and this must be the same number as the coeﬃcient of xn on the
right-hand side. This leads to
n

i=0
n
i
 n
n −i

=
2n
n

.
So we have proved Proposition 3.3.4 in a diﬀerent way.
2
A number of other sums and formulas can be handled similarly.
But if we try some more complicated calculations, we soon encounter,
as an unpleasant restriction, the fact that we work with polynomials
having ﬁnitely many terms only. It turns out that the “right” tool for
such calculations is an object analogous to a polynomial but with pos-
sibly inﬁnitely many powers of x, the so-called power series. This is the
subject of the next section.
Exercises
1. Let a(x) = a0+a1x+a2x2+· · ·+anxn and b(x) = b0+b1x+b2x2+· · ·+
bmxm be two polynomials. Write down a formula for the coeﬃcient of
xk in the product a(x)b(x), where 0 ≤k ≤n + m.

12.2 Calculation with power series
329
2. A coﬀee shop sells three kinds of cakes—Danish cheese cakes, German
chocolate cakes, and brownies. How many ways are there to buy 12
cakes in such a way that at least 2 cakes of each kind are included,
but no more than 3 German chocolate cakes? Express the required
number as a coeﬃcient of a suitable power of x in a suitable product
of polynomials.
3. How many ways are there to distribute 10 identical balls among 2 boys
and 2 girls, if each boy should get at least one ball and each girl should
get at least 2 balls? Express the answer as a coeﬃcient of a suitable
power of x in a suitable product of polynomials.
4. Prove the multinomial theorem 3.3.5 in a similar way as the binomial
theorem was proved in the text.
5. Calculate the sum in Example 12.1.1 by a suitable manipulation of the
expression k
n
k

and by using the binomial theorem.
6. Compute the sum n
i=0(−1)in
i
 n
n−i

.
12.2
Calculation with power series
Properties of power series. A power series is an inﬁnite series of
the form a0 + a1x + a2x2 + · · · , where a0, a1, a2, . . . are real numbers
and x is a variable attaining real values.1 This power series will
usually be denoted by a(x).
A simple example of a power series is
1 + x + x2 + x3 + · · ·
(12.2)
(all the ai are 1). If x is a real number in the interval (−1, 1), then
this series converges and its sum equals
1
1−x (this is the well-known
formula for the sum of an inﬁnite geometric series; if you’re not
familiar with it you should certainly consider doing Exercise 1). In
this sense, the series (12.2) determines the function
1
1−x. Conversely,
this function contains all the information about the series (12.2).
Indeed, if we diﬀerentiate the function k times and then substitute
x = 0 into the result, we obtain exactly k! times the coeﬃcient of xk.
In other words, the series (12.2) is the Taylor series of the function
1
1−x at x = 0. Hence the function
1
1−x can be understood as an
incarnation of the inﬁnite sequence (1, 1, 1, . . .) and vice versa. Such
1It is extremely useful to consider also complex values of x and to apply
methods from the theory of functions of a complex variable. But we are not
going to get this far in our introductory treatment.

330
Generating functions
a transmutation of inﬁnite sequences into functions and back is a
key step in the technique of generating functions.
In order to explain what generating functions are, we have to use
some notions of elementary calculus (convergence of inﬁnite series, deri-
vative, Taylor series), as we have already done in the example just given.
If you’re not familiar with enough calculus to understand this introduc-
tion, you still need not give up reading, because almost nothing from
calculus is usually needed to solve problems using generating functions.
For example, you can accept as a matter of faith that the inﬁnite series
1 + x + x2 + · · · and the function
1
1−x mean the same thing, and use
this and similar facts listed below for calculations.
The following proposition says that if the terms of a sequence
(a0, a1, a2, . . .) do not grow too fast, then the corresponding power
series a(x) = a0 + a1x + a2x2 + · · · indeed deﬁnes a function of the
real variable x, at least in some small neighborhood of 0. Further,
from the knowledge of the values of this function we can reconstruct
the sequence (a0, a1, a2, . . .) uniquely.
12.2.1 Proposition. Let (a0, a1, a2, . . .) be a sequence of real num-
bers, and let us suppose that for some real number K, we have
|an| ≤Kn for all n ≥1. Then for any number x ∈(−1
K , 1
K ), the
series a(x) = ∞
i=0 aixi converges (even absolutely), and hence the
value of its sum deﬁnes a function of the real variable x on this in-
terval. This function will also be denoted by a(x). The values of the
function a(x) on an arbitrarily small neighborhood of 0 determine all
the terms of the sequence (a0, a1, a2, . . .) uniquely. That is, the func-
tion a(x) has derivatives of all orders at 0, and for all n = 0, 1, 2, . . .
we have
an = a(n)(0)
n!
(a(n)(0) stands for the nth derivative of the function a(x) at the
point 0).
A proof follows from basic results of mathematical analysis, and we
omit it here (as we do for proofs of a few other results in this section).
Most natural proofs are obtained from the theory of functions of a
complex variable, which is usually covered in more advanced courses
only. But we need very little for our purposes, and this can be proved,
somewhat more laboriously, also from basic theorems on limits and
derivatives of functions of a real variable. In the subsequent examples,
we will not explicitly check that the power series in question converge
in some neighborhood of 0 (i.e. the assumptions of Proposition 12.2.1).
Usually it is easy. Moreover, in many cases, this can be avoided in

12.2 Calculation with power series
331
the following way: once we ﬁnd a correct solution of a problem using
generating functions, in a possibly very suspicious manner, we can verify
the solution by some other method, say by induction. And, ﬁnally, we
should remark that there is also a theory of the so-called formal power
series, which allows one to work even with power series that never
converge (except at 0) in a meaningful way. So convergence is almost
never a real issue in applications of generating functions.
Now, ﬁnally, we can say what a generating function is:
12.2.2 Definition. Let (a0, a1, a2, . . .) be a sequence of real num-
bers. By the generating function of this sequence2 we understand the
power series a(x) = a0 + a1x + a2x2 + · · · .
If the sequence (a0, a1, a2, . . .) has only ﬁnitely many nonzero terms,
then its generating function is a polynomial. Thus, in the preceding
section, we have used generating functions of ﬁnite sequences without
calling them so.
Manufacturing generating functions.
In applications of gen-
erating functions, we often encounter questions like “What is the
generating function of the sequence (1, 1
2, 1
3, 1
4, . . .)?” Of course, the
generating function is 1 + 1
2x + 1
3x2 + 1
4x3 + · · · by deﬁnition, but is
there a nice closed formula? In other words, doesn’t this power series
deﬁne some function well known to us from calculus, say? (This one
does, namely −ln(1−x)
x
; see below.) The answer can often be found by
an “assembling” process, like in a home workshop. We have a supply
of parts, in our case sequences like (1, 1, 1, . . .), for which we know
the generating function right away. We also have a repertoire of sim-
ple operations on sequences and their corresponding operations on
the generating functions. For example, knowing the generating func-
tion a(x) of some sequence (a0, a1, a2, . . .), the generating function
of the sequence (0, a0, a1, a2, . . .) equals x a(x). With some skill, we
can usually “assemble” the sequence we want.
Let us begin with a supply of the “parts”. Various examples of
Taylor (or Maclaurin) series mentioned in calculus courses belong to
2A more detailed name often used in the literature is the ordinary gener-
ating function. This suggests that other types of generating functions are also
used in mathematics. We brieﬂy mention the so-called exponential generating
functions, which are particularly signiﬁcant for combinatorial applications. The
exponential generating function of a sequence (a0, a1, a2, . . .) is the power series
∞
i=0(ai/i!)xi. For example, the sequence (1, 1, 1, . . .) has the exponential gener-
ating function ex. In the sequel, except for a few exercises, we restrict ourselves
to ordinary generating functions.

332
Generating functions
such a supply. For instance, we have
x
1 + x2
2 + x3
3 + · · · = −ln(1 −x)
(12.3)
(valid for all x ∈(−1, 1)) and
1 + x
1! + x2
2! + x3
3! + · · · = ex
(holds for all real x). Lots of other examples can be found in cal-
culus textbooks. Here is another easy calculus result, which is used
particularly often:
12.2.3 Proposition (Generalized binomial theorem). For an
arbitrary real number r and for any nonnegative integer k, we deﬁne
the binomial coeﬃcient
r
k

by the formula
r
k

= r(r −1)(r −2) . . . (r −k + 1)
k!
(in particular, we set
r
0

= 1). Then the function (1 + x)r is the
generating function of the sequence
r
0

,
r
1

,
r
2

,
r
3

, . . .

. The power
series
r
0

+
r
1

x +
r
2

x2 + · · · always converges for all |x| < 1.
A proof again belongs to the realm of calculus, and it is easily
done via the Taylor series.
For combinatorial applications, it is important to note that for r
being a negative integer, the binomial coeﬃcient
r
k

can be expressed
using the “usual” binomial coeﬃcient (involving nonnegative integers
only):
r
k

= (−1)k−r+k−1
k

= (−1)k−r+k−1
−r−1

. Hence for negative
integer powers of 1 −x we obtain
1
(1 −x)n =
n−1
n−1

+
 n
n−1

x +
n+1
n−1

x2 + · · · +
n+k−1
n−1

xk + · · · .
Note that the equality
1
1−x = 1 + x + x2 + · · · is a particular case
for n = 1.
Operations
with
sequences
and
with
their
generating
functions.
To facilitate the above-mentioned process of “assem-
bling” generating functions for given sequences, we list some impor-
tant operations. In the sequel, let (a0, a1, a2, . . .) and (b0, b1, b2, . . .)
be sequences and let a(x) and b(x) be their respective generating
functions.

12.2 Calculation with power series
333
A. If we add the sequences term by term, then the correspond-
ing operation with generating functions is simply their addition.
That is, the sequence (a0 +b0, a1 +b1, a2 +b2, . . .) has generating
function a(x) + b(x).
B. Another simple operation is multiplication by a ﬁxed real num-
ber α. The sequence (αa0, αa1, αa2, . . .) has generating function
α · a(x).
C. If n is a natural number, then the generating function xna(x)
corresponds to the sequence
(0, 0, . . . , 0



n×
, a0, a1, a2, . . .).
This is very useful for shifting the sequence to the right by a
required number of positions.
D. What do we do if we want to shift the sequence to the left, i.e.
to gain the generating function for the sequence (a3, a4, a5, . . .),
say? Obviously, we have to divide by x3, but we must not for-
get to subtract the 3 initial terms ﬁrst. The correct generating
function of the above sequence is
a(x) −a0 −a1x −a2x2
x3
.
E. Substituting αx for x.
Let α be a ﬁxed real number, and let
us consider the function c(x) = a(αx). Then c(x) is the gener-
ating function of the sequence (a0, αa1, α2a2, . . .). For example,
we know that
1
1−x is the generating function of the sequence of
all 1s, and so by the rule just given,
1
1−2x is the generating func-
tion of the sequence of powers of 2: (1, 2, 4, 8, . . .). This operation
is also used in the following trick for replacing all terms of the
considered sequence with an odd index by 0: as the reader can
easily check, the function 1
2(a(x) + a(−x)) corresponds to the
sequence (a0, 0, a2, 0, a4, 0, . . .).
F. Another possibility is a substitution of xn for x. This gives rise to
the generating function of the sequence whose term number nk
equals the kth term of the original sequence, and all of whose
other terms are 0s. For instance, the function a(x3) produces
the sequence (a0, 0, 0, a1, 0, 0, a2, 0, 0, . . .). A more complicated
operation generalizing both E and F is the substitution of one
power series for x into another power series. We will meet only
a few particular examples in the exercises.

334
Generating functions
Let us see some of the operations listed so far in action.
Problem.
What is the generating function of the sequence
(1, 1, 2, 2, 4, 4, 8, 8, . . .),
i.e. an = 2⌊n/2⌋?
Solution.
As was mentioned in E, the sequence (1, 2, 4, 8, . . .) has
generating function 1/(1 −2x). By F we get the generating function
1/(1 −2x2) for the sequence (1, 0, 2, 0, 4, 0, . . .), and by C, the sequence
(0, 1, 0, 2, 0, . . .) has generating function x/(1 −2x2). By addition we
ﬁnally get the generating function for the given sequence; that is, the
answer is (1 + x)/(1 −2x2).
2
G. Popular operations from calculus, diﬀerentiation and integration
of generating functions, mean the following in the language of
sequences. The derivative of the function a(x), i.e. a′(x), corre-
sponds to the sequence
(a1, 2a2, 3a3, . . .).
In other words, the term with index k is (k + 1)ak+1 (a power
series is diﬀerentiated term by term in the same way as a poly-
nomial). The generating function
 x
0 a(t)dt gives the sequence
(0, a0, 1
2a1, 1
3a2, 1
4a3, . . .); that is, for all k ≥1, the term with
index k equals
1
kak−1. For instance, we can derive the power
series (12.3) for ln(1 −x) by integrating the function
1
1−x.
Here is an example where diﬀerentiation helps:
12.2.4 Problem. What is the generating function for the sequence
(12, 22, 32, . . .) of squares, i.e. for the sequence (a0, a1, a2, . . .) with ak =
(k + 1)2?
Solution. We begin with the sequence of all 1s with the generating
function
1
1−x. The ﬁrst derivative of this function, 1/(1 −x)2, gives the
sequence (1, 2, 3, 4, . . .) by G. The second derivative is 2/(1 −x)3, and
its sequence is (2 · 1, 3 · 2, 4 · 3, . . .), again according to G; the term with
index k is (k + 2)(k + 1) = (k + 1)2 + k + 1. But we want ak = (k + 1)2,
and so we subtract the generating function of the sequence (1, 2, 3, . . .).
We thus get
a(x) =
2
(1 −x)3 −
1
(1 −x)2 .
2
H. The last operation in our list is perhaps the most interesting one:
multiplication of generating functions. The product a(x)b(x) is

12.2 Calculation with power series
335
the generating function of the sequence (c0, c1, c2, . . .), where the
numbers ck are given by the equations
c0
=
a0b0
c1
=
a0b1 + a1b0
c2
=
a0b2 + a1b1 + a2b0
...
and in general we can write
ck =

i,j≥0: i+j=k
aibj.
(12.4)
This is easy to remember—the terms in the product a(x)b(x) up
to the kth one are the same as in the product of the polynomials
(a0 + a1x + · · · + akxk) and (b0 + b1x + · · · + bkxk).
Multiplication of generating functions has a combinatorial inter-
pretation which we now explain in a somewhat childish example.
A natural example comes in Section 12.4. Suppose that we have a
supply of indistinguishable wooden cubes, and we know that we can
build a tower in ai diﬀerent ways from i cubes, i = 0, 1, 2, . . ., and
also that we can build a pyramid in bj diﬀerent ways from j cubes,
j = 0, 1, 2, . . ..
If we now have k cubes altogether, then ck = a0bk+a1bk−1+· · ·+akb0
is the number of ways to build one tower and one pyramid at the
same time (assuming that a tower and a pyramid never have any
cubes in common). In short, the generating function for the number
of ordered pairs (tower, pyramid) is obtained as the product of the
generating functions for the number of towers and for the number of
pyramids.
Remark. The operations listed above are useful not only for ﬁnding
the generating function for a given sequence, but often also for

336
Generating functions
the reverse task, namely ﬁnding the sequence corresponding to a
given generating function. In principle, the sequence can be found
by computing the Taylor series (according to Proposition 12.2.1),
i.e. by a repeated diﬀerentiation, but in practice this is seldom a
good method.
We conclude this section with an example of an application of gen-
erating functions. More examples are supplied in the exercises, and
several
more
sophisticated
problems
follow
in
subsequent
sections.
Problem. A box contains 30 red, 40 blue, and 50 white balls; balls
of the same color are indistinguishable. How many ways are there of
selecting a collection of 70 balls from the box?
Solution. Armed with the results of Section 12.1, we ﬁnd that the
number we seek equals the coeﬃcient of x70 in the product
(1 + x + x2 + · · · + x30)(1 + x + x2 + · · · + x40)(1 + x + x2 + · · · + x50).
We need not multiply this out. Instead, we can rewrite
1 + x + x2 + · · · + x30 = 1 −x31
1 −x .
(12.5)
To see this equality, we can recall a formula for the sum of the ﬁrst
n terms of a geometric series. In case we cannot remember it, we can
help ourselves as follows. We begin with the generating function of
the sequence (1, 1, 1, . . .), which is
1
1−x, and we subtract from it the
generating function of the sequence
(0, 0, . . . , 0



31×
, 1, 1, . . .),
which is x31/(1−x) by item C above. The result is (1−x31)/(1−x),
which is the generating function for the sequence
(1, 1, . . . , 1



31×
, 0, 0, . . .).
This shows that (12.5) holds.

12.2 Calculation with power series
337
Hence we can rewrite the whole product as
1 −x31
1 −x · 1 −x41
1 −x · 1 −x51
1 −x =
1
(1 −x)3 (1 −x31)(1 −x41)(1 −x51).
The factor (1 −x)−3 can be expanded according to the generalized
binomial theorem 12.2.3. In the product of the remaining factors
(1−x31)(1−x41)(1−x51), it suﬃces to ﬁnd the coeﬃcients of powers
up to x70, which is quite easy. We get
2
2

+
3
2

x +
4
2

x2 + · · ·

(1 −x31 −x41 −x51 + · · · ),
where the dots · · · in the second parentheses stand for powers higher
than x70. The coeﬃcient of x70 in this product is
70+2
2

−
70+2−31
2

−
70+2−41
2

−
70+2−51
2

= 1061.
Exercises
1. (a) Show that (1 −x)(1 + x + x2 + · · · + xn) and 1 −xn+1 are the
same polynomials, and deduce the formula 1 + x + x2 + · · · + xn =
(1 −xn+1)/(1 −x). For which values of x is it correct?
(b) Using the formula in (a), show that the inﬁnite series 1+x+x2+· · ·
converges to
1
1−x for all x ∈(−1, 1), and diverges for all x outside this
interval. (This requires basic notions from calculus.)
2. (a) Determine the coeﬃcient of x15 in (x2 + x3 + x4 + · · · )4.
(b) Determine the coeﬃcient of x50 in (x7 + x8 + x9 + x10 + · · · )6.
(c) Determine the coeﬃcient of x5 in (1 −2x)−2.
(d) Determine the coeﬃcient of x4 in
3√1 + x.
(e) Determine the coeﬃcient of x3 in (2 + x)3/2/(1 −x).
(f) Determine the coeﬃcient of x4 in (2 + 3x)5√1 −x.
(g) Determine the coeﬃcient of x3 in (1 −x + 2x2)9.
3. Find generating functions for the following sequences (express them in
a closed form, without inﬁnite series!):
(a) 0, 0, 0, 0, −6, 6, −6, 6, −6, . . .
(b) 1, 0, 1, 0, 1, 0, . . .
(c) 1, 2, 1, 4, 1, 8, . . .
(d) 1, 1, 0, 1, 1, 0, 1, 1, 0, . . .
4. Find the probability that we get exactly 12 points when rolling 3 dice.

338
Generating functions
5. Let an be the number of ordered triples (i, j, k) of integer numbers
such that i ≥0, j ≥1, k ≥1, and i+3j +3k = n. Find the generating
function of the sequence (a0, a1, a2, . . .) and calculate a formula for an.
6. Let an be the number of ordered r-tuples (i1, . . . , ir) of nonnegative
integers with i1+i2+· · ·+ir = n; here r is some ﬁxed natural number.
(a) Find the generating function of the sequence (a0, a1, a2, . . .).
(b) Find a formula for an. (This has been solved by a diﬀerent method
in Section 3.3.)
7. Solve Problem 12.2.4 without using diﬀerentiation—apply the gener-
alized binomial theorem instead.
8. Let an be the number of ways of paying the sum of n doublezons using
coins of values 1, 2, and 5 doublezons.
(a) Write down the generating function for the sequence (a0, a1, . . .).
(b) ∗Using (a), ﬁnd a formula for an (reading the next section can
help).
9. (a) Check that if a(x) is the generating function of a sequence (a0,
a1, a2, . . .) then
1
1−xa(x) is the generating function of the sequence of
partial sums (a0, a0 + a1, a0 + a1 + a2, . . .).
(b) Using (a) and the solution to Problem 12.2.4, calculate the sum
n
k=1 k2.
(c) By a similar method, calculate the sum n
k=1 k3.
(d) For natural numbers n and m, compute the sum m
k=0(−1)kn
k

.
(e) Now it might seem that by this method we can calculate almost
any sum we can think of, but it isn’t so simple. What happens if we
try computing the sum n
k=1
1
k in this manner?
10. ∗Let n, r be integers with n ≥r ≥1. Pick a random r-element subset
of the set {1, 2, . . . , n} and call it R (all the
n
r

possible subsets have
the same probability of being picked for R). Show that the expected
value of the smallest number in R is n+1
r+1 .
11. (Calculus required) ∗Prove the formula for the product of power series.
That is, show that if a(x) and b(x) are power series satisfying the
assumptions of Proposition 12.2.1, then on some neighborhood of 0, the
power series c(x) with coeﬃcients given by the formula (12.4) converges
to the value a(x)b(x).
12. (Calculus required) Let a(x) = a0 +a1x+a2x2 + · · · be a power series
with nonnegative coeﬃcients, i.e. suppose that ai ≥0 for all i. We
deﬁne its radius of convergence ρ by setting
ρ = sup{x ≥0: a(x) converges}.
(a) ∗Prove that a(x) converges for each real number x ∈[0, ρ), and
that the function a(x) is continuous in the interval [0, ρ).

12.2 Calculation with power series
339
(b) Find an example of a sequence (a0, a1, a2, . . .) with ρ = 1 such that
the series a(ρ) diverges.
(c) Find an example of a sequence (a0, a1, a2, . . .) with ρ = 1 such that
the series a(ρ) converges.
13. (A warning example; calculus required) Deﬁne a function f by
f(x) =

e−1/x2
for x ̸= 0
0
for x = 0.
(a) ∗Show that all derivatives of f at 0 exist and equal 0.
(b) Prove that f is not given by a power series at any neighborhood
of 0.
14. (Exponential generating functions) In a footnote, we have deﬁned the
exponential generating function of a sequence (a0, a1, a2, . . .) as the
power series A(x) = ∞
i=0(ai/i!)xi. Here we are going to sketch some
combinatorial applications.
For a group of n people, one can consider various types of arrange-
ments. For instance, one type of arrangement of n people would be to
select one of the people as a leader, and the others as a crowd. For
each group of n people, there are n arrangements of this type. Other
examples of types of arrangements are: arranging the n people in a row
from left to right (n! possibilities), arranging the n people in a ring
for a folk dance (there are (n −1)! possibilities), letting the people be
just an unorganized crowd (1 possibility), arranging the people into
a jury (12 possibilities for n = 12, with electing one chairman, and 0
possibilities otherwise).
(a) For each type of arrangement mentioned above, write down the
exponential generating function of the corresponding sequence (i.e. for
the sequence (a0, a1, a2, . . .), where ai is the number of arrangements
of the considered type of i people).
(b) Let an be the number of possible arrangements of some type A for
a group of n people, and let bn be the number of possible arrangements
of some type B for a group of n people. Let an arrangement of type C
mean the following: divide the given group of n people into two groups,
the First Group and the Second Group, and arrange the First Group
by an arrangement of type A and the Second Group by an arrange-
ment of type B. Let cn be the number of arrangements of type C for
a group of n people. Check that if A(x), B(x), and C(x) are the cor-
responding exponential generating functions, then C(x) = A(x)B(x).
Discuss speciﬁc examples with the arrangement types mentioned in
the introduction to this exercise.
(c) ∗Let A(x) be the exponential generating function for arrangements
of type A as in (b), and, moreover, suppose that a0 = 0 (no empty

340
Generating functions
group is allowed). Let an arrangement of type D mean dividing the
given n people into k groups, the First Group, Second Group, . . . , kth
Group (k = 0, 1, 2, . . .), and arranging each group by an arrangement of
type A. Express the exponential generating function D(x) using A(x).
(d) ∗Let A(x) be as in (c), and let an arrangement of type E mean
dividing the given n people into some number of groups and organiz-
ing each group by an arrangement of type A (but this time it only
matters who is with whom, the groups are not numbered). Express
the exponential generating function E(x) using A(x).
(e) How many ways are there of arranging n people into pairs (it
only matters who is with whom, the pairs are not numbered)? Solve
using (d).
15. ∗Using part (d) of Exercise 14, ﬁnd the exponential generating function
for the Bell numbers, i.e. the number of equivalences on n given points.
Calculate the ﬁrst few terms of the Taylor series to check the result
numerically (see also Exercise 3.8.8).
16. Twelve students should be assigned work on 5 diﬀerent projects. Each
student should work on exactly one project, and for every project at
least 2 and at most 4 students should be assigned. In how many ways
can this be done? Use the idea of part (b) in Exercise 14.
17. (Hatcheck lady strikes again)
(a) ∗Using parts (a) and (d) of Exercise 14, write down the exponential
generating function for arranging n people into one or several rings for
a dance (a ring can have any number of persons including 1). Can you
derive the number of such arrangements in a diﬀerent way?
(b) Consider arrangements as in (a), but with each ring having at least
2 people. Write down the exponential generating function.
(c) ∗Using the result of (b), ﬁnd the number of arrangements of n
people into rings with at least 2 people each. If your calculations are
correct, the result should equal the number of permutations without
a ﬁxed point (see Section 3.8). Explain why. (This actually gives an
alternative solution to the hatcheck lady problem, without inclusion–
exclusion!)
12.3
Fibonacci numbers and the golden section
We will investigate the sequence (F0, F1, F2, . . .) given by the follow-
ing rules:
F0 = 0, F1 = 1, Fn+2 = Fn+1 + Fn
for n = 0, 1, 2, . . .

12.3 Fibonacci numbers and the golden section
341
This sequence has been studied by Leonardo of Pisa, called Fibonacci,
in the 13th century, and it is known as the Fibonacci numbers. Its ﬁrst
few terms are
0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, . . .
Fibonacci illustrated this sequence with a not too realistic example con-
cerning the reproduction of rabbits in his treatise, but the sequence does
have a certain relevance in biology (for instance, the number of petals of
various ﬂowers is very often a Fibonacci number; see Stewart [46] for a
possible explanation). In mathematics and computer science it surfaces
in various connections and quite frequently.
We show how to ﬁnd a formula for the nth Fibonacci number
using generating functions. So let F(x) denote the generating func-
tion of this sequence. The main idea is to express the generating
function of a sequence whose kth term equals Fk −Fk−1 −Fk−2 for
all k ≥2. By the relation deﬁning the Fibonacci numbers, this se-
quence must have all terms from the second one on equal to 0. On
the other hand, such a generating function can be constructed from
F(x) using the operations discussed in Section 12.2. In this way, we
get an equation from which we can determine F(x).
Concretely, let us take the function F(x) −xF(x) −x2F(x) cor-
responding to the sequence
(F0, F1 −F0, F2 −F1 −F0, F3 −F2 −F1, F4 −F3 −F2, . . .)
= (0, 1, 0, 0, 0, . . .).
In the language of generating functions, this means (1−x−x2)F(x) =
x, and hence
F(x) =
x
1 −x −x2 .
(12.6)
If we now tried to compute the Taylor series of this function by
diﬀerentiation, we wouldn’t succeed (we would only recover the def-
inition of the Fibonacci numbers). We must bring in one more trick,
which is well known in calculations of integrals by the name decom-
position into partial fractions. In our case, this method guarantees
that the rational function on the right-hand side of (12.6) can be
rewritten in the form
x
1 −x −x2 =
A
x −x1
+
B
x −x2
,

342
Generating functions
where x1, x2 are the roots of the quadratic polynomial 1−x−x2 and
A and B are some suitable constants. For our purposes, a slightly
diﬀerent form will be convenient, namely
x
1 −x −x2 =
a
1 −λ1x +
b
1 −λ2x ,
(12.7)
where λ1 =
1
x1 , λ2 =
1
x2 , a = −A
x1 , and b = −B
x2 . From (12.7), it
is easy to write down a formula for Fn. As the reader is invited to
check, we get Fn = aλn
1 + bλn
2.
We omit the calculation of the roots x1, x2 of the quadratic equa-
tion here, as well as the calculation of the constants a and b, and we
only present the result:
Fn =
1
√
5
01
1 +
√
5
2
2n
−
1
1 −
√
5
2
2n3
.
It is remarkable that this expression full of irrational numbers should
give an integer for each natural number n—but it does.
With approximate numerical values of the constants, the formula
looks as follows:
Fn = (0.4472135 . . .) [(1.6180339 . . .)n −(−0.6180339 . . .)n] .
We see that for large n, the numbers Fn behave roughly as
1
√
5λn
1, and
that the ratio Fn/Fn+1 converges to the limit
1
λ1 = 0.6180339 . . .. This
ratio was highly valued in ancient Greece; it is called the golden section.
A rectangle with side ratio equal to the golden section was considered
the most beautiful and proportional one among all rectangles. (Using
the picture below, you can check whether your aesthetic criteria for rect-
angles agree with those of the ancient Greeks.) If we cut oﬀa square
from such a golden-section rectangle, the sides of the remaining rectan-
gle again determine the golden section:
Similar to the Fibonacci numbers, the golden section is encountered
surprisingly often in mathematics.
Another derivation: a staircase. Consider a staircase with n stairs.
How many ways are there to ascend the staircase, if we can climb by 1
stair or by 2 stairs in each step?

12.3 Fibonacci numbers and the golden section
343
In other words, how many ways are there to write the number n as a
sum of 1s and 2s, or how many solutions are there to the equation
s1 + s2 + · · · + sk = n,
with si ∈{1, 2}, i = 1, 2, . . . , k, k = 0, 1, 2, . . .? If we denote the number
of such solutions by Sn, we get S1 = 1, S2 = 2, and it is not diﬃcult
to see that for n ≥1, we have Sn+2 = Sn+1 + Sn (think it over!). This
implies that Sn is exactly the Fibonacci number Fn−1.
We derive the generating function of the sequence (S0, S1, S2, . . .) in
a diﬀerent way. According to the recipe from Section 12.1, we get that
for a given k, the number of solutions to the equation s1+s2+· · ·+sk = n
with si ∈{1, 2} equals the coeﬃcient of xn in the product (x+x2)k. We
can choose k arbitrarily, however (it is not prescribed by how many steps
we should ascend the staircase), and therefore Sn is the coeﬃcient of xn
in the sum ∞
k=0(x + x2)k. Thus, this sum is the generating function
of the sequence (S0, S1, S2, . . .). The generating function can be further
rewritten: the sum is a geometric series with quotient x + x2 and so it
equals 1/(1 −x −x2). Hence the generating function for the Fibonacci
numbers is x/(1 −x −x2), as we have already derived by a diﬀerent
approach.
Recipes for some recurrences. By the above method, one can ﬁnd
a general form of a sequence (y0, y1, y2, . . .) satisfying the equation
yn+k = ak−1yn+k−1 + ak−2yn+k−2 + · · · + a1yn+1 + a0yn
(12.8)
for all n = 0, 1, 2, . . ., where k is a natural number and a0, a1, . . . , ak−1
are real (or complex) numbers. For instance, for the Fibonacci numbers
we would set k = 2, a0 = a1 = 1. Let us denote the set of all sequences
(y0, y1, y2, . . .) satisfying (12.8) by the symbol Y (so Y depends on k
and on a0, a1, . . . , ak−1). This set contains many sequences in general,
because the ﬁrst k terms of the sequence (y0, y1, y2, . . .) can be chosen
at will (while all the remaining terms are determined by the relation
(12.8)). In the sequel, we will describe what the sequences of Y look
like, but ﬁrst we take a detour into terminology.
A learned name for Eq. (12.8) is a homogeneous linear recurrence of
kth degree with constant coeﬃcients. Let us try to explain the parts of
this complicated name.

344
Generating functions
• A recurrence or recurrent relation is used as a general notion
denoting a relation (usually a formula) expressing the nth term
of a sequence via several previous terms of the sequence.3
• Homogeneous appears in the name since whenever (y0, y1, y2, . . .) ∈
Y then also (αy0, αy1, αy2, . . .) ∈Y for any real number α. (In
mathematics, “homogeneity” usually means “invariance to scal-
ing”.) On the other hand, an example of an inhomogeneous recur-
rence is yn+1 = yn + 1.
• The word linear here means that the values of yj always appear in
the ﬁrst power in the recurrence and are not multiplied together.
A nonlinear recurrence is, for example, yn+2 = yn+1yn.
• Finally, the phrase with constant coeﬃcients expresses that a0,
a1,. . . , ak−1 are ﬁxed numbers independent of n. One could also
consider a recurrence like yn+1 = (n −1)yn, where the coeﬃcient
on the right-hand side is a function of n.
So much for the long name. Now we are going to formulate a general
result about solutions of the recurrent relation of the considered type.
We deﬁne the characteristic polynomial of the recurrence (12.8) as the
polynomial
p(x) = xk −ak−1xk−1 −ak−2xk−2 −· · · −a1x −a0.
For example, the characteristic polynomial of the recurrence relation for
the Fibonacci numbers is x2 −x −1. Let us recall that any polynomial
of degree k with coeﬃcient 1 at xk can be written in the form
(x −λ1)(x −λ2) . . . (x −λk),
where λ1, . . . , λk are (generally complex) numbers called the roots of
the given polynomial.
12.3.1 Proposition. Let p(x) be the characteristic polynomial of the
homogeneous linear recurrence (12.8).
(i) (Simple roots)
Suppose that p(x) has k pairwise distinct roots
λ1, . . . , λk. Then for any sequence y = (y0, y1, . . .) ∈Y satisfying
(12.8), complex constants C1, C2, . . . , Ck exist such that for all n,
we have
yn = C1λn
1 + C2λn
2 + · · · + Ckλn
k.
(ii) (The general case)
Let λ1, . . . , λq be pairwise diﬀerent complex
numbers, and let k1, . . . , kq be natural numbers with k1 +k2 +· · ·+
3Another name sometimes used with this meaning is diﬀerence equation. This
is related to the so-called diﬀerence of a function, which is a notion somewhat
similar to derivative. The theory of diﬀerence equations is often analogous to the
theory of diﬀerential equations.

12.3 Fibonacci numbers and the golden section
345
kq = k such that
p(x) = (x −λ1)k1(x −λ2)k2 . . . (x −λq)kq.
Then for any sequence y = (y0, y1, . . .) ∈Y satisfying (12.8), com-
plex constants Cij exist (i = 1, 2, . . . , q, j = 0, 1, . . . , ki −1) such
that for all n, we have
yn =
q

i=1
ki−1

j=0
Cij
n
j

λn
i .
How do we solve a recurrence of the form (12.8) using this propo-
sition? Let us give two brief examples. For the recurrence relation
yn+2 = 5yn+1−6yn, the characteristic polynomial is p(x) = x2−5x+6 =
(x −2)(x −3). Its roots are λ1 = 2 and λ2 = 3, and Proposition 12.3.1
tells us that we should look for a solution of the form C12n + C23n.
Being given some initial conditions, say y0 = 2 and y1 = 5, we must de-
termine the constants C1, C2 in such a way that the formula gives these
required values for n = 0, 1. In our case, we would set C1 = C2 = 1.
And one more example, a quite artiﬁcial one, with multiple roots:
the equation yn+5 = 8yn+4 + 25yn+3 −38yn+2 + 28yn+1 −8yn has
characteristic polynomial4 p(x) = (x −1)2(x −2)3, and so Proposi-
tion 12.3.1 says that the solution should be sought in the form yn =
C10 + C11n + C202n + C21n2n + C22
n
2

2n. The values of the constants
should again be calculated according to the values of the ﬁrst 5 terms
of the sequence (y0, y1, y2, . . .).
The procedure just demonstrated for solving the recurrence (12.8)
can be found in all kinds of textbooks, and the generating functions are
seldom mentioned in this connection. Indeed, Proposition 12.3.1 can
be proved in a quite elegant manner using linear algebra (as indicated
in Exercise 16), and once we know from somewhere that the solution
should be of the indicated form, generating functions become unneces-
sary. However, the method explained above for Fibonacci numbers really
ﬁnds the correct form of the solution. Moreover, it is a nice example of
an application of generating functions, and a similar approach can some-
times be applied successfully for recurrences of other types, where no
general solution method is known (or where it is diﬃcult to ﬁnd such a
method in the literature, which is about the same in practice).
4Of course, the authors have selected the coeﬃcients so that the character-
istic polynomial comes out very nicely. The instructions given above for solving
homogeneous linear recurrence relations with constant coeﬃcients (as well as the
method with generating functions) leave aside the question of ﬁnding the roots of
the characteristic polynomial. In examples in various textbooks, the recurrences
mostly have degree 1 or 2, or the coeﬃcients are chosen in such a way that the
roots are small integers.

346
Generating functions
Exercises
1. ∗Determine the number of n-term sequences of 0s and 1s containing
no two consecutive 0s.
2. Prove that any natural number n ∈N can be written as a sum of
mutually distinct Fibonacci numbers.
3. Express the nth term of the sequences given by the following recurrence
relations (generalize the method used for the Fibonacci numbers, or
use Proposition 12.3.1).
(a) a0 = 2, a1 = 3, an+2 = 3an −2an+1 (n = 0, 1, 2, . . .),
(b) a0 = 0, a1 = 1, an+2 = 4an+1 −4an (n = 0, 1, 2, . . .),
(c) a0 = 1, an+1 = 2an + 3 (n = 0, 1, 2, . . .).
4. In a sequence (a0, a1, a2, . . .), each term except for the ﬁrst two is the
arithmetic mean of the preceding two terms, i.e. an+2 = (an+1+an)/2.
Determine the limit limn→∞an (as a function of a0, a1).
5. Solve the recurrence relation an+2 = √an+1an with initial conditions
a0 = 2, a1 = 8, and ﬁnd limn→∞an.
6. (a) Solve the recurrence an = an−1 + an−2 + · · · + a1 + a0 with the
initial condition a0 = 1.
(b) ∗Solve the recurrence an = an−1+an−3+an−4+an−5+· · ·+a1+a0
(n ≥3) with a0 = a1 = a2 = 1.
7. ∗Express the sum
Sn =
2n
0

+ 2
2n −1
1

+ 22
2n −2
2

+ · · · + 2n
n
n

as a coeﬃcient of x2n in a suitable power series, and ﬁnd a simple
formula for Sn.
8. Calculate ⌊n/2⌋
k=0
n−k
k

(−4)−k.
9. ∗Show that the number 1
2

(1 +
√
2)n + (1 −
√
2)n 
is an integer for all
n ≥1.
10. ∗Show that the number (6 +
√
37)999 has at least 999 zeros following
the decimal point.
11. ∗Show that for any n ≥1, the number (
√
2−1)n can be written as the
diﬀerence of the square roots of two consecutive integers.
12. ∗Find the number of n-term sequences consisting of letters a, b, c, d
such that a is never adjacent to b.

12.3 Fibonacci numbers and the golden section
347
13. (“Mini-Tetris”) How many ways are there to ﬁll completely without
overlap an n×2 rectangle with pieces of the following types? The sides
of the pieces are 1 and 2; the pieces can be rotated by a multiple of
the right angle. In (b) and (c), it suﬃces to calculate the generating
function or reduce the problem to solving a recurrence of the form
(12.8), and determine the order of growth (an asymptotic estimate for
large n).
(a)
,
(b) ∗
,
(c) ∗∗
.
14. CS This exercise is related to the theory of formal languages. We give
the necessary deﬁnitions but to appreciate the context, the reader
should consult a textbook on automata and formal languages.
Let Σ be some ﬁxed ﬁnite alphabet (such as Σ = {a, b, c}). A word
over Σ is a ﬁnite sequence of letters (such as babbaacccba). The empty
word having no letters is denoted by ε. A language over Σ is a set of
words over Σ. If u and v are words then uv denotes the concatenation
of u and v, i.e. we write ﬁrst u and then v. The generating function of
a language L is the generating function of the sequence n0, n1, n2, . . .,
where ni is the number of words of length i in L.
Let us say that a language L is very regular if it can be obtained by
ﬁnitely many applications of the following rules:
1. The languages ∅and {ε} are very regular, and the language {ℓ}
is very regular for every letter ℓ∈Σ.
2. If L1, L2 are very regular languages, then also the language L1.L2
is very regular, where L1.L2 = {uv: u ∈L1, v ∈L2}.
3. If L is a very regular language then also L∗is very regular, where
L∗= {ε} ∪L ∪L.L ∪L.L.L ∪. . ..
4. If L1, L2 are very regular languages with L1 ∩L2 = ∅, then also
L1 ∪L2 is very regular.
(a) Show that the following languages over Σ = {a, b} are all very
regular: the language consisting of all words of odd length, the language
consisting of all words beginning with aab and having an even number
of letters a, and ∗the language consisting of all words having no two
consecutive letters a.
(b) ∗Show that the generating function of any very regular language
is a rational function (a polynomial divided by a polynomial), and
describe how to calculate it.
(c) ∗∗Show that if L1, L2 are very regular languages (not necessarily
disjoint ones) then also L1 ∪L2 is very regular. (Therefore, regular
languages have rational generating functions.)

348
Generating functions
(d) (A programming project) Look up information on regular languages
and their relation to ﬁnite automata. Write a program that accepts a
speciﬁcation of a regular language (by a regular expression or by a
nondeterministic ﬁnite automaton) and computes a formula for the
number of words of length n in the language.
15. ∗Prove Proposition 12.3.1 by generalizing the method shown for the
Fibonacci numbers (use a general theorem on decomposing a rational
function into partial fractions from integral calculus).
16. Prove Proposition 12.3.1 directly, using linear algebra, according to
the following hints.
(a) Check that the set Y is a vector space with respect to compo-
nentwise addition of sequences and componentwise multiplication by
a complex number.
(b) Show that the dimension of Y equals k.
(c) ∗Show that in the situation of part (i) of Proposition 12.3.1, the
sequences (λ0
i , λ1
i , λ2
i , . . .) belong to Y (where i = 1, 2, . . . , k), and they
are all linearly independent in Y (hence they form a basis of the vector
space Y by (b)). This proves part (i).
(d) Verify that in the situation of part (ii) of Proposition 12.3.1, each
of the sequences (
n
j

λn
i )∞
n=0 belongs to the solution set Y.
(e) ∗∗Prove that the sequences considered in part (d) are linearly
independent in Y, i.e. they form a basis.
12.4
Binary trees
We are going to consider the so-called binary trees, which are often
used in data structures. Figure 12.1 depicts several diﬀerent binary
trees with 5 vertices. For our purposes, a binary tree can concisely be
deﬁned as follows: a binary tree either is empty (it has no vertex), or
consists of one distinguished vertex called the root, plus an ordered
pair of binary trees called the left subtree and right subtree.5
Let bn denote the number of binary trees with n vertices. Our
goal is to ﬁnd a formula for bn. By listing all small binary trees, we
ﬁnd that b0 = 1, b1 = 1, b2 = 2, and b3 = 5. This can serve for
checking the result obtained below.
As usual, we let b(x) = b0 +b1x+b2x2 +· · · be the corresponding
generating function. For n ≥1, the number of binary trees with n
5This is a deﬁnition by induction. First we say what a binary tree with 0
vertices is and then we deﬁne a binary tree with n vertices using the already
deﬁned binary trees with fewer vertices. By admitting the empty binary tree, we
avoid dealing with various special cases, such as when the root has only a left
subtree or only a right subtree.

12.4 Binary trees
349
t
\
\
t

t
t
J
J
t
t
\
\
t

t
t
T
T
t
t
\
\
t

t

t
J
J
t
t
\
\
t

t
t
T
T
t
t
\
\
t

t

t
T
T
t
Fig. 12.1 Several diﬀerent binary trees on 5 vertices.
vertices equals the number of ordered pairs of the form (B, B′), where
B and B′ are binary trees having together n −1 vertices. That is, if
B has k vertices then B′ has n −k −1 vertices, k = 0, 1, . . . , n −1.
Therefore, the number of such ordered pairs is
bn = b0bn−1 + b1bn−2 + · · · + bn−1b0.
(12.9)
Comparing this with the deﬁnition of multiplication of power series,
we see that bn is exactly the coeﬃcient of xn−1 in the product
b(x) · b(x) = b(x)2. (This is the natural example for the combina-
torial meaning of multiplication of generating functions promised in
Section 12.2.) Hence bn is the coeﬃcient of xn in the function xb(x)2.
And so xb(x)2 is the generating function of the same sequence as
b(x), except that b(x) has constant term b0 = 1 while the power
series xb(x)2 has constant term 0; this is because the formula (12.9)
is only correct for n ≥1. We can write the following equality of
generating functions:
b(x) = 1 + xb(x)2.
Suppose that x is such a real number that the power series b(x)
converges. Then b(x) is also a real number, and it must satisfy the
quadratic equation b(x) = 1 + xb(x)2. By a well-known formula for
the roots of a quadratic equation, b(x) must be one of the numbers
1 + √1 −4x
2x
or 1 −√1 −4x
2x
.
This seems as if there are two possible solutions. But we know that
the sequence (b0, b1, b2, . . .), and thus also its generating function,
are determined uniquely. Since b(x) is continuous as a function of x
(whenever it converges), we must take either the ﬁrst solution (the

350
Generating functions
one with “+”) for all x, or the second solution (the one with “−”)
for all x. If we look at the ﬁrst solution, we ﬁnd that for x tending
to 0 it goes to ∞, while the generating function b(x) must approach
b0 = 1. So whenever b(x) converges, it must converge to the second
solution b(x) = (1 −√1 −4x)/2x.
It remains to calculate the coeﬃcients of this generating function.
To this end, we make use of the generalized binomial theorem 12.2.3.
This theorem gives us the expansion
√
1 −4x =
∞

k=0
(−4)k
1/2
k

xk.
The coeﬃcient of x0 is 1, and hence the power series 1 −√1 −4x
has zero constant term. We can thus divide it by 2x (by shifting to
the left by one position and dividing all coeﬃcients by 2). From this
we get that for all n ≥1,
bn = −1
2(−4)n+1
 1/2
n + 1

.
(12.10)
By further manipulations, which we leave as an exercise, one can
obtain a nicer form:
bn =
1
n + 1
2n
n

.
The numbers bn thus deﬁned are known by the name Catalan num-
bers in the literature (and they are named after Eug`ene Charles Catalan
rather than after Catalonia).
Besides giving the number of binary trees, they have many other
combinatorial meanings. Some of them are indicated in the exercises
below.
More about methods for counting various types of graphs, trees,
etc., can be found, for example, in the book by Harary and Palmer [21].
Publicly available computer software which can solve many such tasks
automatically is described by Flajolet, Salavy, and Zimmermann [36],
together with the underlying theory.
Exercises
1. Convert the expression on the right-hand side of (12.10) to the form
given below that equation.
2. ∗As the reader might have noticed, we have left out a discussion of
convergence of the series b(x). Prove that b(x) converges for some
number x ̸= 0 (but do not use the above-derived formula for bn since
this was obtained under the assumption that b(x) converges).

12.4 Binary trees
351
3. Consider an n × n chessboard:
A
B
Consider the shortest paths from the corner A to the corner B following
the edges of the squares (each of them consists of 2n edges).
(a) How many such paths are there?
(b) ∗Show that the number of paths that never go below the diagonal
(the line AB) is exactly bn, i.e. the Catalan number. One such path is
drawn in the ﬁgure.
(c) ∗∗Give an elementary proof of the formula bn =
1
n+1
2n
n

, without
generating functions.
4. Consider a product of 4 numbers, abcd. It can be “parenthesized” in
5 ways: ((ab)c)d, (a(bc))d, (ab)(cd), a((bc)d), and a(b(cd)). Prove that
the number of such parenthesizings of a product of n numbers is the
Catalan number bn−1.
5. There are 2n people standing in a queue for tickets costing 5 double-
zons each. Everyone wants to buy one ticket; n of the people have a
10-doublezon banknote and n of them a 5-doublezon coin. Initially, the
cashier has no money.
(a) How many orderings of the people in the queue are there for
which the cashier can always give a 5-doublezon coin back to each
10-doublezon note owner?
(b) What is the probability that the cashier will be able to return cash
to everyone for a random ordering of the queue?
6. ∗Consider a regular n-gon. Divide it into triangles by drawing n −3
nonintersecting diagonals (a diagonal is a segment connecting two ver-
tices). Show that the number of such triangulations is bn−2.
7. There are 2n points marked on a circle. We want to divide them into
pairs and connect the points in each pair by a segment (chord) in such
a way that these segments do not intersect. Show that the number of
ways to do this is bn.
8. In this exercise and several subsequent ones, we are going to use the
terms rooted tree and planted tree introduced in Section 5.2 (both are

352
Generating functions
trees with root; for planted trees the left-to-right order of sons of each
vertex is important, for rooted trees it isn’t).
(a) Let cn denote the number of (nonisomorphic) planted trees with
n vertices, where each vertex either is an end-vertex or has exactly 2
sons (examples of such nonisomorphic trees are the ﬁrst and the third
trees in Fig. 12.1). Show that the corresponding generating function
c(x) satisﬁes the equation c(x) = x + xc(x)2, and calculate a formula
for cn.
(b) ∗Can you derive the value of cn from the knowledge of bn (the
number of binary trees)?
(c) Find dn, the number of nonisomorphic planted trees with n vertices,
where each vertex has 0, 1, or 2 sons. (Note that this is something
diﬀerent from binary trees!)
9. Let tn denote the number of nonisomorphic planted trees with n
vertices.
(a) Show that the corresponding generating function t(x) satisﬁes
t(x) =
x
1−t(x) and calculate a formula for tn.
(b) ∗Can you derive the value of tn from the knowledge of bn?
10. We say that a planted tree is old if it has no young vertex, where a
young vertex is a leaf adjacent to the root. Let sn be the number of
old planted trees with n vertices. Express the generating function s(x)
using t(x) from the previous exercise.
11. ∗Now let us consider rooted trees where each nonleaf vertex has exactly
2 sons (for rooted trees, the left-to-right order of sons is immaterial,
so the ﬁrst and third trees in Fig. 12.1 are now isomorphic). Let ¯b(x)
be the corresponding generating function. Derive the equation
¯b(x) = 1 + x
2
¯b(x)2 + ¯b(x2)

.
12. Consider alkane radicals; these are acyclic hydrocarbons where each
carbon atom has 4 single bonds, except for one that has 3 single bonds
and 1 “free” bond. Such molecules can be imagined as rooted trees.
The vertices are the carbon atoms and each vertex has 0, 1, 2, or 3
sons. The order of sons of a vertex is not important. Let rn be the
number of such trees with n vertices (i.e. of alkane radicals with n
carbon atoms), and let r(x) be the generating function.
(a) ∗∗Derive the equation r(x) = 1 + x
6

r(x)3 + 3r(x2)r(x) + 2r(x3)

.
(b) Using this equation, calculate a table of values of rn for small
values of n. Compare to values given in some chemistry handbook.
(c) CS Write a program to do the calculation in (b). A more ambitious
project is a program that calculates coeﬃcients of generating functions
given by equations of various forms.

12.5 On rolling the dice
353
12.5
On rolling the dice
Problem. We roll a die until a 6 appears for the ﬁrst time. How
many times do we need to roll, on the average?6
The probability of getting a 6 in the ﬁrst round is p = 1
6. The
probability of a 6 not appearing in the ﬁrst round and showing up
in the second round is (1 −p)p, and in general, the probability of a
6 appearing in the ith round for the ﬁrst time is qi = (1 −p)i−1p.
The average number of rounds (i.e. the expectation) is then
S =
∞

i=0
iqi =
∞

i=1
i(1 −p)i−1p.
To sum this series, we introduce the generating function q(x) =
q1x + q2x2 + · · · . By diﬀerentiating this series term by term, we get
q′(x) = 1 · q1 + 2 · q2x + 3 · q3x2 + · · · , and hence the desired S equals
the value of q′(1).
With some eﬀort, we calculate that our generating function is
given by the expression
q(x) =
p
1 −p ·
1
1 −(1 −p)x −
p
1 −p .
A small exercise in diﬀerentiation gives q′(x) = p/(1 −(1 −p)x)2,
and hence S = q′(1) = 1
p. In our case, for p = 1
6, the average number
of rounds is thus 6.
Here is a way of reasoning giving a much shorter solution. In any
case, we roll the die at least once. With probability 1 −p, we do not
roll 6 in the ﬁrst round, and then still S more rounds await us on the
average (the die has no memory, and so the situation after the ﬁrst
unsuccessful attempt is the same as if nothing happened). Therefore
S = 1 + (1 −p)S,
and S = 1/p results immediately.
The method with generating functions shown in this section has
many applications in probability theory. If X is some random vari-
able attaining value i with probability qi, i = 0, 1, 2, . . ., and if q(x) =
∞
i=0 qixi is the generating function, then the expectation of X is q′(1).
6After some consideration, we decided on this formulation of the problem, as
opposed to another one which oﬀers itself, involving so-called Russian roulette.

354
Generating functions
Exercises
1. Using generating functions as we did above, calculate
(a) the average number of 6s obtained by rolling a die n times,
(b) ∗the average value of the expression (X −6)2, where X is the
number of times a die must be rolled until the ﬁrst 6 appears (this is
a certain measure of the “typical deviation” from the average number
of rounds; in probability theory, it is called the variance).
(c) Let X be some random variable attaining values 0, 1, 2, . . ., where
i is attained with probability qi, and let q(x) be the generating func-
tion of the qi. Express the variance of X, i.e. the quantity Var [X] =
E

(X −E [X])2 
, using the derivatives of q(x) at suitable points.
12.6
Random walk
Imagine the real axis drawn in the plane with integers marked by
circles. A frog leaps among these circles according to the following
rules of random walk:
• Initially (before the ﬁrst move) the frog sits at number 1.
• In each move, the frog leaps either by two circles to the right
(from i to i + 2) or by one circle to the left (from i to i −1). It
decides on one of these possibilities at random, and both possi-
bilities have the same probability (as if it tossed a fair coin and
decided according to the result heads/tails).
Problem. What is the probability that the frog ever reaches the
number 0?
First, we have to clarify what we mean by such a probability. It
is easy to deﬁne the probability that the frog reaches 0 within the
ﬁrst 7 leaps, say (let us denote this probability by P7). The ﬁrst 7
leaps give 27 diﬀerent possible trajectories, since in each leap, the
frog makes a decision between two possibilities, and these decisions
can be combined arbitrarily. By the rules in our problem, all these
trajectories have the same probability. The above-deﬁned probability
P7 equals the number of trajectories that pass through 0 (there are
75 of them) divided by the total number of trajectories, i.e. by 27.
The probability P required in the problem can be deﬁned as the
limit P = limi→∞Pi, where the deﬁnition of Pi has been explained
above for i = 7. This limit certainly exists because, clearly, P1 ≤
P2 ≤. . ..

12.6 Random walk
355
Let ai denote the number of trajectories of the ﬁrst i leaps such
that the frog reaches 0 by the ith leap and never before. We thus
have
P =
∞

i=1
ai
2i .
If we introduce the generating function a(x) = a1x+a2x2+a3x3+· · · ,
we get P = a(1
2).
For solving the problem, it will be useful to look also at trajecto-
ries starting at numbers other than 1 (but continuing according to
the same rule). For instance, what is the number bi of trajectories
starting at the number 2 that ﬁrst reach 0 by the ith leap? In order
that such a trajectory reaches 0, it has to reach 1 ﬁrst. Let j be the
number of the leap by which 1 is ﬁrst reached. If j is determined,
there are aj possibilities to avoid 1 in leaps 1, 2, . . . , j −1 and reach
it by the jth leap. Then, i−j leaps remain to move from 1 to 0, and
the number of ways for these i −j leaps is ai−j. So for a given j,
there are ajai−j possibilities, and altogether we get7
bi =
i−1

j=1
ajai−j.
In the language of generating functions this means b(x) = a(x)2.
Analogously, let ci be the number of trajectories starting at the
number 3 and ﬁrst reaching 0 by the ith leap. As before, we can see
that c(x) = a(x)b(x) = a(x)3.
Let us investigate trajectories starting at 1 from a diﬀerent point
of view. By the ﬁrst move, either the frog reaches 0 directly (which
gives a1 = 1), or it leaps to the number 3. In the latter case, it has
ci−1 possibilities of reaching 0 for the ﬁrst time after the next i −1
leaps. Hence for i > 1, we have ai = ci−1. Converted to a relation
between the generating functions, this reads
a(x) = x + xc(x) = x + xa(x)3.
(12.11)
7Note that we use the fact that leaps to the left are always by 1, and so the
frog cannot reach 0 from 2 without leaping to 1 ﬁrst.

356
Generating functions
1
0.5
–0.5
–1
–2
1
–1
x
a
x = a/(a3+ 1)
Fig. 12.2 The function x = a/(a3 + 1).
In particular, for x = 1
2, the following equation results (where we
write P = a(1
2)):
P = 1
2 + 1
2 P 3.
This has three solutions: 1, 1
2(
√
5−1), and −1
2(
√
5+1). The negative
root can be excluded right away. The right value is given by the
second root, P = 1
2(
√
5 −1) = 0.618033988 . . . (the golden section
again!).
Why can’t 1 be the answer? Here is one possible argument. Since
the series a(x) converges for x = 1
2 and since all the ai are nonnegative,
a(x) is a nondecreasing continuous function of x on the interval (0, 1
2].
At the same time, a(x) has to be the root of the equation (12.11), or
in other words, we must have x = a(x)/(a(x)3 + 1) for all such x. If
we plot the function a →a/(a3 + 1) as in Fig. ??, we see that a( 1
2)
cannot be given by the rightmost intersection of the x = 1
2 line with
the plotted curve since then we would have a(x) > 1 for all x slightly
smaller than 1
2. This can be made rigorous by using some elementary
calculus (computing the tangent to the curve at the point (1, 1)) but
we hope the picture is convincing enough.
From Eq. (12.11) we could, in principle, calculate the function a(x),
and then try to express the numbers ai using the Taylor series, say
(which is quite laborious). A charming feature of the presented solution
is that we don’t need to do anything like that.

12.7 Integer partitions
357
Exercises
1. Consider a random walk where we start at the number 0 and in each
step we move from i to i + 1 or to i −1 with equal probability.
(a) ∗Prove that we eventually return to 0 with probability 1.
(b) Prove that each number k is visited at least once with probability 1.
2. Consider a random walk as in the preceding exercise.
(a) ∗Let Sn denote the expected number of steps needed to reach n
(from the preceding exercise, we know that n will be reached sooner or
later with probability 1). What is wrong with the following argument?
We claim that Sn = cn for some constant c. This is true for n = 0,
so let n > 0. On the average, we need S1 steps to reach 1, and then
Sn−1 more steps to reach n starting from 1. Hence Sn = S1 + Sn−1 =
c + c(n −1) = cn, where c = S1.
(b) ∗∗What is the expected number of steps needed to get at least n
steps away from 0 (i.e. to reach n or −n)?
12.7
Integer partitions
How many ways are there to write a natural number n as a sum
of several natural numbers? The answer is not too diﬃcult if we
count ordered partitions of n; that is, if we regard the expressions
3 = 2 + 1 and 3 = 1 + 2 as two diﬀerent ways of expressing 3 as
a sum (Exercise 1). The problem becomes much harder and more
interesting if we consider identical expressions diﬀering only by the
order of addends (in this case we will simply speak of partitions of
n throughout this section). For instance, for n = 5, all the possible
partitions are 5 = 1 + 1 + 1 + 1 + 1, 5 = 1 + 1 + 1 + 2, 5 = 1 + 2 + 2,
5 = 1 + 1 + 3, 5 = 2 + 3, 5 = 1 + 4, and 5 = 5. Let pn stand for the
number of partitions of n in this sense.
To make the way of writing the partitions unique, we can for instance
insist that the addends be written in a nondecreasing order, as we wrote
them in the listing of the partitions of 5. So another formulation of
the question is: how many ways are there to build a “nondecreasing
wall” using n bricks, as in the following picture (corresponding to the
partitions 10 = 1 + 1 + 2 + 2 + 4 and 10 = 1 + 1 + 2 + 6)?

358
Generating functions
(Such a picture is called the Ferrers diagram of a given partition.)
The deﬁnition of pn looks quite simple, but, perhaps surprisingly,
there is no “simple” formula for pn (like a binomial coeﬃcient, say).
The problem of estimating pn has been considered in number theory
and it was solved with an almost unbelievable accuracy by Hardy and
Ramanujan in 1918. (The history is described in Littlewood’s book
[23], which can be recommended as great reading about mathemat-
ics.) In fact, Hardy and Ramanujan discovered an amazing exact (and
complicated) formula for pn. Currently, proving or even understanding
these precise results requires knowledge of quite a deep theory (see e.g.
Andrews [32] for a presentation). Here we will only scratch the surface
of this problem; reasonably good asymptotic estimates for pn can be
derived in a simple but clever way using generating functions, and this
is what we will demonstrate.
We know how to express the number of solutions to an equation
of the form
i1 + i2 + · · · + ik = n,
with the values of each ij lying in some prescribed set, as the coef-
ﬁcient of xn in a suitable expression. Here the order of the ij does
matter, though, so it is not obvious how to relate this to the (un-
ordered) partitions of n. The trick is to let ij express the contribution
of the addends equal to j in a partition of n, or, in other words, the
number of bricks in columns of height exactly j in our nondecreasing
wall. Therefore, the partitions of n are in a bijective correspondence
with the solutions of the equation
i1 + i2 + · · · + in = n
with
i1 ∈{0, 1, 2, 3, . . .},
i2 ∈{0, 2, 4, 6, . . .}, . . . ,
ij ∈{0, j, 2j, 3j, . . .}.
For example, the partition 5 = 1 + 2 + 2 would correspond to the
solution i1 = 1, i2 = 4, i3 = i4 = i5 = 0.
The next step is standard. From the new formulation we
immediately get that pn is the coeﬃcient of xn in the product
Pn(x) = (1+x+x2+· · · )(1+x2+x4+x6+· · · ) . . . (1+xn+x2n+· · · )
=
n

k=1
1
1 −xk .

12.7 Integer partitions
359
This ﬁnite product is not the generating function of the sequence
(pn)∞
n=0. To make it the actual generating function (having the coef-
ﬁcient pn at xn for all n simultaneously), we have to use the inﬁnite
product
P(x) =
∞

k=1
1
1 −xk .
But since an inﬁnite product could perhaps look more frightening we
stick to the ﬁnite product Pn(x) in estimating pn.
Now we are going to upper-bound pn using the method already
demonstrated in the proof of Theorem 3.6.1 (estimating binomial
coeﬃcients). For all numbers x ∈(0, 1), we have
pn ≤1
xn Pn(x) = 1
xn
n

k=1
1
1 −xk .
We want to choose x in such a way that the right-hand side is as small
as possible. A suitable way of estimating Pn(x) is another clever part
of the proof. First, for dealing with a product, it is often advisable
to consider the logarithm; in our case this leads to
ln pn ≤ln
 1
xn Pn(x)

= −n ln x −
n

k=1
ln(1 −xk).
We recall the power series (12.3) for the logarithm:
−ln(1 −y) = y
1 + y2
2 + y3
3 + y4
4 + · · ·
for all y ∈(−1, 1). Therefore, we can write
−
n

k=1
ln(1 −xk) =
n

k=1
∞

j=1
xkj
j
=
∞

j=1
1
j
n

k=1
xjk
≤
∞

j=1
1
j
∞

k=1
xjk =
∞

j=1
1
j
xj
1 −xj .
Now, perhaps unexpectedly, we use the formula for the sum of a
geometric progression in a “reverse direction”. We have
1 −xj = (1 −x)(1 + x + x2 + · · · + xj−1) ≥(1 −x)jxj−1
(recall that 0 < x < 1) and so
∞

j=1
1
j
xj
1 −xj ≤
∞

j=1
1
j
xj
(1 −x)jxj−1 =
x
1 −x
∞

j=1
1
j2 .
Next, we need the following remarkable

360
Generating functions
12.7.1 Fact.
∞

j=1
1
j2 = π2
6 .
As a mathematical dessert, we reproduce a proof (going back to
Euler) of this fact at the end of this section. Continuing with our
estimates for ln pn, we get
ln pn ≤−n ln x + π2
6
x
1 −x .
For the subsequent calculation, it is convenient to introduce a new
variable u = x/(1 −x) (hence u can be any number in [0, ∞), and
x = u/(1 + u)). After this substitution, and using the inequality
ln(1 + 1
u) ≤1
u (which is readily derived from Fact 3.5.4), we get
ln pn < n ln

1 + 1
u

+ π2
6 u ≤n
u + π2
6 u.
By substituting u =
√
6n/π into the last expression, we arrive at
ln pn ≤π
4
2
3 n. (Why is u =
√
6n/π the right value to use? A little
calculus shows that this is the value where the considered upper
bound for ln pn attains its minimum as a function of u.) Hence we
have proved the following:
12.7.2 Theorem. For all n ≥1, we have
pn < eπ

2
3 n = e(2.5650...)√n.
A lower bound.
How good is the upper bound just derived? The
results of Hardy and Ramanujan imply that pn ∼
1
4
√
3 neπ√
2
3 n, and so
the upper bound we have derived is quite good—it even has the correct
constant in the exponent. But what if we are in a wilderness (in a broad
sense, meaning cut oﬀfrom mathematical libraries) and want to know
whether the bound in Theorem 12.7.2 is roughly correct? Here is a quick
way of deriving a weaker lower bound which at least tells us we are not
too far oﬀthe mark.
As the reader is invited to calculate in Exercise 1, the number of
ordered partitions on n into k addends is
n−1
k−1

. Since every (unordered)
partition with k addends gives rise to at most k! ordered partitions, we
have
pn ≥
n−1
k−1

k!
≥(n −1)(n −2) . . . (n −k + 1)
(k!)2
for any k ∈{1, 2, . . . , n}. How large should k be to give the best lower
bound? If we increase k by 1, the denominator is multiplied by the factor

12.7 Integer partitions
361
(k+1)2 and the numerator by n−k. Hence if (k+1)2 < n−k then k+1
is better than k, and for the best k, (k + 1)2 should be approximately
equal to n −k. To keep the expressions simple, we set k = ⌊√n⌋. The
intuition is that then (n −1)(n −2) . . . (n −k + 1) behaves roughly like
n
√n. Here is a rigorous calculation supporting this intuition. We have
(n −1)(n −2) . . . (n −k + 1) ≥(n −k)k−1 = nk−1

1 −k
n
k−1
,
and since k
n = ⌊√n⌋/n ≤1/⌊√n⌋= 1
k and (1−1
k)k−1 > e−1, we further
get
nk−1

1 −k
n
k−1
≥nk−1

1 −1
k
k−1
≥nk
en .
By Theorem 3.5.5, we upper-bound k! ≤ek(k/e)k, and so we get
pn ≥
 n
k2
k e2k−3
nk2
≥e2k−3
n2
≥
1
e5n2 e2√n.
If n is large enough then n2 is much smaller than e
√n, and hence
n−2e2√n = Ω(e
√n). So we have shown that for n large enough, pn
lies between ec1
√n and ec2
√n, where c2 > c1 > 0 are suitable constants.
Such information about the order of growth of a function may often be
suﬃcient.
Proof of Fact 12.7.1.
We begin with the de Moivre formula (see
also Exercise 1.3.4): (cos α + i sin α)n = cos(nα) + i sin(nα), where i
is the imaginary unit, i2 = −1. Expanding the left-hand side by the
binomial theorem and considering only the imaginary part of both sides,
we get the identity
n
1

sin α cosn−1 α −
n
3

sin3 α cosn−3 α +
n
5

sin5 α cosn−5 α −· · ·
= sin(nα).
Using the function cot α = cos α
sin α , we can rewrite the left-hand side as
sinn α
n
1

cotn−1 α −
n
3

cotn−3 α +
n
5

cotn−5 α −· · ·

.
From now on, let n = 2m + 1 be odd. Then the expression in brackets
can be written as P(cot2 α), where P(x) is the polynomial
n
1

xm −
n
3

xm−1 +
n
5

xm−2 −· · · .
We claim that the roots of P(x) are the m numbers r1, r2, . . . , rm,
where rk = cot2 kπ
n . Indeed, for α = kπ
n , sin α is nonzero while sin(nα)

362
Generating functions
= 0, and so P(cot2 α) must be 0. Because r1, . . . , rm are all diﬀerent
and P(x) has degree m, this exhausts all the roots. Therefore,
P(x) = n(x −r1)(x −r2) . . . (x −rm).
By comparing the coeﬃcients of xm−1 on both sides of the last equality,
we ﬁnd that n(r1 + r2 + · · · + rm) =
n
3

. In this way, we have derived
the identity
m

k=1
cot2
kπ
2m + 1 = m(2m −1)
3
.
(12.12)
For 0 < α < π
2 , we have cot α = 1/ tan α < 1/α, and so (12.12) provides
the inequality
m

k=1
(2m + 1)2
π2k2
> m(2m −1)
3
or
m

k=1
1
k2 > π2
6
2m(2m −1)
(2m + 1)2
.
Letting m →∞gives ∞
k=1 1/k2 ≥π2/6. An upper bound can be
derived similarly. Using the identity cot2 α = sin−2 α −1 and the in-
equality sin α < α valid for 0 < α < π
2 , (12.12) gives
m(2m −1)
3
−m =
m

k=1
1
sin2
kπ
2m+1
>
m

k=1
(2m + 1)2
π2k2
,
which for m →∞leads to ∞
k=1 1/k2 ≤π2/6.
2
Exercises
1. (a) Check that the number of ordered partitions of n with k summands,
i.e. the number of solutions to i1 + i2 + · · · + ik = n with all ij ≥1, is
n−1
k−1

.
(b) Calculate the number of all ordered partitions of n. For instance,
for n = 3 we have the 4 ordered partitions 3 = 1 + 1 + 1, 3 = 1 + 2,
3 = 2 + 1, and 3 = 3. Give three solutions: a “direct” combinatorial
one, one based on (a), and one using generating functions.
2. (a) CS Write a program to list all the partitions of n, each exactly
once. (This is a nice programming exercise.)
(b) CS Write a program to compute pn for a given n. If you don’t want
to use multiple precision arithmetic, the program should at least be
able to calculate pn for n up to 5000 (where pn is well below 10100) to 8
signiﬁcant digits, say. A contest can be organized for the fastest correct
program. (A consultation of the literature can give a great advantage
in such a competition.)

12.7 Integer partitions
363
3. This time we consider the number ¯pn of partitions of n with all sum-
mands distinct. For instance, for n = 5 we only admit the partitions
5 = 1 + 4 and 5 = 2 + 3.
(a) Express ¯pn as a coeﬃcient of xn in a suitable expression.
(b) Following the proof method of Theorem 12.7.2, prove that ¯pn ≤
e2√n.
(c) ∗Prove a lower bound of the form ¯pn ≥ec√n for all suﬃciently
large values of n, with c > 0 some constant. What is the largest c you
can get?
4. (a) Write down the generating functions for the numbers ¯pn from
Exercise 3 (the generating function is an inﬁnite product).
(b) Find a generating function for the numbers of unordered partitions
of n into odd summands (also an inﬁnite product).
(c) ∗Verify that the generating functions in (a) and (b) are the same,
and hence that the number of partitions into distinct summands equals
the number of partitions into odd summands. ∗∗Can you ﬁnd a direct
argument (a bijection)?
5. The Royal Mint of Middle Coinland issues a1 types of coins of value
1 doublezon, a2 types of coins of value 2 doublezons, etc. Show that
the number of ways of paying the sum of n doublezons by such coins
equals the coeﬃcient of xn in
n

i=1
1
(1 −xi)ai .
6. (a) Express the number of nonisomorphic rooted trees of height at
most 2 with m leaves using the numbers pn. The notion of rooted
trees and their isomorphism are introduced in Section 5.2. The root is
not counted as a leaf, and the height of a rooted tree is the maximum
distance of a leaf from the root.
(b) This time consider rooted trees of height at most 2 with m vertices.
Express their number using the numbers pn.
(c) ∗Express the number rn of rooted trees with n leaves, all of them
at height 3, as the coeﬃcient of xn in a suitable expression (possibly
involving the numbers pi; Exercise 5 can serve as an inspiration).
(d) ∗∗Show that the numbers rn in (c) satisfy rn = eO(n/ log n). Use
(c) and the proof of Theorem 12.7.2. (The calculation may be quite
challenging.)
(e) ∗Show that the estimate in (d) is roughly correct, i.e. that rn ≥
ecn/ log n for some constant c > 0.

13
Applications of linear
algebra
Linear algebra is a part of algebra dealing with systems of linear
equations, matrices, determinants, vector spaces, and similar things.
We have already seen several proofs using linear algebra in pre-
vious chapters, most notably in Section 8.5. Here we will demon-
strate a few more methods and applications. First, we present two
problems, one concerning the existence of so-called block designs
and the other about covering a complete graph by complete bipar-
tite graphs. Hardly anyone would suspect that these problems are
related to matrices, and yet an elegant solution can be given based
on the notion of rank of a matrix. An interested reader can ﬁnd much
more about similar proofs in the very vividly written textbook by
Babai and Frankl [13]. In the subsequent two sections of this chapter,
we assign several vector spaces to each graph; this leads to a com-
pact and insightful description of seemingly very complicated sets.
Finally in Section 13.6, we consider two elegant algorithms where
linear algebra blends with a probabilistic method.
Throughout this chapter, we assume some basic knowledge of lin-
ear algebra. All the required material is summarized in the Appendix.
13.1
Block designs
Here we consider very regular systems of ﬁnite sets, the so-called
block designs. Finite projective planes are a special case of this con-
cept but the general notion of block designs no longer has a geometric
motivation.
Let V be a ﬁnite set and let B be a system of subsets1 of the
set V . In order to emphasize that the set system B lives on the set
1This notation diﬀers from what we have been mostly using in previous chap-
ters, but it is traditional in the theory of block designs.

13.1 Block designs
365
V , we write it as an ordered pair (V, B). (Let us remark that such
a pair (V, B) can also be regarded as a generalization of the notion
of graph, and then it may also be called a hypergraph; the points of
V are then called the vertices and the sets of B the hyperedges.) If
all the sets B ∈B have the same cardinality k, we say that (V, B) is
k-uniform.
We have already met an important example of a k-uniform set sys-
tem in Chapter 9 on ﬁnite projective planes. We have shown that if V
denotes the set of points of a ﬁnite projective plane and B stands for
the set of its lines, then the set system (V, B) is (k + 1)-uniform for a
suitable k, and, moreover, we have |V | = |B| = k2 +k+1. This example
will hopefully help also in understanding the following deﬁnition, which
may look quite technical at ﬁrst sight (but we will illustrate it with
several more examples later on).
13.1.1 Definition. Let v, k, t, and λ be integers. We suppose that
v > k ≥t ≥1 and λ ≥1. A block design of type t-(v, k, λ) is a set
system (V, B) satisfying the following conditions:
(1) V has v elements.
(2) Each set B ∈B has k elements. The sets of B are called the
blocks.
(3) Each t-element subset of the set V is contained in exactly λ
blocks of B.
Here are a few basic examples illustrating this deﬁnition.
Example. Let V be a ﬁnite set and k an integer. We put B =
V
k

(in words, B consists of all the k-element subsets of V ). The pair
(V, B) is called the trivial block design.
It is easy to check that (V, B) is a t-(v, k, λ) block design, where
t ∈{1, 2, . . . , k} can be chosen arbitrarily, v = |V |, and λ =
v−t
k−t

.
(To see this, convince yourself that any t-element subset of V is
contained in exactly
v−t
k−t

blocks B ∈B.)
Example. Let V be a v-element set, and let k ≥1 be an integer
dividing v. Partition the elements of V into disjoint k-element subsets
B1, B2, . . . , Bv/k, and put B = {B1, B2, . . . , Bv/k}. Then (V, B) is a
block design of type 1-(v, k, 1).
Example. Let V be the set of points of a projective plane of order n,
and let B denote the set of its lines. Such a pair (V, B) is a block
design of type 2-(n2 +n+1, n+1, 1). This nontrivial fact was proved
in Section 9.1. Conversely, it can be shown that any block design of

366
Applications of linear algebra
type 2-(n2 + n + 1, n + 1, 1), for n ≥2, is a projective plane of order
n (Exercise 1).
13.1.2 Example. Let V
= {0, 1, 2, 3, 4, 5} and let B consist of
the following triples: {0, 1, 2}, {0, 2, 3}, {0, 3, 4}, {0, 4, 5}, {0, 1, 5},
{1, 2, 4}, {2, 3, 5}, {1, 3, 4}, {2, 4, 5}, {1, 3, 5}. Then (V, B) is a
2-(6, 3, 2) block design. (This can be simply checked by deﬁnition
with no cleverness involved.)
This block design can also be deﬁned in a more “structured” man-
ner. Consider a cycle with vertices 1, 2, . . . , 5 and one extra vertex 0.
The system B then consists of all triples of vertices containing exactly
one edge of the cycle; see the diagram below:
1
2
3
4
5
0
1
2
3
4
5
0
These examples should invoke the (correct) impression that block
designs constitute a certain kind of regularity. Usually it is not easy
to construct block designs of a given type, and the basic question in
this whole area is the question of existence.
Basic problem. For given numbers v, k, λ, t, decide whether a block
design of type t-(v, k, λ) exists or not.
Here we derive some necessary conditions by algebraic means.
At the end of this brief introduction, let us mention that block de-
signs arose and are still being used in mathematical statistics in the
design of experiments. This motivation has also inﬂuenced the notation
introduced above.
Imagine that we want to assess several diﬀerent ways of treating
a certain plant (for suppressing its insect parasites, say). There are v
types of treatment to be compared (v for “variety”). We will compare
the treatments by a series of experiments, and in each experiment we
can apply k types of treatment; this is given by the technical condi-
tions of the experiment. Each experiment will form a block of the tested
treatments. We could in principle test all possible k-tuples, or blocks,
of treatments, but in the situation of ﬁeld experiments, this trivial way
of testing (hence the name “trivial block design”) is far too demanding
even for small values of k and v. For this reason, statisticians started

13.1 Block designs
367
using designs of experiments where one doesn’t test all possible k-tuples
but only some selected blocks. This may of course lead to errors, since
the experiments are incomplete: some possible blocks are not consid-
ered, and hence some possible mutual inﬂuences of the treatments are
neglected. In order to compensate for this (enforced) incompleteness of
the testing, we require that at least each pair of treatments appear to-
gether in the same number of experiments–blocks. The scheme of such
a series of experiments is exactly a block design of type 2-(v, k, λ). If we
require that each triple of treatments appear in the same number, λ, of
experiments, we get a block design of type 3-(v, k, λ), and so on.
Various block designs appear under diﬀerent special names in the
literature: For instance, balanced incomplete block design (or BIBD) for
designs of type 2-(v, k, λ), Steiner systems (for λ = 1), tactical conﬁgu-
rations (for t > 2), etc.
Integrality conditions. It should be clear that a block design of
type t-(v, k, λ) doesn’t exist on each set, i.e. for all values of v. For
instance, a 1-(v, k, 1) design is a partition into k-element sets, and
hence v has to be divisible by k. Another, less trivial example is
obtained for the case of projective planes, where v is determined
uniquely by the size of the lines. The following theorem describes
the most important class of necessary conditions for the existence of
a block design of type t-(v, k, λ).
13.1.3 Theorem (Integrality conditions). Suppose that a block
design of type t-(v, k, λ) exists. Then the following fractions must be
integers:
λ v(v −1) . . . (v −t + 1)
k(k −1) . . . (k −t + 1), λ(v −1) . . . (v −t + 1)
(k −1) . . . (k −t + 1), . . . , λv −t + 1
k −t + 1 .
Proof. This is an application of double-counting. Let (V, B) be a
block design of type t-(v, k, λ). Fix an integer number s with 0 ≤
s ≤t and an s-element set S ⊆V . Let us count the number N of
pairs (T, B) with S ⊆T ∈
V
t

and T ⊆B ∈B:
V
B
T
S
On the one hand, T can be picked in
v−s
t−s

ways, and each T is a
subset of exactly λ blocks B ∈B; hence N = λ
v−s
t−s

.

368
Applications of linear algebra
On the other hand, let M be the number of blocks containing the
set S. Since each block B containing S contains
k−s
t−s

t-element sets
T with S ⊆T, we also have N = M
k−s
t−s

. Hence
M = λ
v−s
t−s

k−s
t−s
 = λ(v −s) . . . (v −t + 1)
(k −s) . . . (k −t + 1) ,
and so the fraction on the right-hand side has to be an integer as the
theorem asserts.
2
Remark. By considering the above proof for s = 0 and for s = 1, we
see that λ v(v−1)...(v−t+1)
k(k−1)...(k−t+1) determines the total number of blocks and
λ (v−1)...(v−t+1)
(k−1)...(k−t+1) is the number of blocks containing a given element x ∈
V , i.e. the “degree of x”. In the statistical interpretation explained
above, this number expresses the number of times the treatment x has
been applied, and it is usually denoted by r (for “repetitions”).
13.1.4 Example (Steiner triple systems). The “ﬁrst” nontrivial
example of a block design of type t-(v, k, λ) is obtained for t = 2,
λ = 1, and k = 3. This is a system of triples where each pair of
points is contained in exactly one triple. In other words, it is a way
of covering all edges of a complete graph by edge-disjoint triangles.
In this case, the integrality conditions from Theorem 13.1.3 re-
quire that the numbers
v(v −1)
6
and
v −1
2
be integers. From this, it is easy to derive that either v ≡1 (mod 6)
or v ≡3 (mod 6). Hence v has to be one of the numbers 3, 7, 9, 13,
15, 19, 21, 25, 27, . . . For all such values of v, a block design of type
2-(v, 3, 1) exists. Such designs are called Steiner triple systems (see
Exercise 4). For v = 7, a Steiner triple system is a projective plane
of order 2 (the Fano plane). For v = 9, we have the following Steiner
system:

13.2 Fisher’s inequality
369
This can be regarded as an aﬃne plane, which arises from a pro-
jective plane of order 3 by deleting one line and all its points (see
Exercise 9.1.10).
Balanced incomplete block designs. For t = 2 (i.e. if we require
that each pair is in exactly λ k-tuples from B), the integrality conditions
look as follows:
λv(v −1)
≡
0

mod k(k −1)

λ(v −1)
≡
0
(mod k −1).
(13.1)
These conditions are no longer suﬃcient in general. But the following
diﬃcult and important result holds:
13.1.5 Theorem (Wilson’s theorem).
For any choice of numbers
k ≥1 and λ ≥1, there exists a number v0(k, λ) such that for all
v ≥v0(k, λ) satisfying the integrality conditions (13.1), a block design
of type 2-(v, k, λ) exists.
In other words, the integrality conditions are also suﬃcient for t = 2
if the ground set is large enough. The theorem says nothing about small
values of v: for instance, about the existence of block designs of type
2-(k2 + k + 1, k + 1, 1), i.e. of ﬁnite projective planes.
We have mentioned this theorem for completeness, and we are not
going to prove it here (a proof can be found in Beth, Jungnickel, and
Lenz [34]).
Exercises
1. (a) Verify that a projective plane of order q is a block design of type
2-(q2 + q + 1, q + 1, 1).
(b) ∗Show that, conversely, any block design of type 2-(q2+q+1, q+1, 1)
is a projective plane of order q.
2. ∗Show that any block design of type 2-(q2, q, 1) for q ≥2 is an aﬃne
plane of order q (see Exercise 9.1.10 for the deﬁnition).
3. Construct a Steiner triple system with 15 elements.
4. ∗Put n = 3m, where m is an odd natural number. We deﬁne a set
system (X, M) as follows: X = {(x, i): i = 1, 2, 3, x = 0, 1, . . . , m−1},
and M consists of all triples {(x, 1), (x, 2), (x, 3)} and all triples of the
form {(x, i), (y, i), (z, i + 1)}, where x ̸= y, x + y ≡2z (mod m),
i = 1, 2, 3, and i + 1 means 1 for i = 3. Prove that (X, M) is a Steiner
triple system.
13.2
Fisher’s inequality
One of the founders of the theory of block designs was an English statis-
tician, R. A. Fisher. Although examples of block designs have been

370
Applications of linear algebra
known for a long time (Steiner triple systems for almost 100 years),
Fisher was the ﬁrst to identify the general deﬁnition and its signiﬁcance
in the statistical context. He also found additional necessary conditions
restricting the existence of block designs.
13.2.1 Theorem (Fisher’s inequality).
Let (V, B) be a block
design of type 2-(v, k, λ) with v > k. Then |B| ≥|V |. Hence, the
design of tests for v treatments requires at least v experiments.
Note that block designs with |B| = |V | exist (for example, the
ﬁnite projective planes), and so, in this sense, Fisher’s inequality is
optimal. The following example illustrates its strength.
Example. Fisher’s inequality implies that there is no block design
of type 2-(16, 6, 1). Indeed, by the remark following Theorem 13.1.3,
the number of blocks in such a design must be 16·15
6·5
= 8 < 16. At
the same time, the integrality conditions are satisﬁed for this choice
of parameters: we have already veriﬁed that the number of blocks
is integral, and also the number r =
15
5
= 3 is an integer. The
reader may check that block designs of type 2-(21, 6, 1) and of type
2-(25, 10, 3) are excluded by Fisher’s inequality as well, although they
again satisfy the integrality conditions.
Fisher’s inequality can be proved by a remarkable application
of elementary linear algebra, as was discovered by an Indian mathe-
matician R. C. Bose. Before we begin with the proof, let us introduce
the incidence matrix of the set system (V, B); this is a similar con-
cept to the incidence matrix of a graph (for oriented graphs it was
discussed in Section 8.5). Let us denote the elements of the set V
by x1, x2, . . . , xv and the sets of B by B1, B2, . . . , Bb. We deﬁne a
v × b matrix A = (aij), with rows corresponding to points of V and
columns to sets of B, by the formula
aij =
 1
if xi ∈Bj
0
otherwise.
The matrix A is called the incidence matrix of the set system (V, B).
Proof of Fisher’s inequality.
For a given block design (V, B),
consider its incidence matrix A = (aij). The matrix transposed to
A, AT , has size b × v, and hence the matrix product AAT has size
v × v. We show that the matrix M = AAT has a very simple form.

13.2 Fisher’s inequality
371
Let us consider an entry mij of M. By the deﬁnition of matrix
multiplication, we have
mij =
b

k=1
aikajk
(the term in the jth column and kth row of the matrix AT is ajk).
Thus, the entry mij expresses the number of sets Bk containing both
xi and xj. By the deﬁnition of a block design, mij can only attain
two possible values:
mij =
 λ
for i ̸= j
λ v−1
k−1
for i = j.
The number λ· v−1
k−1 has been denoted by r in the preceding text, and
so the matrix M is





r
λ
. . .
λ
λ
r
. . .
λ
...
...
...
...
λ
λ
. . .
r




.
We want to show that this matrix is nonsingular, i.e. that its deter-
minant is nonzero. Elementary row operations give
det M = det





r + (v −1)λ
r + (v −1)λ
. . .
r + (v −1)λ
λ
r
. . .
λ
...
...
...
...
λ
λ
. . .
r





=

r + (v −1)λ

det





1
1
. . .
1
λ
r
. . .
λ
...
...
...
...
λ
λ
. . .
r





=

r + (v −1)λ

det





1
1
. . .
1
0
r −λ
. . .
0
...
...
...
...
0
0
. . .
r −λ





=

r + (v −1)λ

· (r −λ)v−1.
We now recall that r = λ · v−1
k−1. Clearly, r + (v −1)λ ̸= 0, and since
v > k, we also have r > λ, and so det M ̸= 0. Therefore, the matrix

372
Applications of linear algebra
M has rank2 v. But if we had b < v then the rank of both the matrices
A and AT would be strictly smaller than v, and consequently also
the matrix M = AAT would have rank < v (here we use a simple
property of matrix rank; see Exercise 2). We conclude that b ≥v.
This ﬁnishes the proof of Fisher’s inequality.
2
This application of matrix rank became the basis of many similar
(and important) proofs in combinatorics. Another proof of the fact
that r(M) = v is indicated in Exercise 4.
Exercises
1. For λ = 1, prove Fisher’s inequality directly (without using linear
algebra).
2. Show that if A is an n×k matrix and B a k ×m matrix then r(AB) ≤
min(r(A), r(B)).
3. ∗Let F be some ﬁeld and G ⊆F some subﬁeld of F (you may want
to imagine the real numbers for F and the rationals for G). Let A be
a matrix with elements from the ﬁeld G. The rank of A can be con-
sidered over the ﬁeld G (with the linear dependencies in the deﬁnition
having coeﬃcients in G) or over the ﬁeld F (linear dependence with
coeﬃcients in F). Explain why we obtain the same rank in both cases.
4. A square n × n real matrix M is called positive deﬁnite if we have
xT Mx > 0 for each nonzero (column) vector x ∈Rn.
(a) Why does any positive deﬁnite n×n matrix M have the full rank n?
(b) Show that the matrix M used in the proof of Fisher’s inequality
in the text is positive deﬁnite (and hence it has rank v, without a
calculation of the determinant).
5. (a) Let C1, C2, . . . , Cm be subsets of an n-element set X. Suppose
that each Ci has an odd cardinality, and that the cardinality of each
intersection Ci ∩Cj (for i ̸= j) is even. Prove that then m ≤n. Look
at the matrix AT A, where A is the incidence matrix of the considered
set system, but work over the 2-element ﬁeld (i.e. modulo 2).
2Let us recall that the rank of a matrix M, usually denoted by the symbol
r(M), is the maximum possible number of linearly independent rows in M. The
rank can also be deﬁned analogously using the columns instead of the rows, and
a basic linear algebra theorem tells us that both deﬁnitions always yield the same
number.

13.3 Covering by complete bipartite graphs
373
(b) Consider a similar problem as in (a), but this time we require that
the sizes of the sets themselves be even while all |Ci ∩Cj| be odd for
i ̸= j. Prove that m ≤n again holds.
(c) ∗And this time we require that the sets Ci all be distinct, and their
sizes and the sizes of all pairwise intersections be even. Show that one
can construct such a system with 2⌊n/2⌋sets.
6. (Generalized Fisher’s inequality) Let X be an n-element set, and let
q be an integer number, 1 ≤q < n. Let C1, C2, . . . , Cm be subsets
of X, and suppose that all the intersections Ci ∩Cj (for i ̸= j) have
cardinality exactly q.
(a) ∗Using the method of Exercise 4, prove that m ≤n. (Treat the
case when |Ci| = q holds for some i separately.)
(b) ∗Why can the claim in (a) be called a “generalized Fisher’s in-
equality”? Derive Fisher’s inequality from it!
13.3
Covering by complete bipartite graphs
The following question has been motivated by a problem in telecom-
munications:
Problem. The set of edges of a complete graph Kn should be
expressed as a disjoint union of edge sets of m complete bipartite
graphs. What is the smallest value of m = m(n) for which this is
possible?
One possible way of expressing E(Kn) as a disjoint union of edge
sets of n −1 complete bipartite graphs uses graphs of type K1,ni
(“stars”). Here is such a disjoint covering for n = 5:
1
2
3
4
5
To produce such a disjoint covering for an arbitrary n, suppose that
E(Kn−1) has already been expressed using n −2 stars. In the graph
Kn, consider one vertex, and cover all edges containing that vertex
by the edge set of a star K1,n−1. Then it remains to cover all edges
of a graph isomorphic to Kn−1, and we can already do this.
It is not at all obvious that we couldn’t do better, say by using
some complete bipartite graphs both of whose color classes are large.

374
Applications of linear algebra
But Graham and Pollak discovered an ingenious way of showing that
no better disjoint covering may exist:
13.3.1 Theorem (Graham–Pollak theorem). We have m(n) ≥
n −1.
Proof. Suppose that complete bipartite graphs B1, B2, . . . , Bm dis-
jointly cover all the edges of Kn, i.e. we have V (Bk) ⊆V (Kn) =
{1, 2, . . . , n} and E(Kn) = E(B1) ˙∪E(B2) ˙∪· · · ˙∪E(Bm). Let Xk and
Yk be the color classes of Bk; this means that the edges of Bk all go
between Xk and Yk.
To each graph Bk, we assign an n × n matrix Ak, whose entry in
the ith row and jth column is
a(k)
ij =
 1
if i ∈Xk and j ∈Yk
0
otherwise.
The deﬁnition of Ak resembles the adjacency matrix of the graph
Bk, except for the fact that Ak is not symmetric—each edge of the
graph Bk only contributes one 1. For example, for the subgraph
1
2
3
4
5
6
Xk
Yk
Bk =
the matrix Ak is








0
0
0
1
1
1
0
0
0
1
1
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0








.
We claim that each of the matrices Ak has rank 1. This is because all
the nonzero rows of Ak equal the same vector, namely the vector with
1s at positions whose indices belong to Yk and with 0s elsewhere.
Let us now consider the matrix A = A1 + A2 + · · · + Am. Each
edge {i, j} belongs to exactly one of the graphs Bk, and hence for
each i ̸= j, we have either aij = 1, aji = 0, or aij = 0, aji = 1,

13.3 Covering by complete bipartite graphs
375
where aij denotes the entry of the matrix A at position (i, j). We
also have aii = 0. From this we get A + AT = Jn −In, where Jn is
the n × n matrix having 1s everywhere and In is the n × n identity
matrix, with 1s on the diagonal and 0s elsewhere. We now want to
show that the rank of such a matrix A is at least n −1. Once we
know this, we get n −1 ≤r(A) ≤r(A1) + · · · + r(Am) = m, because
for arbitrary two matrices M1 and M2 (of the same shape) we have
r(M1+M1) ≤r(M1)+r(M2); this is easy to check from the deﬁnition
of matrix rank (Exercise 1).
For contradiction, let us suppose that r(A) ≤n −2. If we add an
extra row consisting of all 1s to the matrix A, the resulting (n+1)×n
matrix still has rank ≤n−1, and hence there exists a nontrivial linear
combination of its columns equal to the zero vector. In other words,
there exists a (column) vector x ∈Rn, x ̸= (0, 0, . . . , 0)T , such that
Ax = 0 and also n
i=1 xi = 0.
From the last mentioned equality, we get Jnx = 0. We calculate
xT 
A + AT 
x = xT (Jn −In)x = xT (Jnx) −xT (Inx)
= 0 −xT x = −
n

i=1
x2
i < 0.
On the other hand, we have
xT 
AT + A

x =

xT AT 
x + xT(Ax) = 0 · x + x · 0 = 0,
and this is a contradiction. Hence r(A) = n −1 and Theorem 13.3.1
is proved.
2
Exercises
1. Let M1 and M2 be two n × m matrices. Check that r(M1 + M2) ≤
r(M1)+r(M2), and show with examples that sometimes equality holds
and sometimes not.
2. (a) The edges of Kn should be covered by edges of complete bipartite
subgraphs of Kn, but we do not insist on their edge sets being disjoint
(i.e. one edge may be covered several times). Show that there exists a
covering consisting of ⌈log2 n⌉bipartite graphs.
(b) ∗Prove that a covering as in (a) really requires at least ⌈log2 n⌉
bipartite graphs.
3. ∗This time, we want to cover all the edges of Kn by edges of complete
bipartite subgraphs, in such a way that each edge is covered an odd

376
Applications of linear algebra
number of times. Prove that we need at least 1
2(n −1) complete bi-
partite subgraphs. Proceed similarly as in the proof in the text, but
work with matrices over the 2-element ﬁeld. Instead of the matrix Ak,
consider the adjacency matrix of the graph Bk, whose rank is 2.
4. ∗In an ice-hockey tournament, each of n teams played against every
other team, with no draws. It turned out that for every nonempty
group G of teams, a team exists (it may or may not belong to G) that
won an odd number of games with the teams of G. Prove that n is
even. Hint: Work with a suitable matrix over the 2-element ﬁeld.
13.4
Cycle space of a graph
Let G = (V, E) be an undirected graph. Let the symbol KG denote the
set of all cycles, of all possible lengths, in the graph G (more exactly,
the set of all subgraphs that are cycles). It is clear that this set deserves
a complicated notation by a calligraphic letter since it is quite large.
Check that, for instance, for the complete graph Kn we have
|KKn| =
n

k=3
n
k

· (k −1)!
2
,
(13.2)
and for the complete bipartite graph Kn,n we get
KKn,n
 =
n

k=2
n
k
2
· k!(k −1)!
2
.
(13.3)
On the other hand, we may note that KG = ∅if and only if G is a forest
(a disjoint union of trees).
It might seem that the set KG has no reasonable structure. In this
section, we introduce a suitable generalization of the notion of a cycle
(related to the material explained in Section 4.4 on Eulerian graphs)
which extends the set of all cycles to a larger set with a very simple
structure, namely with the structure of a vector space. Ideas presented
in the sequel originally arose in the investigation of electrical circuits.
13.4.1 Definition (Even set of edges).
Let G = (V, E) be an
(undirected) graph. A set E′ ⊆E is called even if the degrees of all
vertices in the graph (V, E′) are even.
For example, the empty set and the edge set of an arbitrary cycle
are even sets.
In the sequel, it will be advantageous to identify a cycle with its
edge set.
13.4.2 Lemma. A set E′ of edges is even if and only if pairwise
disjoint cycles E1, E2, . . ., Et exist such that E′ = E1 ˙∪E2 ˙∪· · · ˙∪Et.

13.4 Cycle space of a graph
377
Proof. If E′ is a nonempty even set then the graph (V, E′) is not a
forest and hence it contains some cycle, E1. The set E′ \ E1 is again
even, and so we can proceed by induction on the number of edges
in E′.
2
We now describe the structure of the family of all even sets of
edges in a given graph G = (V, E) algebraically. We denote the
edges of G by e1, e2, . . . , em, and to each set A ⊆E, we assign its
characteristic vector vA = (v1, v2, . . . , vm) deﬁned by
vi =
 1
if ei ∈A
0
otherwise.
The vectors are added and multiplied modulo 2 (which means that
1 + 1 = 0). Let A, B ⊆E be even sets. Then the reader should be
able to check without diﬃculties that
vA + vB = vC,
where C = (A ∪B) \ (A ∩B) is the symmetric diﬀerence of the sets
A and B.
Let the symbol E denote the set of the characteristic vectors of
all even sets of edges in G. For an easier formulation, we need to
generalize the notion of a spanning tree a little: by a spanning forest
of an arbitrary, possibly disconnected, graph G we understand any
subgraph (V (G), ¯E) of G containing no cycles and inclusion maximal
with respect to this property (i.e. adding any edge of G to ¯E creates
a cycle). For a connected graph G, this deﬁnition coincides with the
deﬁnition of a spanning tree in Section 5.3. For a disconnected graph
we get a forest consisting of spanning trees of all the components.
13.4.3 Theorem (Cycle space theorem).
(1) For any graph G, the set E is a vector space over the 2-element
ﬁeld GF(2). The dimension of this vector space is |E| −|V | + k,
where k is the number of components of the graph G.
(2) Fix an arbitrary spanning forest T = (V, E′) of the graph G,
and for each edge e ∈E \ E′, let Ce denote the (unique) cycle
contained in the graph (V, E′ ∪{e}). Then the characteristic
vectors of the cycles Ce, e ∈E \ E′, form a basis of E.
In this theorem, the symbol GF(2) stands for the 2-element ﬁeld
consisting of the numbers 0 and 1, with arithmetic operations per-
formed modulo 2. The letters GF come from “Galois ﬁeld”, which is

378
Applications of linear algebra
a traditional term for a ﬁnite ﬁeld. A vector space over GF(2) simply
means that if the characteristic vectors of two even sets are added
modulo 2 we again obtain a characteristic vector of an even set.
The cycle Ce is called an elementary cycle (determined by the
edge e, with respect to a given spanning forest).
Proof. First we show that E is a vector space. To this end, we have
to check that by adding two vectors from E we again get a vector
from E, and by multiplying a vector from E by a number from the
ﬁeld GF(2) we also get a vector from E. The latter is rather trivial,
since we can only multiply by 0 (getting 0vA = v∅) or by 1 (and
then 1vA = vA). According to the remark above the theorem, the
addition of vectors corresponds to taking the symmetric diﬀerence
of the respective sets, and so it suﬃces to show that the symmetric
diﬀerence of two even sets A and B is again an even set. Let us pick
an arbitrary vertex v ∈V . Suppose that dA of the edges incident
to v belong to A, dB of them belong to B, and d of them belong to
both A and B. Here dA, dB are both even numbers. The number of
edges incident to v belonging to the symmetric diﬀerence (A ∪B) \
(A ∩B) is dA + dB −2d, and hence the degree of v in the symmetric
diﬀerence is even. Therefore, the symmetric diﬀerence is an even set
and so E is a vector space over GF(2).
Let (V, E′) be a spanning forest of the graph G. Then |E′| =
|V |−k, where k is the number of components of the graph G. It suf-
ﬁces to prove that the characteristic vectors of all elementary cycles
constitute a basis of the vector space E.
We ﬁrst show that the elementary cycles are linearly independent.
Consider an edge ei ̸∈E′. The characteristic vector of the elemen-
tary cycle Cei is the only one among the characteristic vectors of all
elementary cycles having 1 at position i. For this reason, a charac-
teristic vector of an elementary cycle cannot be a linear combination
of other such vectors.
Next, let us show that the elementary cycles generate E. Choose
an even set A and deﬁne a set B by giving its characteristic vector:
vB =

e∈A\E′
vCe.
Which edges are contained in B? These are exactly the edges be-
longing to an odd number of elementary cycles (with respect to the
spanning forest (V, E′)). Surely B does contain the set A\E′ because

13.4 Cycle space of a graph
379
each of its edges lies in a unique elementary cycle. Let C denote the
symmetric diﬀerence of the sets A and B. This is an even set, and at
the same time, it must be contained in E′. Since E′ has no cycles, we
get that C = ∅, which means that A = B. We have thus expressed
the set A as a linear combination of elementary cycles.
2
Example. Let us consider the following graph G = (V, E):
a
i
b
c
d
e
f
g
h
1
2
3
4
5
6
The dimension of its cycle space is 9 −6 + 1 = 4. If we choose the
spanning tree drawn by thick lines, then the corresponding basis of
the cycle space consists of the characteristic vectors of the following
elementary cycles:
Ca = {a, b, c}
Cd = {b, e, h, i, d}
Cg = {g, h, i}
Cf = {c, f, h, e}.
For example, for the cycle C = {a, f, i, d}, we have the expression
vC = vCa + vCd + vCf.
We easily obtain the following consequence of Theorem 13.4.3:
13.4.4 Corollary. The number of even sets of a graph G = (V, E)
with k components is 2|E|−|V |+k.
In spite of the large number of even sets, their structure is simple and
they can be easily generated from a much smaller basis. The dimension
of the cycle space, i.e. the number |E|−|V |+k, is called the cyclomatic
number of the graph G = (V, E). (Also the older name Betti number is
sometimes used. This is related to a topological view of a graph.)
Exercises
1. Verify the formulas (13.2) and (13.3).
2. Prove Corollary 13.4.4.

380
Applications of linear algebra
3. Determine the cyclomatic number of an m×n grid graph; for example,
3 × 4
4. Prove that for any topological planar 2-connected graph (i.e. a planar
2-connected graph with a ﬁxed planar drawing), the border cycles of
the bounded faces form a basis of the cycle space E.
13.5
Circulations and cuts: cycle space revisited
In this section, we put the material of the previous section into a some-
what diﬀerent perspective. We will express things using matrices, and
we will work with both undirected graphs and their orientations. In this
more complicated setting, a cycle space theorem similar to 13.4.3 will
appear in new connections.
Circulations.
Let us recall the notion of an orientation from Sec-
tion 8.5. An orientation of a graph G = (V, E) is a directed graph
⃗G = (V, ⃗E), where the set ⃗E contains, for each edge {x, y} ∈E, exactly
one of the directed edges (x, y) and (y, x). The following picture shows
a graph and one of its possible orientations:
G
⃗G
1
2
3
4
(The number of all possible orientations of a graph G is 2|E|, but many
of them can be isomorphic.)
We assume that some orientation ⃗G = (V, ⃗E) has been chosen once
and for all. A real function f : ⃗E →R is called a circulation if we have,
for each vertex v of the graph G,

x∈V : (v,x)∈⃗E
f(v, x) =

x∈V : (x,v)∈⃗E
f(x, v).
(13.4)
(Let us remark that a circulation may also attain negative values on
some edges.) The set of the undirected edges e with f(⃗e) ̸= 0, where ⃗e
is the chosen orientation of e, is called the carrier of f. It is easy to see

13.5 Circulations and cuts: cycle space revisited
381
that the carrier of any nonzero circulation contains a cycle (but here it
is important that we work with an orientation of an undirected graph,
i.e. that edges (x, y) and (y, x) never occur simultaneously).
The notion of a circulation has various intuitive interpretations. For
example, if ⃗G is a design of an electrical circuit and if f(e) expresses the
current ﬂowing through an edge e, then Eq. (13.4) says that for each
vertex, the amount of current ﬂowing in is the same as the amount ﬂow-
ing out. This is the ﬁrst Kirchhoﬀlaw. This “electrical” interpretation
was one of the ﬁrst motivations for the theory discussed in this section.
One example of a circulation can be obtained as follows. Let C =
(v1, v2, . . . , vk+1 = v1) be the sequence of vertices of an (undirected)
cycle in the graph G. We deﬁne a circulation f in the orientation ⃗G by
putting
f(vi, vi+1)
=
1
if (vi, vi+1) ∈⃗E
f(vi+1, vi)
=
−1
if (vi+1, vi) ∈⃗E
f(x, y)
=
0
if {x, y} is not an edge of C.
We say that this circulation belongs to the cycle C. (Note that it depends
of the direction in which the edges of the undirected cycle C were listed.)
For the graph in the preceding picture and for the cycle with the vertices
1, 2, 3, 4 (in this order), the circulation looks as follows:
1
2
3
4
1
0
−1
1
−1
0
It is easy to see that if f1 and f2 are circulations, then the function
f1 + f2 is a circulation on ⃗G as well. For a real number c, the function
cf1 is also a circulation. (In both cases, it suﬃces to check the condition
(13.4).) Hence the set of all circulations has the structure of a vector
space. This time it is a vector space over the real numbers. Let us denote
this vector space by C and call it the circulation space of G.
It can be shown that for any graph G, the circulation space C is
generated by circulations belonging to cycles in G (see Exercise 1).
Soon we will obtain a more exact information about the structure of C.
Potentials and cuts. Let p: V →R be an arbitrary function (p for
potential). We deﬁne a function δp on the set of directed edges of ⃗G by
the formula
δp(x, y) = p(x) −p(y)
(13.5)
for each directed edge (x, y) ∈⃗E.

382
Applications of linear algebra
The function δp: ⃗E →R is called a potential diﬀerence (anyone
remembering a little of physics can probably see a relation to electrical
circuits). Each function g: ⃗E →R for which a potential p exists with
g = δp is also called a potential diﬀerence.
It is easy to verify that the sum of two potential diﬀerences is a po-
tential diﬀerence, and similarly for multiplying a potential diﬀerence by
a real number. Let us denote the vector space of all potential diﬀerences
by R and call it the cut space.
Why did we choose this name? Let us consider the following sit-
uation: Let a potential p attain the values 0 and 1 only. Let us put
A = {v ∈V : p(v) = 1}, B = V \ A. Then the potential diﬀerence
g = δp is nonzero only for the directed edges with one vertex in A and
the other vertex in B:
g(x, y)
=
1
for x ∈A, y ∈B
g(x, y)
=
−1
for x ∈B, y ∈A
g(x, y)
=
0
otherwise.
It is natural to call the set of all edges going between A and B a cut,
because the number of components increases by its removal (assuming
that there is at least one edge between A and B). In this picture, we
have indicated a potential p
0
0
1
1
+1
−1
−1
+1
and its potential diﬀerence. The edges of the corresponding cut are
drawn bold.
We now describe the whole situation using the incidence matrix. To
this end, we number the vertices, V = {v1, . . . , vn}, and the edges, E =
{e1, . . . , em}. The symbol ⃗ei denotes the directed edge corresponding to
the edge ei in the orientation ⃗G. From Section 8.5 we recall the incidence
matrix of the orientation ⃗G. This matrix, D, has size n × m, and its
entry dik is deﬁned by the following rule:
dik =



−1
if vi is the tail of ⃗ek
1
if vi is the head of ⃗ek
0
otherwise.
The following elegant description of the circulation space and of the cut
space can be given:

13.5 Circulations and cuts: cycle space revisited
383
13.5.1 Theorem. For any graph G, the cut space R is generated by
the rows of the incidence matrix D. The circulation space C is the
orthogonal complement of the cut space R, i.e. C = {x ∈Rm : xT y = 0
for all y ∈R}.
Proof.
Let D = (dij) be the n × m incidence matrix. It is easy to
see that each row is a potential diﬀerence (the corresponding potential
has a 1 sitting at the row’s vertex and 0 elsewhere). Next, consider an
arbitrary potential diﬀerence g = δp. For a directed edge ⃗ej = (vr, vs),
we have g(⃗ej) = p(vr)−p(vs) = n
i=1 dijp(vi); this follows by comparing
Eq. (13.5) to the deﬁnition of the incidence matrix. Hence, if we regard
the function g as a row vector, it is a linear combination of the rows of
the incidence matrix. Therefore, the rows generate the cut space R.
Let us now rewrite the condition (13.4) for a circulation f using the
incidence matrix. From the equality

x∈V : (x,v)∈⃗E
f(x, v) =

x∈V : (v,x)∈⃗E
f(v, x)
we get

(x,v)∈⃗E
f(x, v) −

(v,x)∈⃗E
f(v, x) = 0.
For v = vi, this can also be written as
m

j=1
f(⃗ej)dij = 0.
The function f, regarded as an m-element vector, thus has a zero scalar
product with the ith row of the matrix D. So f is orthogonal to each
row of D; actually, we see that the orthogonality of f to all rows of D is
equivalent to f being a circulation. Therefore, the vector spaces C and
R (regarded as subspaces of the space Rm) are orthogonal complements
of each other.
2
What is the dimension of the cut space R? By the previous theorem,
this is the dimension of the space generated by the rows of the incidence
matrix D. By a basic result of linear algebra we have used several times
already, this is the same as the dimension of the space generated by the
columns of D, and this dimension we are now going to determine.
Let dj denote the jth column of D (corresponding to the directed
edge ⃗ej).
Let us consider some set J ⊆{1, 2, . . . , m} of column indices, and
let us ask when the set of columns {dj : j ∈J} is linearly dependent.
A linear dependence means that there exist numbers cj, j ∈J, not
all of them zero, such that 
j∈J cjdj is the zero n-element vector. Let

384
Applications of linear algebra
us write this out for each component separately. The ith component,
corresponding to the vertex vi, gives

j∈J
cjdij = 0.
If we deﬁne ct = 0 for t ̸∈J, we get the condition (13.4) from the deﬁni-
tion of a circulation, and so c = (c1, c2, . . . , cm) is a nonzero circulation.
Hence the carrier of c contains a cycle (see Exercise 2).
On the other hand, as we already know, for each cycle C there exists
a nonzero circulation with carrier C. Altogether we get that the set of
columns {dj : j ∈J} is linearly independent if and only if the edge set
{ej : j ∈J} contains no cycle. Hence the rank of the matrix D is n−k,
where k is the number of components of the graph G. We have proved:
13.5.2 Theorem. For a graph G with n vertices, m edges, and k com-
ponents, the cut space R has dimension n−k and the circulation space
C has dimension m −n + k.
This result can be regarded as a more sophisticated version of
Theorem 13.4.3 (over the ﬁeld of real numbers).
The relation of the even sets of edges and the cuts, or of the circu-
lations and the potential diﬀerences, has the nature of a duality. The
considerations above can also be extended to other combinatorial ob-
jects than graphs. This is considered in the so-called matroid theory (see
Oxley [26], for instance).
Exercises
1. Deﬁne an “elementary circulation with respect to a given spanning for-
est”. Show that such elementary circulations generate the circulation
space.
2. (a) Let f : ⃗E →R be a nonzero circulation on an orientation ⃗G of an
undirected graph G. Prove that the carrier of f contains a cycle.
(b) True or false? The carrier of any circulation is a even set of edges.
13.6
Probabilistic checking
Checking matrix multiplication. Multiplying two square matri-
ces is a very important operation in many applications. A straight-
forward algorithm multiplying two n × n matrices according to the
deﬁnition of matrix multiplication requires about n3 arithmetic
operations. But, quite surprisingly, ingenious algorithms have been
discovered that can do such a multiplication in an asymptotically
much better time. You can read about such algorithms in Aho,
Hopcroft, and Ullman [11], for instance. The current asymptotic

13.6 Probabilistic checking
385
record is an algorithm executing only O(n2.376) operations. Because
of a really huge constant hidden in the O( ) notation, this algorithm
is only theoretically interesting.
But what is a theory today may enter the commercial sector to-
morrow, and so you can imagine that a software company sells a pro-
gram called MATRIX WIZARD and claims it can multiply square
matrices at a lightning speed. However, since your matrix multipli-
cation application is quite critical and wrong results could be disas-
trous, you would like to be sure that MATRIX WIZARD multiplies
matrices not only fast but also correctly. You can test it on many
examples, but even if it succeeds on many test examples this doesn’t
necessarily mean it’s always correct. It would be most satisfactory
to have a simple checking program appended to MATRIX WIZARD
that would always check whether the resulting matrix, C, really is
the product of the input matrices A and B. Of course, a checking pro-
gram that would actually multiply A and B and compare the result
with C makes little sense; you don’t know how to multiply matri-
ces as quickly as MATRIX WIZARD does and so all the advantages
from its fast multiplication would be lost. But it turns out that if
we allow some slight probability of error in the checking, there really
is a very simple and eﬃcient checker for matrix multiplication. For
simplicity, we assume that the considered matrices consist of ratio-
nal numbers, although everything works without change for matrices
over any ﬁeld.
13.6.1 Theorem (Freivalds’ matrix multiplication checker).
There exists a randomized algorithm which for any three input n×n
matrices A, B, C with rational entries performs O(n2) arithmetic op-
erations and answers either CORRECT or INCORRECT. If A, B, C
are such that AB = C then the algorithm always says CORRECT.
If, on the other hand, AB ̸= C, then the algorithm answers INCOR-
RECT with probability at least 1
2, for any given A, B, C.
By a randomized algorithm, we mean an algorithm that may
make some of its decisions at random. We may imagine that at some
steps, it “tosses a coin” and chooses one of two possible continuations
according to the result heads/tails.
In real computers, there is naturally no dwarf tossing coins, mainly
because such a dwarf or any other known physical device would slow
down the computation considerably, not to mention of other techni-
cal problems. Randomness is simulated by complicated but nonrandom

386
Applications of linear algebra
computations, and doing this well is quite challenging. But in most pro-
gramming languages there is a more or less satisfactory random number
generator at our disposal, so we will ignore this issue here and assume
that truly random numbers are available. The reader can ﬁnd out much
more about randomized algorithms, in particular more material related
to Freivalds’ checker, in the book by Motwani and Raghavan [25]. This
book is delightful reading for anyone interested in combinatorics and
theoretical computer science.
In our case, if A, B, C are ﬁxed matrices with AB ̸= C, the al-
gorithm in the theorem may sometimes answer INCORRECT and
sometimes CORRECT, depending on the outcome of its random in-
ternal decisions. The theorem claims that the (wrong) answer COR-
RECT has probability no more than 1
2.
But wait, the reader may cry, does this mean that if the multi-
plication done by MATRIX WIZARD is incorrect, the checker may
claim the result correct with probability 1
2? Come on, this is much
worse than the reliability of a weather forecast! Well, yes, but there’s
a subtle diﬀerence: we can run the checking algorithm several times
for the same matrices A, B, C, and a single answer INCORRECT
suﬃces to detect that AB ̸= C. If we make k runs for the same
A, B, C with AB ̸= C, the probability that we get the wrong an-
swer CORRECT every time, and thus fail to detect the error in
the multiplication, is only at most
 1
2
k because the results of the
runs are independent. For instance, for k = 50, this probability is
below 10−15, which is most likely much smaller than the chance of
the computer running the matrix multiplication algorithm being de-
stroyed by ants or nuclear war next Friday. (In contrast, listening
to 50 weather forecasts in a row doesn’t seem to provide such a
near-absolute certainty about tomorrow’s weather.) So we can re-
formulate the theorem’s conclusion as follows: with running time
O(n2 log 1
δ), the probability of a wrong answer can be bounded by δ,
for any prescribed parameter δ > 0.
Proof of Theorem 13.6.1. The algorithm is actually very simple.
We choose a random vector x ∈{0, 1}n, all the 2n possible vectors
having the same probability (the underlying probability space is thus
Cn as in Deﬁnition 10.2.2). We calculate the vector y = ABx −Cx,
where x and y are considered as n × 1 matrices. If y = 0 we output
CORRECT, otherwise we output INCORRECT.
A vector can be multiplied by an n × n matrix using O(n2) op-
erations. The right way of computing the product ABx is A(Bx),

13.6 Probabilistic checking
387
where we need two matrix–vector multiplications. Hence y can be
computed in O(n2) steps. It remains to prove the claim about prob-
ability. Obviously, if AB = C then y = ABx−Cx = (AB −C)x = 0;
hence in this case the algorithm always says CORRECT. So assume
that AB ̸= C and write D = AB −C. It suﬃces to prove the
following:
Lemma. Let D be any rational n × n matrix with at least one
nonzero entry. Then the probability of y = Dx being the zero vector,
for a random vector x ∈{0, 1}n, is at most 1
2.
Proof of the lemma. Suppose that dij is a nonzero entry of D. We
show that in such a case, yi = 0 holds with probability at most 1
2.
We have
yi = di1x1 + di2x2 + · · · + dinxn = dijxj + S,
where
S =

k=1,2,...,n
k̸=j
dikxk.
Imagine that we choose the values of the entries of x according to
successive coin tosses and that the toss deciding the value of xj is
made as the last one (since the tosses are independent it doesn’t
matter). Before this last toss, the quantity S is already ﬁxed, be-
cause it doesn’t depend on xj. After the last toss, we either leave S
unchanged (if xj = 0) or add the nonzero number dij to it (if xj = 1).
In at least one of these two cases, we must obtain a nonzero number.
Consequently, the probability of yi = 0 is at most 1
2. This proves the
lemma and also concludes the proof of Theorem 13.6.1.
2
It is very well known that writing error-free software for a compli-
cated task is almost impossible. This is quite alarming since we are
being increasingly surrounded by computer-controlled devices, and the
possibility of their failure is very real (well-known examples were serious
software-related problems in the Apollo or Space Shuttle projects, but
today one really doesn’t have to go to outer space to encounter a soft-
ware malfunction). The idea of using an independent checker to verify
the correctness of results obtained by complicated software is gaining
in popularity. Research in theoretical computer science revealed that
eﬃcient probabilistic checking is possible for almost any computation.
In principle, the work of a very fast computer can be checked, with
a high reliability in the sense of probability, by a much slower com-
puter. Although such results haven’t yet been put widely into practice,

388
Applications of linear algebra
they certainly raise interesting possibilities. Moreover, the “program-
checking” stream of ideas has been merged with ideas from some other
lines of research and has led to some of the deepest and most unex-
pected results in theoretical computer science. These results prove the
“hardness of approximation” for certain algorithmic problems. For ex-
ample, it has been known for a long time that the problem of ﬁnding
the largest complete subgraph of a given graph is a problem in a certain
class of diﬃcult problems called “NP-complete problems”. The new re-
sults say that if there were an eﬃcient algorithm for approximating the
size of the largest complete subgraph within any reasonable factor then
there would also be an eﬃcient algorithm for ﬁnding that size exactly.
It seems somewhat mysterious that the proof of such a result should
be related to probabilistic program checking, but that’s the way it is.
A recent book covering these developments and much more is Arora
and Barak [10].
Probabilistic associativity testing.
Let S be a set and let ◦
be some binary operation on S. This means that any two elements
a, b ∈S are assigned some element c ∈S, and this c is denoted
by a ◦b. Formally, ◦is thus a mapping S × S →S. You know
many binary operations with numbers. For example, “+” (addition)
is a binary operation on the set of all natural numbers and “−”
(subtraction) is a binary operation on the set of all integers. On the
other hand, “−” is not a binary operation on N and “/” (division)
is not a binary operation on the set of all real numbers in the sense
of our deﬁnition. These everyday operations usually satisfy various
laws, like commutativity (a+b = b+a) or associativity ((a+b)+c =
a + (b + c)). Here we will consider a quite arbitrary operation ◦that
may be quite lawless and anarchistic; we assume no commutativity,
no associativity, nothing like that.
In algebra, a set S with an arbitrary binary operation on it is called
a groupoid.
It may be instructive to compare the deﬁnition of a binary operation
on S to the deﬁnition of a binary relation on S: a relation assigns to
every pair (a, b) ∈S×S a yes/no answer (the elements are either related
or not), while an operation assigns to (a, b) a “result” lying again in S.
While nonmathematical examples of relations abound, it seems that
operations in the considered sense are peculiar to mathematics; at least
we can’t see any convincing and nontrivial nonmathematical examples.
Of course, there are many situations when two objects give rise to a
third object of the same kind, but for a binary operation we require
that any two objects can be so combined and that the result is again
from the same a priori chosen set of objects.

13.6 Probabilistic checking
389
We will be interested in the case when S is a ﬁnite set, and we
suppose that the operation ◦is speciﬁed by a table, such as the
following for the case S = {A, B, Γ, ∆}:
◦
A
B
Γ
∆
A
A
A
A
A
B
A
B
Γ
∆
Γ
A
Γ
A
Γ
∆
A
∆
A
B
We would like to check whether the given operation ◦is asso-
ciative or not. Call a triple (a, b, c) ∈S3 an associative triple if
(a ◦b) ◦c = a ◦(b ◦c) holds, and a nonassociative triple otherwise.
An obvious method is to go through all triples (a, b, c) ∈S3 and test
whether they are associative. For each triple (a, b, c), we need two
lookups in the table to ﬁnd (a ◦b) ◦c and two more lookups to com-
pute a ◦(b ◦c). Hence if |S| = n, the number of operations needed
for this straightforward associativity checking grows roughly as n3.
We will present an ingenious algorithm for associativity testing,
due to Rajagopalan and Schulman, which only needs O(n2) oper-
ations. It is again a randomized algorithm and it doesn’t give the
correct answer with 100% certainty.
13.6.2 Theorem. There is a randomized algorithm with the fol-
lowing properties. It accepts as an input a binary operation ◦on an
n-element set given by a table. The running time is O(n2), and the
algorithm outputs one of the answers ASSOCIATIVE or NONAS-
SOCIATIVE. In the case when ◦is associative, the algorithm always
declares it associative, and in the case when ◦is not associative, it
detects the nonassociativity with probability at least 1
8 (for any given
operation ◦).
The probability of an incorrect answer can be made arbitrarily
small by repeating the algorithm enough times, similar to Theo-
rem 13.6.1. That is, if we repeat the algorithm k times for a nonas-
sociative operation it will give the answer NONASSOCIATIVE at
least once with probability at least 1 −
 7
8
k.
An obvious randomized algorithm for associativity testing would
be to repeatedly pick a random triple (a, b, c) ∈S3 and to test its as-
sociativity. But the catch is that the nonassociativity need not man-
ifest itself on many triples. For example, the operation speciﬁed in
the above table has only two nonassociative triples, namely (∆, ∆, Γ)

390
Applications of linear algebra
and (∆, Γ, ∆), while there are 43 = 64 triples altogether. In fact, for
any n ≥3 there is an example of an operation on an n-element set
with just one nonassociative triple (Exercise 4). Hence, even if we
test n2 random triples, the chance of detecting nonassociativity is
only 1
n, and not a constant as for the algorithm in Theorem 13.6.2.
Proof of Theorem 13.6.2. Let S be the given n-element set. First
we deﬁne a vector space over GF(2) related to S. This will be very
similar to the deﬁnitions in Section 13.4 on the cycle space of a graph.
Let V denote the set of all n-tuples of 0s and 1s whose components
are indexed by the elements of S. For a ∈S and v ∈V , we let (v)a ∈
{0, 1} denote the component of v corresponding to the element a.
Moreover, for an element a ∈S, let va be the characteristic vector
of the set {a}; that is, the vector with the component corresponding
to a equal to 1 and with all other components 0.
The set V can be regarded as a vector space over the 2-element
ﬁeld GF(2), which is again similar to the considerations related to
the cycle space of a graph. Vectors are added and multiplied by an
element of GF(2) componentwise.
Since we will be dealing with objects of several diﬀerent kinds, we
will distinguish them notationally as follows: letters u, v, w stand for
elements of V , i.e. n-tuples of 0s and 1s, Greek letters α, β, γ denote
elements of GF(2), i.e. 0 or 1, and a, b, c, p, q, r are elements of S.
Next, based on the operation ◦on S, we deﬁne a binary operation,
denoted also by ◦, on V . For u, v ∈S, we set
u ◦v =

a,b∈S
(u)a(v)bva◦b.
On the right-hand side, the multiplication (u)a(v)b is in GF(2), the
result multiplies va◦b as a vector, and the sum is addition of vectors
in V . To make this more intuitive, suppose for a moment that S =
{p, q, r}, and write u = αpvp +αqvq +αrvr, v = βpvp +βqvq +βrvr.
Of course, αp, αq, αr ∈{0, 1} are just the components of u, and
similarly for v. To ﬁnd the vector u ◦v, we ﬁrst “multiply out” the
parentheses:
(αpvp + αqvq + αrvr) ◦(βpvp + βqvq + βrvr)
= αpβp(vp ◦vp) + αpβq(vp ◦vq) + · · · + αrβr(vr ◦vr).
Then this expression is “simpliﬁed” using the deﬁnition va◦vb = va◦b
for all a, b ∈S.

13.6 Probabilistic checking
391
We claim that the operation ◦on V is associative if and only if
◦on S was associative. Clearly, if (a, b, c) is a nonassociative triple
in S, with (a ◦b) ◦c = p ̸= q = a ◦(b ◦c), we have (va ◦vb) ◦vc =
vp ̸= vq = va ◦(vb ◦vc). Checking the associativity of ◦on V for an
associative ◦on S is an easy exercise (Exercise 5).
For the associativity-checking algorithm, we deﬁne a function
g: V 3 →V by setting
g(u, v, w) = [(u ◦v) ◦w] −[u ◦(v ◦w)].
By the above discussion, all values of g are 0 if and only if ◦is
associative (on V and also on S). Note that if vectors u, v ∈V are
given, u◦v can be calculated using O(n2) operations using the table
specifying ◦on S. Therefore g(u, v, w) can be evaluated in O(n2)
time too.
Now we are ready to formulate the algorithm for Theorem 13.6.2.
13.6.3 Algorithm. Select vectors u, v, w ∈V at random and inde-
pendently of each other (each of the 2n vectors of V has the same
probability of being selected for u etc.). Compute g(u, v, w) and
answer ASSOCIATIVE if g(u, v, w) = 0 and NONASSOCIATIVE
otherwise.
It remains to prove that for a nonassociative operation ◦, the
answer NONASSOCIATIVE is returned with probability at least
1
8. This means that at least
1
8 of the possible triples (u, v, w) ∈
V 3 are nonassociative. To this end, ﬁx some nonassociative triple
(a, b, c) ∈S3. Call two triples (u1, v1, w2) and (u2, v2, w2) equivalent
if u1 and u2 agree in all components but possibly the component cor-
responding to a (i.e. u1 −u2 = αva, α ∈GF(2)), v1 and v2 coincide
everywhere except possibly in the component of b, and w1, w2 diﬀer
only possibly in the c component. Each class of this equivalence has
exactly 8 elements. We show that each class contains at least one
nonassociative triple:
Lemma. Let (a, b, c) be a ﬁxed nonassociative triple. For all u, v,
w ∈V , there exist α, β, γ ∈GF(2) such that
g(u + αva, v + βvb, w + γvc) ̸= 0.
Proof of the lemma. We actually show that the sum
σ =

α,β,γ∈GF(2)
g(u + αva, v + βvb, w + γvc).

392
Applications of linear algebra
is nonzero. By the deﬁnition of the operation ◦on V , we obtain, for
any u, v, w ∈V ,
g(u, v, w) =

p,q,r∈S
(u)p(v)q(w)rvg(p,q,r) .
Substituting this into the sum σ and exchanging the summation
order, we get
σ =

p,q,r∈S


α,β,γ∈GF(2)
(u + αva)p(v + βvb)q(w + γvc)

vg(p,q,r).
The sum in the brackets can be rewritten, using distributivity in
GF(2), as follows:

α,β,γ∈GF(2)
(u + αva)p(v + βvb)q(w + γvc)
= [(u)p + (u + va)p] [(v)q + (v + vb)q] [(w)r + (w + vc)r]
= (2u + va)p(2v + vb)q(2w + vc)r = (va)p(vb)q(vc)r
because 2 = 1 + 1 = 0 in GF(2). Consequently, the only possibly
nonzero term is obtained for p = a, q = b, and c = r, and so σ =
vg(a,b,c) ̸= 0. This proves the lemma and also ﬁnishes the proof of
Theorem 13.6.2.
2
Remarks. The algorithm and its analysis could also be formulated with-
out introducing the vector space V . We could just talk about random
subsets of S, etc., but this seems to obscure the whole thing instead
of making it more accessible. The approach can also be generalized to
verify other identities for one or several binary or k-ary operations on a
given ﬁnite set, although interesting open problems remain. More details
can be found in the original paper Rajagopalan and Schulman [42].
Exercises
1. Suppose that we chose the components of the vector x randomly from
the set {0, 1, . . . , m −1} instead of from {0, 1} in Freivalds’ checker
(Theorem 13.6.1). Show that the probability of a wrong answer is at
most
1
m in this case.
2. ∗Suppose that we have a program for multiplying two polynomials
p(x), q(x) with integer coeﬃcients very quickly. Can you design a
method for a fast probabilistic checking of the correctness of the result,

13.6 Probabilistic checking
393
analogous to Freivalds’ matrix multiplication checker? That is, given
three polynomials p(x), q(x), and r(x), the algorithm should check
whether r(x) = p(x)q(x). Use the fact that a nonzero polynomial of
degree d has at most d roots.
3. How many binary operations on an n-element S are there?
4. ∗For each n ≥3, ﬁnd an example of an n-element set S with a nonasso-
ciative binary operation that possesses only one nonassociative triple.
5. Prove that if ◦is an associative operation on S then also the operation
◦on V deﬁned in the proof of Theorem 13.6.2 is associative.
6. CS Implement the randomized algorithm for associativity testing. Com-
pare its speed with the straightforward O(n3) algorithm for various
inputs.
7. Let S be a ﬁnite set with a binary operation ◦. For a set G ⊆S,
deﬁne G(1) = G and G(k+1) = G(k) ∪{a ◦b: a, b ∈G(k)}, and put
⟨G⟩= ∞
k=1 G(k). A set G ⊆S is called a generator set if ⟨G⟩= S.
(a) Show that if G is some generator set and (a ◦g) ◦c = a ◦(g ◦c)
holds for any a, b ∈S and any g ∈G then ◦must be associative.
(b) ∗Call the operation ◦cancellative if for every a, b ∈S, there exists
a unique x ∈S with a◦x = b and a unique y ∈S with y ◦a = b. Prove
that if ◦is cancellative, A ⊂S is a nonempty set, and b ∈S \⟨A⟩then
|⟨A ∪{b}⟩| ≥2|⟨A⟩|.
(c) Using (b), prove that if |S| = n and ◦is cancellative then a gen-
erator set of size at most log2 n + 1 exists and CS,∗it can be found in
O(n2 log n) time.
Hence, for a cancellative operation ◦, (a) and (c) together give an
associativity testing algorithm with no randomness involved and with
O(n2 log n) running time.
8. (Final exercise)
(a) Having read the book, can you now give solutions to all the prob-
lems raised in Section 1.1?
(b) Explain a solution of at least one of them to an educated nonspe-
cialist (to one who has only learned some high-school mathematics,
say)! This exercise may be one of the most important ones in this
book for the student’s future career.

This page intentionally left blank 

Appendix: Prerequisites
from algebra
This is a microcourse of matrices, vector spaces, ﬁelds, and a few
other things from algebra that are used at some places in the book.
Unlike other chapters, this part is not meant to be a textbook in-
troduction to the subject. It is mainly intended for the reader who
has some knowledge of the subject but may have forgotten the exact
deﬁnitions or may know them in a slightly diﬀerent form.
Matrices.
A matrix is a rectangular table of numbers. The ent-
ries may be real numbers, complex numbers, or sometimes elements
of other algebraic structures. An m × n matrix has m rows and n
columns. If a matrix is called A, then its entry in the ith row and
jth column is usually denoted by aij. So, for example, a 3×4 matrix
A has the general form



a11
a12
a13
a14
a21
a22
a23
a24
a31
a32
a33
a34


.
Note that a matrix is denoted by writing large parentheses to enclose
the table of elements.
A matrix is multiplied by a number α by multiplying each entry
by α. Two m × n matrices A and B are added by adding the cor-
responding entries. That is, if we denote C = A + B, we have
cij = aij + bij for i = 1, 2, . . . , m and j = 1, 2, . . . , n.
Matrix multiplication‘ is more complicated. A product AB, where
A and B are matrices, is deﬁned only if the number of columns of A
is the same as the number of rows of B. If A is an m × n matrix and
B is an n × p matrix, then the product C = AB is an m × p matrix
given by
cij = ai1b1j + ai2b2j + · · · + ainbnj.
Pictorially,

396
Appendix: Prerequisites from algebra
n
m
n
p
row i
column
j
·
=
A
B
C
cij
m
p
If A is an m × n matrix and x = (x1, x2, . . . , xn) is a vector, we
may regard x as an n × 1 matrix (so we think of the vector as being
written in a column) and consider the product Ax as a product of
two matrices.
If A is an m×n matrix then AT denotes the n×m matrix having
the element aji in the ith row and jth column. The matrix AT is
called the transposed matrix A. For example, if x = (x1, x2, . . . , xn)
is regarded as a column vector, i.e. as an n × 1 matrix, then xT is a
1×n matrix or a row vector. (In the literature, vectors are sometimes
regarded as row vectors even without the transposition symbol.) If x
and y are n-component (column) vectors, the product xT y is a 1 × 1
matrix, i.e. a single number, called the scalar product of x and y.
Explicitly, we have xT y = n
i=1 xiyi. The vectors x and y are called
orthogonal if xT y = 0.
For transposing the matrix product, we have the formula (AB)T =
BT AT .
Here is some more terminology for matrices. A square matrix is an
n×n matrix, i.e. one with the same number of rows and columns. The
diagonal (sometimes called the main diagonal) of an n × n matrix
A consists of the elements a11, a22, a33, . . . , ann; see the schematic
ﬁgure:
An upper triangular matrix is one having 0s everywhere below the
diagonal, i.e. with aij = 0 whenever i > j. A diagonal matrix
may only have nonzero entries on the diagonal, i.e. aij = 0 for
i ̸= j. The n × n identity matrix In has 1s on the diagonal and 0s
elsewhere.
Determinants. Every square matrix A is assigned a number det(A)
called the determinant of A. The determinant of A is deﬁned by the

Appendix: Prerequisites from algebra
397
formula
det(A) =

π∈Sn
sgn(π)
n

i=1
ai,π(i),
where the sum is over all permutations π of the set {1, 2, . . . , n}, and
where sgn(π) denotes the sign of a permutation π. The sign of any
permutation is either +1 or −1, and it can be compactly deﬁned as
the sign of the expression

1≤i<j≤n

π(j) −π(i)

.
For example, the determinant of a 2 × 2 matrix A equals a11a22 −
a12a21. This is also almost the only case when the determinant is
computed from the deﬁnition. For larger matrices, the determinant
is calculated by various other methods. Such methods are usually
based on the rules for determinants listed below:
1. The determinant of an upper triangular matrix (in particular,
of a diagonal matrix) equals the product of the entries on the
diagonal.
2. (“Elementary row operation I”) If all entries in some row of a
matrix A are multiplied by some number α then the determinant
is multiplied by α. In particular, if A has a row of all zeros then
det(A) = 0. Similarly for a column.
3. (“Elementary row operation II”) The determinant is not changed
by adding a multiple of one row to another row. That is, if i ̸= k
are row indices and if the entry aij is replaced by aij + αakj for
j = 1, 2, . . . , n, where α is a ﬁxed number, then the determinant
remains the same. Similarly for columns.
4. If A, B are n × n matrices then det(AB) = det(A) det(B).
5. (“Expanding the determinant according to a row”) For any row
index i, we have the formula
det(A) =
n

j=1
(−1)i+jaij det(Aij),
where Aij denotes the matrix arising from A by deleting the ith
row and the jth column.
A square matrix A is called nonsingular if det(A) ̸= 0. A subma-
trix of a matrix A is any matrix that can be obtained from A by delet-
ing some rows and/or columns of A. The rank of a matrix A, denoted

398
Appendix: Prerequisites from algebra
by r(A), is the maximum number k such that a nonsingular k × k
submatrix of the matrix A exists (below, an alternative deﬁnition of
rank will be given). For the product and sum of matrices, we have the
inequalities r(AB) ≤min(r(A), r(B)) and r(A + B) ≤r(A) + r(B).
Groups and fields. Let X be a set. A binary operation on X is a
mapping of X × X into X; that is, any two elements a, b ∈X are
assigned some element of X (see also Section 13.6). For example,
addition and multiplication are binary operations on the set of all
natural numbers (while subtraction is not). Binary operations are
usually written using symbols like ∗, ◦, +, ·, etc., so for example if
∗is a binary operation, we write a ∗b for the element assigned by ∗
to the pair (a, b).
A binary operation ∗is called associative if we have a ∗(b ∗c) =
(a ∗b) ∗c for all a, b, c ∈X. A binary operation ∗is commutative
if a ∗b = b ∗a for all a, b ∈X. Practically all binary operations
one encounters in everyday mathematics are associative, but there
are common and important operations that are not commutative,
such as the multiplication of n × n matrices or the composition of
mappings on some set.
A group is a set G with an associative binary operation ∗satisfy-
ing the following axioms (there are many equivalent versions of the
axioms):
(i) There exists an element e ∈G, called the unit element or the
neutral element, such that a ∗e = e ∗a = a holds for all a ∈G.
(ii) For every a ∈G, there exists an element b ∈G with a ∗b =
b ∗a = e, where e is the unit element. Such a b is called the
inverse of a and it is usually denoted by a−1.
Some important examples of groups are the integers with the
operation of addition, the set of all positive reals with the operation
of multiplication, the set of all nonsingular n × n real matrices with
multiplication, the set of all permutations on a given set X with
the operation of composition, and the set of all rotations around
the origin in 3-dimensional space with the operation of composition
(performing one rotation and then another). On the other hand, the
natural numbers with addition or the real numbers with multiplic-
ation do not form a group.
A ﬁeld is a set K with two binary operations + and · (here these
symbols need not denote the usual operations with numbers), such

Appendix: Prerequisites from algebra
399
that the following hold:
1. The set K with the operation + is a commutative group with
the unit element denoted by 0.
2. The set K \ {0} with the operation · is a commutative group
with the unit element denoted by 1.
3. Distributivity holds: a · (b + c) = (a · b) + (a · c) for any a, b,
c ∈K.
The multiplication a · b is often written just ab. The most important
ﬁelds are the rational numbers, the real numbers, and the complex
numbers (with addition and multiplication as operations). On the
other hand, the integers with addition and multiplication do not
form a ﬁeld.
If K is a ﬁeld and L is a subset of K that contains 0 and 1 of K,
is closed under both the operations of K (i.e a + b ∈L and a · b ∈L
holds for any a, b ∈L), and is a ﬁeld with these operations then L
is called a subﬁeld of K. For example, the ﬁeld of the rationals is a
subﬁeld of the reals.
In combinatorics, we are often interested in ﬁnite ﬁelds. A ﬁnite
ﬁeld with q elements, if one exists, is commonly denoted by GF(q).
The simplest ﬁnite ﬁeld is GF(2), which is just the set {0, 1} with the
usual multiplication of integers and with addition given by 0 + 0 =
1 + 1 = 0, 0 + 1 = 1 + 0 = 1. A ﬁnite ﬁeld GF(q) is known to exist
if and only if q is a power of a prime number. If q is a prime, GF(q)
can be easily described: it is the set {0, 1, 2, . . . , q −1}, where the
addition and multiplication are performed as for integers, but after
adding or multiplying two numbers we take the remainder of the
result after dividing it by q (we say that we do arithmetic modulo q).
The above deﬁnition of GF(2) is a special case of this construction.
On the other hand, if q = pk for a prime p and k > 1, then the
set {0, 1, . . . , q −1} with arithmetic modulo q is not a ﬁeld, and the
construction of GF(q) is diﬀerent and more complicated.
Vector spaces. Let K be a ﬁeld (most of the time one can think
of the real numbers). A vector space over K is a commutative group
V with an operation + and neutral element 0 plus a mapping (ope-
ration) assigning to every pair (α, v) with α ∈K and v ∈V an
element of V . That is, any element of V can be “multiplied” by any
element of K. This multiplication is usually written without any
multiplication sign, so we write just αv for v multiplied by α. We

400
Appendix: Prerequisites from algebra
require the following conditions, for any u, v ∈V and any α, β ∈K:
α(u + v) = αu + αv, (α + β)v = αv + βv (these are two kinds of
distributivity), α(βv) = (αβ)v (a kind of associativity), and 1v = v.
From this one can prove various other properties, such as 0v = 0
(the left 0 is in the ﬁeld K and the right 0 in the vector space V ).
The elements of V are called vectors.
The most usual and most important example of a vector space
is one consisting of all n-tuples of real numbers (for some given int-
eger n), with componentwise addition and multiplication by a real
number. This is a vector space over the ﬁeld of real numbers, but
similarly one can consider the vector space of all n-tuples of elements
of some ﬁeld K, which gives a vector space over K. This vector space
is usually denoted by Kn.
A set A ⊂V is called linearly dependent if there exist elements
v1, v2, . . . , vn ∈V , n ≥1, and elements α1, α2, . . . , αn ∈K, not all
of them 0, with α1v1 + α2v2 + · · · + αnvn = 0. If A is not linearly
dependent it is called linearly independent.
The maximum possible cardinality of a linearly independent set
in V is called the dimension of V , and any linearly independent set
in V of that cardinality is called a basis of V . The vector space Kn
has, as everyone would expect, dimension n.
Every vector space has a basis, and every inclusion maximal lin-
early independent set is a basis. In combinatorics, we almost always
deal with vector spaces of a ﬁnite dimension. If (e1, e2, . . . , en) is a ba-
sis of an n-dimensional vector space V , then any vector v ∈V can be
uniquely represented as v = n
i=1 αiei for some α1, α2, . . . , αn ∈K.
Here α1, . . . , αn are called the coordinates of v with respect to the
basis (e1, . . . , en). Every vector v ∈V is thus assigned a vector from
Kn, namely the n-tuple of its coordinates. In this way, V is put
into a bijective correspondence with the vector space Kn (there are
many such correspondences, depending on the particular choice of
a basis). All n-dimensional vector spaces over a given ﬁeld are “the
same”, i.e. they are all isomorphic in a suitably deﬁned sense. Hence
Kn can be viewed as the n-dimensional vector space over K, and
vectors can usually be thought of as n-tuples of numbers. On the
other hand, sometimes it is convenient to work with other versions
of n-dimensional vector spaces over K as well.
A subspace of a vector space V is a subset W ⊆V that is closed
on addition and on multiplication by elements of K, i.e. such that
u+v ∈W and αu ∈W holds for any α ∈K and u, v ∈W. If X ⊆V

Appendix: Prerequisites from algebra
401
is a set of vectors, then the subspace generated by X or the linear
span of X is the smallest subspace of V containing X. Explicitly, it
is the set {n
i=1 αivi : n ∈N, α1, . . . , αn ∈K, v1, . . . , vn ∈X}.
The rank of an m × n matrix A, with entries from a ﬁeld K,
equals the dimension of the subspace generated by the rows of A
(considered as n-element vectors) in the space Kn. It also equals the
dimension of the subspace generated by the column vectors of A in
Km. In fact, this deﬁnition of rank can be regarded as a “more basic”
one than the deﬁnition via determinants given above.
If V and W are vector spaces over a ﬁeld K, we deﬁne a linear
mapping of V into W as a mapping f : V →W satisfying f(αu) =
αf(u) and f(u + v) = f(u) + f(v) for any α ∈K and u, v ∈V . The
kernel of f is the set ker(f) = {v ∈V : f(v) = 0}. The kernel of f is
a subspace of V and the set f(V ) is a subspace of W. The dimension
of f(V ) is called the rank of f. For any linear mapping f : V →W,
we have dim ker(f) + dim f(V ) = dim V .
Let (e1, e2, . . . , en) be a basis of V and (f1, f2, . . . , fm) a basis of
W. Then linear mappings f : V →W are in a bijective correspon-
dence with the m×n matrices with entries from K. As we know, any
vector v ∈V is uniquely written v = n
i=1 αiei. Any linear mapping
f : V →W can be expressed as follows:
f(v) = f

n

i=1
αivi

=
m

j=1

n

i=1
ajiαi

fj,
where aij are the elements of the m×n matrix A corresponding to f.
In other words, if α is the column vector of coordinates of v with
respect to the basis (e1, . . . , en) and β denotes the column vector of
coordinates of f(v) with respect to the basis (f1, . . . , fm) then we
have β = Aα.
Thus, linear mappings can be regarded as abstract versions of
matrices. Multiplication of matrices corresponds to composition of
linear mappings. The rank of a linear mapping coincides with the
rank of its matrix (with respect to any choice of bases).
In conclusion, let us remark that many notions of linear
algebra have nice geometric interpretations in the usual Euclidean
space which may be helpful for understanding both geometry and
linear algebra. Especially for applications in discrete mathematics,
one should learn linear algebra together with these geometric
connections.

Bibliography
Basic textbooks
[1] N. Biggs: Discrete Mathematics, Revised edition, Clarendon Press,
Oxford, 1989.
[2] B. Bollob´as: Graph Theory. An Introductory Course, corrected 3rd
printing, Graduate Texts in Mathematics 63, Springer-Verlag, Berlin,
1990.
[3] P. Cameron: Combinatorics: Topics, Techniques, Algorithms, Cam-
bridge University Press, Cambridge, 1994.
[4] R. Diestel: Graph Theory, Graduate Texts in Mathematics 173,
Springer-Verlag, Berlin, 1996.
[5] P. Flajolet, R. Sedgewick: Analytic Combinatorics, to appear; a pre-
liminary version was available on the web page of P. Flajolet in summer
2006.
[6] R. Graham, D. Knuth, and O. Patashnik: Concrete Mathemat-
ics: A Foundation for Computer Science, Addison-Wesley, Reading,
Massachusetts, 1989.
[7] J. H. van Lint and R. M. Wilson: A Course in Combinatorics, Cam-
bridge University Press, Cambridge, 1992.
[8] L. Lov´asz: Combinatorial Problems and Exercises, 2nd edition,
Akad´emiai Kiad´o, Budapest, and North-Holland, Amsterdam, 1993.
[9] I. Stewart and D. Tall: The Foundations of Mathematics, Oxford
University Press, Oxford, 1977.
Further reading on related topics
[10] S. Arora and B. Barak: Complexity Theory: A Modern Approach,
preliminary version available on the web (September 2006), scheduled
to appear in 2007.
[11] A. Aho, J. Hopcroft, and J. Ullman: The Design and Analysis of
Computer
Algorithms,
Addison-Wesley,
Reading,
Massachusetts,
1983.
[12] N. Alon and J. Spencer: The Probabilistic Method, 2nd edition, John
Wiley, New York, 2000.
[13] L. Babai and P. Frankl: Linear algebra methods in combinatorics
(Preliminary version 2), Department of Computer Science, The Uni-
versity of Chicago, 1992.

Bibliography
403
[14] E. R. Berlekamp, J. H. Conway, and R. K. Guy: Winning Ways for
Your Mathematical Plays. Vol. 1: Games in general. Vol. 2: Games in
particular. Academic Press, London, 1982.
[15] N. Biggs: Algebraic Graph Theory, 2nd edition, Cambridge University
Press, Cambridge, 1993.
[16] J. M. Borwein and P. B. Borwein: Pi and the AGM, John Wiley, New
York, 1987.
[17] G. Bredon: Topology and Geometry, Graduate Texts in Mathematics
139, Springer-Verlag, Berlin, 1993.
[18] D. Cox, J. Little, and D. O’Shea: Ideals, Varieties, and Algorithms. An
Introduction to Computational Algebraic Geometry and Commutative
Algebra, 2nd edition, Springer-Verlag, Berlin, 1996.
[19] T. Gowers: Mathematics. A Very Short Introduction, Oxford Univers-
ity Press, Oxford 2002.
[20] G. R. Grimmett and D. R. Stirzaker: Probability and Random Pro-
cesses, 2nd edition, Oxford University Press, Oxford, 1992.
[21] F. Harary and E. M. Palmer: Graphical Enumeration, Academic Press,
New York and London, 1973.
[22] N. Koblitz: A Course in Number Theory and Cryptography, 2nd
edition, Graduate Texts in Mathematics 114, Springer-Verlag, Berlin,
1994.
[23] J. Littlewood: A Mathematician’s Miscellany, London, Methuen, 1953.
[24] B. Mohar and C. Thomassen: Graphs on Surfaces, Johns Hopkins
University Press, Baltimore, Maryland, 1997.
[25] R. Motwani and P. Raghavan: Randomized Algorithms, Cambridge
University Press, New York, 1995.
[26] J. G. Oxley: Matroid Theory, Oxford University Press, Oxford, 1992.
[27] J. Pach and P. K. Agarwal: Combinatorial Geometry, John Wiley,
New York, 1995.
[28] A. de Saint-Exup´ery: Le petit prince, English translation The Little
Prince, Harcourt Brace, San Diego, California, 1993.
[29] J. Stillwell: Classical Topology and Combinatorial Group Theory,
Graduate Texts in Mathematics 72, Springer-Verlag, Berlin, 1980.
[30] W. T. Trotter: Combinatorics and Partially Ordered Sets: Dimension
Theory, Johns Hopkins Series in the Mathematical Sciences, Johns
Hopkins University Press, Baltimore, Maryland, 1992.
[31] G. M. Ziegler: Lectures on Polytopes, Revised edition, Graduate Texts
in Mathematics 152, Springer-Verlag, Berlin, 1998.
Other references
[32] G. E. Andrews: The Theory of Partitions (Encyclopedia of math-
ematics and its applications, vol. 2), Addison-Wesley, Reading,
Massachusetts, 1976.

404
Bibliography
[33] D. Bayer and P. Diaconis: Trailing the dovetail shuﬄe to its lair,
Annals of Applied Probability 2(1992), No. 2, 294–313.
[34] Th.
Beth,
D.
Jungnickel,
and
H.
Lenz:
Design
Theory,
B. I.
Wissenschaft Verlag, Mannheim, Wien, Z¨urich, 1985.
[35] K.
Chandrasekhar:
Introduction
to
Analytic
Number
Theory,
Springer-Verlag, Berlin, 1968.
[36] P. Flajolet, B. Salvy, and P. Zimmerman: Automatic average-case
analysis of algorithms, Theoretical Computer Science 79(1991), 37–
109.
[37] G. Gonthier: A computer-checked proof of the Four Color Theorem,
Technical Report, Microsoft Research, Cambridge, United Kingdom.
[38] R. L. Graham, M. Gr¨otschel, and L. Lov´asz (editors): Handbook of
Combinatorics, Vol. 1–2, North-Holland, Amsterdam, 1995.
[39] G. Kant: Drawing planar graphs using the canonical ordering, Alg-
orithmica 16(1996), 4–32.
[40] D. Karger, P. Klein, and R. Tarjan: A randomized linear-time alg-
orithm to ﬁnd minimum spanning trees, Journal of the ACM 42(1995)
321–328.
[41] D. E. Knuth: The Art of Computer Programming, Vol. I: Fundamental
Algorithms, Addison-Wesley, Reading, Massachusetts, 1968.
[42] S. Rajagopalan and L. J. Schulman: Veriﬁcation of identities, SIAM
J. Computing 29,4(2000) 1155–1163.
[43] N. Robertson, D. P. Sanders, P. D. Seymour, and R. Thomas: The Four
Color Theorem, Journal of Combinatorial Theory Ser. B 70(1997),
2–44.
[44] N. Robertson, P. D. Seymour, and R. Thomas: Permanents, Pfaf-
ﬁan orientations, and even directed circuits, Ann. Math. 150(1999),
929–975.
[45] M. Sharir and P. K. Agarwal: Davenport–Schinzel Sequences and
Their Geometric Applications, Cambridge University Press, Cam-
bridge, 1995.
[46] I. Stewart: Mathematical recreations, Scientiﬁc American 272(1995),
No. 1, 76–79.
[47] C. Thomassen: The Jordan–Schoenﬂies theorem and the classiﬁca-
tion of surfaces, American Mathematical Monthly 99(1992), No. 2,
116–130.

This page intentionally left blank 

406
Bibliography
“Modul X” (drawing by Jiˇr´ı Naˇceradsk´y and Jaroslav Neˇsetˇril).

Hints to selected exercises
1.2.2. Denote the right-hand side by m. Hence m is the unique
integer satisfying m2 ≤⌊x⌋< (m + 1)2. This holds if and only if
m2 ≤x < (m + 1)2, and hence m = ⌊√x⌋.
1.2.3(d). See Section 12.4, Exercise 4.
1.2.7. Write each number in the form 2k(2m + 1), k, m ≥0 inte-
gers. Since m can attain at most 500 distinct values, the considered
set contains two numbers of the form 2k(2m + 1) and 2k′(2m + 1),
k < k′.
1.3.3(a). Adding the next line to n lines already present slices ex-
actly n + 1 of the existing regions into two.
1.3.3(b). Use (a) in an induction step. The result is (n3 +
5n + 6)/6.
1.3.5. Prove that the numerator decreases in each step.
1.3.6. Cut the chessboard into four 2n−1 × 2n−1 parts. Place one
L-shape so that it has one square in each part, except for the one
with a missing square.
1.3.7(b). The strategy is to invert the previous moves of the ﬁrst
player. In proving that it works, proceed by contradiction (if there
was a repetition caused by the second player, ﬁnd an earlier repeti-
tion caused by the ﬁrst player).
1.3.8. Induction on n. Let R be the lowest row initially containing a
black square, and let C be the leftmost such column. By the inductive
hypothesis, after moment n −1 all squares above R are white, and
also all squares to the right of C. The only possible remaining black
square at the intersection of R and C disappears at the moment n.
1.3.9. 4. By induction on m, describe the distribution of particles at
time 2m −1.
1.3.10. Deﬁne x1 = min(M), xi+1 = min(M \ {x1, . . . , xi}). Should
M be inﬁnite, the set {x1, x2, . . .} would have no largest element.
1.3.12. For instance, ﬁrst show that if 1 < ni ≤nj, then the sum of
squares doesn’t decrease by replacing ni by ni −1 and nj by nj + 1.

408
Hints to selected exercises
Then prove that by repeating this operation, any initial values of
the ni can be converted to the situation described in the incomplete
solution.
1.3.13. One possibility is by induction on n. Suppose the claim holds
for all n < n0, and consider some M for n = n0. Set M′ = M \
{[1, n]}. If no interval of M′ contains 1 then |M′| ≤n −2 by the
inductive hypothesis. Otherwise let q < n0 be largest with [1, q] ∈
M′, let M1 = {I ∈M ′ : I ⊆[1, q]}, and M2 = M ′ \ M1. Use the
inductive hypothesis for M1 and M2.
1.3.14. k⌊n/2⌋. To prove m ≤k⌊n/2⌋, use the fact that each Ij
contains at least one even number.
1.4.7(c). It suﬃces to prove that there is no bijection from the set
{1, 2, . . . , n} onto a proper subset A ⊂{1, 2, . . . , n}. Proceed by in-
duction on n. The case n = 1 is clear. Suppose there were such a
bijection f : {1, 2, . . . , n} →A, n > 1. If f(n) = n or n ̸∈A then f
restricted to {1, 2, . . . , n −1} gives a bijection of {1, 2, . . . , n −1} to
its proper subset. If f(n) = i ̸= n and f(j) = n for some j < n then
deﬁne g(j) = i, g(k) = f(k) for k ̸= j, n. This g is again a bijection
of {1, 2, . . . , n −1} on its proper subset.
1.5.3(a). There are only ﬁnitely many relations on X.
1.5.3(c). Take (N, <).
1.5.4(b). The pairs related by R◦S correspond to the nonzero entries
of the matrix ARAS.
1.6.9(b). If the considered congruence ∼is not the diagonal, let
q = min{|x −y|: x ̸= y, x ∼y}. Prove that ≡q coincides with ∼.
1.6.9(c). Yes in (a), no in (b) (make all even numbers equivalent,
while each odd number is only equivalent to itself).
2.1.2. Deﬁne a relation S = {(x, x): x ∈X}∪R∪R◦R∪R◦R◦R∪. . .,
prove that this is an ordering.
2.1.3. For instance, the relation of immediate predecessor is empty
for the set of all rational numbers with the usual ordering.
2.1.4(b). A ﬁnite linearly ordered set has a (unique) largest ele-
ment. Map it to the largest element of the other set, and proceed by
induction.
2.1.4(c). The natural ordering, and the opposite ordering (1 ≻2 ≻
3 ≻. . .).

Hints to selected exercises
409
2.1.4(d). Uncountably many: partition N into inﬁnite subsets S1,
S2, . . ., place S2 after S1, S3 after S2, . . . ; each Si is ordered either
as the natural ordering of N or as its reverse.
2.2.9(d). Show that for any A ⊆X, we have inf A = sup{x ∈
X : x ≤a for all a ∈A}.
2.4.7. Induction on |X|. Let x be a minimal element and let y be a
minimal element among those with x ≺y (if no y exists, then X is
an independent set). Let A be an independent set in X \ {x, y} of
maximum possible size. If |A| < α, we use the inductive hypothesis.
Otherwise, we decompose the sets {t ∈X : ∃a ∈A t ≤a} and
{t ∈X : ∃a ∈A t ≥a} into α chains each, and we connect these
chains using the elements of A.
3.1.2. Encode a pair (A, B) by a mapping f : {1, 2, . . . , n} →
{0, 1, 2}: an element x ∈A receives value 2, x ∈B \ A value 1,
and x ̸∈B value 0.
3.1.5(b). Choose the ﬁrst column as any nonzero vector over the
q-element ﬁeld. The second column must not be a multiple of the
ﬁrst, which excludes q vectors. The third column must not be a linear
combination of the ﬁrst 2 columns, and this excludes q2 possibilities
(no two distinct linear combinations of the ﬁrst 2 columns give the
same vector—check!). In general, there are qn −qi possibilities for
the ith column.
3.1.6. Pair up each divisor d with n/d. If n is not a perfect square
then this partitions all divisors into disjoint pairs.
3.2.1. (n −1)!.
3.2.3(b). What is the order of a permutation with a single cycle of
length k? What if there are two cycles of length k1 and k2?
3.2.5(a). Show that if a permutation with k increasing segments is
written backwards, we get a permutation with n + 1 −k increasing
segments.
3.2.5(b). Consider a permutation of the set {1, 2, . . . , n −1} with k
segments (in a one-line notation), and insert the number n into it
on some of the possible n + 1 positions. For how many positions will
the resulting permutation have k + 1 segments, and for how many k
segments?
3.2.5(c). A general formula, derived by Euler, is f(n, k) = k
j=0
(−1)j · (k −j)nn+1
j

.

410
Hints to selected exercises
3.2.5(d). Divide the permutations into classes according to the set
of numbers occurring on the ﬁrst k + 1 positions. All orderings of
the ﬁrst k + 1 numbers have the same probability. For how many
of the (k + 1)! possible orderings of the ﬁrst k + 1 numbers do we
get an initial increasing segment of length exactly k? The resulting
probability is k/(k + 1)!.
3.2.6(c). If the numbers are initially ordered according to a permu-
tation π, then exchanging two neighbors removes at most 1 inversion
of the current permutation.
3.2.7(b). How many among the numbers 1, 2, . . . , n are divisible by
pj, for j = 1, 2, . . .? The resulting formula is ⌊n/p⌋+⌊n/p2⌋+⌊n/p3⌋+
· · · .
3.3.3(b). The right-hand side is the number of (r + 2)-tuples of
nonnegative integers satisfying the equation X1 + X2 + · · · + Xr+2 =
n −r. Divide these (r + 2)-tuples into n −r groups according to the
value of Xr+2, and calculate the number of solutions in each group
separately. This yields the left-hand side.
3.3.4. Transform the kth term into
n
m
n−m
n−k

and take
n
m

in front
of the sum. The result is
n
m

2n−m.
3.3.5(a). Use 1
k
 k
m

= 1
m
 k−1
m−1

and formula (3.9).
3.3.5(b). This time k
 k
m

= (k + 1)
 k
m

−
 k
m

= (m + 1)
 k+1
m+1

−
 k
m

.
3.3.6. Let M be an m-element set and N an n-element set. Both sides
count the number of ordered pairs (X, Y ) with X ⊆M, Y ⊆N ∪X,
and |Y | = m. For the left-hand side, we ﬁrst select X and then Y ,
and for the right-hand side, we ﬁrst pick Y ∩N, then Y ∩M, and
ﬁnally X.
3.3.7. Set ki = f(i + 1) −f(i), i = 0, 1, . . . , n, where we artiﬁ-
cially put f(0) = 1, f(n + 1) = n. The desired number is the num-
ber of nonnegative integer solutions to the equation k0 + k1 + · · · +
kn = n −1.
3.3.9. Consider such a subset {a1, . . . , ak}, a1 < a2 < · · · < ak.
Assign the k-tuple {a1, a2−1, a3−2, . . . , ak−k+1} to it. This given a
bijection
between
all
subsets
of
the
required
kind
and
{1,2,...,n−k+1}
k

.
3.3.10(b). The result is
n+1
2
2.

Hints to selected exercises
411
3.3.10(c). Calculate the area of the big square in two ways. Its side
is 2(1 + 2 + 3 + · · · + n). The k-th “layer” (counted from the center)
consists of 4k squares of size k × k.
3.3.15(b). In the formula (3.1) for the binomial coeﬃcient, exactly
one of the factors in the numerator is divisible by p. So
n
p

is divisible
by p if and only if this factor is divisible by p2 too.
3.3.16(a). Substitute the following values into the binomial theorem:
x = 1, x = −1, x = i, and x = −i (i = √−1 stands for the imaginary
unit). Add the resulting equations together. Rewrite the nth pow-
ers on the right-hand side using de Moivre’s formula. The required
number is given by the remarkable expression 2n−2 + 2n/2−1 cos πn
4 .
Verify its correctness for small values of n.
3.3.16(b). Proceed as in (a) but substitute the 3 (complex) roots of
the equation x3 = 1 for x.
3.3.19. First arrange the elves, which determines eight possible po-
sitions for the goblins, pick 5 of these positions, and ﬁnally arrange
the goblins.
3.3.23. Those with A1 as a vertex:
n−k−1
k−1

. All: this number
times n
k .
3.3.24(b). Show that there are always precisely 2 triangles sharing
2 sides with the n-gon. Starting from a triangle T0 sharing 2 sides
with the n-gon, we can pick a triangle T1 neighboring T0, then T2
neighboring T1, etc. For T0, we have n possibilities, and in each
subsequent steps there are 2 possibilities. Each of the considered
triangulations is obtained twice by this process, since there are 2
choices for the T0 to start with. The result is n2n−5.
3.3.26(a). The left-hand side is the number of distinct orderings of
n objects, with ki objects of the ith kind. The right-hand side: ﬁrst
we choose the kind of object at the ﬁrst position and then we arrange
the remaining objects.
3.3.27(a).
r1+r2+···+rk
r1,r2,...,rk

.
3.3.27(b). (n −1)!/ k
i=1(ri + 1), where n = |X| = 1 + k
i=1
(ri + 1).
3.4.2. For example, f(n) = n⌊n/2⌋, g(n) = n⌊(n+1)/2⌋−1/2.
3.4.4. |f| is bounded by a constant, |g| is bounded from below by a
positive constant, |h| is bounded by some polynomial function.
3.4.5. First, rewrite all the functions in the form ef(n).

412
Hints to selected exercises
3.5.2(b). Write 1 + 1
n = n+1
n
= 1/(1 −
1
n+1) ≥1/e−1/(n+1).
3.5.3. The line y = x+1 is a tangent to the graph of the function ex
at the point (0, 1). The function ex is convex, since its second deriv-
ative is nonnegative, and so it cannot intersect its tangent anywhere
else than at the point of tangency.
3.5.7(b). Put A = (x1 + x2 + · · · + xn−1)/(n −1). By AG(n)
applied to numbers x1, . . . , xn−1, and A, we have An = ((x1 +
x2 + · · · + xn−1 + A)/n)n ≥x1x2 . . . xn−1A, and from this we get
A ≥(x1 . . . xn−1)1/(n−1).
3.5.10. The function ln x is concave, and so each triangle with ver-
tices at points (i, ln i), (i+1, ln i), and (i+1, ln(i+1)) lies completely
below its graph (draw a picture). The area of this triangle is 1
2(ln(i+
1) −ln i). Hence ln n! ≤ln n +
 n
1 ln x dx −1
2
n−1
i=1 (ln(i + 1) −ln i).
A further formula manipulation leads to the required result.
3.5.11. Induction on n.
3.5.12. Lower bound: by induction; prove 2(√n + 1 −√n) ≤1/√n
by transforming the left-hand side to a fraction with denominator
√n + 1 + √n. Upper bound: similarly.
3.6.2(a). All such primes divide
2m
m

.
3.6.2(b). If P denotes the product of primes as in (a), we have
log2 P ≤2m, and on the other hand, P ≥mπ(2m)−π(m). This gives
π(2m) ≤π(m) + O(m/ ln m).
3.6.2(c). The largest power of p dividing n! has exponent ⌊n/p⌋+
⌊n/p2⌋+ · · · . Use the equation
2m
m

= (2m)!/(m!)2 and express the
diﬀerence of the maximum powers of p in the numerator and in the
denominator.
3.6.2(d). By (c), we have 22n/(n + 1) ≤
2n
n

≤(2n)π(2n).
3.7.2. For instance, one can proceed as in the second proof of the
inclusion–exclusion principle, i.e. consider the contribution of a single
element.
3.8.2(b). Choose the points that are ﬁxed. The rest is a permutation
without a ﬁxed point as in the hatcheck lady problem.
3.8.4. Partition all permutations into classes depending on the num-
ber of ﬁxed points. Express the number of permutations in each class
using the function D(k).
3.8.5(a). Consider a distribution of hats to n gentlemen without
ﬁxed points. Gentleman No. 1 exchanges the hat he received for his

Hints to selected exercises
413
hat with some gentleman No. i and leaves. If gentleman No. i received
his own hat in this exchange, he leaves too and we have a situation
with n −2 gentlemen. Otherwise we are left with n −1 gentlemen.
3.8.7(c). Let the mapping go onto the set {1, 2, . . . , m}. Let Ai be
the set of mappings that do not map anything to the element i. Use
inclusion–exclusion to determine |A1 ∪· · · ∪Am|, i.e. the number of
bad mappings. For m = n we must get n!.
3.8.7(d). An onto mapping f : N →M, where |M| = m, deﬁnes
an equivalence relation on N with m equivalence classes. Show that
each equivalence with m classes on N corresponds to exactly m!
mappings onto.
3.8.8(a). You can use the hint to (d) of the preceding exercise and
the result of (c) in that exercise.
3.8.8(b). We sum the result of (a) over all k. We obtain the following
result: n
k=0
1
k!
k
j=0(−1)jk
j

(k −j)n.
3.8.8(c). In the result of (b), write i for k −j, let k formally run
to ∞(this is possible since the inner sum, expressing the number of
equivalences with k classes on an n-element set, is 0 for k > n), and
change the order of summation.
3.8.11(b). Rewrite
each
term
in
the
product
as
1 + pi +
p2
i + · · · + pαi
i .
3.8.11(c). Let n = 2q r
i=1 pαi
i , pi odd primes. By (b), we must have
2n = t (pαi
i + pαi−1
i
+ · · · + 1), t = 2q+1 −1. After dividing this by
the expression t  pαi
i
we have 1+1/t = (1+1/pi+· · ·+1/pαi
i ). At
the same time, some pi divides t. In order that the right-hand side
is not larger than the left-hand side, we must have r = 1, t = p1.
3.8.12. For every prime pi ≤N, let Ai be the set of pairs (m, n)
such that pi|n and pi|m. Use inclusion–exclusion to determine the
number of bad pairs.
3.8.13(a). Deﬁne Ai as the set of all graphs in which the vertex i
has degree 0, and calculate |A1 ∪· · · ∪An|.
3.8.14. Deﬁne Ai as the set of all ways of seating in which the ith
couple is adjacent.
4.1.5(b). A simple counterexample arises as follows: take triangles
with vertex sets {1, 2, 3} and {4, 5, 6}, and add the edges {1, 4},
{2, 5}, and {3, 6}.
4.1.6. (2n −1)(2n −3)(2n −5) · · · · · 5 · 3.

414
Hints to selected exercises
4.1.8. Here is a way of constructing 2n2/2−O(n log n) nonisomorphic
graphs (this is almost as many as we got by the counting argument
in the text). Let n be large enough, and let m be the smallest integer
with 2m ≥n. Denote the n vertices by a, b, c, d, u0, u1, . . . , um−1, and
v0, v1, . . . , vn−m−5; write U = {u0, . . . , um−1}, V = {v0, . . . , vn−m−5}.
Connect a to b, and b to c, to d, and to all vertices of U. Choose some
asymmetric graph on U, say the path u0, u1, . . . , um−1 plus the edge
{u1, u3}. Connect the vertices c and d to every vertex of V (so a is
the only vertex of degree 1). Choose an arbitrary graph on V . Fi-
nally, connect each vertex vi ∈V to the vertices uj1, . . . , ujk, where
0 ≤u1 < u2 < · · · < uk ≤m −1 are the (uniquely determined)
numbers with 2j1 + 2j2 + · · · + 2jk = i (this corresponds to writing i
in binary). It is easy to check that for distinct graphs on V we obtain
nonisomorphic graphs on n vertices. Hence we have at least 2(n−m−5
2
)
nonisomorphic graphs on n vertices.
4.2.2. The most edges are clearly obtained if the components are
complete graphs. If their numbers of vertices are n1, . . . , nk,
 ni = k, we have
n1
2

+ · · · +
nk
2

edges and we need to maxi-
mize this expression as a function of n1, . . . , nk. Show that if, for
example, n1 ≥n2 > 1, the value doesn’t decrease by increasing n1
by 1 and decreasing n2 by 1. Hence the maximum is attained for
n1 = n −k + 1 and ni = 1 for i = 2, 3, . . . , n.
4.2.4. If there is no odd cycle, assume that the graph is connected,
and label the vertices by +1 and −1 by the following process: label an
arbitrarily chosen vertex by +1, and whenever v is a vertex adjacent
to a vertex labeled x label v by −x. Show that the whole graph
gets labeled in this way and no two vertices with the same label are
adjacent.
4.2.5. If v is a vertex with at least 2 neighbors it has to be connected
to all vertices of its component. Each component has to be K3 or
K1,n.
4.2.6. One component: any connected graph on ≤4 vertices, a star
K1,n, a star K1,n with one extra edge (forming a triangle), two stars
K1,n and K1,m whose centers are connected by an edge. (By the cen-
ter
of
a
star
we
mean
the
vertex
connected
to
all
others.)
4.2.8. Graph diameter is usually deﬁned as max{dG(u, v): u, v ∈
V (G)}, and radius as minv∈V (G) maxu∈V (G) dG(u, v).

Hints to selected exercises
415
4.2.11. For a given vertex vi, the entry at position (i, i) in A4
G is the
number of walks of length 4 starting and ending in vi. Each 4-cycle
in G appears 8× as such a walk. But such a walk may also pass the
same edge 4× (and each edge is counted twice for such walks), or
pass 2 edges with a common vertex 2× (and each pair of edges with
a common vertex appears 4×).
4.3.1. For instance, the ﬁrst graph has no cycle of length 4, the
second one 2 such cycles, and the third one 5 of them.
4.3.8(a). Never. Its score would have to be (0, 1, . . . , n −1), but if
there is a vertex adjacent to all other vertices no vertex may have
degree 0.
4.3.8(b). For all n ≥2. By induction, we construct graphs Gn with
score containing the numbers 0, 1, . . . , n −2, with ⌊(n −1)/2⌋re-
peated. G2 are 2 isolated vertices, Gn+1 arises by adding an iso-
lated vertex to the complement of Gn (the complement of a graph
G = (V, E) is the graph (V,
V
2

\ E)).
4.3.10. Consider the coloring with the smallest possible number of
monochromatic edges. If there were a monochromatic path of length
2, one could switch the color of its middle vertex.
4.3.11. Consider a path in G of maximum possible length. Its end-
vertex is connected to at least 2 other vertices of this path; this gives
the desired subgraph.
4.3.12. We must have k ≤n −1, and kn must be even. This is
also suﬃcient; for instance, set V = {0, . . . , n −1}, E = {{i, j} :
(i −j) mod n ∈S}, where S = {1, −1, 2, −2, . . . , k
2, −k
2} for k even,
S = {1, −1, . . . , k−1
2 , −k−1
2 , n
2 } for k odd, n even.
4.3.16. It suﬃces to show that if {u, v} ∈E(G) then degG(u) =
degG(v). Let U be the set of all neighbors of u except for v, and let
V be the set of all neighbors of v except for u. Each vertex in U
has 4 neighbors in V , and similarly each vertex of V has 4 neigh-
bors in U. Hence |U| = |V | (by double-counting the edges among U
and V ).
4.3.17. By contradiction. Suppose that any two vertices have an
odd number of common neighbors. For an arbitrary vertex v, look
at the subgraph induced by the neighbors of v. All degrees in this
subgraph are odd, and hence the degree of v is even. Let us count
walks of length 2 beginning at v. Their total number is even (because
every vertex has an even degree). At the same time, an odd number

416
Hints to selected exercises
of these walks come back to v, but any other vertex is reached by
an odd number of walks, and hence the number of vertices distinct
from v must be even—a contradiction.
4.4.6. The number of odd terms in the sequence is even.
4.4.9(a). Consider the longest path in G and the sets of neighbors
of its end-vertices.
4.4.9(b). In general, no (consider a suitable complete bipartite
graph).
4.5.7. Remove every knight with an ally sitting to his right. The
remaining knights are from alternate castles as we go around the
table, so their number is even, and the number of removed knights
is the same.
4.5.8. By induction on the number of vertices. For an arbitrary ver-
tex v, let T1 be the tournament induced by the vertices with arrows
going to v and T2 the tournament induced by the vertices with ar-
rows going from v. Take a path in T1, continue to v, and then by a
path in T2.
4.5.9. Induction on the number of vertices n. In a tournament T on
n vertices, delete a vertex u, and let v be a vertex as required in the
problem for the remaining tournament. If v cannot be reached from
u by a length 2 directed path in T then (v, u) ∈E(T) and also all
vertices with an arrow going to v have an arrow to u, and hence u
can be reached by at most 2 arrows from everywhere.
4.6.2(b). In Kn+1, subdivide each edge.
4.6.2(c). Let T be a tree (see Chapter 5) in which all vertices except
for leaves have degree n, and having a vertex at distance n from all
leaves (a complete n-ary tree). Make a copy T ′ of T, and identify
each leaf of T ′ with its pre-image in T.
4.6.3(a). Yes.
4.6.3(b). Yes.
4.6.4. For instance, by induction, using a synthesis of the graph by
successive “gluing of ears”.
4.6.5. It is k. Prove that any two vertices of the cube can be con-
nected by k vertex-disjoint paths.
4.6.6. Let A be a set of k vertices. Double-count the edges connect-
ing A to the other vertices, assuming that G −A has more than k
components.

Hints to selected exercises
417
4.6.7. For n = 2k −1, G must be a K2k−1. For n ≥2k, if there is
a vertex of degree ≤2k −3, delete it and use induction. Otherwise,
suppose that G minus some ≤k −1 vertices can be partitioned into
two nonempty parts G1 and G2 with no edges connecting them. Show
that the inductive hypothesis holds for G1 or for G2, for otherwise
G would have too few edges.
5.1.6. We have 2n −2 = 2|E(T)| = 
v∈V (T) degT (v) = n−1
i=1 ipi.
The required inequality follows by a simple manipulation.
5.1.8(c). Induction on n. Let n ≥3, and let (d1, . . . , dn) satisfy
the condition. Then there is some di = 1 and also some dj > 1
(for a simple notation, assume dn = 1, dn−1 > 1). By the inductive
hypothesis, we construct a tree with score (d1, . . . , dn−2, dn−1 −1)
and we connect a new end-vertex to the vertex number n −1.
5.2.5. Divide the sequences into two groups, those beginning with 0
and those beginning with 1, and sort both groups recursively. In an
actual implementation of the algorithm, the sequences themselves
are not actually moved; one only works with suitable pointers to
them.
5.2.6. The code of a tree on n vertices has length 2n. Hence there
are at most 4n distinct codes.
5.3.1. Prove that the mark of any single vertex is changed at most
(log2 n) times.
5.3.2. Let the graph G be given by lists Sv of edges containing each
vertex v ∈V (G). Maintain a doubly linked list N of all edges going
between the set Vi and its complement, and, moreover, for each edge
belonging to N keep a pointer to its occurrence in N. If the newly
added vertex yi has degree d then all the lists and pointers can be
updated in O(d) time.
5.3.3(a). Induction on the number of vertices. Show that for every
edge {v, v′} ∈E(T), an ordering v1, v2, . . . , vn of the vertices exists
such that v1 = v, vn = v′, and the distance of vi and vi+1 is at most
3, i = 1, 2, . . . , n −1.
5.4.1. Use Kruskal’s algorithm for the weight function −w.
5.4.4. The computation of Kruskal’s algorithm is determined
uniquely and it gives some minimum spanning tree. The proof of
correctness shows that any other spanning tree has a strictly larger
weight.

418
Hints to selected exercises
5.4.5. In each group of edges of the same weight, put the edges
of T ﬁrst. Let T ′ be the tree computed by Kruskal’s algorithm,
let e′
1, . . . , e′
n−1 be its edges in the order of their selection, and let
e1, . . . , en−1 be the edges of T numbered according to the chosen
ordering. For contradiction, let k be the smallest index with ek ̸= e′
k.
We get w(e′
k) < w(ek), but from the proof in the text we have
w(e′
i) ≤w(ei) for all i, and so T is not minimum.
5.4.6. Follows from Exercise 5.
5.4.8(a). If v has degree ≥7 then there exist edges {v, u1} and
{v, u2} with angle < 60 degrees. Show that one of them can be
replaced by the edge {u1, u2}.
5.4.8(b). Prove and use the fact that if ABCD is a convex quadran-
gle then |AB|+|CD| ≤|AC|+|BD| and |BC|+|AD| ≤|AC|+|BD|.
Show that two crossing edges may be replaced by two noncrossing
edges.
5.4.9. Cover the unit square by an √n × √n chessboard. Order the
squares of the chessboard into a sequence s1, s2, . . . , sn so that con-
secutive squares are adjacent. Construct a path on the given points
by ﬁrst going through all points in s1, then all points in s2, etc.
5.4.10(a). Let EM be a maximum matching and EG the matching
found by the greedy algorithm. To each edge e ∈EM assign the ﬁrst
edge of EG intersecting it. Each edge ˇe ∈EG is thereby assigned to
at most two edges e1, e2 ∈EM with w(e1), w(e2) ≤w(ˇe).
5.4.11(b). Let e1, . . . , ek be the edges selected by the greedy algo-
rithm and ˇe1, . . . , ˇet edges of some smallest edge cover. Let k1 =
|{i: ei ∩(e1 ∪· · · ∪ei−1) = ∅}|, and similarly t1 = |{i: ˇei ∩(ˇe1 ∪
· · · ∪ˇei−1) = ∅}|. We have |V | = k + k1 = t + t1. Key observation:
in the steps of the greedy algorithm for i > k1, at least one point of
each edge ˇej contributing to t1 must have been covered, and hence
k1 ≥1
2t1. From this we get k = t + t1 −k1 ≤t + 1
2t1 ≤3
2t.
5.4.12. Let V = {1, 2, . . . , 2k+2 −2} ∪{a1, a2, b1, b2, . . . , bk}. The
vertex a1 is connected to 1, 3, 5, . . .; a2 to 2, 4, 6, . . .; bi to 2i−1, 2i, . . . ,
2i+1 −2; a1, a2 are, moreover, connected to all the bi. The greedy
algorithm selects all the bi while the optimum dominating set is
{a1, a2}.
5.5.5(a). Prove that after the ith phase, every component has at
least 2i vertices.

Hints to selected exercises
419
5.5.5(b). For each component, maintain a list of its outgoing edges.
In each phase, each list can be traversed and the edge of minimum
weight in it can be found.
6.1.1(a). Fig. 6.1.
6.1.1(b). Graphs G1, G2 as in (a) can be glued together. For ins-
tance, remove an edge {a, b} adjacent to the outer face of G1, an
edge {c, d} adjacent to the outer face of G2, and add edges {a, c}
and {b, d}.
6.1.3. By induction on the number of edges. Consider a vertex v of
degree at least 4, and look at two adjacent edges incident to v (in
the circular order around v). If possible, choose these edges so that
they go into distinct components of G −v. Replace these two edges
by a single edge connecting the other ends. (It may be advantageous
to allow for graphs with multiple edges.)
6.2.6. On each 5-tuple of vertices we have a copy of K5, and some
two of its edges must cross. We thus obtain at least
n
5

pairs of
crossing edges. One pair of crossing edges has been counted in n −4
copies of K5.
6.2.7(a). Choose a disk D such that k ∩D is a segment. From any
point of R2 \ k we can reach D by a polygonal arc (going along k).
But D \ k has at most two equivalence classes since any two points
on the same side can be connected by a segment.
6.3.5. First, one has to handle the case when G is not 2-connected.
For a 2-connected G, prove that one can add a diagonal into a face
bounded by at least 6 edges. But one has to be careful to create no
triangle! Probably it is necessary to discuss several possible cases.
6.3.7(a). Let us say that a dot with k ≤3 outgoing edges contributes
3 −k degrees of freedom. Initially, there are 3n degrees of freedom,
ﬁnally at least 1, and each move reduces the number of degrees of
freedom by 1.
6.3.7(c). Show that in the ﬁnal position, there is exactly one “un-
used” arm of a cross protruding into each face. The total number of
unused arms remains constant during the game, and therefore there
are 4n faces in the end. Each move either connects two components
into one or increments the number of faces by 1.
6.3.8(a). By induction on n, prove e = v + f −1; when inserting a
new line distinguish newly created intersections and already existing
ones.

420
Hints to selected exercises
6.3.8(c). Derive e ≥3f −n from (b). If di is the number of lines
passing through the ith intersection, we calculate  di = e−n (since
each edge contributes by 2 ends, except for the 2n semiinﬁnite rays).
By substituting for f from (a) we ﬁnally get  di ≤3v −3.
6.3.9. Assume connectedness. Let a red–blue corner be a pair (f, v),
where f is a face and v is a vertex adjacent to f such that if we
go along the boundary of f clockwise (counterclockwise around the
unbounded face), v is preceded by a red edge and followed by a blue
edge. By double-counting and by Euler’s formula, show that there is
a vertex with at most one red–blue corner.
6.4.2. Proceed by induction on the number of vertices. For a given
graph, remove a smallest-degree vertex, color the rest, and put the
vertex back.
6.4.3. A subgraph of an outerplanar graph is outerplanar. Hence by
Exercise 2 it is enough to show that an outerplanar graph always has
a vertex of degree ≤2. This can be extracted from Euler’s formula
using the fact that some face has at least n edges adjacent to it.
6.4.4. Show that δ(G) ≤3 and use Exercise 2.
6.4.6(b). One can even take a tree.
6.4.6(c). Add suﬃciently many new leaves to adjust the degrees.
6.4.10(a). By induction on the number of edges. Consider a face F
and the set of edges EF lying on its boundary. Show that the degrees
of all vertices in the graph (V, EF ) are even. Delete the edges from
EF , color by i nduction, put EF back, and recolor the face F by the
other color.
6.4.10(b). Color the faces by 2 colors. The number of edges can be
counted as the sum of the circumferences of the faces of one color,
and also as the sum of circumferences of the faces of the other color.
But one way of counting gives a number divisible by 3, the other a
number not divisible by 3.
6.4.12(b). For one class, take k vertices v1, . . . , vk, where vi has
the list L(vi) = {k(i −1) + 1, k(i −1) + 2, . . . , ki}. For each set S
containing exactly one element from each of the L(vi), put a vertex
vS with the list L(vS) = S in the other class.
6.4.13. In the inductive step, distinguish two cases. If there is an edge
{vi, vj} with vi and vj lying on the cycle C but not consecutively,
split G along that edge. Otherwise, remove vk and adjust the lists of
its neighbors suitably.

Hints to selected exercises
421
7.1.1. Deﬁne a graph on the faces exactly as in the proof of Sperner’s
lemma.
7.1.2. Imagine two tourists on the same day: one ascending, one
descending.
7.1.4. Yes only for (b), (e). In (a), use a rotation around the center;
in (d), let f(x) be the point of the sphere antipodal to x.
7.1.5(a). Imitate the proof of the planar version. Connect by an
edge tetrahedra sharing a face labeled 1, 2, 3, and use the planar
version to show that the degree of the outer face is odd.
7.1.6(b). Ensure that a is much farther from c than b from d.
7.1.6(c). By contradiction. If Betty has a winning strategy, Alice
will make her ﬁrst move arbitrarily and ﬁnd herself essentially in
Betty’s situation (one marked node from the ﬁrst move can only
help). Then she will use Betty’s winning strategy.
7.1.7(b). Show that if Alice starts then Betty has a very simple
strategy to force a draw.
7.2.3. An independent set system M satisﬁes 
M∈M
 n
|M|
−1 ≤1,
and the system

X
⌊n/2⌋

gives equality. Hence all the sets in a system
M of largest size have size ⌊n/2⌋or ⌈n/2⌉(thus, for an even n we are
done). Show that if M has t sets of size ⌊n/2⌋and 0 < t <

n
⌊n/2⌋

then M has <

n
⌊n/2⌋

−t sets of size ⌈n/2⌉.
7.2.4. For an automorphism h, deﬁne a mapping f : X →X, where
f(x) is the y such that h({x}) = {y}. Show that h = f# (hence all
automorphisms come from permutations and there are n! of them).
7.2.6(a). Apply Sperner’s theorem on the set system {{i: εi =
1}:  εiai ∈(−1, 1)}.
7.2.7. Let n = p1p2 . . . pn be the decomposition of n into prime fac-
tors. To a divisor d = pi1pi2 . . . pik assign the set Md = {i1, . . . , ik} ⊆
{1, 2, . . . , n}; then d1|d2 ⇔Md1 ⊆Md2. Use Sperner’s theorem.
7.3.1. Imitate the proof for K2,2, the only diﬀerence being that one
vertex v contributes at most t −1 elements to M.
7.3.6. Instead of f(x) = x(x−1)/2 consider the function f(x) which
equals 0 for x ≤2 and x(x−1)(x−2)/6 for x > 2. Show it is convex,
and use this similarly as for excluded K2,2.
7.3.7(a). The graph with vertex set P ∪L and edges corresponding
to the pairs being counted contains no K2,2.

422
Hints to selected exercises
7.3.7(b). This time, the graph K2,3 is excluded.
8.1.1. Proceed as we did for the lower bound for the number of
nonisomorphic graphs in Section 4.1. Use Theorem 8.1.1 and the
estimate for n! from Theorem 3.5.5.
8.1.2. Count spanning trees containing a given edge of Kn. By sym-
metry, this number is the same for all edges. Use Cayley’s
formula.
8.1.3. From each spanning tree of Kn, we can remove one edge in
n −1 ways; this yields a pair of trees. Conversely, from a spanning
tree on some k-point subset of the vertices plus a spanning tree on
the remaining n −k vertices, a spanning tree of the whole Kn can
be manufactured by adding one edge, in k(n −k) ways.
8.1.4. Given a spanning tree T of G, consider the subgraph of G∗
consisting of the edges not crossed by the edges of T, and prove that
this is a spanning tree of G∗. See Lov´asz [8] for details.
8.2.1(a). It’s 0 for n odd. For n even we have
n
2 + 1 leaves and
n
2 −1 vertices of degree 3 (by induction). We thus sum the expression
(n −2)!/2n/2−1 over all choices of a vector (d1, . . . , dn) with di ∈
{1, 3} and  di = 2n −2. By the substitution ki = 1
2(di −1) we
see that the number of summands equals the number of (n
2 −1)-
element subsets of an n-element set, and therefore the answer is
(n −2)!

n
n/2−1

2−n/2+1.
8.2.1(b). Perhaps the easiest solution uses generating functions. The
answer is the coeﬃcient of x2n−2 in the expression (n −2)!(x + x2 +
1
2x3)n, which can be found using the multinomial theorem (part (a)
can be solved similarly as well).
8.2.2(a). Both sides of the equality count the number of pairs span-
ning trees (T, T ∗) where degT (n) = k, degT ∗(n) = k + 1, and T ∗
arises from T by the following operation: pick an edge {i, j} ∈E(T)
with i ̸= n ̸= j, delete it, and add either the edge {i, n} or the edge
{j, n}, depending on which of these edges connects the two compo-
nents of T −{i, j}. From one T we can get n −1 −k diﬀerent T ∗,
and one T ∗can be obtained from k(n −1) diﬀerent T.
8.2.2(c). 
k Nk happens to be the expansion of ((n −1) + 1)n−2
according to the binomial theorem.
8.3.3. Imagine that you send two pedestrians from the vertex i in the
directed graph of f. One of them traverses one arrow per minute, the

Hints to selected exercises
423
other one maintains the speed of one arrow in two minutes. Think
about the times the pedestrians will meet at the same vertex.
8.4.1. The inequality degT (i) ≥mi + 1 is easy to see. At the same
time, we have 
i mi = n −2 and 
i(degT (i) −1) = 2(n −1) −n =
n −2, and so equality must hold for all i.
8.5.1(a). All row sums of Q are 0.
8.5.1(b). Since T(G) > 0, we have det Q11 ̸= 0 by Theorem 8.5.1.
8.5.1(c). The sum of the rows of Q corresponding to the vertices of
a component of G is 0, and hence the vector space generated by the
rows of Q has dimension < n −1.
8.5.1(d). The kernel of the linear mapping x →Qx contains (1,
1, . . . , 1) because the sum of rows is 0, and the kernel is one-dimens-
ional by (b).
8.5.1(e). The product of the ith row of Q and the ith column of Q∗
is the expansion of det Q according to the ith row. Use det Q = 0.
Other entries in the product QQ∗are expansions of determinants for
matrices in which one row is repeated twice; hence also 0.
8.5.3. The result is nm−1mn−1.
8.5.4. For one implication, compute the determinant of the incidence
matrix of an odd-length cycle. The required property of M for bi-
partite graphs can be proved by induction, similarly as in the proof
of Lemma 8.5.3.
9.1.3(c). Possibilities: L = {X}, L = {X, {a}} for some a ∈X, and
L = {X \ {a}} ∪{{a, x}: x ∈X \ {a}} for some a ∈X. In a proof,
Exercise 4 may be useful.
9.1.4. Show that 2 points of L1 \L2 and 2 points of L2 \L1 together
form a conﬁguration F as in axiom (P0).
9.1.6. At most 4 sets can contain any given point.
9.1.7. Yes. Draw 8 lines in the plane in general position (no 2 parallel,
no 3 intersecting at a common point). Let the intersections represent
stops and the lines bus routes.
9.1.8(a). One pair is in at most one set. One set covers
n+1
2

pairs.
The total number of covered pairs comes out the same as the number
of all pairs, and hence all pairs must be covered.
9.1.8(b). If there were more such sets they have more than n2+n+1
points altogether.
9.1.8(c). Double-count the pairs (x, L), x ∈L, and use (b).

424
Hints to selected exercises
9.1.8(d). Count that there are n2 + n + 1 distinct lines intersecting
a given line.
9.1.9. Fix a point a ∈A. The n+1 lines containing it cover all points
of
X,
and
each
of
them
has
at
most
one
point
of
A
besides a.
9.1.10(a). Delete the points of one, arbitrarily chosen, line.
9.1.10(b). Transitivity: suppose A1 ∥A2, A2 ∥A3, and x ∈A1 ∩A3.
Then A1 and A3 are two lines passing through x and parallel to A2,
contradicting the third axiom.
9.1.10(c). For showing that two lines A, A′ have the same cardinal-
ity, construct a bijection using lines parallel to some line xy with
x ∈A and y ∈A′.
9.1.10(d). For each equivalence class of ∥, add one new point “at
inﬁnity”, add it to the lines from that class, and also make all the
new points into a line. Check the axioms.
9.3.4(a). To given t orthogonal Latin squares, add one square with
all the entries of the ith row equal to i, i = 1, 2, . . . , n, and one square
having all the entries j in the jth column, j = 1, 2, . . . , n.
9.3.4(b). In order that a liberated square be orthogonal to another,
it has to contain each i ∈{1, 2, . . . , n} exactly n×. Permute entries
of the given t+2 orthogonal liberated squares (the same permutation
in each square) in such a way that the ﬁrst square has all numbers
i in the ith row, i = 1, . . . , n. Then permute entries inside each row
(again, in the same way for all squares) so that the second square
has all the j in the jth column. Check that the remaining t squares
are Latin.
9.3.6. n! × (the number of permutations with no ﬁxed point).
9.4.3(a). Let the vertex classes be A and B; double-counting gives
 di
2

=
n
2

, where the di are the degrees of the vertices in A,
and we have m = |E(G)| = 
i di. This time one has to calculate
exactly: Cauchy–Schwarz gives m2 ≤n  d2
i = 2n  di
2

+ nm ≤
n2(n −1) + nm. Solve the resulting quadratic inequality for m.
10.1.1(a). A function of n variables deﬁnes two functions of n −1
variables, one for xn = 0 and one for xn = 1. One can proceed by
induction on n.
10.1.3. Each weighing has 3 possible outcomes, and hence 3 weigh-
ings can only distinguish one among 33 possibilities.

Hints to selected exercises
425
10.1.4. Code all possible ways of the cars passing through the
depicted system of rails, e.g. “a car from rail A goes to rail I, car
from A to C, car from I to C, car from C to II, . . . ”. Show that at
most Cn possible codes exist (for a large enough constant C); hence
there are at most this many possible car orders on rail B, and this
is smaller than n! for n large.
10.2.2(a). P(A1 ∪· · · ∪An) = 
∅̸=I⊆{1,...,n}(−1)|I|−1P

i∈I Ai

.
10.2.2(b). All the three proofs of inclusion–exclusion in Chapter 3
generalize easily.
10.2.4. For a disconnected graph, there exists a nonempty proper
subset A of vertices such that there are no edges between A and its
complement. Calculate the probability of this event for a ﬁxed A,
and then sum up over all possible choices of A.
10.2.8. If we replace some of the Ai by their complements we still
have a collection of independent events, and hence the intersection
of all these events has a nonzero probability. In this way, one can
produce 2n disjoint events with nonzero probability.
10.2.9. It’s 1
3. (Let the children be A and B. There are 3 equally prob-
able
possibilities:
A=boy,
B=boy;
A=boy,
B=girl;
A=girl,
B=boy.)
10.3.2. Show 0 ≤E

(f −E [f])2 
= E

f2 
−E [f]2.
10.3.3. Use indicators. Let Ai be the event “π(i) = i”. Result:
E [f] = 1.
10.3.4(a). Let Ai be the event “1 is contained in a cycle of length
i”. Show that P(Ai) =
1
n, and express the desired expectation as
n
i=1 i · E [IAi].
10.3.4(b). Let Aij be the event “j lies in a cycle of length i”. By
part (a), we have P(Aij) = 1
n. Check that the resulting expectation
equals n
i,j=1
1
i E

IAij
 
.
10.3.6. It’s better to count boundaries among runs. The probability
that a given position between two tosses is a boundary is 1
2.
10.3.7. We have µ = 
ω∈ΩP({ω})X(ω) ≥
ω∈Ω: X(ω)≥tµ P({ω})
tµ = tµP({ω ∈Ω: X(ω) ≥tµ}).
10.4.4. A Kr-free graph with at least r −1 vertices and with the
maximum possible number of edges has to contain a Kr−1. Tear oﬀ
a Kr−1 and count the number of deleted edges.
10.4.6(b). Let L be the set of numbers smaller than the dividing
element and R the set of the larger numbers. Show that in both

426
Hints to selected exercises
the considered algorithms, the number min(|L|, |R|) has the same
probability distribution. This number fully determines the sizes of
the two pieces in the recursion.
10.4.7. Let R be the r circles that remain black, c the circle that was
colored and emptied again, and S(R) the set of empty circles seen
by the observer for a given R. Let p be the probability of c ∈S(R).
Since c is a random element of R ∪{c} we get p ≤2/(r + 1). On the
other hand, since we could ﬁrst pick R and then c at random among
the remaining n −r empty circles, p is the average, over all choices
of R, of |S(R)|/(n −r). Hence p = E [|S(R)|] /(n −r).
10.4.9. There are at most 2 peaks at level 0. Compute the expected
number of peaks at level 0 for a random sample R of the lines in two
ways as in the proof in the text.
12.1.4. In order to get the term xk1
1 . . . xkm
m
in multiplying out n
parentheses, we have to choose k1 parentheses from which we take
x1, . . . , km parentheses from which we take xm. The number of such
choices is just the multinomial coeﬃcient.
12.1.6. Begin with the equality (1 −x)n(1 + x)n = (1 −x2)n.
12.2.5. a(x) = (1 −x)−1x6(1 −x3)−2 = x6(1 + x + x2)/(1 −x3)3.
Use the generalized binomial theorem.
12.2.8(a). The generating function is 1/((1 −x)(1 −x2)(1 −x5)).
12.2.9(d). Result: (−1)mn−1
m

.
12.2.10. For 1 ≤k ≤n −r + 1, there are
n−k
r−1

subsets with small-
est element k, so we need to sum 
k k
n−k
r−1

. This can be done (for
instance) by considering the equality [xr−1/(1 −x)r][x/(1 −x)2] =
xr/(1 −x)r+2 and comparing the coeﬃcients of xn. Alternatively,
if we denote the expectation by f(n, r), we have the recurrence
f(n, r) = p + (1 −p)(1 + f(n −1, r)) with p =
r
n (since either R
contains 1, which happens with probability p, or it can be regarded
as a random r-element subset of {2, 3, . . . , n}). Then f(n, r) = n+1
r+1
follows by induction.
12.2.12(a). For the continuity, we need to prove that for x0 ∈[0, ρ),
we have limx→x0 |a(x) −a(x0)| = 0. Use the estimate |xi
0 −xi| ≤
|x0 −x|(xi−1
0
+ xi−2
0
x + · · · + xi−1) ≤|x −x0|imi−1, where m = max
(x, x0).
12.2.12(c). For instance, ai = 1/i2.

Hints to selected exercises
427
12.2.13(a). First, show (by induction) that each derivative of the
function f at a point x ̸= 0 has the form R(x)e−1/x2, where R(x)
is a ratio of two polynomials. Then prove that each derivative is 0
at 0.
12.2.13(b). If it were a power series, its coeﬃcients could be ex-
pressed using the derivatives, and hence they would have to be
all 0s.
12.2.14(a). For the ring arrangement: −ln(1 −x).
12.2.14(c). D(x) = 1/(1 −A(x)).
12.2.14(d). E(x) = eA(x).
12.2.14(e). The exponential generating function is ex2/2.
12.2.15. Use part (d) with the subgroups arranged into unorganized
crowds with at least one member, i.e. A(x) = ex −1. The result is
eex−1.
12.2.16. The exponential generating function is (x2/2! + x3/3! +
x4/4!)5. The answer is 12! × (the coeﬃcient of x12).
12.2.17(a).
Such
arrangements
are
in
a
bijective
corres-
pondence with permutations, since they encode the cycles of the
permutation.
12.2.17(b). The result is e−ln(1−x)−x = (1+x+x2 +· · · )(1−x/1!+
x2/2! −x3/3! + · · · ).
12.3.1. Let an, bn be the number of such sequences ending with 0,
1 respectively. Derive recurrence relations for an and bn. Result: the
Fibonacci numbers.
12.3.2. Induction on n. Call a representation of n as a sum of distinct
Fibonacci numbers reduced if it uses no two consecutive Fibonacci
numbers. Any representation can be converted to a reduced one by
repeatedly replacing the largest two consecutive Fibonacci numbers
by their sum. To go from n to n+1 consider a reduced representation
of n, add a 1 to it, and make it reduced again. Let us remark that a
reduced representation is unique.
12.3.5. Consider the sequence bn = log2 an.
12.3.7. A suitable expression is ∞
k=0 xk(1+2x)k = 1/(1−x−2x2) =
1
3[1/(1 + x) + 2/(1 −2x)], and the formula is 2
34n + 1
3.
12.3.8. (n + 2)/2n+1.
12.3.9. It satisﬁes the recurrence an+2 = 2an+1 + an with a0 =
a1 = 1.

428
Hints to selected exercises
12.3.10. Show that xn = (6 +
√
37)n −(6 −
√
37)n satisﬁes xn+2 =
12xn+1 + xn with initial conditions x0 = 2, x1 = 12 and hence is
integral for all n. Further use
√
37 −6 < 0.1.
12.3.11. The equation √x + 1−√x = y solves to x = ((1−y2)/2y)2
(0 < y < 1). Hence it suﬃces to show that this x is an integer for
y = (
√
2−1)n. We get x = a2
n where an = 1
2

(
√
2 + 1)n −(
√
2 −1)n 
.
Find a recurrence for an and show by induction that for n even, an
is integral, and for n odd, an is an integer multiple of
√
2.
12.3.12. Let un denote the number of such sequences beginning
with a or b, and let vn be the number of such sequences begin-
ning with c or d. One gets the recurrences un = un−1 + 2vn−1 and
vn = 2un−1 + 2vn−1. Write down the corresponding equations for
the generating functions u(x) and v(x), solve them, and calculate
the coeﬃcients. The result is un + vn =
√
17+1
4
√
17 (1
2(3 +
√
17))n+1 +
√
17−1
4
√
17 (1
2(3 −
√
17))n+1.
12.3.13(a). The number is Fn+1.
12.3.13(b). Recurrence: an+3 = an+2 +4an+1 +2an, a0 = 1, a1 = 1,
a2 = 5; an ∼
1
√
3(1 +
√
3)n.
12.3.13(c). Let an be the required number, and let bn be the number
of ways to ﬁll an n×2 rectangle (the longer side is vertical) with the
top left corner square removed. Recurrences: an = an−1 + 2bn−1 +
an−2, bn = an−2 + bn−1, a0 = a1 = 1, b0 = b1 = 0. Generating
function equations: b(x) = xb(x) + x2a(x), a(x) = 1 + x(a(x) +
2b(x)) + x2a(x). From this we get a(x) = (1 −x)/(1 −2x −x3) and
an ∼(0.4607 . . .)(2.0556 . . .)n.
12.3.14(a). For the language with no consecutive letters a: write
A = {a}, B = {b}. Then B∗.(A.B.B∗)∗∪B∗.(A.B.B∗)∗.A expresses
the desired language as a very regular one.
12.3.14(b). If a(x) is the generating function of L1 and b(x) of L2,
then a(x)b(x) is the generating function of L1.L2, 1/(1−a(x)) is the
generating function of L∗
1, and a(x) + b(x) is the generating function
of L1 ∪L2 provided that L1 ∩L2 = ∅.
12.3.14(c). One can go via deterministic ﬁnite automata; see a text-
book on automata and formal grammars.
12.3.16(b). Show that the sequences whose jth term is 1 and the
other terms among y0, . . . , yk−1 are 0, for j = 0, 1, . . . , k −1, form a
basis.

Hints to selected exercises
429
12.3.16(c). To show linear independence, it suﬃces to check that
the vectors formed by the ﬁrst k terms of the considered sequences
are linearly independent. To this end, one can use the criterion of
linear independence via determinants. This leads to the so-called
Vandermonde determinant (usually discussed in linear algebra
courses).
12.3.16(e). Perhaps the simplest method is using the order of growth
of the sequences. If the sequences were linearly dependent, the fastest-
growing one could be expressed as a linear combination of the more
slowly growing ones, which is impossible. A problem in this approach
appears only if there are several complex roots with the same abso-
lute value. This case can be handled separately, for instance using
the Vandermonde determinant as in the hint to (c). Alternatively,
one can consider the determinant for the ﬁrst k terms as in (c) and
prove that it is nonzero.
12.4.2. Encode a binary tree with n vertices by a string of O(n)
letters and digits, say, thereby showing that bn ≤Cn for some
constant C.
12.4.3(a).
2n
n

.
12.4.3(b). A path that never goes below the diagonal encodes a
binary tree in the following manner. Divide the path into two parts
at the point where it reaches the diagonal for the ﬁrst time (the
second part may be empty). Remove the ﬁrst and last edges from the
ﬁrst part and leave the second part intact. Both parts then encode
the left and right subtrees recursively (an empty path encodes an
empty tree).
12.4.3(c). For the description of the solution, we introduce coordi-
nates: A is (0, 0), B is (n, n). Extend the chessboard by one column
on the right. Show that the paths that do reach below the diagonal
are in a bijective correspondence with the shortest paths from A to
the point B1 = (n + 1, n −1). Follow the given AB-path until the
end of its ﬁrst edge lying below the diagonal, and ﬂip the part of the
path from that point on around the line y = x −1. This yields an
AB1-path. Check that this deﬁnes a bijection.
12.4.4. Find a bijective correspondence with paths on the chessboard
not
reaching
below
the
diagonal
from
the
preceding
exercise.

430
Hints to selected exercises
12.4.8(b). c2n+1 = bn. A bijection between the planted trees con-
sidered in the exercise and the binary trees considered in the text is
obtained by deleting all the leaves of a given planted tree.
12.4.9(a). Result: tn = bn−1.
12.4.10. An old tree either is the root itself or arises by connecting
some k planted trees with at least 2 vertices each to the root, whence
s(x) = x + x/(1 −t(x) + x).
12.6.1(a). Let ai be the number of trajectories starting at 1 and ﬁrst
entering 0 after i moves. The required probability is a(1
2). Derive the
relation a(x) = x + xa(x)2. The value of ai can also be calculated
explicitly, for instance using the Catalan numbers.
12.6.2(b). The catch is that S1 is inﬁnite (although 1 will be reached
almost surely, the expected time needed to reach it is inﬁnite!).
12.7.1(a). Such an ordered partition can be encoded by dividing the
numbers 1, 2, . . . , n into k segments of consecutive numbers. Deﬁne
a (k −1)-element subset of {1, 2, . . . , n −1} by taking all numbers
that are last in their segment (except for n).
12.7.1(b). A direct solution is as in (a) (all subsets of {1, 2, . . . , n −
1}). The generating function in (a) is xk/(1 −x)k, and the sum over
all k ≥1 is x/(1 −2x).
12.7.3(a). (1 + x)(1 + x2) . . . (1 + xn).
12.7.3(c). Find a bijection between all partitions of n with k distinct
summands and all partitions of n −
k
2

with k summands (not nec-
essarily distinct). Then use the lower bound method demonstrated
in the text.
12.7.4(c). The generating functions are (1 + x)(1 + x2)(1 + x3) . . .
and 1/((1 −x)(1 −x3)(1 −x5) . . .). Multiply both the numerator
and denominator of the ﬁrst expression by (1−x)(1−x2)(1−x3) . . ..
This leads to the expression ((1−x2)(1−x4)(1−x6) . . .)/((1−x)(1−
x2)(1 −x3) . . .), and the factors with even powers of x cancel out.
For a direct argument see, e.g. Van Lint and Wilson [7].
12.7.6(a). p0 + p1 + · · · + pm.
12.7.6(b). There is a bijection mapping such trees to partitions of
m −1: the sizes of the components after deleting the root determine
the partition of m −1.
12.7.6(c). m
i=1 1/(1 −xi)pi.
12.7.6(d). Proceeding as in the proof of Theorem 12.7.2, one gets
ln rn ≤−n ln x+∞
j=1(Pn(xj)−1)/j. In the text, it was shown that

Hints to selected exercises
431
ln Pn(x) ≤Cx/(1 −x). Choose x = 1−
c
ln n for a small enough c > 0.
Then −n ln x is about
n
c ln n, and it still requires some ingenuity to
show that the sum ∞
j=1 eCxj/(1−xj)−1
j
has about the same order as
its ﬁrst term (which is about nc·C).
12.7.6(e). Consider trees of a special form, where the root is attached
to k subtrees with q leaves each. Their number is at least pk
q/k!. Use
the estimate for pn derived in the text, and adjust k and q suitably.
13.1.1(b). This is essentially a problem dual to Exercise 9.1.8.
13.2.2. Each row of the matrix AB is a linear combination of rows
of the matrix B (with coeﬃcients given by the corresponding row of
A), and hence r(AB) ≤r(B).
13.2.3. The nonsingularity of a square matrix is equivalent to its de-
terminant being nonzero, and the deﬁnition of determinant is
independent of the ﬁeld. The rank of a nonsquare matrix can be
expressed as the size of the largest nonsingular square submatrix.
13.2.4(a). If it had rank < n then the system of linear equations
Mx = 0 has a nonzero solution, and such a solution gives xT
Mx = 0.
13.2.4(b). M is a sum of a diagonal matrix D with positive elements
on the diagonal and of a matrix L with all entries λ > 0. For each
nonzero x ∈Rv we have xT Dx > 0 and xT Lx ≥0.
13.2.6(a). If A is the incidence matrix of the set system, then AT A
is a sum of a matrix Q with all entries equal to q and of a diagonal
matrix D whose diagonal entries are |Ci|−q > 0 (assuming |Ci| > q).
Hence AT A is positive deﬁnite and consequently nonsingular.
13.2.6(b). In the situation of Fisher’s inequality, consider the set
system dual to (V, B) and apply (a) to it.
13.3.2(b). By induction on k, prove that if E is a union of edge sets
of k bipartite graphs on the vertex set {1, 2, . . . , n}, then there exists
a set of at least ⌈n/2k⌉vertices with no edge of E on it.
13.3.4. Let A = (aij) be the n×n matrix with aij = 0 or 1 depending
on whether team i won with team j. The condition translates to
Ax ̸= 0 for all nonzero vectors x (arithmetic over GF(2)); hence
A is nonsingular. For n odd, the nonzero terms in the expansion of
det A as a sum over all permutations can be paired up and hence
det A = 0.
13.5.2(b). False.

432
Hints to selected exercises
13.6.2. If r(x) has degree d, calculate r(z) −p(z)q(z) for z chosen
uniformly at random in the set {1, 2, . . . , 2d}, say. If the result is 0
then r(x) −p(x)q(x) is identically 0 with probability at least 1
2.
13.6.4. Suppose that 0 ∈S. Set a ◦b = 0 for all a, b ∈S with a
single exception: x ◦y = x for some x, y ∈S \ {0}.
13.6.7(a). Show that if all triples (a, b, c) with b ∈G(k) are associa-
tive then also all triples with b ∈G(k+1) are associative.
13.6.7(c). Start with G1 = {g1} for an arbitrary g1 ∈S, maintain
⟨Gk⟩, and set Gk+1 = Gk ∪{gk+1} for some gk+1 ∈S \ ⟨Gk⟩. By
(b), the size of ⟨Gk⟩doubles in each step. Showing that ⟨Gk⟩can be
maintained in the claimed time still requires some ingenuity.

Index
V
2

, 110
X
k

, 68(3.3.1)
n
k

, 67

n
k1,...,km

, 73
≺, 43
⪯, 43
, 8, 12
, 9
a | b, 45(2.1.2)
(a, b), 8
[a, b], 8
⌊x⌋, 8
⌈x⌉, 8
{x, y}, 10
(x, y), 10
∅, 11
2X, 12
⊆, 13
⊂, 13
|X|, 12, 31(Ex. 7)
X2, 15
X ˙∪Y , 13
X × Y , 14
{. . .}, 10
R[x], 40
R−1, 38
R ◦S, 35
xRy, 32
f(X), 27
f(x), 26
f −1, 30
f : X →Y , 26
f : X→Y , 29
f : x →y, 26
f ∼g, 84
AT, 395
G + ¯e, 143(4.6.2)
G%e, 144(4.6.2)
G −e, 143(4.6.2)
G −v, 144(4.6.2)
G.e, 212(4.6.2)
G ∼= H, 113(4.1.2)
ab, 262
∆X, 38
Ω(.), 84
Θ(.), 84
α(n), 169
α(G), 308(10.4.2)
α(P), 55
χ(.), 207(6.4.2)
δp, 381
δ(.), 214(Ex. 2)
ω(G), 318
ω(P), 56
π, 91
computation, 92(Ex. 8)
π(n), 97(3.6.3)
AG, 121(4.2.3)
acyclic relation, 48(Ex. 2)
adjacency matrix, 34, 121(4.2.3)
adjacent vertices, 110
aﬃne plane, 271(Ex. 10), 369,
369(Ex. 2)
algebra, linear (application),
247–257, 272–275,
364–393
algorithm
Bor˚uvka’s, 178(5.5.3)
greedy, 172(5.4.2), 174,
176(Ex. 10), 176(Ex. 11),
176(Ex. 12)
Jarn´ık’s, 177(5.5.1)
Kruskal’s, 172(5.4.2)
Prim’s, see Jarn´ık’s algorithm
QUICKSORT, 312–314
randomized, 385
sorting, 67(Ex. 6), 312–314
antichain, 55, 227, 232(Ex. 5)

434
Index
antisymmetric
relation, 37(1.6.1)
arc, 182
arc-connected set, 184
arithmetic mean, 87
associative (operation), 13, 397
asymmetric
graph, 117(Ex. 3)
tree, 165(Ex. 1)
asymptotic analysis, 81
automorphism
of a graph, 117(Ex. 3)
of a poset, 230, 232(Ex. 5)
Bn, 54
band, M¨obius, 185
basis, 399
Bell number, 108(Ex. 8), 340(Ex. 15)
Bernoulli’s inequality, 93
Bertrand postulate, 98
Betti number,
see cyclomatic number
bijection, 28(1.4.3)
binary operation, 388, 397
binary tree, 348–350
Binet–Cauchy theorem, 254(8.5.4)
binomial coeﬃcient, 67–78, 327–328,
329(Ex. 6)
estimate, 93–98
generalized, 332(12.2.3)
binomial theorem, 71(3.3.3)
combinatorial meaning, 327
generalized, 332(12.2.3), 350
bipartite graph, 113, 123(Ex. 4),
258(Ex. 4)
complete, 113
block design, 364–372
Bonferroni inequality, 103
Boolean function, 285, 290(Ex. 1)
Bor˚uvka’s algorithm, 178(5.5.3)
Borsuk–Ulam theorem, 222
bottle, Klein, 185
bounded face, 184
Brouwer’s theorem, 220(7.1.3),
225(Ex. 5)
C, 381
C(G), 163
Cn, 112
Cn, 293(10.2.2)
carrier, 381
Cartesian product, 14
Catalan number, 350–351
Cauchy–Schwarz
inequality, 234(7.3.2), 237(Ex. 4)
Cayley’s formula, 239(8.1.1)
center of a graph, 163
centroid, 165(Ex. 7)
chain, 51(Ex. 2), 55(2.4.4), 227
symmetric, 228
characteristic function, 62
characteristic polynomial, 344
chromatic number, 207(6.4.2),
214(Ex. 2)
list, 216(Ex. 12)
circuit, see cycle
circulation, 380
space, 381
closure, transitive, 41(Ex. 4)
code
of a tree, 160
Pr¨ufer, 245
coeﬃcient
binomial, 67–78, 327–328,
329(Ex. 6)
generalized, 332(12.2.3)
multinomial, 73
coloring
of a graph, 207(6.4.2)
of a map, 206–216
commutative (operation), 13, 397
compactness, 222
complement, 123(Ex. 1)
complete
bipartite graph, 113
graph, 112
complete k-partite graph, 152(Ex. 3)
complexity (of algorithm), 167
component, 120
composition
of functions, 27
of relations, 35
conﬁguration, tactical, 367
connected
graph, 120
set, 184
connectedness
strong, 139(4.5.2)
weak, 139(4.5.2)
connectivity, 143

Index
435
contraction, 212
convex
body, 197
function, 237(Ex. 5)
cover, edge, 176(Ex. 11)
critical 2-connected graph,
148(Ex. 2)
cube (graph), 141
curve, Jordan, 190
cut, 382
space, 382
cycle, 112
elementary, 378
Hamiltonian, 136(Ex. 7),
170(Ex. 3)
in a graph, 119
of a permutation, 65, 306
space, 377
cyclomatic number, 379
dG(., .), 121
De Bruijn graph, 141
de Moivre’s theorem,
23(Ex. 4), 361
de Morgan laws, 14
degG(.), 125
deg+
G(.), 139
deg−
G(.), 139
degree (of a vertex), 125
degree sequence, see score
dependence, linear, 399
depth-ﬁrst search, 121
derangement, 104
design, block, 364–372
determinant, 395
expansion, 396
diagonal, 38, 395
matrix, 395
diagram
Ferrers, 358
Hasse, 47
diameter, 124(Ex. 8)
diﬀerence, symmetric, 377
digraph, see directed graph
Dilworth’s theorem, 58(Ex. 7)
dimension, 399
directed
cycle, 139
edge, 138(4.5.1)
graph, 138(4.5.1)
tour, 139
distance (in a graph), 121
distributive (operation), 13, 398
dominating set, 176(Ex. 12)
double-counting, 68, 217–238,
270(Ex. 8), 367
drawing of a graph, 183(6.1.1)
dual
graph, 209(6.4.3)
spanning trees, 240(Ex. 4)
projective plane, 267
duality, 267, 275
E, 377
E, 302(10.3.6)
e, 88
E(G), 110
ear decomposition, 147
edge, 109(4.1.1)
connectivity, 143
contraction, 212
cover, 176(Ex. 11)
directed, 138(4.5.1)
multiple, 134
subdivision, 144(4.6.2)
weight, 171
element
largest, 50(2.2.4)
maximal, 49(2.2.2)
maximum, 50
minimal, 49(2.2.2)
minimum, 50
smallest, 50(2.2.4)
elementary
cycle, 378
event, 291
row operation, 396
embedding
of ordered sets, 53(2.3.1)
empty
product, 12
set, 11
sum, 12
end-vertex, 156
equivalence, 38(1.6.2)
maintaining, 168(5.3.4),
170(Ex. 1)
number of, 107(Ex. 8)

436
Index
Erd˝os–Szekeres lemma, 57(2.4.6)
Erd˝os, Paul, 317
estimate
of binomial coeﬃcient, 93–98
of factorial, 85–91, 92(Ex. 9)
Euler
formula, 196(6.3.1)
for trees, 155(5.1.2)
function, 105–106, 108(Ex. 9)
number, 88
Eulerian
graph, 130–143, 189(Ex. 3)
tour, 130
even set, 376(13.4.1)
number of, 379(13.4.4)
event, 292
elementary, 291
events, independent, 297–299
excentricity, 163
exG(.), 163
expansion of determinant, 396
expectation, 301–316, 353
deﬁnition, 302(10.3.6)
linearity, 304(10.3.9)
exponential generating function,
339(Ex. 14)
extension
linear, 49
extension, linear, 77(Ex. 27)
extremal theory, 151, 233, 308
face (of a planar graph), 184
face (of a polytope), 198
factorial, 66
divisibility, 67(Ex. 7)
estimate, 85–91, 92(Ex. 9)
family of sets, 11
Fano plane, 264, 280(Ex. 1)
father (in rooted tree), 160
Ferrers diagram, 358
Fibonacci number, 340–343
ﬁeld, 398
ﬁnite probability space, 291(10.2.1)
ﬁnite projective plane, 261–283
deﬁnition, 261(9.1.1), 270(Ex. 4),
270(Ex. 8), 369(Ex. 1)
existence, 271–272
order, 266(9.1.4)
Fisher’s inequality, 370(13.2.1),
373(Ex. 6)
ﬁxed point, 104, 306(Ex. 3)
theorem, 219(7.1.2), 220(7.1.3)
forest, 166
spanning, 377
formula
Cayley’s, 239(8.1.1)
Euler’s, 196(6.3.1)
for trees, 155(5.1.2)
Heawood’s, 214
Leibniz, 75(Ex. 13)
logical, 286, 290(Ex. 1)
Stirling’s, 91
fractions, partial, 341
Freivalds’ checker, 385(13.6.1)
function, 26(1.4.1)
bijective, 28(1.4.3)
Boolean, 285, 290(Ex. 1)
characteristic, 62
convex, 237(Ex. 5)
Euler, 105–106, 108(Ex. 9)
generating, 325–363
exponential, 339(Ex. 14)
of a sequence, 331(12.2.2)
operations, 332–335
graph, 244, 244(Ex. 1), 245(Ex. 3)
identity, 31(Ex. 4)
injective, 28
monotone, 74(Ex. 7)
number of, 60(3.1.1)
one-to-one, 28(1.4.3)
number of, 63(3.1.4)
onto, 28(1.4.3)
number of, 107(Ex. 7)
period, 244(Ex. 3)
surjective, 28
GF(q), 398
GF(2), 377
Gn, 295(10.2.4)
gcd(m, n), 105
generalized binomial theorem,
332(12.2.3), 350
generating function, 325–363
exponential, 339(Ex. 14)
of a sequence, 331(12.2.2)
operations, 332–335
genus, 188(6.1.3)

Index
437
geometric mean, 87
golden section, 342, 356
Graham–Pollak theorem, 374(13.3.1)
graph, 109(4.1.1)
asymmetric, 117(Ex. 3)
bipartite, 113, 123(Ex. 4), 258(Ex. 4)
complete, 113
chromatic number, 207(6.4.2),
214(Ex. 2)
coloring, 207(6.4.2)
complete, 112
complete k-partite, 152(Ex. 3)
connected, 120
De Bruijn, 141
diameter, 124(Ex. 8)
directed, 138(4.5.1)
drawing, 110, 183(6.1.1)
dual, 209(6.4.3)
spanning trees, 240(Ex. 4)
k-edge-connected, 143
Eulerian, 130–143, 189(Ex. 3)
Heawood, 130(Ex. 15), 268
isomorphism, 113(4.1.2)
Kneser, 116, 117(Ex. 1)
line, 136(Ex. 8)
list chromatic number, 216(Ex. 12)
metric, 121
number of, 108(Ex. 13), 115
of a function, 244, 244(Ex. 1),
245(Ex. 3)
of incidence, 267, 282
orientation, 251
oriented, 138
outerplanar, 214(Ex. 3)
Petersen, 116, 117(Ex. 1)
planar, 182–216
maximal, 200(6.3.3)
number of edges, 200(6.3.3)
score, 203(6.3.4)
radius, 124(Ex. 8)
random, 295(10.2.4), 300(Ex. 4),
300(Ex. 3)
randomly Eulerian, 137(Ex. 10)
regular, 129(Ex. 12)
strongly connected, 139(4.5.2)
topological, 183(6.1.1)
tough, 148(Ex. 6)
triangle-free, 148–152, 315(Ex. 1)
2-connected, 143–148
critical, 148(Ex. 2)
k-vertex-connected, 143
weakly connected, 139(4.5.2)
with loops, 135
with multiple edges, 134
without Kk, 308
without K2,2, 233, 282
without K2,t, 236(Ex. 1)
without K3,3, 237(Ex. 6)
greedy algorithm, 172(5.4.2), 174,
176(Ex. 10), 176(Ex. 11),
176(Ex. 12)
Gr¨otsch’s theorem, 214(Ex. 4)
group, 397
Hamiltonian
cycle, 136(Ex. 7), 170(Ex. 3)
path, 142(Ex. 8)
handshake lemma, 126
applications, 217–224
harmonic
mean, 92(Ex. 6)
number, 79(3.4.1), 93(Ex. 13)
Hasse diagram, 47
hatcheck lady problem, 103(3.8.1),
340(Ex. 17)
recurrence, 107(Ex. 4), 107(Ex. 5)
Heawood
formula, 214
graph, 130(Ex. 15), 268
hydrocarbons, number of,
352(Ex. 12)
hypergraph, 365
hypothesis, inductive, 17
In, 395
identity
function, 31(Ex. 4)
matrix, 395
image, 26
incidence
graph, 267, 282
matrix, 252, 258(Ex. 4), 370, 382
inclusion–exclusion, 98–103,
300(Ex. 2)
applications, 103–108
increasing
segment of a permutation,
66(Ex. 5)

438
Index
indegree, 139
independence, linear, 399
independent
events, 297–299
set, 308
set system, 226
independent set, 55(2.4.1), 318
indicator, 303(10.3.7)
induced subgraph, 118(4.2.1)
induction, 16
inductive
hypothesis, 17
step, 17
inequality
Bernoulli’s, 93
Bonferroni, 103
Cauchy–Schwarz, 234(7.3.2),
237(Ex. 4)
Fisher’s, 370(13.2.1), 373(Ex. 6)
Jensen’s, 237(Ex. 5)
LYM, 228
Markov, 306(Ex. 7)
inf A, 52(Ex. 9)
inﬁmum, 52(Ex. 9)
injection, 28
inner face, 184
integers, 7
integrality conditions, 367(13.1.3)
intersection of level k, 309–312,
316(Ex. 9)
inverse relation, 38
inversion (of a permutation), 67(Ex. 6)
isomorphism
of graphs, 113(4.1.2)
of posets, 48(Ex. 4), 230
of trees, 159–165
Jn, 375
Jarn´ık’s algorithm, 177(5.5.1)
Jensen’s inequality, 237(Ex. 5)
Jordan curve, 190
theorem, 190(6.2.1)
Jordan–Sch¨onﬂies theorem, 191
KG, 376
Kn, 112
Kn,m, 113
kernel, 400
Klein bottle, 185
Kneser graph, 116, 117(Ex. 1)
Kruskal’s algorithm, 172(5.4.2)
Kuratowski’s theorem, 194(6.2.4)
Laplace matrix, 248, 257(Ex. 1)
largest element, 50(2.2.4)
Latin
rectangle, 281(Ex. 6)
square, 277–281
squares, orthogonal, 277
leaf, see end-vertex
left maximum, 302(10.3.4), 305
Leibniz formula, 75(Ex. 13)
lemma
Erd˝os–Szekeres, 57(2.4.6)
Sperner’s, 218(7.1.1), 225(Ex. 5)
lexicographic ordering, 44, 163
line
at inﬁnity, 262
of a projective plane, 262(9.1.1)
line graph, 136(Ex. 8)
linear
algebra (application), 247–257,
272–275, 348(Ex. 16),
364–393
extension, 77(Ex. 27)
mapping, 400
span, 400
linear extension, 49
linear ordering, 38(1.6.2)
linearity of expectation, 304(10.3.9)
linearly dependent (set), 399
list chromatic number, 216(Ex. 12)
Littlewood–Oﬀord problem,
233(Ex. 6)
logical formula, 286, 290(Ex. 1)
loop, 135
LYM inequality, 228
Mader’s theorem, 148(Ex. 7)
maintaining an equivalence,
168(5.3.4), 170(Ex. 1)
map, see function
map (coloring), 206–216
mapping, see function
linear, 400
Markov inequality, 306(Ex. 7)
matching, 176(Ex. 10)
mathematical induction, 16

Index
439
matrix, 394
diagonal, 395
identity, 395
incidence, 252, 258(Ex. 4), 370, 382
Laplace, 248, 257(Ex. 1)
multiplication, 122(4.2.4), 394
checking, 385(13.6.1)
nonsingular, 396
permutation, 124(Ex. 12)
positive deﬁnite, 372(Ex. 4)
rank, 372, 397
totally unimodular, 258(Ex. 4)
transposed, 395
matroid, 175, 384
maximal element, 49(2.2.2)
maximum element, 50
maximum spanning tree, 175(Ex. 1)
maximum, left, 302(10.3.4), 305
mean
arithmetic, 87
geometric, 87
harmonic, 92(Ex. 6)
Menger’s theorem, 144
metric, 121, 124(Ex. 7)
metric space, 121
minimal element, 49(2.2.2)
minimum element, 50
minimum spanning tree, 170–181
minor, 195, 216(Ex. 11)
M¨obius band, 185
monotone function (number of),
74(Ex. 7)
monotone sequence, 57
multigraph, 134
multinomial
coeﬃcient, 73
theorem, 73(3.3.5), 242, 329(Ex. 4)
multiple edges, 134
N, 7
natural numbers, 7
neighbor, 110
network, 171
node (of a graph), see vertex
nonsingular matrix, 396
number
Bell, 108(Ex. 8), 340(Ex. 15)
Betti, see cyclomatic number
Catalan, 350–351
chromatic, 207(6.4.2)
list, 216(Ex. 12)
cyclomatic, 379
Euler, 88
Fibonacci, 340–343
harmonic, 79(3.4.1), 93(Ex. 13)
integer, 7
natural, 7
perfect, 108(Ex. 11)
Ramsey, 320
rational, 7
real, 7
number of
alkane radicals, 352(Ex. 12)
arrangements, 73
ball distributions, 69, 76(Ex. 18)
binary rooted trees, 352(Ex. 11)
binary trees, 348–350
divisors, 108(Ex. 11)
edges of a planar graph, 200(6.3.3)
equivalences, 107(Ex. 8)
even sets, 379(13.4.4)
functions, 60(3.1.1)
functions onto, 107(Ex. 7)
graphs, 108(Ex. 13), 115
nonisomorphic, 116
Latin rectangles, 281(Ex. 6)
monotone functions, 74(Ex. 7)
one-to-one functions, 63(3.1.4)
ordered k-tuples, 75(Ex. 17)
partitions of n, 357–363
planted trees, 351(Ex. 8),
352(Ex. 9)
solutions, 69, 326
spanning trees, see number of trees
for general graph, 248(8.5.1)
subsets, 61(3.1.2), 68(3.3.2),
75(Ex. 16)
odd-size, 62(3.1.3), 71
trees, 239–260
nonisomorphic, 165(Ex. 6),
240(Ex. 1), 363(Ex. 6)
with given score, 240(8.2.1)
triangulations (of a polygon),
77(Ex. 24), 351(Ex. 6)
unordered k-tuples, 75(Ex. 17)
O(.), 81(3.4.2)
o(.), 84

440
Index
one-to-one function, 28(1.4.3)
number of, 63(3.1.4)
operation, binary, 388, 397
order
of a Latin square, 277
of a permutation, 66(Ex. 3)
of a projective plane, 266(9.1.4)
ordered
pair, 10
set, 58
ordered set, 43
ordering, 38(1.6.2)
lexicographic, 44, 163
linear, 38(1.6.2)
partial, 44
orientation, 142(Ex. 4), 251
oriented graph, 138
orthogonal
Latin squares, 277
vectors, 395
outdegree, 139
outer face, 184
outerplanar graph, 214(Ex. 3)
P(.), 291(10.2.1)
pn, 357
Pn, 113
P(X), 12
pair
ordered, 10
unordered, 10
partial fractions, 341
partial ordering, 44
partition
of n, 357–363
ordered, 357, 362(Ex. 1)
Pascal triangle, 70
path, 113, 119
Hamiltonian, 142(Ex. 8)
uniqueness, 154(5.1.2)
perfect number, 108(Ex. 11)
period of a function, 244(Ex. 3)
permutation, 64–67
cycle, 65, 306
ﬁxed point, 104, 306(Ex. 3),
340(Ex. 17)
increasing segment, 66(Ex. 5)
inversion, 67(Ex. 6)
left maximum, 302(10.3.4), 305
matrix, 124(Ex. 12)
order, 66(Ex. 3)
random, 67, 104, 293(10.2.3), 298,
306, 309
sign, 396
Petersen graph, 116, 117(Ex. 1)
pigeonhole principle, 319(11.1.2)
planar drawing, 183(6.1.1)
planar graph, 182–216
maximal, 200(6.3.3)
number of edges, 200(6.3.3)
score, 203(6.3.4)
plane
aﬃne, 271(Ex. 10), 369, 369(Ex. 2)
Fano, 264
graph, 183
projective, see projective plane
planted tree, 160
Platonic solids, 197
point
at inﬁnity, 262
of a projective plane, 262(9.1.1)
polynomial, characteristic, 344
polytope, regular, 197
poset, 44, 226
automorphism, 230, 232(Ex. 5)
isomorphism, 48(Ex. 4), 230
positive deﬁnite matrix, 372(Ex. 4)
postulate, Bertrand, 98
potential, 142(Ex. 5), 381
diﬀerence, 382
power series, 329–331, 339(Ex. 13)
power set, 12
Prim’s algorithm, see Jarn´ık’s
algorithm
prime number theorem, 97(3.6.3)
principle
pigeonhole, 319(11.1.2)
principle, inclusion–exclusion,
98–103, 300(Ex. 2)
applications, 103–108
probability, 67(Ex. 5), 86, 104,
108(Ex. 12), 284–316,
322–324, 353–357
space, ﬁnite, 291(10.2.1)
space, inﬁnite, 293
problem
four-color, 206

Index
441
hatcheck lady, 103(3.8.1),
340(Ex. 17)
recurrence, 107(Ex. 4),
107(Ex. 5)
Littlewood–Oﬀord, 233(Ex. 6)
maximum spanning tree, 175(Ex. 1)
minimum spanning tree, 172(5.4.1)
Sylvester’s, 205(Ex. 8)
product, 9
Cartesian, 14
empty, 12
scalar, 395
projection, stereographic, 189
projective plane
construction, 272–275, 279–280
duality, 267, 275
ﬁnite, 261–283
deﬁnition, 261(9.1.1), 270(Ex. 4),
270(Ex. 8), 369(Ex. 1)
existence, 271–272
order, 266(9.1.4)
real, 262, 272
property B, 282
Pr¨ufer code, 245
QUICKSORT, 312–314
R, 7
R, 382
r(A), 397
r(k, ℓ), 320
radius, 124(Ex. 8)
Ramsey number, 320
Ramsey’s theorem, 319(11.2.1)
for p-tuples, 321(Ex. 5)
random
graph, 295(10.2.4), 300(Ex. 4),
300(Ex. 3)
permutation, 67, 104, 293(10.2.3),
298, 306, 309
variable, 301(10.3.1)
walk, 354–357
randomized algorithm, 385
randomly Eulerian graph,
137(Ex. 10)
rank (of a matrix), 372, 397
rationals, 7
real projective plane, 262, 272
reals, 7
rectangle, Latin, 281(Ex. 6)
recurrence, 343–346
recurrent relation, see recurrence
reﬂexive relation, 37(1.6.1)
region (of a Jordan curve), 190(6.2.1)
regular
graph, 129(Ex. 12)
polytope, 197
relation, 32(1.5.1)
acyclic, 48(Ex. 2)
antisymmetric, 37(1.6.1)
composition, 35
inverse, 38
reﬂexive, 37(1.6.1)
symmetric, 37(1.6.1)
transitive, 37(1.6.1), 67(Ex. 6)
root of a tree, 160
row operation, elementary, 396
Sn, 104
Sn, 293(10.2.3)
scalar product, 395
score
of a graph, 125–129
of a planar graph, 203(6.3.4)
of a tree, 159(Ex. 8)
search, depth-ﬁrst, 121
section, golden, 342, 356
sequence
monotone, 57
series, power, 329–331, 339(Ex. 13)
set
connected, 184
dominating, 176(Ex. 12)
empty, 11
independent, 55(2.4.1), 308, 318
ordered, 43, 58
partially ordered, 44
set system, 11
independent, 226
2-colorable, 282, 287–290,
290(Ex. 2)
sgn(π), 396
sign of a permutation, 396
smallest element, 50(2.2.4)
solid, Platonic, 197
son (in rooted tree), 160
sorting
topological, 50

442
Index
sorting algorithm, 67(Ex. 6),
312–314
space
metric, 121
of circulations, 381
of cuts, 382
of cycles, 377(13.4.3)
probability
ﬁnite, 291(10.2.1)
inﬁnite, 293
vector, 398
span, linear, 400
spanning forest, 377
spanning tree, 166–170
algorithm, 166(5.3.2), 169(5.3.5)
maximum, 175(Ex. 1)
minimum, 170–181
Sperner’s lemma, 218(7.1.1),
225(Ex. 5)
Sperner’s theorem, 226(7.2.1)
sphere with handles, 185
square matrix, 395
square, Latin, 277–281
Steiner system, 366(13.1.2),
368(13.1.4), 369(Ex. 4)
Steiner tree, 171
Steinitz theorem, 200
step, inductive, 17
stereographic projection, 189
Stirling’s formula, 91
strongly connected graph, 139(4.5.2)
subdivision (of a graph), 144(4.6.2)
subﬁeld, 398
subgraph, 118(4.2.1)
induced, 118(4.2.1)
submatrix, 396
subsequence, 57
subsets
number of, 61(3.1.2), 68(3.3.2),
75(Ex. 16)
subspace, 400
sum, 8, 12
empty, 12
sup A, 52(Ex. 9)
supremum, 52(Ex. 9)
surjection, 28
Sylvester’s problem, 205(Ex. 8)
sym(.), 139
symmetric
chain, 228
diﬀerence, 377
relation, 37(1.6.1)
symmetrization, 139
system of sets, 11
system, Steiner, 366(13.1.2),
368(13.1.4), 369(Ex. 4)
T(.), 239
tactical conﬁguration, 367
theorem
Binet–Cauchy, 254(8.5.4)
binomial, 71(3.3.3)
combinatorial meaning, 327
generalized, 332(12.2.3), 350
Borsuk–Ulam, 222
Brouwer’s, 220(7.1.3), 225(Ex. 5)
de Moivre’s, 23(Ex. 4), 361
Dilworth’s, 58(Ex. 7)
ﬁxed point, 219(7.1.2), 220(7.1.3)
Gr¨otsch’s, 214(Ex. 4)
Graham–Pollak, 374(13.3.1)
Jordan curve, 190(6.2.1)
Jordan–Sch¨onﬂies, 191
Kuratowski’s, 194(6.2.4)
Mader’s, 148(Ex. 7)
Menger’s, 144
multinomial, 73(3.3.5), 242,
329(Ex. 4)
on score, 126(4.3.3)
prime number, 97(3.6.3)
Ramsey, 319(11.2.1)
Ramsey’s
for p-tuples, 321(Ex. 5)
Sperner’s, 226(7.2.1)
Steinitz, 200
Tur´an’s, 152(Ex. 4), 308(10.4.2),
315(Ex. 4)
Wilson’s, 369(13.1.5)
time complexity, 167
topological graph, 183(6.1.1)
topological sorting, 50
torus, 184
totally unimodular matrix,
258(Ex. 4)
tough graph, 148(Ex. 6)
tour, 131
directed, 139
Eulerian, 130

Index
443
tournament, 142(Ex. 8), 299
transitive
closure, 41(Ex. 4)
relation, 37(1.6.1), 67(Ex. 6)
transposed matrix, 395
tree, 154(5.1.1)
asymmetric, 165(Ex. 1)
binary, 348–350
code, 160
planted, 160
rooted, 160
spanning, 166–170
algorithm, 166(5.3.2)
minimum, 170–181
Steiner, 171
trees, number of, 165(Ex. 6), 239–260,
348–350, 351(Ex. 8), 352(Ex. 9),
352(Ex. 11), 363(Ex. 6)
triangle-free graph, 148–152, 308
triangular matrix, 395
triangulation, 200, 200(6.3.3)
of a polygon, 77(Ex. 24), 351(Ex. 6)
Tur´an’s theorem, 152(Ex. 4),
308(10.4.2), 315(Ex. 4)
2-coloring, 282, 287–290, 290(Ex. 2)
2-connected graph, 143–148
critical, 148(Ex. 2)
unbounded face, 184
uniform (set system), 365
UNION–FIND, 168(5.3.4),
170(Ex. 1)
unordered pair, 10
upper triangular matrix, 395
V (G), 110
variable, random, 301(10.3.1)
variance, 354(Ex. 1)
variations, 63
vector space, 398
vectors, orthogonal, 395
vertex, 109(4.1.1)
connectivity, 143
walk, 120
random, 354–357
weakly connected graph, 139(4.5.2)
well-ordering, 17
Wilson’s theorem, 369(13.1.5)
Z, 7


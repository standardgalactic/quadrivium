Springer Series in Statistics
Anuj Srivastava
Eric P. Klassen
Functional 
and Shape 
Data Analysis

Springer Series in Statistics
Series editors
Peter Bickel, CA, USA
Peter Diggle, Lancaster, UK
Stephen E. Fienberg, Pittsburgh, PA, USA
Ursula Gather, Dortmund, Germany
Ingram Olkin, Stanford, CA, USA
Scott Zeger, Baltimore, MD, USA
More information about this series at http://www.springer.com/series/692

Anuj Srivastava • Eric P. Klassen
Functional and Shape
Data Analysis
123

Anuj Srivastava
Department of Statistics
Florida State University
Tallahassee, FL, USA
Eric P. Klassen
Department of Mathematics
Florida State University
Tallahassee, FL, USA
ISSN 0172-7397
ISSN 2197-568X
(electronic)
Springer Series in Statistics
ISBN 978-1-4939-4018-9
ISBN 978-1-4939-4020-2
(eBook)
DOI 10.1007/978-1-4939-4020-2
Library of Congress Control Number: 2016942552
© Springer-Verlag New York 2016
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of the mate-
rial is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations, recitation, broadcasting,
reproduction on microﬁlms or in any other physical way, and transmission or information storage and retrieval,
electronic adaptation, computer software, or by similar or dissimilar methodology now known or hereafter de-
veloped.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication does
not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant protective
laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this book are
believed to be true and accurate at the date of publication. Neither the publisher nor the authors or the editors
give a warranty, express or implied, with respect to the material contained herein or for any errors or omissions
that may have been made.
Printed on acid-free paper
This Springer imprint is published by Springer Nature
The registered company is Springer Science+Business Media LLC New York

1923–2016
In memory of Prof. Ulf Grenander, who led the way

Preface
Function and shape data analysis are old topics in statistics, studied oﬀand on
over the last several decades. However, the early years of the new millennium
saw a renewed focus and energy in these areas. This focus was both exciting,
because it sought new directions and resources, and productive, because it was
application oriented and data driven. While the new interest was fueled by many
factors, the most prominent amongst them was increasing availability of large
datasets involving function and curve data, especially in the ﬁelds of computer
vision and medical imaging. It was also propelled by increases in computational
power and storage, a growing interest in Riemannian methods, and a favorable
atmosphere for the conﬂuence of ideas from geometry and statistics. Despite a long
history of research and methods in function and shape analysis, several researchers
took a fresh look at shape analysis during this period. As a result, they came
up with novel approaches, based on mathematical tools that were new to this
community, and made them practical using elegant computational solutions. This
book started as a research monograph, as an exposition of these contemporary
ideas in shape analysis of curves. It was heavily motivated by our desire to provide
a self-contained treatment of this new generation of methods in shape analysis
of curves, with a focus on statistical modeling and inference. However, like many
other book projects, this project also grew beyond its original plan. Not only did
it become a textbook with appendices, background material, and exercises, but
also its scope grew to include a detailed treatment on function data analysis. Not
surprisingly, it took much longer than expected to ﬁnish this manuscript. This
delay allowed us to oﬀer several graduate courses in statistics on the basis of this
textbook.
The main topic area of this book is shape analysis of functions and curves—
in one, two, and higher dimensions—both closed and open. What diﬀerentiates
this material from past approaches is that it integrates the registration problem
into shape analysis. Registration is concerned with matching of points across ob-
jects when their shapes are being compared and quantiﬁed. The past methods
mostly treated registration as a pre-processing step, handled using an arbitrary
oﬀ-the-shelf technique, followed by an unrelated metric for shape comparison. In-
stead, this textbook seeks a uniﬁed, comprehensive solution. It develops elegant
Riemannian frameworks that provide both quantiﬁcation of shape diﬀerences and
registration of curves at the same time. Additionally, these methods are used for
statistically summarizing given curve data, performing dimension reduction, and
vii

viii
Preface
modeling the observed variability. This textbook investigates diﬀerent mathemati-
cal representations and associated (invariant) Riemannian metrics that play a role
in facilitating shape analysis. The focus is largely on certain square-root represen-
tations that ﬂatten shape spaces and allow more traditional vector-space-based
statistical analyses to become applicable.
Our main objective in writing this book is to familiarize graduate students with
a broad array of tools that are relevant in developing computational solutions for
shape and related analyses. These tools, gleaned from geometry, algebra, statis-
tics, and computational science, are traditionally scattered across diﬀerent courses
and departments, which makes it diﬃcult for graduate students to learn them in
a reasonable time. While we wish to introduce readers to a diverse array of topics
that are fast becoming important in current research, this book is not intended
to be an elaborate mathematical treatment of those topics. Naturally, one cannot
expect to become an expert in such multidisciplinary areas solely on basis of a sin-
gle book. Besides, there are already many wonderful books that cover these items
individually in great detail. Indeed, in this textbook, we have frequently been lax
in presenting in-depth technical details, focusing instead on intuitive explanations
and implementable solutions. Our hope is to provide a working knowledge of rel-
evant material that is often present across diﬀerent disciplines and better prepare
graduate students for handling future scientiﬁc challenges.
This textbook is intended for graduate students who are interested in improving
their skills in this broad interdisciplinary problem area. These students can be from
statistics, engineering, applied mathematics, neuroscience, biology, bioinformatics,
and other related areas. It seems necessary for a reader to have a background in cal-
culus, linear algebra, and numerical analysis and computations. Of course, one can
selectively read chapters of his/her interest, and a sequential reading of the entire
chapter is not a necessity. This book covers a broad range of ideas, from intro-
ductory theory to algorithmic implementations and some statistical case studies.
It is self-contained in terms of the background material needed for understand-
ing methods described here. The appendices and background material are highly
recommended for students without prior coursework in geometry and algebra.
The textbook starts with a motivation for development of knowledge in this
area, by citing a number of current applications that are primarily dependent
on analyzing shapes of functions and curves. In particular, it makes a case of
using elastic functional representations of these objects, rather than sampling them
with points in a pre-determined manner. Chapter 2 summarizes some current
techniques in shape data analysis. These techniques mostly rely on point-based
representations of objects and lack a natural system for registering points across
objects. In order to understand the theory presented in this textbook, a reader
needs a working knowledge of some relevant parts of algebra and geometry, and
that is provided in Chapter 3. Chapter 4 deals with functional data analysis,
starting with some standard tools from Hilbert space theory. Then, it raises the
problem of pairwise registration of functions, highlights the shortcomings of what
may be considered a natural solution (based on the L2 norm), and proposes a better
solution using the SRSF (square-root slope function) representation of functions
and extending the Fisher-Rao Riemannian metric to general function spaces. We
take this opportunity to formally introduce the concepts of amplitude and relative
phase of functional variables. This chapter ends with impositions of the Fisher-Rao
metric on some spaces of interest, such as the sets of probability density functions
and warping functions.

Preface
ix
We present methods for shape analysis of planar curves in Chapter 5, with
an emphasis on the elastic Riemannian metric and the square-root velocity func-
tion (SRVF) representation. This chapter describes the elastic metric and uses the
SRVF mapping to convert the elastic metric into the standard L2 inner product.
This allows for simultaneous registration of points and deformation of curves or
geodesics using eﬃcient numerical techniques. Chapter 6 focuses on closed curves
by imposing an additional constraint of closure on the allowed curves. This non-
linear constraint results in representation spaces becoming nonlinear manifolds,
and we introduce two general approaches—shooting method and path-straightening
method for computing geodesic paths on such manifolds. In each case, we provide
step-by-step algorithms and many examples to illustrate the ideas.
Chapter 7 serves as a general discussion on deﬁning and computing summary
statistics on sets that are not vector spaces. Here one uses ideas from diﬀerential
geometry to specify the notions of sample means and covariances, and some as-
sociated parametric probability density functions on these sets. This is followed
by tools for modeling functional data in Chapter 8, where an important idea is to
separate functions into their amplitude and phase components, followed by mod-
eling these components individually, but not independently. This chapter makes
repeated using of functional principal component analysis (FPCA) for reaching
ﬁnite-dimensional Euclidean representations of complex functional data. These
representations are then used in analyzing, testing, classifying, and clustering given
functions. Chapter 9 extends these ideas to modeling and analysis of planar curves
using the representations developed in Chapters 5 and 6. It illustrates the use of
geometries of shape spaces to locally linearize and to perform FCPA to reach
tractable representations of planar shapes. These representations are then used in
modeling and classiﬁcations of shapes using statistical shape analysis. Chapter 10
extends these ideas to curves in arbitrary Euclidean spaces and illustrates some of
these ideas using examples from interesting application areas. The textbook ends
with a collection of miscellaneous topics relating to shape analysis of curves.
This textbook can also serve as a textbook for an advanced topics course with
interdisciplinary ﬂavor. There are several possibilities in terms of teaching a course
from this textbook. We list some of these ideas, but of course one can mix and
match these ideas as well:
• A one-semester course in function data analysis using Appendices A and B,
Chaps. 3, 4, 7, and 8, in conjunction with a more classical text on FDA.
• A one-semester course in shape data analysis using Appendices A and B,
Chaps. 2, 3, 5, 6, 7, and 9.
• A two-semester course in function and shape data analysis using Appendices A
and B, Chaps. 2–11, possibly in conjunction with a more classical text on FDA.
Tallahassee, FL, USA
Anuj Srivastava
Eric P. Klassen

Acknowledgments
We were able to ﬁnish this textbook due to signiﬁcant contributions from a large
number of people. Indeed, this book uses a body of knowledge that was developed
in collaboration with a number of wonderful colleagues. Without their contribu-
tions, this material would have never matured into this form. Therefore, we are
deeply in debt to these people for their contributions and support.
Several of the ideas presented here were developed in collaboration with Prof.
Ian H. Jermyn of Durham University, Durham UK. Indeed, he was the ﬁrst one
to introduce us to the literature on square-root representation of probability den-
sity functions and its connections with the Fisher-Rao metric. In some sense, this
whole eﬀort on elastic shape analysis of curves started for us at that moment in
the summer of 2006. We thank him for all his shared wisdom and collaboration.
We also acknowledge the eﬀorts of Dr. Shantanu Joshi of UCLA, who was the
ﬁrst of many graduate students who helped transform conceptual ideas into im-
plementations and demonstrated their impact on real data! We are thankful to
all of our graduate students—Darshan Bryner, Wade Henning, David Kaziska, Se-
bastian Kurtek, Jose Laborde, Sayani Lahiri, Wei Liu, Sentibaleng Ncube, Dan
Robinson, Michael Rosenthal, Chaﬁk Samir, Jingyong Su, J. Derek Tucker, Linda
Crystal White, Qian Xie, and Zhengwu Zhang—all of them chose to work on and
develop topics covered in this book for their PhD dissertations. We also received
continuous encouragement, comments, and feedbacks from many colleagues dur-
ing the writing of this book. Of those, we point out Boulbaba Benamor, Rama
Chellappa, Mohamed Daoudi, Zhaohua Ding, Hassen Drira, Hamid Laga, Steve
Marron, Pavan Turaga, Wei Wu, and Jinfeng Zhang. We are very thankful to them
for all their support.
This book exhibits, in its own small way, the beauty of general pattern the-
ory pioneered by Prof. Ulf Grenander of Brown University. The most wonderful
aspect of this theory is the rich set of tools it uses: algebra, geometry, statistics,
and, of course, computational science. Our wanderings beyond classical multivari-
ate and Euclidean statistics followed paths previously illuminated by Grenander’s
excursions. “Discretize as late as possible”—This was Prof. Grenander’s advice to
students in a class on pattern theory at Washington University in 1995 and became
a source of inspiration for the material presented here. The ﬁrst author is forever
grateful to Prof. Grenander for his teachings, guidance, and mentorship. He also
gratefully acknowledges training and guidance received from Prof. Michael Miller
of Johns Hopkins University.
xi

xii
Acknowledgments
We also acknowledge support from the Army Research Oﬃce for our early work
in shape analysis, especially with a focus on geometry and statistics. This eﬀort was
also supported in part by grants from the Oﬃce of Naval Research, the National
Science Foundation, and the National Institutes of Health. The initialization and
culmination of this writing project coincided with sabbatical leaves for AS dur-
ing 2007–2008 and 2014–2015. He, therefore, acknowledges support from the Uni-
versity of Lille 1, Lille, France, and the Franco-American Fulbright commission
in Paris, France, for supporting his long stays in France. AS also acknowledges
support from the Statistics Division at the National Institute of Standards and
Technology, Gaithersburg, MD, in completing this project.
We are thankful for a highly supportive environment for interdisciplinary re-
search and creativity in our respective departments, Statistics and Mathematics,
and the College of Arts and Sciences at FSU. We are thankful to our colleagues
in these units for encouraging us to take up these challenges.
Finally, and most importantly, we thank our families for their support and
encouragement in accomplishing this project. AS is very thankful of his family—
wife Elise, parents Govind and Indraprabha, sons Alex and Neel, and parents-in-
law Andr´e and Marie-C´ecile—for their continuous support and cheer. EK thanks
his wife Anna and daughter Rosemary because the happiness they bring to his life
makes math so much more fun.

Contents
1
Motivation for Function and Shape Analysis . . . . . . . . . . . . . . . . . .
1
1.1
Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2
1.1.1
Need for Function and Shape Data Analysis Tools . . . . . . . . .
2
1.1.2
Why Continuous Shapes? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
1.2
Important Application Areas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5
1.3
Speciﬁc Technical Goals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
1.4
Issues and Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
1.5
Organization of this Textbook. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
2
Previous Techniques in Shape Analysis . . . . . . . . . . . . . . . . . . . . . . . . 21
2.1
Principal Component Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
2.2
Point-Based Shape-Analysis Methods . . . . . . . . . . . . . . . . . . . . . . . . . . 24
2.2.1
ICP: Point Cloud Analysis. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
2.2.2
Active Shape Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
2.2.3
Kendall’s Landmark-Based Shape Analysis . . . . . . . . . . . . . . . 30
2.2.4
Issue of Landmark Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
2.3
Domain-Based Shape Representations . . . . . . . . . . . . . . . . . . . . . . . . . . 33
2.3.1
Level-Set Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
2.3.2
Deformation-Based Shape Analysis . . . . . . . . . . . . . . . . . . . . . . 35
2.4
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
2.5
Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
3
Background: Relevant Tools from Geometry
. . . . . . . . . . . . . . . . . . 39
3.1
Equivalence Relations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
3.2
Riemannian Structure and Geodesics. . . . . . . . . . . . . . . . . . . . . . . . . . . 41
3.3
Geodesics in Spaces of Curves on Manifolds . . . . . . . . . . . . . . . . . . . . . 48
3.4
Parallel Transport of Vectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
3.5
Lie Group Actions on Manifolds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
3.5.1
Actions of Single Groups . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
3.5.2
Actions of Product Groups . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
3.6
Quotient Spaces of Riemannian Manifolds . . . . . . . . . . . . . . . . . . . . . . 56
3.7
Quotient Spaces as Orthogonal Sections . . . . . . . . . . . . . . . . . . . . . . . . 60
3.8
General Quotient Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65
3.9
Distances in Quotient Spaces: A Summary . . . . . . . . . . . . . . . . . . . . . . 67
3.10 Center of an Orbit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67
xiii

xiv
Contents
3.11 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
3.11.1 Theoretical Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
3.11.2 Computational Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72
3.12 Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72
4
Functional Data and Elastic Registration
. . . . . . . . . . . . . . . . . . . . . 73
4.1
Goals and Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74
4.2
Estimating Function Variables from Discrete Data . . . . . . . . . . . . . . . 75
4.3
Geometries of Some Function Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . 78
4.3.1
Geometry of Hilbert Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78
4.3.2
Unit Hilbert Sphere . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82
4.3.3
Group of Warping Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83
4.4
Function Registration Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85
4.5
Use of L2-Norm and Its Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . 88
4.6
Square-Root Slope Function Representation. . . . . . . . . . . . . . . . . . . . . 91
4.7
Deﬁnition of Phase and Amplitude Components . . . . . . . . . . . . . . . . . 93
4.7.1
Amplitude of a Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94
4.7.2
Relative Phase Between Functions . . . . . . . . . . . . . . . . . . . . . . . 96
4.7.3
A Convenient Approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . 97
4.8
SRSF-Based Registration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98
4.8.1
Registration Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98
4.8.2
SRSF Alignment Using Dynamic Programming . . . . . . . . . . . 99
4.8.3
Examples of Functional Alignments . . . . . . . . . . . . . . . . . . . . . . 100
4.9
Connection to the Fisher-Rao Metric . . . . . . . . . . . . . . . . . . . . . . . . . . . 104
4.10 Phase and Amplitude Distances . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107
4.10.1 Amplitude Space and a Metric Structure . . . . . . . . . . . . . . . . . 107
4.10.2 Phase Space and a Metric Structure . . . . . . . . . . . . . . . . . . . . . 109
4.11 Diﬀerent Warping Actions and PDFs . . . . . . . . . . . . . . . . . . . . . . . . . . 112
4.11.1 Listing of Diﬀerent Actions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112
4.11.2 Probability Density Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . 113
4.12 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118
4.12.1 Theoretical Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118
4.12.2 Computational Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121
4.13 Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122
5
Shapes of Planar Curves . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125
5.1
Goals and Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125
5.2
Parametric Representations of Curves . . . . . . . . . . . . . . . . . . . . . . . . . . 126
5.3
General Framework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128
5.3.1
Mathematical Representations of Curves . . . . . . . . . . . . . . . . . 128
5.3.2
Shape-Preserving Transformations . . . . . . . . . . . . . . . . . . . . . . . 132
5.4
Pre-shape Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133
5.4.1
Riemannian Structure. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135
5.4.2
Geodesics in Pre-shape Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . 138
5.5
Shape Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141
5.5.1
Removing Parameterization. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141
5.6
Motivation for SRVF Representation . . . . . . . . . . . . . . . . . . . . . . . . . . . 144
5.6.1
What Is an Elastic Metric? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144
5.6.2
Signiﬁcance of the Square-Root Representation . . . . . . . . . . . . 148

Contents
xv
5.7
Geodesic Paths in Shape Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149
5.7.1
Optimal Re-Parameterization for Curve Matching . . . . . . . . . 152
5.7.2
Geodesic Illustrations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152
5.8
Gradient-Based Optimization Over Re-Parameterization Group . . . 156
5.9
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160
5.10 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160
5.10.1 Theoretical Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160
5.10.2 Computational Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163
5.11 Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164
6
Shapes of Planar Closed Curves
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167
6.1
Goals and Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167
6.2
Representations of Closed Curves . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168
6.2.1
Pre-shape Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 170
6.2.2
Riemannian Structures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171
6.2.3
Removing Parameterization. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174
6.3
Projection on a Manifold . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 178
6.4
Geodesic Computation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179
6.5
Geodesic Computation: Shooting Method . . . . . . . . . . . . . . . . . . . . . . . 180
6.5.1
Example 1: Geodesics on S2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182
6.5.2
Example 2: Geodesics in Non-elastic Pre-shape Space . . . . . . 185
6.6
Geodesic Computation: Path-Straightening Method . . . . . . . . . . . . . . 189
6.6.1
Theoretical Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189
6.6.2
Numerical Implementation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 194
6.6.3
Example 1: Geodesics on S2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 197
6.6.4
Example 2: Geodesics in Elastic Pre-shape Space . . . . . . . . . . 197
6.7
Geodesics in Shape Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 204
6.7.1
Geodesics in Non-elastic Shape Space . . . . . . . . . . . . . . . . . . . . 204
6.7.2
Geodesics in Elastic Shape Space . . . . . . . . . . . . . . . . . . . . . . . . 208
6.8
Examples of Elastic Geodesics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215
6.8.1
Elastic Matching: Gradient Versus Dynamic Programming
Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 217
6.8.2
Fast Approximate Elastic Matching of Closed Curves . . . . . . 218
6.9
Elastic Versus Non-elastic Deformations . . . . . . . . . . . . . . . . . . . . . . . . 218
6.10 Parallel Transport of Shape Deformations . . . . . . . . . . . . . . . . . . . . . . 219
6.10.1 Prediction of Silhouettes from Novel Views . . . . . . . . . . . . . . . 223
6.10.2 Classiﬁcation of 3D Objects Using Predicted Silhouettes . . . . 224
6.11 Symmetry Analysis of Planar Shapes . . . . . . . . . . . . . . . . . . . . . . . . . . 225
6.12 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 228
6.12.1 Theoretical Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 228
6.12.2 Computational Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 229
6.13 Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 231
7
Statistical Modeling on Nonlinear Manifolds . . . . . . . . . . . . . . . . . . 233
7.1
Goals and Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 233
7.2
Basic Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 234
7.3
Probability Densities on Manifolds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 235
7.4
Summary Statistics on Manifolds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 236
7.4.1
Intrinsic Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 236
7.4.2
Extrinsic Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241

xvi
Contents
7.5
Examples on Some Useful Manifolds . . . . . . . . . . . . . . . . . . . . . . . . . . . 242
7.5.1
Statistical Analysis on S1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 242
7.5.2
Statistical Analysis on S2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 248
7.5.3
Space of Probability Density Functions . . . . . . . . . . . . . . . . . . . 255
7.5.4
Space of Warping Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 259
7.6
Statistical Analysis on a Quotient Space M/G . . . . . . . . . . . . . . . . . . 261
7.6.1
Quotient Space as Orthogonal Section . . . . . . . . . . . . . . . . . . . 262
7.6.2
General Case: Without Using Sections . . . . . . . . . . . . . . . . . . . 263
7.7
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 265
7.7.1
Theoretical Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 265
7.7.2
Computational Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 266
7.8
Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 267
8
Statistical Modeling of Functional Data . . . . . . . . . . . . . . . . . . . . . . . 269
8.1
Goals and Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 270
8.2
Template-Based Alignment and L2 Metric . . . . . . . . . . . . . . . . . . . . . . 271
8.3
Elastic Phase-Amplitude Separation . . . . . . . . . . . . . . . . . . . . . . . . . . . 273
8.3.1
Karcher Mean of Amplitudes . . . . . . . . . . . . . . . . . . . . . . . . . . . 273
8.3.2
Template: Center of the Mean Orbit . . . . . . . . . . . . . . . . . . . . . 274
8.3.3
Phase-Amplitude Separation Algorithm . . . . . . . . . . . . . . . . . . 276
8.4
Alternate Interpretation as Estimation of Model Parameters . . . . . . 281
8.5
Phase-Amplitude Separation After Transformation. . . . . . . . . . . . . . . 282
8.6
Penalized Function Alignment. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 284
8.7
Function Components, Alignment, and Modeling . . . . . . . . . . . . . . . . 286
8.8
Sequential Approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 288
8.8.1
FPCA of Amplitude Functions: A-FPCA . . . . . . . . . . . . . . . . . 289
8.8.2
FPCA of Phase Functions: P-FPCA . . . . . . . . . . . . . . . . . . . . . 290
8.8.3
Joint Modeling of Principle Coeﬃcients . . . . . . . . . . . . . . . . . . 292
8.9
Joint Approach: Elastic FPCA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 294
8.9.1
Model-Based Elastic FPCA in Function Space F . . . . . . . . . . 294
8.9.2
Elastic FPCA Using SRSF Representation . . . . . . . . . . . . . . . . 297
8.10 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 301
8.10.1 Theoretical Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 301
8.10.2 Computational Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 302
8.11 Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 303
9
Statistical Modeling of Planar Shapes . . . . . . . . . . . . . . . . . . . . . . . . . 305
9.1
Goals and Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 306
9.2
Clustering in Shape Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 307
9.2.1
Hierarchical Clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 307
9.2.2
A Minimum-Dispersion Clustering . . . . . . . . . . . . . . . . . . . . . . . 309
9.3
A Finite Representation of Planar Shapes . . . . . . . . . . . . . . . . . . . . . . 311
9.3.1
Shape Representation: A Brief Review . . . . . . . . . . . . . . . . . . . 311
9.3.2
Finite Shape Representation: Planar Curves . . . . . . . . . . . . . . 313
9.3.3
Finite Representation: Planar Closed Curves . . . . . . . . . . . . . . 316
9.4
Models for Planar Curves as Elements of S2 . . . . . . . . . . . . . . . . . . . . 317
9.4.1
Truncated Wrapped-Normal (TWN) Model . . . . . . . . . . . . . . . 317
9.4.2
Learning TWN Model from Training Shapes in S2 . . . . . . . . . 318
9.5
Models for Planar Closed Curves . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 322
9.6
Beyond TWN Shape Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 327

Contents
xvii
9.7
Modeling Nuisance Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 329
9.7.1
Modeling Re-Parameterization Function . . . . . . . . . . . . . . . . . . 330
9.7.2
Modeling Shape Orientations . . . . . . . . . . . . . . . . . . . . . . . . . . . 333
9.8
Classiﬁcation of Shapes With Contour Data . . . . . . . . . . . . . . . . . . . . 334
9.8.1
Nearest-Neighbor Classiﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . 335
9.8.2
Probabilistic Classiﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 335
9.9
Detection/Classiﬁcation of Shapes in Cluttered Point Clouds . . . . . . 339
9.9.1
Point Process Models for Cluttered Data . . . . . . . . . . . . . . . . . 341
9.9.2
Maximum Likelihood Estimation of Model Parameters . . . . . 343
9.10 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 345
9.10.1 Theoretical Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 345
9.10.2 Computational Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 346
9.11 Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 347
10
Shapes of Curves in Higher Dimensions . . . . . . . . . . . . . . . . . . . . . . . 349
10.1 Goals and Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 349
10.2 Mathematical Representations of Curves . . . . . . . . . . . . . . . . . . . . . . . 350
10.3 Elastic and Non-elastic Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 351
10.4 Shape Spaces of Curves in Rn . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 353
10.4.1 Direction Function Representation . . . . . . . . . . . . . . . . . . . . . . . 353
10.4.2 Under SRVF Representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 356
10.4.3 Hierarchical Clustering of Elastic Curves . . . . . . . . . . . . . . . . . 361
10.4.4 Sample Statistics and Modeling of Elastic Curves in Rn . . . . 362
10.5 Registration of Curves . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 365
10.5.1 Pairwise Registration of Curves in Rn . . . . . . . . . . . . . . . . . . . . 366
10.5.2 Registration of Multiple Curves . . . . . . . . . . . . . . . . . . . . . . . . . 368
10.6 Shapes of Closed Curves in Rn . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 369
10.6.1 Non-elastic Closed Curves . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 369
10.6.2 Elastic Closed Curves . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 372
10.7 Shape Analysis of Augmented Curves . . . . . . . . . . . . . . . . . . . . . . . . . . 376
10.7.1 Joint Representation of Augmented Curves . . . . . . . . . . . . . . . 378
10.7.2 Invariances and Equivalence Classes . . . . . . . . . . . . . . . . . . . . . 379
10.8 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 382
10.8.1 Theoretical Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 382
10.8.2 Computational Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 384
10.9 Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 384
11
Related Topics in Shape Analysis of Curves . . . . . . . . . . . . . . . . . . . 385
11.1 Goals and Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 385
11.2 Joint Analysis of Shape and Other Features . . . . . . . . . . . . . . . . . . . . . 386
11.2.1 Geodesics and Geodesic Distances on Feature Spaces. . . . . . . 387
11.2.2 Feature-Based Clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 389
11.3 Aﬃne-Invariant Shape Analysis of Planar Curves . . . . . . . . . . . . . . . . 391
11.3.1 Global Section Under the Aﬃne Action . . . . . . . . . . . . . . . . . . 393
11.3.2 Geodesics Using Path-Straightening Algorithm . . . . . . . . . . . . 396
11.4 Registration of Trajectories on Nonlinear Manifolds . . . . . . . . . . . . . . 401
11.4.1 Transported SRVF for Trajectories . . . . . . . . . . . . . . . . . . . . . . 405
11.4.2 Analysis of Trajectories on S2 . . . . . . . . . . . . . . . . . . . . . . . . . . . 411

xviii
Contents
11.5 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 413
11.5.1 Theoretical Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 413
11.5.2 Computational Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 415
11.6 Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 415
Background Material
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 417
A.1 Basic Diﬀerential Geometry . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 417
A.1.1 Tangent Spaces on a Manifold . . . . . . . . . . . . . . . . . . . . . . . . . . 421
A.1.2 Submanifolds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 425
A.2 Basic Algebra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 427
A.3 Basic Geometry of Function Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . 431
A.3.1 Hilbert Manifolds and Submanifolds . . . . . . . . . . . . . . . . . . . . . 432
The Dynamic Programming Algorithm
. . . . . . . . . . . . . . . . . . . . . . . . . . . 435
B.1 Theoretical Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 435
B.2 Computer Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 436
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 439
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 445

Chapter 1
Motivation for Function and Shape
Analysis
This textbook is dedicated to the study of functional data analysis and shape
analysis of curves in Euclidean spaces. In the ﬁrst item, one develops tools for
statistical analysis of real-valued functional data on ﬁxed intervals. While function
data analysis is a broad topic area, worthy of a textbook in itself, we will focus
heavily on a speciﬁc aspect that deals with alignment or registration of functional
data. In the second item, one studies shapes formed by curves in 2D, 3D, and
higher dimensions, with a goal of performing statistical inferences. Since these
curves are also functions, albeit vector valued, and the issue of registration is of
prime importance in their shape analysis, we will cover these topics under a broad
umbrella of elastic functional and shape data analysis!
There are several meanings of the word shape in the English language. According
to the Oxford English Dictionary, when this word is used as a noun, it may mean:
1. The external form or appearance of someone or something as produced by their
outline.
2. A piece of material, paper, etc., made or cut in a particular form.
3. A particular condition or state: the house was in poor shape.
4. A speciﬁc form or guise assumed by someone or something: a ﬁend in human
shape.
5. Deﬁnite or orderly arrangement.
It is the ﬁrst, and perhaps the most common, usage of the word shape that in-
terests us in this book. The words form, appearance, and outline are all key to
deﬁning and understanding shapes. Shape is a basic, integral physical property
of objects that plays a major role in analysis of their appearances. From devel-
oping a neuroscientiﬁc understanding of human vision to developing algorithms
for automated image understanding, shape analysis is intimately involved in many
processes. The importance of shape is highlighted by the fact that a growing child
learns about the shapes and colors of objects before learning the alphabet or the
numbers. It is no coincidence, thus, that many toys for toddlers involve matching
simple shapes. For all human beings, observing a scene and identifying objects in
that scene involves analyzing shapes and colors in a fundamental way, although
many intricate details of that process are yet to be discovered and understood. A
speciﬁc situation we are interested in arises when the actual scenes are replaced
by their images. Since the scenes are imaged by cameras, or other similar sensors,
the interpretations can naturally beneﬁt from analysis of shapes of the objects
© Springer-Verlag New York 2016
A. Srivastava, E.P. Klassen, Functional and Shape Data Analysis,
Springer Series in Statistics, DOI 10.1007/978-1-4939-4020-2 1
1

2
1 Motivation for Function and Shape Analysis
contained in those scenes. Therefore, shape has emerged as one of the important
ways to characterize objects seen in images. Although the notion of shape is used
widely across the literature, it is seldom made precise in a mathematical sense.
This may not be needed in all contexts, but if we wish to use computers to analyze
images, then a formal, mathematically precise treatment of shapes is of profound
importance. We start by motivating the use of shape analysis in a few applications.
1.1 Motivation
What is the motivation for studying functional and shape data analysis? Further-
more, why should shape analysis be performed using curve or functional data?
We address these questions next.
1.1.1 Need for Function and Shape Data Analysis Tools
We start with the need for studying function data. The problem of statistical anal-
ysis in function spaces is important in a wide variety of applications, arising in
nearly every branch of science, ranging from speech processing to geology, biology,
and chemistry. One encounters many problems where the observations are real-
valued functions on an interval, and the goal is to perform their statistical analysis.
By statistical analysis we mean to compare, align, average, and model a collection
of such random observations. These problems can, in principle, be addressed using
tools from functional analysis, e.g., using the L2 Hilbert structure of the func-
tion spaces, where one can compute L2 distances, cross-sectional (i.e., point-wise)
means and variances, and principal components of the observed functions. How-
ever, a serious challenge arises when functions are observed with ﬂexibility or do-
main warping along the x axis. This warping may come either from an uncertainty
in the measurement process or may simply denote an inherent variability in the
underlying process itself that needs to be separated from the variability along the
y axis (or the vertical axis). As another possibility, the warping may be introduced
as a tool to horizontally align the observed functions, reduce their variance, and
increase parsimony in the resulting model. Keep in mind that we allow only the
x-axis (the domain) to be warped and the y-values to change only consequentially.
In this situation, the mathematical framework needed for function data analysis
is closely related to the notion of shape analysis of curves.
Moving on to the motivation for shape analysis of curves, we ﬁrst consider tradi-
tional 2D image analysis where the goal is to try to recognize objects in images or
videos. We consider this problem because images have become one of the largest
sources of data in our digital society. The objects of interest are characterized
by their appearances, and one looks for some distinguishing features that can be
used to classify these appearances. There are two distinct (but not independent)
features that can be used for such classiﬁcations: shapes and textures. The bound-
aries or silhouettes of objects in images characterize the objects themselves to a
certain degree, and, thus, the shapes of these boundaries become an important
feature. The other type of feature is the texture, or the pattern of pixel values,
formed by the pixels falling on the object in the image. These pixels result from

1.1 Motivation
3
Fig. 1.1 Appearance of objects in images (top) can be partially characterized and analyzed
using the shapes of their boundaries (bottom)
an electromagnetic process that involves a combination of the incident light, the
surface reﬂectivity, the ambient light, and the observation angle and can be mod-
eled using principles of physics. The patterns formed by pixels have already been
utilized in a variety of ways by many researchers for detecting, classifying, and
recognizing objects in images. It should be clariﬁed that, eventually, in an image-
based recognition system, we envision the use of shapes in conjunction with, rather
than instead of, texture analysis.
Tools for studying shapes of objects in images have tremendous potential. The
general area of computer vision that seeks to analyze and annotate elements of
static and video images is full of situations where shape features play important
roles. For example, the top row of Fig. 1.1 shows images of a few animals, whereas
the bottom row displays only the silhouettes of these creatures. Even if one restricts
to these contours as the only source of information, there is a strong possibility that
we can successfully categorize these animals. We would like to develop automated
procedures for this and related tasks. Of course, the task of extracting contours or
silhouettes from real images is nontrivial in itself. However, to focus on the main
task, i.e., shape analysis, we can assume that the contours in images are readily
available. Then, one would like to develop a computer program, founded on sound
mathematical principles, that can classify contours on the basis of their shapes, in
a completely automated fashion.
1.1.2 Why Continuous Shapes?
In all the motivating examples provided so far and later on, we have targeted
functions or continuous curves for shape analysis. We have identiﬁed continuous
objects despite the fact that the actual observations of functions and shapes are
often discrete, a ﬁnite number of points on a certain parameterization domain.
An alternative representation of objects that uses a ﬁnite, sampled set of points
is quite common in the literature. So a reader may ask the question: What is the
need to study continuous objects? Analysis of functions and curves, instead of
more common vectors, brings new challenges since they are elements of inﬁnite-
dimensional spaces. Are we unnecessarily complicating our problem by focusing
on these inﬁnite-dimensional representations of shapes? In our opinion the answer
to the last question is no! Here is why:

4
1 Motivation for Function and Shape Analysis
Fig. 1.2 Illustration of variability in shapes of polygons resulting from diﬀerent samplings of
the same continuous curve
Fig. 1.3 Representation of shapes by point sets ﬁxes their pairing or matching across objects.
A better approach is to consider underlying contours and to solve the registration problem at
that level
1. Firstly, we argue that underlying the shapes we observe—natural, man-made
or biological—are continuous objects with precise contours. Thus, it is funda-
mentally superior to study/analyze these continuous shapes even though their
observations are often obtained by “sampling” points along those shapes. Since
the process of sampling, in general, is often random, one should not completely
rely on the sampled points for analyzing the underlying shapes. As an exam-
ple, consider the continuous curve shown in Fig. 1.2. Here we see ﬁve diﬀerent
examples of samplings of this curve using a ﬁnite number of points. In these
examples the number and the placements of the sampled points are diﬀerent,
and, naturally, result in sampled curves, or polygons, that are quite diﬀerent
in shape from each other. If we restrict to the sampled points only, it will be
diﬃcult to infer that all these polygons are observations of the same shape. In-
stead, our approach mimics the construction in Fig. 1.2. As a ﬁrst step, we shall
develop statistical models for shapes of continuous functions and curves. Then,
we will develop models for sampling these continuous shapes and for generating
ﬁnite observations. Thus, a fully statistical framework will allow us to analyze
shapes of ﬁnite point sets by relating them to inﬁnite-dimensional shape models
in a principled way.
2. Secondly, one of the most important problems in shape analysis is registration
of points. Registration is concerned with setting up a correspondence between
points on one curve and points on a second curve. In case one restricts to discrete
representations of functions and curves, resulting in ﬁnite vectors, the pairing
of points across vectors is already determined. The ﬁrst point is matched with
ﬁrst, second with second, and so on. However, that may be far from optimal in
general situations. Consider Fig. 1.3 where a curve is sampled twice using the
same number of points. If one restricts to ordered lists of the sampled points
for each sampling, the pairing of points seems quite arbitrary. In fact, it is

1.2 Important Application Areas
5
hard to ﬁnd a linear registration that will suﬃce for the two samplings shown
here. A better approach is to analyze underlying contours and to solve for the
registration problem at that level.
1.2 Important Application Areas
What are the important application areas for function and shape analysis?
Biological Growth Curves Function data appears fundamentally in biological
systems that measure some aspect of growth. Consider, for example, the height
evolution of subjects in the famous Berkeley growth data.1 Figure 1.4 shows height
functions (left) and their time derivatives (right), for female and male subjects,
to highlight periods of faster growth. Although the growth rates associated with
diﬀerent individuals are diﬀerent, it is of great interest to discover broad common
patterns underlying the growth data, particularly after aligning functions using
time warping. This requires advanced techniques, beyond just a standard Hilbert
space structure, to discover underlying patterns and to make inferences.
Mass Spectrometry Data Analysis The use of mass spectrometry data to
proﬁle metabolites present in a specimen is important in biomarker discovery,
enzyme substrate assignment, drug development, and similar applications. The
use of liquid chromatography-mass spectrometry (LC-MS) is quite common, and
it provides data for retention times of diﬀerent metabolites identiﬁed by peaks in
observed chromatograms at the corresponding retention times (x axis). However,
an analysis is faced with the challenge of random nonlinear shifts in the peaks, due
to variability in retention times across measurements and equipment. Figure 1.5
show an example of proteomics data collected for patients having therapeutic
treatments for acute myeloid leukemia (AML). These two functions represent the
same chemical specimen but display a nonlinear misalignment in their peaks. Thus,
an important challenge in LC-MS data analysis is to align peaks in a principled
way using nonlinear time warping.
Biosignals Computer intervention in medicine provides many advantages rang-
ing from automatic and quantitative analysis to removal of subjective variation
in diagnosis across physicians. In many medical applications, the quantities used
for disease diagnosis and monitoring are functions of time such as blood pres-
sure, electrocardiogram signals, gait measurements, and so on. But, because of the
Male
Female
0
5
10
15
20
0
5
10
15
20
0
5
10
15
20
0
5
10
15
20
40
60
80
100
120
140
160
180
-5
0
5
10
15
20
25
30
40
60
80
100
120
140
160
-5
0
5
10
15
20
25
30
35
Fig. 1.4 Berkeley growth data for female and male subjects. For each gender we show the height
functions and their time derivatives
1 http://www.psych.mcgill.ca/faculty/ramsay/datasets.html.

6
1 Motivation for Function and Shape Analysis
250
200
150
100
50
0
6.8
7
7.2
7.4
7.6
7.8
8
8.2
8.4
8.6
8.8
Fig. 1.5 Example of LC-MS data samples from a proteomics study
Breathing Pressure
ECG
0
0.2
0.4
0.6
0.8
1
0
500
1000
1500
2000
2500
3000
0
20
40
60
80
100
-12000
-10000
-8000
-6000
-4000
-2000
0
2000
4000
6000
8000
Fig. 1.6 Examples of biosignals used in medical diagnostics: Left side shows breathing pressure
proﬁles and the right side shows some ECG signal recordings
large subject variability in the studies, the observed random temporal variability
has to be correctly accounted for. Figure 1.6 shows some examples of biosignals,
coming from breathing pressure measurements and ECG recordings, that require
alignment techniques for statistical modeling. Another example of a biosignal is
given by neuronal spike trains used to communicate information from the brain
to diﬀerent parts of the body. Although the original data is discrete, researchers
frequently convolve these spike trains with Gaussian kernels to convert them into
smooth data, ready for function data analysis, as shown in Fig. 1.7.
Biometric Security A really important, and fascinating, problem is the charac-
terization of individual humans using easily measurable traits, for applications
ranging from security to social media to welfare services. In cooperative situ-
ations, where the subjects are willing or interested in providing data, one can
use more intimate biometrics such as ﬁngerprints, palm prints, ear shapes, nose
proﬁles, iris, knuckle outlines, etc. In the noncooperative environments, such as
surveillance situations, the imaging sensors dominate. A large variety of sensors—
visible-spectrum cameras, infrared or night vision imagers, hyperspectral sensors,
satellite and aerial cameras, surveillance video monitors—are all capable of gener-
ating high-throughput volume data that requires automated processing. Since the
shape has a potential for characterizing objects in images, it becomes important
for security applications. Consider the problem of recognizing human beings when
observed from a distance in a noncooperative environment using a remote camera.

1.2 Important Application Areas
7
Spike Train Data
0
1
2
3
4
5
0
0.1
0.2
time (sec)
0
1
2
3
4
5
0
0.1
0.2
time (sec)
Fig. 1.7 An example of a neuronal spike train converted into function data
Fig. 1.8 Examples of temporal evolution of human silhouettes (bottom) in an infrared video
sequence (top)
One possibility is to study the gait, i.e., the style of walking, of a person to recog-
nize them from a distance. In a typical scenario for gait-based human recognition,
one obtains a sequence of silhouettes of a person, and the goal is to study this
temporal evolution of shapes and to compare it across people. Shown in Fig. 1.8 is
an example of this case where the top row shows a sequence of surveillance-quality
infrared images of a person walking, while the bottom row shows the correspond-
ing sequence of silhouettes obtained from images using certain image processing
techniques. An emerging and related area is activity recognition, where one tries
to characterize activities of human subjects using video data. An activity is made
up of a sequence of actions performed in a certain order and at a certain execution
rate. One of the central features being used currently in detecting actions and
categorizing activities is the shape.
Anatomical Shape Analysis One of the most important areas for applying
shape analysis is in performing medical diagnosis using medical images. Noninva-
sive imaging has become the centerpiece of modern diagnostics and treatments.
Imaging modalities here include sensors like ultrasound, X-rays, MRI, fMRI, CT-
scans, etc. These images typically are low resolution and low contrast, relative
to even a cheap (visible-spectrum) digital camera. Therefore, the methods based
on texture analysis are not as reliable, and shape analysis becomes even more
important in this situation. If one considers analysis of medical images by human

8
1 Motivation for Function and Shape Analysis
experts, i.e., physicians, this analysis currently relies on a coarse quantiﬁcation of
shapes using distances and sizes. For example, when quantifying growths and de-
cays of tumors, the temporal changes are routinely monitored by manually drawing
polygons around the tumors and by measuring simple quantities such as perime-
ter, diameter, and thickness. Shape changes in organs are important indicators of
the progression of diseases and general health. In particular, ultrasound imaging
is used in cardiology (echocardiography), gynecology, obstetrics, and endocrinol-
ogy to study the shapes, sizes, locations, and orientations of relevant anatomical
parts. It seems natural to use a full analysis of shapes, rather than restrict to some
coarser representations, to perform medical diagnosis. Shape estimation and anal-
ysis is also important in locating organs for planning radiation therapy. Although
there are a large number of applications of shape analysis in medical diagnostics,
we provide only a few examples.
In current medical image analysis, a prominent source of data is magnetic res-
onance imaging (MRI), which provides 3D images of anatomical parts. In partic-
ular, diﬀusion-tensor MRI (DT-MRI) has become increasingly popular to study
the “connectivity” of subparts of the brain and other organs (for instance, refer to
human connectome project http://humanconnectomeproject.org). This connec-
tivity is conceptualized through the tracts formed by bundles of neurons, termed
ﬁber tracts, that form “pipelines” for transmitting signals and thus information
from one region to another. An ongoing eﬀort in the medical image community is
to study the shapes of these ﬁber tracts to associate them with functionality of the
subparts and the health of the patient. Shown in Fig. 1.9 is a graphical rendering
of the bundles of ﬁber tracts in a human brain; these ﬁber tracts were estimated
from DT-MRI scan of a human brain.
Bioinformatics Bioinformatics is the science of using computational and statis-
tical approaches to the ﬁeld of molecular biology. This is a fast growing area of
Fig. 1.9 A graphical rendering of ﬁber tracts in a human brain, generated from DT-MRI data
(Data courtesy of Wikipedia Commons)

1.2 Important Application Areas
9
10
20
30−5051015
−20
−15
−10
−5
0
5
10
0
10
20
−10
0
10
−5
0
5
Fig. 1.10 Structural analysis of proteins includes shapes analysis of their backbones. The ﬁgure
shows backbones of two simple proteins: 1CTF on the left and 2JVD on the right
research and application where a large suite of techniques from statistical pat-
tern recognition is needed. In particular, there is an important need to associate
patterns of shapes of biochemical structures, such as proteins, with their func-
tionality. Proteins are linear polymers formed by concatenating amino acids. The
primary structure in a protein is a linked chain of carbon, nitrogen, and oxygen
atoms known as the backbone. Additionally, a protein also has side chains that are
connected to the backbone, and it is the combined eﬀect of the backbone and the
side chains that ultimately determines the three-dimensional structure of a pro-
tein. There are two types of protein structure comparison problems, comparison of
backbone structures (structure alignment) and comparison of the binding or active
sites of proteins (surface matching). The backbones contain certain distinct geo-
metrical pieces and one prominent type is the so-called α-helix. Figure 1.10 shows
examples of backbones of two simple proteins—1CTF and 2JVD—that contain
three and two α-helices, respectively. In analyzing shapes of backbones, it seems
important to match not only their global geometries but also the local features
(such as α-helices) that appear along these curves.
Forensic Applications Researchers for long have been interested in developing
systems for automated reading and understanding of human handwriting. One
application of this tool can be in automated sorting of mail by the postal service.
Another use can be in authentication of human signatures in banks, credit card
transactions, wills, and so on. The structure of an alphabet, a word, or a sentence
can vary greatly depending on the writer, and that makes automated reading a
challenging problem. Since signatures are forged by either other humans or ma-
chines, there is a possibility of these writers introducing their own characteristics
in the process. The top part of Fig. 1.11 shows examples of genuine signatures and
forgeries. The bottom part displays a representation of signatures based on the
magnitude of the second derivatives along these curves. A systematic comparisons
of shapes of letters in signatures can lead to automated techniques for signature
authentication.
General Morphometrics Beyond the applications involving image data, there
are several branches of sciences—archeology, ﬁsheries, cartography, botany, bio-
chemistry, etc.—that involve the study of shapes of objects. In some of these
applications, data is available in the form of images, although that is not always
the case. In Fig. 1.12, we show four examples of the pubis bone of a dinosaur from
the Hadrosauridae family. Here, one is interested in studying shapes of such bones
for inferring phylogeny, i.e., relatedness among various groups of dinosaurs dur-
ing their evolution. Figure 1.13 shows an example from morphometrics in ﬁsheries

10
1 Motivation for Function and Shape Analysis
Acceleration Functions for Signatures
20
40
60
80
0
0.5
1
1.5
Fig. 1.11 Examples of signatures for a person. The top row shows genuine signatures and the
middle shows forgeries. The bottom row shows a sample signature on the left and the acceleration
functions associated with all genuine signatures of that class (Data courtesy Signature Veriﬁcation
Competition 2004)
Fig. 1.12 Images of pubis bones of a Hadrosaurid dinosaur (Data courtesy Mr. Albert Prieto-
Marquez of Florida State University)
Fig. 1.13 Images of some ﬁshes (top) and their boundaries (bottom) (Data courtesy the Surrey
ﬁsh database [80])
where the objective is to cluster and classify aquatic animals using their shapes,
commonly observed as images. The Surrey database [80] contains silhouettes of
around 1100 marine animals, a rich source for characterizing species according to
the shapes of their specimens.
One potentially important application of shape analysis in images can be the
recognition of symbols (alphabets) from gestures in a sign language. Shown in
Fig. 1.14 are several pictures of hand gestures forming English alphabets in Amer-
ican Sign Language (ASL). Although the full pictures are important for precise
distinction between symbols, a shape analysis of hand contours may provide an
acceptable level of distinction between the signs. Note that, in the context of a

1.3 Speciﬁc Technical Goals
11
Fig. 1.14 Examples of gestures from the American sign language alphabet (Data courtesy ASL
alphabets)
language, one does not need to have each and every alphabet decoded correctly;
instead, the neighboring symbols in a sentence can provide additional knowledge
for classiﬁcation and language understanding.
In summary, there are a large number of applications, spread across the vast
landscape of science and technology, where tools for shape analysis can play a
central role in solving problems.
1.3 Speciﬁc Technical Goals
Functional and shape analysis is a broad subject with a variety of problem formu-
lations. What does one want to accomplish in shape analysis? Focusing on typical
scenarios, we enumerate a set of tasks that one wants to perform in shape analysis
of functions and curves:
1. Registration of Points Across Functions and Curves:
Since we have chosen to represent objects of our interests using functions and
curves, we need to solve the registration problem mentioned earlier. This prob-
lem is concerned with ﬁnding pairings of points across objects that are useful in
their comparisons. Figure 1.15 shows some simple illustrations of this problem.
The left panel shows two functions, and we want to ﬁnd appropriate pairings
of points across their domains. This registration is established by (nonlinearly)
time warping one of the functions so that its peaks and valleys are well aligned
with those of the other (second panel). Similarly, in the context of curves, the
registration implies ﬁnding optimal pairings of ordered points across objects, as
shown in the next two panels, with optimality deﬁned using a chosen objective
function.
2. Quantiﬁcation of Shape Similarities/Dissimilarities:
Perhaps the most frequently asked question about functions and shapes is: How
diﬀerent are their shapes? One would like to quantify the similarities and dis-
similarities between given functional objects. It is important to note that these
quantiﬁcations should not depend on the rotation, placement, and other trans-
formations that can change curves but do not change their shapes. A more
formal question is: How can we deﬁne a metric space of shapes and compute
distances between shapes as elements of this space? There exist many techniques

12
1 Motivation for Function and Shape Analysis
0
0.2
0.4
0.6
0.8
1
0
2
4
6
8
10
0
0.2
0.4
0.6
0.8
1
0
2
4
6
8
10
Fig. 1.15 Illustration of registration of points across objects. The left two panels show regis-
tration of function using time warping. The right two panels show the matched points for two
examples
How different are these shapes?
Fig. 1.16 A central aim in shape analysis is to quantify dissimilarities of given shapes
for comparing shapes and generating some kind of “matching scores,” but we
want a distance function that satisﬁes the three properties of a distance, includ-
ing the triangle inequality. Speciﬁcally, if s1, s2, and s3 are three shapes and
d(·, ·) is a distance for comparing them, then this distance should satisfy the
inequality: d(s1, s2) + d(s2, s3) ≥d(s1, s3).
Shown in Fig. 1.16 are two examples of 2D shapes: a cat and Mickey mouse.
We seek a framework which can automatically quantify the diﬀerences in those
two and other shapes.
Of course, it is desirable to have computerized methods that mimic human
responses (based on visual inspections) to such questions; however, one should
not insist on a full agreement at the outset. A human visual system is a complex
mechanism that is invariably inﬂuenced by many other factors such as the
situation, the presentation of data, and the prior experiences. Instead of seeking
to build such a complex system, we will present several shape metrics and
algorithms, all within a principled mathematical framework, and will try to
obtain their physical interpretations. Such interpretations and explanations will
help justify their usage in certain situations and rule them out in others.
3. Representative Function or Shape:
Just as we can study the average height of trees in a forest, the average value
of homes in a county, or the average score of students in class, we would like
to know the average shape of a collection of objects. Sometimes one needs a
shape that is a good representative or a “summary” of the shapes present in a
collection. In more formal language, one can ask the following question: What
is a statistical mean or a mode for a sample taken from a population of shapes?
The role of such a representative is important in many applications. It can be
used in a compression algorithm to replace a large number of observations by a
single shape. It can be useful in an indexing and retrieval algorithm to search for
shapes from a large database; instead of trying to compare a query with all the

1.3 Speciﬁc Technical Goals
13
Fig. 1.17 Samples from a population of stingrays from the Surrey ﬁsh database on the left and
a representative shape for this sample on the right
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
-5
0
5
10
15
20
-5
0
5
10
15
20
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Fig. 1.18 The problem of alignment of given functions and computation of their mean under
an elastic metric. Left shows given functions, middle shows their alignment and mean, and the
right shows the time-warping functions used in alignment
shapes, one can try to match the query with only the representative shape(s)
for each class and gain eﬃciency. It can also be used in shape classiﬁcation and
hypothesis selection. Shown in the left panel of Fig. 1.17 are some observations
from a population of sting rays, and the task here is to ﬁnd a representative
shape, such as the one shown in the right, to represent this sample. This idea
is especially useful in medical image analysis, where one can build a template
for the shape of a healthy organ or an anatomical unit, using averages, and
compare individual patients with that template to help in the diagnosis.
A related problem in analysis of function data is the computation of the
sample mean of a set of given functions while simultaneously registering points
along them. This framework, termed summarization under elastic metrics, is
illustrated in Fig. 1.18 where the functions shown in the left panel are aligned, as
shown in the middle panel, using nonlinear warping functions shown in the right
panel. These warping functions are diﬀeomorphic deformations of the domain,
[0, 1] in this example, such that the original functions composed with these
warpings are optimally aligned. Overlaid on the aligned functions is the mean
function, or representative, of this given set of functions.

14
1 Motivation for Function and Shape Analysis
Fig. 1.19 Observed variability in a population of shapes of bones
4. Modeling Shape Variability in a Class:
In most applications of interest, shapes exhibit variation even within the same
homogeneous population. Analogous to statistical modeling of real-valued ran-
dom variables, one would like to develop probability models—parametric or
nonparametric—that capture this variability. As an example, Fig. 1.19 shows
the variability of shapes in a small sample of bones taken from the Kimia
database. The challenge is to develop a probabilistic framework to capture this
variability, so that one can analyze it, sample from it, and use it for shape clas-
siﬁcation. Our approach will be to deﬁne and estimate probability densities on
appropriate spaces of shapes; the spaces will diﬀer based upon the application
of interest. In case these densities take a parametric form, both the estimation
and the analysis of resulting shape models becomes more eﬃcient. This is be-
cause the estimation of a probabilistic shape model is then equivalent to an
estimation of parameters which can be performed using maximum likelihood
estimation, for instance. In probabilistic modeling, synthesis of new shapes is
akin to generating random samples from the shape density, and classiﬁcation of
shapes is the same as the problem of hypothesis testing using likelihood func-
tions under class-speciﬁc densities. Probability models of shapes are central to
any comprehensive framework for statistical analysis of shapes.
5. Shape Classiﬁcation:
One basic task in a shape-based application is to classify shapes into predeter-
mined categories. However, the nature of the problem depends on the form in
which the data is presented. For instance, the data can be available directly in
the form of parameterized curves, or it comes in the form of cluttered point
clouds. The ﬁrst case is shown in Fig. 1.20, and the main question is the fol-
lowing: Given a few samples from each of the two distinct populations, classify
an independent sample, called a test shape, into one of the two populations and
provide a conﬁdence level for this classiﬁcation. In case the random variables
of interest are Rn-valued, there is an abundance of classiﬁers in the classical
pattern theory to handle this problem. However, the analysis of shape is more
complex, and it involves modifying the classical theory before applying it to
shape analysis. We brieﬂy mention two possible approaches to shape classiﬁ-
cation. In case there is a way to quantify pairwise shape diﬀerences, one can
use a nearest-neighbor classiﬁer. In this scheme, one computes the “distance”
between the test shape and all the given samples from the two populations
and declares the test class to be that of the nearest shape to the test shape.
A more comprehensive, and often eﬃcient, classiﬁer is the binary hypothesis
testing using the likelihood ratio test. Using results from the previous item,
i.e., estimated probability densities for each of the possible populations, one
can compute likelihoods of the test shape under diﬀerent classes and assign it

1.3 Speciﬁc Technical Goals
15
Samples from Class 1
Test Shape
Samples from Class 2
Fig. 1.20 Example of binary shape classiﬁcation: Which shape class does the test shape belong
to: left or right?
Fig. 1.21 Top: Examples of point clouds extracted from images for use in shape classiﬁcation.
Bottom: estimation of shapes in unordered, cluttered point clouds
to the class with the highest likelihood. Furthermore, one can also provide a
measure of conﬁdence in that assignment.
The second possibility, where the data is in the form of unordered (and clut-
tered) point clouds, is shown in Fig. 1.21. In this case one has to estimate as
well as classify the shape from the point cloud data. One type of shape esti-
mation involves selecting relevant subset of points, impose a certain ordering,
and connect them to form polygonal shapes. Some examples are shown in the
bottom row of Fig. 1.21.
6. Symmetry Analysis:
Symmetry of objects plays an important role in several applications, including
object design, packaging, medical diagnosis, and surgery. In this case one asks
questions of the type: Is a given object symmetric? What is the level (amount)
of asymmetry in an object? What is the nearest symmetric object for a given
asymmetric object? What are the planes(s) of symmetry of a given symmetric
object? Take a look at the sixteen planar shapes shown in Fig. 1.22 and an-
swer the following questions: Which shape is the most symmetric and which
one is the least symmetric? Is the bird contour more symmetric than the cat
contour? How can we deform the wine glass to make it perfectly symmetric?

16
1 Motivation for Function and Shape Analysis
Fig. 1.22 Symmetry analysis: What is the amount of asymmetry in these shapes?
Fig. 1.23 A system of statistical analysis of shapes in images
We will develop a formal approach for analyzing symmetry of objects using
deformations; here the objects are deformed until they become symmetric and
the amount of deformation measures the level of asymmetry.
In summary, our goal is to develop a fully statistical framework in which we
can treat shapes of functions and curves as random variables taking values in well-
deﬁned shape spaces and governed by underlying probability densities. This will
enable us to derive shape-based inferences using well-established statistical tech-
niques, albeit modiﬁed to account for the geometries of shape space. As an exam-
ple, Fig. 1.23 outlines a system for generating shape-based inferences using image

1.4 Issues and Challenges
17
data. In this system, the input images are processed to extract contours of interest,
either manually or automatically or both. These contours are mathematically rep-
resented as points on certain inﬁnite-dimensional shape spaces. The dissimilarities
between shapes of two contours are quantiﬁed using lengths of geodesic paths
between the corresponding points on the shape space (Task 2). This process natu-
rally includes registration of points along the curves (Task 1). Additionally, tools
for statistical analysis of shapes, such as computations of moments or probability
models on shape space, are derived. In particular, the concept of an average shape
is developed on the shape space (Task 3). Probability models, estimated from the
training shapes in a shape class (Task 4), are used for future Bayesian inferences
on image data. A contour estimated in this Bayesian framework can then be used
for classifying objects in images (Task 5). Geodesic paths between a shape and its
reﬂection are useful in symmetry analysis (Task 6).
1.4 Issues and Challenges
What makes statistical analysis of functions and shapes diﬃcult? It is quite easy
for us, as human beings, to observe and to analyze shapes and to perform many
of the aforementioned tasks without much diﬃculty. But doing so on computers
using mathematical representations is a completely diﬀerent proposition. First and
foremost, one has to formulate functions and curves as numerical quantities. As
described in Chaps. 4, 5, and 6, there are a variety of ways of representing functions
and curves mathematically, each with its own pros and cons. However, all these
representations entail certain common challenges from the perspective of function
and shape analysis. In this section, we discuss some of those challenges.
1. Invariance:
One important challenge in shape analysis comes from the fact that our notion
of shape is invariant to certain transformations, such as translations, rotations,
and rescaling. Abstractly, Kendall [49] described shape as a property that re-
mains unchanged under these transformations, and this aspect must be included
in our mathematical formulations. As an example, Fig. 1.24 shows sixteen dif-
ferent closed curves but they all have the same shape. Again, we have to develop
Fig. 1.24 All these closed curves have the same shape

18
1 Motivation for Function and Shape Analysis
mathematical representations and metrics in such a way that the pairwise dis-
tances between shapes of all these curves are zero. There are at least two ways
of obtaining such invariance:
• One is to come up with a mathematical representation such that all the
curves with the same shape map to the same point in the representation
space. For example, if we represent a curve based on the velocity function
along that curve, then that representation is automatically invariant to rigid
translation. No additional work is needed.
• The other is to deﬁne an equivalence relation such that diﬀerent curves with
the same shape are deemed equivalent; i.e., they belong to the same equiva-
lence class. The set of all such equivalence classes then becomes the desired
representation space. Any ensuing analysis of shapes will have to respect this
equivalence.
Another aspect of invariance of functions and curves, although not readily vis-
ible through pictures, is the issue of parameterization. A curve can be param-
eterized in many ways; a re-parameterization does not change its shape. Since
this text is primarily concerned with shapes of functions and curves, it becomes
critical to have the invariance with respect to re-parameterizations also. In the
case of curves, there exists a canonical parameterization, namely, the arc-length
parameterization, so that one can restrict each of the curves under study to this
parameterization. In other words, if a curve is not parameterized by arc length,
then re-parameterize it to make it arc length and then perform its shape anal-
ysis. This will potentially remove the parameterization variability. However,
as will be demonstrated in later chapters, it is often useful to allow curves to
have variable parameterizations, especially to improve matching of points across
curves and, thus, enlarge the representation space. In those cases, one has to
perform additional mathematical operations to ensure that the results remain
invariant to re-parameterization of the curves under study.
2. Nonlinearity:
Although there are a variety of mathematical representations for studying
shapes, many of them share the following important property. The spaces
formed by these representations are nonlinear. That is, they are not vector
spaces and one cannot use the classical vector calculus to perform operations
on shapes. Operations such as addition, multiplication, and subtraction are
not valid on these spaces. How can we add two shapes like we add two real-
valued variables? We cannot! Therefore, one has to devise a special framework
for performing “calculus of shapes,” a framework that will enable us to deﬁne
probability densities on shapes, to perform diﬀerentiation and integration on
shape spaces, to solve optimization problems, to take averages of shapes, and
to perform other statistical inferences.
3. Inﬁnite Dimensionality:
Lastly, the study of shapes of functions and curves introduces an additional
challenge of high, theoretically inﬁnite, dimensionality. This is because curves
are formally represented by functions, and spaces of functions are usually inﬁ-
nite dimensional. Although computer experiments will ultimately involve dis-
cretization of these functions into ﬁnite sets of points, the basic theory shall be
developed assuming continuous, or functional, representations.

1.5 Organization of this Textbook
19
In summary, the nonlinearity and the inﬁnite dimensionality of shapes, and
the need for certain invariances, make it diﬃcult to perform shape analysis of
curves. However, we show in this textbook that with proper mathematical tools
and concepts, this analysis can potentially be as powerful as the statistical analysis
of real-valued random variables. Indeed we will develop a rich set of tools for
performing statistical inferences on shape spaces and for handling the variability
and uncertainties associated with corrupted observations of shapes.
1.5 Organization of this Textbook
This textbook is organized as follows. In Chap. 2, we brieﬂy summarize some of
the commonly used techniques in shape analysis, along with discussions on their
strengths and limitations. In Chap. 3 and Appendix A, we provide a concise sum-
mary of mathematical concepts that are relevant to understanding our approach.
Speciﬁcally, they cover basic material from diﬀerential geometry, algebra, and func-
tional analysis. We have tried to highlight deﬁnitions and examples that are par-
ticularly important in shape analysis. We start the development of our approach
by considering function data analysis, especially the problem of pairwise registra-
tion of functions, in Chap. 4. This chapter introduces the Fisher-Rao Riemannian
metric and its extensions to general functions. This discussion is extended to study
shapes of planar curves in Chap. 5. It presents diﬀerent representations of parame-
terized curves, impositions of Riemannian structures, and constructions of optimal
shape deformations. Chapter 6 imposes an additional closure constraint, to restrict
to only closed planar curves, and provides their shape analysis. Changing focus
to statistical modeling, we introduce some basic techniques in modeling manifold-
valued random variables in Chap. 7. In particular, we discuss the computation of
sample means and covariances from random samples on these manifolds. These
ideas are ﬁrst applied to function data in Chap. 8 and then to planar curves in
Chap. 9. They introduce a framework for computing sample statistics and deﬁn-
ing statistical models for functions and planar curves. Chapter 10 extends this
shape analysis of curves to general Rn. We ﬁnish the textbook with a number of
shape-related topics in Chap. 11.

Chapter 2
Previous Techniques in Shape Analysis
A recent search on Google using the query: shape analysis results returned more
than 24 million hits, approximately 5 million for statistical shape analysis and
71 million for functional data analysis! Of course, not all of them are relevant to
our use of this phrase, but a large fraction of them partly or completely share
the same goals as this textbook. This shows the scope and involvement of these
topics in all areas of science and engineering and indeed life in general. This also
makes it diﬃcult for us to provide a complete picture of the previous eﬀorts in
shape analysis. We choose to focus on those works where statistical tools, based
on precise mathematical representations, have been used to address shape-related
issues.
Perhaps the earliest known eﬀorts in formalizing shape analysis came from
D’Arcy Thompson who tried to relate the shapes of functionally similar objects.
He explored the possibility of making shapes visually similar by applying simple
transformations, making them closer than they originally appeared. His treat-
ment of shapes appears in the form of a 1917 book titled On Growth and Form.
Figure 2.1 shows two examples of his work on using nonrigid transformations for
matching two seemingly diﬀerent but functionally similar objects. The top exam-
ple shows three crocodilian skulls and a way of comparing their shapes. D’Arcy
Thompson transformed the coordinate systems of the two skulls on the right so
as to look as close to the left skull as possible. Notice that the transformation
is applied to the coordinate system in which the object is represented and not
to the object itself. The appearance of the object changes accordingly. As an-
other example, he compared the shapes of two ﬁshes—Argyropelecus olfersi and
Sternoptyx diaphana—using a similar transformation of the coordinates, shown in
the bottom row.
An interesting aspect of shape analysis is that there are multitudes of techniques
that are available. We can try to distinguish them using the way they represent
shape themselves. Consider the problem of analyzing shapes formed by closed,
planar curves. While it seems natural to study their shapes by treating them as
parameterized curves, there are some other possibilities. Figures 2.2, 2.3 illustrate
some of these representations. The top row, second panel, in these ﬁgures show
points sampled along these curves to form an ordered set of points in R2 (or
a polygon). This representation is used in two methods—landmark-based shape
analysis and active shape analysis. A related possibility is to discard the ordering
of points and simply treat them as a set of points, as shown in the top row third
panel. While we lose some information when we discard the ordering of points,
© Springer-Verlag New York 2016
A. Srivastava, E.P. Klassen, Functional and Shape Data Analysis,
Springer Series in Statistics, DOI 10.1007/978-1-4939-4020-2 2
21

22
2 Previous Techniques in Shape Analysis
Fig. 2.1 Examples from D’Arcy Thompson’s work on measuring diﬀerences in shapes of related
objects by means of simple mathematical transformations. The top example studies variations in
shapes of crocodilian skulls, while the bottom example compares the shape of an Argyropelecus
olfersi with that of a Sternoptyx diaphana. (Data courtesy of Wikipedia Commons)
Curves
Ordered Samples
Point Cloud
Deformable Grid
Binary Image
Medial Axis
Signed-Distance
Level Set
Fig. 2.2 Diﬀerent representations of closed curves for use in shape analysis

2.1 Principal Component Analysis
23
Curves
Ordered Samples
Point Cloud
Deformable Grid
Binary Image
Medial Axis
Signed-Distance
Level Set
Fig. 2.3 Same as Fig. 2.2
we can now use set-theoretic methods for shape analysis. In contrast to the point-
based methods, there are also methods involving the full two-dimensional domains
that contain these shapes.
2.1 Principal Component Analysis
We start the discussion by introducing a standard technique for linear dimensional
reduction that is commonly used in multivariate statistical analysis. The basic
idea is to ﬁnd an orthogonal matrix that can project the observed data to a lower
dimensional vector space such that the structure of the original data is maximally
preserved.
Now we make this precise. Let x ∈Rn denote a vector of random variables,
with mean assumed to be to zero. The goal is to ﬁnd a d-dimensional subspace of
Rn, for d << n, such that the orthogonal linear projection of x on that subspace
is optimal. The optimality is measured using the expected value of the squared
error due to projection. If U ∈Rn×d is an orthogonal matrix, i.e., U T U = Id, then
U T x is an orthogonal projection of x and UU T x is a reconstruction of x. In PCA
the goal is to ﬁnd a ˆU ∈Rn×d such that ˆU T ˆU = Id and
ˆU =
argmin
{U∈Rn×d|UT U=Id}
(E[∥x −UU Tx∥2]),
(2.1)
where E denotes the expectation over the joint probability density function of x.
Simplifying this expression,
ˆU =
argmin
{U∈Rn×d|UT U=Id}
E[(x −UU T x)T (x −UU T x)] =
argmax
{U∈Rn×d|UT U=Id}
E[tr(U T xxT U)]
=
argmax
{U∈Rn×d|UT U=Id}
tr(U T E[xxT ]U) =
argmax
{U∈Rn×d|UT U=Id}
tr(U T (V ΛV T )U) ,

24
2 Previous Techniques in Shape Analysis
where E[xxT ]
∈Rn×n is the covariance matrix of x and V ΛV T is its singular
value decomposition (SVD). Hence, the solution ˆU is given by the ﬁrst d columns
of V provided that the entries of diagonal matrix Λ are non-increasing from top
to bottom.
In practical situations, where the covariance K is not known, and only some
observations of X are available, one uses the sample covariance matrix, ˆ
K ∈Rn×n:
ˆΣ =
1
M −1
M

m=1
(xi −¯x)(xi −¯x)T ,
where ¯x = 1
M
M

m=1
xi .
This calculation of ˆU is called principal component analysis (PCA) and is the most
commonly used technique for statistical dimensional reduction.
Once we have the principal components of a random vector x, we can model
it as a linear combination: ˆx ≈¯x + d
i=1 zi ˆU i, where ¯x is the sample mean, ˆU i
denotes the ith column of the matrix ˆU, and zis are independent and identically
distributed standard normal random variables.
2.2 Point-Based Shape-Analysis Methods
In this section we brieﬂy introduce shape analysis methods that represent
the underlying objects using points sampled along their boundaries. Let
x = {x1, x2, . . . , xk} ∈R2k and y = {y1, y2, . . . , yk} ∈R2k denote the sam-
pled points for any two shapes, respectively. Depending on the application, these
points are either ordered or unordered. We start with the more general case where
the points are unordered, and later we will assume a speciﬁc ordering.
2.2.1 ICP: Point Cloud Analysis
In case the points are unordered, we call these sets point clouds. One of the most
common methods for aligning and comparing point clouds is the iterative closest
point (ICP) algorithm. It uses the root-mean squared distance (RMSD) as objec-
tive function to pose an optimization problem as follows. (While the algorithm is
applicable to points in any Euclidean space, we will state it here for planar sets.)
Let ς : {1, 2, . . ., k} →{1, 2, . . ., k} be a mapping that associates an element of y
to each element of x; ς is also called the registration of y to x. Let Σ denotes the
set of all such mappings. Then, we solve for the alignment of two point sets as the
joint optimization problem:
RMSD =
min
O∈SO(2),ρ∈R+,T ∈R2,ς∈Σ
k

i=1
∥(T + ρOxi) −yς(i)∥2 .
(2.2)
The joint optimization problem over the space SO(2) × R+ × R2 × Σ is a complex
one. The variables O, ρ, and T are called the transformation variables since they
rotate, scale, and translate the set x, while ς is called the registration variable.

2.2 Point-Based Shape-Analysis Methods
25
It turns out that given transformation variables, it is relatively easy to solve for
the registration variable and vice-versa.
1. Given Registration Variable: If ς is kept ﬁxed, we can solve for the trans-
formation variables as follows. For the current ς, we utilize only the set
yς = {yς(i), i = 1, . . . , k} and one-by-one we will solve for the three trans-
formation variables, keeping others ﬁxed.
• Translation: In this step we solve for the optimal translation while keeping
the rotation and the scaling of x ﬁxed. In fact, let us assume that xt denotes
the conﬁguration of points in x at the current orientation and scale. Then,
T ∗= argmin
T ∈R2
k

i=1
∥yς(i)−xi,t−T ∥2= argmin
T ∈R2
k

i=1

yi −xi,t −T, yς(i) −xi,t −T

.
Taking the gradient of this cost function with respect to T and setting it to
zero, we get
T ∗= 1
k
k

i=1
yς(i) −1
k
k

i=1
xi,t .
(2.3)
• Rotation: In this step we solve for the optimal rotation while keeping the
translation and scaling ﬁxed. Once again assume that xt denotes the conﬁg-
uration of points in x at the current position and scale. Then, we deﬁne:
O∗= argmin
O∈SO(2)
k

i=1
∥yς(i) −Oxi,t∥2 = argmin
O∈SO(2)
k

i=1

yς(i) −Oxi,t, yi −Oxi,t

= argmin
O∈SO(2)
k

i=1
trace(yς(i)yT
ς(i) + xi,txT
i,t −2Oyς(i)xT
i,t)
= argmax
O∈SO(2)
trace(OA),
A =
k

i=1
yς(i)xT
i,t .
Let A = UΣV T , the SVD of A. Then, the optimal rotation is given by:
O∗=
⎧
⎪
⎨
⎪
⎩
UV T
if det(A) > 0
U

1 0
0 −1

V T
otherwise.
(2.4)
• Scale: Now we denote by xt the conﬁguration points in x at the current
position and orientation, and solve for the optimal scale:
ρ∗= argmin
ρ∈R+
k

i=1
∥yς(i) −ρxi,t∥2 = argmin
ρ∈R+
k

i=1

yς(i) −ρxi,t, yς(i) −ρxi,t

.
Taking the derivative of the right side with respect to ρ and setting it equal
to zero, we obtain:
ρ∗=
k
i=1

yς(i), xi,t

k
i=1 ⟨xi,t, xi,t⟩
.
(2.5)

26
2 Previous Techniques in Shape Analysis
2. Given Transformation Variables: Now we ﬁx the transformation variables,
and treat ς as the variable of interest. Now we need to solve for:
ς∗= argmin
ς∈Σ
k

i=1
∥yς(i) −xi,t∥2 ,
where xt denotes a transformed version of x. The solution to this problem
depends on what further assumptions we make about ς. If we constrain ς to be
one-to-one, then this problem becomes the so-called assignment problem. The
solution to the assignment problem comes from a famous algorithm called the
Hungarian algorithm. However, if this additional constraint is not imposed, i.e.,
ς is allowed to take the same values for diﬀerent arguments, then the solution
comes from the nearest-neighbor search. That is,
ς∗(i) = argmin
j=1,2,...,k
∥xi −yj∥.
(2.6)
Since the individual solutions are well deﬁned, a reasonable approach is to iterate
between the two solutions until convergence. This is often called the ICP algorithm.
We state this algorithm next.
Algorithm 1 (ICP Algorithm). Inputs: x, y, initial values of O, ρ, T ; stopping
criterion ϵ.
1. Find the transformed set xi,t = ρOxi + T .
2. For each i, solve for ς∗(i) using either the nearest-neighbor search (Eq. 2.6) or
the Hungarian algorithm.
3. Compute the objective function k
i=1 ∥xi,t −yς(i)∥2. If the change in its value
from the previous iteration is less than ϵ, then stop. Else, continue.
4. For the ﬁxed ς∗, solve for the transformation variables using Eqs. 2.3, 2.4,
and 2.5. Return to Step 1.
Figure 2.4 shows three examples of matching point clouds using ICP algorithm.
In each row, the ﬁrst two panels show the two clouds y and x, the third panel
shows the optimal transformation of x and the resulting registration ς∗, and the
ﬁnal panel shows the evolution of the cost function versus the iteration index.
There are certain pros and cons associated with the ICP algorithm. Since this
method is based on matching points, it is generally very fast and can be imple-
mented in real time. One of the main strengths of the ICP algorithm is that it
estimates the registration ς, rather than assuming that it is known. This is cer-
tainly important in situations where data consists of unordered points. However,
it has limited utility in situations where ordering of points is available. Another
important limitation of ICP is that the resulting matching function is not a proper
distance. In fact, it is not even symmetric. We illustrate an example in Fig. 2.5
where the RMSD values for two point sets are computed twice, once for the orig-
inal x and y and then for the roles switched. The RMSD values are found to be
3.374 and 1.5356, respectively. This asymmetry in RMSD values arises due to the
nature of the nearest neighbor registration in Eq. 2.6. This process, by deﬁnition,
takes into account all the elements of x but not necessarily all the elements of y.
In some situations one can force the symmetry of RMSD by replacing the nearest
neighbor registration with a more symmetric assignment problem. As mentioned

2.2 Point-Based Shape-Analysis Methods
27
0
5
10
15
20
1
2
3
4
5
6
7
8
9
0
5
10
15
20
0
20
40
60
80
100
120
0
5
10
15
20
0
50
100
150
200
250
300
350
x
y
ς∗
Objective Function
Fig. 2.4 Examples of matching point clouds using ICP algorithm
0
10
20
30
40
50
0
100
200
300
400
500
0
10
20
30
40
50
0
100
200
300
400
500
Fig. 2.5 Asymmetry of RMSD. The RMSD for the top case is 3.374 and for the bottom case is
1.5356. The only diﬀerence between them is the order in which shapes are considered
earlier, here the two conﬁgurations are assumed to have same number of points,
and the mapping ς : {1, 2, . . ., k} →{1, 2, . . ., k} is assumed to be one-to-one and
onto. In other words, one ﬁnds for each element of x a unique element of y (using
the Hungarian algorithm) while minimizing the objective function in Eq. 2.2. In
Fig. 2.6, the two RMSD values for the nearest-neighbor registration are 1.7149 and
0.9181, while the RMSD value for the Hungarian algorithm is 5.2618.
Another limitation in the ICP based matching is the locality of the solution.
The ﬁnal transformation and registration variables are highly dependent on their
initial values. Diﬀerent initializations can lead to diﬀerent results, as demonstrated
in Fig. 2.7. In this example, we take a set x and rotate it arbitrarily to generate
the point set y. Then, we try to match them using ICP algorithm. Since x and
y diﬀer only in rigid motion, the resulting RMSD between them should be zero,

28
2 Previous Techniques in Shape Analysis
y
Registration (Hungarian)
Registration (NN)
x
Fig. 2.6 Diﬀerent optimal registrations of two points sets using the nearest-neighbor (NN)
algorithm and the Hungarian algorithm
0
10
20
30
40
50
5
10
15
20
25
30
35
40
45
50
0
10
20
30
40
50
0
10
20
30
40
50
Fig. 2.7 Dependence of ICP on initial condition
which is the case in the top row. However, in the solution displayed in the bottom
row, the two sets do not align completely and the resulting RMSD is not zero.
Another feature of ICP algorithm is the instability in matching resulting from
the Nearest-neighbor registration. A small transformation of x can result in a com-
pletely diﬀerent registration and, thus, a diﬀerent RMSD value. This is illustrated
in Fig. 2.8 where only a slight initial transformation of x results in two vastly
diﬀerent RMSD values.
In terms of the goals laid out in Sect. 1.3, this method can at best provide the
ﬁrst item: quantiﬁcation of shape similarities/dissimilarities, but not any one of the
remaining items. For instance, in its original form, it cannot help provide a notion
of a statistical mean of shapes. Even in that ﬁrst item, not all the invariances are
satisﬁed.

2.2 Point-Based Shape-Analysis Methods
29
Fig. 2.8 Two seemingly similar matches between two shapes results in RMSD values of 5.6508
and 0.0053, respectively
2.2.2 Active Shape Models
In case one assumes that the point sets are sampled from curves and there are
known orderings associated with them, the task of comparing them becomes con-
siderably easier. Since the two sets x and y are ordered, it is natural to use ς(i) = i
as the registration. (Here we are assuming that the number of points on x and y
are same.) Now that the registration is taken care oﬀ, one only needs to solve for
the transformation variables. This is accomplished as follows.
We start with the simple change in notation. It is often convenient to identify
points in R2 with elements of C, i.e., xi ≡zi = (xi,1 + jxi,2), where j = √−1.
Thus, in this complex representation, a conﬁguration of k points x is now z ∈Ck.
Before analyzing the shape of z, it is “standardized” by moving its centroid to the
origin (of the coordinate system):
zi →(zi −1
k
k

i=1
zi) .
To remove the scale variability, z is rescaled to have norm one, i.e., z →z/∥z∥.
(These transformations eﬀectively project a conﬁguration in the orthogonal section
of its orbit under the nuisance group—translation and scaling—as explained in
Sect. 3.14). Then, one uses tools from standard multivariate statistics to analyze
and model them. So far, the translation and the scale variability of a conﬁguration
are removed but the rotation remains. That is, two conﬁgurations, z and a rotation
of z, will have a nonzero distance between them even when they have the same
shape. Using an additional step of rotational alignment solves the problem, as
follows:
φ∗= argmin
φ∈S1
∥z1 −ejφz2∥2
= argmin
φ∈S1 (∥z1∥2 + ∥z2∥2 −2ℜ(

z1, ejφz2

))
= argmax
φ∈S1 (ℜ(e−jφ ⟨z1, z2⟩)) = θ,
where
⟨z1, z2⟩= rejθ .
(2.7)
Here ℜdenotes the real part of a complex number. The last inner product is
the Hermitian inner product between z1 and z2. The distance between the two
conﬁgurations is then ∥z1 −ejφ∗z2∥=

2(1 −r). The corresponding optimal

30
2 Previous Techniques in Shape Analysis
Fig. 2.9 Examples of geodesic paths between same shapes using ASM
deformation from one shape to another is simply a straight line between z1 and
ejφ∗z2, i.e., α(τ) = (1 −τ)z1 + τejφ∗z2 for τ ∈[0, 1].
One remaining issue in this analysis is the selection of a starting point, z1 or the
origin, on a closed curve. If there are k points sampled on a curve, then there are k
candidates for the origin. The solution is to select the best seed during a pairwise
comparison of conﬁgurations. That is, select any point on the ﬁrst conﬁguration
as the origin for the ﬁrst shape and try all k points in the second conﬁguration
as the origin for the second shape. Of those, select the one that results in the
smallest distance from the ﬁrst conﬁguration. Figure 2.9 shows several examples
of these deformations: one between a pair of human silhouettes, one between a pair
of hands, and so on. These deformations have been computed using k = 100 points
on each conﬁguration so that the resulting polygons look like smooth curves.
While this approach is computationally very eﬃcient, it has some limitations in
terms of shape analysis. As discussed later in Sect. 2.2.4, a major limitation of this
technique is the selection and registration of points across curves. In particular,
the fact that it assumes a linear registration of points leads to unnatural shape
comparisons and analysis. Also, this method does not completely take care of the
scale variability of objects in analyzing their shapes. Although it starts by rescaling
the complex vectors to be of unit length, it ignores this constraint in the remaining
analysis. This particular problem is addressed by the next method that carefully
restricts its analysis to only the unit-length vectors.
2.2.3 Kendall’s Landmark-Based Shape Analysis
David Kendall was among the earliest researchers to identify a certain space, whose
elements represent shapes of interest, as a Riemannian manifold. This pioneering
idea has become a guiding principle for many subsequent ideas in shape analysis,
including those presented in this text. Furthermore, most of the terminology used
in this text, for instance, the use of pre-shape and shape spaces, is also borrowed
from that landmark-based shape analysis literature. In view of the historical im-
portance and relevance of this approach to our framework, we present it in some
detail.
Pre-shape Space Similar to ASM, an object is also represented here by an
ordered set of points, called landmarks and for the purpose of shape analysis,
the objects are replaced by the corresponding landmarks. No other information is

2.2 Point-Based Shape-Analysis Methods
31
retained from the objects. Since shapes are treated as being invariant with respect
to rotations, translations, and scales, one develops an algebraic representation that
incorporates this invariance. In this discussion, we will again identify points in R2
with elements of C, i.e., xi ≡zi = (xi,1 + jxi,2), where j = √−1. A conﬁguration
of k points is thus denoted by a z ∈Ck. Before analyzing the shape of z, it is
“standardized” by moving its centroid to the origin (of the coordinate system):
zi →(zi −1
k
k
i=1 zi) and by rescaling to have norm one, i.e., z →z/∥z∥. This
results in a set:
C = {z ∈Cn|1
k
k

i=1
zi = 0, ∥z∥= 1} .
C is essentially an orthogonal section of the translation and scaling groups on Cn
(see Example 3.14) is not a vector space because if a1, a2 ∈R and z1, z2 ∈C ,
then a1z1 + a2z2 is typically not in C , due to the unit norm constraint. However,
C is a unit sphere and one can utilize the geometry of a sphere to analyze points
on it. It is called a pre-shape space because there are many vectors that have the
same shape but diﬀerent values in C . This is because the rotation has not yet been
removed from this representation. Under the Euclidean metric, the shortest path
between any two elements z1, z2 ∈C , also called a geodesic, is given by the great
circle: αksa : [0, 1] →C , where
αksa(τ) =
1
sin(ϑ) [sin(ϑ(1 −τ))z1 + sin(τϑ)z2] ,
and ϑ = cos−1(ℜ(⟨z1, z2⟩)) .
(2.8)
The geodesic distance between z1 and z2 is given by dc(z1, z2) = ϑ.
Shape Space In order to compare the shapes represented by z1 and z2, we need
to align them rotationally, as was done earlier in ASM, but the shape space is
deﬁned more formally this time. Let [z] be the set of all rotations of a conﬁguration
z according to:
[z] = {ejφz|φ ∈S1}
⊂C .
One deﬁnes an equivalence relation on C as follow: z1 ∼z2 if there exists an angle
φ ∈S1 such that z1 = ejφz2. Equivalence classes under this relation are exactly the
sets [z] for diﬀerent z ∈C . The set of all such equivalence classes is the quotient
space C /U(1) (see Deﬁnition 3.15), where U(1) = SO(2) = S1 is the set of all
rotations in R2. This space is called the complex projective space and is denoted
by CPk−1. It is quickly checked that the action of S1 on C , deﬁned as:
S1 × C →C ,
(φ, z) = ejφz ,
is by isometries under the standard Euclidean metric ∥· ∥. That is, for any
z1, z2 ∈C , we have ∥z1 −z2∥= ∥ejφz1 −ejφz2∥for any φ ∈S1. Therefore, using
Theorem 3.17, the Euclidean distance descends to the quotient space CPn−1, and
we can deﬁne:
ds([z1], [z2]) = min
φ1
dc(ejφ1z1, z2) = min
φ2∈S1 dc(z1, ejφ2z2) .
The last equality comes from the isometric nature of the group action. A geodesic
between two elements z1, z2 ∈CPn−1 is given by computing αksa between z1 and
eφ∗z2, where φ∗is the optimal rotational alignment of z2 to z1. As shown in the

32
2 Previous Techniques in Shape Analysis
Fig. 2.10 Examples of geodesic paths between same shapes using Kendall’s shape analysis
previous item, this optimal rotation is given by φ∗= θ, where ⟨z1, z2⟩= rejθ. With
this rotation, we get ⟨z1, z∗
2⟩= r, and the geodesic path between two equivalence
classes, represented by z1 and z2, is given by:
αksa(τ) =
1
sin(ϑ) [sin(ϑ(1 −τ))z1 + sin(τϑ)z∗
2] ,
(2.9)
where ϑ = cos−1(ℜ(⟨z1, z∗
2⟩)) = cos−1(r). The length of the geodesic is given by
ϑ and that quantiﬁes the diﬀerence in shapes of the boundaries represented by z1
and z2. Figure 2.10 shows several examples of geodesic paths between the same
shapes as for the ASM examples.
One of the most important contributions of this approach has been the advanced
statistical framework that has been developed using it. For a collection of shapes
[z1], [z2], . . . , [zn] in CPk−1, deﬁne the sample mean to be the quantity:
μksa = argmin
[z]∈CPk−1
n

i=1
ds([z], [zi])2 ,
(2.10)
where ds is the geodesic distance as deﬁned earlier. This mean is actually computed
using an iterative algorithm that uses the gradient of the cost function given in
Eq. 2.10 to iteratively update the estimate. (This algorithm is presented for com-
puting means on general Riemannian manifolds later in Chap. 7.) Once the mean
μksa is computed, one can project all the observed shapes into the tangent space
of CPk−1 at μksa. Since this tangent space is a ﬁnite-dimensional vector space,
one can use the standard multivariate calculus for posing and solving statistical
problems. Dryden and Mardia [28] provide an excellent treatment on this subject
with lots of illustrative examples. In the methodology presented in the text, we
will borrow several ideas, notations, and deﬁnitions from this approach. In that
sense, this landmark-based analysis of shapes can be considered a precursor to the
approach outlined in this text.
2.2.4 Issue of Landmark Selection
Although Kendall’s approach succeeds in preserving the unit-length constraints on
the landmark conﬁgurations, it does not address a very important practical issue:
How to systematically select points on objects, say curves, to form representative

2.3 Domain-Based Shape Representations
33
b1 (uniform sampling) b2 (uniform sampling) b2 (non-uniform sampling)
Fig. 2.11 Selection of landmarks using the uniform and a convenient nonuniform sampling.
Nonuniform sampling allows a better matching of features between β1 and β2
point sets? This process is diﬃcult to standardize and diﬀerent selections can lead
to drastically diﬀering solutions. This issue is present in any point-based approach,
including the ASM method discussed above. While it may be tempting to sample
a curve uniformly along its length, i.e., parameterize a unit-length curve β using
arc length and sample {β(ti)|i = 0, 1, 2, . . ., n} where ti = i/n, the results are not
always good since this forces a particular registration of points. The point β1(ti) on
the ﬁrst curve is matched to the point β2(ti) on the second curve, irrespective of the
shapes involved. Figure 2.11 illustrates this point using an example. Shown in the
left two panels are two curves: β1 and β2, sampled uniformly along their lengths.
For ti = i/4, i = 1, 2, 3, 4 the corresponding four points on each curve {β1(ti)} and
{β2(ti)} are shown in the same color. While two of the four pairs seem to match
well, the pairs shown in red and green fall on diﬀerent parts of the body, resulting
in a mismatch of features. This example shows the pitfall of using uniform sampling
of curves. In fact, any predetermined sampling and preregistration of points will, in
general, be problematic. A more natural solution is to treat the boundaries of ob-
jects as continuous curves, rather than discretize them in point sets at the outset,
and ﬁnd an optimal (perhaps nonuniform) sampling, such as the one shown in the
rightmost panel, that better matches features across curves. This way one can de-
velop a more comprehensive solution, including a theory and algorithms, assuming
continuous objects and one discretizes them only at the implementation stage.
2.3 Domain-Based Shape Representations
The next set of representations can be described as domain-based in the sense that
the whole space in which the shape is present is taken into account. In the case of
planar shapes, each shape is treated as a subset of R2, say [0, 1]2, and one represents
shapes as some predetermined family of functions on that domain. Then, shapes
are compared and analyzed by comparing the corresponding functions. We brieﬂy
describe some of these methods next.
2.3.1 Level-Set Methods
One prominent idea has been to use level sets to study dynamics of closed contours
[85]. In the planar case, the contours are embedded in a subset of R2 in an arbitrary

34
2 Previous Techniques in Shape Analysis
0
0
50
100
150
–50
0
50
100
50
100
150
0
0
50
100
150
–100
0
100
50
100
150
0
0
50
100
150
–50
0
50
100
50
100
150
0
0
50
100
150
50
0
–50
100
50
100
150
Fig. 2.12 Level-set representation of closed planar curves (leftmost panel) using signed-distance
functions (remaining three panels)
way, say in [0, 1]2, and are represented by certain functions on [0, 1]2 such that
the level curves of those functions are the original contours. A common choice of
function is the signed-distance function. Let β denotes a simple closed curve in R2.
Deﬁnition 2.1. A signed-distance function φ : R2 →R≥0 is a function whose
value at a point is the Euclidean distance in R2 to the nearest point on the curve.
Additionally, the function values for points inside the contour are negative:
φ(x) =
−d(x, β), x is inside the curve
d(x, β),
otherwise
,
d(x, β) = inf
y∈β ∥x −y∥.
(2.11)
It is easy to see that the set {x ∈R2|φ(x) = 0} is exactly the set of points on
the curve β. In other words, β is the zero level set of φ. Figure 2.12 shows two
examples of this representation: the left curve in each row is represented by its
signed-distance function shown in three diﬀerent ways in the remaining panels.
The second column of this ﬁgure shows the signed-distance functions as gray scale
images, while the last two columns show them as height functions from diﬀerent
views.
For closed contours β1 and β2 with given embeddings in [0, 1]2, let φ1 and φ2
denote the corresponding signed-distance functions on [0, 1]2. Then, the shapes of
β1 and β2 can be compared by using the L2 metric:
d(β1, β2) = ∥φ1 −φ2∥=

[0,1]2(φ1(x) −φ2(x))2 dx .
There are several issues with this distance function. Firstly, any rigid motion of
the βs will change the distance between the corresponding level functions. One
can handle two of the transformations: translation and scaling, by pre-processing
the curves β1 and β2 as in the previous sections. For example, we can translate
them so that their mean is at the origin and scale them so that their norm is one:
β(t) →β(t) −¯β(t),
¯β(t) =

β(t)dt ,

2.3 Domain-Based Shape Representations
35
and
β(t) →
β(t)

∥β(t)∥2dt
.
The removal of the rotation requires solving an optimization problem, as was the
case in the earlier frameworks.
While there are distinct advantages of using this representation in modeling
dynamical evolution of curves, especially when the curves are allowed to split
and merge and to change topologies, their use in shape analysis is more diﬃcult.
The main issues come in imposing invariances with respect to shape-preserving
transformations. Secondly, it is diﬃcult to construct an optimal deformation of
one shape into another using this framework. In the earlier two methods, ASM
and KSA, we gave equations of geodesic paths whose lengths achieve the distance
used to compare shapes. Since the distance between any two level-set functions φ1
and φ2 is L2, it is reasonable to connect using the straightline:
ψ(τ) = (1 −τ)φ1 + τφ2 .
Each element along this path, ψ(τ), is a function from [0, 1]2 to R, but is it a valid
signed-distance function? An important property of a signed-distance function
φ : [0, ]12 →R is that ∥∇φ(x)∥= 1 for all x ∈[0, 1]2. It is easy to see that, for a
τ ∈(0, 1),
∥∇ψ(τ)(x)∥= ∥(1 −τ)∇φ1(x) + τ∇φ2(x)∥
is not necessarily equal to one. Hence, an intermediate point in the path ψ is not
a proper signed-distance function, and it becomes diﬃcult to talk about the level
sets of this function as a shape.
Although this framework is not ideally suited for shape analysis of objects,
it has been used extensively for extracting boundaries of contours in images. Its
strength in dealing with changes in topology is very helpful in ﬁnding multiple
objects in images.
2.3.2 Deformation-Based Shape Analysis
Another fundamental idea for shape analysis comes from Grenander’s theory of
deformable templates. In this approach, the shapes are considered as points on
an inﬁnite dimensional, diﬀerentiable manifold, and variations between shapes are
modeled by actions of Lie groups on this manifold. Low-dimensional groups, such
as rotation, translation, and scaling, change the object instances keeping the shape
ﬁxed, while high-dimensional groups, such as the diﬀeomorphism group, smoothly
change the object shapes. This representation forms the mathematical basis of
deformable templates.
A brief introduction to this approach follows: Let I be the space of all gray
scale images deﬁned on a domain D; so I = RD. Here, D is usually a unit square
in R2 for two-dimensional images or a unit cube in R3 for three-dimensional im-
ages. Changes in shapes of objects contained in images are obtained by deforming
the domains. Let Γ be the space of diﬀeomorphisms from D to itself. Then, the
elements of Γ can be used to deform the elements of I according to the map:
γ · I(x) = I(γ(x)), γ ∈Γ,
x ∈D , I ∈I .

36
2 Previous Techniques in Shape Analysis
Fig. 2.13 (a) Image I1, (b) Image I2, (c) Deformed image is I1(γ), (d) deformation vector ﬁeld
generating γ, (e) deformed grid γ(D)
Associated with an image I, there is an equivalence class of images that can be
reached by deforming that I.
[I] = {γ · I : γ ∈Γ} .
Similar to the previous section, this is deﬁned to be the equivalence class of shapes
associated with I.
A transformation between two images I1 and I2 can be modeled by diﬀeomor-
phically aligning I1 to I2. The resulting registration problem on the image space
is given by:
E(I1, I2) = min
γ

λ1∥I2 −γ · I1∥2 + λ2R(γ)

,
(2.12)
where ∥· ∥is an appropriate distance on the image space I and R is a roughness
penalty on Γ. Shown in Fig. 2.13 are two examples of this idea using the MRI
images of mouse brains; in each case the two images are taken a few weeks apart
to measure the anatomical growth. The leftmost image is I1, the second image is
I2, and the third image is the deformed I1 ◦γ, with γ being the minimizer of the
cost function in Eq. 2.12. The right two panels show this optimal γ in a couple
of ways: using the displacement vector ﬁeld (γ(x) −x) and using a deformed grid
(γ(D)).
Although this method is quite eﬀective in studying variability of objects as they
appear in images, taking into account both the boundaries and the interiors, the
invariance to translation, rotation, and scaling are diﬃcult to apply here. Also, it
is computationally expensive to deform the full image domain D when our interest
is only in some curves or surfaces contained in D.
2.4 Exercises
Theoretical and computational exercises associated with techniques presented in
this chapter are covered in the later chapters.

2.5 Bibliographic Notes
37
2.5 Bibliographic Notes
D’arcy Thompson’s work appears in the second edition of his book On Growth
and Form [114].
ICP is a widely used algorithm and some of the earlier references for ICP are
[15, 9, 25]. Cootes and Taylor introduced the active shape model framework in
a series of papers including [26]. Osher and Fedkiw [84] provide a recent, com-
prehensive review of the techniques used in level-set analysis. A good reference
for principle component analysis is [41]. The ﬁrst formal mathematical/statistical
theory of shapes is due to the pioneering eﬀorts of David Kendall [50, 49]. A
remarkable body of work on this type of shape analysis exists due to works of
Bookstein [18], Dryden and Mardia [28], Small [102], Le [49, 66], and several oth-
ers. Diﬀeomorphism-based image matching and shape analysis is a major focus
in Computational Anatomy [115, 33, 32, 7]. Interested readers are encouraged to
read [34] for further information.

Chapter 3
Background: Relevant Tools
from Geometry
Developing mathematical and/or statistical analysis of shapes involves two
important steps: (i) formulating mathematical representations for shape and
(ii) devising a framework for performing calculations under those representations.
In a general shape analysis system, one would like to accomplish the tasks laid
out in Sect. 1.3, namely, quantify similarities and diﬀerences between shapes,
calculate integrals (or probabilities) on spaces of shapes, optimize cost functions
on shape spaces, and design a representative shape or a template for a class of
shapes. Three peculiar aspects of shape analysis make it necessary to involve some
advanced mathematical tools:
• Nonlinearity →Diﬀerential Geometry:
There are two important reasons for using diﬀerential geometry in the study
of shapes. First, the shapes themselves are often curved, or nonlinear, objects.
Therefore, the commonly used tools for vector-space calculus—addition, multi-
plication, diﬀerentiation, and integration—do not suﬃce for the study of these
shapes. Also, irrespective of how shapes are mathematically represented, the
resulting spaces of shapes are also nonlinear, i.e., they are not vector spaces.
Thus, the study of shapes, as well as spaces of shapes, both require ideas from
diﬀerential geometry, the part of mathematics that deals with calculus on non-
linear manifolds. Since shape spaces are studied as nonlinear manifolds and
individual shapes as points on these manifolds, diﬀerential geometry plays a
very important role in shape analysis.
• Invariance →Algebra:
Another aspect of analyzing shapes (of objects) is their invariance to certain
transformations, such as translations, rotations, and scaling. Abstractly, shape
is described as a property that remains unchanged under these transformations,
but how should one incorporate this phenomenon into mathematical formula-
tions? In the framework used in this book, originally proposed by Grenander
[32] and Kendall [50], these transformations are conveniently represented as ac-
tions of groups on shape spaces. Equivalence classes of shapes are then deﬁned
as orbits under these group actions. Hence, algebra, the study of groups and
group actions, becomes important in the discussion of invariant shape analysis.
• Inﬁnite Dimensionality →Functional Analysis:
Lastly, the study of shapes of (continuous) curves introduces an additional
challenge of high, often inﬁnite, dimensionality. This is because curves are
formally represented as functions, and spaces of functions are usually inﬁnite
© Springer-Verlag New York 2016
A. Srivastava, E.P. Klassen, Functional and Shape Data Analysis,
Springer Series in Statistics, DOI 10.1007/978-1-4939-4020-2 3
39

40
3 Background: Relevant Tools from Geometry
dimensional. To analyze these spaces of functions, we require some tools from
functional analysis. Although computer experiments will ultimately involve dis-
cretizing these functions into ﬁnite sets of points, the basic theory shall be
developed assuming continuous, or functional, representations.
In summary, the three main mathematical tools for the shape analysis presented
in this book are diﬀerential geometry, algebra, and functional analysis. Along with
the appendix, this chapter serves as a basic and a brief refresher for some pertinent
parts of those topics. If the readers wish to gain a deeper understanding of these
areas, they are encouraged to refer to textbooks on these subjects. We recommend
Boothby [19], Lang [64], and Do Carmo [23] for diﬀerential geometry; Munkres [83]
and Milnor [78] for topology; Lang [65] for algebra; and Rudin [96] for functional
analysis.
3.1 Equivalence Relations
In order to focus on the analysis of shapes and their representations as mathe-
matical quantities, and especially to understand invariance of shapes to diﬀerent
transformations (see item 1 in Sect. 1.4), we need to use the concept of equiva-
lence relations. Since a shape will typically be represented by many elements of a
representation space, we will deﬁne equivalence relations to unify these elements.
This will result in a consolidated space in which each shape is represented by a
unique class. In this section, we give the deﬁnition of an equivalence relation and
present some examples.
Deﬁnition 3.1 (Equivalence Relation). A relation ∼on a set X is called an
equivalence relation if, for all x, y, z ∈X, we have the following properties:
• reﬂexivity, i.e., x ∼x,
• symmetry, i.e., x ∼y =⇒y ∼x, and
• transitivity, i.e., x ∼y, y ∼z =⇒x ∼z
The equivalence class of x ∈X, denoted by [x], is the set of all y ∈X such that
y ∼x. For a set X, the quotient space of X under the equivalence relation ∼
is the set of all equivalence classes in X and is denoted by X/∼. An equivalence
relation partitions the set X into disjoint sets, each of which is an equivalence
class. In other words, any two equivalence classes [x] and [y] are either disjoint or
they are identical; they cannot have partial overlaps. Here are some examples.
Example 3.1. 1. For the set X={1, 2, 3, 4}, deﬁne an equivalence relation ∼such
that 1 ∼3 and 2 ∼4. For this relation, we have the quotient space X/∼=
{{1, 3}, {2, 4}}.
2. For X = Rn\{0} and any x, y ∈X, deﬁne x ∼y if there exists a nonzero
scalar a ∈R, such that y = ax. For this relation, an equivalence class is [x] =
{ax|a ∈R, a ̸= 0}, the straight line passing through the origin that contains
x (not including the origin). The quotient space X/∼can be thought of as
the set of all lines passing through the origin. If we represent each line by its
intersection with one hemisphere of the unit sphere, we have a way of visualizing
this quotient space as this hemisphere. Figure 3.1 shows an example for n = 2 in
the left panel. The case of points lying on the equator is exceptional. For those

3.2 Riemannian Structure and Geodesics
41
Fig. 3.1 Pictorial illustrations of the equivalence classes presented in Example 3.1, item 2 (left)
and item 3 (right). Equivalence classes are shown in dark lines and their representatives in broken
lines
points, we need to identify each point with its antipodal point since they both
lie on the same line through the origin. This space is called (n −1)-dimensional
projective space.
3. The equivalence relation complementary to the previous case is for x, y ∈Rn,
x ∼y when |x| = |y| (under the two norm). In this case, all the points lying on
a sphere (centered at the origin) are equivalent and the set of all spheres in Rn
centered at the origin forms Rn/∼. Each sphere can simply be represented by
its radius, so Rn/∼in this case is identical to R≥0, the set of non-negative real
numbers. Figure 3.1 shows an example for n = 2 in the right panel.
Later on, when we introduce mathematical representations of shapes of interest,
we will apply the concept of equivalence relations to unify shape representation
and to facilitate invariance.
Next, we shift attention to some basic tools from diﬀerential geometry for use
in shape representation and analysis.
3.2 Riemannian Structure and Geodesics
To refresh some basic concepts in diﬀerential geometry, we refer the reader to Ap-
pendix A, where we introduce the concept of a diﬀerentiable manifold, its tangent
spaces, and submanifolds. Here we introduce the relatively advanced concept of a
Riemannian manifold. One of the most fundamental tasks in analyzing shapes as
points on a manifold is to calculate distances between them. Distances on shape
manifolds can, for instance, be used to quantify dissimilarities between shapes.
These distances will be calculated by constructing shortest paths between shapes
and by measuring the lengths of these paths. However, both the deﬁnition of a
shortest path and the measurement of its length requires the additional concept
of a Riemannian metric. In this section, we introduce this concept and illustrate
it with a few examples.
This terminology needs some additional clariﬁcation. A Riemannian metric,
often written simply as a metric, should be distinguished from a distance function
on a space. As we shall see, the metric is deﬁned inﬁnitesimally using elements
of the tangent space at a point, whereas the distance function is deﬁned between
any two elements of a space. We will also see that a metric can be used to deﬁne

42
3 Background: Relevant Tools from Geometry
a distance function, while not every distance function may be derived from a
Riemannian metric.
Informally, a Riemannian metric is an inner product deﬁned on the tangent
spaces of the manifold. Since tangent spaces are vector spaces, we begin by deﬁning
a bilinear form on a vector space.
Deﬁnition 3.2. A bilinear form on a vector space V over R is a map Φ : V ×V →
R, that satisﬁes the following requirements:
1. Φ(αv1 + βv2, w) = αΦ(v1, w) + βΦ(v2, w)
2. Φ(v, αw1 + βw2) = αΦ(v, w1) + βΦ(v, w2)
where α, β ∈R and v, w, v1, v2, w1, w2 ∈V .
A bilinear form is symmetric if Φ(v, w) = Φ(w, v) and skew symmetric if Φ(v, w) =
−Φ(w, v). A symmetric form becomes an inner product if it is positive deﬁnite,
i.e., Φ(v, v) ≥0 and Φ(v, v) = 0 ⇐⇒v = 0.
Deﬁnition 3.3. A Riemannian metric on a diﬀerentiable manifold M is a map
Φ that smoothly associates to each point p ∈M a symmetric, bilinear, positive
deﬁnite form on the tangent space Tp(M).
A diﬀerentiable manifold with a Riemannian metric on it is called a Rieman-
nian manifold. Very often we will reduce notation by writing Φ(v1, v2) simply
as ⟨v1, v2⟩. So, ⟨·, ·⟩will not always imply the Euclidean inner product; the actual
deﬁnition will come from the Riemannian metric being used.
Here are a few examples.
In Example 3.2, it is assumed that the reader is familiar with the material in
Appendix A.1.
Example 3.2. 1. As stated in Example A.1, the tangent space at any point in Rn
is Rn itself. It becomes a Riemannian manifold with the Riemannian metric
Φ(v1, v2) = vT
1 v2, the standard Euclidean product. Here, v1 and v2 are assumed
to be column vectors so that vT
1 v2 is their scalar product.
2. The upper half-plane R2
+ = {p = (p1, p2) ∈R2|p2 > 0} is a Riemannian mani-
fold with the hyperbolic inner product, deﬁned as follows. For any point p ∈R2
+,
the tangent space Tp(R2
+) is R2. For any two vectors v1, v2 ∈Tp(R2
+), we deﬁne
the hyperbolic inner product as:
Φ(v1, v2) = 1
p2
2
vT
1 v2.
(3.1)
3. We examined the manifold O(n) in Appendix A.1, Example A.6 and calculated
its tangent space: TAO(n) = {AX : X is skew symmetric}. Deﬁne the inner
product for any Y, Z ∈TAO(n) by Φ(Y, Z) = trace(Y ZT ), where trace denotes
the sum of diagonal elements. With this metric, O(n) becomes a Riemannian
manifold. It will be important later to note that this metric is preserved under
left and right multiplication by elements of O(n):
Φ(AY, AZ) = trace(AY (AZ)T ) = trace(AY ZT AT ) = trace(AY ZT A−1)
= trace(Y ZT ) = Φ(Y, Z).

3.2 Riemannian Structure and Geodesics
43
This shows the metric is preserved by left translations; for right translations,
the calculation is similar.
4. In case of the unit circle, the tangent space at any point p is given by
Tp(S1) = {α(−p2, p1)|α ∈R}. Let us impose the standard Euclidean inner prod-
uct on the tangent space as the Riemannian metric. Then, for any two vectors
α(−p2, p1) and β(−p2, p1) in Tp(S1), we get Φ(α(−p2, p1), β(−p2, p1)) = αβ.
(Note that this is the same inner product we would obtain by considering both
these vectors as tangent vectors to R2 and then evaluating the ordinary Eu-
clidean inner product, as in item (1) above.) This Riemannian metric makes S1
a Riemannian manifold.
5. Similarly, for the unit sphere Sn and a point p ∈Sn, the Euclidean inner product
on the tangent vectors make Sn a Riemannian manifold. That is, for any v1, v2 ∈
Tp(Sn), we used the Riemannian metric Φ(v1, v2) = vT
1 v2.
Deﬁnition 3.4 (Isometry).
A diﬀeomorphism f : M →N, where M, N are
Riemannian manifolds, is an isometry if ⟨u, v⟩p = ⟨dfp(u), dfp(v)⟩f(p), for all p ∈
M and all u, v ∈Tp(M). The left inner product is computed using the Riemannian
metric in M, while the right one is computed using the metric in N.
Example 3.3. Consider the mapping from Rn+1 to itself, given by x →Ox, for
a ﬁxed O ∈SO(n + 1). Restricting it to the unit sphere Sn, endowed with the
Euclidean metric, we obtain a mapping f : Sn →Sn that is an isometry.
Using the Riemannian structure, it becomes possible to deﬁne lengths of paths
on a manifold. Let α : [0, 1] →M be a parameterized path on a Riemannian
manifold M, such that α is diﬀerentiable everywhere on [0, 1]. Then dα
dt , the velocity
vector at t, is an element of the tangent space Tα(t)(M) (as an equivalence class
of curves, this tangent vector can simply be identiﬁed with [α]) and its length is
deﬁned to be

Φ( dα
dt , dα
dt ). The length of the path α is then given by:
L[α] =
 1
0

Φ
dα(t)
dt
, dα(t)
dt

dt .
(3.2)
This is the integral of the lengths of the velocity vectors along α and, hence, is
the length of the whole path α. For any two points p, q ∈M, one can deﬁne the
distance between them as the inﬁmum of the lengths of all smooth paths on M
that start at p and end at q:
d(p, q) =
inf
{α:[0,1]→M|α(0)=p,α(1)=q} L[α] .
(3.3)
Deﬁnition 3.5. If there exists a path ˆα that achieves the above minimum, then
it is called a geodesic between p and q on M.
We will give a more formal deﬁnition of geodesic in Chap. 6 and only observe here
that it is heavily dependent upon the Riemannian metric Φ on M.
It can be seen that the minimization problem given in Eq.3.3 is degenerate.
Let α∗be a path that minimizes L[α]. Since there are inﬁnitely many ways to
re-parameterize α∗, and each re-parameterization has the same length, there are
inﬁnitely many solutions to this problem. The problem of re-parameterization

44
3 Background: Relevant Tools from Geometry
variability is handled by changing the objective functional from length to an energy
functional given by:
E[α] =
 1
0
Φ
dα(t)
dt
, dα(t)
dt

dt .
(3.4)
The only diﬀerence here from the path length in Eq. 3.2 is that the square root has
been removed. It can be shown that a critical point of this functional restricted to
paths that start at p and end at q is a geodesic path on M. Furthermore, of all
the re-parameterizations of a geodesic path, the one with constant speed has the
minimum energy.
We should also note that, while distance-minimizing paths are always geodesics,
there may be other geodesics between p and q that do not minimize distance. As a
simple example, consider two points on a unit circle that are not antipodal. There
are two arcs, or geodesics, connecting them: the major arc and the minor arc.
Only the latter one minimizes the distance between them globally. However, it
is always true that geodesics are “locally distance minimizing”: i.e., if two points
on a geodesic are suﬃciently close to each other, then the geodesic will be a
distance-minimizing path between them. In fact, a path in a Riemannian manifold
is a geodesic if and only if it is locally distance minimizing. So, for now, we can
consider that property as the deﬁnition of the word “geodesic,” though it is not
the usual deﬁnition given by diﬀerential geometers. We should also note that,
depending on the Riemannian manifold and on the points p and q, there may not
exist any geodesics at all between p and q. Here is a rather trivial example of this
phenomenon: denote by M the manifold R2 with the origin (0, 0) removed, and
with the standard Euclidean metric. Then, for any point (x, y) ∈M, there is no
distance-minimizing path, and in fact no geodesic, joining (x, y) to (−x, −y).
Example 3.4. 1. Geodesics on Rn with the Euclidean metric are straight lines:
α(t) = tq + (1 −t)p is a geodesic between p and q. The length of this geodesic
is
 1
0 |q −p|dt = |q −p|. So the distance between points in Rn, as we already
know, is simply the (Euclidean) length of these straight lines.
2. In case of the upper half-plane with the hyperbolic metric, as given in Eq. 3.1,
the geodesic between any two points p, q in R2
+ has two forms:
• If the two points p, q have the same ﬁrst coordinate, i.e., p1 = q1, then the
geodesic between them is the vertical line connecting them.
• Otherwise, the geodesic between them is an arc of a half-circle whose center
lies on the x axis. The center is located at (x1, 0) and radius is r where:
x1 = |p|2 −|q|2
2(p1 −q1)
and r = |p −(x1, 0)| .
Shown in Fig. 3.2 are some examples of geodesics between points in R2
+ un-
der this metric. For comparison, we have also drawn the Euclidean geodesics
(straight lines) between those points.
3. Geodesics on S2 under the Euclidean metric are given by great circles. We can
prove this using polar coordinates (θ, φ), where θ is the latitude and φ is the
longitude angle. Without loss of generality, we will assume that the points lie on
the same longitudinal line. Let one of the points be represented by p1 ≡(θ1, φ1)
and the other by p2 ≡(θ2, φ1) on S2. A continuous path connecting them is

3.2 Riemannian Structure and Geodesics
45
0.5
1
1.5
2
1
1.2
1.4
1.6
1.8
2
2.2
0.8
1
1.2
1.4
1.6
1.8
2
1.7
1.8
1.9
2
2.1
2.2
2.3
2.4
2.5
2.6
1.4
1.6
1.8
2
2.2
2.4
2.6
1
1.1
1.2
1.3
1.4
1.5
1.6
1.7
1.8
1.9
2
3
4
5
6
7
8
9
1
2
3
4
5
6
Fig. 3.2 Geodesics between points in a plane under two diﬀerent Riemannian metrics: Euclidean
(broken lines) and hyperbolic (solid lines)
given by (θ(t), φ(t)), t ∈[0, 1] where θ(0) = θ1, φ(0) = φ1, θ(1) = θ2 and
φ(1) = φ1. The length of this path, using Eq. 3.2, is given by:
L =
 1
0

( ˙θ(t)2 + ˙φ(t)2 sin2(θ(t)))dt ≥
 1
0
| ˙θ(t)|dt = |θ2 −θ1|
The last quantity does not depend on the path and is thus a global minimum.
The equality is achieved when ˙φ(t) = 0 for all t ∈[0, 1]. Thus the meridian
line, characterized by φ(t) = constant is a geodesic path between p1 and p2.
For arbitrary two points on S2, we can rewrite this expression using Cartesian
coordinates, as described next.
4. Similar to the case above, geodesics on a general unit sphere Sn are great circles.
The distance-minimizing geodesic between any two points is the shorter of the
two arcs of a great circle joining them between them. If p and q are points on
the unit sphere (with p ̸= ±q), then the path:
α(t) =
1
sin(ϑ)(sin(ϑ(1 −t))p + sin(ϑt)q)
(3.5)
gives a constant-speed parameterization of the unique shortest geodesic (i.e.,
great circle arc) from p to q, where ϑ is determined by cos(ϑ) = ⟨p, q⟩and
0 < ϑ < π.
Let us prove this result. Clearly there is a unique vector w such that {p, w}
forms an orthonormal pair, and q = (cos(ϑ))p + (sin(ϑ))w. We then have the
standard parameterization of the great circle through p and w, given by α(t) =
(cos(t))p + (sin(t))w. To express this path in terms of p and q, we ﬁrst solve for
w in terms of p and q, obtaining:
w = −
cos(ϑ)
sin(ϑ)

p +

1
sin(ϑ)

q,
and then substitute this into our formula for α(t). A little algebra and trigonom-
etry then yields α(t) =
1
sin(ϑ)(sin(ϑ −t)p + sin(t)q). In order to arrange that
α(0) = p and α(1) = q, we replace t by ϑt, which yields the desired formula.
5. To deﬁne geodesics on O(n), we introduce the notion of matrix exponential. For
a matrix A ∈M(n), deﬁne its matrix exponential exp(A) by:
exp(A) = I + A
1! + A2
2! + A3
3! + . . .
(3.6)

46
3 Background: Relevant Tools from Geometry
The matrix exponential is a mapping from M(n) to GL(n) and is deﬁned for all
elements of M(n). The inverse of this mapping is called the matrix logarithm:
log : GL(n) →M(n). The logarithm (or log) does not exist for all invertible
matrices and, when it does exist, it is not necessarily unique. It is important to
note that the familiar property exp(A + B) = exp(A) exp(B) does not always
hold for matrices A and B, but it does hold if A and B satisfy AB = BA. This
is quite easy to see, since, if A and B commute, we can reorganize the power
series for exp(A+B) just as we do in the case of the scalar exponential. It is also
immediate from the power series deﬁnition that for any matrices A ∈GL(n)
and B ∈M(n), exp(ABA−1) = A exp(B)A−1 and exp(XT ) = (exp(X))T . We
also observe that given any skew-symmetric matrix X, exp(X) ∈O(n). To see
this, assume X is skew symmetric and compute:
exp(X) exp(X)T = exp(X) exp(XT ) = exp(X + XT) = exp(0) = I .
Note that, for the second equality, we used the fact that X commutes with XT ,
since XT = −X. Using the matrix exponential, one can deﬁne geodesics on
O(n) (with respect to the Riemannian metric deﬁned earlier) as follows: for any
A ∈O(n) and any skew-symmetric matrix X:
α(t) ≡A exp(tX) ,
is the unique geodesic in O(n) passing through A with velocity vector AX at
t = 0.
As we have mentioned earlier, the tangent space Tp(M) is a vector space even
though the manifold M may not be. One can view Tp(M) as a locally ﬂat approxi-
mation of M. It is local since this approximation is valid only in some neighborhood
of p. One use of this approximation is in imposing probability models on M by ﬁrst
imposing a density on Tp(M) and then transferring it to M. Deﬁning a density
ﬁrst on Tp(M) is an attractive option because it is a vector space and traditional
multivariate densities (parametric or nonparametric) can be used. To exploit such
a linear approximation, one needs a mapping to transfer points back and forth
between M and Tp(M). Next, we describe one such mapping in the form of the
exponential map (a generalization of the matrix exponential).
Theorem 3.1. Let M be a Riemannian manifold. Given a point p ∈M and
a tangent vector v ∈Tp(M), there exists a unique constant-speed parameterized
geodesic αv : (−ϵ, ϵ) →M, for some ϵ > 0, such that αv(0) = p and ˙αv(0) = v.
It is important to note that the domain (−ϵ, ϵ) of αv depends on the point p
and the tangent vector v. However, if M is complete, then this domain is always
(−∞, ∞). For a proof of this theorem, please refer to [36] pp. 30.
Deﬁnition 3.6 (Exponential Map). If M is a Riemannian manifold and p ∈M,
the exponential map expp : U ⊂Tp(M) →M is deﬁned by expp(v) = αv(1)
where αv is as deﬁned above and the domain U contains an open neighborhood of
0 in TP (M).
The exponential mapping exp maps a vector v ∈Tp(M) to a point of M. In words,
to reach the point expp(v), one starts at p and then moves for time 1 along the
unique constant speed geodesic whose velocity vector at p is v. Note that if v = 0,
the corresponding geodesic is just the constant path at p, so expp(0) = p.

3.2 Riemannian Structure and Geodesics
47
The inverse of an exponential map takes a point on the manifold M and maps
it to an element (or multiple elements) of the tangent space Tp(M). A vector v
is said to be the inverse exponential of q ∈M at p if expp(v) = q. It is denoted
by v = exp−1
p (q) and is often not a unique point. That is, the inverse may be
set-valued.
Example 3.5. 1. For Rn, under the Euclidean metric, since geodesics are given by
straight lines, the exponential map is a simple addition: expp(v) = p + v, for
p, v ∈Rn. The inverse exponential is simply exp−1
p (q) = q −p. Similarly for
M(n), the space of n × n matrices, the exponential map is given by a simple
addition of matrices, i.e., expA(X) = A + X, and the inverse exponential is
exp−1
A (B) = B −A.
2. The geodesics on a sphere Sn under the Euclidean metric can also be parame-
terized in terms of a direction v in Tp(Sn):
αt(v) = cos(t|v|)p + sin(t|v|) v
|v|.
(3.7)
As a result, the exponential map, expp : Tp(Sn) →Sn, has a simple expression:
expp(v) = cos(|v|)p + sin(|v|) v
|v| .
(3.8)
The exponential map is a bijection if we restrict |v| so that |v| ∈[0, π). For
a point q ∈Sn (q ̸= p), the inverse exponential map exp−1
p (q) is given by u,
where:
u =
θ
sin(θ)(q −cos(θ)p),
where θ = cos−1(⟨p, q⟩) .
3. The exponential map for O(n) at any point O ∈O(n) is given by expO :
TO(O(n)) →O(n), expO(OX) = O exp(X), where X is skew symmetric
and the last term is the matrix exponential (Fig. 3.3). Note that this map
is not one-to-one unless we restrict to a small enough neighborhood of O.
The inverse exponential map on O(n) is deﬁned as follows: For O1, O2 ∈O(n),
deﬁne exp−1
O1(O2) = O1 log(OT
1 O2).
Fig. 3.3 Exponential map
of a tangent vector Xp

48
3 Background: Relevant Tools from Geometry
The tangent spaces at diﬀerent points on a manifold are together referred to as
the tangent bundle of the manifold.
Deﬁnition 3.7. The tangent bundle of M, denoted by T M, is deﬁned as the
disjoint union of the tangent spaces at all points of M. It is speciﬁed as T M =
∪p∈MTp(M).
The tangent bundle itself is a smooth manifold whose dimension is twice that
of M. To see that T M is a smooth manifold, we deﬁne charts on it as follows.
Assume M has dimension n. Given any point p ∈M, let (U, φ) be a chart for
M, with p ∈U. Denote by T U ⊂T M the tangent bundle over U and deﬁne
θ : T U →R2n by θ(v) = (φ(p), dφp(v)), where v ∈Tp(M). θ is a bijection from
T U to φ(U) × Rn ⊂R2n. Charts deﬁned in this way clearly cover T M, and their
overlapping maps are smooth, so they provide T M with the structure of a smooth
manifold.
3.3 Geodesics in Spaces of Curves on Manifolds
Let M be a Riemannian manifold and denote by I the unit interval [0, 1]. Also,
for some L > 0, denote by M the space of measurable functions [0, L] →M. M
is also a manifold (see, e.g., [87]); its tangent space is given as follows. If α ∈M,
then:
Tα(M) = {w : [0, L] →T M| ∀τ ∈[0, L], w(τ) ∈Tα(τ)(M)
and
 L
0
⟨w(τ), w(τ)⟩dτ < ∞} .
In other words, this is just the set of ﬁrst-order deformations of α ∈M. We
now make M into a Riemannian manifold. If w1, w2 ∈Tα(M), deﬁne:
⟨w1, w2⟩=
 L
0
⟨w1(τ), w2(τ)⟩α(τ) dτ
where the inner product inside the integral uses the Riemannian metric on M.
Theorem 3.2. Suppose we are given a path in M represented as α : [0, L] × I →
M. For each τ ∈[0, L], deﬁne ατ : I →M by ατ(t) = α(τ, t). Then α is a geodesic
in M if ∀τ ∈[0, L], ατ is a geodesic in M.
Proof. Here we will use the fact that a path on a Riemannian manifold is a geodesic
if and only if the gradient of the energy given in Eq. 3.4, with respect to the path,
is zero. In other words, that path is a critical point of the energy function E.
Now suppose that ˜α : [0, L] × I × (−ϵ, ϵ) →M is an arbitrary variation of α in
the space of curves in M, i.e., we are assuming that for all τ ∈[0, L] and t ∈I,
˜α(τ, t, 0) = α(τ, t) and for all τ ∈[0, L] and h ∈(−ϵ, ϵ), ˜α(τ, 0, h) = α(τ, 0) and

3.3 Geodesics in Spaces of Curves on Manifolds
49
˜α(τ, 1, h) = α(τ, 1). For each value of h ∈(−ϵ, ϵ), we calculate the energy of the
path ˜α(., ., h) in M as follows:
E(˜α(., ., h)) = 1
2
 1
0
 L
0
∂˜α
∂t (τ, t, h) , ∂˜α
∂t (τ, t, h)

˜α(τ,t,h)
dτ dt
= 1
2
 L
0
 1
0
∂˜α
∂t (τ, t, h) , ∂˜α
∂t (τ, t, h)

˜α(τ,t,h)
dt dτ
Diﬀerentiating with respect to h at h = 0 gives:
d
dh|h=0E(˜α(., ., h)) = 1
2
 L
0
d
dh|h=0
 1
0
∂˜α
∂˜t (τ, t, h) , ∂˜α
∂˜t (τ, t, h)

˜α(τ,t,h)
dt

dτ
If we assume that ατ is a geodesic in M for every τ ∈[0, L], then it follows
immediately that the function we are integrating over [0, L] in the right-hand side
of the above expression is 0 for every τ, proving that α is a geodesic in M.
⊓⊔
Example 3.6. 1. Let M = Rn with the Euclidean metric and M be the set of
continuous maps from [0, L] to Rn. For any α ∈M, we will have the Riemannian
metric: for any w1, w2 ∈Tα(M),
⟨w1, w2⟩=
 L
0
⟨w1(τ), w2(τ)⟩dt .
The corresponding distance between any two paths α1 and α2 is simply
 L
0 |α1(τ) −α2(τ)|dτ.
2. This time let M = S2 with the Euclidean metric and M be the set of continuous
maps from [0, L] to S2. For any α ∈M, we will have the Riemannian metric:
for any w1, w2 ∈Tα(M): ⟨w1, w2⟩=
 L
0 ⟨w1(τ), w2(τ)⟩dτ. The corresponding
distance between any two paths α1 and α2 is simply
 L
0 θ(τ)dτ, where θ(τ) =
cos−1(

α(τ), α2(τ)

). This assumes that α1(τ) ̸= −α2(τ) for all τ. A pictorial
depiction of the geodesic path between two curves α1 and α2 on S2 is shown in
Fig. 3.4.
Fig. 3.4 A depiction of
geodesic path between two
curves, α1 and α2 on S2,
under the Euclidean metric,
with dotted lines denoting
great circles

50
3 Background: Relevant Tools from Geometry
3.4 Parallel Transport of Vectors
An important concept in diﬀerential geometry is the parallel transport of tangent
vectors along paths in manifolds. Here is the basic setup: Suppose M is a Rieman-
nian manifold, p ∈M, v ∈Tp(M), and let α : [0, 1] →M be a smooth path in M
such that α(0) = p. We wish to ﬁnd the most “natural” way to transport v along
α. What we mean by this is that, for each t ∈[0, 1], we wish to produce a tangent
vector wv(t) ∈Tα(t)(M) that results from transporting v along α with as little
“twisting” as possible. Consider the special case in which M = Rn; make M into
a Riemannian manifold by using the standard Euclidean inner product on each
tangent space Tx(Rn) ∼= Rn. Then the obvious solution is just to let wv(t) = v
for each t. However, for a general Riemannian manifold, each tangent space is a
diﬀerent vector space, so it is not obvious how to transport vectors from one point
to another.
For general Riemannian manifolds, the precise deﬁnition of “parallel transport”
requires the concept of covariant derivative, which we have not introduced yet. So
we will consider the following special case:
Deﬁnition 3.8. Suppose M is a submanifold of Rn, with Riemannian metric de-
ﬁned by restricting the Euclidean inner product on Rn to each tangent space of
M. Let p ∈M, v ∈Tp(M) and suppose α : [0, 1] →M is a smooth path with
α(0) = p. Then the parallel transport of v along α is deﬁned to be the unique
vector ﬁeld wv : [0, 1] →Rn satisfying:
1. For all t ∈[0, 1], wv(t) ∈Tα(t)(M).
2. wv(0) = v.
3. For all t ∈[0, 1], ˙wv(t) ⊥Tα(t)(M).
The “dot” in item 3 refers to the ordinary derivative with respect to t. It is a fact
that the parallel transport wv(t) always exists and is unique. It follows that, for
each t, parallel transport induces a mapping Ψ : Tα(0)(M) →Tα(t)(M) deﬁned by
Ψ(v) = wv(t). This mapping has the following important properties:
1. It is a linear isomorphism.
2. It is an isometry between these two tangent spaces, i.e., ⟨v1, v2⟩= ⟨Ψ(v1), Ψ(v2)⟩
for each v1, v2 in Tα(0)(M).
If p and q are nearby points in M, we sometimes refer simply to “parallel transport
from Tp(M) to Tq(M),” without specifying the path α. In this case, α is taken
to be the shortest geodesic from p to q. We illustrate this idea with some simple
examples.
Example 3.7. 1. Euclidean Space: Since the tangent spaces Tp(Rn) = Rn for all
p ∈Rn, and a geodesic path under the Euclidean metric is simply a straight line,
a transport of a vector is rather simple in this case. For any vector v ∈Tp(Rn),
we can set a constant vector ﬁeld w(t) = v and since ˙w(t) = 0, we automatically
have ˙w(t) ⊥Tα(t)(Rn). As a result, w(1) = v is simply a copy of the original
vector. This fact holds more generally for any path α : [0, 1] →Rn. If v ∈Rn is
a tangent vector at α(0), then the parallel transport of v along α is simply the
constant vector ﬁeld wv(t) = v.

3.4 Parallel Transport of Vectors
51
2. Unit Sphere: Consider the unit sphere Sn−1 ⊂Rn, with the standard Rieman-
nian metric inherited from Rn. In this case, there is a straightforward formula
for parallel transport of tangent vectors.
Theorem 3.3. Let x1 and x2 be two points in Sn−1, with x1 ̸= ±x2. Let Ψ de-
note the parallel transport map from Tx1(Sn−1) →Tx2(Sn−1), along the shortest
geodesic from x1 to x2. Then Ψ is given by the formula:
Ψ(v) = v −(2(v · x2)/|x1 + x2|2)(x1 + x2).
(3.9)
Proof. First, consider the case S1 ⊂R2. Let {e1, e2} be any orthonormal basis
of R2. There is only one geodesic in S1; its unit-speed parameterization is given
by α(t) = cos(t)e1 +sin(t)e2. An arbitrary element of the tangent space Te1(S1)
will be of the form v = re2, where r is any real number. It is easy to verify
that the parallel transport of v along the above geodesic is given by wv(t) =
−r sin(t)e1 + r cos(t)e2. This exercise has been included in the list at the end of
this chapter. Now consider the given elements x1 and x2 of S1. Without loss of
generality, we may assume that x1 = e1 and x2 = cos(θ)e1 + sin(θ)e2. Then the
proof for S1 is completed by observing that, if we plug these expressions for x1,
x2, and v (in terms of e1, e2, and θ) into the formula for Ψ(v), we obtain wv(θ).
To prove the Theorem for general Sn−1, one simply observes that the formula
given for Ψ is the identity on the component of v perpendicular to the plane
spanned by x1 and x2. Since the derivative of this component will be 0, the
three requirements are still satisﬁed.
This theorem covers only the case in which x1 ̸= ±x2. How do we parallel
transport if x1 = ±x2? If x1 = x2, the answer is easy: the shortest geodesic
is just the constant path, and parallel transport is clearly just given by the
identity. (In fact, the formula in the Theorem still works in this case, if you
substitute x2 = x1.)
If x1 = −x2, things are a little trickier, because there are an inﬁnite number
of equally short geodesics from x1 to x2. To visualize this situation, consider
the 2-sphere (surface of the earth) with x1 and x2 equal to the north and south
poles. Then all of the longitude lines are shortest geodesics from x1 to x2.
By parallel transporting along diﬀerent geodesics, one obtains diﬀerent parallel
transport maps. In fact, each unit vector in Tx1(Sn−1) is tangent to one of these
geodesics. If w ∈Tx1(Sn−1) is a unit vector, and we wish to parallel transport
from Tx1(Sn−1) to T−x1(Sn−1) along the geodesic tangent to w, the formula
is Ψ(v) = v −2(v · w)w. Three pictorial examples of parallel transport along
geodesic paths in S2 are presented in Fig. 3.5. In each case, the thick line shows
a geodesic path α on S2 and vectors show the parallel transport of the vector
at the point α(0).
Now we consider parallel translation of vectors along a path on S2. Let α :
[0, 1] →S2 be a path in S2 and let v ∈Tα(0)(S2) be a tangent vector. There is
no simple formula for the parallel transport of v along α in this case. However,
in the special case that α is a great circle (which is a geodesic on S2), one may
parallel transport tangent vectors along α using the equation developed above:
Rα(v) = v −(2(v · α(1))/(|α(0) + α(1)|2))(α(0) + α(1)).
(3.10)
For the general case in which α is not a great circle, one may proceed as follows.
Choose a large integer N. For i = 1 to N, deﬁne αi to be the small arc of a great

52
3 Background: Relevant Tools from Geometry
Fig. 3.5 Three examples of forward parallel transport of tangents along paths on S2
circle from α( i−1
N ) to α( i
N ). The original curve α is approximated by the union of
the small arcs αi. Hence, we approximate parallel transport of a tangent vector
from Tα(0)(S2) to Tα(1)(S2) by the composition Rα ≈RαN ◦RαN−1 ◦· · · ◦Rα1,
where each of these maps Rαi is simply a rotation, since each αi is a great
circle. As N →∞, this approximation approaches an exact formula.
⊓⊔
We pause here to discuss the concept of “ﬂatness,” which is closely related
to parallel transport of tangent vectors. As seen in the ﬁrst example above, if
we are given a tangent vector w ∈Tp1(Rn) and wish to parallel transport it to
Tp2(Rn), the result will not depend on which path α from p1 to p2 we use. This
is because Rn (with the standard Riemannian metric) is a ﬂat manifold. On
the other hand, S2 is not ﬂat with respect to the standard metric. (In fact, it
can be proven that S2 is not ﬂat with respect to any Riemannian metric.) The
fact that S2 is not ﬂat means that parallel transport of a tangent vector from
Tp1(S2) to Tp2(S2) will depend upon which path α from p1 to p2 we use. Here
is a deﬁnition of ﬂatness:
Deﬁnition 3.9 (Flat Riemannian Manifold). A Riemannian manifold M is
ﬂat if and only if it can be covered by a collection of open sets {Ui}, where each
Ui has the property that given any two points x, y ∈Ui; the parallel transport
map from Tx(M) to Ty(M) does not depend on which path from x to y we use
(as long as the path lies entirely in the set Ui).
3. Special Orthogonal Group: We state the following result without proof. For
any two elements O1 and O2 in SO(n), and a tangent vector W ∈TO1(SO(n)),
the tangent vector at O2 that is the parallel transport of W along the shortest
geodesic from O1 to O2 is:
W ′ = O1(OT
1 O2)1/2(OT
1 W)(OT
1 O2)1/2.
Note that SO(n) is not a ﬂat manifold.
3.5 Lie Group Actions on Manifolds
Central to our formulation of shape analysis is the use of transformations to model
the variations of objects, especially within the same shape classes. These transfor-
mations are mathematically formulated as actions of certain groups on objects, as
described next.

3.5 Lie Group Actions on Manifolds
53
So far we have discussed deﬁnitions and examples of manifolds and groups. In
some cases, the sets have both the structures—manifold and group; these sets are
called Lie groups. In view of their dual structures, they are sets of great importance
in algebra and pattern theory.
Deﬁnition 3.10 (Lie Groups). A group G is a Lie group, if (i) it is a smooth
manifold and (ii) the group operations G × G →G deﬁned by (g, h) →gh and
G →G deﬁned by g →g−1 are both smooth mappings.
Conveniently, most of the groups we have considered earlier are also Lie groups.
Example 3.8. 1. The general linear group GL(n), by the virtue of being an open
subset of Rn×n, is also a diﬀerentiable manifold. It is, therefore, a Lie group.
Any subgroup of GL(n), if it is also a submanifold of GL(n), will be a Lie group.
An important example in this category is the rotation group SO(n).
2. The translation group Rn is both a manifold and a group (with addition oper-
ation). Therefore, it is a Lie group.
3. The scaling group, R× with multiplication operation, is a Lie group.
4. The unit circle S1 is a group although it is not straightforward to see why from
its deﬁnition. Using the mapping:
S1 →SO(2),
given by (x1, x2) →

x1 −x2
x2 x1

,
we can identify S1 to SO(2). (In fact, this mapping provides a diﬀeomorphism
between the two manifolds.) The latter is a group with matrix multiplication
as the group operation. Through this identiﬁcation, S1 also inherits a group
structure. Since it is also a diﬀerentiable manifold, S1 becomes a Lie group.
However, the two-dimensional sphere S2 is not a Lie group as it does not have
a group structure.
3.5.1 Actions of Single Groups
Now we take a manifold M and study how the points change on M when operated
on by some kind of a transformation group. Our motivation, of course, is shape
analysis where M will be a manifold formed by curves and we want to study
variations of their shapes under diﬀerent transformations - rotations, translations,
and scalings. Mathematically, this is managed using group actions on manifolds.
Deﬁnition 3.11 (Group Action). Given a manifold M and a Lie group G, a
left group action of G on M is a map G × M →M, written as (g, p) →g ∗p,
such that:
1. g1 ∗(g2 ∗p) = (g1 · g2) ∗p, ∀g1, g2 ∈G and p ∈M.
2. e ∗p = p, ∀p ∈M.
In item 1, the symbol · denotes the group operation in G.
Another way to phrase this relation is to say that G acts on M. We say that G
acts smoothly on M if the map G × M →M is a smooth map. In the deﬁnition
above, the group element g is applied from the left, so it is also called the left

54
3 Background: Relevant Tools from Geometry
group action. Instead, if it is applied from the right, i.e., (p, g) →p ∗g, then it is
called the right group action.
In case M has a Riemannian structure and, therefore, we can deﬁne distances
between points on M, we can study the eﬀect of the group action on these dis-
tances.
Deﬁnition 3.12. A group action of G on a Riemannian manifold M is called
isometric if it preserves the Riemannian metric on M. In other words, for all
g ∈G, the map M →M given by p →g ∗p is an isometry. For the same situation,
we sometimes say that G acts on M by isometries.
It then also follows that for all g ∈G and x, y ∈M, d(x, y) = d(g ∗x, g ∗y),
where d(x, y) is the distance function on M resulting from the Riemannian metric.
Compare this with Deﬁnition 3.4 that speciﬁes an isometry between Riemannian
manifolds. Clearly, this is a speciﬁc case of that deﬁnition; here the mapping is
between the same manifold and it is deﬁned using a group action.
There is some additional notation associated with group actions that will be
useful later in shape analysis.
Deﬁnition 3.13 (Orbit). Assume that a group G acts on a manifold M. For any
p ∈M, the orbit of p under the action of G is deﬁned as the set G · p = {g · p :
g ∈G}. We will also denote it by [p].
If the orbit of any p ∈M is the whole of M, then the group action is said to be
transitive.
The orbit of a point in M refers to all possible points one can reach in M using
the action of G on that point. The orbit of a point can vary in size from a single
point to the entire manifold M (which is the case if the action is transitive). Let
us consider some examples to understand this point.
Example 3.9. In this example, we study the actions of some familiar transformation
groups on Rn.
1. Translation Group: The translation group Rn acts on the vector space Rn
by the action x ∗y = x + y for any x, y ∈Rn. This group action is transitive
since the orbit of any vector v is the whole of Rn, i.e., [y] = Rn. Also, this
group action is isometric using the Euclidean structure on Rn, since |y1 −y2| =
|(x + y1) −(x + y2)| for all x, v1, v2 ∈Rn.
2. Scaling Group: The scale group R× acts on Rn by a ∗x = ax for any a ∈R×
and x ∈Rn. This group action is not transitive on Rn. Under the action, the
orbit of the zero vector consists of the zero vector alone. The orbit of a nonzero
vector is a straight line spanned by positive scalings of this vector. If we impose
the Euclidean metric on Rn to make it a Riemannian manifold, then the action
of R× on Rn is not isometric because |x1 −x2| ̸= |ax1 −ax2| for any a ̸= 1.
3. Rotation Group: The rotation group SO(n) acts on Rn by the action O ∗x =
Ox, the matrix-vector multiplication, for all O ∈SO(n) and x ∈Rn. The
orbit of any vector x is simply a sphere centered at zero and radius |x|, i.e.,
[x] = {|x|u|u ∈Sn−1}. Therefore, this group action is not transitive but is
isometric, under the Euclidean structure on Rn, because |x1−x2| = |Ox1−Ox2|,
for all O ∈SO(n), x1, x2 ∈Rn.
Our introduction of the group action is for an important reason. We shall use
membership of orbits to deﬁne equivalence relations.

3.5 Lie Group Actions on Manifolds
55
Lemma 3.1. If we deﬁne a relation ∼by saying that p ∼q if and only if p is an
element of the orbit [q], then ∼is an equivalence relation.
The proof is left as an exercise to the reader. These equivalence relations, in turn,
deﬁne quotient spaces that are natural domains for statistical shape analysis. We
cover this topic in more detail in Sect. 3.6.
3.5.2 Actions of Product Groups
In case there are multiple groups acting on a manifold, what is the nature of their
joint action? There are at least two ways of categorizing these joint actions.
3.5.2.1 Direct Product Action
If G and H are groups, the easiest way to endow the product G × H with a group
structure is simply to deﬁne (g1, h1)·(g2, h2) = (g1·g2, h1·h2). With this deﬁnition,
G×H is called the direct product of G and H. Suppose G and H both act on the
manifold M. We say these actions commute if g ∗(h∗p) = h∗(g ∗p) for all g ∈G,
h ∈H, and p ∈M. In the case that these actions commute, they combine to give
us an action of the product group G × H on M deﬁned by (g, h) ∗p = g ∗(h ∗p).
Example 3.10. Scale and Rotate: Consider the actions of the rotation group
SO(n) and the scaling group R× on Rn. We can form the product group SO(n) ×
R× with the group operation: (O1, a1) · (O2, a2) = (O1 · O2, a1, ·a2). We check if
the actions of these two groups on Rn commute. For x ∈Rn, O ∈SO(n), and
a ∈R×, we have:
a(Ox) = O(ax) .
These actions do commute, and therefore, we can deﬁne the combined action of
SO(n) × R× on Rn as:
(SO(n) × R×) × Rn →Rn by (O, a) ∗x = aOx .
3.5.2.2 Semi-direct Product Action
If the actions of G, H on M do not commute, then we do not obtain an action of
the product group G×H on M. In some cases, however, we can deﬁne an action of
a semi-direct product as follows. If G and H are groups, we say that H acts on G
by isomorphisms if we have a group action of H on G (denoted by h∗g) and if, for
every h ∈H, the function G →G given by g →h∗g is a group isomorphism from G
to itself. Given an action of H on G by isomorphisms, we can deﬁne a corresponding
group operation on G×H as follows: (g1, h1)·(g2, h2) = (g1·(h1∗g2), h1·h2). When
endowed with this group operation, G×H is called the semi-direct product of G
and H and is denoted by G⋊H. To reiterate an important point, let us emphasize
that we cannot form the semi-direct product G ⋊H without ﬁrst being provided
with an action of H on G by isomorphisms.

56
3 Background: Relevant Tools from Geometry
Example 3.11. Scale, Rotate, and Translate: As an example, recall that the
three groups R×, SO(n), and Rn all act on Rn (the ﬁrst by scalar multiplication,
the second by matrix multiplication, and the third by vector addition). We can see
that the actions of SO(n) and R× do not commute with the action of Rn. That
is, Ox + v ̸= O(x + v) and a(x + v) ̸= (ax + v), in general, for a ∈R×, v ∈Rn,
O ∈SO(n), and x ∈Rn. Thus, we have an action of R× × SO(n) on Rn, and an
action of Rn on Rn, but not an action of Rn × (R× × SO(n)) on Rn. However,
R××SO(n) acts on the translation group Rn in the same way it acts on the vector
space Rn. Hence, we can form the semi-direct product Rn ⋊(R× × SO(n)) with
the group operation being:
(v1, a1, O1) ∗(v2, a2, O2) = (v1 + a1O1v2, a1a2, O1O2) .
The reader can then check that we then have a well-deﬁned action of Rn ⋊(R× ×
SO(n)) on Rn deﬁned by (v, a, O) ∗x = aOx + v. The action of an element of the
semi-direct product Rn ⋊SO(n), also called SE(n), on a solid object is called a
rigid motion because this action is by isometries.
Extending this idea to a certain shape representation, consider the shapes rep-
resented by an ordered k-tuple in Rn. Each such conﬁguration is represented by
X ∈Rn×k where each column denotes a point in Rn. However, we require that
the space spanned by the columns of X be all of Rn.
Deﬁnition 3.14 (Landmark Space). The landmark space, denoted by Ln,k
(with k ≥n), is deﬁned as the subset of all X ∈Rn×k such that dim(span(X)) = n.
The tangent space at X ∈Ln,k is given by TX(Ln,k) = Rn×k. We will generally
assume the standard Euclidean metric on Ln,k, with the resulting distance between
any two points X1, X2 ∈Ln,k given by |X1 −X2| and where | · | denotes the
Frobenius norm

|X| =

i,j X2
i,j

.
The action of Rn ⋊(R× × SO(n)) on the landmark space Ln,k is given by:
(v, a, O) ∗X = aOX + v1T
k ,
and the orbit of X is:
[X] = {aOX + v1T
k |a ∈R×, O ∈SO(n), v ∈Rn} .
Here, 1k is a column vector of length k containing all ones so that v1T
k is an n × k
matrix with identical columns.
3.6 Quotient Spaces of Riemannian Manifolds
Just as cosets are used to deﬁne equivalence relations, as explained after Deﬁni-
tion A.11, the orbits of G can also be used to deﬁne an equivalence relation in M.
This leads us to the notion of quotient space.
Deﬁnition 3.15. Let M be a ﬁnite-dimensional manifold and G be a Lie group
that acts of M. Then, the quotient space M/G is deﬁned to be the set of all orbits
of G in M:
M/G = {[p]|p ∈M} .
(3.11)

3.6 Quotient Spaces of Riemannian Manifolds
57
We will use group actions and their orbits to incorporate the role of shape-
preserving transformations in shape analysis. Let us motivate this idea using a
simple example.
Example 3.12. 1. Let X ∈Ln,k ⊂Rn×k, a matrix whose k columns denote
k-ordered points in Rn. The action of SO(n) on Ln,k is deﬁned in Example 3.9,
except that the same rotation is applied to all the columns, and this deﬁnes the
orbit of X:
[X] = {OX|O ∈SO(n)} .
This set consists of all n×k matrices whose columns can be obtained by simulta-
neously rotating the columns of X using the same rotation. Treating elements of
[X] as an equivalence class implies that all rotated versions of X are equivalent.
The resulting quotient space is given by Ln,k/SO(n) = {[X]|X ∈Ln,k}.
2. One can go beyond rotations and include a larger set of transformations in
deﬁning an equivalence relation. Consider the action of Rn ⋊(R× × SO(n)) on
Ln,k given by (v, a, O) ∗X = aOX + v1T
k , and the equivalence relation X ∼Y
if Y ∈[X]. The group Rn ⋊(R× × SO(n)) represents translation, scaling, and
rotation (all shape-preserving transformations) of elements of Ln,k. Thus, each
such equivalence class represents a unique shape and individual shapes can be
identiﬁed as elements of the quotient space Ln,k/(Rn ⋊(R× × SO(n))). This
quotient space is termed landmark shape space, the shape space of conﬁgurations
of k-ordered points in Rn and is a manifold.
One can immediately notice the importance of quotient spaces in shape anal-
ysis, where one wants the analysis and metrics to be invariant to certain (shape-
preserving) transformations. An obvious way to accomplish this invariance is to
impose equivalence relations between objects whose shapes are deemed equivalent.
If the chosen metric respects this equivalence relation, then the ensuing analysis
will have the desired invariance. This is exactly what is frequently done in shape
analysis. Since the so-called shape-preserving transformations can be realized as
actions of certain Lie groups, shape spaces are treated as quotient spaces of certain
manifolds, called pre-shape spaces, under the actions of these groups. This discus-
sion underscores the need to better understand and to develop tools for analyzing
elements of quotient spaces.
The ﬁrst item of interest is: Given the Riemannian structure on M, what Rie-
mannian structure can be imposed on the quotient space M/G? Suppose a group
G acts by isometries on a Riemannian manifold M. This implies that the group
action preserves distances between points in M. Depending upon the action, this
quotient space M/G may or may not be a smooth manifold. If it is a smooth
manifold, it inherits a Riemannian metric from the Riemannian metric on M as
follows. Let Tp(M) and Tp([p]) denote the tangent spaces at p to the manifold
M and the orbit [p], respectively. Clearly, Tp([p]) ⊂Tp(M). Let Np(M) be the
orthogonal complement of Tp([p]) in Tp(M), i.e., it is the set of vectors that are
perpendicular to Tp([p]) in Tp(M). Elements of Tp([p]) are tangent to the orbit [p]
at p and the elements of Np(M) are perpendicular to the orbit at p. Figure 3.6
shows an illustration of this setup. The direct sum:
Tp([p]) ⊕Np(M) = Tp(M) ,
is the full tangent space at p. We can identify the perpendicular space Np(M)
with the tangent space on the quotient set T[p](M/G). That is, for every element

58
3 Background: Relevant Tools from Geometry
Fig. 3.6 Illustration of
vector spaces tangent and
normal to an orbit [p] at p ∈
M, and the identiﬁcation
of tangent space T[p](M/G)
with the normal space
Np(M)
v ∈T[p](M/G), there is a corresponding element ˜v ∈Np(M), and vice versa. We
will use this identiﬁcation to specify tangent vectors in the quotient space. Once we
have made this identiﬁcation, we simply deﬁne the inner product of two vectors
in this orthogonal complement to be their inner product using the Riemannian
metric on M.
Deﬁnition 3.16 (Inherited Riemannian Metric). Let M be a Riemannian
manifold and let G act on M by isometries. The inherited Riemannian metric on
the quotient space M/G is given by, for any v1, v2 ∈T[p](M/G), deﬁne:
⟨⟨v1, v2⟩⟩= ⟨˜v1, ˜v2⟩p ,
where ˜v1, ˜v2 are considered as elements of Np(M) ⊂Tp(M) on the right side.
Due to the isometry of the group action, it does not matter which p ∈[p] is selected
for this deﬁnition. We can then deﬁne arc-length, geodesic, and a distance function
on the manifold M/G using this inherited Riemannian metric.
If M/G is not a smooth manifold, then it does not make sense to talk about
a Riemannian metric, but the space M/G still inherits a distance function from
the distance function on M. We can deﬁne the distance function on the quotient
space as follows.
Deﬁnition 3.17. If a Lie group G acts on a Riemannian manifold M by isometries
and the orbits under G are closed, then deﬁne a distance function dq on the quotient
space M/G, using the original distance function dm on the manifold M, as follows:
dq([p1], [p2]) = min
g∈G dm((g, p1), p2) = min
g∈G dm(p1, (g, p2)) .
(3.12)
Theorem 3.4. The distance dq deﬁned in Eq. 3.12 forms a proper distance.
Proof. There are three properties that dq should satisfy—symmetry, positive def-
initeness, and triangle inequality. We will consider them in that order:
(1) Since the action of G is by isometries, it is easy to show that dq is symmetric.
(2) For positive deﬁniteness, we need to show that dq([p1], [p2]) = 0 ⇒[p1] = [p2].
Suppose that dq([p1], [p2]) = 0. By deﬁnition, it follows immediately that, for
all ϵ > 0, there exists a g ∈G such that dm(p1, (g, p2)) < ϵ. From this, it

3.6 Quotient Spaces of Riemannian Manifolds
59
follows that p1 is in the closure of the orbit p2. Since the orbits are assumed
to be closed, it follows that p1 ∈[p2], so [p1] = [p2].
(3) To establish the triangle inequality, we need to show that dq([p1], [p3]) ≤
dq([p1], [p2]) + dq([p2], [p3]), for any p1, p2, p3 ∈M. Seeking contradiction, sup-
pose that dq([p1], [p3]) > dq([p1], [p2]) + dq([p2], [p3]). Let ϵ = 1
3(dq([p1], [p3]) −
dq([p1], [p2]) −dq([p2], [p3])]); by our supposition, ϵ > 0. From the deﬁnition of
ϵ, it follows that dq([p1], [p3]) = dq([p1], [p2]) + dq([p2], [p3]) + 3ϵ.
By the deﬁnition of dq, we can choose g1, g2 ∈G, such that dm((g1, p1), p2) ≤
dq([p1], [p2]) + ϵ and dm(p2, (g2, p3)) ≤dq([p2], [p3]) + ϵ. Now by the triangle
inequality for dm, we know that:
dm((g1, p1), (g2, p3)) ≤dm((g1, p1), p2) + dm(p2, (g2, p3))
≤dq([p1], [p2]) + dq([p2], [p3]) + 2ϵ .
By deﬁnition of dq, it follows that dm([p1], [p3]) ≤dq([p1, [p2]) + dq([p2], [p3]) + 2ϵ.
But this contradicts that fact that dq([p1], [p3]) = dq([p1], [p2]) + dq([p2], [p3]) + 3ϵ.
Hence our supposition that dq(p1], [p3]) > dq([p1], [p2])+dq([p2], [p3]) must be false.
The triangle inequality follows.
⊓⊔
(In case M/G is a smooth manifold, then this distance is the same that results
from the inherited Riemannian metric given in Deﬁnition 3.16.) Assume that dm
is a geodesic distance on M, i.e., for any p1, p2 ∈M, there exists a geodesic path
between p1 and p2 such that the length of that geodesic is dm(p1, p2). In that case
the following question becomes interesting: What is the corresponding geodesic
path in M/G, connecting [p1] and [p2], whose length achieves the quotient distance
dq([p1], [p2])? The following path provides the answer. Let α : [0, 1] →M be the
geodesic between the points p1 and p∗
2 = (g∗, p2) in the original manifold M, where
we are assuming that g∗∈G can chosen in order to minimize the distance between
p1 and (g∗, p2). Then, [α(τ)], indexed by τ ∈[0, 1], forms the desired geodesic path
between [p1] and [p2] in M/G. Here, [α(τ)] denotes the orbit of the point α(τ) in
M under the action of G. This geodesic can be shown to be perpendicular to each
orbit it meets. In other words, the velocity vector dα(τ)
dτ
is in the normal space
Nα(τ)(M). Speciﬁcally, the shooting vector for the geodesic v = dα(τ)
dτ |τ=0 is in
Np1(M), or equivalently in T[p1](M/G).
Example 3.13. Rotation Group: Once again, let X ∈Ln,k be an ordered k-tuple
of points in Rn with the Euclidean metric and consider the action of SO(n) on Ln,k
given by (O, X) = OX. Since SO(n) acts on Ln,k by isometries, we can inherit a
distance on the quotient space Ln,k/SO(n) as follows: for any two X1, X2 ∈Ln,k
dq([X1], [X2]) = argminO∈SO(n)

⟨X1 −OX2, X1 −OX2⟩=
max
O∈SO(n)

trace(X1XT
2 OT )
=
max
O∈SO(n)

trace(UΣV T OT ),
UΣV T = SVD(A),
A ≡X1XT
2
=
max
O∈SO(n)

trace(ΣW )
(3.13)
where W = V T OT U is an element of O(n). In computing the SVD, we assume
that the singular values have been arranged in a descending order from top-left to
bottom-right in Σ. Since W ∈O(n), we know that Wij ≤1 for all i, j. Now there
are two cases:

60
3 Background: Relevant Tools from Geometry
1. If det(A) > 0, then trace(ΣW) ≤trace(Σ) and the equality is achieved when
W = In, which implies that O∗= UV T ∈SO(n).
2. If det(A) < 0, then UV T has the determinant −1. This can be ﬁxed by inserting
a matrix in the product as follows:
O∗= U
⎡
⎢⎢⎢⎣
1 0 . . . 0
0 1 . . . 0
...
0 0 . . . −1
⎤
⎥⎥⎥⎦V T .
The resulting distance on the quotient space is given by dq([X1], [X2]) = |X1 −
O∗X2|, where | · | denotes the Frobenius norm of a matrix. The geodesic between
[X1] and [X2] in Ln,k/SO(n) is computed as follows. Let α(τ) = (1−τ)X1+τO∗X2.
Then, the desired geodesic in the quotient space is given by [α(τ)].
In the later chapters on shapes of curves, we will repeatedly use isometric actions
of groups on Riemannian manifolds to deﬁne distances between orbits. This is
a very important concept in our analysis of shapes and the key idea here is the
isometry of the group action. If the action of G is not by isometries, then this
construction of distances on M/G does not work.
3.7 Quotient Spaces as Orthogonal Sections
We now discuss a method for realizing the quotient space M/G in a simple way,
as a submanifold of M. This method is not always available, but when it is, it
can be very useful! The basic idea is to identify a subset S of M, termed a global
orthogonal section, with the set M/G and, if possible, using an isometric map.
In this way, one can perform all relevant operations on S instead of M/G and
solve inference problems on M/G indirectly. In the following, we develop this idea
starting with a general global section and then particularizing it to the situation
of interest.
Deﬁnition 3.18 (Section).
Deﬁne a section of the action of G on M to be a
submanifold S of M that intersects each orbit of the action in at most one point.
The left panel of Fig. 3.7 shows an illustration of this idea. If S intersects every
orbit, we say S is a global section. In the case of a global section, one can use the set
S as a representative of the quotient space M/G to perform certain computations.
For instance, if we want to compare any two orbits [p] and [q], we can represent
these orbits by their intersections p1 and q1 with S and use a certain distance
between p1 and q1 in S as the desired distance. Note that the choice of metric on
S is arbitrary at this stage and need not necessarily correspond to the Riemannian
structure of M. However, for comparing elements of M/G, it seems natural to
use the metric inherited from M. In that case, to use a section, one needs some
additional conditions.
Deﬁnition 3.19 (Orthogonal Section).
Deﬁne an orthogonal section of the
action of G on M to be a submanifold S of M with the following three properties:

3.7 Quotient Spaces as Orthogonal Sections
61
[p]
[q]
M
S
p1
q1
[p]
[q]
M
Tp([p])
Tp(S)
p
q
S
Fig. 3.7 Left: A section of M under the group G. Right: An orthogonal section of M under the
action of group G
1. S intersects each orbit of the action in at most one point.
2. For each p ∈S, Tp([p]) ⊥Tp(S) (perpendicularity is deﬁned using the Rieman-
nian metric on M).
3. For each p ∈S, Tp([p]) ⊕Tp(S) = Tp(M).
If S intersects every orbit, we say S is a global orthogonal section. Otherwise, we
say S is a local orthogonal section.
The right panel of Fig. 3.7 shows a pictorial illustration of this setup.
Example 3.14. We continue with the example of group actions on the Euclidean
manifold Ln,k. We will use Xj to denote the jth column of X.
1. Translation Group: Let the group G = Rn act on the manifold Ln,k according
to the action: for x ∈Rn and X ∈Ln,k, (x, X) = X + x1T
k , where 1 is column
vector of k ones. The orbits of Ln,k under Rn are given by [X] = {X + x1T
k |x ∈
Rn}. Deﬁne a subset of Ln,k:
St = {X ∈Ln,k|
k

j=1
Xj = 0} ,
(3.14)
We will now show that St is a global, orthogonal section of Ln,k. For an arbitrary
X ∈Ln,k, let the column sum be a nonzero vector x ∈Rn, i.e., k
i=1 Xi = x.
Then, X −(x/n)1T
k is the only element of the orbit [X] that is in S. For any
X ∈St, the space tangent to the orbit is:
TX([X]) = {
⎡
⎢⎢⎣
v1 v1 . . . v1
v2 v2 . . . v2
. . .
vn vn . . . vn
⎤
⎥⎥⎦|(v1, v2, . . . , vn) ∈Rn},

62
3 Background: Relevant Tools from Geometry
and the space tangent to St is:
TX(St) = {
⎡
⎢⎢⎣
w1,1 w1,2 . . . w1,n
w2,1 w2,2 . . . w2,n
. . .
wn,1 wn,2 . . . wn,n
⎤
⎥⎥⎦|
k

j=1
wi,j = 0,
for all i},
It is easy to see that TX([X]) and TX(St) are perpendicular to each other
under the Euclidean metric. Furthermore, we can easily establish that TX([X])⊕
TX(St) = TX(Ln,k) = Rn×k. Thus, the set S deﬁned in Eq. 3.14 is an orthogonal
section of Rn×k.
2. Scaling Group: Let the group G = R× act on manifold Ln,k according to the
action: for a ∈R× and X ∈Ln,k, (a, X) = aX. The orbits of Ln,k under R×
are given by radial lines [X] = {aX|a ∈R×}. Deﬁne a subset of Ln,k:
Ss = {X ∈Ln,k|
k

j=1
n

i=1
X2
i,j = 1} .
(3.15)
We now show that this Ss is an orthogonal section of Ln,k. If for an X, we have
that (k
j=1
n
i=1 X2
i,j) = b ̸= 1, then
1
√
bX is the only element of the orbit [X]
in Ss. The relevant tangent spaces are Tx([X]) = {aX|a ∈R×} and TX(Ss) =
{Y ∈Rn×k| ⟨Y, X⟩= 0}, and it is easy to verify that Tx([X]) ⊥TX(Ss) under
the Euclidean metric. Additionally, we have TX([X]) + TX(Ss) = TX(Ln,k) =
Rn×k. Thus, Ss is an orthogonal section of Ln,k.
3. Rotation Group: Let the action of SO(n) on Ln,k be given by (O, X) = OX.
Does this action admit an orthogonal section of Ln,k, even locally? The answer
is no! We illustrate it with an example for n = 2 and k = 2. We further restrict
to the set:
S3 = {X ∈R2×2|
2

i=1
2

j=1
X2
i,j = 1} .
We restrict to S3 since this set is closed under the action of SO(2). It is well
known that this action of S1 on S3 does not admit any orthogonal section, even
a local one! We will not give a proof of this fact, but it is a straightforward con-
sequence of the famous Frobenius theorem, a fundamental result in the theory
of diﬀerential manifolds (the Frobenius theorem is presented in Boothby [19],
p. 155). Since the smaller set S3 does not admit an orthogonal section, the same
holds for the larger set R2×2.
In case we know the orthogonal sections of a manifold M under the individual
actions of several groups, then sometimes we can determine the orthogonal section
under the joint actions by taking simple intersections.
Example 3.15. Joint Scaling and the Translation Group: Consider the action
of the semi-direct product group Rn ⋊R× on the Euclidean space Ln,k, given by:
((y, a), X) = aX + y1T
k .
The orbit of the joint action associated with X ∈Ln,k is given by:
[X] = {aX + y1T
k |a ∈R×, y ∈Rn} .

3.7 Quotient Spaces as Orthogonal Sections
63
The orthogonal section for scaling is Ss = {X ∈Ln,k| k
j=1
n
i=1 X2
i,j = 1} and
orthogonal section for translation is St = {X ∈Ln,k| k
j=1 Xj = 0}. Then, it can
be shown (see the Exercise list at the end of the chapter) that their intersection:
Sst = {X ∈Ln,k|
k

j=1
n

i=1
X2
i,j = 1,
k

j=1
Xj = 0} .
(3.16)
forms an orthogonal section of the joint scaling-translation group Rn ⋊R×.
Suppose M is a Riemannian manifold, G is a Lie group acting on M by isome-
tries, and S is an orthogonal section for this action. Since S is a submanifold of
M, we can deﬁne a Riemannian metric on S by restricting the Riemannian metric
on M to S. There is an obvious map S →M/G deﬁned to be the composition
S →M →M/G where the ﬁrst map is the inclusion of S into M and the second
is the quotient map from M to M/G (taking p →[p]). It can be shown that,
with respect to the Riemannian metrics on S and M/G, this map is an isometry
of S onto an open submanifold of M/G. If S is a global section, then this map
is simply an isometry from S to M/G. In this case, if we wish to make distance
computations or ﬁnd geodesics in M/G, we may make them in S instead! So, for
the purposes of statistically analyzing points on M/G, one can directly work on
the corresponding points on S, and this greatly simpliﬁes the analysis. Let us take
a few examples.
Example 3.16. Once again, we will consider the actions of diﬀerent groups on the
manifold Ln,k, although sometimes under a metric that is not the standard Eu-
clidean metric.
1. Translation: We assume the standard Euclidean metric on Ln,k, i.e., for any
u, v ∈Rn×k, the Riemannian metric is the standard Euclidean product ⟨u, v⟩.
We have already veriﬁed in Example 3.9 that the action of Rn on Euclidean
spaces is by isometries. Therefore, the section St given in Eq. 3.14 is isometric
to the quotient space Ln,k/Rn.
Now that we have identiﬁed the Riemannian structures of St and the quo-
tient space Ln,k/Rn, we can compute geodesics between elements of Ln,k/Rn
as follows. Let X1, X2 ∈Ln,k and we want to compute a geodesic be-
tween their equivalence classes [X1], [X2] ∈Ln,k/Rn. Let k
i=1 X1,i = x1 and
k
i=1 X2,i = x2 be the two column sums. Then, deﬁne ˜
X1 = X1 −x11T
k and
˜
X2 = X2 −x21T
k as the representatives of the two equivalence classes in St.
The geodesic between ˜
X1 and ˜
X2 in St is given by a straight line:
α(τ) = (1 −τ) ˜
X1 + τ ˜
X2 ,
and the length of this geodesic is | ˜
X1 −˜
X2| = (
i,j( ˜
X1,i,j −˜
X2,i,j)2)1/2.
Therefore, the geodesic path between the two equivalence classes [X1] and
[X2] in the quotient space Ln,k/Rn is [α(τ)] and the distance between them
is d([X1], [X2]) = | ˜
X1 −˜
X2|. This α(τ) is a constant-speed geodesic with the
speed given by | ˜
X1 −˜
X2|. Since St is a vector space with standard metric,
the exponential map and its inverse can be expressed using a simple formula
(Example 3.5).

64
3 Background: Relevant Tools from Geometry
2. Scaling Group: This time let R× act on Ln,k by scalar multiplication. We
already veriﬁed in Example 3.9 that this action is not by isometries with re-
spect to the usual inner product. As a result, the quotient space M/G does not
naturally inherit a Riemannian metric.
We remedy this by deﬁning a new Riemannian metric on Ln,k.
Deﬁnition 3.20 (Scaled-Euclidean metric). If U, V ∈TX(Ln,k), deﬁne the
scaled-Euclidean metric as:
⟨⟨U, V ⟩⟩X = ⟨U, V ⟩
⟨X, X⟩
(3.17)
where ⟨·, ·⟩denotes the standard inner product in Rn×k.
We now establish the desired isometry condition.
Lemma 3.2. With respect to the scaled-Euclidean metric on Ln,k, the action
of R× is by isometries.
Proof. Fix a ∈R× and deﬁne La : Ln,k →Ln,k by La(X) = aX. Since this
action is a linear transformation on the underlying vector space Rn×k, it follows
that its derivative is given by the same formula, i.e., given U ∈TX(Ln,k),
d(La)X(U) = aU. We then compute:
⟨⟨d(La)X(U), d(La)X(V )⟩⟩La(X) = ⟨aU, aV ⟩
⟨aX, aX⟩= ⟨U, V ⟩
⟨X, X⟩= ⟨⟨U, V ⟩⟩X ,
which shows that La is an isometry for all a ∈R×.
⊓⊔
The next question is: Can we ﬁnd a submanifold of Ln,k that is a section of this
action? The answer is yes: in fact, the set Ss deﬁned in Eq. 3.15 is also a section
of this action. How do we see this? Properties (1) and (3) have already been
proven in the previous example (they don’t depend on which Riemannian metric
we are using). Property (2) involves orthogonality and has also been proven
earlier under the Euclidean metric. It follows that it holds under the scaled-
Euclidean metric also, because for an X ∈Ss, the scaled-Euclidean metric
reduces to the standard Euclidean metric. Thus, the set Ss is an orthogonal
section of the action of SO(n) on Ln,k. Note that Ss is a unit sphere in Rn×k,
and under the Euclidean metric, it is rather straightforward to analyze points
in Ss.
Now consider the quotient space Ln,k/R× and for any two X1 and X2 in Ln,k;
we want to ﬁnd a geodesic between their equivalence classes in the quotient space
using the scaled-Euclidean metric. We ﬁnd the intersection points of these orbits
with Ss, which are ˜
X1 = X1/|X1| and ˜
X2 = X2/|X2|. We then parameterize
the arc of the great circle in Ss joining ˜
X1 to ˜
X2:
α(τ) =
1
sin(θ)(sin((1 −τ)θ) ˜
X1 + sin(τθ) ˜
X2),
θ = cos−1(
$
˜
X1, ˜
X2
%
) .
Then, the path of orbits [α(t)] gives the geodesic in Ln,k/R× from [X1] to [X2].
Likewise, the distance between the two orbits is given by:
d([X1], [X2]) = cos−1(
$
˜
X1, ˜
X2
%
) .

3.8 General Quotient Spaces
65
The exponential map and its inverse can be expressed analytically using the
spherical geometry of Ss (see Example 3.5).
One can easily encounter a situation where a group G does not act on M by
isometries but the group action has an orthogonal section S. We would want to
study elements of the quotient space M/G using a Riemannian analysis on the
orthogonal section S. However, since the action is not by isometries, the inherited
metric is not available on the quotient space M/G. In the absence of a metric
on this space, we cannot talk about the isometry of the map from S to M/G,
although the map still exists. One possible solution is to use this natural map
S →M →M/G to induce a metric on M/G. By construction, this map will be
isometric and we can return to analyzing elements of M/G using the corresponding
elements of S.
Let us understand this using an example. As described in Example 3.15, the
semi-direct product Rn ⋊R× acts on Ln,k and this action has an orthogonal
section Sst deﬁned earlier (Eq. 3.16). We want to utilize the Riemannian structure
of Sst, obtained as a restriction of the Riemannian metric on Ln,k, to analyze
elements of the quotient space Ln,k/(Rn ⋊R×). As mentioned already, the action
of Rn ⋊R× on Ln,k is not by isometries and the inherited metric is not available
on the quotient space. So, we are going to identify Sst with the quotient space and
transfer the Euclidean metric on Sst to the quotient space. For any X1, X2 ∈Ln,k,
the intersections of their orbits with Sst is given by:
˜
X1 = ¯X1/| ¯X1|, ¯X1 = X1 −x11T
k
˜
X2 = ¯X2/| ¯X2|, ¯X2 = X2 −x21T
k ,
(3.18)
where x1 and x2 are the column sums of X1 and X2, respectively. With these
representatives, the geodesic and the geodesic distances are same as in the previous
item:
α(τ) =
1
sin(θ)(sin((1 −τ)θ) ˜
X1 + sin(τθ) ˜
X2),
θ = cos−1(
$
˜
X1, ˜
X2
%
) .
and d([X1], [X2]) = cos−1(
$
˜
X1, ˜
X2
%
). Since Sst is a unit sphere, albeit smaller in
dimension than the orthogonal section of Rn, one can easily write down the expo-
nential map and its inverse using the spherical geometry of Sst (see Example 3.5).
3.8 General Quotient Spaces
As we have seen, it becomes much easier to analyze a quotient space if it can be
identiﬁed with an orthogonal section, isometrically or otherwise. What about the
cases where no orthogonal section exists? In such cases, one has to develop the
analysis starting from the ﬁrst principles.
According to Deﬁnition 3.17, a geodesic in M/G is realized using a correspond-
ing geodesic in M that is the shortest between a ﬁxed point on one orbit and all
the elements of the second orbit. Let p1, p2 ∈M; we want to compute a geodesic
between their orbits [p1] and [p2] in M/G. Let ˜p2 ∈M be the point in [p2] nearest
to p1 under the distance dm, a distance that results from the chosen Riemannian

66
3 Background: Relevant Tools from Geometry
metric on M. Then, let α(τ) be a shortest geodesic path between p1 and ˜p2 in M
under this metric. The quotient path [α(τ)], i.e., the quotient of each point along
the path, indexed by the time τ, is the desired geodesic in M/G.
Example 3.17. Consider the action of the group G = Rn ⋊(R× × SO(n)) on Ln,k
and the problem of computing geodesics and geodesic distances on the quotient
space Ln,k/G. Denoting the subgroup (Rn ⋊R×) by G1, we will use two diﬀerent
approaches for the two components of G. For G1, we will use the orthogonal section
of its action on Ln,k, while for SO(n) we will use the approach of ﬁnding nearest
points. Since Ln,k/G1 can be identiﬁed with Sst, we can rewrite the quotient space
Ln,k/G as Sst/SO(n).
For any X1, X2 ∈Ln,k, we can compute the geodesics between them as follows.
Find their representatives in Sst using Eq. 3.18. Then, solve for:
O∗= argmin
O∈SO(n)
cos−1(
$
˜
X1, O ˜
X2
%
) = argmax
O∈SO(n)
$
˜
X1, O ˜
X2
%
.
Then, the desired geodesic in the quotient space Ln,k/G is given by [α(τ)] where:
α(τ) =
1
sin(θ)(sin((1 −τ)θ) ˜
X1 + sin(τθ) ˜
X2),
with θ = cos−1(
$
˜
X1, O∗˜
X2
%
).
The exponential map in M/G can be realized using a unit-speed geodesic in the
given direction. For a point [p] ∈M/G, and a tangent vector v ∈T[p](M/G) (i.e.,
in Np(M)), let us construct a unit-speed geodesic α(τ) from p in the direction
of v (existence of ψ is shown in Theorem 3.1). Then, since ˙α(0) ⊥Tp([p]), we
have ˙α(τ) ⊥Tα(τ)([α(τ)]) for all τ, by isometry, and the exponential map exp :
T[p](M/G) →M/G is given by exp[p](v) = [α(1)]. The inverse of an exponential
map takes a point on the quotient space M/G and maps it to an element (or
multiple elements) of the tangent space T[p](M/G). A vector v ∈T[p](M/G) is
said to be the inverse exponential of [q] ∈M/G at p if exp[p](v) = [q]. It is denoted
by v = exp−1
[p] ([q]) and is often not a unique point. That is, the inverse may be
set-valued.
Example 3.18. Continuing with the quotient space, Sst/SO(n). We want to com-
pute geodesics in this space and deﬁne exponential map and its inverse. For any
˜
X1, ˜
X2 ∈Sst, with the rotation orbits [ ˜
X1] and [ ˜
X2], let O∗∈SO(n) be the opti-
mal rotation on ˜
X2 that minimizes the distance between ˜
X1 and ˜
X2, as described
in the previous example. The shooting vector (initial velocity) of the geodesic path
given there is ˙α(0) = O∗˜
X2 −˜
X1. Hence, the inverse exponential map is given by:
exp−1
[ ˜
X1]([ ˜
X2]) = [
θ
sin(θ)(O∗˜
X2 −cos(θ) ˜
X1)]o,
where [V ]o = {OV |O ∈SO(n)} ,
and θ = cos−1(
$
˜
X1, ˜
X2
%
. Similarly, for any ˜
W ∈Rn×k such that W ⊥[W]o, with
[ ˜
W]o as deﬁned above, the exponential map is given by:
exp[ ˜
X]( ˜V ) = cos(| ˜V |) ˜
X + sin(| ˜V |)
˜V
| ˜V |
.

3.10 Center of an Orbit
67
Table 3.1 Options for inducing distances in quotient spaces
Action is by isometries
Action is not by isometries
Admits an orthogonal section
(1) = (2), or (3)
(2) or (3)
Admits a section that is not orthogonal
(1) or (2) or (3)
(2) or (3)
Does not admit a section
(1)
None
3.9 Distances in Quotient Spaces: A Summary
Consider the general set up where a group G acts on a Riemannian manifold
M and, for simplicity, let us assume that the orbits of G are closed sets in M.
In the previous sections, we have described several ways of computing distances
in quotient space M/G, depending on the nature of the group action. In the
following, we list these diﬀerent possibilities for deﬁning and computing distances
in a quotient space.
In general, we have the following methods available:
1. Method (1): Use Deﬁnition 3.17 to impose the distance:
dM/G([p1], [p2]) = min
g∈G dM(p1, (p2, g)) .
2. Method (2): Restrict the metric from M to S and compute geodesic distances
on the section S. Assign those distances to the corresponding orbits. If dS is
the distance in S, then set dM/G([p1], [p2]) = dS(p∗
1, p∗
2) where p∗
1 = [p1]∩S and
p∗
2 = [p2] ∩S.
3. Method (3): Ignore the original metric on M and choose a diﬀerent metric
on the section S. Deﬁne the distance between any two orbits as the distance
between the corresponding elements of S on S. That is, if dS is the distance in
S, then set dM/G([p1], [p2]) = dS(p∗
1, p∗
2) where p∗
1 = [p1] ∩S and p∗
2 = [p2] ∩S.
This, of course, is a less interesting option since the Riemannian structure of
M is completely ignored here.
We remark that in cases of a group action by isometries, and where the action
admits an orthogonal section, the results from method (1) and (2) will be identical.
It may however be computationally cheaper to use method (2) in this case since one
does not need to solve the optimization over G. In the case where the group action
admits a section but it is not orthogonal, and the action is not by isometries, one
can choose either (2) or (3), but the former is preferred because it uses the original
metric of M rather than choosing an arbitrary metric on the section (Table 3.1).
3.10 Center of an Orbit
As described in the previous sections, the action of a group G on a Riemannian
manifold M generates orbits whose membership deﬁnes an equivalence relation on
M. While technically all the elements of an equivalence class are equally good for
analysis, it is sometimes more eﬃcient or meaningful to select a particular element
of this set to represent its class. This element termed the center of the orbit is

68
3 Background: Relevant Tools from Geometry
deﬁned with respect to a set of points, called the reference set, in M. To deﬁne
the center precisely, let the action of G be by isometries with respect to the metric
of M. We will also need this action to be free, which means the following: if for
any p ∈M, the fact (g, p) = p implies that g = e (the identity element), then the
action is called free. Assume further that there is a metric structure on G itself
that enables one to compute the mean (e.g., Karcher mean) of a ﬁnite number of
points in G.
The center of the orbit of a point p ∈M is deﬁned as follows.
Deﬁnition 3.21 (Center of An Orbit). Let a group G act freely on a Rieman-
nian manifold M such that the action of G is by isometries. For any point p, let
[p] denote the orbit of p and let R = {p1, p2, . . . , pk} for a reference set. Then, the
center of the orbit of p with respect to the set R is given by a point q if it satisﬁes
the following properties:
1. q ∈[p].
2. Let g∗
i = argming dm(g · q, pi)2, for i = 1, 2, . . . , k, where dm is the geodesic
distance on M. Then, the mean of the g∗
i s is the identity in G.
We provide a simple algorithm for computing the center of the orbit of a given
point p, with respect to a reference set R. This algorithm assumes that the natural
action of G on itself is by isometries under the chosen metric of G. Now, for any
set g1, g2, . . . , gk, let their mean be given by:
¯g = argmin
g∈G
dg(g, gi)2 .
For the shifted problem, where each gi is replaced by gi · ¯g−1, the mean is given
by:
¯g0 = argmin
g∈G
dg(g, gi · ¯g−1)2 = argmin
g∈G
dg(g · ¯g, gi)2 ,
This mean ¯g0 satisﬁes ¯g = ¯g0 · ¯g. Hence, ¯g0 = gid, the identity element of G. To
summarize this result, if we replace each gi by gi ·¯g−1, then the mean changes from
¯g to gid. This idea is depicted in Fig. 3.8 and it leads to the following algorithm
for computing the center of an orbit.
Algorithm 2 (Computation of Center of an Orbit).
1. Let r ∈[p] be any arbitrary point.
2. Compute gi = argming∈G dm(g · q, pi), for all i = 1, 2, . . ., k.
3. Compute the mean ¯g of the set {g1, g2, . . . , gk} using the distance dg on G.
4. Update r by q = ¯g−1 · r.
Example 3.19. Consider the action of G = SO(n) on the space M = Ln,k under
the Euclidean metric. We know that this action is by isometries and is free. The
latter implies that whenever we have OX = X, for an X ∈Ln,k, then O = In.
Let G have a metric structure given by the Frobenius norm, i.e., dg(O1, O2) =

l,j(O1(l, j) −O2(l, j))2. The mean of a set of rotation matrices O1, O2, . . . , Ok
is given by forming the element-wise average A =
1
k

i Oi and using its SVD
decomposition A = UΣV T . If the determinant of A is positive, then ¯O = UV T ,

3.11 Exercises
69
Fig. 3.8 The point q =
¯g−1 · r form the center of
the orbit [p] with respect
to the reference set R =
{p1, p2, . . . , pn}
p1
p2
p3
[p]
g1
r
q =g−1 · r
g2
g3
gk
pk
¯
center of orbit [p]
else the last column of V changes sign (this assumes that the entries in Σ are
non-increasing from top-left to bottom-right).
Given a point X ∈Ln,k and a reference set R = {X1, X2, . . . , Xk}, let:
Oi = argmin
O∈SO(3)
|OX −Xi|2 ,
i = 1, 2, . . . , k .
Then, set Y = ¯OT X where ¯O is the average of the rotations {O1, O2, . . . , Ok}.
The point Y ∈Ln,k becomes the center of [X] with respect to the reference set R.
3.11 Exercises
3.11.1 Theoretical Exercises
1. Is “≤” an equivalence relation on the set of real numbers?
2. In R2, deﬁne a relation x ∼y if x and y lie on the same straight line passing
through the origin. Is this an equivalence relation? What if this relation is
deﬁned for the set R2 \ {(0, 0)}?
3. In the set Rn \ {0}, deﬁne the relation x ∼y if there exists a nonzero scalar
a such that y = ax. Verify that this an equivalence relation. (Example 3.1,
item 2).
4. Verify that, in Rn, the relation x ∼y if |x| = |y| forms an equivalence relation.
(Example 3.1, item 3).
5. Verify that the following maps are symmetric, positive deﬁnite, and bilinear:
• Φ : Rn×n × Rn×n →R given by: Φ(A, B) = trace(ABT ).
• For any p ∈Sn, let Φ : Tp(Sn) × Tp(Sn) →R given by Φ(v, w) = vT w.
• For any x ∈R2
+ = {(x1, x2)|x1 ∈R, x2 ≥0}, let Φx : R2 × R2 →R, given
by Φx(v, w) = vT w
x2
2 .

70
3 Background: Relevant Tools from Geometry
6. Show that Φ : R3 × R3 →R given by
Φ(x, y) = x1y1 + x2y2 −x3y3 ,
is not a Riemannian metric on R3.
7. Consider the upper half-plane H = {(x1, x2) ∈R2|x2 > 0}. Show that the
inner product given by ⟨v, w⟩x =
vT w
x2
2
deﬁnes a Riemannian metric on H.
(This metric is called the hyperbolic metric.)
8. Show that the mapping of Rn to itself, given by x →Ox, for a ﬁxed O ∈SO(n),
is an isometry under the Euclidean metric (Example 3.3). If we replace O by
an A /∈SO(n), is the resulting mapping an isometry, under the Euclidean
metric?
9. Is the mapping of Rn to itself, given by x →ax, for a ﬁxed a ∈R+, an
isometry under the Euclidean metric? What about under the metric ⟨v, w⟩x =
(vT w)/(xT x)?
10. Show that the geodesics in the following cases have these known structures:
• Geodesics in Rn under the Euclidean metric are straight lines.
• Geodesics on a unit sphere under the Euclidean metric are great circles.
• Geodesics between points in the 2D upper half-plane (H deﬁne above in
Problem 7), under the hyperbolic metric, are as follows. If the two points
have the same y coordinate, then the geodesic is a straight line passing
through those points. Otherwise, the geodesic is a part of the circle that
goes through these points and has its center on the x axis.
11. Let M = Rn with the Euclidean Riemannian metric and let M be the space
of absolutely continuous functions from [0, 1] to M. Using Theorem 3.2, derive
an expression for the geodesic between any two elements of M. Repeat this
problem for M = S2 with the Euclidean metric.
12. Here we verify the expression for parallel transport of a vector along a geodesic
path in S1, that was used in the proof of Theorem 3.3. Let e1, e2 be an or-
thonormal basis of R2 and let v = re2 be an arbitrary element of Te1(S1). Verify
that the parallel transport of v along the geodesic α(t) = cos(t)e1 + sin(t)e2
is given by −r sin(t) + r cos(t)e2. In other words, show that this expression
satisfy the three properties given in Eq. 3.8.
13. Show that the mapping v →Ψ(v), given in Eq. 3.9, forms a rotation of Sn−1.
14. Verify that the following three maps deﬁne group actions on Rn:
• Translation: Rn × Rn →Rn given by (x, y) = x + y.
• Rotation: SO(n) × Rn →Rn given by (O, x) = Ox.
• Scaling: R× × Rn \ {0} →Rn \ {0} given by (a, x) = ax.
15. First, verify that the semi-direct product Rn ⋊(R× × SO(n)) forms a group.
Then, verify that it forms a group action on Rn \ {0} according to the map:
((y, a, O), x) = aOx + y.
16. Show that if a group G acts on a manifold M, we can deﬁne an equivalence
relation as follows: for any two p, q ∈M, we have p ∼q if p ∈[q]. Show that
this forms an equivalence relation between elements of M. In other words, the
orbits of G form the equivalence classes under this relation.
17. Consider the action of SO(2) on R2 given by (O, x) = Ox with the Euclidean
metric on R2. Write down the quotient space R2/SO(2) and derive an expres-
sion for distance inherited on the quotient space using Deﬁnition 3.17.

3.11 Exercises
71
18. Study of sections of landmark spaces:
a. For an X ∈Ln,k, let [X] = {X+x1T
k |x ∈Rn} and St, as deﬁned in Eq. 3.14,
show that the vector spaces TX([X]) and TX(St) are orthogonal.
b. For an X ∈Ln,k, let [X] = {aX|a ∈R×} and Ss, as deﬁned in Eq. 3.15,
show that the vector spaces TX([X]) and TX(Ss) are orthogonal.
c. Show that the set Sst in Eq. 3.16 is an orthogonal section of the action of
(Rn ⋊R×) on Ln,k.
19. For a group G acting on a Riemannian manifold M, let (a) denote the con-
dition that the group action is by isometries and (b) denote that there is an
orthogonal section for this action. For M = Ln,k, ﬁnd groups that provide an
example of each of the three situations:
a. (a) is true and (b) is not.
b. (b) is true and (a) is not.
c. Both (a) and (b) are true.
20. Consider the action of the translation group G = Rn on M = Ln,k. Here G
acts on M by isometries and the group action admits an orthogonal section. As
described in Sect. 3.9, both methods (1) and (2) can be used in this instance.
Show that the distances resulting from the two methods are identical.
21. Aﬃne Group Action: Here we are going to consider the action of group
G = GL(2) ⋊R2 on L2,k.
a. First, verify that the standard actions of GL(2) and R2 on R2 do not
commute but they form a semi-direct product group using the operation:
(A1, b1) · (A2, b2) = (A1A2, A2b1 + b2). G = GL(2) ⋊R2 is called the aﬃne
group.
b. Show that this semi-direct group G = GL(2) ⋊R2 acts on R2 according to
the map:
((A, b), x) = Ax + b,
A ∈GL(2),
b ∈R2,
x ∈R2 .
c. Furthermore, show that the action of G on R2, under the Euclidean metric,
is not by isometries.
d. Let S ⊂L2,k be a set of k-tuples in R2 that satisfy:
1
k
k

i=1
xi = 0,
and
1
k
k

i=1
xixT
i = I2 ,
where I2 is the 2 × 2 identity matrix. Show that S intersects every orbit of
the action of G on R2×k in at least one point. Is it a section of the action?
The mapping of a conﬁguration x ∈L2,k into the section S is called a stan-
dardization of x. For eﬃcient algorithms on standardization of planar conﬁg-
urations, please refer to [21].
22. Consider the action of R on R2 given by vertical translations: for any a ∈R
and x ∈R2 the action is given by (a, x) = x +

0
a

. For a point x ∈R2 and
a reference set R = {x1, x2, . . . , xk} ⊂R2, ﬁnd the center of the orbit [x] with
respect to R.

72
3 Background: Relevant Tools from Geometry
23. Similarly, now consider the action of the scaling group R× on Rn −{0} given
by for any a ∈R× and x ∈Rn −{0} the action is given by (a, x) = ax. For
a point x ∈Rn −{0} and a reference set R = {x1, x2, . . . , xk} ⊂Rd, ﬁnd the
center of the orbit [x] with respect to R.
3.11.2 Computational Exercises
1. Write a program to compute the shortest, unit-speed geodesic path α : [0, 1] →
S2 between any two non-antipodal points. The program should compute and
display geodesic points at times t = 0, 1
T , 2
T , . . . , 1 for a given positive integer
T = 10.
2. Write subroutines to compute the exponential map, inverse exponential map,
and parallel transport equation along a geodesic path for the unit sphere Sk.
3. Write a program to compute and display geodesic paths (not necessarily con-
stant speed) between arbitrary points in the upper half-plane under the hyper-
bolic metric. (A description of these geodesics is provided in Example 3.4.)
4. Landmark Shape Geodesics: Write a program to compute and display
geodesic paths and geodesic distances between elements of the following
quotient spaces: (1) Ln,k, (2) Ln,k/SO(n), (3) Ln,k/Rn, (4) Ln,k/R×, (5)
Ln,k/(Rn ⋊SO(n)), (5) Ln,k/(Rn ⋊R×) and (6) Ln,k/(Rn ⋊(R× × SO(n))).
Demonstrate this program using some triangular shapes for n = 2 and k = 3.
5. Implement the formulas given in Example 3.17 to compute (discrete-time)
geodesic paths, exponential map, and its inverse on the quotient space
Ln,k/(Rn ⋊(R× × SO(n))).
6. Take two landmark conﬁgurations, i.e., elements of Ln,k; repeatedly apply ran-
dom rotations, translations, and scales to each of them; and compute the
geodesic distance between them in the quotient space: Ln,k/(Rn ⋊(R× ×
SO(n))). Study the variability associated with the resulting distances.
3.12 Bibliographic Notes
This chapter provides an overview of some relevant tools from geometry and al-
gebra. For more detailed discourse on these topics, the reader is referred to full
texts such as Boothby [19], Lang’s textbooks on algebra and geometry [64], Hel-
gason [36], and Warner [120]. The tools for landmark-based shape analysis have
been developed by many researchers, leading to full textbook-level treatments in
Dryden and Mardia [28], Small [102], and Kendall et al. [49]. The concept of the
center of an orbit was discussed in [108] but only for the speciﬁc warping group
introduced in the next few chapters.

Chapter 4
Functional Data and Elastic Registration
Functional data analysis (FDA) is a branch of statistics where one observes, mod-
els, and analyzes quantities that are functions on certain intervals. This kind of
data naturally arises in nearly every branch of science, ranging from engineering to
geology, biology, medicine, and chemistry. Similar to more classical statistics, where
one summarizes, models, regresses, estimates, and tests data involving Euclidean
vectors, in FDA, one performs these tasks while working with functional data. The
functions are treated as elements of function spaces and one performs calculus on
these spaces in a manner similar to the Euclidean calculus. In traditional FDA,
statistical tasks have commonly been performed using the Hilbert structure of the
function spaces resulting from the L2 norm. This framework leads to simple al-
gorithms for computing L2 distances, cross-sectional (i.e., point-wise) means and
variances, and functional principal component analysis (FPCA) of given function
data. Note that even though computer implementations require us ultimately to
discretize functional data, the development of methodologies and statistical solu-
tions in FDA regards them as functions and exploits additional structure present
in functional data. We refer the reader to Appendix A.3 where we present some
basic elements of functional analysis, including deﬁnitions of Banach and Hilbert
manifolds and submanifolds. (We also provide some background material on FPCA
later, in Sect. 4.3.1.)
What is the fundamental diﬀerence between multivariate statistics and FDA?
This question is quite relevant considering the fact that data is generally collected
and stored in the form of discrete point sets. Even if the data is available in a func-
tion form, most computational techniques discretize such functions and perform
computations on the resulting ﬁnite vectors, much like multivariate statistics. So,
what makes FDA diﬀerent from multivariate analysis? We highlight a fundamen-
tal diﬀerence using Fig. 4.1, where the left panel shows a set of points {(ti, yi)}
sampled from a function on the domain [0, 1]. In multivariate statistics, one works
with the vector {yi} using vector calculus and analyzes it using statistical meth-
ods. In FDA, one keeps the association of values {yi} with time indices {ti}, seeks
the underlying function itself, and estimates it using some interpolation or esti-
mation method (see the middle panel). Having obtained this function, one can
resample it at arbitrary points on the domain for performing functional calculus
(diﬀerentiation, integration, etc). This resampling aspect distinguishes FDA from
multivariate analysis. Keep in mind that function spaces are inﬁnite dimensional
and that necessitates some additional considerations beyond Euclidean calculus.
© Springer-Verlag New York 2016
A. Srivastava, E.P. Klassen, Functional and Shape Data Analysis,
Springer Series in Statistics, DOI 10.1007/978-1-4939-4020-2 4
73

74
4 Functional Data and Elastic Registration
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
Interpolation
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
Resampling
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
Fig. 4.1 FDA allows us to interpolate between given data points (middle panel) and sample
the ﬁtted curve at arbitrary points in the domain (right panel). This distinguishes FDA from
multivariate statistics where the sample points are ﬁxed as given
FDA also plays an important role in shape analysis of curves since curves are nat-
urally represented as functions, more precisely parameterized curves, rather than
as Euclidean vectors.
An interesting and challenging situation arises when the given data is not regis-
tered. Generally speaking, this implies that the peaks and valleys of given functions
are not well aligned. This situation is the main focus of this chapter where we dis-
cuss how to handle the problem of registering two or more functions. We describe
some speciﬁc function spaces of interest and study their Riemannian geometries
under a speciﬁc Riemannian metric that is especially relevant for functional regis-
tration and shape analysis. This Riemannian metric, called the Fisher-Rao metric,
is closely related to the elastic metric for shape analysis of curves, introduced later
in Chap. 5. In fact, that elastic metric for curves can be considered as a natural
extension of the Fisher-Rao metric to Rn-valued functions.
4.1 Goals and Challenges
Functional data analysis or FDA is the analysis of random variables that take val-
ues in certain function spaces. In FDA, a serious challenge arises when functions
are observed with ﬂexibility or domain warping along the x axis (or the horizontal
axis). This warping may come either from an uncertainty in the measurement pro-
cess or may simply denote an inherent variability in the underlying process itself
that needs to be separated from the variability along the y axis (or the vertical
axis). As another possibility, the warping may be introduced as a tool to horizon-
tally align the observed functions, reduce their variance, and increase parsimony
in the resulting model. This process is analogous to using polar decomposition of
Euclidean data: take an x ∈Rn and form the pair (|x|, x/|x|) ∈(R+ × Sn−1). This
decomposition maps x into its amplitude |x| and direction x/|x|, and it often is
useful in some situations. For instance, in some cases, we may not be interested in
one of these two components in our analysis. Or, it may be more natural to model
these two separately.
Consider the two functions, f1 and f2, shown in the top-left panel of Fig. 4.2.
Each of these functions has two peaks. However, because the peaks are centered
at diﬀerent points, their cross-sectional mean (a function formed by taking the
mean of heights at each t), shown in the next panel, has three peaks. If we warp
the domain of f1, such that its peaks/valleys are now aligned with those of f2, as
shown in bottom-left panel, then the cross-sectional mean preserves that bimodal
structure (bottom-middle panel). The warping function γ used here is shown in

4.2 Estimating Function Variables from Discrete Data
75
0
0.2
0.4
0.6
0.8
1
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
f1
f2
0
0.2
0.4
0.6
0.8
1
−1
0
1
2
3
4
5
Original Functions
0
0.2
0.4
0.6
0.8
1
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
f1t
f2
0
0.2
0.4
0.6
0.8
1
−0.5
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
γ
γid
Aligned Functions
After Warping
Warping Functions
Mean ± Std Dev
After Warping
Mean ± Std Dev
Before Warping
Fig. 4.2 Averaging of two functions, f1 and f2, before and after warping of f1
the far right panel. This picture illustrates the beneﬁts of decomposing the x-
variability from the y-variability in the observed functions before doing statistical
analysis. After warping of the domain of f1, the two resulting functions f1 ◦γ and
f2 are vertically aligned and represent the y-variability in the data. The actual
warping function used in this alignment, thus, represents the x-variability in the
data. In addition to better preserving the structure of the observed data, a separate
modeling of y and x variability can be more natural, parsimonious, and eﬃcient.
The variability associated with warping of functions is called the phase vari-
ability and the remaining variability along the vertical axis is called the amplitude
variability. We will formally introduce the concept of phase and amplitude compo-
nents of functional data in Sect. 4.7 and will continue studying them in Chap. 8.
We will also study a related problem of registration between any two functions,
i.e., which point on one function corresponds to which point on the second func-
tion. We will use an extension of the classical Fisher-Rao metric, albeit in its
nonparametric form, to tackle this problem.
We start, however, with a very practical problem of estimating full functions
from discrete observations.
4.2 Estimating Function Variables from Discrete Data
We are interested in analyzing functional data, but most applications typically
result in discrete data of the type {(ti, yi) ∈([0, 1] × R)|i = 1, 2, . . ., m}. So, the
very ﬁrst problem one faces is the estimation of a function f from its discrete
observations. The main goal of function estimation, at least from the perspective

76
4 Functional Data and Elastic Registration
0
20
40
60
80
100
0
20
40
60
80
100
2.5
2.6
2.7
2.8
2.9
3
3.1
3.2
3.3
3.4
3.5
2.5
2.6
2.7
2.8
2.9
3
3.1
3.2
3.3
3.4
3.5
Fig. 4.3 If we connect the given data points (left) using straight lines, we get a noisy function
(right)
of shape and functional data analysis, is to be able to provide values of f at
arbitrary points in the domain [0, 1]. While there are numerous solutions to this
problem, depending on the nature of the data and the assumptions about f, we
brieﬂy discuss a couple of standard solutions that will be useful throughout this
textbook. The most straightforward way, of course, is to form a piecewise linear
function by simply connecting the given data points, as shown in Fig. 4.3. This
naive solution is prone to noise and it is hard to control the level of smoothness of
the solution. Therefore, one seeks other ideas for estimating f.
Least-Squares Estimation Using a Basis:
One idea is to formulate a least-squares problem by using a truncated basis of an
appropriate function space. We will assume that the desired function belongs to a
Hilbert space F with a given inner product structure, denoted by ⟨·, ·⟩. Let B =
{b1, b2, . . . } denote a complete basis of F, not necessarily orthogonal. Then, for any
f ∈F, we can represent it using the expansion: f(t) = ∞
k=1 ckbk(t). Truncating
the series to ﬁrst K terms, a least-squares estimate of f can be derived as:
ˆf = argmin
c∈RK
m

i=1

yi −
K

k=1
ckbk(ti)
2
(4.1)
Using a matrix notation where Bik = bk(ti), c = [c1, c2, . . . , cK], and y =
[y(t1), y(t2), . . . , y(tK)], we can rewrite this problem as:
ˆc = argmin
c∈RK

(y −Bc)T (y −Bc)

= (BT B)−1BT y , assuming BT B is non-singular.
The vector of ﬁtted values is then ˆy = B(BT B)−1BT y. In case B forms an or-
thonormal set, then BT B is simply the identity.
We illustrate this idea using a simple example. Assuming that f is a square-
integrable function on the desired interval [0, T ], we choose the following basis
set:
b1(t) = 1,
b2j(t) = sin(2πj t
T ),
b2j+1(t) = cos(2πj t
T ), j = 1, . . . , J ,
with K = 2J + 1 total basis elements. The top-left panel of Fig. 4.4 shows a
collection of ordered points that form the set (ti, yi) with T = 100. Our goal is
to estimate the underlying function f using a least-squares criterion. The top row
shows estimates of f for values J = 1, 3, and 20. As the value of K increases, the

4.2 Estimating Function Variables from Discrete Data
77
2.5
2.6
2.7
2.8
2.9
3
3.1
3.2
3.3
3.4
3.5
2.5
2.6
2.7
2.8
2.9
3
3.1
3.2
3.3
3.4
3.5
2.5
2.6
2.7
2.8
2.9
3
3.1
3.2
3.3
3.4
3.5
2.5
2.6
2.7
2.8
2.9
3
3.1
3.2
3.3
3.4
3.5
J = 1
J = 3
J = 30
J = 1
J = 3
J = 30
Least-square function estimation
0
20
40
60
80
100
0
20
40
60
80
100
0
20
40
60
80
100
0
20
40
60
80
100
0
20
40
60
80
100
0
20
40
60
80
100
0
20
40
60
80
100
0
20
40
60
80
100
2.5
2.6
2.7
2.8
2.9
3
3.1
3.2
3.3
3.4
3.5
2.5
2.6
2.7
2.8
2.9
3
3.1
3.2
3.3
3.4
3.5
2.5
2.6
2.7
2.8
2.9
3
3.1
3.2
3.3
3.4
3.5
2.5
2.6
2.7
2.8
2.9
3
3.1
3.2
3.3
3.4
3.5
Penalized least-square function estimation
Fig. 4.4 Example of ﬁtting a curve to noisy, discrete data using Fourier harmonics of increasing
order
estimate increasingly ﬁts better to the given data but, at the same time, becomes
noisier also. One can control the smoothness of the solution using the parameter K.
An important question here is: How to choose the basis size K? Some insight
into this choice is obtained by the following decomposition:
V ar( ˆf) = E[∥ˆf −E[ ˆf]∥2] = E[∥ˆf −f + f −E[ ˆf]∥2]
= E[∥ˆf −f∥2] + E[∥f −E[ ˆf]∥2] + 2E[⟨ˆf −f, f −E[ ˆf]]⟩
= E[∥ˆf −f∥2] −[∥f −E[ ˆf]∥2] .
Since E[∥ˆf −f∥2] is the mean-squared error of ˆf, or MSE( ˆf), and (E[ ˆf] −f) is
the bias of ˆf, with its norm denoted by Bias( ˆf), we get:
MSE( ˆf) = Bias( ˆf)2 + V ar( ˆf) .
This decomposition represents the trade-oﬀthat governs the selection of
J (= 2K + 1) in the deﬁnition of ˆf. As J increases, the V ar( ˆf) increases and
Bias( ˆf) decreases. This is depicted in Fig. 4.5 where the MSE( ˆf) is shown in the
solid line. It reaches the minimum value at J = 7 and increases in either direction.
One can choose the minimum of MSE as a guiding principal for selecting K,
but that requires the knowledge of f itself (to compute the Bias and the MSE).
Thus, while this principal provides an important insight into the choice being
made, it does not actually help in making a decision.
Penalized Least Squares : Another way to control the bias-variance trade-oﬀ
is by adding an extra penalty term in Eq. 4.1:
ˆf = argmin
c∈RK
⎛
⎝
m

i=1

yi −
K

k=1
ckbk(ti)
2
+ λR(f)
⎞
⎠,
(4.2)
where λ > 0 is a constant and R(f) is a measure of roughness of f. A commonly
used term for capturing roughness of a function is R(f) =
 1
0 ( ¨f(t))2 dt. Continuing

78
4 Functional Data and Elastic Registration
Fig. 4.5 Illustration of
bias-variance trade-oﬀin
choice of K in estimating
f using the least-squares
criterion. The solid line is
MSE, the broken line is
V ar( ˆf), and the marked
line is Bias( ˆf)
0
5
10
15
20
0
0.1
0.2
0.3
0.4
0.5
0.6
with the basis representation of f, we can write ¨f(t) = 
k ck¨bk(t). Truncating the
basis, as earlier, at K terms, let us write the penalty term in the discrete form as
the matrix:
R(f) =
 1
0
¨f(t)2 dt =
 1
0
(

k
ck¨bk(t))(

j
cj¨bj(t)) dt
=

k

j

cjck
 1
0
¨bk(t)¨bj(t) dt

= cT Rc ,
where R is a K ×K symmetric matrix with elements Rjk =
 1
0 ¨bk(t)¨bj(t) dt. Thus,
we reach the discrete optimization problem:
ˆc = argmin
c∈RK

(y −Bc)T (y −Bc) + λcT Rc

.
(4.3)
The solution is given by ˆc = (BT B + λR)−1BT y with the estimated function
being ˆfλ(t) = K
k=1 ˆckbk(t). The bottom row of Fig. 4.4 shows examples of ﬁtted
functions for λ = 0.001 for J = 1, 3, and 30.
4.3 Geometries of Some Function Spaces
In this section, we discuss basic geometries of some function spaces of interest.
4.3.1 Geometry of Hilbert Spaces
Consider the set of square integrable, real-valued functions, on the unit interval
[0, 1], denoted by L2([0, 1], R) or simply L2. L2, is a vector space and, as described
in Appendix A.3, becomes a Hilbert space using the standard L2 inner product:
⟨f, g⟩=
 1
0
f(t)g(t)dt ,
for f, g ∈L2 .
(4.4)

4.3 Geometries of Some Function Spaces
79
The resulting L2 norm is given by ∥f∥=
 1
0 f(t)2dt. Since L2 is a Hilbert space,
it is also a Hilbert manifold, whose tangent space is given by Tf(L2) = L2 for
all f ∈L2. Therefore, one can deﬁne geodesic paths, exponential maps, etc., very
easily, as follows:
1. Shortest Geodesic: For any f1, f2 ∈L2, the shortest geodesic path between
them is given by a “straight line”:
α : [0, 1] →L2,
α(τ)(t) = (1 −τ)f1(t) + τf2(t) .
(4.5)
This is a constant-speed geodesic with the speed given by ∥˙α(τ)∥= ∥f2 −f1∥.
Its shooting vector (initial velocity ˙α(0)) is given by point-to-point diﬀerence
f2 −f1.
2. Exponential Map: Given a starting point f ∈L2 and a shooting direction
v ∈L2, the unique constant-speed geodesic with those initial values is given
by the vector translation α(τ)(t) = f(t) + τv(t). Thus, using Deﬁnition 3.6,
the exponential map expf : L2 →L2 is given by a simple sum (translation):
expf(v) = f + v.
3. Inverse Exponential Map: Since the exponential map is given by a sum,
its inverse is given by a diﬀerence (also a translation). For any f1, f2 ∈L2:
exp−1
f1 (f2) = f2 −f1.
In view of these simple formulas, a basic statistical analysis on L2, such as comput-
ing sample means under the L2 norm, is straightforward. Given a set of functions
f1, f2, . . . , fn ∈L2, their mean is given by:
¯f = argmin
f∈L2
 n

i=1
∥fi −f∥2

= argmin
f∈L2
 n

i=1
 1
0
(fi(t) −f(t))2 dt

Taking the summation inside, and since the cost function is additive over t, one
can solve for each t separately, leading to the solution:
¯f(t) = argmin
f(t)∈L2
 n

i=1
(fi(t) −f(t))2 dt

= 1
n
n

i=1
fi(t) .
(4.6)
This is called the cross-sectional sample mean.
From a practical point of view, the only diﬃculty in analyzing elements of L2
is its dimensionality: L2 is inﬁnite dimensional. This challenge is usually handled
using a dimension-reduction tool, such as the principal component analysis (PCA),
so that the analysis is restricted to a ﬁnite-dimensional subspace of L2. We describe
this idea next.
PCA in Hilbert Spaces The application of PCA to function spaces is naturally
called functional PCA or FPCA. One way to motivate FPCA is to consider the
model:
fi(t) = μf(t) +
∞

j=1
ci,jbj(t) + ϵi(t) ,
(4.7)
where:
• μf(t) is the expected value of fi(t).
• {bj} forms an orthonormal basis of L2.

80
4 Functional Data and Elastic Registration
• ϵi ∈L2 is considered the noise process, typically chosen to be a white Gaussian
process with zero mean.
• ci,j ∈R are coeﬃcients of fi with respect to {bj}. In order to ensure that μf is
the mean of fi, we impose the condition that the sample mean of {c·,j} is zero.
Given a set of observed functions {fi}, the estimation of these model parameters
is performed using the minimization:
(ˆμf,ˆb) =
argmin
μf ,{bj},{ci,j}
⎛
⎝
n

i=1
∥fi −μf −
J

j=1
⟨fi, bj⟩bj∥2
⎞
⎠,
(4.8)
and set ˆci,j =
$
fi,ˆbj
%
.
This minimization is achieved as follows. For the mean function, we can use the
same mean estimate ˆμf(t) = 1
n
n
i=1 fi(t). For estimating the basis set, deﬁne the
sample covariance function ˆC : [0, 1] × [0, 1] →R according to:
ˆC(s, t) =
1
n −1
n

i=1
(fi(s) −ˆμf(s))(fi(t) −ˆμf(t)) ,
with ˆμf as given above. The function ˆC is by deﬁnition symmetric and pos-
itive semideﬁnite, where the latter means that for any g
∈
L2, we have
 1
0
 1
0 ˆC(s, t)g(s)g(t) ds dt ≥0. This covariance function
ˆC deﬁnes a linear
operator on L2 using the formula:
A : L2 →L2,
Af(t) =
 1
0
ˆC(s, t)f(s) ds .
Since ˆC is positive semideﬁnite, we have ⟨Af, f⟩≥0 for all f ∈L2. According to
the Karhunen-Loeve expansion theorem [72], the eigenfunctions of A provide the
principal components of the function data. A function b ∈L2 is an eigenfunction
of ˆC if Ab(t) =
 1
0 ˆC(s, t)b(s)ds = λb(t), for t ∈[0, 1] and λ ∈R is a constant. Let
ˆb1, ˆb2, . . . ,ˆbJ be the eigenfunctions of A, i.e., Aˆbj(t) = λjˆbj(t), so that the cor-
responding eigenvalues {λj} satisfy |λ1| ≥|λ2| ≥. . . . Then, {ˆbj, j = 1, 2, . . ., J}
solves the optimization problem in Eq. 4.8; they are termed the ﬁrst J principal
directions of variations in the given data. The space spanned by them is called the
principal subspace.
In practice, a sampling of functions on a ﬁnite partition of [0, 1] is used
to perform computations. For convenience, consider the uniform partition
{0, 1
T , 2
T , . . . , 1} and, with a slight abuse of notation, let fi ∈RT +1 denote a
vector of values of the original functions sampled at these points. Since these vec-
tors are ﬁnite dimensional, the standard multivariate statistical tools apply. The
sample covariance matrix of this dataset, also denoted by ˆC, is a (T + 1) × (T + 1)
symmetric, positive semideﬁnite matrix. (This matrix is often singular because the
number of observations n is usually less than the partition size T .) Consequently,
the SVD of this covariance matrix leads to the principle directions and dominant
modes of variability in the given data. One can use interpolation to obtain values
of functions at arbitrary points and, thus, to recreate full functions. Figure 4.6
shows an example of FPCA. The leftmost panel shows 10 unimodal functions
with synchronized peaks and the only diﬀerence is in the heights of their peaks.

4.3 Geometries of Some Function Spaces
81
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
2
4
6
8
10
12
14
16
18
20
0
2
4
6
8
10
12
14
16
18
20
1
2
3
4
5
0
50
100
150
200
250
300
350
400
450
0
0.05
0.1
0.15
0.2
0.25
function data {fi}
mean f¯
singular values
ˆb1
Fig. 4.6 FPCA of function well-aligned data
0
0.2
0.4
0.6
0.8
1
0
2
4
6
8
10
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.5
1
1.5
2
0
5
10
15
0
20
40
60
80
100
120
140
160
-0.2
-0.15
-0.1
-0.05
0
0.05
0.1
0.15
0.2
0.25
function data
ˆ
sample mean mf
singular values
ˆ
ˆ
ˆ
b1, b2, b3
-30
-20
-10
0
10
20
30
-30
-20
-10
0
10
20
30
-15
-10
-5
0
5
10
15
20
-10
-5
0
5
10
15
ˆmf ± l1 b1ˆ
ˆmf ± l2 b2ˆ
ˆmf ± l3 b3ˆ
ˆμf ± l4 b4
ˆ
Fig. 4.7 FPCA of the function data shown in top left. We caution the reader that y axes are
scaled diﬀerently in diﬀerent plots
The next panel shows their cross-sectional mean ˆμf(t). The singular values {λi}
are plotted in the third panel. In this example, there is only one nonzero sin-
gular value and the corresponding eigenfunction ˆb1 is shown in the last panel.
This makes intuitive sense since there is only one mode of variation in the given
functions, namely, the height and, thus, only one singular value is nonzero.
We consider another example in Fig. 4.7 where the functions are still unimodal
but now they diﬀer in both the locations and heights of their peaks (top-left panel).
In fact, the diﬀerences in the locations of peaks overshadow the diﬀerences in their
heights. Their cross-sectional mean μ is shown in the next panel. The vector of
singular values of the covariance matrix is shown in the third panel, and the top
three eigenfunctions ˆb1, ˆb2, and ˆb3 are shown in the last panel. In the bottom row,
we see the top four directions of variation in the data, using the function band
ˆμf ± λjˆbj around ˆμf, for j = 1, 2, 3, and 4. This example illustrates an important
limitation of FPCA, if applied directly to these functions. The sample mean ˆμf and
the principle modes fail to capture the main source of variability in the original
functions, namely, the location of the modes. This example motivates the need
for registration of functions, i.e., alignment of peaks and valleys in fis, before or
during FPCA. This alignment is performed by warping the domains of fis and
introduces a certain amount of “elasticity” in the given functions. The problem
of pairwise alignment of functions is considered in Sect. 4.4, while the problem of
joint alignment of multiple functions is studied in Chap. 8.

82
4 Functional Data and Elastic Registration
Next we will look at some submanifolds of L2 that are of particular interest in
our approach to shape analysis.
4.3.2 Unit Hilbert Sphere
A submanifold of L2 that is both interesting and useful in shape analysis is the
unit sphere S∞⊂L2:
S∞= {f ∈L2|∥f∥= 1} .
As mentioned in Appendix A.3, S∞is a submanifold of L2 and becomes a Hilbert
manifold with the L2 Riemannian metric (Eq. 4.4). Although S∞is no longer a
vector space, its geometry is still relatively simple since the formulas presented for
ﬁnite-dimensional spheres also apply to S∞. For an f ∈S∞, the tangent space
Tf(S∞) = {v ∈L2| ⟨v, f⟩= 0} (the inner product is given in Eq. 4.4) and other
basic items are as follows:
1. Geodesic: Similar to Eq. 3.5, the (arc-length parameterized) geodesic path be-
tween any two points f1, f2 ∈S∞(f1 ̸= −f2) is given by:
α(τ) =
1
sin(θ) [sin(θ(1 −τ))f1 + sin(τθ)f2] ∈S∞,
(4.9)
where θ = cos−1(⟨f1, f2⟩). For any τ ∈[0, 1], α(τ) is a function given by:
α(τ)(t) =
1
sin(θ) [sin(θ(1 −τ))f1(t) + sin(τθ)f2(t)] .
Figure 4.8 shows two examples of geodesics between unimodal and bimodal
functions when represented as elements in S∞. In each case, the left panel
shows the functions f1 and f2, and the right panel shows equally spaced func-
tions between them on a constant-speed geodesic connecting them, i.e., plots
of α(τ)(t), τ = 0, 1/5, 2/5, . . . , 1, versus t. It is interesting to note that in the
ﬁrst case where the modes of f1 and f2 are distant, the geodesic path has the
eﬀect of ﬂattening a bump and creating a new bump. In the second case, where
the peaks are aligned, the two bumps simply change their shapes along the
geodesic.
2. Exponential Map: The geodesic on a unit sphere S∞can also be expressed
in terms of a starting point f ∈L2 and a shooting vector v in Tf(S∞):
α(τ)(t) = cos(τ∥v∥)f(t) + sin(τ∥v∥)v(t)
∥v∥,
t ∈[0, 1] .
(4.10)
0
0.2
0.4
0.6
0.8
1
0
0.5
1
1.5
2
2.5
3
3.5
0
0.2
0.4
0.6
0.8
1
2
4
6
8
10
12
14
16
0
0.2
0.4
0.6
0.8
1
0
0.5
1
1.5
2
2.5
3
0
0.2
0.4
0.6
0.8
1
2
4
6
8
10
12
14
16
Case 1
Case 2
Fig. 4.8 Examples of geodesic paths between functions in S∞. The functions along the paths
have been stacked to improve display

4.3 Geometries of Some Function Spaces
83
Therefore, the exponential map expf : Tf(S∞) →S∞, once again using Deﬁni-
tion 3.6, is given by:
expf(v)(t) = cos(∥v∥)f(t) + sin(∥v∥)v(t)
∥v∥,
t ∈[0, 1] .
(4.11)
Similarly to the case of S2, this exponential map is many to one. If we restrict
the domain of expf by requiring ∥v∥< π, then the exponential map becomes
one-to-one and its range is missing only the “antipodal point,” −f.
3. Inverse Exponential Map: Likewise, the inverse of this exponential map has
a simple analytical form. For f1, f2 ∈S∞, exp−1
f1 : S∞/{−f1} →Tf1(S∞) is:
exp−1
f1 (f2)(t) =
θ
sin(θ)(f2(t)−f1(t) cos(θ)),
θ = cos−1(⟨f1, f2⟩) and t ∈[0, 1] .
(4.12)
This expression simply provides the shooting vector for a uniform-speed
geodesic from f1 to f2 on S∞. One can easily verify this formula using
( d
dτ α(τ))|τ=0 for the path α given in Eq. 4.9.
These formulas are useful for computing summary statistics of functional data, as
described later in Chap. 7.
4.3.3 Group of Warping Functions
Yet another set of functions that is of great interest in this textbook is the set
of warpings. These maps deform a function by warping its domain in a certain
constrained way. This set plays an important role in parameterizations of curves
and alignments of functions. We introduce the set of warping functions here, but
its geometry is presented a little later in this chapter.
Consider a function γ : [0, 1] →[0, 1] that satisﬁes the following properties:
γ(0) = 0, γ(1) = 1, γ is invertible, and both γ and γ−1 are smooth. Such a γ is
called a boundary-preserving diﬀeomorphism of [0, 1]; let ΓI denote the set of all
such functions (the subscript I implies that underlying domain is an interval, as
opposed to being a circle that occurs later in this text). For details on the geometry
of this set, we encourage the reader to consult [45]. The set ΓI is interesting for
a variety of reasons. Firstly, it is a Lie group with the group operation given by
composition: for any γ1, γ2 ∈ΓI, the group operation is (γ1 ◦γ2)(t) = γ1(γ2(t)).
The identity element of ΓI is the identity map γid(t) = t, and for any γ ∈ΓI, its
inverse γ−1 is well deﬁned such that γ ◦γ−1 = γ−1 ◦γ = γid. Secondly, it acts on
a function space by right composition. Let F denote the space of all functions on
[0, 1], i.e., F = {f : [0, 1] →R}. Then, a right action of ΓI on F is given by:
F × ΓI →F,
(f, γ) = f ◦γ .
(4.13)
The proof that this mapping forms a group action is left as an exercise. (This
composition is also a group action if F is replaced by certain subspaces, such
as the set of absolutely continuous functions, or the square-integrable functions.)
Shown in Fig. 4.9 are two examples of this action on a function f. As can be seen

84
4 Functional Data and Elastic Registration
0
0.2
0.4
0.6
0.8
1
0
1
2
3
4
5
6
7
8
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
1
2
3
4
5
6
7
8
0
0.2
0.4
0.6
0.8
1
0
1
2
3
4
5
6
7
8
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
1
2
3
4
5
6
7
8
f (t)
g (t) = tx
f (g(t))
Fig. 4.9 Examples of warping a function. The top row is for γ(t) = t5 and the bottom row for
γ(t) = t0.6
in these examples, the eﬀect of warping a function f is to horizontally displace
its values but no new y values are introduced in the process. Another way to say
this is that the phase of f is changed but its amplitude remains the same. The
concepts of phase and amplitude are deﬁned formally later in Chap. 8.
While ΓI is a group, it is not a vector space; a linear combination of elements of
ΓI may not be in ΓI. (However, ΓI is closed under the operation of taking convex
combinations, i.e., linear combinations where the coeﬃcients add up to 1 and are
all non-negative.) It is interesting to note that one can impose a nice manifold
structure on ΓI. However, the model space (the space for deﬁning local charts) is
not a Banach space, but rather a more general Fr´echet space. The tangent structure
of ΓI at γid can be derived from the deﬁnition of tangent spaces (see Sect. A.1.1).
Let α(τ, ·) be a diﬀerentiable path in ΓI passing through γid at 0, i.e., α(0, t) =
γid(t). Since α(τ, ·) ∈ΓI, we have t →α(τ, t) which is a diﬀeomorphism. The
derivative of α(τ, t) with respect to τ, evaluated at τ = 0, given by v(t) ≡dα
dτ (0, t),
is a scalar quantity. At the boundaries dα
dτ (τ, 0) = dα
dτ (τ, 1) = 0 since α(τ, 0) = 0
and α(τ, 1) = 1. Also, because α has smooth derivatives, the function v is a smooth
function on [0, 1]. Thus, the tangent space Tγid(ΓI) is the set of smooth functions
that are zero at the boundaries:
Tγid(ΓI) = {v : [0, 1] →R|v(0) = 0, v(1) = 0,
v is smooth} .
This tangent space is a vector space but unfortunately not a Hilbert space because
it is not complete. There are really two reasons why it is not complete: it is quite
easy to produce a sequence {fi}, with each fi ∈Tγid(ΓI) such that fi →g with
respect to the L2 norm, but g fails to be smooth, or g fails to satisfy the boundary
conditions g(0) = g(1) = 0, or it fails both of these!
We are interested in both the geometry of ΓI and its role in warping functions
on [0, 1]. Speciﬁcally, we would like to impose a Riemannian structure on ΓI and
compute elements such as geodesics, exponential map, and the inverse exponential
map under that metric. We postpone this discussion until Sect. 4.10.2.

4.4 Function Registration Problem
85
4.4 Function Registration Problem
An important problem in statistical analysis of function data (and by extension to
shape analysis of curves) is the alignment of function data using domain warping.
The broad goal of an alignment process is to warp the time (or parameter) axis in
such a way that their peaks and valleys are better aligned. This alignment problem
has also been referred to as the separation of phase and amplitude of functions
in the given data, or the registration of data, or the correspondence problem.
We describe a comprehensive solution to this problem that separates phases and
amplitudes of the given functions and develops their statistical models separately.
One reason for taking this separation approach is that it better preserves the
modal structure of the given functions and leads to eﬃcient statistical models for
capturing the main source of data variability. In this section, we will use F ⊂L2
to denote a certain function space of interest. The actual deﬁnition of F comes
later when we present more details. The group of warping functions ΓI continues
to be the same as in the last section. There are two kinds of registration problems:
1. Pairwise Alignment Problem: Given any two functions f1 and f2 in F, we
deﬁne their pairwise alignment or registration to be the problem of ﬁnding a
warping function γ such that a certain energy term E[f1, f2 ◦γ] is minimized.
That is, we solve for:
γ∗= argmin
γ∈ΓI
E[f1, f2 ◦γ] .
Then, for any t ∈[0, 1], the value f1(t) is said to be registered to f2(γ∗(t)). An
example of this alignment is shown in Fig. 4.10.
2. Groupwise or Multiple Alignment: Given a set of functions {fi ∈F|i =
1, 2, . . ., n}, we call the problem of ﬁnding a set of warping functions {γi|i =
1, 2, . . ., n} such that, for any t ∈[0, 1], the values fi(γi(t)) are said to be reg-
istered with each other, the problem of joint or multiple alignment. Figure 4.11
shows an example of this idea where the given functions {fi} (left panel) are
aligned (middle panel) using the warping functions shown in the right panel.
In this chapter, we address the ﬁrst problem, namely, the pairwise registration, and
postpone the solution of the multiple registration problem until later in Chap. 8.
0
0.2
0.4
0.6
0.8
1
0
2
4
6
8
10
0
0.2
0.4
0.6
0.8
1
0
2
4
6
8
10
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
f1, f2
f1, f2 ◦ g∗
g∗
Fig. 4.10 Illustration of pairwise alignment

86
4 Functional Data and Elastic Registration
-3
-2
-1
0
1
2
3
-3
-2
-1
0
1
2
3
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
1.1
1.2
1.3
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
1.1
1.2
1.3
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
{fi}
{fi ◦ gi}
{gi}
Fig. 4.11 Illustration of multiple function alignment
0
0.2
0.4
0.6
0.8
1
0
0.5
1
1.5
2
2.5
3
3.5
4
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.5
1
1.5
2
2.5
3
3.5
4
f1, f2
f1 ◦ g , f2 ◦ g
g
Fig. 4.12 Identical warping of f1, f2 by γ preserves their registration
The main question in solving the pairwise registration problem is: What should
be the optimization criterion E? In other words, what is a mathematical deﬁnition
of a good registration? Visually one can evaluate an alignment by comparing the
locations of peaks and valleys, but how should one do it in a formal, quantiﬁable,
and, most importantly, automated way? Before we establish a criterion for evalu-
ating alignments, we should ﬁrst focus on the basic properties that such a criterion
should satisfy. Then, we can propose a formal criterion and demonstrate its eﬀec-
tiveness using theory and experiments. Below, we list three important properties
that E should satisfy. Actually, only the ﬁrst one is a fundamental property; the
remaining two are simple consequences of the ﬁrst one (and some additional struc-
ture). Still, we list all three of them to highlight diﬀerent aspects of the registration
problem.
1. Invariance to Simultaneous Warping: We start by noting that an identical
warping of any two functions preserves their registration. That is, for any γ ∈ΓI
and f1, f2 ∈F, the function pair (f1, f2) has the same registration as the
pair (f1 ◦γ, f2 ◦γ). What we mean by that is the application of γ has not
disturbed their point-to-point correspondence. This is easy to see since γ is a
diﬀeomorphism. The two height values across the functions that were matched,
say f1(t0) and f2(t0), for a parameter value t0, remain matched. They are now
labeled f1(γ(t0)) and f2(γ(t0)), and the parameter value has changed to γ(t0),
but they still have the same parameter and, thus, are still matched to each
other. This is illustrated using an example in Fig. 4.12 where the left panel

4.4 Function Registration Problem
87
shows a pair f1 and f2. Note that the two functions are nicely registered since
their peaks and valleys are perfectly aligned. If we apply the warping function
shown in the middle panel to both their domains, we get the functions shown
in the right. The peaks and valleys, and in fact all the points, have the same
matching as before.
This motivates the following invariance property of E. Since E is expected to
be a measure of registration of two functions, it should remain unchanged if the
two functions are warped identically. That is:
Invariance Property :
E[f1, f2] = E[f1 ◦γ, f2 ◦γ], for all γ ∈ΓI . (4.14)
2. Eﬀect of Random Warpings: Suppose we have found the optimal warping
function γ∗∈ΓI, deﬁned by:
γ∗= argmin
γ∈ΓI
E[f1, f2 ◦γ] .
Once this γ∗is found, the resulting matched height values are f1(t) and
f2(γ∗(t)), for all t. Now, let us suppose that we replace f1 and f2 by ran-
dom warpings of these functions, say f1 ◦γ1 and f2 ◦γ2. What is the optimal
correspondence between these functions? It will be given by:
˜γ∗= argmin
γ∈ΓI
E[f1 ◦γ1, (f2 ◦γ2) ◦γ] = argmin
γ∈ΓI
E[f1, f2 ◦(γ2 ◦γ ◦γ−1
1 )],
where the last equality follows from the above invariance property. The last
two equations immediately imply that γ∗= γ2 ◦˜γ∗◦γ−1
1 , in other words ˜γ∗=
γ−1
2
◦γ∗◦γ1. More interestingly, the optimal registration of functions and the
minimum value of E remains unchanged despite the presence of random γ1 and
γ2, i.e.:
min
γ∈ΓI E[f1, f2 ◦γ] = min
γ∈ΓI E[f1 ◦γ1, (f2 ◦γ2) ◦γ] .
This important equality intimately depends on the invariance property and the
group structure of ΓI. Without the invariance property, one can expect the
results of registration to be highly dependent on γ1 and γ2. For instance, this
undesirable situation will occur if we use the L2 metric between the functions
to deﬁne E.
3. Inverse Symmetry: We know that registration is a symmetric property. That
is, if f1 is registered to f2, then f2 is also registered to f1. Similarly, if f1
is optimally registered to f2 ◦γ, then f2 is optimally registered to f1 ◦γ−1.
Therefore, the choice of E should be such that this symmetry is preserved.
That is:
γ∗= argmin
γ∈ΓI
E[f1, f2 ◦γ] ⇒γ∗−1 = argmin
γ∈ΓI
E[f1 ◦γ, f2] .
This symmetry property has also been termed as inverse consistency. If the
invariance property holds, then this inverse symmetry follows immediately.

88
4 Functional Data and Elastic Registration
In addition to registering any two functions, it is often important to compare them
and to quantify their diﬀerences. For this, we need a proper distance function to be
able to compare f1 and f2◦γ∗, where γ∗is the optimal warping of f2 that registers it
with f1. One can always choose an independent distance function on the space, e.g.,
the L2 norm, that measures diﬀerences in the registered functions f1 and f2 ◦γ∗.
However, this makes the process of registration isolated from the mechanism for
comparison. Ideally, we would like to jointly solve these two problems. Therefore,
it will be useful if the quantity infγ∈ΓI E[f1, f2 ◦γ] is also a proper distance in
some sense. That is, in addition to symmetry, it also satisﬁes non-negativity and
the triangle inequality. The sense in which we want it to be a distance is that the
result does not change if we randomly warp the individual functions in diﬀerent
ways.
We will focus on deriving a cost function E that satisﬁes all of these nice
properties and we will do so by extending a famous Riemannian metric called the
(nonparametric) Fisher-Rao metric. However, we start by taking a very popular
approach based on the L2 metric and highlight its limitations.
4.5 Use of L2-Norm and Its Limitations
Although the L2 norm, or its variants, have been used widely in registration of
functional data, it has many shortcomings. We check the desired properties listed
in the last section for cost functions based on this metric.
1. Lack of Isometry Under Warping: Let us check the isometry property
for the L2 norm under identical warping. For the example shown in Fig. 4.12,
the L2 norms of diﬀerences before and after warping are 0.7566 and 0.6263,
respectively. For any general f1, f2 ∈L2 and γ ∈ΓI, we have:
∥f1 ◦γ −f2 ◦γ∥2 =
 1
0
(f1(γ(t)) −f2(γ(t)))2dt
=
 1
0
(f1(s) −f2(s))2
1
˙γ(γ−1(s))ds,
s = γ(t) .
Since, in general ˙γ(γ−1(s)) ̸= 1, we can see that generally:
∥f1 ◦γ −f2 ◦γ∦= ∥f1 −f2∥.
(4.15)
Despite identical warping and preservation of matches between the two func-
tions, the L2 norm of their diﬀerence changes. In other words, the action of
ΓI on F under the L2 metric is not by isometries. This rules out the use of
E[f1, f2] = ∥f1 −f2∥directly in function registration!
2. Pinching Eﬀect: A related problem in using L2 to perform matching is called
the pinching eﬀect. The basic idea here is that in matching of two functions, say
f1 and f2, using infγ∈ΓI ∥f1 −f2 ◦γ∥, one can squeeze or pinch a large part of
f2 and make this cost function arbitrarily close to zero. An illustration of this
problem is presented in Fig. 4.13 using a simple example. Here a part of f2 is
identical to f1 over [0, 0.6] and is completely diﬀerent over the remaining domain
[0.6, 1]. Since f1 is essentially zero and f2 is strictly positive in [0.6, 1], there

4.5 Use of L2-Norm and Its Limitations
89
is no warping that can match f1 with f2 over that subinterval. The optimal
solution is, therefore, to decimate that part of f2 by using the following γ∗: it
coincides with γid over [0, 0.6], climbs rapidly to 1 −ϵ around 0.6, and then
goes slowly from 1 −ϵ to 1 over the interval [0.6, 1]. The precise expression for
a small ϵ > 0 is given by:
γϵ(t) =
⎧
⎨
⎩
t,
for 0 ≤t ≤0.6
( 0.4−ϵ
ϵ
)(t −0.6) + 0.6,
for 0.6 ≤t ≤0.6 + ϵ
(
ϵ
0.4−ϵ)(t −0.6 −ϵ) + 1 −ϵ,
for 0.6 + ϵ ≤t ≤1
It is easy to check that in the limit:
lim
ϵ→0 ∥f1 −f2 ◦γϵ∥= 0 .
The top panel of Fig. 4.13 shows the limiting case where f1 and f2 ◦γ0 are
identical. In the bottom row, we provide results from a numerical procedure
for this optimization problem. Sometimes, in practice, we restrict γ to have
positive, bounded slope and do not obtain the same result as the theoretical
limit. However, one can still see the pinching eﬀect in the second panel of this
row where the second part of f2 is being squeezed into a small domain since it
is mismatched with f1. The last columns of this ﬁgure show the ˙γ∗in the two
cases—the picture in the bottom row shows that the slope of γ in the numerical
implementation was bounded by 12.
To avoid the pinching problem, a common approach is to impose an addi-
tional term in the optimization that constrains the roughness of γ. This term,
also called a regularization term, results in the registration problem of the type:
inf
γ∈ΓI (∥f1 −f2 ◦γ∥+ λR(γ)) ,
(4.16)
where R(γ) is the regularization term, e.g., R(γ) =

¨γ2(t)dt, and λ > 0 is a
constant. This solution has several problems including the fact that it does not
0
0.2
0.4
0.6
0.8
1
0
0.5
1
1.5
2
2.5
3
3.5
4
0
0.2
0.4
0.6
0.8
1
0
0.5
1
1.5
2
2.5
3
3.5
4
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
5
10
15
20
25
0
0.2
0.4
0.6
0.8
1
0
0.5
1
1.5
2
2.5
3
3.5
4
0
0.2
0.4
0.6
0.8
1
0
0.5
1
1.5
2
2.5
3
3.5
4
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
2
4
6
8
10
12
14
f1, f2
f1, f2 ◦ g∗
g∗, gid
˙g∗
Fig. 4.13 Illustration of the pinching eﬀect. The top row shows a degenerate analytical solution
to matching under L2 norm for functions. The bottom row shows a similar problem arising in a
numerical procedure for the same matching

90
4 Functional Data and Elastic Registration
satisfy inverse symmetry. The asymmetry comes from ﬁrst term that has already
been shown to be asymmetric earlier. A more comprehensive and satisfactory
solution to the registration problem is developed in the next few sections.
3. Inverse Inconsistency: For registration under the L2 distances between func-
tions, the results are inconsistent between inverse registrations. That is, the
registration of f1 to f2 may lead to a completely diﬀerent result than that of
f2 to f1. We illustrate this using an example.
Example 4.1. As a simple example, let f1(t) = t and f2(t) = 1+(t−0.5)2. In this
case, for the minimization problem γ12 = minγ∈ΓI ∥f2 −f1 ◦γ∥2, the optimal
solution is as follows. Deﬁne a warping function that climbs quickly (linearly)
from 0 to 1 −ϵ on the interval [0, ϵ] and then climbs slowly (also linearly) from
1 −ϵ to 1 in the remaining interval [ϵ, 1]. The limiting function, when ϵ →0,
results in the optimal γ for this case.
For the inverse problem, γ21 = minγ∈ΓI ∥f1 −f2 ◦γ∥2, the optimal warping
is the following. Deﬁne a warping function that rises quickly from 0 to 0.5 −ϵ
in the interval [0, ϵ], climbs slowly from 0.5−ϵ to 0.5+ϵ in the interval [ϵ, 1−ϵ],
and ﬁnally climbs quickly from 0.5+ϵ to 1 in the interval [1−ϵ, 1]. The optimal
solution is obtained when ϵ →0.
A computer implementation of these two solutions, based on the dynamic
programming algorithm, are shown in Fig. 4.14. Since this implementation al-
lows only a small number of possible slopes for optimal γ, the results are not
as accurate as the analytical solution. Still, it is easy to see a large diﬀerence
in the solutions for the two cases.
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
1.2
1.4
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
f1 (broken), f2 (solid), f1 (g∗12)
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
1.2
1.4
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
g∗12
f2 (broken), f1 (solid), f2 (g∗21)
g∗21
g∗21 ◦ g∗12, g∗12 ◦ g∗21
Fig. 4.14 Example of asymmetry in registration using L2 norm. The top row show solution of
registering f1 to f2, while the bottom row shows the reverse case. Bottom right emphasizes that
the two γ functions are not inverses of each other

4.6 Square-Root Slope Function Representation
91
4.6 Square-Root Slope Function Representation
In order to deﬁne a cost function E that avoids the pinching eﬀect, satisﬁes the
invariance property and, as a consequence, allows inverse symmetry and invariance
of E to random warping, we introduce a new mathematical representation for
functions. Starting fresh, this time we are going to restrict to those f that are
absolutely continuous on [0, 1]; let F denote the set of all such functions. For the
reader’s convenience, we will give a deﬁnition of absolutely continuous. Though
it is not the one most commonly given in real analysis textbooks, it is logically
equivalent to that one and is more useful for our purposes.
Deﬁnition 4.1 (Absolutely Continuous). A function f : [0, 1] →R is abso-
lutely continuous if it satisﬁes the following two conditions:
1. f is diﬀerentiable almost everywhere on [0, 1]; denote its derivative by ˙f.
2. f(t) = f(0) +
 t
0 ˙f(u)du for all t ∈[0, 1].
The absolutely continuity is stronger than continuity, but weaker than C1 (and
much weaker than being smooth!). It is easy to show that F is a vector space.
This is left as an exercise for the reader.
The new representation of functions is based on the following transformation.
Deﬁne a mapping: Q : R →R according to:
Q(x) ≡sign(x)

|x| .
(4.17)
Note that Q is a continuous map. For the purpose of studying the function f, we
will represent it using the SRSF deﬁned as follows.
Deﬁnition 4.2 (SRSF Representation of Functions). Assume that f : I →R
is absolutely continuous. Deﬁne the square-root slope function of f to be the
function q : [0, 1] →R, where:
q(t) ≡Q( ˙f(t)) = sign( ˙f(t))

| ˙f(t)| .
This representation includes those functions whose parameterization can become
singular in the analysis. In other words, if ˙f(t) = 0 at some point, it does not
cause any problem in the deﬁnition of q(t). It can be shown that, if the function f
is absolutely continuous, then the resulting SRSF is square integrable. This is left
as an exercise to the reader. Thus, we will deﬁne L2([0, 1], R) (or simply L2) to be
the set of all SRSFs. For every q ∈L2, there exists a function f (unique up to a
constant) such that the given q is the SRSF of that f. In fact, this function can
be obtained precisely using the equation: f(t) = f(0) +
 t
0 q(s)|q(s)|ds. Thus, the
representation f ⇔(f(0), q) is invertible. Some examples of SRSFs are shown in
Fig. 4.15. Note that the locations of peaks and valleys of f correspond to the zero
crossings of q.
The next question is: If a function is warped, then how does its SRSF change?
For an f ∈F and γ ∈ΓI, let q be the SRSF of f. Then, what is the SRSF of
f ◦γ? This can simply be derived as:
˜q(t) = Q
 d
dt (f ◦γ) (t)

= sign
 d
dt (f ◦γ) (t)
 ****
d
dt(f ◦γ)(t)
**** = q(γ(t))

˙γ(t) .

92
4 Functional Data and Elastic Registration
0
0.2
0.4
0.6
0.8
1
−10
−5
0
5
10
−3
−2
−1
0
1
2
3
−1
−0.5
0
0.5
1
1.5
0
5
10
15
20
−5
0
5
10
15
20
0
0.2
0.4
0.6
0.8
1
−10
−5
0
5
10
−3
−2
−1
0
1
2
3
−1
−0.5
0
0.5
1 1
1.5
0
5
10
15
20
−5
0
5
10
15
20
25
30
Fig. 4.15 Examples of functions f(t) in solid lines and their SRSFs q(t) in broken lines
0
0.2
0.4
0.6
0.8
1
−1
−0.5
0
0.5
1
0
0.2
0.4
0.6
0.8
1
−4
−2
0
2
4
6
f, f ◦ g
q, (q ◦ g )√˙g
Fig. 4.16 Left panel: two functions f(t) (solid line) and its warped version f(γ(t)) (broken
line). Right: their SRSFs q(t) (solid line) and q(γ(t))

˙γ(t) (broken line)
In fact, this denotes an action of group ΓI on L2 from the right side: L2×ΓI →L2,
given by (q, γ) = (q ◦γ)√˙γ. The fact that this mapping is a group action is left
as an exercise. One can show that this action of ΓI on L2 is compatible with its
action on F given earlier, in the following sense:
f
SRSF
−→
q
action on F
↓
↓
action on L2
f ◦γ
SRSF
−→(q ◦γ)√˙γ
We can apply the group action and compute SRSF in any order, and the result re-
mains same. Two examples of the changes in SRSFs, when the underlying function
is warped, are shown in Fig. 4.16. It can be seen that, while the vertical coordinates
of the original functions remain unchanged, the vertical coordinates of SRSFs are
changed by this group action.

4.7 Deﬁnition of Phase and Amplitude Components
93
The most important advantage of using SRSFs in functional data analysis comes
from the following result. Recall that the L2 inner product is given by ⟨v1, v2⟩=
 1
0 v1(t)v2(t) dt.
Lemma 4.1. The mapping L2 × ΓI →L2 given by (q, γ) = (q ◦γ)√˙γ forms an
action of ΓI on L2 by isometries.
Proof. That the mapping is a group action has been mentioned earlier. The proof of
isometry is an easy application of integration by substitution. For any v1, v2 ∈L2:
⟨(v1, γ), (v2, γ)⟩=
 1
0
v1(γ(t))

˙γ(t)v2(γ(t))

˙γ(t)dt
=
 1
0
v1(γ(t))v2(γ(t))˙γ(t)dt =
 1
0
v1(s)v2(s)ds = ⟨v1, v2⟩.⊓⊔
We already know that the geodesics in L2, under the standard metric, are
straight lines and the geodesic distance between any two elements q1, q2 ∈L2 is
given by ∥q1 −q2∥(Sect. 4.3.1). Since the action of ΓI on L2 is by isometries, the
following result is automatic. Still, for the sake of completeness, we provide a short
proof.
Lemma 4.2. For any two SRSFs q1, q2 ∈L2 and γ ∈ΓI, we have that ∥(q1, γ) −
(q2, γ)∥= ∥q1 −q2∥.
Proof. For an arbitrary element γ ∈ΓI, and q1, q2 ∈L2, we have:
∥(q1, γ) −(q2, γ)∥2 =
 1
0
(q1(γ(t))

˙γ(t) −q2(γ(t))

˙γ(t))2dt
=
 1
0
(q1(γ(t)) −q2(γ(t)))2 ˙γ(t)dt = ∥q1 −q2∥2 .⊓⊔
An interesting corollary of this lemma is the following.
Corollary 4.1. For any q ∈L2 and γ ∈ΓI, we have ∥q∥= ∥(q, γ)∥.
This implies that the action of ΓI on L2 is actually a norm-preserving transfor-
mation. Conceptually, it can be equated with the rotation of vectors in Euclidean
spaces. Later on, we will introduce another kind of transformation that relates to
warping of functions and will organize them in diﬀerent groups.
4.7 Deﬁnition of Phase and Amplitude Components
As mentioned earlier, the problem of registration of functions is also called the
problem of phase-amplitude registration. In this section, we will introduce a formal
speciﬁcation of these terms, but before we do that, we allude to the usage of phase
and amplitude of waves in classical physics. In physics, a modulated sine wave is
given by:
f(t) = a(t) sin(2πωt + φ(t)) ,
where a(t) is called the instantaneous amplitude and (ω + ˙φ(t)) the instantaneous
frequency. The function φ(t) determines the instantaneous phase of the function.

94
4 Functional Data and Elastic Registration
One can characterize a wave using a(t), ω, and φ(t), as amplitude, frequency, and
phase functions, respectively. The process of encoding information for transmitting
signals by instantaneous modiﬁcations of these components is termed modulation.
The three types of modulations are amplitude, phase, and frequency modulation,
although the last two are closely related. Motivated by this basic terminology, we
wish to develop formal notions of phase and amplitude for arbitrary function data.
4.7.1 Amplitude of a Function
Recall that F is the set of real-valued, absolutely continuous functions on [0, 1].
For any f ∈F and an arbitrary warping function γ, consider the composition
or warping f ◦γ. We stipulate that amplitude is a property of a function that
remains unchanged under warping; warping of a function only changes its phase.
(This is similar to the deﬁnition of shape as a property that is preserved under
certain transformations (rigid motions, etc.), as described in later chapters.) Thus,
f and f ◦γ have the same amplitude for all γ ∈ΓI. Figure 4.17 shows an example
where a number of functions are generated by warping the same function f0, i.e.,
fi = f0 ◦γi. One can verify that all these functions go through the same set of
heights (or amplitudes) and in the same order, just the rate of traversal is diﬀerent.
According to our deﬁnition, all these functions have the same amplitude.
It is natural to use the notion of orbits, under the action of a warping group, to
specify amplitudes of functions as equivalence classes. However, since the set ΓI
is not closed, we actually use a larger set +
ΓI in order to get a precise deﬁnition of
orbits.
Deﬁnition 4.3. Let +
ΓI denote the set of all functions γ : [0, 1] →[0, 1] that satisfy
the following three properties:
1. γ(0) = 0 and γ(1) = 1.
2. γ is absolutely continuous.
3. For all t1, t2 ∈[0, 1], if t1 < t2, then γ(t1) ≤γ(t2) (i.e., γ is “weakly increasing”).
It is easy to show that +
ΓI satisﬁes all the properties of a group except that not
every element has an inverse. Such an object is called a monoid. Note that ˜ΓI
includes elements γ with ˙γ(t) = 0 on some subset of [0, 1] with nonzero measure.
Figure 4.18 shows an example of such a γ and its action on a bimodal function f.
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
f0
{gi}
{fi = f0 ◦ gi}
Fig. 4.17 Diﬀerent functions generated by warping the same function are said to have the same
amplitude

4.7 Deﬁnition of Phase and Amplitude Components
95
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
1
2
3
4
5
6
Fig. 4.18 An example of an element of 
ΓI whose derivative equals zero over an interval, and its
warping action on a function
An interesting and helpful fact in this setup is this: If ˙f = 0, only on a set
of measure zero, then the closure of the set {f ◦γ|γ ∈ΓI} equals the orbit of f
under ˜ΓI. (The closure is with respect to the L2 metric on the SRSF space of the
function.) The assumption on ˙f is not very limiting since, according to Lemma 4.3,
one can re-parameterize any nontrivial function to satisfy this property. Another
useful fact is that the group ΓI is a dense subset of ˜ΓI, with respect to the Fisher-
Rao metric on ΓI (covered later in Sect. 4.10.2).
Now we can specify amplitudes as orbits under the action of ˜ΓI. Instead of using
the original function space F, we will do so on the SRSF space since it allows the
use of a simpler metric, i.e., the L2 metric. Let f1 : [0, 1] →R and f2 : [0, 1] →R
be two absolutely continuous functions, with SRSFs q1 and q2, respectively. Deﬁne
a relation ∼on L2 by deﬁning q1 ∼q2 if and only if q1 is an element of the L2
closure of [q2]. This relation is easily shown to be an equivalence relation. Let’s see
what it means for q1 ∼q2, in terms of the original functions f1 and f2. We can
use this monoid to deﬁne constant-speed parameterization of a function.
Lemma 4.3. If f : [0, 1] →R is absolutely continuous, then there exists a unique
absolutely continuous function h : [0, 1] →R with |˙h(t)| = constant almost every-
where in [0, 1], and a unique γ ∈+
ΓI such that f = h ◦γ. This function h is called
the constant-speed parameterization of f.
The proof of this Lemma is standard.
Theorem 4.1. Let f1 : [0, 1] →R and f2 : [0, 1] →R be two absolutely continuous
functions, with SRSFs q1 and q2, respectively. Then q1 ∼q2 if and only if f1 and
f2 have the same constant-speed parameterization h.
The proof of this theorem is involved and is skipped for brevity. We refer the
reader to the paper [62] for details. This theorem shows us that when we form the
quotient L2/ ∼, we are identifying two functions precisely if they have identical
constant-speed parameterizations, i.e., if they traverse the same points of R in
the same order. Note that either or both of these functions are allowed (but not
required) to “stop and rest” at various times as they traverse these points.
Now we are ready to deﬁne the amplitude of a function.
Deﬁnition 4.4 (Amplitude). Let f be an element of F such that ˙f ̸= 0 almost
everywhere. The amplitude of f, with associated SRSF q, is deﬁned as its orbit [q]
under the monoid ˜ΓI: [q] = {(q ◦γ)√˙γ|γ ∈˜ΓI}.
We will use A to denote the space of all amplitudes or orbits.

96
4 Functional Data and Elastic Registration
4.7.2 Relative Phase Between Functions
The next step is to specify the phase component of function. Before we do that,
we make a few remarks about what properties such a speciﬁcation should satisfy.
1. While the notion of amplitude is absolute, we can only deﬁne phase of one
function w.r.t. the other.
2. So far we have not used any metric or statistical model (to introduce the notion
of amplitude). In contrast, we will need either a metric or a statistical model
to deﬁne relative phase.
3. The phase of a function, relative to another, is changed by a warping of either
function but their amplitudes remain unchanged by warpings.
4. We expect the relative phase of f1 w.r.t f2 to be exactly the inverse of the
relative phase of f2 w.r.t f1. This is analogous to the fact that if the rotation
from one element of Rn to another is O ∈SO(n), then the rotation from the
second to the ﬁrst is O−1 = OT .
5. Also, similar to rotation, if we change the relative phase of a function twice—
using one warping after another—then the total change should be a composition
of the two warpings. That is, changing phase is an action of a monoid or a group!
6. Multiplying a function by a positive scalar, or adding a constant to a function,
should not change its relative phase with respect to any other function.
7. The relative phase is not well deﬁned if either of the functions involved is
constant.
The SRSF of a function f ∈F is given by q(t) = sign( ˙f(t))

| ˙f(t)| and the
SRSF of f ◦γ, for any γ ∈˜ΓI, is given by (q, γ)(t) = q(γ(t))

˙γ(t). Also, we remind
the reader that ∥(q, γ)∥= ∥q∥for all q ∈L2 and γ ∈˜ΓI (See Lemma 4.1).
Deﬁnition 4.5 (Relative Phase). The relative phase of a function f1 w.r.t a
function f2 is deﬁned by the pair:
(γ∗
1, γ∗
2) = argmin
γ1,γ2∈˜ΓI
(∥(q1, γ1) −(q2, γ2)∥) ∈˜ΓI × ˜ΓI,
(4.18)
where q1 and q2 are the SRSFs of f1 and f2, respectively.
One can show that this relative phase exists if f1 or f2 is a piecewise linear function.
However, it is not necessarily unique. The same holds if f1 and f2 are C1-functions.
For more general results on existence of optimal warpings, please refer to the paper
[20]. The two components of the relative phase are needed to bring q1 and q2 to
the closest points in their respective orbits. Note that the relative phase is deﬁned
only up to a γ ∈˜ΓI. That is, if (γ∗
1, γ∗
2) is a solution to Eq. 4.18, then so is
(γ∗
1 ◦γ, γ∗
2 ◦γ), for any γ ∈˜ΓI.
The optimization problem in Eq. 4.18 is precisely the pairwise alignment solu-
tion for f1 and f2 proposed in Chap. 4. Figure 4.19 (left) illustrates these ideas
using a sketch. The two orbits have been drawn parallel to each other because
∥q1 −q2∥= ∥(q1, γ)−(q2, γ)∥for all γ. One moves along an orbit using time warp-
ings or phase changes and moves orthogonal to the orbits by changing amplitudes.
For the relative phase as deﬁned above, the following statements hold.
1. The relative phase of f1 w.r.t f2 is exactly the reverse of the relative phase of
f2 w.r.t f1, i.e., the relative phase of f2 w.r.t f1 is given by (γ∗
2, γ∗
1).

4.7 Deﬁnition of Phase and Amplitude Components
97
q1
q2
[q1]
[q2]
q1
q2
(q2, γ∗)
(q1, γ1∗)
(q2, γ2∗)
da
dp
[q1]
[q2]
Fig. 4.19 Left: The deﬁnition of phase and amplitudes of function: amplitudes are deﬁned by the
orbits and phases are deﬁned by the pair (γ∗
1 , γ∗
2) that ﬁnds the nearest elements of the orbits.
Right: Approximation of phase using a single warping γ∗and the phase, amplitude distances
between functions
2. Two functions are said to have zero phase between them if their relative phase
is (γid, γid). In Fig. 4.19(left), this situation will occur if q1, q2 happen to be the
nearest points across orbits. For example, this is the case when f1 = f2.
3. The relative phase of f1 w.r.t f2 ◦γ, for any γ ∈ΓI, is (γ∗
1, γ−1 ◦γ∗
2). Similarly,
the relative phase of f1 ◦γ w.r.t f2 is (γ−1 ◦γ∗
1, γ∗
2). Combining these results,
the relative phase of f1 ◦γa w.r.t f2 ◦γb is (γ−1
a
◦γ∗
1, γ−1
b
◦γ∗
2).
4. As a corollary to the last property, the relative phase of f1 ◦γ w.r.t f2 ◦γ, for
any γ ∈ΓI, is (γ−1 ◦γ∗
1, γ−1 ◦γ∗
2).
5. The relative phase between any two functions is unchanged by vertical transla-
tions and scalar multiplications of those functions. That is, the relative phase
of (c1f1 + e1) relative to (c2f2 + e2) is also (γ∗
1, γ∗
2), where c1, c2 ∈R+ and
e1, e2 ∈R. This is a consequence of the fact that SRSF is independent of
vertical translation and Lemma 4.5.
The proofs of these properties have been left as exercises to the reader.
4.7.3 A Convenient Approximation
Although the deﬁnitions laid out so far have nice theoretical properties, the ensu-
ing computations are relatively complicated. For instance, the precise solution of
Eq. 4.18 is not easy to reach in general cases (in case the functions are piecewise
linear, an algorithm is presented in [62]). However, a simple approximation leads
to convenient computational solutions that allows us to use existing algorithms.
Using the fact that ΓI is dense in ˜ΓI, we get the equality:
inf
γ1,γ2∈˜ΓI
∥(q1, γ1) −(q2, γ2)∥=
inf
γ1,γ2∈ΓI ∥(q1, γ1) −(q2, γ2)∥= inf
γ∈ΓI ∥q1 −(q2, γ)∥.
(4.19)
Thus, one can approximate the amplitude distance by ﬁxing q1 and minimizing
only on ΓI orbit of q2. This minimization can be performed in several ways, one
of which is the DPA described in Appendix B. In the same spirit, one can use a
single function:
γ∗
12 = arginf
γ∈ΓI
∥q1 −(q2, γ)∥
(4.20)

98
4 Functional Data and Elastic Registration
as an approximation for the relative phase for f1 w.r.t f2. Note that this γ∗
12
replaces the pair (γ∗
1, γ∗
2) in studying the relative phase. From here onward, we
will use this single function to deﬁne the relative phase between functions, instead
of the pair. Also, note that the optimization used here is exactly the same as deﬁned
as the registration problem in Eq. 4.7. This approximation is depicted pictorially
in the right panel of Fig. 4.19.
4.8 SRSF-Based Registration
Using the tools introduced in this section, we can formulate a new registration
approach as follows.
4.8.1 Registration Problem
Deﬁnition 4.6 (Registration Energy). For any two functions f1 and f2, let q1
and q2 denote their SRSFs, respectively. We deﬁne the cost function for registering
f1 and f2 to be E[f1, f2] = ∥q1 −q2∥.
Let us check if this choice of E satisﬁes the three properties listed earlier.
1. Invariance to Simultaneous Warping: This important property is already
established in Lemma 4.2, which states that E[f1, f2] = E[f1 ◦γ, f2 ◦γ] for all
f1, f2 ∈F and all γ ∈ΓI.
2. Invariance to Random Warpings: For any f1, f2 ∈ΓI and γ, γ1, γ2 ∈ΓI,
we have
inf
γ∈ΓI E[f1 ◦γ1, f2 ◦γ2 ◦γ] = inf
γ∈ΓI ∥(q1, γ1) −((q2, γ2), γ)∥
= inf
γ∈ΓI ∥(q1, γ1) −(q2, γ2 ◦γ)∥
= inf
γ∈ΓI ∥q1 −((q2, γ2 ◦γ), γ−1
1 )∥
= inf
γ∈ΓI ∥q1 −(q2, γ2 ◦γ ◦γ−1
1 )∥= inf
γ∈ΓI ∥q1 −(q2, γ)∥
= inf
γ∈ΓI E[f1, f2]
The three equalities used above are based on the group structure of ΓI and on
Lemma 4.2. Thus, the cost function is invariant to simultaneous warpings of
input functions and the resulting registration problem is unaﬀected by random
warpings of the input data.
3. Inverse Symmetry: This framework ensures inverse symmetry in registration,
as shown in the following lemma.
Lemma 4.4. Let γ∗∈ΓI be a minimizer of ∥q1 −(q2, γ)∥. In general, this may
not exists. But if it does then we have that:
γ∗−1 ∈argmin
γ∈ΓI
∥(q1, γ) −q2∥.
(4.21)

4.8 SRSF-Based Registration
99
Proof. Since ∥q1−(q2, γ∗)∥≤∥q1−(q2, γ)∥, this implies that ∥(q1, γ∗−1)−q2∥≤
∥(q1, γ−1)−q2∥for all γ ∈ΓI. Equation 4.21 directly follows from this condition.
⊓⊔
In other words, this choice of E ensures that interchanging of the arguments
does not change the registration result, and the optimal registration of (f1, f2)
is the same as the registration of (f2, f1).
Additionally, it will be shown later in Sect. 4.10 that the quantity infγ∈ΓI ∥(q1, γ)−
q2∥is actually proper distance, not in L2 but in a quotient space of L2. Having
a distance is useful for the ensuing statistical analysis of registered functions.
Therefore, the processes of registration and analysis are performed in a uniﬁed
framework, rather than having two disjoint steps for each process. We summarize
this discussion by the following problem statement.
Deﬁnition 4.7 (Registration Problem). For any two functions f1 and f2, let
q1 and q2 denote their SRSFs, respectively. Finding the optimal registration for f1
and f2 is the same as solving the problem:
inf
γ∈ΓI E[f1, f2 ◦γ] = inf
γ∈ΓI ∥q1 −(q2, γ)∥.
(4.22)
Note that the minimizer may only be approximate, depending upon the nature of
f1 and f2, in the sense discussed in Sect. 4.7.3. While the precise optimal may not
even exist, we can approximate it using the dynamic programming algorithm in
most cases. This is described in the next section.
An interesting outcome of this framework is that global scaling of functions
does not change their relative phases.
Lemma 4.5. If γ∗∈arg infγ∈ΓI ∥q1 −(q2, γ)∥, for any q1, q2 ∈L2, then the same
γ∗is also in arg infγ∈ΓI ∥cq1 −(q2, γ)∥for any c ∈R+.
The proof is left to the reader.
4.8.2 SRSF Alignment Using Dynamic Programming
We present an implementation of the dynamic programming algorithm (DPA) for
solving the minimization stated in Eq. 4.22. While a more general description of
the algorithm can be found in Appendix B, we particularize it to match our case
here.
As described in the appendix, this algorithm imposes an m × m, 2D grid on
the square domain [0, 1]2 and restricts the solution to a piecewise linear path from
the origin ((0, 0)) to the target ((1, 1)). This path is required to change slopes
only at the nodes of the grid. For each node (i/m, j/m) ∈[0, 1]2, one predeﬁnes
a set of neighboring nodes, lying below and to the left of (i/m, j/m) on the grid,
and denoted by the set N(i,j), that are allowed to reach (i/m, j/m) directly using
a straight line. Then, the optimal path is found using a recursive optimization,
where the optimal cost Hi,j of reaching the node (i/m, j/m), starting from (0, 0),
is computed as follows. The cost of reach elements of Ni,j is already determined
and stored. For each element of Ni,j, one computes the additional cost of joining
that element to (i, j) using the line segment and forms a cost of reaching (i, j).
The minimum over all elements of Ni,j results in Hi,j:

100
4 Functional Data and Elastic Registration
Hi,j =
min
(k/m,l/m)∈Ni,j

Hk,l +
 i/m
k/m
(q1(t) −q2(γ(t))

˙γ(t))2 dt

,
(4.23)
where γ is a straight line joining the nodes (k/m, l/m) and (i/m, j/m) on the
graph.
Algorithm 3 (SRSF Alignment Using DPA).
1. Set Ni,j, the neighbors of a lattice point (i/m, j/m) allowed to reach (i/m, j/m)
directly using a straight line.
2. Set the value of H for the ﬁrst row and the ﬁrst column of the grid, except (0, 0)
to ∞, i.e., H0,: = H:,0 = ∞. Set H0,0 = 0.
3. Computation of H:
For each (0, 0) < (i/m, j/m) ≤(1, 1) in the grid, compute Hi,j using Eq. 4.23.
This requires approximating the integral in Eq. 4.23 using numerical techniques.
Let ((ˆk/m)i,j, (ˆl/m)i,j) denote the optimal elements of Ni,j that minimize the
right side in that equation. Note these are function of i and j.
4. Computation of optimal γ∗:
a. Initialize (i/m, j/m) = (1, 1).
b. Draw a straight line from (i/m, j/m) to ((ˆk/m)i,j, (ˆl/m)i,j).
c. Set (i/m, j/m) = ((ˆk/m)i,j, (ˆl/m)i,j). If (i/m, j/m) = (0, 0), then stop. Else,
go to the previous step.
γ∗is the resulting piecewise linear curve from (0, 0) to (1, 1).
4.8.3 Examples of Functional Alignments
In this section, we demonstrate the eﬀectiveness of this alignment method through
some examples taken from real applications.
Figure 4.20 shows three examples of aligning pairs of functions. In each row,
depicting a diﬀerent case, we see the original functions f1 and f2 in the left panel
and the warping ˜f2 = f2 ◦γ∗in the middle panel, while the optimal warping
function γ∗is shown in the rightmost panel. In the top two cases, f1 and f2 are
unimodal and γ∗results in aligning the two peaks. In the last case, where one peak
is already aligned and the second one diﬀers, the warping function γ∗only moves
the second peak of f2 to align it with the second peak of f1. Note that the warping
function takes a rather predictable form in these cases. It is a piecewise linear
function with three diﬀerent slopes: one for the initial region with zero values, one
for the nonzero region, and one for the last region of zero values.
The next example, shown in Fig. 4.21, is taken from the famous Berkeley growth
data where heights of human subjects were recorded from birth to age 19. This
example uses a smoothed version of the time-derivative of the height functions,
instead of the height functions themselves, as functional data. The peaks in these
functions denote growth spurts and an important goal in this problem is to align
the growth spurts of diﬀerent subjects in order to make inferences about the num-
ber and placement of such spurts for the underlying population. Figure 4.21 shows
two sets of examples in the two rows. The left panels in each row show the original
functions and the middle panels show the warped versions of f2 after their align-
ments with f1. The ﬁnal plot in each row shows the optimal time warping function

4.8 SRSF-Based Registration
101
0
0.2
0.4
0.6
0.8
1
0
2
4
6
8
10
0
0.2
0.4
0.6
0.8
1
0
2
4
6
8
10
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
2
4
6
8
10
0
0.2
0.4
0.6
0.8
1
0
2
4
6
8
10
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
2
4
6
8
10
0
0.2
0.4
0.6
0.8
1
0
2
4
6
8
10
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
f1 (solid), f2  (dotted)
f1 (solid), f2 ◦ g∗ (dotted)
g∗
Fig. 4.20 Optimal warping of f2 to align with f1 using γ∗
0
5
10
15
20
−5
0
5
10
15
20
0
5
10
15
20
−5
0
5
10
15
20
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
5
10
15
20
−5
0
5
10
15
20
25
0
5
10
15
20
−5
0
5
10
15
20
25
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
f1 (solid), f2  (dotted)
f1 (solid), f2 ◦ g∗ (dotted)
g∗
Fig. 4.21 An example of matching growth spurts for two subjects from the Berkeley growth
dataset

102
4 Functional Data and Elastic Registration
γ∗used in that alignment. Note the nice alignment of peaks and valleys in both
the cases after warpings. It becomes much easier for an analyst to estimate the
location and size of the growth spurts in observed subjects and make inferences
about the general population, through this alignment procedure.
Another important application of functional alignment is in mass spectrometry
where one characterizes materials using their metabolite composition observed
as chromatograms. The collection of data from diﬀerent machines, or even same
machine at diﬀerent calibrations, leads to the problem of misalignment of these
chromatograms. Shown in the top panel of Fig. 4.22 are two chromatograms, of
the same material, that need alignment. They exhibit the same peak pattern but
the locations of these peaks have been shifted in one chromatogram relative to the
other. A simple shift is not suﬃcient to align them since some peaks shift to the
left and some to the right. Shown in the middle panel is the result of alignment
using the optimization listed in Eq. 4.22. It can be seen that the peaks are aligned
well; this is further illustrated using a zoom-in of the interval [80, 100] in the
bottom row, which shows a remarkable improvement in the peak alignment after
optimization over γ.
0
50
100
150
200
250
6.5
7
7.5
8
8.5
9
0
50
100
150
200
250
6.5
7
7.5
8
8.5
9
80
85
90
95
100
6.8
7
7.2
7.4
7.6
7.8
8
8.2
8.4
80
85
90
95
100
6.8
7
7.2
7.4
7.6
7.8
Before
After
Fig. 4.22 Alignment of liquid chromatography—mass spectrometry data. The top row shows
the original chromatograms while the bottom show the aligned ones after warping

4.8 SRSF-Based Registration
103
No noise
−3
−2
−1
0
1
2
3
0.2
0.4
0.6
0.8
1
1.2
−3
−2
−1
0
1
2
3
0.2
0.4
0.6
0.8
1
1.2
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
Small noise
−3
−2
−1
0
1
2
3
0.2
0.4
0.6
0.8
1
1.2
−3
−2
−1
0
1
2
3
0.2
0.4
0.6
0.8
1
1.2
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
Large noise
−3
−2
−1
0
1
2
3
0
0.2
0.4
0.6
0.8
1
1.2
1.4
−3
−2
−1
0
1
2
3
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
Original functions
Optimal Warping functions
Aligned Functions
Fig. 4.23 Alignment results after adding white Gaussian noise to functions. The top row has
no noise and the resulting alignment is almost perfect. The middle row has limited noise and the
alignment is largely preserved. However, the bottom level is dominated by noise and the solution
ends up aligning peaks and valleys, causes primarily by noise
An important issue in any data analysis is the immunity or at least some robust-
ness to noise. Since the registration framework prescribed here is based on SRSFs,
which, in turn, involve ﬁrst derivatives, there is a strong possibility of noise inﬂu-
encing the alignment results. Figure 4.23 shows results from an experiment that
increasingly adds white Gaussian noise to functions. The top shows the original
two functions in the left panel, their alignment in the middle, and the optimal
warping function γ∗, obtained using Eq. 4.22 in the right. Now we add a small
amount of noise in the second row and observe the results. Since the level of noise
is small, the alignment still looks decent although the optimal warping function
shows some corruption due to noise. However, if we increase the noise level, as
shown in the third row, the alignment is now governed more by noise than the
original signal. The method ends up aligning peaks resulting from noise. So, how
can we handle this situation?
Our goal in alignment (or registration) is to estimate the diﬀeomorphism γ∗
that aligns the underlying signal and largely ignores the noise. Assuming that
noise is high frequency and signal is low frequency, as is the case in many denoising
algorithms, we can ﬁrst smooth the functions to minimize noise. Then, one can use
these denoised functions to perform alignment and apply the resulting alignment
to the original functions. We illustrate this idea with the same example as in the
bottom row of Fig. 4.23. We use a wavelet-based denoising and the results are
shown in the left two panels of Fig. 4.24. The resulting smoothed functions are
used in Eq. 4.22 to obtain the warping function shown in the third panel. This
warping function, when applied to the original functions, results in the alignment

104
4 Functional Data and Elastic Registration
−3
−2
−1
0
1
2
3
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
−3
−2
−1
0
1
2
3
0
0.2
0.4
0.6
0.8
1
1.2
1.4
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
0
0.2
0.4
0.6
0.8
1
1.2
1.4
f1, Denoised f1
f2, Denoised f2
Optimal Warping
Optimal Registration
Fig. 4.24 Handling noise in given functions: denoise the given functions using a smoothing
technique, obtain registration with denoised functions, and apply that registration
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
0
0.2
0.4
0.6
0.8
1
1.2
1.4
−3
−2
−1
0
1
2
3
0
0.2
0.4
0.6
0.8
1
1.2
1.4
−3
−2
−1
0
1
2
3
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
−3
−2
−1
0
1
2
3
0
0.2
0.4
0.6
0.8
1
1.2
1.4
Fig. 4.25 Denoising-based alignment under diﬀerent levels of denoising: from left to right, the
top panels show results of γ∗obtained after increasing levels of smoothing applied to the noisy
functions. The bottom row shows the corresponding alignments. The results are quite stable with
respect to the amount of smoothing
shown in the last panel. If we compare it with the last row of Fig. 4.23, we can
see that this result is much better in terms of alignment of the underlying signal
(macro-structures).
The next issue is, of course, the amount of inﬂuence of the denoising step in the
eventual registration result between the noisy functions. While the amount and
type of denoising certainly aﬀects the functions themselves, the eventual alignment
remains relatively robust to these choices. Figure 4.25 illustrates this with an ex-
ample. Taking the same two functions as earlier, we denoise them using wavelets
as earlier, but with increasing levels of smoothness, and perform registration. The
resulting optimal γ∗s are shown in the top row, with increasing smoothness from
left to right. The corresponding registrations of the original functions are shown in
the bottom panel. It can be seen that broad alignment of the noisy signals remains
relatively unchanged despite a large change in the denoising level.
4.9 Connection to the Fisher-Rao Metric
The framework we have introduced in the previous section is intimately related to
a well-known tool in Riemannian analysis of probability density functions. This
is based on a Riemannian metric, namely, the Fisher-Rao metric that was

4.9 Connection to the Fisher-Rao Metric
105
introduced in 1945 by C. R. Rao [92] where he used the Fisher information matrix
to compare diﬀerent probability distributions. This metric was studied rigorously
in the 1970s and 1980s by Amari [4], Efron [30], Kass [47], Cencov [24], and oth-
ers. While those earlier eﬀorts were focused on analyzing parametric families, our
discussion is based on the nonparametric version of the Fisher-Rao Riemannian
metric. There are several forms of the nonparametric version and we will start with
the nonparametric version that is closest to our context. To deﬁne this metric, we
will have to restrict to the set F0 = {f ∈F| ˙f > 0}.
Deﬁnition 4.8 (Fisher-Rao Metric). For any f ∈F0 and v1, v2 ∈Tf(F), the
Fisher-Rao Riemannian metric is deﬁned as the inner product:
⟨⟨v1, v2⟩⟩f = 1
4
 1
0
˙v1(t) ˙v2(t) 1
˙f(t)
dt .
(4.24)
One of the most celebrated properties of this metric is that the action of the
warping group on F0 is by isometries under this metric. To see this, recall the
right action of ΓI on the space of functions (Eq. 4.13), this time restricted to F0,
given by F0 × ΓI →F0,
(f, γ) = f ◦γ. For a γ ∈ΓI, deﬁne the mapping
Lγ : F0 →F0 by Lγ(f) = (f, γ) = f ◦γ. Since Lγ is a linear map, its diﬀerential
L∗
γ : Tf(F0) →Tf◦γ(F0) is the same as Lγ. That is, for any v ∈Tf(F), L∗
γ(v) =
v ◦γ.
Lemma 4.6. The action of ΓI on F0, endowed with the Fisher-Rao metric, is by
isometries.
Proof. We need to show that ⟨⟨v1, v2⟩⟩f =

L∗
γ(v1), L∗
γ(v2)

f◦γ. Starting with
the R.H.S.:

⟨L∗
γ(v1), L∗
γ(v2)

⟩f◦γ = ⟨⟨Lγ(v1), Lγ(v2)⟩⟩f◦γ
= 1
4
 1
0
( d
dtv1(γ(t)))( d
dtv2(γ(t)))
1
˙f(γ(t))˙γ(t)
dt
= 1
4
 1
0
˙v1(γ(t)) ˙v2(γ(t))˙γ(t)
1
˙f(γ(t))
dt
= ⟨⟨v1, v2⟩⟩f .⊓⊔
The Fisher-Rao metric is intimately connected with the SRSF framework in-
troduced in the previous section. In fact, the Fisher-Rao metric can be viewed as
a special case of the SRSF framework when restricted to F0, or, in other words,
SRSF representation endowed with the L2 metric, is a generalization of the Fisher-
Rao metric to F. To see this connection, we establish the following results.
Lemma 4.7. Under the SRSF representation, the Fisher-Rao Riemannian metric
on F0 becomes the standard L2 metric.
Proof. As illustrated in Fig. 4.26, the mapping from f to q is as follows: f(t)
d
dt
→
˙f(t)
Q
→q(t). For any v ∈Tf(F), the diﬀerential of this mapping is v(t)
d
dt
→
˙v(t)
Q∗, ˙
f(t)
→
w(t). To evaluate the expression for w, we need the expression for
Q∗. Since x > 0 in this context, we have Q(x) = √x and its directional derivative

106
4 Functional Data and Elastic Registration
v1
v2
˙v2
˙v1
f
˙f
q
w1
w2
d
dt
Q
F0
L2
Fig. 4.26 Mapping from the function space F0 to L2 in two steps
in the direction of y ∈R is y/(2√x). Now, to apply this result to our situation,
consider two tangent vectors v1, v2 ∈Tf(F) and deﬁne their mappings under
Q∗as wi(t) ≡Q∗, ˙f(t)(˙vi(t)) = ˙vi(t)/(2

˙f(t)). Taking the L2 inner product be-
tween the resulting tangent vectors, we get ⟨w1(t), w2(t)⟩=  1
0 w1(t)w2(t)dt =
1
4
 1
0 ˙v1(t)˙v2(t)
1
˙f(t)dt. The R.H.S. is compared with the expression in Deﬁnition 4.8
to complete the proof.
⊓⊔
We can take this relationship further and write geodesic distances under the
Fisher-Rao metric explicitly. By Deﬁnition 3.5, the geodesic distance dF R is given
by:
dF R(f1, f2) =
inf
{α:[0,1]→F|α(0)=f1,α(1)=f2} L[α],
(4.25)
where L[α] =
 1
0

⟨⟨˙α(τ), ˙α(τ)⟩⟩α(τ)dτ. In other words, among all diﬀerentiable
paths in F, going from f1 to f2, we have to ﬁnd one with the shortest length,
with the length measured using the Fisher-Rao metric. Given the nature of Fisher-
Rao metric, this is a diﬃcult task. Since this metric changes from point to point
(note the dependence of ⟨⟨v1, v2⟩⟩f on f in Deﬁnition 4.8), it seems diﬃcult to
reach an explicit expression for the geodesic path or the geodesic distance under
this metric, directly in F. The minimization stated above is nontrivial and only
some numerical algorithms are known to attempt this problem. However, using
Lemma 4.7, this can be done as follows: Simply compute the L2 distance between
the corresponding SRSFs and set dF R to the value dF R(f1, f2) = ∥q1 −q2∥. Also, a
geodesic path between q1 and q2 is a straight line: α(τ)(t) = (1 −τ)q1(t) + τq2(t).
For any τ, we can map the SRSF α(τ) back to the function space using integration
f(τ)(t) =
 t
0 α(τ)(s)|α(τ)(s)| ds and obtain a geodesic in F under the Fisher-Rao
metric. This is a tremendous simpliﬁcation over the minimization problem posed
in Eq. 4.25.
Remark 4.1. In summary, the Fisher-Rao metric is deﬁned only on a subset
F0 ⊂F but we can extend it to the larger space F using SRSF representation
endowed with the L2 metric. We will loosely call this framework the Fisher-Rao
framework, and the L2 metric on SRSF representation space (the full L2 space)
as the extended Fisher-Rao metric.)

4.10 Phase and Amplitude Distances
107
4.10 Phase and Amplitude Distances
Now that we have deﬁned the notions of phase and amplitude, we seek metrics
for quantifying diﬀerences between functions based on these properties. These
metrics will play an important role in deﬁning and computing summary statistics of
function data, and in developing statistical models for handling such data. First, we
deﬁne a metric to compare the amplitudes of two functions, ignoring their relative
phase. (The reader will notice that the concept of amplitude is very similar to the
notion of shape studied for 2D curves later, except there are no rotations in case
of the functional data.)
4.10.1 Amplitude Space and a Metric Structure
We have deﬁned the orbit of a function f, with SRSF q ∈L2, under ˜Γ as the
[q] = {(q ◦γ)√˙γ|γ ∈˜ΓI}, and the amplitude space A as the quotient space L2/ ˜ΓI.
This deﬁnition assumes that ˙f ̸= 0 a. e. Since the amplitude of a function is
represented by its orbit, a comparison of amplitudes naturally implies a comparison
of the corresponding orbits. Stated diﬀerently, the comparison should lead to the
same answer irrespective of which elements from the two orbits are picked to
measure the diﬀerence. We would also like to characterize geodesics, or shortest
paths, between elements of A , such that lengths of these paths match the chosen
distance.
It should be noted that neither the orbits [q] nor the quotient space A here are
diﬀerentiable manifolds! So we cannot use the techniques of Riemannian geometry,
such as orthogonal sections (Sect. 3.7), to analyze this situation. We can still view
A it as a metric space, with the distance inherited from the larger space L2. We
use Eq. 4.18 to deﬁne a proper distance on the quotient space.
Deﬁnition 4.9 (Amplitude Distance). For any two functions f1, f2 ∈F and
the corresponding SRSFs, q1, q2 ∈L2, we deﬁne the amplitude distance da to be:
da([q1], [q2]) =
inf
γ1,γ2∈˜Γ
(∥(q1, γ1) −(q2, γ2)∥) ,
(4.26)
which makes A ≡L2/ ∼a metric space.
Lemma 4.8. The distance da is a proper distance on A .
Proof. We need to establish the following properties of da.
1. Symmetry: The symmetry of da comes directly from the symmetry of L2 norm.
2. Positive Deﬁnite: For positive deﬁniteness, we need to show that da([q1], [q2]) =
0 ⇒[q1] = [q2]. Suppose that da([q1], [q2]) = 0. By deﬁnition, it follows imme-
diately that for all ϵ > 0, there exists a γ ∈ΓI such that ∥q1 −(q2, γ)∥< ϵ.
From this, it follows that q1 is in the orbit of q2. Since we are assuming that
orbits are closed, it follows that q1 ∈[q2], so [q1] = [q2].
3. Triangle Inequality: To establish the triangle inequality, we need to prove
da([q1], [q3]) ≤da([q1], [q2]) + da([q2], [q3]), for any q1, q2, q3 ∈L2. Seek-
ing contradiction, suppose that da([q1], [q3]) > da([q1], [q2]) + d([q2], [q3]). Let

108
4 Functional Data and Elastic Registration
ϵ = 1
3(da([q1], [q3])−da([q1], [q2])−da([q2], [q3])); by our supposition, ϵ > 0. From
the deﬁnition of ϵ, it follows that da([q1], [q3]) = da([q1], [q2])+da([q2], [q3])+3ϵ.
By the deﬁnition of da, we can choose γ1, γ2 ∈
˜ΓI, such that ∥(q1, γ1) −
q2∥≤da([q1], [q2]) + ϵ and ∥q2 −(q3, γ2)∥≤da([q2], [q3]) + ϵ. Now by the
triangle inequality for the L2-norm, we know that ∥(q1, γ1) −(q3, γ2)∥≤
∥(q1, γ1) −q2∥+ ∥q2 −(q3, γ2)∥≤da([q1], [q2]) + da([q2], [q3]) + 2ϵ. It follows
that da([q1], [q3]) ≤da([q1], [q2]) + da([q2], [q3]) + 2ϵ. But this contradicts that
fact that da([q1], [q3) = da([q1], [q2]) + da([q2], [q3]) + 3ϵ. Hence our supposi-
tion that da([q1], [q3) > da([q1], [q2]) + da([q2], [q3]) must be false. The triangle
inequality follows.
⊓⊔
A simple consequence of this deﬁnition is that for any γ1, γ2 ∈ΓI, and q1, q2 ∈
L2, da([q1], [q2]) = da([(q1, γ1)], [(q2, γ2)]). We can simplify the computation of da,
using Eq. 4.19, to obtain:
da([q1], [q2]) =
inf
γ1,γ2∈˜ΓI
∥(q1, γ1) −(q2, γ2)∥
=
inf
γ1,γ2∈ΓI ∥(q1, γ1) −(q2, γ2)∥= inf
γ∈ΓI ∥q1 −(q2, γ)∥.
We illustrate this idea using a simple example.
Example 4.2. Consider the two functions shown in Fig. 4.27; the nonzero parts of
these functions is actually a single cycle of the sine function. These functions are:
fi(t) =
⎧
⎪
⎨
⎪
⎩
0,
0 ≤t < ϵi
sin

2π (t−ϵi)
(1−2ϵi)

, ϵi ≤t < 1 −ϵi
0,
1 −ϵi ≥t ≤1
.
In this example, ϵ1 = 0.33 and ϵ2 = 0.01. Let’s calculate the relative phase, the
amplitude distance da(f1, f2), and the phase distance dp(f1, f2) between them.
The optimal alignment comes from:
γ∗
1(t) =
⎧
⎪
⎨
⎪
⎩
ϵ2
ϵ1 t,
0 ≤t < ϵ1
ϵ2 + (t−ϵ1)
1−2ϵ1 (1 −2ϵ2),
ϵ1 ≤t < 1 −ϵ1
(1 −ϵ2) + ϵ2
ϵ1 (t −(1 −ϵ1)), 1 −ϵ1 ≤t ≤1
.
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
f1
f2
Fig. 4.27 Two functions f1 and f2 used in Example 4.2

4.10 Phase and Amplitude Distances
109
and γ∗
2(t) = t. Since ∥(q1, γ∗
1) −(q2, γ∗
2)∥= 0, i.e., the two functions happen to lie
in the same orbit, their amplitude distance da([q1], [q2]) = 0.
We also note that if at least one of the functions f1 and f2 is piecewise lin-
ear, then there exist elements of [q1] and [q2] that achieve the minimum distance
between these equivalence classes. If both are piecewise linear, than it is possible
to provide an algorithm that calculates these elements [94]. (Bruveris [20] has re-
cently proved that, if f1 and f2 are both C1, then an optimal matching exists.
He has also presented a pair of functions f1, f2 : [0, 1] →R2 such that no matching
exists.) For completely general, absolutely continuous functions f1 and f2, it is not
known when the optimal matching exists. However, this is not too important for
practical applications, since we generally only know a ﬁnite number of points on
functions when analyzing real data.
Even though A is only a metric space, as opposed to a Riemannian manifold,
we can still deﬁne the notion of a geodesic in this set in a way that is consistent
with the deﬁnition for Riemannian manifolds.
Deﬁnition 4.10 (Geodesics in A ). A geodesic in A is a function α : [0, 1] →A
with the property that there exists a number R > 0 such that, for all s0 ∈[0, 1],
there exists ϵ > 0 with the property that for all s ∈[0, 1] satisfying |s −s0| < ϵ,
da(α(s), α(s0)) = R|s −s0|.
Essentially, this deﬁnition says that a path in A is a geodesic if it gives the shortest
path between any two nearby points on the path and if the speed is constant (in
the deﬁnition, the speed is the number R).
If q0 and q1 are elements of L2, α : [0, 1] →L2 is a constant-speed path satisfying
α(0) ∈[q0], α(1) ∈[q1], and the length of the path α in L2 is equal to da([q1], [q2]),
then it’s easy to verify that the map from [0, 1] →F/ΓI given by s →[α(s)] is
a geodesic in F/ΓI. There are certain technical issues in this formulation that a
reader should be aware of. Given q1, q2 ∈L2, there does not necessarily exist an
element γ ∈ΓI such that ∥q1 −(q2, γ)∥= da([q1], [q2]). However, it is true that
for every ϵ > 0, there exists a γ ∈ΓI such that ∥q1 −(q2, γ)∥< da([q1], [q2]) + ϵ.
Similarly we cannot generally expect to ﬁnd actual geodesics in F/ΓI, unless the
two functions are piecewise linear; however, we will approximate these geodesics
by paths from [q1] to [q2] whose lengths approximate the distance da([q1], [q2]).
Now we look carefully at the quantity in Eq. 4.22 and investigate its role as
a valid distance. Clearly, it is not a proper distance in F, since two diﬀerent
functions, e.g., any f and f ◦γ for any γ, will have zero distance between them.
The correct space where this quantity is a distance is a quotient space of F, or
equivalently L2 using the SRSF representation, under the action of ΓI. Later we
will discuss some of the subtleties of this quotient space and its metric.
4.10.2 Phase Space and a Metric Structure
Earlier, in Sect. 4.3.3, we introduced the warping group ΓI that was used to gen-
erate group actions on a function space F. Now we take a careful look at the ge-
ometry of ΓI itself and consider the problem of computing geodesics and geodesic
distances between elements of ΓI under certain chosen metrics. We remind the
reader that the tangent space:

110
4 Functional Data and Elastic Registration
Tγid(ΓI) = {v : [0, 1] →R|v(0) = 0, v(1) = 0,
v is smooth} .
In later chapters, we will also be interested in some ﬁnite-dimensional approxima-
tions of elements of Tγid(ΓI). Toward that goal, we will write down some orthonor-
mal bases of this space that can be conveniently truncated for ﬁnite-dimensional
approximations. The orthonormality, of course, will depend on the choice of metric.
There are several possibilities for the Riemannian metric:
1. L2 Metric: One obvious choice of metric to compare warping functions is the
L2 metric. As in the previous section, we can ask the question: What are the
geodesics between elements of ΓI under this metric? The geodesic path under
the L2 metric is simply a straight line: for any two points γ1, γ2 ∈ΓI a geodesic
path is given by: α(τ) = (1 −τ)γ1 + τγ2.
A complete basis of Tγid(ΓI) under the L2 metric is given by:
 1
√
2 sin(πnt), n = 1, 2, 3 . . .
,
.
(4.27)
2. First-Order Palais Metric: Sometimes it is more appropriate to use a met-
ric that involves derivatives rather than the functions themselves. Since the
Sobolev metrics typically involve both the functions and their derivatives, we
prefer using the Palais metrics that involve only the derivatives and function
evaluations at ﬁxed points. The ﬁrst-order Palais metric is given by:
⟨v1, v2⟩s = v1(0)v2(0) +
 1
0
˙v1(t)˙v2(t) dt .
(4.28)
Later on this metric will play a prominent role in deriving a numerical ap-
proach for computing geodesics in certain Riemannian manifolds (See Eq. 6.29,
for example). An orthonormal basis of Tγid(ΓI) under the Palais metric is given
by:
{
1
√
2πn sin(2πnt),
1
√
2πn(cos(2πnt) −1)|n = 1, 2, 3 . . .} .
(4.29)
Due to the presence of n in the denominators, the contributions from higher-
order harmonics decay naturally and this provides a reason for truncating the
basis at some large value of n.
3. Fisher-Rao Metric: The Fisher-Rao metric introduced in Deﬁnition 4.8, for
general functions, can be particularized to ΓI. For any v1, v2 ∈Tγ(ΓI), it takes
the form:
⟨⟨v1, v2⟩⟩γ =
 1
0
˙v1(t)˙v2(t) 1
˙γ(t)dt .
(4.30)
This is a special case of the Deﬁnition 4.8 when restricted to the set ΓI. How
are the geodesics in ΓI under the Fisher-Rao metric computed? For a γ ∈ΓI,
the derivative ˙γ is an element of P discussed later in Section 4.11.2. Therefore,
we have a natural mapping γ →˙γ →√˙γ, termed SRSF, from ΓI to P to
Q. Once again, the Fisher-Rao metric becomes the L2 metric on Q, which is
the positive orthant of S∞, and the geodesics are simply arcs on great circles
(Eq. 4.9). Each point on this geodesic in Q can be mapped back to ΓI to obtain
the desired geodesic. For any τ ∈[0, 1], the point α(τ) ∈Q gets mapped to a
warping function using α(τ) →γ(t) =
 t
0 α(τ)(s)2ds. Figure 4.28 shows some

4.10 Phase and Amplitude Distances
111
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
Fig. 4.28 Examples of geodesic paths between warping functions of the type γ(t) = ta for
diﬀerent values of a
examples of pairwise geodesic paths between several warping functions of the
type γ(t) = ta for a = 0.1, 1, 5, 10.
An orthogonal basis of Tγid(ΓI), under the Fisher-Rao metric, can be easily
written using the geometry of S∞. Since γid maps to a constant function 1, the
tangent space T1(S∞) = {w : [0, 1] →R|w is smooth ,
 1
0 w(t)dt = 0}. Elements
of this space can be mapped back to Tγid(ΓI) using the square-integral mapping
mentioned above. Thus, any orthonormal basis of T1(S∞) under the L2 metric
results in, through this mapping, an orthonormal basis for Tγid(ΓI).
In order to compare the phase components of two functions, f1 and f2, we look
at the relative phase between them.
Deﬁnition 4.11 (Phase Distance). Let (γ∗
1, γ∗
2) be the relative phase of f1
w.r.t f2. Then, deﬁne the phase distance between f1 and f2 as dp(f1, f2) =
cos−1(

(˙γ∗
1)1/2, (˙γ∗
2)1/2
).
This deﬁnition is based on the Fisher-Rao distance on +
ΓI measured using SRSFs
of the warping functions. It was shown in Sect. 4.10.2 that SRSFs of warping
functions on [0, 1] are elements of a unit Hilbert sphere, so the arc length between
points on the sphere provides a proper metric for their comparison. The distance
dp is exactly the arc-length distance between SRSFs of γ∗
1 and γ∗
2. It is easy to
check the following properties of dp:
1. dp is zero if and only if γ∗
1 = γ∗
2.
2. dp(f1, f2) = dp(f2, f1).
3. For any a ∈R+ and b ∈R, we have dp(f1, f2) = dp(f1, af2 + b).
4. The distance dp satisﬁes the triangle inequality: dp(f1, f3) ≤dp(f1, f2) +
dp(f2, f3), for any f1, f2, f3 ∈F.
The proofs of these properties have been left out as exercises to the reader.
Using the approximation presented in Sect. 4.7.3, the Fisher-Rao phase distance
can be approximated by dp(f1, f2) ≈cos−1(
 1
0

˙
γ∗
12(t)dt).
Example 4.3. Continuing with the two functions used in Example 4.2, we compute
the relative phase γ∗
12 between them. This warping function is given by:
γ∗
12(t) =
⎧
⎪
⎨
⎪
⎩
ϵ1
ϵ2 t,
0 ≤t < ϵ2
ϵ1 + (t−ϵ2)
1−2ϵ2 (1 −2ϵ1),
ϵ2 ≤t < 1 −ϵ2
(1 −ϵ1) + ϵ1
ϵ2 (t −(1 −ϵ2)), 1 −ϵ2 ≤t ≤1
.

112
4 Functional Data and Elastic Registration
This is exactly the inverse of γ∗
1 given in that example, and the Fisher-Rao phase
distance is given by:
dp(f1, f2) = cos−1(2√ϵ1ϵ2 +

(1 −2ϵ1)(1 −2ϵ2)) .
4.11 Diﬀerent Warping Actions and PDFs
4.11.1 Listing of Diﬀerent Actions
In the previous section, we have relied heavily on the action of ΓI (or equivalently
of +
ΓI) on L2 that preserves the norm of functions. What other actions are possible
and what properties do they preserve? Here is a short list:
1. Value-Preserving Warping: The classical action of ΓI on a space of func-
tions, say L2, is by composition: (f, γ) = f ◦γ. The main property of this action
is that no values of f(t) are created or destroyed; they are simply moved from
one ordinate to another. In other words, it is not possible to have f(t0), for
some t0 such that f(t0) ̸= f(γ(t)) for all t ∈[0, 1]. This type of warping is most
commonly used in matching functions as described earlier in this chapter. In
the context of images and higher-dimensional signals, this warping is used to
deform data by displacing the pixels values.
2. Norm-Preserving Warping: Here we have ∥f∥= ∥(f, γ)∥for all f ∈L2 and
γ, and where the chosen norm is L2. The action of ΓI on the space of SRVFs
falls in this category. Accordingly, an example of a norm-preserving action is
(f, γ) = (f ◦γ)√˙γ. In fact, one can deﬁne an operator on the function space
as Dγ : L2 →L2, according to Dγ(f) = (f ◦γ)√˙γ. Dγ is unitary operator
and can be interpreted as a rotation in the Hilbert space L2. But it is not
onto if ˙γ = 0 on a set of positive measure. Note that Dγ(a1f1 + a2f2) =
a1Dγ(f1)+a2Dγ(f2). This interpretation will be useful later on in shape analysis
where both Euclidean rotations and warping will both play a role in registration
of shapes.
3. Area-Preserving Warping: Another possibility is to preserve the area below
the curve,
 1
0 f(t)dt, rather than the function values or the L2 norm. Here, one
is interested in deﬁning an action (f, γ) such that
 1
0 (f, γ)(t)dt =
 1
0 f(t)dt. It
can be veriﬁed that the action (f, γ) = (f ◦γ)˙γ satisﬁes this property. Since
probability density functions are characterized by unit area, among other prop-
erties, we will use this action in analyzing probability density functions in the
next section.
We summarize diﬀerent actions of ΓI on diﬀerent Riemannian manifolds and
the corresponding Fisher-Rao metric that we have seen in this chapter. This table
is only for the restricted set F0 in order to be able to deﬁne the ﬁrst two columns.
The third column is extendable to the full L2, but with more general deﬁnition of
SRSF.

4.11 Diﬀerent Warping Actions and PDFs
113
Case 1 (function)
Case 2 (density)
Case 3 (SRSF)
Function
space
Absolutely continuous
Integrable
Square integrable
F0
L1
L2
f
g = ˙f
q = √g =
 ˙f
Group action (f, γ) = f ◦γ
(g, γ) = (g ◦γ)˙γ
(q, γ) = (q ◦γ)√˙γ
Preservation Value preserving
Area preserving
Norm preserving
Fisher-Rao
metric
⟨v1, v2⟩f =  1
0 ˙v1(t) ˙v2(t)
1
˙f(t) dt ⟨u1, u2⟩f =  1
0 v1(t)v2(t)
1
g(t) dt ⟨w1, w2⟩f =
 1
0 w1(t)w2(t)dt
Fisher-Rao
distance
Diﬃcult to compute directly
Diﬃcult to compute directly
∥q1 −q2∥
4.11.2 Probability Density Functions
A very interesting and important application of Fisher-Rao Riemannian metric is
in a (nonparametric) analysis of probability densities. Consider the set of proba-
bility density functions on the interval [0, 1]:
P = {g : [0, 1] →R≥0|
 1
0
g(t)dt = 1} .
Geometry of P: To prove that P is a manifold, consider the mapping φ :
L1([0, 1], R) →R≥0 given by φ(g) =
 1
0 g(x)dx. The diﬀerential of φ in the direction
of h ∈Tg(L1([0, 1], R)) is given by dφ(h) =  1
0 h(x)dx. Since this derivative is a
surjective linear transformation, the inverse image φ−1(1) = P is a submanifold
of L1([0, 1], R). We point out that L1([0, 1], R) is a Banach space and not a Hilbert
space and, thus, P is a Banach manifold. Another interesting aspect of P is that
it is a manifold with boundary. The constraint that g(x) ≥0 for all x ∈[0, 1]
implies that if a probability density g has at least one zero, then that g is an
element of the boundary ∂P.
We would like a metric on P for use in comparing probability densities us-
ing geodesic distances. In the past, there have been several quantities proposed
for comparing densities but none of them are quite satisfactory in terms of their
use in a statistical analysis on P. For instance, the Kullback-Leibler divergence
is frequently used to quantify diﬀerences between densities but it is not a proper
distance. In fact, it is not even symmetric. Similarly, although one can use L1 or L2
for measuring diﬀerences between density functions, it is still less than satisfactory
in the following sense. The action of ΓI is not by isometries under any of these
metrics. While the literature on diﬀerential geometry and Riemannian analysis of
P is rather rich (see, e.g., [5]), there seems a big disconnect between a “diﬀerential
geometric viewpoint” and traditional statistical methods. In particular, a surpris-
ingly large fraction of techniques in statistics, especially in estimation, modeling,
and inferences, have been developed without any consideration of a metric or a
distance on P. We will show that the Fisher-Rao metric deﬁned in the previous
section applies to this problem with a simple change of variable. The tangent space
Tg(P) is given by:
Tg(P) = {w ∈L1([0, 1], R)|
 1
0
w(t)dt = 0} .

114
4 Functional Data and Elastic Registration
Riemannian Structure: We will use the restriction of the Fisher-Rao metric
to this case. For this purpose, we restrict to the set of non-negative functions:
P0 = {g ∈P|g > 0}.
Deﬁnition 4.12 (Nonparametric Fisher-Rao Metric for Densities). For a
g ∈P0 and vectors v1, v2 ∈Tg(P), the Fisher-Rao metric is deﬁned to be:
((w1, w2))g =
 1
0
w1(t)w2(t) 1
g(t)dt .
(4.31)
In order to compare this deﬁnition with the earlier deﬁnition involving general
functions (Deﬁnition 4.8), we note that g here is equivalent to ˙f and, hence, wi =
˙vi.
When restricted to a parametric family, the Fisher-Rao metric provides a quan-
tiﬁcation of “information” present in the given data and helps impose a lower
bound on the expected error associated with any parameter estimation procedure.
We describe this connection brieﬂy. Let Pe be the subset of P0 containing a
speciﬁc parametric family of interest:
Pe = {gθ ∈P|θ ∈Rn} .
Here gθ(t) is a functional form that depends on θ ∈Rn in a smooth way, and for
each θ ∈Rn, we have
 1
0 gθ(t)dt = 1. In estimation theory, one is interested in
estimating θ using observations of t, denoted by t1, t2, . . . , tn, under the maximum
likelihood framework according to ˆθ = argmaxθ∈Θ
-
i gθ(ti). One way to analyze
the performance of this or any other estimator of θ is to compare the expected
squared error with the Cramer-Rao error bound given by:
E[|(ˆθ −θ)(ˆθ −θ)T |] ≥J−1(θ),
where J(θ) is an n × n matrix with entries given by:
Jij(θ) =
 1
0
 ∂
∂θi
log(gθ(t))
  ∂
∂θj
log(gθ(t))

gθ(t)dt .
This matrix J(θ) is called the Fisher information matrix. Using the fact that
1
gθ(t)
∂gθ(t)
∂θi
= ∂log(gθ(t))
∂θi
, J(θ) can also be written as:
Jij(θ) =
 1
0
∂gθ(t)
∂θi
 ∂gθ(t)
∂θj

1
gθ(t) dt .
(4.32)
Note that the conditions assumed here work for the exponential families often
studied in an information geometry.
What is the restriction of the nonparametric Fisher-Rao metric to the paramet-
ric family Pe? The tangent vectors in this case are given by the partial derivatives:
wi =
∂
∂θi gθ. In other words, these partial derivatives form a basis of the tangent
space of P0 at gθ. Since any other tangent vector can be expressed as a linear
combination to these basis elements, it suﬃces to deﬁne the Riemannian metric
on these elements.
Deﬁnition 4.13 (Parametric Fisher-Rao Metric for Densities). Expressing
the Fisher-Rao metric given in Eq. 4.31 in the parametric terms, we get:

4.11 Diﬀerent Warping Actions and PDFs
115
 ∂
∂θi
gθ, ∂
∂θj
gθ

gθ
=
 1
0
( ∂
∂θi
gθ(t))( ∂
∂θj
gθ(t))
1
gθ(t) dt .
(4.33)
Comparing this with Eq. 4.32, we can see that this is nothing but the i, jth el-
ement of the Fisher information matrix. Due to this connection, the underlying
Riemannian metric carries the name Fisher-Rao!
Remark 4.2. The computation of geodesic paths and geodesic distances under the
parametric Fisher-Rao metric, i.e., when restricted to parametric families of prob-
ability density functions, is quite diﬃcult. This is in contrast to the nonparametric
case where the expressions for geodesic paths and geodesic distances are readily
available.
Another interesting aspect of this metric is its relationship with the commonly
used Kullback-Leibler (K-L) divergence. For any two densities g1, g2 ∈P0, the
K-L divergence is deﬁned to be:
KL(g1||g2) =
 1
0
g1(t) log(g1(t)
g2(t))dt .
It can be shown that, inﬁnitesimally, KL divergence is equivalent to the Fisher-Rao
metric.
Proposition 4.1. Let g ∈P0 and v ∈Tg(P0) and ((·, ·))g denote the nonpara-
metric Fisher-Rao metric on P0 (Eq. 4.31). Then, we have:
lim
ϵ→0
KL(g||g + ϵv)
((ϵv, ϵv))g

= 1
2 .
Proof. The Kullback-Leibler divergence between g and a perturbation g + ϵv, is
given by:
KL(g||g + ϵv) =
 1
0
g(t) log(
g(t)
g(t) + ϵv(t))dt
= −
 1
0
g(t) log(g(t) + ϵv(t)
g(t)
)dt
= −
 1
0
g(t)
ϵv(t)
g(t) −ϵ2
2
v(t)2
g(t)2 + ϵ3
3
v(t)3
g(t)3 + O(ϵ4)

dt
= ϵ2
 1
0
v(t)2
2g(t) −ϵ v(t)3
3g2(t) + O(ϵ2)

dt .
Here we have used the Taylor’s expansion of log(1 + x) = (x −x2
2 + x3
3 −. . . ), and
the fact that  v(t)dt = 0. On the other hand, the Fisher-Rao norm of the tangent
vector ϵv at g is ((ϵv, ϵv))g = ϵ2  1
0 v(t)2
1
g(t) dt. Therefore, the ratio:
KL(g||g + ϵv)
((ϵv, ϵv))g
= 1
2 −ϵ
 1
0 (v(t)3/2g2(t))dt
 1
0 (v(t)2/g(t))dt

+ . . . ..
In the limit, the ratio converges to 1
2.
⊓⊔

116
4 Functional Data and Elastic Registration
This inﬁnitesimal relationship further underscores the importance of Fisher-Rao
metric for use in analysis of probability density functions.
Similar to the functional analysis studied in the last section, a convenient way
to study the Riemannian structure of P is through the square-root representation.
Let q(t) =

g(t) denote the point-by-point, positive square root of a probability
density function g(t). Since g integrates to one, we have  1
0 q(t)2dt =  1
0 g(t)dt = 1.
That is, q is an element of the positive orthant S+
∞of the inﬁnite-dimensional
sphere S∞introduced in Example A.11 and studied earlier in Sect. 4.3.2. This
mapping identiﬁes P with the positive orthant of S∞, including the boundaries.
The function q is also referred to as the half density of g since its square is a full
probability density. For any half density q, we can easily obtain the probability
density function using g(t) = q(t)2. Let Q ⊂S+
∞be the set of all square-root forms
or half-densities of probability density functions on [0, 1]. If a function q is zero for
some subset of the domain, then it lies in the boundary of Q.
Lemma 4.9. The Fisher-Rao metric for probability densities transforms to the L2
metric under the square-root mapping, up to a constant.
Proof. This is analogous to Lemma 4.7 for general functions. Since q(t) =

g(t),
a tangent vector w in Tq(Q) is related to the corresponding vector v in Tg(P) by
v(t) = 2

g(t)w(t). In the new coordinates, the Fisher-Rao metric becomes:
((v1, v2))g =
 1
0
v1(t)v2(t) 1
g(t)dx = 4
 1
0

g(t)w1(t)

g(t)w2(t) 1
g(t)dt
∝
 1
0
w1(t)w2(t)dt = ⟨w1, w2⟩Q ,
(4.34)
the L2 metric in Q.
⊓⊔
Once again this is an important result since, for the L2 structure on Q, we al-
ready have expressions for computing geodesics, exponential maps, etc., for proba-
bility density functions, and these can be used for a direct Riemannian analysis of
probability density functions. (Furthermore, in case a probability density function
has zero values, this does not pose any problem in this analysis. ) For example, if
we have two probability density functions g1 and g2 on the domain [0, 1] and we
want to compute the Riemannian distance, i.e., geodesic length distance, between
them, we can do so simply using:
d(g1, g2) = cos−1
 1
0
√g1(t)√g2(t)dt

.
(4.35)
Also, the geodesic path between them can also be computed easily using Eq. 4.9.
Figure 4.29 shows two examples of computing geodesic paths between elements
of P under the Fisher-Rao metric. First, we compute their half-densities using
element-wise square root, use Eq.4.9 to compute the great circle α(τ) connecting
the half-densities in Q, and ﬁnally compute the squares of the points along the
great circle to get the geodesic in P. For each path, we show nine equally spaced
points {α(τ), τ = 0, 1/9, . . ., 1} in vertical fashion from bottom to top.
Note that the use of the L2 metric as the Riemannian metric does not imply
that the geodesic distances between points in Q are given by L2 norms of their
diﬀerences. This is because Q is not all of L2 but only a subset of it. The geodesic

4.11 Diﬀerent Warping Actions and PDFs
117
0
0.2
a
b
c
d
0.4
0.6
0.8
1
0
1
2
3
4
5
6
7
8
0
0.2
0.4
0.6
0.8
1
0
0.5
1
1.5
2
2.5
3
0
0.2
0.4
0.6
0.8
1
0
5
10
15
20
25
30
35
40
0
0.2
0.4
0.6
0.8
1
0
5
10
15
20
25
30
35
40
45
0
0.2
0.4
0.6
0.8
1
0
1
2
3
4
5
6
7
8
0
0.2
0.4
0.6
0.8
1
0
0.5
1
1.5
2
2.5
3
0
0.2
0.4
0.6
0.8
1
0
5
10
15
20
25
30
35
40
0
0.2
0.4
0.6
0.8
1
0
5
10
15
20
25
30
35
40
45
Fig. 4.29 Two examples of geodesics under Fisher-Rao metric. In each row: (a) density functions
g1, g2 ∈P, (b) the corresponding half-densities q1, q2 ∈Q, (c) the geodesic path between q1
and q2 in Q, and (d) the corresponding path between g1 and g2 in P
distance between any two points is given by the length of the shortest arc connect-
ing them, while the latter is simply the chord length. Incidentally, the chord-length
distance between any two elements of Q is known as the Hellinger distance:
dh(g1, g2) =
 1
0
∥

g1(x) −

g2(x)∥2dx .
The square-root mapping from P to Q was helpful in two ways: (i) the invariant
Fisher-Rao metric on P becomes the L2 metric on Q and (ii) Q is a positive
orthant of a unit sphere with known diﬀerential geometry. Together these two
items enable us to write down geodesics between probability densities in a rather
simple fashion.
Area-Preserving Warping : Consider the warping group ΓI introduced in the
last section. This group acts on P in the following way (note the diﬀerence of this
action from its action on F used in the previous section):
P × ΓI →P,
(g, γ) = (g ◦γ)˙γ .
(4.36)
To show that this is a group action, we only point out that
 1
0 g(γ(t))˙γ(t)dt =
 1
0 g(t)dt = 1. The remaining steps are left as an exercise. It is easy to establish
that the action of ΓI on P under the Fisher-Rao metric is by isometries.
Lemma 4.10. The Fisher-Rao metric, given in Eq. 4.31, is invariant under area-
preserving warping, i.e.:
((˜v1, ˜v2))˜g = ⟨v1, v2⟩g ,
where ˜vi = (vi ◦γ)˙γ and ˜g = (g ◦γ)˙γ.
The proof is similar to that of Lemma 4.6 and left as an exercise to the reader.

118
4 Functional Data and Elastic Registration
4.12 Exercises
4.12.1 Theoretical Exercises
1. Derive the expression for the R ∈RK×K matrix in penalized least-squares
estimation of f (Eq. 4.3) when the penalty term is given by
 1
0
d4f(t)
dt4
dt.
2. By deriving an expression for ( d
dτ α(τ))|τ=0, for α given in Eq. 4.9, verify the
formula for inverse exponential map on S∞as given in Eq. 4.12.
3. Prove that ΓI, the set of all boundary-preserving diﬀeomorphisms from [0, 1]
to itself, forms a group under composition. Additionally, if we deﬁne γa(t) =
tea/(1 −t + tea) , then the set {γa|a ∈R} forms a subgroup of ΓI.
4. Let F = {f : [0, 1] →R} and ΓI be the group of boundary-preserving diﬀeo-
morphisms of [0, 1], as above. Show that the mapping (f, γ) →f ◦γ deﬁnes
an action of ΓI on F. (The notion of a group action is deﬁned in Sect. 3.5.)
5. Find two functions f1 and f2 in L2, such that infγ∈ΓI ∥f1 −f2 ◦γ∥is diﬀerent
from infγ∈ΓI ∥f2 −f1 ◦γ∥. (One such pair is presented in Example 4.1.)
6. Prove that the property of invariance to identical warping implies the property
of inverse symmetry. That is, assume that for a certain cost function E we
have E[f1, f2] = E[f1 ◦γ, f2 ◦γ] for a pair f1, f2, and all γ ∈ΓI. Then, let
γ∗= argminγ∈ΓI E[f1, f2 ◦γ] and show that γ∗−1 ∈argminγ∈ΓI E[f1 ◦γ, f2].
7. Let F be the set of absolutely continuous functions on [0, 1]. Show that F is
a vector space.
8. Deﬁne a parametric family of warping functions given by γa(t) = tea/(1 −t +
tea), for a ∈R. First, show that γa ∈ΓI, the set of all boundary-preserving
group of diﬀeomorphisms from [0, 1] to itself, for all a > 0. Show that the limit
lima→0 γa(t) is not in ΓI. This helps establish the fact that ΓI is not a closed
set under the usual metric.
9. a. Let f be a real-valued function on [0, 1] that is absolutely continuous and
has ˙f > 0. Show that the resulting SRSF q(t) = sign( ˙f(t))

| ˙f(t)| is square
integrable.
b. Now prove the same result without assuming that ˙f > 0.
10. Calculate and display the SRSFs of the functions shown in Fig. 4.30.
11. For any two smooth and positive functions q1, q2 ∈L2, show that there exists
a γ∗∈ΓI such that (q1, γ∗) = cq2 for some constant c. Prove that this γ∗lies
in the set argminγ∈ΓI ∥q1 −(q2, γ)∥.
12. Let q1, q2 ∈L2 be any two functions. Then, for any constant c > 0, show that if
argmin∥q1−(q2, γ)∥exists, then argminγ∈ΓI ∥cq1−(q2, γ)∥= argminγ∈ΓI ∥q1−
(q2, γ)∥.
t
f(t)
1
2
2
0
t
f(t)
1
2
2
0
1.5
t
f(t)
1
2
2
0
0.5
Fig. 4.30 Compute the SRSFs of these functions

4.12 Exercises
119
13. For any two SRSFs q1 and q2, let γ∗∈ΓI be a solution of the problem:
γ∗= arginf
γ∈ΓI
∥q1 −(q2, γ)∥.
Then, prove that, for any γ0 ∈ΓI, we have
(γ∗◦γ0) ∈arginf
γ∈ΓI
∥(q1, γ0) −(q2, γ)∥.
14. For any two q1, q2 ∈L2, show that:
inf
γ1,γ2∈˜ΓI
∥(q1, γ1) −(q2, γ2)∥= inf
γ∈˜Γ
∥q1 −(q2, γ)∥.
15. If f1 and f2 are smooth functions [0, 1] →R with both ˙f1 > 0 and ˙f2 > 0
on [0, 1, and their relative phase is (γ∗
1, γ∗
2), then show that f1 ◦γ∗
1 is a scalar
multiple of f2 ◦γ∗
2 plus a constant. (In other words, their derivatives are a
scalar multiple of each other.)
16. Show that the mapping L2 × ΓI →L2 given by (q, γ) = (q ◦γ)√˙γ deﬁnes an
action of ΓI on L2.
17. Deﬁne a relation ∼on L2 as follows. Set q1 ∼q2 if and only if q1 is an element
of the L2 closure of [q2]. Show that this is an equivalence relation.
18. Show that the following relation is an equivalence relation: for any two func-
tions f and g in F, with F being the set of absolutely continuous functions,
deﬁne them to be related if the closures of their orbits [f] = fΓI and [g] = gΓI
are identical.
19. Show that if two functions belong to two diﬀerent equivalence classes in the
previous problem, then, under da, the distance deﬁned on the quotient space
F/ΓI, their distance is strictly greater than zero.
20. Show that the set +
ΓI as deﬁned in Deﬁnition 4.3 is a monoid. That is, elements
of this set satisfy all other properties of a group except that some elements
may not have an inverse.
21. For any two functions f1, f2 ∈F, if there exists a γ ∈ΓI such that f1 = f2 ◦γ,
then show that [f1] = [f2].
22. For the relative phase deﬁned in Eq. 4.18, verify the following properties:
a. The relative phase of f1 w.r.t f2 is exactly the inverse of the relative phase
of f2 w.r.t f1, i.e., the relative phase of f2 w.r.t f1 is given by (γ∗
2, γ∗
1).
b. Two functions are said to have zero phase between them if their relative
phase is (γid, γid). Show that this is the case when f1 = f2.
c. The relative phase of f1 w.r.t f2 ◦γ, for any γ ∈ΓI, is (γ∗
1, γ−1 ◦γ∗
2).
Similarly, the relative phase of f1 ◦γ w.r.t f2 is (γ−1 ◦γ∗
1, γ∗
2). Combining
these results, the relative phase of f1 ◦γa w.r.t f2 ◦γb is (γ−1
a
◦γ∗
1, γ−1
b
◦γ∗
2).
d. As a corollary to the last property, the relative phase of f1 ◦γ w.r.t f2 ◦γ,
for any γ ∈ΓI, is (γ−1 ◦γ∗
1, γ−1 ◦γ∗
2).
e. The relative phase between any two functions is unchanged by vertical
translations and scalar multiplications. That is, the relative phase of (c1f1+
e1) relative to (c2f2 +e2) is also (γ∗
1, γ∗
2), where c1, c2 ∈R+ and e1, e2 ∈R.

120
4 Functional Data and Elastic Registration
23. For the amplitude distance da deﬁned in Eq. 4.26, and denoting da([q1], [q2])
by da(f1, f2), prove the following properties:
a. da(f1, f2) = da(f2, f1) and da(f1, f3) ≤da(f1, f2) + da(f2, f3) for all
f1, f2, f3 ∈F.
b. For any two f1, f2 ∈[f] for any f ∈F, da(f1, f2) = 0.
c. For any γ1, γ2 ∈ΓI and f1, f2 ∈F, da(f1, f2) = da(f1 ◦γ1, f2 ◦γ2).
d. The distance da is a proper distance on the set of amplitudes A .
24. For the phase distance dp as deﬁned in this chapter, prove that:
a. dp is zero if and only if γ∗
1 = γ∗
2.
b. dp(f1, f2) = dp(f2, f1).
c. For any a ∈R+ and b ∈R, we have dp(f1, f2) = dp(f1, af2 + b).
d. dp satisﬁes the triangle inequality, i.e., dp(f1, f3) ≤dp(f1, f2) + dp(f2, f3)
for any f1, f2, f3 ∈D.
25. ∗Derive a proof for Theorem 4.1.
26. Let P be the set of probability densities on [0, 1] . Show that for any g ∈P,
the tangent space Tg(P) is given by:
Tg(P) = {w ∈L1([0, 1], R)|
 1
0
w(t)dt = 0} .
27. a. Show that the mapping g →(g ◦γ)˙γ, for a γ ∈ΓI, is area preserving. In
other words, the integral of the function on the interval [0, 1] remains the
same.
b. Show that the mapping P × ΓI →P given by (g, γ) = (g ◦γ)˙γ deﬁnes an
action of ΓI on P.
28. Prove Lemma 4.10: Show that under the Fisher-Rao Riemannian metric, the
action of ΓI on P, as given in Eq. 4.36, is by isometries.
29. Show that the set

1
√
2πn
sin(2πnt),
1
√
2πn
(cos(2πnt) −1)|n = 1, 2, 3 . . .
,
forms an orthonormal basis for Tγid(ΓI) under the ﬁrst-order Palais metric.
30. (This example can be found, e.g., in http://arxiv.org/pdf/1210.2354.pdf)
Consider the family of univariate normal densities, parameterized by two pa-
rameters: the mean μ ∈R and the variance σ2 ∈R+. Each element f of this
family N (μ, σ2) can be mapped to a point (μ, σ) in the upper-half-plane H:
f(x; μ, σ) =
1
√
2πσ2 e−1
2 (x−μ)2/2σ2 .
a. Show that the Fisher information matrix is given by:
g(μ, σ) ≡

E[ ∂2f(x;μ,σ)
∂μ2
] E[ ∂2f(x;μ,σ)
∂μ∂σ
]
E[ ∂2f(x;μ,σ)
∂μ∂σ
] E[ ∂2f(x;μ,σ)
∂σ2
]

=
 1
σ2 0
0
2
σ2

,
where E[g(x, μ, σ)] =

x g(x, μ, σ)f(x; μ, σ)dx.

4.12 Exercises
121
b. Using Eq. 3.1, show that the 2×2 matrix representing the hyperbolic metric
is given by (using the coordinates as (μ, σ) instead of (p1, p2) used earlier) :
gH =
 1
σ2 0
0
1
σ2

.
c. Therefore, show that:
dF R(f(x; μ1, σ1), f(x; μ2, σ2)) =
√
2dH(f(x; μ1/
√
2, σ1), f(x; μ2/
√
2, σ2)) .
d. Using Example 3.4, derive an expression for the geodesic path and geodesic
distance dF R between any two univariate normal densities.
31. The Wasserstein distance between any two non-negative probability density
functions g1, g2 ∈P0 is given by: dw(g1, g2) = ∥f −1
1
−f −1
2 ∥, where fi is the
cumulative distribution function of gi, i = 1, 2. Check if this metric is preserved
under the action of Γ on P0. That is, check if dw(g1, g2) = dw((g1 ◦γ)˙γ, (g2 ◦
γ)˙γ) for all γ ∈Γ.
4.12.2 Computational Exercises
1. Given a function f : [0, 1] →R:
a. Write a computer program to sample a given function f at some given
points {ti|i = 1, 2, . . . , n} in [0, 1]. Call these sampled values {yi}.
b. Using a set of sampled points {(ti, yi)|i = 1, 2, . . ., n}, estimate the function
f on the set { i
5n|i = 0, 1, . . . , 5n} using interpolation.
Test your program using f(t) = 2sin(2πt) + cos(4πt) and {ti = i/10|i =
0, 1, . . . , 10}.
2. Write a program to compute and display a discrete geodesic path {α(τ)|τ ∈
{0, 1/10, 2/10, . . .1}} between any two non-antipodal points on a unit Hilbert
sphere S∞using Eq. 4.9.
3. Write a program to compute the exponential and inverse exponential maps on
S∞. For this computation, any function f on [0, 1] can be represented by a
uniform discrete sampling {f(k/T )|k = 0, 1, ˙,T }.
4. Write a program to compute fPCA for a given function dataset. In other words,
given {fi(t)|i = 1, 2, . . ., n, t = 0, 1
T , 2
T , . . . , 1}, compute and display the mean
function ˆμf, the singular values σi, and the ﬁrst three principal directions ˆb1,
ˆb2, and ˆb3. (Use the earlier program for interpolation of functions, in between
the sampled points, to display them as functions.)
5. Verify that the optimal warping functions given in Example 4.1 are as stated
in the example.
6. Write a program to compute the SRSF q of a given function f that is given
to you in the form of a set of sampled points. You will need some form of
approximation for the derivative ˙f from the discrete samples of f. Also, write
a program to compute an inverse of this map from q to f (assuming f(0) = 0).
Here you will need an approach to perform numerical integration.
7. The SRSF of a warped function f ◦γ is given by (q ◦γ)√˙γ, where q is the
SRSF of f. This provides two ways to compute this SRSF: (1) Compute SRSF
of the original function and compute (q◦γ)√˙γ, and (2) compute f ◦γ and then

122
4 Functional Data and Elastic Registration
compute the SRSF of this resulting function. Write a program for implement-
ing each of this approach. Assuming that the original function is given only
on a uniform grid, you will need to use the earlier program for interpolation
of functions. Comment on the numerical accuracies of these two approaches,
relative to each other.
8. Implement the dynamic programming algorithm to solve the discrete version
of the registration problem (Deﬁnition 4.7):
argmin
γ
 T

t=0
|q1(t/T ) −q2(γ(t/T ))

˙γ(t/T )|2

.
9. Combine elements from the previous three problems to write a program for
pairwise alignment under Deﬁnition 4.7 for any two functions deﬁned discretely
on a uniform grid on [0, 1]. Display the original functions, the warped function,
and the optimal warping function.
10. Write a program for computing a geodesic path between any two functions, f1
and f2, computed under the L2 metric on their SRSF representations q1 and
q2. Display the path in the original function space F. Compare this path with
the geodesic computed under the L2 metric directly on the function space F.
11. Write a program to compute the geodesic and the Fisher-Rao distance between
a pair of probability density function. Evaluate this distance for two Gaussian
densities with same variance σ2 and means μ1 and μ2 = μ1 + ϵ, respectively.
Plot the distance as ϵ goes to zero.
12. Write a program to compute discrete geodesic paths and geodesic distances
between elements of ΓI under the Fisher-Rao metric.
13. As mentioned above, the Wasserstein distance between any two non-negative
probability density functions g1, g2 ∈P0 is given by dw(g1, g2) = ∥f −1
1
−f −1
2 ∥,
where fi is the cumulative distribution function of gi, i = 1, 2. Write a problem
to compute the sample mean of a given set of densities g1, g2, . . . , gn, under
the Wasserstein metric. Use the following steps:
a. Compute the cumulative distribution functions fi for each gi, i = 1, 2, . . . , n.
b. Compute the inverses of these fis and ﬁnd their cross-sectional average ¯f.
c. Take the inverse of ¯f and ﬁnd the desired mean density using
d
dt( ¯f −1(t)).
4.13 Bibliographic Notes
Fisher-Rao Riemannian metric was introduced in 1945 by C. R. Rao [92], where he
used the Fisher information matrix to compare diﬀerent probability distributions.
This metric was studied rigorously in the 1970s and 1980s by Amari [4], Efron [30],
Kass [47, 119], Cencov [24], Amari [5], and others [6]. While those earlier eﬀorts
were focused on analyzing parametric families, we use the nonparametric version of
the Fisher-Rao Riemannian metric in this chapter. (This nonparametric form has
found an important use in shape analysis of curves [106].) An important attribute
of this metric is that it is preserved under warping [24]. It is diﬃcult to compute the
distance dF R directly under this metric but Bhattacharya [16] introduced a square-
root representation that greatly simpliﬁes this calculation. More mathematical
treatments on the nature of diﬀeomorphism group can be found in [29, 3, 3]. [108]

4.13 Bibliographic Notes
123
were the ﬁrst paper to develop elastic registration methods for functional data,
although similar ideas already existed in shape analysis literature for curves.
FPCA has been described well in several places including [91, 38]. A large
number of papers have dealt with the problem of registration of functional data
but mostly using L2 norm on the functions directly [71, 113, 67, 90, 91, 55].

Chapter 5
Shapes of Planar Curves
In this chapter we introduce a framework for analyzing shapes of curves that lie
in a two-dimensional plane. Using the mathematical tools introduced in the pre-
vious Chaps. 3 and 4 and Appendices A and A.2, we will develop approaches for
comparing, deforming, and statistical modeling of shapes of curves. The main top-
ics discussed here are the following: (i) parametric representations of curves using
velocity-based functions, (ii) imposition of certain constraints on these functions to
reach desired invariances, (iii) utilization of diﬀerential geometries of the resulting
constrained spaces to impose Riemannian structures and to compute geodesics,
and (iv) removal of re-parameterization variability from the representations, using
the quotient method, to deﬁne shape spaces, and the construction of geodesics
in these shape spaces. In particular, we will illustrate these ideas using two rep-
resentations of curves—angle functions and square-root velocity functions—with
appropriate metrics to obtain inelastic deformations (which allow only bending)
and elastic deformations (which allow stretching as well as bending), respectively,
for shape analysis.
5.1 Goals and Challenges
Our main goals in this chapter are:
• To provide several mathematical representations of planar curves with the pur-
pose of studying their shapes.
• To provide some Riemannian metrics for jointly registering, deforming, measur-
ing, and comparing shapes of curves.
• To provide eﬃcient algorithms for performing above tasks.
What are the main challenges in reaching these goals? First, we are interested
in continuous curves that are represented by functions. Consequently, we will use
tools for functional analysis that have been summarized in the previous chapter.
Second, in contrast to linear spaces of functions, the shape spaces are characterized
by nonlinear constraints that shall be utilized and preserved in our analysis. For
instance, in order to standardize the length, it is intuitively clear that one should
rescale the curves to make them a certain ﬁxed length. This rescaling, however,
makes the set of allowable curves a nonlinear manifold and one has to perform
calculus on that manifold. Third, shape is a geometric property that is invariant
© Springer-Verlag New York 2016
A. Srivastava, E.P. Klassen, Functional and Shape Data Analysis,
Springer Series in Statistics, DOI 10.1007/978-1-4939-4020-2 5
125

126
5 Shapes of Planar Curves
to certain additional transformations. In our formulation, these transformations
will be translation, rotation, and scaling. Since we have chosen to work with the
parametric forms of curves, we will also have to deal with the variability generated
by diﬀerent re-parameterizations of curves, as a re-parameterization of a curve
does not change its shape. So, the challenge before us is to formulate a shape
space (with a metric) such that the curves that are within these transformations
of each other are deemed identical in terms of their shapes and, thus, have zero
shape metric between them. It is possible that several representations and metrics
achieve this goal. Among them, we would like to emphasize the ones that provide
physical interpretations. Summarizing the main challenges, we need to be able to
handle (i) inﬁnite dimensionality and (ii) nonlinearity and (iii) obtain the desired
invariances in shape representations.
5.2 Parametric Representations of Curves
Let β be a continuous curve in R2. It is most naturally represented in a parame-
terized form:
β : [a, b] →R2,
β(t) = (x(t), y(t)) ,
where x, y are scalar-valued functions called the coordinate functions of β and t
is the parameter. As t varies from a to b, the point β(t) traces a path from β(a)
to β(b) in R2. This is called a parameterization of β and it dictates the rate (or
speed if t is time) at which one traces the curve.
This is one parameterization of this curve, but many more are possible. For
example, deﬁne a function:
γα : [a, b] →[a, b] as γα(t) = t + α(t −a)(t −b) ,
−1
b −a < α <
1
b −a .
(5.1)
Note that γα(a) = a and γα(b) = b and γα(t) changes, monotonically from a to
b in between. The middle row of Fig. 5.1 shows some examples of γα for diﬀerent
values of α and for a = 0 and b = 2π. Such functions are interesting because we can
use them to deﬁne new speciﬁcations of curves of the type ˜β(t) = β(γα(t)). What
does the curve ˜β look like? In fact, it looks exactly the same as β, since it passes
through the same points as β, and in the same order (see Fig. 5.1 top). However,
the diﬀerence lies in the parameter values associated with these points. Thus,
re-parameterization is a shape-preserving transformation. Figure 5.1 (bottom row)
shows some examples of re-parameterizations. Except for the case when α = 0, the
curves will have diﬀerent rates of traversal along the curve. For α > 0 the rate of
traversal is slow at the start and fast in the end, while for α < 0 it is the opposite.
The function γα is an example of a re-parameterization function, and ˜β(t) is a
re-parameterization of the curve β.
Deﬁnition 5.1. For a continuous curve β : [a, b] →Rn, a function γ is called a re-
parameterization function if it is an orientation-preserving diﬀeomorphism from
[a, b] →[a, b]. Note that “orientation-preserving” means ˙γ(t) > 0 for all t ∈[a, b].
Please refer to Sect. 4.10.2 for an introduction of diﬀeomorphisms. As an example,
the γα given in Eq. 5.1 is actually a diﬀeomorphism for all allowable α’s. Another
example of re-parameterization function is t →a+(b−a)

t−a
b−a
n
for some positive

5.2 Parametric Representations of Curves
127
0
2
4
6
0
1
2
3
4
5
6
0
2
4
6
0
1
2
3
4
5
6
0
2
4
6
0
1
2
3
4
5
6
Fig. 5.1 Examples of re-parameterization function given in Eq. 5.1 for α = −1/2π, 0, 1/2π
integer n. (Technically these are not diﬀeomorphisms because their derivatives
at t = a are zero, however they serve well enough as illustrations). A cartoon
illustration of the re-parameterization of a curve is shown in the top panel of
Fig. 5.1.
For a diﬀerentiable, parameterized curve β : [a, b] →R2, | ˙β(t)| =
.$
˙β(t), ˙β(t)
%
is its instantaneous speed and
˙β(t)
| ˙β(t)| is the instantaneous direction. The total length
of β can be computed as (using Eq. 3.2):
L[β] =
 b
a
.$
˙β(t), ˙β(t)
%
dt ,
where the inner product is the Euclidean inner product in R2. Let ˜β(t) = β(γ(t))
be a re-parameterized version of β. Then, we have:
L[˜β] =
 b
a
.$ ˙˜β(t), ˙˜β(t)
%
dt =
 b
a
.$
˙β(γ(t))˙γ(t), ˙β(γ(t))˙γ(t)
%
dt
=
 b
a
.$
˙β(γ(t)), ˙β(γ(t))
%
˙γ(t) dt =
 b
a
.$
˙β(s), ˙β(s)
%
ds = L[β] ,

128
5 Shapes of Planar Curves
where we have used the change of variable s = γ(t) in the last equality. So, we
conclude that a re-parameterization of a curve does not change its length. Fur-
thermore, as noted previously, re-parameterization does not change its appearance
or shape either. A particular class of parameterizations is that of constant-speed
parameterizations. Here the distance along the curve between any two points β(t1)
and β(t2) is c(t2 −t1), for all t1, t2 ∈[a, b], where c > 0 is a constant. Mathemati-
cally, this implies that
 s
a
.$
˙β(t), ˙β(t)
%
dt = c(s −a) , ∀s.
Such a situation is possible if and only if we have
$
˙β(t), ˙β(t)
%
= c2 for all t ∈[a, b].
That is, the instantaneous speed of the parameterization is constant and, therefore,
the name constant-speed parameterization. In this case, the length of the curve is
simply the speed multiplied by the length of the interval, L[β] = c(b−a). A special
element of this class is when c = 1. It is called the arc-length parameterization; in
this case, one traverses the curve at the unit speed and L[β] = (b −a).
Example 5.1. Let β be a circle of unit radius, β(t) = (cos(t), sin(t)) ∈R2 for
t ∈[0, 2π]. Since
$
˙β(t), ˙β(t)
%
= ⟨(−sin(t), cos(t)), (−sin(t), cos(t))⟩= 1,
β is an arc-length parameterized curve with the total length given by 2π.
5.3 General Framework
In this section, we develop a general framework that will allow us to represent,
measure, and deform shapes of planar curves.
5.3.1 Mathematical Representations of Curves
When analyzing the shape of a curve β, we will be interested in the eﬀects of
rotations and translations on β and we will do so using the notion of group action
(see Sect. 3.5 for a deﬁnition of group action). The translation group R2 acts on
the set of parameterized curves according to
(x, β) →β + x
(for x ∈R2)
and the rotation group SO(2) acts according to
(O, β) = Oβ
(for O ∈SO(2)).
Several mathematical representations of β are possible:
1. Coordinate Functions: For each curve β, we can consider βx and βy coordi-
nates as functions of the parameter t along the curve. This pair (βx, βy) contains

5.3 General Framework
129
0
2
4
6
0
2
4
6
0
2
4
6
0
2
4
6
0
2
4
6
0
2
4
6
-0.5
0
0.5
1
1.5
2
-2
0
2
4
6
a
b
c
d
-20
-10
0
10
20
-2
-1.5
-1
-0.5
0
0.5
0
2
4
6
8
-15
-10
-5
0
5
10
15
20
Fig. 5.2 (a) A simple closed curve β, (b) the two coordinate functions x and y plotted against
the arc length, (c) the angle function θ, and (d) the curvature function κ
all the information about β and can be used for analyzing its shape. For the
curve shown in Fig. 5.2a, the plot in (b) shows the corresponding coordinate
functions. It is important to note that these coordinate functions change when
we rotate and translate the original curve. That is, the coordinate functions
of β + x or Oβ are diﬀerent from those of β, as long as x ̸= 0 and O ̸= I.
Similarly, a re-parameterization of β also changes its coordinate functions. One
can handle the translations and the rotations by “standardizing” curves, using
some alignment techniques, but the variability due to re-parameterization is
more diﬃcult to remove. Therefore, from the perspective of shape invariance,
this is not considered a good representation.
2. Angle Function: Assuming that β is diﬀerentiable, ˙β is continuous, and β
is parameterized by the arc length parameter s1; we have | ˙β(s)| = 1 for all
s ∈[a, b], where | · | is the two-norm. That is, the speed of traversal along β
has a constant value of one. Therefore, the only information contained in ˙β(s)
is the direction of ˙β(s). This is better understood using complex analysis and,
for that purpose, let us identify R2 with the complex space C in the usual way:
any point β(s) on the curve is a point βx(s)+ i βy(s) in C. In this notation, the
velocity vector ˙β(s) can be written as a unit complex number eiθ(s), where θ(s)
is the angle formed by this vector with the x axis or the real axis. Note that
for each s, θ(s) is only well deﬁned up to the addition of an integer multiple
of 2π. Since we are assuming that ˙β(s) is continuous, we can choose θ(s) to be
continuous. Once we have chosen such a continuous angle function θ(s), all other
angle functions for β will be of the form θ(s) + 2πn, where n is an integer. The
function θ, with the parameter s as the variable, contains all the information
about the shape of β and can be used as a mathematical representation of that
shape.
In terms of the desired invariances, θ is invariant to translations of β but it
does change with rotations. That is, the angle function of β + x is same as that
of β, for all x ∈R2, while that of Oβ diﬀers from the angle function of β as long
1 We use s for the arc-length parameter and t for a general parameter of a curve.

130
5 Shapes of Planar Curves
as O ̸= I. If we rotate the curve β counterclockwise around the origin by an
angle θ0, then the angle function of the rotated curve is θ(s) + θ0. Figure 5.2c
shows the angle function of the curve in (a), plotted versus the parameter s. In
summary, the angle function of β is invariant to translation but not to rotation.
3. Curvature Function: When we take the derivative of the angle function,
with respect to the arc-length parameter, we obtain the curvature function of
β. Denoting this function by κ, we have that κ(s) = ˙θ(s) for all s ∈[a, b]. Since
a rotation of β adds a constant to its angle function, the curvature function κ
remains unchanged. Also, it is already invariant to translation. In other words,
the curvature functions of β + x and Oβ, for any x ∈R2 and O ∈SO(2),
are the same as that of β. Although the curvature function has the desired
invariance to rotation and translation, its use in practical situations is quite
susceptible to observation noise. If the original curve β is observed under noisy
conditions, then κ, which involves the second derivative of β, shows a noticeable
ampliﬁcation of this noise. Shown in Fig. 5.2d is the curvature function of the
curve in panel (a).
In case the curves of interest are parameterized using arc length, these
representations—the angle function and the curvature function—are individu-
ally suﬃcient to analyze the shapes of the original curves. However, in many
situations we get shape data in the form of curves that are arbitrarily parame-
terized. Also, as we shall see later, it is advantageous to include the variability
associated with diﬀerent parameterizations of curves in the process of comparing
their shapes. This is because the points across curves may match better if we
allow nonlinear re-parameterizations, (i.e. variable speed parameterizations). In
this situation, the representation space needs to be augmented to include this ad-
ditional information. In fact, the inclusion of parameterization variability becomes
a tool for improving registration across curves, similar to how warping was used
to register functions in the previous chapter. So, now let β : [a, b] →R2 be a curve
with arbitrary parameterization. Often, we will assume that the curve β is scaled
to be of certain length l:
L[β] =
 b
a
| ˙β(t)|dt = l .
(5.2)
Due to arbitrary parameterizations of curves, the speed | ˙β(t)| is not constant
anymore, and we need an additional function to keep track of its variation along
the curve. Now the function | ˙β(t)| denotes the instantaneous speed of the param-
eterization at the point β(t). Inclusion of | ˙β(t)| augments the representation space
and gives rise to the following possibilities:
1. Coordinate Functions: As earlier, one can use the two coordinate functions
to represent the curve, forming the double (βx, βy), where βx, βy : [a, b] →R
are two real-valued functions. Note that these coordinate functions also specify
the speed of traversal along the curve. So, no additional function is required to
represent the speed function.
2. Angle and Log-Speed Function: For this representation, we ﬁrst impose a
condition that the parameterization is non-singular, or | ˙β(t)| ̸= 0 for all t. Now
we can write the velocity function ˙β(t) in the complex notation as | ˙β(t)|eiθ(t),
where the ﬁrst term is the speed function and the second term is the direction

5.3 General Framework
131
function . Since the speed is constrained to be non-negative, it poses a challenge
during shape analysis. To get around this constraint, one can use its logarithm
φ(t) = log(| ˙β(t)|) and work with the real-valued function φ. Since β is a curve
of length l, we have that
 b
a eφ(t)dt =
 b
a | ˙β(t)|dt = l. We can now represent the
curve β with the pair (φ, θ) as they completely describe both the shape and the
parameterization of β. Here, φ, θ : [a, b] →R.
3. Curvature and Log-Speed Function: Repeat the previous idea with the
curvature function, instead of the angle function. This representation is based
on the pair (φ, κ), each of them being a real-valued function on [a, b] with
appropriate constraints.
4. Square-Root Velocity Function (SRVF): There is an interesting possibility
of merging the two variabilities—shape and parameterization—together to form
a single (vector-valued) function, while keeping all the relevant information.
This is done by forming a function, called the square-root velocity function of
β. This is a natural extension of the square-root slope functions seen in the
previous chapter. Let F : R2 →R2 be a mapping given by
F(v) =

v/

|v|
when |v| ̸= 0
0
when |v| = 0 .
(5.3)
Then, deﬁne the square-root velocity function (SRVF) to be:
q(t) ≡F( ˙β(t)) =
˙β(t)

| ˙β(t)|
.
(5.4)
It is thus named because the vector q(t) has the same direction as the velocity
vector ˙β(t) but its magnitude is given by the square root of the instantaneous
speed:
|q(t)| =
| ˙β(t)|

| ˙β(t)|
=

| ˙β(t)| , and
q(t)
|q(t)| =
˙β(t)
| ˙β(t)|
.
We make a few remarks about this deﬁnition. Firstly, since F is a continuous
map, even where | ˙β(t)| = 0, the SRVF is well deﬁned. In other words, SRVF
makes sense for all absolutely continuous functions (deﬁned later). This repre-
sentation exists even for curves with singular parameterizations. In fact, this def-
inition of the SRVF is valid for curves in Rn, not just R2. (We will use it to study
the shapes of curves in higher dimensions later, in Chap. 10.) Finally, while this
SRVF can be seen as an extension of the SRSF mentioned earlier in Sect. 4.6,
from R to Rn, it is not the only possibility. In fact, qw(t) =

ρ(t)eiwθ(t), for any
positive real number w, is potentially useful in shape analysis, although some
values of w are better than others. The case of w = 0.5 has been studied in
more detail by Younes and others (see citations in bibliography). For w = 0.5,
q0.5(t) is simply a complex square root of when ˙β(t) is written as a complex
number ˙βx(t) + i ˙βy(t). To gain some insight in this representation, we iden-
tify ˙β(t) with ρ(t)eiθ(t) and obtain q0.5(t) =

ρ(t)eiθ(t)/2. There are certain
problems that arise in the exponential term. Firstly, since θ(t) can be replaced
by θ(t) ± 2nπ for any integer n, this division by 2 can lead to quite diﬀerent
answers. For instance, note that while ϵ and 2π + ϵ are the same angles, their
halves ϵ/2 and π + ϵ/2 are not the same. So, one has to be careful about this

132
5 Shapes of Planar Curves
operation. This problem can lead to a real ambiguity near the points where
θ(t) = 0. Secondly, this deﬁnition of q0.5 relies completely on complex analysis
and, therefore, is diﬃcult to extend to curves in higher dimensions.
In this setup, the SRVF (Eq. 5.4) results from the value w = 1, i.e., q(t) =

ρ(t)eiθ(t), and it does not suﬀer from the problems mentioned above. Since
the exponential term remains unchanged, there is no phase ambiguity in making
the transformation. Furthermore, the corresponding deﬁnition of q applies to
curves in higher dimensions also. Any other choice of w restricts one to planar
curves only.
If the curve β is of length l, the SRVF q satisﬁes
 b
a |q(t)|2dt =
 b
a | ˙β(t)|dt = l.
One can reconstruct the curve β from q, up to a translation, using the formula
β(t) =
 t
a q(τ)|q(τ)|dτ. In contrast to the (φ, θ) representation, the angle and
speed functions have been mixed together in this square-root velocity represen-
tation q(t) =

ρ(t)eiθ(t). Note that we are deﬁning ρ(t) = | ˙β(t)|. The number
of functions remains the same; q(t) = [qx(t) qy(t)] are still two scalar-valued
functions, but the mixing leads to two important simpliﬁcations later. Firstly,
if we restrict the length of the curve to be l, then the function q lies on a sphere
in the space of all (square-integrable) R2-valued functions on [a, b] (this is due
to the fact that  b
a |q(t)|2dt = l). Since the diﬀerential geometry of a sphere is
completely understood, we can use it explicitly to simplify the ensuing shape
analysis. Later sections will have more on that topic. Secondly, for our choice
of metric on this representation of curves, the action of the re-parameterization
group on curves will be by isometries! This will be important in deﬁning a
proper distance in shape analysis of curves.
5.3.2 Shape-Preserving Transformations
We now have a few mathematical representations of parameterized curves in R2.
For the purpose of shape analysis, we need to make our representation invariant
to all the shape-preserving transformations—translation, rotation, scaling, and re-
parameterization. By scaling the curves to be of a ﬁxed length, the scale variability
has been removed. All curves are of the same length and, therefore, are compared
fairly. Similarly, by using the velocity function ˙β, and functions derived from it,
to represent the shape of β, we have also removed the translation variability. In
other words, if β is translated in any fashion, its representative that depends only
on ˙β will not change. Thus, all representations mentioned previously, except for
(βx, βy), are already invariant to translations and uniform scalings of β. However,
the rotation transformation has not been addressed yet. A rigid rotation of β
will change its representation and that is undesirable. This will result in many
elements of the representation set, each with diﬀering rotation, having the same
shape but diﬀerent mathematical representations, with possibly nonzero distances
between them. In order to remove this redundancy and achieve a uniqueness of
representation (of shape), we want to reach a rotational invariance. This is done
diﬀerently for diﬀerent representations. In case of angle function representation,
we use the notion of a section (Sect. 5.4), but for the SRVF representation, we use
the notion of orbits (Sect. 5.5).
So far we have discussed the “standard” shape-preserving transformations—
translation, rotation, and scaling. However, due to the parametric nature of our

5.4 Pre-shape Spaces
133
representations of curves, we have introduced an extra variability that will need to
be addressed. If we re-parameterize β, within the constraints imposed on it, we get
a new parametric form whose shape remains unchanged. An example was presented
in Sect. 5.2 using Eq. 5.1. In the later part on shape analysis, we will account for
this variability using the actions of re-parameterization groups on spaces of curves.
5.4 Pre-shape Spaces
In this section we start restricting to the sets that are relevant for our shape anal-
ysis. In particular, we will focus on two of the representations—angle function
θ for arc-length parameterized curves and SRVF q for arbitrarily parameterized
curves—and will investigate their use in analyzing shapes of curves. For this dis-
cussion, we will focus on the sets of ﬁxed-length curves and study the diﬀerential
geometries of the resulting sets. These spaces are termed pre-shape spaces since
we have not accounted for all the variability generated by shape-preserving trans-
formations. For instance, in case of square-root velocity representations, there are
points in the corresponding set that are simply re-parameterizations of each other
and, hence, have the same shape. The deﬁning characteristic of pre-shape spaces
is that the shape representations have been identiﬁed but not uniﬁed. Later on,
when we reach shape spaces, we will see a uniﬁcation of all points in pre-shape
spaces that represent the same shape. Thus, shape spaces will be quotient spaces
of the pre-shape spaces.
Although many more pre-shape spaces can be constructed, we will deal with
the following two:
1. Case 1: As the simplest example of shape analysis of curves, we will consider
ﬁxed-length curves under the arc-length parameterization, each represented by
its angle function. By rescaling the curves to a ﬁxed length, we get rid of the
scale variability. Similarly, by restricting to the arc-length parameterization, we
get rid of diﬀerent parameterizations associated with curves. Loosely speaking,
these restrictions amount to forming a section of the space of curves for the
scaling and re-parameterization group actions. Let [0, 2π] be the domain of pa-
rameterization, and we will restrict to those angle functions that are absolutely
continuous on this domain. We will use the L2 metric on this set to compare
and analyze angle functions. By the virtue of arc-length parameterization over
[0, 2π], the curves represented by these angle functions are of length 2π. The
reader may notice that L2([0, 2π], R) is actually a topological completion of our
space of curves.
Orthogonal Section Under the Rotation Group: The rotation of a curve
is represented by the translation of its angle function. This translation group R
acts on L2([0, 2π], R) according to:
(θ0, θ)(s) = (θ(s) + θ0) .
We can remove this translation variability by forming the quotient space
L2([0, 2π], R)/R and studying points in that space. However, let us consider
the set:
C1 = {θ ∈L2([0, 2π], R)| 1
2π
 2π
0
θ(s)ds = π }
⊂L2([0, 2π], R).
(5.5)

134
5 Shapes of Planar Curves
The constraint on the right side ﬁxes the average orientation to be π. It can
be shown that C1 is an orthogonal section (see Deﬁnition 3.19) of the Hilbert
manifold L2([0, 2π], R) under the action of R by translation. Furthermore, since
R acts on L2 by isometries, we can form an isometric map between C1 and L2/R
(under the inherited metric). This allows us to study elements of the quotient
space by studying the corresponding elements of C1 under the L2 metric.
The set C1 is not a vector space but an aﬃne space, since it does not contain
the zero function. (An aﬃne space is a linear subspace of a vector space that has
been translated so that it does not pass through the origin.) This space has the
property that if it contains two points, then it contains the entire straight line
between the two points. Hence, the shortest geodesic between any two points is
simply the straight line connecting them.
2. Case 2: Secondly, we will study the pre-shape space of ﬁxed-length, absolutely
continuous curves under arbitrary parameterizations, using the square-root ve-
locity representations. In this case we will assume the parameter interval to be
[0, 1], with the resulting space being given by:
C2 = {q ∈L2([0, 1], R2)|
 1
0
|q(t)|2dt = 1} .
(5.6)
Here L2([0, 1], R2) = {q : [0, 1] →R2|
 1
0 |q(t)|2dt < ∞}. Recall that ∥q∥2 also
equals the length of the curve, so the constraint on the right side in Eq. 5.6
restricts the curves to be of unit length.
Orthogonal Section Under the Scaling Group: Similar to the case of the
scaling group action in Example 3.14, we can show that C2 is an orthogonal
section of the scaling group on L2([0, 1], R2). Furthermore, under the scaled-L2
metric as in Example 3.16, we can show that the action of the scaling group R×
is by isometries and construct an isometric map between the orthogonal section
C2 and the quotient space L2([0, 1], R2)/R×. Therefore, from now on we will use
the orthogonal section C2 for analyzing elements of the quotient space. Since
the scaled-L2 metric is the standard L2 metric on C2, we will continue with
the standard metric on that space. As mentioned in Example 3.14, the action
of the rotation group on the space of landmarks does not admit an orthogonal
section. The same holds here and we will have to use more general techniques
for removing the rotation group.
C2 is the set of all square-root velocity functions with L2 norm one and,
hence, is a sphere in L2([0, 1], R2). Using the Example A.11, C2 is a Hilbert
submanifold of L2([0, 1], R2) with codimension 1. Since a lot is known about
the geometry of a sphere, including geodesics, exponential map, etc, the shape
analysis of curves under this representation (without additional constraints) is
relatively simple.
At this point, it is worthwhile to point out exactly which classes of curves
have representatives in C2. This representation makes use of the ﬁrst derivative,
so the curves represented must of course satisfy some sort of diﬀerentiability
requirement. It turns out, however, that the class of curves having represen-
tatives in C2 includes curves that are not diﬀerentiable in the usual sense but
indeed satisfy the weaker condition of absolute continuity. For our purposes the
following deﬁnition will suﬃce.

5.4 Pre-shape Spaces
135
Deﬁnition 5.2. A function f is absolutely continuous if and only if there exists
an integrable function g : [0, 1] →R2 such that
f(t) −f(0) =
 t
0
g(τ)dτ,
for all t ∈[0, 1] .
This is sometimes referred to as the fundamental theorem of Lebesgue integral
calculus. Thus function g is unique up to sets of Lebesgue measure zero. Further,
in this situation, f is diﬀerentiable almost everywhere, and ˙f(t) = g(t) almost
everywhere. Note that although the result is stated for functions from an interval
[0, 1] to R2, it immediately generalizes to functions [0, 1] →Rn since integration
is a componentwise operation. Now let F2 denote the set of all functions β :
[0, 1] →R2 satisfying: (1) β is absolutely continuous, (2) β(0) = 0, and (3)
L[β] =
 1
0 ∥˙β(t)∥dt = 1. Let Q : F2 →C2 denote the map that sends each
curve to its SRVF. It is straightforward to show that Q is a bijection (left as
an exercise for the reader), with inverse given by
Q−1(q)(t) =
 t
0
q(u)|q(u)|du,
for all t ∈[0, 1] .
Thus, F2 is precisely the set of curves with representatives in C2.
We have now identiﬁed these two pre-shape spaces, denoted by C1 ⊂L2([0, 2π], R)
and C2 ⊂L2([0, 1], R2), as the spaces of interest for further analysis.
5.4.1 Riemannian Structure
An important step in our shape analysis is to deﬁne a Riemannian structure on
a shape space and to compute geodesic paths between shapes with respect to the
chosen metric. Since shape spaces are quotient spaces of pre-shape spaces, with the
Riemannian metric inherited from the larger set, the geodesics in the shape spaces
are closely related to those in the pre-shape spaces. In this section, we present our
choices of Riemannian structures on the two pre-shape spaces C1 and C2. In each
case we will establish the tangent spaces and will deﬁne inner products on these
spaces to impose Riemannian structures.
Note that both the pre-shape spaces are level sets of functionals on Hilbert
spaces. In the ﬁrst case, the Hilbert space is L2([0, 2π], R), while in the second
case it is L2([0, 1], R2). This fact can be used to characterize the tangent spaces of
these pre-shape spaces as follows. (Please refer to Sect. A.1.1 for deﬁnitions and
examples of tangents and normals to manifolds.) We start with the general notion.
Let Φ : H →Rn be a diﬀerentiable mapping, with H being one of the earlier Hilbert
spaces, and let x ∈Rn be a regular value of Φ (i.e., dΦp : H →Rn is onto for all
p ∈Φ−1(x)). By Theorem A.1, we know that M = Φ−1(x) is a submanifold of L2,
and the tangent space Tp(M) is just the kernel of dΦp for all p ∈Φ−1(x). As a
consequence, dΦp decomposes H into a direct sum: H = Tp(M) ⊕Np(M), where
Np(M) is the set of vectors normal to M at p in H. Since dΦp is surjective, the
normal space Np(M) has the dimension n and can be speciﬁed using a ﬁnite basis.
This is in contrast to Tp(M), which is an inﬁnite-dimensional space. Therefore,

136
5 Shapes of Planar Curves
in this context, it is much easier to specify the normal space as compared to the
tangent space. We now use these ideas to derive the tangent spaces of C1 and C2.
These spaces are level sets of Rn-valued functionals on Hilbert spaces, and we will
follow the practice of specifying their normal spaces in the larger Hilbert spaces.
1. Case 1: First, we verify that C1 is indeed a manifold. To do this, deﬁne a map
Φ1 : L2([0, 2π], R) →R by
Φ1(θ) = 1
2π
 2π
0
θ(s) ds .
(5.7)
Then, C1 = Φ−1
1 (π) ⊂L2([0, 2π], R). Because Φ1 is a bounded linear transfor-
mation to a ﬁnite-dimensional vector space, it is automatically smooth, and,
furthermore, it is its own derivative, i.e.,
dΦ1,θ(f) = Φ1(f) = 1
2π
 2π
0
f(s) ds = 1
2π ⟨f, 1⟩
(5.8)
Clearly, dΦ1,θ is surjective at every function θ ∈L2 (for example, dΦ1,θ(1) = 1),
which shows that C1 is a submanifold of L2 of codimension one. Furthermore,
Eq. 5.8 shows that the kernel of dΦ1,θ is simply {f ∈L2([0, 2π], R) : f ⊥1}.
It follows immediately that the normal space of C1 at θ is given by NθC1 =
span{1}, the one-dimensional vector space spanned by a constant function 1.
Hence the tangent space is given by:
Tθ(C1) = null space of dΦ1,θ
= {f ∈L2([0, 2π], R)|f ⊥Nθ(C1)},
where Nθ(C1) = all constant functions ,
= {f ∈L2([0, 2π], R)|
 2π
0
f(s)ds = 0} .
Since the tangent space Tθ(C1) is a subspace of L2, we can restrict the L2-inner
product to deﬁne a Riemannian structure on C1. For any two elements g1, g2 ∈
Tθ(C1), the Riemannian metric ⟨g1, g2⟩is given by ⟨g1, g2⟩=
 2π
0
g1(s)g2(s)ds.
There is a nice physical interpretation associated with the use of this Rieman-
nian metric. We claim that this metric, when measuring distortions in shapes
of curves, quantiﬁes the amount of bending needed to obtain one curve from
the other. Since θ(s) at the point β(s) on the curve is the angle made by the
vector ˙β(s) with the X axis, g1(s) and g2(s) denote inﬁnitesimal changes in
that angle. A change in the angle of ˙β(s) results in bending of the curve at
that point and, hence, an L2 metric for measuring changes in angle functions
is called the bending metric. This terminology does not have the same formal
deﬁnition as in physics but carries the same connotation. Refer to Fig. 5.3 for
an example where the angle function θ of the ellipse is perturbed by a small
function f ∈Tθ(C1), resulting in the bending of the ellipse. The squared L2
norm of f measures the amount of bending in the ellipse.
Using the Riemannian structure, it becomes possible to deﬁne lengths of paths
on C1 (See Sect. 3.2 for the deﬁnition). Let α : [0, 1] →C1 be a parameterized
path on C1 that is diﬀerentiable everywhere on [0, 1]. Then
dα
dτ , the velocity
vector at τ, is an element of the tangent space Tα(τ)(C1) and its length is

5.4 Pre-shape Spaces
137
0
0.2
0.4
0.6
0.8
1
−2
0
2
4
6
8
10
12
Fig. 5.3 Left panel shows the perturbation of an angle function θ (marked line) by f (broken
line) into θ + f, and the right panel shows the corresponding bending of the curve. The quantity
∥f∥2 measures the bending energy
deﬁned to be
 dα
dτ , dα
dτ

with the inner product being L2. The length of the
path α is then given by:
L[α] =
 1
0
dα
dτ , dα
dτ

dτ .
(5.9)
This is the integral of the speed along α and, hence, is the length of the whole
path α. For any two points θ1, θ2 ∈C1, one can deﬁne the distance between
them as the inﬁmum of the lengths of all smooth paths on C1 that start at θ1
and end at θ2:
dC1(θ1, θ2) =
inf
{α:[0,1]→C1|α(0)=θ1,α(1)=θ2} L[α] .
(5.10)
A path ˆα that achieves the above minimum, if it exists, is a geodesic between
θ1 and θ2 in C1. We know that C1 is an aﬃne space. As mentioned earlier, given
any two points in this space, the entire straight line joining them is still in the
space. Hence, the geodesics in C1 are simply straight lines.
2. Case 2: For the SRVF representation, we select a Riemannian structure as
follows. Given a curve represented by q ∈L2([0, 1], R2), and the tangent vectors
u, v ∈Tq(L2([0, 1], R2)), respectively, the standard L2 inner product between
u, v is deﬁned as
⟨u, v⟩=
 1
0
⟨u(t), v(t)⟩dt .
(5.11)
To begin analyzing C2, deﬁne a mapping Φ2 : L2([0, 1], R2) →R by: Φ2(q) =
 1
0 |q(t)|2dt. Then C2 can also be written as Φ−1
2 (1). To check that C2 is a mani-
fold, we need to calculate the linear transformation d(Φ2)q : L2([0, 1], R2) →R.
Hence,
d(Φ2)q(w) = d
dϵ|ϵ=0Φ2(q + ϵw) = 2
 2π
0
⟨w(t), q(t)⟩dt = 2 ⟨q, w⟩.

138
5 Shapes of Planar Curves
Clearly, d(Φ2)q is surjective for every q ̸= 0, and, hence, it is surjective for every
q ∈Φ−1
2 (1). It follows by Theorem A.2 that C2 is a submanifold of L2([0, 1], R2)
of codimension one. (In fact, it is simply the sphere of radius one centered at
the origin in this Hilbert space.) An explicit calculation of the tangent space
Tq(C2) is given by
Tq(C2) = ker(d(Φ2)q) = {w ∈L2([0, 1], R2) : ⟨q, w⟩= 0}.
From this, it follows that the normal space of C2 at q is given by
Nq(C2) = span(q),
(5.12)
i.e., the one-dimensional linear space spanned by q. Of course, this is another
way of saying that the tangent space of a sphere at a point q is the orthogo-
nal complement of the radial vector from the center to q! The metric deﬁned
in Eq. 5.11 has a nice physical interpretation in being an elastic metric, as ex-
plained later in Sect. 5.6.
Similar to the previous case, we can now discuss the geodesics on C2. Let α :
[0, 1] →C2 be a parameterized path on C2 that is diﬀerentiable everywhere on
[0, 1]. The length of this path is given by L[α] =
 1
0
 dα
dτ , dα
dτ

dτ, with the
inner product in the integral deﬁned by Eq. 5.11. We have already seen that C2
is a unit sphere in L2([0, 1], R2). Now we see that it is a Riemannian manifold
with the metric inherited from the larger Hilbert space. With this Riemannian
structure, the geodesics between points on a sphere are known to be the arcs
of great circles containing those points, where a great circle is deﬁned to be the
intersection of the sphere with any 2-dimensional plane through the origin.
5.4.2 Geodesics in Pre-shape Spaces
One of the most important tools in shape analysis is the construction of geodesic
paths. For any two given elements of a space, pre-shape or shape, we want to be
able to construct a geodesic path connecting them in that space. We start the
discussion with pre-shape spaces. For the pre-shape spaces studied in this chapter,
it is possible to write down the geodesics analytically, as discussed next.
1. Case 1: Since C1 is an aﬃne space, the geodesic between any two elements θ1
and θ2 of C1 is given by a straight line:
α(τ) = (1 −τ)θ1 + τθ2,
τ ∈[0, 1] .
(5.13)
This geodesic has a constant-speed parameterization, i.e., ∥˙α(τ)∥= constant,
and it starts from θ1 and ends at θ2. The geodesic distance between θ1 and θ2
is given by dC1(θ1, θ2) = ∥θ1 −θ2∥.
Some examples of geodesic paths between elements of C1 are presented next.
These geodesics are actually computed using angle functions as elements in
C1 for all τ, but are displayed using coordinate functions and are drawn at
convenient displacements.

5.4 Pre-shape Spaces
139
β1
β2
α(0)
α(1)
α(0.5)
Fig. 5.4 Example of a geodesic path in C1: The ﬁrst two panels show curves β1 to β2 while the
third shows the geodesic path from β1 to β2 in C1
Fig. 5.5 Comparisons of geodesics in pre-shape spaces—C1 (left) and C2 (right)—for the same
pairs of curves
Figure 5.4 shows an example of geodesic between two curves shown on the
left, at the points α(τ) for τ = 0, 1/6, 2/6, . . ., 1. Both the original curves
have a bump each but at diﬀerent locations. A look at the geodesics between
them underscores the nature of this representation. In going from one curve to
another, the deformation eﬀectively removes one bump and creates another one.
This has the appearance of bending a steel wire from one shape into another,
without allowing for any stretching or compression of the wire. This is exactly
what the bending framework suggests.
The left column of Fig. 5.5 shows a set of geodesic paths between arbitrary
open curves computed using Eq.5.13. In each row of this column the ﬁrst and
the last curves are given and intermediate curves denote equally sampled points
on the geodesic connecting those two curves in C1. Once again the geodesics
have the appearance of bending curves from one into another without allowing
for stretching.
A third set of examples is presented in Fig. 5.6. What is special here is that
the original two curves β1 and β2 are closed curves. However, these curves
are represented as elements of C1, ignoring the fact that they are closed. It
is interesting to note that the intermediate points on α(τ) are not necessarily
closed curves! This is visible in the two examples shown in this ﬁgure. The issue

140
5 Shapes of Planar Curves
α (τ)
β1
β1
β2
β2
Fig. 5.6 Examples of geodesics in C1 between closed curves. The original curves are closed, but
the intermediate curves are far from closed. The openings are highlighted using circles in some
curves
of closure is an important one and is addressed in the next chapter. Another,
even more diﬃcult, issue is that of curves crossing themselves. The question
is: even if the original curves do not cross themselves, will the geodesic pass
through curves that are self-intersecting? Experiments indicate that in case of
highly twisted shapes, the curves along the geodesics can cross themselves. This
is an issue that is not handled in this textbook.
The exponential map and its inverse on C1 are straightforward. For θ1, θ2 ∈C1,
and an f ∈Tθ1(C1), we have:
expθ1(f) = θ1 + f
exp−1
θ1 (θ2) = θ2 −θ1 .
2. Case 2: The space of all SRVFs of curves in R2 is C2, a hypersphere in
L2([0, 1], R2) with the inherited Riemannian metric. The minimal geodesics in
C2 are given by the shorter arcs on great circles. Given any two parameter-
ized curves β1 and β2 in R2, we can compute geodesic paths between their
representations q1 and q2 using Eq. 3.5 (see Example 3.4 for details):
α(τ) =
1
sin(ϑ) [sin(ϑ(1 −τ))q1 + sin(τϑ)q2]
(5.14)
where ϑ = cos−1(⟨q1, q2⟩). This geodesic starts at q1, at τ = 0, and reaches q2
at τ = 1 while traveling at a constant speed. The geodesic distance between
any two points in C2 is given by:
dC2(q1, q2) = ϑ = cos−1(⟨q1, q2⟩) .
(5.15)
A geodesic on C2 can also be characterized in terms of a tangent direction
w ∈Tq(C2):
αt(w) = cos(t∥w∥)q + sin(t∥w∥) w
∥w∥.
(5.16)
This equation gives the constant-speed parameterization of the geodesic passing
through q with velocity vector w at t = 0. According to Example 3.5, the
exponential map, exp : Tq(C2) →C2 is given by:

5.5 Shape Spaces
141
expq(w) = cos(∥w∥)q + sin(∥w∥) w
∥w∥
(5.17)
The right column of Fig. 5.5 shows examples of geodesic paths between given
in curves in C2. Recall that the left column shows the geodesic paths between
the same curves in the pre-shape space C1. (The curves have been rescaled for
display purposes. Recall that elements of C1 have length 2π while elements of
the set C2 are of length one.) Thus, we can compare the nature of geodesics for
the same curves under the two metrics. Although the diﬀerences between the
two sets of geodesics are not drastic, they are still perceptible in some cases. It
can be seen that for geodesics in C1, the corners and bends in the intermediate
shapes are smoother than those for shapes along geodesics in C2. For example,
in the topmost example, the corner remains sharper on the right side, as it
deforms from one shape to other.
For any q2 ∈C2, the inverse of the exponential map at q1 ∈C2, denoted by
exp−1
q1 : C2 →Tq1(C2), is computed as follows: exp−1
q1 (q2) = v, where:
v =
ϑ
sin(ϑ)(q2 −cos(ϑ)q1),
where ϑ = cos−1(⟨q1, q2⟩) .
(5.18)
5.5 Shape Spaces
Of the diﬀerent shape-preserving transformations—translation, rotation, scale, and
re-parameterization—we have taken care of translation and scale in our repre-
sentations. Additionally, the angle-function representation removes the rotation
variability by ﬁxing the average angle associated with a curve. The next step is
to take care of variability introduced by diﬀerent re-parameterizations of curves;
the re-parameterization of curves was introduced in Sect. 5.2. We will also ad-
dress the eﬀect of diﬀerent rotations in the SRVF representation. A mathemati-
cally elegant and convenient way to account for these variabilities is to deﬁne a
re-parameterization group (and a rotation group, where needed) and to use the
actions of these groups on pre-shape spaces established earlier. The orbits gener-
ated by such group actions will contain curves with the same shapes and, thus,
will deﬁne equivalence classes of shapes. The resulting quotient spaces, preshape
spaces modulo the re-parameterization group, will form the shape spaces for our
analysis. Next we elaborate on this basic framework.
5.5.1 Removing Parameterization
We consider the two representations in order:
1. Case 1: This case is easy since the curves have been assumed to be arc-length
parameterized, and, hence, no re-parameterization of a curve is possible. Ad-
ditionally, we have ﬁxed the rotation of a curve by imposing a constraint on
its average angle function. With no variability to remove, the shape space is

142
5 Shapes of Planar Curves
identical to the pre-shape space, S1 = C1. All the previous discussions about
the geometry, the Riemannian metric, and the construction of geodesics on the
pre-shape space C1 apply verbatim to the shape space S1. Consequently, we
have dS1(θ1, θ2) = dC1(θ1, θ2).
2. Case 2: The task of unifying re-parameterization and deﬁning equivalence
classes is more complicated when arbitrary parameterizations are included in
the representation. In this case where the curves are represented using their
SRVFs, the variability introduced by having diﬀerent parameterizations needs
to be addressed.
Let ΓI denote the set of all orientation-preserving diﬀeomorphisms [0, 1] →
[0, 1], as introduced in Sect. 4.3.3. (“orientation-preserving” means the diﬀeo-
morphism preserves direction, i.e., 0 maps to 0 and 1 maps to 1.) As described
in Sect. 5.2, the re-parameterization of a curve β : [0, 1] →R2 by a γ ∈ΓI is
given by β ◦γ. In terms of the SRVFs, what is the eﬀect of re-parameterization?
Similar to the case of SRSFs in Chap. 4, it is given by the following group
action. Deﬁne: C2 × ΓI →C2 by
(q, γ) →(q ◦γ)

˙γ .
(5.19)
The reason this is the correct action of ΓI is as follows. Suppose we are given
two curves β1 and β2, where each βi : [0, 1] →R2. Assume they are related
by an element γ ∈ΓI, so β2 = β1 ◦γ. If q1 and q2 are the corresponding
representative functions, what is the relation between q1 and q2? First, recall
that q1(t) = ˙β1(t)/

| ˙β1(t)|. Likewise, q2(t) = ˙β2(t)/

| ˙β2(t)|. Using the chain
rule, we then compute that
q2(t) =
˙β1(γ(t))˙γ(t)

| ˙β1(γ(t))˙γ(t)|
=
˙β1(γ(t))˙γ(t)

| ˙β1(γ(t))|

˙γ(t)
=
˙β1(γ(t))

| ˙β1(γ(t))|

˙γ(t)
= q1(γ(t))

˙γ(t).
Thus, we have justiﬁed the action of ΓI deﬁned above.
Similarly, the rotations of a curve can be represented by the action of SO(2):
SO(2) × C2 →C2:
(O, q) →{t →Oq(t)} .
(5.20)
How do the two group actions interact with each other?
Lemma 5.1. The actions of SO(2) and ΓI on C2 commute.
Proof. If we apply a rotation O and a re-parameterization γ to a curve β, the
SRVF of the resulting curve ˜β(t) = Oβ(γ(t)) becomes:
˜q(t) =
˜β(t)

| ˙˜β(t)|
=
O ˙β(γ(t))˙γ(t)

|O ˙β(γ(t))˙γ(t)|
= O
β(γ(t))

| ˙β(γ(t))|

˙γ(t) = O(q ◦γ)(t)

˙γ(t) .
The last term is the same if we apply γ and O in any order. Hence, the two
group actions commute.
⊓⊔
This provides the equivalence class, or orbit, associated with a curve q under
the actions of ΓI and SO(2):

5.5 Shape Spaces
143
[q] = {Oq(γ(t))

˙γ(t)|(γ, O) ∈ΓI × SO(2)} .
(5.21)
The shape space, using the SRVF representation, is deﬁned by:
S2 = C2/(ΓI × SO(2)) .
(5.22)
As discussed previously in Sect. 4.10, there are some theoretical diﬃculties with
this quotient construction. We would like to use the basic fact that if a compact
Lie group G acts freely (i.e., no elements of M are ﬁxed by g ∈G unless g is the
identity) on a Riemannian manifold M by isometries, and the orbits are closed,
then the quotient M/G is a manifold and inherits a Riemannian metric from
M. The trouble is that while we have the product group ΓI × SO(2) acting by
isometries, the orbits are not closed. This because the space of diﬀeomorphisms
is not compact and is not even closed with respect to either the L2 or the Palais
metric, since a sequence of diﬀeomorphisms might approach a map that is not
a diﬀeomorphism under either of these two metrics. Please refer to Sect. 4.10
for a discussion of this issue in the case of real-valued functions.
Similar to Sect. 4.10, we resolve this diﬃculty by using the closures of ΓI-
orbits, rather than by ΓI-orbits themselves. (This step is related to the fact
that, if q ̸= 0 almost everywhere, then the closure of a ΓI-orbit of q is actually
the orbit of q under the monoid +
ΓI of absolutely continuous, weakly-increasing
functions.) Thus, if there is a sequence qi in the orbit [q1], and this sequence
converges to a function ˜q in C2 (with respect to the L2-metric), then we identify
q1 with ˜q in this quotient construction. As evidence that this method of resolving
the problem has merit, it is not diﬃcult to prove that in this situation, if we
let β1 and ˜β be the curves corresponding to q1 and ˜q, both β1 and ˜β contain
exactly the same points. (This is assuming that we normalize both of these paths
to start at the same point.) We will henceforth refer to the quotient space as
C2/(ΓI × SO(2)), and suppress the fact that we have to take the closure of the
orbits. Although this quotient space will not be a Riemannian manifold, it can
still be treated as a metric space with the distance inherited from C2.
Lemma 5.2. The action of the product group ΓI × SO(2) on C2 is by isometries
with respect to the chosen metric.
Proof. For a q ∈C2, let u, v, ∈Tq(C2). Since ⟨Ou(t), Ov(t)⟩= ⟨u(t), v(t)⟩, for all
O ∈SO(2) and t ∈[0, 1], the proof for SO(2) follows. Now, ﬁx an arbitrary element
γ ∈ΓI and deﬁne a map φ : C2 →C2 by φ(q) = (q, γ). A glance at the formula
for (q, γ) conﬁrms that φ is a linear transformation. Hence, its derivative dφ has
the same formula as φ. In other words, the mapping dφ : Tq(C2) →T(γ,q)(C2) is
given by: u →˜u ≡(u ◦γ)√˙γ. The Riemannian metric after the transformation is:
⟨˜u, ˜v⟩=
 1
0
⟨˜u(t), ˜v(t)⟩dt
=
 1
0
$
u(γ(t))

˙γ(t), v(γ(t))

˙γ(t)
%
dt =
 1
0
⟨u(r), v(r⟩dr,
r = γ(t) .
Putting these two results together, the joint action of ΓI × SO(2) on C2 is by
isometries with respect to the chosen metric.
⊓⊔

144
5 Shapes of Planar Curves
The lemma just proved is perhaps the most important reason for using the
SRVF representation of curves.
5.6 Motivation for SRVF Representation
The main representation put forward for analyzing shapes of curves in this text-
book is the SRVF. It is actually the main representation proposed for an elastic
analysis of shapes of curves. So a natural question is: What is the fundamental
need for this peculiar representation? Another question that is closely related is:
What is an elastic analysis of shapes? In this section we try to answer these two
and some other questions about choices of representations and metrics. As we shall
see, there are three main reasons for selecting the SRVF representation in shape
analysis of curves. These are:
1. The re-parameterization group, under the elastic metric, acts by isometries on
the representation space of SRVFs.
2. If one uses an elastic metric for shape analysis of curves, then, under the SRVF
representation, the elastic metric reduces to the simple L2 metric.
3. The space of SRVFs of ﬁxed-length curves is a unit sphere under the L2 metric.
All these three points are fundamental and interrelated. We explain these items
next, starting with a deﬁnition of the elastic metric.
5.6.1 What Is an Elastic Metric?
In elastic analysis of shapes, we consider deformations of curves resulting not just
from bending but from stretching as well. The amount of deformation is quantiﬁed
using a Riemannian metric called the elastic metric. Before we present that metric,
we discuss the motivation behind it in general terms. Consider the two curves
shown in Fig. 5.8a; each of these has two bumps but these bumps have slightly
diﬀerent shapes and placements on the two curves. If we have to deform the top
curve into the bottom curve, what is a good way of doing it? It is clear that the
deformation could be eﬃcient if it matches the corresponding bumps on the two
curves—ﬁrst bump on the ﬁrst curve to the ﬁrst bump on the second curve, and
the same for the second bumps. This way we can simply modify the shapes and
locations of the bumps from one to another without creating or destroying these
bumps. This matching requires compressing the long horizontal piece between the
two bumps on the top curve into a shorter piece present on the bottom curve.
Also, since the two curves have the same length, some part of the ﬁrst curve has
to be stretched to compensate for this compression. This motivation is similar
to that presented in Sect. 4.4 where peaks and valleys are to be matched across
real-valued functions. Figure 5.8b shows this desired matching of points across
the two curves—the bumps register with the bumps and the horizontal pieces
register with the horizontal pieces. Not only does this matching seem intuitively
eﬃcient but the actual path of deformation, from one curve to the other, also
looks more natural, as shown in Fig. 5.9a. Notice that this deformation preserves

5.6 Motivation for SRVF Representation
145
b1 (arc-length)
b2 (arc-length)
b2 (matching)
Fig. 5.7 Registration of points across two curves using the arc length and a convenient nonuni-
form sampling. Nonuniform sampling allows a better matching of features between β1 and β2
0
20
40
60
80
100
10
20
30
40
50
60
70
80
90
100
a
b
c
Fig. 5.8 Illustration of elastic metric. In order to compare shapes of the two curves shown in
(a), some combination of bending and stretching seems eﬃcient. The elastic metric measures the
amounts of these deformations in a way that is invariant to the parameterizations of curves. An
optimal matching of points along the curves, under a speciﬁc elastic metric, and the resulting
optimal matching function, are shown in (b) and (c), respectively
the two bumps in all the curves along the deformation path while simultaneously
changing their shapes and positions. This is termed an elastic deformation. On
the other hand, if one uses a non-elastic metric, one based completely on bending
one curve into the other, the resulting deformation will look unnatural, as shown
in Fig. 5.9b. The non-elastic path shows additional bends introduced in the curves
since the algorithm is trying to bend the ﬁrst curve into another and the bumps
are located at diﬀerent points along the curves. This non-elastic deformation is
computed using Eq. 5.13, i.e., linear interpolation of angle functions of the two
curves (under the arc-length parameterization).
As another motivation, consider the two human silhouettes shown in the left
two panels of Fig. 5.7. Each one is parameterized by arc length, a fact highlighted
by uniformly placing points along those curves. If we start from the top of the head
and walk down the left side on those two curves, the corresponding points fall on
the same features until we reach the right side of the bodies. By the time we reach
the right hand, the tip of the hand in the left panel corresponds to the armpit of
the second panel. This is another example to highlight the fact that arc length, or
any other pre-determined parameterization, does not provide natural matching of
features across curves. The right panel shows a nonlinear parameterization of the
second silhouette where the points are perfectly matched with the left panel.
This example shows that a central step in the process of elastic deformation is
elastic matching. We explain this important point further. Consider again the two
curves shown in Fig. 5.8a. Let us ﬁx the parameterization of the top curve to be

146
5 Shapes of Planar Curves
arc length. That is, we are going to traverse that curve with speed equal to one.
The question is: At what rate should we move along the second curve so that the
matching points, the points reached on the two curves at the same times, are as
close as possible under some geometric criterion? For this to happen, the rate of
traversal on the second curve should be such that we reach the peaks and valleys
on both the curves at the same time. A function that accomplishes this is shown
in Fig. 5.8c. This is an increasing function whose derivative at any point gives the
instantaneous speed of traversal on the second curve. This curve has a higher slope
on the two corners but is very ﬂat in the middle, denoting a slowing down in the
middle when traversing the second curve. This is, of course, justiﬁed because, in
order to match the horizontal pieces in the middle across curves, we have to travel
slowly on the second curve. This rate function incorporates the desired matching
between curves and any deformation that respects this matching will necessarily
be an elastic deformation.
Now we make these ideas more precise using a “polar” representation of curves.
Let β : [0, 1] →R2 be a curve in R2 and assume that for all t ∈[0, 1], ˙β(t) ̸= 0.
Recall the log-speed, angle representation introduced earlier in Sect. 5.3.1. Here
φ : [0, 1] →R is a function deﬁned as φ(t) = log(| ˙β(t)|), and θ : [0, 1] →R
is deﬁned such that ˙β(t)/| ˙β(t)| = eiθ(t), i = √−1. (The complex plane is being
identiﬁed with R2 in the standard way for this comparison.) Clearly, φ and θ
completely specify ˙β, since for all t, ˙β(t) = eφ(t)eiθ(t). Thus, we have deﬁned a
map from the space of planar curves to Φ × Θ, where Φ = {φ : [0, 1] →R} and
Θ = {θ : [0, 1] →R}. This map is surjective; it is not injective, but two curves are
mapped to the same pair (φ, θ) if and only if they are translates of each other, i.e.,
if they diﬀer by an additive constant. Intuitively, φ tells us the (log of the) speed
of traversal of the curve, while θ tells us the direction of the curve at each time
t. We should note here that the curve β does not completely specify the function
θ; if we add any integer multiple of 2π to θ, it will not change the original curve.
However this ambiguity will not in any way interfere with the Riemannian metric
that we are using on θ and φ.
In order to quantify the magnitudes of perturbations of β (and enable ourselves
to do geometry on the space of these curves), we wish to impose a Riemannian
metric on the space of curves that is invariant under translation, and we will do
this by putting a metric on Φ × Θ. First, we note that the tangent space of Φ × Θ
at any point (φ, θ) is given by
T(φ,θ)(Φ × Θ) = {(u, v) : u ∈Φ and v ∈Φ} = Φ × Θ .
Suppose (u1, v1) and (u2, v2) are both elements of T(φ,θ)(Φ × Θ). Let a and b be
positive real numbers.
Deﬁnition 5.3 (Elastic Riemannian Metric for Planar Curves). For every
point (φ, θ) ∈(Φ × Θ), deﬁne an inner product on the tangent space T(φ,θ)(Φ × Θ)
as
⟨(u1, v1), (u2, v2)⟩(φ,θ) = a2
 1
0
u1(t)u2(t)eφ(t) dt+b2
 1
0
v1(t)v2(t)eφ(t) dt. (5.23)
This deﬁnes a Riemannian metric on the manifold Φ × Θ.
This metric has the interpretation that the ﬁrst integral measures the amount
of “stretching,” since u1 and u2 are variations of the log-speed φ of the curve,

5.6 Motivation for SRVF Representation
147
Fig. 5.9 Deformations
between curves using (a)
elastic and (b) non-elastic
matchings
a
b
while the second integral measures the amount of “bending,” since v1 and v2 are
variations of the angle function θ of the curve. The constants a2 and b2 are weights
that we choose depending on how much we want to penalize these two types of
deformations. Therefore, this inner product has been called the elastic metric.
Notice, this is not just one metric but rather a family of metrics parameterized by
a and b.
Perhaps the most important property of this Riemannian metric is that the
groups SO(2) and ΓI both act by isometries. To elaborate on this, recall that O ∈
SO(2) can be identiﬁed with an angle θ0 ∈S1, and, in our current notation, it acts
on a curve β by (θ0, β)(t) = eiθ0β(t), and γ ∈ΓI acts on β by (β, γ)(t) = β(γ(t)).
Using our identiﬁcation of the set of curves with the space Φ × Θ results in the
following actions of these groups. θ0 ∈SO(2) and γ ∈ΓI act on (φ, θ) by
(θ0, (φ, θ)) = (φ, θ0 + θ)
(5.24)
(γ, (φ, θ)) = (φ ◦γ + log ◦˙γ, θ ◦γ) .
(5.25)
We now need to understand the diﬀerentials of these group actions on the tangent
spaces of Φ × Θ. SO(2) is easy, since each θ0 ∈S1 acts by an additive translation,
so its diﬀerential is the identity. The action of γ ∈ΓI given in Eq. 5.24 is not linear,
but aﬃne linear, because of the additive term (log ◦˙γ). Hence, its action on the
tangent space is the same, but without this additive term: (γ, (u, v)) = (u◦γ, θ◦γ),
where (u, v) ∈T(φ,θ)(Φ×Θ), and (u◦γ, θ ◦γ) ∈T(γ,(φ,θ))(Φ×Θ). Combining these
actions of SO(2)(≡S1) and ΓI with the above inner product on Φ × Θ, it is an
easy veriﬁcation that these actions are by isometries, i.e.,
⟨(θ0, (u1, v1)), (θ0, (u2, v2))⟩(θ0,(φ,θ)) = ⟨(u1, v1), (u2, v2)⟩(φ,θ)
⟨(γ, (u1, v1)), (γ, (u2, v2))⟩(γ,(φ,θ)) = ⟨(u1, v1), (u2, v2)⟩(φ,θ).
(5.26)
This result is related to Lemma 5.2.
Since we have identiﬁed the space of planar curves with Φ × Θ, we may iden-
tify the space of planar shapes with the quotient space (Φ × Θ)/(ΓI × SO(2)).
Furthermore, since these group actions are by isometries with respect to all the
metrics we have introduced above (no matter what values we assign to a and b),

148
5 Shapes of Planar Curves
we get a corresponding two-parameter family of metrics on the quotient space
(Φ × Θ)/(ΓI × SO(2)). Note that in distinguishing between the structures (for
example, geodesics) associated with these metrics, only the ratio of b to a is im-
portant, since if you multiply both by the same real number you just rescale the
metric, which results in the same geodesics and all distances multiplied by the same
constant. Given two shapes, we can ﬁnd the geodesics between the corresponding
elements of (Φ × Θ) with respect to any of these metrics (i.e., with respect to any
choice of a and b). The choice of relative weights will determine whether these
deformations are biased toward allowing more stretching or more bending.
5.6.2 Signiﬁcance of the Square-Root Representation
Now that we have established a family of elastic Riemannian metrics, dictated by
the ratio of weights a and b, and have motivated their use in shape comparisons,
we ask the next logical question: Is there a particular choice of weights that will be
especially natural and that will result in the geodesics being easier to compute? We
now show that the SRVF representation is a natural answer to this question. Recall
that the SRVF is the function q : [0, 1] →R2 given by q(t) =
˙β(t)
√
| ˙β(t)|. Relating this
to the (φ, θ) representation of the curve gives q(t) = e
1
2 φ(t)

cos(θ(t))
sin(θ(t))

. A couple of
simple diﬀerentiations show that if (u, v) ∈T(φ,θ)(Φ × Θ), then the corresponding
tangent vector f to L2([0, 1], R2) at q is given by
f = 1
2e
1
2 φu
cos(θ)
sin(θ)

+ e
1
2 φ
−sin(θ)
cos(θ)

v .
Theorem 5.1. The L2 metric on the space of SRVFs corresponds to the elastic
metric on Φ × Θ with a = 1
2 and b = 1.
Proof. Let (u1, v1) and (u2, v2) denote two elements of T(φ,θ)(Φ × Θ), and let f1
and f2 denote the corresponding tangent vectors to L2([0, 1], R2) at q. Computing
the L2 inner product of f1 and f2 yields: ⟨f1, f2⟩=
 1
0
 1
2e
1
2 φu1
 cos(θ)
sin(θ)

+ e
1
2 φ
 −sin(θ)
cos(θ)

v1

,
 1
2e
1
2 φu2
 cos(θ)
sin(θ)

+ e
1
2 φ
 −sin(θ)
cos(θ)

v2

dt
=
 1
0
1
4 eφu1u2 + eφv1v2 dt .
(5.27)
This is the same as the right side of Eq. 5.23 with a = 1
2 and b = 1. This implies
that the L2 metric on the space of SRVF representations (i.e., q-functions) cor-
responds precisely to the elastic metric on Φ × Θ, with those values of a and b.
⊓⊔
There are several advantages of using this representation of curves for elastic
shape analysis. Firstly, expressed in terms of the q-functions, the L2-metric is
the “same” at every point of L2([0, 1], R2) (it is simply
 1
0 ⟨f1, f2⟩dt and does
not depend on the q-function at which these tangent vectors are deﬁned), and

5.7 Geodesic Paths in Shape Spaces
149
we will thus have available more eﬃcient ways of computing geodesics in our pre-
shape and shape spaces. Secondly, under this representation and the elastic metric,
the pre-shape space C2 is simply a unit hypersphere. This further simpliﬁes the
geodesic calculations to the extent that analytical formulas are available.
Connections with Fisher-Rao Metric Interestingly, the elastic metric in
Eq. 5.23 is intimately connected to the extension of the nonparametric Fisher-
Rao metric given in Chap. 4 for scalar-valued functions. In case of curves in R1,
the second term disappears and this metric is identical to the metric discussed in
the previous chapter. Conversely, the elastic metric in Eq. 5.23 can be viewed as
an extension of the framework in Chap. 4 to curves in R2 (and higher dimensions).
We have chosen the SRVF q(t) = e
1
2 φ(t)

cos(θ(t))
sin(θ(t))

to represent a curve β
for elastic shape analysis. This representation provides certain advantages when
using the elastic metric with a = 1
2 and b = 1. Are there other representations
that provide similar advantages? It turns out that there is a whole family of rep-
resentations, namely, qb = e
1
2 φ

cos(bθ(t))
sin(bθ(t))

, under which (i) the elastic metric
becomes the L2-metric, (ii) the space of qb functions is a subset of L2, and conse-
quently (iii) the geodesics between shapes of curves are easy to establish. The re-
parameterization group continues to act on the corresponding spaces by isometries
(this can be established as earlier). It is remarkable that this desirable structure
is present for any real value of b. This implies that the geodesic computations can
be simpliﬁed for any ratio of the two terms in the elastic metric. Now, the speciﬁc
choice of b = 1/2 is even more special; it results in an additional structure that
is very useful for studying closed curves. It can be shown that the space of q1/2
functions representing unit-length, planar, closed curves is a Stiefel manifold. We
will return to this issue when we discuss the shape analysis of closed curves.
5.7 Geodesic Paths in Shape Spaces
So far we have studied two pre-shape spaces of parameterized curves, have im-
posed Riemannian structures on them, and have computed geodesic paths under
those metrics. Now we consider the problem of computing shortest paths in the
corresponding shape spaces, under the inherited metrics. We will use the frame-
work discussed in Sect. 3.6 that uses the inherited distance (Deﬁnition 3.16) for
constructing geodesics in a quotient space. It should be noted that the shape space
may not be a manifold, as discussed earlier in Sect. 4.10, but we can still impose
a distance on it and talk about shortest paths, i.e., paths that achieve those dis-
tances. We will call these paths geodesics even though they do not originate from a
Riemannian structure on the quotient space. We remind the reader of the general
setup. According to Deﬁnition 3.17, if a group G acts on a Riemannian manifold
M by isometries and the orbits under G are closed, then the inherited distance on
M/G is given by
dM/G([p1], [p2]) =
inf
r1∈[p1],r2∈[p2] dM(r1, r2) =
inf
r2∈[p2] dM(p1, r2) .
If there exists a shortest path between [p1] and [p2] in the quotient space M/G,
then this path may be obtained by forming geodesics (in M) between all possible
crosspairs in the sets [p1] and [p2], and selecting the shortest. If no shortest path

150
5 Shapes of Planar Curves
Fig. 5.10 A cartoon il-
lustration of computing
geodesics in the quotient
space M/G. We keep one
point r1 ∈[p1] ﬁxed and
search over the second orbit
[p2] to ﬁnd the shortest
geodesic. This geodesic is
perpendicular to all the
orbits it intersects
r1
r2
r2∗
r1∗
[p1]
[p2]
M
M/G
exists in M/G, then this procedure will only yield a path in M/G whose length
is within a small ϵ of dM/G([p1], [p2]). Since the algorithms we are aiming at are
numerical, we will be content with the approximate nature of this solution. Because
the group G acts by isometries, we may ﬁx p1 in the ﬁrst orbit, and search over
all r2 ∈[p2] to ﬁnd the shortest path (again, within ϵ). The geodesic between
p1 and r∗
2, the optimal point on the second orbit, is perpendicular to all the G-
orbits that it meets (if these orbits are submanifolds). This last property can
be instrumental in our search for the shortest geodesic. A cartoon illustration
of this idea is presented in Fig. 5.10. The roles of M and M/G are played by
the pre-shape and shape spaces, respectively. Summarizing this idea, the tool for
computing geodesics in shape spaces between two orbits is based on repeatedly
applying the geodesic computation in the pre-shape space in order to reach the
shortest geodesic between all pairs formed by elements of the two orbits.
There are two cases to investigate—one for angle function representations and
one for the SRVF representations. In the former case, the pre-shape space C1 is
identical to the shape space S1 and need not be repeated. This leaves only the
SRVF representation under the elastic metric.
As described earlier, the orbits of SO(2)×ΓI in pre-shape space C2 are given by
[q] = closure{O

˙γ(q ◦γ)|(γ, O) ∈ΓI × SO(2))} .
The resulting shape space is a collection of these orbits:
S2 = {[q] : q ∈C2} .
The shape space S2 is a metric space with the distance inherited from the pre-
shape space C2. This distance between any two orbits [q1] and [q2] is given by
dS2([q0], [q1]) =
inf
˜q0∈[q0],˜q1∈[q1] dC2(˜q0, ˜q1)
=
inf
γ1∈˜ΓI,(γ2,O)∈˜ΓI×SO(2)
dC2((q0, γ1), O(q1 ◦γ2)

˙γ)
=
inf
(γ,O)∈ΓI×SO(2) dC2(q0, O(q1 ◦γ)

˙γ) ,
(5.28)

5.7 Geodesic Paths in Shape Spaces
151
where dC2 is the distance function in the pre-shape space, which, in turn, is given by
Eq. 5.15. (The last equality comes from the same approximation arguments as those
used earlier in Sect. 4.7.3.) Thus, computing ds requires solving an optimization
problem over the space ΓI × SO(2). Once the optimal pair (γ∗, O∗) is known, the
actual geodesic between [q1] and [q2] in S2 is given by [αt], where αt is the geodesic
in C2 between q1 and
 ˙γ∗O∗(q2 ◦γ∗) (using Eq. 5.14):
α(τ) =
1
sin(θ) [sin(θ(1 −τ))q1 + sin(τθ)O∗(q2 ◦γ∗)]
(5.29)
where θ = cos−1(⟨q1, O∗(q2 ◦γ∗)⟩).
At this point, we pause to ask a fundamental question: for a given pair of orbits,
[q1] and [q2], does there necessarily exist an optimal element (γ∗, O∗) ∈ΓI ×SO(2)
such that the distance dC2(q1,
 ˙γ∗O∗(q2 ◦γ∗)) realizes the inﬁmum in the above
equation? Since equivalence classes [·] are deﬁned using closures of the orbits under
ΓI ×SO(2), the optimal points may actually be on the boundaries of these classes.
However, the numerical procedures we will use for this optimization will provide
points in ΓI×SO(2) that are arbitrarily close to the optimal points. For example, in
certain cases, the optimal matching may be achieved by collapsing a whole section
of the ﬁrst curve to a single point of the second curve. Clearly the corresponding
re-parameterization would not be a diﬀeomorphism. In cases like this, our methods
will produce a correspondence that matches a large section of the ﬁrst curve to a
very small section of the second curve, rather than a single point.
So, the remaining problem is to solve the joint optimization on ΓI × SO(2).
For each of the components—rotation and re-parameterization—there exist cer-
tain optimization techniques that can be applied here. Our strategy is to develop
algorithms for optimizing over each of these groups individually and then to iterate
between the individual solutions. A closer look at that distance function reveals
the following:
argmin
γ∈ΓI,O∈SO(2)
cos−1 $
q1,

˙γO(q2 ◦γ)
%
=
argmin
γ∈ΓI,O∈SO(2)
∥q1 −

˙γO(q2 ◦γ)∥2 ,
(5.30)
where the last norm is simply the L2 norm on the space L2([0, 1], R2). This equality
says that minimizing the arc length on a sphere is the same as minimizing the
square of the chord length. If one is minimized then so is the other. Therefore, for
the purpose of ﬁnding the minimizer, we can use the L2 norm, and that opens up
the possibility of a computationally eﬃcient solution. Now we have the problem:
(γ∗, O∗) =
argmin
γ∈ΓI,O∈SO(2)
∥q1 −

˙γO(q2 ◦γ)∥2
=
argmax
γ∈ΓI,O∈SO(2)
$
q1,

˙γO(q2 ◦γ)
%
.
(5.31)
This uses the fact that ∥O(q2 ◦γ)√˙γ∥= ∥q2∥.
First, we consider the optimization over SO(2). For a ﬁxed γ ∈ΓI, the opti-
mization problem in Eq. 5.31 over SO(2) is solved as shown in Example 3.13. Let
˜q2 = √˙γ(q2 ◦γ) and deﬁne:
O∗= argmax
O∈SO(2)
⟨q1(t), O˜q2(t)⟩dt

152
5 Shapes of Planar Curves
=
⎧
⎨
⎩
UV T
if
det(A) > 0
U
1 0
0 −1

V T
otherwise
(5.32)
Here UΣV T = svd(A), for A =
 1
0 q1(t)˜qT
2 (t)dt. The more diﬃcult problem of
optimizing over the re-parameterization group ΓI is described in the next section.
5.7.1 Optimal Re-Parameterization for Curve Matching
For the current rotation O ∈SO(2), let ˜q2 = Oq2 and deﬁne a cost function:
H : ΓI →R≥0 by
H(γ) =
 1
0
∥q1(t) −

˙γ(t)˜q2(γ(t))∥2dt .
(5.33)
Our goal is to ﬁnd a minimum of H in ΓI. There are at least two ways of solving
this problem. One is to use the dynamic programming algorithm (DPA) given in
Appendix B and another is to use a gradient approach. The use of DPA seems bet-
ter in this problem, both for getting an optimal solution and from a computational
perspective. Therefore, we will focus on that approach ﬁrst.
Since H is additive over the path (t, γ(t)), the DPA can be used to solve the
minimization problem in Eq. 5.33. A standard version of the DPA has already
been described in Appendix B and need not be repeated here. Instead, we will
just provide a short discussion on its implementation. This algorithm is based on
forming an N × N square-grid in [0, 1]2 and approximating re-parameterization
(i.e., γ) functions by piecewise linear paths through the grid points. Furthermore,
we restrict to those paths that (1) start at (0, 0), (2) end at (1, 1), and (3) have
positive slope p/q, where p and q are positive integers, and an upper bound has
been chosen for p and q. The DPA provides an iterative mechanism for searching
over all such paths and results in an approximation to the optimal γ. We will
denote that estimated piecewise linear graph by γ∗:
Figure 5.11 shows an example of minimizing the cost function H in Eq. 5.33
using the DPA. In this ﬁgure, panels (a) and (b) show the two curves β1 and β2
that are represented by their respective SRVFs q1 and q2 in Eq. 5.33. Shown in
panel (d) is the optimal re-parameterization γ∗of the second curve that minimizes
the function H. Another way to show this function is to mark the corresponding
points on the two curves β1(t) and β2(γ∗(t)); this is shown in panel (c).
5.7.2 Geodesic Illustrations
The computations of geodesic paths in shape spaces are illustrated here using
simple examples. We start with Fig. 5.12 that shows a geodesic path between two
curves we have seen earlier in Fig. 5.4. This time the geodesic is computed between
their representations in S2. In the top row we plot the two curves. In the bottom
row, ﬁrst we display the geodesic path between these two curves in the non-elastic

5.7 Geodesic Paths in Shape Spaces
153
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
a
b
c
d
1
Fig. 5.11 (a) and (b) show the curves β1 and β2, (c) shows the optimal correspondence between
points on two curves, and (d) shows the optimal γ∗
Given curves β1 and β2.
0
0.2 0.4 0.6 0.8
1
0
0.2
0.4
0.6
0.8
1
Geodesic in
Geodesic in
γ ∗
2
1
Fig. 5.12 Examples of geodesic paths in S1 and S2
space S1. Next, we see the corresponding example in the shape space S2, and,
ﬁnally, we see the optimal registration between the two curves in two diﬀerent
ways. The ﬁrst of the two panels uses thin lines to connect β1(t) with β2(γ∗(t))
for some t while the right panel shows a plot of γ∗itself. These results suggest
that the matching of points across the curves is nonlinear; some intervals have
been stretched and others have been compressed in order to better match the
points. The result, as seen in the geodesic path, is that one curve is elastically
deformed into the other curve. This eﬀect is further highlighted when we compare
the geodesic between the same two curves in S1. This comparison clearly shows
the bending-only nature of the deformation for geodesics in S1 and the elastic
nature of the deformations in S2.

154
5 Shapes of Planar Curves
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Fig. 5.13 Examples of geodesic paths between shapes. The left part shows the geodesic while
the right part shows the optimal γ∗function used in ﬁnding that geodesic
Fig. 5.14 Comparisons of geodesics between the same two curves in spaces S1 (top) and S2
(bottom), respectively
Figures 5.13 and 5.14 show some additional examples of these geodesics. Fig-
ure 5.13 shows geodesic paths between pairs of similar shapes in S2. Although the
original shapes are closed curves, we treat them as elements of S2 (without the
closure condition, which is handled in the next chapter). As a point of comparison,
Fig. 5.14 shows geodesic paths between the same pair of shapes in both S1 and
S2.
Figure 5.15 shows an example of a geodesic path between two face proﬁles.
These proﬁles belong to the same person but with two diﬀerent facial expressions:
one with the mouth closed and other with the mouth open. The two individual
proﬁles are simply the contours of the faces looking sideways and are shown in
the leftmost panel. This panel also shows the optimal matching between the two
faces obtained using the DPA. It is interesting to note that parts in one proﬁle
match the corresponding parts in the second, even though the second proﬁle has
the mouth open. The nose matches the nose, the lips match the lips, and so on. A
geodesic path between them in S2 is shown in the middle panel and the optimal
re-parameterization γ∗is shown in the right panel. This γ∗shows that we need
to stretch the ﬁrst face in only one place—around the lips—with a jump in the
corresponding location of γ∗and the rest of this function is quite linear.
To demonstrate a simple application of these geodesic distances in a practical
application, we consider the problem of analyzing signatures. While the signatures

5.7 Geodesic Paths in Shape Spaces
155
0
20
40
60
80
100
10
20
30
40
50
60
70
80
90
100
a
b
c
Fig. 5.15 Elastic geodesics between two facial proﬁles in S2: left panel shows two facial proﬁles,
middle panel shows the geodesic path between them on S2, and right panel shows the optimal γ
0
2
4
6
0
1
2
3
4
5
6
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
Fig. 5.16 Top two rows: elastic geodesics between two handwritten letters in S2. Each row
shows the geodesic path between the letters, the corresponding points, and the optimal γ. Bottom
row: elastic matching and geodesic between two words
are quite complicated, consisting of several letters and symbols, we take some
individual letters of a person’s signature and study their shape variations. Using
the methods described in this section, we compute geodesics between each pair in
S2 and note the pairwise geodesic distances. Figure 5.16 shows examples of elastic
geodesic paths between handwritten letters.

156
5 Shapes of Planar Curves
5.8 Gradient-Based Optimization Over Re-Parameterization
Group
Even though the DPA provides a reasonable solution for optimization of H over
ΓI, such an algorithm will not be applicable in all circumstances. As an example,
in the next chapter we will study shape analysis of closed curves and the addi-
tional constraints present in that problem results on a cost function that is not
additive over the graph (t, γ(t)). Thus the DP algorithm does not apply directly.
In situations such as these, it will be good to have a more general methodology
that is more universally applicable. Since we are trying to solve a minimization
problem, a gradient-based approach seems broadly applicable.
To develop a framework for a gradient approach, we develop some useful ter-
minology ﬁrst. We remark at the outset that this development overlooks certain
mathematical technicalities in order to reach eﬃcient numerical implementations
of the alignment process. The issue here is that the shape space does not have
a manifold structure even though the pre-shape space is a manifold. Due to this
situation, we view the shape space only as a set with a distance inherited from
the pre-shape space. However, in the context of deriving numerical procedures,
such as the gradient-based approach for alignment, we will utilize ideas such as
the tangent spaces of orbits and the tangent spaces of the quotient space to help
out conceptually.
Let [q]ΓI denote the orbit of q ∈C2 under the action of ΓI (as apposed to [q],
which denotes the closure of this orbit). We want to develop our understanding of
the tangent space Tq([q]ΓI ). The tangent space Tq([q]ΓI ) can be identiﬁed to the
space Tγid(ΓI), where Tγid(ΓI) is the set of all smooth functions that are zero at
the boundaries:
Tγid(ΓI) = {v : [0, 1] →R|v(0) = 0, v(1) = 0,
v is a smooth function} .
To formalize this identiﬁcation, let φ be a mapping from ΓI to [q]ΓI deﬁned as:
φ(γ) = √˙γ(q ◦γ). Note that φ is simply deﬁned using the action of ΓI on C2. We
are interested in the diﬀerential of φ at the identity, denoted by dφγid. We know
that dφγid is a linear mapping from Tγid(ΓI) to Tq([q]ΓI), but what form does it
take? This is answered by the following lemma.
Lemma 5.3. For any point γ ∈ΓI, the diﬀerential of φ : ΓI →[q]ΓI is the linear
transformation dφγ : Tγ(ΓI) →T√˙γ(q◦γ)([q]ΓI ) deﬁned by the equation
(dφγ(v))(s) =

˙γ(s) ˙q(γ(s))v(s) +
1
2

˙γ(s)
˙v(s)q(γ(s)) .
(5.34)
Proof. Let Ψ : (−ϵ, ϵ) × [0, 2π] →[0, 2π] be a mapping such that Ψ(t, ·) is a
diﬀerentiable path in ΓI passing through γ at t = 0. Let the velocity of this path
at t = 0 be given by v ∈Tγ(ΓI). In other words, this path satisﬁes the following
initial conditions:
Ψ(0, s) = γ(s),
Ψt(0, s) = v(s),
Ψs(0, s) = ˙γ(s),
Ψts(0, s) = ˙v(s) .
Since Ψ(t, ·) is a path in ΓI, φ(Ψ(t, ·)) is the corresponding path in [q]ΓI, and since
the initial velocity of Ψ(t, ·) is v, the initial velocity of the corresponding path in
[q]ΓI is dφγ(v). Using the fact that
φ(Ψ(t, s)) =

Ψs(t, s)q(Ψ(t, s)) ,

5.8 Gradient-Based Optimization Over Re-Parameterization Group
157
we get:
d
dt|t=0(φ(Ψ(t, s)) =


Ψs(t, s) ˙q(Ψ(t, s))Ψt(t, s) +
1
2

Ψs(t, s)
Ψt,sq(Ψ(t, s))

|t=0
=

˙γ(s) ˙q(γ(s))v(s) +
1
2

˙γ(s)
˙v(s)q(γ(s)) ,
which is the desired formula for dφγ(v).
⊓⊔
For γ = γid, we have
(dφγid(v))(s) = ˙q(s)v(s) + 1
2q(s)˙v(s),
s ∈[0, 1] .
(5.35)
This result can be used to specify the tangent space T[q](S2). An orthonor-
mal basis for Tγid(ΓI) under the Palais metric is given by {(
1
nπ
√
2 sin(2πnt),
1
nπ
√
2(cos(2πnt) −1))|n = 1, 2, . . . }. Then, using Lemma 5.3, the set{dφγid(v1),
dφγid(v2), . . . } spans the vector space Tq([q]ΓI). The elements of this set span
Tq([q]ΓI), but they do not necessarily form an orthonormal basis, since the map-
ping dφγid is not an isometry. In case one needs an orthonormal basis, we can use
the Gram-Schmidt algorithm to make these elements orthogonal. Also, note that
the space tangent to the SO(2) orbit of q is a one-dimensional space spanned by
the function
u(t) =
0 −1
1 0

q(t) .
Consequently, we can deﬁne the tangent space:
T[q](S2) = {w ∈L2([0, 1], R2)| ⟨w, q⟩= 0, ⟨w, dφγid(vi)⟩= 0, ∀i, and ⟨w, u⟩= 0} .
(5.36)
With these results in hand, we return to the problem of minimizing H using
a gradient method, with the following setup. In each iteration, we seek an incre-
mental γ for minimizing H according to its negative gradient. We will denote the
incremental group element by γk, for the k-th iteration, and γ(k) to denote the
cumulative re-parameterization, i.e.,γ(k) = γ1 ◦γ2 ◦· · · ◦γk. In this notation, at
the (k + 1)-st iteration, we seek the increment γk+1 that minimizes H(γ(k+1)).
Let ˜q2 denote the current element of the orbit [q2], i.e., ˜q2 = (q2 ◦γ(k))

˙
γ(k). For
any v ∈Tγid(ΓI), we need to ﬁnd the directional derivative of H at γid in the
direction of v. (This result can also be proven using the more general result given
in Lemma 5.3.)
Theorem 5.2. The directional derivative of H in the direction of v ∈Tγid(ΓI) is
given by
∇vH = −2
 1
0

q1(t) −˜q2(t), ( ˙˜q2(t)v(t) + 1
2 ˜q2(t)˙v(t))

dt .
(5.37)
Proof. Let α : (−ϵ, ϵ) →ΓI be a path such that α(0) = γid and ˙α(0) = v. We can
rewrite the cost function as
H(α(τ)) =
 1
0
⟨q1(t) −(α(τ), ˜q2)(t), q1(t) −(α(τ), ˜q2)(t)⟩dt .

158
5 Shapes of Planar Curves
Taking the derivative with respect to τ and setting τ = 0, we obtain
−2
 1
0

q1(t) −˜q2(t), d
dτ |τ=0((α(τ), ˜q2)(t))

dt .
Substituting for the last term from Eq. 5.35, we obtain the desired result.
⊓⊔
Let {vi, i = 1, 2, . . .} form an orthonormal basis of the vector space Tγid(ΓI).
As an example, we can take the set:
{
1
√
2πn
sin(2πnt)|n = 1, 2, 3, . . .}
/
{
1
√
2πn
(cos(2πnt) −1)|n = 1, 2, 3 . . .} .
(5.38)
Then, the full gradient of the cost function H is approximated by
∇H =
N

i=1
(∇viH)vi , ,
(5.39)
for a large N and can be used to ﬁnd the incremental γk+1 using a small step size
δ > 0.
Algorithm 4. 1. Set k = 0, γk = γ(k) = γid.
2. Compute ˜q2 = (γ(k), q2).
3. Compute the gradient of H with respect to γ according to Eq. 5.39.
4. Set γk+1 = γid + δ∇H and compute γ(k+1) = γ(k) ◦γk+1.
5. If ∥∇H∥is small, then stop. Else, set k = k + 1 and return to Step 2.
One issue in this implementation is the choice of step size δ. If δ is very small,
then of course the algorithm converges very slowly. If, however, it is too large, then
the incremental function γk+1 may not be a proper re-parameterization function.
For instance, since ˙γk+1 = 1+δ d
dt(∇H) and the last term can have negative values,
it is possible to have ˙γk+1(t) < 0 for some t. One can avoid this problem by having
a small enough δ but the optimal choice of δ is not known in advance.
To overcome this issue, one possibility is to use a SRSF representation of γ.
Deﬁne hk = √˙γk and use hk to represent γk in the optimization process. Note
that the space of all h-functions is a subset of a sphere of unit radius; denote it by
H . We initialize with γk+1(t) = t, with the corresponding representation being 1,
the constant function with value one. Now take the gradients of H, with respect to
hk+1, and update these individually. The directional derivative of H in a direction
c ∈T1(H ) is given by
∇hH(c) =
 1
0

q1(t) −q2(t),

2 ˙˜q2(t)˜c(t) + ˜q2(t)c(t)

dt,
˜c(t) =
 t
0
c(s)ds .
Form an approximate basis for the tangent space T1(H )={f :[0, 1]→R| ⟨f, 1⟩=0}
using
{(
1
√
2π
sin(2πnt),
1
√
2π
cos(2πnt))|n = 1, 2, . . . } ,
and approximate the gradient using m
i=1 ∇hH(ci)ci, where the cis are the basis
elements. Then, update the h function according to
1 →hk+1 ≡cos(∥c∥)1 + sin(∥c∥) c
∥c∥.

5.8 Gradient-Based Optimization Over Re-Parameterization Group
159
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
a
b
c
d
e
0
5
10
15
0.7
0.8
0.9
1
1.1
1.2
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
Fig. 5.17 (a), (b) The original two shapes represented by q1 and q2, respectively. (c) Gradient-
based update of γ(k) versus k. (d) evolution of H in matching ˜q2 with q1, (e) Estimated optimal
γ using the gradient method (solid line) and the DPA (broken line)
This hk+1 in turn gives γk+1(t) =
 t
0 hk+1(s)2ds and thus γ(k+1).
Figures 5.17 and 5.18 show examples of the optimization of H using the function
h. The top row shows the curves that are to be matched. They are represented by
their SRVFs q1 and q2 in Eq. 5.33. The ﬁrst panel in the bottom row shows the
evolution of γ(k) as a function of the iteration index k. The initial condition for the
iteration is chosen here to be γid. The middle panel shows the evolution of H as a
function of k (solid line) and the right panel shows the optimal γ obtained using
the gradient approach (drawn in solid line). To compare this procedure with the
DPA, we show the optimal H value and the optimal re-parameterization function
obtained using that algorithm using broken lines in the last two panels.
Looking at the results we can make the following remarks:
1. It is possible that the gradient method is caught in local solutions, and does
not reach the globally optimal re-parameterization functions. The latter can be
obtained using the DPA. This is a major limitation of gradient approaches in
general.
2. We think that this problem can be mitigated by using two ideas. One is to use
an adaptable step size in the gradient update. A ﬁxed update seems too rigid to
allow a convergence of the algorithm to the global solution. Second is improvise
so that the gradient algorithm is allowed to move freely on the boundaries of the
search domain. It seems that the algorithm reaches the boundary and becomes
stuck there rather than continuing to update along the boundary.

160
5 Shapes of Planar Curves
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
5
10
15
20
1
1.2
1.4
1.6
1.8
2
2.2
2.4
a
b
c
d
e
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
Fig. 5.18 Same as Fig. 5.17
3. In terms of the computational cost, the gradient method is an order or magni-
tude faster than the DPA in the current setup. So, the actual choice of a method
is a choice essentially between speed and accuracy.
5.9 Summary
Here we provide a tabulated summary of the important elements of shape analysis
of planar open curves (Table 5.1).
5.10 Exercises
5.10.1 Theoretical Exercises
1. Prove that for a continuously diﬀerentiable, arc-length parameterized curve
β : [0, 2π] →R2 , there exists a unique angle function θ : [0, 2π] →R that is
continuous and θ(0) ∈[0, 2π).
2. Consider the angle function representation of arc-length parameterized curves
and the set L2([0, 2π], R) of all angle functions.
a. Establish that R acts on L2([0, 2π], R) according to the map (θ0, θ)(s) =
θ(s) + θ0.

5.10 Exercises
161
Table 5.1 Summary of shape analysis of planar curves
Property
Bending-only planar curves
Elastic planar curves
Representation
Angle
function
θ
(under
arc-length
param.)
SRVF q (arbitrary param.)
˙β(s) = eiθ(s) with | ˙β(s)| = 1
q(t) =
˙β(t)
√
| ˙β(t)|
Metric/larger space L2 metric/L2([0, 2π], R)
L2 metric/L2([0, 1], R2)
Pre-shape space
Pre-shape space
C1 = {θ ∈L2([0, 2π], R)| 1
2π
 2π
0
θ(t)dt =
π}
C2
=
{q
∈
L2([0, 1], R2)|  1
0 ∥q(t)∥2dt = 1}
angle functions of curves of length 2π
SRVFs of curves of unit length
with a ﬁxed orientation π
Normal space
Nθ(C1) = span{1}
Nq(C2) = span{q}
Tangent space
Tθ(C1) = {v ∈L2([0, 2π], R)| ⟨v, 1⟩= 0}
Tq(C2)
=
{w
∈
L2([0, 1], R2)| ⟨w, q⟩= 0}
Geodesic distance
dC1 = ∥θ1 −θ2∥
dC2 = ϑ = cos−1(⟨q1, q2⟩)
Geodesics
straight line α(τ) = τθ2 + (1 −τ)θ1
arc on a great circle
α(τ) =
1
sin(ϑ) (sin(ϑ(1 −τ))q1 +
sin(τϑ)q2)
Shape Space
Shape-preserving
None
Rotation SO(2), action: (O, q) =
Oq
transformations
Re-parameterization ΓI, action:
(γ, q) = √˙γ(q ◦γ)
Shape space
S1 = C1
S2 = C2/(SO(2) × ΓI)
Tangent space
Tθ(S1) = Tθ(C1)
Tq(S2) = {w ∈Tq(C2)|w ⊥
Tq([q])}
Tq([q])
=
{E(√˙γ( ˙q ◦γ)v +
1
2√˙γ (q ◦γ) ˙v)|
v
∈
Tγ(ΓI), E ∈TO(SO(2))}
Optimization
None
Iterate over the two alignment
steps until convergence:
1. Find optimal rotation O∗in
SO(2)
2.
Find
optimal
re-
parameterization γ∗in ΓI
us-
ing DP
Set q∗
2 =
 ˙γ∗O∗(q2 ◦γ∗)
Geodesic distance
dS1 = ∥θ1 −θ2∥
dS2 = ϑ = cos−1 
q1, q∗
2

)
Geodesics
Same as C1
α(τ) =
1
sin(ϑ) (sin(ϑ(1 −τ))q1 +
sin(τϑ)q∗
2)

162
5 Shapes of Planar Curves
q1
q2 +κ
[q1]
[q2]
q2
C1
q1
q2
aq2
C2
{aq2|a ∈R×}
{aq1|a ∈R×}
Fig. 5.19 C1 and C2 are orthogonal sections of respective group actions on relevant Hilbert
spaces
b. Show that C1 is an orthogonal section (see Deﬁnition 3.19) of L2([0, 2π], R)
under this action of R. See the left panel of Fig. 5.19.
3. Let θ1, θ2
be any two elements of C1. Show that the minimizer in
argminκ∈[0,2π] ∥θ1 −(θ2 + κ)∥is zero. (This is actually a consequence of
the previous result. If θ1 is an element of an orthogonal section, and the action
is by isometries, then the nearest element on θ2 orbit is also an element of the
same section.)
4. Find the angle function of the following half circles centered at the origin and
having the following angles (with respect to the positive x axis: (1) 0deg to
180deg, (2) 90deg to 270deg, and (3) 180deg to 360deg. Note that we need the
radius of the circle to be two so that these half circles have length 2π. Find
the intersections of their orbits with the section C1.
5. Show that the mapping Q : F2 →C2 is a bijection. Recall that F2 is the set
of absolutely continuous, unit-length curves with starting value zero.
6. Consider the SRVF representation of parameterized curves.
a. Prove that the multiplication group R× acts on L2([0, 1], R2) by the map-
ping (a, q)(t) = aq(t).
b. Then, verify that this action is not by isometries under the L2 metric.
7. For a point q ∈L2 such that ∥q∦= 0, deﬁne the scaled-L2 metric between two
functions v, w ∈L2 at a point q as
⟨⟨v, w⟩⟩q = ⟨v, w⟩
⟨q, q⟩.
Prove the following statements:
a. The scaling group R× acts on L2 by isometries under the scaled-L2 metric.
b. C2 is an orthogonal section of L2 under the scaling group, under the scaled-
L2 metric. See the right side of Fig. 5.19.
(We comment that as a consequence of C2 being an orthogonal sec-
tion of L2 under the scaling group, we have that for any q1, q2 ∈C2,
argminc∈R+ dsc(q1, cq2) = 1, where dsc is the geodesic distance under the
scaled-L2 metric.)
8. Show that the elastic metric, given in Deﬁnition 5.3, reduces to the Fisher-Rao
metric, given in Deﬁnition 4.8, in the case of curves in R1.
9. Shown that the expression given in Eq. 5.23 provides a Riemannian metric on
the space of curves under the joint log-speed and angle function representation.

5.10 Exercises
163
10. a. Let (φ, θ) represent the log-speed and the angle functions of a parameter-
ized curve β. For any re-parameterization function γ ∈ΓI, show that the
corresponding functions of β ◦γ are (φ ◦γ + log ◦˙γ, θ ◦γ).
b. Show that the re-parameterization group ΓI acts on the space of curves
according to the previous mapping.
c. Prove that this group action is by isometries under the metric given in
Eq. 5.23.
11. Using the (φ, θ) representation of a curve, deﬁne a new representation accord-
ing to
h(t) = e
1
2 φ(t)

cos( θ(t)
k )
sin( θ(t)
k )

,
for any k > 0. Let (u, v) be an element of the tangent space T(φ,θ)(Φ × Θ).
a. Show that the tangent vector f to L2([0, 1], R2) at h, corresponding to (u, v),
is given by
f = 1
2e
1
2 φu
cos(θ/k)
sin(θ/k)

+ 1
k e
1
2 φ
−sin(θ/k)
cos(θ/k)

v .
b. Next, compute the L2 inner product between such tangent vectors—⟨f1, f2⟩.
Show that the result is equivalent to the elastic metric on Φ×Θ with a = 1/4
and b = 1/k2.
c. Plot the coordinate functions of h (with k = 2) for a unit circle. Compare
that with the coordinate functions of the SRVF of the same curve.
5.10.2 Computational Exercises
1. Write a program to calculate the length of a piecewise linear curve β : [a, b] →
Rn under a given Riemannian metric on Rn.
2. Compute the length of the curve t →β(t) = (sin(2πt), cos(2πt)) on the interval
[0, 1] using the Euclidean metric. Is this a constant-speed parameterization?
3. Given a discrete curve {β(t) ∈R2|t ∈[0, 1
T , . . . , 1]} and a discrete representa-
tion of a warping function {γ(t)|t ∈[0, 1
T , . . . , 1]}, write a program to estimate
the warped curve { ˜β(t) ≡β(γ(t))|t ∈[0, 1
T , . . . , 1]}. Since you will need a tech-
nique to interpolate between given values of β, the program should be able to
choose one of the two options—piecewise linear and spline—for that purpose.
Draw the following plots:
a. Plot the two curves β and ˜β over each other to highlight diﬀerences in their
shapes.
b. Plot the coordinate functions before—β1(t) and β2(t) versus t—and after
˜β1(t) and ˜β2(t) versus t—warping.
c. Plot the triple before (t, β1(t), β2(t)) in R3 and after (t, ˜β1(t), ˜β2(t)) warping.
4. Write a program to resample a given curve (in a discrete form) by the arc-
length parameterization.
5. Write a program that (1) rescales a given curve β to length 2π, (2) resamples
it by the arc-length parameterization (Problem 4), and (3) computes its angle
function θ. Furthermore, add a constant to θ such that it becomes an element

164
5 Shapes of Planar Curves
of C2. Additionally, write a program that takes an angle function θ and a
starting point β(0) and reconstructs the curve β over the interval [0, 2π).
6. Write a program to compute a discrete geodesic path between any two given
curves in S1. Note that the set C1 is the same as S1, so the geodesics are the
same in the two cases.
7. Write a program that (1) rescales a curve β to unit length and (2) computes its
SRVF to form an element of C2. Additionally, write a program to reconstruct
the curve from its SRVF using the initial condition β(0) = 0. Illustrate your
results on a unit circle.
8. Given any two curves, β1 and β2, use the program written for Problem 7
to ﬁnd their SRVFs q1, q2. Show that they satisfy the property ∥q1 −q2∥=
∥(q1, γ) −(q2, γ)∥up to numerical precision, for any γ ∈ΓI.
9. Write a program to compute a discrete geodesic path between any two given
curves in C2 and display these results by mapping SRVFs to the corresponding
curve space.
10. Write a program to rotationally align one curve with another using Eq. 5.32
and display the results.
11. Implement the dynamic programming algorithm to minimize the cost function
given in Eq. 5.33 and display the results.
12. Write a program that computes a discrete geodesic path between any two given
curves in S2:
a. Computes the scaled SRVFs of the given curves using Problem 7.
b. Iteratively solve for the optimal re-parameterization (Problem 11) and op-
timal rotation (Problem 10) of the second curves to obtain the optimal
alignment.
c. Compute geodesic path between the SRVFs of the aligned curves using
Problem 9.
13. Write a program to compute the shooting vector associated with a uniform-
speed geodesic from one curve to another under the SRVF representation.
5.11 Bibliographic Notes
The earliest work in elastic shape analysis of planar curves, using complex square-
roots of coordinates, appears in [125]. (We should mention that there are several
other papers on alignment of curves, albeit without Riemannian considerations,
such as [100].) A summary of diﬀerent Sobolev metrics for shape analysis of planar
curves can be found in [77]. Klassen et al. [54] was the ﬁrst paper to focus on spaces
formed by closed curves. Later on, several papers studied the use of elastic metrics
for planar curves using complex representations [124, 126, 112].
The general elastic metric for planar curves was studied in [79]. Srivastava
et al. [42, 43, 106] introduced the SRVF representation curves in general Euclidean
spaces. The existence results for optimal re-parameterizations under SRVF repre-
sentation has been discussed in [20], and some generalization of SRVF for variable
relative weight between stretching/bending term is developed in [11]. The use of
higher-order Sobolev metrics for elastic shape analysis is motivated in [10]. White
[121] studied the extensions of complex square-root approaches to analysis of curves
in more than two dimensions. The use of Riemannian optimization technique for

5.11 Bibliographic Notes
165
speeding up the registration of curves, in lieu of the dynamic programming algo-
rithm, has been studied in [39]. A transform similar to SRVF, but with arbitrary
elasticity, has been introduced in [12]. While it has the strength of encoding arbi-
trary elasticity in the metric, it does not transform to a fully Euclidean space as
is the case for SRVFs.
Some applications of planar, elastic shape analysis can be found in [70] (protein
structure analysis), [61] (plant leaves), [2] (video coding), [22] (SONAR image
analysis), and [1] (human activity recognition).

Chapter 6
Shapes of Planar Closed Curves
6.1 Goals and Challenges
In this chapter we are interested in analyzing shapes of planar curves that are
closed. A closed curve is a curve that starts and ends at the same point. If diﬀer-
entiability is needed (for example, in the case of the angle function representation),
then we will require the derivative at the initial point of the curve to agree with
the derivative at the ending point. However, if only absolute continuity is required
(for example, in the case of the square-root velocity representation), then it will
be enough to assume the curve starts and ends at the same point. Consequently,
these curves do not have any boundary and every point has neighbors on both
sides. The boundaries of objects in images typically form simple closed curves in
the image plane (simple curves are those curves that do not cross themselves) and
shape analysis of imaged objects motivates the study of simple, closed curves. We
will treat closure as an additional constraint on the sets of curves studied in the
previous chapter and the question arises: how does this constraint alter the diﬀer-
ential geometries of the previously discussed pre-shape and shape spaces? Further,
how can we modify the current ideas for computing geodesics in these spaces, un-
der the non-elastic and elastic representations of curves, in order to accommodate
the closure constraint? Our main goals in this chapter are:
• To modify the previous two mathematical representations of planar curves,
based on the angle functions and the square-root velocity functions (SRVFs),
for restricting the analysis to planar, closed curves.
• To utilize the same Riemannian metrics for registering, deforming, measuring,
and comparing shapes of such curves.
• Finally, to provide algorithms for performing tasks such as geodesic computa-
tions in the resulting pre-shape and shape spaces.
The main challenge in shape analysis of closed curves comes from the added
closure restriction. In the previous chapter, planar curves have been represented by
functions that are elements of a vector space (when using angle functions) or a unit
sphere (when using SRVFs). Since these spaces have simple geometries, we could
obtain explicit expressions for geodesics, at least in the pre-shape spaces. The clo-
sure constraint introduces a strong nonlinearity in the representation. As a quick
example, let θ1 and θ2 be the angle functions oftwo arc-length parameterized closed
© Springer-Verlag New York 2016
A. Srivastava, E.P. Klassen, Functional and Shape Data Analysis,
Springer Series in Statistics, DOI 10.1007/978-1-4939-4020-2 6
167

168
6 Shapes of Planar Closed Curves
curves. It will be made clear later on that their linear combination c1θ1 + c2θ2,
for arbitrary c1, c2 ∈R, will not represent a closed curve. Two pictorial examples
of this idea were presented in the previous chapter (see Fig. 5.6). The manifolds
resulting from the closure constraint become relatively more complicated. Conse-
quently, explicit expressions for geodesics are no longer available and we have to
resort to numerical approaches for constructing geodesics.
6.2 Representations of Closed Curves
Let β : [a, b] →R2 be a parameterized curve that is diﬀerentiable with respect
to the parameter. The exact restrictions on the curves will depend on their repre-
sentations and are clariﬁed later in those sections. For studying shapes of closed
curves, we impose an additional condition that β(a) = β(b). In the case of the an-
gle function representation, we also require that ˙β(a) = ˙β(b) so that the derivative
is continuous at a. In view of this condition, it is natural to have the domain of
parameterization be the unit circle S1 for closed curves. If we think of the point
(1, 0) ∈R2 as the “starting point” or “origin” of S1, then S1 can be identiﬁed with
the interval [a, b] using the function t →(cos(2π (t−a)
(b−a)), sin(2π (t−a)
(b−a))) ∈R2, under
which t = a and t = b are both identiﬁed with the point (1, 0). If we assume that
the curve is continuously parameterized by S1, then our curve is automatically
closed!
The main diﬀerence in parameterizations of open and closed curves is that the
choice of starting point, or origin, for open curves is limited to just two points. In
the following, assume that the interval of parameterization is [a, b] = [0, 1]. One
end of the curve is designated β(0) and the other β(1). Once an end is chosen as
a starting point, all re-parameterizations will have to respect that choice (recall
that γ(0) = 0 and γ(1) = 1, for a γ ∈ΓI in the previous chapter). In contrast,
the number of choices for the placement of origin on a closed curve is inﬁnite. A
re-parameterization of a closed curve can not only change the rate of traversal
along the curve but also the placement of the origin. Thus, a re-parameterization
function for a closed curve is a certain diﬀeomorphism of S1 to itself. We will use
ΓS to denote the set of such γs, with the subscript S denoting that the underlying
domain is S1 and not an interval.
Deﬁnition 6.1 (Re-parameterization of Closed Curve). If β : S1 →Rn is
a closed curve, and γ : S1 →S1 is an orientation-preserving diﬀeomorphism (γ ∈
ΓS), then we say that the composition β ◦γ is a non-singular re-parameterization
of β.
The visualization of such a γ is a little tricky since both the domain and the
range are a unit circle and the graph of γ is actually a closed path on a torus. To
visualize this function, we use the fact that there is a natural bijection between ΓS
and ΓI × S1. Consequently, we arbitrarily select points on the domain and range
as origins, and then γ can be plotted as a function from [0, 1] to itself.
Example 6.1. Let β(t) = (cos(t), sin(t)) be the arc-length parameterized unit circle,
and deﬁne γ : S1 →S1 by
γ(t) = t + α sin(t),
−1 < α < 1 .
(6.1)

6.2 Representations of Closed Curves
169
Then, ˜β = β ◦γ is a re-parameterized circle where the rate of traversal is no longer
constant.
In Chap. 5, we discussed several mathematical representations of parameterized
curves in R2 and selected two of them—the angle functions and the SRVFs—for
further consideration. Also, we presented formulas and algorithms for computing
geodesics between curves in shape spaces, under the chosen Riemannian metrics.
We continue to use these representations and metrics, but this time focusing on
closed curves:
1. Angle Function Representation: Suppose we have a curve β : [0, 2π] →R2
that is para- meterized by arc length (i.e., | ˙β(s)| = 1 for all s ∈[0, 2π]) and,
in addition, assume that ˙β is continuous. Then the only information contained
in ˙β(s) is its direction. The velocity vector ˙β(s) can be identiﬁed with a unit
complex number eiθ(s), where θ(s) is the angle formed by this vector with the
positive x-axis (i.e., the real axis). Note that the angle function is not aﬀected
by a rigid translation of the curve β. Also note that for each s, θ(s) is only well
deﬁned up to the addition of an integer multiple of 2π. Since we are assuming
˙β(s) is continuous, we will choose θ(s) to be continuous.
For representing the shape of a closed curve, one has to clarify one more
characteristic, namely, its rotation index. As we traverse β, the velocity vector
˙β goes through an integer number of full rotations. We call the number of these
rotations the rotation index of the curve. It is well known that for a simple
closed curve (recall that a simple closed curve is one that never meets itself),
the rotation index is always plus or minus one; it is plus one for counterclockwise
parameterizations (see, e.g., [23], p. 396). Due to our interest in the simple closed
curves, we will restrict to the set of all closed curves with rotation index plus
one. While this set is larger than the set of simple closed curves, it contains the
set of simple closed curves as an open subset. Also, we will see that this larger
set is a complete Riemannian manifold, a property that will be important later
when we start computing geodesic paths between points on this set.
Although we have ﬁxed the direction and the speed of traversal along the
curve, the starting point β(0), referred to as the origin, is still free to vary along
the curve. Since β is a closed curve, there is no natural choice of origin on β,
an issue that we will deal with later. If we place the origin on a certain point
of β, then θ becomes a function from [0, 2π] to R. Often it is convenient to
extend θ to the entire real line, both in the domain and the range, using the
deﬁnition: θ(2nπ + s) = 2nπ + θ(s), for all n ∈N. (For this extension to be
continuous, it is required that the rotation index is 1, which we have already
assumed to be true.) Note that for a closed curve having rotation index 1, we
have θ(2π) −θ(0) = 2π. In summary, the angle function of β is invariant to
translation but it varies with rotation and the choice of origin on β.
If the curve β is closed, its angle function θ also satisﬁes the additional
conditions:
 2π
0
cos(θ(s))ds = 0 and
 2π
0
sin(θ(s))ds = 0 .
(6.2)
These conditions are nonlinear in the sense that if θ1 and θ2 are two angle
functions that satisfy these conditions, then their linear combination c1θ1+c2θ2,
for arbitrary c1, c2 ∈R, will, in general, not satisfy them.

170
6 Shapes of Planar Closed Curves
2. Square-Root Velocity Representation (SRVF): The second representa-
tion described in Sect. 5.3.1 is based on the square-root velocity function q of
an absolutely-continuous curve β : S1 →R2 given by q(t) =
˙β(t)
√
| ˙β(t)|. Observe
that we do not need the condition ˙β(t) ̸= 0, and this representation exists even
for curves with singular parameterizations. Note that when using the SRVF
representation on closed curves, it is our practice to consider the domain of
the functions β and q as the unit interval [0, 1]. Thus we are essentially iden-
tifying the unit interval [0, 1] with the unit circle S1, in a way that matches
both endpoints of [0, 1] with the single point (1, 0) of S1. In what follows, when
the domain of an integral is indicated to be S1, it should actually be com-
puted as an integral over [0, 1]. If the curve β is of unit length, then q satisﬁes

S1 |q(t)|2dt =

S1 | ˙β(t)|dt = 1. Also, one can reconstruct the curve β from q,
up to a translation, using the formula β(t) =  t
0 q(u)|q(u)|du. If we choose to
represent a curve by its SRVF, then the closure condition becomes

S1
˙β(t)dt =

S1 q(t)|q(t)|dt = 0 .
(6.3)
Once again, this closure condition is a nonlinear condition. If q1 and q2 are
two SRVFs that satisfy Eq. 6.3, then their linear combination c1q1 + c2q2, for
arbitrary c1, c2 ∈R, will, in general, not satisfy that condition.
6.2.1 Pre-shape Spaces
Next, we look at how the closure condition changes the pre-shape and the shape
spaces studied in Chap. 5.
1. Case 1: Consider closed curves of length 2π with continuous ﬁrst derivatives
and rotation index 1, under arc-length parameterization, each represented by
its angle function. The pre-shape space of such angle functions is given by
C c
1 = {continuous θ ∈L2([0, 2π], R)| 1
2π
 2π
0
θ(s)ds = π, θ(2π) = θ(0) + 2π,
 2π
0
sin(θ(s))ds = 0,
 2π
0
cos(θ(s))ds = 0 }
⊂C1
⊂L2([0, 2π], R).
(6.4)
Compared to Eq. 5.5, this set has three additional constraints; the two coming
from Eq. 6.2 ensure the closure and the one (θ(2π) = θ(0) + 2π) coming from
the condition that the curve has rotation index one. Note that the condition of
average rotation removes the rotation variability from the curves.
2. Case 2: This case involves unit-length, closed curves under arbitrary parame-
terizations, using SRVF representation:
C c
2 = {q ∈L2(S1, R2)|

S1 |q(t)|2dt = 1,

S1 q(t)|q(t)|dt = 0} ⊂L2([0, 1], R2).
(6.5)
Compared to Eq.5.6, the last term has been added to ensure closure of curves. In
this representation we do not explicitly impose the condition of average rotation
and will deal with it later when forming shape spaces.

6.2 Representations of Closed Curves
171
We have identiﬁed these two pre-shape spaces of closed curves as C c
1 ⊂C1 ⊂
L2([0, 2π], R) and C c
2 ⊂C2 ⊂L2(S1, R2) as spaces of interest for further analysis.
6.2.2 Riemannian Structures
By analogy with Sect. 5.4.1, we describe tangent spaces to C c
1 and C c
2 and endow
them with Riemannian metrics.
1. Case 1: Let us start with the pre-shape space C c
1 . For any two elements u, v ∈
L2, we have the usual inner product:
⟨u, v⟩=
 2π
0
u(s)v(s)ds .
(6.6)
In order to deﬁne a Riemannian structure on C c
1 , we will restrict this inner
product to the tangent spaces of C c
1 .
Theorem 6.1. The pre-shape space C c
1 is a manifold.
Proof. Let’s begin by deﬁning
D = {continuous θ ∈L2([0, 2π], R) : θ(2π) = θ(0) + 2π} .
(6.7)
D is an aﬃne space, i.e., it is a linear subspace that has been translated so that
it no longer passes through the origin. Note that D contains C c
1 as a subset.
The tangent space of D is the vector space:
B = {continuous θ ∈L2([0, 2π], R) : θ(2π) = θ(0)}.
This space can also be characterized as the set of elements of L2([0, 2π], R) that
can be expressed as the diﬀerence between two elements of D. An element of
D needs to satisfy three additional conditions in order to be in C c
1 . Hence we
deﬁne the following map Φ1 = (Φ1
1, Φ2
1, Φ3
1): D →R3 by
Φ1
1(θ) = 1
2π
 2π
0
θ(s) ds,
Φ2
1(θ) =
 2π
0
cos(θ(s)) ds,
Φ3
1(θ) =
 2π
0
sin(θ(s)) ds .
(6.8)
In terms of Φ1, C c
1 can be rewritten as the pullback set Φ−1
1 (π, 0, 0). Using the
L2 inner product, the directional derivative dΦ1, at a point θ ∈D and in the
direction of an f ∈B, is given by
dΦ1
1(f) = 1
2π
 2π
0
f(s) ds = 1
2π ⟨f, 1⟩,
dΦ2
1(f) = −
 2π
0
sin(θ(s))f(s)ds = −⟨f, sin ◦θ⟩,
dΦ3
1(f) =
 2π
0
cos(θ(s))f(s)ds = ⟨f, cos ◦θ⟩.
(6.9)

172
6 Shapes of Planar Closed Curves
We would like to conclude that C c
1 is a codimension-3 smooth submanifold of D
using Theorem A.2. To do this, we need to verify that dΦ1 : B →R3 is surjective
at every θ ∈Φ−1
1 (π, 0, 0) (posed as an exercise). Once we have that dΦ1 is
surjective, we conclude that C c
1 is a codimension-3 smooth submanifold of D.
⊓⊔
It follows that the tangent space Tθ(C c
1 ) is given by
Tθ(C c
1 ) = null space of dΦ1
= {f ∈B|f ⊥Nθ(C c
1 )}, where Nθ(C c
1 ) = span{1, cos◦θ, sin ◦θ} .
Since the tangent space Tθ(C c
1 ) is a subspace of L2, we can restrict the L2
inner product to deﬁne a Riemannian structure on C c
1 . For any two elements
u, v ∈Tθ(C c
1 ), the Riemannian metric is given in Eq. 6.6. The resulting geodesic
distance on C c
1 will be denoted by dC c
1 .
2. Case 2: For the SRVF representation, we select a Riemannian structure as
follows. Given a curve represented by q ∈L2(S1, R2), and the tangent vectors
u, v ∈Tq(L2(S1, R2)), the inner product between u, v is deﬁned as
⟨u, v⟩=

S1 ⟨u(t), v(t)⟩dt .
(6.10)
Deﬁne a mapping Φ2 : L2(S1, R2) →R3 by
Φ1
2(q) =

S1 |q(t)|2dt,
Φ2
2(q) =

S1 q1(t)|q(t)|dt,
Φ3
2(q) =

S1 q2(t)|q(t)|dt ,
(6.11)
where q(t) = (q1(t), q2(t)). Then C c
2 is deﬁned as Φ−1
2 (1, 0, 0). The directional
derivative of the map Φ2 at a point q in the direction of w ∈L2(S1, R2) is given
by
dΦ1
2(w) =

S1 ⟨w(t), q(t)⟩dt
dΦ2
2(w) =

S1

w(t), q1(t)
|q(t)|q(t) + |q(t)|e1

dt
dΦ3
2(w) =

S1

w(t), q2(t)
|q(t)|q(t) + |q(t)|e2

dt ,
where e1 = [1 0]T and e2 = [0 1]T . We note that at the points where |q(t)| = 0,
we assign q1(t)
|q(t)|q(t), q2(t)
|q(t)|q(t) equal to zero. Therefore, having |q(t)| = 0 in the
denominator is not a problem in these integrands.
Theorem 6.2. The pre-shape space C c
2 is a manifold.
Proof. Since C c
2
= Φ−1
2 (1, 0, 0), we can show that it is a submanifold of
L2(S1, R2) if dΦ2 is surjective, using Theorem A.2. To establish that, we consider
the three functions appearing in the above integrals:

6.2 Representations of Closed Curves
173
q(t),
 q1(t)
|q(t)|q(t) + |q(t)|e1

, and
 q2(t)
|q(t)|q(t) + |q(t)|e2

.
To show that dΦ2 is surjective, we need to show these three functions are linearly
independent. The proof is as follows: if they are dependent then there exist real
constants a, b, and c such that
aq(t) + b(A(t)q(t) + B(t)e1) + c(C(t)q(t) + B(t)e2) = 0 ,
where A, B, and C are the scalar functions given by A(t) = q1(t)
|q(t)|, B(t) = |q(t)|,
and C(t) = q2(t)
|q(t)|. This simpliﬁes to −B(t)(be1 + ce2) = (a + bA(t) + cC(t))q(t).
Clearly this implies that for all t, q(t) is in the same direction as the constant
vector be1 +ce2. This proves that for any q-function that doesn’t lie completely
in a single one-dimensional subspace, the function Φ2 is surjective, so the pre-
shape space is a manifold except at those points. These exceptional functions
correspond to curves that lie entirely in a straight line in R2. We have proven
that C c
2 is a manifold at every function q whose values do not lie entirely in a
one-dimensional subspace of R2.
So now consider a function q ∈C c
2 whose values do lie entirely in a
one-dimensional subspace. Without loss of generality, we may assume that
q(t) = (r(t), 0), so q1(t) = r(t) and q2(t) = 0. Then, the three functions we are
considering become (r(t), 0), (2|r(t)|, 0), and (0, |r(t)|). Since

S1 q(t)|q(t)|dt = 0
and

S1 |q(t)|2dt = 1, it follows that r(t) must take both positive and nega-
tive values. From this it follows easily that the ﬁrst two functions are lin-
early independent, and even more easily that the third is independent of
the ﬁrst two. Thus, C c
2 is a codimension-three submanifold of L2(S1, R2).
⊓⊔
In order to deﬁne the space of tangent vectors to C c
2 , we derive the normal
space of C c
2 in L2(S1, R2) at q ﬁrst. For any q ∈C c
2 , the space of normals to C c
2
in L2(S1, R2) is
Nq(C c
2 ) = span {q(t),
 q1(t)
|q(t)|q(t) + |q(t)|e1

,
 q2(t)
|q(t)|q(t) + |q(t)|e2

} .
(6.12)
and the space of tangent vectors is
Tq(C c
2 ) =
0
w : S1 →R2|w ⊥Nq(C c
2 )
1
.
(6.13)
We reiterate that while it may appear that the terms
q1(t)
|q(t) ,
q2(t)
|q(t) are not
well deﬁned in situations where q(t) vanishes, it is not the case. Note that
q1(t), q2(t) ≤|q(t)|, so we can set q1(t)
|q(t) q(t), q2(t)
|q(t) q(t) to be zero for the values of
t at which |q(t)| = 0. As another remark, we note that since the three normal
vectors vary in a continuous way with the function q, one can show that C c
2 is
a C1-manifold [64].
We will use dC c
2 to denote the resulting geodesic distance on C c
2 , associated
with the L2 Riemannian metric.

174
6 Shapes of Planar Closed Curves
6.2.3 Removing Parameterization
In Sect. 5.5 we introduced the idea of using group actions to study re-
parameterization variability of general curves. Now we apply this idea to closed
curves.
1. Case 1: We start with the case of arc-length parameterized curves, represented
by their angle functions and viewed as elements of the pre-shape space C c
1 .
Since the speed of traversing curves is ﬁxed to be one, the only remaining
way to re-parameterize a closed curve is to change the placement of the origin
on it. In other words, a “translation” of origin along S1 is the same as a ro-
tation of the points in R2. Since the net eﬀect of these re-parameterizations
is simply translations of points along S1 by a constant distance, these re-
parameterizations are also called translation diﬀeomorphisms . Incidentally,
they are also called distance-preserving diﬀeomorphisms since they preserve
distances between points on S1. A translation of the origin by u is given by
s →(s −u)mod 2π. Here, u ∈S1 stands for the angle associated with a point
on S1, expressed in radians. So, the space of all possible translations along S1
is S1 itself and it becomes the set of all translation diﬀeomorphisms of S1.
The process of changing the origin on a closed curve can be expressed math-
ematically as an action of S1 on C c
1 . Deﬁne the mapping S1 × C c
1 →C c
1 as
follows. For any u ∈S1 and θ ∈C c
1 , let
(u, θ)(s) = θ((s −u)mod 2π) + u,
s ∈[0, 2π]
(6.14)
Addition of u in this equation ensures that the average value of (u, θ) is π and
this shifted function remains an element of C c
1 . To verify this, we ﬁrst extend
θ to the function θe : R →R using θe(2nπ + s) = 2nπ + θ(s) for all s ∈[0, 2π]
and n ∈Z. The extended function θe agrees with θ on [0, 2π] but has extended
θ by repeatedly translating its graph along the 45◦line. The corresponding
translation action of R on θe is simply (u, θe)(s) = θe(s −u) + u, which when
restricted back to [0, 2π] gives the right side of Eq. 6.14. Pictorially, one can
think of this action as translation of the graph of θe in a direction parallel to
the graph of y = x. To understand the reason for this translation, note that
ﬁrst we are translating horizontally by a distance of u (this has the eﬀect of
changing the location of the origin of our curve ), and then we are translating
vertically by u (this has the eﬀect of rotating the curve to where its “average
angle” is once again π).
A pictorial illustration of this group action is given in Fig. 6.1. The columns
(a) and (b) show closed curves, and the corresponding angle functions θ for
certain placements of the origin while the columns (c) and (d) show the same
curve but diﬀerent placements of the origin and the corresponding angle func-
tions (u, θ), respectively. In terms of shapes, the shape of the curve represented
by the function (u, θ) is same as that of the curve represented by θ. To unify
all such representations, we deﬁne an equivalence relation based on this group
action. We deﬁne two functions θ1, θ2 ∈C1 to be equivalent if and only if there
exists a u ∈S1 such that (u, θ1) = θ2. The orbit associated with a representation
θ ∈C c
1 , given by
[θ] = {(u, θ)|u ∈S1} ,

6.2 Representations of Closed Curves
175
0
0.2
0.4
0.6
0.8
1
1
2
3
4
5
6
7
a
b
c
d
e
8
0
0.2
0.4
0.6
0.8
1
1
2
3
4
5
6
7
8
0
0.2
0.4
0.6
0.8
1
1
2
3
4
5
6
7
8
9
0
0.2
0.4
0.6
0.8
1
−4
−2
0
2
4
6
8
0
0.2
0.4
0.6
0.8
1
−6
−4
−2
0
2
4
6
0
0.2
0.4
0.6
0.8
1
−6
−4
−2
0
2
4
6
8
Fig. 6.1 Example of action of S1 on C c
1 : (a) a curve β, (b) its angle function θ, (c) curve with
shifted origin β(s −u) , and (d) resulting angle function (u, θ). (e) shows some examples of
elements in [θ]
is the equivalence class of shapes containing θ. Figure 6.1e shows several el-
ements (u, θ) of the orbit [θ], for the θ shown in Fig. 6.1b. This equivalence
relation partitions C c
1 into disjoint equivalence classes and the set of all such
classes is denoted by S c
1
S c
1 = {[θ]|θ ∈C c
1 } .
(6.15)
S c
1 is the quotient space of C c
1 modulo the group S1, i.e., S c
1 = C c
1 /S1 and is
the shape space of closed, arc-length parameterized curves in R2. Every element
of S c
1 has a diﬀerent shape and, conversely, two curves with the same shape
map to the same element of S c
1 as long as they are arc-length parameterized.
The cartoon illustration in Fig. 6.3a shows this idea where each vertical line
denotes an orbit and the quotient space is a collection of such orbits.
Next, we look at the problem of computing distances between points in S c
1 .
We have already established a Riemannian metric on C c
1 in Eq. 6.6 and can
inherit it to S c
1 . Since S c
1 is the quotient space of C c
1 , under the action of S1,
we need to verify the condition (stated in Deﬁnition 3.16) that the action of S1
is by isometries.
Theorem 6.3. The action of S1 on C c
1 given by Eq. 6.14 is an action by isome-
tries.
We will leave the proof as an exercise for the reader.
So, the action of S1 on C c
1 is by isometries and, by Deﬁnition 3.16, the
Riemannian metric on C c
1 descends to S c
1 . Consequently, the distance between
any two points on S c
1 , say [θ1], [θ2], according to Deﬁnition 3.17, is:
dS c
1 ([θ1], [θ2]) = min
τ∈S1 dC c
1 (θ1, (τ, θ2)) .
This min exists because S1 is a compact set. This is depicted pictorially on the
right side of Fig. 6.3.
2. Case 2: The task of unifying re-parameterization and deﬁning equivalence
classes for open curves represented by SRVFs was studied in Sect. 5.5. The case
for closed curves is similar, except this time the domain of parameterization

176
6 Shapes of Planar Closed Curves
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Fig. 6.2 Top row: examples of (origin-preserving) increasing diﬀeomorphisms on a circle, dis-
played by splitting S1 into [0, 1] at an arbitrary point. The ﬁrst function is identity. Bottom row:
the corresponding parameterization of β demonstrated using some grid points on [0, 2π]
v
v⊥
v∥
q
[q ]
S c
1
Cc
1
w
θ1
q2
[q2]
[q1]
d(q1, q2)
d([q1],[q2])
q∗
2
a
b
Fig. 6.3 (a) The vertical lines denote orbits in the pre-shape spaces C c
1 (C c
2 ). The set of these
orbits forms the shape space S c
1 (S c
2 ). (b) The distances between orbits are computed by ﬁxing
one of the shapes, say θ1 (q1), and optimizing over the other orbit, say [θ2] ([q2])
is S1, instead of [0, 1]. Now the re-parameterization group, ΓS, is the group
of all orientation-preserving diﬀeomorphisms from S1 to itself. For a curve
β : S1 →R2 and γ ∈ΓS, a re-parameterization function, β ◦γ, is the re-
parameterized curve. Some examples of re-parameterization functions are shown
in the top row of Fig. 6.2; for the purpose of visualization we select an origin
on the curve and then the re-parameterization function can be thought of as a
diﬀeomorphism from some closed unit interval in R, say [0, 1], to itself. The bot-
tom row shows the corresponding re-parameterized curves with ticks on them.
In the ﬁrst ﬁgure, where the ticks are equally spaced, the parameterization is
by arc length.
For elements of C c
2 , the variability associated with re-parameterizations of
curves is represented using the following group action. The group action C c
2 ×
ΓS →C c
2 is the same as in Eq. 5.19: (q, γ) →(q ◦γ)√˙γ. The function ˙γ here
should be thought of as a function from S1 to the positive real numbers and is
deﬁned by ﬁrst thinking of γ as a diﬀeomorphism from [0, 1] to itself and then
diﬀerentiating this function.

6.2 Representations of Closed Curves
177
Similarly, the rotations of a curve can be represented by the action SO(2) ×
C c
2 →C c
2 deﬁned by
(O, q) →{Oq(t)|t ∈S1} .
(6.16)
How do the two group actions interact with each other?
Lemma 6.1. The actions of SO(2) and ΓS on C c
2 commute.
Proof. Same as Lemma 5.1 in Chap. 5.
Since these actions commute, they combine to give an action of the product
group ΓS × SO(2) on C c
2 . We can then obtain the equivalence class, or orbit,
associated with a curve q under the actions of ΓS and SO(2):
[q] = closure{(Oq, γ)|(γ, O) ∈ΓS × SO(2)} ,
(6.17)
and the shape space using the SRVF representation is deﬁned by
S c
2 = {[q]|q ∈C c
2 } .
Once again, the cartoon illustration in Fig. 6.3a shows this idea where each
vertical line denotes an orbit and the quotient space is a collection of such
orbits. The reason for including the closure of these orbits, rather than the
orbits themselves, is the same as in Sect. 5.5 and is not repeated here.
The next consideration is the metric structure on S c
2 . To see that S c
2 inherits
a metric structure from C c
2 , we establish the following important result.
Lemma 6.2. The action of the product group ΓS×SO(2) on C c
2 is by isometries
with respect to the metric deﬁned in Eq. 6.10.
Proof. Same as Lemma 5.2 in Chap. 5.
Since the action of ΓS×SO(2) is by isometries, the distance resulting from the
Riemannian metric in Eq. 6.10 descends to the quotient space S c
2 . Therefore,
the distance between any two orbits, say [q1] and [q2], (using Deﬁnition 3.17) is
dS c
2 ([q1], [q2]) =
inf
(γ1,O1),(γ2,O2)∈ΓS×SO(2) dC c
2 (O1(q1, γ1), O2(q2, γ2))
=
inf
(γ,O)∈ΓS×SO(2) dC c
2 (q1, ((O, q2), γ)) .
We have narrowed our representation of shapes to the elements of these two
shape spaces - S c
1 and S c
2 . What are the properties of these spaces? First, an
important property is that two distinct elements of a shape space denote two dif-
ferent shapes. We have taken care of unifying all possible equivalences—all curves
obtained by rotating, translating, scaling, and re-parameterizing the same curve
β are in the same equivalence class as β, and they all map to the same point in
the shape space. The ﬁrst representation is quite restricted where the curves are
assumed to be arc-length parameterized, while the second representation allows
for more general parameterizations of curves. Secondly, the diﬀerential geometries
of shape spaces closely relate to those of the corresponding pre-shape spaces. As a
result, the construction of geodesics in shape spaces will be similar to construction
of geodesics in the corresponding pre-shape spaces.

178
6 Shapes of Planar Closed Curves
6.3 Projection on a Manifold
We are going to develop numerical approaches for ﬁnding geodesics on pre-shape
and shape spaces of closed planar curves. Since the pre-shape spaces are nonlinear
manifolds embedded inside larger Hilbert spaces, it is reasonable to obtain points,
through observations or processing, that are not in the desired manifolds but in
the larger spaces. So, we would like have a general numerical tool for projecting
points of the larger space into the manifolds of interest. We point out that for
some manifolds, there are analytical expressions to perform this projection. For
example, in case of a unit sphere S2, the projection of a point x in R3 to the nearest
point in S2 is simply
x →x
|x| .
However, for the pre-shape spaces deﬁned in this chapter we will have to resort to a
numerical technique. Since this tool will be quite useful, we ﬁrst try to understand
the basic idea behind it with a speciﬁc example. Consider the ﬁrst pre-shape space,
C c
1 ; its elements satisfy the following constraints:
1
2π
 2π
0
θ(s)ds = π,
 2π
0
sin(θ(s))ds = 0,
 2π
0
cos(θ(s))ds = 0 ,
and the constraint θ(2π) = θ(0) + 2π. If we have an angle function that is in the
larger space L2([0, 2π], R), but does not satisfy some of these conditions, it will
not be in C c
1 . If the ﬁrst condition is not satisﬁed, then one can simply rotate the
underlying curve to get the correct orientation. However, if the closure condition
is not satisﬁed, there is no obvious ﬁx to close an open curve. More precisely, the
question that we are asking is: for the given θ ∈L2([0, 2π], R) and θ /∈C c
1 , how to
ﬁnd the closest point in C c
1 ?
Here, we derive a projection technique for a general manifold M ⊂H , where
H is a Hilbert space and M has been deﬁned as a level set of Φ : H →Rn, much
like our two pre-shape spaces, C c
1 and C c
2 .
Problem Statement: Let Φ be a smooth mapping such that dΦ is surjective at
each point on its domain. Let the level set of Φ at a, i.e., M = Φ−1(a), be a
manifold of interest, for some a ∈Rn. For any given p ∈H (but not in M), we
want a tool to project it into M. In other words, we want to change p as eﬃciently
as possible in such a way that Φ(p) reaches a.
Solution: Our approach is to move along a path perpendicular to the level sets
of Φ such that the images under Φ of points in this path move toward a in Rn
along a straight line. For the starting point p, let Φ(p) = b (b ̸= a), which means
that p is in the level set Φ−1(b). To update p, there are two orthogonal spaces:
the tangent space Tp(Φ−1(b)) and the normal space Np(Φ−1(b)). Since moving
p along a tangent direction will not change its Φ-value, the most eﬃcient way to
change the value of Φ(p) is to move in the normal direction. As Φ maps H to Rn,
the derivative dΦp at p also maps H to Rn. It follows that the restriction of dΦp
induces an isomorphism Np(Φ−1(b)) →Rn. Since Np(Φ−1(b)) is an n-dimensional
vector space, we can express this isomorphism (dΦp restricted to Np(Φ−1(b))) as
an n × n matrix J with respect to an orthonormal basis {wi, i = 1, 2, . . ., n} of
Np(Φ−1(b)).

6.4 Geodesic Computation
179
Fig. 6.4 A cartoon illustration of projecting an arbitrary point p ∈L2 into the level set M =
Φ−1(a) using the Φ map and its Jacobian J
We want to ﬁnd a displacement vector pδ ∈Np(Φ−1(b)) at p such that Φ(p +
pδ) ≈a, up to the ﬁrst order. If we deﬁne the residual vector r(p) = a −Φ(p), the
displacement vector is given by pδ = δ n
i=1 βiwi, where β = J−1r(p) and δ > 0 is
an empirically chosen step size. Update the point p using p = p + pδ, and iterate
until the norm ∥r(p)∥converges to zero. These steps are summarized below:
Algorithm 5. 1. Compute the residual vector r(p) = a −Φ(p).
2. Compute the n × n Jacobian matrix J of dΦp with respect to a basis
{w1, w2, . . . , wn} of Np(Φ−1(b)), where Φ(p) = b.
3. Compute β = J−1r(p) and set pδ = δ n
i=1 βiwi, for a small δ > 0.
4. Update p to p + pδ.
We will use this tool repeatedly to map arbitrary points into pre-shape spaces.
Examples of this algorithm for projecting curves into pre-shape spaces C c
1 and C c
2
are presented later (Figs. 6.8 and 6.14).
6.4 Geodesic Computation
The next task is to compute geodesics between elements of pre-shape and shape
spaces of closed curves. It was possible to determine geodesics in the previous chap-
ter using analytical expressions, at least on the pre-shape spaces, but the current
cases are more complicated. For these cases we shall develop two computational
approaches for ﬁnding geodesics that are broadly applicable. This discussion ap-
plies to any Riemannian manifold, although our interest remains in the pre-shape
and shape spaces, C c
i s and S c
i s.
There are two main ideas in numerical construction of geodesic paths on man-
ifolds. In the ﬁrst approach, called the shooting method, one tries to “shoot” a
geodesic from the ﬁrst point, iteratively adjusting the shooting direction until the
resulting geodesic passes through the second point. A cartoon illustration of this
idea is shown in the left panel of Fig. 6.5. The second method, called the path-
straightening method, one initializes with an arbitrary path between the given two
points on the manifold and then iteratively “straightens” it until a geodesic is
reached. The right panel of Fig. 6.5 shows a cartoon illustration of this approach.
In the next two sections, we describe these mathematical constructions with a
focus on our pre-shape spaces C c
1 and C c
2 :

180
6 Shapes of Planar Closed Curves
M
Initial Shooting
Direction
Final Shooting
Direction
p2
p1
H
v
M
p2
p1
Final Path
Initial Path
Fig. 6.5 Numerical approaches for construction of geodesics: shooting method (left) and path-
straightening method (right)
6.5 Geodesic Computation: Shooting Method
Assume that M is a submanifold of a Hilbert space H , with the Riemannian
structure that is inherited from H . Our goal is to compute geodesics between any
two points p1 and p2 ∈M, with respect to that metric. The basic approach is as
follows:
1. Select one of the points, say p1, as the starting point and the other, p2, as the
target point.
2. Construct a geodesic starting from p1, in an arbitrary direction v ∈Tp1(M);
denote it by ατ(p1; v) where τ is the time parameter for the geodesic ﬂow.
3. If this geodesic reaches p2 in unit time, i.e., α1(p1; v) = p2, then we are done.
If not, measure the amount of miss, the discrepancy between α1(p1; v) and
p2, using a simple measure, e.g.,
the distance in H . Call this discrepancy
function F.
4. Iteratively, update the shooting direction v to reduce this discrepancy F to zero.
We will use the gradient of F to update v in the tangent space Tp1(M), but
there are other ideas that are equally eﬀective.
Implementation of these algorithms requires two important pieces. First, given a
point p1 ∈M and a direction v ∈Tp1(M), we need to construct the geodesic path
ατ(p1; v), and we will do so in a general setting using numerical approximations.
Second, given two points p1 and p2, we need to ﬁnd a direction v ∈Tp1(M) such
that α1(p1; v) = p2. This ordering of the problem is important because the second
piece will use the ﬁrst piece. Actually, the solution of the ﬁrst piece can be used
to evaluate the exponential map and that of the second piece to evaluate the
inverse exponential map. Please refer to Sect. 3.2 for deﬁnitions and examples of
the exponential map and its inverse for some simple manifolds.
1. Evaluation of the Exponential Map: Given a point p ∈M and a tangent
vector v ∈TP (M), the goal is to ﬁnd the value of the exponential map expp(v) ∈
M. This point expp(v) is the point reached by the constant-speed geodesic
starting from p and with the initial velocity v at time τ = 1. Additionally, we
want the geodesic path that goes from p to expp(v). Our construction is based

6.5 Geodesic Computation: Shooting Method
181
p
q′
q
time, t
1
0
v
X (t)
Fig. 6.6 A numerical approximation of the exponential map
on numerically solving the diﬀerential equation d
dtα(τ) = V (α(τ)), where V is
a vector ﬁeld on M obtained by parallel transport of v, along geodesic paths,
to all of M. The parallel transport of a vector along a geodesic was discussed
in Sect. 3.4 (Deﬁnition 3.8) but the transport along more general paths will be
deﬁned later.
A numerical solution to this problem is illustrated in Fig. 6.6. The idea is to
start at α(0) = p and follow the vector V (p) = v for a small time in the larger
space H . At τ = ϵ, we reach α(0) + ϵv, but this new point ˜pϵ ≡α(0) + ϵv
may not be in M, since M is a nonlinear manifold and our increment is linear.
Assuming that we have a mechanism (an analytical expression or a numerical
procedure of the type presented in Algorithm 5) to project ˜pϵ to a nearby point
in M, using the structure of M, we can obtain that point, say pϵ. This way
we get an approximation of the geodesic’s location at time t = ϵ. We repeat
this process for the next time interval [ϵ, 2ϵ], this time starting from pϵ. For the
tangent direction at pϵ we use V (pϵ), obtained by parallel transport of v from p
to pϵ. For this part we can assume that ϵ was small enough so that the path from
p to pϵ is approximately a geodesic. There are two steps that are important in
this implementation: (i) projection of a point ˜pϵ ∈H to the nearest point in M
and (ii) parallel transport of a vector v ∈Tp(M) to a point pϵ ∈M that is not
far from p. To extend the geodesic to a desired time τ, we repeat these steps,
taking time increments of size ϵ until we reach τ. In this setup, the geodesic
is actually computed only at the times αkϵ(p1; v), for k = 1, 2, . . .. Since we
interested in the value of at time τ = 1, we choose ϵ to be 1
T and take T steps
of the geodesic to compute α1(p1; v). Using straightforward calculus, one can
show that the value of αkϵ(p1; v) converges to the desired continuous geodesic
ατ(p1; v) as T →∞. Also, the resulting path approximates a constant-speed
geodesic.
2. Evaluation of Inverse Exponential Map: Now that we have a tool for
computationally approximating a geodesic path from a point p with the initial
velocity v ∈Tp(M), we can tackle the problem of ﬁnding a geodesic path
between any two given points p1, p2 ∈M. We can rephrase this as follows: ﬁnd
the initial velocity v ∈Tp1(M) such that the constant-speed geodesic from p1
in the direction v passes through p2 at time τ = 1. If we choose v arbitrarily,

182
6 Shapes of Planar Closed Curves
this condition will not be satisﬁed unless we get very lucky. So the alternative
is to start with an initial condition for v and iteratively update it in such a way
that the end point of the geodesic gets closer and closer to p2 and ultimately
reaches it. By end point, we mean the point reached by the geodesic at time
τ = 1. This update can be performed in many ways and we will introduce a
simple gradient method to perform these updates.
One can treat the search for this initial velocity as an optimization problem
on the space Tp1(M). The cost function we wish to minimize is given by the
functional: F : Tp1(M) →R, F[v] = ∥α1(p1; v) −p2)∥2, under the norm associ-
ated with H , and we are looking for that v ∈Tp1(M) for which (i) F[v] is zero
and (ii)

⟨v, v⟩(under the chosen Riemannian metric) is minimum among all
such tangents.
Remark. In case M and Tp1(M) are inﬁnite dimensional, this optimization is
not straightforward, and we need to use a ﬁnite-dimensional approximation of
Tp1(M) to ﬁnd the optimal direction. Take the case when H = L2([0, 2π], R)
and M ⊂L2([0, 2π], R); now v ∈L2 is a real-valued function. There are sev-
eral ways to approximate elements of Tp1(M), one of them being the Fourier
approximation described in Sect. A.3.1. The function v can be approximated
using a ﬁnite number of terms in the Fourier expansion. In other words, replace
v ∈Tp1(M) by π(m
n=0(an cos(nt) + bn sin(nt))) for a large positive integer m.
The mapping π here denotes a projection from the space L2 to the tangent space
Tp1(M). (This mapping π becomes the third requirement for implementing the
shooting method, in addition to the abovementioned needs for projection into
M and parallel transport on M.) With this approximation, the cost function
modiﬁes to ˜F : R2m+1 →R+:
˜F(a, b) = ∥α1(p1; π(
m

n=0
an cos(ns) + bn sin(ns))) −p2∥2 .
(6.18)
Now one can use a gradient-based approach for minimizing ˜F as a function
of the Fourier coeﬃcients an, bn. The gradient, in general, will be computed
numerically for the shape manifolds. The tangent vector can then be updated
in the direction of negative gradient to update the shooting direction.
Another idea for iteratively reﬁning the shooting direction, this time without
the use of the gradient of F, is to compute the miss vector w ≡(p2−α1(p1; v)) ∈
H , for the current α, project it into the tangent space of α1(p1; v), and parallel
transport it to p1. Call this transport ˜w and update v according to v + δ ˜w for
a small δ.
We illustrate the shooting method on two spaces: (1) S2, as a simple example
and (2) C c
1 , as the main tool for constructing geodesics between non-elastic planar
closed curves.
6.5.1 Example 1: Geodesics on S2
As a simple example of the shooting method, we consider the problem of con-
structing geodesics between points on S2 under the standard Euclidean metric.
Even though this problem has a well-known analytical solution, we solve it using

6.5 Geodesic Computation: Shooting Method
183
the shooting method for the purpose of illustration. As stated previously, we need
the following three items to implement the shooting method:
1. Projection on S2: Any nonzero point x ∈R3 can be mapped to the nearest
point in S2 using the projection:
x →x
|x|.
(6.19)
2. Parallel Transport of Tangents: For points x and y in S2, x ̸= −y, the
parallel transport of a vector v ∈Tx(S2) along the shortest geodesic (i.e., great
circle) from x to y, is
v −→(v −2 ⟨v, y⟩(x + y)
|x + y|2
) ∈Ty(S2) .
(6.20)
This result has been derived earlier in Eq. 3.9.
3. Projection on Tangent Space: A vector w ∈R3 can be projected on to the
tangent space Tx(S2) using
w −→(w −⟨w, x⟩x) .
(6.21)
The shooting method solves for the inverse exponential map on a manifold and,
in turn, requires a tool for evaluating the exponential map. An algorithm for
numerically evaluating the exponential map expx(v) on S2 follows. Once again, we
already have an analytical form for this map but present a numerical method only
for illustration.
Algorithm 6 (Exponential Map on S2).
Let x ∈S2 and v ∈Tx(S2). Fix an ϵ = 1/T small. Set i = 0, α0 = x and w(iϵ) = v.
1. Compute αiϵ + ϵw(iϵ) in R3 and project it on S2 using Eq. 6.19. Call this point
α(i+1)ϵ.
2. Transport w(iϵ) at αiϵ to the new point α(i+1)ϵ using the Eq. 6.20.
3. If i = T , stop and return αT ϵ. Else, set i = i + 1 and go to Step 1.
Now the algorithm for ﬁnding the initial velocity for going from x1 to x2 in S2, or
for evaluating the inverse of the exponential map exp−1
x1 (x2), is as follows.
Algorithm 7 (Shooting Method on S2).
Let x1, x2 ∈S2. Let e1
x1, e2
x1 be an orthonormal basis of Tx1(S2). Initialize a
direction v ∈Tx1(S2), perhaps using
v = w −⟨w, x1⟩x1,
w = (x2 −x1) .
Select small numbers ϵ,
δ > 0.
1. Current Cost: Compute the exponential map expx1(v) using Algorithm 6.
Compute the cost function of reaching this point: F(v) = |x2 −expx1(v)|2.
2. Gradient of Cost: For v′
j = v + δej
x1, j = 1, 2, Compute the exponential map
expx1(v′
j) using Algorithm 6 and the cost function F(v′
j) = ∥x2 −expx1(v′
j)∥2.

184
6 Shapes of Planar Closed Curves
x2 = (0,0,1)
x2 = (0,0,1)
x2 = (0,0,1)
x1 = (1,0,0)
x1 = (1,0,0)
x1 = (1,0,0)
0
5
10
15
20
25
30
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
Iterations
L2 Energy
Fig. 6.7 Shooting method on S2. Top: iterative improvements in the shooting direction. Bottom:
the evolution of F during minimization
Approximate the partial derivatives
∂F
∂v(j) ≈1
δ (F(v′
j) −F(v)) for j = 1, 2. The
gradient of F at v is then given by g =
∂F
∂v(1)e1
x1 +
∂F
∂v(2)e2
x1.
3. Update Direction: Replace v by v −ϵg.
4. If the norm of the gradient g is small, then stop. Else, return to Step 1.
Shown in Fig. 6.7 is an example of computing a geodesic between x1 = (1, 0, 0) and
x2 = (0, 0, 1) using the shooting method. The ﬁrst panel shows an initial shooting
direction v going horizontally and the geodesic shot in that direction. The next
two panels show iterative updates on v so that the geodesic hits x2 at time τ = 1.
Eventually, the shooting direction becomes vertical and reaches x2. The plot on the
bottom shows the decrease in the value of F as the shooting direction is iteratively
improved.
This experiment also highlights a major limitation of the shooting method. On
a sphere, there are two geodesics connecting any two given points, both along
the same great circle containing the two points, but one forming the shorter arc
and the other forming the longer arc. If the initial shooting direction v is closer
to the longer arc, this gradient-based search for geodesic will result in the longer
geodesic. Thus, this approach has the possibility of reaching a local solution to the
optimization problem. It will ﬁnd a geodesic but there is no guarantee that it will
be the minimal geodesic. In some cases, for example, in the sphere, one can choose
an initial shooting direction carefully so as to be closer to the desired solution. Of
course, for more general manifolds this may not be as straightforward.

6.5 Geodesic Computation: Shooting Method
185
6.5.2 Example 2: Geodesics in Non-elastic Pre-shape
Space
The second example that we consider is C c
1 , the pre-shape space of closed, pla-
nar curves represented by their angle functions and endowed with the non-elastic
metric. Given any two points θ1, θ2 ∈C c
1 , we want to construct a geodesic path
between them in C c
1 under the chosen Riemannian metric. As in the case of S2,
we need the following three items to implement the shooting method.
1. Projection on C c
1 : Recall that C c
1 is a codimension-3 submanifold of the am-
bient space
D = {continuous θ ∈L2([0, 2π], R) : θ(2π) = θ(0) + 2π},
and that the tangent space of D is given by
B = {continuous θ ∈L2([0, 2π], R) : θ(2π) = θ(0)}.
Unlike S2, we do not have an analytical expression for projecting points from
D into the manifold C c
1 . So we will use the technique described in Algorithm 5.
Recall that C c
1 is the level set of Φ1 : D →R3 for the value (π, 0, 0). Thus,
to project a given θ ∈D to C c
1 , we need to alter θ as eﬃciently as possible
to arrange that Φ1(θ) becomes (π, 0, 0). The ﬁrst condition, namely, Φ1
1(θ) =
1
2π
 2π
0
θ(s)ds = π, is easy to impose. Simply change θ by θ −1
2π
 2π
0
θ(s)ds + π.
The remaining two conditions require an iterated procedure. We take the current
point θ and deﬁne a displacement θδ as follows. Recall that the normal space
Nθ(C c
1 ) is spanned by the functions 1, sin(θ(s)), and cos(θ(s)), ignoring the
constant function 1, since the corresponding constraint is already taken care of,
we focus on the other two functions. The 2×2 Jacobian matrix J1 of [dΦ2
1 dΦ3
1]
is given by
J1 =

−⟨sin ◦θ, sin ◦θ⟩−⟨sin ◦θ, cos ◦θ⟩
⟨cos ◦θ, sin ◦θ⟩
⟨cos ◦θ, cos◦θ⟩

.
(6.22)
Deﬁne the residual vector as r(θ) = [0
0] −[Φ2
1(θ)
Φ3
1(θ)] ∈R2. Then, the
desired increment is given by
θδ = δ
2

i=1
βiwi,
(6.23)
where β = J1(θ)−1 r(θ) and (w1(s), w2(s)) = (sin(θ(s)), cos(θ(s))). This leads
to the following procedure.
Algorithm 8. Project a given θ ∈D into C c
1
a. Set θ = θ −
1
2π
 2π
0
θ(s)ds + π.
b. Compute θδ according to Eq. 6.23.
c. Update the angle function θ →θ + θδ. If |θδ| > ϵ, go to Step 1.
Shown in Fig. 6.8 are some examples of projecting curves, represented by
their angle functions in D, to the nearest points in C c
1 . The original points
θ ∈D are shown in terms of their corresponding curves, drawn in solid lines.
The projected angle functions are displayed by their curves in marked lines.

186
6 Shapes of Planar Closed Curves
−2
−1.5
−1
−0.5
0
0.5
0
0.5
1
1.5
−1.5
−1
−0.5
0
0.5
−0.2
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
−2.5
−2
−1.5
−1
−0.5
0
0.5
1
−1
−0.5
0
0.5
1
1.5
Fig. 6.8 Examples of open curves in represented by points in L2([0, 2π], R) (solid lines) and
their projections in C c
1 (marked lines) using Algorithm 8
2. Parallel Translation of Vectors: The second requirement is a tool for trans-
porting a vector v ∈Tθ(C c
1 ) to a nearby point θ′ ∈C c
1 . Since θ and θ′ are close,
this transport can be approximated, up to the ﬁrst order, by simply projecting
v into Tθ′(C c
1 ) and re-scaling it to be of the correct length.
If wi, i = 1, 2, 3 are orthonormal basis elements of the normal space
Nθ′(C c
1 ) = span{1, sin(θ(s)), cos(θ(s))} in L2([0, 2π], R), then this approxi-
mation is given by
v →|v|

v −3
i=1 ⟨v, wi⟩wi
|v −3
i=1 ⟨v, wi⟩wi|

.
(6.24)
3. Projection into Tangent Spaces: The last requirement is a formula for pro-
jecting elements of B into tangent spaces. For any function v ∈B, it can be
projected into the tangent space Tθ(C c
1 ) using the equation:
v →v −
3

i=1
⟨v, wi⟩wi ,
(6.25)
where wi’s are the orthonormal basis functions mentioned in the previous item.
Using these three items, we can write down an algorithm for ﬁnding geodesics
in C c
1 using the shooting method. We start with the forward problem of computing
the geodesic when the starting point and the starting direction are given.
Algorithm 9 (Exponential Map expθ(v)).
Let θ ∈L2([0, 2π], R) and v ∈Tθ(C c
1 ). Fix an ϵ = 1/T small. Set τ = 0, αiϵ = θ
and wτϵ = v.
1. Compute ατϵ + ϵwτϵ and project it on C c
1 using Algorithm 8. Call this point
α(τ+1)ϵ.
2. Transport wτϵ at ατϵ to the new point α(τ+1)ϵ using the Eq. 6.24.
3. If τ = T , stop and return αT ϵ. Else, set τ = τ + 1 and go to Step 1.
With this procedure for approximating the exponential map on C c
1 , we can try
to solve the optimization problem for ﬁnding the desired initial velocity. Recall
the earlier remark that when the underlying manifold is inﬁnite dimensional, one

6.5 Geodesic Computation: Shooting Method
187
has to restrict the search for optimal direction to a ﬁnite-dimensional subspace of
the tangent space at the starting point. We can particularize the resulting cost
function, given in Eq. 6.18, for C c
1 resulting in
˜F (a, b) = ∥expθ1

π
 m

n=1
an cos(ns) + bn sin(ns)

−θ2∥2 .
(6.26)
Here, π refers to the projection of an arbitrary element of B into the tangent space
Tθ1(C c
1 ), given in Eq. 6.25. We point out that the two Fourier terms for n = 0 have
not been included in the summation because (i) the cosine term for n = 0 is a
constant that will be readjusted in the projection π and (ii) the sine term is zero
anyway.
For the gradient-based update of the shooting direction, we will need to ap-
proximate the gradients of ˜F with respect to the coeﬃcient vectors a and b. One
simple approximation is given by ﬁnite diﬀerences: for n = 1, 2, . . . m,
∂˜F
∂an
≈1
2ϵ

˜F(a + ϵen, b)) −˜F(a −ϵen, b))

∂˜F
∂bn
≈1
2ϵ

˜F(a, b + ϵen)) −˜F(a, b −ϵen))

.
(6.27)
Here {en} is the canonical basis vector of Rm: en has zeros everywhere except in
the nth location where it is one.
Using these ideas, we can now sketch an algorithm for ﬁnding the geodesics in
C c
1 using the shooting method.
Algorithm 10 (Shooting Method on C c
1 ).
Given points θ1, θ2 ∈C c
1 , initialize the coeﬃcients a = {a1, . . . , am}, b =
{b1, . . . , bm}. Fix a δ > 0 small.
1. Current
Cost:
Find
the
current
shooting
direction
v
by
projecting
m
n=1 an cos(ns) + bn sin(ns) into Tθ1(C1) using Eq. 6.25. Compute the ex-
ponential map expθ1(v) using Algorithm 9 and evaluate the cost function
F(v) = ∥θ2 −expθ1(v)∥2.
2. Compute Gradient: Approximate the partial derivatives
∂˜
F
∂an and
∂˜
F
∂bn , n =
1, 2, . . . , m, using Eq. 6.27.
3. Update: Update a, b using the negative gradient of ˜F:
a →a −δ∇a ˜F,
b →b −δ∇b ˜F .
4. Stopping Criterion: If the norms of the two gradient vectors are both small,
then stop. Otherwise, return to Step 1.
In Figs. 6.9, 6.10, we present some examples of geodesic paths in C c
1 obtained
using Algorithm 10. Each row in these ﬁgures shows a geodesic path: the ﬁrst and
the last curves correspond to the given angle functions θ1 and θ2, respectively, and
the intermediate curves denote equally spaced points along the geodesic ατ(θ1; v)
in C c
1 , for τ = 0, 1/T, 2/T, . . ., 1, that passes through θ2 at τ = 1. Figure 6.9 shows
some geodesic paths between some polygons and curvilinear objects. For each
of the curves shown in these examples, the corresponding angle function results

188
6 Shapes of Planar Closed Curves
Fig. 6.9 Examples of geodesics paths between some curves in C c
1
Fig. 6.10 Geodesics between sample two curves in C c
1 , with the diﬀerence that the origin has
been placed diﬀerently in the target curves
from a certain placement of origin on that curve. For the initial and the target
curves, these placements are assumed given while for the intermediate curves,
the algorithm dictates the placements of origins. We draw the attention of the
reader to an earlier statement about the chosen Riemannian metric on C c
1 and its
interpretation as allowing only bending in shapes. These geodesics represent the
paths of minimum bending energy in going from θ1 to θ2, and this eﬀect can be
seen in the examples presented here.
Figure 6.10 highlights a limitation of geodesics in C c
1 in terms of shape analysis.
In this example, all the target curves have the same shape as the initial curve except
that the origin has been placed diﬀerently. These targets are diﬀerent elements of
an orbit of S1 in C c
1 , and, as a consequence, the geodesics are visibly diﬀerent and
have nonzero path lengths. We emphasize that the re-parameterization group for
this case, S1, is still to be removed, and thus the optimal placement of origin on
the target curve (for optimally aligning the two curves) has not been reached. This
part will be accomplished when we consider geodesics in the quotient space S c
1 .

6.6 Geodesic Computation: Path-Straightening Method
189
6.6 Geodesic Computation: Path-Straightening Method
The second numerical approach for ﬁnding geodesic paths between points on a
Riemannian manifold is based on a procedure called path straightening. The main
idea here is the following: initialize a path between the two given points on the
manifold and iteratively straighten it, using the gradient of an appropriate energy
function, until it cannot be straightened any further. The resulting path will be a
geodesic path. A cartoon illustration of this approach is shown in the right panel
of Fig. 6.5. Relative to the shooting method, there are several advantages to this
method:
1. The solution is, by construction, guaranteed to start and end at the desired
points in the manifold. In contrast, the shooting method can result in geodesic
paths that do not quite reach the target point because the miss function did
not reach zero.
2. In case the gradient of the chosen energy can be written analytically, which is
the case in this chapter, the computational cost of implementing it is relatively
low.
3. There is no need to approximate the tangent spaces (in case of inﬁnite-
dimensional manifolds) with ﬁnite-dimensional subspaces. This was done when
using the shooting method on a pre-shape space so that the optimization prob-
lem could be restricted to a ﬁnite-dimensional space.
At ﬁrst we explain the theoretical setup for the path-straightening method and
then present numerical procedures for implementing it.
6.6.1 Theoretical Background
We describe this procedure on a general Riemannian manifold M with the following
restriction. We will assume that the manifold M is a submanifold of a Hilbert space,
with the Riemannian structure inherited from that larger space. We will denote
this larger ambient space by H . This condition certainly holds for our pre-shape
spaces. (In those cases, the ambient vector space is H = L2.) Now we pose the
formal problem of ﬁnding geodesics on M. Say we are given two points p1 and p2
in M that we want to join using a geodesic path in M. Let A be the set of all
diﬀerentiable paths in M, whose ﬁrst derivatives are L2 functions, parameterized
by τ ∈[0, 1]:
A = {α : [0, 1] →M| α is diﬀerentiable and ˙α ∈L2([0, 1], M)} ,
and A0 be the subset of A consisting of those paths that start at p0 and end at
p1:
A0 = {α ∈A |α(0) = p1 and α(1) = p2} .
The desired geodesic is an element of A0. For elements of A , deﬁne an energy
function E : A →R+ by

190
6 Shapes of Planar Closed Curves
E[α] = 1
2
 1
0
⟨˙α(τ), ˙α(τ)⟩dτ .
(6.28)
We have some remarks about this deﬁnition of E:
• Note that for each τ, ˙α(τ) is an element of Tα(τ)(M), and, by our assumption
on A , ˙α : [0, 1] →T M is an L2 function. The inner product appearing inside
the integral sign comes, of course, from the Riemannian metric on M.
• E is not the length of the path α, although it is closely related. If we use the
square-root of the integrand (and remove the 1/2 factor from the front), we
obtain the length of the path α (see Eq. 5.9). Thus, E is 1/2 the integral of the
square of the instantaneous speed along the curve (the integral is taken with
respect to the parameter τ).
• Later we will show that the critical points of E on the space A0 are precisely
the constant-speed geodesic paths on M between p1 and p2. Therefore, one way
to ﬁnd a geodesic is to use the gradient of E to reach its critical points. This is
the method we will describe.
We will be using the gradient of E to ﬁnd its critical points on A0. So we start
with the diﬀerential structure of A0. The tangent spaces of A and A0 are
Tα(A ) = {w : [0, 1] →T M| Dw
dτ ∈L2 and ∀τ ∈[0, 1], w(τ) ∈Tα(τ)(M)} ,
where Tα(τ)(M) is the tangent space of M at the point α(τ) ∈M, and
Tα(A0) = {w ∈Tα(A )|w(0) = w(1) = 0} .
Note that the tangent space element w is, by deﬁnition, a vector ﬁeld along the
path α tangent to M at each point of α. Next, we introduce some tools from covari-
ant calculus, the calculus dealing with diﬀerentiation and integration of tangent
vector ﬁelds along paths on manifolds. More speciﬁcally, we will deﬁne covari-
ant derivatives and integrals of vector ﬁelds. A vector ﬁeld w along α implies a
time-indexed collection of tangent vectors along α:
{w(τ) ∈Tα(τ)(M), τ ∈[0, 1]} .
Refer to the Fig. 6.11 for an example each of the elements of Tα(A ) and Tα(A0)
for a path on a unit sphere.
Fig. 6.11 Examples of path α and tangent vector ﬁeld w on it. The left pictures show an
example of w ∈Tα(A ) while the right picture shows a w ∈Tα(A0)

6.6 Geodesic Computation: Path-Straightening Method
191
Deﬁnition 6.2 (Covariant Derivative). For a given path α ∈A and a vector
w ∈Tα(A ) (a vector ﬁeld along α), deﬁne the covariant derivative of w along α,
denoted Dw
dτ , to be the vector ﬁeld obtained by projecting dw
dτ (τ) onto the tangent
space Tα(τ)(M), for all τ. Note that given w ∈Tα(A ), Dw
dτ is an L2 vector ﬁeld
along α.
Since M ⊂H is not a linear submanifold, the vector dw
dτ , for any τ, is an element of
H , but not necessarily of Tα(τ)(M). The projection ensures that the resulting set
of vectors form a vector ﬁeld along α that is tangent to M at each point. (Since this
deﬁnition assumes that M is a submanifold of a Hilbert space, with the inherited
metric, this is not the most general deﬁnition for the covariant derivative.) The
opposite of a covariant derivative is the covariant integral.
Deﬁnition 6.3 (Covariant Integral). A vector ﬁeld u ∈Tα(A ) is called a
covariant integral of w along α if the covariant derivative of u is w, i.e., Du
dτ = w.
Since we are interested in the gradient of E on A , we need to impose a Rieman-
nian structure on A also. The most obvious way to do this would be to deﬁne
the inner product of w1 and w2 (where w1 and w2 are vector ﬁelds along α) by
 1
0 ⟨w1(τ), w2(τ)⟩dτ. However, this obvious inner product would have some fatal
disadvantages for us. The main one is that Tα(A0) would not be a closed sub-
space of Tα(A ). (That’s because one can easily construct a sequence of elements
of Tα(A0) that converges to an element of Tα(A ) −Tα(A0) with respect to this
metric.) To remedy this problem, we instead make A a Riemannian manifold using
the ﬁrst-order Palais metric on A [87]: for w1, w2 ∈Tα(A ), deﬁne
⟨⟨w1, w2⟩⟩= ⟨w1(0), w2(0)⟩+
 1
0
Dw1
dτ (τ), Dw2
dτ (τ)

dτ ,
(6.29)
where Dw/dτ denotes the covariant derivative of w along α. With respect to the
Palais metric, Tα(A0) is a closed linear subspace of Tα(A ), and A0 is a closed
subspace of A .
The next step is to calculate the gradient of E, with respect to α, as an element
of Tα(A0). To do this, we ﬁrst ﬁnd the gradient of E in the larger space Tα(A )
(with respect to the Palais metric) and then orthogonally project it into Tα(A0)
(where, again, the projection is with respect to the Palais metric).
Theorem 6.4. The gradient vector of E in Tα(A ) is given by a vector ﬁeld u
along α satisfying Du/dτ = dα/dτ and u(0) = 0. In other words, u is the covariant
integral of the vector ﬁeld ˙α(τ), with zero initial value at τ = 0.
Proof. Deﬁne a variation of α to be a function h : [0, 1] × (−ϵ, ϵ) →H (with L2
ﬁrst derivative) such that h(τ, 0) = α(τ) for all τ ∈[0, 1]. The variational vector
ﬁeld corresponding to h is given by v(τ) = hs(t, 0) where s denotes the second
argument in h (subscripts imply partial derivative here). Thinking of h as a path
of curves in A , we deﬁne E(s) as the energy of the curve obtained by restricting
h to [0, 1] × {s}. That is,
E(s) = 1
2
 1
0
⟨hτ(τ, s), hτ(τ, s)⟩dτ .

192
6 Shapes of Planar Closed Curves
We now compute:
˙E(0) =
 1
0
Dhτ
ds (τ, 0), hτ(τ, 0)

dτ =
 1
0
Dhs
dτ (τ, 0), hτ(τ, 0)

dτ
=
 1
0
Dv
dτ (τ), dα
dτ (τ)

dτ ,
since hτ(τ, 0) is simply dα
dτ (τ). Now, the gradient of E should be a vector ﬁeld u
along α such that ˙E(0) = ⟨⟨v, u⟩⟩. That is,
˙E(0) = ⟨v(0), u(0)⟩+
 1
0
Dv
dτ , Du
dτ

dτ .
Clearly, if u satisﬁes the conditions that u(0) = 0 and
Du
dτ
=
dα
dτ , these
two expressions for
˙E(0) are equal to each other, proving the theorem.
⊓⊔
Given a velocity vector ﬁeld, dα/dτ, its covariant integral u is either obtained
analytically or using numerical approximations, depending on the nature of M. If
we apply the negative of the vector ﬁeld u to update the path α, we expect the
value of E to decrease. Furthermore, because of the condition u(0) = 0, the initial
point of the curve α will not change and hence will remain at p1. However, there
is no reason to believe u(1) = 0, so the value of the terminal point α(1) will be
perturbed and will not remain at p2.
This is not satisfactory because we want to reduce the value of E on the space A0
(i.e., keeping the endpoints of α ﬁxed), not on A . To do this, we need to calculate
the gradient of E thought of as a function on A0. To obtain this gradient, we
simply take the gradient of E on A , which is a vector in TαA , and project it
(using the Palais metric) into TαA0. When we use this projected gradient vector
to update α, we will decrease E as rapidly as possible while keeping the endpoints
of α ﬁxed.
To accomplish this projection, we need the following deﬁnitions.
Deﬁnition 6.4 (Covariantly Constant). A vector ﬁeld w along the path α is
called covariantly constant if Dw/dτ is zero at all points along α.
Deﬁnition 6.5 (Geodesic). A path α on M is called a geodesic if its velocity
vector ﬁeld dα/dτ is covariantly constant. That is, α is a geodesic if
D
dτ ( dα
dτ ) = 0
for all τ.
Recall that we have seen a deﬁnition of the geodesic in Sect. 3.2, but the deﬁnition
presented here is more general.
Deﬁnition 6.6 (Covariantly Linear). A vector ﬁeld w along the path α is called
covariantly linear if Dw/dτ is a covariantly constant vector ﬁeld.
Lemma 6.3. The orthogonal complement of Tα(A0) in Tα(A ) (with respect to the
Palais metric) is the space of all covariantly linear vector ﬁelds w along α.
Proof. Suppose that v ∈Tα(A0) (i.e., v(0) = v(1) = 0), and w ∈Tα(A ) is
covariantly linear. Then, using (covariant) integration by parts and the deﬁnition
of the Palais metric given in Eq. 6.29:

6.6 Geodesic Computation: Path-Straightening Method
193
⟨⟨v, w⟩⟩=
 1
0
Dv(τ)
dτ
, Dw(τ)
dτ

dτ
=

v(τ), Dw(τ)
dτ
1
0
−
 1
0

v(τ), D
dτ
Dw(τ)
dτ

dτ = 0 .
The last term is zero since
D
dτ
 Dw
dτ

= 0 for all τ (i.e., w is covariantly linear).
Hence, Tα(A0) is orthogonal to the space of covariantly linear vector ﬁelds along
α in Tα(A ). This proves that the space of covariantly linear vector ﬁelds is con-
tained in the orthogonal complement of Tα(A0). To prove that these two spaces
are equal, observe ﬁrst that given any choice of tangent vectors at α(0) and α(1),
there is a unique covariantly linear vector-ﬁeld interpolating them. It follows that
every vector ﬁeld along α can be uniquely expressed as the sum of a covari-
antly linear vector ﬁeld and a vector ﬁeld in Tα(A0). The lemma then follows.
⊓⊔
Deﬁnition 6.7 (Parallel Translation). A vector ﬁeld ˜w is called the forward
parallel transport of a tangent vector w ∈Tα(0)(M), along α, if ˜w(0) = w and
D ˜
w(τ)
dτ
= 0 for all τ ∈[0, 1].
Similarly, ˜w is called the backward parallel translation of a tangent vector w ∈
Tα(1)(M), along α, if ˜w(1) = w and D ˜w(τ)
dτ
= 0 for all τ ∈[0, 1].
This deﬁnition is similar to the one in Chap. 3 (Deﬁnition 3.8), but in the current
notation. Please refer to the cases studied in Example 3.7. It must be noted that
forward or backward parallel transports along a path α lead, by deﬁnition, to
vector ﬁelds along α that are covariantly constant.
Recall that we have computed the gradient u of the energy function E : A →R
at any path α in M. According to Lemma 6.3, to project the gradient u into
Tα(A0), we simply need to subtract oﬀa covariantly linear vector ﬁeld that agrees
with u at τ = 0 and τ = 1. Clearly, the correct covariantly linear ﬁeld is simply
τ ˜u(τ), where ˜u(τ) is the covariantly constant ﬁeld obtained by parallel translating
u(1) backward along α. Hence, we have proved the following theorem.
Theorem 6.5. Let α : [0, 1] →M be a path such that α(0) = p1 and α(1) = p2,
i.e., α ∈A0. Then, with respect to the Palais metric (Eq. 6.29):
1. The gradient of the energy function E on A at α is the vector ﬁeld u along α
satisfying u(0) = 0 and Du
dτ = dα
dτ .
2. The gradient of the energy function E restricted to A0 is w(τ) = u(τ) −τ ˜u(τ),
where u is the vector ﬁeld deﬁned in the previous item, and ˜u is the vector ﬁeld
obtained by parallel translating u(1) backward along α.
Example 6.2. As a simple example, consider the gradient of E for a path in the
plane M = R2. We want to straighten this path into a geodesic while keeping
the end points ﬁxed at the current α(0) and α(1) in R2. Of course, a geodesic
in R2 is the straight line joining these two points. As shown in Fig. 6.12, the
particularization of Theorem 6.5 leads to a simple, intuitive result. In R2, the
covariant derivative (integral) is replaced by the ordinary derivative (integral).
Therefore, u(τ) =
 τ
0
dα
dτ (s)ds = α(τ) −α(0), and since u(1) = α(1) −α(0), and
backward parallel transport results in the same vector at all points, τ ˜u(τ) is simply

194
6 Shapes of Planar Closed Curves
α(t)
α(0)
α(1)
t(α(1)−α(0))
Fig. 6.12 Illustration of path-straightening update on a curve in R2
τ(α(1) −α(0)). For any point α(τ) on the curve, the gradient vector is given by
w(τ) = (α(τ) −α(0)) −τ(α(1) −α(0)). As shown in Fig. 6.12, it is the vector that
takes the point α(τ) to the corresponding point on the straight line (the geodesic)
joining α(0) and α(1), and that is exactly the vector ﬁeld to straighten α into a
straight line in one step! Since R2 is a vector (ﬂat) space, one can straighten a path
in one step of gradient update. For nonlinear manifolds, iterating these updates
will eventually converge to a geodesic.
Now we are ready to show that a critical point of the energy E is indeed a
geodesic as we had suggested earlier.
Theorem 6.6. For a given pair p1, p2 ∈M, a path α from p1 to p2 is a critical
point of E on A0 if and only if α is a geodesic from p1 and p2.
Proof. Given a path α in M from p1 to p2, deﬁne the vector ﬁeld u along α by
the conditions that Du/dτ = ˙α(τ) and u(0) = 0. Recall that u is the gradient of
E : A →R at α. We then have the following logical equivalences:
α is a geodesic ⇔dα
dτ is covariantly constant
⇔Du/dτ is covariantly constant
⇔u is covariantly linear
⇔the projection of u to Tα(A0) is 0
(because the covariantly linear vector ﬁelds are orthogonal to
Tα(A0)
⇔the gradient of E : A0 →R at α is 0
⇔α is a critical point of E : A0 →R.
This completes the proof.
⊓⊔
6.6.2 Numerical Implementation
To implement a path straightening approach on a computer, one has to work with a
discrete version of the path α. We will assume a uniform partition {0, 1
k, 2
k, . . . , 1}
of [0, 1] on which the calculations will be performed. Step by step, we present
numerical procedures for computing diﬀerent quantities of interest.

6.6 Geodesic Computation: Path-Straightening Method
195
The ﬁrst requirement is to initialize a path in M between the two given points
p1 and p2. The method we present here is to draw a geodesic between them in the
ambient Hilbert space H , simply taking the straight line between them and then to
project this path onto M using the previous projection algorithm. (In some cases,
other methods are also useful.) As we outline this process, we will mark (using †)
the basic procedures that are needed repeatedly. So, when we particularize this
process on a speciﬁc manifold, we will need to derive these basic procedures ﬁrst.
Algorithm 11 (Initialize a path between p1 and p2 in M).
For all τ = 0, 1, . . . , k,
1. Compute (τ/k)p1 + (1 −(τ/k))p2 in H .
2. Project this value to a nearby point in M to obtain α(τ/k). († Needs a mecha-
nism to project points from H into M.)
The next item is to compute the velocity vector dα
dt at points sampled along the
curve. Our idea is to use a ﬁnite-diﬀerence approximation in H and project into
the appropriate tangent space to estimate the velocity vector.
Algorithm 12 (Compute dα
dt along α).
For all τ = 0, 1, . . . , k,
1. Compute: c(τ/k) = k(α(τ/k) −α((τ −1)/k)). This diﬀerence is computed in
H .
2. Project c(τ/k) into Tα(τ/k)(M) to get an approximation for dα
dt (τ/k). († Needs
a mechanism to project vectors from H into Tp(M).)
Next, we want to approximate the covariant integral of dα
dt along α, using partial
sums. While moving along α from α(0) to α(1), we want to add the current sum, say
u((τ −1)/k), to the velocity dα
dt (τ/k). However, these two quantities are elements of
two diﬀerent tangent spaces and cannot be added directly. Therefore, we parallel
transport u((τ −1)/k) to the point α(τ/k) ﬁrst and then add it to dα
dt (τ/k) to
estimate u(τ/k).
Algorithm 13 (Compute covariant integral of dα
dt along α).
Set u(0) = 0 ∈Tα(0)(M). For all τ = 1, 2, . . ., k,
1. Parallel transport u((τ −1)/k) to the point α(τ/k) to result in u∥((τ −1)/k).
(† Needs a mechanism to parallel transport tangent vectors along geodesics to
nearby points in M.)
2. Set u(τ/k) = 1
k
dα
dt (τ/k) + u∥((τ −1)/k).
This covariant integration results in a vector ﬁeld u along α that is the gradient
of E : A →R at α. To obtain its gradient in A0, we need to subtract a covari-
antly linear component as follows. First, we compute an estimate for the backward
parallel transport of u(1):
Algorithm 14 (Backward parallel transport of u(1)).
Set ˜u(1) = u(1) and l = ∥u(1)∥. For all τ = k −1, k −2, . . . , 0,
1. Project ˜u((τ + 1)/k) into Tα(τ/k)(M) to obtain c(τ/k).

196
6 Shapes of Planar Closed Curves
2. Set ˜u(τ/k) = lc(τ/k)/∥c(τ/k)∥.
Now we can compute the desired gradient:
Algorithm 15 (Gradient vector ﬁeld of E in A0).
For all τ = 1, 2, . . . , k, compute:
w(τ/k) = u(τ/k) −(τ/k)˜u(τ/k) .
By construction, this vector ﬁeld, w, is zero at τ = 0 and τ = k. As a ﬁnal step,
we need to update the path α in direction opposite to the gradient of E.
Algorithm 16 (Path update).
Select a small ϵ > 0 as the update step size. For all τ = 0, 1, . . . , k, perform
1. Compute the gradient update α′(τ/k) = α(τ/k) −ϵw(τ/k). This update is per-
formed in the ambient space H .
2. Project α′(τ/k) to M to obtain the updated α(τ/k)
This completes a numerical recipe for computing geodesics using the path-
straightening method. What are the main ingredients needed for applying this
recipe for any given manifold M, given as a submanifold of an ambient Hilbert
space H ? Similar to the case of the shooting method, one needs the following
tools (marked by † in the algorithms above):
1. Projection on M: For any point p ∈H , we need an analytical or computa-
tional tool for projecting p to the nearest point in M. This item is needed in
Algorithms 11 and 16.
2. Parallel Transport: Given a tangent vector v ∈Tp(M), we need a tool to
transport it along a geodesic to a nearby point q ∈M. This item is needed in
Algorithm 13.
3. Projection on Tangent Space: Given an arbitrary vector w ∈H , we require
a procedure for projecting w into Tp(M). This item is needed in Algorithms 12
and 14.
If we can accomplish these three tasks on a manifold M, we can apply the path-
straightening method to M, as summarized in the following algorithm.
Algorithm 17 (Path Straightening on M).
Initialize a path α between p1 and p2.
1. Compute the velocity vector dα
dτ using Algorithm 12.
2. Compute the covariant integral u of dα
dτ using Algorithm 13.
3. Compute the backward parallel transport of u(1) along α using Algorithm 14.
4. Compute the gradient w of E on A0 using Algorithm 15.
5. Update the path α in the direction of w ∈Tα(A0) using Algorithm 16. If ∥w∥
is small, then stop. Else, return to Step 1.

6.6 Geodesic Computation: Path-Straightening Method
197
6.6.3 Example 1: Geodesics on S2
We ﬁrst illustrate the path-straightening approach for ﬁnding geodesics on the
unit sphere in R3. In this case, the three required items are well known; these were
given in Eqs. 6.19–6.21 earlier and are repeated here for convenience. Note that we
are assuming that S2 is a subset of R3 with the Euclidean metric.
1. Projection on S2: Any nonzero point x ∈R3 can be mapped to the nearest
point in S2 using the projection (Eq. 6.19):
x →x
|x|.
(6.30)
2. Parallel Transport of Tangents: A vector v ∈Tx(S2) can be transported
along a great circle to a point y ̸= −x ∈S2 using the formula (Eq. 3.10):
v →

v −2 ⟨v, y⟩
|x + y|2 (x + y)

.
(6.31)
This is based on Householder reﬂection of the vector v so that v undergoes
the same rotation as one that takes x to y. If we need to parallel transport a
tangent vector along a path that is not a great circle, we subdivide the path
and use the above formula for each subdivision, as described in the second part
of Example 3.7.
3. Projection on Tangent Space: Any vector w ∈R3 can be projected on the
tangent space Tx(S2) using
w →w −⟨w, x⟩x .
(6.32)
With these tools the path-straightening algorithm for computing a geodesic be-
tween any two points x1, x2 ∈S2 can be carried out using Algorithm 17.
Figure 6.13 shows an example of obtaining a geodesic between two points using
path straightening. The leftmost panel shows two points x1 and x2 in S2, the
second panel shows the initial path α and the gradient vector ﬁeld w obtained using
Algorithm 15. Finally, the last panel shows several iterations of path straightening
until α converges to the geodesic between x1 and x2. The convergence of α is
measured using the norm of the gradient vector ﬁeld w. Since the geodesics on S2
are great circles, one can verify that the resulting α is an arc on the great circle
going through both p1 and p2.
6.6.4 Example 2: Geodesics in Elastic Pre-shape Space
Next we focus our attention on shape analysis and consider the problem of ﬁnding
geodesics in C c
2 , the pre-shape space of curves represented by their SRVFs (deﬁned
in Eq. 6.5). Elements of C c
2 are those elements of L2([0, 1], R2) that satisfy the
conditions of unit length and closure. To provide some insight in the structure of
C c
2 , recall the set

198
6 Shapes of Planar Closed Curves
x2 = (0,0,1)
x1 = (1,0,0)
x1 = (1,0,0)
x2 = (0,0,1)
x2 = (0,0,1)
1
2
3
4
5
6
7
1.5
1.6
1.7
1.8
1.9
2
2.1
2.2
iterations
Path Length
x1 = (1,0,0)
Fig. 6.13 An example of path-straightening method for computing geodesics between two points
on S2. The right panel shows the decrease in the path length
C2 =

q ∈L2([0, 1], R2)|
 1
0
|q(t)|2dt = 1
,
,
deﬁned in the previous chapter. The hierarchy of spaces is C c
2 ⊂C2 ⊂L2([0, 1], R2).
(Recall that functions in C2 correspond to curves of length 1 but are not necessarily
closed.) Instead of treating C c
2 as a submanifold of L2([0, 1], R2), as was suggested
in the general approach presented in Sect. 6.6.1, we will consider it as a subman-
ifold of C2. Since C2 is a unit hypersphere inside L2([0, 1], R2), many aspects of
its geometry—tangent spaces, projections, exponential maps, and geodesics—are
already known (see Example A.11). This knowledge helps us in many ways, includ-
ing a reduction in the computational cost of path straightening in C c
2 . Although
we will work with C2 as the larger space containing C c
2 , the Riemannian structure
of C c
2 remains same. We can rewrite C c
2 as
C c
2 =

q ∈C2|
 1
0
q(t)|q(t)|dt = 0
,
(6.33)
i.e., C c
2 represents those curves in C2 that satisfy the closure condition. Algo-
rithms 11–16 will have to be modiﬁed to take into account the geometry of C2,
rather than L2([0, 1], R2), as the larger space. We remind the reader that if we
remove the closure constraint, then the curves can be represented as elements of
C2, a unit hypersphere, and the geodesics between them are straightforward, as
discussed in the previous chapter. This implies that shape analysis of open curves
is much simpler than that of closed curves.

6.6 Geodesic Computation: Path-Straightening Method
199
A Riemannian structure on C c
2 is imposed in Sect. 5.4.1. Recall that in or-
der to deﬁne C c
2 as a submanifold of L2([0, 1], R2), we introduced a function
Φ2 : L2([0, 1], R2) →R3, where the three components of Φ2 were denoted by
(Φ1
2, Φ2
2, Φ3
2). (The function Φ1
2 gave the length of the curve, while the other two
functions measured how far the curve was from being closed.) With respect to this
function, we used the fact that C c
2 = Φ−1
2 (1, 0, 0). Note that our set C2 can be
described precisely by C2 = (Φ1
2)−1(1). Hence, to deﬁne C c
2 as a submanifold of
C2, we only need to consider the remaining functions Φ2
2 and Φ3
2. We denote this
simpliﬁed function by Ψ : C2 →R2, where
Ψ(q) = (Φ2
2(q), Φ3
2(q)) =
 1
0
q1(t)|q(t)|dt,
 1
0
q2(t)|q(t)|dt

.
We can redeﬁne C c
2 as a subset of C2 by C c
2 = Ψ −1(0, 0). The tangent spaces have
the hierarchy:
Tq(C c
2 )
⊂
Tq(C2)
⊂
Tq(L2([0, 1], R2)) ≡L2([0, 1], R2) .
Also, we can decompose Tq(C2) according to Tq(C2) = Tq(C c
2 ) ⊕N b
q(C c
2 ), where
N b
q(C c
2 ) is the set of normals at q to C c
2 inside Tq(C2). We use the additional
superscript b to distinguish it from Nq(C c
2 ), which was used to denote the set of
normals to C c
2 at q inside L2([0, 1], R2). Since we want to restrict our operations
to C2, we seek the normal space N b
q(C c
2 ) inside Tq(C2). From Sect. 5.4.1, the
directional derivative of Ψ is given by for any w ∈Tq(B),
dΨ 1(w) =
 1
0

w(t), q1(t)
|q(t)|q(t) + |q(t)|e1

dt
dΨ 2(w) =
 1
0

w(t), q2(t)
|q(t)|q(t) + |q(t)|e2

dt .
To obtain the normal space N b
q(C c
2 ), we need to take the basis functions
 q1(t)
|q(t)|q(t) + |q(t)|e1,
q2(t)
|q(t)|q(t) + |q(t)|e2
,
,
and project them into TqC2; this means we need subtract oﬀthe component of
each of them in the direction of q.
Thus, deﬁne:
bi(t) ≡qi(t)
|q(t)|q(t) + |q(t)|ei −q(t)
 1
0

q(u), qi(u)
|q(u)|q(u) + |q(u)|ei

du
= qi(t)
|q(t)|q(t) + |q(t)|ei −2q(t)
 1
0
qi(u)|q(u)|du.
(6.34)
To implement the path-straightening procedure for ﬁnding geodesics in C c
2 , con-
sidered as a submanifold of C2, we need the following three ingredients:
1. Projection on C c
2 : We particularize the general approach presented in
Sect. 6.3. The idea is to deﬁne a residual vector r(q) = Ψ(q) ∈R2 and to
evolve q in the direction perpendicular to the level set of Ψ so as to move its Ψ

200
6 Shapes of Planar Closed Curves
Fig. 6.14 Projection of an open curve (left panel), represented by its SRVF, in C c
2 using Algo-
rithm 18 (middle panel). The display uses the coordinate functions while the computations are
performed using SRVFs. The right panel shows the corresponding result using Algorithm 8
image toward the origin in R2. Algorithm 18 describes the procedure to project
an open curve q ∈C2 into C c
2 . The Jacobian for this projection is a 2×2 matrix
whose elements are given by Jij = ⟨bi(t), bj(t)⟩.
The algorithm for projection is as follows:
Algorithm 18. Projection of q ∈C2 to C c
2 . Let ϵ > 0.
a. Compute r(q) = Ψ(q). If |r(q)| < ϵ, stop, otherwise continue.
b. Calculate the Jacobian matrix J(q) given above.
c. Solve the equation J(q)β = −r(q) for β.
d. Deﬁne dq = 2
i=1 βibi, where b1, b2 are given in Eq. 6.34.
e. Update using q →cos(∥dq∥)q + sin(∥dq∥) dq
∥dq∥.
f. If converged, stop. Else, go to Step a.
Figure 6.14 shows an example of projecting open curves q ∈C2 onto C c
2 using
Algorithm 18.
2. Projection on Tangent Space: For any given function w ∈L2([0, 1], R2),
we want a procedure for projecting it into the tangent space Tq(C c
2 ), for any
q ∈C c
2 . We will do this in two steps: ﬁrst project w into the larger space Tq(C2)
and then project it into Tq(C c
2 ). Although we can go straight to the second step,
involving the ﬁrst step improves the accuracy and stability of this numerical
procedure.
a. Start by projecting w into Tq(C2) by
w →w −⟨w, q⟩q .
(6.35)
If the starting tangent is already in Tq(C2), then this step can be skipped.
b. Compute an orthonormal basis of the normal space N b
q(C c
2 ) inside Tq(C2)
by performing Gram-Schmidt on b1 and b2; those were deﬁned earlier in
Eq. 6.34. Call the resulting elements bo
1 and bo
2.
c. Then, the projection of w into Tq(C c
2 ) is given by
˜w ≡w −
2

i=1
⟨˜w, bo
i ⟩bo
i .
(6.36)
3. Parallel Transport: The next item we need is to transport a tangent vector
w ∈Tq1(C c
2 ) to the tangent space at a nearby point q2 ∈C c
2 . This we accomplish
in two steps. First, we recall that the points q1, q2 are also elements of the

6.6 Geodesic Computation: Path-Straightening Method
201
hypersphere C2 since C c
2 is a subset of C2. Therefore, we can use the structure
of C2 to transport the tangent w to the tangent space Tq2(C2). Secondly, we
will project it into the desired space Tq2(C c
2 ).
a. For the ﬁrst step, we use the following equation:
w →˜w ≡

w −
2 ⟨w, q2⟩
∥q1 + q2∥2 (q1 + q2)

.
(6.37)
This is the same equation we used to translate tangent vectors along geodesics
in S2 (e.g., in Eq. 6.31), but now applied to the hypersphere C2.
b. Compute the norm of ˜w, call it l. Form the projection of ˜w into Tq2(C c
2 )
using
¯w = ˜w −
2

i=1
⟨˜w, bo
i ⟩bo
i ,
where bo
1, bo
2 form an orthonormal basis of N b
q2(C2) inside Tq(C2) as earlier.
Rescale the resulting projection to obtain the answer: ¯w →¯wl∥/∥¯w∥.
Having accomplished these three tasks on C c
2 , we can apply our path-straightening
algorithm to C c
2 . The resulting algorithm is very similar to the earlier cases but
repeated here for convenience:
Algorithm 19 (Path Straightening on C c
2 ).
1. Initialize a path α between q1 and q2. One way is to form a geodesic between
them in C2 and then project each point on the path to C c
2 . The geodesic on a
hypersphere is given by for τ = 0, 1/k, . . ., 1
α(τ) =
1
sin(θ) [sin(θ −τθ)q1 + sin(τθ)q2]
(6.38)
where θ = cos−1(⟨q1, q2⟩). Note that α(0) = q1 and α(1) = q2. The projection
into C c
2 is accomplished using Algorithm 18.
2. Compute the velocity vector dα
dt using Algorithm 12.
3. Compute the covariant integral u of dα
dt using the Algorithm 13 and using the
initial condition u(0) = 0.
4. Compute the backward parallel transport of u(1) along α using the Algorithm 14.
5. Compute the gradient w of E on A0 using the Algorithm 15.
6. Update the path α in the direction of w ∈Tα(A0) using Algorithm 16. If ∥w∥
is small, then stop. Else, return to Step 2.
We start with some examples of the path-straightening process. Shown in the
top panel of Fig. 6.15 is a path that forms the initial condition for Algorithm 19.
It is essentially a path from one curve (bird) to another (human) and then back
to the ﬁrst curve (bird). The evolution of this path under the path-straightening
algorithm is shown in the remaining rows with the evolution of E shown in bottom
right. The ﬁnal path after eight iterations is shown in bottom left row. Since the
ﬁrst and the last curves of this path are the same, the results are as expected,
a constant path with constant value given by that curve. The path energy E is
zero for this constant path. Figure 6.16 shows two more examples of this path

202
6 Shapes of Planar Closed Curves
Initial
Iter 1
Iter 2
Iter 3
Final
0
2
4
6
8
10
0
1
2
3
4
5
Fig. 6.15 Illustration of Algorithm 19. The initial path (top), several iterations of path straight-
ening (middle panels), the ﬁnal path (bottom left), and the evolution of the path energy E (bottom
right)
0
2
4
6
8
10
0
0.5
1
1.5
2
2.5
3
3.5
0
2
4
6
8
10
0
1
2
3
4
5
Fig. 6.16 Illustration of path straightening: each example shows an initial path (top), the ﬁnal
path (bottom left), and the evolution of the path energy E (bottom right)
straightening into constant paths to demonstrate the success of Algorithm 19 is
ﬁnding geodesics in C c
2 .
Next we show some general examples of geodesics in C c
2 obtained using Algo-
rithm 19 in Figs. 6.17, 6.18. The ﬁrst set of results involves some polygonal shapes.
Each row in Fig. 6.17 shows geodesic paths between two polygons. The ﬁrst row
shows the geodesic between the hexagon and a right triangle, the second between a
triangle and a star, and so on. Consider the third row of this ﬁgure that shows the

6.6 Geodesic Computation: Path-Straightening Method
203
Fig. 6.17 Examples of geodesic paths in the pre-shape space C c
2 for some polygonal shapes
Fig. 6.18 Geodesics in C c
2 when the target curves have the same shape as the initial curve but
diﬀer in orientations and parameterizations
geodesic between a triangle and a star rectangle. A look at the intermediate shapes
suggests that the two opposite corners are kept while the remaining two corners
disappear. Instead, two corners are formed in new places so as to match the lengths
of sides in the rectangle. This is an illustration of some bending in transforming
one shape into another. Since this is a geodesic in the pre-shape space (and not
the shape space), the resulting paths have not been optimized over SO(2) and ΓS.
Later when we study a geodesic between the same pair in the shape space S c
2 , we
will return to this ﬁgure to compare results.
The underlying issue is that these are geodesics in C c
2 where the rotations and
the parameterizations of the curves are used as given. Therefore, the features in
one shape do not always match with the features in the other. To demonstrate this
point further, Fig. 6.18 shows the geodesics between the same two shapes but with
diﬀerent placements of the starting point (origin) on the second curve is diﬀerent
for each row. As a result, the registration of points across the curves and the actual
geodesic path is diﬀerent for each case.
For future use, we are interested in computing the exponential map on C c
2 .
Given a point q ∈C c
2 and a tangent vector v ∈Tq(C c
2 ), we want to construct a
constant-speed geodesic path α : [0, 1] →C c
2 such that α(0) = q and ˙α(0) = v.
Then, the point reached at the unit time α(1) is actually the desired exponential
expq(v). The vector v is also called the shooting vector of the geodesic α.

204
6 Shapes of Planar Closed Curves
Algorithm 20 (Shooting Geodesics on C c
2 ).
Initialize α(0) = q, w(0) = v, set τ = 0.
1. Set qtemp = α(τ/k) + 1
kw(τ/k). Project qtemp onto C c
2 and call it α((τ + 1)/k).
2. Parallel translate w(τ/k) from α(τ/k) to α((τ + 1)/k).
3. If τ = k −1, stop. Else, set τ = τ + 1 and return to Step 1.
This algorithm provides a numerical way of approximating the exponential map
in C c
2 .
6.7 Geodesics in Shape Spaces
So far we have looked at the problem of computing geodesics between given curves
as elements of the pre-shape spaces C c
1 and C c
2 . As demonstrated through several
examples, this is not suﬃcient because the registration and the rotations of curves
are not optimized in order to achieve the most eﬃcient deformation from one
to the other. One simply takes the given orientations and the parameterizations,
and computes geodesics between the resulting curves. This is not appropriate for
shape analysis on two counts. One, due to a lack of optimal alignment between the
curves, the resulting geodesic paths look unnatural. Second, and more important,
the resulting analysis of shapes, based on geodesics in pre-shape spaces, is not
invariant to these operations: rigid transformations and re-parameterizations. Our
solution to both of these problems is to go to the shape spaces.
We have seen how to compute geodesics in the pre-shape spaces. The question
is: how can we extend this construction to the shape spaces? Remembering that
the shape spaces are quotient spaces of the respective pre-shape spaces, we can
adapt the previous tools to get the desired results. Recall that by a geodesic in
a quotient space we mean the shortest geodesic between the two corresponding
orbits in the pre-shape space. This was discussed previously in Sect. 5.7. Since we
have used a shooting method to compute geodesics in C c
1 and a path-straightening
method to compute geodesics in C c
2 , the required adaptations are diﬀerent in these
two cases and will be discussed separately.
6.7.1 Geodesics in Non-elastic Shape Space
As described in Sect. 6.2.3, the shape space S c
1 is a quotient space of the pre-shape
space C c
1 under the action of S1. Therefore, the problem of ﬁnding geodesics in S1
reduces to the problem of ﬁnding those geodesics in C c
1 that are orthogonal to the
S1 orbits. The fact that S1 acts by isometries also implies that if a geodesic in a
pre-shape space is orthogonal to one S1 orbit, then it is orthogonal to all S1 orbits
that it meets and, hence, projects to a geodesic in the corresponding shape space.
Let us assume arbitrary points θ1, θ2 ∈C c
1 ; our goal is to ﬁnd a geodesic between
the orbits [θ1] and [θ2] in S c
1 = C c
1 /S1. The geodesic distance in S c
1 is given by]
dS c
1 ([θ1], [θ2]) = inf
τ∈S1 dC c
1 (θ1, (τ, θ2)) .

6.7 Geodesics in Shape Spaces
205
Using Algorithm 10, we can ﬁnd geodesics between any two points of C c
1 , as demon-
strated in Sect. 6.5.2. For ﬁnding the geodesic in S c
1 , we will perform the following
additional steps:
1. Select any point on the orbit [θ1] in C c
1 and ﬁx it. Say we select θ1 itself.
2. We need to restrict the shooting direction at Tθ1(C c
1 ) to ensure that the geodesic
is perpendicular to [θ1]. That is, the shooting direction is perpendicular to the
tangent space Tθ1([θ1]). Recall that the action of S1 on C c
1 is given by for a
τ ∈S1
(τ, θ1) = θ1((s −τ)2π) + τ .
The derivative of that group action with respect to τ, evaluated at τ = 0 is
1 −˙θ1. Therefore, the one-dimensional space Tθ1([θ1]) is spanned by 1 −˙θ1.
Consequently, the desired shooting direction v should be perpendicular to the
function 1 −˙θ1. (Here we restrict to those elements of C c
1 that have continuous
ﬁrst derivative.)
In terms of the modiﬁcations to Algorithm 10, the only change from this item
comes in the projection on tangent space (Eq. 6.25) that is changed as follows.
The new projection of a vector v ∈L2([0, 2π], R) to the desired shooting space
is given by
v →v −
4

i=1
⟨v, wi⟩wi ,
(6.39)
where wi’s form an orthonormal basis of the space spanned by {1, cos◦θ1,
sin ◦θ1, ˙θ1}. The last element in this set is the only change from earlier.
3. Then, we need to search over all elements of [θ2] to ﬁnd the one nearest to θ1
in terms of the geodesic length. How will this be accomplished? Recall that in
Algorithm 10, we search for the shooting direction v ∈Tθ1(C c
1 ) that minimizes
the miss function F[v] = ∥α1(θ1; v) −θ2)∥2, where ατ is a geodesic shot from
θ1 with the initial velocity v. We will change this deﬁnition of F to be
F[v] = inf
τ∈S1 ∥α1(θ; v) −(τ, θ1)∥2 .
(6.40)
Furthermore, the initial velocity v is now restricted to be perpendicular to the
orbit [θ1], i.e., v ⊥˙θ1. This equation redeﬁnes the miss function to be the L2
distance squared from α1(θ; v) to the nearest point in the orbit [θ2]. In case
the direction v is represented by its Fourier approximation π(m
i=0(ai cos(is) +
bi sin(is))), where π is the projection from L2 to the tangent space T[θ1](S c
1 ),
we get the miss function:
˜F(a, b) = inf
τ∈S1 ∥α1(θ;
m

i=0
(ai cos(is) + bi sin(is)) −(τ, θ1)∥2 .
(6.41)
We can modify Algorithm 10 to take these three steps into account and reach
the following algorithms for ﬁnding geodesics between [θ1] and [θ2] in S c
1 .
Algorithm 21 (Shooting Method on S c
1 ).
Given points θ1, θ2
∈
C c
1 , initialize the coeﬃcients a
=
{a1, . . . , am} and
b = {b1, . . . , bm} and ﬁnd the tangent direction v ∈Tθ1(S1) by projecting
m
i=1 ai cos(is) + bi sin(is) as described in (Eq. 6.39). Fix a δ > 0 small.

206
6 Shapes of Planar Closed Curves
1. Current Cost: Compute the exponential map expθ1(v) using Algorithm 9.
Compute the cost function F(v) = infu∈S1 ∥(u, θ2) −expθ1(v)∥2 as given in
Eq. 6.40. This inﬁmum is estimated by evaluating the function on the right for
u values on a dense grid on S1 and taking the smallest value.
2. Gradient of Cost Function: Use diﬀerence quotients to approximate the
gradient ∂˜
F
∂ai and ∂˜
F
∂bi .
3. Update a and b using the negative gradients of ˜F, form v(s) = m
i=1 ai cos(is)+
bi sin(is) and project it into Tθ1(S c
1 ) using Eq. 6.39.
4. If the added norms of the vectors ∇a ˜F and ∇b ˜F are small, then stop. Else,
return to Step 1.
We present some results on the shape space S c
1 generated using this algorithm.
To highlight the advantage of removing the re-parameterization group S1, we want
to compare these results with those obtained for the larger space C c
1 . As a sim-
ple comparative example, consider the two end shapes shown in the ﬁrst row of
Fig. 6.19. Both the curves are perturbations of a heart shape with the diﬀerence
that the perturbations (little dents) are on diﬀerent sides. The top row shows a
geodesic path between these two curves in C c
1 , for some arbitrarily chosen origins
on these curves. Since the origins are ﬁxed, the correspondences between points
on the two curves are not very natural and the resulting deformation along the
geodesic path in C c
1 looks awkward. However, we should point out that for the
given placements of origins this is the path of least bending energy spent in going
from one shape to another. Next, consider the geodesic shown in the lower row
of this ﬁgure. In this case, the geodesic is computed in the quotient space S c
1 ,
which means that the geodesic is perpendicular to the orbits of S1. In other words,
the algorithm automatically searches over all possible placements of origin on the
second curve (through the inﬁmum deﬁnition of ˜F in Eq. 6.41) to ﬁnd the best
one. As a result, the correspondence of points across the two shapes looks more
appropriate and the geodesic path shows a natural deformation of the ﬁrst shape
into the second one.
Figure 6.20, 6.21 show some additional examples of bending-only geodesics be-
tween shapes that we have already seen in Sect. 6.5.2. The ﬁrst case is that of
some polygonal shapes. Figure 6.20 shows geodesic paths between polygons that
are drawn as end shapes in each row. The intermediate shapes represent equally
spaced points along the geodesic ψt between those end shapes in S c
1 . For com-
parison with geodesics in C c
1 , look at the corresponding geodesics in Fig. 6.9. In
particular, compare the second rows of both the ﬁgures where a square is being
Pre-shape space
Geodesic
c
1
c
1
Fig. 6.19 Geodesics between the same two curves under the non-elastic representation. The
top geodesic is computed in C c
1 with the given orientation and placement of origins, while the
bottom geodesic is computed in S c
1 where one ﬁnds the optimal alignment of shapes

6.7 Geodesics in Shape Spaces
207
Fig. 6.20 Examples of geodesic paths between polygons in S c
1 . Compare with the corresponding
paths in C c
1 shown in Fig. 6.9
Fig. 6.21 Examples of geodesic paths between some curves in S c
1 . Compare with the corre-
sponding paths in C c
1 shown in Fig. 6.9
deformed into a rectangle. Just by comparing these two paths visually, one can
convince oneself that bending-wise, the result for S c
1 seems more eﬃcient than
the one for C c
1 . In the latter case, the given parameterization is such that all four
corners of the square disappear and new corners appear to make the rectangle,
while in the former case two of the corners remain unchanged and the remaining
two are modiﬁed to go from the square to the rectangle. Similar observations can
be made for all the cases shown in these two ﬁgures:
Figure 6.21 shows more examples of geodesics in S c
1 , this time using more
natural shapes. In each case, we can see the eﬀect of the algorithm ﬁnding the
optimal placement of origin (on the second curve) so as to best match the two
curves. The net eﬀect seems to be that the algorithm picks up the most dominant
feature (a protrusion or a corner) in both the shapes and matches them ﬁrst and
it matches the remaining points accordingly. With this matching, it bends the ﬁrst
shape into the second in the most eﬃcient way. For example, in the ﬁrst row, some
legs of the two elephant shapes are matched to each other and the remaining points
match accordingly. So one leg of the ﬁrst shape has to deform into the trunk of
the second shape.

208
6 Shapes of Planar Closed Curves
6.7.2 Geodesics in Elastic Shape Space
Next we consider the task of ﬁnding geodesics between shapes in the space S c
2 .
As described in Sect. 6.2.3, this set of elastic shapes is realized as a quotient space
C2/(SO(2) × ΓS). So, the equivalence class of an element q ∈C c
2 is given by
Eq. 5.21 (repeated here for convenience):
[q] = closure{O(q ◦γ)

˙γ|γ ∈˜ΓS, O ∈SO(2)} .
For any two planar closed curves, β1 and β2, let their SRVFs be given by q1 and
q2, respectively. Then, our goal is to ﬁnd a geodesic path between the orbits [q1]
and [q2] in S c
2 . In principle, a geodesic in S c
2 = C c
2 /(ΓS × SO(2)) is obtained by
forming geodesics between all possible pairs in the set ([q1]×[q2]) ⊂C c
2 and then
selecting the shortest. Thus, the geodesic distance in S c
2 is given by
dS c
2 ([q1], [q2]) =
inf
r1∈[q1],r2∈[q2] dC c
2 (r1, r2)
=
inf
O∈SO(2),γ∈ΓS dC c
2 (q1,

˙γO(q ◦γ)) .
(6.42)
The last inequality comes from the fact that the action of SO(2) × ΓS on C c
2 is by
isometries and ΓS is dense in ˜ΓS. So, one needs to minimize only over one orbit and
not two. We remind the reader that we have already developed a path-straightening
approach for ﬁnding geodesics between any two points in C c
2 (Algorithm 19) and
we will modify it to reach an algorithm for ﬁnding geodesics in S c
2 .
Before we proceed further, we remind the reader this development overlooks
certain mathematical technicalities in order to reach eﬃcient numerical imple-
mentations of the alignment process. Similar to the previous chapter, the issue
here is that the shape space does not have a manifold structure. Due to this sit-
uation, we view the shape space only as a set with a distance inherited from the
pre-shape space. However, in the context of deriving numerical procedures, such
as the gradient-based approach for alignment, we will utilize ideas such as the
tangent spaces of orbits and the tangent spaces of the quotient space to help out
conceptually.
Equation 6.42 outlines a problem of optimization over the joint space ΓS×SO(2)
and we will use an iterative numerical approach to ﬁnd the solution. Each iteration
of this search takes the following form: take an arbitrary pairing (q1, r) from the set
({q1}×[q2]) and form a geodesic α between them in C c
2 . If this geodesic happens to
be orthogonal to orbit [q2], as measured by computing the inner product between
tangent to the geodesic at r and basis elements of Tγid(ΓS) (when placed at r),
then we can stop, as we already have a solution to the optimization problem. In
other words, if the norm of the projection of the tangent to α at r into the set
Tγid(ΓS) (when placed at r) is not zero, then we update r in such a way that the
norm of this projection is reduced. Naturally, the direction in which to update r
arises from the projection itself. Apply this idea repeatedly until the projection
becomes zero. A schematic illustration of this idea is shown in Fig. 6.22. A word
of caution here for the reader—this particular discussion on iterative optimization
seeks a local solution to the optimization problem. That is, depending on the initial
point r, we can end up at a point that is a local minimizer of the cost function.
To search over a larger space, some other ideas are needed.

6.7 Geodesics in Shape Spaces
209
[q1]
[q2]
r
α
q1
Re-parameterization
Path Straightening
(q2, γ∗)
q2
Fig. 6.22 Gradient-based update of elements in [q2], while keeping q1 ﬁxed, to ﬁnd the shortest
geodesic between the orbits of [q1] and [q2]
0
0.5
0.6
0.7
Oopt
0.8
0.9
1
1.1
1.2
20
40
60
Path Energy
80
100
θ
120 140 160 180 200
Fig. 6.23 Top: The cost function dC c
2 (q1, Oq2) vs. the angle of rotation in O. Bottom: Geodesic
Path corresponding to the optimal alignment
Next we describe the projection-based iterative update of the element r ∈[q2].
Since the two groups SO(2) and ΓS commute, we can present the updates under
each action separately.
1. Optimal Rotation: The orbit of r generated by the action of SO(2) on r is
given by {Or|O ∈SO(2)}. In Fig. 6.23, we show the plot of path energy, de-
ﬁned in Eq. 6.28, for the geodesic path between q1 and elements of this orbit,
for the two bird shapes shown at the ends of the bottom row. In this example, r
corresponds to arc-length parameterization of the second curve. This example
supports what we already know, that the energy of a path between q1 and Or
depends on O and there are several local minima in this optimization prob-
lem. Depending on the computational resources, we can put a coarse grid on
SO(2), evaluate the cost function over the grid points, and select the best point
among those candidates. We can use this as an initial condition for the gradient

210
6 Shapes of Planar Closed Curves
iteration. To set up the projection-based or the gradient update, we need to
establish the tangent space to the rotation orbit. For r ∈[q2], this is a one-
dimensional space spanned by the function Er, where E =
1
√
2
 0 −1
1 0

is the
basis element of the tangent space TI(SO(2)) (see Sect. A.1.2 for a discussion
on geometry of SO(2)).
We are going to optimize over the rotation O iteratively and let r be the
current estimate of the closest point to q1 in [q2]. We already have a geodesic
path between q1 and r in C c
2 , denoted by α, such that w is the initial velocity
vector to the geodesic α at r. That is, w ≡exp−1
r (q1). Taking the projection of
w in the rotation orbit of q2, we obtain x = ⟨w, Er⟩. The iterative update of r
on rotation orbit of q2 is given by r →e−xϵEr, with ϵ > 0 being a small step
size.
In summary, an iteration of the optimization over SO(2) is as follows: Let α
be the geodesic path in C c
2 from r to q1 and let w be its initial velocity vector.
a. Compute the gradient at r ∈[q2] using x = ⟨w, Er⟩, where E =
1
√
2
0 −1
1 0

.
b. Update r using ˜q2 →e−xϵEr.
c. Stop if |x| is small, otherwise return to Step a.
In some cases, where the speed of optimization is an important issue, we may
choose a fast, but approximate, optimization over SO(2) as follows. Here we
change the cost function for optimization over SO(2), from the one in Eq. 6.42
to the L2 distance between q1 and Oq2. In other words, instead of using the
geodesic distance in C c
2 , we use the geodesic distance in the larger Hilbert space
L2([0, 2π], R2), resulting in the optimization problem:
ˆO = argmin
O∈SO(2)
∥q1 −O · r∥2 ,
(6.43)
where the norm is L2. For matching some kinds of shapes, this approximation
may be good enough. The solution to this problem is available analytically:
ˆO =
⎧
⎨
⎩
UV T ,
if det(A) > 0
U

1 0
0 −1

V T ,
otherwise
.
Here A = UΣV T is the SVD of the 2×2 matrix A =  1
0 q1(t)r(t)T dt. Recall that
in our notation q(t) is a 2 × 1 vector for any t. Again, the disadvantage of this
method is that the distance function we are minimizing will not be precisely
the one based on geodesics in C c
2 . However, the advantages of avoiding the
gradient descent procedure and thereby avoiding getting stuck in local minima
are considerable.
2. Optimal Re-Parameterization: The main problem in ﬁnding geodesics in
S c
2 is the optimization over the re-parameterization group ΓS. Before we present
our gradient-based approach, we brieﬂy discuss the nature of elements of ΓS.
a. We ﬁrst point out a couple of important subgroups of ΓS. First, we can think
of S1 itself as a subgroup. To see this, consider S1 as consisting of the unit
complex numbers. Then, given any w ∈S1, we obtain the diﬀeomorphism
z →wz. Thus, S1 itself (as a Lie group) is isomorphic to the subgroup of
ΓS consisting of rotations. As a re-parameterization, such a diﬀeomorphism

6.7 Geodesics in Shape Spaces
211
corresponds to choosing a diﬀerent point of a closed curve as a “starting
point”. Another important subgroup of ΓS is the group of “Moebius trans-
formations”. Given any two complex numbers a and b satisfying |a| > |b|, we
deﬁne a function:
z →az + b
¯bz + ¯a .
(6.44)
This function is a diﬀeomorphism S1 →S1 and the set of all these functions is
actually a subgroup of ΓS. While it may appear that this subgroup has a real
dimension of four, since it depends on the choice of two complex numbers,
its real dimension is three, since multiplying both a and b by the same real
scalar does not aﬀect the action of the diﬀeomorphism.
b. As discussed in Chap. 4, Sect. 4.10.2, ΓS is a diﬀerentiable manifold with
the tangent space at γid ∈ΓS given by
Tγid(ΓS) = L2(S1, R) = {smooth g : S1 →R|

S1 g(s)2ds < ∞} .
We will need a basis of this set to perform gradient-based minimizations on
it. One possible basis is to use the Fourier components:
{cos(nt), sin(nt), n = 0, 1, 2, . . .} .
As we know, these functions form an orthonormal basis of Tγid(ΓS) un-
der the L2 metric. If we take the ﬁrst component, the constant function,
and map its span to ΓS using the exponential map, then the resulting set,
expid(span(1)), is simply the set of all rotations S1, described in the previous
paragraph. Similarly, if we take the ﬁrst three components and form their
span, span{1, cos(s), sin(s)}, then the exponential map of this set gives the
well-known 3-dimensional subgroup of ΓS known as the “Moebius transfor-
mations”, also described in the previous paragraph. Another basis of Tγid(ΓS)
that can be useful is

1, sin(nt)
nπ
, cos(nt) −1)
nπ
, n = 1, 2, 3, . . .
,
.
(6.45)
These functions form an orthonormal basis of the set Tγid(ΓS) under the
Palais metric (Eq. 6.29). Often there are some numerical advantages of using
this basis in the gradient process used to optimize over ΓS, which is described
next.
Basic Idea: The idea underlying this gradient-based approach, for optimizing
the cost function in Eq. 6.42 over ΓS, is as follows. We want to search over the
ΓS-orbit of q2 and ﬁnd the point that minimizes the distance from q1 in C c
2 .
Since this orbit has a one to one correspondence with ΓS, we can transfer the
problem to ΓS. (This is actually how the problem is written in Eq. 6.42.) We are
going to solve the problem iteratively and let r ∈[q2] be the current estimate,
an element of [q2]. Deﬁne a mapping Υ : ΓS →[q2] by γ →(r, γ) = (r ◦γ)√˙γ
that establishes a correspondence between ΓS and the ΓS-orbit of [q2]. Note
that this mapping identiﬁes r with the identity map γid. With the help of the
diﬀerential of Υ, we will write the derivative of the cost function with respect
to γ. Under the steepest descent scheme, this gradient provides a direction to
update γid; call this update dγ. Applying this incremental dγ to γ provides γ for

212
6 Shapes of Planar Closed Curves
Fig. 6.24 Schematic view of the computation of geodesic in S c
2
the next iteration and the process is repeated until the norm of this increment
is small enough to stop. Shown in Fig. 6.24 is a cartoon diagram illustrating
this process.
The important piece here is the gradient of the cost function in Eq. 6.42 and
that is derived next.
Theorem 6.7. Let v ∈Tγid(ΓS) be a tangent vector. The directional derivative
of the cost function dC c
2 (q1, Υ(γ))2 in the direction of v is given by ⟨w, dΥγid(v)⟩
where w = exp−1
Υ(γ)(q1) and
dΥγid(v) = ˙r(t)v(t) + 1
2r(t)˙v(t) .
(6.46)
Proof. We prove this theorem using the chain rule that includes two partial
derivatives. The ﬁrst partial derivative is obtained using a result established
later in the book (Theorem 7.1). This theorem states that on a Riemannian
manifold M, the gradient of the square of the geodesic distance between p1, p2 ∈
M, with respect to p, is given by the inverse exponential map of p2 onto the
tangent space at p1. Since the cost function here is exactly the geodesic distance
squared in C c
2 , this theorem applies directly. Hence, the gradient of dC c
2 (q1, γ)2
with respect to r is given by the vector w = exp−1
r (q1); note that w is an element
of Tr(C c
2 ).
The second partial derivative in the chain rule comes from Lemma 5.3. It
states that for any point γ ∈ΓS, the diﬀerential of Υ : ΓS →[q2] is the linear
transformation dΥγ : Tγ(ΓS) →TΥ(γ)([q2]) deﬁned by the equation:
(dΥγ(v))(s) =

˙γ(s) ˙r(γ(s))v(s) +
1
2

˙γ(s)
˙v(s)r(γ(s)) .
(6.47)
Applying this equation at γ = γid and combining the two partial derivatives
proves the theorem.

6.7 Geodesics in Shape Spaces
213
We summarize steps for an iterative optimization over ΓS: let r be the current
element of [q2] and let γ ∈ΓS such that r = (q2, γ). Also, let α be the geodesic
between r and q1 in C c
2 , obtained using Algorithm 19. The initial velocity of
this geodesic is the desired tangent vector w = exp−1
r (q1). Let {bi}, i = 1, . . . , d
be an orthonormal basis of the set Tγid(ΓS). We suggest using the basis given
in Eq. 6.45.
a. Compute the gradient of the cost function according to
dγ =
n

i=1
⟨w, dΥγid(bi)⟩bi .
(6.48)
where dΥγid is deﬁned in Eq. 6.46.
b. If ∥dγ∥< ϵ, then stop. Else, continue.
c. Re-parameterize r using dγ:
r →

˙dγ(r ◦dγ) .
In practice, there is some computational advantage to keeping track of the
cumulative γ and applying it to the original curve β2. That is, update γ →
γ + dγ and ﬁnd ˜β2 = β2 ◦γ and compute r from ˜β2.
Combining the iterative optimization over both ΓS and SO(2), we obtain the
following algorithm for ﬁnding geodesics in the elastic shape space S c
2 .
Algorithm 22 (Geodesics on Shape Space of Elastic Closed Curves). Let
q1 and q2 be two elements of C c
2 , and let β2 be the parameterized path corre-
sponding to q2, i.e., β2(t) =
 t
0 |q2(s)|q2(s)ds. Set r = q2, γ = γid, and O = I2.
Choose the step sizes δ1 and δ2 for the gradient updates of the rotation and the
re-parameterization, respectively:
1. Compute a geodesic path from γ to q1 using Algorithm 19. Let w be the initial
velocity of this geodesic path. That is, w = exp−1
r (q1).
2. Compute the two gradients as follows:
a. Compute the gradient of the cost function with respect to the rotation O using
x = ⟨w, Eγ⟩, where E =

0 −1
1 0

. Set dO = exp(δ1xE).
b. Compute the gradient of the cost function with respect to the re-parameterization
according to
dγ = δ2
n

i=1
⟨w, dΥγid(bi)⟩bi .
(6.49)
where dΥγid is deﬁned in Eq. 6.46 and {bi}, i = 1, . . . , d are as given in
Eq. 6.45.
3. Update γ →γ ◦(γid + dγ) and O →O.dO. Caution: In this step it is important
that δ2 was chosen small enough that γid + dγ is a diﬀeomorphism (i.e., has
positive derivative at each t).
4. Update β2 according to ˜β2 = O(β2 ◦γ). Compute r(t) = SRVF(˜β2).
5. If |x| and ∥dγ∥are small, then stop. Otherwise, return to Step 1.
We present some results to demonstrate this algorithm. For any two shapes, the
optimal orientations and re-parameterizations of the second shape are determined

214
6 Shapes of Planar Closed Curves
c
2 /(S1 × SO(2))
c
2 /ΓS
c
2
c
2 /(S1 × SO(2))
c
2 /ΓS
c
2
Fig. 6.25 Geodesics in elastic pre-shape and shape spaces
using Algorithm 22. In the process we also obtain the geodesic paths using path
straightening in the shape space S c
2 . Figure 6.25 displays two sets of examples.
In both the cases, the top row shows the geodesic path when the optimal rotation
and only the subgroup S1 ⊂ΓS have been optimized for the second shape. That
is, the path shown is a geodesic in the space C c
2 /(SO(2) × S1). The second row
shows the resulting geodesic when only the re-parameterization group ΓS (but
not the rotation group SO(2)) has been removed. Finally, the third row shows
the case when both the rotation and the re-parameterization groups have been
removed completely. In both these cases, and the ones shown in later examples,
it is easy to see the improvement in resulting geodesic paths when the full shape-
preserving group (ΓS × SO(2)) has been removed. The matching of points across
the shapes is better and the deformation of shapes along the connecting geodesics
is more natural. Take, for instance, the ﬁrst case where one camel shape is being
compared to another camel shape. In the ﬁrst row, the two curves have been
aligned rotationally and in placement of origins. Still the deformation from one to
other does not seem natural, as the ﬁrst leg almost disappears in the middle of
the path. In the second row, the two curves lack rotational alignment, while in the
last row the two curves are completely aligned in all variables. Consequently, the
resulting geodesic path in the last seems most natural; the legs match the legs, the
tail matches the tail, the head matches the head, etc.
Figure 6.26 shows some additional examples of geodesics between curves in S c
2 .
These curves have ticks placed on them to help visualize the speed of traversal
along those curves. The ticks start with a uniform separation on the leftmost
shape, denoting the arc-length parameterization in the ﬁrst shape, but become
nonuniform as we progress toward the target shape along the geodesic. This non-
linear sampling in the second shape helps improve matching of similar features
across the two shapes. Visually it appears as if the ﬁrst curve is being stretched
and/or compressed locally, in addition to being bent, to reach the second curve.
For example, in the ﬁrst row, the small tail of the tortoise is matched to a much
larger tail of the dog; this matching of tails, in addition of a matching of their
legs, becomes possible due to stretching and compressing of other parts along the
tortoise curve.

6.8 Examples of Elastic Geodesics
215
Fig. 6.26 Geodesic paths in S c
2 between the pair of curves shown to the left
Another way to analyze elastic geodesics is to study the way points are matched
between two shapes, resulting from optimal rotations and re-parameterizations.
Figure 6.27 shows elastic geodesics between pairs of shapes on the top and the cor-
responding elastic matching on the bottom. The success in matching similar parts
across shapes results in more meaningful deformations in the geodesic process.
Take the case of two hands in the last row, for example. The two hands (leftmost
and the rightmost) diﬀer in at least two ways: the middle ﬁnger is stretched and
the last ﬁnger is bent. A study of the matching result shows us that despite these
diﬀerences the corresponding parts on the two hands are matched well—the tips
of the ﬁngers in one hand match with the tips in the other, the thumb with the
thumb, and so on. As a result, the geodesic path shows a deformation that seems
natural—the middle ﬁnger stretches out from left to right, the last ﬁnger bends
from left to right, etc. The algorithm is successful in using an optimal combination
of bending and stretching to perform the matching and the deformation.
6.8 Examples of Elastic Geodesics
As further examples of geodesic computations, we look at the shapes of capital
English letters under two fonts: Times New Roman and Comic Sans, as shown in
Fig. 6.28. In order to compare the shapes of letters in these fonts, we compute the
geodesic paths and geodesic distances between all the letters of one font with all the
letters in the second font. Shown in the bottom part of this ﬁgure are two examples
of geodesic paths and optimal matching between the corresponding curves. For the
full comparison, we obtain a 26 × 26 distance matrix that is shown as a gray scale
image in the right panel. As expected, the geodesic distances between the same
letters are smaller and the image has darker pixels along the diagonals. Other
darker spots result from shape similarities of other letters, e.g., M and W, D and
O, etc.

216
6 Shapes of Planar Closed Curves
Fig. 6.27 Top: elastic geodesics between pairs of 2D curves. Bottom: optimal matching between
points across given shapes
Fig. 6.28 Top: letters in Times New Roman and Comic Sans alphabet and the matrix of elastic
shape distances between them. Bottom: examples of geodesic paths and the pairwise matching
of parts between same letters of diﬀerent fonts
Figure 6.29 shows an interesting example from biology where one is interested
in classifying plant species according to the shape of their leaves. Several datasets
have been constructed to study this classiﬁcation. We show examples of leaves from
32 diﬀerent species of plants, taken from the Flavia data. For some of these leaves,
we display the elastic geodesic paths between their shapes in S c
2 . The reader can
see the eﬀect of stretching boundary parts in matching sharper features across leaf
boundaries.

6.8 Examples of Elastic Geodesics
217
Fig. 6.29 Top: representative leaves from 32 diﬀerent species of plants, taken from the Flavia
dataset. Bottom four rows: examples of geodesic paths between some sample shapes computed
in S c
2
6.8.1 Elastic Matching: Gradient Versus Dynamic
Programming Algorithm
In the previous section we have presented a gradient-based approach for ﬁnding
optimal re-parameterization of the second curve in order to minimize the geodesic
distance between shapes in C c
2 . As described in Appendix B, one can also use the
dynamic programming algorithm (DPA) to solve this problem. Since DPA provides
an optimal solution, albeit on a discrete grid in [0, 1] × [0, 1], what is the need for
this gradient-based solution?
The gradient-based methods can sometimes be advantageous over DPA. First,
it is a very general approach and works for more general objective functions. In
contrast, DPA is applicable only when the objective functions is additive over

218
6 Shapes of Planar Closed Curves
Table 6.1 Comparisons between DP and the gradient approach for elastic shape matching
Elastic matching using
Elastic matching using
dynamic programming
gradient method
Solution type
Global solution for a ﬁxed grid Local solution
Cost function requirement Requires additive cost function Allows general cost function
Computational cost
Order O(kT 2) (open curves)
or O(kT 3) (closed curves) T
are the number of samples,
k << T
Order
O(Tn),
n
is
the
num-
ber of basis elements of the re-
parameterization algebra
the graph (t, γ(t)), e.g., the L2 norm. However, either this norm may not be the
appropriate metric on the space of shape conﬁgurations or the cost function may
take a more complicated form involving L2 norms. Secondly, the gradient approach
may oﬀer signiﬁcantly lower computational complexity as it is O(T ) where T is
the number of sample points on the curve. In contrast, the smallest computational
cost in DPA for matching closed curves is generally O(T 3) (some algorithms can
do better and use O(T 2 log(T ))). Even if we search over the S1 component in ΓS
exhaustively and search the rest using the gradient approach, the computational
cost remains low, on the order of O(T ).
Table 6.1 summarizes important diﬀerences between the two methods:
6.8.2 Fast Approximate Elastic Matching of Closed Curves
We have empirical evidence that the matching of points across curves is fast and
optimal, albeit for the chosen grid, once the starting points on the two curves
are determined. The diﬃculty comes in choosing optimal starting or seed points.
Since the optimization over these seed points potentially has several local minima,
in general, a more basic grid search can be eﬃcient here. Note that one needs to
search only on one curve, the other seed can be ﬁxed arbitrarily. One can partition
the domain [0, 1] into coarse intervals, either uniform or nonuniform, and use the
partition points as candidates for the seed during matching.
Let {ti} ⊂S1 represent the candidate seeds on curve 2. The seed on curve 1 is
kept ﬁxed. Then, for each i, we match the seed ti on curve 2 with the ﬁxed seed
on curve 1, using the DPA for matching the remaining points. The seed with the
least overall cost function is kept as the ﬁnal solution.
Another possible approximation, in the interest of gaining speed, is to solve
the geodesic problem on the larger space S2 and project the ﬁnal path, element
by element, on the space S c
2 . Depending on the shapes involved, the projected
geodesic in S c
2 may not be very diﬀerent from the exact one computed using
the techniques described earlier. In some application, this approximation will be
acceptable, especially when it comes with a large gain in speed.
6.9 Elastic Versus Non-elastic Deformations
In this chapter we have presented two comprehensive frameworks for comparing
shapes of planar, closed curves, termed non-elastic and elastic. In both cases we
specify a representation of shapes, the groups that leave the shapes invariant,

6.10 Parallel Transport of Shape Deformations
219
Fig. 6.30 Odd rows show non-elastic geodesic paths while even rows show elastic geodesics
the chosen Riemannian structures, and the algorithms for ﬁnding geodesics in the
resulting shape spaces. It is natural to compare the two frameworks. We will do
so using a number of examples, looking at the geodesic paths resulting from each
method for the same pair of shapes.
As a ﬁrst set of examples, Fig. 6.30 shows a comparison between the non-elastic
geodesics and the elastic geodesics. For each pair, the top row is the non-elastic
geodesic and the bottom row is the elastic geodesic. In the case of the non-elastic
geodesic, we are allowed to deform one shape into another using only the bending
process. No stretching or compressing is allowed. This is reﬂected in the results
where, in order to match some salient features, the algorithm has to ﬂatten a
feature and create a new one at a new location. In the ﬁrst example, the algorithm
needs to grow the right leg in this human shape while keeping the hands matched.
However, in view of the diﬀerent relative locations of the legs (with respect to the
hands), the algorithms ends up removing the left leg and growing two legs at new
locations. This task is accomplished more naturally in the elastic framework where
the features are matched ﬁrst and the intermediate parts are stretched/compressed
accordingly. As a result, the right leg grows smoothly along the geodesic path while
the other parts (hands, leg) remain well matched and unchanged.
Figures 6.31 show a larger comparison set with geodesics in pre-shape and
shape spaces for both non-elastic and elastic frameworks. The ﬁrst ﬁgure deals
with polygonal shapes while the second one shows more natural shapes.
Figure 6.32 shows examples of geodesic paths between closed curves with and
without the registration step. A summary of the two approaches for shape analysis
of closed curves is presented in Table 2.
6.10 Parallel Transport of Shape Deformations
We have previously discussed, in passing, an important tool that has many appli-
cations in shape analysis. This tool is parallel transport of tangent vectors from
one point on a manifold to another, along a geodesic path between the two points.
In the context of shape analysis, where the manifold of interest is a shape space,
say S c
2 , and a tangent vector represents a deformation of a shape, the notion of

220
6 Shapes of Planar Closed Curves
Space
Geodesic (nonelastic)
Space
Geodesic (elastic)
c
1
c
1
c
2
c
2
c
1
c
1
c
2
c
2
c
2
c
2
c
1
c
1
Fig. 6.31 Geodesic paths between polygonal shapes in pre-shape and shape spaces for both
non-elastic and elastic frameworks
Shape 1
/
c
2 SO(2)
Shape 2
Geodesic in
Shape 1
Geodesic in
Geodesic in
/
c
2 SO(2)
Shape 2
Geodesic in
Shape 1
/
c
2 SO(2)
Shape 2
Geodesic in
c
2
c
2
c
2
Geodesic in
Fig. 6.32 Examples of geodesic paths under SRVF representation with and without the regis-
tration. In each case the top row shows the geodesic under a ﬁxed arc-length parameterization
for both curves, while the bottom row shows the geodesic under optimal re-parameterization of
the second curve

6.10 Parallel Transport of Shape Deformations
221
Table 6.2 Summary of shape analysis of planar closed curves
Property
Bending-only planar closed
curves
Elastic planar closed curves
Representation
Angle function θ (under
arc-length param.)
Sqrt-velocity function q (arbitrary
param.)
˙β(s) = eiθ(s) with | ˙β(s)| = 1
q(t) =
˙β(t)
√
| ˙β(t)|
Metric/larger space L2 metric/L2([0, 2π], R)
L2 metric/L2(S1, R2)
Pre-shape space
Pre-shape space
C c
1 = {θ ∈
L2([0, 2π], R)|  2π
0
θ(t)dt = π,
θ(2π) =
θ(0) + 2π,  2π
0
cos(θ(t))dt =
0  2π
0
sin(θ(t))dt = 0}
C c
2 = {q ∈L2(S1, R2)| 
S1 |q(t)|2dt = 1,

S1 q(t)|q(t)|dt = 0}
Angle functions of closed
curves of length 2π with ﬁxed
with orientation π
Sqrt-velocity functions of closed curves of
length 1
Normal space
Nθ(C c
1 ) =
span{1, cos ◦θ, sin ◦θ}
Nq(C c
2 ) =
span{q, {

qi(t)
|q(t)| q(t) + |q(t)|ei	
, i = 1, 2}}
Tangent space
Tθ(C c
1 ) = {v ∈
L2([0, 2π], R)|v ⊥Nθ(C c
1 )}
Tq(C c
2 ) = {w ∈L2(S1, R2)|v ⊥Nq(C c
2 )}
Geodesics
Computed using a shooting
method
Computed using a path-straightening
method
Shape space
Shape-preserving
Placement of origin: S1
Rotation SO(2), (O, q) = Oq
transformations
Re-parameterization ΓS,
(q, γ) = (q ◦γ)√˙γ
(ΓS: orientation-preserving diﬀeos of S1)
Equivalence class
[θ] = {(u, θ)|u ∈S1}
[q] = {O(q ◦γ)√˙γ|O ∈SO(2), γ ∈ΓS}
Shape space
S c
1 = C c
1 /S1 = {[θ]|θ ∈C c
1 }
S c
2 = C c
2 /(SO(2) × ΓS) = {[q]|q ∈C c
2 }
Tangent space
Tθ(S c
1 ) = {v ∈
Tθ(C c
1 )| 
v, ˙θ
= 0}
Tq(S c
2 ) = {w ∈Tq(C c
2 )|w ⊥Tq([q])}
Tq([q]) = {A(√˙γ(q ◦γid)v +
1
2√
˙γid q ˙v)|v ∈
Tγid(ΓS), A ∈TI(SO(2))}
Geodesics
Iterated over the optimal
starting point on the second
curve—u∗
Iterate over the two alignment steps until
convergence:
1. Find optimal rotation O∗in SO(2)
2. Find optimal re-parameterization γ∗in
ΓS using a gradient process
Set θ∗
2 = (u∗, θ2) and compute
geodesic between θ1 and θ2∗in
C c
1
Set q∗
2 = O∗(q2 ◦γ∗)
 ˙γ∗and compute
geodesic between q1 and q∗
2 in C c
2 .
parallel transport has a natural interpretation. This implies transferring the defor-
mation from one shape to another, in such a way that represents least distortion
in the deformation ﬁeld during the transfer. There are many situations in which
we want to transfer deformations from one shape to another. We motivate this
application with two examples, although one can construct several more.
• Prediction of Growth-Related Changes: Consider the change in shape as-
sociated with the growth of a leaf over time. While it may also grow in size (or
scale), let’s focus primarily on the changes in its shape. These changes, over

222
6 Shapes of Planar Closed Curves
a certain interval [T0, T1], can be treated as observation of an underlying con-
tinuous path α : [T0, T1] →S c
2 . A short-term change in the shape at a time
τ ∈[T0, T1] is captured by the velocity vector ˙α(τ). In other words, using these
velocity vectors, also called deformations, one can represent and analyze instan-
taneous growth. Now, consider another leaf whose shape is given at time T0 and
we want to predict its shape at a time τ ∈[T0, T1]. One way to accomplish that
is to transfer deformations from the previous leaf and apply those deformations
to obtain predicted shapes for the new leaf.
• Prediction of Silhouette Shapes from Novel Views: One diﬃculty in
using shapes for recognizing 3D objects is that their 2D appearances change
according to the viewing angle. Since a large majority of imaging technology
is oriented toward 2D images, there is a striking focus on planar shapes, their
analysis and modeling, despite the viewing variability. Within this focus area,
there is an interesting problem of predicting shapes of 3D objects from novel
viewing angles. Our solution to the problem of shape prediction is the following.
If we know how a known object deforms under a viewpoint change, perhaps we
can apply the “same” deformation to a similar (yet novel) object and predict
its deformation under the same viewpoint change.
These and similar applications require taking deformations from one shape and
applying them to another. The basic technical issue in this procedure is that since
shape spaces are nonlinear spaces, and the deformations are tangent vectors on
these manifolds, the deformations at one point or shape cannot simply be applied
to another. One would need to transport the required deformation from one shape
to the other, before applying that deformation.
The mathematical statement of this problem is as follows: let [q1] and [q2] be
two given shapes and let v1 ∈T[q1](S c
2 ) be a tangent vector, or a deformation,
at the ﬁrst shape. Our goal is to form a vector v2 ∈T[q2](S c
2 ) that is the parallel
transport of v1 along the geodesic connecting [q1] and [q2]. We remind the reader
that one transports a vector v1 along a path α on a manifold by constructing a
vector ﬁeld w : [0, 1] →T S c
2 along that path such that: (1) the initial value of the
ﬁeld is v1, i.e., w(0) = v1 and (2) the covariant derivative of w is zero along the
whole path, i.e., Dw
dτ = 0. Then, the end vector w(1) is the desired v2. See Chap. 3,
Sect. 3.4 for details. Without any loss of generality, we assume that q1 and q2 are
the nearest points on the two orbits [q1] and [q2] and let α : [0, 1] →C c
2 be the
geodesic path in C c
2 that represents the corresponding geodesic in the shape space.
That is, α is a path that is perpendicular to the rotation and re-parameterization
orbits in C c
2 . The algorithm for performing parallel transport has been studied
earlier and is repeated here.
Algorithm 23 (Parallel Transport Along Geodesics in S c
2 ).
1. Use Algorithm 22 to compute the geodesic α between q1 and q2 in C c
2 . (Note
that this geodesic represents the shortest path between [q1] and [q2] in S c
2 .)
This path is actually computed at uniformly spaced times with spacing 1/k:
α(0/k), α(1/k), . . . , α(k/k).
2. Set w(0) = v1. Let l = ∥v1∥be the L2 norm of v1.
3. For τ = 1, 2, . . ., k, perform the following steps:
a. Project w((τ −1)/k) into Tα(τ/k)(M) to obtain c(τ/k).
b. Set w(τ/k) = lc(τ/k)/∥c(τ/k)∥.

6.10 Parallel Transport of Shape Deformations
223
To implement this, we only require the mechanism for projecting an arbitrary
element of L2(S1, R2) into Tq(C c
2 ). It must be kept in mind that since v1⊥Tq1([q1]),
all resulting vectors w(τ/k) will also be perpendicular to the Tα(τ/k)([α(τ/k)]). The
projection into the tangent Tq(C c
2 ) is given in Eq. 6.36.
6.10.1 Prediction of Silhouettes from Novel Views
Now we illustrate the use of deformation transfer in predicting shapes of silhouettes
of 3D objects from novel viewing angles.
The mathematical statement of this problem is as follows. Let [qa
1] and [qb
1] be
the shapes of silhouettes a 3D object O1 when viewed from two viewing angles
θa, θb ∈SO(3), respectively. The deformation in contours, in going from [qa
1] to
[qb
1] depends on some physical factors: the geometry of O1 and the viewing angles
involved. Consider another object O2 that is similar but not identical to O1 in
geometry. Given the shape of its silhouette [qa
2] from the viewing angle θa, our
goal is to predict the corresponding shape [qb
2] from the viewing angle θb. Our
solution is based on taking the deformation that deforms [qa
1] to [qb
1] and applying
it to [qa
2] after some adjustments. Here is the algorithm:
1. Let α1(τ) be a geodesic between [qa
1] and [qb
1] in S c and v1 ≡˙α1(0) ∈T[qa
1 ](S c)
be its initial velocity.
2. We need to transport v1 to [qa
2]; this is done using a forward parallel translation.
Let α12(τ) be a geodesic from [qa
1] to [qa
2] in S c. Construct a vector ﬁeld w(t)
such that w(0) = v1 and Dw
dτ = 0 for all points along α12. This is accomplished
in practice using Algorithm 23. Then, v2 ≡w(1) ∈T[qa
2 ](S c) is a parallel
translation of v1.
3. Construct a geodesic starting from [qa
2] in the direction of v2 with the path
length equal to the norm of v2.
Figure 6.33 shows two examples of this idea. In the top case, a hexagon ([qa
1]) is
deformed into a square ([qb
1]) using an elastic geodesic; this deformation is then
transported to a circle ([qa
2]) and applied to it to result in the prediction [qb
2]. A
similar transport is carried out in the bottom example.
Case 1
Case 2
Fig. 6.33 In each case: a geodesic from the template shape (hexagon) to the training shape
(top) and deformation of the test shape (circle) with the transported deformation (bottom)

224
6 Shapes of Planar Closed Curves
θb
24°
48°
72°
96°
120°
168°
216°
336°
[qa
1]
[qb
1]
[qa
2]
[qb
2]
10
20
30
40
50
60
10
20
30
40
50
60
10
20
30
40
50
60
10
20
30
40
50
60
10
20
30
40
50
60
10
20
30
40
50
60
10
20
30
40
50
60
10
20
30
40
50
60
Fig. 6.34 Shape predictions for novel pose. In each column, the ﬁrst two are given shapes of the
M60 from θa = 0 and θb. The deformation between these two is used to deform the T72 shape in
the third row and obtain a predicted shape in the fourth row. The accompanying pictures show
the true shapes of the T72 at those views
We present another example using the M60 tank as O1 and the T72 as O2. Given
shapes for diﬀerent azimuthal pose (ﬁxed elevation) of M60 and one azimuth for the
T72, we would like to predict shapes for the T72 from the other azimuthal angles.
Since both the objects are tanks, they have similar but not identical geometries.
For instance, both have mounted guns but the T72 has a longer gun than the M60.
In this experiment, we select θa = 0 and predict the shape of the T72 for several
θb The results are shown in Fig. 6.34. The ﬁrst and the third rows show the shapes
for [qa
1] and [qa
2], respectively, the shapes for the M60 and the T72 looking from
head on. The second row shows [qb
1] for diﬀerent θb given in the last column, while
the fourth row shows the predicted shapes for the T72 from those θb.
6.10.2 Classiﬁcation of 3D Objects Using Predicted
Silhouettes
How can we evaluate the quality of these predictions? One application of prediction
of silhouettes of 3D objects from novel views is in recognition. We perform a
simply binary classiﬁcation with and without the predicted shapes and compare
results. Here is the experimental setup. We have 62 and 59 total azimuthal views
of the M60 and the T72, respectively. Of these, we randomly select 31 views of
M60 and one view of the T72 as the training data; the remaining 31 (58) views
of the M60 (the T72) are used for testing. The classiﬁcation results, using the
nearest neighbor classiﬁer and the elastic distance dS c
2 (Eq. 6.42), are shown in
the table below. While the classiﬁcation for the M60 is perfect, as expected, the
classiﬁcation for the T72 is 46.55 %. (Actually, this number is somewhat higher
than expected—we would expect a smaller performance with only one training
shape.) Now we generate additional 31 shapes for the T72 using the prediction
method described earlier. Using the 31 training shapes of the M60, we generate 31
corresponding shapes of the T72 using parallel transport. The θa used here was
90◦. The classiﬁcation result after including the 31 predicted shapes is found to

6.11 Symmetry Analysis of Planar Shapes
225
Table 6.3 Classiﬁcation rate with (bold fonts) and without (normal fonts) use of predicted
shapes for the T72
Experiment 1 (θa = 90◦)
Experiment 2 (θa = 0◦)
Est./True
M60
T72
M60
T72
M60
100 % (100 %)
53.45 % (39.66 %)
100 % (100 %)
93.2 % (82.8 %)
T72
0 % (0 %)
46.55 % (60.34 %)
0 % (0 %)
6.8 % (17.2 %)
be 60.34 %, a 15 % increase in the performance when using shape predictions. We
performed the same experiment for another azimuth, θa = 0◦, and the results are
listed under experiment 2 in the table. In this case we improve the classiﬁcation
performance from 6.8 % to 17.2 %, an increase of almost 11 %, using the predicted
shapes of the T72 (Table 6.3). While this experiment was performed with only one
training shape, one can repeat this idea using multiple given shapes for the novel
object and then perform prediction for a novel view using joint information from
these views.
6.11 Symmetry Analysis of Planar Shapes
Reﬂection symmetry of objects is a property of interest in many applications.
Several papers in computer vision and image analysis have been concerned with
either evaluating the symmetry or exploiting its symmetric nature in its detection
and classiﬁcation. A 2D object is considered (reﬂection) symmetric around a line
if its shape remains unchanged after reﬂection in that line. Thus, naturally, some
quantiﬁcation of diﬀerences between curves and their reﬂections will help quantify
their symmetry. In this section, we present the use of elastic shape analysis in
characterizing symmetry of planar curves.
Let H be a 2 × 2 reﬂection matrix (a reﬂection matrix satisﬁes the conditions
H = HT , HT H = HHT = I2 and det(H) = −1), and let H be the set of all
such matrices. For any v ∈R2 (|v| ̸= 0), we can construct a reﬂection matrix using
H = I2−2 vvT
vT v . For a point x ∈R2, Hx is its reﬂection about the line orthogonal to
v and passing through the origin. For a planar curve β : D →R2 (where D = [0, 1]
or S1), the curve ˜β(t) = (H · β)(t) is its reﬂection in the same line.
Remark 6.1. We remind the reader that our shape analysis requires a ﬁxed direc-
tion of parameterization of curves throughout this textbook. However, a reﬂection
changes the direction of parameterization. Therefore, along with the multiplication
by H, one also has to reverse the direction of parameterization, in order to pre-
serve the original direction and to perform shape analysis. With a slight abuse of
notation, we will denote this direction-preserving reﬂection by H. In other words,
for β : [0, 1] →R2, (H · β)(t) ≡Hβ(1 −t).
If the SRVF of β is given by q, then the SRVF of H · β is given by −H · q (proof
is left as an exercise).
Recall that C2 is the set of SRVFs of absolutely continuous, unit-length curves
(without the closure constraint). It is a Hilbert sphere and for any two points
q1, q2 ∈C2, such that q1 ̸= −q2, there exists a unique, shortest (and constant
speed) geodesic going from one to the other. The following result characterizes the
midpoint of the geodesic between a curve and its reﬂection.

226
6 Shapes of Planar Closed Curves
Lemma 6.4. Let q ∈C2 be the SRVF of a curve β and ˜q = −H ·q be the SRVF of
˜β = H ·β, for any H ∈H . Then, the midpoint of the constant-speed geodesic path
connecting them is a perfectly symmetric curve under H. That is, (H · q0) = q0
where q0 denotes the midpoint of the geodesic.
Proof. Let α : [0, 1] →C2 denote the unique, constant-speed, shortest-geodesic
path such that α(0) = q and α(1) = (H · q). Form a new path ˜α(τ) ≡(H · α(τ))
by reﬂecting each curve along α by the same H. Accordingly, we get ˜α(0) =
(H · α(0)) = (H · q) and ˜α(1) = (H · α(1)) = (H · (H · q)) = q. Next, we
claim that ˜α is also a constant-speed geodesic path between the two SRVFs. (This
claim is left as an exercise for the reader.) Since this geodesic was assumed to
be unique, α and ˜α are exactly the same paths but traversed in diﬀerent direc-
tion, i.e., ˜α(τ) = α(1 −τ). Furthermore, since α is a constant-speed path, we
conclude that ˜α( 1
2) = (H · α( 1
2)) = α( 1
2) and that α( 1
2) is symmetric under H.
⊓⊔
One can also use the length of that geodesic α, denoted by dC c
2 (q, H · q), as a
measure of symmetry of β under that given H. Indeed, if the length of α is zero,
then the original curve is perfectly symmetric under H to start with! A parameter-
ized curve may be symmetric under one reﬂection and may not be symmetric under
another. Thus, to assess reﬂection symmetry, one has to search over all possible
reﬂections and ﬁnd if there exists a reﬂection under which the curve is symmetric.
In that case, one can use minH∈H dc(q, H · q) as a measure of asymmetry of the
given curve with SRVF q. Note that:
min
H∈H dc(q, H · q) = min
v∈R2

cos−1(
 1
0

q(t), (I −vvT
vT v )q(1 −t)

dt)

=
min
O∈SO(2)

cos−1(
 1
0

q(t), O(I −v0vT
0
vT
0 v0
)q(1 −t)

dt)

,
for any v0 ∈R2(|v0| ̸= 0). The last equation uses the fact that any arbitrary re-
ﬂection in R2 can be replaced by an arbitrary (ﬁxed) reﬂection and an appropriate
rotation! This suggests taking an arbitrary reﬂection of q, using a vo ∈R2, and
then computing geodesic paths between the SO(2) orbit of q and its arbitrary
reﬂection.
One can take this discussion a step further and compare the symmetry of shapes
of curves, rather than the curves themselves. In this situation, one compares shapes
of a curve and its reﬂection, using a shape metric:
min
O∈SO(2),γ∈ΓI

cos−1(
 1
0

q(γ(t))

˙γ(t), O(I −v0vT
0
vT
0 v0
)q(1 −t)

dt)

.
(6.50)
This is exactly ds([q], [(H · q)]) for any H ∈H , where ds is the distance between
shapes of curve in S2. This concept can be extended to study symmetry of shapes
of closed curves using the geodesic distance ds deﬁned on S c
2 . The only diﬀerence
is that the re-parameterization group ΓI in this equation is replaced by ΓS. As
described earlier, using the identiﬁcation between ΓS and S1 × ΓI, this implies
repeated use of Eq. 6.50 for each shift of the origin on the second curve.
We illustrate this idea using a couple of examples. Shown in the top-left panel of
Fig. 6.35 is a curve and the next panel shows its reﬂection around an arbitrary line
(passing through the origin). The next panel shows the minimum value obtained

6.11 Symmetry Analysis of Planar Shapes
227
0
20
40
60
80
100 120
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
1.1
Given shape
Its reﬂection
Energy vs. shifts
Midpoint shape
Fig. 6.35 Symmetrizing arbitrary shapes using elastic deformations
0
10
20
30
40
50
60
0.45
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
Given shape
Its reﬂection
Energy vs. shifts
Midpoint shape
Fig. 6.36 Symmetrizing arbitrary shapes using elastic deformations
in Eq. 6.50 versus diﬀerent placements of origin on the second curve. The last plot
in the ﬁrst row shows the midpoint of elastic geodesic, while the geodesic itself is
shown in the second row. Figure 6.36 shows another example of this idea, using a
diﬀerent curve.
We summarize the results on symmetry analysis using elastic geodesics between
a curve β and its arbitrary reﬂection Hβ:
1. The geodesic elastic distance, dS c
2 in S c
2 , can be used to quantify the level of
asymmetry in a curve. In other words, the shape distance dS c
2 between β and
Hβ is a quantiﬁcation of asymmetry of β.
2. For any given curve, the halfway point along the geodesic between that curve
and its reﬂection provides the nearest symmetric shape, under the distance dS c
2 .
3. This analysis provides the most eﬃcient way to deform any given closed curve
into a symmetric curve. We sketch a proof of this statement here. Note that
for an arbitrary reﬂection matrix H about a line of symmetry L, the map
H : L2(S1, R2) →L2(S1, R2) is an “involution”, i.e., it has the property that
H · (H · q) = q for all q ∈L2(S1, R2). As a result, we obtain a direct sum
decomposition L2(S1, R2) = S ⊕A, where S is the +1-eigen space of H and
A is the −1-eigen space. Note that the subspace S consists precisely of SRVFs
of curves that are symmetric with respect to L. Given q ∈L2(S1, R2), we can

228
6 Shapes of Planar Closed Curves
0
5
10
15
20
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Fig. 6.37 Left panel shows a set of 20 curves and the right panel provides a quantiﬁcation of
their symmetries using geodesic lengths. The curves have been arranged in the order of increasing
asymmetry
uniquely represent it under this direct sum decomposition by q = s + a, where
s ∈S and a ∈A; then, H · q = s −a. Note that s represents the orthogonal
projection of q onto the subspace S. Hence, s represents the closest curve to q
that is symmetric about L. Furthermore, s is the midpoint of the line from q
to H · q. Since the space of all reﬂection matrices is compact, we can choose the
particular H that minimizes ∥a∥. This choice of H will minimize the distance
from q to H ·q and will produce the shortest deformation from q to a symmetric
curve: this is simply the straight line from q to s, where s is deﬁned using the
minimizing choice of H.
4. It also provides the line of symmetry in any symmetric object. If v is the original
reﬂection and O∗is the optimal rotation obtained during geodesic computation,
then O∗v deﬁnes the axis of reﬂection or the line of symmetry.
Figure 6.37 shows a set of 20 curves (left panel) and the quantiﬁcation of their
level of asymmetry in the bar chart on the right. The curves have been displayed in
the order of increasing asymmetry, starting with the half moon (most symmetric)
and ending with a cow silhouette (least symmetric).
6.12 Exercises
6.12.1 Theoretical Exercises
1. Show that there is a natural bijection between ΓS and ΓI × S1. Hint: ﬁrst
identify ΓI with the subgroup of ΓS that ﬁxes 1 ∈S1 (thinking of S1 as the
set of unit complex numbers). Then, associate to (γ, z) ∈ΓI × S1 the diﬀeo
˜γ ∈ΓS that ﬁrst applies γ and then rotates by z.
2. Derive expressions for angle functions of the following curves, ignoring those
points in the domain where the derivative is not continuous:
a. A unit circle
b. An equilateral triangle with one side aligned to the x axis and a side length
2π/3
c. An ellipse deﬁned by the condition:
β2
x
a2 +
β2
y
b2 = 1 for a = 1 and b = 2.
Rescale the curve to be of length 2π.

6.12 Exercises
229
Plot them with the starting point corresponding to θ(0) = 0. Evaluate and
display geodesic paths between these curves in C1.
3. Show that the space D deﬁned in Eq. 6.7 is an aﬃne space.
4. Show that for all θ ∈D, the functions {1, sin(θ(s)), cos(θ(s))} are linearly
independent and lie in B. Furthermore, verify that dΦ1 : B →R3 is surjective
at every θ ∈Φ−1
1 (π, 0, 0).
5. For a parameterized curve β : [0, 1] →R2 with the SRVF q : [0, 1] →R2, show
that β(0) = β(1) implies that
 1
0 q(t)|q(t)|dt = 0. Also, verify that L[β] = ∥q∥.
6. Show that the closure constraint, i.e., the constraint that a curve is closed, is
a nonlinear constraint in both the angle function and SRVF representation.
That is, if θ1, θ2 ∈C c
1 , then aθ1 + bθ2 may not be in C c
1 for some a, b ∈R, and
repeat it for the SRVF representation in C c
2 space.
7. Show that for any u ∈R, we have  2π
0
v(s −u)w(s −u) ds =  2π
0
v(s)w(s) ds
if v, w : R →R are both periodic with a period of 2π.
8. Prove Theorem 6.3: The action of S1 on C c
1 given by Eq. 6.14 is an action by
isometries.
9. For the following paths α with their given parameterizations, calculate the
path energy E[α] =
 1
0 ⟨˙α(τ), ˙α(τ)⟩dt in R2 under the Euclidean metric. Set
p1 = (−1, 0) and p2 = (1, 0).
• A straight line between p1 and p2
• A unit half circle from p1 to p2 with its center at (0, 0)
• The shorter part of ellipsoid x2
1 + x2
2
4 = 1 that passes through p1 and p2
10. Denote elements of S1 by unit complex numbers and deﬁne a map Gw : S1 →S1
as Gw(z) = wz where w is also a unit complex number. Show that Gw is a
diﬀeomorphism for all w and the set of all Gw’s is a subgroup of ΓS.
11. Prove that the mapping given in Eq. 6.44, Moebius transformation, is a diﬀeo-
morphism from S1 to itself. Also, show that the set of all such mappings forms
a group. Verify the statement that the functions 1, cos(t) and sin(t) span the
tangent space of this manifold at the identity γid.
12. Show that the basis given in Eq. 6.45 forms an orthonormal basis of Tγid(ΓS)
under the Palais metric given in Eq. 6.29.
13. Show that H = I2 −2 vvT
vT v, where v ∈R2 is a column vector with |v| ̸= 0, is a
reﬂection matrix.
14. For any 2×2 reﬂection matrix H0 ∈H , show that the coset [H0] = {OH0|O ∈
SO(2)} equals the set H . That is, for any H ∈H , there exists a rotation
O ∈SO(2) such that H = OH0.
15. If the SRVF of β : [0, 1] →R2 is given by q, then show that the SRVF of
˜β ≡H · β is given by −H · q. Recall that (H · β)(t) = Hβ(1 −t).
16. If α : [0, 1] →C2 is a geodesic path then show that its reﬂection (H · α(τ))
is also a geodesic path in C2. If α is a constant-speed path, then so is its
reﬂection.
6.12.2 Computational Exercises
1. Write a program to compute the angle function of a given parameterized curve.
(Ignore those points where the derivative of a curve is not continuous.) Run this

230
6 Shapes of Planar Closed Curves
program on the following curves: (1) a unit circle, (2) an equilateral triangle
of side length 2π/3 and one side parallel to the x axis, and (3) a general
ellipse of the type t →(a cos(2πt), b sin(2πt)) (rescale it to have length 2π
and re-parameterize it to have constant speed). Compare your results with
the analytical expressions obtained in Problem 2.
2. Provide an example to illustrate that even if θ1, θ2 satisfy Eq. 6.2, their linear
combination c1θ1 + c2θ2, for some c1, c2 ∈R, may not satisfy it.
3. Provide an example to illustrate that even if q1, q2 satisfy Eq. 6.3, their linear
combination c1q1 + c2q2, for some c1, c2 ∈R, may not satisfy it.
4. Implement a function to compute an orthonormal basis of the normal space
Nθ(C c
1 ) for a given θ ∈C c
1 . Then, use this function to project an arbitrary
element of L2([0, 2π], R) in Tθ(C c
1 ).
5. Implement a function to compute an orthonormal basis of the normal space
Nq(C c
2 ) for a given q ∈C c
2 . Then, use this function to project an arbitrary
element of L2(S1, R2) in Tq(C c
2 ).
6. Implement a function to project an arbitrary element θ ∈L2([0, 2π], R) in
the set C c
1 using Algorithm 8. Similarly, implement a function to project an
arbitrary element of L2(S1, R2) into the set C c
2 using Algorithm 18.
7. Write a program to implement the shooting algorithm (Algorithm 10) for
computing geodesics between any two closed curves in C c
1 .
8. Implement a numerical procedure for computing a covariant derivative of a
vector ﬁeld v along a smooth path α on S2 under the Euclidean Riemannian
metric. Test your program on the following setup: Let α be a half circle, forming
a part of the equator on S2. Deﬁne a vector ﬁeld w on it as follows: w(τ) at any
point α(τ) is the unit vector shooting from α(τ) to the north pole. Compute
and display the covariant derivative of w.
9. Use the previous program to compute the covariant derivative of a velocity
vector ﬁeld along the great circle between any two non-antipodal points.
10. Write a program that adapts the general path-straightening algorithm (Algo-
rithm 17) for computing geodesics between any two points on S2. Test this
program for any two non-antipodal points on S2 and compare with the ana-
lytical expressions for geodesics on S2 given earlier.
11. Write a program to implement the path-straightening algorithm (Algo-
rithm 19) for computing geodesics between any two curves in C c
2 using
their SRVF representation.
12. Write a program to compute the length of an arbitrary path α : [0, 1] →C c
2
under the elastic metric. Use this program to compare the lengths of the fol-
lowing two paths. For any two closed curves in C c
2 : (1) compute a geodesic
path between them in C2 (i.e., the pre-shape space without the closure con-
straint) and (2) compute a geodesic between them in C c
2 (i.e., the pre shape
space with the closure constraint).
13. We will compare resulting deformations between shapes of closed curves using
diﬀerent strategies. For any given pair of closed curves: (1) compute a uniform-
speed geodesic path between them in S c
2 using Algorithm 22 and (2) compute
a uniform-speed geodesic path between them in S2, i.e., without the closure
constraint, as described in Sect. 5.7, and project each point of this geodesic in
S c
2 using Algorithm 18. Approximate both these paths by the same number
of time points and compare the corresponding shapes along these two paths.
You can also use the previous program to compute the lengths of these two
curves and study how close they are!

6.13 Bibliographic Notes
231
14. Implement the parallel transport procedure given in Algorithm 23 to transport
deformations from one shape to another in S c
2 .
6.13 Bibliographic Notes
Klassen et al. [54] were the ﬁrst to compute geodesics on shape spaces of closed
curves, using angle function representation. The identiﬁcation of the space of pla-
nar closed curves, using the complex square-root representation, with a Grassmann
manifold, was discussed in [126]. The general elastic metric for planar curves
was studied in [79] and using SRVF representation in [42, 43, 106]. The path-
straightening algorithm for ﬁnding geodesics between closed curves was derived in
[53], while the shooting method was presented in [54]. The use of parallel trans-
port of tangent vectors for transferring deformations was covered in [106]. Samir
et al. [99] studied symmetry analysis of planar closed curves. Applications of shape
analysis of planar curves include 3D face recognition [97], silhouette analysis [1],
and analysis of segmented contours [58].

Chapter 7
Statistical Modeling on Nonlinear
Manifolds
It has been emphasized frequently in the earlier chapters that the representation
spaces of our interest are both nonlinear and inﬁnite dimensional. It is therefore
expected that some standard ideas in statistics, where the domains are usually
ﬁnite-dimensional vector spaces, will not be applicable here directly. Since these
ideas involve addition, subtraction, and linear combinations of vectors, they will ex-
pectedly need some modiﬁcations before application to our representation spaces.
In fact, one needs to use Riemannian geometry and related analytical or compu-
tational tools, developed for representation spaces earlier in Chaps. 4, 5, and 6, to
deﬁne and compute statistical quantities. Before we consider statistical modeling
of functional and shape data in later chapters, we use this chapter to introduce
basic ideas for some familiar nonlinear manifolds, such as the unit spheres Sn for
n = 1 and 2, S∞and some quotient spaces of the type M/G. We will restrict to
a setup where G is a group that acts on a Riemannian manifold M in such a way
that the action is by isometries under the metric of M.
7.1 Goals and Challenges
The main goals for this chapter are:
1. To establish the notions of means and covariances for Riemannian manifolds and
their quotient spaces and, in case of sample data, to estimate these parameters
using sample statistics. Although there is a possibility of viewing these statistics
as estimators of population parameters, we will primarily treat them as tools for
generating statistical summaries of the datasets that take values on the under-
lying manifolds. The challenge here is to deﬁne and compute these quantities in
spaces where addition, subtraction, and multiplication are no longer valid and
need to be replaced by geometric operations.
2. To extend the notion of a multivariate normal density to nonlinear Riemannian
manifolds, such as the unit sphere Sn, and their quotient spaces. The main
challenges here come from nonlinearity and compactness of these spaces. Several
choices will be discussed and one of them, involving truncated wrapped-normal
densities, will be considered in detail.
3. To establish some basic tools for statistical inferences using these probability
models. For instance, the truncated wrapped-normal densities can be used for
© Springer-Verlag New York 2016
A. Srivastava, E.P. Klassen, Functional and Shape Data Analysis,
Springer Series in Statistics, DOI 10.1007/978-1-4939-4020-2 7
233

234
7 Statistical Modeling on Nonlinear Manifolds
statistical inferences in diﬀerent ways. They can be used to form prior densities
for Bayesian inferences, or they can be used to describe in-class variability for
use in statistical decision theory. They can serve as generative models for the
data in the sense that one can sample from them. These samples can then be
useful, for instance, in Monte Carlo-type methods for generating inferences.
7.2 Basic Setup
We start with a basic question about statistical analysis on nonlinear manifolds.
What are the main challenges in applying classical statistics if the underlying
domain is nonlinear? Take the case of the most basis statistics in a Euclidean case,
the sample mean, for a sample set (x1, x2, . . . , xk) on Rn:
¯xk = 1
k
k

i=1
xi,
xi ∈Rn .
(7.1)
Since ¯xk is a widely used and studied statistic, the pros and cons of using ¯xk, as
an estimate of the population mean, are well known. Assuming that xi ∼f(x),
with μ = Ef[x], we know that ¯xk is an unbiased and eﬃcient estimator of μ, but
is susceptible to the outliers. Now what if the underlying space is not Rn but a
nonlinear manifold, say Sn, instead? In this situation, the summation in Eq. 7.1 is
no longer a valid operation and that equation is not useful anymore. Even in the
simple case of S1, if one tries to average points on S1 as real numbers, the results
are likely to be wrong. Take, for example, the Euclidean average of two points
with angles (relative to say X axis) given by ϵ and 2π −ϵ, for an ϵ > 0 small. This
value 2π−ϵ+ϵ
2
= π is not a good representative of the original points and, thus,
is not a useful summary. So, how can one deﬁne the sample mean for points on
a nonlinear manifold, let alone higher moments, and obtain summaries that are
representatives of the original data?
To answer this question, we consider an n-dimensional Riemannian manifold
M. Let d(p, q) be the length of the shortest geodesic between arbitrary points
p, q ∈M. To facilitate a discussion of diﬀerent options available, we will assume
that there exists an embedding E : M →V where V is an m-dimensional Hilbert
space (n ≤m). In this context, an embedding is an injective (one-to-one) map
that preserves the Riemannian structure between M and V . (We clarify that this
embedding is needed only to perform a certain type of statistical analysis, described
later as an extrinsic analysis, and not for the main ideas presented in this chapter.)
Stated diﬀerently, the Riemannian metric on M coincides with the standard inner
product of V . We have chosen V to be a vector space so that we can perform more
standard multivariate statistical analysis in V . The distance between any two
elements p, q ∈M, is the geodesic distance d(p, q) when the geodesic is restricted
to be in M and it is |E(p)−E(q)|, with |·| being the norm of V , when the geodesic
is allowed to be in V . The latter distance, of course, depends on the choice of the
embedding E.
Assume that we have a probability density function f on M with respect to a
chosen standard measure on M. This function f : M →R≥0 satisﬁes the property

7.3 Probability Densities on Manifolds
235
that

M f(p)dp = 1, where dp denotes a base measure on M with respect to which
the density f is deﬁned. We can extend f to the larger set V by simply setting:
˜f(x) =
 f(p)
if x = E(p), p ∈M
0
if x ̸∈E(M)
.
(7.2)
That is, ˜f(x) = f(E−1(x))1x(E(M))JE(x) where 1x(A) denotes the indicator func-
tion for the set A and JE(x) is the Jacobian of E at p. ˜f is called the extension of
f from M to V and is unique by construction. Naturally, ˜f is a probability density
function on V . Next, we will present some choices of f on a general M and will
address the issue of deﬁning means and covariances on M for some fs.
7.3 Probability Densities on Manifolds
Similar to the Euclidean domains, the two broad categories of probability densities
on M are: parametric and nonparametric.
1. Parametric densities: A probability density function that is completely spec-
iﬁed by a handful of parameters is said to take a parametric form. Denote the
probability density by f(p; a), where a is a set of parameters specifying the
density function. In case of M = Rn, a simple and important example of a
parametric family is the multivariate normal density:
f(x; x0, C) =
1
(2π)n/2 det(C)1/2 e−1
2 (x−x0)T C−1(x−x0) .
(7.3)
The mean x0 ∈Rn and the covariance C ∈Rn×n are the two parameters that
completely specify a normal density. In case of nonlinear domains, a precise
notion of a normal density may not exist. However, often there are some similar
constructions that can be derived and used on a case-by-case basis.
In case of a nonlinear manifold M, at least three types of parametric den-
sities are possible depending upon the geometry of M. Firstly, in cases where
it applies, we will start with a uniform density. A uniform density is important
for its role in sampling from other, more interesting densities. Secondly, we will
study densities of the type e−d(p,μ)2/σ2, where d is the geodesic distance on M
under the chosen Riemannian metric. Such a density is not feasible when M is
inﬁnite dimensional, but can be applicable to a ﬁnite-dimensional submanifold
of M. Finally, we will study the truncated wrapped-normal densities, densities
that are ﬁrst deﬁned on tangent spaces and then mapped to the manifold using
the exponential or a similar map. To reach an explicit expression for the result-
ing density, one needs expressions for the Jacobians of these maps, and in case
of M = Sn, we shall present an explicit formula.
To estimate a parametric density from given data, one only has to estimate
the relevant parameters, and there may be several estimators that are available
for this task. For example, if we are given independent and identically dis-
tributed samples p1, p2, . . . , pk on M, from a parametric density f(p; a), then
one can estimate a using the criterion:
ˆa = argmax
a
k
2
i=1
f(pi; a) = argmax
a
k

i=1
log(f(pi; a)) .
(7.4)

236
7 Statistical Modeling on Nonlinear Manifolds
This is called the maximum-likelihood estimate (MLE) of a and the esti-
mated density is given by f(p; ˆa).
2. Nonparametric densities: In this case we do not make any prior assumption
about the functional form of the density. The only constraints on f are the
ones from its deﬁnition as a probability density—it is a non-negative function
that integrates to one on its domain. Occasionally one imposes a smoothness
constraint, e.g., assume that the ﬁrst k derivatives of f are continuous, for the
purpose of estimating f from the data. How can one estimate a nonparamet-
ric density function from the given sample data {p1, p2, . . . , pk}? Even though
several types of estimators exist, the most common one is the kernel density
estimator. This concept of kernel density estimator for nonlinear Riemannian
domains has been developed by several researchers. For a point p ∈M, we will
use θp : M →R to denote the volume density function on M. This volume
density function θq(p) is the determinant of the Jacobian of the exponential
map expq : Tq(M) →M evaluated at p in the neighborhood of mean μ. Then,
for a collection of points p1, p2, . . . , pk ∈M, one can deﬁne a Kernel density
estimator to be:
ˆfk(p) = 1
k
k

i=1
1
hn
kθpi(p)K(d(p, pi)
hk
)
(7.5)
where K is a chosen kernel function with the desired smoothness properties, n
is the dimension of M, hk is the bandwidth for the sample size k, and d(·, ·) is
the geodesic distance. The estimated density ˆfk is the superposition of kernels
centered at diﬀerent sample points pi. If a certain point p is close to many
samples pi, then the estimated density function at that point will be high. If p
is far away from the sample points, then the estimated density function will be
low. By deﬁnition, ˆfk is non-negative and it integrates to one on M.
Now that we have some ideas for imposing a probability density f on M, we
can return to the problem of deﬁning and ﬁnding summary statistics for data
sampled from f on M. These statistics can be viewed as estimates of the ﬁrst few
centralized moments under a probability density f on M, but we will motivate it
from the perspective of generating statistical summaries. That is, given a set of
points on M, we seek a method for generating data summary that is both intuitive
and convenient. There are at least two possibilities for deﬁning these summaries.
The ﬁrst one is to completely restrict to M and use its Riemannian structure to
perform calculations. The second is to use the embedding E of M in V , perform
the desired analysis on the extended pdf ˜f on V , using tools from multivariate
statistics, and then project the ﬁnal solution back to M. The former is called an
intrinsic analysis and the latter is called an extrinsic analysis. Both have their
advantages and disadvantages. We discuss these two ideas next.
7.4 Summary Statistics on Manifolds
7.4.1 Intrinsic Statistics
An intrinsic analysis results when the considerations are completely restricted to
M. Once a Riemannian metric is deﬁned and tools for computing geodesics, the

7.4 Summary Statistics on Manifolds
237
exponential map and its inverse are available, there is no use of the ambient space
V in this approach. In this context, we repeat a question we posed earlier: What
is a suitable notion of mean of a probability density f, or a “sample mean,” on
a Riemannian manifold M? In other words, what is the counterpart of Eq. 7.1 on
nonlinear manifolds? Of course, if M happens to be a vector space, with a given
inner product, then we want this notion to coincide with the deﬁnition (Eq. 7.1)
that we already know and use. A popular method for deﬁning a mean on a manifold
was proposed by Karcher who used the centroid of a density as its mean.
Deﬁnition 7.1 (Karcher Mean [46]). The Karcher mean μkar of a probability
density function f on M is deﬁned as a local minimizer of the cost function:
ρ : M →R≥0, where
ρ(p) =

M
d(p, q)2f(q) dq .
(7.6)
Here, dq denotes the reference measure used in deﬁning the probability density f
on M. The value of ρ at the Karcher mean is called the Karcher variance. The
existence and uniqueness of Karcher mean has to be studied on a case-by-case ba-
sis, depending upon the curvature of M and the nature of f. In case the minimum
exists but is not unique, then the set of minimizers is said to be the Karcher mean
on f. Instead of using a local minimum, one can also choose a global minimum in
the deﬁnition. While this will be more satisfying from the perspective of a statis-
tical formulation, the task of ﬁnding a global minimum, especially on complicated
shape spaces, makes the practical use of this characterization somewhat limited.
Thus, we content ourselves with having just a local minimum in the deﬁnition of
Karcher mean. If we have multiple such means, we select the ones with the smallest
value of ρ.
How does the deﬁnition of Karcher mean adapt to the sample set, i.e., a ﬁnite
set of points drawn from an underlying probability density? Let p1, p2, . . . , pk be
independent random samples from the density f. Then, the sample Karcher mean
of these points, denoted by ˆμkar, is deﬁned to be the local minimizer of the function:
ρk(p) = 1
k
k

i=1
d(p, pi)2 .
(7.7)
Since the deﬁnition of ˆμkar involves solving an optimization problem, its com-
putation requires some additional discussion. As ˆμkar is only a local minimum,
an iterative gradient-based search of this point is usually suﬃcient. To derive a
gradient algorithm, we use the following important fact from Karcher [46].
Theorem 7.1. For any two points p, q ∈M, assume that there exists a unique
shortest geodesic between them in M. Then, the gradient of the function −d(p, q)2,
with respect to p, is given by the tangent vector 2v ∈Tp(M) such that expp(v) = q.
Proof. This result can be proven more technically using Gauss’s law (see, e.g.,
[46]) but we will establish it using some elementary arguments. We have assumed
that there is precisely one shortest geodesic from p to q, and we wish to show that
the direction of the gradient of −d(p, q)2 at p is v ∈Tp(M) such that expp(v) = q.
We will argue this for the function g(p) .= −d(p, q) rather than −d(p, q)2. (This
makes no diﬀerence in the direction of the gradient since the square is a monotone
function). Now, recall the fact that the direction of the gradient is simply the
direction in which the directional derivative of a function is the largest. Let ϵ be a

238
7 Statistical Modeling on Nonlinear Manifolds
Fig. 7.1 The gradient of the distance function is along the geodesic
small positive number. Clearly, if we move a distance ϵ along the geodesic from p
to q, the value of g will increase by precisely ϵ, i.e., it will be g(p) + ϵ.
Now, let w be any other vector in the tangent space at p, as shown in Fig. 7.1.
Suppose the directional derivative of g in the direction w was larger than the
directional derivative along v. Then, if we travel a distance ϵ in the direction w
(say, along that geodesic), the value of g will increase more than it increased when
we moved along v. Now we will arrive at a point r such that g(r) > g(p) + ϵ. In
other words, d(r, q) < d(p, q) −ϵ. However, this gives the following contradiction:
Consider the path from p to q given ﬁrst by the geodesic from p to r and then by
a geodesic from r to q; its length is given by: d(p, r) + d(r, q) or ϵ + d(r, q). We
have shown that:
d(p, q) > d(r, q) + ϵ = d(r, q) + d(p, r) .
This is a violation of the triangle inequality. Hence, the directional derivative of
d(p, q) was largest in the direction of v, proving that v is the direction of the
gradient of g.
Now that we have the direction, what should be the magnitude of the gradient?
It is easy to see that the magnitude of v should be the geodesic distance between p
and q. Why? This is because an update (exponential map) from p in the direction
of v reaches q exactly if the magnitude of v is d(p, q). This proves the theorem.
⊓⊔
Since the gradient of −d(p, q)2 is 2v, with v = exp−1
p (q), the gradient of the cost
function in Eq. 7.7 is simply the sum 2
k
k
i=1 vi, where vi = exp−1
p (qi). Therefore,
the next iterate in the estimation of Karcher mean is expp( ϵ
k
k
i=1 vi), where ϵ/2
is the step size in the gradient direction.
An iterative algorithm for computing the sample Karcher mean is as follows.
Algorithm 24 (Karcher Mean on M). Let μ0 be an initial estimate of the
Karcher mean. Set j = 0.
1. For each i = 1, . . . , k, compute the tangent vector vi such that vi = exp−1
μj (qi).
2. Compute the average direction ¯v = 1
k
k
i=1 vi.
3. If ∥¯v∥is small, then stop. Else, update μj in the update direction using μj+1 =
expμj(ϵ¯v), where ϵ > 0 is small step size, typically 0.5.
4. Set j = j + 1 and return to Step 1.

7.4 Summary Statistics on Manifolds
239
It can be shown that this algorithm converges to a local minimum of the cost
function given in Eq. 7.7, which is the deﬁnition of the sample Karcher mean ˆμkar.
Depending upon the initial value μ0 and the step size ϵ, it converges to the nearest
local minimum.
Higher-Order Moments
We now have a way of estimating the Karcher mean
of a sample set on M. In a sense, we have tackled the ﬁrst and most diﬃcult
challenge posed by the nonlinearity of M. This is because the deﬁnition of higher
moments, under certain mild conditions, will be based on a linearization of M
around this mean point ˆμkar. This linearization is often based on the inverse of
exponential map that will transform points in the neighborhood of ˆμkar to the
corresponding points in the tangent space Tˆμkar(M) that, being a vector space,
allows standard multivariate statistics.
To illustrate this idea, we consider the covariance and higher-order (central)
moments of f on M. The classical deﬁnition of the covariance of a density f on Rn
is:

Rn(x −μkar)(x −μkar)T f(x)dx. Since M is not a vector space, the operation
p −μ is no longer valid and we cannot use this expression. Still, we would still
like to have some quantity that captures the second-order variation of shapes in a
population. In particular, we would like a mechanism to study the directions of
major variations in shapes, also termed modes of variation, similar to the dominant
eigenvectors of covariance matrix in standard multivariate statistics. We exploit
the fact that the tangent spaces of M are vector spaces and provide a natural
domain for deﬁning covariances. We can transfer the probability density f from
M to a tangent space Tp(M), using the inverse exponential map, and then use the
standard deﬁnition of central moments in that vector space. The role of x −p is
played by exp−1
p (x) and x + p by expp(x).
Since M is nonlinear and Tp(M) is linear, we will have some distortion in
mapping points from M to Tp(M). Speciﬁcally, when we take a set of points
{p1, . . . , pk} in M and study how the distance between them in M changes when
mapped to Tp(M), it is clear that the distances are not preserved unless M is a
ﬂat Riemannian manifold. Let vi = exp−1
p (pi), i = 1, 2, . . ., n, and we compare the
geodesic distance d(pi, pj) in M with the Euclidean distance ∥vi −vj∥. One way
to measure this distortion is using the quantity:
Dp(p1, p2, . . . , pn) =

i,j(d(pi, pj) −∥vi −vj∥)2

i,j d(pi, pj)2
.
(7.8)
It seems intuitive that this distortion is minimum when the point p happens to
be the Karcher mean ˆμkar of pis. It will be good to establish this statement theo-
retically, but we simply leave it as an assertion based only on empirical evidence.
Shown in Fig. 7.2 is an illustration of the idea. It shows a set of points on S2 that
are mapped to tangent spaces at two diﬀerent points using the inverse exponential
map. The point p for the left case is the Karcher mean ˆμkar, while p for the right
case is a nearby point other than ˆμkar. The ﬁgure shows the original points on S2
and their corresponding vectors vi ∈Tp(S2) from two diﬀerent viewing angles. It
is noted that in this example, the distortion given in Eq. 7.8 is smaller for the left
side than for the right side.
Choosing the Karcher mean μkar as the focal point for ﬂattening the manifold,
let, for any p ∈M, p →exp−1
μ (p) denote the inverse exponential map at μ from
M to Tμ(M). The coordinate system on Tμkar(M) is chosen such that the point

240
7 Statistical Modeling on Nonlinear Manifolds
View 1
View 1
p = Karcher mean
p = anarbitrary pi
View 2
View 2
Fig. 7.2 Mapping a set of points from S2 to Tp(S2) shows smaller distortion when p is the
Karcher mean of the points. Distortion for the left case is 4.98e −04 and distortion for the right
case is 9.28e −04
μkar maps to the origin 0 ∈Tμ(M). Now we can deﬁne the Karcher covariance
matrix as:
Kkar =

Tμ(M)
vvT fv(v)dv,
where v = exp−1
μkar(q) ,
and fv is the probability density induced on Tμkar(M) by the original density f on
M. Recall from Chap. 2 that if M is an n-dimensional manifold, then the tangent
space Tp(M) is a vector space of dimension n. Therefore, the matrix Kkar deﬁned
as the above is an n × n covariance matrix. For a ﬁnite sample set, the sample
Karcher variance is given by
ˆKkar =
1
k −1
k

i=1
vivT
i ,
where vi = exp−1
ˆμkar(qi) .
(7.9)
Note that we need not subtract the mean here since it is zero under the cho-
sen representation on the tangent space. ˆKkar is an n × n symmetric, positive-
deﬁnite matrix that represents the observed covariance matrix on the vector space
Tˆμkar(M). Let
ˆKkar = UΣU T =
n

i=1
σ2
i uiuT
i
denote the SVD of ˆKkar. Here U = [u1, u2, . . . , un] are the singular vectors and
{σ2
i } are the corresponding singular values. Assuming that σ2
1 ≥σ2
2 ≥σ2
3 . . . , the
vectors u1, u2, etc, in that order, are the directions of largest variations in the
population represented by f. One can depict these directions on M by mapping
them back under the exponential map expˆμkar. For instance, the geodesic starting
at ˆμkar in the direction u1 ∈Tˆμkar(M), i.e. t →expˆμkar(tu1) is the ﬁrst dominant
path of f on M. It is also called the ﬁrst principal geodesic curve. Similarly, the
geodesic along u2, i.e., t →expˆμkaru(tu2) is the second principal geodesic curve
and so on.
What happens if M is an inﬁnite-dimensional Riemannian manifold or its quo-
tient space? One can still apply the abovementioned framework for computing
summary statistics, with the following caveat. Since the tangent space Tˆμkar(M)
is now inﬁnite dimensional, an additional step of dimension reduction, as described

7.4 Summary Statistics on Manifolds
241
in Sect. 4.3.1, becomes necessary. Treating elements of Tˆμkar(M) as functions, we
can apply an FPCA step to restrict our analysis to a relevant ﬁnite-dimensional
subspace of Tˆμkar(M). This process is called the Tangent PCA or TPCA and is
related to FPCA covered earlier in Chap. 4.
7.4.2 Extrinsic Statistics
The other possibility for generating summary statistics of sample data on M is to
use the vector-space structure of the embedding space V to simplify calculations.
We remind the readers that E : M →V is an embedding that preserves the
Riemannian metric between M and V . Here one transfers the probability measure
to V , computes pertinent statistical quantities in V and projects the ﬁnal results
back to M. Let Π : V →M be a projection map deﬁned in such a way that
Π(v) = argmin
p∈M
∥v −E(p)∥2 .
(7.10)
The existence and the uniqueness of Π, of course, depend on the nature of M, p,
and E. Now, the extrinsic mean of a density f on M is deﬁned as follows.
Deﬁnition 7.2 (Extrinsic Mean). The extrinsic mean of density f on M, spec-
iﬁed with respect to an embedding E of M in a larger vector space V , is given by
μext = Π(

V v ˜f(v)dv), where Π is the projection deﬁned in Eq. 7.10,

V v ˜f(v)dv
is the standard mean of ˜f in V , and ˜f is the unique extension of f from M to V
(given by Eq. 7.2).
Once the embedding E has been chosen, and a mechanism for projection Π has
been established, the rest of the process is quite straightforward. It requires com-
puting the mean of ˜f in V and projecting it down to M. In case M is a Euclidean
space, and the E is simply the identity map, the extrinsic mean coincides with the
classical sample mean.
What about the covariance analysis in an extrinsic framework? An extrinsic
covariance can be deﬁned similarly to the extrinsic mean. Let P : V →Tν(M) be
an orthogonal projection. Since it is a linear map, it can be written as a n × m
matrix A so that P(v) = Av, where n = dim(V ) and m = dim(M). Deﬁne the co-
variance Kv =

V (v −ν)(v −ν)t ˜f(v)dv, in the vector space V and project it using:
Kext = AKvAT . Similar to the intrinsic case, one can deﬁne principal geodesic
curves in the extrinsic setup. Let Kext = UextΣextU T
ext = n
i=1 σ2
ext,iuext,iuT
ext,i
denote the SVD of Kext. Assuming that the singular values are labeled in a de-
scending order, the corresponding directions given by uext,1, uext,2, etc, in that
order, are the directions of largest variations in the population represented by
f. The geodesic starting at μext in the direction uext,1 ∈Tμext(M), given by
t →expμext(tuext,1), is the ﬁrst extrinsic principal geodesic curve. Similarly, the
geodesic along uext,2, given by t →expμext(tuext,2), is the second extrinsic princi-
pal geodesic curve, and so on. These geodesics are ﬁrst constructed in the vector
space V , using straight lines, and then projected on M using Π.
The pros and cons of using the extrinsic mean, as opposed to the Karcher
mean, are quite straightforward. The main advantage is its computational sim-
plicity. Once an embedding E is chosen, the rest of the analysis is quite standard

242
7 Statistical Modeling on Nonlinear Manifolds
and typically very eﬃcient. In contrast, the computation of the Karcher mean re-
quires repeated computations of the exponential and the inverse exponential maps
on the manifold. The main disadvantage of the extrinsic analysis is that the result
Π(ν) depends on the choice of embedding E, which is quite arbitrary. Diﬀerent
embeddings will result in diﬀerent solutions, and the projection Π itself may not
be unique. Another limitation of the extrinsic analysis is that in some cases in-
volving quotient manifolds—manifolds obtained as quotients of groups acting on
certain other manifolds—it is diﬃcult to ﬁnd vector spaces in which we can embed
the quotient spaces. This complicates the use of an extrinsic statistical analysis in
quotient spaces. Since our shape spaces are quotient spaces of certain Hilbert man-
ifolds, these arguments suggest that it is more natural to use intrinsic techniques
for statistical analysis of shapes.
7.5 Examples on Some Useful Manifolds
7.5.1 Statistical Analysis on S1
In the next few sections, we will study these deﬁnitions and concepts for some
well-known and relatively simple manifolds, such as S1, S2, S∞as a warmup for
the functional spaces in the next two chapters. In each case we will deﬁne diﬀerent
representations of elements on these manifolds, choose Riemannian metrics, intro-
duce some probability models, and eventually outline algorithms for computing
sample statistics. The geometric and the algebraic properties of these manifolds
have already been discussed in Chap. 3. Although this discussion on S1 and S2 is
presented as preliminary to statistics on more complicated shape spaces, they are
quite important in their own right.
Riemannian Structure As presented earlier in an example in Chap. 3, we will
impose the standard Euclidean Riemannian metric on S1. Under this Riemannian
structure, the geodesic between any two points (that do not form an antipodal pair)
is given by the shorter arc on the circle and the distance is given by the length of
that arc. To make it more concrete, let us represent an element of S1 by the angle
θ ∈[0, 2π], with the point 0 identiﬁed with the point 2π; with this identiﬁcation
each point of S1 is represented uniquely. Under the Euclidean Riemannian metric,
the geodesic distance between any two points θ1, θ2 ∈[0, 2π] is given by:
d(θ1, θ2) = min{|θ1 −θ2|, |θ1 + 2π −θ2|, |θ1 −2π −θ2|} .
(7.11)
For any θ ∈S1 and v ∈Tθ(S1) ≡R, the exponential map expθ : Tθ(S1) →S1 is
given by:
expθ(v) = (θ + v)mod 2π .
(7.12)
The inverse of this map is given by: exp−1
θ1 : S1 →Tθ1(S1),
exp−1
θ1 (θ2) =
⎧
⎨
⎩
θ2 −θ1,
if d(θ1, θ2) = |θ2 −θ1|
θ2 + 2π −θ1,
if d(θ1, θ2) = |θ2 + 2π −θ1|
θ2 −2π −θ1,
if d(θ1, θ2) = |θ2 −2π −θ1|
.
(7.13)
These quantities and maps will allow us to perform intrinsic analysis on S1.

7.5 Examples on Some Useful Manifolds
243
For an extrinsic analysis, on the other hand, we have to look for ways to embed
S1 in larger Euclidean spaces. A natural embedding comes from the mapping
E1(θ) = [cos(θ)
sin(θ)] ∈R2. Another embedding, this time in R4, is obtained
using:
θ →E2(θ) =

cos(θ) −sin(θ)
sin(θ) cos(θ)

.
The range of this mapping is actually the special orthogonal group SO(2), which
can be identiﬁed with S1 using precisely this mapping.
Under the embedding E1, the distance between any two points is given by:
|E1(θ1) −E1(θ2)| =
√
2

1 −cos(θ1 −θ2) .
In case we choose the embedding in SO(2), E2(θ), the distance between elements of
S1 will be 2

1 −cos(θ1 −θ2). Now that we have both a Riemannian structure and
Euclidean embeddings of S1, we are ready to discuss possible stochastic models
and statistical inferences on this space.
Probability Models To impose a probability model on S1, we can choose from
both parametric and nonparametric families. Since a parametric family is often
more eﬃcient to work with, it becomes our ﬁrst choice. Also, since S1 is a compact
manifold, some of the traditional parametric families, such as the normal density or
the exponential density functions, will not be applicable directly as these densities
have inﬁnite tail(s). So, what are the alternatives? Here are some suggestions:
1. Uniform Density: Of course, the simplest choice is the uniform density func-
tion: f(θ) =
1
2π. That is, all the elements of S1 occur with equal probability.
2. von Mises Density: Analogous to the normal random variable on R, one would
like a density such that: (i) it is unimodal, (ii) it is symmetric around the mode,
and (iii) its spread is controlled by one parameter. One such density on S1 can
be developed using the exponential of negative squared (extrinsic) distance, as
follows. Consider a point on S1 as the mean μ and let θ be a generic point on S1.
Using E1 as deﬁned above, we can compute the extrinsic distance, chord length
or chordal distance, between θ and μ: ∥E(θ) −E(μ)∥=

2(1 −cos(θ −μ)).
Then, we deﬁne a density function on S1 that is proportional to e
−d(θ,μ)2
σ2
, for
some value of the variance parameter σ2. After dropping the constant terms,
terms that do not depend on θ, and normalizing we obtain the probability
density:
f(θ; μ, σ2) =
1
I0( 1
σ2 ) exp(cos(θ −μ)
σ2
) ,
(7.14)
where μ is the mean and σ2 controls the variance. The value
1
σ2 is also called
the dispersion. I0 is the modiﬁed Bessel function of order zero and serves as the
normalizing constant for f. This density is called one-dimensional von Mises
density. Figure 7.3 shows some examples von Mises density for a ﬁxed μ and
decreasing values of σ2. As σ2 goes to inﬁnity, the resulting density converges
to a uniform density on S1.
We take a small detour to investigate this density. An important tool in gen-
erating statistical inference, especially using computational tools, is the Monte
Carlo approach. Here one uses samples from the underlying density to esti-
mate quantities (parameters) associated with that density. In case of statistical
inferences from a von Mises density on S1, it will be useful to know how to

244
7 Statistical Modeling on Nonlinear Manifolds
Fig. 7.3 Examples of von Mises density function on S1 for σ2 = 10, 1, 0.1, and 0.01 from left to
right
Fig. 7.4 Histograms of samples from von Mises density function on S1, with
1
σ2 = 5, for
increasing numbers of accepted samples from left to right
simulate from this density. In general there are two main techniques for directly
simulating a random variable. Of these methods—inverse transform method
and acceptance-rejection method—the second one is general enough to be use-
ful here. Note that while the strength of an acceptance-rejection method is its
wide applicability, its main weakness is the ineﬃciency resulting from rejection
of proposed values. Let the von Mises density function have mean μ and dis-
persion
1
σ2 , and let the candidate probability be the uniform density function.
Then, the bounding coeﬃcient is given by:
f(θ; μ, σ2)
1/2π
=
2π
I0( 1
σ2 ) exp(cos(θ −μ)
σ2
) ≤
2π
I0( 1
σ2 ) exp( 1
σ2 ) ≡c .
Hence, the ratio:
R(θ) ≡f(θ; μ, σ2)
c/2π
= exp((cos(θ −μ) −1)
σ2
) .
The acceptance-rejection sampling algorithm is given by:
Algorithm 25 (Acceptance-Rejection for von Mises Density on S1).
1. Generate U1 ∼U[0, 1] and set φ = 2πU1.
2. Generate U2 ∼U[0, 1].
3. If U2 < R(φ), then set θ = φ.
Else, return to Step 1.
Shown in Fig. 7.4 are some examples of this procedure. Each panel shows his-
tograms of samples generated using this algorithm; the number of samples in-
creases from left to right. The solid line shows the observed histograms and the
broken line shows the underlying von Mises density function. As the sample

7.5 Examples on Some Useful Manifolds
245
size gets larger, the reader can verify that the estimated density (histogram)
converges to the underlying true density.
3. Wrapped-Normal Density: Since the previous item used an extrinsic dis-
tance on S1, it is natural to ask the question: what will happen if we use the
intrinsic (Riemannian) distance, given in Eq. 7.11, instead. The distance be-
tween a point θ and the mean μ is the arc-length distance on S1, which is
equivalent to the Euclidean distance in a tangent space at μ as long as one
measures distances from μ. This approach has also been studied extensively for
S1 and is deﬁned as follows. Let x ∈R be a normal random variable with mean
μ = 0 and variance σ2. Let θ = expμ(x) to be the exponential map of x, as
deﬁned in Eq. 7.12. Since θ = (x + μ)mod 2π, we have θ = x + μ + i2π for some
integer i. This mapping from x to θ can be viewed as a wrapping of the real line
around the unit circle. Therefore, we can write the probability density of θ as:
f(θ; μ, σ2) =
∞

i=−∞
1
√
2πσ e−
1
2σ2 (x+μ+i2π)2 ,
where the summation accounts for all i associated with a given θ. The simulation
of values from this wrapped-normal density is straightforward. Simply generate
normal random variables and map them to S1 using the exponential map. An
interesting observation here is that we are able to keep the full density on
the tangent space Tμ(S1) and wrap it on S1 using inﬁnite summation. In higher
dimensional manifolds, this full wrapping is diﬃcult and we resort to a truncated
density in the tangent space before wrapping in on to the manifold.
4. Truncated Wrapped-Normal (TWN) Density: If we restrict the probabil-
ity density on the real line to a proper subset [−λ, λ] of [−π, π], using truncation,
then the exponential map from R to S1 is invertible. As a consequence, a wrap-
ping of this density on the S1 avoids the inﬁnite sum present in the previous
item. Deﬁne a truncated normal density on R, denoted by TN, to be:
f(x; μ, σ2, λ) =
1
Zσ,λ
e−1
2
(x−μ)2
σ2
1|x−μ|<λ ,
Zσ,λ =
 λ
−λ
e
−x2
2σ2 dx .
(7.15)
Since the mapping from S1 to R is simply θ = x + μ, the corresponding density
on S1 is exactly this expression.
Summary Statistics Now that we have a few choices for deﬁning probability
models on S1, we can turn our attention to the problem of deﬁning and estimating
central moments, such as mean and variance, using samples from a probability
model. We will illustrate the ideas of extrinsic and intrinsic statistics on S1 using
samples from a von Mises density.
1. Intrinsic Statistics: The ﬁrst quantity that we want to estimate is the Karcher
mean of f, which is deﬁned using Deﬁnition 7.1. This is a purely intrinsic analy-
sis and, thus, does not require any Euclidean embedding or extrinsic distances.
Assume that we have independent samples θ1, θ2, . . . , θn ∼f. We can particu-
larize Eq. 7.7 to deﬁne their Karcher mean on S1:
ˆμkar = argmin
θ∈S1
n

i=1
d(θ, θi)2 ,
(7.16)

246
7 Statistical Modeling on Nonlinear Manifolds
Fig. 7.5 Four examples of sample intrinsic and extrinsic means on a unit circle. The sample
points show an increasing divergence from left to right. The intrinsic mean is connected to the
origin using a solid line, while the extrinsic mean is connected using a dotted line
where d is given in Eq. 7.11. A local solution to this optimization problem can
be found using Algorithm 24. The formulas for the exponential map and its
inverse on S1 are given in Eqs. 7.12 and 7.13, respectively.
Let’s look at some examples of this idea, displayed in Fig. 7.5. The top
row shows four diﬀerent scenarios—in each case we see some sample points,
θ1, . . . , θn, on the unit circle and the cost function 1
n
n
i=1 d(θ, θi)2 drawn as a
height function on the circle. The bottom row of this ﬁgure shows the estimated
Karcher means for these cases. To highlight their positions on the circle, these
Karcher means have been connected with the origin using solid lines. In the
leftmost case where the sample points are clustered close to each other, the
location of the Karcher mean falls somewhere in the middle of those points, as
our intuition would suggest. Moving right in this ﬁgure, the sample points are
scattered further apart and the Karcher mean tries to follow these points. In
the third column, we see that the cost function has at least two local minimum
and the algorithm selects one of them arbitrarily as an estimate for the Karcher
mean. In the last case, the points are distributed uniformly around the circle
and the cost function is nearly a constant. Any point on the circle is as good an
estimate for the mean as any although the algorithm has been forced to select
a point on the circle.
Once we have estimated the Karcher mean, we can estimate the sample
variance as follows. For the estimated intrinsic mean ˆμkar, let vi = exp−1
ˆμkar(θi)
be the inverse exponential of a sample point θi, as deﬁned in Eq. 7.13. Each vi is
an element of the tangent space Tˆμkar(S1) and, by deﬁnition, their mean is zero.
The sample variance of vis is simply:
1
n−1
n
i=1 v2
i . It turns out that, except for
the constant
1
n−1, this is exactly the minimum value of the cost function given
in Eq. 7.16. This becomes the sample Karcher variance of the dataset θ1, . . . , θn
on S1.

7.5 Examples on Some Useful Manifolds
247
2. Extrinsic Statistics: The basic idea here is to embed the underlying space in
a larger Euclidean space V and to compute statistics in that larger space. Since
the computed statistic often does not lie on the manifold, one has to project
the Euclidean solution back to the required space. We apply this procedure to
S1 using some standard embeddings.
As the ﬁrst example, we take V = R2 with the Euclidean metric and we
embed S1 in R2 using the mapping E1(θ) = [cos(θ)
sin(θ)]. The sample mean
on R2 is straightforward (Eq. 7.1) and the projection Π : R2 →S1 is given by:
Π(v) = argmin
θ∈S1
∥v −E1(θ)∥2 = tan−1(v2
v1
) .
(7.17)
To view this projection as an element of E1(S1), we can use the concatenation:
E1(Π(v)) =
v
∥v∥.
This projection is valid only if ∥v∦= 0. In case v = 0, i.e., it coincides with
the origin in R2, there is no preferred projection and any point of S1 is equally
valid. For all other points in R2, this projection is well deﬁned and unique.
Now, for a given set of points θ1, θ2, . . . , θn ∈S1, we can derive an expres-
sion for its extrinsic mean in the chosen embedding. Deﬁne xi = E1(θi) =
[cos(θi) sin(θi)] and the sample mean ¯xn = 1
n
n
i=1 xi in R2. Finally, we project
this mean from R2 to S1 using Eq. 7.17. This projected point ˆμn is called the
sample extrinsic mean of the observed set. Figure 7.5 also shows some exam-
ples of the extrinsic means on the circle, this time connected to the origin by a
dotted line. The bottom row simply shows the plots in the top row but from a
diﬀerent viewing angle. In the leftmost case, where the sample points are close
together, we see that the two estimates of mean: extrinsic and intrinsic, coin-
cide. However, as the points become further apart, the two means start being
diﬀerent. The rightmost case is the case of uniform distribution on S1 where
any points on S1 is equally good as either intrinsic or extrinsic mean.
We re-emphasize the disadvantages of using extrinsic statistics using the ex-
ample of S1. The main disadvantage is that the results depend on the choice of
embedding. Diﬀerent embeddings will result in diﬀerent solutions. Earlier we have
used the standard embeddings of S1 in R2, but many more non-standard choices
are possible. Figure 7.6 a few illustrations of this issue. Consider an ellipsoidal
embedding of S1 in R2 using:
E3(θ) = [2 cos(θ), sin(θ)] .
3
1
2
3
1
2
3
1
2
Fig. 7.6 Extrinsic means for diﬀerent embeddings on S1 in R2. Each panel shows samples {θi}
as points E(θi), one extrinsic mean E(μext) (labeled as 1), and another extrinsic mean E(μ2
ext)
(labeled as 3)

248
7 Statistical Modeling on Nonlinear Manifolds
Note that this embedding does not preserve the Riemannian metric on S1. For this
embedding, we can deﬁne the projection Π2 : R2 →S1 as:
Π2(v) = argmin
θ∈S1
|v −E3(θ)|2 .
This projection is well deﬁned and unique for all points other than those that
fall on a line connecting the focal points. This sets up the computation of the
extrinsic mean under this ellipsoidal embedding. We take the samples θ1, . . . , θn,
compute their embeddings E3(θi), and then compute their sample mean in R2:
1
n
n
i=1 E3(θi). This mean is projected back to E3(S1) using Π2 and that results in
the extrinsic mean of the samples in this embedding. Call it ˆμ2
ext.
In order to compare this point in E3(S1), with the extrinsic mean for E1, we
can map it back to the circle using E1(ˆμext). Figure 7.6 shows several examples
of this comparison. Each ﬁgure shows the following items: (i) the original sample
points θ1, θ2, . . . , θn displayed on the unit circle (using E1(θi)) and the ellipse (using
E3(θi)), (ii) the extrinsic mean on the circle for the ﬁrst embedding E1(μext), labeled
as point 1, and (iii) the extrinsic mean on the ellipse for the second embedding
E3(μext), labeled as point 2. To compare them on the same domain, we can map
the point 2 from the ellipse to the unit circle using the inverse of E3(S1), and that
point is labeled 3. The diﬀerences between point 1 and 3 underscores the assertion
that extrinsic means are critically dependent on the choice of embedding.
7.5.2 Statistical Analysis on S2
Although S1 is an interesting example to start with, the ﬁrst real challenge comes
from S2, the two-dimensional sphere. As in the previous section, we will look at
some probability models on S2 and study the computation of both intrinsic and
extrinsic summary statistics on S2.
Riemannian Structure In Chap. 3, we have discussed the common choice of
Euclidean Riemannian metric and other elements of the diﬀerential geometry of
S2. Given any two points p1, p2 ∈S2, Eq. 3.5 provides an expression for the
geodesic from p1 to p2. Similarly, for p ∈S2 and a v ∈TP (S2), Eq. 3.8 provides
an expression for the geodesic starting from p in the direction v. In particular,
these two equations provides expressions for computing the exponential map and
its inverse. Therefore, we have the basic tools needed for performing intrinsic
statistical analysis on S2. We can use the spherical coordinates to index points on
a unit sphere. These coordinates are (φ1, φ2) with φ1 ∈[0, 2π] changing along the
latitude and φ2 ∈[0, π] changing along the altitude and with φ2 = 0 being the
north pole. This representation is not unique since, for example, for φ2 = 0, all
the values of φ1 map to the same point on S2. Still, it is useful to refer points on
S2 by their spherical coordinates. The geodesic distance between points p1 and p2
is given by: d(p1, p2) = cos−1(⟨p1, p2⟩).
For the extrinsic analysis, the elements of S2 can be represented as points in
R3 with the north pole identiﬁed with (0, 0, 1) and the south pole with (0, 0, −1).
This parameterization is given by:
ϵ(p) ≡ϵ(φ1, φ2) = (sin(φ1) sin(φ2), cos(φ1) sin(φ2), cos(φ2)) ∈R3 .

7.5 Examples on Some Useful Manifolds
249
The extrinsic, or chord-length, distance between any two points p1 and p2 is
given by:
∥ϵ(p(1))−ϵ(p(2))∥=

−2(sin(φ(1)
2 ) sin(φ(2)
2 ) cos(φ(1)
1
−φ(2)
1 ) + cos(φ(1)
2 ) cos(φ(2)
2 )) .
Probability Models What are the diﬀerent choices of probability densities on
S2? We discuss the same three classes as we studied for S1:
1. Uniform Density: The simplest case is the uniform density f(p) =
1
4π for all
p ∈S2. How can we generate samples from f? While this task was trivial in
the case of S1, it requires some more eﬀort for S2. We exploit the fact that the
inﬁnitesimal area element on S2 is given by sin(φ2)dφ2dφ1, where (φ1, φ2) are
the spherical coordinates. As earlier, φ1 ∈[0, 2π] changes along the latitude and
φ2 ∈[0, π] changes along the altitude, with φ2 = 0 being the north pole. So
if we generate φ1 uniformly in [0, 2π] and φ2 according to the density sin(φ2)
using the inverse transform method, then we will have a uniform sample on S2.
The following algorithm implements this idea.
Algorithm 26 (Uniform Sampling on S2).
a. Generate a uniform random variable u1 and set φ1 = 2πu1.
b. Generate a uniform random variable u2 and set φ2 = cos−1(u2).
c. Generate a uniform random variable u3. If u3 < 0.5, then set φ2 = π −φ2.
The last step is needed to oﬀset the domain of the cos−1 function in the stan-
dard computer libraries from [−π/2, π/2] to [0, π]. Some examples of samples
generated by Algorithm 26 are shown in Fig. 7.7. The two rows show indepen-
dent samples with increasing number of points from left to right, with the same
number of samples in each column.
2. Fisher’s Density Function: Next we study a density that is an extension
of the von Mises density to S2. Let μ ∈R3 such that ∥μ∥= 1 and κ > 0 be
positive number. Representing points on S2 with their Euclidean coordinates
x ∈R3, deﬁne a probability density function in R3 by
Fig. 7.7 Random samples from a uniform density on S2 with increasing number of samples from
left to right. The two rows show independent samples

250
7 Statistical Modeling on Nonlinear Manifolds
f(x) =
3
1
Z(κ)eκμT x x ∈S2
0
x ̸∈S2
.
(7.18)
Here Z(κ) is the normalizing constant given by: Z(κ) = 
S2 eκμT xdx =
κ0.5
(2π)1.5I0.5(κ), where I0.5 is the modiﬁed Bessel function of ﬁrst kind and or-
der 0.5. By deﬁnition, this density is restricted to S2 ⊂R3. The mode and
the mean of f are identical and are given by the unit vector μ. Its variance is
inversely proportional to κ; κ is also called the concentration parameter . This
f is known as the Fisher density and is the analog of the von Mises density on
S1 (Eq. 7.14).
Now we consider the problem of simulating points from the Fisher density.
Similar to the case of von Mises density on S1, we will use the acceptance-
rejection idea. As a reference density, we choose the uniform density g(x) =
1
4π
on S2 since we already know how to sample from it. We can bound the ratio of
these two densities according to:
f(x; μ, κ)
g(x)
= 4πeκμT x
Z(κ)
≤4πeκ
Z(κ) ≡c .
Hence, the desired constant for comparing with uniform random variable is:
R(x) ≡f(x; μ, κ)
g(x)c
= eκ(μT x−1) .
(7.19)
The desired acceptance-rejection sampling algorithm is given by:
Algorithm 27 (Acceptance-Rejection for Fisher’s Density on S2).
1. Generate x ∼U(S2) the uniform density on S2.
2. Generate U2 ∼U[0, 1].
3. If U2 < R(x), with R(x) as given in Eq. 7.19, then set y = x.
Else, return to Step 1.
This algorithm generates a sample y from the Fisher distribution f(x; μ, κ) on
S2. Shown in Fig. 7.8 are some examples of samples generated from the Fisher
density using Algorithm 7.19. From left to right, we see the same number of
samples with the same mode μ but a decreasing value of the concentration
parameter κ. The values of κ are 50, 10, and 1 in these cases. The two rows
show two independent samples from the same density in each column.
An extension of this Fisher density that allows a non-isotropic distribution
of mass around the mean is called a Fisher-Bingham or Kent density. It deﬁnes
a ﬁve-parameter family given by:
f(x) =
1
c(κ, β) exp(κγ1 · x + β[(γ2 · x)2 −(γ3 · x)2]) ,
where x ∈S2 ⊂R3 is viewed as a 3-vector, c(κ, β) is a normalizing constant, κ
is the concentration parameter, and β is a measure of ellipticity of the levels sets
of f. The vectors γ1, γ2, and γ3 are orthogonal to each other. Here γ1 deﬁnes
the mean direction, while γ2 and γ3 deﬁne the major and minor axes of the
level sets of f.

7.5 Examples on Some Useful Manifolds
251
Fig. 7.8 Random samples from a Fisher density on S2 with increasing variance (decreasing κ)
from left to right
3. Truncated Wrapped-Normal (TWN) Density: Another possibility that
allows a non-isotropic density, i.e., diﬀerent variances in diﬀerent directions,
akin to the multivariate normal density in Rn, is the wrapped normal density
function. The basic idea is to use the tangent space at the mean, Tμ(S2), a
vector space, to deﬁne a bivariate normal density and then transfer it to the
sphere. This transfer is often performed using the exponential map. Since the
exponential map can be seen as wrapping the tangent space onto the sphere, we
will call this density the wrapped-normal density. However, this case is diﬀerent
from that of S1 since the exponential map is a nonlinear Map, and its Jacobian
appears in the ﬁnal expression for the density resulting from wrapping. Further-
more, the magnitude of this Jacobian goes to inﬁnity as we go away from the
mean μ in the tangent space Tμ(S2). This is easy to see because a non-compact
set R2 is being mapped (wrapped) to a compact set S2, and the diﬀerential of
this mapping is going to be unbounded at some point. A simple solution is to
restrict the density in the tangent space using truncation and then map it to
the sphere. This way, a compact set is being mapped to a compact set and the
Jacobian remains stable.
The details are as follows. The exponential map from a tangent space Tμ(S2)
to S2 (using Eq. 3.8) is: expμ(v) = cos(∥v∥)μ + sin(∥v∥) v
∥v∥. In order to make
the exponential map invertible, we need to restrict the density to a subset of
Tμ(S2) using truncation. While choosing a disc of radius π around the origin
will be suﬃcient for this purpose, this will lead to a singularity in the resulting
density at the point antipodal to the mean. To avoid that we will truncate the
density at a smaller distance, say π/2, from the origin in the tangent space.
Thus, expμ becomes invertible in this domain and its inverse is given by: for
p ∈S2, exp−1
μ (p) = (θ/ sin(θ))(p −μ cos(θ)), where θ = cos−1(⟨p, μ⟩) and where
p and μ are viewed as vectors in R3.
Let w1, w2 form an orthonormal basis of Tμ(S2). The set {μ, w1, w2} forms an
orthogonal basis for R3. Using the basis {w1, w2}, we can identify any element
v of Tμ(S2) with its coordinates x = (x1, x2) ∈R2 such that v = x1w1 +
x2w2. Deﬁne a truncated bivariate normal (TBN) density on Tμ(S2) using its
identiﬁcation with R2:

252
7 Statistical Modeling on Nonlinear Manifolds
f(x; K, λ) =
1
ZK,λ
e(−1
2 xT K−1x)1∥x∥≤λ ,
where K ∈R2×2 is a covariance matrix and ZK,λ is simply the normalizing
constant. Next we map this density, for λ = π/2, onto the two sphere S2 using
the exponential map so that the origin of the tangent space coincides with the
mean μ. For a θ ∈R, and the point θw1 in Tμ(S2), the exponential map equation
becomes θw1 →expμ(θw1) ≡cos(θ)μ + sin(θ)w1. Let this point on S2 be called
p. We need to establish an orthogonal basis for the tangent space Tp(S2) and
the vectors b1 = (−sin(θ)μ + cos(θ)w1), and b2 = w2 provide a convenient
orthonormal basis. To derive the diﬀerential of the map expμ, denoted expμ∗,
we take each of the basis elements of Tμ(S2) and map them to Tp(S2) under
expμ∗. In order to induce the truncated Gaussian density on a sphere, using the
exponential map, we need to compute the determinant of the Jacobian of the
exponential map. In this case, the resulting determinant of the Jacobian matrix
turns out to be (sin(θ)/θ).
Now we can write the expression for the induced density. For a point p ∈S2,
the local coordinates of the inverse map in Tμ(S2) are: for i = 1, 2, xi =

wi, exp−1
μ (p)

= (θ/ sin(θ)) ⟨wi, p⟩, where θ = cos−1(⟨p, μ⟩). Then, the induced
truncated normal density on S2 is given by
f(p; μ, K) = 1
Z2
θ
sin(θ) exp{−1
2(pT K−1
w p)}1θ≤π/2 ,
(7.20)
with K−1
w
= WK−1W T and where W = [w1 w2] ∈R3×2. Sometimes it is more
convenient to express the density in a polar coordinate system. Let (φ1, φ2)
denote polar coordinates on S2 such that φ2 is the depression angle from μ
(φ2 = 0 denotes the mean μ, the north pole) and φ1 is the azimuth, i.e., p =
[sin(φ2) sin(φ1), sin(φ2) cos(φ1),cos(φ2)]T , and μ = [0 0 1]T . For convenience,
we choose the basis {w1 = [1 0 0]T , w2 = [0 1 0]T } and the induced density
becomes:
f(φ1, φ2; μ, K)=
φ2
Z2 sin(φ2)e−φ2
2
2 (K−1
11 sin(φ1)2+2K−1
12 cos(φ1) sin(φ1)+K−1
22 cos(φ1)2)1φ2≤π/2 .
(7.21)
In the case of K = σ2I2, we get
f(φ1, φ2; μ, σ2) =
φ2
Z2 sin(φ2)e{−φ2
2/2σ2}1φ2≤π/2 ,
(7.22)
and the normalizing constant takes the form Z2 = 2πσ2(1 −e−(π2/8σ2)). Three
examples of a truncated wrapped-normal density are shown in Fig. 7.9. In each
case we show the original truncated normal density in R2 (top row) and the
corresponding wrapped densities on S2, both as intensity functions (middle row)
and the level curves (bottom row).
Summary Statistics We have discussed three examples of density functions on
S2 and have also discussed techniques for sampling from them. Now we return
to the problem of computing some simple statistics given a sample set on S2.
Let p1, . . . , pn be independent samples from a density f, and we want to derive

7.5 Examples on Some Useful Manifolds
253
Fig. 7.9 Top row shows truncated normal densities in R2 that are wrapped on S2 using the
exponential map. The next two rows display the resulting densities on S2 using an intensity map
(middle) and level curves (bottom)
expressions for intrinsic and extrinsic statistics of this sample. For the extrinsic
analysis, we choose the most natural embedding of S2 in R3. Designate a point on
S2 as the north pole (0, 0, 1), the diametrically opposite point as the south pole
(0, 0, −1), and an arbitrary point on the equator as (1, 0, 0). This ﬁxes the mapping
of all points on S2 into R3; we will call the mapping of pi into R3 as E : S2 →R3,
E(pi) = pi. Then, the extrinsic mean of the given set can be computed as:
μext =
¯p
∥¯p∥,
where ¯p = 1
n
n

i=1
pi .
(7.23)
For the intrinsic analysis, we ﬁrst want to compute the intrinsic mean under
the standard Riemannian metric. We simply particularize Algorithm 24 to S2 to
obtain the following steps:
Algorithm 28 (Karcher Mean on S2). Let μ0 be an initial estimate of the
Karcher mean of f. One can use μext for this purpose. Set j = 0.
1. For each i = 1, . . . , k, compute
vi =
θi
sin(θi)(pi −μ cos(θi)),
where θi = cos−1(⟨pi, μ⟩) .
2. Compute the average direction ¯v = 1
k
k
i=1 vi.

254
7 Statistical Modeling on Nonlinear Manifolds
0
5
10
15
20
25
0
10
20
30
40
50
60
70
0
5
10
15
20
25
5
10
15
20
25
30
35
40
45
0
5
10
15
20
25
14
16
18
20
22
24
26
28
0
5
10
15
20
25
25
30
35
40
45
50
55
60
Fig. 7.10 Examples of sample mean on S2: the top row shows sample points and convergence
of Algorithm 28 for two diﬀerent initializations. Intrinsic mean is where these two points meet.
Also, for comparison, we show the extrinsic mean as the yellow point. The middle row shows the
evolution of Karcher variance for two paths, and the bottom row shows a zoom in to highlight
diﬀerences between the means
3. If ∥¯v∥is small, then stop. Else, update μj in the update direction using
μj+1 = cos(ϵ∥¯v∥)μ + sin(ϵ∥¯v∥) ¯v
∥¯v∥,
where ϵ > 0 is small step size, typically 0.5.
4. Set j = j + 1 and return to Step 1.
Let us look at some results obtained by this algorithm in Fig. 7.10. In each col-
umn we show a diﬀerent sets of points sampled from a Fisher’s density on S2 and
compute their intrinsic and extrinsic means. In each case, we run Algorithm 28
twice with two random initial conditions and show their evolutions using marked
lines on S2. The corresponding evolution of the Karcher variance function, deﬁned
in Eq. 7.6, for these two runs are shown in the middle row. In the bottom row, we
show a magniﬁed view of S2 near the point of convergence. Note that despite dif-
ferent initial conditions, the two runs of the algorithm converge to the same point.
Shown in a thick point is the extrinsic mean of the same dataset. In cases where
the sample points are close to each other, i.e., the variability is small, both the
intrinsic and extrinsic means fall at the same point (ﬁrst two columns). However,
when the data exhibits larger variability, the two means are actually diﬀerent (last
two columns). As emphasized earlier, these extrinsic means were computed using
a speciﬁc embedding E and will change with the embedding. The intrinsic means
are however dependent only on the Riemannian structure of S2 and do not need
any Euclidean embedding.

7.5 Examples on Some Useful Manifolds
255
7.5.3 Space of Probability Density Functions
The next space we consider is the set of all probability density functions on the unit
interval [0, 1], i.e., P = {g : [0, 1] →R≥0|
 1
0 g(x)dx = 1}. The overall geometry of
P was presented earlier in Sect. 4.11.2. Speciﬁcally, we discussed the construction
of geodesic paths and geodesic distances between elements of P under the Fisher-
Rao Riemannian metric. We start here with a brief recap of that discussion.
Riemannian Structure As described in Sect. 4.11.2, an important step in an-
alyzing elements of P is using the half-density representation, q(t) =

g(t), to
represent any g ∈P. Under this representation, the Fisher-Rao metric becomes
the standard L2 metric (Lemma 4.9) and the set P can be identiﬁed with Q, the
positive orthant of the Hilbert sphere S∞:
Q = {q : [0, 1] →R≥0 | ∥q∥= 1} ,
where ∥· ∥denotes the L2 norm as usual. This is illustrated pictorially in
Fig. 7.11. Consequently, the geodesic distance between any two densities is given by
(Eq. 4.35): d(g1, g2) = cos−1  1
0

g1(t)

g2(t)dt

and, since the set Q is geodesi-
cally convex, as a subset of S∞, the geodesic path between them can also be
computed easily using Eq. 4.9. Also, we can consider Q directly for performing
statistical analysis since such an analysis on Q using the L2 metric is equivalent to
(and simpler than) an analysis on P under the Fisher-Rao metric. For any q ∈Q,
the tangent space Tq(Q) is the set of all functions on [0, 1] that are perpendicular
to q under the L2 metric, i.e., ⟨v, q⟩= 0 implies v ∈Tq(Q). For any v ∈Tq(Q),
the exponential map expp(v) = cos(∥v∥)p+sin(∥v∥) v
∥v∥, where ∥·∥denotes the L2
norm. Similarly, for any q1, q2 ∈S∞,and q1 ̸= −q2, the inverse exponential map is
given by exp−1
q1 (q2) = (θ/ sin(θ))(q2 −q1 cos(θ)), where θ = cos−1(⟨q1, q2⟩).
Probability Models Next we take up some probability models that can poten-
tially be used to model observed elements of P. While there is a large literature
on non-informative priors the can be used in this situation, see, e.g., Bayesian
nonparametric methods, our interest lies in more eﬃcient parametric expressions,
similar to the forms that were studied for S2 in the previous section. We will use the
identiﬁcation of P with Q ⊂S∞to attach a spherical structure to this space, and
then extend ideas of the previous section, from S2 to Sk, for some large k, that re-
places S∞as the representation space in practical situations. A ﬁnite-dimensional
approximation of the elements of Q is warranted in practice for several reasons:
P
Q
gid
qid
√
·
g
q = √g
Fig. 7.11 Illustration of the square-root mapping from P to Q

256
7 Statistical Modeling on Nonlinear Manifolds
the observations are mostly ﬁnite-dimensional, the number of observations itself is
ﬁnite, and so on.
1. Uniform Density: It is not possible to deﬁne a uniform density on an inﬁnite-
dimensional space but may be possible on a ﬁnite-dimensional, compact subset
of Q.
2. Squared-Distance Density: The second option explored for S2 was of the
form e−d2, up to a normalization constant, where d is a distance on underlying
manifold. Can we use a similar expression for imposing density on Q? The
answer is no and the reason is the inﬁnite dimensionality of this space. Let us
explain this further.
Consider the space of square-integrable functions on [0, 1], denoted by
L2([0, 1], R). The Euclidean distance between any two points is given by the
L2 norm of their diﬀerence: ∥f1 −f2∥=
 1
0 (f1(t) −f2(t))2dt. For any
f0 ∈L2([0, 1], R), we can try to form the squared-distance density using the
form
1
Z(f0,σ)e−∥f−f0∥2/2σ2, for a positive constant σ. Here Z is the normaliza-
tion constant and is given by the integral:
Z(f0, σ) =

L2([0,1],R)
e−∥f−f0∥2/2σ2df ,
with df being the reference measure on L2([0, 1], R). For a ﬁxed σ, what will be
the value of Z? First, we try to compute this integral along a one-dimensional
subspace of L2([0, 1], R). For a ﬁxed g ∈L2([0, 1], R), with ∥g∥= 1, deﬁne a
subspace: {f0 + gt|t ∈R} and then:
 ∞
−∞
e−∥f0−f0−gt∥2/2σ2dt =
 ∞
−∞
e−t2∥g∥2/2σ2dt =
√
2π σ
∥g∥=
√
2πσ .
So, integration along a one-dimensional subspace of L2([0, 1], R), passing
through f0, results in a term that is linear in σ.
Now consider a two-dimensional subspace of L2([0, 1], R), spanned by f0, g1,
and g2, for some square-integrable functions g1 and g2 with unit norm ∥g1∥=
∥g2∥= 1 and ⟨g1, g2⟩= 0. The integral for evaluating the normalizing constant
gives:
 ∞
−∞
 ∞
−∞
e−∥f0−f0−g1t1−g2t2∥2/2σ2dt1 dt2
=
 ∞
−∞
e−t2
1∥g1∥2/2σ2dt1 .
 ∞
−∞
e−t2
2∥g2∥2/2σ2dt2
= 2πσ2 .
This term is quadratic in σ. As we include more directions in the model, the
normalizing constant is a polynomial in σ of higher order. What will be the
result if we integrate the squared-distance density over the whole space? Since
there are inﬁnite number of independent (orthogonal) directions in L2([0, 1], R),
this integral will be inﬁnite! Thus, it is not possible to deﬁne a probability
model of the type e−d2 on the entire space Q. The main reason for this is that
this model is designed to be isotropic in inﬁnitely many directions. To avoid

7.5 Examples on Some Useful Manifolds
257
this situation, there are two possibilities: one is to use only a ﬁnite-dimensional
submanifold of Q to model the variability and other is to use a more complicated
form that distributes its variance in only a ﬁnite number of dimensions. The
truncated wrapped-normal distribution provides the latter possibility, and, thus,
we will focus on that probability model for representing variability in Q.
3. Truncated Wrapped-Normal Density: The basic idea here is to: (1) select a
ﬁnite-dimensional subspace of a tangent space Tq(Q), for a certain central point
q, (2) deﬁne a truncated multivariate normal density on that vector space, and
(3) use the exponential map to transfer this density onto Q ⊂S∞. We will
demonstrate the idea using a disk region centered at q.
For a point q ∈Q, let {bi} denote a complete, orthonormal basis for the
tangent space Tq(Q). That is, we can write any v ∈Tq(Q) as v = ∞
i=1 xibi,
xi ∈R. For any positive integer k, deﬁne a subspace of the tangent space using:
Vk ≡span{b1, b2, . . . , bk} ⊂Tq(Q) .
Elements of Vk can be mapped onto Q using the exponential map as expq. Let
D ⊂Vk be a compact disk, i.e., D = {k
i=1 xibi ∈Tq(Q)| k
i=1 x2
i ≤π/2}.
Then, we can impose a truncated normal density on D and map to the relevant
subset of Q using the exponential map:
{xi} ∈Rk →expq(
k

i=1
xibi) = cos(|x|)q + sin(|x|)
k
i=1 xibi
|x|
.
(7.24)
Note that the range of D under the exponential map can be identiﬁed with a
subset of the ﬁnite-dimensional sphere Sk.
Now we will derive a truncated multivariate normal density on Sk and will
use it to impose a probability density on the subset expq(D) ∈Q. This is a
simple extension of the derivation presented in the previous section on S2. A
truncated normal density in Rk is given by: fx(x) =
1
Zk e(−1
2 xT K−1x)1|x|≤π/2,
where K is a k × k covariance matrix and Zk is the normalizing constant.
Using Eq. 7.24, we will perform a change of variable from x ∈Rk to p ∈Q
and induce a density on a subset of Q by pushing fx forward. The resulting
expression is given by:
fp(p; μ, K) = 1
Zk

θ
sin(θ)
(k−1)
e(−1
2 xT K−1x)1θ≤π/2 ,
where θ = |x| = cos−1(⟨p, q⟩, xi =

bi, exp−1
q (p)

.
If K=σ2Ik, then the induced density reduces to
1
Zk (θ/sin(θ))(k−1)e−(θ2/2σ2)1θ≤π/2.
Summary Statistics Given a set of probability density functions, we seek to
compute their summary statistics so that we can capture the variability of the
given functions using a smaller number of descriptors. Let g1, g2, . . . , gn ∈P be
the given set of probability density functions and let qi = √gi ∈S∞be the
corresponding half-density functions. We can compute the Karcher mean of {qi}
using Algorithm 28 since all the expressions needed in that algorithm are identical
for Sk, for all k including inﬁnity. Figure 7.12 shows some examples of computing
Karcher means of unimodal densities where the location, height, and breadth of

258
7 Statistical Modeling on Nonlinear Manifolds
0
0.2
0.4
0.6
0.8
1
0
1
2
3
4
5
6
7
8
0
0.2
0.4
0.6
0.8
1
0
1
2
3
4
5
6
0
0.2
0.4
0.6
0.8
1
0
1
2
3
4
5
6
0
0.2
0.4
0.6
0.8
1
0
1
2
3
4
5
6
7
0
0.2
0.4
0.6
0.8
1
0
1
2
3
4
5
6
7
0
0.2
0.4
0.6
0.8
1
0
1
2
3
4
5
6
Fig. 7.12 Examples of computing Karcher mean of the given probability density functions. In
each panel, we show ﬁve density functions using broken lines and their Karcher mean using the
solid line
0
1
2
3
4
5
6
7
8
0
1
2
3
4
5
6
7
8
0
2
4
6
8
10
0
10
20
30
40
50
60
70
80
-20
-10
0
10
20
-15
-10
-5
0
5
10
15
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
(a)
(b)
(c)
(d)
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
0
1
2
3
4
5
6
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
Fig. 7.13 Tangent PCA of pdf data on Q. Top row: (a) the original pdfs and their Karcher
mean, (b) the Karcher mean and extrinsic mean, (c) the singular values of the tangent covariance
matrix, and (d) the scatter plot of the observed principal coeﬃcients. Bottom row: three most
dominant eigen directions mapped back in the pdf space P
the mode is diﬀerent for diﬀerent densities. We note that in cases where the modes
of diﬀerent densities are located close to each other, the Karcher mean also has
a single mode. However, when the peaks of the individual densities are scattered
apart, the Karcher mean can become multimodal.
Once we have an estimate for the Karcher mean, say ˆμ, we can estimate the
covariance matrix in a straightforward manner. Simply take the observed half
densities {qi} and map them onto the tangent space Tˆμ(Q) using the inverse ex-
ponential map; call these points {vi}. Then, form the sample covariance matrix
K of the set {vi} to estimate the underlying covariance. A dimension reduction
technique, such as FPCA, can be used to reduce dimension of the observed data
and to capture the essential modes of variations in the data. Figure 7.13 shows

7.5 Examples on Some Useful Manifolds
259
d
dt
√·
Γ
P
S∞
γ
g
q
g = γ
q =
√g =

˙γ
˙
Fig. 7.14 Illustration of the square-root mapping from Γ to P to Q
an example of this idea where the top-left panel shows the original pdfs and their
Karcher mean. The next panel compares the Karcher mean with the extrinsic mean
( 1
n

i=1 fi). The singular values of the sample covariance matrix are plotted in
the third panel, and the observed principal scores (in the ﬁrst two dominant direc-
tions) are shown in the rightmost panel. The PCA of shooting vectors results the
dominant modes of variability, shown in the bottom panel. From left to right, we
see top three modes of variability—from −0.6σi to +0.6σi—mapped back to P.
7.5.4 Space of Warping Functions
Earlier, in Sect. 4.10.2, we have studied the geometry of ΓI using the natural
Fisher-Rao metric and have demonstrated the computation of geodesic paths be-
tween arbitrary warping functions. These constructions are based on the following
idea. For a γ ∈ΓI, the derivative ˙γ is an element of P deﬁned in the previous
section. Therefore, we have a natural mapping γ →˙γ →√˙γ from Γ to P to Q.
This is illustrated in Fig. 7.14. Once again, the Fisher-Rao metric becomes the L2
metric on Q, which is a positive orthant of S∞and the geodesics are simply arcs
on great circles (Eq. 4.9). Each point on this geodesic in Q can be mapped back
to ΓI to obtain the desired geodesic.
Now we can use this representation to deﬁne and compute some elementary
statistics such as Karcher means and covariance of a set of warping functions. Let
γ1, γ2, . . . , γn ∈ΓI be a set of observed warping functions. Our goal is to develop
a probability model on ΓI that can be estimated from the data directly. There
are two problems in doing this is in a standard way: (1) ΓI is not a vector space
although it has aﬃne structure and (2) it is inﬁnite dimensional. In fact, ΓI has a
nonlinear structure under the metric of interest, and this nonlinearity is handled
using a convenient transformation, which coincidentally is similar to the deﬁnition
of SRVF. The issue of inﬁnite dimensionality is handled using dimension reduction,
e.g., FPCA. We are going to represent an element γ ∈ΓI by its SRVF ψ = √˙γ. The
identity map γid maps to a constant function with value ψid(t) = 1. As described in
Sect. 4.3.3, the square-root representation simpliﬁes the complicated geometry of
ΓI to a unit sphere. The Fisher-Rao distance between any two warping functions
is found to be the arc length between the corresponding SRVFs dF R(γ1, γ2) =
dψ(ψ1, ψ2) ≡cos−1(
 1
0 ψ1(t)ψ2(t)dt). Now we can deﬁne the Karcher mean of a

260
7 Statistical Modeling on Nonlinear Manifolds
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Fig. 7.15 Karcher mean of warping functions under Fisher-Rao distance on ΓI. Each panel
shows ﬁve random warping functions (dotted curves), their Karcher mean (solid curve), and
their cross-sectional mean (spotted curve)
set of warping functions using the Karcher mean on Q. For a given set of warping
functions γ1, γ2, . . . , γn ∈ΓI, their Karcher mean is γ(t) ≡
 t
0 ψ(s)2ds where ψ is
the Karcher mean of √˙γ1, √˙γ2, . . . , √˙γn in Q. (Computation of Karcher mean of
a ﬁnite set of points in Q is discussed in the previous section and is not repeated
here.) We present some examples of computing the Karcher mean of warping
functions in Fig. 7.15. Each panel shows ﬁve random warping functions (dotted
curves), their Karcher mean (solid curve), and their cross-sectional mean (dashed
curve). In most cases the cross-sectional mean is quite similar to the Karcher mean
under the Fisher-Rao distance, although they are not equal.
To apply functional PCA to the warping data, we once again utilize the SRVF
representation. Let {γi} be the given warping functions and ψi = √˙γi ∈Q be the
corresponding SRVFs. Let μψ denote the Karcher mean of {ψi} on Q under the
standard L2 metric. Then, the shooting vector vi = exp−1
μψ(φi) is the mapping of ψi
to the tangent space Tμψ(Q). At this stage, we assume that each ψi is represented
as a vector using a uniform partition of [0, 1] with T elements. Then, the sample
covariance K of {vi} is a T × T covariance matrix and let its SVD be given by
K = UψΣψU T
ψ . Assuming that the singular values are arranged in a non-increasing
order from top left to bottom right, the ﬁrst few columns of Uψ denote the principle
modes of variations. Note that it is easy to map elements from Tμψ(Q) to ΓI using
the following steps:
v ∈Tμψ(Q)
→ψ = cos(∥v∥)μψ + sin(∥v∥)
∥v∥
v ∈Q
→
γ(t) =
 t
0
ψ(s)2ds ∈ΓI .
(7.25)

7.6 Statistical Analysis on a Quotient Space M/G
261
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
a
b
c
d
e
f
0.9
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
2
4
6
8
10
0
0.5
1
1.5
2
-4
-2
0
2
4
-0.4
-0.3
-0.2
-0.1
0
0.1
0.2
0.3
0.4
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Fig. 7.16 (a) A set of warping functions, (b) their Karcher mean, (c) singular values of the sam-
ple covariance matrix, (d) top-two principal coeﬃcients of observed functions, (e) ﬁrst principal
model of variation, and (f) second principal mode of variation
Thus, one can study the principal modes of variation in ΓI by converting columns
of Uψ into the corresponding warping functions. We illustrate this idea using an
example shown in Fig. 7.16. The top-leftmost panel shows a collection of warping
functions that form the set {γi}. In this example, γi(t) = t + ait(1 −t), for ai =
(2(i −1) −n)/n, for i = 1, 2, . . . , n and n = 20. The top-right panel shows a plot
of the singular values, diagonal elements of the matrix Σ. Since the data are taken
from a one-parameter family, only the ﬁrst singular value is signiﬁcant; the rest
are quite small. The bottom middle and right plots show the two main principal
modes of variation in the data by taking xUψ(j), where x goes from −2σj to +2σj
and Uψ(j) is the jth column of Uψ, into the warping space using Eq. 7.25. It can
be seen that the ﬁrst principal mode nicely captures the variability in the original
dataset.
7.6 Statistical Analysis on a Quotient Space M/G
So far the discussion has been focused on Riemannian manifolds, with S1, S2, and
Q as examples, but we realize that the shape spaces of interest are actually not
manifolds by themselves but are quotient spaces of manifolds. Therefore, in prepa-
ration for shape spaces, we should study the techniques for deﬁning probability
models, computing summary statistics, and estimating probabilities on some quo-
tient spaces. Here is a general setup of interest. Let M be a Riemannian manifold
and let G act on M in such a way that: (1) the action of G on M is by isometries
and (2) the orbits of G in M are closed. We are interested in statistical analysis

262
7 Statistical Modeling on Nonlinear Manifolds
on the quotient space M/G as deﬁned in Deﬁnition 2. We can divide the discus-
sion into two possibilities, (1) where the quotient space can be identiﬁed with an
orthogonal section (see Deﬁnition 3.19) and the statistical analysis can be com-
pletely shifted to this section, and (2) such an identiﬁcation is not possible and
one has to deal with the space M/G in general terms.
7.6.1 Quotient Space as Orthogonal Section
In case M/G can be identiﬁed with an orthogonal section S of M, then the desired
analysis can be performed on S, as described in Sect. 3.7. Since S is a submanifold
of M, and a Riemannian manifold by itself, the earlier discussion about statistical
analysis on a general Riemannian manifold applies directly. Therefore, instead of
repeating that framework, we will simply illustrate the ideas using a couple of
simple examples.
Example 7.1. 1. Landmark Shapes Modulo Scaling (But Not Rotation):
Consider the set of k landmarks in Rn, given by M = Rn×k −{0} and the
action of the scaling group G = R× on M. According to Example 3.14, the
orthogonal section of M under the action of G is given by:
S = {X ∈Rn×k|
k

j=1
n

i=1
X2
i,j = 1} .
Since this S is Snk−1, a unit sphere in Rn×k, one can use directly the techniques
for a unit sphere on S, as described in Sect. 7.5.2. We refer the reader to that
section for details.
2. Rescaled Functions: In this case, let M be the vector space of square-
integrable, real-valued functions on the interval [0, 1], i.e., M = L2([0, 1], R).
The scaling group G = R× acts on M with the action (a, f) = af. The orbit of
a function is given by [f] = {af|a ∈R×}. Similar to the previous example, it
can be shown that an orthogonal section of M under the action of G is given by:
S = {f ∈L2([0, 1], R)|
 1
0
|f(t)|2dt = 1} .
S is nothing but a unit Hilbert sphere S∞in L2([0, 1], R) and a statistical
analysis on the quotient space M/G is performed using the geometry of this
sphere, as described in Sect. 7.5.3. To illustrate this point further, take the case
of computing the Karcher mean in the quotient space. Given a set of functions
{fi ∈L2([0, 1], R)| i = 1, 2, . . ., k}, one can compute the Karcher means of their
orbits under G as follows. Project each function in its orthogonal section by
simply scaling it: fi →˜fi =
fi
∥fi∥∈S. Now, since these projected points are
elements of a unit sphere, use Algorithm 28 to compute their Karcher mean in
S. The orbit of the resulting mean represents the sample Karcher mean of this
given orbits in the quotient space M/G.

7.6 Statistical Analysis on a Quotient Space M/G
263
7.6.2 General Case: Without Using Sections
In situations where the quotient space M/G cannot be identiﬁed directly with an
orthogonal section, then the analysis is more involved. The basic steps for a statis-
tical analysis on M/G remain similar, at least conceptually, to the corresponding
steps on M, except one has to deﬁne the central ingredients, such as the geodesics,
geodesic distances, exponential map and its inverse, etc, accordingly.
As the ﬁrst step, we deﬁne these central items for a quotient space and then
discuss procedures for a statistical analysis on that space. Consider a point p ∈M
and let [p] = {(g, p)|g ∈G} be the orbit of p under G. We will assume that [p] is
a manifold in itself, and, therefore, one can talk about Tp([p]), the space tangent
to [p] at p ∈M.
1. Tangent Space: Let Tp(M) and Tp([p]) denote the tangent spaces at p to
the manifold M and the orbit [p], respectively. Clearly, Tp([p]) ⊂Tp(M). Let
Np(M) be the set of vectors that are perpendicular to Tp([p]) in Tp(M). That
is, the elements of Tp([p]) are tangent to the orbit [p] at p and the elements of
Np(M) are perpendicular to the orbit at p. Their direct sum:
Tp([p]) ⊕Np(M) = Tp(M) ,
is the full tangent space at p. We can identify the perpendicular space Np(M)
with the tangent space on the quotient set T[p](M/G). That is, for every element
v ∈T[p](M/G), there is a corresponding element w ∈Np(M), and vice versa.
We will use this identiﬁcation to specify tangent vectors in the quotient space.
Example 7.2. a. Let M be the representation space of k landmarks in R2 or
C, M = Ck, with the standard Euclidean Riemannian metric, and let G =
SO(2) or S1 or U(1) be the rotation group acting on it. We already know
that there is no orthogonal section to represent the quotient space M/G. For
any z ∈Ck, its orbit is given by: [z] = {eiθz|θ ∈S1}, with i = √−1, and the
space tangent to the orbit at z is given by: Tz([z]) = {ixz|x ∈R}. Therefore,
the space perpendicular to the orbit is given by:
Nz(Ck) = {v ∈Ck| ⟨v, iz⟩= 0} .
This also speciﬁes the tangent space in the quotient set T[z](Ck/S1) =
Nz(Ck).
2. Geodesic and Geodesic Distance: According to Deﬁnition 3.17, the inher-
ited distance between elements in the quotient space is given by:
dM/G([p1], [p2]) = dM(p1, (g∗, p2)) ,
where g∗= argming∈G dM(p1, (g, p2)). Since this distance is a geodesic distance,
one would like to know the corresponding geodesic path in M/G, connecting [p1]
and [p2], whose length achieves this distance. This geodesic is realized as follows.
Let ψ : [0, 1] →M be the geodesic between the points p1 and p∗
2 = (g∗, p2) in
the original manifold M. Then, [ψ(τ)], indexed by τ ∈[0, 1], forms the desired
geodesic path between [p1] and [p2] in M/G. Here, [ψ(τ)] denotes the orbit of
the point ψ(τ) in M under the action of G.

264
7 Statistical Modeling on Nonlinear Manifolds
This geodesic, by deﬁnition, is perpendicular to each orbit it meets. In other
words, the velocity vector dψ(τ)
dτ
is in the normal space Nψ(τ)(M). Speciﬁcally,
the shooting vector for the geodesic v = dψ(τ)
dτ |τ=0 is in Np1(M) or equivalently
in τ[p1](M/G).
With this notation, one can deﬁne the exponential map (and its inverse) in
the quotient space M/G as follows.
3. Exponential Map: According to Deﬁnition 3.6, the exponential map is ob-
tained using a unit-speed geodesic in the given direction. Applying this idea
in this situation, we can obtain the exponential map as follows. For a point
[p] ∈M/G, and a tangent vector v ∈τ[p](M/G) (i.e., in Np(M)), we need to
construct a constant-speed geodesic in M/G whose shooting direction is v. As
mentioned in the previous item, a geodesic in M/G is realized using a corre-
sponding geodesic in M that is perpendicular to all the orbits it meets. Let us
construct a constant-speed geodesic ψ(τ) from p in the direction of v (existence
of ψ is shown in Theorem 3.1). Then, since ˙ψ(0) ⊥τp([p]), it can be shown
that ˙ψ(τ) ⊥τψ(τ)([ψ(τ)]) for all τ, due to the isometry condition. Therefore,
[ψ(τ)] is the desired constant-speed geodesic in M/G and the exponential map
exp : τ[p](M/G) →M/G is given by: exp[p](v) = [ψ(1)].
4. Inverse Exponential Map: The inverse of an exponential map takes a point
on the quotient space M/G and maps it to an element (or multiple elements) of
the tangent space T[p](M/G). A vector v ∈T[p](M/G) is said to be the inverse
exponential of [q] ∈M/G at p if exp[p](v) = [q]. It is denoted by v = exp−1
[p] ([q])
and is often not a unique point. That is, the inverse may be set-valued.
Example 7.3. Landmark Shapes: As described in Sect. 2.2.3, the shapes formed
by a set of planar landmarks are conveniently represented as elements of the quo-
tient space C/U(1) = {[z]|z ∈C} and where C = {z ∈Cn| 1
k
k
i=1 zi = 0, ∥z∥= 1}.
Recall that U(1) = SO(2) = S1. The two constraints on the elements of C denote
the removal of the translation and the scaling group. Each orbit represents all
possible rotations of a conﬁguration, [z] = {ejφz|φ ∈S1}
⊂C. One can deﬁne the
Karcher mean shape of several conﬁgurations z1, z2, . . . , zn as the conﬁguration
that minimizes the sum of squares of distances:
ˆμ = argmin
z∈C
 k

i=1
ds([z], [zi])2

= argmin
z∈C
 n

i=1

min
φi (cos−1(

z, ziejφi
)

= argmin
z∈C
 n

i=1
cos−1(| ⟨z, zi⟩|)

.
Algorithm 29 (Karcher Mean of Landmark Shapes). Initialize μ0 ∈C and
set t = 0.
1. For each i = 1, . . . , k, compute θi and ri where ⟨μt, zi⟩= riejθi. Set: vi =
θi
sin(θi)(zi −μ cos(θi)).
2. Compute the average direction ¯v = 1
k
k
i=1 vi.
3. If ∥¯v∥is small, then stop. Else, update μt in the update direction using
μt+1 = cos(ϵ∥¯v∥)μt + sin(ϵ∥¯v∥) ¯v
∥¯v∥,
where ϵ > 0 is small step size, typically 0.5.

7.7 Exercises
265
Sample Shapes
Karcher Mean
Fig. 7.17 Karcher mean shapes for given sets under Kendall’s shape analysis
4. Set t = t + 1 and return to Step 1.
Figure 7.17 shows a number of examples of Karcher means of several sets of shapes,
obtained using this algorithm. It is observed that several shape features present
in the original curves have been blurred to some extent in the mean shapes. Take,
for example, the ﬁngers in the hand shape shown in the second row; ﬁngers in
the mean shape are not as explicit as the ﬁngers in the original sample. This is
due to a lack of alignment of features along diﬀerent curves and the smoothing of
features when averaged together with straighter parts of other curves. The issue of
nonalignment of similar geometric features was explained previously in Sect. 2.2.4
with a simple example. Here we see some consequences of this nonalignment.
7.7 Exercises
7.7.1 Theoretical Exercises
1. Let xi ∼N(μ, σ2), for i = 1, 2, . . ., k, be independent samples. Find the maxi-
mum likelihood estimate of μ given x1, x2, . . . , xk using Eq. 7.4.
2. Show that if ˆf is a Kernel density estimator (Eqn. 7.5), then
 ˆf(x)dx = 1.
3. Let M = S2 with the standard Euclidean metric and viewed as a subset of
R3 with the standard embedding. Let q = (0, 0, 1) denote the north pole and
p = (0, 1, 0) represent a point on the equator. Evaluate the gradient of d(p, q)2,
the geodesic distance squared, with respect to p on M, expressed as an element
of Tp(S2).
4. Verify that the projection of a point v in R2 to the circle S1 is, according to
Eq. 7.17, tan−1( v2
v1 ), as long as ∥v∦= 0.
5. Show that the determinant of the Jacobian of the exponential map from Tμ(S2)
to S2 is given by sin(θ)
θ
.

266
7 Statistical Modeling on Nonlinear Manifolds
6. Let S∞= {q : [0, 1] →R|
 1
0 |q(t)|2dt = 1}. For any q ∈S∞, let {bi}
form an orthonormal basis of Tq(S∞). Show that the set expq(D), where
D = {k+1
i=1 αibi| k
i=1 α2
i ≤π/2}, can be identiﬁed with a subset of Sk.
7. Check that if γ ∈ΓI, i.e., γ is a diﬀeomorphism from [0, 1] to itself with bound-
ary points preserved, then ˙γ is probability density function on [0, 1]. Further-
more, show that √˙γ is an element of the unit Hilbert sphere S∞.
7.7.2 Computational Exercises
1. Implement Algorithm 26 to generate samples from the uniform density on S2.
2. Implement Algorithm 27 to generate samples from a Fisher’s density on S2.
3. Implement an algorithm for generating samples from a truncated normal density
on S2 given in Eq. 7.21.
4. Write a program to estimate a probability density function f on S2 given its
samples {xi ∈S2|i = 1, 2, . . ., n} using the kernel method. Use a von-Mises
kernel and study the results for diﬀerent values of the kernel bandwidth h.
5. Write a program to compute the Karcher mean of a set of points {xi ∈S2|i =
1, 2, . . ., n} using Algorithm 28. Then, using the inverse exponential map of
the observed points into the tangent space at the Karcher mean, compute the
sample Karcher covariance matrix.
6. Assume that we are given a set of probability density functions on [0, 1]:
{g1, g2, . . . , gk|gk ∈P}. Each function is available numerically in form of T
uniform samples on the interval [0, 1].
a. Write a program to compute the Karcher mean of points in this set using
the discussion in Sect. 7.5.3. Note that this computation is easier to perform
after square-root mapping to the space S∞.
b. Use the inverse exponential map at the Karcher mean and compute the
sample Karcher covariance matrix of this data.
c. Using SVD of this covariance matrix, compute the ﬁrst k0 principal directions
of variations. Display these principal directions using exponential maps in
both positive and negative directions.
7. Implement an algorithm to generate a set of random warping functions as fol-
lows.
a. For a large N, form smooth functions using a Fourier basis according to:
v(t) =
N

n=1
(ai cos(2πnt) + bi sin(2πnt)) ,
where ai, bi ∼N(0, σ2/n).
b. Project v into Tψid(S∞) using v →v −
 1
0 v(t)dt.
c. Map the resulting ψ onto the sphere using v →ψ = expψid(v).
d. Integrate according to γ(t) =
 t
0 ψ(s)2ds to obtain a random warping func-
tion.
8. Implement an algorithm compute the Karcher mean and sample Karcher vari-
ance of a set of random warping functions.

7.8 Bibliographic Notes
267
7.8 Bibliographic Notes
The notion of intrinsic Fr`echet or Karcher mean has been covered by many au-
thors, including [46, 88, 17]. Speciﬁcally, [88] outlined the concepts of intrinsic and
extrinsic means on nonlinear Riemannian manifolds. Grenander et al. [35] used the
Hilbert-Schmidt norm to develop extrinsic estimators on some matrix Lie groups.
Similar ideas are used for some quotient spaces of matrix Lie groups in [103]. The
special case of a circle was discussed in [48].
The classic density estimation for Euclidean domains is well covered in [101],
while that for nonlinear domains is studied in [89]. Ghosh [31] provides a nice
treatment of Bayesian nonparametric techniques.
Statistical methods for unit vectors in three-dimensional space have been stud-
ied extensively in the ﬁeld of directional statistics [75]. In the landmark-based
shape analysis of objects [28, 44, 56], where 2-D objects are represented by con-
ﬁgurations of salient points or landmarks, the set of all such conﬁgurations, after
removing translation and scale is a real sphere S2n−3 (for conﬁgurations with n
landmarks).

Chapter 8
Statistical Modeling of Functional Data
In this chapter we will look at the problem of developing statistical models that
capture the essential modes of variability in a given functional dataset. To keep
the discussion simple, we will assume that all the functions are deﬁned on a
ﬁxed domain, say [0, 1]. A piece of the puzzle, dealing with pairwise alignment
of functions, was introduced earlier in Chap. 4. Now we face a bigger task—to
develop generative models for function variables where model parameters can be
estimated from past (training) data. We shall break this task down into several
smaller pieces, seek convenient mathematical representations of functions, choose
task-appropriate metrics, and develop statistical models that ﬁt naturally to these
choices. One important piece in this process is to align the given set of functions
using nonlinear time warping. This step, also known as phase-amplitude separa-
tion, helps decompose the more complex variability of general functional data
into two separate sets of relatively simpler variabilities—phase variability and
amplitude variability. Another important challenge is to deal with the inﬁnite-
dimensional nature of functional data. The two components in functional data,
phase and amplitude, are still, despite separation and simpliﬁcation, elements of
inﬁnite-dimensional spaces. One needs a dimension-reduction tool to approximate
and represent them as elements of ﬁnite-dimensional spaces. Finally, once they
are projected onto a ﬁnite-dimensional space, one needs probability models, either
parametric or nonparametric, to model the projected values.
The ﬁrst task—alignment of multiple functions or group-wise registration of
functional data—is an extension of the problem studied in Sect. 4.4. In that sec-
tion, we developed a principled, metric-based framework for pairwise registration
of functions; this metric was as an extension of the nonparametric Fisher-Rao
Riemannian metric, originally deﬁned for functions with positive derivatives, to
a broader class of real-valued functions. While the original form of the metric is
complicated, a remarkable computational eﬃciency results from the use of a novel
representation, termed square-root slope function (SRSF), so that the objective
function becomes the L2 distance between SRSFs of the functions being regis-
tered. This objective function has a special property that it is invariant to iden-
tical warping of functions and, consequently, the registration solution is inverse
symmetric. Now, we consider the group-wise registration problem where we try to
register peaks and valleys of several functions at the same time. Furthermore, we
develop techniques for discovering principal modes of variations in functional data
© Springer-Verlag New York 2016
A. Srivastava, E.P. Klassen, Functional and Shape Data Analysis,
Springer Series in Statistics, DOI 10.1007/978-1-4939-4020-2 8
269

270
8 Statistical Modeling of Functional Data
and to use these modes in imposing tractable statistical models on function spaces.
These models can be used in drawing statistical inference, pattern recognition, and
general data analysis using functional data.
8.1 Goals and Challenges
We start with a listing of goals and challenges. Some goals listed here are sim-
ple extensions of those studied in Chap. 4, but there are some additional, more
ambitious goals.
1. Alignment of Multiple Functions or Phase-Amplitude Separation: We
want a formal framework and an eﬃcient algorithm for aligning peaks and
valleys of several real-valued functions, deﬁned on a common domain, using
nonlinear time warpings. More precisely, given a set of functions {fi ∈F|i =
1, 2, . . ., n}, where F is the set of absolutely continuous functions on [0, 1], our
goal is to ﬁnd a set of warping functions {γi ∈ΓI} such that, for any t ∈
[0, 1], the values {fi(γi(t))} are said to be registered or aligned. The challenge,
of course, is in deﬁning a mathematical framework, equipped with a proper
objective function, that formalizes this problem. This problem is also called
the phase-amplitude separation, where {γi} are called the phases and {fi ◦
γi} are treated as representatives of the equivalence classes representing their
amplitudes.
2. Joint Modeling of Phase-Amplitude Components: In situations where
one is interested in developing statistical models for capturing variability in
given functional data, the phase-amplitude separation provides a novel per-
spective to the problem. Instead of modeling the original functions, one can
model the two components individually, but not necessarily independently. As
mentioned above, there are two general challenges in modeling the phase and
amplitude components. The ﬁrst is inﬁnite dimensionality and is handled us-
ing a truncated basis and, thus, a ﬁnite number of coeﬃcients to represent a
function. These basis elements can come from a predetermined family or from
an empirical analysis such as FPCA. The second challenge, more diﬃcult to
handle, comes from the nonlinearity of the phase components. As mentioned
in Chap. 4 Sect. 4.10.2, the geometry of ΓI is nonlinear and the direct use of
FPCA is not possible. We will use a special geometric structure, obtained by
transforming the warping functions, to help overcome that challenge.
3. Joint FPCA and Registration of Functional Data: Another task, critical
in exploring and modeling functional data, is to account for the phase variability
of given functions while, at the same time, discovering the principal modes
in the amplitude data. This problem is slightly diﬀerent from the last one,
where the phase-amplitude separation was performed ﬁrst and then followed
by FPCA of each component. Now, we want to solve for the principal modes
while simultaneously registering the data. The challenge is to formulate a joint
optimization problem that can perform both these tasks in one solution.
In the following sections, we develop solutions to achieve these three goals.

8.2 Template-Based Alignment and L2 Metric
271
8.2 Template-Based Alignment and L2 Metric
We start with the problem of nonlinear alignment of multiple functions. A natural
extension of the pairwise solution to the group-wise alignment is a template-based
registration. Here one designs a template function in a certain way and aligns each
of the given functions to this template using the pairwise solution. Furthermore,
these two steps can be performed iteratively to improve the quality of the tem-
plate and to improve overall alignment. Thus, each iteration of multiple alignment
involves:
1. Averaging to form template: Take the current versions of the functions
{fi|i = 1, 2, . . . , n} and average them under a proper metric to form the tem-
plate μ.
2. Pairwise alignment to the template: For each i = 1, 2, . . . , n, align the
given function fi to the template μ in a pairwise fashion. Call the optimal
warping function γi, and update fi ←fi ◦γi.
The ﬁrst item requires a distance on the function space F in order to deﬁne and
compute the mean function μ. The second item requires an objective function for
pairwise registration, a topic that was discussed in detail in Sect. 4.4. Furthermore,
these two items cannot be solved in isolation. It is important for the metric, used
for registration in the second step, to be compatible with the one used for deﬁning
mean in the ﬁrst step. We will use the extended Fisher-Rao distance, discussed in
Sect. 4.10, for both these items.
Before we introduce our approach, we present and evaluate a seemingly natural
idea involving the L2 metric and demonstrate its limitations. The two main short-
comings of L2 norm for pairwise registration, namely, the pinching eﬀect and the
lack of inverse symmetry, are already described in Sect. 4.4. We want to empha-
size that the pinching eﬀect is still present in the multiple alignment problem, even
though the need of symmetry is not there due to the presence of multiple functions.
One can use a penalty term to avoid the pinching eﬀect, but the results are not sat-
isfactory, as shown below. So, let’s use the L2 metric in the iterative framework sug-
gested above and study the result. The iteration consists of the following two steps:
Algorithm 30 (Averaging Under Penalized L2 Norm).
1. Form a template: Take the current versions of the aligned functions {fi|i =
1, 2, . . . , n} and average them according to μ(t) = 1
n
n
i=1 fi(t).
2. Align to the template: For each i = 1, 2, . . ., n, solve the problem:
γi = arg inf
γ

∥μ −fi ◦γ∥2 + λR(γ)

,
and update fi ←fi ◦γi. Here, R represents a chosen roughness penalty on the
warping function γ and λ > 0 is a scalar that controls the inﬂuence of R on
the solution.
3. Test for a stopping criterion. Return to Step 1, if convergence is not achieved.
This process, of course, requires making a sensible choice for the parameter λ. In
case λ = 0, these two steps can be merged together in the form of the optimization
problem:
min
μ∈L2
 n

i=1
inf
γi ∥μ −fi ◦γi∥2

.
(8.1)

272
8 Statistical Modeling of Functional Data
In Algorithm 30, the ﬁrst step solves the outer optimization (over μ) while ﬁxing
the warping functions {γi}, and the second step solves the inner optimization (over
γi) while ﬁxing the mean μ. To evaluate the results of these choices, we take a case
study. For comparisons, we also present results for the case λ > 0 and where the
roughness penalty is based on the ﬁrst derivative of γ, i.e., R(γ) =
 1
0 (˙γ(t))2dt.
Example 8.1. Figure 8.1 shows a set of bimodal functions {fi} that diﬀer slightly
in locations and heights of their peaks (see the top row). These functions were
generated as follows; the individual functions are given by: yi(t) = zi,1e−(t−1.5)2/2+
zi,2e−(t+1.5)2/2, i = 1, 2, . . . , 21, where zi,1 and zi,2 are i.i.d normal with mean one
−3
−2
−1
0
1
2
3
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
{fi}
-4
-2
0
2
4
-4
-2
0
2
4
-4
-2
0
2
4
-4
-2
0
2
4
-4
-2
0
2
4
0.2
0.4
0.6
0.8
1
1.2
1.4
0.2
0.4
0.6
0.8
1
1.2
1.4
-2
0
2
-2
0
2
-2
0
2
-2
0
2
-3
-2
-1
0
1
2
3
f1 (thin), m (thick)
f1 ◦g1 (l=0), m
g1 (l=0)
0.2
0.4
0.6
0.8
1
1.2
1.4
0.2
0.4
0.6
0.8
1
1.2
1.4
0.2
0.4
0.6
0.8
1
1.2
1.4
f1 ◦g1 (small l), m
f1 ◦g1 (medium l), m
f1 ◦g1 (large l), m
-3
-2
-1
0
1
2
3
-3
-2
-1
0
1
2
3
-3
-2
-1
0
1
2
3
g1 (small l)
g1 (medium l)
γ1 (large λ)
Fig. 8.1 An example of template-based alignment under the L2 norm

8.3 Elastic Phase-Amplitude Separation
273
and standard deviation 0.25. Each of these functions is then warped according
to: γi(t) = 6( eai(t+3)/6−1
eai −1
) −3 if ai ̸= 0; otherwise γi = γid, where ai are equally
spaced between −1 and 1, and the functions of interest are computed using fi(t) =
yi(γi(t)). A set of 21 such functions forms the original data and is shown in the
top panel of this ﬁgure. We apply Algorithm 30 on these functions and initialize
the template with their cross-sectional mean μ, i.e., sample mean under the L2
metric. The mean function (drawn with a thick line) is also a similar bimodal
function, as can be seen in this panel. The next two rows display results from
pairwise alignment of one of the functions—say f1—to this mean μ for diﬀerent
values of λ. The second row shows the alignment of f1 to the mean μ when λ = 0,
where pinching and ﬂattening of parts of f1 are clearly visible. This can also be
seen in the warping function plotted in the rightmost panel. As we increase λ,
the pinching eﬀect is reduced but so is the overall warping, thus reducing the
alignment level itself. This is shown in the third row. The bottom row shows the
corresponding warping functions getting smoother but also getting closer to γid,
as λ increases. These results highlight the main problem in using the penalized
L2-distance for function alignment: the choice of λ. One can avoid the pinching
eﬀect using an external penalty term, but how much weight this penalty should
carry is not obvious. There is no simple way to select a λ automatically, and the
results can be very sensitive to this choice.
8.3 Elastic Phase-Amplitude Separation
As an alternative to the L2 metric, let us try the amplitude distance da, as given
in Eq. 4.19, for multiple alignment. We shall use it to establish and compute the
Karcher mean of functions, which , in turn, will serve as template for multiple
alignment. Since this distance is actually deﬁned on the quotient space A ≡F/ ˜ΓI,
the Karcher mean will be an amplitude or an orbit rather than an individual
function. Therefore, we will need to select a special element of this mean orbit,
termed the center of the orbit (see Sect. 3.10), as the template μ. More speciﬁcally,
we will deﬁne a template μ ∈L2 whose amplitude and relative phase denote the
respective sample means of amplitudes and relative phases of the given functions.
8.3.1 Karcher Mean of Amplitudes
Continue representing each absolutely continuous function fi by its SRSF qi(t) =
sign( ˙fi(t))

| ˙fi(t)|. The amplitude of f in the SRSF space is given by the orbit:
[q] = closure{(q, γ) = (q ◦γ)

˙γ|γ ∈ΓI} .
This allows for deﬁning an energy function for pairwise registration, between func-
tions f1 and f2 (Deﬁnition 4.7)—infγ∈ΓI ∥q1 −(q2, γ)∥. If a γ∗∈ΓI is (approxi-
mately) a minimizer of this energy term, then f1(t) is said to be optimally aligned
to f2(γ∗(t)). Furthermore, as described in Sect. 4.10, this quantity also approxi-
mates a proper distance between amplitudes (or orbits) [q1] and [q2] considered as

274
8 Statistical Modeling of Functional Data
elements of the quotient space A = F/ ˜ΓI. With this distance, we have enough
tools to deﬁne the Karcher mean of the amplitudes of a set of functions in F.
Deﬁnition 8.1. Deﬁne the Karcher mean [μq] of the amplitudes {[qi]} of given
functions f1, f2, . . . , fn, with SRSFs q1, q2, . . . , qn, as a local minimum of the sum
of squares of distances:
[μq] = arginf
[q]∈A
n

i=1
da([q], [qi])2 ,
(8.2)
where the deﬁnition of da is as given in Eq. 4.19.
We reiterate that [μq] is an orbit and not just a single element of L2. To form a
template, we will select a speciﬁc element of this orbit, as described later. The full
algorithm for computing the Karcher mean of amplitudes is as follows.
Algorithm 31 (Karcher Mean in A ).
1. Initialization Step: Select μq = qj, where j is any index in argmin1≤i≤n ||qi −
1
n
n
k=1 qk||.
2. For each qi ﬁnd γi by solving: γi = arginfγ∈ΓI ∥μq−(qi◦γ)√˙γ∥. This is the pair-
wise alignment studied in Sect. 4.6 where we used DPA to solve the optimization
problem.
3. Compute the aligned SRSFs using ˜qi →(qi ◦γi)√˙γi.
4. If the increment ∥1
n
n
i=1 ˜qi −μq∥is small, then stop. Else, update the mean
using μq →1
n
n
i=1 ˜qi and return to Step 2.
The iterative update in Steps 2–4 is based on the gradient of the cost function given
in Eq. 8.2. (Due to the local nature of the iterative optimization, the convergence
of this algorithm to a global solution is not guaranteed.) The mean amplitude is
then given by the orbit [μq]. Any element of this orbit, say μq, can be converted
into the corresponding element of F by integration: μf(t) =
 t
0 μq(s)|μq(s)|ds.
An example of this computation is shown in Fig. 8.2 where we start with fi(t) =
sin(2πtai), ai ∼N (1, 0.09), i = 1, 2, . . . , 10, as shown in the top-left panel. Then,
we use Algorithm 31 to compute the mean amplitude of these given functions. The
algorithm generates optimal warping functions {γi} (second panel), the aligned
functions {fi ◦γi} (third panel), and the mean amplitude function μf (rightmost
panel). What happens if we warp all the fis by the same γ ∈ΓI? How will it aﬀect
the amplitude mean? We see that in the second row where we further warp each fi
by γ(t) = t1.2 and run the algorithm again. As expected the warping functions do
not change but the aligned functions and μf reﬂect this extra warping. This is a
special example where all the functions involved have the same amplitude and the
result belongs to the same amplitude class. However, since Algorithm 31 generates
an element of this class, we see diﬀerences in results between top and bottom rows.
8.3.2 Template: Center of the Mean Orbit
Now that we have deﬁned the mean amplitude [μf] of given functions, the next
task is to ﬁnd a particular element of this mean orbit. This function, an element

8.3 Elastic Phase-Amplitude Separation
275
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
{ fi}
{gi}
{gi}
{ fi ◦gi}
{ fi ◦gi}
mf
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
˜
{ fi ≡fi ◦t1.2 }
˜
˜mf
Fig. 8.2 Computation of mean amplitude. In each row we plot the given functions fis, the
optimal warping functions {γi}, the aligned functions {fi ◦γi}, and the mean function μf . The
input functions in the bottom row are obtained by applying an additional warping t1.2 to the
input functions in the top row
of F, can then be used as a template to align the given functions. Toward this
purpose, we will deﬁne the notion of a center of an orbit with respect to some
given functions.
Deﬁnition 8.2. (Center of an Orbit) For a given set of SRSFs q1, q2, . . . , qn
and q, deﬁne an element ˜q of [q] as the center of [q] with respect to the set {qi}
if the relative phases {γi}, where γi = arginfγ∈Γ ∥˜q −(qi, γ)∥, have their sample
Karcher mean as identity, γid(t) = t.
(See Sect. 3.10 for a general deﬁnition.) In other words, the relative phases of all
qi’s with respect to the template function ˜q should average γid under the chosen
metric. Assume that qi’s are such that the action of ΓI on each qi is free.
To make sense of this deﬁnition, we will of course need a metric on the set of
warping functions. A simple idea is to obtain the desired mean under the L2 norm,
leading to the cross-sectional mean 1
n
n
i=1 γi(t). It is interesting to note that this
mean is a valid warping function, i.e., it is an element of the set ΓI, despite the
fact that ΓI is not a vector space. While this deﬁnition may be suﬃcient for the
current purposes, we also bring up another possibility. Recall that the concept
of sample Karcher mean of warping functions, under the Fisher-Rao metric, has
already been presented in Sect. 7.5.4. We refer the reader to that section for the
deﬁnition and an algorithm for computing this quantity. Either one of these two
deﬁnitions, the L2-mean or the Fisher-Rao mean, will work, as long as we are
consistent in our choice. Let ¯γ denote the chosen mean of {γ1, γ2, . . . , γn}. Now,
we can use this mean to ﬁnd the center of an orbit.
The algorithm for computing the center of the orbit [q] is rather simple. It is
depicted pictorially in Fig. 8.3. Take any element, say q, of the orbit [q], compute
the warping functions {γi} that align {qi} to this q, compute their sample mean
¯γ, and apply the inverse of ¯γ to q.

276
8 Statistical Modeling of Functional Data
γ1
q1
q2
q3
qn
γ2
γ3
γn
¯γn
γ∗
1
γ∗
2
γ∗
3
γ∗
n
γid
(q, ¯γn−1)
q
mean
center of orbit [q]
mean
[q ]
Fig. 8.3 Finding center of the orbit [q] with respect to the set {qi}
Algorithm 32 (Finding Center of an Orbit). Let q be any element of the
orbit [μq].
1. For each qi ﬁnd γi by solving: γi = arginfγ∈ΓI

∥μq −(qi ◦γ)√˙γ∥

.
2. Compute the mean ¯γ of all {γi} as described in Sect. 7.5.4. The center of [μq]
wrt {qi} is given by ˜q = (μq, ¯γ−1).
We can check that ˜q ∈[μq] resulting from Algorithm 32 satisﬁes the mean
condition in Deﬁnition 8.2. We know that γi is chosen to minimize ∥μq −(qi, γ)∥,
and also that
∥˜q −(qi, γ)∥= ∥(μq, ¯γ−1) −(qi, γ)∥= ∥μq −(qi, γ ◦¯γ)∥.
Therefore, γ∗
i = γi ◦¯γ−1 minimizes ∥˜q −(qi, γ)∥. That is, γ∗
i is a warping that
aligns qi to ˜q. To verify the Karcher mean of γ∗
i (under the Fisher-Rao metric),
we compute the sum of squared distances n
i=1 dF R(γ, γ∗
i )2 = n
i=1 dF R(γ, γi ◦
¯γ−1)2 = n
i=1 dF R(γ ◦¯γ, γi)2. As ¯γ is already the mean of γi, this sum of squares
is minimized when γ = γid. That is, the mean of γ∗
i is γid.
Some examples of this idea are shown in Fig. 8.4. In the top-left panel, we
plot a set of warping functions {γi}, and their Karcher mean ¯γ, the middle panel
shows the mean ¯γ and its inverse ¯γ−1, and the right panel shows the centered
relative phases {γi ◦¯γ−1}. The bottom row shows another example along the same
lines. This time the relative phases are further away from identity and the eﬀect of
centering is, therefore, more pronounced. Once again, the idea of ﬁnding the center
of an orbit is essentially to warp all the given functions so that their relative phases,
with respect to the chosen element of the mean amplitude, are also centered.
8.3.3 Phase-Amplitude Separation Algorithm
Now we can combine these pieces together to form an algorithm for separation of
phase and amplitude components in a given set of functional data.

8.3 Elastic Phase-Amplitude Separation
277
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Fig. 8.4 In each row: (1) the left panel shows a number of warping functions {γi} and their
Karcher mean ¯γ under the elastic metric, (2) the middle panel shows the mean ¯γ and its inverse
¯γ−1, and (3) the right panel shows the centered set {γi ◦¯γ−1}
Algorithm 33 (Phase-Amplitude Separation).
Given a set of functions
f1, f2, . . . fn on [0, 1], let q1, q2, . . . , qn denote their SRSFs, respectively.
1. Compute the Karcher mean of amplitudes [q1], [q2], . . . , [qn] in A = F/ ¯ΓI
using Algorithm 31. Denote it by [μq].
2. Find the center of the orbit [μq] with respect to the set {qi} using Algorithm 32;
call it ˜q. (Note that this algorithm, in turn, requires a tool for computing the
Karcher mean of warping functions).
3. For i = 1, 2, . . . , n, ﬁnd γi by solving: γi = arginfγ∈ΓI ∥˜q −(qi, γ)∥. This mini-
mization is approximated using the DPA, as described in Appendix B.
4. Compute the aligned SRSFs ˜qi = (qi, γi) and aligned functions ˜fi = fi ◦γi.
5. Return the template ˜q, the warping functions {γ∗
i }, and the aligned functions
{ ˜fi}.
To illustrate this method, we use a number of simulated and real datasets.
Although this SRSF-based framework is developed for functions on [0, 1], it can
easily be adapted to an arbitrary interval using an aﬃne transformation.
MSE-Based Quantiﬁcation of Alignment To quantify the extent of separa-
tion of original functions into amplitude and phase components, we can use the
mean squared error (MSE) of these components. The MSE of any set of functions
{gi} is given by n
i=1 ∥gi −¯g∥2, where ¯g is the cross-sectional mean of these func-
tions and ∥·∥denotes the L2 norm as usual. For our quantiﬁcation, we can choose
to compute the MSE in the function space F or the SRSF space L2. Since these
numbers are simply for illustration purposes and not used in any optimization,

278
8 Statistical Modeling of Functional Data
-3
-2
-1
0
1
2
3
-3
-2
-1
0
1
2
3
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
1.1
1.2
1.3
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
1.1
1.2
1.3
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Original { fi}
Amplitude {fi}
˜
Phase {gi }∗
Original { fi}
Amplitude { fi}
˜
Phase {gi }∗
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
1
2
3
4
5
6
7
8
9
0
2
4
6
8
10
0
0.2
0.4
0.6
0.8
1
Fig. 8.5 Phase-amplitude separation of two simulated datasets, top, and bottom
we will go with MSE computation in F, despite the known problems associated
with using L2 norm in the function space. For quantifying the amplitude compo-
nents, we will use the MSE of { ˜fi}, the aligned functions as elements of F, and
for quantifying the phase components, we will use the MSE of { ¯˜f ◦γi}, where ¯˜f is
the (cross-sectional) mean of the aligned functions { ˜fi} and {γi} are the warping
functions.
Example 8.2. 1. As the ﬁrst example, we study a set of simulated functions pre-
sented previously in Fig. 8.1. The top-left panel of Fig. 8.5 shows these functions
again, and the remaining two panels in that row show the results of Algo-
rithm 33. The middle panel presents the resulting aligned functions { ˜fi}, and
the rightmost panel plots the corresponding warping functions {γ∗
i }. The plot
of { ˜fi} shows a tighter alignment of functions with sharper peaks and valleys.
The two peaks are at ±1.5, which is exactly what we expect. This means that
the eﬀects of warping generated by the γis have been completely removed and
only the amplitude variability remains. To evaluate the extent of separation, we
note that the MSE of the amplitude components is 0.41, the MSE of the phase
components is 1.26, and the MSE of the original data is 1.66.
2. As the second example, we take the another simulated dataset. This time we
study a family of Gaussian kernel functions with the same shape but with sig-
niﬁcant phase variability, in the form of horizontal shifts, and minor amplitude
(height) variability. Figure 8.5 (bottom row) shows the original 29 functions
{fi} (left), the aligned functions { ˜fi} (middle), and the warping functions {γ∗
i }
(right). Once again we notice a tighter alignment of functions with only minor
variability left in { ˜fi} reﬂecting the diﬀering heights in the original data. In
this experiment, the MSE of the original data is 125.59, MSE of the amplitude
components is 0.12, and the MSE of the phase components is 139.33.

8.3 Elastic Phase-Amplitude Separation
279
-5
0
5
10
15
20
25
30
35
0
5
10
15
20
-5
0
5
10
15
20
25
30
35
0
0.2
0.4
0.6
0.8
1
0
5
10
15
20
0
5
10
15
20
0
5
10
15
20
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
-5
0
5
10
15
20
25
30
-5
0
5
10
15
20
25
30
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Original {fi}
Amplitude { fi}
˜
Phase {gi  }
∗
Original {fi}
Amplitude { fi}
Phase {gi }
∗
Fig. 8.6 Results of Algorithm 33 on biological growth data. Top: female data. Bottom: male
data
3. Berkeley Growth Data: Next we consider the Berkeley growth dataset for
54 female and 39 male subjects. For better illustrations, we have used the ﬁrst
derivatives of the growth curves as the functions {fi} in our analysis. (In this
case, since SRSF is based on the ﬁrst derivative of f, we actually end up using
the second derivatives of the growth functions.)
The results from Algorithm 33 on the female growth curves are shown in
Fig. 8.6 (top row). The left panel shows the original data, which shows that
while the growth spurts for diﬀerent individuals occurs at slightly diﬀerent
times, there are some underlying patterns to be discovered. In the second panel,
we see the aligned functions { ˜fi} and they exhibit a much tighter alignment of
the functions and, in turn, an enhancement of peaks and valleys in the aligned
mean. In fact, this mean function suggests the presence of several growth spurts,
one between 3 and 4 years, one between 5 and 7 years, and another between 10
and 12 years, on average. The MSE of the amplitude component is 2.09 × 103,
the MSE of the phase component is 2.15 × 103, and the MSE of the original
data is 2.22 × 103.
A similar analysis is performed on the male growth curves, and the bottom
row shows the results: the original data (consisting of 39 derivatives of the
original growth functions) and the aligned functions ˜fi(t). The cross-sectional
mean functions also show a much tighter alignment of the functions and, in
turn, an enhancement of peaks and valleys in the aligned mean. This alignment
suggests the presence of several growth spurts, between 3 and 5, 6 and 7, and 13
and 14 years, on average. The MSE of the amplitude component is 0.99 × 103,
the MSE of the phase component is 1.44 × 103, and the MSE of the original
data is 1.70 × 103.

280
8 Statistical Modeling of Functional Data
0
10
20
30
40
50
60
70
0
10
20
30
40
50
60
70
×106
×106
-0.5
0
0.5
1
1.5
2
2.5
3
3.5
Original Chromatograms
-0.5
0
0.5
1
1.5
2
2.5
3
3.5
Aligned Chromatograms
Before
After
Zoom-in 1
5
10
15
20
5
10
15
20
×106
×106
×106
×106
0
0.5
1
1.5
2
2.5
0
0.5
1
1.5
2
2.5
Zoom-in 2
30
35
40
45
50
30
35
40
45
50
0
0.5
1
1.5
2
2.5
3
0
0.5
1
1.5
2
2.5
3
Fig. 8.7 Alignment results for a set of liquid chromatography-mass spectrometry chromatograms

8.4 Alternate Interpretation as Estimation of Model Parameters
281
0
1
2
3
4
5
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0
1
2
3
4
5
−0.1
0
0.1
0.2
0.3
0.4
0.5
0.6
0
0.5
1
0
0.2
0.4
0.6
0.8
1
Original Data { f i}
Amplitude { f i}
˜
Phase {gi  }
∗
Fig. 8.8 Alignment of spike train data. The left panel shows the original functions, the mid-
dle panel shows the aligned functions of amplitudes, and the right panel displays their phase
components
4. Mass-Spectrometry Data: Next, we study a set of eight chromatograms
shown in the top panel on Fig. 8.7. In this data, some of the major peaks are
not aligned. The outcome of Algorithm 33 on this data is shown in the second
row where the peaks appear to be sharply aligned throughout the spectrum. To
emphasize the quality of alignment, we look at a couple of smaller intervals in
the spectrum more carefully. The zoom ins of these smaller regions are shown
in the last two row of the ﬁgure. In each of the last two rows, we show the
before and after alignment spectra for these two domains: 0–20 and 28–50. It
can be seen in these zoom ins that the algorithm aligns both the major and
minor peaks remarkably well. Note that this procedure does not require any
prior peak detection or matching to reach this alignment.
5. Neuronal Spike Train Data: Statistical analysis of spike trains is one of the
central problems in neural coding and can be pursued in several ways. While
one option is to assume parametric or semi-parametric models, such as the
Poisson model, for spike trains and use them in decoding spike trains, the other
is a nonparametric option based on metrics for comparing the numbers and
the placements of spikes in diﬀerent trains. Here we demonstrate the use of
phase-amplitude separation as a tool for aligning spike trains and estimating
underlying neural signals. In order to enable this processing, one pre-processes
discrete spike trains with Gaussian smoothing kernels. An example set of such
smoothed functions is shown in the left panel of Fig. 8.8. The result of phase-
amplitude separation algorithm on this dataset is shown in the next two panels.
8.4 Alternate Interpretation as Estimation of Model
Parameters
The algorithm we have speciﬁed for separating phase and amplitude components
of functional data can also be interpreted as estimation of parameters of a model.
This model ﬁrst transforms the given functions into their SRSFs and then assumes
the SRSFs to be observations of a mean signal contaminated by white Gaussian
observation noise. More precisely,
qi = SRSF(fi) ,
(qi, γi) = μq + ϵi .
(8.3)

282
8 Statistical Modeling of Functional Data
Under the same assumptions as in FPCA analysis of points L2 (see Eq. 4.7), we
get the optimization function for estimating model parameters:
(ˆμq, {ˆγi}) = argmin
μq,{γi}
 n

i=1
∥(qi, γi) −μq∥2

.
Algorithm 31 is essentially a coordinate-descent procedure for minimizing this ob-
jective function. Additionally, if we impose the condition that the sample Karcher
mean of {γi} is identity, this results in Algorithm 33. The discussion on conver-
gence properties of these estimators have been left out for the future.
8.5 Phase-Amplitude Separation After Transformation
We see that Algorithm 33 is quite successful in aligning functional data, but it
can sometimes be more useful indirectly. For instance, instead of applying the
algorithm to the given data, one can transform the given functions into a more
convenient form and then apply the separation algorithm. We illustrate this idea
using several examples.
1. Absolute Value Transform: This transform is useful in situations where the
signs of peaks (and valleys) do not matter in the analysis. In other words, the
positive peaks can be matched with negative peaks and zero crossings with zero
crossings. Consider the set of functions shown in the top-left panel of Fig. 8.9.
The top row shows the original data, its alignment using Algorithm 33 and the
corresponding phase components {γi}. One can see that the warped functions
are well aligned, peaks with peaks and valleys with valleys. However, if we
perform FPCA of this aligned data, we will ﬁnd that there are more than one
signiﬁcant singular values associated with the covariance operator of this data.
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
{fi}
{fi ◦gi}
{fi ◦gi}
{gi}
{ fi }
{ fi ◦gi }
{gi}
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
|
|
|
|
˜
˜
˜
Fig. 8.9 Top row: phase separation of the original functional data using Algorithm 33. Bottom
row: results of Algorithm 33 after taking absolute values of the functions

8.5 Phase-Amplitude Separation After Transformation
283
0
0.1
0.2
0.3
0.4
0.5
0.6
0
0.1
0.2
0.3
0.4
0.5
0.6
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.1
0.2
0.3
0.4
0.5
0.6
˙
˙
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
{ fi}
{ fi ◦gi}
{ fi ◦gi}
{gi}
{ fi}
{fi ◦gi}
{gi}
˜
˜
˜
Fig. 8.10 Top row: phase separation of the original functional data using Algorithm 33. Bottom
row: results of Algorithm 33 after taking derivatives of the functions
In other words, these functions do not lie in a one-dimensional subspace of F.
This is not a problem in itself but a diﬀerent result can be achieved if we ﬁrst
transform the given functions using the absolute value. This time when we
apply Algorithm 33 to the absolute transformed data, we obtain results shown
in the second row of this ﬁgure. Furthermore, if we take the resulting warping
functions and apply them to the original functions, we get warped functions
that are shown in the rightmost panel of this row. Imagine that the goal of this
exercise was to form an eﬃcient PCA basis of the given data, rather than just
matching peaks to peaks and valleys to valleys. One can see that the aligned
functions obtained using the absolute transformation provide a more succinct
data for extracting an FPCA basis. In fact, a single principal component is now
suﬃcient to describe the given functions!
2. Derivative Transform: The need for this transform arises when dealing with
monotonically increasing or decreasing functions. Examples include survival
functions in biostatistics and height functions in human growth analysis, The
top row in Fig. 8.10 shows an example of such data and its alignment using Al-
gorithm 33 directly on that data. One can see that although these functions are
aligned, some of the features (bumps) have been smoothed out in this alignment.
The algorithm seems to have straightened the functions into lines and thus the
alignment is good. (Actually, one can show that for monotonic functional data,
the aligned functions are scalar multiples of each other.) If, instead of aligning
original functions, we take ﬁrst derivatives { ˙fi} and then extract phase compo-
nents {˜γi} using Algorithm 33, the results look much better. When we visualize
{fi ◦˜γi}, we ﬁnd that this approach is better in preserving data features while
performing alignment at the same time.
The other potential transformations of data include log, higher-order deriva-
tives, square root, and so on, depending on the needs of the problem.

284
8 Statistical Modeling of Functional Data
8.6 Penalized Function Alignment
In some situations, it may be desirable to control the amount of warping, or the
roughness level of the phase components. This is sometimes necessitated by the
need to improve immunity against noise in the data or sometimes simply due to
some contextual knowledge. One way to do this, of course, is to smooth the phase
components after they have been separated. Another approach is to add a rough-
ness penalty during the separation process itself. This penalization is analogous
to Eq. 4.16 in Chap. 4 where one adds a roughness penalty on γ, controlled by a
weight parameter λ, during optimization on γI.
Deﬁne a new optimization problem according to
inf
γ∈ΓI Eλ(q1, (q2, γ)),
where Eλ(q1, (q2, γ)) =

∥q1 −(q2, γ)∥2 + λR(γ)
1/2 ,
(8.4)
where R(γ) is a quantity that measures the roughness of a given warping γ and
λ > 0 is a constant. As earlier, q1 and q2 are the SRSFs of the given functions to be
aligned. The choices for R include functionals of ﬁrst- or higher-order derivatives
of γ and distances between γ and γid in the set ΓI:
1. A term that includes the ﬁrst derivative of γ is: R(γ) =  1
0 (˙γ(t))2dt. This
penalty favors constant γ functions. Since γ is constrained to have

˙γ(t)dt = 1,
this is minimized when γ = γid.
2. Similarly, the second-order roughness penalty involves the second derivative of
γ according to: R(γ) =
 1
0 (¨γ(t))2dt. This penalty favors smooth functions and
is minimized when γ is γid.
3. There are several choices of distances, in diﬀerent associated spaces, that one
can used as penalty functions:
a. The squared L2 norm between γ and γid: R(γ) = ∥γ −γid∥2.
b. The squared L2 norm between their derivatives: R(γ) = ∥˙γ −1∥2, where 1
is the constant function with value one. This is same as the ﬁrst item above,
up to a constant.
c. The squared L2 norm between their SRSFs: R(γ) = ∥√˙γ −1∥2, where 1 is
the constant function with value one.
d. The arc length between their SRSFs on the unit sphere: R(γ)
=
cos−1(
√˙γ, 1

). As described in Sect. 4.10.2, this is exactly the Fisher-
Rao distance between γ and γid.
In the following, we will look at some experimental results based on the ﬁrst-order
penalty R(γ) = ∥1 −√˙γ∥2. The resulting Eλ serves as a criterion for pairwise
alignment of functions:
Eλ(q1, (q2, γ)) =

∥q1 −(q2, γ)∥2 + λ∥1 −

˙γ∥21/2
,
The parameter λ can be used to control the level of smoothness of relative phase
components. One can solve this pairwise registration problem using DPA. For
multiple alignment, one ﬁrst computes a template (e.g., as the cross-section mean
of given functions {fi, i = 1, 2, . . ., n}) and then aligns the individual functions
to this template. Since this procedure is deﬁned for L2 directly, rather than its

8.6 Penalized Function Alignment
285
quotient space, there is no need to perform the centering step described earlier
(although it might still be interesting to study its eﬀects).
Algorithm 34 (Penalized Alignment of Functions).
1. Initialization Step: Select μq = qj, where j is any index in argmin1≤i≤n
E0(qi, 1
n
n
k=1 qk).
2. For each qi ﬁnd γi by solving: γi = arginfγ∈ΓI Eλ(μq, (qi, γ)). This is the pair-
wise alignment solved using a modiﬁcation of the DPA presented in Algorithm 3.
3. Compute the aligned SRSFs using ˜qi →(qi ◦γi)√˙γi.
4. If the increment ∥1
n
n
i=1 ˜qi −μq∥is small, then stop. Else, update the mean
using μq →1
n
n
i=1 ˜qi and return to Step 2.
Shown in Fig. 8.11 is an illustration of this idea on a simulated dataset. In the
top-left panel, we show the given functions, with the resulting alignment results
in the next four panels under λ = 0, 75, 150, and 300, respectively. In the second
row, we show the corresponding (column-wise) time-warping functions. The results
show that the warping functions stay closer to γid as λ is increased, at the loss of
alignment level relative to the case for λ = 0. To quantify the extent of separation
of original functions into amplitude and phase components, we compute the mean
squared error (MSE) of these components. As stated earlier, the MSE of any set
of functions {gi} is given by n
i=1 ∥gi −¯g∥2. Thus, for quantifying the amplitude
original functions
λ = 0
λ = 75
λ = 300
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
-1
-0.5
0
0.5
1
Original data
-1
-0.5
0
0.5
1
-1
-0.5
0
0.5
1
-1
-0.5
0
0.5
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
100
200
300
400
500
600
700
0
100
200
300
400
500
600
700
0
0.2
0.4
0.6
0.8
1
1.2
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
MSE (amplitude)
MSE (phase)
Fig. 8.11 Illustration of phase-amplitude separation with penalty term. First row: the original
data (left) and the aligned functions with the four λ values. Second row: the corresponding
time-warping functions. Bottom row: MSE of the amplitude data (aligned functions) and MSE
of phase data ({γi} applied to mean of aligned functions) versus λ

286
8 Statistical Modeling of Functional Data
components, we compute MSE of { ˜fi}, the aligned functions as elements of F,
and for phase components, we compute MSE of { ¯˜f ◦γi}, where ¯˜f is the mean of
the aligned functions and {γi} are the warping functions. The last row shows the
evolution of these two MSEs—for amplitude and for phase—as we increase λ.
We point out that one loses the formal notion of amplitude and phase in this
setup, especially the earlier identities involving da and dp. One can still refer to
the resulting warping functions as relative phases, but they will not satisfy the
properties listed in Sect. 4.7 for the non-penalized (λ = 0) case.
8.7 Function Components, Alignment, and Modeling
An overarching goal in this chapter is to develop faithful statistical models for
function variables. The essential steps in this process are to discover a suitable
ﬁnite representation of functional data, e.g., using FPCA, and to impose models
on that representation. The use of FPCA to extract principal modes of variation
in functional data was described earlier in Sect. 4.3.1. To remind the reader of
the setup, the basic idea is to ﬁnd orthonormal functions φ1, φ2, . . . , φJ, for some
predetermined J, for a given dataset {f1, f2, . . . , fn}, with fi ∈L2, that minimize
the squared residual error. The computation of FPCA basis solves the following
optimization problem:
{ˆφ1, . . . , ˆφJ} =
arginf
{φj ∈L2([0, 1], R)|j = 1, 2, . . ., J},
{ci,j ∈R|i = 1, 2, . . ., n; j = 1, 2, . . . , J}
⎛
⎝
n

i=1
∥fi −
J

j=1
ci,jφj∥2
⎞
⎠,
(8.5)
and the actual minimization is based on SVD of the sample covariance operator
estimated from the given data (see Sect. 4.3.1).
Now consider a situation where the functions fis are observed under unknown
time warpings or arbitrary phase variability. That is, instead of observing fis
directly, we observe fi ◦γi instead, with γi ∈ΓI, the group of boundary-preserving
diﬀeomorphisms of [0, 1]. What happens if we perform FPCA of this dataset?
Figure 8.12 shows an illustration of this idea using three simple examples. In the
following, we use N (μ, σ2) to denote the normal pdf with mean μ and variance σ2.
1. Case 1—Row 1: We construct the data using fi(t) = ci sin(2πt), with ci ∼
N (0, 5). The fis are shown in the top-left panel of this ﬁgure. Note that these
fis happen to lie a one-dimensional space—span(sin(2πt)). Also, note that while
the zero crossings of the derivatives coincide, the peaks and valleys are not
well registered across functions here. The second panel in this row shows the
top three principle basis functions (eigenfunctions) obtained using FPCA, and
the third panel shows the reconstruction of f1 using ˆf = ⟨f1, φ1⟩φ1, drawn
over f1. The top principle basis function happens to be φ1(t) = −sin(2πt) in
this example, and the reconstruction ˆf1 is very similar to f1. The last panel
plots the total residual error (minimum value of the object function in Eq. 8.5)
versus J, and it conﬁrms the one-dimensional nature of the data.

8.7 Function Components, Alignment, and Modeling
287
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
-2
-1.5
-1
-0.5
0
0.5
1
1.5
2
-0.25
-0.2
-0.15
-0.1
-0.05
0
0.05
0.1
0.15
0.2
0.25
-1.5
-1
-0.5
0
0.5
1
1.5
0
2
4
6
8
10
0
2
4
6
8
10
0
2
4
6
8
10
0
50
100
150
200
250
300
350
-1.5
-1
-0.5
0
0.5
1
1.5
-0.2
-0.15
-0.1
-0.05
0
0.05
0.1
0.15
0.2
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
0
50
100
150
200
250
300
350
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
-0.2
-0.15
-0.1
-0.05
0
0.05
0.1
0.15
0.2
-0.15
-0.1
-0.05
0
0.05
0.1
0.15
0
20
40
60
80
100
120
140
160
Data
f1, Reconstruction of f1
Residual Error
Top 3 Principal Basis
Fig. 8.12 FPCA of functional data constructed from a single basis function but with diﬀerent
levels of phase variability. In each row, the left panel shows original data, the second panel shows
the three principle basis functions, the third panel shows f1 and its reconstruction using the ﬁrst
principle direction, and the last panel shows the evolution of of the residual error plotted against
the number of basis elements
2. Case 2—Row 2: Now we construct the data using fi(t) = ci sin(2πγi(t)), with
ci ∼N (0, 5), where γi(t) = tai and ai = 0.75 + bi, with bi ∼exp(0.25). As a
result, for the {fi}, shown in the left panel of the second row, the zero crossings
of derivatives are no longer aligned. They also show some variability in the loca-
tions of peaks and valleys. When we apply FPCA to this data, we obtain results
shown in the remaining panels of the second row. As we can see in the third
panel, a single basis function is no longer suﬃcient to reconstruct f1 perfectly.
The presence of phase variability has created the need for an additional basis
element. Thus, one needs two basis functions to reach a perfect reconstruction,
as shown by the residual error plot in the last panel.
3. Case 3—Row 3: Next, we ﬁrst increase amount of warpings by using γi(t) = tai
and ai = 0.4 + bi, with bi ∼exp(0.6). The resulting fis, shown in the bottom-
left panel, display as increased phase variability. Consequently, it now takes four
principal basis elements to bring the residual error closer to zero.
Another example, albeit more complex, of the FPCA is shown in Fig. 8.13. In
this data, the function fis are more complex than the previous example, and the
extent of phase variability is not obvious just by observing the data. One can see
that the peaks are not aligned but the matching of peaks, i.e., which peak in a
function matches with which peak in another function, is also not as obvious as in
the last example. In this data, it takes eight basis elements to bring the residual
error down.
As this discussion illustrates, an increase in phase variability of functional data
implies increases variance in the functional data, requiring a larger number of

288
8 Statistical Modeling of Functional Data
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
-2.5
-2
-1.5
-1
-0.5
0
0.5
1
1.5
2
0
2
4
6
8
10
0
50
100
150
200
250
300
350
400
-0.2
-0.15
-0.1
-0.05
0
0.05
0.1
0.15
0.2
Data
-1.5
-1
-0.5
0
0.5
1
1.5
-1.5
-1
-0.5
0
0.5
1
1.5
-1.5
-1
-0.5
0
0.5
1
1.5
Reconstructing f1 using 2 basis
4 basis
10 basis
Residual Error
Top 3 Basis
Fig. 8.13 FPCA of functional data with potential phase variability
principle components to model and represent the given functions. What is the
alternative to a direct FPCA on functions in situations where they contain some
phase variability? Here are some possibilities:
1. Sequential Approach: Separate the functional data into phase and ampli-
tude components, perform individual FPCA on these components, and impose
statistical models on the resulting coeﬃcients.
2. Joint Approach Using L2 Metric: Perform phase-amplitude separation and
FPCA jointly using an objective function involving L2 metric on the function
space, and then impose statistical models on the coeﬃcients.
3. Joint Approach Using Elastic Metric: Perform phase-amplitude separation
and FPCA jointly using an objective function based on the elastic metric, and
then impose statistical models on the coeﬃcients.
We discuss these three ideas in the next few sections.
8.8 Sequential Approach
In this approach we use Algorithm 33 to ﬁrst separate the phase and amplitude
components and then develop generative models for the two components. Focusing
on the latter task, one way to accomplish this task is to derive an FPCA basis of
these functions individually and then impose a joint statistical model on principle
coeﬃcients. FPCA is simply viewed as a dimension-reduction step here, used in
order to bring the representation to a ﬁnite dimension and to enable standard
multivariate modeling.

8.8 Sequential Approach
289
8.8.1 FPCA of Amplitude Functions: A-FPCA
Once the given functions have been aligned using Algorithm 33, we can use the
aligned SRSFs as representatives of their respective amplitudes and perform cross-
sectional computations. Recall that cross-sectional analysis implies the use of the
L2 metric in the SRSF space, which, in turn, denotes the use of the elastic metric
in the original function space. One can compute their principal directions and
use coeﬃcients as low-dimensional representations. Since we are focused on the
amplitude variability in this section, we will call this analysis A-FPCA.
Let f1, · · · , fn be a given set of functions, and q1, · · · , qn be the corresponding
SRSFs. Also, let μq be the center of their mean orbit, and let ˜qis be the corre-
sponding aligned SRSFs obtained using Algorithm 33. In performing A-FPCA,
one should not forget about the variability associated with the initial values, i.e.,
{fi(0)}, of the given functions. Since representing functions by their SRSFs loses
this initial value, this information is represented separately. That is, a functional
variable f is analyzed using the pair (q, f(0)) rather than just q. In this way the
mapping from the function space F to L2 × R becomes a bijection. We will use
the product metric on the space L2 × R to analyze data in that space.
The underlying framework for computing principal directions of functional data
has been laid out in Sect. 4.3.1. Thus, we proceed directly to computations using
discrete time samples of the SRSFs. An aligned SRSF ˜q is represented using a ﬁnite
partition of [0, 1], say with cardinality T , so that qj = ˜q(j/T ), j = 1, 2, . . ., T . The
combined vector hi = [qi fi(0)] has dimension (T + 1). We can deﬁne a sample
covariance operator for the aligned combined vector as:
Kh =
1
n −1
n

i=1

(hi −μh)(hi −μh)T 
∈R(T +1)×(T +1) ,
(8.6)
where μh = [μq
¯f(0)]. Here ¯f(0) is the sample mean of the function values at
0. Taking the SVD, Kh = UhΣhV T
h we can calculate the directions of principle
variability in the given SRSFs using the ﬁrst p ≤(T + 1) columns of Uh and can
be converted back to F, via integration, for displaying the principal components
in the original function space. Moreover, we can calculate the observed principal
coeﬃcients as ⟨hi, Uh,j⟩.
Figure 8.14 shows the results of A-FPCA on the simulated dataset from the
top row of Fig. 8.5. It shows the aligned functions {˜qi}, the three principal di-
rections, and the singular values. The ﬁrst three singular values for the data are
0.0481, 0.0307, and 0.0055, the remaining being negligibly small. We also show the
corresponding pairwise scatter plots of the observed principle coeﬃcients in the
bottom row of this ﬁgure. In some cases these scatter plots can be used to evaluate
statistical models associated with functional data. For instance, the scatter plots
reveal a strong pairwise dependence between the three coeﬃcients.
A similar experiment involving the female growth velocity functions is shown
in Fig. 8.15. Here the top row shows the original data in the SRSF space {qi},
the aligned data {˜qi}, the top three singular vectors (or principal directions of
amplitude variations), and the singular values. The scatter plots of the observed
coeﬃcients are shown in the bottom row. These plots show only a weak dependence
between the coeﬃcients, and some support for Gaussian models for the underlying
random variables.

290
8 Statistical Modeling of Functional Data
-3
-2
-1
0
1
2
3
-3
-2
-1
0
1
2
3
-3
-2
-1
0
1
2
3
-1.5
-1
-0.5
0
0.5
1
1.5
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
-0.2
-0.15
-0.1
-0.05
0
0.05
0.1
0.15
0.2
0.25
1
2
3
4
5
6
7
8
0
1
2
3
4
5
6
7
8
9
10
{qi}
˜qi}
U1 ,U2 ,U3
Singular Values
5
0
5
-
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
5
0
5
-
-1
-0.5
0
0.5
0
1
2
3
4
5
-1
-0.5
0
0.5
One Vs Two
One Vs Three
Two Vs Three
{
Fig. 8.14 FPCA of the amplitude components of a simulated data
0
5
10
15
20
0
5
10
15
20
0
5
10
15
20
-5
0
5
10
15
20
25
30
35
-8
-6
-4
-2
0
2
4
6
8
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0
5
10
15
0
1
2
3
4
5
6
7
-8
-6
-4
-2
0
2
-8
-6
-4
-2
0
2
0
2
4
6
8
10
12
14
-4
-3
-2
-1
0
1
2
3
4
5
0
2
4
6
8
10
12
14
-4
-3
-2
-1
0
1
2
3
4
5
{qi}
˜qi}
U1 ,U2 ,U3
Singular Values
One Vs Two
One Vs Three
Two Vs Three
{
Fig. 8.15 FPCA of the amplitude components of the female growth velocity data
Formally, one can test if the observed principal coeﬃcients follow a normal
distribution. There are several tests possible, one of them is the Kolmogorov-
Smirnov test. Since the principal coeﬃcients are uncorrelated, they can be tested
independently for Gaussian behavior. In the case of female growth velocity data,
we ﬁnd that the null hypothesis of having a Gaussian distribution is rejected for
the top-two principal coeﬃcients at 5 % conﬁdence level, but this hypothesis is not
rejected for the third principal coeﬃcient.
8.8.2 FPCA of Phase Functions: P-FPCA
Here we want to model the phase components of the functional data, available to us
in the form of the warping functions {γ∗
i } outputted by Algorithm 33. An explicit
statistical modeling of the warping functions can be of interest to an analyst since

8.8 Sequential Approach
291
they represent an important part of the original data. This is the exact problem
studied in Sect. 7.5.4 where we used the SRSF representation of warping functions
to reach a Hilbert sphere structure. Here we illustrate those ideas using some
experimental results.
Let γ1, γ2, . . . , γn ∈ΓI be a set of observed warping functions. Our goal is to
develop a probability model on ΓI, albeit implicitly, that can be estimated from
the data directly. There are two problems in doing this in a standard way: (1) ΓI
is not a vector space and (2) it is inﬁnite dimensional. The issue of nonlinearity
is handled using a convenient transformation, which is similar to the deﬁnition
of SRSF, and the issue of inﬁnite dimensionality is handled using dimension re-
duction, e.g., FPCA, which we will call phase-FPCA or P-FPCA. We are going
to represent an element γ ∈ΓI by the square root of its derivative ψ = √˙γ.
Note that this is the same as the SRSF deﬁned earlier for fis and takes this form
since ˙γ > 0. Refer to Fig. 7.14 for an illustration of this mapping. The identity
map γid maps to a constant function with value ψid(t) = 1. Since γ(0) = 0,
the mapping from γ to ψ is a bijection and one can reconstruct γ from ψ using
γ(t) =
 t
0 ψ(s)2ds. An important advantage of this transformation is that since
∥ψ∥2 =
 1
0 ψ(t)2dt =
 1
0 ˙γ(t)dt = γ(1) −γ(0) = 1, the set of all such ψs forms
the positive orthant of Hilbert sphere S∞, the unit sphere in the Hilbert space
L2. In other words, the SRSF representation simpliﬁes the complicated geometry
of ΓI to a unit sphere. The distance between any two warping functions is ex-
actly the arc length between their corresponding SRSFs on the unit sphere S∞:
dψ(ψ1, ψ2) ≡cos−1  1
0 ψ1(t)ψ2(t)dt

. The deﬁnition of a distance on S∞helps
deﬁne a Karcher mean of sample points on S∞, as described in Sect. 7.5.4.
Deﬁnition 8.3. For a given set of points ψ1, ψ2, . . . , ψn ∈S∞, their Karcher mean
in S∞is deﬁned to be a local minimum of the cost function ψ →n
i=1 dψ(ψ, ψi)2.
Now we can deﬁne the Karcher mean of a set of warping functions using the
Karcher mean in S∞. For a given set of warping functions γ1, γ2, . . . , γn ∈ΓI,
their Karcher mean in ΓI is ¯γ(t) ≡
 t
0 μψ(s)2ds where μψ is the Karcher mean of
√˙γ1, √˙γ2, . . . , √˙γn in S∞. The search for this minimum is performed as described
in Sect. 7.5.4.
Since S∞is a nonlinear space (a sphere), one cannot perform principal com-
ponent analysis on it directly. Instead, we choose a vector-space tangent to the
space, at a certain ﬁxed point, for analysis. This idea is also called Tangent
PCA or simply TPCA. The tangent space at any point ψ ∈S∞is given by:
Tψ(S∞) = {v ∈L2|
 1
0 v(t)ψ(t)dt = 0}. In the following, we will use the tangent
space at μψ to perform analysis. Note that the outcomes of the mean computation
include the Karcher mean μψ and the tangent vectors {vi} ∈Tμψ(S∞). These
tangent vectors, also called the shooting vectors, are the mappings of ψis into the
tangent space Tμψ(S∞). In this tangent space we can deﬁne a sample covariance
function: (t1, t2) →
1
n−1
n
i=1 vi(t1)vi(t2). In practice, this covariance is computed
using a ﬁnite number of points, say T , on these functions and one obtains a T × T
sample covariance matrix instead, denoted by Kψ. The SVD of Kψ = UψΣψV T
ψ
provides the estimated principal components of {ψi}: the principal directions Uψ,j
and the observed principal coeﬃcients ci,j = ⟨vi, Uψ,j⟩.
As an example, we compute the Karcher mean of a set of random warping
functions. These warping functions and their sample Karcher mean are shown in
the left panel of Fig. 8.16. Using the {vi}’s that result from the mean computation,

292
8 Statistical Modeling of Functional Data
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Fig. 8.16 Left panel: observed warping functions and their Karcher mean. Next three panels:
principal directions of the observed data
we form their covariance matrix Kψ and take its SVD. The ﬁrst three columns of
Uψ are used to visualize the principal geodesic paths, in the last three panels of
this ﬁgure.
8.8.3 Joint Modeling of Principle Coeﬃcients
To develop statistical models for capturing the phase and amplitude variability,
there are several possibilities. Once we have obtained the FPCA coeﬃcients for
these components, we can impose probability models on the coeﬃcients (while
keeping the basis functions ﬁxed) and induce a distribution on the function space
F. In this section, we explore two such possibilities: a joint Gaussian model and
a nonparametric model, and demonstrate them with experiments.
Let ca ∈Rk1 and cp ∈Rk2 be the dominant principal coeﬃcients of the ampli-
tude and phase components, respectively, as described in the previous two sections.
Recall that ca,j = ⟨h, Uh,j⟩and cp,j = ⟨v, Uψ,j⟩. We can reconstruct the ampli-
tude component using q = μq + k1
j=1 ca,jUh,j and f(t) = f(0) +  t
0 q(s)|q(s)|ds.
Here, f(0) is a random initial value. Similarly, we can reconstruct the phase
component (a warping function) using v = k2
j=1 cp,jUψ,j and then using ψ =
cos(∥v∥)μψ + sin(∥v∥) v
∥v∥, and γ(t) =
 t
0 ψ(s)2ds. Combining the two random
quantities, we obtain a random function f ◦γ.
1. Gaussian Models on FPCA Coeﬃcients In this setup the model spec-
iﬁcation reduces to the choice of models for f(0), cp and ca. We are going
to model them as multivariate normal random variables. The mean of f(0) is
¯f(0), while the means of ca and cp are zero vectors. Their joint covariance ma-
trix is of the type:
⎡
⎣
σ2
0 L1 L2
LT
1 Σh S
LT
2
S Σψ
⎤
⎦∈R(k1+k2+1)×(k1+k2+1). Here, L1 ∈R1×k1
is the sample covariance between f(0) and ca, L2 ∈R1×k2 between f(0) and
cp, and S ∈Rk1×k2 between ca and cp. As discussed in the previous sections,
Σh ∈Rk1×k1 and Σψ ∈Rk2×k2 are diagonal matrices and are estimated directly
from the data.
2. Nonparametric Models on FPCA Coeﬃcients An alternative to the para-
metric model suggested above is the nonparametric approach, where the density
function for each coeﬃcient is allowed to take an arbitrary form. One can esti-

8.8 Sequential Approach
293
mate the densities of all the variables of interest: f(0), each of the k1 components
of ca, and the k2 components of cp, using kernel density estimation:
1
nb
n

i=1
K
x −xi
b

,
where K (·) is the kernel function (a valid kernel function is a symmetric func-
tion that integrates to 1) and b > 0 is the smoothing parameter or bandwidth.
A range of kernel functions can be used, but the most common choice is the
Gaussian kernel. Note that separate estimation of densities for each coeﬃcient
implies independence of the coeﬃcients, a condition that will be diﬃcult to
satisfy in practice. Still, from a modeling perspective, one can try this model
on real data and evaluate its eﬀectiveness.
We illustrate the use of these models via random sampling. That is, we ﬁrst com-
pute sample statistics (means and the covariances) from the given data, estimate
the model parameters, and then generate random samples from these estimated
models. We demonstrate results on the same two simulated datasets used in the
previous examples. For the ﬁrst simulated dataset, shown in Fig. 8.5 (top-left),
we randomly generate 35 samples from the amplitude model, 35 domain-warping
functions from the phase model, and then apply group action to generate random
functions. These random samples are shown in Fig. 8.17, where the ﬁrst panel is a
set of random warping functions {γi}, the second panel is a set of corresponding
amplitude functions { ˜fi}, and the third panel shows their compositions { ˜fi ◦γi}.
Comparing them with the original datasets (Fig. 8.5 top-left), we conclude that
the random samples are very similar to the original data and the proposed model
is successful in capturing the variability in the given data. Furthermore, if we
compare these sampling results to the FPCA-based Gaussian model directly on f
(without separating the phase and amplitude components) in the last panel, we
notice that this model is more consistent with the original data. A good portion
of the samples from the non-separated model just contain three peaks or have
a higher variation than the original data and poorly represent the original data.
Beyond visual validation of the composition model, one can actually select among
the two competing models using a proper likelihood ratio test. The basic idea is to
evaluate the likelihood of the training data under the competing model and select
the one providing the larger value.
For the second simulated dataset, we use the data shown in Fig. 8.5 (bot-
tom left) and perform A-FPCA, P-FPCA. As before, we randomly generate 35
0
0.5
1
0
0.2
0.4
0.6
0.8
1
−3
0
3
−3
0
3
−3
0
3
0.3
0.8
1.3
0.3
0.8
1.3
0.3
0.8
1.3
Random Phases
Gaussian Samples
Composition
Random Amplitude
Fig. 8.17 Random samples from jointly Gaussian models on FPCA coeﬃcients of γ (left) and
f (middle) and their combinations f ◦γ (right) for Simulated Data 1. The last plot are random
samples if a Gaussian model is imposed on f directly without any phase and amplitude separation

294
8 Statistical Modeling of Functional Data
0
1
0
0.2
0.4
0.6
0.8
1
0
0.5
0.5
1
0
2
4
6
8
0
0.5
1
0
2
4
6
8
0
0.5
−5
0
5
10
Random Phases
Gaussian Samples
Composition
Random Amplitude
1
Fig. 8.18 Random samples from jointly Gaussian models on FPCA coeﬃcients of γ (left) and
f (middle) and their combinations f ◦γ (right) using parameters estimated from the training
data. The last panel shows the random samples resulting from a Gaussian model imposed on f
directly, without separating phase and amplitude components
functions from the amplitude model and 35 domain-warping functions from the
phase model and then combine them to generate random functions. The corre-
sponding results are shown row of Fig. 8.18, where the ﬁrst panel is a set of random
warping functions, the second panel is a set of corresponding amplitude functions,
and the last panel shows their compositions. Under a visual inspection, the pro-
posed models seem successful in capturing the variability in the given data. One
can validate these models more formally using likelihood-based tests. In this ex-
ample, performing FPCA directly on the function space does not correctly capture
the data and fails to generate any single unimodal function, as shown in the last
panel.
8.9 Joint Approach: Elastic FPCA
In case one is interested in low-dimensional representations of functional data, es-
pecially for data containing phase variability, one can also formulate this problem
directly, without performing phase-amplitude separation as a preliminary step. In
some cases it may seem more natural to perform these two steps—dimension-
reduction and phase-amplitude separation—simultaneously. The resulting low-
dimensional representations can then be used in statistical modeling, pattern
recognition, or other applications. One can also view this formulation as a model-
based approach where low-dimensional representations become parameters in a
model. Then, the search for representations becomes a problem of estimation or
optimization under an appropriate objective function. Depending on the choice
of model, or the objective function, there are several possibilities. Here we look
at two possibilities, involving the L2 metric on two diﬀerent spaces, one on the
original function space F and the other on the SRSF space L2.
8.9.1 Model-Based Elastic FPCA in Function Space F
We start with the case where each functional observation fi is described in terms
of its components: amplitude gi and phase γi. We caution the reader that in
this model-based approach, we will develop a completely new notion of phase

8.9 Joint Approach: Elastic FPCA
295
and amplitude, diﬀerent from the deﬁnitions provided in Sect. 4.7. So, there is
no connection between the usage of these terms phase and amplitude here and
that in Sect. 4.7. We model the amplitude variable gi to be an element of a low-
dimensional subspace, and the phase γi is left to be completely arbitrary. Thus,
the amplitude can be expressed using an orthonormal basis set {bj} so that it is
simply a linear combination of these basis elements.
fi(t) = gi(γ−1
i
(t)),
gi(t) = μf(t) +
J

j=1
ci,jbj(t) + ϵi(t) , i = 1, 2, . . ., n,
where
• μf is the mean of the amplitude,
• {fi ∈L2|i = 1, 2, . . ., n} are the given observations,
• {γi ∈ΓI|i = 1, 2, . . ., n} represent arbitrary phases of the observations,
• {bj ∈L2|j = 1, 2, . . . , J} is a chosen basis set,
• {ci,j ∈R|i = 1, 2, . . . , n, j = 1, 2, . . ., J} are the low-dimensional representations
of interest, and
• {ϵi ∈L2|i = 1, 2, . . ., n} is observation noise modeled as a white Gaussian noise
process.
(This is a simple extension of the model presented in Eq. 4.7, which did not involve
any time warping.) Another way to express this model is:
fi(γi(t)) = μf(t) +
J

j=1
ci,jbj(t) + ϵi(t) , i = 1, 2, . . ., n .
(8.7)
Having selected the model, we now focus our attention on the estimation problem:
Given the observations {fi}, the main goal is to estimate the coeﬃcients {ci,j}.
However, since the phases {γi} are unknown, and they play the role of nuisance
parameters in this estimation, one has to estimate them too from the data. Fur-
thermore, in case we do not want to assume any predetermined basis, then the
basis set {bj} also becomes a parameter of the model and needs to be estimated.
Under the white Gaussian noise model, the full estimation problem is:

ˆμf, ˆc,ˆb, ˆγ

=
arginf
μf ,{γi},{ci,j},{bj}
⎛
⎝
n

i=1
arginf
γi∈ΓI
⎛
⎝∥fi ◦γi −μf −
J

j=1
ci,jbj∥2
⎞
⎠
⎞
⎠.
(8.8)
One can solve for these unknowns using a suitable optimization approach. A par-
ticularly simple solution is to use coordinate descent, where one solves for one set
while ﬁxing the others. These conditional solutions can be evaluated as follows:
1. Updating Phase Components: For ﬁxed {ci,j} and {bj}, the optimization
over relative phases {γi} is given by:
ˆγi = argmin
γ∈ΓI
⎛
⎝∥fi ◦γ −μf −
J

j=1
ci,jbj∥2
⎞
⎠,
i = 1, 2, . . ., n .
(8.9)

296
8 Statistical Modeling of Functional Data
Notice that computations of ˆγi, for diﬀerent i, are independent of each other.
As discussed earlier in this textbook, these problems can be solved using the
DPA, one for each i, and the solution can actually be implemented in parallel
due to this independence across i’s.
2. Updating Mean Function: We can estimate the mean function μf using the
cross-sectional average:
ˆμf(t) = 1
n
n

i=1
fi(ˆγi(t)) .
3. Updating Basis Set Using PCA: For a ﬁxed set of warping functions {ˆγi},
the problem of estimating the basis set is given by:
ˆb =
arginf
{bj ∈L2([0, 1], R)|j = 1, 2, . . . , J,
bj ⊥bi, i ̸= j
∥bj∥= 1, j = 1, 2, . . . , J}
⎛
⎝
n

i=1
arginf
γi∈Γ
∥fi ◦ˆγi −ˆμf −
J

j=1
ci,jbj∥2
⎞
⎠.
(8.10)
It is easy to see that this is exactly the problem of ﬁnding FPCA of given
function data {fi ◦ˆγi} (see Sect. 4.3.1). FPCA has been used several times in
this chapter already.
4. Updating Coeﬃcients: Given the phase components {γi} and an orthonormal
basis {bj}, the coeﬃcients {ci,j} can be computed using simple projections:
ci,j = ⟨fi ◦ˆγi, bj⟩, i = 1, 2, . . ., n,
j = 1, . . . , J .
(8.11)
An iterative algorithm for implementing this estimation procedure is as follows.
Algorithm 35 (Joint FPCA Under Phase Variability).
1. Initialize the warping functions to be identity ˆγi = γid, i = 1, 2, . . ., n.
2. FPCA: Perform FPCA on the warped data {fi ◦ˆγi} and select the basis ele-
ments for the top J principal components. This results in an orthonormal basis
{bj, j = 1, 2, . . ., J}.
3. Coeﬃcients: Given the basis elements {bj}, compute {ci,j} using Eq. 8.11.
4. Alignment: Given the basis {bj} and the coeﬃcients, update the warping func-
tions {ˆγi} according to Eq. 8.9 using the DPA, for each i.
5. Check for convergence. If not converged, return to Step 2.
The convergence properties of this approach have been left out of the discussion
here. Generally speaking, the cost function has to be convex in all the variables
being optimized to guarantee convergence of coordinate descent to a global solu-
tion.
We study this approach empirically using a couple of simulated examples. In
Fig. 8.19, we show results on a dataset of 20 bimodal functions used earlier in
phase-amplitude separation experiments. The ﬁgure shows the original functions,
the estimated warping functions {ˆγi}, the warped functions {fi ◦ˆγi}, and the
evolution of the objective function during optimization. In this experiment we use
J = 3. In the results we can clearly see the presence a pinching eﬀect. The warping

8.9 Joint Approach: Elastic FPCA
297
-3
-2
-1
0
1
2
3
-3
-2
-1
0
1
2
3
-3
-2
-1
0
1
2
3
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
1.1
1.2
1.3
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
1.1
1.2
1.3
1
1.5
2
2.5
3
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
{fi}
{gi}
{ fi ◦gi}
Objective function
Fig. 8.19 FPCA with data warping using Algorithm 35. Here we use J = 3 to perform FPCA.
The results shows a pinching of functions during joint alignment, a shortcoming of the L2 norm
used in this algorithm
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
0
2
4
6
8
10
12
0.1
0.2
0.3
0.4
0.5
0.6
0.7
{ fi}
{gi}
{fi ◦gi}
Objective function
Fig. 8.20 FPCA with data warping using Algorithm 35. Here we use J = 2 to perform FPCA.
The pinching of aligned functions is clearly visible and is a drawback of this framework
functions show near-vertical jumps that result in pinching of the corresponding fi’s,
in order to minimize the objective function.
We take another example, this time using data relating to the absolute, warped
sine functions, shown in the top-left panel of Fig. 8.20. We apply Algorithm 35
on this dataset for J = 3. Once again, we observe a large pinching eﬀect in these
results.
Thus, we conclude that the main problem with this formulation is pinching,
similar to the discussion presented in Sect. 4.5. In the absence of any additional
constraint on {γi}, we observe sharp jumps in these warping functions resulting in
a pinching of the given functions. This suggests that we reformulate this problem
using the Fisher-Rao distance, which, in turn, motivates the use of SRSFs under
the L2 norm.
8.9.2 Elastic FPCA Using SRSF Representation
In order to avoid the pinching eﬀect, and to avoid the use of external roughness
penalties (since they come with a diﬃcult choice of relative weights), we suggest
another model to study the same problem. Similar to a generalized linear model,
we ﬁrst transform the given functions into their SRSFs and then model them using
a linear model as earlier. Let qis denote the SRSFs of the given fis, and impose
the model:
qi(t) = (gi, γ−1
i
)(t),
gi(t) = μ(t) +
J

j=1
ci,jbj(t) + ϵi(t) , i = 1, 2, . . . , n,

298
8 Statistical Modeling of Functional Data
where the quantities in the model have the same roles as in Eq. 8.7. The diﬀerence
from Eq. 8.7 is that the current model is posed in the SRSF space, and, thus,
the warping group action is diﬀerent. Once again, we can rearrange terms in this
model to get:
(qi, γi)(t) = μ(t) +
J

j=1
ci,jbj(t) + ϵi(t) , i = 1, 2, . . ., n .
(8.12)
Note that for J = 0, this equation reduces to Eqn. 8.3. Under the assumption of
white Gaussian noise, the full estimation problem becomes:

ˆμ, ˆc,ˆb, ˆγ

=
arginf
μ,{γi},{ci,j},{bj}
⎛
⎝
n

i=1
arginf
γi∈ΓI
∥(qi, γi) −μ −
J

j=1
ci,jbj∥2
⎞
⎠.
(8.13)
Compared to Eq. 8.8, this formulation is more stable since the action of ΓI is norm
preserving here and prevents pinching of the SRSFs being aligned. Similar to the
previous section, we can apply a coordinate-descent technique and the resulting
algorithm can be summarized in the following.
Algorithm 36 (Elastic Function Principal Component Analysis).
1. Initialization: Set γ∗
i = γid for all i.
2. Form the warped SRSFs ˜qi = (qi, γ∗
i ) for all i, and estimate μ using ˆμ =
1
n
n
i=1 ˜qi.
3. Compute the covariance matrix
Kq(s, t) =
1
n −1
n

i=1
(˜qi(s) −ˆμ(s))(˜qi(t) −ˆμ(t)) .
4. Take the SVD of Kq, and set bjs to be the ﬁrst J eigenvectors of Kq.
5. Compute the coeﬃcients ci,j = ⟨˜qi, bj⟩for i = 1, . . . , n and j = 1, . . . , J.
6. For each i, solve the optimization problem using the DPA:
γ∗
i = arginf
γ∈ΓI
⎛
⎝∥(qi, γ) −ˆμ −
J

j=1
ci,jbj∥2
⎞
⎠.
7. Check for convergence. If not converged, return to Step 2.
The convergence can be tested using the decrease in the objective function from
one iteration to the other.
We apply this framework to the same datasets as in the previous approach.
Figure 8.21 shows the original functions, their SRSFs, the warped SRSFs, and
the estimated μ in the top row. The bottom row shows the warping functions
{γi} and the aligned functions {fi ◦γi}. In the rightmost panel, we also see the
singular values of the functional data before and after the alignment. Based on
these results, several remarks are in order. Firstly, it is easy to see that the aligned
data does not show any signs of pinching eﬀects. The use of the SRSF avoids
the pinching problem as hypothesized earlier in the formulation. In the original
data, we have four signiﬁcant singular values, but after alignment, there is no

8.9 Joint Approach: Elastic FPCA
299
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
1.1
1.2
1.3
-1.5
-1
-0.5
0
0.5
1
1.5
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
{fi}
{qi}
{(qi ,gi)}
m
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
-3
-2
-1
0
1
2
3
-3
-2
-1
0
1
2
3
-3
-2
-1
0
1
2
3
-3
-2
-1
0
1
2
3
-3
-2
-1
0
1
2
3
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
1.1
1.2
1.3
0
2
4
6
8
10
0
0.1
0.2
0.3
0.4
0.5
0.6
{gi}
{(fi◦gi)}
Singular values
Fig. 8.21 FPCA with data warping in the SRSF space using Algorithm 36 using J = 2 to
perform FPCA. Compare these results with those in Fig. 8.19
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
-6
-4
-2
0
2
4
6
-3
-2
-1
0
1
2
3
-2
-1.5
-1
-0.5
0
0.5
1
1.5
2
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
2
4
6
8
10
0
0.1
0.2
0.3
0.4
0.5
0.6
{fi}
{qi}
{(qi ,gi)}
m
{gi}
{(fi◦gi)}
Singular values
Fig. 8.22 FPCA with data warping in the SRSF space using Algorithm 36 using J = 3 to
estimate model parameters. Compare these results with those in Fig. 8.20
signiﬁcant variability left in the data. Note that we used J = 2 in this experiment.
Compare the shape of the aligned functions with those shown in the bottom row
of Fig. 8.5. The aligned functions are unimodal in both cases, but the modal shape
is much broader in this case than the previous alignment. It is diﬃcult to justify
these {fi ◦γi} as simple alignments of the original functions; there is an additional
distortion in these functions, due to the nature of the underlying model being used
to estimate the warping functions.
Another example, involving sine functions, is shown in Fig. 8.22.
Alternative Deﬁnition of Phase and Amplitude This model-based es-
timation of {γi} can be viewed as an alternative deﬁnition of phase (and thus
amplitude) of a functional variable. One can compare this deﬁnition with the

300
8 Statistical Modeling of Functional Data
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
5
10
15
20
0
5
10
15
20
0
5
10
15
20
0
5
10
15
20
0
5
10
15
20
25
30
35
-5
0
5
10
15
20
25
30
35
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
-5
0
5
10
15
20
25
30
-5
0
5
10
15
20
25
30
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
{fi}
{gi}
{ fi ◦gi}
Fig. 8.23 Model-based separation of phase and amplitude in some functional datasets. Even
though these functions are not always on [0, 1], the warping functions are scaled for display as
elements of ΓI
metric-based deﬁnition presented in Sect. 4.7. We make some remarks about
these two competing deﬁnitions (Fig. 8.23).
1. Firstly, the phase-amplitude separation based on the model in F, given in
Eq. 8.7, is plagued by the pinching eﬀect and destroys the structure of the given
data. Even if one can impose a regularization term, or restrict to only simple
warping functions, in order to tame the pinching eﬀect, these choices are diﬃcult
and often require a manual selection to yield interesting results. Consequently,
this model does not yield a satisfactory deﬁnition of phase and amplitude in
general and we are left only with SRSF models (e.g., Eq. 8.12) for a model-based
approach.
2. This model-based approach is primarily for multiple function alignment and
does not provide useful results for pairwise alignment. Recall from Sect. 4.4
that it is desirable for a pairwise solution to satisfy certain properties: invariance
to simultaneous warping, robustness to random warping, and inverse symme-
try. The alignment obtained from a model-based approach will not necessarily
satisfy these properties. The metric-based approach, on the other hand, han-

8.10 Exercises
301
dles both the pairwise and multiple function alignment in a consistent, elegant
manner.
3. In the metric-based approach, the deﬁnition of amplitude is absolute—it consists
of a whole equivalence class (all possible warpings) of a function. Once the set
of allowed warping functions are chosen, this deﬁnition is ﬁxed—independent of
choice of metric or representation. Only the deﬁnition of phase depends on the
metric in this approach. This characterization seems more natural and satisfac-
tory than the model-based approach where all components (phase and ampli-
tude) depend on model choices—representation space, noise model, number of
basis elements, and so on.
4. Relative to the metric-based approach, the model-based ideas seem better
placed for statistical analysis, since we are able to specify an underlying model
and pose the phase-amplitude separation as estimation. In this setup, one can
potentially develop asymptotic analyses relating to the eﬃciency and consis-
tency of the estimators of phase and amplitude. To accomplish the same task
in the metric-based approach, one would need to specify an underlying model
that is compatible with all the algebraic deﬁnitions laid out in that approach.
5. Interestingly, the alignment results are quite similar under the two approaches—
metric-based and model-based—for the datasets studied here.
In summary, we conclude that the metric-based approach is mathematically more
fundamental and elegant for phase-amplitude separation in functional data, while
the model-based approach is more amenable to a statistical analysis.
8.10 Exercises
8.10.1 Theoretical Exercises
1. For the functions f1, f2, . . . , fn ∈L2([0, 1], R), show that their Karcher mean
under the L2 norm is the same as the cross-sectional mean. That is,
μ =
arginf
f∈L2([0,1],R)
n

i=1
∥f −fi∥2 = 1
n
n

i=1
fi .
2. Let γ1, γ2, . . . , γn ∈ΓI be a set of warping functions. Show that:
a. Their sample average under the L2 norm is simply t →1
n
n
i=1 γi(t).
b. This average is an element of ΓI, i.e., it is a diﬀeomorphism, invertible, and
its inverse is a diﬀeomorphism.
3. Provide an example of a situation where:
1
n
n

i=1
f(γi(t)) ̸= f( 1
n
n

i=1
γi(t)) .
Repeat the problem when the cross-sectional average of γi’s is replaced by
their sample Karcher mean.

302
8 Statistical Modeling of Functional Data
4. Show that the cost function n
i=1 da([μq], [qi])2 does not increase from one
iteration to the next in Algorithm 31.
5. What happens in the Deﬁnition 8.2 when n = 1? That is, for an arbitrary
q, q1 ∈L2([0, 1], R), what is the center of the orbit [q] with respect to the
singleton set {q1}?
6. Averaging within an orbit: Let q ∈L2([0, 1], R) (assume that q ̸= 0 a.e.)
and let [q] be its orbit under ˜ΓI. Let q1, q2, . . . qn be elements of this orbit.
In other words, there exist γ1, γ2, . . . , γn ∈˜ΓI such that qi = (q, γi) for all
i = 1, 2, . . ., n.
• L2 Average: If we deﬁne a cross-sectional mean ˆq(t) = 1
n
n
i=1 qi(t), will
the average in general lie in the orbit [q]?
• da Average: Now, if we deﬁne a sample Karcher mean [μq] of the am-
plitudes [q1], [q2], . . . , [qn] according to Deﬁnition 8.1, how will the mean
amplitude relate to [q]?
7. Once again, let q1, q2, . . . qn be elements of the same orbit [q], i.e., qi = (q, γi)
for some γi ∈˜ΓI, and let [μq] be the sample Karcher mean of the amplitudes
[q1], [q2], . . . , [qn] under da. Let q0 ∈[μq] be the center of this orbit with respect
to q1, q2, . . . , qn (according to Deﬁnition 8.2). Show that the calculation of q0
boils down to averaging of the corresponding γis in ˜ΓI (under the Fisher-Rao
distance).
8.10.2 Computational Exercises
1. Write a program to compute the cross-sectional mean of a set of functions
f1, f2, . . . , fn on [0, 1]. Assume that the function values are given to you at
arbitrary sample points in [0, 1]. Thus, you will need a function for interpolation
on the full domain. Display the original functions and the results. Test this
program with the functions fi(t) = sin(2πit).
2. Write a program to compute the SRSF q of a given function f and another pro-
gram to recover the function back from its SRSF (up to a vertical translation).
3. Implement the dynamic-programming algorithm (DPA) to align any two given
functions f1 and f2 by minimizing the objective function:
arginf
γ∈ΓI

∥f1 −f2 ◦γ∥2 + λR(γ)

,
where R(γ) can be: (1)
 1
0 ˙γ(t)2dt, or (2)
 1
0 ¨γ(t)2dt. Try the program with
f1(t) = sin(2πt) and f2(t) = 2 sin(2πt) and diﬀerent values of λ > 0.
4. Implement Algorithm 31 for computing means of amplitudes of a given set of
functions. Test this program on the following sets of functions:
a. fi(t) = sin(2πγi(t)), γi(t) = t2i/n, for i = 1, 2, . . ., n and where n is an even
number.
b. fi(t) = sin(2πt −(i −n
2 )π/n), for i = 1, 2, . . ., n, and where n is an even
number.
5. Write a program to compute the center of an orbit [q] with respect to the
functions q1, q2, . . . , qn ∈L2([0, 1], R), according to Algorithm 32.

8.11 Bibliographic Notes
303
6. Implement Algorithm 33 to perform phase-amplitude separation of a given set of
functions. Test your program on simulated data created as follows: let yi(t) =
zi,1e−(t−1.5)2/2 + zi,2e−(t+1.5)2/2, i = 1, 2, . . . , 21, where zi,1 and zi,2 are i.i.d
normal with mean one and standard deviation 0.25. Each of these functions
is then warped according to: γi(t) = 6( eai(t+3)/6−1
eai −1
) −3 if ai ̸= 0; otherwise,
γi = γid, where {ai} are equally spaced between −1 and 1, and the functions
of interest are computed using fi(t) = yi(γi(t)).
7. Implement the penalized alignment presented in Algorithm 34, and test it on
the data generated in the previous example for diﬀerent values of λ.
8.11 Bibliographic Notes
The problem of alignment of multiple functions has been studied in several places,
including [71, 113, 67, 90, 91, 55]. Most of these methods rely on using the L2 norm
on functional data directly and suﬀer from the limitations described in this chapter.
An introduction to the problem of phase-amplitude separation can be found in [76].
The framework for phase-amplitude separation using elastic Riemannian metric
was ﬁrst proposed in [108]. Modeling of phase and amplitude components using
FPCA was discussed in [117].
Several case studies involving real datasets and phase amplitude have been
presented in [122, 116, 59, 123].

Chapter 9
Statistical Modeling of Planar Shapes
As emphasized in the introduction chapter (Sect. 1.3), one of the main goals in this
textbook is to develop statistical models of shapes of curves. Within that broad
topic, there are several tools that we wish to develop:
1. Clustering of Shapes: A basic task in any pattern analysis of objects is
their clustering into homogeneous groups. Since we are focused on the shape
of objects in this textbook, we seek a method for clustering curves according
to their shapes. The basic idea is to divide given objects into subsets such
that the shape variability is small within the subsets, and as large as possible
across subsets. The quantiﬁcation of shape variability is based on shape metrics
derived in the earlier chapters.
2. Summary Statistics of Shapes: An important tool in shape analysis is the
computation of summary statistics. One is given a set of observations of shapes,
from the same class or diﬀerent classes, and the goal is to replace them with a
short summary that characterizes the observed set. In our context, one may be
interested in ﬁnding a representative shape for a given collection of shapes (see
item 2 in Sect. 1.3). This can be obtained using sample mean, sample truncated
mean, or sample median shape. Once a representative shape is obtained, the
next item of interest is covariance, a quantity that encodes the principal or
dominant modes of variability in shape data. These summaries may also provide
estimates for corresponding parameters under certain probability distributions,
but that aspect is not developed in this textbook.
3. Stochastic Modeling of Shapes: If we treat shapes as random quantities,
then we would like to specify the probabilistic rules that govern these random
quantities. For instance, we may want to model the shapes of the hippocampi
in the human brain, the silhouettes of ducks ﬂoating in a pond, the shapes of
tanks in a battleﬁeld, or the shapes of human silhouettes under speciﬁc poses.
What this means is that we want to deﬁne probability density functions on shape
spaces, a diﬀerent density for each shape class, that take higher values in regions
where more shapes are observed or expected and lower values where fewer are
observed. One can view this as a problem of estimating density functions—
parametric or nonparametric—using past observations of shapes. In computer
science, this problem of estimating densities is often termed as that of learning.
A natural option is to use sample means and covariances and impose densities
on shape spaces that are analogous to Gaussian densities but are adapted to
© Springer-Verlag New York 2016
A. Srivastava, E.P. Klassen, Functional and Shape Data Analysis,
Springer Series in Statistics, DOI 10.1007/978-1-4939-4020-2 9
305

306
9 Statistical Modeling of Planar Shapes
shape spaces (or rather their ﬁnite-dimensional subsets). In case the densities
belong to a parametric family, the task of learning reduces to that of estimating
parameters.
4. Testing/Classiﬁcation of Shapes: A common problem in shape analysis is
the classiﬁcation of given shapes into predetermined shape classes. For example,
given a contour of an object in an image, we can attempt to classify it as a
vehicle, a building, or a human silhouette. As another example, we may be
interested in studying the shape of a ventricle in a human brain, in order to
classify patients as healthy or unhealthy. From a statistical point of view, this
problem can be formulated as a problem in hypothesis testing (see item 4 in
Sect. 1.3 of the introduction chapter). Similarly, the problem of detecting an
object can also be studied as that of a binary hypothesis testing, to determine
whether an object is present in given data or not. Solutions to such testing
problems require statistical models for candidate shape classes.
9.1 Goals and Challenges
We set up the following speciﬁc goals for this chapter:
1. Given a set of planar curves, closed or open, we want to cluster them or subdi-
vide them into a given number of subsets so as to optimize a prescribed objective
function. This objective function is typically constructed using shape metrics
derived in the previous chapters.
2. Extend ideas of Chap. 7 and deﬁne statistical summaries—sample means, trun-
cated means, and medians—of given sets of shapes associated with speciﬁc
objects and shape classes.
3. Deﬁne parametric probability densities on (dominant subsets of) shape spaces
of general and closed curves. In particular, we will adapt the notion of a trun-
cated wrapped-normal (TWN) density to shape spaces of curves. Speciﬁcally,
our modeling approach will be to: (1) Select a representative shape μ ∈S2 (or
Sc
2) as the central tendency of the population. (2) Select a ﬁnite-dimensional
subspace of a tangent space T[μ](S2) (or T[μ](Sc
2)). This will give us a ﬁnite-
dimensional representation of shapes. (3) Deﬁne a probability density, such as
the truncated multivariate normal density, on that vector space. (4) Use the ex-
ponential map to transfer this density onto a ﬁnite-dimensional subset of S2 (or
Sc
2). The previous item on computing summaries can be related to estimating
parameters of truncated wrapped-normal densities using observed data.
4. Develop algorithms for generating random samples from shape models. This
tool will aid us in applying Monte Carlo-type methods for generating statistical
inferences, especially in Bayesian frameworks. For the TWN densities, this task
becomes quite simple with the use of exponential maps.
5. Formulate hypothesis tests for shape classiﬁcations and deﬁne likelihood ratios,
and other classiﬁcation tools.
Statistical models of shapes can have many further uses, including Bayesian shape
estimation, shape tracking in dynamical scenes, and pattern analysis of a collection
of shapes, but we will focus mainly on the items mentioned above.
Challenges The challenges associated with accomplishing these goals are similar
to those discussed in the previous chapters. While a metric-based clustering is rel-

9.2 Clustering in Shape Spaces
307
ative easy, given a shape metric, the remaining goals are still diﬃcult to achieve
using standard multivariate statistics. To generate statistical summaries, we will
need to deal with the nonlinear nature of shape spaces, using computational tools
for calculating geodesics and geodesic distances between arbitrary shapes. In sum-
mary, the nonlinearity, inﬁnite dimensionality, and complexity of shape spaces are
the main challenges in reaching the goals laid out here.
9.2 Clustering in Shape Spaces
We start by clustering curves on the basis of their shapes. There is an impor-
tant need in shape analysis to cluster similar shapes together using an automated
procedure. The goal is to allocate n curves into k subsets (k < n) such that the ho-
mogeneity within these subsets is maximized and across the subsets is minimized.
This process is also called an unsupervised learning process. There are two sets of
unknowns in a clustering solution: (1) the number of clusters k and (2) the n × k
membership matrix M that describes the allocation of n objects to k clusters. Most
of the elements of M are zero; each row has only one nonzero element, which is one.
In this chapter, we will assume that k is given and we focus on the search for an
optimal M. We will treat shapes as elements of Sc
2, the elastic shape space of closed
planar curves as described in Chap. 6, and note that we have already developed
algorithms for computing geodesic distances between elements of this space.
Traditional algorithms for clustering in Euclidean spaces are well studied re-
searched and generally fall into two main categories: partitional and agglomerative.
These techniques are also called hierarchical clustering. The partitional algorithms
are top-down. They start with one big cluster of n shapes and successively divide
the current clusters, while optimizing a chosen cost function, until the desired num-
ber of clusters is reached. The agglomerative algorithms, on the other hand, take a
bottom-up approach. They start with each shape as a cluster and they iteratively
merge sets until the number of clusters is reduced to k. The objective functions
are based on metrics between clusters themselves, which, in turn, are often based
on pairwise distances between shapes. For example, one objective function can be
to minimize the sum of all pairwise distances between clusters, where the distance
between any two clusters is deﬁned to be the average of all distances between
points across those sets.
The main diﬀerence between clustering in a Euclidean space and a shape space
is the nonlinearity of the shape space. One can use the notion of the mean and
the covariance on the shape space Sc
2 and use it to perform clustering. However,
we mostly restrict to ideas that rely only on pairwise geodesic distances between
shapes. This avoids the computation of means and covariances for the purpose of
clustering.
9.2.1 Hierarchical Clustering
Consider the problem of clustering n shapes into k clusters. Let a conﬁguration
C consist of clusters denoted by C1, C2, . . . , Ck, with cluster sizes n1, n2, . . . , nk,
respectively. A hierarchical clustering iteratively either merges small clusters or

308
9 Statistical Modeling of Planar Shapes
11191614201315121718 1 5 2 6 8 7 10 3 4 9
11191416131520171812 1 5 2 6 8 7 10 9 3 4
11191614201513121718 1 5 2 8 6 7 10 3 4 9
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0.55
Shapes
Average
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Nearest
Furthest
Fig. 9.1 A set of 20 shapes of the left have been clustered using diﬀerent linkage criterion:
average (top-right), nearest distance (bottom left), and compete or furthest distance (bottom
right)
divides large clusters until the required number of clusters is reached. Some com-
putational softwares have built-in routines for clustering data points according to
a chosen metric between the sets {Ci}. For instance, one can deﬁne linkage using
the nearest-neighbor criterion, d(Ci, Cj) = min[q1]∈Ci,[q2]∈C2 ds([q1], [q2]), or the
furthest d(Ci, Cj) = max[q1]∈Ci,[q2]∈C2 ds([q1], [q2]). In case of average linkage, the
distance is given by d(Ci, Cj) =
1
|Ci||Cj|

[q1]∈Ci,[q2]∈C2 ds([q1], [q2]). One can also
use the median distance or a weighted distance, or other combinations of pairwise
shape distances.
An example of this idea is shown in Fig. 9.1 where we use the 20 contours
displayed in the left panel. In the remaining three panels, we show dendrograms
resulting from three diﬀerent kinds of linkages in the clustered sets: average, nearest
distance, and furthest distance. Since the shapes here are very well separated
into two sets, the clustering result is quite stable with respect to diﬀerent linkage
criteria.
We show another example in Fig. 9.2 where we have four shape classes and the
shapes are not as diﬀerent as in the previous example. In this example, there are
some changes, albeit small, as we change the criterion for clustering. Speciﬁcally,
shapes 13 and 15 change clusters based on the deﬁnition of distance between

9.2 Clustering in Shape Spaces
309
111214131617201918 6 8 10 7 9 15 1 2 4 5 3
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Shapes
Average
11121413151617201918 6 8 10 9 7 1 2 4 3 5
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0.55
1112141617201918 6 8 10 7 9 1315 1 2 4 5 3
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Nearest
Furthest
Fig. 9.2 Same as Fig. 9.1
clusters. In this example, the nearest-neighbor clustering provides the best result
as the four classes are all clustered separately.
9.2.2 A Minimum-Dispersion Clustering
Consider the problem of clustering n shapes (in Sc
2) into k clusters. A general
approach is to form clusters in such a way that they minimize total within-cluster
variance, which relates to a scaled sum of distances (squared). Let a conﬁguration
C consist of clusters denoted by C1, C2, . . . , Ck, with sizes n1, n2, . . . , nk, and deﬁne
a cost function:
Q(C) =
k

i=1
2
ni
⎛
⎝
qa∈Ci

b<a,qb∈Ci
d([qa], [qb])2
⎞
⎠.
(9.1)
We seek conﬁgurations that minimize Q, i.e., C∗= argmin Q(C).
An important question in any clustering problem is: How to choose k? In some
cases, such as those involving shape retrieval, the answer comes from the context.
Here, k determines a balance between the retrieval speed and performance, and a

310
9 Statistical Modeling of Planar Shapes
wide range of k can be tried. In the worst case, one can set k = n/2 at every level
of hierarchy and still obtain O(log(n)) retrieval speeds. (This assumes that the
shapes are uniformly distributed in the clusters.) However, the choice of k is much
more important in the case of learning. Probability models estimated from the
clustered shapes are sensitive to the clustering performance. To obtain a possible
k automatically, one option is to study the variation of Q(C∗) for diﬀerent values of
k and select a k that provides the largest decrease in Q(C∗) from its value at k−1.
Another possibility is to use human supervision in selecting k. A third possibility
is to use a Bayesian approach, under a Dirichlet process prior, for searching over
diﬀerent values of k [127].
Clustering Algorithm We will take a stochastic simulated annealing approach
to solve for C∗. (For the reader that are not familiar with simulated annealing,
it is an optimization approach that searches for solutions in a larger space and
iteratively reduces the search space as it gets closer to a solution. This search
domain is controlled by altering the proﬁle of energy landscape using a temperature
parameter—high temperature implies ﬂatter proﬁle and larger search space, and
vice versa.) We will minimize the clustering cost using a Markov chain search
process on the conﬁguration space. Please refer to [93] for a version of Markov
chain simulated annealing algorithm. The basic idea is to start with a conﬁguration
of k clusters and to reduce Q by rearranging shapes among the clusters. The
rearrangement is performed in a stochastic fashion using two kinds of moves. These
moves are performed with probability proportional to the negative exponential of
the Q-value of the resulting conﬁguration. The two types of moves are:
1. Move a shape: Here we select a shape randomly and reassign it to another
cluster. Let Q(i)
j
be the clustering cost when a shape qj is reassigned to the
cluster Ci keeping all other clusters ﬁxed. If qj is not a singleton, i.e., not the
only element in its cluster, then the transfer of qj to cluster Ci is performed
with probability: PM(j, i; T ) =
exp(−Q(i)
j
/T )
k
i=1 exp(−Q(i)
j
/T ), i = 1, 2, . . ., k. Here T plays
a role similar to temperature in simulated annealing. If qj is a singleton, then
moving it is not allowed in order to ﬁx the number of clusters at k.
2. Swap two shapes: Here we select two shapes randomly from two diﬀerent clus-
ters and swap them. Let Q(1) and Q(2) be the Q-values of the original conﬁgura-
tion (before swapping) and the new conﬁguration (after swapping), respectively.
Then, swapping is performed with probability: PS(T ) =
exp(−Q(2)/T )
2
i=1 exp(−Q(i)/T ).
Additional moves can also be designed to improve the eﬃciency of search over the
conﬁguration space, although their computational cost becomes a factor too. In
view of the computational simplicity of moving a shape and swapping two shapes,
we have restricted our algorithm to these two moves.
In order to seek global optimization, we have adopted a simulated annealing
approach. That is, we start with a high value of T and reduce it slowly as the
algorithm searches for conﬁgurations with smaller dispersions. Additionally, the
moves are performed according to an acceptance-rejection procedure that is a
variant of more conventional simulated annealing. Here, the candidates are pro-
posed randomly and accepted according to certain probabilities (PM and PS are
deﬁned above). Although simulated annealing and the random nature of the search
help in avoiding local minima, the convergence to a global minimum is diﬃcult to

9.3 A Finite Representation of Planar Shapes
311
establish. The output of this algorithm is a Markov chain that is neither homo-
geneous nor convergent to a stationary chain. If the temperature T is decreased
slowly, then the chain is guaranteed to converge to a global minimum [93]. How-
ever, it is diﬃcult to make explicit the required rate of decrease in T and instead
we rely on empirical studies to justify this algorithm. First, we state the algorithm
and then describe some experimental results.
Algorithm 37 (Clustering Using Simulated Annealing). For n shapes and
k clusters, initialize by randomly distributing n shapes among k clusters. Set a
high initial temperature T .
1. Compute pairwise geodesic distances between all n shapes. This requires n(n −
1)/2 geodesic computations.
2. With equal probabilities, pick one of the two moves:
• Move a shape: Choose a shape qj randomly. If it is not a singleton in its
cluster, then compute Q(i)
j
for all i = 1, 2, . . ., k. Compute the probability
PM(j, i; T ) for all i = 1, . . . , k and reassign qj to a cluster chosen according
to the probability PM.
• Swap two shapes: Select two clusters randomly and select a shape from
each. Compute the probability PS(T ) and swap the two shapes according to
that probability.
3. Update the temperature using T = T/c and return to Step 2. We suggest using
c = 1.0001.
It is important to note that once the pairwise distances are computed, they are
not computed again in the iterations. Secondly, unlike k-mean clustering, the mean
shapes are never calculated in this clustering. These factors make Algorithm 37
eﬃcient and eﬀective in clustering diverse shapes.
Figure 9.3 shows two examples of clustering MPEG7 shapes into a predeter-
mined number (k = 6) of clusters using Algorithm 37. In each row, the left panel
shows the given shapes arranged into clusters (rows), while the evolution of Q
versus iterations is displayed in the corresponding right panel.
9.3 A Finite Representation of Planar Shapes
The main challenges in shape modeling stem from inﬁnite dimensionality and
nonlinearity of shape spaces. We suggest a solution, naturally an approximation,
that reduces the representation space to a ﬁnite-dimensional vector space.
9.3.1 Shape Representation: A Brief Review
Recall shape representations of planar curves, developed in Chap. 5, where we have
multiple choices of representations/metrics: a non-elastic representation that uses
the angle functions of arc-length curves and an elastic representation that uses
SRVFs of parameterized curves. Since the modeling procedure is similar for the

312
9 Statistical Modeling of Planar Shapes
×104
0
2
4
6
8
10
×104
0
2
4
6
8
10
2
4
6
8
10
12
14
2
3
4
5
6
7
8
9
10
Fig. 9.3 Two examples of clustering of given shapes using Algorithm 37. In each case, we see
the resulting clusters (row-wise) and the corresponding evolution of Q during the algorithm
two representations, we shall cover only one of them in detail, namely, the shape
analysis of elastic curves under the SRVF representation as elements of S2 or Sc
2.
We remind the reader of the underlying framework for an elastic shape analysis
of planar curves introduced in Chap. 5. A parameterized curve β : [0, 1] →R2
is represented by its SRVF q, where q(t) =
˙β(t)
√
| ˙β(t)|. In case ˙β(t) = 0 for some
t, we simply set q(t) = 0 for that t. According to Eq. 5.6, the set of SRVFs of
unit-length curves forms a Hilbert sphere: C2 = {q : [0, 1] →R2|
 1
0 |q(t)|2dt = 1}.
For any q ∈C2, the tangent space Tq(C2) = {v ∈L2([0, 1], R2)| ⟨v, q⟩= 0}. The
shape space is the quotient space of C2 modulo rotation group SO(2) and re-
parameterization group ˜ΓI, i.e., S2 = C2/(SO(2) × ˜ΓI). The elements of S2 are
given by the orbits: [q] = closure{O(q, γ)|(O, γ) ∈(SO(2) × ΓI)}. Since C2 is a
unit sphere in the Hilbert space L2 ≡L2([0, 1], R2), with a natural embedding,
both the extrinsic distance (chord length) and the intrinsic distance (arc length on
the sphere) are easily computable. Also, one can easily project an element of L2
to C2 by rescaling. Therefore, it becomes easier to perform statistical analysis in
C2 using both extrinsic and intrinsic approaches (see Chap. 7). However, we will
restrict ourselves to only the intrinsic approach for performing statistical analysis
on the shape space S2. As described in Chap. 5 (Sect. 5.5), certain computational

9.3 A Finite Representation of Planar Shapes
313
tools are available for analysis on S2 and Sc. For any two curves β1 and β2, with
the SRVFs q1, q2 ∈C2, we can compute the following:
1. Registration: An optimal registration between SRVFs of the two curves us-
ing a combination of Procrustes method (for optimal rotation) and dynamic
programming (for optimal re-parameterization).
2. Geodesic path and geodesic distance: Using an explicit expression, given in
Eq. 5.29, we can compute a constant-speed geodesic α between the two shapes
[q1] and [q2] such that α(0) = [q1] and α(1) = [q2]. We use ds([q1], [q2]) = L[α]
to denote the geodesic distance or length of α.
3. Exponential and its inverse: Given any q1 and q2, we can obtain the shooting
vector from the ﬁrst shape to the second using v = ˙α(0) ∈T[q1](S2). This v is
also termed the inverse exponential map v = exp−1
[q1]([q2]). Similarly, we can also
compute the exponential map exp[q] : T[q](S2) →S2, using:
exp[q](v) =

cos(∥v∥)q + sin(∥v∥)
∥v∥
v

.
(9.2)
In the above equation, if ∥v∥= 0, then the value of the exponential map is
simply [q].
Planar Closed Curves If we restrict ourselves to closed curves, we obtain the
pre-shape space Cc
2, see Eq. 6.5, and the shape space Sc
2, the set of all orbits under
the actions of SO(2)) × ˜
ΓS on Cc
2. The only diﬀerence from representation for
general curves comes from the closure constraint: SRVFs representing closed curves
have to satisfy an additional condition:

S1 q(t)|q(t)|dt = 0. (Notice that the domain
of representation changes from [0, 1] to S1, both for curves and re-parameterization
functions.) This is actually a two-dimensional vector equation implying two scalar
constraints on q. Due to this constraint, the pre-shape space Cc
2 becomes a proper
subset of the Hilbert sphere C2. Consequently, the tangent space is reduced by
having two additional perpendicular elements:
Tμ(Cc
2) = {v ∈Tμ(C2)|v ⊥
 q1(t)
|q(t)|q(t) + |q(t)|e1

and v ⊥
 q2(t)
|q(t)|q(t) + |q(t)|e2

.
(We remind the reader that we set qi(t)
|q(t)|q(t) = 0 whenever q(t) = 0.) Here e1 and e2
are canonical basis elements for R2. The expressions for geodesic paths and geodesic
distances are no longer available explicitly. They are computed using a numerical
path-straightening algorithm (see Sect. 6.6). This algorithm also provides, with
obvious modiﬁcations, tools for computing the exponential map and its inverse.
9.3.2 Finite Shape Representation: Planar Curves
Next we seek a ﬁnite-dimensional, vector-space representation of planar shapes.
Since the shape space is nonlinear and inﬁnite dimensional, we need to locally
ﬂatten this set into a vector space and then restrict to a ﬁnite-dimensional subspace
of this vector space. However, we recall that the shape space does not have a
manifold structure even though the pre-shape space is a manifold. Due to this
situation, we can only view the shape space as a set with a distance inherited from

314
9 Statistical Modeling of Planar Shapes
the pre-shape space. However, in the context of deriving statistical models, and
reaching ﬁnite-dimensional representations of shapes, we will utilize ideas such as
the tangent spaces of orbits and the tangent spaces of the quotient space to help
out conceptually.
Our approach is to take an orthonormal basis of the full tangent space and ﬁnd
a relevant ﬁnite-dimensional subspace. While there are several choices of bases
for a function space, we will start with a basic Fourier basis. Later, in the next
section, we will advocate the use of training data, if available, to ﬁnd an appro-
priate basis using a statistical procedure, say principal component analysis. The
structure of T[μ](S2) is described in Sect. 5.8, speciﬁcally in Eq. 5.36, where we use
its identiﬁcation with Nμ([μ]), which, in turn, is deﬁned by:
Tμ([μ]) ⊕Nμ([μ]) = Tμ(C2)
⊂
L2([0, 1], R2) ,
where Tμ([μ]) = Tμ([μ]SO(2)) ⊕Tμ([μ]ΓI). Figure 9.4 illustrates this decomposi-
tion of tangent spaces into their constituents. Here Tμ([μ]SO(2)) and Tμ([μ]ΓI ) are
spaces tangent to the rotation and re-parameterization orbits of μ, respectively.
Recall that, if a w ∈L2([0, 1], R2) is perpendicular to μ, then it is in Tμ(C2) (since
C2 is a unit sphere). Additionally:
Tμ([μ]SO(2)) = span{u},
u(t) =
 0 −1
1 0

μ(t), and
Tμ([μ]ΓI ) = span{dφγid(vi), i = 1, 2, . . .} .
Recall that φ : ΓI →[μ] is essentially the group action on μ according to
φ(γ) = (μ, γ), and the diﬀerential of φ at γid, dφγid is repeated below in Eq. 9.4
for convenience. In summary, an element w of T[μ](S2) is perpendicular to (i) μ
to respect the unit-length constraint, (ii) dφid(vi)s to be orthogonal to the re-
parameterization orbit, and (iii) u to be orthogonal to the rotation orbit.
We can use this structure to construct elements of the tangent space T[μ](S2). We
will start with the larger space L2([0, 1], R2) and remove these smaller subspaces
to reach the desired set.
Tμ(C2)
[μ]
Tμ([μ])
Nμ([μ])≡T[μ](S2)
Tμ([μ])
Tμ([μ]SO(2))
Tμ([μ]˜Γ)
Fig. 9.4 Left: Decomposition of the tangent space at a pre-shape μ ∈C2 into spaces parallel
and perpendicular to the orbit [μ]. Right: Decomposition of the tangent space parallel to the
orbit into spaces parallel to SO(2) and ˜
ΓI orbits

9.3 A Finite Representation of Planar Shapes
315
• Basis for the full space L2([0, 1], R2): We know that:
Bn = span{ 1
√
2 cos(2πit), 1
√
2 sin(2πit); i = 0, 1, 2, . . ., n},
(9.3)
form a 2n+1-dimensional orthonormal subspace of L2([0, 1], R). Then, Bn ×Bn
forms a convenient orthonormal subspace of L2([0, 1], R2). An element of this
product space can be written as:

 n
i=0( ai,1
√
2 cos(2πit) + bi,1
√
2 sin(2πit))
n
i=0( ai,2
√
2 cos(2πit) + bi,2
√
2 sin(2πit))

, ai,1, ai,2, bi,1, bi,2 ∈R .
• Basis for the tangent space Tμ(C2): Since C2 is simply a unit sphere in-
side L2([0, 1], R2), the space normal to Tμ(C2), inside L2([0, 1], R2), is a one-
dimensional space with the basis element given by μ. Thus, one can take the
elements of Bn × Bn and make them orthogonal to μ, followed by the Gram-
Schmidt algorithm to reach the desired basis.
• Basis for the space tangent to ˜ΓI at γid: The elements of this space are
smooth functions on [0, 1] that are zero on the boundaries. One orthonormal
basis of Tγid(ΓI) is given by:
{sin(2πit)
√
2iπ
, (cos(2πit) −1)
√
2iπ
, i = 1, 2, 3, ..} .
(See Sect. 4.10.2 for a discussion on possible bases for Tγid(ΓI).)
• Space Tangent to the Orbit Tμ([μ]). Here [μ] is the joint re-parameterization
and rotation orbit of μ in C2. Since the group ΓI is dense in the set ˜ΓI, we will
actually use a basis for the Tμ([μ]ΓI) instead.
Algorithm 38. Computation of orthonormal basis of Tμ([μ])
1. Start with a ﬁnite orthogonal basis {v1, v2, . . . , vn} of a subspace of Tγid(ΓI)
for a large value n. Since, in a computer implementation, one works with a
ﬁnite partition of the domain [0, 1], the number n is typically smaller than
the size of this partition.
2. Using Eq. 5.35, we can compute the basis vectors dφid(vi) of the re-
parameterization orbit:
˜vi ≡(dφid(vi))(s) = ˙μ(s)vi(s) + 1
2μ(s)˙vi(s),
s ∈[0, 1] .
(9.4)
3. Compute the function u(t) =
 0 −1
1 0

μ(t).
4. Using Gram-Schmidt (under the standard L2 metric), construct an orthonor-
mal basis for the span of the set {˜v1, ˜v2, . . . , ˜vn, u}. Call these elements
{c1, c2, . . . , ck} where k ≤(n + 1); each one is a function from [0, 1] to R2.
• Basis for T[μ](S2): For each element w in the larger basis, Bn×Bn, perform two
projections. First, use w = w−⟨w, μ⟩to project w into the tangent space Tμ(C2).
Then, use w →˜w = w−k
i=1 ⟨w, ci⟩ci to project the resulting w to the tangent
space T[μ](S2). We apply the Gram-Schmidt algorithm to these basis elements

316
9 Statistical Modeling of Planar Shapes
to reach an orthonormal basis of a ﬁnite-dimensional subspace of T[μ](S2). We
will call this basis set W = { ˜wi, i = 1, 2, . . ., m}, where m = (2n+ 1)2 −(k + 1).
Using a set of coeﬃcients with respect to this basis set W, i.e., v = m
i=1 xi ˜wi ∈
T[μ](S2), we obtain a parametric representation for tangent vectors to the shape
space of planar curves. Under the exponential map at the point μ, the span of
W projects to a ﬁnite-dimensional sphere Sm ⊂S2. Once again, note that since
the elements of W are perpendicular to Tμ([μ]), we can conveniently identify the
exponential map of the span of W as a subset of S2.
Represent : [q] ∈S2
exp−1
[μ]([q])
−−−−−→v ∈T[μ](S2)
Basis W
−−−−−→x ∈Rm
Reconstruct : x ∈Rm Basis W
−−−−−→v ∈T[μ](S2)
exp[μ](v)
−−−−−→[q] ∈S2
9.3.3 Finite Representation: Planar Closed Curves
Here we make the following modiﬁcation from the previous section. The domain of
a function changes from [0, 1] to S1. We use s ∈[0, 1] →t ≡(cos(2πs), sin(2πs)) ∈
S1 to identify the two domains and compose the basis elements in the previous
section with this map to reach basis elements for the new domain.
• Basis for the full space L2(S1, R2): Since the domain of a closed curve
is S1, we adjust the basis elements of the full space accordingly. Let Bn =
span{ 1
√π cos(it),
1
√π sin(it); i = 0, 1, . . . , n}, a 2n + 1-dimensional orthogonal
subspace of L2(S1, R). Then, Bn × Bn forms a convenient orthogonal basis for
a subspace of L2(S1, R2).
• Basis for the tangent space Tμ(Cc
2): This time, the normal space is actually
a three-dimensional space spanned by:
{μ,
 μ1(t)
|μ(t)|μ(t) + |μ(t)|e1

,
 μ2(t)
|μ(t)|μ(t) + |μ(t)|e2

} .
(9.5)
• Space Tangent to ΓS at γid: Now we build a ﬁnite basis for the space tangent
to the rotation and re-parameterization orbits of μ. Let span{v1, v2, . . . , vn} be a
subspace of Tγid(ΓS). The basis functions vi’s essentially span the set of smooth
functions on S1 with no boundary conditions. An orthonormal basis of Tγid(ΓS),
under the ﬁrst-order Palais metric, is given by:
{sin(it)
√πi , cos(it)
√πi , i = 1, 2, 3, ..} .
• Space Tangent to the Orbit Tμ([μ]): This part remains identical to the
previous section and Algorithm 38 applies directly with the above deﬁnitions.
This step results in a basis {c1, c2, . . . , ck} of the Tμ([μ]).
• Basis for T[μ](Sc
2): For each element w in the larger basis, Bn × Bn, per-
form two projections. First, form an orthonormal basis {g1, g2, g3} for the
three-dimensional space spanned by the elements of the set given in Eq. 9.5.
Remove these elements from w using ˜w = w −3
i=1 ⟨w, gi⟩gi. Then, re-
move the basis element of the space tangent to the orbit [μ] according to

9.4 Models for Planar Curves as Elements of S2
317
˜w →˜w = ˜w −k
i=1 ⟨˜w, ci⟩ci to obtain a basis element of the tangent space
Tμ(Sc
2). These basis elements are rescaled so that they form an orthonormal
basis of a ﬁnite-dimensional subspace of Tμ(Sc
2). We will call this basis set
W = { ˜wi, i = 1, 2, . . . , m}, where m = (2n + 1)2 −(k + 3).
9.4 Models for Planar Curves as Elements of S2
Having obtained a (truncated) ﬁnite-dimensional representation space for planar
shapes, we can now develop statistical models to capture their variability.
9.4.1 Truncated Wrapped-Normal (TWN) Model
What kinds of probability models can be imposed on the shape space S2? We will
closely follow the discussion in Chap. 7 where we studied stochastic models for P,
the space of all probability density functions on [0, 1]; ΓI, the group of all warping
functions; and L2/ΓI, the quotient of square-integrable functions modulo the group
of warping functions. As discussed there, there is no canonical way to impose a
uniform density on full S2, except when we restrict to a ﬁnite-dimensional and
bounded subset. Similarly, it is not possible to use an isotropic density on S2 as it
has inﬁnitely many directions and we will not be able to integrate it to reach the
normalization constant. Thus, we go directly to imposing a truncated wrapped-
normal (TWN) density on S2, induced via a corresponding density on a Euclidean
space (say Rm).
We will impose a truncated normal model on the coeﬃcients {xi} and reach a
parametric model on the shape space of planar curves. Similar to Sect. 7.5.3, we
are using T WN(x; K, λ) to denote the density:
T WN(x; Kx, λ) ≡
1
Zm
e−1
2 xT K−1
x
x1|x|≤λ ,
λ < π .
One can add a mean to this model also, if needed. In addition to the threshold
λ, the normalization constant also depends on the dimension m. Throughout this
chapter, we truncate the density at λ = π/2 although one can allow a larger
value, up to π, and still keep the exponential map invertible. A random element
of the space spanned by W can be mapped to the shape space S2 using the ex-
ponential map given in Eq. 9.2. The range space of this mapping is a subset of
the m-dimensional unit sphere Sm. An explicit expression for the resulting density
function on Sm is available:
f(x; μ, Kx)= 1
Zm

θ
sin(θ)
(m−1)
e(−1
2 xT K−1
x
x)1θ≤π/2 , θ= cos−1(
4 m

i=1
xi ˜wi, μ
5
).
(9.6)
This lays out a mechanism for constructing a truncated wrapped-normal (TWN)
probability model on shapes of planar curves.
Shown in Fig. 9.5 are some examples of random samples from a wrapped-
normal model. The ﬁrst shape in each row is the mean shape [μ] and the remain-

318
9 Statistical Modeling of Planar Shapes
Mean
Random Samples
Fig. 9.5 In each row, the leftmost panel shows a mean shape μ and the other panels show
random samples of a truncated wrapped-normal density with mean μ
ing shapes in that row are random samples generated from the wrapped-normal
density on the S2 with that mean. For the ﬁrst row we used m = 9 with
the coeﬃcients xi ∼T N(0, 0.2e−0.3i, π/2), for the second row we had m = 19
with xi ∼T N(0, 0.1e−0.3i, π/2), and for the third row m = 39 with xi ∼
T N(0, 0.1e−0.3i, π/2).
9.4.2 Learning TWN Model from Training Shapes in S2
So far we have used a basic Fourier basis as a starting point for reaching a para-
metric representation of shapes. However, in the situations where we are given a
set of training exemplars, and our goal is to develop a model to capture the shape
variability in the given set, we may be better oﬀextracting a basis from the data
itself. More generally, we may be interested in estimating the TWN model from
the given data or, in other words, estimating the parameters of this model from
the data. There are two parameters to be estimated: the shape mean μ and the
covariance Kx in the tangent space at Karcher mean Tμ(S2). A spectral represen-
tation of this covariance operator also provides a convenient basis for parametric
representation of shapes.
Remark 9.1. We clarify that we do not develop a formal asymptotic theory for
estimating these mean and covariance parameters of TWN model. Instead, we
use the notion of Karcher mean and variance introduced in the previous chapter
to estimate these parameters. A rigorous analysis of these sample estimates, espe-
cially their convergence to the population parameters under asymptotic situations,
remains to be performed.
While the pre-shape space C2 can be embedded in the Hilbert space L2([0, 1], R2)
naturally, it does not seem natural to embed the shape space S2 inside a Hilbert
space. Therefore, we do not pursue the idea of computing extrinsic means and
covariances for shapes of curves. For the purpose of comparison, we will deﬁne and

9.4 Models for Planar Curves as Elements of S2
319
compute the extrinsic mean in C2 and will discuss it in relation to the intrinsic
deﬁnition of mean on S2. Let {β1, β2, . . . , βn} be a given collection of curves, with
shape representations {[q1], [q2], . . . , [qn]}. Considering the SRVFs as elements of
the space L2([0, 1], R2), we can compute their average:
μext(t) = 1
n
n

i=1
qi(t) and project using
μext →
μext
∥μext∥∈C2 .
(9.7)
The resulting μext ∈C2 is the extrinsic mean of curves in the pre-shape space.
Next we look at the problem of computing the sample Karcher mean as speciﬁed
in Deﬁnition 7.1. For the given SRVFs, {q1, q2, . . . , qn}, the sample Karcher mean
is given by:
¯μn = argmin
[q]∈S2
n

i=1
ds([q], [qi])2 ,
(9.8)
where ds denotes the geodesic distance in the shape space S2 under the elastic
metric (Eq. 5.28). A gradient-based approach for ﬁnding a Karcher mean on a
general Riemannian manifold is given in Algorithm 24. Its modiﬁcation to include
the truncation and its adaptation to S2 is presented here.
Algorithm 39 (Truncated Karcher Mean on S2). Let μ0 be an initial esti-
mate of the Karcher mean. Since the qis are elements of a hypersphere C2, it seems
natural to use their extrinsic mean (Eq. 9.7) to initialize the gradient algorithm.
Other choices may also be available in speciﬁc contexts. Set j = 0.
1. For each i = 1, . . . , n, compute the tangent vector vi such that vi = exp−1
[μj]([qi]).
This can be done as follows. First ﬁnd the optimal rotation and the optimal
re-parameterization of qi to best match μj; call it ˜qi. Then, compute the inverse
exponential (using Eq. 5.18):
vi =
θi
sin(θi)(˜qi −cos(θi)μj),
θi = cos−1(⟨˜qi, μj⟩) .
2. Discard those vectors whose magnitude is larger than π/2 (to facilitate trunca-
tion). Compute the average of the remaining vectors: ¯v =
n
i=1 vi1∥vi∥<π/2
n
i=1 1∥vi∥<π/2 .
3. If ∥¯v∥is small, then stop. Else, update μj in the direction ¯v using (Eq. 5.17):
μj+1 = cos(ϵ∥¯v∥)μj + sin(ϵ∥¯v∥) ¯v
∥¯v∥
where ϵ > 0 is small step size, typically 0.5.
4. Set j = j + 1 and return to Step 1.
We present an example to illustrate this computation. First, take the set of nine
shapes of glasses formed by the open curves shown in Fig. 9.6. Figure 9.7 shows the
results of Algorithm 39 for four diﬀerent initial conditions—once with the extrinsic
mean (Eq. 9.7) shown in the top row, twice with individual shapes from the original
sets (the bottom two rows), and once with a completely arbitrary shape (second
row). The ﬁnal estimate of the mean shape is similar in all cases. If we look at the
variance function that is being minimized here, the ﬁnal value is close to 0.001,
which was the stopping criterion for the iteration.

320
9 Statistical Modeling of Planar Shapes
Fig. 9.6 A sample set of nine curves, drawn individually and all together (bottom right)
Initial
Iteration 1
Iteration 2
.......
Final
Energy
......
0
5
10
15
20
0
0.005
0.01
0.015
0.02
0.025
......
0
5
10
15
0
0.2
0.4
0.6
0.8
1
......
0
5
10
15
20
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
......
1
2
3
4
5
6
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
Fig. 9.7 Estimation of Karcher mean of the nine shapes shown in Fig. 9.6. Each row corresponds
to a diﬀerent run for Algorithm 39 with diﬀerent initial condition. The left panel shows the initial
shape and the subsequent panels show some iterations along the ﬁnal estimate. The last panel
shows the change in the cost function versus iteration index
Data
Mean
Data
Mean
Fig. 9.8 Examples of sample Karcher means of 10 handwritten signatures each, for two persons
Figure 9.8 shows an example of computing the truncated sample mean, this time
of signature curves for two persons. For each pair, the left panel shows the sample
shapes and the right panel shows the corresponding truncated mean shapes.
Once the sample Karcher mean has been computed, the evaluation of the
Karcher covariance follows. Let vi = exp−1
[μ]([˜qi]), i = 1, 2, . . . , n be the shoot-
ing vectors generated by the above algorithm. If their magnitude is greater than

9.4 Models for Planar Curves as Elements of S2
321
π/2, then they are automatically discarded. The covariance kernel can be deﬁned
as a function K : [0, 1] × [0, 1] →R given by:
K(s, t) =
n
i=1 vi(s)vi(t)1∥vi∥<π/2
n
i=1 1∥vi∥<π/2
.
Note that the trace of this covariance function is proportional to the Karcher
variance, i.e.:
 1
0
K(t, t)dt =
n
i=1 ds([μ], [qi])21ds([μ],[qi])<π/2
n
i=1 1∥vi∥<π/2
.
In practice, since the curves have to be sampled with a ﬁnite number of points, say
T , the resulting covariance matrices are ﬁnite dimensional. Often the observation
size n is much less than T and, consequently, n controls the degree of variability in
the stochastic model. In other words, n dictates m, the dimension of the subspace
(of T[μ](S2)) on which the Gaussian model is eventually imposed.
Principal Component Basis for Shape Representation In the previous sec-
tion, we used a generic basis for the tangent space T[μ](S2); we started with the
Fourier basis for L2([0, 1], R2) and made its components orthogonal to the subspace
Tμ([μ]). However, in the case of learning shape models from the observations, one
can reach a more eﬃcient basis for T[μ](S2) using the traditional principal compo-
nent analysis, as described next.
With a slight abuse of notation, let vi ∈R2T denote the tangent vector function
vi(t) sampled at T points and the components concatenated to form a tall vector.
Then, the sample covariance matrix ˆK =
1
n−1
n
i=1 vivT
i is a 2T × 2T symmetric,
non-negative matrix with the singular value decomposition ˆK = UΣU T. We will
assume that the elements of Σ have been arranged in a non-increasing order from
top left to bottom right. Then, a multivariate Gaussian model for the tangent
vector v ∈R2T is given by:
v =
n

i=1
xiUi,
(9.9)
where xi ∼T N(0, Σii) (independently) and Ui are columns of U matrix. One can
rearrange the elements of v ∈R2T (to undo the earlier concatenation) and form
an approximation of a corresponding element of the tangent space Tμ(S2); call
it v. This procedure leads to a basis {Ui}, which is typically more eﬃcient than
the earlier Fourier construction. Note that, by deﬁnition, these Uis are orthogonal
to the subspace Tμ([μ]) and do not require any further orthogonalization. Also,
these Uis are precisely in the directions where the actual variability of shapes lies.
This random v can then be projected in the shape using the exponential map
v →expμ(v) (Eq. 9.2) to obtain a random shape. This provides a technique for
sampling from the wrapped-Gaussian model on S2. The functional form of this
density is the same as that given in Eq. 9.6, except the basis W used there is
replaced by the basis U constructed here.
We illustrate this idea using the nine shapes shown in Fig. 9.6; we have already
computed their Karcher mean shape earlier. Now we can project these nine shapes
into the tangent space Tμ(S2) and compute the sample covariance matrix. In this
implementation, these curves are sampled using T = 100 points each. Therefore,
the sample covariance matrix is of size 200 × 200 although its rank is much lower

322
9 Statistical Modeling of Planar Shapes
0
2
4
6
8
10
0
0.5
1
1.5
2
Fig. 9.9 Top: Karcher mean (left) and the singular values of the sample covariance matrix
(right) for the nine wineglass shapes. Bottom two rows: Random samples from the wrapped-
Gaussian density on S2 whose parameters (mean and covariance) were estimated from the given
nine shapes
since we use only nine independent samples. Figure 9.9 top row shows a plot of
the singular values Σii in the decreasing order. Now, using these singular vectors
and the singular values, we deﬁne a normal model on the tangent space Tμ(S2)
according to Eq. 9.9. Wrapping these random vectors onto the shape space S2,
using the exponential map, results in random shapes from this wrapped-Gaussian
model. Some of these random shapes are shown in the bottom two rows of Fig. 9.9.
A visual comparison of these random shapes with the original nine shapes in
Fig. 9.6 tells us that the model is not only successful in capturing the variability
in the original set; it generates realistic random shapes.
Figure 9.10 shows the variations of shapes along the principal geodesic curves
around the mean shape μ. Each of the three rows show shapes along the paths
t →expμ(±2t√ΣiiUi) for i = 1, 2, and 3. These plots start from t = −1 on the
left and increase t until we reach +1 on the right. The middle shape in each row
is the mean μ that corresponds to t = 0. The three rows capture the three main
“directions” of deformations (from the mean) in the original nine curves.
9.5 Models for Planar Closed Curves
The next step is to study the statistics of shapes of planar-closed curves using the
elastic representation. For comparisons, however, we will also provide results from
the non-elastic representation, without providing the full details. We will focus
on learning of TWN model and its parameters: mean, covariance, and the basis
elements, from the training data itself. We will assume that we have some training
curves in several shape classes and our goal is to develop and study some stochastic

9.5 Models for Planar Closed Curves
323
μ
2
−
μ
2
−
U1
μ
μ
μ
√
Σ11
U2
√
Σ22
μ
2
−
U3
√
Σ33
μ
2
+
μ
2
+
U1
√
Σ11
U2
√
Σ22
μ
2
+
U3
√
Σ33
Fig. 9.10 Principal geodesic curves: the three rows show the paths t →expμ(±2√ΣiitUi) for
i = 1, 2, and 3, where Uis are the principal singular vectors of the sample covariance matrix
models that capture shapes in those training sets. Given a set of closed curves, we
ﬁrst want to compute its Karcher mean and then use that mean for deﬁning the
tangent space on which the covariance is computed.
1. Sample Karcher Mean: The algorithm for computing the Karcher mean of
elastic closed curves is given below:
Algorithm 40 (Sample Karcher Mean in Sc
2).
Set k = 0. Choose some time increment ϵ ≤1
n. Choose a point [μ0] ∈S2 as an
initial guess of the mean. (For example, one could just take μ0 = q1.)
a. For each i = 1, . . . , n, choose the tangent vector vi ∈T[μk](Sc
2) that is tangent
to the geodesic from [μk] to [qi], and whose norm is equal to the length of
this shortest geodesic. This is accomplished using Algorithm 22 presented in
Chap. 6.
b. Compute the shooting vector ¯v = 1
n
n
i=1 vi ∈T[μk](Sc
2).
c. Flow for time ϵ along the geodesic that starts at [μk] and has velocity vector
¯v. Call the point where you end up [μk+1], i.e., [μk+1] = exp[μk](ϵ¯v). This is
implemented numerically using Algorithm 20.
d. Set k = k + 1 and go to Step 1.
Figure 9.11 shows examples of Karcher means of some sets of shapes. For each
example, shown in a separate row, the given shapes are shown in the left panel
and their Karcher mean is shown in the right panel. A visual inspection of
these mean shapes reveals that although the means appeared to have been
smoothed out, they still retain the important geometric features characterizing
the original sets. In the case of human running silhouettes, the average shape
has distinct body parts such as head, legs, feet, etc. Similarly, the mean crown
shape retains the ﬁve crown tips that are present in each of the given crown
shapes; these tips have diﬀerent sizes and locations in the individual shapes

324
9 Statistical Modeling of Planar Shapes
...
..
Fig. 9.11 Karcher mean of elastic shapes. The sample shapes are shown in the top three rows,
while the Karcher mean and the evolution of the norm (gradient) are shown in the bottom row
but the process of elastic matching allows them to be matched and averaged
together for computing the Karcher mean.
2. Sample Karcher Median: In situations where there is an outlier present in
the sample set, it has the possibility of inﬂuencing the sample mean. Therefore,
as an alternative, one can compute the sample median as a more robust estimate
of the population mean shape. The algorithm for computing the Karcher median
of elastic curves is given below:
Algorithm 41 (Sample Karcher Median in Sc
2).
Set k = 0. Choose some time increment ϵ ≤1
n. Choose a point [μ0] ∈S2 as an
initial guess of the median.
a. For each i = 1, . . . , n, ﬁnd the shooting vector (i.e., tangent vector vi ∈
T[μk](Sc
2) that is tangent to the geodesic) from [μk] to [qi], and whose norm
is equal to the length of this shortest geodesic. This is accomplished using
Algorithm 22 presented in Chap. 6.
b. Let di = ∥vi∥∈R be the norm of the shooting vectors. Then, deﬁne the
update direction to be ¯v =
n
i=1 vi/di
n
i=1 1/di ∈T[μk](Sc
2).
c. Flow for time ϵ along the geodesic that starts at [μk] and has shooting vector
¯v. Call the point where you end up [μk+1], i.e., [μk+1] = Ψ([μk], ϵ, ¯v). This
is implemented numerically using Algorithm 20.
d. Set k = k + 1, and go to Step 1.
In order to demonstrate the robustness of Karcher median shape to outliers,
we perform the following experiment. We take 12 shapes from a set of car
silhouettes (some examples are shown in the top row on Fig. 9.12) and estimate
their sample means by changing the sample sets. Starting with 12 cars, we add
replace 2 cars by 2 runner shapes (examples in the second row of Fig. 9.12) in
successive experiment. The sample mean shapes are shown in the right column
of Fig. 9.13. As more and more cars are replaced by runners, the eﬀect on the
sample mean starts growing and ﬁnally, in the last row, the average shape is

9.5 Models for Planar Closed Curves
325
...
...
Fig. 9.12 Database of shapes used in comparing mean and median shapes
Set
Median
Median Energy
Mean
Mean Energy
12 cars
0
5
10
15
0
5
10
15
0
5
10
15
0
5
10
15
0
5
10
15
0
5
10
15
0
5
10
15
0
5
10
15
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0
0.5
1
1.5
10 cars + 2 runners
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
8 cars + 4 runners
0
0.5
1
1.5
2
2.5
3
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
6 cars + 6 runners
0
0.5
1
1.5
2
2.5
3
3.5
4
0
0.5
1
1.5
2
2.5
Fig. 9.13 Median and mean shapes obtain for diﬀerent combinations of cars and runner shapes
taken from the set shown in Fig. 9.12. Cars are the main set and runners are considered outliers
far from a car shape. In contrast, consider the corresponding median shapes
shown in the left panel of this ﬁgure. Despite an increasing number of outliers,
the median shape is found to be relatively more stable and better resembles a
car silhouette.
3. Covariance Estimation: Once we have an estimate of the Karcher mean, the
tangent space at that point is used to deﬁne and compute a covariance matrix
for the given sample. Let fi = exp−1
[μ]([qi]) be the projection of the ith shape in
the tangent space T[μ](Sc
2). This is actually implemented using the algorithm for
ﬁnding geodesics between given shapes in Sc
2. Also, since the implementation
involves using discrete set of points on curves, the covariance matrices are of
size 2T × 2T , where T is the number of points sampled on each curve. Let ˆK
be a covariance matrix for a shape class and let ˆK = UΣU T be its singular
value decomposition. Assuming that the singular values in Σ are arranged in
a non-increasing order, the corresponding singular vectors U1, U2, . . . , denote
the principal directions of variations for that class. These directions can be
mapped back to the shape space using the exponential map, and the result-
ing geodesic curves are called the principal geodesic curves. For instance, the

326
9 Statistical Modeling of Planar Shapes
μ −2
√
Σ11 U1
μ
μ + 2
√
Σ11 U1
μ −2
√
Σ22 U2
μ
μ + 2
√
Σ22 U2
μ −2
√
Σ33 U3
μ
μ + 2
√
Σ33 U3
Fig. 9.14 Display of shape variations around the mean in the tangent eigen directions. The ﬁrst
row shows the ﬁrst principal direction, the second row shows the second direction, and so on
curve t →exp[μ](t√Σ11Ui) denotes the ﬁrst principal geodesic curve of that
class. Figure 9.14 shows the three top principal geodesic curves associated with
the shape class containing wine glasses. As we move along the ﬁrst principal
geodesic, the main deformation in the shape is the bending up and down of
the rim of the glass. The stem and the base of the glass remain unchanged.
The second principal geodesic curve, on the other hand, changes the shape of
the step—bending it left and right—while keeping the rim and the base un-
changed. It is interesting to note how these principal directions (and the asso-
ciated principal geodesics) capture certain dominant yet unrelated components
of the deformations present in the original data.
As another example, Fig. 9.15 shows the principal geodesic curves associated
with the given runner silhouettes. The deformations associated with the three
principal directions make an interesting study. The ﬁrst principal geodesic curve
shows a deformation that bends and straightens the two legs—as if to capture a
stride. The bending of the front leg is much more pronounced than that of the
hind leg. In case of the second principal geodesic, the bending of the hind leg is
much more signiﬁcant than that of the front leg. The third principal geodesic
keeps the legs ﬁxed while changing the posture of the back. Since the movement
of legs is much more than the back, the deformation along the third principal
geodesic is relatively small.
4. TWN Shape Model: Now we have a simpliﬁed representation of shapes—each
shape q is represented by a set of coeﬃcients with respect to an orthonormal
basis {U1, U2, . . . , Un} in the tangent space Tμ(Sc
2). In other words:
q ≡expμ(
n

i=1
xiUi) .
(Using Algorithm 20.)
(9.10)
In order to specify a stochastic model on q, it is suﬃcient to specify a model on
the coeﬃcients x = {x1, x2, . . . , xn}.

9.6 Beyond TWN Shape Models
327
μ −2
√
Σ11U1
μ
μ + 2
√
Σ11U1
μ −2
√
Σ22U2
μ
μ + 2
√
Σ22U2
μ −2
√
Σ33U3
μ
μ + 2
√
Σ33U3
Fig. 9.15 Display of shape variations around the mean in the tangent eigen directions. The ﬁrst
row shows the ﬁrst principal direction, the second row shows the second direction, and so on
9.6 Beyond TWN Shape Models
In the previous sections, we have suggested the use of independent normal den-
sities for ﬁnite-dimensional representations of shapes. Although this choice seems
convenient for analysis and simulation, is it actually supported by the data? To
answer this question, we study the observed statistics of the principal coeﬃcients
for some biological cell shapes. This application, coming from image cytometry, is
concerned with measuring shapes of biological cell populations in vitro using elec-
tron microscopy images of blood samples. This technique is used in cancer studies,
drug screening, and genomic research, among other applications.
Shown in the top panel of Fig. 9.16 are 54 examples (out of a set of 100 used in
this experiment) of cell contours extracted automatically from electron microscopy
images. These cells are characterized by elongated spindle-like structures with 2, 3,
4, or even more corners. Shown in the bottom row of that ﬁgure is the mean shape
of these 100 samples—it shows similar elongated structure with two prominent
corners and a third smaller corner. The right panel shows a plot of the eigenvalues
of the covariance matrix. One can see that most of the variability can be captured
using 20–25 principal components. It should be noted that these contours have
100 points on them.
Using the principal components of the covariance matrix, we obtain the observed
values of the principal coeﬃcients. Seeking a nonparametric model, we estimate
the underlying probability density for each coeﬃcient independently using a kernel
density estimator. This assumption of independence may not be appropriate but
the task of estimating the joint density nonparametrically is quite diﬃcult. The
results are presented in Fig. 9.17 where we show the estimated pdfs for the top ten
principal coeﬃcients. In this case, most of the estimated pdfs are unimodal and
can pass the test for normality. One of them (the dominant one) has two modes,
and some are heavily skewed in one direction. In general situations where TWN
does not apply, we can use the following alternatives.

328
9 Statistical Modeling of Planar Shapes
0
5
10
15
20
25
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
Fig. 9.16 Sample Karcher mean (bottom left) of a set of cell shapes shown at the top. The
bottom right shows the evolution of the sum of squared distances during the mean computation
-10
-5
0
5
10
0
0.05
0.1
0.15
-6
-4
-2
0
2
4
6
-6
-4
-2
0
2
4
6
0
0.05
0.1
0.15
0.2
0.25
0.3
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
-4
-2
0
1
2
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
-5
-4
-3
-2
-1
0
1
2
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
-3
-2
-1
0
1
2
3
-3
-2
-1
0
1
2
3
-3
-2
-1
0
1
2
3
-3
-2
-1
0
1
2
3
-3
-2
-1
0
1
2
3
0
0.1
0.2
0.3
0.4
0.5
0.6
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0
0.1
0.2
0.3
0.4
0.5
0.6
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Fig. 9.17 Kernel density estimates for the top ten principal coeﬃcients of the covariance matrix
for NIH cell data

9.7 Modeling Nuisance Variables
329
Fig. 9.18 Random samples from a wrapped-Gaussian model (left) and the independent non-
parametric model (right) for cell shapes
1. Nonparametric Model: In this case, we assume that each coeﬃcient is an
independent random variable with the probability given by the nonparametric
estimate of its pdf from the training data. Let the estimated pdfs be given
by fi and we model the coeﬃcient zi ∼fi. The joint density function of the
coeﬃcients is given by f(z1, z2, . . . , zn) = -n
i=1 fi(zi). The shape variable is
generated using the mapping (z1, z2, . . . , zn) →q ≡expμ(n
i=1 ziUi). In order
to simulate from this model, we can simply simulate each coeﬃcient zi from its
density function fi and then reconstruct the resulting SRVF q using the above
map. Then, we can integrate q to form coordinates of the random shapes: β(t) =
 t
0 q(s)|q(s)|ds. Figure 9.18 (right) shows some examples of the samples from
this nonparametric model learned from the training shapes shown in Fig. 9.16.
2. Mixture of Gaussian Model: The other idea, to handle the non-Gaussian
nature of the principle coeﬃcients, is to model them as mixtures of Gaussians.
As the plot for the estimated density of ﬁrst principal coeﬃcient, shown in
Fig. 9.17, suggests, one should be able to model it using two components in
the mixture. More generally, each coeﬃcient is modeled independently and the
parameters of the components (mean, variance, and proportion) can be diﬀerent
for diﬀerent coeﬃcients. Formally:
zi ∼fi ≡wi,1N(μi,1σ2
i,1) + wi,2N(μi,2σ2
i,2) .
Estimation of parameters {μi,j, σi,j, wi,j|i = 1, 2, . . ., n, j = 1, 2} is performed
using the standard expectation maximization (EM) algorithm.
9.7 Modeling Nuisance Variables
We have discussed statistical models for shapes of continuous curves in the pre-
vious three sections. In some problems, one also needs to model all the nuisance
variables that were so meticulously removed in the shape analysis. An example
of this situation is when we develop a model for the full curve, and not just its
shape. The nuisance variables can in general be translation, rotation, scale, and
re-parameterization. Since translation and scaling can be removed and parame-
terization reintroduced in a relatively simple manner, we restrict our discussion
to modeling the two remaining variables. We start with the re-parameterization
variability.

330
9 Statistical Modeling of Planar Shapes
9.7.1 Modeling Re-Parameterization Function
We start by considering models for elements of ΓI (one can treat ΓS similarly)
that can be used to re-parameterize curves. Since ΓI is inﬁnite dimensional, it is
not possible to use standard probability models on this set and we will restrict
ourselves to a ﬁnite-dimensional subset. The other issue, albeit a minor one, is
that ΓI is an aﬃne space and one needs to take that into account. In fact, if we
use SRVF representation for elements of ΓI, as suggested in Sect. 7.5.4 earlier,
then the resulting space is actually nonlinear (an orthant of a unit sphere) and we
will need to use the geometry of a sphere to model a re-parameterization function.
Since, for any γ ∈ΓI, ˙γ is a proper pdf, we can use this relation to implicitly
impose a model on γ. Therefore, a natural ﬁnite-dimensional subset is to ﬁnd
a parametric family of pdfs on [0, 1], and to impose probability models on the
corresponding parameters. Some examples are:
1. Power Family: A simple one-parameter family of pdfs on [0, 1] is pa(t) = ata−1,
for a > 0, and the corresponding re-parameterization function γa(t) = ta, a > 0.
We can make γa random by assuming a ∼exp(θ), exponential random variable
with mean θ > 0, or a scaled-gamma density. Figure 9.19 shows a set of 20
random samples of γa generated using a scaled-gamma density for a with the
shape parameter set to 1.0 and the scale parameter being 10.0.
2. Beta Family: Another possibility, this time a two-parameter family, is the beta
density pa,b(t) = ta(1−t)b
B(a,b)
and the corresponding re-parameterization function
γa,b(t) =
 t
0 pa,b(s) ds. The pdfs in this family are all unimodal with the location
of the mode decided by the mode parameters. When b = 0, the pdf reduces
to the one in the previous item. We can now make γa,b random by imposing
probability models on a and b. Similar to the previous case, we can assume a
and b to be random variables, e.g., exponential or scaled gamma. Figure 9.19
shows 20 random samples from Beta family with a, b being scaled gamma with
shape and scale parameters being 1.0 and 10.0, respectively.
3. Truncated Normal Distribution: Since the normal distribution is deﬁned
over R, we can restrict it to [0, 1] by truncation and obtain a desired pdf. The
general expression for this pdf is given by, for μ ∈(0, 1), σ > 0, and t ∈[0, 1],
pμ,σ(t) =
1
σ φ( t−μ
σ )
Φ( 1−μ
σ ) −Φ( −μ
σ ) ,
φ(t) =
1
√
2π exp(−1
2t2),
Φ(t) =
 t
−∞
φ(s) ds .
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Power
Truncated generalized
Laplacian
Truncated normal
Beta
Fig. 9.19 Random simulations of γ from diﬀerent parametric families

9.7 Modeling Nuisance Variables
331
The resulting re-parameterization function is given by:
γμ,σ : [0, 1] →[0, 1],
γμ,σ(t) = Φ( t−μ
σ ) −Φ( −μ
σ )
Φ( 1−μ
σ ) −Φ( −μ
σ ) .
For sharper peaks in the pdfs, one can also use the truncated Laplacian density
and a special case of that is the truncated double exponential density.
4. Other Families: There are a number of other parametric families of pdfs on
[0, 1] that can also be used for this purpose. The key is a choose family that
allows diﬀerent numbers, locations and, heights of the modes in pdfs, resulting
in a rich family of re-parameterization functions. For instance, one can use a
truncated mixture of normal densities. The diﬃculty in choosing such complex
families stems from the challenge in estimating the warping parameters from
the given data.
Distance-Based Model So far we have looked at the parametric families, but
one may need a larger class of re-parameterization functions in practical situations.
One can try for a nonparametric approach albeit with some probabilistic structure.
One nonparametric idea is to impose a Gaussian-type distribution (on a ﬁnite
dimensional subset of Γ) of the form:
p(γ|γ0) ∝e
−
1
2σ2s dγ(γ,γ0)2
,
(9.11)
where dγ is the Fisher-Rao distance on ΓI and γ0 is the chosen central point. This,
of course, is not a proper pdf since, in general, its integral may not be deﬁned.
Nevertheless, one can use this as an improper pdf, especially in situations where
sampling from a pdf does not require the knowledge of the normalization constant.
As stated in Sect. 4.10.2, the expression for the Fisher-Rao distance on ΓI is simply
dγ(γ1, γ2) = cos−1(
√˙γ1, √˙γ2

). We discuss two possibilities for γ0:
1. Uniform Sampling: The simplest possibility is to emphasize the samplings
of a curve that are uniform with respect to its arc-length parameterization by
choosing γ0(s) = s.
2. Curvature-Based Sampling: Alternatively, γ0 may depend on local geomet-
rical properties; for example, the sampling density may increase with increasing
curvature of the underlying curve β. Deﬁne E(s) =
 s
0 exp(|κ(s)|/ρ) ds′, where
κ(s) is the curvature of β at arc-length parameter point s′ and ρ ∈R+ is a
constant. The ratio γI(s) = E(s)/E(1) is a diﬀeomorphism, from [0, 1] to itself,
and the desired sampling for that curve is γ = γ−1
I . The inverse of γI can be
numerically estimated using a spline interpolation. To deﬁne a single γ0 for each
class, we use training curves, as follows. First we compute γq for each training
curve, and then, using the techniques presented in Sect. 7.5.4, we compute their
Karcher mean, which we use as γ0. Here we use the Karcher variance for σ2
s.
For this computation, the training curves are aligned, something which is done
automatically when geodesics are computed between the shapes of the curves.
We now illustrate these ideas with some examples.
Remark 9.2. It is interesting to note that the re-parameterization variability is
closely related to variability in the way curves can be discretized. A discretization
requires two items: the number of points and their placements on the curve. It is

332
9 Statistical Modeling of Planar Shapes
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
a
b
c
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Fig. 9.20 Curvature-driven sampling: (a) a curve and its smoothed version, (b) the curvature
function κ(s) and the corresponding parameterization function γ(s), and (c) curve samples using
γ(i/n)
the placement part that can be controlled by re-parameterization, as follows. Fix
the number of points to be n and start with a uniform partition of the [0, 1] in n
subintervals. If we parameterize a unit-length curve β by its arc length, then these
points {β( i
n)|i = 0, 1, 2, . . ., n} form a uniform discretization of β. However, for
any γ ∈ΓI, if we select the points {β(γ( i
n))|i = 0, 1, 2, . . ., n}, this represents a
diﬀerent discretization of β. Thus, one can control discretization of curves using
piecewise-linear re-parameterization functions.
Shown in the left column of Fig. 9.20, column (a), are two contours. We smooth
these curves using Gaussian ﬁlters and their smoothed versions are drawn on top
of them. For these smoothed curves, we compute the curvature function κ and then
E(s). These functions are displayed in (b), along with the resulting γI’s. Column
(c) shows the original curves sampled using the resulting γI. Figure 9.21 shows
some examples of class-speciﬁc means of the γI’s for two classes. By using these
means as γ0 for each class, we can form class-speciﬁc priors of the form given in
Eq. 9.11.
To simulate random samples from distance-based models (of the form given in
Eq. 9.11), we take the following steps: Deﬁne ψ0 = √˙γ0.
Algorithm 42 (Random Sampling of γ from a Gaussian-Type Model).
1. Form an orthogonal basis of the tangent space Tψ0(S∞) as follows. We know
that Tψ0(S∞) = {v ∈L2([0, 1], R)| ⟨v, ψ0⟩= 0}. So, we can start with any basis
of L2([0, 1], R) and make it orthogonal to ψ0 to reach a basis {b1, b2, . . . , } for
the tangent space Tψ0(S∞).
2. Generate a set of coeﬃcients from i.i.d. normal distribution N(0, σ2
s) and use
the chosen basis to construct an element f(t) = l
i=1 cibi(t) ∈Tψ0(S∞), for a
large enough l.

9.7 Modeling Nuisance Variables
333
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
Fig. 9.21 Each row shows two examples of training curves in a class, the sampling functions
for that class, and their Karcher means
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Fig. 9.22 Random samples generated using Algorithm 42 with σ2
s increasing from left to right
3. Finally, use the exponential map on S∞, according to ψ = cos(|c|)ψ0 +
sin(|c|)f/|c|.
4. The random re-parameterization function is then given by γ(t) =  t
0 ψ(s)2 ds.
Figure 9.22 shows some examples of random simulations from such a class-speciﬁc
prior density for increasing values of σ2
s. If σs is too large, then many of the sampled
points will lie outside the desired set since ψ can become negative. Such points
may, however, simply be rejected from the samples, thus preserving the form of the
density given above. For eﬃciency’s sake, though, the proportion of such points
should not be too large, and this implies a constraint on σs.
9.7.2 Modeling Shape Orientations
For a planar shape, its orientation in a ﬁxed coordinate system is given by a
rotation matrix O ∈SO(2). Since SO(2) ≡S1, one can impose a probability
model on S1 to capture the rotation variability of placing a shape in a scene.
The are several choices available for imposing probability distributions on S1, and
some of them are discussed in Sect. 7.5.1. The two main cases discussed there are
the uniform density and the von Mises density. The parameters of the von Mises
density and their estimation are described previously.

334
9 Statistical Modeling of Planar Shapes
9.8 Classiﬁcation of Shapes With Contour Data
One of the main reasons for developing statistical models for shapes and associated
variables is their use in classiﬁcation of previously unobserved shapes. The frame-
work for shape representations and statistical models on shape spaces developed so
far has important applications in decision theory. Statistical models of shapes can
be used to make a variety of decisions such as: Does this shape belong to a given
family of shapes? Do the given two families of shapes have similar means and/or
variances? Given a test shape and several competing probability models, which
one explains the test shape better? These questions can be addressed using TWN
and other models within the framework of decision theory. Next, we are going to
develop techniques for answering these questions. In this section, we will assume
that the data is already available in the form of planar, closed curves. Therefore,
we can directly use shape models developed for the shape space Sc
2 and apply them
in classiﬁcation.
We shall use several examples to demonstrate our methods. One of the datasets
we will use is that of several civilian vehicles viewed from diﬀerent horizontal
views. These vehicles—cars, vans, and pickup trucks—are rotated horizontally
and are imaged from a ﬁxed camera location. We extract their silhouettes from
the resulting images and study the problem of classifying these vehicles by only
using the shapes of these contours. There are 10 vehicles used in this experiment
and labeled as Avalon, Camry, Jeep93, Jeep99, Maxima, MazdaMPV, Mitsubishi,
Sentra, Tacoma, TaurusSE96, and Civic4dr. Some examples of their contours are
shown in Fig. 9.23 where we display the silhouettes of each of the 10 vehicles
Fig. 9.23 Shapes of silhouettes of 10 vehicles from three diﬀerent poses: 0◦,
50◦,
90◦, and
180◦. The vehicles are Avalon, Camry, Jeep93, Jeep99, Maxima, MazdaMPV, Mitsubishi, Sentra,
Tacoma, TaurusSE96, and Civic4dr

9.8 Classiﬁcation of Shapes With Contour Data
335
from four viewing angles: 0◦(sideways), 50◦, 90◦, and 180◦. For each vehicle, we
obtain a total of 36 silhouettes, taken from viewing angles separated by 10◦. In the
following, we will use the notation i(j) to denote a proﬁle where i = 1, 2, . . . , 10 is
the vehicle index and j = 1, 2, . . ., 36 is the viewing angle index.
We establish some notation for the classiﬁcation problem. Let Ci, i = 1, 2, . . . , 10
denote the ten shape classes and let qtest be the SRVF of a test silhouette. We
deﬁne a classiﬁer to be a function that assigns a class Ci to the test silhouette.
More precisely, deﬁne a classiﬁer to be the mapping ζ : Sc
2 →{Ci|i = 1, 2, . . ., 10}.
9.8.1 Nearest-Neighbor Classiﬁcation
In cases where the feature space is a metric space, i.e., one can compute a distance
between any two values of the chosen feature; one of the simplest classiﬁers to
implement is the nearest-neighbor classiﬁer. Here one takes the feature of the
test object; compares with all the elements in the training set, using the distance
function; and ﬁnds the nearest training objects. Then, the class associated with
the nearest object is selected for classifying the test object.
We particularize this method to the classifying of contours according to their
shapes and will use the distance function for elastic closed curves (in Sc
2) for
comparing shapes. Mathematically, the nearest-neighbor classiﬁer is deﬁned to be
a mapping ζnear : Sc
2 →{Ci|i = 1, 2, . . ., 10} according to:
ζnear([qtest]) = Cˆi,
where ˆi = argmin
i

min
j
ds([qtest], [qi(j)])

.
(9.12)
Here qi(j) denotes the SRVF of the jth silhouette of the ith vehicle. If, instead
of ﬁnding the nearest shapes, we select k-nearest shapes, we obtain a k-nearest-
neighbor (kNN) classiﬁer.
We present an example of this idea in Fig. 9.24. In this experiment, we choose
the even-numbered proﬁles (2, 4, 6, . . ., 36) of each vehicle as the training set and
the odd-numbered proﬁles (1, 3, 5, . . ., 35) as the test set. For each test shape, we
search over the even-numbered proﬁles for each object and select the ﬁve (k = 5)
closest shapes in the training set. In Fig. 9.24, we show several examples this
classiﬁcation. In each row, we ﬁrst show the test shape and then show the ﬁve
nearest shapes in the order of increasing distances.
With the same experimental setting, i.e., the same test and training data, we can
evaluate the average classiﬁcation performance of the nearest-neighbor classiﬁer.
In Fig. 9.24 (bottom), we present the change in the classiﬁcation performance of
the k-NN classiﬁer as k changes from one to 20.
9.8.2 Probabilistic Classiﬁcation
Another possibility for shape classiﬁcation is to use a model-based classiﬁer. For
instance, we can deﬁne a probability distribution associated with each shape class
and for a given test shape ﬁnd the class that maximizes the value of the likelihood
function. To understand this idea for shape analysis, let us start with a binary

336
9 Statistical Modeling of Planar Shapes
2(9)
10(8)
1(30)
1(8)
10(30)
5(30)
9(33)
9(6)
9(32)
9(34)
9(4)
9(8)
2(33)
2(4)
2(34)
5(32)
7(34)
7(4)
3(19)
3(18)
3(20)
6(20)
6(18)
4(18)
0
5
10
15
20
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
Fig. 9.24 First four rows: In each row, we ﬁrst show a test shape and then its ﬁve nearest
neighbors using the elastic distance in Sc
2. The number below each shape denotes the vehicle
index (pose index). Bottom: The classiﬁcation performance of the k-NN classiﬁer, versus k, for
the 10 class vehicle classiﬁcation problem
problem. The goal now is to use the statistical models of shape variations within
two shape populations (classes) to classify a test shape into one of the two classes.
Consider two shape families speciﬁed by their probability models: h1 and h2. In
the following, we will use the TWN model deﬁned earlier. In order to compare any
two such shape models, there are two possibilities:
1. Comparisons in Shape Space: Here the probability densities are deﬁned
directly on the shape space Sc
2 and are compared as functions on that space.
Since TWN densities are deﬁned only on ﬁnite-dimensional subsets of Sc
2, we
will require that the supports of both the densities include the regions where
the test shapes are observed. Then, one can follow the process of computing
likelihood ratios and standard decision theory modiﬁed to this speciﬁc domain.
2. Comparison in a Tangent Space: The other possibility is to compare the
truncated normal densities, before the wrapping step, on a certain tangent
space. Since the two shape models originate from a truncated normal density
on a tangent space, this approach sounds natural, especially because it avoids
the need for a complicated Jacobian term that comes from the wrapping step.
However, the two models involve two diﬀerent tangent spaces and, as earlier,
one requires a common domain for comparing shape models. The solution is to
parallel transport along geodesics the probability densities to a common tangent
space. The candidate points for placing this common tangent space are the
two original means or a new point that lies half way between the two means.
In addition to the mean point, these shape models are completely speciﬁed
using the principal components in the tangent space. This simpliﬁes the task

9.8 Classiﬁcation of Shapes With Contour Data
337
of transporting a model as one has to only transport the principal directions of
variations from the original mean to a new point.
3. Comparing Likelihoods Computed in Diﬀerent Tangent Spaces: A
third possibility is to compute the likelihood of a test shape under each com-
peting model, in its own tangent space, and then to compare the resulting
likelihoods. We will develop this option in more detail. For a given test shape,
represented by its SRVF q ∈Sc
2, we are interested in selecting one of two fol-
lowing hypotheses:
H0 : [q] ∼h1
H1 : [q] ∼h2
Our classiﬁcation, or a selection of a class for q, is based on ﬁnding the log-
likelihood ratio:
l(q) ≡log
h1(q)
h2(q)

,
and comparing it to a suitable threshold value. In case we use the TWN densi-
ties, we can derive the log-likelihood ratio in more detail. Recall that a TWN
is essentially a truncated multivariate normal density in a ﬁnite-dimensional
subspace of the tangent space at the mean and then projected on the shape
space using the exponential map. Let [μ1], [μ2] ∈Sc
2 be the mean shapes asso-
ciated with the two shape classes. Also, let M ∥
1 be the m-dimensional principal
subspace of the tangent space T[μ1](Sc
2), obtained using the tangent principal
component analysis (TPCA) of observed shapes from the ﬁrst class, and let
M ⊥
1 be the space orthogonal to M ∥
1 in T[μ1](Sc
2) so that M ∥
1 ⊕M ⊥
1 = T[μ1](Sc
2).
Denote by K1 the m × m covariance matrix of the tangent vector projected in
M ∥
1 . That is, if [q] ∼h1([μ1], K1), then for v1 ≡exp−1
[μ1]([q]), the projection v1
into M ∥
1 , denoted by v∥
1, we have that v∥
1 ∼N(0, K1). To complete a model on
all of v1, we assume that the components of v⊥
1 are independent and normally
distributed with mean zero and variance ϵ2 for a small ϵ. (Note that this last
item is meaningful only in practical situations where the shapes and tangent
vectors are computed as ﬁnite-dimensional vectors for some large dimension,
say T . If T is inﬁnite, as it is in theory, this model for v⊥
1 is not meaningful.)
The probability density function of a shape [q], under this shape class, is given
by:
h1([q]) =
1

(2π)m det(K1)
exp(−1
2(v∥
1)T K−1
1 (v∥
1))
1

(2πϵ2)(T −m)
exp(−1
2
∥v⊥
1 ∥2
2ϵ2
)J1([μ1], q) ,
(9.13)
where:
• v1 ≡exp−1
[μ1]([q]), and
• J1 is the determinant of the Jacobian of the exponential map at [μ1], when
evaluated at [q].

338
9 Statistical Modeling of Planar Shapes
Fig. 9.25 Top: Two examples each from the 20 shape classes in MPEG7 database used in
likelihood-based classiﬁcation. Bottom: Mean shapes of each shape class
The probability density function under the second shape class, h2, will have
a similar expression. Note that we need to use the same dimension m for the
principal subspace as in the case of h1. Taking the log of their ratios, we obtain
the log-likelihood of a shape q as:
L(q) ≈−(v∥
1)T K−1
1 v∥
1 + (v∥
2)T K−1
2 v∥
2 −(∥v⊥
1 ∥2 −∥v⊥
2 ∥2)/(2ϵ2) −log(det(K1))
+ log(det(K2)) .
(9.14)
Since the Jacobians J1 and J2 are hard to compute in practice, we make a
simplifying approximation, albeit without further justiﬁcation, that they cancel
each other out and we get the expression given above. The likelihood ratio test
reduces to:
L(q)
h1
>
<
h2
ν ,
(9.15)
where ν is a certain predetermined threshold. For the moment, we can take ν
to be zero.
We describe an experiment that demonstrates the use of this framework. In
this experiment, we take MPEG7 shape database that contains 20 shape classes
with 10 exemplars each. Figure 9.26 (top) shows two examples for each of these
20 classes. For each of the shape classes Ci, i = 1, 2, . . ., 20, we compute their
mean [μi], m-dimensional principal subspace M ∥
i , and m×m tangent covariance
Ki (with m = 8). The bottom part of Fig. 9.25 shows the mean shapes of each of
these classes. Then, we take a random shape out of these 200 shapes, compute
its SRVF qtest, and add noise according to:
qtest(t) = q0(t) + σδ(t),
δ(t) ∼N(0, 1) ,

9.9 Detection/Classiﬁcation of Shapes in Cluttered Point Clouds
339
σ = 0.2
σ = 0.4
σ = 0.6
σ = 0.8
σ = 1.0
σ = 1.2
σ = 1.4
0
0.2
0.4
0.6
0.8
1
1.2
1.4
10
20
30
40
50
60
70
80
90
100
Classﬁcation Performance vs. σ
Fig. 9.26 Top: Examples of noisy curve data used in the classiﬁcation experiment. Bottom:
Decrease in classiﬁcation rate as noise increases
where δ(t) is a random observation noise added independently at each time t.
This provides a test shape for classiﬁcation and evaluation. Shown in Fig. 9.26
(top two rows) are some examples of these random shapes for diﬀerent values
of σ. For small values of σ, the shapes are still recognizable visually, but as σ
gets higher, the shapes are corrupted to the level that they are unrecognizable.
We then evaluate the likelihood of this shape belonging to each of the classes
and assign it to the class with the highest likelihood. If the highest likelihood
belongs to the class from which the original shape q0 came, then we call the
classiﬁcation successful.
In order to quantify the classiﬁcation performance, we generate 100 test
shapes, taken uniformly across the 20 shape classes, and calculate the percent-
age of correct classiﬁcation. Shown in Fig. 9.26 (bottom panel) is a plot of the
classiﬁcation performance as a function of increasing σ. When σ is zero or small
(∼0.2), the classiﬁcation performance remains perfect, but it starts deteriorat-
ing as σ decreases.
9.9 Detection/Classiﬁcation of Shapes in Cluttered Point
Clouds
In practical situations, the data available for detection and classiﬁcation of shapes
may be more complicated. Instead of observing contours directly, we may get
data in the form of discrete, unordered points that are both noisy and cluttered.
To be precise, we are given a point cloud Y = {yi ∈D , i = 1, 2, . . . , m} in a
domain D ⊂R2 and we want to develop a statistical framework for deciding if
there is a predetermined shape contained in this set. Only the shape is known but
its location, orientation, and scale in the scene is unknown. One can extend this
idea to detection of full shape classes, i.e., a set of shapes belonging to the same
population, using statistical shape models. In order to analyze such point clouds,

340
9 Statistical Modeling of Planar Shapes
40
60
80
100 120
20
a
b
c
d
40
60
80
100
120
40
60
80
100 120
20
40
60
80
100
120
40
60
80
100 120
20
40
60
80
100
120
40
60
80
100 120
20
40
60
80
100
120
Fig. 9.27 A generative model for a cluttered point cloud containing a sampled shape. (a)
Sampling a curve. (b) Sampled shape. (c) Samples points plus noise. (d) Cluttered point cloud
we will need generative models for the observed data. In this section, we develop
one such model that governs generation of cluttered point clouds from arbitrary
elements of our shape space Sc
2.
Let [q] ∈Sc
2 denote an arbitrary planar shape. In order to reach a noisy point
cloud, starting from [q], we need to specify the steps illustrated pictorially in
Fig. 9.27: (a) assign a position, orientation, and parameterization to the given
shape and form a parameterized curve β; (b) sample the given curve according to
a statistical model into a random number of points {β(ti)}; (c) add observation
noise to the given points {β(ti)+ ϵi}; and (d) add clutter points to the set, {yi} =
{β(ti) + ϵi} ∪{yc
j}, to result in a 2D point cloud with clutter. We will develop
models for each of these steps.
There are at least two models for random selection of points along a parame-
terized curve:
1. Point Process Model: Use a point process for generation of discrete points
along a curve. For instance, we can assume that the points are sampled from
a Poisson process with a known intensity function along β. The number n is
a Poisson random variable with mean given by the integral of the intensity
function along the full curve, and the placement is dictated by the value of the
intensity function.
2. Random Re-parameterization Model: Another possibility is to impose a
statistical model on n, the number of points, and seek an independent model for
their placement. As described in Remark 9.2, we can also control the placement
variability via re-parameterization of β. Assume that the curve is arc-length
parameterized and n points are placed at equal distances {0, 1/n, 2/n, . . ., 1}
along the curve. Now, if we take a random re-parameterization γ ∈ΓS, then
the selected points become {0, γ(1/n), γ(2/n), . . ., 1}. This way one can control
selection of points using random γ.
In this section, we will use the point process approach, developing a fully statistical
model for generation of cluttered point clouds from a given parameterized curve.
Returning to the problem of shape detection and classiﬁcation, we will start
with the problem of binary hypothesis testing—the null hypothesis is that Y is
simply clutter, i.e., the shape of interest is NOT present in Y, and the alternate
hypothesis is that Y is generated from that shape, i.e., a shape is present in Y.
H0 : Shape is absent, likelihood
P(Y|C)

9.9 Detection/Classiﬁcation of Shapes in Cluttered Point Clouds
341
H1 : Shape is present, likelihood P(Y|S)
Here, C denotes the clutter and S denotes the shape of interest. The challenge, of
course, is to develop appropriate probability models that will enable us to evaluate
the two likelihoods.
9.9.1 Point Process Models for Cluttered Data
Shape is a characteristic that is invariant to similarity transformations, but when a
shape occurs in a scene, it has a speciﬁc scale, position, and orientation. From the
perspective of shape detection, these variables are considered nuisance variables
that have to be either estimated or integrated out. In order to better explain the
model description and a detection solution, we will start with a simpler problem
where we seek a speciﬁc object, i.e., known shape, position, orientation, and scale.
Let β : S1 →R2 be a parameterized curve of interest. We will assume that the
curves are parameterized by a constant-speed parameter in the 2D case, i.e., β(s) ∈
R2 such that | ˙β(s)|= constant. To develop the data model, we make the following
assumptions:
1. Points belonging to β: We assume that these points are realizations of a
Poisson process on the parameterized object β. Let g : S1 →R≥0 be the inten-
sity function of the Poisson process along β. The number of points generated
from any part of the object is a Poisson random variable with mean being the
integral of g on that part. In particular, k, the total number of points belonging
to the object, is a Poisson random variable with mean G = 
S1 g(s) ds ∈R≥0.
Let the points sampled from β be denoted by X = [x1, x2, . . . , xk], xj ∈Rn.
The actual observations yj are assumed to be noisy versions of xj. For given
x1, x2, . . . , xk, the yjs are assumed to be independent of each other with
the identical density f(yj|xj). Under this model, the two hypotheses can be
rewritten as:
H0 :
g = 0,
Likelihood P(Y|C)
H1 :
g > 0,
Likelihood P(Y|S) .
2. Points associated with clutter: This subset of observations, independent of
the ﬁrst subset, comes from the clutter and we model them as realizations of a
Poisson process with the intensity l : D(⊂R2) →R≥0, where D is the region
containing observed points, e.g., D = [a, b]2. Let L =

D l(y)dy ∈R≥0.
The full observation Y can now be modeled as a Poisson process with the inten-
sity function: ξ(y) = l(y)+

S1 f(y|β(s))g(s) ds. The probability density function of
Y, given β, g, l, and for a ﬁxed m, is given by Pm(Y|β, g, l) = (-m
i=1 ξ(yi))e−L−G,
where m is the total number of points in the data. The null hypothesis is that all
the points belong to the Poisson clutter. In that case, the likelihood function is
given by Qm(Y|l) = e−L -m
i=1 l(yi). The likelihoods for both the cases, H0 and H1,
involve certain parameters that are generally not known beforehand. Thus, taking
a simple likelihood ratio is not possible and we resort to the generalized likelihood
ratio test (GLRT). This is based on maximum likelihood estimates (MLEs) of pa-
rameters, under the respective hypotheses, and uses the MLEs for evaluating the
likelihood ratio. The generalized likelihood ratio is given by:

342
9 Statistical Modeling of Planar Shapes
Qm(Y|C)
Pm(Y|S) =
maxl Qm(Y|l)
maxl,g Pm(Y|β, g, l) =
maxl(e−L -m
i=1 l(yi))
maxl,g(e−G−L(-m
i=1 ξ(yi))).
(9.16)
So far the unknown parameters are full functions and that involves tremendous
computational complexity. We will simplify the evaluation of GLR in Eq. 9.16 by
making the following additional assumptions:
1. The noise added to the points sampled from β is i.i.d. Gaussian with mean zero
and variance σ2I2×2. Therefore, the conditional density f(y|x) takes the form
1
(2π)σ2 e−
1
2σ2 ∥y−x∥2 for y, x ∈R2.
2. Both the Poisson intensities are constant, i.e., l(y) = l and g(s) = g and we
get L = l

D dy and G = g

S1 ds. To simplify the discussion, we scale both the
integrals to be one such that L = l and G = g.
With these assumptions, the likelihood ratio simpliﬁes to:
Qm(Y|C)
Pm(Y|S) =
maxl(e−l -m
i=1 l)
maxl,g,σ(e−g−l(-m
i=1(l + gασ(yi))) .
The numerator on maximization becomes e−mmm. The quantity ασ : Rn →R+
in the denominator is a scalar map given by ασ(yi) =
1
(2π)σ2

D e−
1
2σ2 ∥yi−β(s)∥2ds.
Notice that ασ(yi) is high if a point yi is close to the object β, with the closeness
being measured relative to the scale σ. Some illustrations of ασ in R2 are shown
as gray scale images in Fig. 9.28. The top row shows the case for diﬀerent σs (from
left to right: σ = 0.01, 0.02, 0.03) but a ﬁxed curve. As σ increases, the region of
high likelihood spreads further away from the curve. The bottom row shows ασ
maps for diﬀerent curves but a ﬁxed σ = 0.02.
Fig. 9.28 Likelihood maps ασ for some curves. The top row shows the case for the same curve
but diﬀerent σs and the bottom row shows diﬀerent curves but with the same σ

9.9 Detection/Classiﬁcation of Shapes in Cluttered Point Clouds
343
9.9.2 Maximum Likelihood Estimation of Model
Parameters
Deﬁne a function H to be the logarithm of Pm(Y|β, g, l). Let θ = [g, l, σ] ∈R3
denote three unknown parameters associated with the shape. Then, the function
H : R3 →R+ is given by H(θ) = −g −l + m
i=1 log(l + gασ(yi)) and let ˆθ =
argmaxθ H(θ) be maximizer (actually, ˆθ is the MLE of θ under the hypothesis H1).
We solve for the MLE of θ using a gradient approach. Of the three components of
θ, we search exhaustively for the parameter σ and use a gradient-based approach
to search over the remaining two g and l. For each value of σ in a certain range,
say [σl, σu], we maximize H over the pair (g, l).
For a ﬁxed σ, the function H : R2 →R, given by Hσ(l, g) = −g−l+m
j=1 log(l+
gασ(yj)), has the following properties:
1. Its derivatives with respect to l and g are given by:
∂Hσ
∂l
= −1 +
m

i=1
1
l + gασ(yi),
∂Hσ
∂g
= −1 +
m

i=1
ασ(yi)
l + gασ(yi) .
(9.17)
2. Its Hessian matrix is given by:
m
i=1
−(ασ(yi))2
(l+gασ(yi))2
m
i=1
−ασ(yi)
(l+gασ(yi))2
m
i=1
−ασ(yi)
(l+gασ(yi))2
m
i=1
−1
(l+gασ(yi))2

.
It is easy to show that the two eigenvalues of the Hessian matrix are non-
positive, so that Hσ is a concave function in l and g.
Therefore, one can use the gradient search over l and g and reach a global optimizer.
For σ, the situation is diﬀerent and we use an exhaustive grid search over allowable
values of σ to reach a global maximizer. This combined gradient and grid search
algorithm is summarized below:
Algorithm 43 (MLE of θ).
• For each σ ∈[σl, σu], perform the following:
1. Set t = 0 and initialize the pair [gt, lt] with random values in the range [0, m].
2. Update the estimates using

gt+1
lt+1

=

gt
lt

+ δ
 ∂Hσ
∂g (gt, lt)
∂Hσ
∂l (gt, lt)

, for a small
δ > 0.
3. If the norm of the gradient vector is small, then stop the loop. Else, set
t = t + 1 and return to Step 2.
• Set the current values to be (ˆg(σ), ˆl(σ)).
• Deﬁne the MLE ˆθ to be (ˆg(ˆσ), ˆl(ˆσ), ˆσ) where ˆσ = argmaxσ∈[σl,σu] H(ˆg(σ), ˆl(σ), σ).
With the estimated parameters, the log-likelihood ratio (LLR) becomes:
R(Y) = log Qm(Y|C)
Pm(Y|S) = −m + m log(m) −H(ˆθ).
(9.18)

344
9 Statistical Modeling of Planar Shapes
The generalized likelihood ratio test is given by:
R(Y)
C
>
<
S
ν.
Next, we will focus on how to choose the threshold ν.
Empirical Detection of Threshold In the binary test, the LLR R(Y) is to be
compared with a threshold ν to decide if a shape is present in the data or not.
Ideally, this threshold is dictated by the probability distributions of R(Y) under
the null hypothesis. In practical situations, where it is diﬃcult to ascertain these
distributions, one uses either the asymptotic theory or an empirical approach to
reach an optimal value of ν. To demonstrate the empirical approach, we estimate
the pdf of R(Y) under the null hypothesis. We generate 1000 realizations of Y,
each using g = 0 (null hypothesis = clutter) and a ﬁxed m equal to the observed
number of points in our data, and compute a histogram of R(Y) values. Using
this estimated density function, we can decide the threshold ν for a speciﬁc type I
error rate, denoted by α. One can repeat this for diﬀerent values of m to catalog
distributions of R(Y) for diﬀerent ms.
The main advantage of a numerical evaluation of the threshold is that we need
not assume any speciﬁc form for the underlying density, nor do we need to invoke
any asymptotic argument. The disadvantage, however, is that we need to do this
for every shape we are interested in, since we do not have an analytical expression.
We point out that this computation is oﬄine and can be performed for each of the
shapes beforehand.
Consider the point cloud shown in the left column of Fig. 9.29. For this data,
we apply Algorithm 43 for two curves—a runner and a wineglass, shown in the
second column. The third column shows the estimated αˆσ map for each curve and
the rightmost column shows the value of αˆσ(yi) for each of the data points using
its thickness. Recall that αˆσ(yi) is large if the point yi is close to the curve β.
Since the observation Y here was generated from the runner curve, αˆσ(yi)s have
Fig. 9.29 A point cloud, two hypothesized curves, αˆσ proﬁles, and αˆσ at yis for the two curves

9.10 Problems
345
Fig. 9.30 Examples of shape estimation in cluttered point clouds (black, point clouds; blue,
estimated shapes)
larger values for that shape and lower values for the wineglass shape. This points
to a higher likelihood of the runner shape being present in the given point cloud.
Another interesting subproblem here, in addition to the detection of a given
curve, is the estimation of that curve in the point cloud. For a given point cloud
and a hypothesis shape β0, we ﬁrst detect if the shape is present in the data. If
yes, using the estimates ˆg, ˆσ, and ˆω, we select ˆg number of points in Y with the
largest values of αˆσ. Then, we connect them in the same order as their nearest
neighbors on β0. Some examples of this process are shown in Fig. 9.30. Note that
in this fully statistical framework, one not only veriﬁes the presence of a shape but
also estimates the shape and provides the likelihood of this estimated shape being
present in the given data.
9.10 Problems
9.10.1 Theoretical Problems
1. Let Ci and Cj be any two nonempty subsets of the set B = {β1, β2, . . . , βn}, a
set of given curves.
a. Show that the quantity:
D(Ci, Cj) =
min
[q1]∈Ci,[q2]∈Cj ds([q1], [q2])
is not a metric on the power set of B.
b. What happens if instead we use average of all pairwise distances across Ci
and Cj?
c. Hausdorﬀ: Verify that the following quantity is a metric on the power set
of B:
D(Ci, Cj) = max{ max
[q1]∈Ci min
[q2]∈Cj ds([q1], [q2]), max
[q2]∈Cj min
[q1]∈Ci ds([q1], [q2])} .

346
9 Statistical Modeling of Planar Shapes
2. For any nontrivial μ ∈C2, show that the function u(t) =
0 −1
1 0

μ(t) spans the
space Tμ([μ]SO(2)), the tangent space to the rotation orbit of μ, at μ.
3. Recall that the ΓI is the group of positive diﬀeomorphisms on [0, 1] and ˜ΓI is
the monoid of weakly increasing, absolutely continuous functions on [0, 1] (see
Deﬁnition 4.3). We know that the set ΓI is dense in ˜ΓI using the L2 norm.
Show that a set orthogonal functions that spans Tμ([μ]ΓI) also spans the set
Tμ([μ] ˜ΓI).
4. Show that the set Bn, given in Eq. 9.3, forms an orthonormal basis for
L2([0, 1], R) in the limit as n goes to inﬁnity.
9.10.2 Computational Problems
1. Write a program to implement the simulated annealing search for optimal
clustering, as given in Algorithm 37.
2. Write a program to generate the basis set Bn for a given n and the sample
points {0, 1
T , 2
T , . . . , 1} ⊂[0, 1].
3. Similarly, write a program to generate a basis of Tγid(ΓI), as provided in this
chapter.
4. Implement Algorithm 38 to generate an orthonormal basis of the tangent space
Tμ([μ]) for [μ] ∈S2.
5. Write a program that takes (1) the SRVF of mean curve μ ∈L2([0, 1], R2),
(2) an orthogonal basis of Tμ([μ]), and (3) SRVF of an arbitrary curve q ∈
L2([0, 1], R2); to ﬁnd the ﬁnite-dimensional representation x ∈Rm of [q] ∈S2.
6. Write a program to generate a basis of Tγid(ΓS), as provided in this chapter.
7. Adapt Algorithm 38 to the case for closed curves, to generate an orthonormal
basis of the tangent space Tμ([μ]) for [μ] ∈Sc
2.
8. Write a program that takes (1) the SRVF of mean curve μ ∈L2(S1, R2), (2) an
orthogonal basis of Tμ([μ]), and (3) SRVF of an arbitrary curve q ∈L2(S1, R2);
to ﬁnd the ﬁnite-dimensional representation x ∈Rm of [q] ∈Sc
2.
9. Write a program implementing Algorithm 39 to compute sample Karcher mean
of a given set of curves in the shape space S2.
10. Write a program implementing Algorithm 40 to compute sample Karcher mean
of a given set of closed curves in the shape space Sc
2.
11. Write a program implementing Algorithm 41 to compute sample Karcher me-
dian of a given set of closed curves in the shape space Sc
2.
12. Write a program to generate a basis W to provide a ﬁnite-dimensional basis
of closed, planar shapes using the tangent space of a mean shape [μ] ∈Sc
2.
13. Write a program that performs principal component analysis of shooting vec-
tors {vi} obtained in Algorithm 40. Display the top three modes of variability
in the given data using the wrapping (exponential map) of the tangent space
geodesics on the shape space Sc
2.
14. Write a program to generate random shapes using the TWN model with model
parameters (mean and covariance) estimated using previous problems from a
training dataset.
15. Write a program that takes in an arc-length parameterized curve and generates
a 2D point cloud according to the Poisson model speciﬁed in Sect. 9.9.1.

9.11 Bibliographic Notes
347
16. Implement Algorithm 43 that takes in a given point cloud, and a parameterized
curve, and generates MLE of the parameter θ.
17. Write a program to compute the likelihood ratio for a given test curve according
to Eq. 9.14. (As suggested in this equation, ignore the terms involving Jacobians
of the exponential map.)
18. Write a program to generate an empirical distribution of the test statistics,
under the null hypothesis, for a given value of m and a given curve β.
9.11 Bibliographic Notes
The textbook by Jain and Dubes [40] is an excellent presentation of diﬀerent ideas
in clustering techniques. Several authors have explored the use of annealing in
clustering problems, including soft clustering [95] and deterministic clustering [37].
An interesting idea presented in [37] is to solve an approximate problem, termed
mean-ﬁeld approximation, where Q is replaced by a function in which the roles
of elements qis are decoupled. The advantage is the resulting eﬃciency although
it comes at the cost of error in approximation. Monte Carlo simulated annealing
has been used in optimization in several places. See, for example, Algorithm A.20,
pg. 200 [93] for a discussion. Srivastava et al. [105] studied the problems of clus-
tering, modeling, and testing shapes of curves using the non-elastic framework.
The problem of modeling shapes and diﬀerent appearance parameters (rotation,
position, scale, and parameterization) of curves, for the purpose of estimating
shapes in point clouds, has been tackled in [104]. Su et al. [110] discussed the
problem detecting and estimating 2D and 3D shapes in cluttered point clouds
using Poisson point process models.

Chapter 10
Shapes of Curves in Higher Dimensions
So far in this text we have considered only planar curves. Although the shapes of
planar curves are quite important, especially for recognizing objects in images, the
shapes of curves in higher-dimensional spaces also have an important role to play.
While the shape analysis of curves in R3 and higher dimensions is directly useful
for studying many important objects, such as the white matter ﬁber tracts, curves
formed by sulcal folds, backbones in protein structures, level curves of geological
terrains and oceans, and ﬂight paths of migratory birds, they also have indirect
uses in many situations. One example is in shape analysis of facial surfaces where
each surface is represented by a collection of certain intrinsically deﬁned curves.
These curves are either open or closed curves in R3, and a shape comparison of two
facial surfaces can be, in eﬀect, performed by comparing shapes of their respective
curves. Another interesting example is in shape analysis of augmented curves. An
augmented curve is a parameterized curve in a certain space, say β(t) ∈Rn, which
has a certain auxiliary function, say f(t) ∈Rk, associated with it. An example of
this situation is in a color image: the parameterized curve comes from the boundary
of an object in the image, while the auxiliary function represents the colors or
textures along that boundary. Together the mapping t →(β(t), f(t)) ∈Rn+k can
be treated as a curve in a higher-dimensional space, albeit with some modiﬁcations
to the general method.
10.1 Goals and Challenges
In this chapter, we will develop techniques for shape analysis of curves in Rn, with
n being three or more, and will demonstrate them with some applications. The
constructions and techniques will be straightforward extensions of the correspond-
ing ideas for the planar case in Chaps. 5 and 6. Our goals are to:
1. Develop mathematical representations, Riemannian metrics, and algorithms for
geodesic computations for analyzing shapes of curves in Rn for n ≥3. We will
consider both open and closed curves.
2. Develop techniques for computing shape summaries and for modeling shapes of
populations of curves.
© Springer-Verlag New York 2016
A. Srivastava, E.P. Klassen, Functional and Shape Data Analysis,
Springer Series in Statistics, DOI 10.1007/978-1-4939-4020-2 10
349

350
10 Shapes of Curves in Higher Dimensions
3. Demonstrate these ideas using applications involving open and closed curves
in areas such as human biometrics, bioinformatics, medical image analysis, and
geology.
10.2 Mathematical Representations of Curves
Previously we have utilized two representations for studying shapes of curves:
the angle function and the square-root velocity function. The use of L2 metric
in each space leads to a diﬀerent framework, for ﬁnding optimal matching and
deformations between curves—bending-only deformations in the ﬁrst case and
elastic deformations (bending-stretching combination) in the second. Although
we will extend both these representations to curves in Rn, we will treat the elastic
framework in more detail, as there are distinct advantages to using it in many
applications. Here are the mathematical representations for the two frameworks:
1. Direction Function Representation: Let β : [0, 1] →Rn be an absolutely
continuous curve of length one, parameterized by the arc length. For θ(s) ≡
˙β(s) ∈Rn, we have |θ(s)| = 1 for all s ∈[0, 1], due to the use of the arc-length
parameterization. Here | · | denotes the Euclidean norm in Rn. The function θ
is called the direction function of β; it is the extension of the angle function
θ, used in Chaps. 5 and 6 for studying planar curves, to curves in Rn. Since
|θ(s)| = 1 for all s, the function θ itself can be viewed as a curve on the unit
sphere Sn−1, i.e., θ : [0, 1] →Sn−1. Shown in Fig. 10.1 is an illustration of this
idea where a closed curve β in R3 is represented by a curve θ in S2. We will use
the direction function θ to represent and analyze the shape of the curve β. By
the virtue of unit-speed parameterization and the domain size being one, the
length of curves represented by a θ is also one. Note that we can reconstruct
the curve from a given angle function θ, up to a translation, using the formula:
β(t) =
 t
0 θ(s)ds.
2. Square-Root Velocity Function Representation: The second representa-
tion is based on the function q(t) that has the same direction as the velocity
vector ˙β(t) but its magnitude is given by

| ˙β(t)|. We will restrict to the curves
β that are absolutely continuous. This implies that their q function is square
integrable. Deﬁne a mapping F : Rn →Rn according to:
F(v) ≡

v/

|v|,
if |v| ̸= 0
0,
otherwise
(10.1)
p(s)
v(s)
Fig. 10.1 A closed curve in Rn is denoted by a curve on Sn−1

10.3 Elastic and Non-elastic Metrics
351
Here, | · | is the 2 norm in Rn and note that F is a continuous map. For the
purpose of studying the shape of β, we will represent it using the square-root
velocity function deﬁned as q : [0, 1] →Rn, where:
q(t) ≡F( ˙β(t)) = ˙β(t)/

|
˙
β(t)| .
(10.2)
Since the curve β is absolutely continuous, its square-root velocity function is
an element of L2([0, 1], Rn). This representation includes those curves whose
parameterizations can become singular in the analysis. Also, for every q ∈
L2([0, 1], Rn), there exists a curve β (unique up to a translation) such that the
given q is the square-root velocity function of that β. One can reconstruct this
curve from q, up to a translation, using the formula β(t) =
 t
0 q(τ)|q(τ)|dτ. If the
curve β is of unit length, the function q satisﬁes
 1
0 |q(t)|2dt =
 1
0 | ˙β(t)|dt = 1.
10.3 Elastic and Non-elastic Metrics
An interesting point about our earlier discussion on shape analysis of the planar
curves was that the same L2 metric under diﬀerent representations leads to two
diﬀerent quantiﬁcations of shape diﬀerences—both having their own physical in-
terpretations. This point still holds when we extend to curves in higher dimensions.
Similar to Sect. 5.6.1, we can motivate the two metrics for the higher-dimensional
curves. Furthermore, we will make a tighter connection between the two metrics.
More speciﬁcally, the bending-only metric will be shown to be a special case of the
elastic metric when one restricts to curves with arc-length parameterizations.
Let β : [0, 1] →Rn be a smooth curve in Rn. (In this section, we will as-
sume additional properties, such as smoothness and non-singularity of param-
eterizations, to facilitate discussion.) Assume that for all t ∈[0, 1], ˙β(t) ̸= 0.
We then deﬁne φ : [0, 1] →R by φ(t) = ln(| ˙β(t)|), and θ : [0, 1] →Sn−1 by
θ(t) = ˙β(t)/| ˙β(t)|. The diﬀerence from Sect. 5.6.1 is that this time θ(t) is an
element of Sn−1 rather than the unit circle S1. As earlier, φ and θ completely
determine ˙β, since for all t, ˙β(t) = eφ(t)θ(t). Thus, we have deﬁned a map from the
space of parameterized curves in Rn to Φ × Θ, where Φ = {φ : [0, 1] →R smooth}
and Θ = {θ : [0, 1] →Sn−1 smooth}. From the point of physical interpretations,
φ(t) is the instantaneous (log of the) speed, while θ(t) is the instantaneous direction
of the curve at each time t.
In order to quantify the magnitudes of perturbations of β, we wish to impose
a Riemannian metric on the space of curves that is invariant under rigid motions
and re-parameterizations, and we will do this by putting a Riemannian metric on
Φ × Θ. The tangent of space of Φ × Θ at any point (φ, θ) is given by:
T(φ,θ)(Φ × Θ) = {(u, v) : u ∈Φ and v : [0, 1] →Rn smooth and v(t) ⊥θ(t), ∀t ∈[0, 1]}
Deﬁnition 10.1 (Elastic Metric for Curves in Rn). For any point (φ, θ) ∈
(Φ, Θ), and a pair (u1, v1) and (u2, v2) in T(φ,θ)(Φ×Θ), deﬁne an inner product by
⟨(u1, v1), (u2, v2)⟩(φ,θ) = a2
 1
0
u1(t)u2(t)eφ(t) dt + b2
 1
0
⟨v1(t), v2(t)⟩eφ(t) dt ,
(10.3)

352
10 Shapes of Curves in Higher Dimensions
where a and b are positive real numbers. This deﬁnes a Riemannian metric on the
manifold Φ × Θ, which is called the elastic metric.
The two terms measure the amount of stretching and bending, respectively, and
the numbers a and b provide relative weights to the two measurements. Note that
⟨·, ·⟩in the second integral denotes the Euclidean inner product in Rn.
Similar to the case of planar curves, under the elastic metric, the groups SO(n)
and ΓI both act on the representation space by isometries. To elaborate on this,
recall that O ∈SO(n) acts on a curve β by (O, β)(t) = Oβ(t), and γ ∈ΓI acts on
β by (γ, β)(t) = β(γ(t)). Using our identiﬁcation of the set of curves with the space
Φ× Θ results in the following actions of these groups. O ∈SO(n) acts on (φ, θ) by
(O, (φ, θ)) = (φ, Oθ). γ ∈ΓI acts on (φ, θ) by (γ, (φ, θ)) = (φ ◦γ + ln ◦˙γ, θ ◦γ). As
stated, SO(n) acts on Θ × Φ from the left and ΓI acts from the right, making it
diﬃcult to form the joint action directly. Thus, if needed, one can switch the action
of SO(n) to the right using ((u, v), O) = (u, O−1v). However, we will overlook this
discrepancy here.
Now we look at the diﬀerentials of these group actions on the tangent spaces
of Φ × Θ. SO(n) is easy; since each O ∈SO(n) acts by the restriction of a linear
transformation on Φ×L2([0, 1], Rn), it acts in exactly the same way on the tangent
spaces (O, (u, v)) = (u, Ov), where (u, v) ∈T(φ,θ)(Φ×Θ), and (u, Ov) ∈T(φ,Oθ)(Φ×
Θ). The action of γ ∈ΓI given in the above formula is not linear, but aﬃne
linear, because of the additive term ln ◦˙γ. Hence, its action on the tangent space
is the same, but without this additive term (γ, (u, v)) = (u ◦γ, θ ◦γ), where
(u, v) ∈T(φ,θ)(Φ × Θ), and (u ◦γ, θ ◦γ) ∈T(γ,(φ,θ))(Φ × Θ). Combining these
actions of SO(n) and ΓI with the above inner product on Φ × Θ, it is an easy
veriﬁcation that these actions are by isometries, i.e.:
⟨(O, (u1, v1)), (O, (u2, v2))⟩(O,(φ,θ)) = ⟨(u1, v1), (u2, v2)⟩(φ,θ)
⟨(γ, (u1, v1)), (γ, (u2, v2))⟩(γ,(φ,θ)) = ⟨(u1, v1), (u2, v2)⟩(φ,θ).
(10.4)
Since we have identiﬁed the space of curves with Φ× Θ, we may identify the space
of shapes with the quotient space (Φ× Θ)/(SO(n)× ΓI). Furthermore, since these
group actions are by isometries with respect to all the metrics we introduced above,
no matter what values we assign to a and b, we get a corresponding two-parameter
family of metrics on the quotient space (Φ × Θ)/(SO(n) × ΓI). Given two shapes,
we can ﬁnd the geodesics between the corresponding elements of (Φ × Θ) with
respect to any of these metrics (i.e., with respect to any choice of a and b).
Given a curve β : [0, 1] →Rn, its square-root velocity representation is the
function q : [0, 1] →Rn given by q(t) =
˙β(t)
√
| ˙β(t)| (Eq. 10.2). Relating this to the
(φ, θ) representation of the curve gives q(t) = e
1
2 φ(t)θ(t). A couple of simple dif-
ferentiations show that if (u, v) ∈T(φ,θ)(Φ × Θ), then the corresponding tangent
vector to L2([0, 1], Rn) at q is given by f = 1
2e
1
2 φuθ + e
1
2 φv.
Theorem 10.1. The L2 metric on the space of square-root velocity functions for
curves in Rn corresponds to the elastic metric on Φ × Θ with a = 1
2 and b = 1.
Proof. Let (u1, v1) and (u2, v2) denote two elements of T(φ,θ)(Φ × Θ) and let f1
and f2 denote the corresponding tangent vectors to L2([0, 1], Rn) at q = e
1
2 φθ.
Computing the L2 inner product of f1 and f2 yields:

10.4 Shape Spaces of Curves in Rn
353
⟨f1, f2⟩=
 1
0
1
2e
1
2 φu1θ + e
1
2 φv1, 1
2e
1
2 φu2θ + e
1
2 φv2

dt
=
 1
0
1
4eφu1u2 + eφ ⟨v1, v2⟩dt.
(10.5)
In this calculation, we have used the facts that ⟨θ(t), θ(t)⟩= 1, since θ(t) is an
element of the unit sphere, and that ⟨θ(t), vi(t)⟩= 0, since each vi(t) is a tangent
vector to the unit sphere at θ(t). Comparing the right side with the Eq. 10.3 give
us the desired result.
⊓⊔
This theorem shows that the square-root velocity representation with the L2 metric
provides a framework for elastic shape analysis of curves.
The non-elastic metric is a special case of the elastic metric in the following
sense. In case we restrict to curves that are parameterized by the arc length, then
the log-speed φ(t) = 0 for all curves and θ(t) is the only relevant function left.
Restricting to that part of the elastic metric, we obtain the non-elastic metric.
Deﬁnition 10.2 (Non-elastic Metric). For any point θ ∈Θ, and a pair v1, v2
in Tθ(Θ), deﬁne an inner product by:
⟨v1, v2⟩=
 1
0
⟨v1(t), v2(t)⟩dt .
(10.6)
This deﬁnes a Riemannian metric on Θ and is called the non-elastic metric.
These two representations for analyzing shapes of curves in Rn are considered
in the next two sections.
10.4 Shape Spaces of Curves in Rn
In this section, we consider curves in Rn and describe elements of shape analysis
of these curves using the two representations. In each case, we will describe the
mathematical representation, the Riemannian metric, and the constructions of
geodesic path between arbitrary shapes. The case of elastic curves is considered in
more detail, where the computation of Karcher mean and the stochastic modeling
of shapes is also considered. The ideas presented here are similar to those used in
Chap. 5 for planar curves.
10.4.1 Direction Function Representation
We will develop a set of basic tools needed to perform shape analysis of curves
using the direction function representation. We start with the pre-shape space,
compute geodesics on it, and repeat these steps for the shape space.
Pre-shape Space
In this case, the curves are rescaled to be of unit length and
are parameterized by the arc length. Each curve is represented by its direction
function and the pre-shape space of open curves is given by the set of all such
direction functions:
C1 = {θ : [0, 1] →Sn−1} .
(10.7)

354
10 Shapes of Curves in Higher Dimensions
Note that C1 ⊂L2([0, 1], Rn) since the L2 norm of any of its elements is one. Also
note that we are using the same notation here for pre-shape and shape spaces
as was used for planar curves. As mentioned in the previous section, we use the
L2 metric to impose a Riemannian structure on the manifold C1. Similar to the
planar case, this metric, with the directional function representation, is called the
bending-only metric as the curves are ﬁxed to be arc-length parameterized and
are not allowed to stretch or compress in this representation. For any θ ∈C1, the
tangent space Tθ(C1) is given by:
Tθ(C1) = {v : [0, 1] →Rn|v(t) ⊥θ(t), ∀t ∈[0, 1]} .
Geodesics in Pre-shape Space
How do we compute geodesic paths between curves in this representation? In the
planar case, we saw that the geodesic paths are straight lines. In this case also, the
geodesics are simple but with a small diﬀerence. Since each point θ(t) is an element
of the unit sphere Sn−1, this structure has to be maintained in the computation of
the geodesics. Let θ1, θ2 be two elements of C1. Then, the uniform-speed geodesic
connecting them in C1 is given by α(τ), with τ ∈[0, 1], such that for all t ∈[0, 1]:
α(τ)(t) =
1
sin(c(t)) [sin((1 −τ)c(t))θ1(t) + sin(τc(t))θ2(t)]
(10.8)
where c(t) is the angle between the points θ1(t) and θ2(t) on the sphere Sn−1,
i.e., c(t) = cos−1(⟨θ1(t), θ2(t)⟩). The geodesic distance between any two curves,
represented by their direction functions θ1 and θ2 in C1, is given by:
dc(θ1, θ2) =
 1
0
c(t) dt =
 1
0
cos−1(⟨θ1(t), θ2(t)⟩) dt .
(10.9)
Shape Space
Next we look at the shape-preserving transformations that we wish to remove
in our shape analysis. The transformations resulting from translation and scaling
have already been removed. The former due to the dependence of the direction
function on ˙β(t) and the latter due to the arc-length parameterization of the curve
with the parameter taking values in [0, 1]. However, the variability due to rotations
of curves remains. In other words, the curve β and the curve Oβ, for O ∈SO(n)
not equal to the identity, will have two distinct direction functions despite having
the same shape. The action of SO(n) on the pre-shape space C1 is given by:
SO(n) × C1 →C1,
(O, θ) = Oθ .
Here, Oθ(t) denotes the multiplication of an n × n matrix O with the unit vector
θ(t) ∈Rn. With respect to the chosen Riemannian metric on C1, the group SO(n)
acts on C1 by isometries, and we can deﬁne the shape space as the quotient space:
S1 = C1/SO(n) .
Since the action of SO(n) is by isometries, the metric on C1 descends to S1. The
computation of geodesics in this shape space relies on the following optimization
problem:

10.4 Shape Spaces of Curves in Rn
355
ds([θ1], [θ2]) = argmin
O∈SO(n)
d(θ1, Oθ2) = argmin
O∈SO(n)
 1
0
cos−1(⟨θ1(t), Oθ2(t)⟩)dt

(10.10)
We ﬁx the element θ1 of the orbit [θ1] and search over the whole orbit [θ2] to
ﬁnd a point nearest to θ1 under the distance dc. This nearest distance gives us
ds([θ1], [θ2]).
One way to solve this optimization is using the gradient method. As described
in Example A.6, the tangent space TIn(SO(n)) is the set all n×n skew-symmetric
matrices. Let Ei, i = 1, 2, . . ., n(n −1)/2) be the standard orthonormal basis of
that space; Ei is an n × n matrix that is zero everywhere except two symmetric
places where it has values
1
√
2 and −1
√
2. The directional derivative of the cost
function, in the direction of Ei, is computed as following. Let A(t) be an n × n
matrix given by θ2(t)θT
1 (t). Then:
∇Eids([θ1], [Oθ2]) = ∇Ei
 1
0
cos−1(trace(OA(t)))dt
=
 1
0
1

1 −trace(OA(t))2 trace(OEiA(t))dt . (10.11)
The full gradient of the cost function is given by n(n−1)/2
i=1
∇Eids([θ1], [Oθ2])OEi,
and the gradient-based iterative update of O becomes:
O →O exp(−δ
n(n−1)/2

i=1
∇Eids([θ1], [Oθ2])Ei) ,
where δ > 0 is the step size.
Algorithm 44. Given two curves in Rn, represented by their angle functions θ1
and θ2 in C1, our goal is to compute a geodesic between [θ1] and [θ2] in S1.
1. Compute A(t) = θ1θT
2 ∈Rn×n for all t ∈[0, 1].
2. Let E1, E2, . . . , En(n−1)/2 be an orthonormal basis of all n × n skew-symmetric
matrices. For each of these basis elements, compute the directional derivatives:
κi =
 1
0
1

1 −trace(OA(t))2 trace(OEiA(t))dt .
3. Compute the full gradient E = n(n−1)/2
i=1
κiEi and update θ2 →exp(−δE)θ2
for a small step size δ > 0.
Some examples of geodesics resulting from this algorithm are shown in Fig. 10.2.
Here we look at some cylindrical helical curves with diﬀerent numbers and place-
ments of loops. The deformations along these geodesics clearly show the bending
nature of this representation. In order to go from one helix to another, the algo-
rithm has to straighten some loops and create some new loops, all by bending the
ﬁrst curve. No stretching or compression of the curves is allowed. It is worth noting
that the intermediate curves hardly preserve the helical structure of the original
curves.

356
10 Shapes of Curves in Higher Dimensions
Fig. 10.2 Examples of bending-only geodesics between helical curves in the shape space S1
The steps for computing sample statistics in S1 and for imposing a stochastic
model on non-elastic shapes are identical to those presented in Chap. 5. We refer
the reader to that chapter for statistical analysis of shapes of non-elastic curves
in Rn.
10.4.2 Under SRVF Representation
The next representation to be studied is the elastic curve representation—the
square-root velocity function under the L2 metric. As we have seen earlier, this
representation allows for a combination of bending and stretching of curves to
allow for a better matching of features than the previous case and, thus, is a more
attractive framework for shape analysis of curves in general. To remove the scale
variability, we ﬁx the length of curves. Similar to the previous chapters, we ﬁx the
length to be one.
1. Pre-shape Space: The pre-shape space of all square-root velocity representa-
tions of curves in Rn is:
C2 = {q : [0, 1] →Rn|
 1
0
|q(t)|2dt = 1} .
By deﬁnition, C2 is a hypersphere of radius one in L2([0, 1], Rn) with the stan-
dard metric:
⟨w1, w2⟩=
 1
0
⟨w1(t), w2(t)⟩dt ,
(10.12)
where the inner product inside the integral is the simple Euclidean product
between vectors in Rn. For any q ∈C2, the tangent space Tq(C2) is:
Tq(C2) = {w : [0, 1] →Rn| ⟨w, q⟩= 0} .
As described in the previous section, the use of the L2 metric on the space
of square-root velocity representations generates a framework for elastic shape
comparisons and the manifold C2 becomes a Riemannian manifold under this
metric.

10.4 Shape Spaces of Curves in Rn
357
2. Geodesics in Pre-shape Space: With this Riemannian structure and the
spherical geometry of the pre-shape space, one can write explicit forms for
geodesics between curves: Given any two length 1, parameterized curves β1 and
β2 in Rn, we can compute geodesic paths between their representations q1 and
q2 using the shorter arc on C2 (same as Eq. 5.14): α : [0, 1] →C2:
α(τ) =
1
sin(θ) [sin(θ(1 −τ))q1 + sin(τθ)q2]
(10.13)
where θ = cos−1(⟨q1, q2⟩) and q1 ̸= ±q2. Even though these curves are in Rn,
the expressions for geodesics and geodesic distances remain the same as those
for planar curves. The geodesic distance between any two points in C2 is given
by (same as Eq. 5.15):
dc(q1, q2) = cos−1 (⟨q1, q2⟩) .
(10.14)
According to Eq. 5.17, the exponential map, exp : Tq(C2) →C2 is given by:
expq(w) = cos(∥w∥)q + sin(∥w∥) w
∥w∥.
(10.15)
For any q2 ∈C2, the inverse of the exponential map at q1 ∈C2, denoted by
exp−1
q1
: C2 →Tq1(C2), is computed as follows: exp−1
q1 (q2) = v, where v is
computed according to:
v =
θ
sin(θ)(q2 −cos(θ)q1),
where θ = cos−1(⟨q1, q2⟩).
(10.16)
3. Shape Space: Similar to the notation in Chap. 5, C2 is the pre-shape space
since its elements do not represent the shape of a curve uniquely. A re-
parameterization of β, using an element γ ∈ΓI, where ΓI is the group of
diﬀeomorphisms from [0, 1] to itself, results in a diﬀerent square-root veloc-
ity function while preserving its shape. The action of ΓI on C2 is given by
(q, γ) = √˙γ(q ◦γ). Additionally, any rigid rotation of β changes q but not its
shape. The action of the rotation group SO(n) on C2 is given by (O, q) = Oq.
It can be shown that the actions of these two groups on C2 commute, i.e., a
re-parameterization of a curve followed by its rotation results in the same rep-
resentation when the two operations are applied in a diﬀerent order. Also, the
two groups act on C2 by isometries.
Lemma 10.1. The action of the product group ΓI × SO(n) on C2 is by isome-
tries with respect to the chosen metric.
Proof. Same as Lemma 5.2.
For these reasons, we will call C2 the pre-shape space of open curves and we
deﬁne the individual shapes as the orbits:
[q] = closure{O

˙γ(q ◦γ)|γ ∈ΓI, O ∈SO(n)} .
The set of all such orbits is deﬁned as the shape space: S2 = {[q]|q ∈C2}. The
shape space S2 inherits a distance from the pre-shape space C2; the distance
between any two orbits [q1] and [q2] is given by:
ds([q1], [q2]) =
inf
γ∈ΓI,O∈SO(n) dc(q1,

˙γO(q2 ◦γ)) .
(10.17)

358
10 Shapes of Curves in Higher Dimensions
where dc is as deﬁned in Eq. 10.14. (In some cases where no optimal γ exists
in ΓI, one may be able to ﬁnd a pair (γ∗
1, γ∗
2) ∈˜ΓI × ˜ΓI for optimal alignment,
as discussed in Sects. 4.10 and 5.5.1.) The actual geodesic between [q1] and
[q2] in S2 is given by [α(τ)], where α(τ) is the geodesic in C2 between q1
and
 ˙γ∗O∗(q2 ◦γ∗) as deﬁned in Eq. 10.13. Here (O∗, γ∗) are the optimal
transformations of q2 that minimize the right side in Eq. 10.17. A closer look at
that distance function reveals the following:
argmin
γ∈ΓI,O∈SO(n)
cos−1 $
q1,

˙γO(q2 ◦γ)
%
= arg infγ∈ΓI,O∈SO(n)∥q1 −

˙γO(q2 ◦γ)∥2
= arg
sup
γ∈ΓI,O∈SO(n)
$
q1, O(q2 ◦γ)

˙γ
%
where the last norm is simply the L2 norm on the representation space. This
equality says that minimizing the arc length on a unit sphere is the same as
minimizing the chord length. If one is minimized, then so is the other. Therefore,
for the purpose of ﬁnding the minimizer, we can use the L2 norm, which opens
up the possibility of a computationally eﬃcient solution.
4. Geodesics in Shape Space: Consider the problem:
(γ∗, O∗) = arg infγ∈ΓI,O∈SO(n)∥q1 −

˙γO(q2 ◦γ)∥2 .
(10.18)
This is a joint optimization problem with a well-known solution.
a. Optimal Rotation: For a ﬁxed γ ∈ΓI, the optimization problem in
Eq. 10.18 over SO(n) is solved by O∗= UV T , where USV T is the singular
valued decomposition of A =  1
0 q1(t)(√˙γq2(γ(t)))T dt. In case the determi-
nant of A is negative, one needs to modify V by making its last column
negative (of it current value) before multiplying to U to obtain O∗. This is a
known result from rigid alignment of objects when the points across objects
are already registered.
b. Optimal Registration: For a ﬁxed O, the optimization problem in
Eq. 10.18 over ΓI can be solved using the dynamic-programming (DP)
algorithm or the gradient method described earlier in Chap. 5. Since the
cost function is deﬁned by the L2 norm and, thus, is additive over the
path (t, γ(t)), the DP algorithm applies. As described in Appendix B, the
DP algorithm forms a ﬁnite-dimensional grid in [0, 1]2 and searches over
all the paths on that grid, satisfying the required constraints, to obtain an
approximation to γ∗. WLOG, we will denote that estimated diﬀeomorphism
by γ∗.
Once the optimal rotation and re-parameterization of q2 are obtained, we
can compute the geodesic path between the orbits [q1] and [q2], as mentioned
above.
Algorithm 45. Compute a geodesic path between two curves in Rn represented
by their square-root velocity functions q1 and q2.
a. Initialize γ = γid.
b. Compute the n × n matrix A =
 1
0 q1(t)(√˙γq2(γ(t)))T dt. Perform SVD of
A = USV T and compute the optimal rotation O∗= UV T .

10.4 Shape Spaces of Curves in Rn
359
−0.5 0
0.5
1
−0.5
0
0.5
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
−0.6−0.4 −0.2 0 0.2 0.4 0.6 0.8 1
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
0
20
40
60
80
100
0
10
20
30
40
50
60
70
80
90
100
a
d
b
c
Fig. 10.3 (a) Two views of the curves β1 and β2, (b) the optimal matching γ∗between them
and (c) the right panel shows the optimal γ∗estimated by the DP algorithm. (d) The geodesic
path between the two shapes in S2 from two viewing angles
c. Perform the gradient process (or DP algorithm) to ﬁnd the optimal γ∗∈ΓI.
d. Update q2 →
 ˙γ∗O∗(q2 ◦γ∗).
e. If not converged, go to step (b).
Shown in Fig. 10.3 is an example of computing geodesics between open curves
in R3. In this toy example, we take two 2D contours and add a linear third
coordinate to them to make them open curves in R3. In the top-left panels, we
show the curves, β1 and β2, from two diﬀerent angles. The procedure outlined
above is used to ﬁnd the optimal rotation and re-parameterization of q2; these
matching results are shown in the top-right panels. The bottom two rows show
the resulting geodesic path in S2 from two viewing angles. Since the original
curves came from 2D shapes, it is easier to verify the correct registration and
the elastic nature of the resulting geodesic path.
Shown in Fig. 10.4 are two examples of geodesics between some cylindrical
helices. In each case, the panels (a) and (b) show the two helices, and (c) is
the optimal matching between them obtained using the estimated γ function
shown in panel (d). The resulting geodesic paths in S2 between these curves
are shown in the bottom row. It is easy to see the combination of bending and
stretching/compression that goes into deforming one shape into another. In the
left example, where the turns are quite similar and the curves diﬀer only in the
placements of these turns along the curve, a simple stretching/compression is
suﬃcient to deform one into another. However, in the right example, where the
number of turns is diﬀerent, the algorithm requires both bending and stretching
to reach from the ﬁrst to the second shape.
We can use the example of helical curves to compare the nature of geodesics
in the two shape spaces S1 and S2. Figure 10.5 shows an example of geodesics
between the same two curves but in diﬀerent spaces. The top example is for the
non-elastic shape space and the bottom one is for the elastic shape space. It is

360
10 Shapes of Curves in Higher Dimensions
0
50
100
150
20
40
60
80
100
120
140
0
50
100
150
20
40
60
80
100
120
140
0.2 0.4 0.6 0.8
1
1.2 1.4
−0.2
00.2
0
0.2
0.4
0.6
a
b
c
d
a
b
c
d
0.2 0.4 0.6 0.8
1 1.2 1.4
−0.2
0
0.2
0
0.2
0.4
0.6
Fig. 10.4 Coil matching: Two example of computing geodesics between 3D curves containing
spiral features. In each case (a) and (b) show the original curves, (c) shows the optimal regis-
tration between them, and (d) the optimal γ function. The lower panels show the corresponding
geodesic paths between them
Fig. 10.5 Comparison of geodesic paths between the same two curves in S1 (top) and S2
(bottom). The bottom right shows the matching of points in the elastic matching of curves
easy to see the preservation of features (loops, vertical parts, etc.) in the elastic
geodesic, as compared to the non-elastic one.
Finally, in Fig. 10.6, we present an example of comparing two protein back-
bones. In this experiment, we use two simple proteins—1CTF and 2JVD—that
contain three and two α-helices, respectively. The top row of this ﬁgure shows
depictions of the two backbones, while the bottom row shows the geodesic path
between them in S . The bottom left shows the registration of points between
the curves and the bottom right shows the estimated γ function that results in
that registration. These results on both simulated helices and real backbones
suggest a role for elastic shape analysis in protein structure analysis.

10.4 Shape Spaces of Curves in Rn
361
10
20
30 −5 0 51015
−20
−15
−10
−5
0
5
10
0
10
20
−10
0
10
−5
0
5
0
20
40
60
80
100
10
20
30
40
50
60
70
80
90
100
Fig. 10.6 Elastic deformations to compare shapes of two proteins: 1CTF and 2JVD (obtained
from PDB). The top row shows the two proteins: 1CTF on the left and 2JVD on the right.
The bottom row shows the elastic geodesic between them. The bottom left shows the optimal
registration between the two curves and the bottom right shows the optimal γ function
 2  3  1  4  5  6  7  9  8 10 12 11
0.1
0.2
0.3
0.4
0.5
0.6
(1)
(2)
(3)
(4)
(5)
(6)
(7) (8) (9)
(10) (11) (12)
Fig. 10.7 A set of helices with diﬀerent numbers and placements of spirals and their clustering
using the elastic distance function
10.4.3 Hierarchical Clustering of Elastic Curves
Figure 10.7 shows an example of using the elastic distances between curves for
clustering and classiﬁcation. In this example, we experiment with 12 cylindrical
helices that contain diﬀerent numbers and placements of turns. The ﬁrst three
helices have only one turn, the next three have two turns, and so on. (We point
out that the radii of these turns have some randomness that is diﬃcult to see with
the naked eye.) Using the elastic geodesic distances between them in S2, and the
dendrogram clustering program in MATLAB, we obtain the clustering shown in
the right panel. This clustering demonstrates the success of the proposed elastic
metric in that helices with similar numbers of turns are clustered together.

362
10 Shapes of Curves in Higher Dimensions
10.4.4 Sample Statistics and Modeling of Elastic
Curves in Rn
Now that the tool for computing geodesic in S2 is available, we can discuss the
computation of sample statistics and a probability model to capture shape varia-
tion in a population. We start with the sample statistics and Karcher mean.
Karcher Mean The very ﬁrst step in stochastic modeling of shapes is to compute
a statistical mean shape for a given collection of shapes. For this purpose, we use
the notion of Karcher mean that has been used by many papers for ﬁnding means
on nonlinear manifolds. For a given collection of curves {β1, β2, . . . , βn}, with shape
representations, {q1, q2, . . . , qn}, the Karcher mean is deﬁned as:
¯μn = argmin
[q]∈S2
n

i=1
ds([q], [qi])2 .
(10.19)
Although a global mean would be desirable, a local minimum is considered suf-
ﬁcient in this deﬁnition. Therefore, a gradient-based approach is used for ﬁnding
the Karcher mean, as follows.
Algorithm 46 (Karcher Mean on S2).
Let μ0 be an initial estimate of the
Karcher mean. Since the qi’s are elements of a hypersphere C2, it seems natural
to use their extrinsic mean (Eq. 9.7) in L2([0, 1], Rn) to initialize the gradient
algorithm. Set j = 0.
1. For each i = 1, . . . , n, compute the tangent vector vi ∈Tμj(S2) as follows: ﬁnd
the optimal (O∗
i , γ∗
i ) in Eq. 10.18 for minimizing the distance between μj and
[qi]. Set q∗
i = O∗
i

˙γ∗
i (qi ◦γ∗
i ), and compute:
vi =
θi
sin(θi)(q∗
i −cos(θi)μj),
where
cos(θi) = ⟨μj, q∗
i ⟩.
2. Compute the average direction ¯v = 1
n
n
i=1 vi.
3. If ∥¯v∥is small, then stop. Else, update μj in the direction ¯v using (Eq. 5.17)
μj+1 = cos(ϵ∥¯v∥)μj + sin(ϵ∥¯v∥) ¯v
∥¯v∥
where ϵ > 0 is small step size, typically 0.5.
4. Set j = j + 1 and return to Step 1.
An example of computing the Karcher mean of some curves in R3 is shown in
Fig. 10.8. The two left panels in (a) show ﬁve curves in R3 (from diﬀerent viewing
angles), panel (b) shows the estimated Karcher mean of their shapes, and panel (c)
shows the evolution of the cost function (given in Eq. 10.19) during the gradient
iterations.
Wrapped-Normal Density Given a mean shape μ ∈S2 and a covariance ma-
trix on a subspace of Tμ(S2), we can deﬁne a wrapped-normal density on S2.

10.4 Shape Spaces of Curves in Rn
363
0
0.5
1
−0.5
0
0.5
−0.6
−0.4
−0.2
0
0.2
0.4
a
b
c
0
0.5
1
−0.6
−0.4
−0.2
0
0.2
0.4
−0.6
−0.4
−0.2
0
0.2
0.4
−0.1
−0.05
0
0
0.2
0.4
0
0.1
0.2
0.3
0.4
0.5
0
2
4
6
8
10
12
14
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
Fig. 10.8 (a) A collection of ﬁve curves (shown from two diﬀerent viewpoints in the ﬁrst two pan-
els), (b) their Karcher mean (middle panel), and (c) the decrease in the cost function (Eq. 10.19)
during gradient iterations
Let the covariance operator K admit an orthonormal basis {w1, w2, . . . , wj} and
variances {σ2
1, σ2
2, . . . , σ2
j } such that:
K(s, t) =
j

i=1
σ2
i wi(s)wi(t) .
To generate samples from this wrapped-normal density, one simply generates co-
eﬃcients ai ∼N (0, σ2
i ) and forms a tangent function: w(t) = j
i=1 aiwi(t),
t ∈
[0, 1]. Then, using the exponential map w →cos(∥w∥)μ + sin(∥w∥) w
∥w∥, this tan-
gent vector is mapped to the shape space.
Figure 10.9 shows an experimental illustration of this idea. The top panel (a) in
this ﬁgure shows 20 spiral shapes that are used in this experiment. The ﬁrst step
is to compute a Karcher mean of these shapes as discussed above. The panel (b)
shows the decrease in the variance function (the cost function in Eq. 9.8) during
the estimation of the mean, panel (c) shows the estimated Karcher mean μ, and
panel (d) plots the singular values σ2
i of the covariance matrix. Since the ﬁrst two
singular values capture 95 % of the total variations, we use j = 2 components in
the wrapped-normal model. The ﬁrst two singular vectors form the orthonormal
basis elements w1 and w2. The bottom panel shows some random samples from
this estimated wrapped-normal density.
There are several important applications of shape analysis of open curves in
R3. Two common applications involve shapes of curves associated with the brain
anatomy—neuronal ﬁber tracts (especially in human brain) and sulcal curves
formed by folds of white matter. A sulcus is a depression or ﬁssure in the sur-
face of the brain that surrounds the gyri, creating the characteristic appearance
of the brain in humans (see Fig. 10.10 top). The problem of automatic labeling of
sulcal curves, i.e., putting an anatomical label on a sulcal curve using its diﬀerent
physical features, is a well-known problem. One feature that can be useful in this
process is the shape, and thus, shape analysis of sulcal curves becomes important.
Shown in Fig. 10.10 is an example of comparing shapes of two sulcal curves as
elements of S2.

364
10 Shapes of Curves in Higher Dimensions
−1 0 1
−1
0
1
0
2
4
6
8
a
−1 0 1
−1
0
1
0
2
4
6
8
−1 0 1
−1
0
1
0
2
4
6
8
−1 0 1
−1
0
1
0
2
4
6
8
−1 0 1
−1
0
1
0
2
4
6
8
−1 0 1
−1
0
1
0
2
4
6
8
−1 0 1
−1
0
1
0
2
4
6
8
−1 0 1
−1
0
1
0
2
4
6
8
−1 0 1
−1
0
1
0
2
4
6
8
−1 0 1
−1
0
1
0
2
4
6
8
−1 0 1
−1
0
1
0
2
4
6
8
−1 0 1
−1
0
1
0
2
4
6
8
−1 0 1
−1
0
1
0
2
4
6
8
−1 0 1
−1
0
1
0
2
4
6
8
−1 0 1
−1
0
1
0
2
4
6
8
−1 0 1
−1
0
1
0
2
4
6
8
−1 0 1
−1
0
1
0
2
4
6
8
−1 0 1
−1
0
1
0
2
4
6
8
−1 0 1
−1
0
1
0
2
4
6
8
−1 0 1
−1
0
1
0
2
4
6
8
0
5
10
15
20
0
0.002
0.004
b
c
d
0.006
0.008
0.01
−0.10
0.1
−0.1
0
0.1
−0.1
0
0.1
0.2
0.3
0.4
0
2
4
6
8
10
0
0.1
0.2
0.3
0.4
0.5
−0.10
0.1
−0.1
0
0.1
−0.1
0
0.1
0.2
0.3
0.4
−0.10
0.1
−0.1
0
0.1
−0.1
0
0.1
0.2
0.3
0.4
−0.10
0.1
−0.1
0
0.1
−0.1
0
0.1
0.2
0.3
0.4
−0.10
0.1
−0.1
0
0.1
−0.1
0
0.1
0.2
0.3
0.4
−0.10
0.1
−0.1
0
0.1
−0.1
0
0.1
0.2
0.3
0.4
−0.10
0.1
−0.1
0
0.1
−0.1
0
0.1
0.2
0.3
0.4
−0.10
0.1
−0.1
0
0.1
−0.1
0
0.1
0.2
0.3
0.4
−0.10
0.1
−0.1
0
0.1
−0.1
0
0.1
0.2
0.3
0.4
−0.10
0.1
−0.1
0
0.1
−0.1
0
0.1
0.2
0.3
0.4
−0.10
0.1
−0.1
0
0.1
−0.1
0
0.1
0.2
0.3
0.4
Fig. 10.9 (a) A collection of 20 spiral curves used in this experiment, (b) the decrease in the
Karcher variance function during mean estimation, (c) the estimated Karcher mean and (d) the
estimated singular values of the covariance matrix. (e)–(i) random samples from the estimated
wrapped-normal density in S2

10.5 Registration of Curves
365
−20 −100 10 20 30
−10
0
10
20
30
−20
−10
0
10
20
30
−20
0
20
−10
0
10
20
30
−20
−10
0
10
20
30
0
20
40
60
80
100
0
10
20
30
40
50
60
70
80
90
100
Fig. 10.10 The top cartoon shows the sulcal curves formed by wrinkled surface of a brain. The
next row shows two example sulcal curves and an optimal registration between them. The bottom
two rows show the geodesic path between then in S2, shown from two diﬀerent viewpoints
10.5 Registration of Curves
In this section, we take a look at the problem of registering curves, both pairwise
and group-wise, in Rn. These curves may come, for example, as descriptions of
a dynamic system evolving over time. Let β(t) ∈Rn denote a feature vector de-
scribing properties of interest in a system and let β = {β(t)|t ∈[0, T ]} deﬁne the
full trajectory of this evolution. One goal may be to develop statistical models for
analyzing observations of such systems. One can either use time series models, e.g.,
autoregressive models, or standard function data analysis, using the Hilbert struc-
ture of the space of such curves, to perform modeling and analysis. However, the
following issue makes this problem both interesting and challenging. The temporal
rate of evolution of the system is captured in the parameterization of the curve:
t →β(t). If the system goes through the same feature values, in the same order,
but at diﬀerent rates, then we can express the new curve as a re-parameterization
of the earlier trajectory: ˜β(t) = β(γ(t)). The challenge is to compare trajectories
in such a way that the rate of evolution of the trajectories does not aﬀect the infer-
ence. In other words, the trajectories β and ˜β are equivalent from the perspective
of analysis. Another related problem is that of registration: which point on one
curve β1 is registered with which point on another curve β2. In order to improve
comparisons between observations of dynamic systems, it is important to ﬁnd a
suitable registration of time points across observations.

366
10 Shapes of Curves in Higher Dimensions
10.5.1 Pairwise Registration of Curves in Rn
Pairwise registration of curves in Rn is essentially an extension of the problem
studied in Sect. 4.4, where we discussed registration of real-valued functions. Now
we are considering vector-valued functions. Also, this problem is very similar to
shape analysis of contours in Rn except we do not perform rotational alignment.
Following Deﬁnition 4.7, the pairwise registration problem can be stated as:
Deﬁnition 10.3 (Registration Problem). For any two curves β1 and β2, with
SRVFs denoted by q1 and q2, the registration for β1 and β2 is solved using:
inf
γ∈ΓI ∥q1 −(q2, γ)∥.
(10.20)
We illustrate this idea using an application in computer vision. Here one tries
to identify human action using a depth camera (e.g., Microsoft’s Kinect) that pro-
vides a representation of human shape in the form of a body skeleton at each
observation time. The observation of an action over a time interval provides a
sequence of skeletons. This sequence shows the changes in body pose and articu-
lation during performance of that action. Figures 10.11, 10.12, 10.13 show some
examples of these sequences. The ﬁrst ﬁgure shows two observations of the same
Sequence 1, b1
Sequence 2, b2
Sequence 2 re-parameterized, b2 ◦g1∗
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
Warping g∗
Plots of |q1(t)−q2(t)| and |q1(t)−q2(g ∗(t))

˙g ∗(t)|
2
2
Fig. 10.11 First two rows: Sequences of skeletons display the action “two-hand wave” by dif-
ference people. Notice the time lag in β1 before the swing starts. Third row: Re-parameterized
β2 after temporal alignment with the second sequence

10.5 Registration of Curves
367
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0.1
Sequence 1, b1
Sequence 2, b2
Sequence 2 re-parameterized, b2 ◦g1∗
Warping g∗
Plots of |q1(t)−q2(t)| and |q1(t)−q2(g ∗(t))

˙g ∗(t)|
2
2
Fig. 10.12 First two rows: Sequences of skeletons display the action “one-arm wave” by diﬀer-
ence people. Notice the time lag in β1 before the swing starts. Third row: Re-parameterized β2
after temporal alignment with the second sequence
action two-hand wave, the second shows one-arm wave, and the third shows pickup
and throw. Since a skeleton is made up of 20 joints, each having a position in R3,
we can represent a sequence as a trajectory β : [0, T ] →R60. The SRVF of a tra-
jectory is deﬁned as usual by q(t) =
˙β(t)
√
| ˙β(t)|, for ˙β(t) ̸= 0, and q(t) = 0 if ˙β(t) = 0.
Such an SRVF is the mapping q : [0, T ] →R60.
Given SRVFs q1 and q2 of two such sequences, we ﬁnd the optimal alignment
between them using Eq. 10.20 and the dynamic-programming algorithm. The im-
plementation details are same as in the scalar alignment (Sect. 4.4) and are not
repeated here. We present some experimental results in Figs. 10.11–10.13. In each
case, we take two sequences of the same action—a two-hand wave in the ﬁrst case,
a one-arm wave in the second, and a pickup and throw in the last—and show their
alignment. The top two rows in each ﬁgure show the given sequences β1 and β2,
and one can see the time lags between them in starting of the actions. Then, we
show the aligned sequences β2 ◦γ∗in the third row, and one can see the improve-
ment in the level of alignment between β1 and β2 ◦γ∗. This can be also seen in the
plot of |q1(t) −q2(γ∗(t))

˙γ∗(t)|2 versus t; this plot generally lies below the plot
of |q1(t) −q2(t)|2, quantifying the improvements made in temporal registration of
two sequences. The bottom left shows the optimal warping function γ∗in each
case.

368
10 Shapes of Curves in Higher Dimensions
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0.1
Sequence 1, b1
Sequence 2, b2
Sequence 2 re-parameterized, b2 ◦g1∗
Warping g∗
Plots of |q1(t)−q2(t)| and|q1(t)−q2
2
2
(g ∗(t))

˙g ∗(t)|
Fig. 10.13 Same as Fig. 10.11 but with the action “pickup and throw”
10.5.2 Registration of Multiple Curves
This process can be extended to the alignment of multiple curves using the frame-
work described in Sect. 8.3. While we studied scalar functions there, the focus
here is on vector-valued functions. This, however, does not make much of a dif-
ference in the underlying concepts and even the algorithms remain mostly the
same. Following that section, we can deﬁne the notions of phases and amplitudes
of vector-valued functions and use the same deﬁnitions of phase and amplitude
distances. In view of the close similarity of the solutions in the two cases, we will
not cover the problem of registration of multiple curves in detail but present an
interesting example to illustrate the idea. Shown in the top panel of Fig. 10.14
are three examples of skeletal sequences involving the action “one-arm horizontal
wave.” Treating each sequence as a curve in R60, we use a simple extension of
the phase-amplitude separation procedure presented in Algorithm 33 to align six
such sequences. The middle panel shows the sequence mean before and after the
alignment and the bottom panel shows the decrease in the cost function during
alignment. Although the two mean sequences appear very similar, a closer look
shows that the action is more clearly deﬁned in the aligned mean, especially toward
the end of the action.

10.6 Shapes of Closed Curves in Rn
369
Three of Six Sequences Used in Experiment
Pre-Alignment Mean
Post-Alignment Mean
0
5
10
15
20
25
30
1
1.5
2
2.5
3
3.5
4
4.5
Decrease in Objective Function During Aligment
Fig. 10.14 An example of alignment six action sequences, all involving a one-arm horizontal
wave, using a simple extension of Algorithm 33. Each sequence is viewed as a curve in R60
10.6 Shapes of Closed Curves in Rn
Next we look at extending our analysis of curves in higher dimensions to closed
curves. We will brieﬂy describe the bending-only framework for handing closed
curves and will largely focus on the elastic framework for handling this type of
data.
10.6.1 Non-elastic Closed Curves
Let β : [0, 1] →Rn be a diﬀerentiable unit-length curve, parameterized by the arc
length, and let θ(s) = ˙β(s) ∈Sn−1 be its direction function, as deﬁned earlier.

370
10 Shapes of Curves in Higher Dimensions
Since we are interested in closed curves, we describe that subset next. Deﬁne a
map Φ1 by Φ1(θ) =
 1
0 θ(s)ds, and let:
C c
1 = Φ−1
1 (0) ≡{θ : [0, 1] →Sn−1|Φ1(θ) = 0} .
(10.21)
Here 0 denotes the origin in Rn. Since Φ1 is a smooth map and 0 is a regular point
of Rn, the set C c
1 is a submanifold. C c
1 is actually the set of all closed curves in Rn,
each represented by its angle function. For any θ ∈C c
1 , the tangent space Tθ(C c
1 )
is given by:
Tθ(C c
1 ) = {v : [0, 1] →Rn|v(s) ⊥θ(s), ∀s ∈[0, 1], and
 1
0
v(s)ds = 0} . (10.22)
We will continue to use the L2 metric to impose a Riemannian structure on the
manifold C c
1 . By now the reader will be familiar with the non-elastic deformation
resulting from the use of the L2 metric in the space of direction functions.
The next goal is to develop a tool for computing geodesic paths between ele-
ments of C c
1 . Due to the closure constraint, C c
1 is a complicated nonlinear manifold
and there is no ready expression for evaluating geodesics in it. We will use the path-
straightening method, ﬁrst described in Sect. 6.6, for this problem. In order to use
this method, we need several basic tools relating to projections on C c
1 and its
tangent spaces.
1. Projection of θ ∈C1 into C c
1 : If θ /∈C c
1 , i.e., it denotes an open curve, we
want to close it by projecting it into C c
1 . We perform this iteratively by moving
perpendicular to the level sets of the map Φ1 in such a way that Φ1 of the re-
sulting point moves toward the origin in Rn. In other words, we seek a direction
g such that a perturbation of θ in that direction results in Φ1(θ + g) = 0, to the
ﬁrst order.
We develop some theory for implementing this idea. Consider the linear map-
ping dΦ1,θ : Tθ(L2([0, 1], Rn)) →Rn deﬁned by dΦ1,θ(v) =
 1
0 v(s)ds. It can
be shown that dΦ1,θ is surjective, as long as θ([0, 1]) is not contained in a one-
dimensional subspace of Rn, and therefore C c
1 is a codimension n submanifold of
L2([0, 1], Rn). The adjoint of dΦ1,θ, dΦ∗
1,θ : Rn →Tθ(L2([0, 1], Rn)) is the unique
linear transformation with the property that for all v ∈Tθ(L2([0, 1], Rn)) and
x ∈Rn, (dΦ1,θ(v) · x) =
$
v, dΦ∗
1,θ(x)
%
. This adjoint is given by dΦ∗
1,θ(x) ≡v
such that v(s) = x −(x · θ(s))θ(s). In other words, dΦ∗
1,θ takes a vector x in Rn
and forms a tangent vector ﬁeld on f by making x perpendicular to θ(s) for
all s (or by projecting x onto the tangent space Tθ(s)(Sn−1) for each s). This
formula makes explicit the role of θ in deﬁnition of dΦ∗
1,θ.
Proposition 10.1. The range space of dΦ∗
1,θ is the orthogonal complement of
the null space of dΦ1,θ:
{v ∈Tθ(P)|v = dΦ∗
1,θ(x) for some x ∈Rn}
= {v ∈Tθ(L2([0, 1], Rn))|dΦ1,θ(v) = 0}⊥.
(10.23)
Proof. If v is in range space of dΦ∗
1,θ, i.e., there exists a x such that dΦ∗
1,θ(x) = v.
Then, for any g in the null space of dΦ1,θ, we have ⟨v, g⟩=
$
dΦ∗
1,θ(x), g
%
=
(x · dΦ1,θ(g)) = 0. That is, v is perpendicular to the null space of Φ1,θ. The

10.6 Shapes of Closed Curves in Rn
371
proposition follows from the fact that the dimension of the range space of dΦ∗
1,θ
is equal to the codimension of the null space of dΦ1,θ.
⊓⊔
Let e1, e2, . . . , en be the canonical basis of Rn and deﬁne Eθ,i = dΦ∗
1,θ(ei) ∈
Tθ(L2([0, 1], Rn)) for all i. The functions Eθ,i form a basis of the range space of
dΦ∗
1,θ. Using Eq. 10.23, these three functions also form a basis for the orthogonal
complement of null space of dΦ1,θ.
Since g is perpendicular to the level set of Φ1 at θ, it is orthogonal to the null
space of dΦ1,θ and is in the range space of dΦ∗
1,θ. Therefore, g can be written
as a linear combination of the functions Ei,θ, i = 1, 2, . . . , n. Using Taylor’s
approximation, we get:
Φ1(θ + g) ≈Φ1(θ) + dΦ1,θ(g) = 0
if
g = −A−1
θ Φ1(θ) ,
where Aθ ∈Rn×n is the Jacobian of dΦθ restricted to the space orthogonal to
the null space of dΦ1,θ. The Jacobian Aθ can also be speciﬁed using the same
basis functions. The algorithm to implement this projection is given next.
Algorithm 47 (Projection of Arbitrary θ into C c
1 ).
a. For i = 1, 2, . . . , n, compute Ei,θ = dΦ1,θ(ei) and Aθ(:, i) = dΦ1,θ(Ei,θ).
b. Compute b = −A−1
θ Φ1(θ).
c. Set g = n
i=1 b(i)Ei,θ.
d. For all s in [0, 1], update θ(s) using:
θ(s) →cos(|g(s)|)θ(s) + sin(|g(s)|) g(s)
|g(s)| .
e. If |Φ1(θ)| < ϵ, stop. Else, return to Step 1.
Shown in Fig. 10.15 are three examples of this projection. There we see three
open curves in solid plane lines and their projections into C c
1 using marked
lines.
2. Projecting v ∈L2([0, 1], Sn−1) into the subspace Tθ(C c
1 ). For a vector ﬁeld
v ∈Tθ(L2([0, 1], Sn−1)), we have that (v(s) · θ(s)) = 0. However,  1
0 v(s)ds may
−1.5−1−0.50
0
0.5
1
1.5
2
−1
−0.5
0
0.5
1
1.5
2
2.5
3
3.5
−1.5
−1
−0.5
0
0
0.5
1
1.5
2
2.5
−1.5
−1
−0.5
0
0.5
1
1.5
2
2.5
−3
−2
−1
0
−1.5
−1
−0.5
0
0.5
1
0
0.5
1
1.5
2
Fig. 10.15 Projection of open curves into C c
1 using Algorithm 47. The plots show open curves
(plane lines) and their projections (marked lines) in R3

372
10 Shapes of Curves in Higher Dimensions
not be zero, and we need to ensure that for it to be an element of Tθ(C c
1 ). This
can be achieved using a one-time projection:
Algorithm 48 (Projection of Arbitrary v into Tθ(C c
1 )).
a. For i = 1, 2, . . . , n, compute Ei,θ = dΦ∗
1,θ(ei) and Aθ(:, i) = dΦ1,θ(Ei,θ).
b. Set b = −A−1
θ Φ1(θ).
c. Compute g = n
i=1 b(i)Ei,θ.
d. Update: for all s in [0, 1), set v(:, s) = v(s) −g(s).
These two algorithms are suﬃcient to specify the path-straightening algorithm
for computing a geodesic between any two points θ1, θ2 ∈C c
1 . This procedure is
basically same as Algorithm 19 with a few modiﬁcations for the pre-shape space
here.
Algorithm 49 (Path Straightening on C c
1 ).
1. Initialize a path α between θ1 and θ2. One way is to form a geodesic between
them in C1 (using Eq. 10.8) and then project each point on the path into C c
1 .
Note that α(0) = θ1 and α(1) = θ2. The projection into C c
1 is accomplished
using Algorithm 47.
2. Compute the velocity vector
dα
dt
using ﬁnite diﬀerences (similar to Algo-
rithm 12). (This and the next two steps use Algorithm 48.)
3. Compute the covariant integral u of
dα
dt using ﬁnite sums (similar to Algo-
rithm 13).
4. Compute the backward parallel transport of u(1) along α (similar to Algo-
rithm 14). Call it ˜u.
5. Compute the gradient w of the path energy according to:
w(τ/k) = u(τ/k) −(τ/k)˜u(τ/k) ,
τ = 1, 2, . . . , k .
6. Update the path α in the direction of w using the exponential map on C1. (For
each time τ, this is same as computing expα(τ)(δw(τ)), where exp is the expo-
nential map on the unit sphere Sn−1.) Then, project each point on the resulting
path to C c
1 using Algorithm 47. If ∥w∥is small, then stop. Else, return to Step 2.
Figure 10.16 illustrates a geodesic computed using this algorithm. In this ﬁgure,
the two end shapes (top-left two panels), evolution of the energy (top right), and
two views of the ﬁnal geodesic path (middle and lower panels) are displayed.
10.6.2 Elastic Closed Curves
Now we look at the task of analyzing shapes of closed curves in Rn using the elastic
representation. Given a parameterized closed curve β : [0, 1] →Rn, we choose to
represent it using the square-root velocity function q(t) =
˙β(t)
√
|
˙
β(t)|, and since β is of
length one,
 1
0 | ˙β(t)|dt = 1. Additionally, in order to restrict to the closed curves,

10.6 Shapes of Closed Curves in Rn
373
0
0.2
0.4
0.6
0.8
1
1.2
−0.2
0
0.2
0.4
0.6
0.8
1
−1
−0.5
0
−0.2
0
0.2
0.4
0.6
0.8
1
1.2
0
0.2
0.4
0.6
0.8
1
1.2
−0.8
−0.6
−0.4
−0.20
0.2
0
2
4
6
8
10
12
14
11
12
13
14
15
16
17
18
19
20
21
0
a
b
c
2
4
6
8
10
12
14
16
−2
−1
0
0
2
4
6
8
10
12
14
16
−1
0
1
2
Fig. 10.16 (a) and (b) show the two curves β1 and β2, and (c) shows the evolution of E as
Algorithm 49 proceeds. Bottom two rows show two views of the resulting geodesic path between
β1 and β2
we impose the condition,
 1
0 q(t)|q(t)|dt = 0. Thus, we have a space of unit-length
closed curves represented by their square-root velocity functions:
C c
2 = {q ∈L2([0, 1], Rn)|
 1
0
|q(t)|2dt = 1,
 1
0
q(t)|q(t)|dt = 0}.
C c
2 is a submanifold of the Hilbert space L2([0, 1], Rn). The tangent space to C c
2 at
a point q is a subset of L2([0, 1], Rn) with the standard L2 inner product. Due to
the nature of C c
2 , it is easier to specify the normal space, i.e., the space of functions
in L2([0, 1], Rn) perpendicular to Tq(C c
2 ). The normal space is given by:
Nq(C c
2 ) = span {q, ( qi
||q||q + ||q||ei), i = 1, . . . , n} ,
(10.24)
where ei is a unit vector in Rn along the ith coordinate axis. Hence:
Tq(C c
2 ) = {w ∈L2([0, 1], Rn)| ⟨w, v⟩= 0,
∀v ∈Nq(C c
2 )} ,
where ⟨, ⟩denotes the standard inner product on L2([0, 1], Rn).
Projection on C c
2 This procedure requires an important step of closing the
curves, which we describe next. We particularize the general approach presented

374
10 Shapes of Curves in Higher Dimensions
in Sect. 6.3 to project arbitrary elements of C2 into C c
2 . We will basically repeat
Algorithm 18 but this time for curves in Rn. We deﬁne a mapping Ψ : C2 →Rn,
where:
Ψ(q) =
 1
0
q1(t)|q(t)|dt,
 1
0
q2(t)|q(t)|dt, . . . ,
 1
0
qn(t)|q(t)|dt

.
Also, for i = 1, 2, . . ., n, deﬁne:
bi(t) ≡qi(t)
|q(t)|q(t) + |q(t)|ei −q(t)
 1
0

q(u), qi(u)
|q(u)|q(u) + |q(u)|ei

du
= qi(t)
|q(t)|q(t) + |q(t)|ei −2q(t)
 1
0
qi(u)|q(u)|du.
(10.25)
The next step is to deﬁne a residual vector r(q) = Ψ(q) ∈Rn and to evolve q in
the direction perpendicular to the level set of Ψ so as to move its Ψ image toward
the origin in Rn. Algorithm 50 describes the procedure to project an open curve
q ∈C2 into C c
2 . The Jacobian for this projection is an n×n matrix whose elements
are given by Jij = ⟨bi(t), bj(t)⟩, with bis deﬁned in Eq. 10.25. The algorithm for
projection is as follows:
Algorithm 50 (Projection of q ∈C2 to C c
2 ). Let ϵ > 0.
1. Compute r(q) = Ψ(q) ∈Rn. If |r(q)| < ϵ, stop; otherwise, continue.
2. Calculate the Jacobian matrix J(q) given above.
3. Solve the equation J(q)β = −r(q) for β.
4. Deﬁne dq = 2
i=1 βibi, where bis are given in Eq. 10.25.
5. Update using q →cos(∥dq∥)q + sin(∥dq∥) dq
∥dq∥.
6. Go to Step 1.
Geodesic Computations The next task is to develop a computational tool that,
for a given pair q0, q1 ∈C , computes a geodesic path α : [0, 1] →C2 such that
α(0) = q0 and α(1) = q1. For this task, we will use the path-straightening approach
described in Chap. 6. In this approach, the given shapes are connected by an initial
arbitrary path that is iteratively “straightened” until it becomes a geodesic. This
iteration is performed using the gradient of an energy E[α], where:
E[α] = 1
2
 1
0
⟨˙α(t), ˙α(t)⟩dt .
(10.26)
Let dc(q0, q1) be the length of the resulting geodesic between them.
Algorithm 51 (Path Straightening on C c
2 ).
1. Initialize a path α between q1 and q2. One way is to form a geodesic between
them in C2 (unit hypersphere) and then project each point on the path in C c
2 .
The geodesic on a hypersphere is given by, for τ = 0, 1/k, . . ., 1:
α(τ) =
1
sin(θ) [sin(θ −τθ)q1 + sin(τθ)q2]
(10.27)

10.6 Shapes of Closed Curves in Rn
375
where θ = cos−1(⟨q1, q2⟩). Note that α(0) = q1 and α(1) = q2. The projection
into C c
2 is accomplished using Algorithm 50.
2. Compute the velocity vector dα
dt (similar to Algorithm 12 but for curves in Rn
rather than planar curves).
3. Compute the covariant integral u of dα
dt (similar to Algorithm 13).
4. Compute the backward parallel transport of u(1) along α (similar to Algo-
rithm 14).
5. Compute the gradient w of the path energy (similar to Algorithm 15).
6. Update the path α in the direction of w (similar to Algorithm 16). If ∥w∥is
small, then stop. Else, return to Step 2.
Now we have a numerical procedure for constructing geodesics between points
in C c
2 . An element in C c
2 is invariant to translation and scaling of the curve it
represents but is aﬀected by rotations and re-parameterizations of curves. This
remaining variability is modeled as the actions of the groups SO(n) and ΓS on C c
2
as follows:
SO(n) × C c
2 →C2,
(O, q) = Oq
ΓS × C c
2 →C c
2 ,
(γ, q) = (q ◦γ)

˙γ .
It has been discussed previously in Chap. 5 that the actions of SO(n) and ΓS on C c
2
commute. Therefore, we can form a joint action of the product group SO(n) × ΓS
on C c
2 according to:
((O, γ), q) = O(q ◦γ)

˙γ .
Furthermore, this joint action is by isometries of C c
2 . Therefore, we can deﬁne a
quotient space S c
2 = C c
2 /(SO(n)×ΓS) with the L2 metric inherited from the larger
space C c
2 . Let [q] denote the orbit of q ∈C c
2 under the joint action of SO(n) × ΓS.
The geodesic distance between any two points in S c
2 is given by:
ds([q]0, [q]1) =
arginf
(O,γ)∈SO(n)×ΓS
dc(q0, O(q1 ◦γ)

˙γ) .
(10.28)
Thus, we need to solve a joint minimization problem on SO(n) × ΓS to form
geodesics in S . We will use a gradient-based search to minimize over the two
groups.
The procedure for computing geodesics in the shape space S c
2 is very similar
to Algorithm 22, except for the changes due to rotation space.
Algorithm 52 (Geodesic Computation in S c
2 ). Let q1 and q2 be two elements
of C c
2 . Set ˜q2 = q2, γ = γid, and O = I2.
1. Compute a geodesic path from ˜q2 to q1 using Algorithm 51. Let w be the initial
velocity of this geodesic path. That is, w = exp−1
˜q2 (q1).
2. Compute the two gradients as follows:
a. Compute the gradient of the cost function with respect to the rotation O
using xi = ⟨w, Ei˜q2⟩, where Ei forms an orthonormal basis of the space
TIn(SO(n)). Set dO = exp(ϵ1

i=1 xiEi).
b. Compute the gradient of the cost function with respect to the re-parameterization
according to:
dγ = ϵ2
n

i=1
⟨w, dΥγid(bi)⟩bi .
(10.29)

376
10 Shapes of Curves in Higher Dimensions
−0.2
0
0.2
0.4
0.6
0.8
1
−0.6
−0.5
−0.1
−0.05
0
0.5
1
1.5
2
−0.6
−0.5
−0.4
−0.15
−0.1
−0.05
Fig. 10.17 Examples of geodesic paths between closed curves in R3 in the shape space S c
2
where dΥγid is deﬁned in Eq. 6.46 and {bi}, i = 1, . . . , d are as given in
Eq. 6.45.
3. Update γ →γ + dγ and O →O.dO.
4. Update β2 according to ˜β2 = O(β2 ◦γ). Compute ˜q2 =
˙˜β2

| ˙˜β2|
.
5. If |x| and ∥dγ∥are small, then stop. Otherwise, return to Step 1.
Figure 10.17 displays two examples of computing geodesic paths between closed
curves in R3, treated as elements of the shape space S c
2 .
10.7 Shape Analysis of Augmented Curves
So far we have looked at the shapes formed by curves in Euclidean spaces and have
developed techniques for analyzing these shapes. In case there is any additional
information associated with curves, other than just their shapes, it can also be used
in this framework to help improve comparisons and registrations of curves. We will
term this additional information as auxiliary information and seek to involve it in
matching, deformation, and comparison of curves. Let’s motivate this goal with a
number of examples.
• Shape analysis of tubes: Consider the problems of studying shapes of general-
ized cylinders—3D structure represented by medial axes (a non-self-intersecting
curve in R3) and scalar functions along axes representing thickness or radii.
These objects can be viewed as ﬂexible tubes with a continuous (and variable)
radius function along the medial axis. If the thickness were constant, we could
just focus on the medial axis of the tube and analyze its shape. In general cases,
however, where the thickness changes along the curve, we would like to analyze
shapes of tubes by taking into account both the shape of the medial axis and
the varying thickness of tube around that axis. Thus, we treat the medial axis
as a parameterized curve in R3 and the thickness as an auxiliary function along
the curve.
• Shape analysis of colored curves: Consider color images of objects on the
right side of Fig. 10.18. There are two types of features that characterize ob-
jects in these images: shapes of their boundaries and textures (or colors) along

10.7 Shape Analysis of Augmented Curves
377
0
100 200 300 400 500 600 700
0
100
200
300
400
0
200
400
Generalized cylinder
Colored curve
Fig. 10.18 High-dimensional functions representing both shape and color coordinates along
objects’ boundaries
these boundaries. Together these two features capture the full appearance of
the boundary in an image.
Thus, we are interested in studying colored contours—their shapes as well as
their colors. The shape coordinates are viewed as a simple closed curve in the
image plane, while the color coordinates are viewed as real- or vector-valued
functions along that curve. If the image is gray scaled, then the auxiliary func-
tion is one-dimensional, but if the image is colored (with RGB scale), then
this function is three-dimensional. More generally, one can form a vector-valued
function, of arbitrary dimensions, from texture values in and around the bound-
ary curves.
• Shape analysis of curves with landmarks: Another possible scenario for
augmented curves is the curves having some predetermined landmarks associ-
ated with them. Let a curve represent the boundary of an anatomical object in
an MRI or an X-ray image. Very often there are anatomical landmarks, such
as corners, junctions, etc., associated with the objects that can be detected and
labeled separately (using manual techniques or otherwise). When comparing
shapes of similar anatomical objects, it becomes relevant or even necessary to
ensure that the corresponding landmarks match each other. In this case, we can
create an artiﬁcial auxiliary function, a scalar function to represent landmark
information. This function peaks at given landmarks and smoothly decays to
zero away from them.
• Shape analysis of functional curves: Yet another scenario, related to the
earlier examples, is when a curve has an arbitrary function deﬁned on it that can
potentially inﬂuence shape matching and comparisons. Take the example from
brain anatomy where one is interested in the shapes of neuronal ﬁber tracks us-
ing the diﬀusion MRI image data. Along the estimated ﬁber tracts, one can char-
acterize certain anatomically relevant functions—diﬀusivity, anisotropy, etc.—
that provide clues about the anatomical functionality of that region. We can
formulate the given information into two components: one, the coordinates of a
ﬁber tract in R3 depicting its shape and the other a scalar-valued anatomically
relevant function along the curve. The latter forms the auxiliary component in
this application.
In these and other related scenarios, we obtain scalar- or vector-valued auxiliary
functions, along with the coordinate function of curves, and we would like the

378
10 Shapes of Curves in Higher Dimensions
process of matching, deformation, and comparison to be based on joint shape-
auxiliary information. All this while maintaining appropriate constraints and in-
variances.
What are the main challenges in doing so? We are going to concatenate the
shape and the auxiliary coordinates to form curves in higher dimensions and will
try to perform shape analysis of these resulting curves. The challenge, however,
comes from the fact that shape and auxiliary coordinates feature diﬀerent in-
variances. The shape coordinates are invariant to rotation, translation, and scale
transformations, but the auxiliary coordinates may have only some or none of
these invariances. For instance, one cannot rotate the larger vector as the shape
and auxiliary components will get mixed with each other. Also, note that the reg-
istration of shape and auxiliary coordinates goes together. In other words, one has
to be re-parameterized in the same way as the other. We cannot treat them as two
independent re-parameterization (registration) problems.
10.7.1 Joint Representation of Augmented Curves
Let the shape coordinate function along a closed curves be given by βs : D →Rn
and the auxiliary function be given by βt : D →Rk, where k is an arbitrary
dimension related to the auxiliary function. Here, D = [0, 1] for a general curve
and D = S1 for a closed curve. For a planar shape, βs simply represents the xy
coordinates of the curve in the plane. We can combine these two components to
form a joint shape and texture curve: β(t) =
 βs(t)
bβt(t)

∈Rn+k. Here b > 0 is a
parameter introduced to control the inﬂuence of the auxiliary function, relative to
the shape function. The image example in Fig. 10.18 (right side) shows an example
where the auxiliary function is given by the magnitude of the color vector (RGB
values) along the curve. In this simple case, the auxiliary function is simply the
height function with the augmented curve becoming a parameterized curve in R3
(n = 2, k = 1).
For the resulting augmented curve, we can deﬁne the square-root velocity func-
tion q using Eq. 10.2. Note that, since q is deﬁned using the time derivative of β,
q is invariant to the translation of β. That is, if we add a constant vector to β, its
mathematical representative q will not change. While that is desirable for the shape
component, it may or may not be desirable for the auxiliary component. In case we
want the auxiliary component to be dependent on its translation, we simply add
its mean value separately in the representation, as described later. Continuing, we
can deﬁne the pre-space, this time called the augmented pre-space, as:
C c
2 = {q : D →R(n+k)|

D
⟨q(t), q(t)⟩dt = 1,

D
|q(t)|q(t) dt = 0} .
(10.30)
The last condition is needed only if the original curves are closed. An element of
C c
2 is a (n + k)-dimensional curve that is both closed and of ﬁxed length. C c
2 is an
inﬁnite-dimensional nonlinear manifold and we endow it with the L2 Riemannian
metric. For a point q ∈C c
2 , and any two tangents g, f ∈Tq(C c
2 ), we deﬁne the inner
product ⟨g, f⟩=
 1
0 ⟨g(t), f(t)⟩dt, where the inner product inside the integral is the
standard Euclidean product in Rn+k. With this Riemannian metric, the augmented

10.7 Shape Analysis of Augmented Curves
379
pre-space becomes a Riemannian manifold and one can compute geodesics in it
using the path-straightening approach. The algorithm for computing geodesics in
this space is identical to Algorithm 51 (presented in the previous section) and is
not repeated here.
10.7.2 Invariances and Equivalence Classes
Now we consider the desired invariances in the representation and use the notion
of equivalence classes to achieve those invariances. We remind the reader that
the shape component should be invariant to translation, rotation, scale, and re-
parameterization, while the auxiliary component should, in a general situation,
only be invariant to re-parameterization. If the auxiliary function has an additional
invariance, we can include it in the representation accordingly. In case of images,
where the auxiliary function denotes the intensity or colors along the curves, these
intensity values are often centered and scaled to a unit norm, in order to make
them invariant to translation and scale.
1. Scaling: In shape analysis of a curve, we need to remove the scaling variability
but we may or may not want to rescale the auxiliary component. If we look at
the augmented pre-space, the condition

D ⟨q(t), q(t)⟩dt = 1 ensures that the
two components have together been rescaled to a standard value. Note that this
condition scales the two components of β by the same amount. If one wants to
preserve the original scale of the auxiliary components, this can be done using
the constant b in the deﬁnition of β. This constant can also be used to scale the
two components diﬀerently.
2. Rotation: For shape analysis, we need to remove the rotational variance and
we have done that in the past by forming equivalence classes (orbits) under the
action of the rotation group. In the current situation, we seldom want to remove
the rotation for the augmented curves. This removal is restricted only to the
shape components, leaving the auxiliary components unchanged. This can be
done as follows: Deﬁne a n(n−1)/2 dimensional subgroup of the rotation group
SO(n + k) according to:
R =
SO(n) 0
0
Ik

⊂SO(n + k),
where Ik is the k × k identity matrix. For any q ∈C c
2 and O ∈R, the func-
tion Oq represents an augmented function whose shape component has been
rotated while the texture component remains unchanged. We deﬁne the rota-
tional equivalence class q as the set:
[q]R = {Oq|O ∈R} .
Two elements of [q]R diﬀer only by a rotation of the shape component. For
reaching the augmented shape space of augmented curves, we will remove only
the subgroup R and not the full rotation space SO(n + k).
3. Translation: While the shape component is invariant to translations of the
object, the auxiliary component may or may not change with the translation.

380
10 Shapes of Curves in Higher Dimensions
As noted earlier, the deﬁnition of q, involving the time derivative of β, has
already removed the translation for both the components. In case we want the
auxiliary component to be dependent on its translation, we need to bring back
the translation of β0 in the representation. We do so by introducing a constant
vector:
¯β0 =

D βt(t)dt

D dt
∈Rk .
This is the mean value of the auxiliary function along the contour. The new
representation of β is given by the pair (q, ¯β0).
4. Re-parameterization: We seek to use both shape and auxiliary coordinates
in performing curve registrations. Setting the shape of the augmented curve
to be invariant to re-parameterizations, we simply follow the same approach
as earlier. Let ˜ΓD be the re-parameterization group on a domain D. For any
q ∈C2, we deﬁne its equivalence class under re-parameterization to be:
[q] 
ΓD = closure{(q ◦γ)

˙γ|γ ∈˜ΓD} .
For reaching the shape space of augmented curves, we will remove ˜ΓD from the
pre-shape space C2. It should be emphasized that the two components are not
being re-parameterized diﬀerently. Any point on the curve has a shape coor-
dinate and an auxiliary coordinate, and these two coordinates remain coupled
together despite a re-parameterization of this curve.
Combining diﬀerent equivalence relations, we obtain the shape space S c
2 = C c
2 /
(R × ˜ΓD) whose elements are equivalence classes of the type:
[q] = {O(q ◦γ)

˙γ|O ∈R, γ ∈˜ΓD} .
The full representation now includes [q], the equivalence class of q, and ¯β0, the
mean of the texture function. Therefore, the total space for joint analysis of shapes
and textures is given by:
T = S c
2 × Rk ,
and b ∈R+ acts as a control parameter.
To compare any two objects, represented by ([q1], ¯β1
0) and ([q2], ¯β2
0), we use the
distance function:
d(β1, β2; b) =

ds([q1], [q2])2 + |¯β1
0 −¯β2
0|2

(10.31)
where ds(q1, q2) is the geodesic distance in S c
2 and |·| is the Euclidean distance in
Rk. Note that for diﬀerent values of b, the resulting geodesic path and the geodesic
distance will be diﬀerent.
The resulting geodesic provides (i) an optimal deformation of one augmented
curve to another and (ii) an optimal registration of points across these two curves.
The latter needs more elaboration. The geodesic matching across the two curves
is inﬂuenced by both the shape and the auxiliary coordinates; the actual level of
inﬂuence depends on the control parameter b. Consider the planar shapes of two
hand outlines and immerse them in an artiﬁcial texture image made of Gaussian

10.7 Shape Analysis of Augmented Curves
381
180
160
140
120
100
80
60
40
0
20
40
60
80
100
180
160
140
120
100
80
60
40
0
20
40
60
80
100
Fig. 10.19 Top row: two hand shapes immersed in artiﬁcial texture. Bottom row: the texture
functions along the two curves after smoothing
random ﬁelds with diﬀerent means, as shown in the top row of Fig. 10.19. For
convenience, we show the corresponding texture functions (obtained after some
smoothing for noise reduction) along the two curves in the bottom row. Combining
the 2D coordinate function βs and the 1D texture function βt, we obtain augmented
3D shape-texture function β for each image.
We compute the square-root velocity function q for each object and compute
geodesic paths between them in S c
2 using the path straightening described in
Algorithm 52. Shown in Fig. 10.20 are the resulting geodesic paths for diﬀerent
values of the control parameter b. Rather than drawing augmented curves in R3, we
use a color scheme to show the third (texture) coordinate. All the curves are drawn
using their shape components and are colored using their texture components. For
a very small value of b, the contribution from the texture function is very small
and the geodesic is primarily based on the shape components of the two curves. In
the top row, where b is rather small, the matching of points across curves is based
mostly on the shape features. The ﬁngers in one hand are accurately matched to
the ﬁngers in the second hand, irrespective of the texture values associated with
them. As we go down the rows, the role of texture becomes increasingly prominent
in matching and deformations of curves. In the bottom row, where b is quite large,
the matching is completely governed by the texture function. Here, the blue part
of one hand is matched to the blue part of the second hand, irrespective of their
shapes. The actual matchings of points on the two curves, for the two extreme
cases (b very small and b very large) are displayed in Fig. 10.21.

382
10 Shapes of Curves in Higher Dimensions
Fig. 10.20 (Colored ﬁgure) Geodesics between joint shape-texture functions in the space S2
for diﬀerent values of b. A small value of b puts more emphasis on the shape component, while a
large value of b emphasizes the texture component more. All curves are drawn using their shape
components and are colored using their texture components
−0.2−0.15−0.1−0.05 0
0.05 0.1 0.15 0.2 0.25
−0.1
−0.05
0
0.05
0.1
0.15
0.2
0.25
−0.05
0
0.05
0.1
0.15
0.2
0
0.05
0.1
0.15
A
A
B
B
C
C
D
D
E
E
F
F
G
G
H
H
I
I
J
J
Fig. 10.21 Registration of points across curves. The matching on the left is for 2D shape
analysis, while that on the right is for 3D joint shape-texture analysis when a high weight (b) is
placed on the texture component
10.8 Problems
10.8.1 Theoretical Problems
1. Let θ(s) = ˙β(s) be the direction function of an arc-length parameterized curve
β in Rn. If θ ∈L2([0, 1], Rn), then what space does the corresponding β lie in?
2. For the mapping F : Rn →Rn as deﬁned in Eq. 10.1, show that F is continuous
at 0 ∈Rn.
3. Using the deﬁnition of SRVF, verify the formula β(t) = β(0) +
 t
0 |q(s)|q(s)ds.

10.8 Problems
383
4. Assuming that all the components of β : [0, 1] →Rn are absolutely continuous,
with the absolute continuity as deﬁned in Eq. 4.1, show that the corresponding
SRVF q is in L2([0, 1], Rn).
5. For a diﬀerentiable, parameterized curve β : [0, 1] →Rn, let θ and φ be the
direction and the log-speed functions of β, respectively. Verify the following
statements:
a. If (φ, θ) is the representation of β, then (φ, Oθ) is the representation of the
rotated curve Oβ for any O ∈SO(n).
b. Similarly, show that (φ ◦γ + ln ◦˙γ, θ ◦γ) is the representation of the re-
parameterized curve β ◦γ, for any γ ∈ΓI.
6. Show that the actions of the rotation and re-parameterizations groups are by
isometries under the elastic metric. In other words, verify Eq. 10.3.
7. Derive an expression for the SRVF q of a curve β in terms of its direction and
log-speed functions, θ and φ, respectively.
8. Show that the elastic metric, given in Eq. 10.3, reduces to the non-elastic met-
ric, given in Eq. 10.6, when the log-speed function is forced to be a zero func-
tion.
9. Prove that Eq. 10.8 provides an expression for the shortest geodesic path be-
tween any two (non-degenerate) curves under the non-elastic metric. In other
words, given any two direction functions θ1, θ2, that are curves themselves on
Sn−1, show that α satisﬁes the properties of a geodesic path (as deﬁned in
Sect. 3.2).
10. Verify the expression for the gradient given in Eq. 10.11.
11. Show that the action of the product group SO(n) × +
ΓI on C2, given by
((O, γ), q) = O(q ◦γ)√˙γ, is by isometries under the L2 metric.
12. Verify the expression for the tangent space Tθ(C c
1 ) given in Eq. 10.22.
13. Similarly, verify the expression for the normal space of C c
2 inside L2([0, 1], Rn)
given by Eq. 10.24.
14. Deﬁne a curve β1 : [0, 1] →Rn by β1(t) = tu, where u ∈Rn is any con-
stant vector. Let β2 be an absolutely continuous curve with the property
that
$
u, ˙β2(t)
%
> 0 for all t ∈[0, 1]. Find a formula for re-parameterization
γ : [0, 1] →[0, 1] such that β1 ◦γ is optimally matched to β2 according to Eq.
10.20.
15. Repeat the previous problem without the assumption that
$
u, ˙β2(t)
%
> 0
for all t ∈[0, 1]. (You must carefully consider the subintervals of [0, 1] where
$
u, ˙β2(t)
%
> 0 and
$
u, ˙β2(t)
%
< 0.)
16. Show that embedding of SO(n) inside SO(n+k), denoted by R in this chapter,
forms a subgroup of SO(n + k).
17. Prove that d(β1, β2; b), given in Eq. 10.31, forms a proper distance on the
space of augmented curves, for all 0 < b < ∞. Provide interpretations for the
extreme cases: b = 0 and b →∞.

384
10 Shapes of Curves in Higher Dimensions
10.8.2 Computational Problems
1. Write a program to re-parameterize a given curve in Rn using the arc-length
parameterization and provide a given number T of sample points along the
curve. Also, add a procedure to rescale the curve to be of length one.
2. Write a program that takes an arc-length parameterized curve and computes
its angle function for at T uniform points in the interval [0, 1].
3. Write a program to implement Algorithm 44 and to compute the non-elastic
geodesic path between any two given curves in Rn.
4. Write a program to compute the SRVF of an arbitrary parameterized curve
β : [0, 1] →Rn.
5. Write a program to compute a geodesic path between any two parameterized
curves in the pre-shape space C2 using Eq. 10.8.
6. Implement Algorithm 45 to compute a geodesic path between any two curves,
represented by their SRVFs q1 and q2, in the shape space S2.
7. Write a program to compute mean of k given parameterized curves using
Algorithm 46.
8. Write a program to estimate parameters of the truncated wrapped-normal
density using PCA of the shooting vectors obtained in the previous problem.
Additionally, write a code that randomly samples from this TWN model and
shows the corresponding random shapes generated from TWN model.
9. Write a program to perform temporal alignment of two arbitrary curves in Rn
using Eq. 10.20. Display the function |q1(t) −q2(t)|2 versus t before and after
the alignment.
10. Write a program to temporally align k given curves in Rn, following Algo-
rithm 33, modiﬁed suitably to handle vector-valued functions.
11. Implement Algorithm 47 to project an arbitrary θ ∈L2([0, 1], Sn−1) into C c
1 .
12. Implement Algorithm 49 to compute a geodesic path between arbitrary arc-
length parameterized, closed curves in Rn.
13. Write a program to project an arbitrary (non-degenerate) element of C2 into
the set C c
2 , using Algorithm 50.
14. Implement the path-straightening Algorithm 51 for computing geodesic paths
in C c
2 , the pre-shape space of elastic, closed curves.
15. Implement Algorithm 52 for computing geodesic paths in S c
2 , the shape space
of elastic, closed curves in Rn.
10.9 Bibliographic Notes
The framework for shape analysis of curves in Euclidean spaces was described
in the paper [106]. The use of shape analysis of protein backbones in protein
structure analysis was explored in a series of papers, including [70]. Shape analysis
of 3D curves has also been used successfully in face recognition using range images
[107, 98, 27], RNA shape analysis [60], shape-based classiﬁcation of sulcal curves
and DTI ﬁber tracts [73], and the use of skeletal trajectories for characterizing
human actions [8, 13]. Shape analysis of augmented curves was introduced in
[69, 68]. More recently, the idea of shape analysis of augmented curves has been
used to analyze axonal trees in [82, 81].

Chapter 11
Related Topics in Shape Analysis
of Curves
Previously we have developed theory and computational solutions for shape
analysis associated with curves in Euclidean spaces. We end this textbook by
presenting some related topics that fall outside the main framework. These mis-
cellaneous topics do not ﬁt an organized theme but are bunched together in this
chapter for convenience. They include: (1) Investigate the use of shape in conjunc-
tion with other features, such as scale, pose, and position, to characterize curves.
(2) Extend the group of shape-invariant transformations, from the similarity group
to the aﬃne group, in the case of planar closed curves. While most shape analy-
sis works consider similarity transformations (rigid motions and global scales) as
the main shape-preserving transformations, some applications, including imaging,
may require us to nullify aﬃne distortions of curves. (3) Develop techniques for
analyzing trajectories on nonlinear Riemannian manifolds. While we have studied
only the Euclidean curves so far, there is also a strong need for analyzing curves on
other, perhaps nonlinear, domains. One may not use the word shape for character-
izing the desired properties of these curves, but this analysis should be invariant
at least to parameterizations of these trajectories.
11.1 Goals and Challenges
The goals and challenges associated with the topics covered in this section are the
following:
1. Joint analysis of shape and related features: This book emphasizes that
the removal of unwanted (nuisance) variables is the main challenge in shape
analysis. One often starts with Euclidean representations and moves toward
nonlinearity (and quotient spaces) as the nuisance variables are sequentially re-
moved. So, what happens if we want to involve some of these nuisance variables
in our studies? One way of doing this is to form product spaces, involving shape
spaces and spaces of other variables, with the natural product metrics. For in-
stance, to combine shape and scale, we can form the product space S c
2 × R+,
with the elastic metric on S c
2 and a Euclidean metric on R+. While this is a
perfectly valid approach, it leaves an extra degree of freedom in choosing the
relative weight of the two components. Another approach, perhaps preferable
© Springer-Verlag New York 2016
A. Srivastava, E.P. Klassen, Functional and Shape Data Analysis,
Springer Series in Statistics, DOI 10.1007/978-1-4939-4020-2 11
385

386
11 Related Topics in Shape Analysis of Curves
in some situations, is to not remove the variables that are desirable, since they
are no longer considered nuisance. As the removal of these variables actually
complicates shape analysis, the non-removal simpliﬁes it. Taking the second
approach, we will form representation spaces that leave some variables of inter-
est involved while still removing the remaining nuisance variables. A common
theme here is that the parameterization variability of curves is always treated
as nuisance, and, consequently, the analysis is always based on the elastic Rie-
mannian framework.
2. Aﬃne-invariant shape analysis: Here we want to consider the aﬃne group
GL(2), and we would like to quotient out the action of this group from a chosen
representation of planar closed curves. Additionally, we want to maintain the
invariance to the re-parameterization group as earlier. In summary, we seek
elastic shape analysis of planar curves while deﬁning equivalence relations under
the larger (aﬃne) group. The diﬃculty comes from the fact that for all the
metrics discussed earlier in this text, the action of the aﬃne group is not by
isometries. Hence, we cannot deﬁne shape space as the quotient space and
inherit the metric from the larger representation space. This case was discussed
in Sect. 3.9 where: (1) the action is not by isometries and (2) a group action
admits a section that is not orthogonal. We will take the approach suggested
there: identify a section S of the manifold M under the action of the group G
and deﬁne a metric on S. (It turns out that the chosen section is orthogonal.)
Then, use this metric structure to deﬁne a metric structure on the quotient
space M/G.
3. Rate-invariant analysis of trajectories on Riemannian manifolds: We
explain the goals and challenges for this item by considering a relatively simple
manifold S2. Consider a set of smooth curves of the type βi : [0, 1] →S2 and our
goal is to develop a framework for comparisons, registration, and summarization
of these trajectories. Furthermore, we want this framework to be invariant to
how the trajectories are parameterized. That is, the metric between βi and βj
has the same value as that for βi ◦γi and βj ◦γj, where γi, γj ∈ΓI. In the same
spirit as the elastic comparison of curves, the principal challenge here is the
registration of points across curves, in a principled manner. The use of SRVFs
is not feasible since derivatives of these trajectories are vector ﬁelds along the
curves and cannot be directly compared due to nonlinearity of the domain. The
solution comes from transporting these vector ﬁelds to a common domain where
diﬀerent curves can be naturally compared.
11.2 Joint Analysis of Shape and Other Features
While shape analysis is important in many applications, there has also been a need
to study shape in conjunction with other features. For example, in the studies of
biological growth, it is important to measure the overall size of the growing objects
in addition to their changing shapes. Similarly, in certain anatomical structures,
it is important to take their relative locations and orientations into account while
deciding on their normality or abnormality. If we consider a continuous curve, then
all its physical characteristics can be summarized using shape, scale, location, and
orientation. While it is convenient to work with parameterized curves, a parame-
terization is merely for the convenience of analysis and is not an intrinsic property

11.2 Joint Analysis of Shape and Other Features
387
of a curve, like the previous four properties. Similar to the Chaps. 5 and 6, we
seek a framework for analyzing curves that can incorporate any arbitrary subset
of these properties, depending upon the needs of an application. This allows for
generating meaningful comparisons between subjects and populations as well as
for performing classiﬁcation. Furthermore, it provides tools for computation of
statistics such as the mean and covariance. To demonstrate these ideas in a con-
crete setting, we shall develop tools for comparing, clustering, and classifying white
matter ﬁbers in the human brain, obtained from diﬀusion-tensor MRI (DT-MRI)
data as parameterized curves in R3.
Similar to the framework laid out in Chap. 10, consider the set of all absolutely
continuous parameterized curves in Rn. The set of all rigid rotations of these curves
is SO(n), the set of all re-parameterizations of curves is +
ΓI, the set of scales is R+,
and for translation is Rn. Let β : [0, 1] →Rn be a parameterized curve.
11.2.1 Geodesics and Geodesic Distances
on Feature Spaces
In this section we describe diﬀerent feature spaces, their Riemannian structures
and the corresponding geodesic equations.
1. Shape, Scale, and Orientation: In case we are interested in comparing curves
using their shapes, scales and orientations, we can do so directly using their
SRVFs in L2 with the Riemannian structure described in Chap. 10. The only
variability we have to remove here is the re-parameterization. To remove the
re-parameterization variability, we utilize the algebraic operation of forming
a quotient space Ssso ≡L2([0, 1], Rn)/+
ΓI. Recall that the action of +
ΓI on
L2([0, 1], Rn) is given by: (q, γ)(t) ≡q(γ(t))

˙γ(t). Hence, an orbit under ˜ΓI is
given by [q] = {q ◦γ)√˙γ|γ ∈˜ΓI} = closure{(q ◦γ)√˙γ|γ ∈ΓI}, and Ssso =
{[q]|q ∈L2}. The space Ssso inherits a Riemannian structure from L2 and one
can deﬁne geodesics and distances in Ssso as follows. For any q1, q2 ∈L2,
deﬁne an approximation to the optimal re-parameterization according to: γ∗=
arginfγ∈ΓI ∥q1−(q2◦γ)√˙γ∥. Let q∗
2 = (q2◦γ∗)
 ˙γ∗be the optimal representation
of the second curve. Then, the geodesic distance between the orbits of q1 and q2
is dsso([q1], [q2]) .= ∥q1 −q∗
2∥and the geodesic path between them in Sa is the
straight line α(τ)(t) = (1−τ)q1(t)+τq∗
2(t). Figure 11.1a displays an example of
a geodesic path and correspondence between two curves with diﬀerent shapes,
scales, and orientations.
2. Shape and Scale: In this case we want to include only the shape, and the size
of the curves in the analysis and the orientations are no longer relevant. We can
modify the previous setup to reﬂect this change as follows. The quotient space
of equivalent curves is now deﬁned by Sss .= Ssso/SO(n) = L2([0, 1], Rn/( ˜ΓI ×
SO(n)). The orbits are now given by [q] = {O(q ◦γ)√˙γ|γ ∈˜ΓI, O ∈SO(n)} ,
and Sss = {[q]|q ∈L2([0, 1], Rn)}. For any q1, q2 ∈L2, deﬁne the optimal
re-parameterization and rotation of the second curve according to: (γ∗, O∗) =
arginfγ∈ΓI,O∈SO(n) ∥q1 −O(q2 ◦γ)√˙γ∥. The optimization over ΓI is performed
using the DP algorithm and over SO(n) is performed using the SVD (Procrustes
alignment). Let q∗
2 = O∗(q2 ◦γ∗)
 ˙γ∗be the optimal representation of the

388
11 Related Topics in Shape Analysis of Curves
Features Included
(a)
(b)
(c)
(d)
Geodesic Path
Matching
sso: Shape
+Orientation+Scale
ss: Shape+Scale
so: Shape+Orientation
2: Shape
Fig. 11.1 An example of geodesic paths in diﬀerent feature spaces
second curve. Then, the geodesic distance between the orbits of q1 and q2 is
dss([q1], [q2]) .= ∥q1 −q∗
2∥. Similarly, the geodesic path between them in Sb is
simply the straight line. Figure 11.1b displays an example of a geodesic path and
optimal correspondence between two curves with diﬀerent shapes and scales.
3. Shape and Orientation: If we are interested in comparing curves accord-
ing to their shapes and orientations, but not their scales and positions, we
can simply rescale all the curves to a ﬁxed length, say one, and then ana-
lyze the SRVFs. As discussed earlier, the set of all SRVFs representing curves
of length one is C2 = S∞. While the scale variability has been removed,
the parameterization variability still remains. To take care of that, we de-
ﬁne a quotient space Sso
.= C2/ ˜ΓI. The elements of this quotient space
are the orbits given by [q] = {(q ◦γ)√˙γ|q ∈C2, γ ∈+
ΓI}. The geodesic
distance in Sc is calculated by solving the following minimization problem:
γ∗= arginfγ∈ΓI cos−1(

q1, √˙γ(q2 ◦γ)

) = arginfγ∈ΓI ∥q1−√˙γ(q2◦γ)∥. The last
equality comes from the fact that minimizing the arc length between points on
a sphere is the same as minimizing the chord length between them. This latter
minimization can be solved using the DP algorithm. Let q∗
2 =
 ˙γ∗(q2 ◦γ∗).
Then, the geodesic path between [q1] and [q2] in Sc is given by the great
circle connecting them in C2, and the geodesic distance between them is
dso([q1], [q2]) .= cos−1(
$
q1,
 ˙γ∗(q2 ◦γ∗)
%
). Figure 11.1c displays an example of

11.2 Joint Analysis of Shape and Other Features
389
a geodesic path and correspondence between two curves with diﬀerent shapes
and orientations.
4. Incorporating Curve Positions in Analysis: In some applications there is
a need for including the position in the analysis of curves. For example, in the
problem of classifying and labeling sulcal curves in a human brain, the relative
locations of these curves can play an important role. There are two solutions to
this issue. The ﬁrst solution is to use the SRVFs directly to compute distances
between curves and then add a weighted portion of a translational distance.
This allows for some ﬂexibility, in that the weight of translation in the ﬁnal
distance is decided by the user. On the other hand, we may be inclined to use
a diﬀerent representation for curves that automatically includes their relative
positions in the metric and still remains invariant to the group action of +
ΓI.
In order to study shape and other features of curves, we will represent pa-
rameterized curves using either their SRVFs or a new representation introduced
next.
Deﬁnition 11.1 (Square-Root Function (SRF)).
Deﬁne the SRF to be
a function h : [0, 1] →R3 given by h(t) =

| ˙β(t)|β(t). If the curve β is re-
parameterized by a γ ∈ΓI, its SRF changes to h(t) →(h, γ)(t) ≡h(γ(t))

˙γ(t).
This representation has the advantage (from our current perspective) that it
includes the position information about the curve. The drawback is that it is
not straightforward to reconstruct a curve from its SRF. While this limits our
ability to draw a geodesic path between any two curves, or to compute sample
statistics of curves, we can still obtain a distance with all the desired properties.
The group actions of SO(n) and ΓI on SRFs remain the same as they were for
SRVFs. The shape space is deﬁned as Sall .= L2([0, 1], Rn)/+
ΓI. The distance
between two SRFs in L2 is deﬁned as ∥h1 −h2∥, while the distance in Sall
is: dall([h1], [h2]) .= arginfγ∈ΓI ∥h1 −√˙γ(h2 ◦γ)∥. Once again, the optimization
over γ is performed using the dynamic programming algorithm.
11.2.2 Feature-Based Clustering
We illustrate these deﬁnitions by clustering a set of curves several times, each
time using a diﬀerent metric deﬁned above. We present clustering results using an
artiﬁcial dataset ﬁrst and then DT-MRI brain ﬁbers.
1. Clustering of Artiﬁcial Data: It is easier to evaluate the clustering perfor-
mance for an artiﬁcial dataset since the ground truth is known. First, we will
study clustering of artiﬁcial curves based on their shapes (ds); shapes and ori-
entations (dso); shapes and scales (dss); shape, scales, and orientations (dsso);
and shapes, scales, orientations, and positions (dall), using the Ward clustering
technique (in MATLAB) with a complete linkage function.
Figure 11.2 (top row) displays some examples the data used here that consists
of curves with combinations of three distinct shapes, two scales, two rotations,
and two diﬀerent positions. The total data consists of 96 diﬀerent curves such
that each combination of shape, scale, rotation, and translation is included in
the data three times. The bottom row shows the (96 × 96) geodesic distance

390
11 Related Topics in Shape Analysis of Curves
0
2
4
0
2
4
0
a
b
c
d
e
1
2
3
4
5
−4 −2 0
2
4
6
8
−2
0
2
4
6
−2
0
2
4
6
View 1
ds
dso
dss
dsso
dall
View 2
Fig.
11.2 Top:
types
of
3D
curves
used
in
the
clustering
experiment.
Bottom:
ma-
trices
of
distances
using
features:
(a)
shape,
(b)
shape+orientation,
(c)
shape+scale,
(d) shape+orientation+scale, and (e) shape+orientation+scale+translation
matrices as gray scale images for each combination of features for this artiﬁ-
cial data. We see a very nice separation between curves with diﬀerent features
based on our distances. This validates our claim that SRVF and SRF represen-
tations, and the associated geodesic distances are successful in clustering curves
according to diﬀerent feature combinations.
2. Clustering of DT-MRI Brain Fibers: Now we consider ﬁber tract data from
speech regions of a human brain. The ﬁber clustering here partitions brain ﬁbers
into groups that provide physical evidence of the complexity and intricacy of
the connecting patterns between the Broca’s and Wernicke’s areas. It should
be pointed out that this clustering is solely data driven—whether or not it
represents the true physiological picture remains unknown. One way to validate
this is to correlate the clustering results with fMRI data, which can be used to
depict functional connections between the language areas.
The datasets considered here consist of 388 ﬁbers from four subjects: subject
1 had 176 total ﬁbers, subject 2 had 68 total ﬁbers, subject 3 had 48 total ﬁbers,
and subject 4 had 88 total ﬁbers. Since we consider the locations of ﬁbers as an
important feature, we use SRFs (dall) to compute pairwise distances between
them. The results are shown in Fig. 11.3. We see a clear separation of clusters
based on geodesic distances, which take into account shape, translation, scale,
and orientation. In order to enhance the display of the clusters, we have used
a multidimensional scaling (MDS) method to obtain 2D coordinates of each
ﬁber and displayed them in a scatter plot. The distance matrix and the MDS
plot for subject 1 indicate that there should be two clear clusters. This is in
fact the case, and we see that translation plays a very important role in this
particular case. The distance matrix and the MDS plot for subject 2 indicate
that there should be two clear clusters here as well. But, looking at the plotted
ﬁbers, this separation is not as clear. We note that the green cluster should be
split up into two separate clusters, based on the diﬀerent shapes, translations,
and orientations of the ﬁbers. In the case of subject 3, there are three clusters.
The blue and the green clusters diﬀer most in shape, while the green and the
red clusters diﬀer a lot in their position in R3. There are two clusters present
for subject 4, but it appears that some of the ﬁbers in the red cluster have a

11.3 Aﬃne-Invariant Shape Analysis of Planar Curves
391
Subject 1
Subject 2
Subject 3
Subject 4
55
60
65
70
75
80
85
80
85
90
25
30
35
40
55
60
65
70
75
80
80
85
90
20
25
30
55
60
65
70
75
80
80
85
15
20
25
30
35
50
55
60
65
70
75
80
85
80
85
90
20
25
30
35
40
−100
0
100 200 300 400 500 600
−200
−100
0
100
200
300
−400
−300
−200
−100
0
100
−200
−150
−100
−50
0
50
100
150
200
−100 −50
0
50
100
150
200
−150
−100
−50
0
50
100
−200 −100
0
100
200
300
400
−250
−200
−150
−100
−50
0
50
100
150
200
250
Fig. 11.3 Top: clustered ﬁbers. Middle: distance matrix between clusters. Bottom: MDS plot
(diﬀerent colors denote diﬀerent ﬁber clusters)
signiﬁcantly diﬀerent orientation and could possibly be split into two diﬀerent
clusters. The shapes in the green cluster are fairly similar.
Similar to Chaps. 9 and 10, one can develop tools for computing mean, co-
variances, principal subspaces, and generating statistical models for curves under
diﬀerent combinations of features.
11.3 Aﬃne-Invariant Shape Analysis of Planar Curves
As mentioned at the start of this chapter, the past chapters have focused on making
the framework invariant to similarity transformations (translation, rotation, and
global scaling). However, in a variety of practical situations, especially those arising
in imaging contexts, the observations are aﬀected by transformations that are
more complicated than the similarity group alone. These types of transformations
occur, for example, when the image plane of a camera is not parallel to the plane
containing the deﬁning part of the shape or when a camera images the same scene
from diﬀerent viewing angles. Here shapes become transformed by perspective
eﬀects, and one often needs to go beyond the similarity group to deﬁne shape
equivalences. The aﬃne and projective groups are both larger than (and contain)
the similarity group and are commonly used to model such shape deformations
brought about by perspective skew. In this context, the focus is now on developing
shape analysis methods that are invariant to aﬃne and projective transformations.
1. Aﬃne Group: The aﬃne group for a plane is the semi-direct product GA ≡
GL(2) ⋉R2 with the action given by: GA × R2 →R2
((A, b), x) = Ax + b .

392
11 Related Topics in Shape Analysis of Curves
In case the matrix component A is restricted to matrices with determinant +1,
the resulting group is called the special aﬃne group. In general, A has all four
degrees of freedom, as opposed to the similarity shape analysis where the A
matrix has only two degrees of freedom (one for rotation and one for scaling).
Along with the translation, GA has six degrees of freedom.
2. Projective Group: The projective transformation of a point in R2 is based
on the action of the projective general linear (PGL) group as outlined in one of
the exercises at the end of this chapter. It is expressed as the quotient group
PGL(3) = GL(3)/Ω where Ω is a one-dimensional group of 3 × 3 diagonal
matrices with all diagonal entries being the same. The action of PGL(3) on
R2 is deﬁned by embedding R2 in R3, applying GL(3), and then projecting
back to R2. One way to embed R2 in R3 is to add a ﬁxed value, say 1, as the
last component, i.e., x ≡(x1, x2) ∈R2 →(x1, x2, 1) ∈R3. With this choice of
embedding, the projective transformation PGL(3) × R2 →R2 is given by:
(B, x) = ˜x,
where

˜x
1

=
B

x
1


B
 x
1

3
.
The number of degrees of freedom in choosing a projective transformation is
9 −1 = 8, 9 for GL(3) and 1 for Ω.
These transformations and their degrees of freedom are summarized in Table 11.1.
Figure 11.4 shows examples of similarity, aﬃne, and projective transformations
applied to a square, to help explain their diﬀerences. The aﬃne and projective
Table 11.1 Summaries of the diﬀerent group actions on R2
Groups
Their actions (R2 →R2)
Space
Degrees of freedom
Similarity
x →aOx + y
O ∈SO(2), y ∈R2, a ∈R
Four
Aﬃne
x →Ax + b
A ∈GL(2), b ∈R2
Six
Projective
x →
B[x,1]T
(B[x,1]T )3
B ∈P GL(3)
Eight
Similarity
Afﬁne
Projective
Fig. 11.4 Each row shows three examples of diﬀerent transformations of a unit square

11.3 Aﬃne-Invariant Shape Analysis of Planar Curves
393
transformations can better capture the distortion introduced in imaged contours
when objects are imaged from diﬀerent angles. Although the aﬃne group allows
for skew eﬀects, parallel lines still remain parallel after aﬃne transformations. The
extra degrees of freedom present in the projective group can model perspective
distortions where parallel lines are skewed to converge to vanishing points located
at inﬁnity. The aﬃne group can be used to approximate these perspective eﬀects
seen in some situations, such as when the camera is far enough away from the
imaging plane or when there is only a slight change in viewing angle. In this chap-
ter, we will focus only on aﬃne-invariant shape analysis of curves, especially using
elastic Riemannian metrics. The corresponding analysis for projective-invariant
shape analysis remains to be developed.
11.3.1 Global Section Under the Aﬃne Action
Can we directly apply the previously developed framework (Chap. 6) for aﬃne-
invariant shape analysis of planar closed curves? The answer is no, and the reason
is that the action of the aﬃne group on R2 is not by isometries under the Euclidean
norm (this is simple to show and is left as an exercise for the reader). Suppose we
choose the SRVF representation of curves (and the L2 norm) for analyzing shapes
of curves, then the curves are viewed as elements of the set L2([0, 1], R2). In order
to form an aﬃne-invariant shape space, we will need to construct the quotient
space L2([0, 1], R2)/(GA × ˜ΓS) and seek to inherit the L2 norm from the larger
space L2([0, 1], R2) to this quotient space. Ignoring ˜ΓS momentarily, we focus only
on the action of GA. As mentioned above, GA does not act by isometries on the
set L2([0, 1], R2) under the L2 norm. So, we cannot inherit the L2 metric on to the
quotient space L2([0, 1], R2)/GA. Thus, we will take the approach called Method
(3) in Table 3.1 (Sect. 3.9). This method involves deriving a global section of
the larger space under the action of the group and choosing a metric structure
on this section. (Recall from Deﬁnition 3.18 that the global section of a set M,
under the action of a group G, is the subset of M such that it intersects each
orbit of G in one and only one point.) In the current case, where the larger space
is L2([0, 1], R2) and the group is GA, we will reach a global section in two steps.
We will ﬁrst specify a set F c
a of standardized curves such that each orbit of GA
in L2([0, 1], R2) is associated with a standardized curve but under an arbitrary
rotation. In other words, F c
a is a section of L2([0, 1], R2), under the action of GA,
only up to the action of SO(2). By itself, it is not quite a global section, but its
quotient space F c
a/SO(2) will act as the desired global section. Since this quotient
space is bijective to the quotient space L2([0, 1], R2)/GA, we can simply induce a
metric on the latter using a metric on the former. As described next, it is natural to
use the elastic shape metric on that section. We remark that the section identiﬁed
here is a global section but not an orthogonal section of L2([0, 1], R2) under the
action of GA.
Sectioning of Curve Space
Next we derive a global section of L2([0, 1], R2)
under GA. We already know (see Sect. 5.4) that the SRVF representation provides
a bijection between F2, the set of unit-length absolutely continuous curves that
start at the origin (deﬁned ﬁrst in Sect. 5.4), and C2. Similarly, F c
2 ⊂F2, the
subset of closed curves, has a bijection with C c
2 . Therefore, instead of deﬁning a
section of C c
2 , we can equivalently deﬁne a section of F c
2, especially since it is

394
11 Related Topics in Shape Analysis of Curves
an easier task. In fact, this task of ﬁnding a canonical element in the aﬃne orbit
of any given curve has been termed aﬃne standardization in the literature. Let
β : S1 →R2 be an element of F c
2 , and the GA-orbit of β is the set
[β]GA =
0
Aβ + b | A ∈GL(2), b ∈R21
.
Recall that Lβ =

S1 | ˙β(t)|dt is the length of the curve β. Additionally, deﬁne
Cβ =
1
Lβ

S1 β(t)| ˙β(t)|dt ∈R2 as the centroid and
Σβ = 1
Lβ

s1(β(t) −Cβ)(β(t) −Cβ)T | ˙β(t)|dt ∈R2×2
as the covariance of β. Note that for our purposes below, a closed curve is called
degenerate if it is completely contained in a line.
Theorem 11.1. For any non-degenerate β there exists a standardized element
β∗∈[β]GA, the GA-orbit of β, that satisﬁes the following three conditions: (1)
Lβ∗= 1, (2) Cβ∗= 0, and (3) Σβ∗∝I. Furthermore, for any two curves β1, β2 ∈
[β]GA, the corresponding standardized elements, β∗
1 and β∗
2, are related by a rotation
and re-parameterization, β∗
2 = O(β∗
1 ◦γ), where O ∈SO(2) and γ ∈ΓS.
For a proof of this result, we direct the reader to the online supplementary material
associated with the paper [21]. We deﬁne the set of aﬃne-standardized curves as
F c
a = {β ∈F2 | Lβ = 1, Cβ = 0, Σβ = cβI2, and β is closed} ⊂F c
2 ,
(11.1)
where cβ is a real scalar. Note that due to the unit-length condition, we may not
achieve Σβ = I2 exactly but only within a constant. This section F c
a implicitly de-
ﬁnes the curve-aﬃne pre-shape space. Furthermore, the quotient space F c
a/SO(2)
forms a bijection with the quotient space L2([0, 1], R2)/GA.
Aﬃne Standardization of Curves
This approach requires that given an ar-
bitrary element β ∈F c
2 we should be able to project it in F c
a and ﬁnd its aﬃne-
standardized representative. Since an analytical expression for standardization is
not known, we develop an iterative algorithm to standardize a given curve β; call
that standardized curve β∗. It is trivial to ﬁnd the transformations that satisfy the
ﬁrst two conditions, i.e., scaling and centering, and therefore we focus on the more
complicated task of satisfying the third condition, i.e., the covariance condition,
and assume we are given β with unit length and centroid located at the origin.
Thus, we focus on ﬁnding a group element A∗∈GL(2) such that ΣA∗β ∝I.
In fact, it can be shown that it is suﬃcient to search only over the subspace
S(2) ∩GL(2) ⊂GL(2), where S(2) is the space of all 2 × 2 symmetric matrices.
S(2) is a three-dimensional vector space (TI(S(2)) = S(2)) and let B1, B2, B3 form
an orthonormal basis of S(2). For example,
B1 =

1 0
0 0

, B2 =

0 0
0 1

, and B3 =

0
1
√
2
1
√
2
0

.
Any matrix in S(2) can be expressed as an element of R3 consisting of coeﬃcients
from a linear combination of these basis vectors.
Deﬁne a function F : S(2) × F c
2 →S(2) as F(A; β) = LAβΣAβ. We will use a
Newton-Raphson procedure to ﬁnd A∗∈S(2) such that F(A∗; β) −I = 0. Then,

11.3 Aﬃne-Invariant Shape Analysis of Planar Curves
395
we can simply scale A∗accordingly to satisfy the unit-length condition on β∗. The
Newton-Raphson algorithm updates the estimate of A∗and β∗incrementally at
each iteration. Let the (m+1)-th estimate of A∗be A(m+1) = Am+1Am · · · A1, and
let the (m+1)-th estimate of β∗be βm+1 = Am+1βm−CAm+1βm. At each iteration
we calculate Am+1 as a perturbation from identity, and thus we must deﬁne the
directional derivative of F(I; βm). For any B ∈TI(S(2)), and let a = −Cβm.
The directional derivative of F at identity in the direction of B is denoted by
dFI(B; βm), which turns out to be:
 1
0
(BβmβT
m + βmβT
mB + BβmaT + aβT
mB + aaT )| ˙
βm|dt
+
 1
0
1
| ˙
βm|
(βmβT
m + βmaT + aβT
m + aaT ) ˙
βm
T B ˙βmdt.
(11.2)
We can express dFI as a 3 × 3 matrix J under the chosen basis B1, B2, B3. That
is Jij = ⟨dFI(Bi, βm), Bj⟩.
Algorithm 53 (Curve-Aﬃne Standardization). Given β0 with Lβ0 = 1 and
Cβ0 = 0, initialize A(0) = A0 = I and select a step size δ. Let m = 0.
1. Calculate the residual rm = F(I; βm)−I. Rewrite rm as a vector of coeﬃcients
with respect to the basis vectors B1, B2, B3. If |rm| < ϵ go to (7), else
2. Calculate dFI(Bi; βm) for i = 1, 2, 3 using Eq. 11.2, and form the Jacobian J.
3. Compute x = J−1rm and let D = 3
i=1 xiBi.
4. Let Am+1 = I −δD.
5. Update βm+1 = Am+1βm −CAm+1βm and A(m+1) = Am+1A(m).
6. Return to (1) and let m = m + 1.
7. Let A∗= A(m+1)/Lβm+1, and let β∗= βm+1/Lβm+1.
Figure 11.5 shows the result of applying this subroutine to curves from four
diﬀerent aﬃne orbits; each orbit is shown as a diﬀerent row. The left panel
shows curves before and the right panel shows the corresponding curves after
the standardization. Note that a standardization diﬀers only in rotation and re-
parameterization within each row. This standardization provides us with a tech-
nique to take any arbitrary closed curve in F c
2 (unit length, absolutely continuous)
and project it uniquely into an element of F c
a.
Fig. 11.5 Aﬃne standardization of curves. The original curves are shown in the left, and their
standardizations are shown in the right

396
11 Related Topics in Shape Analysis of Curves
11.3.2 Geodesics Using Path-Straightening Algorithm
After standardization our problem reduces to comparing elements of F c
a, the
manifold of aﬃne-standardized, unit-length closed curves modulo rotation and
re-parameterization. This task is a repeat of the ideas presented in Chap. 6 for
similarity shape analysis of closed curves. Since each curve is represented by the
standard element of its aﬃne orbit, we need to further restrict the analysis to
SRVFs of aﬃne-standardized curves by imposing a few added conditions beyond
that of the closure condition.
For a parameterized curve β : S1 →R2, let q : [0, 1] →R2 denote its SRVF.
Let x(q; t) =
 t
0 q(u)|q(u)|du be our original curve β(t) but with the starting point
located at the origin, i.e., β(0) = 0. As noted in Chap. 5, |q(t)|2 = | ˙β(t)| and the set
of all unit-length curves is the unit hypersphere S∞. The centroid and covariance
of a curve x(q; t) can be stated in terms of the q-function as follows. The centroid
in R2 is given by Cq =
 1
0 x(q; t)|q(t)|2dt, and the covariance in R2×2 is given by
Σq =  1
0 (a + x(q; t))(a + x(q; t))T |q(t)|2dt, where a = −Cq. In order to impose the
condition that the curve β be closed, we set x(q; 1) = 0, i.e., the endpoint of the
curve x(q; t) is equal to the initial point, the origin. Deﬁne a mapping Ψ : S∞→R4
as:
Ψ1(q) =

S1

(a1 + x1(q; t))2 −(a2 + x2(q; t))2
|q(t)|2dt
Ψ2(q) =

S1 (a1 + x1(q; t)) (a2 + x2(q; t)) |q(t)|2dt
Ψ3(q) = 
S1 q1(t)|q(t)|dt
Ψ4(q) =

S1 q2(t)|q(t)|dt,
(11.3)
where a subscript indicates the ith coordinate in Euclidean space, that is, Ψi(q) ∈R
for each i. Ψ1(q) = 0 implies that the diﬀerence of diagonal entries in the covariance
matrix is 0. Ψ2(q) = 0 implies that the oﬀ-diagonal entry in the covariance matrix
is 0. Together, they imply that Σβ = cβI. The constraint Ψ3(q) = Ψ4(q) = 0 implies
the closure condition. Since SRVFs are translation invariant, we don’t need any
explicit condition on the centroid. The space of all aﬃne-standardized, unit length,
closed curves is therefore the level set C c
a = Ψ −1((0, 0, 0, 0)) ⊂S∞. The section C c
a
has a one-to-one correspondence with F c
a and is therefore also called curve-aﬃne
pre-shape space. We shall use the notation S c
a to denote curve-aﬃne shape space
C c
a/(SO(2) × ˜ΓS). In order to compute geodesics on C c
a and its quotient space, we
will use the path-straightening algorithm introduced earlier in Sect. 6.6 of Chap. 6.
Deﬁning the Normal Space of C c
a
An integral step in the subroutines nec-
essary for path-straightening on C c
a is obtaining a basis for the normal space
Nq(C c
a ), the set of elements that are normal to C c
a inside L2(S1, R2). The normal
space is a four-dimensional space, and here we provide analytical expressions for
the functions {hj(t), j = 1, . . . , 4} that serve as a basis for this space. These func-
tions arise from the calculation of the directional derivative of Ψ in Eq. 11.3, and
we leave the full derivation as an exercise. In order to deﬁne the normal space, we
must ﬁrst ﬁnd the directional derivative of Ψ (see Eq. 11.3) at any point q ∈C c
a.
Let w ∈L2(S1, R2). We write the derivative dΨj,q(w) =
d
dsΨj(q(t) + sw(t))|s=0,
for j = 1, 2, 3, 4. By expressing the directional derivative as an inner product as
follows: dΨj,q(w) = ⟨w(t), hj(t)⟩for some functions h1, . . . , h4, where ⟨·, ·⟩is the
standard L2 inner product, these functions will serve as a basis for the normal
space. The restriction of these functions inside Tq(C c
a) is obtained by removing
the projection along the function q: bj(t) = hj(t) −q(t)

q(t), hj(t)

and {bj} span

11.3 Aﬃne-Invariant Shape Analysis of Planar Curves
397
the normal space Nq(C c
a ) inside Tq(B). Let f i(t) = |q(t)|ei + qi(t)
|q(t)|q(t) for i = 1, 2.
Recall that qi(t)
|q(t)|q(t) = 0 in case |q(t)| = 0 at some t ∈S1. Let Q(t) =
 t
0 |q(τ)|2dτ
and let Gi(t) =
 t
0 |q(τ)|2xi(q; τ)dτ for i = 1, 2. In this notation,
h1(t) = 4q(t)a1x1(q; t) −4q(t)a2x2(q; t) + 2a1f 1(t)(1 −Q(t)) −2a2f 2(t)(1 −Q(t))
+ 2q(t)x1(q; t)2 −2q(t)x2(q; t)2 + 2f 1(t)(a1 −G1(t)) −2f 2(t)(a2 −G2(t)) ,
(11.4)
and
h2(t) = 2a2q(t)x1(q; t) + 2a1q(t)x2(q; t) + a2f 1(t)(1 −Q(t)) + a1f 2(t)(1 −Q(t))
+ 2q(t)x1(q; t)x2(q; t) + f 1(t)(a2 −G2(t)) + f 2(t)(a1 −G1(t)).
(11.5)
The remaining two are given by h3(t) = f 1(t), and h4(t) = f 2(t). The restriction
of these functions inside Tq(S∞) is obtained by removing the projection along the
function q: bj(t) = hj(t)−q(t)⟨q(t), hj(t)⟩and {bj} span the normal space Nq(C c
a)
inside Tq(S∞). Next we describe three additional subroutines needed to implement
the path-straightening algorithm on C c
a.
For any point q ∈S∞, we need a tool to project q to the nearest point in C c
a. The
procedure presented below is diﬀerent from Algorithm 53 in that, although they
accomplish a similar task, standardization is restricted to the same orbit, while the
projection is not. To clarify further, Algorithm 53 is used on coordinate functions
for curve standardization, and Algorithm 54 is used on SRVFs within the path-
straightening algorithm. These algorithms are very similar to their counterparts
used in Chap. 6.
Algorithm 54 (Projection onto C c
a). Let ϵ > 0.
1. Compute the residual vector r = Ψ(q) ∈R4, with Ψ as given in Eq. 11.3. If
|r| < ϵ, stop. Otherwise continue to step 2.
2. Calculate the basis vectors {bj, j = 1, . . . , 4}, for Nq(C c
a ), and form the Jacobian
matrix J via Jij = ⟨bi(t), bj(t)⟩.
3. Solve Jx = −r for x.
4. Deﬁne dq = 4
j=1 xjbj. Update q using q →cos(∥dq∥)q + sin(∥dq∥) dq
∥dq∥. Go to
step 1.
Algorithm 55 (Projection onto Tq(C c
a )). Given any function w ∈L2(S1, R2),
1. If w /∈Tq(S∞), then project w to Tq(S∞) via w →w −⟨w, q⟩q.
2. Compute a basis {bj}, j = 1, 2, 3, 4, for Nq(C c
a ) as earlier, and then obtain a
Gram-Schmidt orthonormalization, {bj
o}.
3. Project w into Tq(C c
a) via w →w −4
j=1⟨w, bj
o⟩bj
o.
Note that we can skip the ﬁrst step even if w /∈Tq(S∞), but it will compromise
numerical stability.
In case two points q1, q2 ∈C c
a are close to each other, we can approximately
parallel translate a tangent vector from q1 to q2 using a projection.

398
11 Related Topics in Shape Analysis of Curves
Algorithm 56 (Parallel Translation). Given two points q1, q2 ∈C c
a and w ∈
Tq1(C c
a ).
1. Compute the analytic expression for parallel translation on S∞as
w →˜w ≡
2⟨w, q2⟩
∥q1 + q2∥2 (q1 + q2).
2. Let l = | ˜w|. Project ˜w onto Tq2(C c
a) using Algorithm 55 to obtain ¯w.
3. Rescale ¯w via ¯w →
¯
wl
∥¯
w∥.
We now have all the tools necessary to compute geodesics via path-straightening
on S c
a , and the general procedure of aﬃne-invariant, elastic shape analysis is as
follows, assuming we are given two curves β1 and β2. First, use Algorithm 53 to
obtain β∗
1 and β∗
2. Then, convert to SRVF representation to obtain q∗
1, q∗
2 ∈C c
a
and hence the unique orbits [q∗
1] and [q∗
2] under the group action of SO(2) × ˜ΓS.
Since this group action is by isometries on C , we deﬁne the distance on S c
a as
dS c
a ([q∗
1], [q∗
2]) =
inf
O∈SO(2),γ∈ΓSdC c
a(q∗
1, O(q∗
2, γ)).
(11.6)
The optimal group elements that achieve this distance minimization, say O∗and
γ∗, are solved, respectively, by Procrustes rigid body alignment and by either
dynamic programming or gradient-based optimization (see Chap. 5). The distance
dC c
a(q∗
1, O∗(q∗
2, γ∗)) is thus computed with path straightening.
In order to highlight the deformations resulting along geodesic paths between
shapes, we use an additional step of de-standardization in our displays. Note that
our algorithms start by standardization of given curves β1 and β2 into standard-
ized curves β∗
1 = A∗
1β1 and β∗
2 = A∗
2β2. Here A∗
1, A∗
2 are symmetric, non-singular
matrices found using Algorithm 53. In order to revert to the original curve, one
can use their inverses, A∗
1
−1 and A∗
2
−1, on the corresponding curves. In order to
de-standardize shapes along the geodesic path between the standardized shapes,
we use a geodesic between A∗
1
−1 and A∗
2
−1 in S(2) and apply the transformation
at time τ to the corresponding shape at time τ along the geodesic.
Various examples of geodesic paths in S c
a can be seen in Figs. 11.6, 11.7,
and 11.8. Figure 11.6 shows an example of path-straightening iterations that lead
to a geodesic path in S c
a . Likewise, Fig. 11.7 shows four examples of ﬁnal geodesic
paths in aﬃne shape space along with their corresponding de-standardized paths.
Figure 11.8 provides a visual illustration of the beneﬁt of aﬃne invariance as
well as elasticity in a shape analysis framework. Here we compute the geodesic
path between two shapes in various shape spaces, whereby the two shapes diﬀer
in placement of bumps and an aﬃne transformation (shown in top and bottom of
the last column in Fig. 11.8). We compute the geodesics in the following shape
spaces: (a) closed curve shape space with a non-elastic, bending only metric in
S c
1 (b) closed curve, similarity-invariant shape space with the elastic metric in
S c
2 , (c) closed curve, aﬃne-invariant shape space with the elastic metric in S c
a ,
and for display purposes (d) the de-standardization of (c). We can see that the
deformation in geodesic path (b) is more natural and smaller than that of path
(a) due to the addition of elasticity to appropriately stretch and match features.
Furthermore, path (c) shows a smaller deformation than that of path (b) due to
the addition of aﬃne invariance. Note that any aﬃne transformation of either the

11.3 Aﬃne-Invariant Shape Analysis of Planar Curves
399
0
0.5
1
1.5
2
2.5
3
Iterations
Energy
0
5
10
15
20
Fig. 11.6 Path straightening on S c
a . The left side shows iterations of the path-straightening
algorithm from top (initial path) to bottom (ﬁnal path). The right panel shows the corresponding
evolution of the path energy
Fig. 11.7 Each case shows a geodesic in S c
a (top row) and its de-standardization (bottom row)

400
11 Related Topics in Shape Analysis of Curves
Fig. 11.8 Geodesic paths between two shapes in diﬀerent spaces. (a) Similarity invariant with
bending-only metric, (b) similarity invariant with elastic metric, (c) S c
a (d) de-standardized
version of path (c)
Fig. 11.9 From left to right: original data, the mean in the similarity shape space, and random
samples
Fig. 11.10 From left to right: standardized versions of shapes in Fig. 11.9, the Karcher mean
in S c
a , random samples, and their de-standardizations
beginning or ending shape will not change the geodesic path shown in (c) due to
our aﬃne-invariant shape analysis framework.
Figures 11.9 and 11.10 demonstrate the advantage of using statistical models in
S c
a over the similarity shape space. Here, we use the MPEG-7 shape class “hat”
under random aﬃne transformations. In Fig. 11.10 we see that we obtain gains in

11.4 Registration of Trajectories on Nonlinear Manifolds
401
Table 11.2 Leave-one-out classiﬁcation rates for human activity sequence dataset under various
simulated camera angles
Broadside Narrow-side Broadside Narrow-side
level
level
elevated
elevated
Similarity (%) 98
97
51
87
Aﬃne (%)
98
98
98
98
modeling these aﬃne shapes by separating the variability into the spaces S c
a and
GL(2).
Pose-Invariant Activity Classiﬁcation An important application of aﬃne-
invariant shape analysis is in the ﬁeld of human activity or human motion analysis
where a major need here is to be invariant under diﬀering pose or camera angles.
Note that if the pose changes so much that certain body parts are occluded and new
parts become visible, then the shape changes are too complicated to be modeled
as a simple transformation. However, in case of moderate changes (< 45◦), one can
model these changes using aﬃne or projective transformations. In that situation
the proposed algorithms can be used for a pose-invariant activity classiﬁcation.
In this experiment we use the UMD activity dataset, which consists of 100
sequences of 80 shapes each, where each sequence represents a frame-by-frame
outline of a person performing a task. The dataset is divided into 10 classes, or
“activities,” of 10 sequences each. Our goal is to classify a test sequence under an
arbitrary viewing angle; we simulate diﬀerent viewing angles by applying appro-
priate aﬃne transformations on a given sequence. Since these data were captured
using broadside imaging, we simulate test sequences for diﬀerent views: original
(camera level and broadside), narrow side (camera at level height and slightly fac-
ing the subject), top view (camera at an elevated position and broadside), and top
left (camera elevated and slightly facing the subject). To generate a test sequence,
we simulate a stochastic process on GL(2) with an appropriate mean and apply
each point of the process to the corresponding shape in the sequence. Figure 11.11
shows an example of an activity sequence under these simulated views. Then, we
classify this test sequence using the nearest-neighbor classiﬁer under diﬀerent met-
rics. (The distance for classifying a sequence of shapes is the sum of the distances
for individual shapes.)
Note that the classiﬁcation rate of the aﬃne-standardized test sequences will
match that of the original, un-transformed sequences. Since classiﬁcation rate de-
creases under diﬀerent camera angles without standardization, we conclude that
an aﬃne-invariant metric helps in classifying sequences of human activity under
various camera angles (Table 11.2). From the drastic improvement in classiﬁcation
rate from 51 % to 98 % in the top camera angle, we can see that standardization
would be especially useful when applied to sequences from mounted surveillance
cameras.
11.4 Registration of Trajectories on Nonlinear Manifolds
In the third and ﬁnal part of this chapter, we will study techniques for comparing
objects that are observations of stochastic processes on nonlinear Riemannian
manifolds. In other words, we are interested in objects of the type β : [0, 1] →M,

402
11 Related Topics in Shape Analysis of Curves
Fig. 11.11 Aﬃne standardization of shape sequences
where M is a nonlinear Riemannian manifold. Although they are essentially curves,
we will call them trajectories to distinguish them from curves in Euclidean spaces
studied in previous chapters.
The need to summarize and model trajectories arises in many situations, es-
pecially in pattern recognition of complex systems. An important challenge in
handling real data is that trajectories are often not observed at standard times; in
fact, they are often observed at arbitrary times. If this temporal variability is not
accounted for in the analysis, then the resulting statistical summaries will not be
precise. The mean trajectory may not be a representative of individual trajecto-
ries and the cross-sectional variance will be artiﬁcially inﬂated. This, in turn, will
greatly reduce the eﬀectiveness of any subsequent modeling or analysis based on
estimated mean and covariance. As a simple example consider the trajectory on

11.4 Registration of Trajectories on Nonlinear Manifolds
403
Fig. 11.12 Summary of trajectories on S2: (a) a simulated example, (b) bird migration paths,
(c) hurricane tracks, and (d) cross-sectional mean of two trajectories without (top) and with
(bottom) registration (colored picture)
S2 shown in the top panel of Fig. 11.12a. We simulate a set of random, discrete
observation times and generate observations of this trajectory at these random
times. These simulated trajectories are identical in terms of the points traversed
but their evolutions, or parameterizations are quite diﬀerent. If we compute cross-
sectional mean and variance, the results are shown in the bottom panel. We draw
the sample mean trajectory in black and the sample variance at discrete times us-
ing tangential ellipses. Not only is the mean fairly diﬀerent from the original curve,
the variance is purely due to randomness in observation times and is somewhat
artiﬁcial. If we had observed the trajectory at ﬁxed, synchronized times, then this
problem will not exist.
To motivate this issue further, consider the phenomenon of bird migration that
is the regular seasonal journey undertaken by many species of birds. There are
variabilities in migration trajectories, even within the same species, including the
variability in their rates of travels. In other words, either birds can travel along
diﬀerent paths or, even if they travel the same path, diﬀerent birds (or subgroups)
can ﬂy at diﬀerent speed patterns along that path. This results in variability in
observation times of migration paths for diﬀerent birds and artiﬁcially inﬂates
the cross-sectional variance in the data. Another issue is that such trajectories
are naturally studied as paths on a unit sphere, which is a nonlinear manifold.
We will study the migration data for Swainson’s Hawk, with some example paths
shown in the top of Fig. 11.12b. Swainson’s Hawk inhabits North America mainly
in the spring and summer and winters in South America. The bottom panel in
Fig. 11.12b shows cross-sectional sample mean and variance of the trajectories.
Another motivating application comes from hurricane tracking, where one is in-
terested in studying the shapes of hurricane tracks in certain geographical regions.
As in the previous application, the hurricane tracks are also naturally treated as
trajectories on a unit sphere. The top panel of Fig. 11.12c shows a set of hurricane
tracks originating from the Atlantic region. The sample mean and the variance
of these trajectories are adversely aﬀected by this phase variability present in

404
11 Related Topics in Shape Analysis of Curves
data, as shown in the bottom row of Fig. 11.12c. As the last motivating example,
consider two trajectories, drawn in red and blue in the top of Fig. 11.12d These
two trajectories have the same shape, i.e., two bumps each, and a curve repre-
senting their mean is also expected to have two bumps. A simple cross-sectional
mean, shown by the black trajectory in the same picture, has three bumps! If we
incorporate optimal temporal alignment in our analysis, as we did for function
data and curves previously, then such inconsistencies are avoided and the black
trajectory in bottom panel shows the resulting mean.
Let’s look at the problem in more mathematical terms: Let α : [0, 1] →M, where
M is a Riemannian manifold, be an absolutely continuous map; it denotes a trajec-
tory on M. We will study such trajectories as elements of an appropriate subset of
M [0,1], the set of all maps from [0, 1] to M. Rather than observing a trajectory α di-
rectly, say in the form of time observations α(t1), α(t2), . . . , we instead observe the
(arbitrarily) time-warped trajectory α(γ(t1)), α(γ(t2)), . . . , where γ ∈ΓI governs
the rate of evolution. The mean and variance of {α1(t), α2(t), . . . , αn(t)} for any t,
where n is the number of observed trajectories, are termed the cross-sectional mean
and variance at that t. If we use the observed samples {αi(γi(t)), i = 1, 2, . . ., n}
for analysis, then the cross-sectional variance maybe inﬂated due to random γis.
Our hypothesis is that this problem can be mitigated by temporally registering
the trajectories. Thus, we are interested in the following four tasks:
1. Temporal Registration: This is a process of establishing a one-to-one corre-
spondence between points along multiple trajectories. That is, given any n tra-
jectories, say α1, α2, . . . , αn, we are interested in ﬁnding functions γ1, γ2, . . . , γn
such that the points αi(γi(t)) are matched optimally for all t.
2. Metric Comparison: We want to develop a metric that is invariant to diﬀerent
evolution rates of trajectories. Speciﬁcally, we want to deﬁne a distance dα(·, ·)
such that for arbitrary evolution functions γ1, γ2 and arbitrary trajectories α1
and α2, we have dα(α1, α2) = dα(α1 ◦γ1, α2 ◦γ2).
3. Statistical Summaries: The main use of this metric will be in deﬁning and
computing a (Karcher) mean trajectory μ(t) and a cross-sectional variance func-
tion ρ(t), associated with any given set of trajectories. The main reason for
performing registration is to reduce the cross-sectional variance that is artiﬁ-
cially introduced in the data due to random observation times. The reduction
in variance is quantiﬁed using ρ.
4. Statistical Modeling and Evaluation: We will use the estimated mean and
covariance of registered trajectories to deﬁne a “Gaussian-type” model on ran-
dom trajectories. This model will then be used to evaluate p-values associated
with new trajectories. Here p-value implies the proportion of trajectories with
density under the model smaller than the current trajectory.
For performing comparison and summarization of trajectories, we need a metric
and, at ﬁrst, we consider a more conventional solution. Since M is a Riemannian
manifold, we have a natural distance dm, i.e., the geodesic distance under the given
Riemannian metric, between points on M. Using dm, one can compare any two
trajectories: α1, α2 : [0, 1] →M, as
dx(α1, α2) =
 1
0
dm(α1(t), α2(t))dt .
(11.7)

11.4 Registration of Trajectories on Nonlinear Manifolds
405
See Sect. 3.3 for a proof that dx is a distance between trajectories. Although this
quantity represents a natural extension of dm from M to M [0,1], it suﬀers from the
problem that dx(α1, α2) ̸= dx(α1 ◦γ, α2 ◦γ) generally. If this equality held, for all
γ ∈ΓI, then one could develop a fully invariant distance and use it to properly
register trajectories. So, the failure to have this equality is in fact a key issue that
forces us to look for other solutions in situations where trajectories are observed
at arbitrary temporal evolutions. When a trajectory α is observed as α ◦γ, for an
arbitrary temporal re-parameterization γ, we call this perturbation compositional
noise. In these terms, dx is not useful in comparing trajectories observed under
compositional noise.
11.4.1 Transported SRVF for Trajectories
Continue to use α to denote an absolutely continuous trajectory on a Rieman-
nian manifold of interest M, where M is endowed with a Riemannian met-
ric ⟨·, ·⟩p. Let M denote the set of all such trajectories: M = {α : [0, 1] →
M|α
is absolutely continuous}. If α is a trajectory on M, then α ◦γ, for any
γ ∈˜ΓI, is a trajectory that follows the same sequence of points as α but at
the evolution rate governed by γ. More technically, the group ˜ΓI acts on M ,
M × ˜ΓI →M , according to (α, γ) = α ◦γ.
Given two smooth trajectories α1, α2 ∈M , we want to register points along
the trajectories and compute a time-warping invariant distance between them. A
seemingly natural choice for this purpose would be:
inf
γ∈˜ΓI
 1
0
dm(α1(t), α2(γ(t)))dt

,
but it fails for the same reasons as those highlighted in Sects. 4.4 and 10.5, in-
cluding the fact that it is not even symmetric. Fundamentally speaking, this and
other quantities used in previous literature are not appropriate for solving the reg-
istration problem because they are not measuring registration in the ﬁrst place.
To highlight this issue, take the registration of points between the pair (α1, α2)
and the pair (α1 ◦γ, α2 ◦γ), for any γ ∈ΓI. It can be seen that the pairs (α1, α2)
and (α1 ◦γ, α2 ◦γ) have exactly the same registration of points. In fact, any iden-
tical time warping of two trajectories does not change the registration of points
between them. But the quantity given in Eq. 11.7 and the equation written above,
both provide diﬀerent values for these pairs, despite them having the same reg-
istration. Hence, they are not good measures of registration. We emphasize that
the invariance under identical time warping is a key property that is needed in the
desired framework.
We introduce a new representation of trajectories that will be used to compare
and register them. We will assume that for any two points p, q ∈M, we have
an expression for parallel transporting any vector v ∈Tp(M) along the shortest
geodesic from p to q, denoted by (v)p→q. As long as p and q do not fall in the cut
loci of each other, the shortest geodesic between them is unique and the parallel
transport is well deﬁned. The measure of the set of cut locus on the manifolds
of our interest is typically zero. So, the practical implications of this limitation
are negligible. Let c be a point in M that we will designate as a reference point.

406
11 Related Topics in Shape Analysis of Curves
α(t)
c
Tc (M)
hα(t)
˙α(t)
M
Fig. 11.13 Parallel translation of scaled ˙α(t) from α(t) to c to form the TSRVF hα(t)
We will assume that none of the observed trajectories pass through the cut locus
of c to avoid the problem mentioned above.
Deﬁnition 11.2 (Transported Square-Root Vector Field).
For any abso-
lutely continuous trajectory α ∈M , deﬁne its transported square-root vector ﬁeld
(TSRVF) to be a parallel transport of a scaled velocity vector ﬁeld of α to a ref-
erence point c ∈M according to: hα(t) =
˙α(t)α(t)→c
√
| ˙α(t)|α(t) ∈Tc(M), where | · | denotes
the norm related to the Riemannian metric on M.
This deﬁnition is illustrated in Fig. 11.13 using a spherical manifold M. Since α is
absolutely continuous, the vector ﬁeld hα is square integrable. Let H ⊂Tc(M)[0,1]
be the set of square-integrable curves in Tc(M) obtained as TSRVFs of trajectories
in M, H = {hα|α ∈M }. If M = Rn with the Euclidean metric, then h reduces
to the SRVF deﬁned in previous chapters.
The choice of reference point c used in Deﬁnition 11.2 is important in this
framework and can potentially aﬀect the results. The choice of c would typically
depend on the application, the data and the manifold M under study. In case all
the trajectories pass through a point or pass close to a point, then that point is a
natural candidate for c. This would be true, for example, in the case of hurricane
tracks, if we are focused on all hurricanes starting from the same region in the
Atlantic Ocean. While the choice of c can, in principle, aﬀect distances, some
experiments suggest that the results of registration, distance-based clustering and
classiﬁcation are quite stable with respect to this choice. An example is presented
later in Fig. 11.14.
We will mathematically represent a trajectory α ∈M by the pair (α(0), hα) ∈
(M × H ). Given this representation, we can reconstruct the path, an element of
M , as follows. For any time t, let Vt be a time-varying tangent vector ﬁeld on M
obtained by parallel transporting hα(t) (along respective geodesics) over the whole
M (except the cut locus of α(t)), i.e., for any p ∈M, Vp(t) = |hα(t)|(hα(t))c→p.

11.4 Registration of Trajectories on Nonlinear Manifolds
407
c =[-1, 0, 0]
c=[-1, 0, 0]
c =[0, 0, -1]
c =[0, 0,1]
=[0, 0,1]
,
c =[0, 1, 0]
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
α1, α2
α1, α2 ◦γ∗
γ∗under c =[0, 0, 1]
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
c =[0, 0, −1]
c =[−1, 0, 0]
c =[0, 1, 0]
Fig. 11.14 Registration of trajectories on S2
Then, deﬁne an integral curve β such that ˙β(t) = Vβ(t)(t) with the starting point
β(0) = α(0) ∈M. This resulting curve β will exactly be same as the original curve
α.
The starting points of diﬀerent curves can be compared using the Riemannian
distance dm on M. However, these points do not play an important role in the
alignment of trajectories since they are already matched to each other. Therefore,
the main focus of analysis, in terms of alignment, is on TSRVFs. Since a TSRVF
is a path in Tc(M), one can use the L2 norm to compare such paths.
Deﬁnition 11.3 (TSRVF Distance). Let α1 and α2 be two smooth trajectories
on M and let hα1 and hα2 be the corresponding TSRVFs. The distance between
them is:
dh(hα1, hα2) ≡
 1
0
|hα1(t) −hα2(t)|2dt
 1
2
.
The distance dh, being the standard L2 norm, satisﬁes symmetry, positive deﬁ-
niteness, and triangle inequality. Also, note that due to invertibility of mapping
from M to (M × H ), one can use dh (along with dm) to deﬁne a distance on M .
The main motivation of this setup—TSRVF representation and L2 norm—comes
from the following fact. If a trajectory α is warped by γ, to result in α ◦γ, the
TSRVF of α ◦γ is given by:
hα◦γ(t) = ( ˙α(γ(t))˙γ(t))α(γ(t))→c

| ˙α(γ(t))˙γ(t)|
= ( ˙α(γ(t)))α(γ(t))→c

˙γ(t)

| ˙α(γ(t))|
= hα(γ(t))

˙γ(t).

408
11 Related Topics in Shape Analysis of Curves
We will often use (hα, γ) to denote hα◦γ. As stated earlier, we need a distance for
registration that is invariant to identical time warping of trajectories. Next, we
show that dh satisﬁes this property.
Theorem 11.1. For any α1, α2 ∈M and γ ∈
˜ΓI, the distance dh satisﬁes
dh(hα1◦γ, hα2◦γ) = dh(hα1, hα2). In other words, the action of ˜ΓI on H under
the L2 metric is by isometries.
Proof. Starting with the left side,
dh(hα1◦γ, hα2◦γ) =
 1
0
|hα1(γ(t))

˙γ(t) −hα2(γ(t))

˙γ(t)|2dt
 1
2
=
 1
0
|hα1(s) −hα2(s)|2ds
 1
2
= dh(hα1, hα2),
where s = γ(t).
⊓⊔
Next we deﬁne a quantity that can be used as a distance between trajectories
while being invariant to their temporal variability. To set up this deﬁnition, we ﬁrst
introduce an equivalence relation between trajectories. For any two trajectories α1
and α2, we deﬁne them to be equivalent, α1 ∼α2, when:
1. α1(0) = α2(0), and
2. there exists a sequence {γk} ∈ΓI such that limk→∞h(α1◦γk) = hα2; this con-
vergence is measured under the L2 metric.
In other words, any two trajectories are equivalent if they have the same starting
point and the TSRVF of one can be time-warped into the TSRVF of the other
using a sequence of warpings. It can be easily checked that ∼forms an equivalence
relation on H (and correspondingly M ).
Since we want our distance to be invariant to time warping of trajectories, we
wish to compare trajectories by comparing their equivalence classes. Thus, our next
step is to inherit the distance dh to the set of such equivalence classes. Toward this
goal, we use the action of the monoid ˜ΓI. For a TSRVF hα ∈H , its equivalence
class, or orbit under ˜ΓI, is given by [hα] = {(hα, γ)|hα ∈H , γ ∈˜ΓI} It can be
shown that the orbits under ˜ΓI are exactly same as the closures of the orbits of
ΓI, deﬁned as [hα]0 = {(hα, γ)|γ ∈ΓI}, as long as α has nonvanishing derivatives.
(The last condition is not restrictive since we can always re-parameterize α by the
arc length.) The closure is with respect to the L2 metric on H .
Now we are ready to deﬁne the quantity that will serve as both the cost function
for registration and the distance for comparison. This quantity is essentially dh
measured between not the individual trajectories but their equivalence classes.
Deﬁnition 11.4 (Trajectory Shape Distance). Deﬁne a distance ds on H / ∼
by computing the shortest dh distance between equivalence classes in H :
ds([hα1], [hα2]) ≡
inf
γ1,γ2∈˜ΓI
dh((hα1, γ1), (hα2, γ2))
=
inf
γ1,γ2∈˜ΓI
 1
0
|hα1(γ1(t))

˙γ1(t) −hα2(γ2(t))

˙γ2(t)|2dt
 1
2
.
(11.8)

11.4 Registration of Trajectories on Nonlinear Manifolds
409
Theorem 11.2. The distance ds is a proper distance on H / ∼.
Proof. The symmetry of ds comes directly from the symmetry of dh. For positive
deﬁniteness, we need to show that ds([hα1], [hα2]) = 0 ⇒[hα1] = [hα2]. Suppose
that ds([hα1], [hα2]) = 0. By deﬁnition, it follows immediately that for all ϵ > 0,
there exists a γ ∈ΓI such that dh(hα1, (hα2, γ)) < ϵ. From this, it follows that hα1
is in the orbit hα2. Since we are assuming that orbits are closed, it follows that
hα1 ∈[hα2], so [hα1] = [hα2].
To establish the triangle inequality, we need to prove ds([hα1], [hα3])
≤
ds([hα1], [hα2]) + ds([hα2], [hα3] , for any hα1, hα2, hα3
∈H . Seeking con-
tradiction, suppose that ds([hα1], [hα3]) > ds([hα1], [hα2]) + ˜d([hα2], [hα3]). Let
ϵ =
1
3(ds([hα1], [hα3]) −ds([hα1], [hα2]) −ds([hα2], [hα3])]); by our supposition,
ϵ > 0. From the deﬁnition of ϵ, it follows that ds([hα1], [hα3]) = ds([hα1], [hα2]) +
ds([hα2], [hα3]) + 3ϵ. By the deﬁnition of ds, we can choose γ1, γ2 ∈˜ΓI, such that
dh((hα1, γ1), hα2) ≤ds([hα1], [hα2])+ϵ and dh(hα2, (hα3, γ2)) ≤ds([hα2], [hα3])+ϵ.
Now by the triangle inequality for dh, we know that dh((hα1, γ1), (hα3, γ2)) ≤
dh((hα1, γ1), hα2) + dh(hα2, (hα3, γ2)) ≤ds([hα1], [hα2]) + ds([hα2], [hα3]) + 2ϵ. It
follows that ds([hα1], [hα3]) ≤ds([hα1], [hα2]) + ds([hα2], [hα3]) + 2ϵ. But this
contradicts that fact that ds([hα1], [hα3]) = ds([hα1], [hα1]) + ds([hα2], [hα3]) + 3ϵ.
Hence our supposition that ds([hα1], [hα3]) > ds([hα1], [hα2]) + ds([hα2], [hα3])
must be false. The triangle inequality follows.
⊓⊔
Now, since ΓI is dense in ˜ΓI, for any δ > 0, there exists a γ∗∈ΓI such that:
|dh(hα1, hα2◦γ∗) −ds([hα1], [hα2])| < δ .
(11.9)
This γ∗may not be unique but any such γ∗is suﬃcient for our purpose. Further-
more, since γ∗∈ΓI, it has an inverse that can be used in further analysis. The
minimization over ΓI in Eq. 11.9 is performed in using the dynamic programming
(DP) algorithm presented in Appendix B.
Our goal of warping-invariant comparisons of trajectories is achieved using ds.
For any γ1, γ2 ∈ΓI and α1, α2 ∈M , we have
[hα1◦γ1] = [hα1],
[hα2◦γ2] = [hα2],
and, therefore, we get ds([hα1◦γ1], [hα2◦γ2]) = ds([hα1], [hα2]). The next goal is to
perform registration of points along trajectories. Let our approximation to the
optimal warping be as deﬁned in Eq. 11.9. This solves for the registration between
α1 and α2. It says that the point α1(t) on the ﬁrst trajectory is optimally matched
to the point α2(γ∗(t)) on the second trajectory.
Equation 11.8 also solves for the registration between trajectories. Therefore,
we are able to achieve our goal of joint registration and comparison (via a proper
metric) in a uniﬁed fashion. In general registration methods, the cost function has
two separate terms, one for matching and one for regularization, with an arbitrary
weight parameter that is to be chosen by the user. However, in Eq. 11.8, the two
terms have been merged into a single natural form. Recall that the change in
TSRVF h due to the time warping of α by γ is given by (h, γ) = (h ◦γ)√˙γ, and
the distance ds is based on these warped TSRVFs. It turns out that the term
√˙γ provides an intrinsic regularization on γ in the matching process. This term
provides an elastic penalty against excessive warping since ˙γ becomes large at

410
11 Related Topics in Shape Analysis of Curves
those places. Lastly, the optimal registration in Eq. 11.8 remains the same if we
change the order of the input functions. That is, the registration process is inverse
consistent!
11.4.1.1 Summarization and Registration of Multiple Trajectories
An additional advantage of this framework is that one can compute an average of
several trajectories and use it as a template for future classiﬁcation. Furthermore,
this template can, in turn, be used for registering multiple trajectories. We will
use the notion of the Karcher mean to deﬁne and compute average trajectories.
Given a set of sample trajectories α1, . . . , αn on M, we represent them using the
corresponding pairs (α1(0), hα1), (α2(0), hα2), . . . , (αn(0), hαn). We will compute
the Karcher means of each component in its respective space: (1) the Karcher
mean of αi(0)s are computed with respect to dm in M, and (2) the Karcher mean
of hαis are with respect to ds in H / ∼. The latter Karcher mean is deﬁned
by: [hμ] = argmin[hα]∈H /∼
n
i=1 ds([hα], [hαi])2. Note that [hμ] is actually an
equivalence class of trajectories and one can select any element of this mean class
to help in alignment of multiple trajectories. The standard algorithm to compute
the Karcher mean, adapted to this problem, is as follows:
Algorithm 57 (Karcher Mean of Multiple Trajectories). Compute the
Karcher Mean of {αi(0)}s and set it to be μ(0).
1. Initialization step: Select μ to be one of the original trajectories and compute
its TSRVF hμ.
2. Align each hαi, i = 1, , n, to hμ according to Eq. 11.9. That is, solve for γ∗
i
using the DP algorithm and set ˜αi = αi ◦γ∗
i .
3. Compute TSRVFs of the warped trajectories, h˜αi, i = 1, 2, . . ., n, and update
hμ as a curve in Tc(M) according to: hμ(t) = 1
n
n
i=1 h ˜
αi(t).
4. Deﬁne μ to be the integral curve associated with a time-varying vector ﬁeld on
M generated using hμ, i.e., dμ(t)
dt
= (hμ)(t)c→μ(t), and the initial condition μ(0).
5. Compute E = n
i=1 ds([hμ], [hαi])2 = n
i=1 dh(hμ, h˜αi)2 and check it for con-
vergence. If the change in E from the previous iteration is larger than the stop-
ping criterion, then return to step 2.
For computing and analyzing the second and higher moments of a sample trajec-
tory, the tangent space Tμ(t)(M), for t ∈[0, 1], is used. This is convenient because
it is a vector space and one can apply more traditional methods here. First, for
each aligned trajectory ˜αi(t) at time t, the vector vi(t) ∈Tμ(t)(M) is computed
such that a geodesic that goes from μ(t) to ˜αi(t) in unit time has the initial ve-
locity vi(t). This is also called the shooting vector from μ(t) to ˜αi(t). Let ˆ
K(t) be
the sample covariance matrix of all the shooting vectors from μ(t) to ˜αi(t). The
sample Karcher covariance at time t is given by
ˆ
K(t) =
1
n −1
n

i=1
vi(t)vi(t)T ,
with ˆρ(t) = trace( ˆ
K(t)) .
(11.10)
This ˆρ(t) represents a quantiﬁcation of the cross-sectional variance, as a function of
t, and can be used to study the level of alignment of trajectories. Also, for capturing

11.4 Registration of Trajectories on Nonlinear Manifolds
411
the essential variability in the data, one can perform principal component analysis
(PCA) of the shooting vectors. The basic idea is to compute the singular value
decomposition (SVD) ˆ
K(t) = U(t)Σ(t)U T (t), where U(t) is an orthogonal matrix
and Σ(t) is the diagonal matrix of singular values. Assuming that the entries
along the diagonal in Σ(t) are organized in a non-increasing order, U1(t), U2(t),
etc. represent the dominant directions of variability in the data.
11.4.2 Analysis of Trajectories on S2
To illustrate this framework, in a relatively simple setting, we take the case of
M = S2, with the standard Euclidean Riemannian metric. For any two points
p, q ∈S2 (p ̸= −q) and a tangent vector v ∈Tp(S2), the parallel transport (v)p→q
along the shortest geodesic (i.e., great circle) from p to q is given by v−2⟨v,q⟩
|p+q|2 (p+q).
Registration of Trajectories As mentioned earlier, for any two trajectories on
S2, we can use their TSRVFs and the DP algorithm in Eq. 11.9 to ﬁnd the optimal
registration between them. In Fig. 11.14 we show one example of registering such
trajectories. The parameterization of trajectories is displayed using colors. In the
top row, the left column shows the given trajectories α1 and α2, the middle column
shows α1 and α2 ◦γ∗, and the right column shows γ∗using c = [0, 0, 1]. The corre-
spondences between two trajectories are depicted by black lines connecting points
along them. Due to optimization over γ in Eq. 11.9, the dh value between them
reduces from 1.67 to 0.36, and the correspondences appear quite reasonable after
the alignment. We also try diﬀerent choices of c’s (c = [0, 0, −1], [−1, 0, 0], [0, 1, 0]).
The registration results are very close despite diﬀerent c’s as shown in the bottom
row.
To emphasize practical utility of this framework, we will apply it to two speciﬁc
applications: bird migration data and hurricane tracks, and show how the cross-
sectional variance of mean trajectories is reduced by registration. We use the mean
of starting points of trajectories as the reference point c in Deﬁnition 11.2 for both
applications.
Analysis of Bird Migration Data This dataset has 35 migration trajectories
of Swainson’s Hawk, measured from 1995 to 1997, each having geographic coor-
dinates measured at some random times. Several sample paths are shown in the
top row in Fig. 11.15a. In the bottom panel of Fig. 11.15a, we show the optimal
warping functions {γ∗
i } used in aligning them, and this clearly underscores the
signiﬁcant temporal variability present in the data. In Fig. 11.15b and c, we show
the Karcher mean μ and the cross-sectional variance ˆρ without and with registra-
tion, respectively. For the deﬁnition of cress-sectional variance ˆρ, please refer to
Eq. 11.10. In the top row, ˆρ is displayed using colors, where red areas correspond
to higher variability in the given data. In the bottom row, the principal modes
of variation are displayed using ellipses on tangent spaces. We use the ﬁrst and
second principal tangential directions as the major and minor axes of ellipses and
the corresponding singular values as their lengths. We observe that: (1) the mean
after registration better preserves the shapes of trajectories and (2) the variance
ellipses before registration have major axis along the trajectory, while the ellipses
after registration exhibit the actual variability in the data. Most of the variability

412
11 Related Topics in Shape Analysis of Curves
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
a
b
c
Fig. 11.15 Swainson’s Hawk migration: (a) {αi} (top) and {γ∗
i } (bottom), (b) μ and ˆρ without
registration, (c) μ and ˆρ with registration
10
20
30
40
50
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0.16
Without registration
With registration
5
10
15
20
25
30
0
1
2
3
4
5
6
7 x10−3
Without registration
With registration
5
10
15
20
25
30
35
40
0
0.005
0.01
0.015
0.02
0.025
a
b
c
Without registration
With registration
0
10
20
30
0
0.2
0.4
0.6
0.8
1
1
2
3
4
5
6
7
8
9 10
0
0.2
0.4
0.6
0.8
1
1
2
3
4
5
6
7
0
0.2
0.4
0.6
0.8
1
Fig. 11.16 Comparison of ˆρ (ﬁrst row) and p-values (second row) without (red) and with (blue)
registration. (a) Bird migration. (b) Hurricane tracks subset 1. (c) Hurricane tracks subset 2
after registration is limited to the top end where the original trajectories indeed
had diﬀerences. The top row of Fig. 11.16a shows a decrease in the function ˆρ due
to the registration.
Next we construct a “Gaussian-type” model for these trajectories using esti-
mated summaries for the two cases (with and without temporal registration), as
described previously, and compute p-values of individual trajectories using Monte
Carlo simulation. The results are shown in the bottom of Fig. 11.16a, where we note
a general increase in the p-values for the original trajectories after the alignment.

11.5 Problems
413
Fig. 11.17 Summary of hurricane tracks without and with temporal registration
This is attributed to a reduced variance in the model due to temporal alignment
and the resulting movement of individual samples closer to the mean values.
Analysis of Hurricane Tracks We choose two subsets of Atlantic Tracks File
1851–2011, available on the National Hurricane Center website.1 The ﬁrst subset
has ten tracks and another has seven tracks, with observations at 6-hour sepa-
ration. We show the data, their Karcher mean, and variance without and with
registration in Fig. 11.17 for each subset. The decrease in the value of ˆρ is shown
in the top of Fig. 11.16b and c. Although the decrease here is not as large as the
previous example, we observe about 20 % reduction in ˆρ in average due to regis-
tration. In the bottom plots of Fig. 11.16b and c, it is also shown that there is a
general increase of p-values after registration although it decreases in a few cases.
11.5 Problems
11.5.1 Theoretical Problems
1. The pre-shape space for a joint analysis of curves using their orientation, scale
and shape is the full L2([0, 1], Rn). Show that a geodesic path in this space
under the standard L2 metric is a straight line.
2. In the case where both shape and scale are included in the analysis of curves,
the pre-shape space (before removal of ˜ΓI) is given by L2([0, 1], Rn)/SO(n).
Derive an expression for a geodesic path in this space.
1http://www.nhc.noaa.gov/pastall.shtml.

414
11 Related Topics in Shape Analysis of Curves
3. In the case where both shape and orientation are included in the analysis, the
pre-shape space (before removal of ˜ΓI) is given by C2. Derive an expression
for geodesic path in this space.
4. For the square-root function given in Deﬁnition 11.1, show that the action of
the re-parameterization group ˜ΓI is given by (h, γ) = (h ◦γ)

˙γ(t).
5. Show that the mapping (GL(2) ⋉R2) × L2([0, 1], R2) →L2([0, 1], R2), given
by ((A, x), q)(t) ≡(Aq(t) + x) forms an action of GL(2) ⋉R2 on L2([0, 1], R2).
Furthermore, verify that this action is not by isometries under the L2 metric.
That is, in general, we have:
⟨q1, q2⟩̸= ⟨((A, x), q1), ((A, x), q2)⟩,
(A, x) ∈GL(2) ⋉R2 .
6. Deﬁne Ω = {ωI3|ω ∈R+} to be the set of 3 × 3 positive, scalar matrices and
show that it is a subgroup of GL(3). (In fact, it is a normal subgroup.) Deﬁne
the natural action of Ω on GL(3) according to the map Ω × GL(3) →GL(3)
given by (ω, A) = ωA. Under this action, the orbit of any A ∈GL(3) is given
by:
[A] = {AB|B ∈Ω} = {ωA|ω ∈R+} .
Form the quotient space PGL(3) ≡GL(3)/Ω = {[A]|A ∈GL(3)} and show
that PGL forms a group under matrix multiplication. (This set is called the
projective linear group or projective general linear group.)
7. Show that the mapping PGL(3) × L2([0, 1], R2) →L2([0, 1], R2) given by
([A], q)(t) ≡˜q(t), where ˜q is deﬁned using

B,
q(t)
1

→
B

q(t)
1


B
q(t)
1

3
=
 ˜q(t)
1

,
for any B ∈[A] .
forms an action of PGL(3) on L2([0, 1], R2. (Here, (·)3 denotes the third ele-
ment of a 3-vector. Note that this action is not deﬁned for a q if, for some t,

B

q(t)
1

3
= 0.) Furthermore, verify that this action is not by isometries
under the L2 metric.
8. Show that the section C c
a is not a section of the set C c
2 .
9. Derive analytical expressions for the basis elements h1, h2, h3, and h4 given in
Eqs. 11.4–11.5 and afterward.
10. Consider the problem of ﬁnding a group element A∗∈GL(2) such that ΣA∗β ∝
I2, for a given β ∈F2. Show that it is suﬃcient to search for A∗only over the
subspace S(2) ∩GL(2), where S(2) is the set of all 2 × 2 symmetric matrices.
11. Using the deﬁnition given in Sect. 11.4.1, show that ˜ΓI acts on M according
to (α, γ) = α ◦γ.
12. For a Riemannian manifold M, let dm denote the geodesic distance between
points and let dx(α1, α2) =
 1
0 dm(α1(t), α2(t))dt be the distance between tra-
jectories on M. Show that, in general,
dx(α1, α2) ̸= dx(α1 ◦γ, α2 ◦γ), for γ ∈ΓI .

11.6 Bibliographic Notes
415
13. Verify that the following is an equivalence relation. For any two trajectories
α1 and α2, deﬁne α1 ∼α2, when:
a. α1(0) = α2(0), and
b. there exists a sequence {γk} ∈ΓI such that limk→∞h(α1◦γk) = hα2; this
convergence is measured under the L2 metric.
11.5.2 Computational Problems
1. Write a program to compute distances between any two curves in Rn using the
following combination of features: (1) shape, orientation, and scale, (2) shape
and orientation, (3) shape and scale, (4) shape.
2. Write a program to compare any two Euclidean curves based on all their features
(shape, position, scale, and orientation) using the square-root representation.
3. Write a program to cluster a set of given Euclidean curves using distances
derived in the previous two programs.
4. Write a program to aﬃne-standardize any planar closed curve using Algorithm
53.
5. Write a program to implement the path-straightening algorithm for computing
geodesic paths between planar curves as elements of C c
a .
6. Develop and implement an expression for geodesic path in GL(n) for de-
standardizing geodesics obtained in the last problem.
7. Write a program to compute TSRVF hα of a given trajectory α on a Riemannian
manifold M, using ﬁnite diﬀerences and parallel transport.
8. Implement the optimization suggested in Eqn. 11.9 for alignment of TSRVFs
of any two trajectories on a Riemannian manifold M.
9. Implement Algorithm 57 to compute Karcher mean of multiple trajectories.
10. Write a program to compute the cross-sectional scalar variance function ˆρ(t)
for a set of trajectories. Evaluate this function on a dataset before and after
registration of trajectories.
11.6 Bibliographic Notes
The material on analyzing curves using shape and other features is primarily taken
from the paper [57]. An application of this framework for clustering white matter
ﬁber tracts is presented in [73].
Some of the early work on projective shape analysis of landmark conﬁgurations
has been described in [51, 74, 52]. The material on aﬃne-invariant elastic shape
analysis of planar curves was ﬁrst presented in [21]. This paper also studied projec-
tive shape analysis but only for landmark representations, not curves. The UMD
dataset used here is take from [118].
An interesting problem of estimating smooth trajectories (splines) on nonlinear
manifolds, from discrete-time and noisy observations, is discussed in [109]. The rep-
resentation of trajectories using vector ﬁelds have been discussed by many papers.
For instance, forming a curve in a Euclidean space using translation of vector ﬁeld
˙α(t) to the starting point of a trajectory is described in [44]. The TSRVF represen-
tation was introduced by Su et al. in [111]. The use of bird migration data here is

416
11 Related Topics in Shape Analysis of Curves
motivated by the work of Owen and Moore [86] who studied migration patterns for
Swainson’s Hawk birds and their eﬀects on their immune system. This framework
has more recently been applied for human action recognition using depth sensors
in [8]. Also, diﬀerent mathematical representations, involving SRVFs in the tan-
gent spaces of starting points of curves, have been introduced for a more intrinsic
analysis of trajectories [128].

Appendix A
Background Material
A.1 Basic Diﬀerential Geometry
In this section, we will introduce the concepts of diﬀerentiable manifolds, tangent
spaces, exponential maps, and integral ﬂows. We start with the question: What is
a manifold? Loosely speaking, a manifold is a space that can be ﬂattened locally,
although this may not be possible globally. We will start by deﬁning the ﬁnite-
dimensional manifolds, taking a few steps to develop a formal deﬁnition.
Deﬁnition A.1. A topology on a set X is a collection T of subsets of X, called
open sets, satisfying
1. X and the empty set ∅are in T .
2. The union of an arbitrary collection of open sets in T is in T .
3. The intersection of a ﬁnite collection of open sets in T is in T .
The pair (X, T ) is referred to as a topological space. Given a topological space
(X, T ), a collection U of subsets of X forms a basis for the topology T , if
1. Every set in U is open (i.e., U ⊂T ).
2. For every open set V ∈T and for every x ∈V , there exists a set U ∈U such
that x ∈U ⊂V .
One way to obtain a topological space is to start with a metric space, which is a
set with a distance function deﬁned on it.
Deﬁnition A.2. A metric space is a set X equipped with a distance function
or a metric d : X × X →R such that for all x, y, z ∈X:
1. d(x, y) > 0 for x ̸= y and d(x, x) = 0.
2. d(x, y) = d(y, x).
3. d(x, z) ≤d(x, y) + d(y, z).
Given a metric space X with a distance function d, and given x ∈X and ϵ > 0,
we deﬁne the open ϵ-ball centered at x by
Bϵ(x) = {y ∈X : d(x, y) < ϵ}.
© Springer-Verlag New York 2016
A. Srivastava, E.P. Klassen, Functional and Shape Data Analysis,
Springer Series in Statistics, DOI 10.1007/978-1-4939-4020-2
417

418
A Background Material
A metric on a space X gives rise to a very natural topology, called the metric
topology, in which a set U ⊂X to deﬁned to be open if for every x ∈U, there
exists an ϵ > 0 such that Bϵ(x) ⊂U. It is easy to verify that this deﬁnes a topology
on X and that the set of open ϵ-balls in X provides a basis for this topology.
Deﬁnition A.3. A topological space is said to be Hausdorﬀif for every pair of
distinct points p, q ∈X, there exist disjoint open subsets U ⊂X containing p and
V ⊂X containing q.
It is an easy exercise to show that every metric space is Hausdorﬀwhen endowed
with the metric topology. Let X and Y be topological spaces. A map f : X →Y
is said to be continuous if for every open set U ⊂Y , the inverse image f −1(U) is
open in X. If it is both one-to-one (injective) and onto (surjective), then it is called
a bijective map. A continuous, bijective map f : X →Y with continuous inverse
is called a homeomorphism. X and Y are homeomorphic to each other when
there exists a homeomorphism between them. A manifold is a topological space
that is locally homeomorphic to Rn. To be more precise, we have the following
deﬁnition. In this deﬁnition, for a point p ∈M, any open set U containing p is
termed a neighborhood of p.
Deﬁnition A.4 (Manifold). A topological space M is called a manifold of di-
mension n if:
1. It is Hausdorﬀ,
2. It has a countable basis, and
3. For each point p ∈M, there is a neighborhood U of p that is homeomorphic to
an open subset of Rn.
The last property is termed the locally Euclidean property of M. According
to this property, for each p ∈M, there exists an open neighborhood U of p and
a mapping φ : U →Rn such that φ(U) is open in Rn and φ : U →φ(U) is
a homeomorphism. The pair (U, φ) is called a coordinate chart for the points
that fall in U; for any point y ∈U, one can view the Euclidean coordinates
φ(y) = (φ1(y), φ2(y), . . . , φn(y)) as the coordinates of y. The dimension of the
manifold M is n. This is a way of ﬂattening the manifold locally. Using φ and φ−1,
one can move between the sets U and φ(U) and perform calculations in the more
convenient Euclidean space. (See Fig. A.1.)
M
U
x
x (U)
Fig. A.1 A coordinate chart (U, x) on a manifold (The mapping should be φ not x)

A Background Material
419
In our applications of geometry, having just homeomorphic charts is not suﬃ-
cient. We would like to be able to evaluate ﬁrst and higher derivatives of functions
on our manifold. Two charts (U, φ) and (V, ψ) are called smoothly compatible
if the functions φ◦ψ−1 : ψ(U ∩V ) →φ(U ∩V ) and ψ ◦φ−1 : φ(U ∩V ) →ψ(U ∩V )
(which are homeomorphisms between open sets in Rn) are smooth, i.e., they have
continuous partial derivatives of all orders. A smooth atlas A is a collection
of charts which are pairwise smoothly compatible and whose domains cover M,
i.e. M = ∪U∈A U. For a given manifold, one can imagine having many diﬀerent
atlases. A smooth atlas A on M is said to be maximal if every chart on M
which is smoothly compatible with all the charts in A is already in A . A smooth
structure on a manifold M is a maximal smooth atlas. A manifold with such a
smooth structure is termed a diﬀerentiable manifold . The nice thing about a
diﬀerentiable manifold M is that we can calculate derivatives of functions on M in
terms of any coordinate chart we wish, and if we change coordinate charts, these
derivatives will be related by the chain rule.
Example A.1. 1. The Euclidean space Rn is an n-dimensional diﬀerentiable man-
ifold which can be covered by the single chart (Rn, φ), φ(x) = x. Similarly, any
open subset of Rn is also an n-dimensional diﬀerentiable manifold which can,
again, be covered by a single chart.
2. Any open subset of a diﬀerentiable manifold is itself a diﬀerentiable manifold.
(Obtain charts on the open set by intersecting it with charts on the original
manifold.) A well-known example of this idea comes from linear algebra. Let
M(n) be the set of all n×n matrices; M(n) can be identiﬁed with the set Rn×n
and is, therefore, a diﬀerentiable manifold. Deﬁne the subset GL(n) as the set
of non-singular matrices, i.e.
GL(n) = {A ∈M(n)| det(A) ̸= 0} ,
where det(·) denotes the determinant of a matrix. Since GL(n) is an open
subset of M(n), it is also a diﬀerentiable manifold. It is, in fact, an important
manifold and some of its subsets will play a crucial role in formulating invariance
of shapes.
3. If M and N are two diﬀerentiable manifolds of dimensions m and n, respec-
tively, then their Cartesian product M × N is also a diﬀerentiable manifold, of
dimension m+n. For example, the Cartesian product GL(n)×Rn is a manifold
of dimension n2 + n.
4. An important example of a diﬀerentiable manifold, both for our treatment of
shapes and in general, is the unit sphere in Rn+1:
Sn = {p ∈Rn+1|
n+1

i=1
p2
i = 1}
To show that it is a diﬀerentiable manifold, we consider the case n = 2. The
local charts on S2 can be obtained using stereographic projection onto the plane.
Stereographic projection maps points on S2 to the xy plane in Euclidean space
R3. This projection is obtained by taking the intersection of the line connecting
the north pole N = (0, 0, 1) and the point to be projected, with the xy plane,
and can be written mathematically as: φ−1 : R2 →S2 as

420
A Background Material
Fig. A.2 Illustration of the stereographic projection of S2
φ−1(u, v) →

2u
u2 + v2 + 1,
2v
u2 + v2 + 1, u2 + v2 −1
u2 + v2 + 1

.
Under φ, the lower hemisphere of S2 maps to the region inside the unit circle
in the xy plane, the equator maps to the unit circle, and the upper hemisphere
(except for N) maps to the region outside the unit circle. Figure A.2 shows the
stereographic projection of a point p on the xy plane. This chart covers all of S2
except the north pole N. A similar chart can be constructed by projecting from
the south pole (0, 0, −1). The domain of this second chart is the complement
of the south pole. These two charts are smoothly compatible and cover S2, so
together they form a smooth atlas on S2.
In this book we will deal only with the diﬀerentiable manifolds and from now on
we will refer to them simply as manifolds.
Shifting attention to mappings between manifolds, how does one deﬁne their
smoothness? For Euclidean spaces, the smoothness of maps is deﬁned using partial
derivatives. A mapping f : Rm →Rn is smooth if all its partial derivatives exist
and are continuous. Now in a more general case, let f : M →N be a mapping
between two manifolds M and N. For any point p ∈M, consider local charts (U, φ)
with p ∈U and (V, ψ) with f(p) ∈V . If the mapping: ψ◦f ◦φ−1 : φ(U ∩f −1(V )) →
ψ(V ) is smooth at the point φ(p), in the sense that all its partial derivatives exist
and are continuous, then f is called smooth at the point p. Figure A.3 shows a
pictorial illustration of this idea.
Because of the smooth compatibility of charts, this determination will not de-
pend on which charts are used. If f is smooth for all points on M, then it is
called a smooth mapping from M to N. Additionally, if f is a bijective smooth
mapping with an inverse that is also smooth, then it is called a diﬀeomor-
phism. We will discuss these mappings on simple domains such as [0, 1], S1, and
[0, 1]2 in detail later in this chapter. As an example, let a point on a unit cir-
cle be represented by (cos(θ), sin(θ)). We can deﬁne a mapping f : S1 →S1 by
f(cos(θ), sin(θ)) = (cos(θ + 1), sin(θ + 1)). This f is in fact a diﬀeomorphism from
the unit circle to itself. (f is simply a rotation by one radian.) Later in Chap. 6, in
the context of parameterized closed curves, the diﬀeomorphisms of this kind will
be called translational re-parameterizations.

A Background Material
421
Fig. A.3 Illustration of compositions of smooth mappings
A.1.1 Tangent Spaces on a Manifold
In order to perform diﬀerential calculus, i.e. to compute gradients, directional
derivatives, critical points, etc., of functions on manifolds, one needs to understand
the tangent structure of those manifolds. The central idea here is to deﬁne tangent
spaces at diﬀerent points on the manifold and to relate tangent vectors with dif-
ferential operators. Although there are several ways to deﬁne tangent spaces, one
intuitive approach is to consider diﬀerentiable (C1) curves on the manifold passing
through the point of interest and to study the velocity vectors of these curves at
that point. Diﬀerent velocity vectors, corresponding to all possible curves, will be
tangential to the manifold at that point and form a vector space that is termed as
the tangent space at that point (see Fig. A.5). In simple words, the tangent space
at a point is the set of all directions one can be traveling in while traversing the
manifold at that point.
More formally, let M be an n-dimensional manifold and, for a point p ∈M,
consider a diﬀerentiable curve γ : (−ϵ, ϵ) →M such that γ(0) = p. Let (U, φ)
be a coordinate chart that includes p. Since γ is diﬀerentiable and φ is smooth,
the composition μ ≡(φ ◦γ) : (−ϵ, ϵ) →Rn is also diﬀerentiable. The derivative
˙μ(0) ≡
d
dtμ(t)|t=0 denotes the velocity of γ at p in local coordinates. This vector
has the same dimension as the manifold M itself. For any two diﬀerentiable curves
γ1 and γ2 passing through p at t = 0, deﬁne an equivalence relation by γ1 ∼γ2 if
d
dtμ1(0) = d
dtμ2(0) ,
where μi = φ ◦γi, i = 1, 2 .
This deﬁnes an equivalence relation on the space of diﬀerentiable curves on M
passing through p. Each equivalence class takes the form
[γ] = {β : (−ϵ, ϵ) →M| β(0) = p and d
dt(φ ◦β)(t)|t=0 = d
dt(φ ◦γ)(t)|t=0} .
A tangent vector to M at p is deﬁned to be one of these equivalence classes.
The set of all such tangent vectors (equivalence classes) is called the tangent
space to M at p, or Tp(M). There is a bijective map Tp(M) →Rn deﬁned by
[γ] →
d
dtφ ◦γ|t=0. Using this map, one transfers the operations of addition and
scalar multiplication from Rn to Tp(M), making Tp(M) a real vector space of

422
A Background Material
Fig. A.4 Tangent vector v deﬁned as the instantaneous velocity of a C1 curve passing through
p for R3 and S2
dimension n. (Using the chain rule, it is easy to see that the structure of Tp(M) as
a vector space does not depend on which chart φ we used.) This is a very important
fact that we will rely on heavily in statistical analysis of shapes. Even though the
manifold M may be nonlinear, the tangent space Tp(M) is always linear and one
can impose probability models on it using more traditional approaches.
Example A.2. 1. In the case of the Euclidean space Rn, the tangent space Tp(Rn) =
Rn for all p ∈Rn. To see this, consider a curve γ : (−ϵ, ϵ) →Rn such that
γ(0) = p; for this curve ˙γ(0) will be a vector in Rn; see the left panel of
Fig. A.4. (Recall that for Rn the coordinate chart is simply the identity map
φ(x) = x.) In fact, for any vector v ∈Rn, we can form a curve γ(t) = v t + p,
such that γ(0) = p and ˙γ(0) = v. It follows that the tangent space to Rn at p,
Tp(Rn), will be all of Rn.
2. For GL(n), the space of non-singular matrices and for an A ∈GL(n), let γ(t)
be a path in GL(n) passing through A ∈GL(n) at t = 0. Its velocity vector at
A, ˙γ(0), is an element of M(n), the set of all n × n matrices. As in the previous
example, given any element V ∈M(n), we can construct a diﬀerentiable path
γ(t) = V t + A; hence, V is in the tangent space at that point. (One has to be
careful to keep the domain of γ small enough to ensure that its image lies in
GL(n).) Thus, the tangent space TA(GL(n)) is all of M(n) and this holds for
any A ∈GL(n).
3. For the unit circle, S1 ⊂R2, let γ : (−ϵ, ϵ) →S1 be a diﬀerentiable curve such
that γ(0) = p ≡(p1, p2). Since S1 ⊂R2, we can write γ(t) = [γ1(t) γ2(t)], such
that γ1(t)2 + γ2(t)2 = 1. Taking derivatives with respect to t, we get:
γ1(t) ˙γ1(t) + γ2(t) ˙γ2(t) = 0,
or
⟨γ(t), ˙γ(t)⟩= 0 ,
where ⟨·, ·⟩denotes the Euclidean inner product. Setting t = 0, we see that
the tangent vectors to S1 at p are orthogonal to p under the Euclidean inner
product. Since all the orthogonal vectors take a speciﬁc form, we can write the
tangent space as Tp(S1) = {α(−p2, p1)|α ∈R}. This can also be seen by taking
the velocity vector of a curve, e.g. γ(t) = (p1 cos(tα) −p2 sin(tα), p1 sin(tα) +
p2 cos(tα)). γ is a diﬀerentiable curve such that γ(0) = (p1, p2) and ˙γ(0) =
α(−p2, p1).
4. Similar to the circle, the tangent space of Sn at a point p is the set of all velocity
vectors associated with the C1 curves passing through p (see the right panel of
Fig. A.4 for an example). The resulting space is an n-dimensional hyperplane

A Background Material
423
Fig. A.5 Tangent space to M at p
in Rn+1 orthogonal to the vector p under the Euclidean metric:
Tp(Sn) = {v ∈Rn+1| ⟨v, p⟩= 0}
Among other things, the tangent vectors play an important role in computing
derivatives of functions where they are called derivations. In fact, one can com-
pletely develop tangent vectors and their spaces from this point of view. Let C∞(p)
be the set of all functions from Rn →R that are smooth at p ∈Rn. For a point
p ∈Rn, a linear map X : C∞(p) →R is called a derivation at p if it satisﬁes the
Leibnitz rule:
X(fg) = X(f)g(p) + X(g)f(p), for all f, g ∈C∞(p)
For instance, the partial derivative operators
∂
∂xi are examples of derivations. It
turns out that the space of all such derivations can be identiﬁed with the tangent
space Tp(M), and there is a one-to-one correspondence between tangent vectors
and derivations. Since Tp(Rn) = Rn, any tangent vector v is simply an element of
Rn. To identify it with a derivation, deﬁne the mapping from Rn to the space of
derivations as v →Xv ≡n
i=1 vi
∂
∂xi . This mapping is a bijection and can be used
to interchange between the two interpretations of tangent vectors. Associated with
any tangent vector v is a unique derivation Xv and vice versa. This association
underlines the usage of tangent vectors for computing directional derivatives. As
mentioned earlier, a tangent vector provides a valid direction of traversal on the
manifold, while a derivation provides derivatives of functions. Their identiﬁcation
naturally leads to the notion of directional derivatives. For example, for a
function f ∈C∞(p), the notation vf(p) stands for the directional derivative of f
in the direction of v at p and is deﬁned to be vf(p) ≡Xvf(p) = n
i=1 vi
∂f
∂xi (p).
This can also be done by computing the directional derivative of the composition
f ◦γ(t), where γ is a diﬀerentiable curve in Rn such that γ(0) = p and ˙γ(0) = v,
and then evaluating its derivative at t = 0, i.e. vf(p) = d(f◦γ)
dt
|t=0.
This identiﬁcation between tangent vectors and derivations extends more gen-
erally to arbitrary diﬀerentiable manifolds (see Fig. A.6). For a manifold M and a

424
A Background Material
Fig. A.6 Coordinate curves w.r.t. the parametrization x
point p ∈M, let C∞(p) be the set of all smooth functions M →R whose domain
includes p. As in the case of Rn, deﬁne a linear transformation X : C∞(p) →R to
be a derivation at p if it satisﬁes the Leibnitz rule
X(fg) = X(f)g(p) + X(g)f(p).
To see how each tangent vector in Tp(M) corresponds to a derivation at p, suppose
γ : (−ϵ, ϵ) →M is a diﬀerentiable curve such that γ(0) = p, and let v = [γ], the
equivalence class of C1 curves with the velocity v at that point. Then, deﬁne
Xv(f) = d
dt(f ◦γ(t))|t=0 .
It is not diﬃcult to verify that Xv is a derivation at p and that the correspondence
v →Xv provides a bijection between Tp(M) and the set of derivations on M at p.
So, one can interchangeably consider a tangent vector as a derivative operator for
functions at p or velocity vector of a curve passing through p.
Example A.3. 1. For any vector v ∈Rn and a smooth function f : Rn →R, the di-
rectional derivative of f in the direction of v is given by vf(p) = n
i=1 vi
∂f
∂xi (p).
2. Similarly, in GL(n), the directional derivative of a smooth function f
:
GL(n) →R at A ∈GL(n), in the direction V
∈M(n) is given by:
V f(A) = n
i,j=1 Vij
∂f
∂xij (A).
3. On a unit circle, let f : S1 →R be a smooth function; we will now compute the
directional derivative of f in the direction of α(−p2, p1) ∈TP (S1), at p ∈S1.
Deﬁne a curve γ(t) = (p1 cos(tα) −p2 sin(tα), p1 sin(tα) + p2 cos(tα)) passing
through p at t = 0. Note that ˙γ(0) = α(−p2, p1). Thus, the directional derivative
of f at p is:
d(f ◦γ)
dt
|t=0 = (−p2
∂f
∂p1
+ p1
∂f
∂p2
)(p) .
(To make sense of this formula, we assume that f is not only deﬁned on S1,
but it has been extended to a small neighborhood of S1 in R2. Otherwise, the
derivatives
∂f
∂p1 and ∂f
∂p2 would not be deﬁned.) On more complicated manifolds,
it is not straightforward to come up with curves, associated with given tangent

A Background Material
425
vectors, that can be used to evaluate directional derivatives. Later on we will
introduce the notion of integral curves that can be used to deﬁne and compute
directional derivatives of functions. We postpone additional examples for later.
A.1.2 Submanifolds
Although many sets of interest can be studied intrinsically as manifolds, some
others are better seen as submanifolds of larger manifolds. This will be especially
useful in the case of inﬁnite-dimensional manifolds where most of the sets we will
encounter will be treated as submanifolds of larger better-known spaces.
Deﬁnition A.5 (Submanifold). Let M be a smooth m-dimensional manifold.
A subset N ⊂M is an n-dimensional submanifold if for every p ∈N, there is a
coordinate chart (U, φ) for M, with p ∈U, such that N ∩U = φ−1(Rn×{0}(m−n)).
This means that although under the chart φ, the points of U ⊂M correspond to
points of Rm, and the points in the subset N ∩U correspond to points of Rm for
which the last m −n coordinates are 0.
Next we introduce the notion of a diﬀerential which is important in deﬁning
the submanifolds of interest to us.
Deﬁnition A.6 (Diﬀerential of a Mapping).
The diﬀerential of a smooth
mapping f : M →N at p ∈M, denoted by dfp, is a linear map dfp : Tp(M) →
Tf(p)(N) speciﬁed as follows. Let g : N →R be a smooth function. Then, for any
v ∈Tp(M), deﬁne
(dfp(v))(g) = v(g ◦f)(p) .
Here, dfp(v) ∈Tf(p)(N) is treated as a derivation which when applied to g results
in the directional derivative of g. According to the right side, this derivative is
deﬁned by forming a new function f ◦g : M →R and computing its directional
derivative at p using v. In other words, the directional derivative of g on N is
deﬁned as the directional derivative of the composition f ◦g on M.
Example A.4. 1. In case M = Rm and N = Rn, and for any f : M →N, the dif-
ferential dfp : Rm →Rn is represented by the matrix of ﬁrst partial derivatives
of the components of f; dfp = { ∂fi
∂xj }.
2. Let M = GL(n) and N = R and deﬁne f : M →N by f(A) = 1
2trace(AT A).
Given a tangent vector V ∈TA(GL(n)) ≡M(n), we compute the directional
derivative:
V f(A) = d
dt
1
2trace((A + tV )T (A + tV ))

|t=0 = trace(V T A) .
The diﬀerential of f, dfA : M(n) →R, is given by the mapping dfA(V ) =
trace(V T A).
If we think of tangent vectors at p as equivalence classes of curves passing
through p that share the same derivative at p, we can deﬁne the diﬀerential by
dfp([γ]) = [f ◦γ]. The right side is an equivalence class of diﬀerentiable curves
passing through an f(p) in N. This deﬁnition is equivalent to the one just given.

426
A Background Material
As we will see, the rank of the diﬀerential of f is an important descriptor of the
behavior of f near p.
We now describe a useful way of obtaining submanifolds. A point p ∈M is
deﬁned to be a critical point of f if the diﬀerential dfp is not onto and is called
a regular point if dfp is onto. The image of a critical point, f(p), is called a
critical value of f. Any point q ∈N that is not a critical value is said to be a
regular value of f, i.e., a point q ∈N is a regular value if every point in f −1(q)
is a regular point.
Example A.5. 1. For f : R →R2 given by t →(sin(t), cos(t)), the diﬀerential is
dft(x) = (cos(t)x, −sin(t)x). Since this dft is not onto (no linear map R →R2
is onto!), all the points in R are critical points of f.
2. If we deﬁne f : R2 →R by f(x, y) = x2 + y2, then we get a diﬀerential
df(x,y)(u, v) = 2xu + 2yv. This df(x,y) is onto at every point (x, y) except the
origin (0, 0). Hence, (0, 0) is a critical point of f, while every other point of R2
is a regular point.
Theorem A.1. Suppose M and N are manifolds of dimensions m and n, respec-
tively, and let f : M →N be a smooth map, with a regular value y ∈N. Then
f −1(y) is a submanifold of M of dimension m−n. Furthermore, the tangent space
of f −1(y) at a point p is given by the kernel of dfp.
(Note that the kernel of a linear transform, denoted by ker(·), is the set of vectors
in its domain whose image under that mapping is zero. For example, the kernel of
dfp is given by ker(dfp) = {x ∈Tp(M)|dfp(x) = 0}.) An elementary proof of this
theorem can be found in [78] pp.11.
Example A.6. 1. Unit Sphere: Using this theorem, let us check if Sn is indeed a
submanifold of Rn+1. Let f : Rn+1 →R be a map given by f(p) = n+1
i=1 p2
i ,
where p = (p1, . . . , pn+1). The diﬀerential of f is given dfp(u) = 2⟨p, u⟩, which
is clearly onto for all p ∈f −1(1). Thus, 1 is a regular value of f and the set
f −1(1) given by Sn is an n-dimensional submanifold of Rn+1. Also, the tangent
space TpSn = ker(dfp), which is just the orthogonal complement of p ∈Rn+1,
conﬁrming a result we ﬁrst derived in Example A.2.
2. Orthogonal Matrices: We now consider the set O(n) of orthogonal matrices,
which is a subset of the manifold GL(n). We deﬁne O(n) to be the set of all
n × n invertible matrices A that satisfy AAT = I. We will apply Theorem A.1
to prove that O(n) is a submanifold of GL(n). To do this, deﬁne S(n) to be
the set of n × n symmetric matrices, and then deﬁne f : GL(n) →S(n) by
f(A) = AAT . It can easily be shown that I is a regular value of f, and, hence,
f −1(I) = O(n) is a submanifold of GL(n). Note that O(n) is not connected
but has two components: those orthogonal matrices with determinant +1 and
those with determinant −1. The set of orthogonal matrices with determinant 1 is
called the special orthogonal group and denoted by SO(n). The dimension of
O(n) can be determined by the above theorem; it is n2−n(n+1)/2 = n(n−1)/2.
Since O(n) is a very important example which will be of use to us later, we
examine it in a little more detail. First, note that if A is an element of O(n),
so is A−1; also, if A and B are elements of O(n), so is AB. By deﬁnition, this
makes O(n) a group (see Sect. A.2). Let us calculate TIO(n), the tangent space
of O(n) at the identity matrix. We compute
dfI(X) = d
dt(I + tX)(I + tX)T|t=0 = X + XT .

A Background Material
427
It follows that TIO(n) = ker(dfI) = {X : X + XT = 0}, which is the set
of skew-symmetric matrices. We now determine the tangent space of O(n) at
an arbitrary matrix A ∈O(n). Since left multiplication by A is a diﬀeomor-
phism O(n) →O(n) (its inverse is left multiplication by A−1), it follows that
TAO(n) = {AX|X is skew symmetric}.
A.2 Basic Algebra
An important property in our notion of shape is its invariance under operations
like rotations, translations, and rescalings of objects. To take such invariance into
account in a mathematical framework, the main mathematical tool is group the-
ory. Here one considers the set of all transformations, for example rotations, as a
group, where the group operation is concatenation (or composition) of two trans-
formations. For instance, if an object is transformed by two rotations, one after
another, what is the cumulative eﬀect of these two transformations? It will be a
rotation that is composed of the original two rotations combined through a group
operation. The same holds for the translations, scalings, and others. In this sec-
tion, we introduce the deﬁnitions and concepts associated with group theoretic
representations of transformations of shapes.
We start with the deﬁnition of a group.
Deﬁnition A.7 (Group). A group G is a set having an associative binary oper-
ation, denoted by ·, such that:
1. there is an element e in G such that e · g = g · e = g for all g ∈G, and
2. for every g ∈G, there exists a unique h such that g · h = h · g = e.
e is called the identity element of G and h is called the inverse of g, denoted by
g−1.
Consider some simple examples of groups:
Example A.7. • The set of all real numbers is a group with addition being the
binary operation, and the identity element being 0. Similarly, Rn is a group
with vector addition as the group operation and the zero vector as the identity
element. This group is called the translation group for the following reason.
If we take an object deﬁned with respect to some coordinate system and add
a vector v ∈Rn to all points of this object, the result will be the same object
located at a new point in that coordinate system, i.e. the object will be trans-
lated by the vector v. Two translations v1 and v2, one after another, will result
in a total translation of v1 + v2 in the translation group. Addition by a zero
vector does not change the location of the object.
• The set of positive real numbers is a group with binary operation being mul-
tiplication and the identity element being one. We will denote this group by
R×
+. It is called the scaling group because for any a ∈R×
+, a vector av, for a
v ∈Rn is simply a scaled version of v. Two scalings, a1 and a2, applied to the
same vector result in a single scaling by a1a2 , and a scaling by a = 1 does not
change v.
• Consider the set of n×n non-singular matrices, GL(n), studied earlier. GL(n) is
a group with the group operation being matrix multiplication and the identity

428
A Background Material
element being the n×n identity matrix I. Due to this group structure, GL(n) is
also called the general linear group. This is the group of all non-singular linear
transformations on Rn—any A ∈GL(n) is a linear transformation on Rn.
In other words, for any a1, a2 ∈R and v1, v2 ∈Rn, we have A(a1v1 + a2v2) =
a1(Av1) + a2(Av2).
• The direct product of any two groups is also a group. If G1 and G2 are two
groups with their respective group operations, then G = G1×G2 is a group with
the group operation (g1, h1)·(g2, h2) = (g1·g2, h1·h2). For example, GL(n)×Rn
is a group with the group operation (A1, v1) · (A2, v2) = (A1A2, v1 + v2).
Some additional properties of a group can be useful in the analysis of shapes.
A group is called abelian if it satisﬁes the commutative law, that is g · h = h · g
for all g, h ∈G. Otherwise, it is called a non-abelian group. As an example, the
real line with addition operation is an abelian group, while GL(n) is a non-abelian
group for n > 1.
GL(n) is a rather large group containing many types of linear transformations
that can be applied to elements of Rn. To focus on more speciﬁc transformations,
we introduce the notion of subgroups. A subset S of a group G is said to be closed
under multiplication if for every pair g, h ∈S, the product is also in S, i.e. g·h ∈S.
Deﬁnition A.8 (Subgroup). A subset H of a group G is said to be a subgroup
if it is nonempty, closed under the group operation, and for each g ∈H, the inverse
g−1 is also in H.
Example A.8. • The set Z of all integers is a subgroup of R under addition.
• There are several interesting subgroups of GL(n) that will be central to shape
analysis. For instance, consider the set of all n × n matrices that have determi-
nant +1:
SL(n) = {A ∈GL(n)
** det(A) = +1}
SL(n) is called the special linear group and is a subgroup of GL(n). It
is a subgroup because: (i) it is closed, i.e. for A1, A2 ∈SL(n), we have
det(A1A2) = det(A1) det(A2) = 1 and (ii) the inverse of A ∈SL(n) is in SL(n),
i.e. det(A−1) = (det(A))−1 = 1. SL(n) is the set of all volume-preserving, linear
transformations of Rn. The volume enclosed by a parallelepiped formed by vec-
tors v1, v2, . . . , vn ∈Rn is given by the determinant of matrix V with columns
given by vi. So det(AV ) = det(A) det(V ) = det(V ) if A ∈SL(n).
• Rotation Group: Consider the orthogonal group, O(n) = {A ∈GL(n)
**AT A =
I}, and the special orthogonal group, SO(n) = {A ∈GL(n)
**det(A) =
+1
**AT A = In}, which we have introduced as examples of submanifolds of
GL(n) in Example A.6. SO(n) is a subgroup of GL(n), O(n) and SL(n). SO(n)
is important in shape analysis because it is the set of all rotations of Rn. If
x ∈Rn, O ∈SO(n), then O · x is simply a rotation of x; it does not change the
length of x, i.e. ∥Ox∥2 = xT OT Ox = xT x = ∥x∥2. If O1, O2 ∈SO(n) denote
two rotations of an object, then O1 ·O2 denotes the cumulative rotation applied
to that object. Similarly, for O ∈SO(n), the inverse O−1 = OT provides the
rotation to undo the eﬀect of O on an object. For n = 2, elements of SO(2) take
the form:

cos(θ) −sin(θ)
sin(θ) cos(θ)

. Recall that SO(n) is an n(n −1)/2-dimensional
manifold; thus, SO(2) is one dimensional and SO(3) is three dimensional.

A Background Material
429
Next, we look at the mappings from one group to another.
Deﬁnition A.9 (Homomorphism). For two groups G1 and G2, a mapping f :
G1 →G2 is a homomorphism if f(g · h) = f(g) · f(h), ∀g, h ∈G1.
The operation on the left is performed in G1 while the operation on the right
is performed in G2. A homomorphism implies that the group operation can be
performed either before or after the mapping f is applied, without changing the
result. The evaluation of determinant of a matrix, as a mapping from GL(n)
to R×, is in fact, a homomorphism under the respective group operations, since
det(A1A2) = det(A1) det(A2). As a counterexample, the exponential of a matrix:
exp : M(n) →GL(n), deﬁned earlier, is not a homomorphism since exp(A + B) ̸=
exp(A) exp(B) in general.
Deﬁnition A.10 (Isomorphism). An isomorphism between two groups is a
homomorphism f : G1 →G2 that is one-to-one and onto. G1 is said to be isomor-
phic to G2, and vice versa, if there exists an isomorphism from G1 to G2.
If f : G1 →G2 and g : G2 →G3 are two isomorphisms, then (g ◦f) : G1 →G3 is
also a isomorphism. If f is an isomorphism, then f −1 is also an isomorphism.
As mentioned earlier in Sect. 3.1, the notion of equivalence relations is important
in introducing invariance of shapes. An equivalence relation ∼on a set X leads
to a quotient set X/∼whose elements are equivalence classes under that relation;
elements of an equivalence class are deemed equivalent under that relation. In
group theory, an equivalence relation occurs naturally due to the group operations.
This involves subgroups and cosets of subgroups that are deﬁned next.
Deﬁnition A.11 (Coset). Let H be a subgroup of G. For any element g ∈G,
deﬁne a left coset of H in G by gH = {g · h
**h ∈H}.
In general, the cosets are not subgroups and the only coset that is a subgroup of
G is H itself (eH). For diﬀerent elements g1 and g2, the cosets g1H and g2H will
either be identical or disjoint. They will be identical when g−1
2 g1 is an element
of H; otherwise they will be disjoint. This is similar to an equivalence relation
that partitions a set into disjoint equivalence classes. In fact, one can deﬁne an
equivalence relation using membership of these cosets: we deﬁne g1 ∼g2 if g1 ∈
g2H, i.e. g1 = g2h for some h ∈H. In the notation of equivalence classes, we have
[g] = gH. The quotient space G/∼, also denoted by G/H to emphasize the role
of H in deﬁning ∼, is the set of all left cosets of H in G. In general, the quotient
space is not a group; however, an exception results when H is a normal subgroup.
This case is presented later in this chapter. The quotient space G/H is also called
the space G modulo H, or the space that results when H is removed from G.
Similar to the left cosets, one can deﬁne the right cosets, Hg = {h · g
**h ∈
H}, and an equivalence relation based on right cosets. If G is an abelian group
(g1 · g2 = g2 · g1), then the left and the right cosets coincide. The rotation group
SO(2) is abelian, since for any O1, O2 ∈SO(2), O1O2 = O2O1. In this case the
left and the right cosets are same. On the other hand SO(3) is non-abelian, i.e.
O1, O2 ∈SO(3), O1O2 ̸= O2O1 and, consequently, the left and the right cosets are
diﬀerent. Here are some examples of the left cosets:
Example A.9. 1. We know that the rotation group H = SO(n) forms a sub-
group of the generalized linear group GL(n). The left cosets of H are given
by: A SO(n) = {AO|O ∈SO(n)}, for A ∈GL(n). The quotient space is the set

430
A Background Material
of such cosets: G/H = {A·SO(n)
**A ∈GL(n)}. A physical interpretation of this
operation is the following. Let n rows of a matrix A ∈GL(n) denote n points in
Rn. This set can be used to denote a rigid object represented by n landmarks.
For example, for n = 3 the three rows of A represent the three vertices of a
triangle in R3. Then, the coset A · SO(3) consists of all sets obtained by rigidly
rotating this triangle about the origin. By rigid rotation we mean that all the
points in A are rotated by the same rotation.
2. An interesting quotient space results when a set of lower dimensional rotations
is removed from a set of higher dimensional rotations. Let G be the rotation
group SO(n), and H be the rotation group SO(n−d) for d < n. H is a subgroup
of G with the embedding:
SO(n −d) −→
⎧
⎨
⎩
⎡
⎣
Id |
−−−
| O
⎤
⎦|O ∈SO(n −d)
⎫
⎬
⎭
The left cosets H in G are
V SO(n −d) =
⎧
⎨
⎩V
⎡
⎣
Id |
−−−
| O
⎤
⎦|O ∈SO(n −d)
⎫
⎬
⎭,
for V
∈SO(n), with the quotient space G/H being the set of left cosets
V SO(n −d) for all V ∈SO(n). How can we interpret this quotient space?
Take an n × n rotation matrix V and consider the set V SO(n −d) as deﬁned
above. Each element of this set is denoted an n×n matrix whose ﬁrst d columns
are the same as those of V and the remaining n −d columns have been rotated
by a (n−d)×(n−d) rotation matrix. Setting all these elements of V SO(n−d)
equivalent, by putting them in a left coset, results in an equivalence class of
matrices whose ﬁrst d columns are same and the remaining columns include all
possible ways of completing the ﬁrst d columns to a positively oriented orthonor-
mal basis. In other words, every W ∈V SO(n −d) is of the type W = [V1 W2],
where W2 = V2O for some O ∈SO(n −d). Hence, each coset represents a
unique n × d matrix whose columns are orthonormal, and the quotient space
SO(n)/SO(n −d) is nothing but the set of all such matrices. This set is also
called a Stiefel manifold . Summarizing this discussion, the quotient set of
SO(n) modulo SO(n −d) is a Stiefel manifold. A particular case of this situ-
ation is when d = n −1 and the resulting quotient set is a unit sphere Sn−1
in Rn.
A further extension of this idea is the quotient set O(n)/(O(d) × O(n −d))
which is called a Grassmann manifold . Here, the subgroup O(d) × O(n −d)
is embedded in O(n) according to:
⎧
⎨
⎩
⎡
⎣
O1 |
−−−
| O2
⎤
⎦|O1 ∈O(d), O2 ∈O(n −d)
⎫
⎬
⎭
and the quotient set is computed as earlier. Each element of the quotient set
represents a d-dimensional subspace in Rn and O(n)/(O(d) × O(n −d)) is the
set of all d-dimensional subspaces of Rn.

A Background Material
431
A.3 Basic Geometry of Function Spaces
We start by introducing some basic notation. A set V is called a vector space if
for all v1, v2 ∈V and α1, α2 ∈R, we have α1v1 + α2v2 ∈V . A vector space V is
called normed if there exists a function ∥·∥: V →R such that: (i) ∥v∥≥0 for all
v ∈V , (ii) ∥αv∥= |α|∥v∥for all α ∈R, v ∈V , and (iii) ∥v1 + v2∥≤∥v1∥+ ∥v2∥
for all v1, v2 ∈V . Given a normed vector space, we can make it into a metric space
by deﬁning the distance function d(v, w) = ∥v −w∥. A normed vector space V is
deﬁned to be complete if every Cauchy sequence in V converges to a limit in V .
Deﬁnition A.12 (Banach Space). A Banach space is a complete, normed
vector space.
Example A.10. 1. Lp Spaces: Let F be the set of all measurable functions deﬁned
on the interval [0, 1]. For an f ∈F, deﬁne the Lp-norm as:
∥f∥p =
 1
0
|f(x)|pdx
1/p
.
(A.1)
With the Lp norm, we can deﬁne the Lp([0, 1], R) space:
Lp([0, 1], R) = {f : [0, 1] →R|∥f∥p < ∞} .
Lp([0, 1], R) is a Banach space for all p ≥1 (see for example [72]).
2. Products of Banach Spaces: If V and W are two Banach spaces with the
norms ∥· ∥1 and ∥· ∥2, then the product space V × W is also a Banach space
with the norm ∥(v, w)∥≡∥v∥1 + ∥w∥2. (There are other ways of deﬁning norm
on the product space, although we have listed one as an example.)
If V and W are Banach spaces and T : V →W is a linear transformation, then T
is deﬁned to be bounded if there exists a real number B such that ∥T (v)∥≤B∥v∥
for all v ∈V . We denote by L(V, W) the set of all bounded linear transformations
from V to W. (It’s easy to prove that a linear transformation V →W is bounded
if and only if it is continuous.) We now observe that L(V, W) is itself a Banach
space. The norm is deﬁned by ∥T ∥= B, where B is the smallest real number with
the property that ∥T (v)∥≤B∥v∥for all v ∈V .
Suppose V and W are Banach spaces, U ⊂V is an open subset, and f : U →W
is any function. We will deﬁne the derivative of f at a point v ∈U, when it exists,
to be the bounded linear transformation A : V →W which best approximates f
near v. More precisely:
Deﬁnition A.13. Suppose V and W are Banach spaces, U ⊂V is an open subset,
f : U →W is a function, and v ∈U. If there is a bounded linear transformation
A : V →W satisfying the equation
lim
x→0
f(v + x) −f(v) −A(x)
∥x∥
= 0,
then we say that f is diﬀerentiable at v, and deﬁne A to be the derivative of
f at v. In this case, we write dfv = A.
We say that f is diﬀerentiable on the entire open subset U ⊂V if it is
diﬀerentiable at every v ∈U. Note that if f is diﬀerentiable on U, then the

432
A Background Material
assignment v →dfv deﬁnes a function f ′ : U →L(V, W). Since L(V, W) is a
Banach space, we can ask if the function f ′ is diﬀerentiable at a point v ∈U. If it
is, we call this derivative the second derivative of f at v. Note that the second
derivative of f at v is a bounded linear transformation V →L(V, W). We can
continue this process in the obvious way: If f ′ is diﬀerentiable on all of U, then
we obtain a function f ′′ : U →L(V, L(V, W)). If this process can be continued
indeﬁnitely, i.e., if f has an n-th derivative f (n) for all n, then we say that f is
smooth. If U ⊂V and X ⊂W are both open sets, a function f : U →X is
deﬁned to be a smooth isomorphism if f is smooth and bijective and f −1 is
also smooth.
A.3.1 Hilbert Manifolds and Submanifolds
Deﬁnition A.14 (Hilbert Space). A Hilbert space is a Banach space in which
the norm is deﬁned in terms of an inner product, ∥· ∥=

⟨·, ·⟩.
An interesting instance of this is the space L2([0, 1], R). This space has the following
inner product: for f1, f2 ∈L2([0, 1], R),
⟨f1, f2⟩=
 1
0
f1(x)f2(x)dx .
(A.2)
L2([0, 1], R) is a complete space under this norm, i.e. it is a Hilbert space. Shape
analysis of continuous curves and surfaces will invariably involve representing
them using functions on diﬀerent domains. Although the eventual spaces rep-
resenting shapes will become more restricted and speciﬁc, as shape-related con-
straints are applied, the starting point in many discussions will be the L2 spaces.
These are inﬁnite-dimensional vector spaces with several choices of complete or-
thonormal bases. For example, one can use the Fourier series to decompose any
f ∈L2([0, 1], R) into components according to:
f(x) = a0 +
∞

i=1
(ai cos(2πix) + bi sin(2πix)) , x ∈[0, 1] ,
where ai = ⟨f, cos(2πi·)⟩and bi = ⟨f, sin(2πi·)⟩, with the inner product given in
Eq. A.2. The convergence in the equation above is with respect to the L2 metric. If
we restrict to the ﬁrst m basis functions of each kind, sines and cosines, we obtain
a subspace Vm of L2:
Vm = 1, span{(cos(2πix), sin(2πix))|i = 1, 2, . . . , m −1} .
Vm is a ﬁnite-dimensional space that allows a more traditional multivariate calculus
and statistics for analysis, as opposed to L2([0, 1], R) whose inﬁnite dimensionality
is a major obstacle. In the later chapters, involving statistical analysis of shapes,
we will regularly restrict to such ﬁnite-dimensional subspaces of L2([0, 1], R) for
the purposes of statistical analysis. We can write the larger Hilbert space as the
direct sum: L2([0, 1], R) = Vm ⊕V ⊥
m , and the projection from L2([0, 1], R) to Vm
will be used for approximating functions. This example used the Fourier basis of
L2 but other bases may perform better in diﬀerent applications. In particular, the

A Background Material
433
use of principal component analysis (PCA) to ﬁnd empirical orthogonal functions
is quite common in functional statistics.
We will need to take the derivatives of functionals on L2 spaces. Let E :
L2([0, 1], R) →R be a real-valued functional deﬁned on square-integrable func-
tions on [0, 1]. At a point f ∈L2([0, 1], R), the directional derivative of E, in the
direction g ∈L2([0, 1], R), is given by:
∇E[g] = lim
t→0
1
t (E[f + tg] −E[f]),
t ∈R .
(A.3)
The derivative of a function E : L2([0, 1], R) →R is the same derivative deﬁned
above in Deﬁnition A.14 for functions between Banach spaces, since L2([0, 1], R)
and R are both Banach spaces.
In this textbook, we are greatly interested in manifolds and submanifolds formed
by constraining functions and forming submanifolds of L2. These are inﬁnite-
dimensional manifolds and require a slightly diﬀerent introduction than the ﬁnite-
dimensional case treated in Appendix A. We ﬁrst deﬁne the notion of a smooth
atlas and then use that to deﬁne a smooth manifold modeled on an inﬁnite-
dimensional space. Most of the material in this introduction is taken from Lang
[63] and we encourage the reader to consult that book for more details.
Deﬁnition A.15 (Smooth Atlas). Let X be a topological space. A smooth
atlas on X is a collection of pairs (Ui, φi) satisfying the following conditions:
1. Each Ui is an open subset of X and the Ui’s cover X.
2. Each φi is a homeomorphism of Ui onto the open set φi(Ui) of some Banach
space Ei, and for any i, j, the set φi(Ui ∩Uj) is open in Ei.
3. The map:
φj ◦φ−1
i
: φi(Ui ∩Uj) →φj(Ui ∩Uj)
is a smooth isomorphism for each pair i, j.
Deﬁnition A.16. A new chart (U, φ) is called compatible with a given smooth
atlas {(Ui, φi)} if the map:
φi ◦φ−1 : φ(U ∩Ui) →φi(U ∩Ui) ,
is a smooth isomorphism for all i.
Two smooth atlases are called compatible if every chart in one is compatible with
the other atlas. This notion of compatibility can be used to deﬁne an equivalence
relation between smooth atlases. Any two atlases are deﬁned to be equivalent if
they are compatible. An equivalence class of smooth atlases deﬁnes a structure on
the space X that makes it a smooth manifold.
Deﬁnition A.17 (Smooth Manifold). A topological space X with a choice of
an equivalence class of smooth atlases is called a smooth manifold.
If E used in the deﬁnition of the atlas is a Banach space, as we have assumed, then
X is called a smooth manifold modeled on a Banach space or a Banach
manifold. Similarly, if E is a Hilbert space, then X is called a Hilbert manifold.
While comparing to the similar deﬁnition for the ﬁnite-dimensional case (Deﬁni-
tion A.4), the readers may notice that there is basically only one diﬀerence. In the

434
A Background Material
previous cases E was Rn but now E is an inﬁnite-dimensional Banach or Hilbert
space.
Deﬁnition A.18 (Submanifold). Let X be a smooth manifold. A subset N ⊂X
is a submanifold if for every p ∈N, there is a coordinate chart (U, φ) for X,
with p ∈U, such that φ is a smooth isomorphism from U to V1 × V2, where V1
and V2 are open subsets of Banach spaces E1 and E2 (respectively) and where
φ(N ∩U) = V1 × {a2} for some point a2 ∈V2.
This deﬁnition is similar to Deﬁnition A.1.2 for the ﬁnite-dimensional manifolds.
Although this provides a way of deﬁning a submanifold, we realize submanifolds
in practice using the inverse images of smooth maps; recall Theorem A.1 for the
ﬁnite-dimensional case. We will state a similar result for the general submanifolds.
Let X and Y be two smooth Banach manifolds and let f : X →Y be a smooth
mapping. For a point y ∈Y , consider the pullback set f −1(y) ⊂X. The map f is
called transversal over y if the diﬀerential of f at every point of f −1(y) is onto.
(For ﬁnite-dimensional cases, we used the term regular value for such a g.) This
leads to a more useful characterization of a submanifold of X.
Theorem A.2. If f is transversal over y, then f −1(y) is a submanifold of X.
Let us take some examples of submanifolds of L2.
Example A.11. Inﬁnite-Dimensional Sphere: L2([0, 1]) and R are Hilbert
spaces, and a function f : L2([0, 1]) →R deﬁned by f(g) = ⟨g, g⟩= ∥g∥is a
smooth map. Its derivative is given by f ′(g)h = 2 ⟨g, h⟩and for any g ̸= 0, this
derivative is onto. Therefore, f is a transversal function over 1 ∈R and the
pullback set of {1},
S∞≡{g ∈L2([0, 1])|∥g∥= 1}
(A.4)
is a submanifold of L2. We will call this set an inﬁnite-dimensional sphere
or a hypersphere. Geometrically, it is rather similar to its ﬁnite-dimensional
counterpart Sn that we have seen earlier. The center of this sphere is given by the
zero function. For any g ∈S∞, the tangent space Tg(S∞) is given by:
Tg(S∞) = {h ∈L2([0, 1])| ⟨g, h⟩= 0} .
(See Example A.2 for a derivation.) It is a Riemannian manifold with the L2 inner
product on the tangent spaces.

Appendix B
The Dynamic Programming Algorithm
As described in earlier chapters, diﬀeomorphisms of a certain type are synony-
mous with re-parameterizations of functions and curves. In order to make shape
analysis invariant to re-parameterizations, an optimization problem on the space
of diﬀeomorphisms ΓI needs to be solved. With that motivation, we consider sit-
uations where there is a need for ﬁnding optimal diﬀeomorphisms of [0, 1], with
the optimality deﬁned using a certain type of cost function. A key requirement
for the ensuing solution to be applicable is that the objective function is additive
over [0, 1]. This is true for most of the scenarios considered in this textbook. Then,
there exists an eﬃcient numerical procedure for approximating the solution to
such problems. This solution is based on the idea of dynamic programming (DP),
a class of algorithms described in great detail in [14], and is summarized here next.
B.1 Theoretical Setup
First we set up a typical optimization problem that we will face. Let f, g : [0, 1] →R
be two given functions and we want to solve for:
ˆγ = argmin
γ∈ΓI
 1
0
|f(t) −g(γ(t))|2dt .
(B.1)
In the integrand, γ is a function that matches the point g(γ(t)) with the point f(t),
and ˆγ is the optimal matching function. We can solve a discrete approximation of
this problem using DP. As mentioned above, a necessary condition for applying DP
to such problems is that the cost function is additive in time t. We will conveniently
view γ as a graph from (0, 0) to (1, 1) in R2 such that the slope of this graph is
always strictly between 0 and 90 degrees. We can verify that the cost function in
Eqn. B.1 is indeed additive over the graph. To decompose the large problem into
several subproblems, deﬁne a partial cost function:
E(s, t; γ) =
 t
s
|f(τ) −g(γ(τ))|2dτ
(B.2)
so that our original cost function is simply E(0, 1; γ). Our goal is to ﬁnd an optimal
path from (0, 0) to (1, 1)in R2, corresponding to (t, γ(t)), that minimizes this cost
function.
© Springer-Verlag New York 2016
A. Srivastava, E.P. Klassen, Functional and Shape Data Analysis,
Springer Series in Statistics, DOI 10.1007/978-1-4939-4020-2
435

436
B The Dynamic Programming Algorithm
B.2 Computer Implementation
In order to use a numerical approach, we will replace the domain [0, 1]×[0, 1] with
a ﬁnite grid and restrict our search to that grid. Although it is not necessary, we
will simplify the setup by using the uniform partition Gn = {0, 1/n, 2/n, . . ., (n −
1)/n, 1} of [0, 1] and form a grid Gn × Gn. We will search over the set of all
restrictions of γ to this grid; any such restriction is now a piecewise linear path
which is never vertical or horizontal (see the left panel of Fig. B.1). The total
cost associated with the path is now a sum of the costs associated with its linear
segments. On an n× n grid, there are only a ﬁnite number of paths, more so when
we impose any additional constraint. However, this number grows exponentially
with n, and we cannot possibly search over all possible paths in an exhaustive
enumerative fashion. The DP algorithm, however, ﬁnds the optimal path in O(n2)
time!
Denote a point on the grid (i/n, j/n) by (i, j). We will impose an additional
constraint by bounding the slope of the path at any point. As a result, there are
only certain nodes that are allowed to go to (i, j); denote by Nij be the set of
nodes that are allowed to go to (i, j). For instance:
Nij = {(k/n, l/n)|0 ≤k < i, 0 ≤l < j} .
is a valid set. In practice, one often restricts to a smaller subset to seek a com-
putational speed up. The net eﬀect is that the number of possible values for the
slope along the path are further restricted (see the middle panel of Fig. B.1). Let
L(k, l; i, j) denote a straight line joining the nodes (k/n, l/n) and (i/n, j/n); for
(k/n, l/n) ∈Nij this is a line with slope strictly between 0 and 90 degrees. This
sets up the iterative optimization problem:
(ˆk/n, ˆl/n) =
argmin
(k/n,l/n)∈Nij
[E(k/n, l/n; L(k, l; i, j)) + H(k/n, l/n)] ,
(B.3)
with E as deﬁned in Eq. B.2. Deﬁne the minimum energy of reaching the point
(i, j), in an iterative fashion as:
H(i/n, j/n) = E(ˆk/n, ˆl/n; L(ˆk, ˆl; i, j)) + H(ˆk/n, ˆl/n) ,
with H(0, 0) = 0 .
(0,j)
(0,0)
(0,0)
(i,j)
(i,0)
(0,0)
(1,1)
k /n
l /n
j /n
i /n
Fig. B.1 Left: an example of a γ function restricted to a ﬁnite graph. Right: an illustration of
some nodes that are allowed to go to the (i/n, j/n) point on the graph

B The Dynamic Programming Algorithm
437
This subproblem is solved sequentially for each node (i, j), starting from (1/n, 1/n)
and increasing i, j until one reaches the node (1, 1). Tracing the path that results
in the optimal energy H(1, 1) provides a discrete version of the optimal γ.
Algorithm 58 (Dynamic Programming Algorithm).
E = zeros(n, n); E(1, :) = 1; E(:, 1) = 1; E(1, 1) = 0;
for i = 2 : n
for j = 2 : n
for Num = 1:size(N ,1)
k = i - N (Num,1);
l = j - N (Num,2);
if (k> 0 & l > 0)
Hc(Num) = H(k,l) + FunctionE(f,g,k,i,l,j);
else
Hc(Num) = ∞;
end
H(i,j) = min(Hc);
end
end
end
Here N is a list of sites used to deﬁne Nij. Typically, N is a two-column matrix of
type {(1, 1); (1, 2); (2, 1); (1, 3); (2, 3); (3, 1); (3, 2); . . ., } depending on the number
of preceding neighbors included in the implementation. FunctionE is a subrou-
tine that computes E(k/n, l/n; L(k, l; i, j)), the partial cost function speciﬁed in
Eq. B.2:
Algorithm 59 (FunctionE).
Input: f, g, k, i, l, j, n
Output: E
m = size(g,2);
x = [k:1:i];
y = (x-k)*m + l;
idx = round(y*m/n);
v = q2(:,idx);
E = norm(q1(:,x) - v,’fro’)ˆ2/n;
Algorithm 58 stores the values of (ˆk(i,j), ˆl(i,j)), the optimal incoming node, for
each (i, j) and uses it to reconstruct the optimal ˆγ. One starts from (n, n) and
goes to (ˆk(n,n), ˆl(n,n)), the optimal incoming nodes at (n, n). Then, the next step
is go to optimal nodes coming into (ˆk(n,n), ˆl(n,n)) and so on. This piecewise-linear
path is ˆγ.
Example B.1. To illustrate the dynamic programming algorithm, we consider the
problem of matching two functions:
f(x) = e
−(x−μ1)2
2σ2
and
g(x) = e
−(x−μ2)2
2σ2
,
for a certain ﬁxed σ. In Fig. B.2, we show results of using the dynamic programming
algorithm for ﬁnding the optimal γ that minimizes the cost function given in
Eq. B.1. In the ﬁrst row, we have μ1 = μ2 but the separation is increasing in the

438
B The Dynamic Programming Algorithm
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
−0.2
−0.1
0
0.1
0.2
0.3
0.4
0.5
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
−0.2
−0.1
0
0.1
0.2
0.3
0.4
0.5
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
−0.2
−0.1
0
0.1
0.2
0.3
0.4
0.5
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
−0.2
−0.1
0
0.1
0.2
0.3
0.4
0.5
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
−0.2
−0.1
0
0.1
0.2
0.3
0.4
0.5
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
−0.2
−0.1
0
0.1
0.2
0.3
0.4
0.5
Fig. B.2 Matching of functions using dynamic programming. In each row the left panel shows
two function f and g. The middle row shows the optimal ˆγ that minimizes the cost function in
Eq. B.1, drawn over the partial cost function H. The right panel shows the functions f and g(ˆγ)
with the resulting correspondences
lower rows. The two original functions for each case are shown in the left panel. The
middle panel of each row shows the estimated optimal ˆγ drawn over the partial
cost function H. The darker intensity in the image denotes lower values of H.
Notice that the optimal diﬀeomorphism stays close to the darker regions in the
image. Finally, in the right panel of each row, we see the functions f(t) and g(ˆγ(t))
and some line correspondences between the two functions. In case of μ1 = μ2, the
optimal γ is naturally identity, but when the two means are pulled further apart,
the optimal matching ˆγ also moves further away from identity to account for this
mismatch.

References
1. M.F. Abdelkader, W. Abd-Almageed, A. Srivastava, R. Chellappa,
Gesture and action
recognition via modeling trajectories on shape manifolds. Comput. Vis. Image Underst. J.
115(3), 439–455 (2011)
2. A. Abou-Elailah, F. Dufaux, J. Farah, M. Cagnazzo, A. Srivastava, B. Pesquet-Popescu,
Fusion of global and local motion estimation using foreground objects for distributed video
coding. IEEE Trans. Circ. Syst. Video Technol. 25(6), 973–987 (2015)
3. M. Adams, T. Ratiu, R. Schmid, The lie group structure of diﬀeomorphism groups and
invertible fourier integral operators, with applications.
In: Inﬁnite-Dimensional Groups
with Applications, ed. by V. Kac (Springer, New York, 1985)
4. S. Amari, Diﬀerential Geometric Methods in Statistics. Lecture Notes in Statistics, Vol. 28
(Springer, New York, 1985)
5. S. Amari, H. Nagaoka, Methods of Information Geometry, Mathematical Monographs Vol-
ume 191 (Oxford University Press, Oxford, 2000)
6. S.-I. Amari, O.E. Barndorﬀ-Nielsen, R.E. Kass, S.L. Lauritzen, C.R. Rao,
Diﬀerential
Geometry in Statistical Inference, Monograph Series (Institute of Mathematical Statistics,
Hayward, 1987)
7. Y. Amit, U. Grenander, M. Piccioni,
Structural image restoration through deformable
templates. J. Am. Stat. Assoc. 86(414), 376–387 (1991)
8. B. Ben Amor, J. Su, A. Srivastava,
Action recognition using rate-invariant analysis of
skeletal shape trajectories. IEEE Trans. Pattern Anal. Mach. Intell. 38(1), 1–13 (2016)
9. K.S. Arun, T.S. Huang, S.D. Blostein, Least-squares ﬁtting of two 3-d point sets. IEEE
Trans. Pattern Anal. Mach. Intell. PAMI-9(5), 698–700 (1987)
10. M. Bauer, M. Bruveris, P. Harms, J. Mller-Andersen, Second order elastic metrics on the
shape space of curves. arXiv preprint, arXiv:1507.08816 (2015)
11. M. Bauer, M. Bruveris, S. Marsland, P.W. Michor, Constructing reparameterization invari-
ant metrics on spaces of plane curves. Diﬀer. Geom. Appl. 34, 139–165 (2014)
12. M. Bauer, M. Bruveris, P.W. Michor, R-transforms for sobolev h2-metrics on spaces of
plane curves. Geom. Imaging Comput. 1(1), 1–56 (2014)
13. M. Bauer, M. Eslitzbichler, M. Grasmair,
Landmark-gauided elastic shape analysis of
human character motions. arXiv, arXiv:1502.07666 (2015)
14. D.P. Bertsekas, Dynamic Programming and Optimal Control (Athena Scientiﬁc, Belmont,
1995)
15. P.J. Besl, N.D. McKay, A method for registration of 3-D shapes. IEEE TPAMI 14(2),
239–256 (1992)
16. A. Bhattacharya, On a measure of divergence between two statistical populations deﬁned
by their probability distributions. Bull. Calcutta Math. Soc. 35, 99–109 (1943)
17. A. Bhattacharya, R. Bhattacharya, Nonparametric Inference on Manifolds: with Applica-
tions to Shape Spaces (Cambridge University Press, Cambridge, 2012)
18. F.L. Bookstein, Morphometric Tools for Landmark Data: Geometry and Biology (Cam-
bridge University Press, Cambridge, 1991)
19. W.M. Boothby,
An Introduction to Diﬀerential Manifolds and Riemannian Geometry
(Academic Press, New York, 1986)
© Springer-Verlag New York 2016
A. Srivastava, E.P. Klassen, Functional and Shape Data Analysis,
Springer Series in Statistics, DOI 10.1007/978-1-4939-4020-2
439

440
References
20. M. Bruveris,
Optimal reparameterizations in square root velocity framework.
arXiv,
arXiv:1507.02728 (2015)
21. D. Bryner, E. Klassen, H. Le, A. Srivastava, 2d aﬃne and projective shape analysis. IEEE
Trans. Pattern Anal. Mach. Intell. 36(5), 998–1011 (2014)
22. D. Bryner, A. Srivastava, Q. Huynh,
Elastic shape models for improving segmentation
of object boundaries in synthetic aperture sonar images.
Comput. Vis. Image Underst.
117(12), 1695–1710 (2013)
23. M.P. Do Carmo, Diﬀerential Geometry of Curves and Surfaces (Prentice-Hall, Englewood
Cliﬀs, 1976)
24. N.N. ˇCencov, Statistical Decision Rules and Optimal Inferences, volume 53 of Translations
of Mathematical Monographs (AMS, Providence, 1982)
25. Y. Chen, G. Medioni, Object modeling by registration of multiple range images. Image
Vis. Comput. 10(3), 145–155 (1992)
26. T.F. Cootes, C.J. Taylor, D.H. Cooper, J. Graham, Active shape models: Their training
and application. Comput. Vis. Image Underst. 61(1), 38–59 (1995)
27. H. Drira, B. Ben Amor, A. Srivastava, M. Daoudi, R. Slama, 3d face recognition under
expressions, occlusions, and pose variations. IEEE Trans. Pattern Anal. Mach. Intell. 35(9),
2270–2283 (2013)
28. I.L. Dryden, K.V. Mardia, Statistical Shape Analysis (Wiley, London, 1998)
29. D.G. Ebin, J. Marsden, Groups of diﬀeomorphisms and the motion of an incompressible
ﬂuid. Ann. Math. Second Ser. 92(1), 102–163 (1970)
30. B. Efron, Deﬁning the curvature of a statistical problem (with applications to second order
eﬃciency). Ann. Stat. 3, 1189–1242 (1975)
31. J.K. Ghosh, R.V. Ramamoorthi, Bayesian Nonparametrics. Springer Series in Statistics
(Springer, New York, 2003)
32. U. Grenander, General Pattern Theory (Oxford University Press, Oxford, 1993)
33. U. Grenander, M.I. Miller,
Computational anatomy: An emerging discipline.
Q. Appl.
Math. LVI(4), 617–694 (1998)
34. U. Grenander, M.I. Miller, Pattern Theory: From Representation to Inference
(Oxford
University Press, Oxford, 2007)
35. U. Grenander, M.I. Miller, A. Srivastava, Hilbert-schmidt bounds on matrix lie groups for
atr. IEEE Trans. Pattern Anal. Mach. Intell. 20(8), 790–800 (1998)
36. S. Helgason, Diﬀerential Geometry, Lie Groups and Symmetric Spaces (Academic Press,
New York, 1978)
37. T. Hofmann, J.M. Buhmann, Pairwise data clustering by deterministic annealing. IEEE
Trans. Pattern Anal. Mach. Intell. 19(1), 1–14 (1997)
38. L. Horvath, P. Kkozska, Inference for Functional Data with Applications. Springer Series
in Statistics (Springer, New York, 2012)
39. W. Huang, K.A. Gallivan, A. Srivastava, P.-A. Absil, Riemannian optimization for regis-
tration of curves in elastic shape analysis. J. Math. Imag. Vis. 54(3), 320–343 (2016)
40. A.K. Jain, R.C. Dubes, Algorithms for Clustering Data (Prentice-Hall, Englewood Cliﬀs,
1988)
41. I.T. Jolliﬀe,
Principal Component Analysis
Springer Series in Statistics (Springer,
New York, 2002)
42. S.H. Joshi, E. Klassen, A. Srivastava, I.H. Jermyn,
A novel representation for eﬃcient
computation of geodesics between n-dimensional curves. In: IEEE CVPR, 2007
43. S.H. Joshi, E. Klassen, A. Srivastava, I.H. Jermyn, Removing shape-preserving transforma-
tions in square-root elastic (SRE) framework for shape analysis of curves. In: EMMCVPR,
LNCS 4679, ed. by A. Yuille et al., pp. 387–398 (2007)
44. P.E. Jupp, J.T. Kent, Fitting smooth paths to spherical data. J. R. Stat. Soc. Ser. C (Appl.
Stat.) 36(1), 34–46 (1987)
45. V.G. Kac,
Inﬁnite-Dimensional Lie Algebras, 3rd edn.
(Cambridge University Press,
Cambridge, 1990)
46. H. Karcher, Riemann center of mass and molliﬁer smoothing. Commun. Pure Appl. Math.
30, 509–541 (1977)
47. R.E. Kass, P.W. Vos, Geometric Foundations of Asymptotic Inference (Wiley, London,
1997)
48. D. Kaziska, A. Srivastava, The karcher mean of a class of symmetric distributions on a unit
circle. Stat. Probab. Lett. 78, 1314–1316 (2008)
49. D.G. Kendall, D. Barden, T.K. Carne, H. Le, Shape and Shape Theory (Wiley, London,
1999)

References
441
50. D.G. Kendall, Shape manifolds, procrustean metrics and complex projective spaces. Bull.
Lond. Math. Soc. 16, 81–121 (1984)
51. J. Kent, K. Mardia, Procrustes methods for projective shape. Syst. Biol. Stat. Bioinf. 37–40
(2007)
52. J. Kent, K. Mardia,
A geometric approach to projective shape and the cross ratio.
Biometrika 99(4), 833–849 (2012)
53. E. Klassen, A. Srivastava, Geodesics between 3d closed curves using path-straightening. In:
European Conference on Computer Vision, LNCS 3951, ed. by A. Leonardia, H. Bischof,
A. Pinz (2006)
54. E. Klassen, A. Srivastava, W. Mio, S. Joshi, Analysis of planar shapes using geodesic paths
on shape spaces. IEEE Pattern Anal. Mach. Intell. 26(3), 372–383 (2004)
55. A. Kneip, T. Gasser, Statistical tools to analyze data representing a sample of curves. Ann.
Stat. 20, 1266–1305 (1992)
56. A. Kume, I.L. Dryden, H. Le, Shape-space smoothing splines for planar landmark data.
Biometrika 94, 513–528 (2007)
57. S. Kurtek, A. Srivastava, E. Klassen, Z. Ding, Statistical modeling of curves using shapes
and related features. J. Am. Stat. Assoc. 107(499), 1152–1165 (2012)
58. S. Kurtek, J. Su, C. Grimm, M. Vaughan, R.T. Sowell, A. Srivastava, Statistical analysis
of manual segmentations of structures in medical images. Comput. Vis. Image Underst.
117(9), 1036–1050 (2013)
59. S. Kurtek, Q. Xie, A. Srivastava, Analysis of juggling data: Alignment, extraction, and
modeling of juggling cycles. Electron. J. Stat. 8, 1865–1873 (2014)
60. J. Laborde, D. Robinson, A. Srivastava, E. Klassen, J. Zhang, Rna alignment in the joint
sequence-structure space using elastic shape analysis. J. Nucleic Acids Res. 41(11, e114)
(2013)
61. H. Laga, S. Kurtek, A. Srivastava, S.J. Miklavcic, Landmark-free statistical analysis of the
shape of plant leaves. J. Theor. Biol. 363, 41–52 (2014)
62. S. Lahiri, D. Robinson, E. Klassen, Precise matching of PL curves in RN in square root
velocity framework. Geom. Imaging Comput. 2(3), 133–186 (2015)
63. S. Lang, Diﬀerential and Riemannian Manifolds, Third Edition (Springer: Graduate Texts
in Mathematics, New York, 1995)
64. S. Lang, Fundamentals of Diﬀerential Geometry (Springer, New York, 1999)
65. S. Lang, Algebra (Springer, New York, 2002)
66. H. Le, D.G. Kendall, The Riemannian structure of euclidean shape spaces: A novel envi-
ronment for statistics. Ann. Stat. 21(3), 1225–1271 (1993)
67. X. Leng, H.G. Mueller, Time ordering of gene coexpression. Biostatistics 7(4), 569–584
(2006)
68. W. Liu, A Riemannian framework for annotated curve analysis. PhD thesis, Florida State
University, August 2011
69. W. Liu, A. Srivastava, E. Klassen, Joint shape and texture analysis of objects boundaries
in images using a Riemannian approach. In: Asilomar Conference on Signals, Systems,
and Computers, October, 2008
70. W. Liu, A. Srivastava, J. Zheng, A mathematical framework for protein structure compar-
ison. PLOS Comput. Biol. 7(2), 1–10 (2011)
71. X. Liu, H.G. Mueller, Functional convex averaging and synchronization for time-warped
random curves. J. Am. Stat. Assoc. 99, 687–699 (2004)
72. D.G. Luenberger, Optimization by Vector Space Methods (Wiley, New York, 1969)
73. M. Mani, S. Kurtek, A. Srivastava, C. Barillot, A comprehensive riemannian framework for
analysis of white matter ﬁber tracts. In: Proc. of International Symposium on Biomedical
Imaging (ISBI), 2010
74. K. Mardia, J. Kent, A new representation for projective shape. In: Proceedings in Inter-
disciplinary Statistics and Bioinformatics, pp. 75–78 (2006)
75. K.V. Mardia, P. Jupp, Directional Statistics (2nd edition) (Wiley, London, 2000)
76. J.S. Marron, J.O. Ramsay, L.M. Sangalli, A. Srivastava, Statistics of time warpings and
phase variations. Electron. J. Stat. 8(2), 1697–1702 (2014)
77. P.W. Michor, D. Mumford, Riemannian geometries on spaces of plane curves. J. Eur. Math.
Soc. 8, 1–48 (2006)
78. J.W. Milnor,
Topology from the Diﬀerentiable Viewpoint
(Princeton University Press,
Princeton, 1997)
79. W. Mio, A. Srivastava, S. Joshi, On shape of plane elastic curves. Int. J. Comput. Vis.
73(3), 307–324 (2007)

442
References
80. F. Mokhtarian, S. Abbasi, J. Kittler, Eﬃcient and robust shape retrieval by shape content
through curvature scale space. In: Proceedings of First International Conference on Image
Database and MultiSearch, 1996
81. A. Mottini, Axon Morphology Analysis: From Image Processing to Modelling. PhD thesis,
University of Nice Sophia Antipolis, 2014
82. A. Mottini, X. Descombes, F. Besse, From curves to trees: A tree-like shapes distance using
the elastic shape analysis framework. Neuroinformatics 13(2), 175–191 (2015)
83. J. Munkres, Topology (Prentice-Hall, Englewood Cliﬀs, 2000)
84. S. Osher, R. Fedkiw, Level Set Methods and Dynamic Implicit Surfaces (Springer, New
York, 2003)
85. S.J. Osher, R.P. Fedkiw, Level Set Methods and Dynamic Implicit Surfaces (Springer, New
York, 2000)
86. J.C. Owen, F.R. Moore,
Swainson’s thrushes in migratory disposition exhibit reduced
immune function. J. Ethol. 26, 383–388 (2008)
87. R.S. Palais, Morse theory on Hilbert manifolds. Topology 2, 299–340 (1963)
88. V. Patrangenaru, R. Bhattacharya, Large sample theory of intrinsic and extrinsic sample
means on manifolds. Ann. Stat. 31(1), 1–29 (2003)
89. B. Pelletier, Non-parametric regression estimation on closed riemannian manifolds. Non-
parametric Stat. 18(1), 57–67 (2006)
90. J.O. Ramsay, X. Li, Curve registration. J. R. Stat. Soc. Ser. B 60, 351–363 (1998)
91. J.O. Ramsay, B.W. Silverman, Functional Data Analysis, Second Edition. Springer Series
in Statistics (Springer, New York, 2005)
92. C.R. Rao, Information and accuracy attainable in the estimation of statistical parameters.
Bull. Calcutta Math. Soc. 37, 81–91 (1945)
93. C.P. Robert, G. Casella,
Monte Carlo Statistical Methods.
Springer Text in Statistics
(Springer, New York, 1999)
94. D. Robinson, Functional Analysis and Partial Matching in the Square Root Velocity Frame-
work. PhD thesis, Florida State University, August 2012
95. K. Rose, Deterministic annealing for clustering, compression, classiﬁcation, regression, and
related optimization problems. Proc. IEEE 86(11), 2210–2239 (1998)
96. W. Rudin, Functional Analysis, 2nd Edition (McGraw-Hill Higher Education, New York,
1991)
97. C. Samir, A. Srivastava, M. Daoudi, Three-dimensional face recognition using shapes of
facial curves. IEEE Trans. Pattern Anal. Mach. Intell. 28(11), 1858–1863 (2006)
98. C. Samir, A. Srivastava, M. Daoudi, E. Klassen, An intrinsic framework for analysis of
facial surfaces. Int. J. Comput. Vis. 82(1), 80–95 (2009)
99. C. Samir, A. Srivastava, M. Daoudi, S. Kurtek, On analyzing symmetry of objects using
elastic deformations. In: 4th International Conference on Computer Vision Theory and
Applications, pp. 194–200, 2009
100. T.B. Sebastian, P.N. Klein, B.B. Kimia, On aligning curves. IEEE Trans. Pattern Anal.
Mach. Intell. 25(1), 116–125 (2003)
101. B.W. Silverman, Density Estimation for Statistics and Data Analysis (Chapman and Hall,
London, 1985)
102. C.G. Small, The Statistical Theory of Shape (Springer, New York, 1996)
103. A. Srivastava, A bayesian approach to geometric subspace estimation. IEEE Trans. Signal
Process. 48(5), 1390–1400 (2000)
104. A. Srivastava, I.H. Jermyn, Looking for shapes in two-dimensional, cluttered point cloud.
IEEE Trans. Pattern Anal. Mach. Intell. 31(9), 1616–1629 (2009)
105. A. Srivastava, S. Joshi, W. Mio, X. Liu, Statistical shape analysis: Clustering, learning,
and testing. IEEE Trans. Pattern Anal. Mach. Intell. 27(4), 590–602 (2005)
106. A. Srivastava, E. Klassen, S.H. Joshi, I.H. Jermyn,
Shape analysis of elastic curves in
Euclidean spaces. IEEE Trans. PAMI 33, 1415–1428 (2011)
107. A. Srivastava, C. Samir, S.H. Joshi, M. Daoudi, Elastic shape models for face analysis using
curvilinear coordinates. J. Math. Imaging Vis. 33(2), 253–265 (2009)
108. A. Srivastava, W. Wu, S. Kurtek, E. Klassen, J.S. Marron, Registration of functional data
using ﬁsher-rao metric. arXiv, arXiv:1103.3817 (2011)
109. J. Su, I.L. Dryden, E. Klassen, H. Le, A. Srivastava, Fitting optimal curves to time-indexed,
noisy observations on nonlinear manifolds. J. Image Vis. Comput. 30(6–7), 428–442 (2012)
110. J. Su, F. Huﬀer, A. Srivastava, Detection, classiﬁcation and estimation of shapes in 2d and
3d point clouds. Comput. Stat. Data Anal. 58, 227–241 (2013)

References
443
111. J. Su, S. Kurtek, E. Klassen, A. Srivastava, Statistical analysis of trajectories on riemannian
manifolds: Bird migration, hurricane tracking, and video surveillance. Ann. Appl. Stat. 8(1),
530–552 (2014)
112. G. Sundaramoorthi, A. Mennucci, S. Soatto, A.J. Yezzi, A new geometric metric in the
space of curves, and applications to tracking deforming objects by prediction and ﬁltering.
SIAM J. Imaging Sci. 4(1), 109–145 (2011)
113. R. Tang, H.G. Mueller, Pairwise curve synchronization for functional data.
Biometrika
95(4), 875–889 (2008)
114. D.W. Thompson, On Growth and Form: The Complete Revised Edition (Dover, Cambridge,
1992)
115. A. Trouve, Diﬀemorphisms groups and pattern matching in image analysis. Int. J. Comput.
Vis. 28(3), 213–221 (1998)
116. D. Tucker, W. Wu, A. Srivastava, Analysis of proteomics data: Phase amplitude separation
using an extended ﬁsher-rao metric. Electron. J. Stat. 8, 1724–1733 (2014)
117. J.D. Tucker, W. Wu, A. Srivastava, Generative models for functional data using phase and
amplitude separation. Comput. Stat. Data Anal. 61, 50–66 (2013)
118. A. Veeraraghavan, A. Srivastava, A.K. Roy-Chowdhury, R. Chellappa, Rate-invariant recog-
nition of humans and their activities. IEEE Trans. Image Process. 8(6), 1326–1339 (2009)
119. P.W. Vosm, R.E. Kass,
Geometrical Foundations of Asymptotic Inference
(Wiley-
Interscience, New York, 1997)
120. F.W. Warner, Foundations of Diﬀerentiable Manifolds and Lie Groups
(Springer, New
York, 1994)
121. L.C. White, Shape Analysis of Curves in Higher Dimensions. PhD thesis, Florida State
University, April 2013
122. W. Wu, A. Srivastava,
Analysis of spike train data: Alignment and comparisons using
extended the ﬁsher-rao metric. Electron. J. Stat. 8(2), 1786–1792 (2014)
123. Q. Xie, S. Kurtek, A. Srivastava, Analysis of aneurisk65 data: Elastic shape registration of
curves. Electron. J. Stat. 8, 1920–1929 (2014)
124. L. Younes, Computable elastic distance between shapes. SIAM J. Appl. Math. 58, 565–586
(1998)
125. L. Younes,
Optimal matching between shapes via elastic deformations.
J. Image Vis.
Comput. 17(5/6), 381–389 (1999)
126. L. Younes, P.W. Michor, J. Shah, D. Mumford, R. Lincei, A metric on shape space with
explicit geodesics. Matematica E Applicazioni 19(1), 25–57 (2008)
127. Z. Zhang, D. Pati, A. Srivastava, Bayesian clustering of shapes. J. Stat. Plann. Inference
166, 171–186 (2015)
128. Z. Zhang, J. Su, E. Klassen, H. Le, A. Srivastava, Video-based action recognition using
rate-invariant analysis of covariancetrajectories. arXiv, arXiv:1503.06699 (2015)

Index
Symbols
GL(n), 427
O(n), 42
P GL(n), 414
SE(n), 56
SL(n), 428
SO(n), 428
GA, aﬃne group, 391
A
active shape models (ASMs), 29
aﬃne standardization, 394
amplitude distance, functions, 107
amplitude, function, 95
anatomy, 7
B
bending metric, planar curves, 136
bending-only metric, Euclidean curves, 354
Berekeley growth data, 279
bioinformatics, 8
biometry, 6
biosignals, 5
C
Center of orbit, 275
concentration parameter, 250
coset, 429
covariant derivative, vector ﬁeld, 191
covariant integral, vector ﬁeld, 191
covariantly constant, vector ﬁeld, 192
covariantly linear, vector ﬁeld, 192
Curve registration problem, 366
D
deformable template, 35
deformation-based shape analysis, 35
derivations, 423
diﬀeomorphism, 420
diﬀerentiable manifold, 419
diﬀerential of map, 425
direction function, 350
direction function, planar curves, 131
directional derivative, 424
dynamic programming algorithm, 437
Dynamic Programming algorithm, SRSF, 100
E
elastic deformation, planar shapes, 145
Elastic FPCA, 298
elastic metric, Euclidean curves, 351
elastic metric, planar curves, 144, 146
equivalence class, 40
equivalence relation, 40
exp map, Sn, 47
exp map, O(n), 47
exp map, Euclidean space, 47
exponential map, 46
extrinsic statistics, extrinsic mean241, 241
extrinsic covariance, 241
extrinsic statistics
principal geodesic curve, 241
F
Fisher density
sphere, 250
Fisher information matrix, 114
Fisher-Rao metric, functions, 105
Fisher-Rao metric, nonparametric, 114
Fisher-Rao metric, parametric, 114
ﬂat manifold, 52
forensic applications, 9
FPCA with phase variability, L2 norm, 296
function registration problem, 99
function registration, groupwise, 85
function registration, energy, 98
© Springer-Verlag New York 2016
A. Srivastava, E.P. Klassen, Functional and Shape Data Analysis,
Springer Series in Statistics, DOI 10.1007/978-1-4939-4020-2
445

446
Index
function registration, growth curves, 102
function registration, invariance property, 87
function registration, inverse symmetry, 87
function registration, mass spectrometry, 102
function registration, pairwise, 85
function registration, pinching eﬀect, 88
Functional PCA (FPCA), 79
G
general morphometrics, 9
geodesic, 43
geodesic
elastic pre-shape space, Euclidean curves,
357
elastic pre-shape space, planar closed
curves, 197
elastic pre-shape space, planar curves, 140
elastic shape space, Euclidean curves, 358
elastic shape space, planar closed curves,
208
elastic shape space, planar curves, 151
non-elastic pre-shape space, Euclidean
curves, 354
non-elastic pre-shape space, planar closed
curves, 185
non-elastic pre-shape space, planar curves,
138
non-elastic shape space, Euclidean curves,
355
non-elastic shape space, planar closed
curves, 204
Geodesic distance,
probability density
functions 116
Geodesic path, probability density functions
116
geodesic path, 192
geodesic, S2, 44
geodesic, O(n), 46
geodesic, Euclidean space, 44
geodesic, hyperbolic metric, 44
geodesics, Sn, 45
Grassmann manifold, 430
group, 427
group action on manifolds, 53
group action, free, 68
growth curves, 5
H
half density, 116
Hellinger distance, 117
Hilbert Space, geometry 78
Hilbert sphere, 82
homomorphism, 429
hyperbolic metric, 42
I
ICP algorithm, 26
intrinsic statistics, 236
isometry, 43
isomorphism, 429
K
Karcher covariance, 240
Karcher mean, 237
open curves in Rn, 362
Karcher mean
sample, 237
Karcher mean algorithm, amplitude, function,
274
Karcher mean, amplitude, function, 274
Kendall’s shape analysis, 30
Kullback-Leibler divergence, 115
L
landmark shape space, 57
landmark space Ln,k, 56
level-set methods, 33
Lie group, 53
M
manifold, 418
mass spectrometry, 5
N
non-elastic metric, Euclidean curves, 353
O
orbit of group action, 54
orthogonal group, 426
P
Palais metric, ﬁrst order, 191
parallel transformation, deformations, 222
parallel translation, tangent vector, 193
parallel transport, 50
parallel transport, Sn, 51
parallel transport, SO(n), 52
parallel transport, Euclidean space, 50
path-straightening algorithm, C c
2 , 201
path-straightening algorithm, general, 196
path-straightening method, 179
Penalized alignment, functions, 285
penalized registration, 89
phase distance, functions, 111
pre-shape space
Euclidean curve, elastic—C2, 356
Euclidean curve, non-elastic—C1, 353
planar curve, elastic—C2, 134
planar curve, non-elastic—C1, 133
planar, closed curve, elastic—C c
2 , 170
planar, closed curve, non-elastic—C c
1 , 170
Principal Component Analysis (PCA), 23
principal geodesic curve, 240
Procrustes Alignment, Rotation, 59

Index
447
Procrustes Rotation, 25
Procrustes scaling, 25
Procrustes translation, 25
Projection on nonlinear manifold, 178
projective group, 392
projective space, 41
Q
quotient space, 429
R
rate invariance, 386
re-parameterization group, curves, 126
relative phase, functions, 96
Riemannian manifold, 42
Riemannian metric, 42
root-mean squared distance (RMSD)
alignment, 24
rotation group, 54
roughness penalty, 284
S
scale and rotation group, 55
scale, rotation, and translation group, 56
scaled-Euclidean metric, 64
scaling group, 54, 427
section under group action, 60
section, orthogonal, 60
section, orthogonal, global, 61
shape and orientation space, 388
shape and scale space, 387
shape space
Euclidean curves, elastic S2, 357
Euclidean curves, non-elastic S1, 354
planar closed curves, elastic S c
2 , 177
planar closed curves, non-elastic S c
1 , 175
planar curves, elastic S2, 143
planar curves, non-elastic S1, 142
shape, scale and orientation space, 387
shooting algorithm, C c
2 , 204
shooting method, 179
Two-sphere, S2, 183
shooting method
pre-shape space, non-elastic closed curves
C c
1 , 187
SRF (Square-root function), 389
SRSF representation, 91
SRVF representation, 170
SRVF, Euclidean curves, 350
stereographic projection, 419
Stiefel manifold, 430
subgroup, 428
Subgroup of ΓS, S1, 210
Subgroup of ΓS, Moebius transformation, 211
submanifold, 425
T
tangent bundle, 48
tangent space, 421
tangent vector, 421
trajectory shape distance, 408
translation group, 54, 427
TSRVF, 406
TSRVF distance, 407
V
Von Mises Density, 243
W
Warping Group, 83
warping, area-preserving, 112
warping, norm-preserving, 112
warping, value-preserving, 112
wrapped normal density
unit sphere, 251
wrapped normal density
unit circle, 245

